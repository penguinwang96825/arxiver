{"title": "A Numerical Reasoning Question Answering System with Fine-grained\n  Retriever and the Ensemble of Multiple Generators for FinQA", "abstract": "The numerical reasoning in the financial domain -- performing quantitative\nanalysis and summarizing the information from financial reports -- can greatly\nincrease business efficiency and reduce costs of billions of dollars. Here, we\npropose a numerical reasoning question answering system to answer numerical\nreasoning questions among financial text and table data sources, consisting of\na retriever module, a generator module, and an ensemble module. Specifically,\nin the retriever module, in addition to retrieving the whole row data, we\ninnovatively design a cell retriever that retrieves the gold cells to avoid\nbringing unrelated and similar cells in the same row to the inputs of the\ngenerator module. In the generator module, we utilize multiple generators to\nproduce programs, which are operation steps to answer the question. Finally, in\nthe ensemble module, we integrate multiple programs to choose the best program\nas the output of our system. In the final private test set in FinQA\nCompetition, our system obtains 69.79 execution accuracy.", "published": "2022-06-17 01:55:29", "link": "http://arxiv.org/abs/2206.08506v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Quantitative and Qualitative Analysis of Suicide Ideation Detection\n  using Deep Learning", "abstract": "For preventing youth suicide, social media platforms have received much\nattention from researchers. A few researches apply machine learning, or deep\nlearning-based text classification approaches to classify social media posts\ncontaining suicidality risk. This paper replicated competitive social\nmedia-based suicidality detection/prediction models. We evaluated the\nfeasibility of detecting suicidal ideation using multiple datasets and\ndifferent state-of-the-art deep learning models, RNN-, CNN-, and\nAttention-based models. Using two suicidality evaluation datasets, we evaluated\n28 combinations of 7 input embeddings with 4 commonly used deep learning models\nand 5 pretrained language models in quantitative and qualitative ways. Our\nreplication study confirms that deep learning works well for social media-based\nsuicidality detection in general, but it highly depends on the dataset's\nquality.", "published": "2022-06-17 10:23:37", "link": "http://arxiv.org/abs/2206.08673v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CookDial: A dataset for task-oriented dialogs grounded in procedural\n  documents", "abstract": "This work presents a new dialog dataset, CookDial, that facilitates research\non task-oriented dialog systems with procedural knowledge understanding. The\ncorpus contains 260 human-to-human task-oriented dialogs in which an agent,\ngiven a recipe document, guides the user to cook a dish. Dialogs in CookDial\nexhibit two unique features: (i) procedural alignment between the dialog flow\nand supporting document; (ii) complex agent decision-making that involves\nsegmenting long sentences, paraphrasing hard instructions and resolving\ncoreference in the dialog context. In addition, we identify three challenging\n(sub)tasks in the assumed task-oriented dialog system: (1) User Question\nUnderstanding, (2) Agent Action Frame Prediction, and (3) Agent Response\nGeneration. For each of these tasks, we develop a neural baseline model, which\nwe evaluate on the CookDial dataset. We publicly release the CookDial dataset,\ncomprising rich annotations of both dialogs and recipe documents, to stimulate\nfurther research on domain-specific document-grounded dialog systems.", "published": "2022-06-17 12:23:53", "link": "http://arxiv.org/abs/2206.08723v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Crowdsourcing Relative Rankings of Multi-Word Expressions: Experts\n  versus Non-Experts", "abstract": "In this study we investigate to which degree experts and non-experts agree on\nquestions of difficulty in a crowdsourcing experiment. We ask non-experts\n(second language learners of Swedish) and two groups of experts (teachers of\nSwedish as a second/foreign language and CEFR experts) to rank multi-word\nexpressions in a crowdsourcing experiment. We find that the resulting rankings\nby all the three tested groups correlate to a very high degree, which suggests\nthat judgments produced in a comparative setting are not influenced by\nprofessional insights into Swedish as a second language.", "published": "2022-06-17 12:23:55", "link": "http://arxiv.org/abs/2206.08724v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The ITU Faroese Pairs Dataset", "abstract": "This article documents a dataset of sentence pairs between Faroese and\nDanish, produced at ITU Copenhagen. The data covers tranlsation from both\nsource languages, and is intended for use as training data for machine\ntranslation systems in this language pair.", "published": "2022-06-17 12:27:20", "link": "http://arxiv.org/abs/2206.08727v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-supervised speech unit discovery from articulatory and acoustic\n  features using VQ-VAE", "abstract": "The human perception system is often assumed to recruit motor knowledge when\nprocessing auditory speech inputs. Using articulatory modeling and deep\nlearning, this study examines how this articulatory information can be used for\ndiscovering speech units in a self-supervised setting. We used vector-quantized\nvariational autoencoders (VQ-VAE) to learn discrete representations from\narticulatory and acoustic speech data. In line with the zero-resource paradigm,\nan ABX test was then used to investigate how the extracted representations\nencode phonetically relevant properties. Experiments were conducted on three\ndifferent corpora in English and French. We found that articulatory information\nrather organises the latent representations in terms of place of articulation\nwhereas the speech acoustics mainly structure the latent space in terms of\nmanner of articulation. We show that an optimal fusion of the two modalities\ncan lead to a joint representation of these phonetic dimensions more accurate\nthan each modality considered individually. Since articulatory information is\nusually not available in a practical situation, we finally investigate the\nbenefit it provides when inferred from the speech acoustics in a\nself-supervised manner.", "published": "2022-06-17 14:04:24", "link": "http://arxiv.org/abs/2206.08790v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language with Vision: a Study on Grounded Word and Sentence Embeddings", "abstract": "Grounding language in vision is an active field of research seeking to\nconstruct cognitively plausible word and sentence representations by\nincorporating perceptual knowledge from vision into text-based representations.\nDespite many attempts at language grounding, achieving an optimal equilibrium\nbetween textual representations of the language and our embodied experiences\nremains an open field. Some common concerns are the following. Is visual\ngrounding advantageous for abstract words, or is its effectiveness restricted\nto concrete words? What is the optimal way of bridging the gap between text and\nvision? To what extent is perceptual knowledge from images advantageous for\nacquiring high-quality embeddings? Leveraging the current advances in machine\nlearning and natural language processing, the present study addresses these\nquestions by proposing a simple yet very effective computational grounding\nmodel for pre-trained word embeddings. Our model effectively balances the\ninterplay between language and vision by aligning textual embeddings with\nvisual information while simultaneously preserving the distributional\nstatistics that characterize word usage in text corpora. By applying a learned\nalignment, we are able to indirectly ground unseen words including abstract\nwords. A series of evaluations on a range of behavioural datasets shows that\nvisual grounding is beneficial not only for concrete words but also for\nabstract words, lending support to the indirect theory of abstract concepts.\nMoreover, our approach offers advantages for contextualized embeddings, such as\nthose generated by BERT, but only when trained on corpora of modest,\ncognitively plausible sizes. Code and grounded embeddings for English are\navailable at https://github.com/Hazel1994/Visually_Grounded_Word_Embeddings_2.", "published": "2022-06-17 15:04:05", "link": "http://arxiv.org/abs/2206.08823v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "niksss at HinglishEval: Language-agnostic BERT-based Contextual\n  Embeddings with Catboost for Quality Evaluation of the Low-Resource\n  Synthetically Generated Code-Mixed Hinglish Text", "abstract": "This paper describes the system description for the HinglishEval challenge at\nINLG 2022. The goal of this task was to investigate the factors influencing the\nquality of the code-mixed text generation system. The task was divided into two\nsubtasks, quality rating prediction and annotators disagreement prediction of\nthe synthetic Hinglish dataset. We attempted to solve these tasks using\nsentence-level embeddings, which are obtained from mean pooling the\ncontextualized word embeddings for all input tokens in our text. We\nexperimented with various classifiers on top of the embeddings produced for\nrespective tasks. Our best-performing system ranked 1st on subtask B and 3rd on\nsubtask A.", "published": "2022-06-17 17:36:03", "link": "http://arxiv.org/abs/2206.08910v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Correction of Human Translations", "abstract": "We introduce translation error correction (TEC), the task of automatically\ncorrecting human-generated translations. Imperfections in machine translations\n(MT) have long motivated systems for improving translations post-hoc with\nautomatic post-editing. In contrast, little attention has been devoted to the\nproblem of automatically correcting human translations, despite the intuition\nthat humans make distinct errors that machines would be well-suited to assist\nwith, from typos to inconsistencies in translation conventions. To investigate\nthis, we build and release the Aced corpus with three TEC datasets. We show\nthat human errors in TEC exhibit a more diverse range of errors and far fewer\ntranslation fluency errors than the MT errors in automatic post-editing\ndatasets, suggesting the need for dedicated TEC models that are specialized to\ncorrect human errors. We show that pre-training instead on synthetic errors\nbased on human errors improves TEC F-score by as much as 5.1 points. We\nconducted a human-in-the-loop user study with nine professional translation\neditors and found that the assistance of our TEC system led them to produce\nsignificantly higher quality revised translations.", "published": "2022-06-17 07:30:55", "link": "http://arxiv.org/abs/2206.08593v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Understanding Aesthetics with Language: A Photo Critique Dataset for\n  Aesthetic Assessment", "abstract": "Computational inference of aesthetics is an ill-defined task due to its\nsubjective nature. Many datasets have been proposed to tackle the problem by\nproviding pairs of images and aesthetic scores based on human ratings. However,\nhumans are better at expressing their opinion, taste, and emotions by means of\nlanguage rather than summarizing them in a single number. In fact, photo\ncritiques provide much richer information as they reveal how and why users rate\nthe aesthetics of visual stimuli. In this regard, we propose the Reddit Photo\nCritique Dataset (RPCD), which contains tuples of image and photo critiques.\nRPCD consists of 74K images and 220K comments and is collected from a Reddit\ncommunity used by hobbyists and professional photographers to improve their\nphotography skills by leveraging constructive community feedback. The proposed\ndataset differs from previous aesthetics datasets mainly in three aspects,\nnamely (i) the large scale of the dataset and the extension of the comments\ncriticizing different aspects of the image, (ii) it contains mostly UltraHD\nimages, and (iii) it can easily be extended to new data as it is collected\nthrough an automatic pipeline. To the best of our knowledge, in this work, we\npropose the first attempt to estimate the aesthetic quality of visual stimuli\nfrom the critiques. To this end, we exploit the polarity of the sentiment of\ncriticism as an indicator of aesthetic judgment. We demonstrate how sentiment\npolarity correlates positively with the aesthetic judgment available for two\naesthetic assessment benchmarks. Finally, we experiment with several models by\nusing the sentiment scores as a target for ranking images. Dataset and\nbaselines are available (https://github.com/mediatechnologycenter/aestheval).", "published": "2022-06-17 08:16:20", "link": "http://arxiv.org/abs/2206.08614v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "BITS Pilani at HinglishEval: Quality Evaluation for Code-Mixed Hinglish\n  Text Using Transformers", "abstract": "Code-Mixed text data consists of sentences having words or phrases from more\nthan one language. Most multi-lingual communities worldwide communicate using\nmultiple languages, with English usually one of them. Hinglish is a Code-Mixed\ntext composed of Hindi and English but written in Roman script. This paper aims\nto determine the factors influencing the quality of Code-Mixed text data\ngenerated by the system. For the HinglishEval task, the proposed model uses\nmulti-lingual BERT to find the similarity between synthetically generated and\nhuman-generated sentences to predict the quality of synthetically generated\nHinglish sentences.", "published": "2022-06-17 10:36:50", "link": "http://arxiv.org/abs/2206.08680v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Statistical and Neural Methods for Cross-lingual Entity Label Mapping in\n  Knowledge Graphs", "abstract": "Knowledge bases such as Wikidata amass vast amounts of named entity\ninformation, such as multilingual labels, which can be extremely useful for\nvarious multilingual and cross-lingual applications. However, such labels are\nnot guaranteed to match across languages from an information consistency\nstandpoint, greatly compromising their usefulness for fields such as machine\ntranslation. In this work, we investigate the application of word and sentence\nalignment techniques coupled with a matching algorithm to align cross-lingual\nentity labels extracted from Wikidata in 10 languages. Our results indicate\nthat mapping between Wikidata's main labels stands to be considerably improved\n(up to $20$ points in F1-score) by any of the employed methods. We show how\nmethods relying on sentence embeddings outperform all others, even across\ndifferent scripts. We believe the application of such techniques to measure the\nsimilarity of label pairs, coupled with a knowledge base rich in high-quality\nentity labels, to be an excellent asset to machine translation.", "published": "2022-06-17 11:57:08", "link": "http://arxiv.org/abs/2206.08709v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Unified Evaluation of Textual Backdoor Learning: Frameworks and\n  Benchmarks", "abstract": "Textual backdoor attacks are a kind of practical threat to NLP systems. By\ninjecting a backdoor in the training phase, the adversary could control model\npredictions via predefined triggers. As various attack and defense models have\nbeen proposed, it is of great significance to perform rigorous evaluations.\nHowever, we highlight two issues in previous backdoor learning evaluations: (1)\nThe differences between real-world scenarios (e.g. releasing poisoned datasets\nor models) are neglected, and we argue that each scenario has its own\nconstraints and concerns, thus requires specific evaluation protocols; (2) The\nevaluation metrics only consider whether the attacks could flip the models'\npredictions on poisoned samples and retain performances on benign samples, but\nignore that poisoned samples should also be stealthy and semantic-preserving.\nTo address these issues, we categorize existing works into three practical\nscenarios in which attackers release datasets, pre-trained models, and\nfine-tuned models respectively, then discuss their unique evaluation\nmethodologies. On metrics, to completely evaluate poisoned samples, we use\ngrammar error increase and perplexity difference for stealthiness, along with\ntext similarity for validity. After formalizing the frameworks, we develop an\nopen-source toolkit OpenBackdoor to foster the implementations and evaluations\nof textual backdoor learning. With this toolkit, we perform extensive\nexperiments to benchmark attack and defense models under the suggested\nparadigm. To facilitate the underexplored defenses against poisoned datasets,\nwe further propose CUBE, a simple yet strong clustering-based defense baseline.\nWe hope that our frameworks and benchmarks could serve as the cornerstones for\nfuture model development and evaluations.", "published": "2022-06-17 02:29:23", "link": "http://arxiv.org/abs/2206.08514v2", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "VLMbench: A Compositional Benchmark for Vision-and-Language Manipulation", "abstract": "Benefiting from language flexibility and compositionality, humans naturally\nintend to use language to command an embodied agent for complex tasks such as\nnavigation and object manipulation. In this work, we aim to fill the blank of\nthe last mile of embodied agents -- object manipulation by following human\nguidance, e.g., \"move the red mug next to the box while keeping it upright.\" To\nthis end, we introduce an Automatic Manipulation Solver (AMSolver) system and\nbuild a Vision-and-Language Manipulation benchmark (VLMbench) based on it,\ncontaining various language instructions on categorized robotic manipulation\ntasks. Specifically, modular rule-based task templates are created to\nautomatically generate robot demonstrations with language instructions,\nconsisting of diverse object shapes and appearances, action types, and motion\nconstraints. We also develop a keypoint-based model 6D-CLIPort to deal with\nmulti-view observations and language input and output a sequence of 6 degrees\nof freedom (DoF) actions. We hope the new simulator and benchmark will\nfacilitate future research on language-guided robotic manipulation.", "published": "2022-06-17 03:07:18", "link": "http://arxiv.org/abs/2206.08522v2", "categories": ["cs.RO", "cs.CL", "cs.CV"], "primary_category": "cs.RO"}
{"title": "BridgeTower: Building Bridges Between Encoders in Vision-Language\n  Representation Learning", "abstract": "Vision-Language (VL) models with the Two-Tower architecture have dominated\nvisual-language representation learning in recent years. Current VL models\neither use lightweight uni-modal encoders and learn to extract, align and fuse\nboth modalities simultaneously in a deep cross-modal encoder, or feed the\nlast-layer uni-modal representations from the deep pre-trained uni-modal\nencoders into the top cross-modal encoder. Both approaches potentially restrict\nvision-language representation learning and limit model performance. In this\npaper, we propose BridgeTower, which introduces multiple bridge layers that\nbuild a connection between the top layers of uni-modal encoders and each layer\nof the cross-modal encoder. This enables effective bottom-up cross-modal\nalignment and fusion between visual and textual representations of different\nsemantic levels of pre-trained uni-modal encoders in the cross-modal encoder.\nPre-trained with only 4M images, BridgeTower achieves state-of-the-art\nperformance on various downstream vision-language tasks. In particular, on the\nVQAv2 test-std set, BridgeTower achieves an accuracy of 78.73%, outperforming\nthe previous state-of-the-art model METER by 1.09% with the same pre-training\ndata and almost negligible additional parameters and computational costs.\nNotably, when further scaling the model, BridgeTower achieves an accuracy of\n81.15%, surpassing models that are pre-trained on orders-of-magnitude larger\ndatasets. Code and checkpoints are available at\nhttps://github.com/microsoft/BridgeTower.", "published": "2022-06-17 09:42:35", "link": "http://arxiv.org/abs/2206.08657v6", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "What can Speech and Language Tell us About the Working Alliance in\n  Psychotherapy", "abstract": "We are interested in the problem of conversational analysis and its\napplication to the health domain. Cognitive Behavioral Therapy is a structured\napproach in psychotherapy, allowing the therapist to help the patient to\nidentify and modify the malicious thoughts, behavior, or actions. This\ncooperative effort can be evaluated using the Working Alliance Inventory\nObserver-rated Shortened - a 12 items inventory covering task, goal, and\nrelationship - which has a relevant influence on therapeutic outcomes. In this\nwork, we investigate the relation between this alliance inventory and the\nspoken conversations (sessions) between the patient and the psychotherapist. We\nhave delivered eight weeks of e-therapy, collected their audio and video call\nsessions, and manually transcribed them. The spoken conversations have been\nannotated and evaluated with WAI ratings by professional therapists. We have\ninvestigated speech and language features and their association with WAI items.\nThe feature types include turn dynamics, lexical entrainment, and\nconversational descriptors extracted from the speech and language signals. Our\nfindings provide strong evidence that a subset of these features are strong\nindicators of working alliance. To the best of our knowledge, this is the first\nand a novel study to exploit speech and language for characterising working\nalliance.", "published": "2022-06-17 15:32:34", "link": "http://arxiv.org/abs/2206.08835v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "MineDojo: Building Open-Ended Embodied Agents with Internet-Scale\n  Knowledge", "abstract": "Autonomous agents have made great strides in specialist domains like Atari\ngames and Go. However, they typically learn tabula rasa in isolated\nenvironments with limited and manually conceived objectives, thus failing to\ngeneralize across a wide spectrum of tasks and capabilities. Inspired by how\nhumans continually learn and adapt in the open world, we advocate a trinity of\ningredients for building generalist agents: 1) an environment that supports a\nmultitude of tasks and goals, 2) a large-scale database of multimodal\nknowledge, and 3) a flexible and scalable agent architecture. We introduce\nMineDojo, a new framework built on the popular Minecraft game that features a\nsimulation suite with thousands of diverse open-ended tasks and an\ninternet-scale knowledge base with Minecraft videos, tutorials, wiki pages, and\nforum discussions. Using MineDojo's data, we propose a novel agent learning\nalgorithm that leverages large pre-trained video-language models as a learned\nreward function. Our agent is able to solve a variety of open-ended tasks\nspecified in free-form language without any manually designed dense shaping\nreward. We open-source the simulation suite, knowledge bases, algorithm\nimplementation, and pretrained models (https://minedojo.org) to promote\nresearch towards the goal of generally capable embodied agents.", "published": "2022-06-17 15:53:05", "link": "http://arxiv.org/abs/2206.08853v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Making first order linear logic a generating grammar", "abstract": "It is known that different categorial grammars have surface representation in\na fragment of first order multiplicative linear logic (MLL1). We show that the\nfragment of interest is equivalent to the recently introduced extended tensor\ntype calculus (ETTC). ETTC is a calculus of specific typed terms, which\nrepresent tuples of strings, more precisely bipartite graphs decorated with\nstrings. Types are derived from linear logic formulas, and rules correspond to\nconcrete operations on these string-labeled graphs, so that they can be\nconveniently visualized. This provides the above mentioned fragment of MLL1\nthat is relevant for language modeling not only with some alternative syntax\nand intuitive geometric representation, but also with an intrinsic deductive\nsystem, which has been absent.\n  In this work we consider a non-trivial notationally enriched variation of the\npreviously introduced ETTC, which allows more concise and transparent\ncomputations. We present both a cut-free sequent calculus and a natural\ndeduction formalism.", "published": "2022-06-17 18:11:34", "link": "http://arxiv.org/abs/2206.08955v5", "categories": ["cs.CL", "cs.LO", "math.LO"], "primary_category": "cs.CL"}
{"title": "Simultaneous Speech Extraction for Multiple Target Speakers under the\n  Meeting Scenarios", "abstract": "The common target speech separation directly estimate the target source,\nignoring the interrelationship between different speakers at each frame. We\npropose a multiple-target speech separation model (MTSS) to simultaneously\nextract each speaker's voice from the mixed speech rather than just optimally\nestimating the target source. Moreover, we propose a speaker diarization (SD)\naware MTSS system (SD-MTSS), which consists of a SD module and MTSS module. By\nexploiting the TSVAD decision and the estimated mask, our SD-MTSS model can\nextract the speech signal of each speaker concurrently in a conversational\nrecording without additional enrollment audio in advance. Experimental results\nshow that our MTSS model achieves 1.38dB SDR, 1.34dB SI-SDR, and 0.13 PESQ\nimprovements over the baseline on the WSJ0-2mix-extr dataset, respectively. The\nSD-MTSS system makes 19.2% relative speaker dependent character error rate\n(CER) reduction on the Alimeeting dataset.", "published": "2022-06-17 03:16:13", "link": "http://arxiv.org/abs/2206.08525v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "NU-Wave 2: A General Neural Audio Upsampling Model for Various Sampling\n  Rates", "abstract": "Conventionally, audio super-resolution models fixed the initial and the\ntarget sampling rates, which necessitate the model to be trained for each pair\nof sampling rates. We introduce NU-Wave 2, a diffusion model for neural audio\nupsampling that enables the generation of 48 kHz audio signals from inputs of\nvarious sampling rates with a single model. Based on the architecture of\nNU-Wave, NU-Wave 2 uses short-time Fourier convolution (STFC) to generate\nharmonics to resolve the main failure modes of NU-Wave, and incorporates\nbandwidth spectral feature transform (BSFT) to condition the bandwidths of\ninputs in the frequency domain. We experimentally demonstrate that NU-Wave 2\nproduces high-resolution audio regardless of the sampling rate of input while\nrequiring fewer parameters than other models. The official code and the audio\nsamples are available at https://mindslab-ai.github.io/nuwave2.", "published": "2022-06-17 04:40:14", "link": "http://arxiv.org/abs/2206.08545v2", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Avoid Overfitting User Specific Information in Federated Keyword\n  Spotting", "abstract": "Keyword spotting (KWS) aims to discriminate a specific wake-up word from\nother signals precisely and efficiently for different users. Recent works\nutilize various deep networks to train KWS models with all users' speech data\ncentralized without considering data privacy. Federated KWS (FedKWS) could\nserve as a solution without directly sharing users' data. However, the small\namount of data, different user habits, and various accents could lead to fatal\nproblems, e.g., overfitting or weight divergence. Hence, we propose several\nstrategies to encourage the model not to overfit user-specific information in\nFedKWS. Specifically, we first propose an adversarial learning strategy, which\nupdates the downloaded global model against an overfitted local model and\nexplicitly encourages the global model to capture user-invariant information.\nFurthermore, we propose an adaptive local training strategy, letting clients\nwith more training data and more uniform class distributions undertake more\nlocal update steps. Equivalently, this strategy could weaken the negative\nimpacts of those users whose data is less qualified. Our proposed FedKWS-UI\ncould explicitly and implicitly learn user-invariant information in FedKWS.\nAbundant experimental results on federated Google Speech Commands verify the\neffectiveness of FedKWS-UI.", "published": "2022-06-17 16:05:35", "link": "http://arxiv.org/abs/2206.08864v1", "categories": ["cs.LG", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
