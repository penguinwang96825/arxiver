{"title": "Joint Space Neural Probabilistic Language Model for Statistical Machine\n  Translation", "abstract": "A neural probabilistic language model (NPLM) provides an idea to achieve the\nbetter perplexity than n-gram language model and their smoothed language\nmodels. This paper investigates application area in bilingual NLP, specifically\nStatistical Machine Translation (SMT). We focus on the perspectives that NPLM\nhas potential to open the possibility to complement potentially `huge'\nmonolingual resources into the `resource-constraint' bilingual resources. We\nintroduce an ngram-HMM language model as NPLM using the non-parametric Bayesian\nconstruction. In order to facilitate the application to various tasks, we\npropose the joint space model of ngram-HMM language model. We show an\nexperiment of system combination in the area of SMT. One discovery was that our\ntreatment of noise improved the results 0.20 BLEU points if NPLM is trained in\nrelatively small corpus, in our case 500,000 sentence pairs, which is often the\ncase due to the long training time of NPLM.", "published": "2013-01-16 07:56:20", "link": "http://arxiv.org/abs/1301.3614v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Estimation of Word Representations in Vector Space", "abstract": "We propose two novel model architectures for computing continuous vector\nrepresentations of words from very large data sets. The quality of these\nrepresentations is measured in a word similarity task, and the results are\ncompared to the previously best performing techniques based on different types\nof neural networks. We observe large improvements in accuracy at much lower\ncomputational cost, i.e. it takes less than a day to learn high quality word\nvectors from a 1.6 billion words data set. Furthermore, we show that these\nvectors provide state-of-the-art performance on our test set for measuring\nsyntactic and semantic word similarities.", "published": "2013-01-16 18:24:43", "link": "http://arxiv.org/abs/1301.3781v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Rhetorical Analysis Approach to Natural Language Processing", "abstract": "The goal of this research was to find a way to extend the capabilities of\ncomputers through the processing of language in a more human way, and present\napplications which demonstrate the power of this method. This research presents\na novel approach, Rhetorical Analysis, to solving problems in Natural Language\nProcessing (NLP). The main benefit of Rhetorical Analysis, as opposed to\nprevious approaches, is that it does not require the accumulation of large sets\nof training data, but can be used to solve a multitude of problems within the\nfield of NLP. The NLP problems investigated with Rhetorical Analysis were the\nAuthor Identification problem - predicting the author of a piece of text based\non its rhetorical strategies, Election Prediction - predicting the winner of a\npresidential candidate's re-election campaign based on rhetorical strategies\nwithin that president's inaugural address, Natural Language Generation - having\na computer produce text containing rhetorical strategies, and Document\nSummarization. The results of this research indicate that an Author\nIdentification system based on Rhetorical Analysis could predict the correct\nauthor 100% of the time, that a re-election predictor based on Rhetorical\nAnalysis could predict the correct winner of a re-election campaign 55% of the\ntime, that a Natural Language Generation system based on Rhetorical Analysis\ncould output text with up to 87.3% similarity to Shakespeare in style, and that\na Document Summarization system based on Rhetorical Analysis could extract\nhighly relevant sentences. Overall, this study demonstrated that Rhetorical\nAnalysis could be a useful approach to solving problems in NLP.", "published": "2013-01-16 01:42:53", "link": "http://arxiv.org/abs/1301.3547v1", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Learning New Facts From Knowledge Bases With Neural Tensor Networks and\n  Semantic Word Vectors", "abstract": "Knowledge bases provide applications with the benefit of easily accessible,\nsystematic relational knowledge but often suffer in practice from their\nincompleteness and lack of knowledge of new entities and relations. Much work\nhas focused on building or extending them by finding patterns in large\nunannotated text corpora. In contrast, here we mainly aim to complete a\nknowledge base by predicting additional true relationships between entities,\nbased on generalizations that can be discerned in the given knowledgebase. We\nintroduce a neural tensor network (NTN) model which predicts new relationship\nentries that can be added to the database. This model can be improved by\ninitializing entity representations with word vectors learned in an\nunsupervised fashion from text, and when doing this, existing relations can\neven be queried for entities that were not present in the database. Our model\ngeneralizes and outperforms existing models for this problem, and can classify\nunseen relationships in WordNet with an accuracy of 75.8%.", "published": "2013-01-16 08:05:35", "link": "http://arxiv.org/abs/1301.3618v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Two SVDs produce more focal deep learning representations", "abstract": "A key characteristic of work on deep learning and neural networks in general\nis that it relies on representations of the input that support generalization,\nrobust inference, domain adaptation and other desirable functionalities. Much\nrecent progress in the field has focused on efficient and effective methods for\ncomputing representations. In this paper, we propose an alternative method that\nis more efficient than prior work and produces representations that have a\nproperty we call focality -- a property we hypothesize to be important for\nneural network representations. The method consists of a simple application of\ntwo consecutive SVDs and is inspired by Anandkumar (2012).", "published": "2013-01-16 08:37:39", "link": "http://arxiv.org/abs/1301.3627v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Feature Learning in Deep Neural Networks - Studies on Speech Recognition\n  Tasks", "abstract": "Recent studies have shown that deep neural networks (DNNs) perform\nsignificantly better than shallow networks and Gaussian mixture models (GMMs)\non large vocabulary speech recognition tasks. In this paper, we argue that the\nimproved accuracy achieved by the DNNs is the result of their ability to\nextract discriminative internal representations that are robust to the many\nsources of variability in speech signals. We show that these representations\nbecome increasingly insensitive to small perturbations in the input with\nincreasing network depth, which leads to better speech recognition performance\nwith deeper networks. We also show that DNNs cannot extrapolate to test samples\nthat are substantially different from the training examples. If the training\ndata are sufficiently representative, however, internal features learned by the\nDNN are relatively stable with respect to speaker differences, bandwidth\ndifferences, and environment distortion. This enables DNN-based recognizers to\nperform as well or better than state-of-the-art systems based on GMMs or\nshallow networks without the need for explicit model adaptation or feature\nnormalization.", "published": "2013-01-16 07:23:19", "link": "http://arxiv.org/abs/1301.3605v3", "categories": ["cs.LG", "cs.CL", "cs.NE", "eess.AS"], "primary_category": "cs.LG"}
