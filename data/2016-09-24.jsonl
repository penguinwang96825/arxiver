{"title": "Distilling an Ensemble of Greedy Dependency Parsers into One MST Parser", "abstract": "We introduce two first-order graph-based dependency parsers achieving a new\nstate of the art. The first is a consensus parser built from an ensemble of\nindependently trained greedy LSTM transition-based parsers with different\nrandom initializations. We cast this approach as minimum Bayes risk decoding\n(under the Hamming cost) and argue that weaker consensus within the ensemble is\na useful signal of difficulty or ambiguity. The second parser is a\n\"distillation\" of the ensemble into a single model. We train the distillation\nparser using a structured hinge loss objective with a novel cost that\nincorporates ensemble uncertainty estimates for each possible attachment,\nthereby avoiding the intractable cross-entropy computations required by\napplying standard distillation objectives to problems with structured outputs.\nThe first-order distillation parser matches or surpasses the state of the art\non English, Chinese, and German.", "published": "2016-09-24 02:58:26", "link": "http://arxiv.org/abs/1609.07561v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Character-level Convolutional Neural Network for Distinguishing\n  Similar Languages and Dialects", "abstract": "Discriminating between closely-related language varieties is considered a\nchallenging and important task. This paper describes our submission to the DSL\n2016 shared-task, which included two sub-tasks: one on discriminating similar\nlanguages and one on identifying Arabic dialects. We developed a\ncharacter-level neural network for this task. Given a sequence of characters,\nour model embeds each character in vector space, runs the sequence through\nmultiple convolutions with different filter widths, and pools the convolutional\nrepresentations to obtain a hidden vector representation of the text that is\nused for predicting the language or dialect. We primarily focused on the Arabic\ndialect identification task and obtained an F1 score of 0.4834, ranking 6th out\nof 18 participants. We also analyze errors made by our system on the Arabic\ndata in some detail, and point to challenges such an approach is faced with.", "published": "2016-09-24 04:02:13", "link": "http://arxiv.org/abs/1609.07568v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "An Investigation of Recurrent Neural Architectures for Drug Name\n  Recognition", "abstract": "Drug name recognition (DNR) is an essential step in the Pharmacovigilance\n(PV) pipeline. DNR aims to find drug name mentions in unstructured biomedical\ntexts and classify them into predefined categories. State-of-the-art DNR\napproaches heavily rely on hand crafted features and domain specific resources\nwhich are difficult to collect and tune. For this reason, this paper\ninvestigates the effectiveness of contemporary recurrent neural architectures -\nthe Elman and Jordan networks and the bidirectional LSTM with CRF decoding - at\nperforming DNR straight from the text. The experimental results achieved on the\nauthoritative SemEval-2013 Task 9.1 benchmarks show that the bidirectional\nLSTM-CRF ranks closely to highly-dedicated, hand-crafted systems.", "published": "2016-09-24 08:45:17", "link": "http://arxiv.org/abs/1609.07585v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The distribution of information content in English sentences", "abstract": "Sentence is a basic linguistic unit, however, little is known about how\ninformation content is distributed across different positions of a sentence.\nBased on authentic language data of English, the present study calculated the\nentropy and other entropy-related statistics for different sentence positions.\nThe statistics indicate a three-step staircase-shaped distribution pattern,\nwith entropy in the initial position lower than the medial positions (positions\nother than the initial and final), the medial positions lower than the final\nposition and the medial positions showing no significant difference. The\nresults suggest that: (1) the hypotheses of Constant Entropy Rate and Uniform\nInformation Density do not hold for the sentence-medial positions; (2) the\ncontext of a word in a sentence should not be simply defined as all the words\npreceding it in the same sentence; and (3) the contextual information content\nin a sentence does not accumulate incrementally but follows a pattern of \"the\nwhole is greater than the sum of parts\".", "published": "2016-09-24 23:36:31", "link": "http://arxiv.org/abs/1609.07681v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Existence of Hierarchies and Human's Pursuit of Top Hierarchy Lead to\n  Power Law", "abstract": "The power law is ubiquitous in natural and social phenomena, and is\nconsidered as a universal relationship between the frequency and its rank for\ndiverse social systems. However, a general model is still lacking to interpret\nwhy these seemingly unrelated systems share great similarity. Through a\ndetailed analysis of natural language texts and simulation experiments based on\nthe proposed 'Hierarchical Selection Model', we found that the existence of\nhierarchies and human's pursuit of top hierarchy lead to the power law.\nFurther, the power law is a statistical and emergent performance of\nhierarchies, and it is the universality of hierarchies that contributes to the\nubiquity of the power law.", "published": "2016-09-24 23:22:50", "link": "http://arxiv.org/abs/1609.07680v1", "categories": ["cs.CL", "physics.soc-ph"], "primary_category": "cs.CL"}
