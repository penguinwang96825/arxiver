{"title": "Improving Grammatical Error Correction via Pre-Training a Copy-Augmented\n  Architecture with Unlabeled Data", "abstract": "Neural machine translation systems have become state-of-the-art approaches\nfor Grammatical Error Correction (GEC) task. In this paper, we propose a\ncopy-augmented architecture for the GEC task by copying the unchanged words\nfrom the source sentence to the target sentence. Since the GEC suffers from not\nhaving enough labeled training data to achieve high accuracy. We pre-train the\ncopy-augmented architecture with a denoising auto-encoder using the unlabeled\nOne Billion Benchmark and make comparisons between the fully pre-trained model\nand a partially pre-trained model. It is the first time copying words from the\nsource context and fully pre-training a sequence to sequence model are\nexperimented on the GEC task. Moreover, We add token-level and sentence-level\nmulti-task learning for the GEC task. The evaluation results on the CoNLL-2014\ntest set show that our approach outperforms all recently published\nstate-of-the-art results by a large margin. The code and pre-trained models are\nreleased at https://github.com/zhawe01/fairseq-gec.", "published": "2019-03-01 03:08:03", "link": "http://arxiv.org/abs/1903.00138v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Chinese-Japanese Unsupervised Neural Machine Translation Using\n  Sub-character Level Information", "abstract": "Unsupervised neural machine translation (UNMT) requires only monolingual data\nof similar language pairs during training and can produce bi-directional\ntranslation models with relatively good performance on alphabetic languages\n(Lample et al., 2018). However, no research has been done to logographic\nlanguage pairs. This study focuses on Chinese-Japanese UNMT trained by data\ncontaining sub-character (ideograph or stroke) level information which is\ndecomposed from character level data. BLEU scores of both character and\nsub-character level systems were compared against each other and the results\nshowed that despite the effectiveness of UNMT on character level data,\nsub-character level data could further enhance the performance, in which the\nstroke level system outperformed the ideograph level system.", "published": "2019-03-01 03:57:15", "link": "http://arxiv.org/abs/1903.00149v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning\n  Over Paragraphs", "abstract": "Reading comprehension has recently seen rapid progress, with systems matching\nhumans on the most popular datasets for the task. However, a large body of work\nhas highlighted the brittleness of these systems, showing that there is much\nwork left to be done. We introduce a new English reading comprehension\nbenchmark, DROP, which requires Discrete Reasoning Over the content of\nParagraphs. In this crowdsourced, adversarially-created, 96k-question\nbenchmark, a system must resolve references in a question, perhaps to multiple\ninput positions, and perform discrete operations over them (such as addition,\ncounting, or sorting). These operations require a much more comprehensive\nunderstanding of the content of paragraphs than what was necessary for prior\ndatasets. We apply state-of-the-art methods from both the reading comprehension\nand semantic parsing literature on this dataset and show that the best systems\nonly achieve 32.7% F1 on our generalized accuracy metric, while expert human\nperformance is 96.0%. We additionally present a new model that combines reading\ncomprehension methods with simple numerical reasoning to achieve 47.0% F1.", "published": "2019-03-01 05:32:01", "link": "http://arxiv.org/abs/1903.00161v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Open Information Extraction from Question-Answer Pairs", "abstract": "Open Information Extraction (OpenIE) extracts meaningful structured tuples\nfrom free-form text. Most previous work on OpenIE considers extracting data\nfrom one sentence at a time. We describe NeurON, a system for extracting tuples\nfrom question-answer pairs. Since real questions and answers often contain\nprecisely the information that users care about, such information is\nparticularly desirable to extend a knowledge base with.\n  NeurON addresses several challenges. First, an answer text is often hard to\nunderstand without knowing the question, and second, relevant information can\nspan multiple sentences. To address these, NeurON formulates extraction as a\nmulti-source sequence-to-sequence learning task, wherein it combines\ndistributed representations of a question and an answer to generate knowledge\nfacts. We describe experiments on two real-world datasets that demonstrate that\nNeurON can find a significant number of new and interesting facts to extend a\nknowledge base compared to state-of-the-art OpenIE methods.", "published": "2019-03-01 06:26:50", "link": "http://arxiv.org/abs/1903.00172v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Grounded Natural Language Understanding through Human-Robot\n  Dialog", "abstract": "Natural language understanding for robotics can require substantial domain-\nand platform-specific engineering. For example, for mobile robots to\npick-and-place objects in an environment to satisfy human commands, we can\nspecify the language humans use to issue such commands, and connect concept\nwords like red can to physical object properties. One way to alleviate this\nengineering for a new domain is to enable robots in human environments to adapt\ndynamically---continually learning new language constructions and perceptual\nconcepts. In this work, we present an end-to-end pipeline for translating\nnatural language commands to discrete robot actions, and use clarification\ndialogs to jointly improve language parsing and concept grounding. We train and\nevaluate this agent in a virtual setting on Amazon Mechanical Turk, and we\ntransfer the learned agent to a physical robot platform to demonstrate it in\nthe real world.", "published": "2019-03-01 01:43:11", "link": "http://arxiv.org/abs/1903.00122v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "A Framework for Detecting Event related Sentiments of a Community", "abstract": "Social media has revolutionized human communication and styles of\ninteraction. Due to its easiness and effective medium, people share and\nexchange information, carry out discussion on various events, and express their\nopinions. For effective policy making and understanding the response of a\ncommunity on different events, we need to monitor and analyze the social media.\nIn social media, there are some users who are more influential, for example, a\nfamous politician may have more influence than a common person. These\ninfluential users belong to specific communities. The main object of this\nresearch is to know the sentiments of a specific community on various events.\nFor detecting the event based sentiments of a community we propose a generic\nframework. Our framework identifies the users of a specific community on\ntwitter. After identifying the users of a community, we fetch their tweets and\nidentify tweets belonging to specific events. The event based tweets are\npre-processed. Pre-processed tweets are then analyzed for detecting sentiments\nof a community for specific events. Qualitative and quantitative evaluation\nconfirms the effectiveness and usefulness of our proposed framework.", "published": "2019-03-01 10:10:36", "link": "http://arxiv.org/abs/1903.00232v2", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Data-driven Approach for Quality Evaluation on Knowledge Sharing\n  Platform", "abstract": "In recent years, voice knowledge sharing and question answering (Q&A)\nplatforms have attracted much attention, which greatly facilitate the knowledge\nacquisition for people. However, little research has evaluated on the quality\nevaluation on voice knowledge sharing. This paper presents a data-driven\napproach to automatically evaluate the quality of a specific Q&A platform\n(Zhihu Live). Extensive experiments demonstrate the effectiveness of the\nproposed method. Furthermore, we introduce a dataset of Zhihu Live as an open\nresource for researchers in related areas. This dataset will facilitate the\ndevelopment of new methods on knowledge sharing services quality evaluation.", "published": "2019-03-01 16:04:27", "link": "http://arxiv.org/abs/1903.00384v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Using natural language processing techniques to extract information on\n  the properties and functionalities of energetic materials from large text\n  corpora", "abstract": "The number of scientific journal articles and reports being published about\nenergetic materials every year is growing exponentially, and therefore\nextracting relevant information and actionable insights from the latest\nresearch is becoming a considerable challenge. In this work we explore how\ntechniques from natural language processing and machine learning can be used to\nautomatically extract chemical insights from large collections of documents. We\nfirst describe how to download and process documents from a variety of sources\n- journal articles, conference proceedings (including NTREM), the US Patent &\nTrademark Office, and the Defense Technical Information Center archive on\narchive.org. We present a custom NLP pipeline which uses open source NLP tools\nto identify the names of chemical compounds and relates them to function words\n(\"underwater\", \"rocket\", \"pyrotechnic\") and property words (\"elastomer\",\n\"non-toxic\"). After explaining how word embeddings work we compare the utility\nof two popular word embeddings - word2vec and GloVe. Chemical-chemical and\nchemical-application relationships are obtained by doing computations with word\nvectors. We show that word embeddings capture latent information about\nenergetic materials, so that related materials appear close together in the\nword embedding space.", "published": "2019-03-01 17:14:33", "link": "http://arxiv.org/abs/1903.00415v1", "categories": ["cs.CL", "cond-mat.mtrl-sci"], "primary_category": "cs.CL"}
{"title": "KT-Speech-Crawler: Automatic Dataset Construction for Speech Recognition\n  from YouTube Videos", "abstract": "In this paper, we describe KT-Speech-Crawler: an approach for automatic\ndataset construction for speech recognition by crawling YouTube videos. We\noutline several filtering and post-processing steps, which extract samples that\ncan be used for training end-to-end neural speech recognition systems. In our\nexperiments, we demonstrate that a single-core version of the crawler can\nobtain around 150 hours of transcribed speech within a day, containing an\nestimated 3.5% word error rate in the transcriptions. Automatically collected\nsamples contain reading and spontaneous speech recorded in various conditions\nincluding background noise and music, distant microphone recordings, and a\nvariety of accents and reverberation. When training a deep neural network on\nspeech recognition, we observed around 40\\% word error rate reduction on the\nWall Street Journal dataset by integrating 200 hours of the collected samples\ninto the training set. The demo (http://emnlp-demo.lakomkin.me/) and the\ncrawler code (https://github.com/EgorLakomkin/KTSpeechCrawler) are publicly\navailable.", "published": "2019-03-01 09:14:50", "link": "http://arxiv.org/abs/1903.00216v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Learning To Follow Directions in Street View", "abstract": "Navigating and understanding the real world remains a key challenge in\nmachine learning and inspires a great variety of research in areas such as\nlanguage grounding, planning, navigation and computer vision. We propose an\ninstruction-following task that requires all of the above, and which combines\nthe practicality of simulated environments with the challenges of ambiguous,\nnoisy real world data. StreetNav is built on top of Google Street View and\nprovides visually accurate environments representing real places. Agents are\ngiven driving instructions which they must learn to interpret in order to\nsuccessfully navigate in this environment. Since humans equipped with driving\ninstructions can readily navigate in previously unseen cities, we set a high\nbar and test our trained agents for similar cognitive capabilities. Although\ndeep reinforcement learning (RL) methods are frequently evaluated only on data\nthat closely follow the training distribution, our dataset extends to multiple\ncities and has a clean train/test separation. This allows for thorough testing\nof generalisation ability. This paper presents the StreetNav environment and\ntasks, models that establish strong baselines, and extensive analysis of the\ntask and the trained agents.", "published": "2019-03-01 16:50:02", "link": "http://arxiv.org/abs/1903.00401v2", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
