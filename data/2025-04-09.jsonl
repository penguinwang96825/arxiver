{"title": "Implied Integrality in Mixed-Integer Optimization", "abstract": "Implied-integer detection is a well-known presolving technique that is used\nby many Mixed-Integer Linear Programming solvers. Informally, a variable is\nsaid to be implied integer if its integrality is enforced implicitly by\nintegrality of other variables and the constraints of a problem. In this paper\nwe formalize the definition of implied integrality by taking a polyhedral\nperspective. Our main result characterizes implied integrality as occurring\nwhen a subset of integer variables is fixed to integer values and the\npolyhedron on the remaining variables is integral. While integral polyhedra are\nwell-understood theoretically, existing detection methods infer implied\nintegrality only for one variable at a time. We introduce new detection methods\nbased on the detection of integral polyhedra, extending existing techniques to\nmultiple variables. Additionally, we discuss the computational complexity of\nrecognizing implied integers. We conduct experiments using a new detection\nmethod that uses totally unimodular submatrices to identify implied\nintegrality. For the MIPLIB 2017 collection dataset our results indicate that,\non average, 18.8% of the variables are classified as implied integer after\npresolving, compared to just 3.3% identified by state-of-the-art techniques. We\nare able to reduce the average percentage of variables whose integrality needs\nto be enforced after presolving from 70.2% to 59.0%.", "published": "2025-04-09 18:36:22", "link": "http://arxiv.org/abs/2504.07209v1", "categories": ["cs.DM", "math.OC", "90C11 (Primary) 90C10, 90C57 (Secondary)"], "primary_category": "cs.DM"}
{"title": "Disjunctive domination in maximal outerplanar graphs", "abstract": "A disjunctive dominating set of a graph $G$ is a set $D \\subseteq V(G)$ such\nthat every vertex in $V(G)\\setminus D$ has a neighbor in $D$ or has at least\ntwo vertices in $D$ at distance $2$ from it. The disjunctive domination number\nof $G$, denoted by $\\gamma_2^d(G)$, is the minimum cardinality of a disjunctive\ndominating set of $G$. In this paper, we show that if $G$ is a maximal\nouterplanar graph of order $n \\ge 7$ with $k$ vertices of degree $2$, then\n$\\gamma_2^d(G)\\le \\lfloor\\frac{2}{9}(n+k)\\rfloor$, and this bound is sharp.", "published": "2025-04-09 18:05:29", "link": "http://arxiv.org/abs/2504.07186v1", "categories": ["math.CO", "cs.DM", "05C69"], "primary_category": "math.CO"}
{"title": "Solving \"pseudo-injective\" polynomial equations over finite dynamical systems", "abstract": "We consider the semiring of abstract finite dynamical systems up to\nisomorphism, with the operations of alternative and synchronous execution. We\ncontinue searching for efficient algorithms for solving polynomial equations of\nthe form $P(X) = B$, with a constant side B, with the goal of decomposing\ncomplex behaviors into simpler systems. Taking inspiration from the\ncharacterization of injective polynomials P over dynamical systems, which is\nbased on a condition on the lengths of limit cycles of their coefficients, we\nintroduce a more general notion of pseudo-injectivity by relaxing this\nconstraint. We prove that the associated equations can be solved efficiently,\neven in certain cases where the input is encoded in an exponentially more\ncompact way.", "published": "2025-04-09 15:49:39", "link": "http://arxiv.org/abs/2504.06986v1", "categories": ["cs.DM", "math.DS"], "primary_category": "cs.DM"}
{"title": "Grouping Strategies on Two-Phase Methods for Bi-objective Combinatorial Optimization", "abstract": "Two-phase methods are commonly used to solve bi-objective combinatorial\noptimization problems. In the first phase, all extreme supported nondominated\npoints are generated through a dichotomic search. This phase also allows the\nidentification of search zones that may contain other nondominated points. The\nsecond phase focuses on exploring these search zones to locate the remaining\npoints, which typically accounts for most of the computational cost. Ranking\nalgorithms are frequently employed to explore each zone individually, but this\napproach leads to redundancies, causing multiple visits to the same solutions.\nTo mitigate these redundancies, we propose several strategies that group\nadjacent zones, allowing a single run of the ranking algorithm for the entire\ngroup. Additionally, we explore an implicit grouping approach based on a new\nconcept of coverage. Our experiments on the Bi-Objective Spanning Tree Problem\ndemonstrate the beneficial impact of these grouping strategies when combined\nwith coverage.", "published": "2025-04-09 13:19:26", "link": "http://arxiv.org/abs/2504.06869v1", "categories": ["cs.DS", "cs.DM"], "primary_category": "cs.DS"}
{"title": "On a Characterization of Spartan Graphs", "abstract": "The eternal vertex cover game is played between an attacker and a defender on\nan undirected graph $G$. The defender identifies $k$ vertices to position\nguards on to begin with. The attacker, on their turn, attacks an edge $e$, and\nthe defender must move a guard along $e$ to defend the attack. The defender may\nmove other guards as well, under the constraint that every guard moves at most\nonce and to a neighboring vertex. The smallest number of guards required to\ndefend attacks forever is called the eternal vertex cover number of $G$,\ndenoted $evc(G)$.\n  For any graph $G$, $evc(G)$ is at least the vertex cover number of $G$,\ndenoted $mvc(G)$. A graph is Spartan if $evc(G) = mvc(G)$. It is known that a\nbipartite graph is Spartan if and only if every edge belongs to a perfect\nmatching. We show that the only K\\\"onig graphs that are Spartan are the\nbipartite Spartan graphs. We also give new lower bounds for $evc(G)$,\ngeneralizing a known lower bound based on cut vertices. We finally show a new\nmatching-based characterization of all Spartan graphs.", "published": "2025-04-09 12:47:29", "link": "http://arxiv.org/abs/2504.06832v1", "categories": ["cs.DM", "math.CO"], "primary_category": "cs.DM"}
{"title": "Beyond Reproducibility: Advancing Zero-shot LLM Reranking Efficiency with Setwise Insertion", "abstract": "This study presents a comprehensive reproducibility and extension analysis of\nthe Setwise prompting methodology for zero-shot ranking with Large Language\nModels (LLMs), as proposed by Zhuang et al. We evaluate its effectiveness and\nefficiency compared to traditional Pointwise, Pairwise, and Listwise approaches\nin document ranking tasks. Our reproduction confirms the findings of Zhuang et\nal., highlighting the trade-offs between computational efficiency and ranking\neffectiveness in Setwise methods. Building on these insights, we introduce\nSetwise Insertion, a novel approach that leverages the initial document ranking\nas prior knowledge, reducing unnecessary comparisons and uncertainty by\nfocusing on candidates more likely to improve the ranking results. Experimental\nresults across multiple LLM architectures (Flan-T5, Vicuna, and Llama) show\nthat Setwise Insertion yields a 31% reduction in query time, a 23% reduction in\nmodel inferences, and a slight improvement in reranking effectiveness compared\nto the original Setwise method. These findings highlight the practical\nadvantage of incorporating prior ranking knowledge into Setwise prompting for\nefficient and accurate zero-shot document reranking.", "published": "2025-04-09 18:44:34", "link": "http://arxiv.org/abs/2504.10509v1", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR"}
{"title": "KG-LLM-Bench: A Scalable Benchmark for Evaluating LLM Reasoning on Textualized Knowledge Graphs", "abstract": "Knowledge graphs have emerged as a popular method for injecting up-to-date,\nfactual knowledge into large language models (LLMs). This is typically achieved\nby converting the knowledge graph into text that the LLM can process in\ncontext. While multiple methods of encoding knowledge graphs have been\nproposed, the impact of this textualization process on LLM performance remains\nunder-explored. We introduce KG-LLM-Bench, a comprehensive and extensible\nbenchmark spanning five knowledge graph understanding tasks, and evaluate how\ndifferent encoding strategies affect performance across various base models.\nOur extensive experiments with seven language models and five textualization\nstrategies provide insights for optimizing LLM performance on KG reasoning\ntasks.", "published": "2025-04-09 17:58:47", "link": "http://arxiv.org/abs/2504.07087v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Poly-Vector Retrieval: Reference and Content Embeddings for Legal Documents", "abstract": "Retrieval-Augmented Generation (RAG) has emerged as an effective paradigm for\ngenerating contextually accurate answers by integrating Large Language Models\n(LLMs) with retrieval mechanisms. However, in legal contexts, users frequently\nreference norms by their labels or nicknames (e.g., Article 5 of the\nConstitution or Consumer Defense Code (CDC)), rather than by their content,\nposing challenges for traditional RAG approaches that rely solely on semantic\nembeddings of text. Furthermore, legal texts themselves heavily rely on\nexplicit cross-references (e.g., \"pursuant to Article 34\") that function as\npointers. Both scenarios pose challenges for traditional RAG approaches that\nrely solely on semantic embeddings of text, often failing to retrieve the\nnecessary referenced content. This paper introduces Poly-Vector Retrieval, a\nmethod assigning multiple distinct embeddings to each legal provision: one\nembedding captures the content (the full text), another captures the label (the\nidentifier or proper name), and optionally additional embeddings capture\nalternative denominations. Inspired by Frege's distinction between Sense and\nReference, this poly-vector retrieval approach treats labels, identifiers and\nreference markers as rigid designators and content embeddings as carriers of\nsemantic substance. Experiments on the Brazilian Federal Constitution\ndemonstrate that Poly-Vector Retrieval significantly improves retrieval\naccuracy for label-centric queries and potential to resolve internal and\nexternal cross-references, without compromising performance on purely semantic\nqueries. The study discusses philosophical and practical implications of\nexplicitly separating reference from content in vector embeddings and proposes\nfuture research directions for applying this approach to broader legal datasets\nand other domains characterized by explicit reference identifiers.", "published": "2025-04-09 17:54:11", "link": "http://arxiv.org/abs/2504.10508v1", "categories": ["cs.IR", "cs.AI", "I.2.8"], "primary_category": "cs.IR"}
{"title": "PinRec: Outcome-Conditioned, Multi-Token Generative Retrieval for Industry-Scale Recommendation Systems", "abstract": "Generative retrieval methods utilize generative sequential modeling\ntechniques, such as transformers, to generate candidate items for recommender\nsystems. These methods have demonstrated promising results in academic\nbenchmarks, surpassing traditional retrieval models like two-tower\narchitectures. However, current generative retrieval methods lack the\nscalability required for industrial recommender systems, and they are\ninsufficiently flexible to satisfy the multiple metric requirements of modern\nsystems.\n  This paper introduces PinRec, a novel generative retrieval model developed\nfor applications at Pinterest. PinRec utilizes outcome-conditioned generation,\nenabling modelers to specify how to balance various outcome metrics, such as\nthe number of saves and clicks, to effectively align with business goals and\nuser exploration. Additionally, PinRec incorporates multi-token generation to\nenhance output diversity while optimizing generation. Our experiments\ndemonstrate that PinRec can successfully balance performance, diversity, and\nefficiency, delivering a significant positive impact to users using generative\nmodels. This paper marks a significant milestone in generative retrieval, as it\npresents, to our knowledge, the first rigorous study on implementing generative\nretrieval at the scale of Pinterest.", "published": "2025-04-09 17:46:12", "link": "http://arxiv.org/abs/2504.10507v1", "categories": ["cs.IR", "cs.LG"], "primary_category": "cs.IR"}
{"title": "CHIME: A Compressive Framework for Holistic Interest Modeling", "abstract": "Modeling holistic user interests is important for improving recommendation\nsystems but is challenged by high computational cost and difficulty in handling\ndiverse information with full behavior context. Existing search-based methods\nmight lose critical signals during behavior selection. To overcome these\nlimitations, we propose CHIME: A Compressive Framework for Holistic Interest\nModeling. It uses adapted large language models to encode complete user\nbehaviors with heterogeneous inputs. We introduce multi-granular contrastive\nlearning objectives to capture both persistent and transient interest patterns\nand apply residual vector quantization to generate compact embeddings. CHIME\ndemonstrates superior ranking performance across diverse datasets, establishing\na robust solution for scalable holistic interest modeling in recommendation\nsystems.", "published": "2025-04-09 11:08:49", "link": "http://arxiv.org/abs/2504.06780v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Unifying Search and Recommendation: A Generative Paradigm Inspired by Information Theory", "abstract": "Recommender systems and search engines serve as foundational elements of\nonline platforms, with the former delivering information proactively and the\nlatter enabling users to seek information actively. Unifying both tasks in a\nshared model is promising since it can enhance user modeling and item\nunderstanding. Previous approaches mainly follow a discriminative paradigm,\nutilizing shared encoders to process input features and task-specific heads to\nperform each task. However, this paradigm encounters two key challenges:\ngradient conflict and manual design complexity. From the information theory\nperspective, these challenges potentially both stem from the same issue -- low\nmutual information between the input features and task-specific outputs during\nthe optimization process.\n  To tackle these issues, we propose GenSR, a novel generative paradigm for\nunifying search and recommendation (S&R), which leverages task-specific prompts\nto partition the model's parameter space into subspaces, thereby enhancing\nmutual information. To construct effective subspaces for each task, GenSR first\nprepares informative representations for each subspace and then optimizes both\nsubspaces in one unified model. Specifically, GenSR consists of two main\nmodules: (1) Dual Representation Learning, which independently models\ncollaborative and semantic historical information to derive expressive item\nrepresentations; and (2) S&R Task Unifying, which utilizes contrastive learning\ntogether with instruction tuning to generate task-specific outputs effectively.\nExtensive experiments on two public datasets show GenSR outperforms\nstate-of-the-art methods across S&R tasks. Our work introduces a new generative\nparadigm compared with previous discriminative methods and establishes its\nsuperiority from the mutual information perspective.", "published": "2025-04-09 09:15:37", "link": "http://arxiv.org/abs/2504.06714v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Toward Holistic Evaluation of Recommender Systems Powered by Generative Models", "abstract": "Recommender systems powered by generative models (Gen-RecSys) extend beyond\nclassical item ranking by producing open-ended content, which simultaneously\nunlocks richer user experiences and introduces new risks. On one hand, these\nsystems can enhance personalization and appeal through dynamic explanations and\nmulti-turn dialogues. On the other hand, they might venture into unknown\nterritory-hallucinating nonexistent items, amplifying bias, or leaking private\ninformation. Traditional accuracy metrics cannot fully capture these\nchallenges, as they fail to measure factual correctness, content safety, or\nalignment with user intent.\n  This paper makes two main contributions. First, we categorize the evaluation\nchallenges of Gen-RecSys into two groups: (i) existing concerns that are\nexacerbated by generative outputs (e.g., bias, privacy) and (ii) entirely new\nrisks (e.g., item hallucinations, contradictory explanations). Second, we\npropose a holistic evaluation approach that includes scenario-based assessments\nand multi-metric checks-incorporating relevance, factual grounding, bias\ndetection, and policy compliance. Our goal is to provide a guiding framework so\nresearchers and practitioners can thoroughly assess Gen-RecSys, ensuring\neffective personalization and responsible deployment.", "published": "2025-04-09 08:08:16", "link": "http://arxiv.org/abs/2504.06667v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "BBQRec: Behavior-Bind Quantization for Multi-Modal Sequential Recommendation", "abstract": "Multi-modal sequential recommendation systems leverage auxiliary signals\n(e.g., text, images) to alleviate data sparsity in user-item interactions.\nWhile recent methods exploit large language models to encode modalities into\ndiscrete semantic IDs for autoregressive prediction, we identify two critical\nlimitations: (1) Existing approaches adopt fragmented quantization, where\nmodalities are independently mapped to semantic spaces misaligned with\nbehavioral objectives, and (2) Over-reliance on semantic IDs disrupts\ninter-modal semantic coherence, thereby weakening the expressive power of\nmulti-modal representations for modeling diverse user preferences.\n  To address these challenges, we propose a Behavior-Bind multi-modal\nQuantization for Sequential Recommendation (BBQRec for short) featuring\ndual-aligned quantization and semantics-aware sequence modeling. First, our\nbehavior-semantic alignment module disentangles modality-agnostic behavioral\npatterns from noisy modality-specific features through contrastive codebook\nlearning, ensuring semantic IDs are inherently tied to recommendation tasks.\nSecond, we design a discretized similarity reweighting mechanism that\ndynamically adjusts self-attention scores using quantized semantic\nrelationships, preserving multi-modal synergies while avoiding invasive\nmodifications to the sequence modeling architecture. Extensive evaluations\nacross four real-world benchmarks demonstrate BBQRec's superiority over the\nstate-of-the-art baselines.", "published": "2025-04-09 07:19:48", "link": "http://arxiv.org/abs/2504.06636v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "A Serendipitous Recommendation System Considering User Curiosity", "abstract": "To address the problem of narrow recommendation ranges caused by an emphasis\non prediction accuracy, serendipitous recommendations, which consider both\nusefulness and unexpectedness, have attracted attention. However, realizing\nserendipitous recommendations is challenging due to the varying proportions of\nusefulness and unexpectedness preferred by different users, which is influenced\nby their differing desires for knowledge. In this paper, we propose a method to\nestimate the proportion of usefulness and unexpectedness that each user desires\nbased on their curiosity, and make recommendations that match this preference.\nThe proposed method estimates a user's curiosity by considering both their\nlong-term and short-term interests. Offline experiments were conducted using\nthe MovieLens-1M dataset to evaluate the effectiveness of the proposed method.\nThe experimental results demonstrate that our method achieves the same level of\nperformance as state-of-the-art method while successfully providing\nserendipitous recommendations.", "published": "2025-04-09 07:15:06", "link": "http://arxiv.org/abs/2504.06633v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Learning to erase quantum states: thermodynamic implications of quantum learning theory", "abstract": "The energy cost of erasing quantum states depends on our knowledge of the\nstates. We show that learning algorithms can acquire such knowledge to erase\nmany copies of an unknown state at the optimal energy cost. This is proved by\nshowing that learning can be made fully reversible and has no fundamental\nenergy cost itself. With simple counting arguments, we relate the energy cost\nof erasing quantum states to their complexity, entanglement, and magic. We\nfurther show that the constructed erasure protocol is computationally efficient\nwhen learning is efficient. Conversely, under standard cryptographic\nassumptions, we prove that the optimal energy cost cannot be achieved\nefficiently in general. These results also enable efficient work extraction\nbased on learning. Together, our results establish a concrete connection\nbetween quantum learning theory and thermodynamics, highlighting the physical\nsignificance of learning processes and enabling efficient learning-based\nprotocols for thermodynamic tasks.", "published": "2025-04-09 23:51:01", "link": "http://arxiv.org/abs/2504.07341v1", "categories": ["quant-ph", "cond-mat.stat-mech", "cs.CC", "cs.IT", "cs.LG", "math.IT"], "primary_category": "quant-ph"}
{"title": "Bregman-Hausdorff divergence: strengthening the connections between computational geometry and machine learning", "abstract": "The purpose of this paper is twofold. On a technical side, we propose an\nextension of the Hausdorff distance from metric spaces to spaces equipped with\nasymmetric distance measures. Specifically, we focus on the family of Bregman\ndivergences, which includes the popular Kullback--Leibler divergence (also\nknown as relative entropy).\n  As a proof of concept, we use the resulting Bregman--Hausdorff divergence to\ncompare two collections of probabilistic predictions produced by different\nmachine learning models trained using the relative entropy loss. The algorithms\nwe propose are surprisingly efficient even for large inputs with hundreds of\ndimensions.\n  In addition to the introduction of this technical concept, we provide a\nsurvey. It outlines the basics of Bregman geometry, as well as computational\ngeometry algorithms. We focus on algorithms that are compatible with this\ngeometry and are relevant for machine learning.", "published": "2025-04-09 22:42:29", "link": "http://arxiv.org/abs/2504.07322v1", "categories": ["cs.LG", "cs.CG", "cs.IT", "math.IT"], "primary_category": "cs.LG"}
{"title": "Quantum Reverse Shannon Theorem Revisited", "abstract": "Reverse Shannon theorems concern the use of noiseless channels to simulate\nnoisy ones. This is dual to the usual noisy channel coding problem, where a\nnoisy (classical or quantum) channel is used to simulate a noiseless one. The\nQuantum Reverse Shannon Theorem is extensively studied by Bennett and\nco-authors in [IEEE Trans. Inf. Theory, 2014]. They present two distinct\ntheorems, each tailored to classical and quantum channel simulations\nrespectively, explaining the fact that these theorems remain incomparable due\nto the fundamentally different nature of correlations they address. The authors\nleave as an open question the challenge of formulating a unified theorem that\ncould encompass the principles of both and unify them. We unify these two\ntheorems into a single, comprehensive theorem, extending it to the most general\ncase by considering correlations with a general mixed-state reference system.\nFurthermore, we unify feedback and non-feedback theorems by simulating a\ngeneral side information system at the encoder side.", "published": "2025-04-09 17:37:20", "link": "http://arxiv.org/abs/2504.07068v1", "categories": ["quant-ph", "cs.IT", "math.IT"], "primary_category": "quant-ph"}
{"title": "Finite Field Multiple Access III: from 2-ary to p-ary", "abstract": "This paper extends finite-field multiple-access (FFMA) techniques from binary\nto general $p$-ary source transmission. We introduce element-assemblage (EA)\ncodes over GF($p^m$), generalizing element-pair (EP) codes, and define two\nspecific types for ternary transmission: orthogonal EA codes and double\ncodeword EA (D-CWEA) codes. A unique sum-pattern mapping (USPM) constraint is\nproposed for the design of uniquely-decodable CWEA (UD-CWEA) codes, including\nadditive inverse D-CWEA (AI-D-CWEA) and basis decomposition D-CWEA (BD-D-CWEA)\ncodes. Moreover, we extend EP-coding to EA-coding, focusing on non-orthogonal\nCWEA (NO-CWEA) codes and their USPM constraint in the complex field.\nAdditionally, $p$-ary CWEA codes are constructed using a basis decomposition\nmethod, leveraging ternary decomposition for faster convergence and simplified\nencoder/decoder design. We present a comprehensive performance analysis of the\nproposed FFMA system from two complementary perspectives: channel capacity and\nerror performance. We demonstrate that equal power allocation (EPA) achieves\nthe theoretical channel capacity bound, while independently developing a\nrate-driven capacity alignment (CA) theorem based on the capacity-to-rate ratio\n(CRR) metric for error performance analysis. We then explore the multiuser\nfinite blocklength (FBL) characteristics of FFMA systems. Finally, a\ncomparative analysis of $p$-ary transmission systems against classical binary\nsystems is conducted, revealing that low-order $p$-ary systems (e.g., $p=3$)\noutperform binary systems at small loading factors, while higher-order systems\n(e.g., $p=257$) excel at larger loading factors. These findings highlight the\npotential of $p$-ary systems, although practical implementations may benefit\nfrom decomposing $p$-ary systems into ternary systems to manage complexity.", "published": "2025-04-09 14:41:36", "link": "http://arxiv.org/abs/2504.06937v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Optimality of Gradient-MUSIC for Spectral Estimation", "abstract": "The goal of spectral estimation is to estimate the frequencies and amplitudes\nof a nonharmonic Fourier sum given noisy time samples. This paper introduces\nthe Gradient-MUSIC algorithm, which is a novel nonconvex optimization\nreformulation of the classical MUSIC algorithm. Under the assumption that\n$m\\Delta\\geq 8\\pi$, where $\\pi/m$ is the Nyquist rate and $\\Delta$ is the\nminimum separation of the frequencies normalized to be in $[0,2\\pi)$, we\nprovide a thorough geometric analysis of the objective functions generated by\nthe algorithm. Gradient-MUSIC thresholds the objective function on a set that\nis as coarse as possible and locates a set of suitable initialization for\ngradient descent. Although the objective function is nonconvex, gradient\ndescent converges exponentially fast to the desired local minima, which are the\nestimated frequencies of the signal. For deterministic $\\ell^p$ perturbations\nand any $p\\in [1,\\infty]$, Gradient-MUSIC estimates the frequencies and\namplitudes at the minimax optimal rate in terms of the noise level and $m$. For\nexample, if the noise has $\\ell^\\infty$ norm at most $\\epsilon$, then the\nfrequencies and amplitudes are recovered up to error at most $C\\epsilon/m$ and\n$C\\epsilon$, respectively, which are optimal in $\\epsilon$ and $m$. Aside from\nlogarithmic factors, Gradient-MUSIC is optimal for white noise and matches the\nrate achieved by nonlinear least squares for various families of nonstationary\nindependent Gaussian noise. Our results show that classical MUSIC is equally\noptimal, but it requires an expensive search on a thin grid, whereas\nGradient-MUSIC is always computationally more efficient, especially for small\nnoise. As a consequence of this paper, for sufficiently well separated\nfrequencies, both Gradient-MUSIC and classical MUSIC are the first provably\noptimal and computationally tractable algorithms for deterministic $\\ell^p$\nperturbations.", "published": "2025-04-09 13:00:49", "link": "http://arxiv.org/abs/2504.06842v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Analog Computing with Microwave Networks", "abstract": "Analog computing has been recently revived due to its potential for\nenergy-efficient and highly parallel computations. In this paper, we\ninvestigate analog computers that linearly process microwave signals, named\nmicrowave linear analog computers (MiLACs), and their applications in signal\nprocessing for communications. We model a MiLAC as a multiport microwave\nnetwork with tunable impedance components, which enables the execution of\nmathematical operations by reconfiguring the microwave network and applying\ninput signals at its ports. We demonstrate that a MiLAC can efficiently compute\nthe linear minimum mean square error (LMMSE) estimator, widely used in\nmultiple-input multiple-output (MIMO) communications beamforming and detection,\nwith remarkably low computational complexity, unachievable through digital\ncomputing. Specifically, the LMMSE estimator can be computed with complexity\ngrowing with the square of its input size, rather than the cube, with\nrevolutionary applications to gigantic MIMO beamforming and detection.", "published": "2025-04-09 11:29:39", "link": "http://arxiv.org/abs/2504.06790v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Locally Repairable Convertible Codes: Improved Lower Bound and General Construction", "abstract": "In this paper, we consider the convertible code with locally repairable\nproperty. We present an improved lower bound on access cost associated with\n$(r,\\delta)$. Then, we provide a general construction of convertible codes with\noptimal access cost which shows that those codes can be with super-linear\nlength or maximum repairable property. Additionally, employing the known\nlocally repairable codes with super-linear length or maximum repairable\nproperty, we provide explicit constructions of convertible codes with\nsuper-linear length or maximum repairable property.", "published": "2025-04-09 09:46:08", "link": "http://arxiv.org/abs/2504.06734v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Several new infinite families of NMDS codes with arbitrary dimensions supporting $t$-designs", "abstract": "Near maximum distance separable (NMDS) codes, where both the code and its\ndual are almost maximum distance separable, play pivotal roles in combinatorial\ndesign theory and cryptographic applications. Despite progress in fixed\ndimensions (e.g., dimension 4 codes by Ding and Tang \\cite{Ding2020}),\nconstructing NMDS codes with arbitrary dimensions supporting $t$-designs\n($t\\geq 2$) has remained open. In this paper, we construct two infinite\nfamilies of NMDS codes over $\\mathbb{F}_q$ for any prime power $q$ with\nflexible dimensions and determine their weight distributions. Further, two\nadditional families with arbitrary dimensions over $\\mathbb{F}_{2^m}$\nsupporting $2$-designs and $3$-designs, and their weight distributions are\nobtained. Our results fully generalize prior fixed-dimension\nworks~\\cite{DingY2024,Heng2023,Heng20231,Xu2022}, and affirmatively settle the\nHeng-Wang conjecture \\cite{Heng2023} on the existence of NMDS codes with\nflexible parameters supporting $2$-designs.", "published": "2025-04-09 03:02:10", "link": "http://arxiv.org/abs/2504.06546v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "The Exploratory Study on the Relationship Between the Failure of Distance Metrics in High-Dimensional Space and Emergent Phenomena", "abstract": "This paper presents a unified framework, integrating information theory and\nstatistical mechanics, to connect metric failure in high-dimensional data with\nemergence in complex systems. We propose the \"Information Dilution Theorem,\"\ndemonstrating that as dimensionality ($d$) increases, the mutual information\nefficiency between geometric metrics (e.g., Euclidean distance) and system\nstates decays approximately as $O(1/d)$. This decay arises from the mismatch\nbetween linearly growing system entropy and sublinearly growing metric entropy,\nexplaining the mechanism behind distance concentration. Building on this, we\nintroduce information structural complexity ($C(S)$) based on the mutual\ninformation matrix spectrum and interaction encoding capacity ($C'$) derived\nfrom information bottleneck theory. The \"Emergence Critical Theorem\" states\nthat when $C(S)$ exceeds $C'$, new global features inevitably emerge,\nsatisfying a predefined mutual information threshold. This provides an\noperational criterion for self-organization and phase transitions. We discuss\npotential applications in physics, biology, and deep learning, suggesting\npotential directions like MI-based manifold learning (UMAP+) and offering a\nquantitative foundation for analyzing emergence across disciplines.", "published": "2025-04-09 02:19:58", "link": "http://arxiv.org/abs/2504.08807v1", "categories": ["cs.IT", "cond-mat.stat-mech", "math.IT", "nlin.AO"], "primary_category": "cs.IT"}
{"title": "Modeling Response Consistency in Multi-Agent LLM Systems: A Comparative Analysis of Shared and Separate Context Approaches", "abstract": "Large Language Models (LLMs) are increasingly utilized in multi-agent systems\n(MAS) to enhance collaborative problem-solving and interactive reasoning.\nRecent advancements have enabled LLMs to function as autonomous agents capable\nof understanding complex interactions across multiple topics. However,\ndeploying LLMs in MAS introduces challenges related to context management,\nresponse consistency, and scalability, especially when agents must operate\nunder memory limitations and handle noisy inputs. While prior research has\nexplored optimizing context sharing and response latency in LLM-driven MAS,\nthese efforts often focus on either fully centralized or decentralized\nconfigurations, each with distinct trade-offs.\n  In this paper, we develop a probabilistic framework to analyze the impact of\nshared versus separate context configurations on response consistency and\nresponse times in LLM-based MAS. We introduce the Response Consistency Index\n(RCI) as a metric to evaluate the effects of context limitations, noise, and\ninter-agent dependencies on system performance. Our approach differs from\nexisting research by focusing on the interplay between memory constraints and\nnoise management, providing insights into optimizing scalability and response\ntimes in environments with interdependent topics. Through this analysis, we\noffer a comprehensive understanding of how different configurations impact the\nefficiency of LLM-driven multi-agent systems, thereby guiding the design of\nmore robust architectures.", "published": "2025-04-09 21:54:21", "link": "http://arxiv.org/abs/2504.07303v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA"}
{"title": "Self-organisation of common good usage and an application to Internet services", "abstract": "Natural and human-made common goods present key challenges due to their\nsusceptibility to degradation, overuse, or congestion. We explore the\nself-organisation of their usage when individuals have access to several\navailable commons but limited information on them. We propose an extension of\nthe Win-Stay, Lose-Shift (WSLS) strategy for such systems, under which\nindividuals use a resource iteratively until they are unsuccessful and then\nshift randomly. This simple strategy leads to a distribution of the use of\ncommons with an improvement against random shifting. Selective individuals who\nretain information on their usage and accordingly adapt their tolerance to\nfailure in each common good improve the average experienced quality for an\nentire population. Hybrid systems of selective and non-selective individuals\ncan lead to an equilibrium with equalised experienced quality akin to the ideal\nfree distribution. We show that these results can be applied to the server\nselection problem faced by mobile users accessing Internet services and we\nperform realistic simulations to test their validity. Furthermore, these\nfindings can be used to understand other real systems such as animal dispersal\non grazing and foraging land, and to propose solutions to operators of systems\nof public transport or other technological commons.", "published": "2025-04-09 18:00:01", "link": "http://arxiv.org/abs/2504.07175v1", "categories": ["cs.MA", "cs.GT", "cs.NI", "nlin.AO"], "primary_category": "cs.MA"}
{"title": "Multi-Object Tracking for Collision Avoidance Using Multiple Cameras in Open RAN Networks", "abstract": "This paper deals with the multi-object detection and tracking problem, within\nthe scope of open Radio Access Network (RAN), for collision avoidance in\nvehicular scenarios. To this end, a set of distributed intelligent agents\ncollocated with cameras are considered. The fusion of detected objects is done\nat an edge service, considering Open RAN connectivity. Then, the edge service\npredicts the objects trajectories for collision avoidance. Compared to the\nrelated work a more realistic Open RAN network is implemented and multiple\ncameras are used.", "published": "2025-04-09 17:36:40", "link": "http://arxiv.org/abs/2504.07163v1", "categories": ["cs.MA", "cs.LG", "cs.RO"], "primary_category": "cs.MA"}
{"title": "Review of Case-Based Reasoning for LLM Agents: Theoretical Foundations, Architectural Components, and Cognitive Integration", "abstract": "Agents powered by Large Language Models (LLMs) have recently demonstrated\nimpressive capabilities in various tasks. Still, they face limitations in tasks\nrequiring specific, structured knowledge, flexibility, or accountable\ndecision-making. While agents are capable of perceiving their environments,\nforming inferences, planning, and executing actions towards goals, they often\nface issues such as hallucinations and lack of contextual memory across\ninteractions. This paper explores how Case-Based Reasoning (CBR), a strategy\nthat solves new problems by referencing past experiences, can be integrated\ninto LLM agent frameworks. This integration allows LLMs to leverage explicit\nknowledge, enhancing their effectiveness. We systematically review the\ntheoretical foundations of these enhanced agents, identify critical framework\ncomponents, and formulate a mathematical model for the CBR processes of case\nretrieval, adaptation, and learning. We also evaluate CBR-enhanced agents\nagainst other methods like Chain-of-Thought reasoning and standard\nRetrieval-Augmented Generation, analyzing their relative strengths. Moreover,\nwe explore how leveraging CBR's cognitive dimensions (including\nself-reflection, introspection, and curiosity) via goal-driven autonomy\nmechanisms can further enhance the LLM agent capabilities. Contributing to the\nongoing research on neuro-symbolic hybrid systems, this work posits CBR as a\nviable technique for enhancing the reasoning skills and cognitive aspects of\nautonomous LLM agents.", "published": "2025-04-09 14:51:02", "link": "http://arxiv.org/abs/2504.06943v2", "categories": ["cs.AI", "cs.MA", "68", "I.2; I.2.7"], "primary_category": "cs.AI"}
{"title": "AI-Driven Consensus: Modeling Multi-Agent Networks with Long-Range Interactions through path-Laplacian Matrices", "abstract": "Extended connectivity in graphs can be analyzed through k-path Laplacian\nmatrices, which permit the capture of long-range interactions in various\nreal-world networked systems such as social, transportation, and multi-agent\nnetworks. In this work, we present several alternative methods based on machine\nlearning methods (LSTM, xLSTM, Transformer, XGBoost, and ConvLSTM) to predict\nthe final consensus value based on directed networks (Erd\\\"os-Renyi,\nWatts-Strogatz, and Barab\\'asi-Albert) and on the initial state. We highlight\nhow different k-hop interactions affect the performance of the tested methods.\nThis framework opens new avenues for analyzing multi-scale diffusion processes\nin large-scale, complex networks.", "published": "2025-04-09 13:53:57", "link": "http://arxiv.org/abs/2504.06894v1", "categories": ["cs.SI", "cs.MA"], "primary_category": "cs.SI"}
{"title": "Adaptive Human-Robot Collaborative Missions using Hybrid Task Planning", "abstract": "Producing robust task plans in human-robot collaborative missions is a\ncritical activity in order to increase the likelihood of these missions\ncompleting successfully. Despite the broad research body in the area, which\nconsiders different classes of constraints and uncertainties, its applicability\nis confined to relatively simple problems that can be comfortably addressed by\nthe underpinning mathematically-based or heuristic-driven solver engines. In\nthis paper, we introduce a hybrid approach that effectively solves the task\nplanning problem by decomposing it into two intertwined parts, starting with\nthe identification of a feasible plan and followed by its uncertainty\naugmentation and verification yielding a set of Pareto optimal plans. To\nenhance its robustness, adaptation tactics are devised for the evolving system\nrequirements and agents' capabilities. We demonstrate our approach through an\nindustrial case study involving workers and robots undertaking activities\nwithin a vineyard, showcasing the benefits of our hybrid approach both in the\ngeneration of feasible solutions and scalability compared to native planners.", "published": "2025-04-09 10:07:15", "link": "http://arxiv.org/abs/2504.06746v1", "categories": ["cs.MA", "cs.RO"], "primary_category": "cs.MA"}
{"title": "FJ-MM: The Friedkin-Johnsen Opinion Dynamics Model with Memory and Higher-Order Neighbors", "abstract": "The Friedkin-Johnsen (FJ) model has been extensively explored and validated,\nspanning applications in social science, systems and control, game theory, and\nalgorithmic research. In this paper, we introduce an advanced generalization of\nthe FJ model, termed FJ-MM which incorporates both memory effects and multi-hop\n(higher-order neighbor) influence. This formulation allows agents to naturally\nincorporate both current and previous opinions at each iteration stage. Our\nnumerical results demonstrate that incorporating memory and multi-hop influence\nsignificantly reshapes the opinion landscape; for example, the final opinion\nprofile can exhibit reduced polarization. We analyze the stability and\nequilibrium properties of the FJ-MM model, showing that these properties can be\nreduced to those of a comparison model--namely, the standard FJ model with a\nmodified influence matrix. This reduction enables us to leverage established\nstability results from FJ dynamics. Additionally, we examine the convergence\nrate of the FJ-MM model and demonstrate that, as can be expected, the time lags\nintroduced by memory and higher-order neighbor influences result in slower\nconvergence.", "published": "2025-04-09 09:43:04", "link": "http://arxiv.org/abs/2504.06731v1", "categories": ["eess.SY", "cs.MA", "cs.SY", "math.OC", "physics.soc-ph"], "primary_category": "eess.SY"}
{"title": "SDHN: Skewness-Driven Hypergraph Networks for Enhanced Localized Multi-Robot Coordination", "abstract": "Multi-Agent Reinforcement Learning is widely used for multi-robot\ncoordination, where simple graphs typically model pairwise interactions.\nHowever, such representations fail to capture higher-order collaborations,\nlimiting effectiveness in complex tasks. While hypergraph-based approaches\nenhance cooperation, existing methods often generate arbitrary hypergraph\nstructures and lack adaptability to environmental uncertainties. To address\nthese challenges, we propose the Skewness-Driven Hypergraph Network (SDHN),\nwhich employs stochastic Bernoulli hyperedges to explicitly model higher-order\nmulti-robot interactions. By introducing a skewness loss, SDHN promotes an\nefficient structure with Small-Hyperedge Dominant Hypergraph, allowing robots\nto prioritize localized synchronization while still adhering to the overall\ninformation, similar to human coordination. Extensive experiments on Moving\nAgents in Formation and Robotic Warehouse tasks validate SDHN's effectiveness,\ndemonstrating superior performance over state-of-the-art baselines.", "published": "2025-04-09 08:41:57", "link": "http://arxiv.org/abs/2504.06684v1", "categories": ["cs.RO", "cs.MA"], "primary_category": "cs.RO"}
{"title": "Application of CTS (Computer to Screen) Machine in Printing Industries for Process Improvement & Material Optimization", "abstract": "The printing and labeling industries are struggling to meet the need for more\ncomplex and dynamic design requirements coming from the customers. It is now\ncrucial to implement technological advancements to manage workflow,\nproductivity, process optimization, and continual improvement. There has never\nbeen a time when the imagery and embellishments of apparel has been more\ncommercially viable as it is now. Images and text are fused directly to fabric\nby heat transfer printing and labeling. For screen development which is\nrequired for heat transfer label mass production, many industries are still\nusing the conventional method of screen development process. A CTS\n(computer-to-screen) innovates the printing and labeling industries by\nenhancing workflow, lowering consumable consumptions and chemical usage,\nspeeding up setup, guaranteeing flawless design, and raising the print quality\nof the producing screens. The study's objective is to assess how CTS machines\nare used and how they affect existing heat transfer screen development\nprocesses in one of Bangladesh's leading printing and labeling companies. The\nstudy's primary goal is to highlight and analyze how the use of CTS machines\nreduces material and operational costs by optimizing the process. Costs for\nCapEx and OpEx are computed and compared for using CTS technology before and\nafter adoption. Savings data such as material, consumable, and operating cost\nsavings versus depreciation and machine payback period analysis were taken into\nconsideration. It is clear from this study that CTS machines in the printing\nand labeling industries can guarantee profitability on top of Capital\nExpenditures.", "published": "2025-04-09 21:37:29", "link": "http://arxiv.org/abs/2504.07294v1", "categories": ["q-fin.MF"], "primary_category": "q-fin.MF"}
{"title": "Density Approximation of Affine Jump Diffusions via Closed-Form Moment Matching", "abstract": "We develop a recursive approach for deriving closed-form solutions to both\nconditional and unconditional moments of affine jump diffusions with\nstate-independent jump intensities. Using these moment solutions, we construct\nclosed-form density approximations (up to a normalization constant) via moment\nmatching for both conditional and unconditional distributions. Our framework\nenables important financial applications, including efficient option pricing\nand exact simulation for affine jump diffusions. Numerical experiments\ndemonstrate the method's superior computational efficiency compared to existing\nsimulation techniques, while preserving numerical precision.", "published": "2025-04-09 14:50:18", "link": "http://arxiv.org/abs/2504.06942v1", "categories": ["q-fin.MF"], "primary_category": "q-fin.MF"}
{"title": "Diffusion Factor Models: Generating High-Dimensional Returns with Factor Structure", "abstract": "Financial scenario simulation is essential for risk management and portfolio\noptimization, yet it remains challenging especially in high-dimensional and\nsmall data settings common in finance. We propose a diffusion factor model that\nintegrates latent factor structure into generative diffusion processes,\nbridging econometrics with modern generative AI to address the challenges of\nthe curse of dimensionality and data scarcity in financial simulation. By\nexploiting the low-dimensional factor structure inherent in asset returns, we\ndecompose the score function--a key component in diffusion models--using\ntime-varying orthogonal projections, and this decomposition is incorporated\ninto the design of neural network architectures. We derive rigorous statistical\nguarantees, establishing nonasymptotic error bounds for both score estimation\nat O(d^{5/2} n^{-2/(k+5)}) and generated distribution at O(d^{5/4}\nn^{-1/2(k+5)}), primarily driven by the intrinsic factor dimension k rather\nthan the number of assets d, surpassing the dimension-dependent limits in the\nclassical nonparametric statistics literature and making the framework viable\nfor markets with thousands of assets. Numerical studies confirm superior\nperformance in latent subspace recovery under small data regimes. Empirical\nanalysis demonstrates the economic significance of our framework in\nconstructing mean-variance optimal portfolios and factor portfolios. This work\npresents the first theoretical integration of factor structure with diffusion\nmodels, offering a principled approach for high-dimensional financial\nsimulation with limited data.", "published": "2025-04-09 04:01:35", "link": "http://arxiv.org/abs/2504.06566v1", "categories": ["q-fin.ST", "cs.LG", "q-fin.MF"], "primary_category": "q-fin.ST"}
{"title": "Polyspectral Mean based Time Series Clustering of Indian Stock Market", "abstract": "In this study, we employ k-means clustering algorithm of polyspectral means\nto analyze 49 stocks in the Indian stock market. We have used spectral and\nbispectral information obtained from the data, by using spectral and bispectral\nmeans with different weight functions that will give us varying insights into\nthe temporal patterns of the stocks. In particular, the higher order\npolyspectral means can provide significantly more information than what we can\ngather from power spectra, and can also unveil nonlinear trends in a time\nseries. Through rigorous analysis, we identify five distinctive clusters,\nuncovering nuanced market structures. Notably, one cluster emerges as that of a\nconglomerate powerhouse, featuring ADANI, BIRLA, TATA, and unexpectedly,\ngovernment-owned bank SBI. Another cluster spotlights the IT sector with WIPRO\nand TCS, while a third combines private banks, government entities, and\nRELIANCE. The final cluster comprises publicly traded companies with dispersed\nownership. Such clustering of stocks sheds light on intricate financial\nrelationships within the stock market, providing valuable insights for\ninvestors and analysts navigating the dynamic landscape of the Indian stock\nmarket.", "published": "2025-04-09 16:36:39", "link": "http://arxiv.org/abs/2504.07021v1", "categories": ["q-fin.ST", "stat.AP"], "primary_category": "q-fin.ST"}
{"title": "Maximizing Battery Storage Profits via High-Frequency Intraday Trading", "abstract": "Maximizing revenue for grid-scale battery energy storage systems in\ncontinuous intraday electricity markets requires strategies that are able to\nseize trading opportunities as soon as new information arrives. This paper\nintroduces and evaluates an automated high-frequency trading strategy for\nbattery energy storage systems trading on the intraday market for power while\nexplicitly considering the dynamics of the limit order book, market rules, and\ntechnical parameters. The standard rolling intrinsic strategy is adapted for\ncontinuous intraday electricity markets and solved using a dynamic programming\napproximation that is two to three orders of magnitude faster than an exact\nmixed-integer linear programming solution. A detailed backtest over a full year\nof German order book data demonstrates that the proposed dynamic programming\nformulation does not reduce trading profits and enables the policy to react to\nevery relevant order book update, enabling realistic rapid backtesting. Our\nresults show the significant revenue potential of high-frequency trading: our\npolicy earns 58% more than when re-optimizing only once every hour and 14% more\nthan when re-optimizing once per minute, highlighting that profits critically\ndepend on trading speed. Furthermore, we leverage the speed of our algorithm to\ntrain a parametric extension of the rolling intrinsic, increasing yearly\nrevenue by 8.4% out of sample.", "published": "2025-04-09 14:38:09", "link": "http://arxiv.org/abs/2504.06932v2", "categories": ["q-fin.TR", "cs.SY", "eess.SY", "math.OC"], "primary_category": "q-fin.TR"}
{"title": "Optimal Execution and Macroscopic Market Making", "abstract": "We propose a stochastic game modelling the strategic interaction between\nmarket makers and traders of optimal execution type. For traders, the permanent\nprice impact commonly attributed to them is replaced by quoting strategies\nimplemented by market makers. For market makers, order flows become endogenous,\ndriven by tactical traders rather than assumed exogenously. Using the\nforward-backward stochastic differential equation (FBSDE) characterization of\nNash equilibria, we establish a local well-posedness result for the general\ngame. In the specific Almgren-Chriss-Avellaneda-Stoikov model, a decoupling\napproach guarantees the global well-posedness of the FBSDE system via the\nwell-posedness of an associated backward stochastic Riccati equation. Finally,\nby introducing small diffusion terms into the inventory processes, global\nwell-posedness is achieved for the approximation game.", "published": "2025-04-09 09:18:37", "link": "http://arxiv.org/abs/2504.06717v1", "categories": ["q-fin.TR"], "primary_category": "q-fin.TR"}
{"title": "Visual-Aware Speech Recognition for Noisy Scenarios", "abstract": "Humans have the ability to utilize visual cues, such as lip movements and\nvisual scenes, to enhance auditory perception, particularly in noisy\nenvironments. However, current Automatic Speech Recognition (ASR) or\nAudio-Visual Speech Recognition (AVSR) models often struggle in noisy\nscenarios. To solve this task, we propose a model that improves transcription\nby correlating noise sources to visual cues. Unlike works that rely on lip\nmotion and require the speaker's visibility, we exploit broader visual\ninformation from the environment. This allows our model to naturally filter\nspeech from noise and improve transcription, much like humans do in noisy\nscenarios. Our method re-purposes pretrained speech and visual encoders,\nlinking them with multi-headed attention. This approach enables the\ntranscription of speech and the prediction of noise labels in video inputs. We\nintroduce a scalable pipeline to develop audio-visual datasets, where visual\ncues correlate to noise in the audio. We show significant improvements over\nexisting audio-only models in noisy scenarios. Results also highlight that\nvisual cues play a vital role in improved transcription accuracy.", "published": "2025-04-09 19:09:54", "link": "http://arxiv.org/abs/2504.07229v1", "categories": ["cs.CL", "eess.AS", "eess.SP"], "primary_category": "cs.CL"}
{"title": "TASTE: Text-Aligned Speech Tokenization and Embedding for Spoken Language Modeling", "abstract": "Large Language Models (LLMs) excel in text-based natural language processing\ntasks but remain constrained by their reliance on textual inputs and outputs.\nTo enable more natural human-LLM interaction, recent progress have focused on\nderiving a spoken language model (SLM) that can not only listen but also\ngenerate speech. To achieve this, a promising direction is to conduct\nspeech-text joint modeling. However, recent SLM still lag behind text LLM due\nto the modality mismatch. One significant mismatch can be the sequence lengths\nbetween speech and text tokens. To address this, we introduce Text-Aligned\nSpeech Tokenization and Embedding (TASTE), a method that directly addresses the\nmodality gap by aligning speech token with the corresponding text transcription\nduring the tokenization stage. We propose a method that can achieve this\nthrough the special aggregation mechanism and with speech reconstruction as the\ntraining objective. We conduct extensive experiments and show that TASTE can\npreserve essential paralinguistic information while dramatically reducing the\ntoken sequence length. Furthermore, by leveraging TASTE, we can adapt\ntext-based LLMs into effective SLMs with parameter-efficient fine-tuning\ntechniques such as Low-Rank Adaptation (LoRA). Experimental results on\nbenchmark tasks, including SALMON and StoryCloze, demonstrate that TASTE-based\nSLMs perform similarly to previous full-finetuning methods. To our knowledge,\nTASTE is the first end-to-end approach that utilizes a reconstruction objective\nto automatically learn a text-aligned speech tokenization and embedding\nsuitable for spoken language modeling. Our demo, code, and models are publicly\navailable at https://github.com/mtkresearch/TASTE-SpokenLM.", "published": "2025-04-09 17:14:33", "link": "http://arxiv.org/abs/2504.07053v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "RNN-Transducer-based Losses for Speech Recognition on Noisy Targets", "abstract": "Training speech recognition systems on noisy transcripts is a significant\nchallenge in industrial pipelines, where datasets are enormous and ensuring\naccurate transcription for every instance is difficult. In this work, we\nintroduce novel loss functions to mitigate the impact of transcription errors\nin RNN-Transducer models. Our Star-Transducer loss addresses deletion errors by\nincorporating \"skip frame\" transitions in the loss lattice, restoring over 90%\nof the system's performance compared to models trained with accurate\ntranscripts. The Bypass-Transducer loss uses \"skip token\" transitions to tackle\ninsertion errors, recovering more than 60% of the quality. Finally, the\nTarget-Robust Transducer loss merges these approaches, offering robust\nperformance against arbitrary errors. Experimental results demonstrate that the\nTarget-Robust Transducer loss significantly improves RNN-T performance on noisy\ndata by restoring over 70% of the quality compared to well-transcribed data.", "published": "2025-04-09 15:18:29", "link": "http://arxiv.org/abs/2504.06963v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "CAFA: a Controllable Automatic Foley Artist", "abstract": "Foley is a key element in video production, refers to the process of adding\nan audio signal to a silent video while ensuring semantic and temporal\nalignment. In recent years, the rise of personalized content creation and\nadvancements in automatic video-to-audio models have increased the demand for\ngreater user control in the process. One possible approach is to incorporate\ntext to guide audio generation. While supported by existing methods, challenges\nremain in ensuring compatibility between modalities, particularly when the text\nintroduces additional information or contradicts the sounds naturally inferred\nfrom the visuals. In this work, we introduce CAFA (Controllable Automatic Foley\nArtist) a video-and-text-to-audio model that generates semantically and\ntemporally aligned audio for a given video, guided by text input. CAFA is built\nupon a text-to-audio model and integrates video information through a modality\nadapter mechanism. By incorporating text, users can refine semantic details and\nintroduce creative variations, guiding the audio synthesis beyond the expected\nvideo contextual cues. Experiments show that besides its superior quality in\nterms of semantic alignment and audio-visual synchronization the proposed\nmethod enable high textual controllability as demonstrated in subjective and\nobjective evaluations.", "published": "2025-04-09 10:58:54", "link": "http://arxiv.org/abs/2504.06778v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Diffusion Factor Models: Generating High-Dimensional Returns with Factor Structure", "abstract": "Financial scenario simulation is essential for risk management and portfolio\noptimization, yet it remains challenging especially in high-dimensional and\nsmall data settings common in finance. We propose a diffusion factor model that\nintegrates latent factor structure into generative diffusion processes,\nbridging econometrics with modern generative AI to address the challenges of\nthe curse of dimensionality and data scarcity in financial simulation. By\nexploiting the low-dimensional factor structure inherent in asset returns, we\ndecompose the score function--a key component in diffusion models--using\ntime-varying orthogonal projections, and this decomposition is incorporated\ninto the design of neural network architectures. We derive rigorous statistical\nguarantees, establishing nonasymptotic error bounds for both score estimation\nat O(d^{5/2} n^{-2/(k+5)}) and generated distribution at O(d^{5/4}\nn^{-1/2(k+5)}), primarily driven by the intrinsic factor dimension k rather\nthan the number of assets d, surpassing the dimension-dependent limits in the\nclassical nonparametric statistics literature and making the framework viable\nfor markets with thousands of assets. Numerical studies confirm superior\nperformance in latent subspace recovery under small data regimes. Empirical\nanalysis demonstrates the economic significance of our framework in\nconstructing mean-variance optimal portfolios and factor portfolios. This work\npresents the first theoretical integration of factor structure with diffusion\nmodels, offering a principled approach for high-dimensional financial\nsimulation with limited data. Our code is available at\nhttps://github.com/xymmmm00/diffusion_factor_model.", "published": "2025-04-09 04:01:35", "link": "http://arxiv.org/abs/2504.06566v2", "categories": ["q-fin.ST", "cs.LG", "q-fin.MF"], "primary_category": "q-fin.ST"}
{"title": "Diffusion Factor Models: Generating High-Dimensional Returns with Factor Structure", "abstract": "Financial scenario simulation is essential for risk management and portfolio\noptimization, yet it remains challenging especially in high-dimensional and\nsmall data settings common in finance. We propose a diffusion factor model that\nintegrates latent factor structure into generative diffusion processes,\nbridging econometrics with modern generative AI to address the challenges of\nthe curse of dimensionality and data scarcity in financial simulation. By\nexploiting the low-dimensional factor structure inherent in asset returns, we\ndecompose the score function--a key component in diffusion models--using\ntime-varying orthogonal projections, and this decomposition is incorporated\ninto the design of neural network architectures. We derive rigorous statistical\nguarantees, establishing nonasymptotic error bounds for both score estimation\nat O(d^{5/2} n^{-2/(k+5)}) and generated distribution at O(d^{5/4}\nn^{-1/2(k+5)}), primarily driven by the intrinsic factor dimension k rather\nthan the number of assets d, surpassing the dimension-dependent limits in the\nclassical nonparametric statistics literature and making the framework viable\nfor markets with thousands of assets. Numerical studies confirm superior\nperformance in latent subspace recovery under small data regimes. Empirical\nanalysis demonstrates the economic significance of our framework in\nconstructing mean-variance optimal portfolios and factor portfolios. This work\npresents the first theoretical integration of factor structure with diffusion\nmodels, offering a principled approach for high-dimensional financial\nsimulation with limited data. Our code is available at\nhttps://github.com/xymmmm00/diffusion_factor_model.", "published": "2025-04-09 04:01:35", "link": "http://arxiv.org/abs/2504.06566v3", "categories": ["q-fin.ST", "cs.LG", "q-fin.MF"], "primary_category": "q-fin.ST"}
{"title": "Diffusion Factor Models: Generating High-Dimensional Returns with Factor Structure", "abstract": "Financial scenario simulation is essential for risk management and portfolio\noptimization, yet it remains challenging especially in high-dimensional and\nsmall data settings common in finance. We propose a diffusion factor model that\nintegrates latent factor structure into generative diffusion processes,\nbridging econometrics with modern generative AI to address the challenges of\nthe curse of dimensionality and data scarcity in financial simulation. By\nexploiting the low-dimensional factor structure inherent in asset returns, we\ndecompose the score function--a key component in diffusion models--using\ntime-varying orthogonal projections, and this decomposition is incorporated\ninto the design of neural network architectures. We derive rigorous statistical\nguarantees, establishing nonasymptotic error bounds for both score estimation\nat O(d^{5/2} n^{-2/(k+5)}) and generated distribution at O(d^{5/4}\nn^{-1/2(k+5)}), primarily driven by the intrinsic factor dimension k rather\nthan the number of assets d, surpassing the dimension-dependent limits in the\nclassical nonparametric statistics literature and making the framework viable\nfor markets with thousands of assets. Numerical studies confirm superior\nperformance in latent subspace recovery under small data regimes. Empirical\nanalysis demonstrates the economic significance of our framework in\nconstructing mean-variance optimal portfolios and factor portfolios. This work\npresents the first theoretical integration of factor structure with diffusion\nmodels, offering a principled approach for high-dimensional financial\nsimulation with limited data. Our code is available at\nhttps://github.com/xymmmm00/diffusion_factor_model.", "published": "2025-04-09 04:01:35", "link": "http://arxiv.org/abs/2504.06566v4", "categories": ["q-fin.ST", "cs.LG", "q-fin.MF"], "primary_category": "q-fin.ST"}
