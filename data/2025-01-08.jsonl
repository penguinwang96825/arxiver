{"title": "FinSphere: A Conversational Stock Analysis Agent Equipped with Quantitative Tools based on Real-Time Database", "abstract": "Current financial Large Language Models (LLMs) struggle with two critical\nlimitations: a lack of depth in stock analysis, which impedes their ability to\ngenerate professional-grade insights, and the absence of objective evaluation\nmetrics to assess the quality of stock analysis reports. To address these\nchallenges, this paper introduces FinSphere, a conversational stock analysis\nagent, along with three major contributions: (1) Stocksis, a dataset curated by\nindustry experts to enhance LLMs' stock analysis capabilities, (2) AnalyScore,\na systematic evaluation framework for assessing stock analysis quality, and (3)\nFinSphere, an AI agent that can generate high-quality stock analysis reports in\nresponse to user queries. Experiments demonstrate that FinSphere achieves\nsuperior performance compared to both general and domain-specific LLMs, as well\nas existing agent-based systems, even when they are enhanced with real-time\ndata access and few-shot guidance. The integrated framework, which combines\nreal-time data feeds, quantitative tools, and an instruction-tuned LLM, yields\nsubstantial improvements in both analytical quality and practical applicability\nfor real-world stock analysis.", "published": "2025-01-08 07:50:50", "link": "http://arxiv.org/abs/2501.12399v1", "categories": ["cs.AI", "cs.CL", "cs.IR", "q-fin.CP"], "primary_category": "cs.AI"}
{"title": "IOLBENCH: Benchmarking LLMs on Linguistic Reasoning", "abstract": "Despite the remarkable advancements and widespread applications of deep\nneural networks, their ability to perform reasoning tasks remains limited,\nparticularly in domains requiring structured, abstract thought. In this paper,\nwe investigate the linguistic reasoning capabilities of state-of-the-art large\nlanguage models (LLMs) by introducing IOLBENCH, a novel benchmark derived from\nInternational Linguistics Olympiad (IOL) problems. This dataset encompasses\ndiverse problems testing syntax, morphology, phonology, and semantics, all\ncarefully designed to be self-contained and independent of external knowledge.\nThese tasks challenge models to engage in metacognitive linguistic reasoning,\nrequiring the deduction of linguistic rules and patterns from minimal examples.\nThrough extensive benchmarking of leading LLMs, we find that even the most\nadvanced models struggle to handle the intricacies of linguistic complexity,\nparticularly in areas demanding compositional generalization and rule\nabstraction. Our analysis highlights both the strengths and persistent\nlimitations of current models in linguistic problem-solving, offering valuable\ninsights into their reasoning capabilities. By introducing IOLBENCH, we aim to\nfoster further research into developing models capable of human-like reasoning,\nwith broader implications for the fields of computational linguistics and\nartificial intelligence.", "published": "2025-01-08 03:15:10", "link": "http://arxiv.org/abs/2501.04249v1", "categories": ["cs.CL", "I.2"], "primary_category": "cs.CL"}
{"title": "Graph-Based Multimodal Contrastive Learning for Chart Question Answering", "abstract": "Chart question answering (ChartQA) is challenged by the heterogeneous\ncomposition of chart elements and the subtle data patterns they encode. This\nwork introduces a novel joint multimodal scene graph framework that explicitly\nmodels the relationships among chart components and their underlying\nstructures. The framework integrates both visual and textual graphs to capture\nstructural and semantic characteristics, while a graph contrastive learning\nstrategy aligns node representations across modalities enabling their seamless\nincorporation into a transformer decoder as soft prompts. Moreover, a set of\ntailored Chain of Thought (CoT) prompts is proposed to enhance multimodal large\nlanguage models (MLLMs) in zero-s ot scenarios by mitigating hallucinations.\nExtensive evaluations on benchmarks including ChartQA, OpenCQA, and ChartX\ndemonstrate significant performance improvements and validate the efficacy of\nthe proposed approach.", "published": "2025-01-08 06:27:07", "link": "http://arxiv.org/abs/2501.04303v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Who Does the Giant Number Pile Like Best: Analyzing Fairness in Hiring\n  Contexts", "abstract": "Large language models (LLMs) are increasingly being deployed in high-stakes\napplications like hiring, yet their potential for unfair decision-making and\noutcomes remains understudied, particularly in generative settings. In this\nwork, we examine the fairness of LLM-based hiring systems through two\nreal-world tasks: resume summarization and retrieval. By constructing a\nsynthetic resume dataset and curating job postings, we investigate whether\nmodel behavior differs across demographic groups and is sensitive to\ndemographic perturbations. Our findings reveal that race-based differences\nappear in approximately 10% of generated summaries, while gender-based\ndifferences occur in only 1%. In the retrieval setting, all evaluated models\ndisplay non-uniform selection patterns across demographic groups and exhibit\nhigh sensitivity to both gender and race-based perturbations. Surprisingly,\nretrieval models demonstrate comparable sensitivity to non-demographic changes,\nsuggesting that fairness issues may stem, in part, from general brittleness\nissues. Overall, our results indicate that LLM-based hiring systems, especially\nat the retrieval stage, can exhibit notable biases that lead to discriminatory\noutcomes in real-world contexts.", "published": "2025-01-08 07:28:10", "link": "http://arxiv.org/abs/2501.04316v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding Before Reasoning: Enhancing Chain-of-Thought with\n  Iterative Summarization Pre-Prompting", "abstract": "Chain-of-Thought (CoT) Prompting is a dominant paradigm in Large Language\nModels (LLMs) to enhance complex reasoning. It guides LLMs to present\nmulti-step reasoning, rather than generating the final answer directly.\nHowever, CoT encounters difficulties when key information required for\nreasoning is implicit or missing. This occurs because CoT emphasizes the\nsequence of reasoning steps while overlooking the early extraction of essential\ninformation. We propose a pre-prompting method called Iterative Summarization\nPre-Prompting (ISP^2) to refine LLM reasoning when key information is not\nexplicitly provided. First, entities and their corresponding descriptions are\nextracted to form potential key information pairs. Next, we use a reliability\nrating to assess these pairs, then merge the two lowest-ranked pairs into a new\nentity description. This process is repeated until a unique key information\npair is obtained. Finally, that pair, along with the original question, is fed\ninto LLMs to produce the answer. Extensive experiments demonstrate a 7.1%\nimprovement compared to existing methods. Unlike traditional prompting, ISP^2\nadopts an inductive approach with pre-prompting, offering flexible integration\ninto diverse reasoning frameworks. The code is available at\nhttps://github.com/zdhgreat/ISP-2.", "published": "2025-01-08 08:26:56", "link": "http://arxiv.org/abs/2501.04341v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SEO: Stochastic Experience Optimization for Large Language Models", "abstract": "Large Language Models (LLMs) can benefit from useful experiences to improve\ntheir performance on specific tasks. However, finding helpful experiences for\ndifferent LLMs is not obvious, since it is unclear what experiences suit\nspecific LLMs. Previous studies intended to automatically find useful\nexperiences using LLMs, while it is difficult to ensure the effectiveness of\nthe obtained experience. In this paper, we propose Stochastic Experience\nOptimization (SEO), an iterative approach that finds optimized model-specific\nexperience without modifying model parameters through experience update in\nnatural language. In SEO, we propose a stochastic validation method to ensure\nthe update direction of experience, avoiding unavailing updates. Experimental\nresults on three tasks for three LLMs demonstrate that experiences optimized by\nSEO can achieve consistently improved performance. Further analysis indicates\nthat SEO-optimized experience can generalize to out-of-distribution data,\nboosting the performance of LLMs on similar tasks.", "published": "2025-01-08 10:10:29", "link": "http://arxiv.org/abs/2501.04393v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "End-to-End Bangla AI for Solving Math Olympiad Problem Benchmark:\n  Leveraging Large Language Model Using Integrated Approach", "abstract": "This work introduces systematic approach for enhancing large language models\n(LLMs) to address Bangla AI mathematical challenges. Through the assessment of\ndiverse LLM configurations, fine-tuning with specific datasets, and the\nimplementation of Retrieval-Augmented Generation (RAG), we enhanced the model's\nreasoning precision in a multilingual setting. Crucial discoveries indicate\nthat customized prompting, dataset augmentation, and iterative reasoning\nimprove the model's efficiency regarding Olympiad-level mathematical\nchallenges.", "published": "2025-01-08 11:18:36", "link": "http://arxiv.org/abs/2501.04425v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "When LLMs Struggle: Reference-less Translation Evaluation for\n  Low-resource Languages", "abstract": "This paper investigates the reference-less evaluation of machine translation\nfor low-resource language pairs, known as quality estimation (QE).\nSegment-level QE is a challenging cross-lingual language understanding task\nthat provides a quality score (0-100) to the translated output. We\ncomprehensively evaluate large language models (LLMs) in zero/few-shot\nscenarios and perform instruction fine-tuning using a novel prompt based on\nannotation guidelines. Our results indicate that prompt-based approaches are\noutperformed by the encoder-based fine-tuned QE models. Our error analysis\nreveals tokenization issues, along with errors due to transliteration and named\nentities, and argues for refinement in LLM pre-training for cross-lingual\ntasks. We release the data, and models trained publicly for further research.", "published": "2025-01-08 12:54:05", "link": "http://arxiv.org/abs/2501.04473v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PolInterviews -- A Dataset of German Politician Public Broadcast\n  Interviews", "abstract": "This paper presents a novel dataset of public broadcast interviews featuring\nhigh-ranking German politicians. The interviews were sourced from YouTube,\ntranscribed, processed for speaker identification, and stored in a tidy and\nopen format. The dataset comprises 99 interviews with 33 different German\npoliticians across five major interview formats, containing a total of 28,146\nsentences. As the first of its kind, this dataset offers valuable opportunities\nfor research on various aspects of political communication in the (German)\npolitical contexts, such as agenda-setting, interviewer dynamics, or\npoliticians' self-presentation.", "published": "2025-01-08 13:09:45", "link": "http://arxiv.org/abs/2501.04484v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep\n  Thinking", "abstract": "We present rStar-Math to demonstrate that small language models (SLMs) can\nrival or even surpass the math reasoning capability of OpenAI o1, without\ndistillation from superior models. rStar-Math achieves this by exercising \"deep\nthinking\" through Monte Carlo Tree Search (MCTS), where a math policy SLM\nperforms test-time search guided by an SLM-based process reward model.\nrStar-Math introduces three innovations to tackle the challenges in training\nthe two SLMs: (1) a novel code-augmented CoT data sythesis method, which\nperforms extensive MCTS rollouts to generate step-by-step verified reasoning\ntrajectories used to train the policy SLM; (2) a novel process reward model\ntraining method that avoids na\\\"ive step-level score annotation, yielding a\nmore effective process preference model (PPM); (3) a self-evolution recipe in\nwhich the policy SLM and PPM are built from scratch and iteratively evolved to\nimprove reasoning capabilities. Through 4 rounds of self-evolution with\nmillions of synthesized solutions for 747k math problems, rStar-Math boosts\nSLMs' math reasoning to state-of-the-art levels. On the MATH benchmark, it\nimproves Qwen2.5-Math-7B from 58.8% to 90.0% and Phi3-mini-3.8B from 41.4% to\n86.4%, surpassing o1-preview by +4.5% and +0.9%. On the USA Math Olympiad\n(AIME), rStar-Math solves an average of 53.3% (8/15) of problems, ranking among\nthe top 20% the brightest high school math students. Code and data will be\navailable at https://github.com/microsoft/rStar.", "published": "2025-01-08 14:12:57", "link": "http://arxiv.org/abs/2501.04519v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On The Origin of Cultural Biases in Language Models: From Pre-training\n  Data to Linguistic Phenomena", "abstract": "Language Models (LMs) have been shown to exhibit a strong preference towards\nentities associated with Western culture when operating in non-Western\nlanguages. In this paper, we aim to uncover the origins of entity-related\ncultural biases in LMs by analyzing several contributing factors, including the\nrepresentation of entities in pre-training data and the impact of variations in\nlinguistic phenomena across languages. We introduce CAMeL-2, a parallel\nArabic-English benchmark of 58,086 entities associated with Arab and Western\ncultures and 367 masked natural contexts for entities. Our evaluations using\nCAMeL-2 reveal reduced performance gaps between cultures by LMs when tested in\nEnglish compared to Arabic. We find that LMs struggle in Arabic with entities\nthat appear at high frequencies in pre-training, where entities can hold\nmultiple word senses. This also extends to entities that exhibit high lexical\noverlap with languages that are not Arabic but use the Arabic script. Further,\nwe show how frequency-based tokenization leads to this issue in LMs, which gets\nworse with larger Arabic vocabularies. We will make CAMeL-2 available at:\nhttps://github.com/tareknaous/camel2", "published": "2025-01-08 18:15:47", "link": "http://arxiv.org/abs/2501.04662v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cued Speech Generation Leveraging a Pre-trained Audiovisual\n  Text-to-Speech Model", "abstract": "This paper presents a novel approach for the automatic generation of Cued\nSpeech (ACSG), a visual communication system used by people with hearing\nimpairment to better elicit the spoken language. We explore transfer learning\nstrategies by leveraging a pre-trained audiovisual autoregressive\ntext-to-speech model (AVTacotron2). This model is reprogrammed to infer Cued\nSpeech (CS) hand and lip movements from text input. Experiments are conducted\non two publicly available datasets, including one recorded specifically for\nthis study. Performance is assessed using an automatic CS recognition system.\nWith a decoding accuracy at the phonetic level reaching approximately 77%, the\nresults demonstrate the effectiveness of our approach.", "published": "2025-01-08 19:26:43", "link": "http://arxiv.org/abs/2501.04799v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Building Foundations for Natural Language Processing of Historical\n  Turkish: Resources and Models", "abstract": "This paper introduces foundational resources and models for natural language\nprocessing (NLP) of historical Turkish, a domain that has remained\nunderexplored in computational linguistics. We present the first named entity\nrecognition (NER) dataset, HisTR and the first Universal Dependencies treebank,\nOTA-BOUN for a historical form of the Turkish language along with\ntransformer-based models trained using these datasets for named entity\nrecognition, dependency parsing, and part-of-speech tagging tasks.\nAdditionally, we introduce Ottoman Text Corpus (OTC), a clean corpus of\ntransliterated historical Turkish texts that spans a wide range of historical\nperiods. Our experimental results show significant improvements in the\ncomputational analysis of historical Turkish, achieving promising results in\ntasks that require understanding of historical linguistic structures. They also\nhighlight existing challenges, such as domain adaptation and language\nvariations across time periods. All of the presented resources and models are\nmade available at https://huggingface.co/bucolin to serve as a benchmark for\nfuture progress in historical Turkish NLP.", "published": "2025-01-08 20:29:00", "link": "http://arxiv.org/abs/2501.04828v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Advancing Retrieval-Augmented Generation for Persian: Development of\n  Language Models, Comprehensive Benchmarks, and Best Practices for\n  Optimization", "abstract": "This paper examines the specific obstacles of constructing\nRetrieval-Augmented Generation(RAG) systems in low-resource languages, with a\nfocus on Persian's complicated morphology and versatile syntax. The research\naims to improve retrieval and generation accuracy by introducing\nPersian-specific models, namely MatinaRoberta(a masked language model) and\nMatinaSRoberta(a fine-tuned Sentence-BERT), along with a comprehensive\nbenchmarking framework. Three datasets-general knowledge(PQuad), scientifically\nspecialized texts, and organizational reports, were used to assess these models\nafter they were trained on a varied corpus of 73.11 billion Persian tokens. The\nmethodology involved extensive pretraining, fine-tuning with tailored loss\nfunctions, and systematic evaluations using both traditional metrics and the\nRetrieval-Augmented Generation Assessment framework. The results show that\nMatinaSRoberta outperformed previous embeddings, achieving superior contextual\nrelevance and retrieval accuracy across datasets. Temperature tweaking, chunk\nsize modifications, and document summary indexing were explored to enhance RAG\nsetups. Larger models like Llama-3.1 (70B) consistently demonstrated the\nhighest generation accuracy, while smaller models faced challenges with\ndomain-specific and formal contexts. The findings underscore the potential for\ndeveloping RAG systems in Persian through customized embeddings and\nretrieval-generation settings and highlight the enhancement of NLP applications\nsuch as search engines and legal document analysis in low-resource languages.", "published": "2025-01-08 22:16:40", "link": "http://arxiv.org/abs/2501.04858v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Future of AI: Exploring the Potential of Large Concept Models", "abstract": "The field of Artificial Intelligence (AI) continues to drive transformative\ninnovations, with significant progress in conversational interfaces, autonomous\nvehicles, and intelligent content creation. Since the launch of ChatGPT in late\n2022, the rise of Generative AI has marked a pivotal era, with the term Large\nLanguage Models (LLMs) becoming a ubiquitous part of daily life. LLMs have\ndemonstrated exceptional capabilities in tasks such as text summarization, code\ngeneration, and creative writing. However, these models are inherently limited\nby their token-level processing, which restricts their ability to perform\nabstract reasoning, conceptual understanding, and efficient generation of\nlong-form content. To address these limitations, Meta has introduced Large\nConcept Models (LCMs), representing a significant shift from traditional\ntoken-based frameworks. LCMs use concepts as foundational units of\nunderstanding, enabling more sophisticated semantic reasoning and context-aware\ndecision-making. Given the limited academic research on this emerging\ntechnology, our study aims to bridge the knowledge gap by collecting,\nanalyzing, and synthesizing existing grey literature to provide a comprehensive\nunderstanding of LCMs. Specifically, we (i) identify and describe the features\nthat distinguish LCMs from LLMs, (ii) explore potential applications of LCMs\nacross multiple domains, and (iii) propose future research directions and\npractical strategies to advance LCM development and adoption.", "published": "2025-01-08 18:18:37", "link": "http://arxiv.org/abs/2501.05487v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLM4SR: A Survey on Large Language Models for Scientific Research", "abstract": "In recent years, the rapid advancement of Large Language Models (LLMs) has\ntransformed the landscape of scientific research, offering unprecedented\nsupport across various stages of the research cycle. This paper presents the\nfirst systematic survey dedicated to exploring how LLMs are revolutionizing the\nscientific research process. We analyze the unique roles LLMs play across four\ncritical stages of research: hypothesis discovery, experiment planning and\nimplementation, scientific writing, and peer reviewing. Our review\ncomprehensively showcases the task-specific methodologies and evaluation\nbenchmarks. By identifying current challenges and proposing future research\ndirections, this survey not only highlights the transformative potential of\nLLMs, but also aims to inspire and guide researchers and practitioners in\nleveraging LLMs to advance scientific inquiry. Resources are available at the\nfollowing repository: https://github.com/du-nlp-lab/LLM4SR", "published": "2025-01-08 06:44:02", "link": "http://arxiv.org/abs/2501.04306v1", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "NSA: Neuro-symbolic ARC Challenge", "abstract": "The Abstraction and Reasoning Corpus (ARC) evaluates general reasoning\ncapabilities that are difficult for both machine learning models and\ncombinatorial search methods. We propose a neuro-symbolic approach that\ncombines a transformer for proposal generation with combinatorial search using\na domain-specific language. The transformer narrows the search space by\nproposing promising search directions, which allows the combinatorial search to\nfind the actual solution in short time. We pre-train the trainsformer with\nsynthetically generated data. During test-time we generate additional\ntask-specific training tasks and fine-tune our model. Our results surpass\ncomparable state of the art on the ARC evaluation set by 27% and compare\nfavourably on the ARC train set. We make our code and dataset publicly\navailable at https://github.com/Batorskq/NSA.", "published": "2025-01-08 11:17:40", "link": "http://arxiv.org/abs/2501.04424v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Hidden Entity Detection from GitHub Leveraging Large Language Models", "abstract": "Named entity recognition is an important task when constructing knowledge\nbases from unstructured data sources. Whereas entity detection methods mostly\nrely on extensive training data, Large Language Models (LLMs) have paved the\nway towards approaches that rely on zero-shot learning (ZSL) or few-shot\nlearning (FSL) by taking advantage of the capabilities LLMs acquired during\npretraining. Specifically, in very specialized scenarios where large-scale\ntraining data is not available, ZSL / FSL opens new opportunities. This paper\nfollows this recent trend and investigates the potential of leveraging Large\nLanguage Models (LLMs) in such scenarios to automatically detect datasets and\nsoftware within textual content from GitHub repositories. While existing\nmethods focused solely on named entities, this study aims to broaden the scope\nby incorporating resources such as repositories and online hubs where entities\nare also represented by URLs. The study explores different FSL prompt learning\napproaches to enhance the LLMs' ability to identify dataset and software\nmentions within repository texts. Through analyses of LLM effectiveness and\nlearning strategies, this paper offers insights into the potential of advanced\nlanguage models for automated entity detection.", "published": "2025-01-08 12:18:11", "link": "http://arxiv.org/abs/2501.04455v1", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "Improving Image Captioning by Mimicking Human Reformulation Feedback at\n  Inference-time", "abstract": "Incorporating automatically predicted human feedback into the process of\ntraining generative models has attracted substantial recent interest, while\nfeedback at inference time has received less attention. The typical feedback at\ntraining time, i.e., preferences of choice given two samples, does not\nnaturally transfer to the inference phase. We introduce a novel type of\nfeedback -- caption reformulations -- and train models to mimic reformulation\nfeedback based on human annotations. Our method does not require training the\nimage captioning model itself, thereby demanding substantially less\ncomputational effort. We experiment with two types of reformulation feedback:\nfirst, we collect a dataset of human reformulations that correct errors in the\ngenerated captions. We find that incorporating reformulation models trained on\nthis data into the inference phase of existing image captioning models results\nin improved captions, especially when the original captions are of low quality.\nWe apply our method to non-English image captioning, a domain where robust\nmodels are less prevalent, and gain substantial improvement. Second, we apply\nreformulations to style transfer. Quantitative evaluations reveal\nstate-of-the-art performance on German image captioning and English style\ntransfer, while human validation with a detailed comparative framework exposes\nthe specific axes of improvement.", "published": "2025-01-08 14:00:07", "link": "http://arxiv.org/abs/2501.04513v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "OpenOmni: Advancing Open-Source Omnimodal Large Language Models with\n  Progressive Multimodal Alignment and Real-Time Self-Aware Emotional Speech\n  Synthesis", "abstract": "Recent advancements in omnimodal learning have significantly improved\nunderstanding and generation across images, text, and speech, yet these\ndevelopments remain predominantly confined to proprietary models. The lack of\nhigh-quality omnimodal datasets and the challenges of real-time emotional\nspeech synthesis have notably hindered progress in open-source research. To\naddress these limitations, we introduce \\name, a two-stage training framework\nthat integrates omnimodal alignment and speech generation to develop a\nstate-of-the-art omnimodal large language model. In the alignment phase, a\npre-trained speech model undergoes further training on text-image tasks,\nenabling (near) zero-shot generalization from vision to speech, outperforming\nmodels trained on tri-modal datasets. In the speech generation phase, a\nlightweight decoder is trained on speech tasks with direct preference\noptimization, enabling real-time emotional speech synthesis with high fidelity.\nExperiments show that \\name surpasses state-of-the-art models across omnimodal,\nvision-language, and speech-language benchmarks. It achieves a 4-point absolute\nimprovement on OmniBench over the leading open-source model VITA, despite using\n5x fewer training samples and a smaller model size (7B vs. 7x8B). Additionally,\n\\name achieves real-time speech generation with <1s latency at\nnon-autoregressive mode, reducing inference time by 5x compared to\nautoregressive methods, and improves emotion classification accuracy by 7.7\\%", "published": "2025-01-08 15:18:09", "link": "http://arxiv.org/abs/2501.04561v4", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Assessing Language Comprehension in Large Language Models Using\n  Construction Grammar", "abstract": "Large Language Models, despite their significant capabilities, are known to\nfail in surprising and unpredictable ways. Evaluating their true\n`understanding' of language is particularly challenging due to the extensive\nweb-scale data they are trained on. Therefore, we construct an evaluation to\nsystematically assess natural language understanding (NLU) in LLMs by\nleveraging Construction Grammar (CxG), which provides insights into the meaning\ncaptured by linguistic elements known as constructions (Cxns). CxG is\nwell-suited for this purpose because provides a theoretical basis to construct\ntargeted evaluation sets. These datasets are carefully constructed to include\nexamples which are unlikely to appear in pre-training data, yet intuitive and\neasy for humans to understand, enabling a more targeted and reliable\nassessment. Our experiments focus on downstream natural language inference and\nreasoning tasks by comparing LLMs' understanding of the underlying meanings\ncommunicated through 8 unique Cxns with that of humans. The results show that\nwhile LLMs demonstrate some knowledge of constructional information, even the\nlatest models including GPT-o1 struggle with abstract meanings conveyed by\nthese Cxns, as demonstrated in cases where test sentences are dissimilar to\ntheir pre-training data. We argue that such cases provide a more accurate test\nof true language understanding, highlighting key limitations in LLMs' semantic\ncapabilities. We make our novel dataset and associated experimental data\nincluding prompts and model responses publicly available.", "published": "2025-01-08 18:15:10", "link": "http://arxiv.org/abs/2501.04661v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards System 2 Reasoning in LLMs: Learning How to Think With Meta\n  Chain-of-Thought", "abstract": "We propose a novel framework, Meta Chain-of-Thought (Meta-CoT), which extends\ntraditional Chain-of-Thought (CoT) by explicitly modeling the underlying\nreasoning required to arrive at a particular CoT. We present empirical evidence\nfrom state-of-the-art models exhibiting behaviors consistent with in-context\nsearch, and explore methods for producing Meta-CoT via process supervision,\nsynthetic data generation, and search algorithms. Finally, we outline a\nconcrete pipeline for training a model to produce Meta-CoTs, incorporating\ninstruction tuning with linearized search traces and reinforcement learning\npost-training. Finally, we discuss open research questions, including scaling\nlaws, verifier roles, and the potential for discovering novel reasoning\nalgorithms. This work provides a theoretical and practical roadmap to enable\nMeta-CoT in LLMs, paving the way for more powerful and human-like reasoning in\nartificial intelligence.", "published": "2025-01-08 18:42:48", "link": "http://arxiv.org/abs/2501.04682v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "EpiCoder: Encompassing Diversity and Complexity in Code Generation", "abstract": "Effective instruction tuning is indispensable for optimizing code LLMs,\naligning model behavior with user expectations and enhancing model performance\nin real-world applications. However, most existing methods focus on code\nsnippets, which are limited to specific functionalities and rigid structures,\nrestricting the complexity and diversity of the synthesized data. To address\nthese limitations, we introduce a novel feature tree-based synthesis framework\ninspired by Abstract Syntax Trees (AST). Unlike AST, which captures syntactic\nstructure of code, our framework models semantic relationships between code\nelements, enabling the generation of more nuanced and diverse data. The feature\ntree is constructed from raw data and refined iteratively to increase the\nquantity and diversity of the extracted features. This process enables the\nidentification of more complex patterns and relationships within the code. By\nsampling subtrees with controlled depth and breadth, our framework allows\nprecise adjustments to the complexity of the generated code, supporting a wide\nrange of tasks from simple function-level operations to intricate multi-file\nscenarios. We fine-tuned widely-used base models to create the EpiCoder series,\nachieving state-of-the-art performance at both the function and file levels\nacross multiple benchmarks. Notably, empirical evidence indicates that our\napproach shows significant potential in synthesizing highly complex\nrepository-level code data. Further analysis elucidates the merits of this\napproach by rigorously assessing data complexity and diversity through software\nengineering principles and LLM-as-a-judge method.", "published": "2025-01-08 18:58:15", "link": "http://arxiv.org/abs/2501.04694v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Reproducing HotFlip for Corpus Poisoning Attacks in Dense Retrieval", "abstract": "HotFlip is a topical gradient-based word substitution method for attacking\nlanguage models. Recently, this method has been further applied to attack\nretrieval systems by generating malicious passages that are injected into a\ncorpus, i.e., corpus poisoning. However, HotFlip is known to be computationally\ninefficient, with the majority of time being spent on gradient accumulation for\neach query-passage pair during the adversarial token generation phase, making\nit impossible to generate an adequate number of adversarial passages in a\nreasonable amount of time. Moreover, the attack method itself assumes access to\na set of user queries, a strong assumption that does not correspond to how\nreal-world adversarial attacks are usually performed. In this paper, we first\nsignificantly boost the efficiency of HotFlip, reducing the adversarial\ngeneration process from 4 hours per document to only 15 minutes, using the same\nhardware. We further contribute experiments and analysis on two additional\ntasks: (1) transfer-based black-box attacks, and (2) query-agnostic attacks.\nWhenever possible, we provide comparisons between the original method and our\nimproved version. Our experiments demonstrate that HotFlip can effectively\nattack a variety of dense retrievers, with an observed trend that its attack\nperformance diminishes against more advanced and recent methods. Interestingly,\nwe observe that while HotFlip performs poorly in a black-box setting,\nindicating limited capacity for generalization, in query-agnostic scenarios its\nperformance is correlated to the volume of injected adversarial passages.", "published": "2025-01-08 19:29:33", "link": "http://arxiv.org/abs/2501.04802v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Leveraging Log Probabilities in Language Models to Forecast Future\n  Events", "abstract": "In the constantly changing field of data-driven decision making, accurately\npredicting future events is crucial for strategic planning in various sectors.\nThe emergence of Large Language Models (LLMs) marks a significant advancement\nin this area, offering advanced tools that utilise extensive text data for\nprediction. In this industry paper, we introduce a novel method for AI-driven\nforesight using LLMs. Building on top of previous research, we employ data on\ncurrent trends and their trajectories for generating forecasts on 15 different\ntopics. Subsequently, we estimate their probabilities via a multi-step approach\nbased on log probabilities. We show we achieve a Brier score of 0.186, meaning\na +26% improvement over random chance and a +19% improvement over\nwidely-available AI systems.", "published": "2025-01-08 23:28:28", "link": "http://arxiv.org/abs/2501.04880v1", "categories": ["cs.CL", "cs.LG", "60-08", "I.2.3; I.2.7"], "primary_category": "cs.CL"}
{"title": "Agent Laboratory: Using LLM Agents as Research Assistants", "abstract": "Historically, scientific discovery has been a lengthy and costly process,\ndemanding substantial time and resources from initial conception to final\nresults. To accelerate scientific discovery, reduce research costs, and improve\nresearch quality, we introduce Agent Laboratory, an autonomous LLM-based\nframework capable of completing the entire research process. This framework\naccepts a human-provided research idea and progresses through three\nstages--literature review, experimentation, and report writing to produce\ncomprehensive research outputs, including a code repository and a research\nreport, while enabling users to provide feedback and guidance at each stage. We\ndeploy Agent Laboratory with various state-of-the-art LLMs and invite multiple\nresearchers to assess its quality by participating in a survey, providing human\nfeedback to guide the research process, and then evaluate the final paper. We\nfound that: (1) Agent Laboratory driven by o1-preview generates the best\nresearch outcomes; (2) The generated machine learning code is able to achieve\nstate-of-the-art performance compared to existing methods; (3) Human\ninvolvement, providing feedback at each stage, significantly improves the\noverall quality of research; (4) Agent Laboratory significantly reduces\nresearch expenses, achieving an 84% decrease compared to previous autonomous\nresearch methods. We hope Agent Laboratory enables researchers to allocate more\neffort toward creative ideation rather than low-level coding and writing,\nultimately accelerating scientific discovery.", "published": "2025-01-08 01:58:42", "link": "http://arxiv.org/abs/2501.04227v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.HC"}
{"title": "TimelineKGQA: A Comprehensive Question-Answer Pair Generator for\n  Temporal Knowledge Graphs", "abstract": "Question answering over temporal knowledge graphs (TKGs) is crucial for\nunderstanding evolving facts and relationships, yet its development is hindered\nby limited datasets and difficulties in generating custom QA pairs. We propose\na novel categorization framework based on timeline-context relationships, along\nwith \\textbf{TimelineKGQA}, a universal temporal QA generator applicable to any\nTKGs. The code is available at: \\url{https://github.com/PascalSun/TimelineKGQA}\nas an open source Python package.", "published": "2025-01-08 08:30:44", "link": "http://arxiv.org/abs/2501.04343v1", "categories": ["cs.LO", "cs.AI", "cs.CL"], "primary_category": "cs.LO"}
{"title": "Developing a Modular Compiler for a Subset of a C-like Language", "abstract": "The paper introduces the development of a modular compiler for a subset of a\nC-like language, which addresses the challenges in constructing a compiler for\nhigh-level languages. This modular approach will allow developers to modify a\nlanguage by adding or removing subsets as required, resulting in a minimal and\nmemory-efficient compiler. The development process is divided into small,\nincremental steps, where each step yields a fully functioning compiler for an\nexpanding subset of the language. The paper outlines the iterative\ndevelopmental phase of the compiler, emphasizing progressive enhancements in\ncapabilities and functionality. Adherence to industry best practices of modular\ndesign, code reusability, and documentation has enabled the resulting\ncompiler's functional efficiency, maintainability, and extensibility. The\ncompiler proved to be effective not only in managing the language structure but\nalso in developing optimized code, which demonstrates its practical usability.\nThis was also further assessed using the compiler on a tiny memory-deficient\nsingle-board computer, again showing the compiler's efficiency and suitability\nfor resource-constrained devices.", "published": "2025-01-08 13:42:54", "link": "http://arxiv.org/abs/2501.04503v1", "categories": ["cs.PL", "cs.CL", "cs.DC", "cs.PF"], "primary_category": "cs.PL"}
{"title": "Supervision-free Vision-Language Alignment", "abstract": "Vision-language models (VLMs) have demonstrated remarkable potential in\nintegrating visual and linguistic information, but their performance is often\nconstrained by the need for extensive, high-quality image-text training data.\nCuration of these image-text pairs is both time-consuming and computationally\nexpensive. To address this challenge, we introduce SVP (Supervision-free Visual\nProjection), a novel framework that enhances vision-language alignment without\nrelying on curated data or preference annotation. SVP leverages self-captioning\nand a pre-trained grounding model as a feedback mechanism to elicit latent\ninformation in VLMs. We evaluate our approach across six key areas: captioning,\nreferring, visual question answering, multitasking, hallucination control, and\nobject recall. Results demonstrate significant improvements, including a 14%\naverage improvement in captioning tasks, up to 12% increase in object recall,\nand substantial reduction in hallucination rates. Notably, a small VLM using\nSVP achieves hallucination reductions comparable to a model five times larger,\nwhile a VLM with initially poor referring capabilities more than doubles its\nperformance, approaching parity with a model twice its size.", "published": "2025-01-08 15:32:12", "link": "http://arxiv.org/abs/2501.04568v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "InfiGUIAgent: A Multimodal Generalist GUI Agent with Native Reasoning\n  and Reflection", "abstract": "Graphical User Interface (GUI) Agents, powered by multimodal large language\nmodels (MLLMs), have shown great potential for task automation on computing\ndevices such as computers and mobile phones. However, existing agents face\nchallenges in multi-step reasoning and reliance on textual annotations,\nlimiting their effectiveness. We introduce \\textit{InfiGUIAgent}, an MLLM-based\nGUI Agent trained with a two-stage supervised fine-tuning pipeline. Stage 1\nenhances fundamental skills such as GUI understanding and grounding, while\nStage 2 integrates hierarchical reasoning and expectation-reflection reasoning\nskills using synthesized data to enable native reasoning abilities of the\nagents. \\textit{InfiGUIAgent} achieves competitive performance on several GUI\nbenchmarks, highlighting the impact of native reasoning skills in enhancing GUI\ninteraction for automation tasks. Resources are available at\n\\url{https://github.com/Reallm-Labs/InfiGUIAgent}.", "published": "2025-01-08 15:45:21", "link": "http://arxiv.org/abs/2501.04575v1", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI"}
{"title": "Quantum-inspired Embeddings Projection and Similarity Metrics for\n  Representation Learning", "abstract": "Over the last decade, representation learning, which embeds complex\ninformation extracted from large amounts of data into dense vector spaces, has\nemerged as a key technique in machine learning. Among other applications, it\nhas been a key building block for large language models and advanced computer\nvision systems based on contrastive learning. A core component of\nrepresentation learning systems is the projection head, which maps the original\nembeddings into different, often compressed spaces, while preserving the\nsimilarity relationship between vectors.\n  In this paper, we propose a quantum-inspired projection head that includes a\ncorresponding quantum-inspired similarity metric. Specifically, we map\nclassical embeddings onto quantum states in Hilbert space and introduce a\nquantum circuit-based projection head to reduce embedding dimensionality. To\nevaluate the effectiveness of this approach, we extended the BERT language\nmodel by integrating our projection head for embedding compression. We compared\nthe performance of embeddings, which were compressed using our quantum-inspired\nprojection head, with those compressed using a classical projection head on\ninformation retrieval tasks using the TREC 2019 and TREC 2020 Deep Learning\nbenchmarks. The results demonstrate that our quantum-inspired method achieves\ncompetitive performance relative to the classical method while utilizing 32\ntimes fewer parameters. Furthermore, when trained from scratch, it notably\nexcels, particularly on smaller datasets. This work not only highlights the\neffectiveness of the quantum-inspired approach but also emphasizes the utility\nof efficient, ad hoc low-entanglement circuit simulations within neural\nnetworks as a powerful quantum-inspired technique.", "published": "2025-01-08 16:11:31", "link": "http://arxiv.org/abs/2501.04591v1", "categories": ["cs.CL", "cond-mat.dis-nn", "quant-ph"], "primary_category": "cs.CL"}
{"title": "FlairGPT: Repurposing LLMs for Interior Designs", "abstract": "Interior design involves the careful selection and arrangement of objects to\ncreate an aesthetically pleasing, functional, and harmonized space that aligns\nwith the client's design brief. This task is particularly challenging, as a\nsuccessful design must not only incorporate all the necessary objects in a\ncohesive style, but also ensure they are arranged in a way that maximizes\naccessibility, while adhering to a variety of affordability and usage\nconsiderations. Data-driven solutions have been proposed, but these are\ntypically room- or domain-specific and lack explainability in their design\ndesign considerations used in producing the final layout. In this paper, we\ninvestigate if large language models (LLMs) can be directly utilized for\ninterior design. While we find that LLMs are not yet capable of generating\ncomplete layouts, they can be effectively leveraged in a structured manner,\ninspired by the workflow of interior designers. By systematically probing LLMs,\nwe can reliably generate a list of objects along with relevant constraints that\nguide their placement. We translate this information into a design layout\ngraph, which is then solved using an off-the-shelf constrained optimization\nsetup to generate the final layouts. We benchmark our algorithm in various\ndesign configurations against existing LLM-based methods and human designs, and\nevaluate the results using a variety of quantitative and qualitative metrics\nalong with user studies. In summary, we demonstrate that LLMs, when used in a\nstructured manner, can effectively generate diverse high-quality layouts,\nmaking them a viable solution for creating large-scale virtual scenes. Project\nwebpage at https://flairgpt.github.io/", "published": "2025-01-08 18:01:49", "link": "http://arxiv.org/abs/2501.04648v1", "categories": ["cs.GR", "cs.CL", "cs.CV"], "primary_category": "cs.GR"}
{"title": "Multi-task retriever fine-tuning for domain-specific and efficient RAG", "abstract": "Retrieval-Augmented Generation (RAG) has become ubiquitous when deploying\nLarge Language Models (LLMs), as it can address typical limitations such as\ngenerating hallucinated or outdated information. However, when building\nreal-world RAG applications, practical issues arise. First, the retrieved\ninformation is generally domain-specific. Since it is computationally expensive\nto fine-tune LLMs, it is more feasible to fine-tune the retriever to improve\nthe quality of the data included in the LLM input. Second, as more applications\nare deployed in the same real-world system, one cannot afford to deploy\nseparate retrievers. Moreover, these RAG applications normally retrieve\ndifferent kinds of data. Our solution is to instruction fine-tune a small\nretriever encoder on a variety of domain-specific tasks to allow us to deploy\none encoder that can serve many use cases, thereby achieving low-cost,\nscalability, and speed. We show how this encoder generalizes to out-of-domain\nsettings as well as to an unseen retrieval task on real-world enterprise use\ncases.", "published": "2025-01-08 18:05:30", "link": "http://arxiv.org/abs/2501.04652v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Enhancing Financial VQA in Vision Language Models using Intermediate\n  Structured Representations", "abstract": "Chart interpretation is crucial for visual data analysis, but accurately\nextracting information from charts poses significant challenges for automated\nmodels. This study investigates the fine-tuning of DEPLOT, a modality\nconversion module that translates the image of a plot or chart to a linearized\ntable, on a custom dataset of 50,000 bar charts. The dataset comprises simple,\nstacked, and grouped bar charts, targeting the unique structural features of\nthese visualizations. The finetuned DEPLOT model is evaluated against its base\nversion using a test set of 1,000 images and two metrics: Relative Mapping\nSimilarity (RMS), which measures categorical mapping accuracy, and Relative\nNumber Set Similarity (RNSS), which evaluates numerical interpretation\naccuracy. To further explore the reasoning capabilities of large language\nmodels (LLMs), we curate an additional set of 100 bar chart images paired with\nquestion answer sets. Our findings demonstrate that providing a structured\nintermediate table alongside the image significantly enhances LLM reasoning\nperformance compared to direct image queries.", "published": "2025-01-08 18:33:17", "link": "http://arxiv.org/abs/2501.04675v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "URSA: Understanding and Verifying Chain-of-thought Reasoning in\n  Multimodal Mathematics", "abstract": "Chain-of-Thought (CoT) reasoning is widely used to enhance the mathematical\nreasoning capabilities of large language models (LLMs). The introduction of\nprocess supervision for CoT trajectories has sparked discussions on improving\ntest-time scaling, thereby unlocking the System 2-style thinking capabilities\nof these models. However, in multimodal mathematical reasoning, the scarcity of\nhigh-quality CoT training data has hindered existing models from achieving both\ndeliberate reasoning and fine-grained verification. In this work, we propose a\nnovel framework that introduces System 2-style thinking to multimodal\nmathematical reasoning. We introduce a three-module CoT data synthesis process\nthat integrates CoT distillation, trajectory-format rewriting, and format\nunification. This process generates MMathCoT-1M, a high-quality CoT reasoning\ninstruction fine-tuning dataset. Furthermore, we implement a dual-view\ntrajectory labeling automation that targets both visual grounding fidelity and\ndeductive chain validity, resulting in the DualMath-1.1M dataset. The URSA-8B\nmodel, trained on MMathCoT-1M, achieves new state-of-the-art (SOTA) performance\namong similarly sized multimodal LLMs on six popular reasoning benchmarks.\nTraining URSA-8B further on the DualMath-1.1M dataset yields URSA-RM-8B, a\nverifier that enhances URSA-8B's test-time performance and surpasses strong\nclosed-source multimodal MLLMs like GPT-4o. The model weights, training data,\nand code have been open-sourced: https://github.com/URSA-MATH/URSA-MATH.", "published": "2025-01-08 18:49:41", "link": "http://arxiv.org/abs/2501.04686v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unifying the Extremes: Developing a Unified Model for Detecting and\n  Predicting Extremist Traits and Radicalization", "abstract": "The proliferation of ideological movements into extremist factions via social\nmedia has become a global concern. While radicalization has been studied\nextensively within the context of specific ideologies, our ability to\naccurately characterize extremism in more generalizable terms remains\nunderdeveloped. In this paper, we propose a novel method for extracting and\nanalyzing extremist discourse across a range of online community forums. By\nfocusing on verbal behavioral signatures of extremist traits, we develop a\nframework for quantifying extremism at both user and community levels. Our\nresearch identifies 11 distinct factors, which we term ``The Extremist\nEleven,'' as a generalized psychosocial model of extremism. Applying our method\nto various online communities, we demonstrate an ability to characterize\nideologically diverse communities across the 11 extremist traits. We\ndemonstrate the power of this method by analyzing user histories from members\nof the incel community. We find that our framework accurately predicts which\nusers join the incel community up to 10 months before their actual entry with\nan AUC of $>0.6$, steadily increasing to AUC ~0.9 three to four months before\nthe event. Further, we find that upon entry into an extremist forum, the users\ntend to maintain their level of extremism within the community, while still\nremaining distinguishable from the general online discourse. Our findings\ncontribute to the study of extremism by introducing a more holistic,\ncross-ideological approach that transcends traditional, trait-specific models.", "published": "2025-01-08 20:17:24", "link": "http://arxiv.org/abs/2501.04820v1", "categories": ["cs.SI", "cs.CL", "cs.CY"], "primary_category": "cs.SI"}
{"title": "Enhancing Listened Speech Decoding from EEG via Parallel Phoneme\n  Sequence Prediction", "abstract": "Brain-computer interfaces (BCI) offer numerous human-centered application\npossibilities, particularly affecting people with neurological disorders. Text\nor speech decoding from brain activities is a relevant domain that could\naugment the quality of life for people with impaired speech perception. We\npropose a novel approach to enhance listened speech decoding from\nelectroencephalography (EEG) signals by utilizing an auxiliary phoneme\npredictor that simultaneously decodes textual phoneme sequences. The proposed\nmodel architecture consists of three main parts: EEG module, speech module, and\nphoneme predictor. The EEG module learns to properly represent EEG signals into\nEEG embeddings. The speech module generates speech waveforms from the EEG\nembeddings. The phoneme predictor outputs the decoded phoneme sequences in text\nmodality. Our proposed approach allows users to obtain decoded listened speech\nfrom EEG signals in both modalities (speech waveforms and textual phoneme\nsequences) simultaneously, eliminating the need for a concatenated sequential\npipeline for each modality. The proposed approach also outperforms previous\nmethods in both modalities. The source code and speech samples are publicly\navailable.", "published": "2025-01-08 21:11:35", "link": "http://arxiv.org/abs/2501.04844v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Real-Time Textless Dialogue Generation", "abstract": "Recent advancements in large language models (LLMs) have led to significant\nprogress in text-based dialogue systems. These systems can now generate\nhigh-quality responses that are accurate and coherent across a wide range of\ntopics and tasks. However, spoken dialogue systems still lag behind in terms of\nnaturalness. They tend to produce robotic interactions, with issues such as\nslow response times, overly generic or cautious replies, and a lack of natural\nrhythm and fluid turn-taking. This shortcoming is largely due to the\nover-reliance on the traditional cascaded design, which involve separate,\nsequential components, as well as the use of text as an intermediate\nrepresentation. This paper propose a real-time, textless spoken dialogue\ngeneration model (RTTL-DG) that aims to overcome these challenges. Our system\nenables fluid turn-taking and generates responses with minimal delay by\nprocessing streaming spoken conversation directly. Additionally, our model\nincorporates backchannels, filters, laughter, and other paralinguistic signals,\nwhich are often absent in cascaded dialogue systems, to create more natural and\nhuman-like interactions. The implementations and generated samples are\navailable in our repository: https://github.com/mailong25/rts2s-dg", "published": "2025-01-08 23:21:43", "link": "http://arxiv.org/abs/2501.04877v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "S2 Chunking: A Hybrid Framework for Document Segmentation Through\n  Integrated Spatial and Semantic Analysis", "abstract": "Document chunking is a critical task in natural language processing (NLP)\nthat involves dividing a document into meaningful segments. Traditional methods\noften rely solely on semantic analysis, ignoring the spatial layout of\nelements, which is crucial for understanding relationships in complex\ndocuments. This paper introduces a novel hybrid approach that combines layout\nstructure, semantic analysis, and spatial relationships to enhance the cohesion\nand accuracy of document chunks. By leveraging bounding box information (bbox)\nand text embeddings, our method constructs a weighted graph representation of\ndocument elements, which is then clustered using spectral clustering.\nExperimental results demonstrate that this approach outperforms traditional\nmethods, particularly in documents with diverse layouts such as reports,\narticles, and multi-column designs. The proposed method also ensures that no\nchunk exceeds a specified token length, making it suitable for use cases where\ntoken limits are critical (e.g., language models with input size limitations)", "published": "2025-01-08 09:06:29", "link": "http://arxiv.org/abs/2501.05485v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards a scalable AI-driven framework for data-independent Cyber Threat\n  Intelligence Information Extraction", "abstract": "Cyber Threat Intelligence (CTI) is critical for mitigating threats to\norganizations, governments, and institutions, yet the necessary data are often\ndispersed across diverse formats. AI-driven solutions for CTI Information\nExtraction (IE) typically depend on high-quality, annotated data, which are not\nalways available. This paper introduces 0-CTI, a scalable AI-based framework\ndesigned for efficient CTI Information Extraction. Leveraging advanced Natural\nLanguage Processing (NLP) techniques, particularly Transformer-based\narchitectures, the proposed system processes complete text sequences of CTI\nreports to extract a cyber ontology of named entities and their relationships.\n  Our contribution is the development of 0-CTI, the first modular framework for\nCTI Information Extraction that supports both supervised and zero-shot\nlearning. Unlike existing state-of-the-art models that rely heavily on\nannotated datasets, our system enables fully dataless operation through\nzero-shot methods for both Entity and Relation Extraction, making it adaptable\nto various data availability scenarios. Additionally, our supervised Entity\nExtractor surpasses current state-of-the-art performance in cyber Entity\nExtraction, highlighting the dual strength of the framework in both\nlow-resource and data-rich environments.\n  By aligning the system's outputs with the Structured Threat Information\nExpression (STIX) format, a standard for information exchange in the\ncybersecurity domain, 0-CTI standardizes extracted knowledge, enhancing\ncommunication and collaboration in cybersecurity operations.", "published": "2025-01-08 12:35:17", "link": "http://arxiv.org/abs/2501.06239v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "A partition cover approach to tokenization", "abstract": "Tokenization is the process of encoding strings into tokens from a fixed\nvocabulary of size $k$ and is widely utilized in Natural Language Processing\napplications. The leading tokenization algorithm today is Byte Pair Encoding\n(BPE), which formulates the tokenization problem as a compression problem and\ntackles it by performing sequences of merges. In this work, we formulate\ntokenization as an optimization objective, show that it is NP-hard via a simple\nreduction from vertex cover, and propose a polynomial-time greedy algorithm\nGreedTok. Our formulation naturally relaxes to the well-studied weighted\nmaximum coverage problem which has a simple $(1 - 1/e)$-approximation algorithm\nGreedWMC. Through empirical evaluations on real-world corpora, we show that\nGreedTok outperforms BPE, while achieving a comparable objective score as\nGreedWMC (which could have achieved a higher score due to relaxation).", "published": "2025-01-08 17:07:07", "link": "http://arxiv.org/abs/2501.06246v1", "categories": ["cs.CL", "cs.AI", "cs.DS"], "primary_category": "cs.CL"}
{"title": "Methods to Increase the Amount of Data for Speech Recognition for Low\n  Resource Languages", "abstract": "This study explores methods to increase data volume for low-resource\nlanguages using techniques such as crowdsourcing, pseudo-labeling, advanced\ndata preprocessing and various permissive data sources such as audiobooks,\nCommon Voice, YouTube. While these methods are well-explored for highresource\nlanguages, their application for low-resource languages remains underexplored.\nUsing Armenian and Georgian as case studies, we demonstrate how linguistic and\nresource-specific characteristics influence the success of these methods. This\nwork provides practical guidance for researchers to choose cost-effective and\nquality-driven dataset extension strategies for low-resource languages. The key\ntakeaway from various data extension approaches is that paid crowd-sourcing\noffers the best balance between cost and quality, outperforming volunteer\ncrowd-sourcing, open-source audiobooks, and unlabeled data usage. Ablation\nstudy shows that models trained on the expanded datasets outperform existing\nbaselines and achieve 5.73% for Gergian and 9.9% for Armenian ASR word error\nrate using a relatively small FastConformer architecture. We open-sourced both\nthe Armenian and Georgian models to allow further research and practical\napplications.", "published": "2025-01-08 15:18:42", "link": "http://arxiv.org/abs/2501.14788v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Circuit Complexity Bounds for Visual Autoregressive Model", "abstract": "Understanding the expressive ability of a specific model is essential for\ngrasping its capacity limitations. Recently, several studies have established\ncircuit complexity bounds for Transformer architecture. Besides, the Visual\nAutoRegressive (VAR) model has risen to be a prominent method in the field of\nimage generation, outperforming previous techniques, such as Diffusion\nTransformers, in generating high-quality images. We investigate the circuit\ncomplexity of the VAR model and establish a bound in this study. Our primary\nresult demonstrates that the VAR model is equivalent to a simulation by a\nuniform $\\mathsf{TC}^0$ threshold circuit with hidden dimension $d \\leq O(n)$\nand $\\mathrm{poly}(n)$ precision. This is the first study to rigorously\nhighlight the limitations in the expressive power of VAR models despite their\nimpressive performance. We believe our findings will offer valuable insights\ninto the inherent constraints of these models and guide the development of more\nefficient and expressive architectures in the future.", "published": "2025-01-08 06:07:33", "link": "http://arxiv.org/abs/2501.04299v1", "categories": ["stat.ML", "cs.AI", "cs.CC", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Decoding EEG Speech Perception with Transformers and VAE-based Data\n  Augmentation", "abstract": "Decoding speech from non-invasive brain signals, such as\nelectroencephalography (EEG), has the potential to advance brain-computer\ninterfaces (BCIs), with applications in silent communication and assistive\ntechnologies for individuals with speech impairments. However, EEG-based speech\ndecoding faces major challenges, such as noisy data, limited datasets, and poor\nperformance on complex tasks like speech perception. This study attempts to\naddress these challenges by employing variational autoencoders (VAEs) for EEG\ndata augmentation to improve data quality and applying a state-of-the-art\n(SOTA) sequence-to-sequence deep learning architecture, originally successful\nin electromyography (EMG) tasks, to EEG-based speech decoding. Additionally, we\nadapt this architecture for word classification tasks. Using the Brennan\ndataset, which contains EEG recordings of subjects listening to narrated\nspeech, we preprocess the data and evaluate both classification and\nsequence-to-sequence models for EEG-to-words/sentences tasks. Our experiments\nshow that VAEs have the potential to reconstruct artificial EEG data for\naugmentation. Meanwhile, our sequence-to-sequence model achieves more promising\nperformance in generating sentences compared to our classification model,\nthough both remain challenging tasks. These findings lay the groundwork for\nfuture research on EEG speech perception decoding, with possible extensions to\nspeech production tasks such as silent or imagined speech.", "published": "2025-01-08 08:55:10", "link": "http://arxiv.org/abs/2501.04359v1", "categories": ["eess.AS", "cs.CL", "cs.HC", "cs.LG", "cs.SD", "68T07, 92C55", "H.5.2; I.2.6; J.3"], "primary_category": "eess.AS"}
{"title": "Utility-inspired Reward Transformations Improve Reinforcement Learning\n  Training of Language Models", "abstract": "Current methods that train large language models (LLMs) with reinforcement\nlearning feedback, often resort to averaging outputs of multiple rewards\nfunctions during training. This overlooks crucial aspects of individual reward\ndimensions and inter-reward dependencies that can lead to sub-optimal outcomes\nin generations. In this work, we show how linear aggregation of rewards\nexhibits some vulnerabilities that can lead to undesired properties of\ngenerated text. We then propose a transformation of reward functions inspired\nby economic theory of utility functions (specifically Inada conditions), that\nenhances sensitivity to low reward values while diminishing sensitivity to\nalready high values. We compare our approach to the existing baseline methods\nthat linearly aggregate rewards and show how the Inada-inspired reward feedback\nis superior to traditional weighted averaging. We quantitatively and\nqualitatively analyse the difference in the methods, and see that models\ntrained with Inada-transformations score as more helpful while being less\nharmful.", "published": "2025-01-08 19:03:17", "link": "http://arxiv.org/abs/2501.06248v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "econ.GN", "q-fin.EC"], "primary_category": "cs.LG"}
{"title": "Meta-learning-based percussion transcription and $t\\bar{a}la$\n  identification from low-resource audio", "abstract": "This study introduces a meta-learning-based approach for low-resource Tabla\nStroke Transcription (TST) and $t\\bar{a}la$ identification in Hindustani\nclassical music. Using Model-Agnostic Meta-Learning (MAML), we address the\nchallenge of limited annotated datasets, enabling rapid adaptation to new tasks\nwith minimal data. The method is validated across various datasets, including\ntabla solo and concert recordings, demonstrating robustness in polyphonic audio\nscenarios. We propose two novel $t\\bar{a}la$ identification techniques based on\nstroke sequences and rhythmic patterns. Additionally, the approach proves\neffective for Automatic Drum Transcription (ADT), showcasing its flexibility\nfor Indian and Western percussion music. Experimental results show that the\nproposed method outperforms existing techniques in low-resource settings,\nsignificantly contributing to music transcription and studying musical\ntraditions through computational tools.", "published": "2025-01-08 05:59:36", "link": "http://arxiv.org/abs/2501.04742v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "DrawSpeech: Expressive Speech Synthesis Using Prosodic Sketches as\n  Control Conditions", "abstract": "Controlling text-to-speech (TTS) systems to synthesize speech with the\nprosodic characteristics expected by users has attracted much attention. To\nachieve controllability, current studies focus on two main directions: (1)\nusing reference speech as prosody prompt to guide speech synthesis, and (2)\nusing natural language descriptions to control the generation process. However,\nfinding reference speech that exactly contains the prosody that users want to\nsynthesize takes a lot of effort. Description-based guidance in TTS systems can\nonly determine the overall prosody, which has difficulty in achieving\nfine-grained prosody control over the synthesized speech. In this paper, we\npropose DrawSpeech, a sketch-conditioned diffusion model capable of generating\nspeech based on any prosody sketches drawn by users. Specifically, the prosody\nsketches are fed to DrawSpeech to provide a rough indication of the expected\nprosody trends. DrawSpeech then recovers the detailed pitch and energy contours\nbased on the coarse sketches and synthesizes the desired speech. Experimental\nresults show that DrawSpeech can generate speech with a wide variety of prosody\nand can precisely control the fine-grained prosody in a user-friendly manner.\nOur implementation and audio samples are publicly available.", "published": "2025-01-08 03:47:54", "link": "http://arxiv.org/abs/2501.04256v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Phone-purity Guided Discrete Tokens for Dysarthric Speech Recognition", "abstract": "Discrete tokens extracted provide efficient and domain adaptable speech\nfeatures. Their application to disordered speech that exhibits articulation\nimprecision and large mismatch against normal voice remains unexplored. To\nimprove their phonetic discrimination that is weakened during unsupervised\nK-means or vector quantization of continuous features, this paper proposes\nnovel phone-purity guided (PPG) discrete tokens for dysarthric speech\nrecognition. Phonetic label supervision is used to regularize maximum\nlikelihood and reconstruction error costs used in standard K-means and VAE-VQ\nbased discrete token extraction. Experiments conducted on the UASpeech corpus\nsuggest that the proposed PPG discrete token features extracted from HuBERT\nconsistently outperform hybrid TDNN and End-to-End (E2E) Conformer systems\nusing non-PPG based K-means or VAE-VQ tokens across varying codebook sizes by\nstatistically significant word error rate (WER) reductions up to 0.99\\% and\n1.77\\% absolute (3.21\\% and 4.82\\% relative) respectively on the UASpeech test\nset of 16 dysarthric speakers. The lowest WER of 23.25\\% was obtained by\ncombining systems using different token features. Consistent improvements on\nthe phone purity metric were also achieved. T-SNE visualization further\ndemonstrates sharper decision boundaries were produced between K-means/VAE-VQ\nclusters after introducing phone-purity guidance.", "published": "2025-01-08 09:45:14", "link": "http://arxiv.org/abs/2501.04379v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ZSVC: Zero-shot Style Voice Conversion with Disentangled Latent\n  Diffusion Models and Adversarial Training", "abstract": "Style voice conversion aims to transform the speaking style of source speech\ninto a desired style while keeping the original speaker's identity. However,\nprevious style voice conversion approaches primarily focus on well-defined\ndomains such as emotional aspects, limiting their practical applications. In\nthis study, we present ZSVC, a novel Zero-shot Style Voice Conversion approach\nthat utilizes a speech codec and a latent diffusion model with speech prompting\nmechanism to facilitate in-context learning for speaking style conversion. To\ndisentangle speaking style and speaker timbre, we introduce information\nbottleneck to filter speaking style in the source speech and employ Uncertainty\nModeling Adaptive Instance Normalization (UMAdaIN) to perturb the speaker\ntimbre in the style prompt. Moreover, we propose a novel adversarial training\nstrategy to enhance in-context learning and improve style similarity.\nExperiments conducted on 44,000 hours of speech data demonstrate the superior\nperformance of ZSVC in generating speech with diverse speaking styles in\nzero-shot scenarios.", "published": "2025-01-08 11:04:30", "link": "http://arxiv.org/abs/2501.04416v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Right Label Context in End-to-End Training of Time-Synchronous ASR\n  Models", "abstract": "Current time-synchronous sequence-to-sequence automatic speech recognition\n(ASR) models are trained by using sequence level cross-entropy that sums over\nall alignments. Due to the discriminative formulation, incorporating the right\nlabel context into the training criterion's gradient causes normalization\nproblems and is not mathematically well-defined. The classic hybrid neural\nnetwork hidden Markov model (NN-HMM) with its inherent generative formulation\nenables conditioning on the right label context. However, due to the HMM\nstate-tying the identity of the right label context is never modeled\nexplicitly. In this work, we propose a factored loss with auxiliary left and\nright label contexts that sums over all alignments. We show that the inclusion\nof the right label context is particularly beneficial when training data\nresources are limited. Moreover, we also show that it is possible to build a\nfactored hybrid HMM system by relying exclusively on the full-sum criterion.\nExperiments were conducted on Switchboard 300h and LibriSpeech 960h.", "published": "2025-01-08 14:14:19", "link": "http://arxiv.org/abs/2501.04521v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "FleSpeech: Flexibly Controllable Speech Generation with Various Prompts", "abstract": "Controllable speech generation methods typically rely on single or fixed\nprompts, hindering creativity and flexibility. These limitations make it\ndifficult to meet specific user needs in certain scenarios, such as adjusting\nthe style while preserving a selected speaker's timbre, or choosing a style and\ngenerating a voice that matches a character's visual appearance. To overcome\nthese challenges, we propose \\textit{FleSpeech}, a novel multi-stage speech\ngeneration framework that allows for more flexible manipulation of speech\nattributes by integrating various forms of control. FleSpeech employs a\nmultimodal prompt encoder that processes and unifies different text, audio, and\nvisual prompts into a cohesive representation. This approach enhances the\nadaptability of speech synthesis and supports creative and precise control over\nthe generated speech. Additionally, we develop a data collection pipeline for\nmultimodal datasets to facilitate further research and applications in this\nfield. Comprehensive subjective and objective experiments demonstrate the\neffectiveness of FleSpeech. Audio samples are available at\nhttps://kkksuper.github.io/FleSpeech/", "published": "2025-01-08 17:52:35", "link": "http://arxiv.org/abs/2501.04644v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Comparison of fundamental frequency estimators with subharmonic voice\n  signals", "abstract": "In clinical voice signal analysis, mishandling of subharmonic voicing may\ncause an acoustic parameter to signal false negatives. As such, the ability of\na fundamental frequency estimator to identify speaking fundamental frequency is\ncritical. This paper presents a sustained-vowel study, which used a\nquality-of-estimate classification to identify subharmonic errors and\nsubharmonics-to-harmonics ratio (SHR) to measure the strength of subharmonic\nvoicing. Five estimators were studied with a sustained vowel dataset: Praat,\nYAAPT, Harvest, CREPE, and FCN-F0. FCN-F0, a deep-learning model, performed the\nbest both in overall accuracy and in correctly resolving subharmonic signals.\nCREPE and Harvest are also highly capable estimators for sustained vowel\nanalysis.", "published": "2025-01-08 19:11:51", "link": "http://arxiv.org/abs/2501.04789v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MADUV: The 1st INTERSPEECH Mice Autism Detection via Ultrasound\n  Vocalization Challenge", "abstract": "The Mice Autism Detection via Ultrasound Vocalization (MADUV) Challenge\nintroduces the first INTERSPEECH challenge focused on detecting autism spectrum\ndisorder (ASD) in mice through their vocalizations. Participants are tasked\nwith developing models to automatically classify mice as either wild-type or\nASD models based on recordings with a high sampling rate. Our baseline system\nemploys a simple CNN-based classification using three different spectrogram\nfeatures. Results demonstrate the feasibility of automated ASD detection, with\nthe considered audible-range features achieving the best performance (UAR of\n0.600 for segment-level and 0.625 for subject-level classification). This\nchallenge bridges speech technology and biomedical research, offering\nopportunities to advance our understanding of ASD models through machine\nlearning approaches. The findings suggest promising directions for vocalization\nanalysis and highlight the potential value of audible and ultrasound\nvocalizations in ASD detection.", "published": "2025-01-08 05:32:55", "link": "http://arxiv.org/abs/2501.04292v2", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Evaluating Interval-based Tokenization for Pitch Representation in\n  Symbolic Music Analysis", "abstract": "Symbolic music analysis tasks are often performed by models originally\ndeveloped for Natural Language Processing, such as Transformers. Such models\nrequire the input data to be represented as sequences, which is achieved\nthrough a process of tokenization. Tokenization strategies for symbolic music\noften rely on absolute MIDI values to represent pitch information. However,\nmusic research largely promotes the benefit of higher-level representations\nsuch as melodic contour and harmonic relations for which pitch intervals turn\nout to be more expressive than absolute pitches. In this work, we introduce a\ngeneral framework for building interval-based tokenizations. By evaluating\nthese tokenizations on three music analysis tasks, we show that such\ninterval-based tokenizations improve model performances and facilitate their\nexplainability.", "published": "2025-01-08 17:22:03", "link": "http://arxiv.org/abs/2501.04630v1", "categories": ["cs.IR", "cs.SD", "eess.AS"], "primary_category": "cs.IR"}
{"title": "Planing It by Ear: Convolutional Neural Networks for Acoustic Anomaly\n  Detection in Industrial Wood Planers", "abstract": "In recent years, the wood product industry has been facing a skilled labor\nshortage. The result is more frequent sudden failures, resulting in additional\ncosts for these companies already operating in a very competitive market.\nMoreover, sawmills are challenging environments for machinery and sensors.\nGiven that experienced machine operators may be able to diagnose defects or\nmalfunctions, one possible way of assisting novice operators is through\nacoustic monitoring. As a step towards the automation of wood-processing\nequipment and decision support systems for machine operators, in this paper, we\nexplore using a deep convolutional autoencoder for acoustic anomaly detection\nof wood planers on a new real-life dataset. Specifically, our convolutional\nautoencoder with skip connections (Skip-CAE) and our Skip-CAE transformer\noutperform the DCASE autoencoder baseline, one-class SVM, isolation forest and\na published convolutional autoencoder architecture, respectively obtaining an\narea under the ROC curve of 0.846 and 0.875 on a dataset of real-factory planer\nsounds. Moreover, we show that adding skip connections and attention mechanism\nunder the form of a transformer encoder-decoder helps to further improve the\nanomaly detection capabilities.", "published": "2025-01-08 20:17:18", "link": "http://arxiv.org/abs/2501.04819v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Open-Source Manually Annotated Vocal Tract Database for Automatic\n  Segmentation from 3D MRI Using Deep Learning: Benchmarking 2D and 3D\n  Convolutional and Transformer Networks", "abstract": "Accurate segmentation of the vocal tract from magnetic resonance imaging\n(MRI) data is essential for various voice and speech applications. Manual\nsegmentation is time intensive and susceptible to errors. This study aimed to\nevaluate the efficacy of deep learning algorithms for automatic vocal tract\nsegmentation from 3D MRI.", "published": "2025-01-08 00:19:52", "link": "http://arxiv.org/abs/2501.06229v2", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
