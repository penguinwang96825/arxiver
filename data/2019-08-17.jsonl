{"title": "Build it Break it Fix it for Dialogue Safety: Robustness from\n  Adversarial Human Attack", "abstract": "The detection of offensive language in the context of a dialogue has become\nan increasingly important application of natural language processing. The\ndetection of trolls in public forums (Gal\\'an-Garc\\'ia et al., 2016), and the\ndeployment of chatbots in the public domain (Wolf et al., 2017) are two\nexamples that show the necessity of guarding against adversarially offensive\nbehavior on the part of humans. In this work, we develop a training scheme for\na model to become robust to such human attacks by an iterative build it, break\nit, fix it strategy with humans and models in the loop. In detailed experiments\nwe show this approach is considerably more robust than previous systems.\nFurther, we show that offensive language used within a conversation critically\ndepends on the dialogue context, and cannot be viewed as a single sentence\noffensive detection task as in most previous work. Our newly collected tasks\nand methods will be made open source and publicly available.", "published": "2019-08-17 18:34:11", "link": "http://arxiv.org/abs/1908.06083v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating an Overview Report over Many Documents", "abstract": "How to efficiently generate an accurate, well-structured overview report\n(ORPT) over thousands of related documents is challenging. A well-structured\nORPT consists of sections of multiple levels (e.g., sections and subsections).\nNone of the existing multi-document summarization (MDS) algorithms is directed\ntoward this task. To overcome this obstacle, we present NDORGS (Numerous\nDocuments' Overview Report Generation Scheme) that integrates text filtering,\nkeyword scoring, single-document summarization (SDS), topic modeling, MDS, and\ntitle generation to generate a coherent, well-structured ORPT. We then devise a\nmulti-criteria evaluation method using techniques of text mining and\nmulti-attribute decision making on a combination of human judgments, running\ntime, information coverage, and topic diversity. We evaluate ORPTs generated by\nNDORGS on two large corpora of documents, where one is classified and the other\nunclassified. We show that, using Saaty's pairwise comparison 9-point scale and\nunder TOPSIS, the ORPTs generated on SDS's with the length of 20% of the\noriginal documents are the best overall on both datasets.", "published": "2019-08-17 01:11:04", "link": "http://arxiv.org/abs/1908.06216v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Graph Distillation for Low-Resource Machine Translation", "abstract": "Neural machine translation on low-resource language is challenging due to the\nlack of bilingual sentence pairs. Previous works usually solve the low-resource\ntranslation problem with knowledge transfer in a multilingual setting. In this\npaper, we propose the concept of Language Graph and further design a novel\ngraph distillation algorithm that boosts the accuracy of low-resource\ntranslations in the graph with forward and backward knowledge distillation.\nPreliminary experiments on the TED talks multilingual dataset demonstrate the\neffectiveness of our proposed method. Specifically, we improve the low-resource\ntranslation pair by more than 3.13 points in terms of BLEU score.", "published": "2019-08-17 08:01:05", "link": "http://arxiv.org/abs/1908.06258v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hard but Robust, Easy but Sensitive: How Encoder and Decoder Perform in\n  Neural Machine Translation", "abstract": "Neural machine translation (NMT) typically adopts the encoder-decoder\nframework. A good understanding of the characteristics and functionalities of\nthe encoder and decoder can help to explain the pros and cons of the framework,\nand design better models for NMT. In this work, we conduct an empirical study\non the encoder and the decoder in NMT, taking Transformer as an example. We\nfind that 1) the decoder handles an easier task than the encoder in NMT, 2) the\ndecoder is more sensitive to the input noise than the encoder, and 3) the\npreceding words/tokens in the decoder provide strong conditional information,\nwhich accounts for the two observations above. We hope those observations can\nshed light on the characteristics of the encoder and decoder and inspire future\nresearch on NMT.", "published": "2019-08-17 08:09:33", "link": "http://arxiv.org/abs/1908.06259v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EmotionX-IDEA: Emotion BERT -- an Affectional Model for Conversation", "abstract": "In this paper, we investigate the emotion recognition ability of the\npre-training language model, namely BERT. By the nature of the framework of\nBERT, a two-sentence structure, we adapt BERT to continues dialogue emotion\nprediction tasks, which rely heavily on the sentence-level context-aware\nunderstanding. The experiments show that by mapping the continues dialogue into\na causal utterance pair, which is constructed by the utterance and the reply\nutterance, models can better capture the emotions of the reply utterance. The\npresent method has achieved 0.815 and 0.885 micro F1 score in the testing\ndataset of Friends and EmotionPush, respectively.", "published": "2019-08-17 08:59:51", "link": "http://arxiv.org/abs/1908.06264v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Message Passing Attention Networks for Document Understanding", "abstract": "Graph neural networks have recently emerged as a very effective framework for\nprocessing graph-structured data. These models have achieved state-of-the-art\nperformance in many tasks. Most graph neural networks can be described in terms\nof message passing, vertex update, and readout functions. In this paper, we\nrepresent documents as word co-occurrence networks and propose an application\nof the message passing framework to NLP, the Message Passing Attention network\nfor Document understanding (MPAD). We also propose several hierarchical\nvariants of MPAD. Experiments conducted on 10 standard text classification\ndatasets show that our architectures are competitive with the state-of-the-art.\nAblation studies reveal further insights about the impact of the different\ncomponents on performance. Code is publicly available at:\nhttps://github.com/giannisnik/mpad .", "published": "2019-08-17 09:18:47", "link": "http://arxiv.org/abs/1908.06267v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Sentence Similarity in Natural Language Generation: Improving\n  Beam Search using Range Voting", "abstract": "We propose a method for natural language generation, choosing the most\nrepresentative output rather than the most likely output. By viewing the\nlanguage generation process from the voting theory perspective, we define\nrepresentativeness using range voting and a similarity measure. The proposed\nmethod can be applied when generating from any probabilistic language model,\nincluding n-gram models and neural network models. We evaluate different\nsimilarity measures on an image captioning task and a machine translation task,\nand show that our method generates longer and more diverse sentences, providing\na solution to the common problem of short outputs being preferred over longer\nand more informative ones. The generated sentences obtain higher BLEU scores,\nparticularly when the beam size is large. We also perform a human evaluation on\nboth tasks and find that the outputs generated using our method are rated\nhigher.", "published": "2019-08-17 10:36:43", "link": "http://arxiv.org/abs/1908.06288v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Sensitivity Analysis of Attention-Gated Convolutional Neural Networks\n  for Sentence Classification", "abstract": "In this paper, we investigate the effect of different hyperparameters as well\nas different combinations of hyperparameters settings on the performance of the\nAttention-Gated Convolutional Neural Networks (AGCNNs), e.g., the kernel window\nsize, the number of feature maps, the keep rate of the dropout layer, and the\nactivation function. We draw practical advice from a wide range of empirical\nresults. Through the sensitivity analysis, we further improve the\nhyperparameters settings of AGCNNs. Experiments show that our proposals could\nachieve an average of 0.81% and 0.67% improvements on AGCNN-NLReLU-rand and\nAGCNN-SELU-rand, respectively; and an average of 0.47% and 0.45% improvements\non AGCNN-NLReLU-static and AGCNN-SELU-static, respectively.", "published": "2019-08-17 08:40:18", "link": "http://arxiv.org/abs/1908.06263v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Language Features Matter: Effective Language Representations for\n  Vision-Language Tasks", "abstract": "Shouldn't language and vision features be treated equally in vision-language\n(VL) tasks? Many VL approaches treat the language component as an afterthought,\nusing simple language models that are either built upon fixed word embeddings\ntrained on text-only data or are learned from scratch. We believe that language\nfeatures deserve more attention, and conduct experiments which compare\ndifferent word embeddings, language models, and embedding augmentation steps on\nfive common VL tasks: image-sentence retrieval, image captioning, visual\nquestion answering, phrase grounding, and text-to-clip retrieval. Our\nexperiments provide some striking results; an average embedding language model\noutperforms an LSTM on retrieval-style tasks; state-of-the-art representations\nsuch as BERT perform relatively poorly on vision-language tasks. From this\ncomprehensive set of experiments we propose a set of best practices for\nincorporating the language component of VL tasks. To further elevate language\nfeatures, we also show that knowledge in vision-language problems can be\ntransferred across tasks to gain performance with multi-task training. This\nmulti-task training is applied to a new Graph Oriented Vision-Language\nEmbedding (GrOVLE), which we adapt from Word2Vec using WordNet and an original\nvisual-language graph built from Visual Genome, providing a ready-to-use\nvision-language embedding: http://ai.bu.edu/grovle.", "published": "2019-08-17 18:01:27", "link": "http://arxiv.org/abs/1908.06327v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "U-CAM: Visual Explanation using Uncertainty based Class Activation Maps", "abstract": "Understanding and explaining deep learning models is an imperative task.\nTowards this, we propose a method that obtains gradient-based certainty\nestimates that also provide visual attention maps. Particularly, we solve for\nvisual question answering task. We incorporate modern probabilistic deep\nlearning methods that we further improve by using the gradients for these\nestimates. These have two-fold benefits: a) improvement in obtaining the\ncertainty estimates that correlate better with misclassified samples and b)\nimproved attention maps that provide state-of-the-art results in terms of\ncorrelation with human attention regions. The improved attention maps result in\nconsistent improvement for various methods for visual question answering.\nTherefore, the proposed technique can be thought of as a recipe for obtaining\nimproved certainty estimates and explanation for deep learning models. We\nprovide detailed empirical analysis for the visual question answering task on\nall standard benchmarks and comparison with state of the art methods.", "published": "2019-08-17 14:39:36", "link": "http://arxiv.org/abs/1908.06306v4", "categories": ["cs.CV", "cs.CL", "cs.LG", "eess.IV"], "primary_category": "cs.CV"}
{"title": "What is needed for simple spatial language capabilities in VQA?", "abstract": "Visual question answering (VQA) comprises a variety of language capabilities.\nThe diagnostic benchmark dataset CLEVR has fueled progress by helping to better\nassess and distinguish models in basic abilities like counting, comparing and\nspatial reasoning in vitro. Following this approach, we focus on spatial\nlanguage capabilities and investigate the question: what are the key\ningredients to handle simple visual-spatial relations? We look at the SAN,\nRelNet, FiLM and MC models and evaluate their learning behavior on diagnostic\ndata which is solely focused on spatial relations. Via comparative analysis and\ntargeted model modification we identify what really is required to\nsubstantially improve upon the CNN-LSTM baseline.", "published": "2019-08-17 20:12:39", "link": "http://arxiv.org/abs/1908.06336v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "JVS corpus: free Japanese multi-speaker voice corpus", "abstract": "Thanks to improvements in machine learning techniques, including deep\nlearning, speech synthesis is becoming a machine learning task. To accelerate\nspeech synthesis research, we are developing Japanese voice corpora reasonably\naccessible from not only academic institutions but also commercial companies.\nIn 2017, we released the JSUT corpus, which contains 10 hours of reading-style\nspeech uttered by a single speaker, for end-to-end text-to-speech synthesis.\nFor more general use in speech synthesis research, e.g., voice conversion and\nmulti-speaker modeling, in this paper, we construct the JVS corpus, which\ncontains voice data of 100 speakers in three styles (normal, whisper, and\nfalsetto). The corpus contains 30 hours of voice data including 22 hours of\nparallel normal voices. This paper describes how we designed the corpus and\nsummarizes the specifications. The corpus is available at our project page.", "published": "2019-08-17 06:04:46", "link": "http://arxiv.org/abs/1908.06248v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Onset detection: A new approach to QBH system", "abstract": "Query by Humming (QBH) is a system to provide a user with the song(s) which\nthe user hums to the system. Current QBH method requires the extraction of\nonset and pitch information in order to track similarity with various versions\nof different songs. However, we here focus on detecting precise onsets only and\nuse them to build a QBH system which is better than existing methods in terms\nof speed and memory and empirically in terms of accuracy. We also provide\nstatistical analogy for onset detection functions and provide a measure of\nerror in our algorithm.", "published": "2019-08-17 14:44:32", "link": "http://arxiv.org/abs/1908.07409v2", "categories": ["stat.AP", "cs.IR", "cs.SD", "eess.AS"], "primary_category": "stat.AP"}
