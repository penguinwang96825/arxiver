{"title": "A Hybrid Model for Enhancing Lexical Statistical Machine Translation\n  (SMT)", "abstract": "The interest in statistical machine translation systems increases currently\ndue to political and social events in the world. A proposed Statistical Machine\nTranslation (SMT) based model that can be used to translate a sentence from the\nsource Language (English) to the target language (Arabic) automatically through\nefficiently incorporating different statistical and Natural Language Processing\n(NLP) models such as language model, alignment model, phrase based model,\nreordering model, and translation model. These models are combined to enhance\nthe performance of statistical machine translation (SMT). Many implementation\ntools have been used in this work such as Moses, Gizaa++, IRSTLM, KenLM, and\nBLEU. Based on the implementation, evaluation of this model, and comparing the\ngenerated translation with other implemented machine translation systems like\nGoogle Translate, it was proved that this proposed model has enhanced the\nresults of the statistical machine translation, and forms a reliable and\nefficient model in this field of research.", "published": "2015-06-03 09:18:05", "link": "http://arxiv.org/abs/1506.01171v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Personalizing Universal Recurrent Neural Network Language Model with\n  User Characteristic Features by Social Network Crowdsouring", "abstract": "With the popularity of mobile devices, personalized speech recognizer becomes\nmore realizable today and highly attractive. Each mobile device is primarily\nused by a single user, so it's possible to have a personalized recognizer well\nmatching to the characteristics of individual user. Although acoustic model\npersonalization has been investigated for decades, much less work have been\nreported on personalizing language model, probably because of the difficulties\nin collecting enough personalized corpora. Previous work used the corpora\ncollected from social networks to solve the problem, but constructing a\npersonalized model for each user is troublesome. In this paper, we propose a\nuniversal recurrent neural network language model with user characteristic\nfeatures, so all users share the same model, except each with different user\ncharacteristic features. These user characteristic features can be obtained by\ncrowdsouring over social networks, which include huge quantity of texts posted\nby users with known friend relationships, who may share some subject topics and\nwording patterns. The preliminary experiments on Facebook corpus showed that\nthis proposed approach not only drastically reduced the model perplexity, but\noffered very good improvement in recognition accuracy in n-best rescoring\ntests. This approach also mitigated the data sparseness problem for\npersonalized language models.", "published": "2015-06-03 10:14:21", "link": "http://arxiv.org/abs/1506.01192v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Traversing Knowledge Graphs in Vector Space", "abstract": "Path queries on a knowledge graph can be used to answer compositional\nquestions such as \"What languages are spoken by people living in Lisbon?\".\nHowever, knowledge graphs often have missing facts (edges) which disrupts path\nqueries. Recent models for knowledge base completion impute missing facts by\nembedding knowledge graphs in vector spaces. We show that these models can be\nrecursively applied to answer path queries, but that they suffer from cascading\nerrors. This motivates a new \"compositional\" training objective, which\ndramatically improves all models' ability to answer path queries, in some cases\nmore than doubling accuracy. On a standard knowledge base completion task, we\nalso demonstrate that compositional training acts as a novel form of structural\nregularization, reliably improving performance across all base models (reducing\nerrors by up to 43%) and achieving new state-of-the-art results.", "published": "2015-06-03 00:38:25", "link": "http://arxiv.org/abs/1506.01094v2", "categories": ["cs.CL", "cs.AI", "cs.DB", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Summarization of Films and Documentaries Based on Subtitles and Scripts", "abstract": "We assess the performance of generic text summarization algorithms applied to\nfilms and documentaries, using the well-known behavior of summarization of news\narticles as reference. We use three datasets: (i) news articles, (ii) film\nscripts and subtitles, and (iii) documentary subtitles. Standard ROUGE metrics\nare used for comparing generated summaries against news abstracts, plot\nsummaries, and synopses. We show that the best performing algorithms are LSA,\nfor news articles and documentaries, and LexRank and Support Sets, for films.\nDespite the different nature of films and documentaries, their relative\nbehavior is in accordance with that obtained for news articles.", "published": "2015-06-03 15:07:14", "link": "http://arxiv.org/abs/1506.01273v3", "categories": ["cs.CL", "cs.AI", "cs.IR", "I.2.7"], "primary_category": "cs.CL"}
