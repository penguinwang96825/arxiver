{"title": "Targeted Sentiment Analysis: A Data-Driven Categorization", "abstract": "Targeted sentiment analysis (TSA), also known as aspect based sentiment\nanalysis (ABSA), aims at detecting fine-grained sentiment polarity towards\ntargets in a given opinion document. Due to the lack of labeled datasets and\neffective technology, TSA had been intractable for many years. The newly\nreleased datasets and the rapid development of deep learning technologies are\nkey enablers for the recent significant progress made in this area. However,\nthe TSA tasks have been defined in various ways with different understandings\ntowards basic concepts like `target' and `aspect'. In this paper, we categorize\nthe different tasks and highlight the differences in the available datasets and\ntheir specific tasks. We then further discuss the challenges related to data\ncollection and data annotation which are overlooked in many previous studies.", "published": "2019-05-09 03:01:54", "link": "http://arxiv.org/abs/1905.03423v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Transparency in Maintenance of Recruitment Chatbots", "abstract": "We report on experiences with implementing conversational agents in the\nrecruitment domain based on a machine learning (ML) system. Recruitment\nchatbots mediate communication between job-seekers and recruiters by exposing\nML data to recruiter teams. Errors are difficult to understand, communicate,\nand resolve because they may span and combine UX, ML, and software issues. In\nan effort to improve organizational and technical transparency, we came to rely\non a key contact role. Though effective for design and development, the\ncentralization of this role poses challenges for transparency in sustained\nmaintenance of this kind of ML-based mediating system.", "published": "2019-05-09 14:04:44", "link": "http://arxiv.org/abs/1905.03640v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "A joint text mining-rank size investigation of the rhetoric structures\n  of the US Presidents' speeches", "abstract": "This work presents a text mining context and its use for a deep analysis of\nthe messages delivered by the politicians. Specifically, we deal with an expert\nsystems-based exploration of the rhetoric dynamics of a large collection of US\nPresidents' speeches, ranging from Washington to Trump. In particular, speeches\nare viewed as complex expert systems whose structures can be effectively\nanalyzed through rank-size laws. The methodological contribution of the paper\nis twofold. First, we develop a text mining-based procedure for the\nconstruction of the dataset by using a web scraping routine on the Miller\nCenter website -- the repository collecting the speeches. Second, we explore\nthe implicit structure of the discourse data by implementing a rank-size\nprocedure over the individual speeches, being the words of each speech ranked\nin terms of their frequencies. The scientific significance of the proposed\ncombination of text-mining and rank-size approaches can be found in its\nflexibility and generality, which let it be reproducible to a wide set of\nexpert systems and text mining contexts. The usefulness of the proposed method\nand the speech subsequent analysis is demonstrated by the findings themselves.\nIndeed, in terms of impact, it is worth noting that interesting conclusions of\nsocial, political and linguistic nature on how 45 United States Presidents,\nfrom April 30, 1789 till February 28, 2017 delivered political messages can be\ncarried out. Indeed, the proposed analysis shows some remarkable regularities,\nnot only inside a given speech, but also among different speeches. Moreover,\nunder a purely methodological perspective, the presented contribution suggests\npossible ways of generating a linguistic decision-making algorithm.", "published": "2019-05-09 14:17:15", "link": "http://arxiv.org/abs/1905.04705v1", "categories": ["cs.CL", "physics.soc-ph"], "primary_category": "cs.CL"}
{"title": "Analysis of Deep Clustering as Preprocessing for Automatic Speech\n  Recognition of Sparsely Overlapping Speech", "abstract": "Significant performance degradation of automatic speech recognition (ASR)\nsystems is observed when the audio signal contains cross-talk. One of the\nrecently proposed approaches to solve the problem of multi-speaker ASR is the\ndeep clustering (DPCL) approach. Combining DPCL with a state-of-the-art hybrid\nacoustic model, we obtain a word error rate (WER) of 16.5 % on the commonly\nused wsj0-2mix dataset, which is the best performance reported thus far to the\nbest of our knowledge. The wsj0-2mix dataset contains simulated cross-talk\nwhere the speech of multiple speakers overlaps for almost the entire utterance.\nIn a more realistic ASR scenario the audio signal contains significant portions\nof single-speaker speech and only part of the signal contains speech of\nmultiple competing speakers. This paper investigates obstacles of applying DPCL\nas a preprocessing method for ASR in such a scenario of sparsely overlapping\nspeech. To this end we present a data simulation approach, closely related to\nthe wsj0-2mix dataset, generating sparsely overlapping speech datasets of\narbitrary overlap ratio. The analysis of applying DPCL to sparsely overlapping\nspeech is an important interim step between the fully overlapping datasets like\nwsj0-2mix and more realistic ASR datasets, such as CHiME-5 or AMI.", "published": "2019-05-09 09:22:40", "link": "http://arxiv.org/abs/1905.03500v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Mappa Mundi: An Interactive Artistic Mind Map Generator with Artificial\n  Imagination", "abstract": "We present a novel real-time, collaborative, and interactive AI painting\nsystem, Mappa Mundi, for artistic Mind Map creation. The system consists of a\nvoice-based input interface, an automatic topic expansion module, and an image\nprojection module. The key innovation is to inject Artificial Imagination into\npainting creation by considering lexical and phonological similarities of\nlanguage, learning and inheriting artist's original painting style, and\napplying the principles of Dadaism and impossibility of improvisation. Our\nsystem indicates that AI and artist can collaborate seamlessly to create\nimaginative artistic painting and Mappa Mundi has been applied in art\nexhibition in UCCA, Beijing", "published": "2019-05-09 13:51:46", "link": "http://arxiv.org/abs/1905.03638v2", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI"}
{"title": "When Deep Learning Met Code Search", "abstract": "There have been multiple recent proposals on using deep neural networks for\ncode search using natural language. Common across these proposals is the idea\nof $\\mathit{embedding}$ code and natural language queries, into real vectors\nand then using vector distance to approximate semantic correlation between code\nand the query. Multiple approaches exist for learning these embeddings,\nincluding $\\mathit{unsupervised}$ techniques, which rely only on a corpus of\ncode examples, and $\\mathit{supervised}$ techniques, which use an\n$\\mathit{aligned}$ corpus of paired code and natural language descriptions. The\ngoal of this supervision is to produce embeddings that are more similar for a\nquery and the corresponding desired code snippet. Clearly, there are choices in\nwhether to use supervised techniques at all, and if one does, what sort of\nnetwork and training to use for supervision. This paper is the first to\nevaluate these choices systematically. To this end, we assembled\nimplementations of state-of-the-art techniques to run on a common platform,\ntraining and evaluation corpora. To explore the design space in network\ncomplexity, we also introduced a new design point that is a $\\mathit{minimal}$\nsupervision extension to an existing unsupervised technique. Our evaluation\nshows that: 1. adding supervision to an existing unsupervised technique can\nimprove performance, though not necessarily by much; 2. simple networks for\nsupervision can be more effective that more sophisticated sequence-based\nnetworks for code search; 3. while it is common to use docstrings to carry out\nsupervision, there is a sizeable gap between the effectiveness of docstrings\nand a more query-appropriate supervision corpus.\n  The evaluation dataset is now available at arXiv:1908.09804", "published": "2019-05-09 18:47:38", "link": "http://arxiv.org/abs/1905.03813v4", "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Modeling user context for valence prediction from narratives", "abstract": "Automated prediction of valence, one key feature of a person's emotional\nstate, from individuals' personal narratives may provide crucial information\nfor mental healthcare (e.g. early diagnosis of mental diseases, supervision of\ndisease course, etc.). In the Interspeech 2018 ComParE Self-Assessed Affect\nchallenge, the task of valence prediction was framed as a three-class\nclassification problem using 8 seconds fragments from individuals' narratives.\nAs such, the task did not allow for exploring contextual information of the\nnarratives. In this work, we investigate the intrinsic information from\nmultiple narratives recounted by the same individual in order to predict their\ncurrent state-of-mind. Furthermore, with generalizability in mind, we decided\nto focus our experiments exclusively on textual information as the public\navailability of audio narratives is limited compared to text. Our hypothesis\nis, that context modeling might provide insights about emotion triggering\nconcepts (e.g. events, people, places) mentioned in the narratives that are\nlinked to an individual's state of mind. We explore multiple machine learning\ntechniques to model narratives. We find that the models are able to capture\ninter-individual differences, leading to more accurate predictions of an\nindividual's emotional state, as compared to single narratives.", "published": "2019-05-09 20:57:14", "link": "http://arxiv.org/abs/1905.05701v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Sound texture synthesis using convolutional neural networks", "abstract": "The following article introduces a new parametric synthesis algorithm for\nsound textures inspired by existing methods used for visual textures. Using a\n2D Convolutional Neural Network (CNN), a sound signal is modified until the\ntemporal cross-correlations of the feature maps of its log-spectrogram resemble\nthose of a target texture. We show that the resulting synthesized sound signal\nis both different from the original and of high quality, while being able to\nreproduce singular events appearing in the original. This process is performed\nin the time domain, discarding the harmful phase recovery step which usually\nconcludes synthesis performed in the time-frequency domain. It is also\nstraightforward and flexible, as it does not require any fine tuning between\nseveral losses when synthesizing diverse sound textures. A way of extending the\nsynthesis in order to produce a sound of any length is also presented, after\nwhich synthesized spectrograms and sound signals are showcased. We also discuss\non the choice of CNN, on border effects in our synthesized signals and on\npossible ways of modifying the algorithm in order to improve its current long\ncomputation time.", "published": "2019-05-09 13:51:27", "link": "http://arxiv.org/abs/1905.03637v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Block-Online Multi-Channel Speech Enhancement Using DNN-Supported\n  Relative Transfer Function Estimates", "abstract": "This work addresses the problem of block-online processing for multi-channel\nspeech enhancement. Such processing is vital in scenarios with moving speakers\nand/or when very short utterances are processed, e.g., in voice assistant\nscenarios. We consider several variants of a system that performs beamforming\nsupported by DNN-based voice activity detection (VAD) followed by\npost-filtering. The speaker is targeted through estimating relative transfer\nfunctions between microphones. Each block of the input signals is processed\nindependently in order to make the method applicable in highly dynamic\nenvironments. Owing to the short length of the processed block, the statistics\nrequired by the beamformer are estimated less precisely. The influence of this\ninaccuracy is studied and compared to the processing regime when recordings are\ntreated as one block (batch processing). The experimental evaluation of the\nproposed method is performed on large datasets of CHiME-4 and on another\ndataset featuring moving target speaker. The experiments are evaluated in terms\nof objective and perceptual criteria (such as signal-to-interference ratio\n(SIR) or perceptual evaluation of speech quality (PESQ), respectively).\nMoreover, word error rate (WER) achieved by a baseline automatic speech\nrecognition system is evaluated, for which the enhancement method serves as a\nfront-end solution. The results indicate that the proposed method is robust\nwith respect to short length of the processed block. Significant improvements\nin terms of the criteria and WER are observed even for the block length of 250\nms.", "published": "2019-05-09 13:47:30", "link": "http://arxiv.org/abs/1905.03632v3", "categories": ["cs.SD", "cs.SY", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Universal Adversarial Perturbations for Speech Recognition Systems", "abstract": "In this work, we demonstrate the existence of universal adversarial audio\nperturbations that cause mis-transcription of audio signals by automatic speech\nrecognition (ASR) systems. We propose an algorithm to find a single\nquasi-imperceptible perturbation, which when added to any arbitrary speech\nsignal, will most likely fool the victim speech recognition model. Our\nexperiments demonstrate the application of our proposed technique by crafting\naudio-agnostic universal perturbations for the state-of-the-art ASR system --\nMozilla DeepSpeech. Additionally, we show that such perturbations generalize to\na significant extent across models that are not available during training, by\nperforming a transferability test on a WaveNet based ASR system.", "published": "2019-05-09 19:35:30", "link": "http://arxiv.org/abs/1905.03828v2", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Adversarially Trained Autoencoders for Parallel-Data-Free Voice\n  Conversion", "abstract": "We present a method for converting the voices between a set of speakers. Our\nmethod is based on training multiple autoencoder paths, where there is a single\nspeaker-independent encoder and multiple speaker-dependent decoders. The\nautoencoders are trained with an addition of an adversarial loss which is\nprovided by an auxiliary classifier in order to guide the output of the encoder\nto be speaker independent. The training of the model is unsupervised in the\nsense that it does not require collecting the same utterances from the speakers\nnor does it require time aligning over phonemes. Due to the use of a single\nencoder, our method can generalize to converting the voice of out-of-training\nspeakers to speakers in the training dataset. We present subjective tests\ncorroborating the performance of our method.", "published": "2019-05-09 21:34:13", "link": "http://arxiv.org/abs/1905.03864v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
