{"title": "Dynamic graph neural networks for enhanced volatility prediction in financial markets", "abstract": "Volatility forecasting is essential for risk management and decision-making\nin financial markets. Traditional models like Generalized Autoregressive\nConditional Heteroskedasticity (GARCH) effectively capture volatility\nclustering but often fail to model complex, non-linear interdependencies\nbetween multiple indices. This paper proposes a novel approach using Graph\nNeural Networks (GNNs) to represent global financial markets as dynamic graphs.\nThe Temporal Graph Attention Network (Temporal GAT) combines Graph\nConvolutional Networks (GCNs) and Graph Attention Networks (GATs) to capture\nthe temporal and structural dynamics of volatility spillovers. By utilizing\ncorrelation-based and volatility spillover indices, the Temporal GAT constructs\ndirected graphs that enhance the accuracy of volatility predictions. Empirical\nresults from a 15-year study of eight major global indices show that the\nTemporal GAT outperforms traditional GARCH models and other machine learning\nmethods, particularly in short- to mid-term forecasts. The sensitivity and\nscenario-based analysis over a range of parameters and hyperparameters further\ndemonstrate the significance of the proposed technique. Hence, this work\nhighlights the potential of GNNs in modeling complex market behaviors,\nproviding valuable insights for financial analysts and investors.", "published": "2024-10-22 09:52:15", "link": "http://arxiv.org/abs/2410.16858v1", "categories": ["q-fin.MF", "cs.LG", "q-fin.ST"], "primary_category": "q-fin.MF"}
{"title": "Optimal consumption under relaxed benchmark tracking and consumption drawdown constraint", "abstract": "This paper studies an optimal consumption problem with both relaxed benchmark\ntracking and consumption drawdown constraint, leading to a stochastic control\nproblem with dynamic state-control constraints. In our relaxed tracking\nformulation, it is assumed that the fund manager can strategically inject\ncapital to the fund account such that the total capital process always\noutperforms the benchmark process, which is described by a geometric Brownian\nmotion. We first transform the original regular-singular control problem with\nstate-control constraints into an equivalent regular control problem with a\nreflected state process and consumption drawdown constraint. By utilizing the\ndual transform and the optimal consumption behavior, we then turn to study the\nlinear dual PDE with both Neumann boundary condition and free boundary\ncondition in a piecewise manner across different regions. Using the smoothfit\nprinciple and the super-contact condition, we derive the closed-form solution\nof the dual PDE, and obtain the optimal investment and consumption in feedback\nform. We then prove the verification theorem on optimality by some novel\narguments with the aid of an auxiliary reflected dual process and some\ntechnical estimations. Some numerical examples and financial insights are also\npresented.", "published": "2024-10-22 01:49:25", "link": "http://arxiv.org/abs/2410.16611v1", "categories": ["math.OC", "q-fin.MF"], "primary_category": "math.OC"}
{"title": "Kendall Correlation Coefficients for Portfolio Optimization", "abstract": "Markowitz's optimal portfolio relies on the accurate estimation of\ncorrelations between asset returns, a difficult problem when the number of\nobservations is not much larger than the number of assets. Using powerful\nresults from random matrix theory, several schemes have been developed to\n\"clean\" the eigenvalues of empirical correlation matrices. By contrast, the (in\npractice equally important) problem of correctly estimating the eigenvectors of\nthe correlation matrix has received comparatively little attention. Here we\ndiscuss a class of correlation estimators generalizing Kendall's rank\ncorrelation coefficient which improve the estimation of both eigenvalues and\neigenvectors in data-poor regimes. Using both synthetic and real financial\ndata, we show that these generalized correlation coefficients yield Markowitz\nportfolios with lower out-of-sample risk than those obtained with rotationally\ninvariant estimators. Central to these results is a property shared by all\nKendall-like estimators but not with classical correlation coefficients: zero\neigenvalues only appear when the number of assets becomes proportional to the\nsquare of the number of data points.", "published": "2024-10-22 19:01:10", "link": "http://arxiv.org/abs/2410.17366v1", "categories": ["q-fin.ST", "cond-mat.stat-mech"], "primary_category": "q-fin.ST"}
{"title": "A Statistical Analysis of LLMs' Self-Evaluation Using Proverbs", "abstract": "Large language models (LLMs) such as ChatGPT, GPT-4, Claude-3, and Llama are\nbeing integrated across a variety of industries. Despite this rapid\nproliferation, experts are calling for caution in the interpretation and\nadoption of LLMs, owing to numerous associated ethical concerns. Research has\nalso uncovered shortcomings in LLMs' reasoning and logical abilities, raising\nquestions on the potential of LLMs as evaluation tools. In this paper, we\ninvestigate LLMs' self-evaluation capabilities on a novel proverb reasoning\ntask. We introduce a novel proverb database consisting of 300 proverb pairs\nthat are similar in intent but different in wordings, across topics spanning\ngender, wisdom, and society. We propose tests to evaluate textual consistencies\nas well as numerical consistencies across similar proverbs, and demonstrate the\neffectiveness of our method and dataset in identifying failures in LLMs'\nself-evaluation which in turn can highlight issues related to gender\nstereotypes and lack of cultural understanding in LLMs.", "published": "2024-10-22 02:38:48", "link": "http://arxiv.org/abs/2410.16640v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Atomic Fact Decomposition Helps Attributed Question Answering", "abstract": "Attributed Question Answering (AQA) aims to provide both a trustworthy answer\nand a reliable attribution report for a given question. Retrieval is a widely\nadopted approach, including two general paradigms: Retrieval-Then-Read (RTR)\nand post-hoc retrieval. Recently, Large Language Models (LLMs) have shown\nremarkable proficiency, prompting growing interest in AQA among researchers.\nHowever, RTR-based AQA often suffers from irrelevant knowledge and rapidly\nchanging information, even when LLMs are adopted, while post-hoc\nretrieval-based AQA struggles with comprehending long-form answers with complex\nlogic, and precisely identifying the content needing revision and preserving\nthe original intent. To tackle these problems, this paper proposes an Atomic\nfact decomposition-based Retrieval and Editing (ARE) framework, which\ndecomposes the generated long-form answers into molecular clauses and atomic\nfacts by the instruction-tuned LLMs. Notably, the instruction-tuned LLMs are\nfine-tuned using a well-constructed dataset, generated from large scale\nKnowledge Graphs (KGs). This process involves extracting one-hop neighbors from\na given set of entities and transforming the result into coherent long-form\ntext. Subsequently, ARE leverages a search engine to retrieve evidences related\nto atomic facts, inputting these evidences into an LLM-based verifier to\ndetermine whether the facts require expansion for re-retrieval or editing.\nFurthermore, the edited facts are backtracked into the original answer, with\nevidence aggregated based on the relationship between molecular clauses and\natomic facts. Extensive evaluations demonstrate the superior performance of our\nproposed method over the state-of-the-arts on several datasets, with an\nadditionally proposed new metric $Attr_{p}$ for evaluating the precision of\nevidence attribution.", "published": "2024-10-22 05:25:54", "link": "http://arxiv.org/abs/2410.16708v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Magnetic Preference Optimization: Achieving Last-iterate Convergence for\n  Language Model Alignment", "abstract": "Self-play methods have demonstrated remarkable success in enhancing model\ncapabilities across various domains. In the context of Reinforcement Learning\nfrom Human Feedback (RLHF), self-play not only boosts Large Language Model\n(LLM) performance but also overcomes the limitations of traditional\nBradley-Terry (BT) model assumptions by finding the Nash equilibrium (NE) of a\npreference-based, two-player constant-sum game. However, existing methods\neither guarantee only average-iterate convergence, incurring high storage and\ninference costs, or converge to the NE of a regularized game, failing to\naccurately reflect true human preferences. In this paper, we introduce Magnetic\nPreference Optimization (MPO), a novel approach capable of achieving\nlast-iterate convergence to the NE of the original game, effectively overcoming\nthe limitations of existing methods. Building upon Magnetic Mirror Descent\n(MMD), MPO attains a linear convergence rate, making it particularly suitable\nfor fine-tuning LLMs. To ensure our algorithm is both theoretically sound and\npractically viable, we present a simple yet effective implementation that\nadapts the theoretical insights to the RLHF setting. Empirical results\ndemonstrate that MPO can significantly enhance the performance of LLMs,\nhighlighting the potential of self-play methods in alignment.", "published": "2024-10-22 05:51:34", "link": "http://arxiv.org/abs/2410.16714v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Forewarned is Forearmed: Leveraging LLMs for Data Synthesis through\n  Failure-Inducing Exploration", "abstract": "Large language models (LLMs) have significantly benefited from training on\ndiverse, high-quality task-specific data, leading to impressive performance\nacross a range of downstream applications. Current methods often rely on\nhuman-annotated data or predefined task templates to direct powerful LLMs in\nsynthesizing task-relevant data for effective model training. However, this\ndependence on manually designed components may constrain the scope of generated\ndata, potentially overlooking critical edge cases or novel scenarios that could\nchallenge the model. In this paper, we present a novel approach, ReverseGen,\ndesigned to automatically generate effective training samples that expose the\nweaknesses of LLMs. Specifically, we introduce a dedicated proposer trained to\nproduce queries that lead target models to generate unsatisfactory responses.\nThese failure-inducing queries are then used to construct training data,\nhelping to address the models' shortcomings and improve overall performance.\nOur approach is flexible and can be applied to models of various scales (3B,\n7B, and 8B). We evaluate ReverseGen on three key applications (safety, honesty,\nand math), demonstrating that our generated data is both highly effective and\ndiverse. Models fine-tuned with ReverseGen-generated data consistently\noutperform those trained on human-annotated or general model-generated data,\noffering a new perspective on data synthesis for task-specific LLM enhancement.", "published": "2024-10-22 06:43:28", "link": "http://arxiv.org/abs/2410.16736v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Context-Aware LLM Translation System Using Conversation Summarization\n  and Dialogue History", "abstract": "Translating conversational text, particularly in customer support contexts,\npresents unique challenges due to its informal and unstructured nature. We\npropose a context-aware LLM translation system that leverages conversation\nsummarization and dialogue history to enhance translation quality for the\nEnglish-Korean language pair. Our approach incorporates the two most recent\ndialogues as raw data and a summary of earlier conversations to manage context\nlength effectively. We demonstrate that this method significantly improves\ntranslation accuracy, maintaining coherence and consistency across\nconversations. This system offers a practical solution for customer support\ntranslation tasks, addressing the complexities of conversational text.", "published": "2024-10-22 07:45:18", "link": "http://arxiv.org/abs/2410.16775v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Optimizing Chain-of-Thought Reasoning: Tackling Arranging Bottleneck via\n  Plan Augmentation", "abstract": "Multi-step reasoning ability of large language models is crucial in tasks\nsuch as math and tool utilization. Current researches predominantly focus on\nenhancing model performance in these multi-step reasoning tasks through\nfine-tuning with Chain-of-Thought (CoT) steps, yet these methods tend to be\nheuristic, without exploring nor resolving the bottleneck. In this study, we\nsubdivide CoT reasoning into two parts: arranging and executing, and identify\nthat the bottleneck of models mainly lies in arranging rather than executing.\nBased on this finding, we propose a plan-based training and reasoning method\nthat guides models to generate arranging steps through abstract plans. We\nexperiment on both math (GSM8k) and tool utilization (ToolBench) benchmarks.\nResults show that compared to fine-tuning directly with CoT data, our approach\nachieves a better performance on alleviating arranging bottleneck, particularly\nexcelling in long-distance reasoning generalization.", "published": "2024-10-22 08:38:50", "link": "http://arxiv.org/abs/2410.16812v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analyzing and Evaluating Correlation Measures in NLG Meta-Evaluation", "abstract": "The correlation between NLG automatic evaluation metrics and human evaluation\nis often regarded as a critical criterion for assessing the capability of an\nevaluation metric. However, different grouping methods and correlation\ncoefficients result in various types of correlation measures used in\nmeta-evaluation. In specific evaluation scenarios, prior work often directly\nfollows conventional measure settings, but the characteristics and differences\nbetween these measures have not gotten sufficient attention. Therefore, this\npaper analyzes 12 common correlation measures using a large amount of\nreal-world data from six widely-used NLG evaluation datasets and 32 evaluation\nmetrics, revealing that different measures indeed impact the meta-evaluation\nresults. Furthermore, we propose three perspectives that reflect the capability\nof meta-evaluation: discriminative power, ranking consistency, and sensitivity\nto score granularity. We find that the measure using global grouping and\nPearson correlation coefficient exhibits the best performance in both\ndiscriminative power and ranking consistency. Besides, the measures using\nsystem-level grouping or Kendall correlation are the least sensitive to score\ngranularity.", "published": "2024-10-22 09:14:21", "link": "http://arxiv.org/abs/2410.16834v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Trustworthy Alignment of Retrieval-Augmented Large Language Models via\n  Reinforcement Learning", "abstract": "Trustworthiness is an essential prerequisite for the real-world application\nof large language models. In this paper, we focus on the trustworthiness of\nlanguage models with respect to retrieval augmentation. Despite being supported\nwith external evidence, retrieval-augmented generation still suffers from\nhallucinations, one primary cause of which is the conflict between contextual\nand parametric knowledge. We deem that retrieval-augmented language models have\nthe inherent capabilities of supplying response according to both contextual\nand parametric knowledge. Inspired by aligning language models with human\npreference, we take the first step towards aligning retrieval-augmented\nlanguage models to a status where it responds relying merely on the external\nevidence and disregards the interference of parametric knowledge. Specifically,\nwe propose a reinforcement learning based algorithm Trustworthy-Alignment,\ntheoretically and experimentally demonstrating large language models'\ncapability of reaching a trustworthy status without explicit supervision on how\nto respond. Our work highlights the potential of large language models on\nexploring its intrinsic abilities by its own and expands the application\nscenarios of alignment from fulfilling human preference to creating trustworthy\nagents.", "published": "2024-10-22 09:25:21", "link": "http://arxiv.org/abs/2410.16843v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ETHIC: Evaluating Large Language Models on Long-Context Tasks with High\n  Information Coverage", "abstract": "Recent advancements in large language models (LLM) capable of processing\nextremely long texts highlight the need for a dedicated evaluation benchmark to\nassess their long-context capabilities. However, existing methods, like the\nneedle-in-a-haystack test, do not effectively assess whether these models fully\nutilize contextual information, raising concerns about the reliability of\ncurrent evaluation techniques. To thoroughly examine the effectiveness of\nexisting benchmarks, we introduce a new metric called information coverage\n(IC), which quantifies the proportion of the input context necessary for\nanswering queries. Our findings indicate that current benchmarks exhibit low\nIC; although the input context may be extensive, the actual usable context is\noften limited. To address this, we present ETHIC, a novel benchmark designed to\nassess LLMs' ability to leverage the entire context. Our benchmark comprises\n1,986 test instances spanning four long-context tasks with high IC scores in\nthe domains of books, debates, medicine, and law. Our evaluations reveal\nsignificant performance drops in contemporary LLMs, highlighting a critical\nchallenge in managing long contexts. Our benchmark is available at\nhttps://github.com/dmis-lab/ETHIC.", "published": "2024-10-22 09:35:42", "link": "http://arxiv.org/abs/2410.16848v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IPL: Leveraging Multimodal Large Language Models for Intelligent Product\n  Listing", "abstract": "Unlike professional Business-to-Consumer (B2C) e-commerce platforms (e.g.,\nAmazon), Consumer-to-Consumer (C2C) platforms (e.g., Facebook marketplace) are\nmainly targeting individual sellers who usually lack sufficient experience in\ne-commerce. Individual sellers often struggle to compose proper descriptions\nfor selling products. With the recent advancement of Multimodal Large Language\nModels (MLLMs), we attempt to integrate such state-of-the-art generative AI\ntechnologies into the product listing process. To this end, we develop IPL, an\nIntelligent Product Listing tool tailored to generate descriptions using\nvarious product attributes such as category, brand, color, condition, etc. IPL\nenables users to compose product descriptions by merely uploading photos of the\nselling product. More importantly, it can imitate the content style of our C2C\nplatform Xianyu. This is achieved by employing domain-specific instruction\ntuning on MLLMs and adopting the multi-modal Retrieval-Augmented Generation\n(RAG) process. A comprehensive empirical evaluation demonstrates that the\nunderlying model of IPL significantly outperforms the base model in\ndomain-specific tasks while producing less hallucination. IPL has been\nsuccessfully deployed in our production system, where 72% of users have their\npublished product listings based on the generated content, and those product\nlistings are shown to have a quality score 5.6% higher than those without AI\nassistance.", "published": "2024-10-22 12:56:04", "link": "http://arxiv.org/abs/2410.16977v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Forgetting in Large Language Model Pre-Training", "abstract": "Catastrophic forgetting remains a formidable obstacle to building an\nomniscient model in large language models (LLMs). Despite the pioneering\nresearch on task-level forgetting in LLM fine-tuning, there is scant focus on\nforgetting during pre-training. We systematically explored the existence and\nmeasurement of forgetting in pre-training, questioning traditional metrics such\nas perplexity (PPL) and introducing new metrics to better detect entity memory\nretention. Based on our revised assessment of forgetting metrics, we explored\nlow-cost, straightforward methods to mitigate forgetting during the\npre-training phase. Further, we carefully analyzed the learning curves,\noffering insights into the dynamics of forgetting. Extensive evaluations and\nanalyses on forgetting of pre-training could facilitate future research on\nLLMs.", "published": "2024-10-22 13:39:47", "link": "http://arxiv.org/abs/2410.17018v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SG-FSM: A Self-Guiding Zero-Shot Prompting Paradigm for Multi-Hop\n  Question Answering Based on Finite State Machine", "abstract": "Large Language Models with chain-of-thought prompting, such as OpenAI-o1,\nhave shown impressive capabilities in natural language inference tasks.\nHowever, Multi-hop Question Answering (MHQA) remains challenging for many\nexisting models due to issues like hallucination, error propagation, and\nlimited context length. To address these challenges and enhance LLMs'\nperformance on MHQA, we propose the Self-Guiding prompting Finite State Machine\n(SG-FSM), designed to strengthen multi-hop reasoning abilities. Unlike\ntraditional chain-of-thought methods, SG-FSM tackles MHQA by iteratively\nbreaking down complex questions into sub-questions, correcting itself to\nimprove accuracy. It processes one sub-question at a time, dynamically deciding\nthe next step based on the current context and results, functioning much like\nan automaton. Experiments across various benchmarks demonstrate the\neffectiveness of our approach, outperforming strong baselines on challenging\ndatasets such as Musique. SG-FSM reduces hallucination, enabling recovery of\nthe correct final answer despite intermediate errors. It also improves\nadherence to specified output formats, simplifying evaluation significantly.", "published": "2024-10-22 13:47:38", "link": "http://arxiv.org/abs/2410.17021v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DIRI: Adversarial Patient Reidentification with Large Language Models\n  for Evaluating Clinical Text Anonymization", "abstract": "Sharing protected health information (PHI) is critical for furthering\nbiomedical research. Before data can be distributed, practitioners often\nperform deidentification to remove any PHI contained in the text. Contemporary\ndeidentification methods are evaluated on highly saturated datasets (tools\nachieve near-perfect accuracy) which may not reflect the full variability or\ncomplexity of real-world clinical text and annotating them is resource\nintensive, which is a barrier to real-world applications. To address this gap,\nwe developed an adversarial approach using a large language model (LLM) to\nre-identify the patient corresponding to a redacted clinical note and evaluated\nthe performance with a novel De-Identification/Re-Identification (DIRI) method.\nOur method uses a large language model to reidentify the patient corresponding\nto a redacted clinical note. We demonstrate our method on medical data from\nWeill Cornell Medicine anonymized with three deidentification tools: rule-based\nPhilter and two deep-learning-based models, BiLSTM-CRF and ClinicalBERT.\nAlthough ClinicalBERT was the most effective, masking all identified PII, our\ntool still reidentified 9% of clinical notes Our study highlights significant\nweaknesses in current deidentification technologies while providing a tool for\niterative development and improvement.", "published": "2024-10-22 14:06:31", "link": "http://arxiv.org/abs/2410.17035v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Arabic Dataset for LLM Safeguard Evaluation", "abstract": "The growing use of large language models (LLMs) has raised concerns regarding\ntheir safety. While many studies have focused on English, the safety of LLMs in\nArabic, with its linguistic and cultural complexities, remains under-explored.\nHere, we aim to bridge this gap. In particular, we present an\nArab-region-specific safety evaluation dataset consisting of 5,799 questions,\nincluding direct attacks, indirect attacks, and harmless requests with\nsensitive words, adapted to reflect the socio-cultural context of the Arab\nworld. To uncover the impact of different stances in handling sensitive and\ncontroversial topics, we propose a dual-perspective evaluation framework. It\nassesses the LLM responses from both governmental and opposition viewpoints.\nExperiments over five leading Arabic-centric and multilingual LLMs reveal\nsubstantial disparities in their safety performance. This reinforces the need\nfor culturally specific datasets to ensure the responsible deployment of LLMs.", "published": "2024-10-22 14:12:43", "link": "http://arxiv.org/abs/2410.17040v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data-driven Coreference-based Ontology Building", "abstract": "While coreference resolution is traditionally used as a component in\nindividual document understanding, in this work we take a more global view and\nexplore what can we learn about a domain from the set of all document-level\ncoreference relations that are present in a large corpus. We derive coreference\nchains from a corpus of 30 million biomedical abstracts and construct a graph\nbased on the string phrases within these chains, establishing connections\nbetween phrases if they co-occur within the same coreference chain. We then use\nthe graph structure and the betweeness centrality measure to distinguish\nbetween edges denoting hierarchy, identity and noise, assign directionality to\nedges denoting hierarchy, and split nodes (strings) that correspond to multiple\ndistinct concepts. The result is a rich, data-driven ontology over concepts in\nthe biomedical domain, parts of which overlaps significantly with\nhuman-authored ontologies. We release the coreference chains and resulting\nontology under a creative-commons license, along with the code.", "published": "2024-10-22 14:30:40", "link": "http://arxiv.org/abs/2410.17051v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Aligning Large Language Models via Self-Steering Optimization", "abstract": "Automated alignment develops alignment systems with minimal human\nintervention. The key to automated alignment lies in providing learnable and\naccurate preference signals for preference learning without human annotation.\nIn this paper, we introduce Self-Steering Optimization ($SSO$), an algorithm\nthat autonomously generates high-quality preference signals based on predefined\nprinciples during iterative training, eliminating the need for manual\nannotation. $SSO$ maintains the accuracy of signals by ensuring a consistent\ngap between chosen and rejected responses while keeping them both on-policy to\nsuit the current policy model's learning capacity. $SSO$ can benefit the online\nand offline training of the policy model, as well as enhance the training of\nreward models. We validate the effectiveness of $SSO$ with two foundation\nmodels, Qwen2 and Llama3.1, indicating that it provides accurate, on-policy\npreference signals throughout iterative training. Without any manual annotation\nor external models, $SSO$ leads to significant performance improvements across\nsix subjective or objective benchmarks. Besides, the preference data generated\nby $SSO$ significantly enhanced the performance of the reward model on\nRewardbench. Our work presents a scalable approach to preference optimization,\npaving the way for more efficient and effective automated alignment.", "published": "2024-10-22 16:04:03", "link": "http://arxiv.org/abs/2410.17131v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-calibration for Language Model Quantization and Pruning", "abstract": "Quantization and pruning are fundamental approaches for model compression,\nenabling efficient inference for language models. In a post-training setting,\nstate-of-the-art quantization and pruning methods require calibration data, a\nsmall set of unlabeled examples. Conventionally, this is randomly sampled web\ntext, aiming to reflect the model training data. However, this poses two key\nproblems: (1) unrepresentative calibration examples can harm model performance,\nand (2) organizations increasingly avoid releasing model training data. In this\npaper, we propose self-calibration as a solution. Our approach requires no\nexternal data, instead leveraging the model itself to generate synthetic\ncalibration data, with a view to better approximating the pre-training data\ndistribution. We extensively compare the performance of self-calibration with\nseveral baselines, across a variety of models, compression methods, and tasks.\nOur approach proves consistently competitive in maximizing downstream task\nperformance, frequently outperforming even using real data.", "published": "2024-10-22 16:50:00", "link": "http://arxiv.org/abs/2410.17170v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Attention to Activation: Unravelling the Enigmas of Large Language\n  Models", "abstract": "We study two strange phenomena in auto-regressive Transformers: (1) the\ndominance of the first token in attention heads; (2) the occurrence of large\noutlier activations in the hidden states. We find that popular large language\nmodels, such as Llama attend maximally to the first token in 98% of attention\nheads, a behaviour we attribute to the softmax function. To mitigate this\nissue, we propose a reformulation of softmax to softmax-1. Furthermore, we\nidentify adaptive optimisers, e.g. Adam, as the primary contributor to the\nlarge outlier activations and introduce OrthoAdam, a novel optimiser that\nutilises orthogonal matrices to transform gradients, to address this issue.\nFinally, not only do our methods prevent these phenomena from occurring, but\nadditionally, they enable Transformers to sustain their performance when\nquantised using basic algorithms, something that standard methods are unable to\ndo. In summary, our methods reduce the attention proportion on the first token\nfrom 65% to 3.3%, the activation kurtosis in the hidden states from 1657 to\n3.1, and perplexity penalty under 4-bit weight quantisation from 3565 to 0.3.", "published": "2024-10-22 16:51:27", "link": "http://arxiv.org/abs/2410.17174v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MiniPLM: Knowledge Distillation for Pre-Training Language Models", "abstract": "Knowledge distillation (KD) is widely used to train small, high-performing\nstudent language models (LMs) using large teacher LMs. While effective in\nfine-tuning, KD during pre-training faces efficiency, flexibility, and\neffectiveness issues. Existing methods either incur high computational costs\ndue to online teacher inference, require tokenization matching between teacher\nand student LMs, or risk losing the difficulty and diversity of the\nteacher-generated training data. In this work, we propose MiniPLM, a KD\nframework for pre-training LMs by refining the training data distribution with\nthe teacher LM's knowledge. For efficiency, MiniPLM performs offline teacher\ninference, allowing KD for multiple student LMs without adding training costs.\nFor flexibility, MiniPLM operates solely on the training corpus, enabling KD\nacross model families. For effectiveness, MiniPLM leverages the differences\nbetween large and small LMs to enhance the training data difficulty and\ndiversity, helping student LMs acquire versatile and sophisticated knowledge.\nExtensive experiments demonstrate that MiniPLM boosts the student LMs'\nperformance on 9 common downstream tasks, improves language modeling\ncapabilities, and reduces pre-training computation. The benefit of MiniPLM\nextends to larger training scales, evidenced by the scaling curve\nextrapolation. Further analysis reveals that MiniPLM supports KD across model\nfamilies and enhances the pre-training data utilization. Our code, data, and\nmodels can be found at https://github.com/thu-coai/MiniPLM.", "published": "2024-10-22 17:40:32", "link": "http://arxiv.org/abs/2410.17215v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Context-aware Prompt Tuning: Advancing In-Context Learning with\n  Adversarial Methods", "abstract": "Fine-tuning Large Language Models (LLMs) typically involves updating at least\na few billions of parameters. A more parameter-efficient approach is Prompt\nTuning (PT), which updates only a few learnable tokens, and differently,\nIn-Context Learning (ICL) adapts the model to a new task by simply including\nexamples in the input without any training. When applying optimization-based\nmethods, such as fine-tuning and PT for few-shot learning, the model is\nspecifically adapted to the small set of training examples, whereas ICL leaves\nthe model unchanged. This distinction makes traditional learning methods more\nprone to overfitting; in contrast, ICL is less sensitive to the few-shot\nscenario. While ICL is not prone to overfitting, it does not fully extract the\ninformation that exists in the training examples. This work introduces\nContext-aware Prompt Tuning (CPT), a method inspired by ICL, PT, and\nadversarial attacks. We build on the ICL strategy of concatenating examples\nbefore the input, but we extend this by PT-like learning, refining the context\nembedding through iterative optimization to extract deeper insights from the\ntraining examples. We carefully modify specific context tokens, considering the\nunique structure of input and output formats. Inspired by adversarial attacks,\nwe adjust the input based on the labels present in the context, focusing on\nminimizing, rather than maximizing, the loss. Moreover, we apply a projected\ngradient descent algorithm to keep token embeddings close to their original\nvalues, under the assumption that the user-provided data is inherently\nvaluable. Our method has been shown to achieve superior accuracy across\nmultiple classification tasks using various LLM models.", "published": "2024-10-22 17:45:47", "link": "http://arxiv.org/abs/2410.17222v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "All Entities are Not Created Equal: Examining the Long Tail for\n  Fine-Grained Entity Typing", "abstract": "Pre-trained language models (PLMs) are trained on large amounts of data,\nwhich helps capture world knowledge alongside linguistic competence. Due to\nthis, they are extensively used for ultra-fine entity typing tasks, where they\nprovide the entity knowledge held in its parameter space. Given that PLMs learn\nfrom co-occurrence patterns, they likely contain more knowledge or less\nknowledge about entities depending on their how frequent they are in the\npre-training data. In this work, we probe PLMs to elicit encoded entity\nprobabilities and demonstrate that they highly correlate with their frequency\nin large-scale internet data. Then, we demonstrate that entity-typing\napproaches that rely on PLMs struggle with entities at the long tail on the\ndistribution. Our findings suggests that we need to go beyond PLMs to produce\nsolutions that perform well for rare, new or infrequent entities.", "published": "2024-10-22 18:47:46", "link": "http://arxiv.org/abs/2410.17355v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Scalable Influence and Fact Tracing for Large Language Model Pretraining", "abstract": "Training data attribution (TDA) methods aim to attribute model outputs back\nto specific training examples, and the application of these methods to large\nlanguage model (LLM) outputs could significantly advance model transparency and\ndata curation. However, it has been challenging to date to apply these methods\nto the full scale of LLM pretraining. In this paper, we refine existing\ngradient-based methods to work effectively at scale, allowing us to retrieve\ninfluential examples for an 8B-parameter language model from a pretraining\ncorpus of over 160B tokens with no need for subsampling or pre-filtering. Our\nmethod combines several techniques, including optimizer state correction, a\ntask-specific Hessian approximation, and normalized encodings, which we find to\nbe critical for performance at scale. In quantitative evaluations on a fact\ntracing task, our method performs best at identifying examples that influence\nmodel predictions, but classical, model-agnostic retrieval methods such as BM25\nstill perform better at finding passages which explicitly contain relevant\nfacts. These results demonstrate a misalignment between factual *attribution*\nand causal *influence*. With increasing model size and training tokens, we find\nthat influence more closely aligns with factual attribution. Finally, we\nexamine different types of examples identified as influential by our method,\nfinding that while many directly entail a particular fact, others support the\nsame output by reinforcing priors on relation types, common entities, and\nnames. We release our prompt set and model outputs, along with a web-based\nvisualization tool to explore influential examples for factual predictions,\ncommonsense reasoning, arithmetic, and open-ended generation for an\n8B-parameter LLM.", "published": "2024-10-22 20:39:21", "link": "http://arxiv.org/abs/2410.17413v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dynamic Adaptive Rank Space Exploration for Efficient Sentiment Analysis\n  with Large Language Models", "abstract": "Sentiment analysis has become increasingly important for assessing public\nopinion and informing decision-making. Large language models (LLMs) have\nrevolutionized this field by capturing nuanced language patterns. However,\nadapting LLMs to domain-specific sentiment analysis tasks remains challenging\ndue to computational constraints and the need for optimal fine-tuning. To\naddress these challenges, we propose a novel Dynamic Adaptive Rank Space\nExploration (DARSE) framework for efficient and effective sentiment analysis\nusing LLMs. DARSE consists of a coarse-grained greedy algorithm to identify the\noptimal rank range, a fine-grained exploration algorithm to refine rank\nselection, and a dynamic rank allocation method to determine the optimal rank\ncombination for each LLM layer. Extensive experiments demonstrate that DARSE\nsignificantly improves sentiment analysis accuracy, achieving a 15.1%\nimprovement in MSE and a 4.3% improvement in accuracy compared to previous\nwork. Our framework strikes a balance between computational efficiency and\nmodel performance, making it a promising approach for sentiment analysis with\nLLMs.", "published": "2024-10-22 00:14:36", "link": "http://arxiv.org/abs/2410.16589v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Distill-SynthKG: Distilling Knowledge Graph Synthesis Workflow for\n  Improved Coverage and Efficiency", "abstract": "Knowledge graphs (KGs) generated by large language models (LLMs) are becoming\nincreasingly valuable for Retrieval-Augmented Generation (RAG) applications\nthat require knowledge-intensive reasoning. However, existing KG extraction\nmethods predominantly rely on prompt-based approaches, which are inefficient\nfor processing large-scale corpora. These approaches often suffer from\ninformation loss, particularly with long documents, due to the lack of\nspecialized design for KG construction. Additionally, there is a gap in\nevaluation datasets and methodologies for ontology-free KG construction. To\novercome these limitations, we propose SynthKG, a multi-step, document-level\nontology-free KG synthesis workflow based on LLMs. By fine-tuning a smaller LLM\non the synthesized document-KG pairs, we streamline the multi-step process into\na single-step KG generation approach called Distill-SynthKG, substantially\nreducing the number of LLM inference calls. Furthermore, we re-purpose existing\nquestion-answering datasets to establish KG evaluation datasets and introduce\nnew evaluation metrics. Using KGs produced by Distill-SynthKG, we also design a\nnovel graph-based retrieval framework for RAG. Experimental results demonstrate\nthat Distill-SynthKG not only surpasses all baseline models in KG quality --\nincluding models up to eight times larger -- but also consistently excels in\nretrieval and question-answering tasks. Our proposed graph retrieval framework\nalso outperforms all KG-retrieval methods across multiple benchmark datasets.\nWe release the SynthKG dataset and Distill-SynthKG model publicly to support\nfurther research and development.", "published": "2024-10-22 00:47:54", "link": "http://arxiv.org/abs/2410.16597v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Graph-Structured Trajectory Extraction from Travelogues", "abstract": "Previous studies on sequence-based extraction of human movement trajectories\nhave an issue of inadequate trajectory representation. Specifically, a pair of\nlocations may not be lined up in a sequence especially when one location\nincludes the other geographically. In this study, we propose a graph\nrepresentation that retains information on the geographic hierarchy as well as\nthe temporal order of visited locations, and have constructed a benchmark\ndataset for graph-structured trajectory extraction. The experiments with our\nbaselines have demonstrated that it is possible to accurately predict visited\nlocations and the order among them, but it remains a challenge to predict the\nhierarchical relations.", "published": "2024-10-22 02:21:42", "link": "http://arxiv.org/abs/2410.16633v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Chatting with Bots: AI, Speech Acts, and the Edge of Assertion", "abstract": "This paper addresses the question of whether large language model-powered\nchatbots are capable of assertion. According to what we call the Thesis of\nChatbot Assertion (TCA), chatbots are the kinds of things that can assert, and\nat least some of the output produced by current-generation chatbots qualifies\nas assertion. We provide some motivation for TCA, arguing that it ought to be\ntaken seriously and not simply dismissed. We also review recent objections to\nTCA, arguing that these objections are weighty. We thus confront the following\ndilemma: how can we do justice to both the considerations for and against TCA?\nWe consider two influential responses to this dilemma - the first appeals to\nthe notion of proxy-assertion; the second appeals to fictionalism - and argue\nthat neither is satisfactory. Instead, reflecting on the ontogenesis of\nassertion, we argue that we need to make space for a category of\nproto-assertion. We then apply the category of proto-assertion to chatbots,\narguing that treating chatbots as proto-assertors provides a satisfactory\nresolution to the dilemma of chatbot assertion.", "published": "2024-10-22 02:45:09", "link": "http://arxiv.org/abs/2410.16645v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Adsorb-Agent: Autonomous Identification of Stable Adsorption\n  Configurations via Large Language Model Agent", "abstract": "Adsorption energy is a key reactivity descriptor in catalysis, enabling\nefficient screening for optimal catalysts. However, determining adsorption\nenergy typically requires evaluating numerous adsorbate-catalyst\nconfigurations. Current algorithmic approaches rely on exhaustive enumeration\nof adsorption sites and configurations, which makes the process computationally\nintensive and does not inherently guarantee the identification of the global\nminimum energy. In this work, we introduce Adsorb-Agent, a Large Language Model\n(LLM) agent designed to efficiently identify system-specific stable adsorption\nconfigurations corresponding to the global minimum adsorption energy.\nAdsorb-Agent leverages its built-in knowledge and emergent reasoning\ncapabilities to strategically explore adsorption configurations likely to hold\nadsorption energy. By reducing the reliance on exhaustive sampling, it\nsignificantly decreases the number of initial configurations required while\nimproving the accuracy of adsorption energy predictions. We evaluate\nAdsorb-Agent's performance across twenty representative systems encompassing a\nrange of complexities. The Adsorb-Agent successfully identifies comparable\nadsorption energies for 83.7% of the systems and achieves lower energies,\ncloser to the actual global minimum, for 35% of the systems, while requiring\nsignificantly fewer initial configurations than conventional methods. Its\ncapability is particularly evident in complex systems, where it identifies\nlower adsorption energies for 46.7% of systems involving intermetallic surfaces\nand 66.7% of systems with large adsorbate molecules. These results demonstrate\nthe potential of Adsorb-Agent to accelerate catalyst discovery by reducing\ncomputational costs and improving the reliability of adsorption energy\npredictions.", "published": "2024-10-22 03:19:16", "link": "http://arxiv.org/abs/2410.16658v2", "categories": ["cs.CL", "cond-mat.mtrl-sci"], "primary_category": "cs.CL"}
{"title": "SafetyAnalyst: Interpretable, transparent, and steerable safety\n  moderation for AI behavior", "abstract": "The ideal AI safety moderation system would be both structurally\ninterpretable (so its decisions can be reliably explained) and steerable (to\nalign to safety standards and reflect a community's values), which current\nsystems fall short on. To address this gap, we present SafetyAnalyst, a novel\nAI safety moderation framework. Given an AI behavior, SafetyAnalyst uses\nchain-of-thought reasoning to analyze its potential consequences by creating a\nstructured \"harm-benefit tree,\" which enumerates harmful and beneficial actions\nand effects the AI behavior may lead to, along with likelihood, severity, and\nimmediacy labels that describe potential impact on any stakeholders.\nSafetyAnalyst then aggregates all harmful and beneficial effects into a\nharmfulness score using fully interpretable weight parameters, which can be\naligned to particular safety preferences. We applied this conceptual framework\nto develop, test, and release an open-source LLM prompt safety classification\nsystem, distilled from 18.5 million harm-benefit features generated by frontier\nLLMs on 19k prompts. On a comprehensive set of prompt safety benchmarks, we\nshow that SafetyReporter (average F1=0.81) outperforms existing LLM safety\nmoderation systems (average F1$<$0.72) on prompt safety classification, while\noffering the additional advantages of interpretability, transparency, and\nsteerability.", "published": "2024-10-22 03:38:37", "link": "http://arxiv.org/abs/2410.16665v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "CausalEval: Towards Better Causal Reasoning in Language Models", "abstract": "Causal reasoning (CR) is a crucial aspect of intelligence, essential for\nproblem-solving, decision-making, and understanding the world. While language\nmodels (LMs) can generate rationales for their outputs, their ability to\nreliably perform causal reasoning remains uncertain, often falling short in\ntasks requiring a deep understanding of causality. In this paper, we introduce\nCausalEval, a comprehensive review of research aimed at enhancing LMs for\ncausal reasoning, coupled with an empirical evaluation of current models and\nmethods. We categorize existing methods based on the role of LMs: either as\nreasoning engines or as helpers providing knowledge or data to traditional CR\nmethods, followed by a detailed discussion of methodologies in each category.\nWe then assess the performance of current LMs and various enhancement methods\non a range of causal reasoning tasks, providing key findings and in-depth\nanalysis. Finally, we present insights from current studies and highlight\npromising directions for future research. We aim for this work to serve as a\ncomprehensive resource, fostering further advancements in causal reasoning with\nLMs.", "published": "2024-10-22 04:18:19", "link": "http://arxiv.org/abs/2410.16676v4", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Methods of improving LLM training stability", "abstract": "Training stability of large language models(LLMs) is an important research\ntopic. Reproducing training instabilities can be costly, so we use a small\nlanguage model with 830M parameters and experiment with higher learning rates\nto force models to diverge. One of the sources of training instability is the\ngrowth of logits in attention layers. We extend the focus of the previous work\nand look not only at the magnitude of the logits but at all outputs of linear\nlayers in the Transformer block. We observe that with a high learning rate the\nL2 norm of all linear layer outputs can grow with each training step and the\nmodel diverges. Specifically we observe that QKV, Proj and FC2 layers have the\nlargest growth of the output magnitude. This prompts us to explore several\noptions: 1) apply layer normalization not only after QK layers but also after\nProj and FC2 layers too; 2) apply layer normalization after the QKV layer (and\nremove pre normalization). 3) apply QK layer normalization together with\nsoftmax capping. We show that with the last two methods we can increase\nlearning rate by 1.5x (without model divergence) in comparison to an approach\nbased on QK layer normalization only. Also we observe significant perplexity\nimprovements for all three methods in comparison to the baseline model.", "published": "2024-10-22 04:27:03", "link": "http://arxiv.org/abs/2410.16682v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PLDR-LLM: Large Language Model from Power Law Decoder Representations", "abstract": "We present the Large Language Model from Power Law Decoder Representations\n(PLDR-LLM), a language model that leverages non-linear and linear\ntransformations through Power Law Graph Attention mechanism to generate\nwell-defined deductive and inductive outputs. We pretrain the PLDR-LLMs of\nvarying layer sizes with a small batch size of 32 and $\\sim$8B tokens from the\nRefinedWeb dataset, and show that they achieve competitive performance in\nzero-shot and few-shot settings compared to scaled dot-product LLMs of similar\nmodel size reported in the literature. We show that deductive outputs of\nPLDR-LLMs can be used to compare model characteristics or improve the\nperformance by introducing the Directed Acyclic Graph (DAG) loss as a metric\nand regularizer. Our results indicate that the initial maximum learning rate\nand warm-up steps have a lasting impact on deductive outputs throughout the\npretraining. We provide a detailed description of PLDR-LLM architecture, its\nimplementation and the pretraining procedure.", "published": "2024-10-22 05:16:19", "link": "http://arxiv.org/abs/2410.16703v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Correct after Answer: Enhancing Multi-Span Question Answering with\n  Post-Processing Method", "abstract": "Multi-Span Question Answering (MSQA) requires models to extract one or\nmultiple answer spans from a given context to answer a question. Prior work\nmainly focuses on designing specific methods or applying heuristic strategies\nto encourage models to predict more correct predictions. However, these models\nare trained on gold answers and fail to consider the incorrect predictions.\nThrough a statistical analysis, we observe that models with stronger abilities\ndo not predict less incorrect predictions compared with other models. In this\nwork, we propose Answering-Classifying-Correcting (ACC) framework, which\nemploys a post-processing strategy to handle incorrect predictions.\nSpecifically, the ACC framework first introduces a classifier to classify the\npredictions into three types and exclude \"wrong predictions\", then introduces a\ncorrector to modify \"partially correct predictions\". Experiments on several\nMSQA datasets show that ACC framework significantly improves the Exact Match\n(EM) scores, and further analysis demostrates that ACC framework efficiently\nreduces the number of incorrect predictions, improving the quality of\npredictions.", "published": "2024-10-22 08:04:32", "link": "http://arxiv.org/abs/2410.16788v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Controlled Low-Rank Adaptation with Subspace Regularization for\n  Continued Training on Large Language Models", "abstract": "Large language models (LLMs) exhibit remarkable capabilities in natural\nlanguage processing but face catastrophic forgetting when learning new tasks,\nwhere adaptation to a new domain leads to a substantial decline in performance\non previous tasks. In this paper, we propose Controlled LoRA (CLoRA), a\nsub-space regularization method on LoRA structure. Aiming to reduce the scale\nof output change while introduce minimal constraint on model capacity, CLoRA\nimposes constraint on the direction of updating matrix's null space.\nExperimental results on one-stage LLM finetuning tasks and continual learning\nsettings highlight the superority of CLoRA as a effective parameter efficient\nfinetuning method with catastrophic forgetting mitigating.Further investigation\nfor model parameters indicates that CLoRA effectively balances the trade-off\nbetween model capacity and degree of forgetting.", "published": "2024-10-22 08:27:23", "link": "http://arxiv.org/abs/2410.16801v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Context-aware Inductive Knowledge Graph Completion with Latent Type\n  Constraints and Subgraph Reasoning", "abstract": "Inductive knowledge graph completion (KGC) aims to predict missing triples\nwith unseen entities. Recent works focus on modeling reasoning paths between\nthe head and tail entity as direct supporting evidence. However, these methods\ndepend heavily on the existence and quality of reasoning paths, which limits\ntheir general applicability in different scenarios. In addition, we observe\nthat latent type constraints and neighboring facts inherent in KGs are also\nvital in inferring missing triples. To effectively utilize all useful\ninformation in KGs, we introduce CATS, a novel context-aware inductive KGC\nsolution. With sufficient guidance from proper prompts and supervised\nfine-tuning, CATS activates the strong semantic understanding and reasoning\ncapabilities of large language models to assess the existence of query triples,\nwhich consist of two modules. First, the type-aware reasoning module evaluates\nwhether the candidate entity matches the latent entity type as required by the\nquery relation. Then, the subgraph reasoning module selects relevant reasoning\npaths and neighboring facts, and evaluates their correlation to the query\ntriple. Experiment results on three widely used datasets demonstrate that CATS\nsignificantly outperforms state-of-the-art methods in 16 out of 18\ntransductive, inductive, and few-shot settings with an average absolute MRR\nimprovement of 7.2%.", "published": "2024-10-22 08:28:05", "link": "http://arxiv.org/abs/2410.16803v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Assessment of Transformer-Based Encoder-Decoder Model for Human-Like\n  Summarization", "abstract": "In recent times, extracting valuable information from large text is making\nsignificant progress. Especially in the current era of social media, people\nexpect quick bites of information. Automatic text summarization seeks to tackle\nthis by slimming large texts down into more manageable summaries. This\nimportant research area can aid in decision-making by digging out salient\ncontent from large text. With the progress in deep learning models, significant\nwork in language models has emerged. The encoder-decoder framework in deep\nlearning has become the central approach for automatic text summarization. This\nwork leverages transformer-based BART model for human-like summarization which\nis an open-ended problem with many challenges. On training and fine-tuning the\nencoder-decoder model, it is tested with diverse sample articles and the\nquality of summaries of diverse samples is assessed based on human evaluation\nparameters. Further, the finetuned model performance is compared with the\nbaseline pretrained model based on evaluation metrics like ROUGE score and\nBERTScore. Additionally, domain adaptation of the model is required for\nimproved performance of abstractive summarization of dialogues between\ninterlocutors. On investigating, the above popular evaluation metrics are found\nto be insensitive to factual errors. Further investigation of the summaries\ngenerated by finetuned model is done using the contemporary evaluation metrics\nof factual consistency like WeCheck and SummaC. Empirical results on BBC News\narticles highlight that the gold standard summaries written by humans are more\nfactually consistent by 17% than the abstractive summaries generated by\nfinetuned model.", "published": "2024-10-22 09:25:04", "link": "http://arxiv.org/abs/2410.16842v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Tracing the Development of the Virtual Particle Concept Using Semantic\n  Change Detection", "abstract": "Virtual particles are peculiar objects. They figure prominently in much of\ntheoretical and experimental research in elementary particle physics. But\nexactly what they are is far from obvious. In particular, to what extent they\nshould be considered \"real\" remains a matter of controversy in philosophy of\nscience. Also their origin and development has only recently come into focus of\nscholarship in the history of science. In this study, we propose using the\nintriguing case of virtual particles to discuss the efficacy of Semantic Change\nDetection (SCD) based on contextualized word embeddings from a domain-adapted\nBERT model in studying specific scientific concepts. We find that the SCD\nmetrics align well with qualitative research insights in the history and\nphilosophy of science, as well as with the results obtained from Dependency\nParsing to determine the frequency and connotations of the term \"virtual.\"\nStill, the metrics of SCD provide additional insights over and above the\nqualitative research and the Dependency Parsing. Among other things, the\nmetrics suggest that the concept of the virtual particle became more stable\nafter 1950 but at the same time also more polysemous.", "published": "2024-10-22 09:43:39", "link": "http://arxiv.org/abs/2410.16855v1", "categories": ["cs.CL", "physics.hist-ph"], "primary_category": "cs.CL"}
{"title": "Math Neurosurgery: Isolating Language Models' Math Reasoning Abilities\n  Using Only Forward Passes", "abstract": "Math reasoning is an active area of Large Language Model (LLM) research\nbecause it is a hallmark of artificial intelligence and has implications in\nseveral domains, including math education. However, few works have explored how\nmath reasoning is encoded within LLM parameters and if it is a skill that can\nbe isolated within models. Doing so could allow targeted intervention to\nimprove math performance without altering non-math behavior and foster\nunderstanding of how models encode math reasoning. We introduce Math\nNeurosurgery (MathNeuro), a computationally efficient method we use to isolate\nmath-specific parameters in LLMs using only forward passes. MathNeuro builds on\nexisting work by using weights and activations to calculate parameter\nimportance, but isolates math-specific parameters by filtering out those\nimportant for general language tasks. Through pruning parameters MathNeuro\nidentifies, we delete a LLM's math reasoning ability without significantly\nimpacting its general language ability. Scaling the identified parameters by a\nsmall constant improves a pretrained or instruction-tuned LLM's performance by\n4-17% on GSM8K and 5-35% on MATH while leaving non-math behavior unaltered.\nMathNeuro is also data efficient: most of its effectiveness holds when\nidentifying math-specific parameters using a single sample. MathNeuro\nhighlights the potential for future work to intervene on math-specific\nparameters.", "published": "2024-10-22 12:00:58", "link": "http://arxiv.org/abs/2410.16930v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing Answer Attribution for Faithful Text Generation with Large\n  Language Models", "abstract": "The increasing popularity of Large Language Models (LLMs) in recent years has\nchanged the way users interact with and pose questions to AI-based\nconversational systems. An essential aspect for increasing the trustworthiness\nof generated LLM answers is the ability to trace the individual claims from\nresponses back to relevant sources that support them, the process known as\nanswer attribution. While recent work has started exploring the task of answer\nattribution in LLMs, some challenges still remain. In this work, we first\nperform a case study analyzing the effectiveness of existing answer attribution\nmethods, with a focus on subtasks of answer segmentation and evidence\nretrieval. Based on the observed shortcomings, we propose new methods for\nproducing more independent and contextualized claims for better retrieval and\nattribution. The new methods are evaluated and shown to improve the performance\nof answer attribution components. We end with a discussion and outline of\nfuture directions for the task.", "published": "2024-10-22 15:37:46", "link": "http://arxiv.org/abs/2410.17112v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "PAPILLON: Privacy Preservation from Internet-based and Local Language\n  Model Ensembles", "abstract": "Users can divulge sensitive information to proprietary LLM providers, raising\nsignificant privacy concerns. While open-source models, hosted locally on the\nuser's machine, alleviate some concerns, models that users can host locally are\noften less capable than proprietary frontier models. Toward preserving user\nprivacy while retaining the best quality, we propose Privacy-Conscious\nDelegation, a novel task for chaining API-based and local models. We utilize\nrecent public collections of user-LLM interactions to construct a natural\nbenchmark called PUPA, which contains personally identifiable information\n(PII). To study potential approaches, we devise PAPILLON, a multi-stage LLM\npipeline that uses prompt optimization to address a simpler version of our\ntask. Our best pipeline maintains high response quality for 85.5% of user\nqueries while restricting privacy leakage to only 7.5%. We still leave a large\nmargin to the generation quality of proprietary LLMs for future work. Our data\nand code is available at https://github.com/siyan-sylvia-li/PAPILLON.", "published": "2024-10-22 16:00:26", "link": "http://arxiv.org/abs/2410.17127v3", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Improving Pinterest Search Relevance Using Large Language Models", "abstract": "To improve relevance scoring on Pinterest Search, we integrate Large Language\nModels (LLMs) into our search relevance model, leveraging carefully designed\ntext representations to predict the relevance of Pins effectively. Our approach\nuses search queries alongside content representations that include captions\nextracted from a generative visual language model. These are further enriched\nwith link-based text data, historically high-quality engaged queries,\nuser-curated boards, Pin titles and Pin descriptions, creating robust models\nfor predicting search relevance. We use a semi-supervised learning approach to\nefficiently scale up the amount of training data, expanding beyond the\nexpensive human labeled data available. By utilizing multilingual LLMs, our\nsystem extends training data to include unseen languages and domains, despite\ninitial data and annotator expertise being confined to English. Furthermore, we\ndistill from the LLM-based model into real-time servable model architectures\nand features. We provide comprehensive offline experimental validation for our\nproposed techniques and demonstrate the gains achieved through the final\ndeployed system at scale.", "published": "2024-10-22 16:29:33", "link": "http://arxiv.org/abs/2410.17152v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Non-myopic Generation of Language Models for Reasoning and Planning", "abstract": "Large Language Models have demonstrated remarkable abilities in reasoning and\nplanning by breaking down complex problems into sequential steps. Despite their\nsuccess in various domains like mathematical problem-solving and coding, LLMs\nface challenges in ensuring reliable and optimal planning due to their inherent\nmyopic nature of autoregressive decoding. This paper revisits LLM reasoning\nfrom an optimal-control perspective, proposing a novel method,\nPredictive-Decoding, that leverages Model Predictive Control to enhance\nplanning accuracy. By re-weighting LLM distributions based on foresight\ntrajectories, Predictive-Decoding aims to mitigate early errors and promote\nnon-myopic planning. Our experiments show significant improvements in a wide\nrange of tasks for math, coding, and agents. Furthermore, Predictive-Decoding\ndemonstrates computational efficiency, outperforming search baselines with\nreduced computational resources. This study provides insights into optimizing\nLLM planning capabilities.", "published": "2024-10-22 17:13:38", "link": "http://arxiv.org/abs/2410.17195v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Creativity in AI: Progresses and Challenges", "abstract": "Creativity is the ability to produce novel, useful, and surprising ideas, and\nhas been widely studied as a crucial aspect of human cognition. Machine\ncreativity on the other hand has been a long-standing challenge. With the rise\nof advanced generative AI, there has been renewed interest and debate regarding\nAI's creative capabilities. Therefore, it is imperative to revisit the state of\ncreativity in AI and identify key progresses and remaining challenges. In this\nwork, we survey leading works studying the creative capabilities of AI systems,\nfocusing on creative problem-solving, linguistic, artistic, and scientific\ncreativity. Our review suggests that while the latest AI models are largely\ncapable of producing linguistically and artistically creative outputs such as\npoems, images, and musical pieces, they struggle with tasks that require\ncreative problem-solving, abstract thinking and compositionality and their\ngenerations suffer from a lack of diversity, originality, long-range\nincoherence and hallucinations. We also discuss key questions concerning\ncopyright and authorship issues with generative models. Furthermore, we\nhighlight the need for a comprehensive evaluation of creativity that is\nprocess-driven and considers several dimensions of creativity. Finally, we\npropose future research directions to improve the creativity of AI outputs,\ndrawing inspiration from cognitive science and psychology.", "published": "2024-10-22 17:43:39", "link": "http://arxiv.org/abs/2410.17218v4", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Fine-Tuning Large Language Models to Appropriately Abstain with Semantic\n  Entropy", "abstract": "Large Language Models (LLMs) are known to hallucinate, whereby they generate\nplausible but inaccurate text. This phenomenon poses significant risks in\ncritical applications, such as medicine or law, necessitating robust\nhallucination mitigation strategies. While recent works have proposed\nfine-tuning methods to teach LLMs to abstain from answering questions beyond\ntheir knowledge or capabilities, these methods rely on the existence of\nground-truth labels or are limited to short-form responses. To address these\nlimitations, we propose fine-tuning using semantic entropy, an uncertainty\nmeasure derived from introspection into the model which does not require\nexternal labels. We demonstrate that our approach matches or outperforms models\nfine-tuned using prior work and achieves strong performance for both short and\nlong-form generations on a range of datasets.", "published": "2024-10-22 17:54:03", "link": "http://arxiv.org/abs/2410.17234v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Reliable Evaluation of Behavior Steering Interventions in LLMs", "abstract": "Representation engineering methods have recently shown promise for enabling\nefficient steering of model behavior. However, evaluation pipelines for these\nmethods have primarily relied on subjective demonstrations, instead of\nquantitative, objective metrics. We aim to take a step towards addressing this\nissue by advocating for four properties missing from current evaluations: (i)\ncontexts sufficiently similar to downstream tasks should be used for assessing\nintervention quality; (ii) model likelihoods should be accounted for; (iii)\nevaluations should allow for standardized comparisons across different target\nbehaviors; and (iv) baseline comparisons should be offered. We introduce an\nevaluation pipeline grounded in these criteria, offering both a quantitative\nand visual analysis of how effectively a given method works. We use this\npipeline to evaluate two representation engineering methods on how effectively\nthey can steer behaviors such as truthfulness and corrigibility, finding that\nsome interventions are less effective than previously reported.", "published": "2024-10-22 17:59:39", "link": "http://arxiv.org/abs/2410.17245v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "PyramidDrop: Accelerating Your Large Vision-Language Models via Pyramid\n  Visual Redundancy Reduction", "abstract": "In large vision-language models (LVLMs), images serve as inputs that carry a\nwealth of information. As the idiom \"A picture is worth a thousand words\"\nimplies, representing a single image in current LVLMs can require hundreds or\neven thousands of tokens. This results in significant computational costs,\nwhich grow quadratically as input image resolution increases, thereby severely\nimpacting the efficiency of both training and inference. Previous approaches\nhave attempted to reduce the number of image tokens either before or within the\nearly layers of LVLMs. However, these strategies inevitably result in the loss\nof crucial image information, ultimately diminishing model performance. To\naddress this challenge, we conduct an empirical study revealing that all visual\ntokens are necessary for LVLMs in the shallow layers, and token redundancy\nprogressively increases in the deeper layers of the model. To this end, we\npropose PyramidDrop, a visual redundancy reduction strategy for LVLMs to boost\ntheir efficiency in both training and inference with neglectable performance\nloss. Specifically, we partition the LVLM into several stages and drop part of\nthe image tokens at the end of each stage with a pre-defined ratio, creating\npyramid-like visual tokens across model layers. The dropping is based on a\nlightweight similarity calculation with a negligible time overhead. Extensive\nexperiments demonstrate that PyramidDrop can achieve a 40% training time and\n55% inference FLOPs acceleration of LLaVA-NeXT with comparable performance.\nBesides, the PyramidDrop could also serve as a plug-and-play strategy for\ninference acceleration without training, with better performance and lower\ninference cost than counterparts. Code is available at\nhttps://github.com/Cooperx521/PyramidDrop.", "published": "2024-10-22 17:59:53", "link": "http://arxiv.org/abs/2410.17247v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Altogether: Image Captioning via Re-aligning Alt-text", "abstract": "This paper focuses on creating synthetic data to improve the quality of image\ncaptions. Existing works typically have two shortcomings. First, they caption\nimages from scratch, ignoring existing alt-text metadata, and second, lack\ntransparency if the captioners' training data (e.g. GPT) is unknown. In this\npaper, we study a principled approach Altogether based on the key idea to edit\nand re-align existing alt-texts associated with the images. To generate\ntraining data, we perform human annotation where annotators start with the\nexisting alt-text and re-align it to the image content in multiple rounds,\nconsequently constructing captions with rich visual concepts. This differs from\nprior work that carries out human annotation as a one-time description task\nsolely based on images and annotator knowledge. We train a captioner on this\ndata that generalizes the process of re-aligning alt-texts at scale. Our\nresults show our Altogether approach leads to richer image captions that also\nimprove text-to-image generation and zero-shot image classification tasks.", "published": "2024-10-22 17:59:57", "link": "http://arxiv.org/abs/2410.17251v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Do Vision-Language Models Represent Space and How? Evaluating Spatial\n  Frame of Reference Under Ambiguities", "abstract": "Spatial expressions in situated communication can be ambiguous, as their\nmeanings vary depending on the frames of reference (FoR) adopted by speakers\nand listeners. While spatial language understanding and reasoning by\nvision-language models (VLMs) have gained increasing attention, potential\nambiguities in these models are still under-explored. To address this issue, we\npresent the COnsistent Multilingual Frame Of Reference Test (COMFORT), an\nevaluation protocol to systematically assess the spatial reasoning capabilities\nof VLMs. We evaluate nine state-of-the-art VLMs using COMFORT. Despite showing\nsome alignment with English conventions in resolving ambiguities, our\nexperiments reveal significant shortcomings of VLMs: notably, the models (1)\nexhibit poor robustness and consistency, (2) lack the flexibility to\naccommodate multiple FoRs, and (3) fail to adhere to language-specific or\nculture-specific conventions in cross-lingual tests, as English tends to\ndominate other languages. With a growing effort to align vision-language models\nwith human cognitive intuitions, we call for more attention to the ambiguous\nnature and cross-cultural diversity of spatial reasoning.", "published": "2024-10-22 19:39:15", "link": "http://arxiv.org/abs/2410.17385v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "AdvWeb: Controllable Black-box Attacks on VLM-powered Web Agents", "abstract": "Vision Language Models (VLMs) have revolutionized the creation of generalist\nweb agents, empowering them to autonomously complete diverse tasks on\nreal-world websites, thereby boosting human efficiency and productivity.\nHowever, despite their remarkable capabilities, the safety and security of\nthese agents against malicious attacks remain critically underexplored, raising\nsignificant concerns about their safe deployment. To uncover and exploit such\nvulnerabilities in web agents, we provide AdvWeb, a novel black-box attack\nframework designed against web agents. AdvWeb trains an adversarial prompter\nmodel that generates and injects adversarial prompts into web pages, misleading\nweb agents into executing targeted adversarial actions such as inappropriate\nstock purchases or incorrect bank transactions, actions that could lead to\nsevere real-world consequences. With only black-box access to the web agent, we\ntrain and optimize the adversarial prompter model using DPO, leveraging both\nsuccessful and failed attack strings against the target agent. Unlike prior\napproaches, our adversarial string injection maintains stealth and control: (1)\nthe appearance of the website remains unchanged before and after the attack,\nmaking it nearly impossible for users to detect tampering, and (2) attackers\ncan modify specific substrings within the generated adversarial string to\nseamlessly change the attack objective (e.g., purchasing stocks from a\ndifferent company), enhancing attack flexibility and efficiency. We conduct\nextensive evaluations, demonstrating that AdvWeb achieves high success rates in\nattacking SOTA GPT-4V-based VLM agent across various web tasks. Our findings\nexpose critical vulnerabilities in current LLM/VLM-based agents, emphasizing\nthe urgent need for developing more reliable web agents and effective defenses.\nOur code and data are available at https://ai-secure.github.io/AdvWeb/ .", "published": "2024-10-22 20:18:26", "link": "http://arxiv.org/abs/2410.17401v2", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Evaluating AI-Generated Essays with GRE Analytical Writing Assessment", "abstract": "The recent revolutionary advance in generative AI enables the generation of\nrealistic and coherent texts by large language models (LLMs). Despite many\nexisting evaluation metrics on the quality of the generated texts, there is\nstill a lack of rigorous assessment of how well LLMs perform in complex and\ndemanding writing assessments. This study examines essays generated by ten\nleading LLMs for the analytical writing assessment of the Graduate Record Exam\n(GRE). We assessed these essays using both human raters and the e-rater\nautomated scoring engine as used in the GRE scoring pipeline. Notably, the\ntop-performing Gemini and GPT-4o received an average score of 4.78 and 4.67,\nrespectively, falling between \"generally thoughtful, well-developed analysis of\nthe issue and conveys meaning clearly\" and \"presents a competent analysis of\nthe issue and conveys meaning with acceptable clarity\" according to the GRE\nscoring guideline. We also evaluated the detection accuracy of these essays,\nwith detectors trained on essays generated by the same and different LLMs.", "published": "2024-10-22 21:30:58", "link": "http://arxiv.org/abs/2410.17439v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "In Context Learning and Reasoning for Symbolic Regression with Large\n  Language Models", "abstract": "Large Language Models (LLMs) are transformer-based machine learning models\nthat have shown remarkable performance in tasks for which they were not\nexplicitly trained. Here, we explore the potential of LLMs to perform symbolic\nregression -- a machine-learning method for finding simple and accurate\nequations from datasets. We prompt GPT-4 to suggest expressions from data,\nwhich are then optimized and evaluated using external Python tools. These\nresults are fed back to GPT-4, which proposes improved expressions while\noptimizing for complexity and loss. Using chain-of-thought prompting, we\ninstruct GPT-4 to analyze the data, prior expressions, and the scientific\ncontext (expressed in natural language) for each problem before generating new\nexpressions. We evaluated the workflow in rediscovery of five well-known\nscientific equations from experimental data, and on an additional dataset\nwithout a known equation. GPT-4 successfully rediscovered all five equations,\nand in general, performed better when prompted to use a scratchpad and consider\nscientific context. We demonstrate how strategic prompting improves the model's\nperformance and how the natural language interface simplifies integrating\ntheory with data. We also observe how theory can sometimes offset noisy data\nand, in other cases, data can make up for poor context. Although this approach\ndoes not outperform established SR programs where target equations are more\ncomplex, LLMs can nonetheless iterate toward improved solutions while following\ninstructions and incorporating scientific context in natural language.", "published": "2024-10-22 21:50:52", "link": "http://arxiv.org/abs/2410.17448v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Intera\u00e7\u00e3o entre rob\u00f4s humanoides: desenvolvendo a\n  colabora\u00e7\u00e3o e comunica\u00e7\u00e3o aut\u00f4noma", "abstract": "This study investigates the interaction between humanoid robots NAO and\nPepper, emphasizing their potential applications in educational settings. NAO,\nwidely used in education, and Pepper, designed for social interactions, of er\nnew opportunities for autonomous communication and collaboration. Through a\nseries of programmed interactions, the robots demonstrated their ability to\ncommunicate and coordinate actions autonomously, highlighting their potential\nas tools for enhancing learning environments. The research also explores the\nintegration of emerging technologies, such as artificial intelligence, into\nthese systems, allowing robots to learn from each other and adapt their\nbehavior. The findings suggest that NAO and Pepper can significantly contribute\nto both technical learning and the development of social and emotional skills\nin students, of ering innovative pedagogical approaches through the use of\nhumanoid robotics.", "published": "2024-10-22 21:55:54", "link": "http://arxiv.org/abs/2410.17450v2", "categories": ["cs.RO", "cs.CL"], "primary_category": "cs.RO"}
{"title": "Decoding Time Series with LLMs: A Multi-Agent Framework for Cross-Domain\n  Annotation", "abstract": "Time series data is ubiquitous across various domains, including\nmanufacturing, finance, and healthcare. High-quality annotations are essential\nfor effectively understanding time series and facilitating downstream tasks;\nhowever, obtaining such annotations is challenging, particularly in\nmission-critical domains. In this paper, we propose TESSA, a multi-agent system\ndesigned to automatically generate both general and domain-specific annotations\nfor time series data. TESSA introduces two agents: a general annotation agent\nand a domain-specific annotation agent. The general agent captures common\npatterns and knowledge across multiple source domains, leveraging both\ntime-series-wise and text-wise features to generate general annotations.\nMeanwhile, the domain-specific agent utilizes limited annotations from the\ntarget domain to learn domain-specific terminology and generate targeted\nannotations. Extensive experiments on multiple synthetic and real-world\ndatasets demonstrate that TESSA effectively generates high-quality annotations,\noutperforming existing methods.", "published": "2024-10-22 22:43:14", "link": "http://arxiv.org/abs/2410.17462v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Analyzing Nobel Prize Literature with Large Language Models", "abstract": "This study examines the capabilities of advanced Large Language Models\n(LLMs), particularly the o1 model, in the context of literary analysis. The\noutputs of these models are compared directly to those produced by\ngraduate-level human participants. By focusing on two Nobel Prize-winning short\nstories, 'Nine Chapters' by Han Kang, the 2024 laureate, and 'Friendship' by\nJon Fosse, the 2023 laureate, the research explores the extent to which AI can\nengage with complex literary elements such as thematic analysis,\nintertextuality, cultural and historical contexts, linguistic and structural\ninnovations, and character development. Given the Nobel Prize's prestige and\nits emphasis on cultural, historical, and linguistic richness, applying LLMs to\nthese works provides a deeper understanding of both human and AI approaches to\ninterpretation. The study uses qualitative and quantitative evaluations of\ncoherence, creativity, and fidelity to the text, revealing the strengths and\nlimitations of AI in tasks typically reserved for human expertise. While LLMs\ndemonstrate strong analytical capabilities, particularly in structured tasks,\nthey often fall short in emotional nuance and coherence, areas where human\ninterpretation excels. This research underscores the potential for human-AI\ncollaboration in the humanities, opening new opportunities in literary studies\nand beyond.", "published": "2024-10-22 13:03:28", "link": "http://arxiv.org/abs/2410.18142v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Benchmarking Large Language Models for Image Classification of Marine\n  Mammals", "abstract": "As Artificial Intelligence (AI) has developed rapidly over the past few\ndecades, the new generation of AI, Large Language Models (LLMs) trained on\nmassive datasets, has achieved ground-breaking performance in many\napplications. Further progress has been made in multimodal LLMs, with many\ndatasets created to evaluate LLMs with vision abilities. However, none of those\ndatasets focuses solely on marine mammals, which are indispensable for\necological equilibrium. In this work, we build a benchmark dataset with 1,423\nimages of 65 kinds of marine mammals, where each animal is uniquely classified\ninto different levels of class, ranging from species-level to medium-level to\ngroup-level. Moreover, we evaluate several approaches for classifying these\nmarine mammals: (1) machine learning (ML) algorithms using embeddings provided\nby neural networks, (2) influential pre-trained neural networks, (3) zero-shot\nmodels: CLIP and LLMs, and (4) a novel LLM-based multi-agent system (MAS). The\nresults demonstrate the strengths of traditional models and LLMs in different\naspects, and the MAS can further improve the classification performance. The\ndataset is available on GitHub:\nhttps://github.com/yeyimilk/LLM-Vision-Marine-Animals.git.", "published": "2024-10-22 01:49:49", "link": "http://arxiv.org/abs/2410.19848v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "ViMGuard: A Novel Multi-Modal System for Video Misinformation Guarding", "abstract": "The rise of social media and short-form video (SFV) has facilitated a\nbreeding ground for misinformation. With the emergence of large language\nmodels, significant research has gone into curbing this misinformation problem\nwith automatic false claim detection for text. Unfortunately, the automatic\ndetection of misinformation in SFV is a more complex problem that remains\nlargely unstudied. While text samples are monomodal (only containing words),\nSFVs comprise three different modalities: words, visuals, and non-linguistic\naudio. In this work, we introduce Video Masked Autoencoders for Misinformation\nGuarding (ViMGuard), the first deep-learning architecture capable of\nfact-checking an SFV through analysis of all three of its constituent\nmodalities. ViMGuard leverages a dual-component system. First, Video and Audio\nMasked Autoencoders analyze the visual and non-linguistic audio elements of a\nvideo to discern its intention; specifically whether it intends to make an\ninformative claim. If it is deemed that the SFV has informative intent, it is\npassed through our second component: a Retrieval Augmented Generation system\nthat validates the factual accuracy of spoken words. In evaluation, ViMGuard\noutperformed three cutting-edge fact-checkers, thus setting a new standard for\nSFV fact-checking and marking a significant stride toward trustworthy news on\nsocial platforms. To promote further testing and iteration, VimGuard was\ndeployed into a Chrome extension and all code was open-sourced on GitHub.", "published": "2024-10-22 00:30:08", "link": "http://arxiv.org/abs/2410.16592v1", "categories": ["cs.LG", "cs.CL", "cs.CY"], "primary_category": "cs.LG"}
{"title": "LLMScan: Causal Scan for LLM Misbehavior Detection", "abstract": "Despite the success of Large Language Models (LLMs) across various fields,\ntheir potential to generate untruthful, biased and harmful responses poses\nsignificant risks, particularly in critical applications. This highlights the\nurgent need for systematic methods to detect and prevent such misbehavior.\nWhile existing approaches target specific issues such as harmful responses,\nthis work introduces LLMScan, an innovative LLM monitoring technique based on\ncausality analysis, offering a comprehensive solution. LLMScan systematically\nmonitors the inner workings of an LLM through the lens of causal inference,\noperating on the premise that the LLM's `brain' behaves differently when\nmisbehaving. By analyzing the causal contributions of the LLM's input tokens\nand transformer layers, LLMScan effectively detects misbehavior. Extensive\nexperiments across various tasks and models reveal clear distinctions in the\ncausal distributions between normal behavior and misbehavior, enabling the\ndevelopment of accurate, lightweight detectors for a variety of misbehavior\ndetection tasks.", "published": "2024-10-22 02:27:57", "link": "http://arxiv.org/abs/2410.16638v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "RKadiyala at SemEval-2024 Task 8: Black-Box Word-Level Text Boundary\n  Detection in Partially Machine Generated Texts", "abstract": "With increasing usage of generative models for text generation and widespread\nuse of machine generated texts in various domains, being able to distinguish\nbetween human written and machine generated texts is a significant challenge.\nWhile existing models and proprietary systems focus on identifying whether\ngiven text is entirely human written or entirely machine generated, only a few\nsystems provide insights at sentence or paragraph level at likelihood of being\nmachine generated at a non reliable accuracy level, working well only for a set\nof domains and generators. This paper introduces few reliable approaches for\nthe novel task of identifying which part of a given text is machine generated\nat a word level while comparing results from different approaches and methods.\nWe present a comparison with proprietary systems , performance of our model on\nunseen domains' and generators' texts. The findings reveal significant\nimprovements in detection accuracy along with comparison on other aspects of\ndetection capabilities. Finally we discuss potential avenues for improvement\nand implications of our work. The proposed model is also well suited for\ndetecting which parts of a text are machine generated in outputs of Instruct\nvariants of many LLMs.", "published": "2024-10-22 03:21:59", "link": "http://arxiv.org/abs/2410.16659v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Influential Language Data Selection via Gradient Trajectory Pursuit", "abstract": "Curating a desirable dataset for training has been the core of building\nhighly capable large language models (Touvron et al., 2023; Achiam et al.,\n2023; Team et al.,2024). Gradient influence scores (Pruthi et al., 2020; Xia et\nal., 2024) are shown to be correlated with model performance and are commonly\nused as the criterion for data selection. However, existing methods are built\nupon either individual sample rankings or inefficient matching process, leading\nto suboptimal performance or scaling up issues.In this paper, we propose\nGradient Trajectory Pursuit (GTP), an algorithm that performs pursuit of\ngradient trajectories via jointly selecting data points under an L0-norm\nregularized objective. The proposed algorithm highlights: (1) joint selection\ninstead of independent top-k selection, which automatically de-duplicates\nsamples; (2) higher efficiency with compressive sampling processes, which can\nbe further sped up using a distributed framework. In the experiments, we\ndemonstrate the algorithm in both in-domain and target-domain selection\nbenchmarks and show that it outperforms top-k selection and competitive\nalgorithms consistently, for example, our algorithm chooses as low as 0.5% data\nto achieve full performance on the targeted instruction tuning tasks", "published": "2024-10-22 05:32:40", "link": "http://arxiv.org/abs/2410.16710v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "DENOASR: Debiasing ASRs through Selective Denoising", "abstract": "Automatic Speech Recognition (ASR) systems have been examined and shown to\nexhibit biases toward particular groups of individuals, influenced by factors\nsuch as demographic traits, accents, and speech styles. Noise can\ndisproportionately impact speakers with certain accents, dialects, or speaking\nstyles, leading to biased error rates. In this work, we introduce a novel\nframework DENOASR, which is a selective denoising technique to reduce the\ndisparity in the word error rates between the two gender groups, male and\nfemale. We find that a combination of two popular speech denoising techniques,\nviz. DEMUCS and LE, can be effectively used to mitigate ASR disparity without\ncompromising their overall performance. Experiments using two state-of-the-art\nopen-source ASRs - OpenAI WHISPER and NVIDIA NEMO - on multiple benchmark\ndatasets, including TIE, VOX-POPULI, TEDLIUM, and FLEURS, show that there is a\npromising reduction in the average word error rate gap across the two gender\ngroups. For a given dataset, the denoising is selectively applied on speech\nsamples having speech intelligibility below a certain threshold, estimated\nusing a small validation sample, thus ameliorating the need for large-scale\nhuman-written ground-truth transcripts. Our findings suggest that selective\ndenoising can be an elegant approach to mitigate biases in present-day ASR\nsystems.", "published": "2024-10-22 05:39:24", "link": "http://arxiv.org/abs/2410.16712v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Enhancing Low-Resource ASR through Versatile TTS: Bridging the Data Gap", "abstract": "While automatic speech recognition (ASR) systems have achieved remarkable\nperformance with large-scale datasets, their efficacy remains inadequate in\nlow-resource settings, encompassing dialects, accents, minority languages, and\nlong-tail hotwords, domains with significant practical relevance. With the\nadvent of versatile and powerful text-to-speech (TTS) models, capable of\ngenerating speech with human-level naturalness, expressiveness, and diverse\nspeaker profiles, leveraging TTS for ASR data augmentation provides a\ncost-effective and practical approach to enhancing ASR performance.\nComprehensive experiments on an unprecedentedly rich variety of low-resource\ndatasets demonstrate consistent and substantial performance improvements,\nproving that the proposed method of enhancing low-resource ASR through a\nversatile TTS model is highly effective and has broad application prospects.\nFurthermore, we delve deeper into key characteristics of synthesized speech\ndata that contribute to ASR improvement, examining factors such as text\ndiversity, speaker diversity, and the volume of synthesized data, with text\ndiversity being studied for the first time in this work. We hope our findings\nprovide helpful guidance and reference for the practical application of\nTTS-based data augmentation and push the advancement of low-resource ASR one\nstep further.", "published": "2024-10-22 06:25:16", "link": "http://arxiv.org/abs/2410.16726v1", "categories": ["eess.AS", "cs.AI", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Beyond Retrieval: Generating Narratives in Conversational Recommender\n  Systems", "abstract": "The recent advances in Large Language Model's generation and reasoning\ncapabilities present an opportunity to develop truly conversational\nrecommendation systems. However, effectively integrating recommender system\nknowledge into LLMs for natural language generation which is tailored towards\nrecommendation tasks remains a challenge. This paper addresses this challenge\nby making two key contributions.\n  First, we introduce a new dataset (REGEN) for natural language generation\ntasks in conversational recommendations. REGEN (Reviews Enhanced with\nGEnerative Narratives) extends the Amazon Product Reviews dataset with rich\nuser narratives, including personalized explanations of product preferences,\nproduct endorsements for recommended items, and summaries of user purchase\nhistory. REGEN is made publicly available to facilitate further research.\nFurthermore, we establish benchmarks using well-known generative metrics, and\nperform an automated evaluation of the new dataset using a rater LLM. Second,\nthe paper introduces a fusion architecture (CF model with an LLM) which serves\nas a baseline for REGEN. And to the best of our knowledge, represents the first\nattempt to analyze the capabilities of LLMs in understanding recommender\nsignals and generating rich narratives. We demonstrate that LLMs can\neffectively learn from simple fusion architectures utilizing interaction-based\nCF embeddings, and this can be further enhanced using the metadata and\npersonalization data associated with items. Our experiments show that combining\nCF and content embeddings leads to improvements of 4-12% in key language\nmetrics compared to using either type of embedding individually. We also\nprovide an analysis to interpret how CF and content embeddings contribute to\nthis new generative task.", "published": "2024-10-22 07:53:41", "link": "http://arxiv.org/abs/2410.16780v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "EnvBridge: Bridging Diverse Environments with Cross-Environment\n  Knowledge Transfer for Embodied AI", "abstract": "In recent years, Large Language Models (LLMs) have demonstrated high\nreasoning capabilities, drawing attention for their applications as agents in\nvarious decision-making processes. One notably promising application of LLM\nagents is robotic manipulation. Recent research has shown that LLMs can\ngenerate text planning or control code for robots, providing substantial\nflexibility and interaction capabilities. However, these methods still face\nchallenges in terms of flexibility and applicability across different\nenvironments, limiting their ability to adapt autonomously. Current approaches\ntypically fall into two categories: those relying on environment-specific\npolicy training, which restricts their transferability, and those generating\ncode actions based on fixed prompts, which leads to diminished performance when\nconfronted with new environments. These limitations significantly constrain the\ngeneralizability of agents in robotic manipulation. To address these\nlimitations, we propose a novel method called EnvBridge. This approach involves\nthe retention and transfer of successful robot control codes from source\nenvironments to target environments. EnvBridge enhances the agent's\nadaptability and performance across diverse settings by leveraging insights\nfrom multiple environments. Notably, our approach alleviates environmental\nconstraints, offering a more flexible and generalizable solution for robotic\nmanipulation tasks. We validated the effectiveness of our method using robotic\nmanipulation benchmarks: RLBench, MetaWorld, and CALVIN. Our experiments\ndemonstrate that LLM agents can successfully leverage diverse knowledge sources\nto solve complex tasks. Consequently, our approach significantly enhances the\nadaptability and robustness of robotic manipulation agents in planning across\ndiverse environments.", "published": "2024-10-22 11:52:22", "link": "http://arxiv.org/abs/2410.16919v1", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Learning Mathematical Rules with Large Language Models", "abstract": "In this paper, we study the ability of large language models to learn\nspecific mathematical rules such as distributivity or simplifying equations. We\npresent an empirical analysis of their ability to generalize these rules, as\nwell as to reuse them in the context of word problems. For this purpose, we\nprovide a rigorous methodology to build synthetic data incorporating such\nrules, and perform fine-tuning of large language models on such data. Our\nexperiments show that our model can learn and generalize these rules to some\nextent, as well as suitably reuse them in the context of word problems.", "published": "2024-10-22 12:51:51", "link": "http://arxiv.org/abs/2410.16973v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "UnStar: Unlearning with Self-Taught Anti-Sample Reasoning for LLMs", "abstract": "The key components of machine learning are data samples for training, model\nfor learning patterns, and loss function for optimizing accuracy. Analogously,\nunlearning can potentially be achieved through anti-data samples (or\nanti-samples), unlearning method, and reversed loss function. While prior\nresearch has explored unlearning methods and reversed loss functions, the\npotential of anti-samples remains largely untapped. In this paper, we introduce\nUnSTAR: Unlearning with Self-Taught Anti-Sample Reasoning for large language\nmodels (LLMs). Our contributions are threefold; first, we propose a novel\nconcept of anti-sample-induced unlearning; second, we generate anti-samples by\nleveraging misleading rationales, which help reverse learned associations and\naccelerate the unlearning process; and third, we enable fine-grained targeted\nunlearning, allowing for the selective removal of specific associations without\nimpacting related knowledge - something not achievable by previous works.\nResults demonstrate that anti-samples offer an efficient, targeted unlearning\nstrategy for LLMs, opening new avenues for privacy-preserving machine learning\nand model modification.", "published": "2024-10-22 14:30:03", "link": "http://arxiv.org/abs/2410.17050v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Continuous Speech Tokenizer in Text To Speech", "abstract": "The fusion of speech and language in the era of large language models has\ngarnered significant attention. Discrete speech token is often utilized in\ntext-to-speech tasks for speech compression and portability, which is\nconvenient for joint training with text and have good compression efficiency.\nHowever, we found that the discrete speech tokenizer still suffers from\ninformation loss. Therefore, we propose a simple yet effective continuous\nspeech tokenizer named Cont-SPT, and a text-to-speech model based on continuous\nspeech tokens. Our results show that the speech language model based on the\ncontinuous speech tokenizer has better continuity and higher estimated Mean\nOpinion Scores (MoS). This enhancement is attributed to better information\npreservation rate of the continuous speech tokenizer across both low and high\nfrequencies in the frequency domain. The code and resources for Cont-SPT can be\nfound in https://github.com/Yixing-Li/Continuous-Speech-Tokenizer", "published": "2024-10-22 15:02:37", "link": "http://arxiv.org/abs/2410.17081v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Science Out of Its Ivory Tower: Improving Accessibility with\n  Reinforcement Learning", "abstract": "A vast amount of scholarly work is published daily, yet much of it remains\ninaccessible to the general public due to dense jargon and complex language. To\naddress this challenge in science communication, we introduce a reinforcement\nlearning framework that fine-tunes a language model to rewrite scholarly\nabstracts into more comprehensible versions. Guided by a carefully balanced\ncombination of word- and sentence-level accessibility rewards, our language\nmodel effectively substitutes technical terms with more accessible\nalternatives, a task which models supervised fine-tuned or guided by\nconventional readability measures struggle to accomplish. Our best model\nadjusts the readability level of scholarly abstracts by approximately six U.S.\ngrade levels -- in other words, from a postgraduate to a high school level.\nThis translates to roughly a 90% relative boost over the supervised fine-tuning\nbaseline, all while maintaining factual accuracy and high-quality language. An\nin-depth analysis of our approach shows that balanced rewards lead to\nsystematic modifications in the base model, likely contributing to smoother\noptimization and superior performance. We envision this work as a step toward\nbridging the gap between scholarly research and the general public,\nparticularly younger readers and those without a college degree.", "published": "2024-10-22 15:14:54", "link": "http://arxiv.org/abs/2410.17088v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Human-LLM Hybrid Text Answer Aggregation for Crowd Annotations", "abstract": "The quality is a crucial issue for crowd annotations. Answer aggregation is\nan important type of solution. The aggregated answers estimated from multiple\ncrowd answers to the same instance are the eventually collected annotations,\nrather than the individual crowd answers themselves. Recently, the capability\nof Large Language Models (LLMs) on data annotation tasks has attracted interest\nfrom researchers. Most of the existing studies mainly focus on the average\nperformance of individual crowd workers; several recent works studied the\nscenarios of aggregation on categorical labels and LLMs used as label creators.\nHowever, the scenario of aggregation on text answers and the role of LLMs as\naggregators are not yet well-studied. In this paper, we investigate the\ncapability of LLMs as aggregators in the scenario of close-ended crowd text\nanswer aggregation. We propose a human-LLM hybrid text answer aggregation\nmethod with a Creator-Aggregator Multi-Stage (CAMS) crowdsourcing framework. We\nmake the experiments based on public crowdsourcing datasets. The results show\nthe effectiveness of our approach based on the collaboration of crowd workers\nand LLMs.", "published": "2024-10-22 15:22:58", "link": "http://arxiv.org/abs/2410.17099v1", "categories": ["cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploring RL-based LLM Training for Formal Language Tasks with\n  Programmed Rewards", "abstract": "Proximal Policy Optimization (PPO) is commonly used in Reinforcement Learning\nfrom Human Feedback to align large language models (LLMs) with downstream\ntasks. This paper investigates the feasibility of using PPO for direct\nreinforcement learning (RL) from explicitly programmed reward signals, as\nopposed to indirect learning from human feedback via an intermediary reward\nmodel. We focus on tasks expressed through formal languages, such as\nmathematics and programming, where explicit reward functions can be programmed\nto automatically assess the quality of generated outputs. We apply this\napproach to a sentiment alignment task, a simple arithmetic task, and a more\ncomplex game synthesis task. The sentiment alignment task replicates prior\nresearch and serves to validate our experimental setup. Our results show that\npure RL-based training for the two formal language tasks is challenging, with\nsuccess being limited even for the simple arithmetic task. We propose a novel\nbatch-entropy regularization term to aid exploration, although training is not\nyet entirely stable. Our findings suggest that direct RL training of LLMs may\nbe more suitable for relatively minor changes, such as alignment, than for\nlearning new tasks altogether, even if an informative reward signal can be\nexpressed programmatically.", "published": "2024-10-22 15:59:58", "link": "http://arxiv.org/abs/2410.17126v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Can General-Purpose Large Language Models Generalize to English-Thai\n  Machine Translation ?", "abstract": "Large language models (LLMs) perform well on common tasks but struggle with\ngeneralization in low-resource and low-computation settings. We examine this\nlimitation by testing various LLMs and specialized translation models on\nEnglish-Thai machine translation and code-switching datasets. Our findings\nreveal that under more strict computational constraints, such as 4-bit\nquantization, LLMs fail to translate effectively. In contrast, specialized\nmodels, with comparable or lower computational requirements, consistently\noutperform LLMs. This underscores the importance of specialized models for\nmaintaining performance under resource constraints.", "published": "2024-10-22 16:26:03", "link": "http://arxiv.org/abs/2410.17145v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Interchangeable Token Embeddings for Extendable Vocabulary and\n  Alpha-Equivalence", "abstract": "We propose a novel approach for learning interchangeable tokens in language\nmodels to obtain an extendable vocabulary that can generalize to new tokens.\nOur method addresses alpha-equivalence, the principle that renaming bound\nvariables preserves semantics. This property arises in many formal languages\nsuch as temporal logics, where all proposition symbols represent the same\nconcept but remain distinct. To handle such tokens, we develop a dual-part\nembedding approach. The first part is shared across all interchangeable tokens,\nenforcing that they represent the same core concept. The second part is\nrandomly generated for each token, enabling distinguishability. As a baseline,\nwe consider a simpler approach that uses alpha-renaming for data augmentation.\nWe also present alpha-covariance, a metric for measuring robustness against\nalpha-conversions. When evaluated in a Transformer encoder-decoder model for\nsolving linear temporal logic formulae and copying with extendable vocabulary,\nour method demonstrates promising generalization capabilities as well as a\nfavorable inductive bias for alpha-equivalence.", "published": "2024-10-22 16:34:36", "link": "http://arxiv.org/abs/2410.17161v2", "categories": ["cs.CL", "cs.LG", "cs.LO"], "primary_category": "cs.CL"}
{"title": "VoiceBench: Benchmarking LLM-Based Voice Assistants", "abstract": "Building on the success of large language models (LLMs), recent advancements\nsuch as GPT-4o have enabled real-time speech interactions through LLM-based\nvoice assistants, offering a significantly improved user experience compared to\ntraditional text-based interactions. However, the absence of benchmarks\ndesigned to evaluate these speech interaction capabilities has hindered\nprogress of LLM-based voice assistants development. Current evaluations focus\nprimarily on automatic speech recognition (ASR) or general knowledge evaluation\nwith clean speeches, neglecting the more intricate, real-world scenarios that\ninvolve diverse speaker characteristics, environmental and content factors. To\naddress this, we introduce VoiceBench, the first benchmark designed to provide\na multi-faceted evaluation of LLM-based voice assistants. VoiceBench also\nincludes both real and synthetic spoken instructions that incorporate the above\nthree key real-world variations. Extensive experiments reveal the limitations\nof current LLM-based voice assistant models and offer valuable insights for\nfuture research and development in this field.", "published": "2024-10-22 17:15:20", "link": "http://arxiv.org/abs/2410.17196v3", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Audio-to-Score Conversion Model Based on Whisper methodology", "abstract": "This thesis develops a Transformer model based on Whisper, which extracts\nmelodies and chords from music audio and records them into ABC notation. A\ncomprehensive data processing workflow is customized for ABC notation,\nincluding data cleansing, formatting, and conversion, and a mutation mechanism\nis implemented to increase the diversity and quality of training data. This\nthesis innovatively introduces the \"Orpheus' Score\", a custom notation system\nthat converts music information into tokens, designs a custom vocabulary\nlibrary, and trains a corresponding custom tokenizer. Experiments show that\ncompared to traditional algorithms, the model has significantly improved\naccuracy and performance. While providing a convenient audio-to-score tool for\nmusic enthusiasts, this work also provides new ideas and tools for research in\nmusic information processing.", "published": "2024-10-22 17:31:37", "link": "http://arxiv.org/abs/2410.17209v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Exploring Possibilities of AI-Powered Legal Assistance in Bangladesh\n  through Large Language Modeling", "abstract": "Purpose: Bangladesh's legal system struggles with major challenges like\ndelays, complexity, high costs, and millions of unresolved cases, which deter\nmany from pursuing legal action due to lack of knowledge or financial\nconstraints. This research seeks to develop a specialized Large Language Model\n(LLM) to assist in the Bangladeshi legal system. Methods: We created\nUKIL-DB-EN, an English corpus of Bangladeshi legal documents, by collecting and\nscraping data on various legal acts. We fine-tuned the GPT-2 model on this\ndataset to develop GPT2-UKIL-EN, an LLM focused on providing legal assistance\nin English. Results: The model was rigorously evaluated using semantic\nassessments, including case studies supported by expert opinions. The\nevaluation provided promising results, demonstrating the potential for the\nmodel to assist in legal matters within Bangladesh. Conclusion: Our work\nrepresents the first structured effort toward building an AI-based legal\nassistant for Bangladesh. While the results are encouraging, further\nrefinements are necessary to improve the model's accuracy, credibility, and\nsafety. This is a significant step toward creating a legal AI capable of\nserving the needs of a population of 180 million.", "published": "2024-10-22 17:34:59", "link": "http://arxiv.org/abs/2410.17210v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Dhoroni: Exploring Bengali Climate Change and Environmental Views with a\n  Multi-Perspective News Dataset and Natural Language Processing", "abstract": "Climate change poses critical challenges globally, disproportionately\naffecting low-income countries that often lack resources and linguistic\nrepresentation on the international stage. Despite Bangladesh's status as one\nof the most vulnerable nations to climate impacts, research gaps persist in\nBengali-language studies related to climate change and NLP. To address this\ndisparity, we introduce Dhoroni, a novel Bengali (Bangla) climate change and\nenvironmental news dataset, comprising a 2300 annotated Bangla news articles,\noffering multiple perspectives such as political influence,\nscientific/statistical data, authenticity, stance detection, and stakeholder\ninvolvement. Furthermore, we present an in-depth exploratory analysis of\nDhoroni and introduce BanglaBERT-Dhoroni family, a novel baseline model family\nfor climate and environmental opinion detection in Bangla, fine-tuned on our\ndataset. This research contributes significantly to enhancing accessibility and\nanalysis of climate discourse in Bengali (Bangla), addressing crucial\ncommunication and research gaps in climate-impacted regions like Bangladesh\nwith 180 million people.", "published": "2024-10-22 17:47:05", "link": "http://arxiv.org/abs/2410.17225v2", "categories": ["cs.CL", "cs.CY", "cs.LG", "stat.AP"], "primary_category": "cs.CL"}
{"title": "Automated Spinal MRI Labelling from Reports Using a Large Language Model", "abstract": "We propose a general pipeline to automate the extraction of labels from\nradiology reports using large language models, which we validate on spinal MRI\nreports. The efficacy of our labelling method is measured on five distinct\nconditions: spinal cancer, stenosis, spondylolisthesis, cauda equina\ncompression and herniation. Using open-source models, our method equals or\nsurpasses GPT-4 on a held-out set of reports. Furthermore, we show that the\nextracted labels can be used to train imaging models to classify the identified\nconditions in the accompanying MR scans. All classifiers trained using\nautomated labels achieve comparable performance to models trained using scans\nmanually annotated by clinicians. Code can be found at\nhttps://github.com/robinyjpark/AutoLabelClassifier.", "published": "2024-10-22 17:54:07", "link": "http://arxiv.org/abs/2410.17235v1", "categories": ["eess.IV", "cs.CL", "cs.CV"], "primary_category": "eess.IV"}
{"title": "Large Language Models Empowered Personalized Web Agents", "abstract": "Web agents have emerged as a promising direction to automate Web task\ncompletion based on user instructions, significantly enhancing user experience.\nRecently, Web agents have evolved from traditional agents to Large Language\nModels (LLMs)-based Web agents. Despite their success, existing LLM-based Web\nagents overlook the importance of personalized data (e.g., user profiles and\nhistorical Web behaviors) in assisting the understanding of users' personalized\ninstructions and executing customized actions. To overcome the limitation, we\nfirst formulate the task of LLM-empowered personalized Web agents, which\nintegrate personalized data and user instructions to personalize instruction\ncomprehension and action execution. To address the absence of a comprehensive\nevaluation benchmark, we construct a Personalized Web Agent Benchmark\n(PersonalWAB), featuring user instructions, personalized user data, Web\nfunctions, and two evaluation paradigms across three personalized Web tasks.\nMoreover, we propose a Personalized User Memory-enhanced Alignment (PUMA)\nframework to adapt LLMs to the personalized Web agent task. PUMA utilizes a\nmemory bank with a task-specific retrieval strategy to filter relevant\nhistorical Web behaviors. Based on the behaviors, PUMA then aligns LLMs for\npersonalized action execution through fine-tuning and direct preference\noptimization. Extensive experiments validate the superiority of PUMA over\nexisting Web agents on PersonalWAB.", "published": "2024-10-22 17:54:45", "link": "http://arxiv.org/abs/2410.17236v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "SELA: Tree-Search Enhanced LLM Agents for Automated Machine Learning", "abstract": "Automated Machine Learning (AutoML) approaches encompass traditional methods\nthat optimize fixed pipelines for model selection and ensembling, as well as\nnewer LLM-based frameworks that autonomously build pipelines. While LLM-based\nagents have shown promise in automating machine learning tasks, they often\ngenerate low-diversity and suboptimal code, even after multiple iterations. To\novercome these limitations, we introduce Tree-Search Enhanced LLM Agents\n(SELA), an innovative agent-based system that leverages Monte Carlo Tree Search\n(MCTS) to optimize the AutoML process. By representing pipeline configurations\nas trees, our framework enables agents to conduct experiments intelligently and\niteratively refine their strategies, facilitating a more effective exploration\nof the machine learning solution space. This novel approach allows SELA to\ndiscover optimal pathways based on experimental feedback, improving the overall\nquality of the solutions. In an extensive evaluation across 20 machine learning\ndatasets, we compare the performance of traditional and agent-based AutoML\nmethods, demonstrating that SELA achieves a win rate of 65% to 80% against each\nbaseline across all datasets. These results underscore the significant\npotential of agent-based strategies in AutoML, offering a fresh perspective on\ntackling complex machine learning challenges.", "published": "2024-10-22 17:56:08", "link": "http://arxiv.org/abs/2410.17238v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.SE"], "primary_category": "cs.AI"}
{"title": "JMMMU: A Japanese Massive Multi-discipline Multimodal Understanding\n  Benchmark for Culture-aware Evaluation", "abstract": "Accelerating research on Large Multimodal Models (LMMs) in non-English\nlanguages is crucial for enhancing user experiences across broader populations.\nIn this paper, we introduce JMMMU (Japanese MMMU), the first large-scale\nJapanese benchmark designed to evaluate LMMs on expert-level tasks based on the\nJapanese cultural context. To facilitate comprehensive culture-aware\nevaluation, JMMMU features two complementary subsets: (i) culture-agnostic (CA)\nsubset, where the culture-independent subjects (e.g., Math) are selected and\ntranslated into Japanese, enabling one-to-one comparison with its English\ncounterpart MMMU; and (ii) culture-specific (CS) subset, comprising newly\ncrafted subjects that reflect Japanese cultural context. Using the CA subset,\nwe observe performance drop in many LMMs when evaluated in Japanese, which is\npurely attributable to language variation. Using the CS subset, we reveal their\ninadequate Japanese cultural understanding. Further, by combining both subsets,\nwe identify that some LMMs perform well on the CA subset but not on the CS\nsubset, exposing a shallow understanding of the Japanese language that lacks\ndepth in cultural understanding. We hope this work will not only help advance\nLMM performance in Japanese but also serve as a guideline to create\nhigh-standard, culturally diverse benchmarks for multilingual LMM development.\nThe project page is https://mmmu-japanese-benchmark.github.io/JMMMU/.", "published": "2024-10-22 17:59:56", "link": "http://arxiv.org/abs/2410.17250v2", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Literature Meets Data: A Synergistic Approach to Hypothesis Generation", "abstract": "AI holds promise for transforming scientific processes, including hypothesis\ngeneration. Prior work on hypothesis generation can be broadly categorized into\ntheory-driven and data-driven approaches. While both have proven effective in\ngenerating novel and plausible hypotheses, it remains an open question whether\nthey can complement each other. To address this, we develop the first method\nthat combines literature-based insights with data to perform LLM-powered\nhypothesis generation. We apply our method on five different datasets and\ndemonstrate that integrating literature and data outperforms other baselines\n(8.97\\% over few-shot, 15.75\\% over literature-based alone, and 3.37\\% over\ndata-driven alone). Additionally, we conduct the first human evaluation to\nassess the utility of LLM-generated hypotheses in assisting human\ndecision-making on two challenging tasks: deception detection and AI generated\ncontent detection. Our results show that human accuracy improves significantly\nby 7.44\\% and 14.19\\% on these tasks, respectively. These findings suggest that\nintegrating literature-based and data-driven approaches provides a\ncomprehensive and nuanced framework for hypothesis generation and could open\nnew avenues for scientific inquiry.", "published": "2024-10-22 18:00:00", "link": "http://arxiv.org/abs/2410.17309v3", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Are Large Language Models Ready for Travel Planning?", "abstract": "While large language models (LLMs) show promise in hospitality and tourism,\ntheir ability to provide unbiased service across demographic groups remains\nunclear. This paper explores gender and ethnic biases when LLMs are utilized as\ntravel planning assistants. To investigate this issue, we apply machine\nlearning techniques to analyze travel suggestions generated from three\nopen-source LLMs. Our findings reveal that the performance of race and gender\nclassifiers substantially exceeds random chance, indicating differences in how\nLLMs engage with varied subgroups. Specifically, outputs align with cultural\nexpectations tied to certain races and genders. To minimize the effect of these\nstereotypes, we used a stop-word classification strategy, which decreased\nidentifiable differences, with no disrespectful terms found. However,\nhallucinations related to African American and gender minority groups were\nnoted. In conclusion, while LLMs can generate travel plans seemingly free from\nbias, it remains essential to verify the accuracy and appropriateness of their\nrecommendations.", "published": "2024-10-22 18:08:25", "link": "http://arxiv.org/abs/2410.17333v1", "categories": ["cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.AI"}
{"title": "Captions Speak Louder than Images (CASLIE): Generalizing Foundation\n  Models for E-commerce from High-quality Multimodal Instruction Data", "abstract": "Leveraging multimodal data to drive breakthroughs in e-commerce applications\nthrough Multimodal Foundation Models (MFMs) is gaining increasing attention\nfrom the research community. However, there are significant challenges that\nhinder the optimal use of multimodal e-commerce data by foundation models: (1)\nthe scarcity of large-scale, high-quality multimodal benchmark datasets; and\n(2) the lack of effective multimodal information integration methods. To\naddress these challenges, in this paper, we introduce MMECInstruct, the\nfirst-ever, large-scale, and high-quality multimodal instruction dataset for\ne-commerce. We also develop CASLIE, a simple, lightweight, yet effective\nframework for integrating multimodal information for e-commerce. Leveraging\nMMECInstruct, we fine-tune a series of e-commerce MFMs within CASLIE, denoted\nas CASLIE models. Our comprehensive evaluation demonstrates that CASLIE models\nsubstantially outperform 5 categories of advanced baseline models in the\nin-domain evaluation. Moreover, CASLIE models show strong generalizability to\nout-of-domain settings. MMECInstruct and CASLIE models are publicly accessible\nthrough https://ninglab.github.io/CASLIE/.", "published": "2024-10-22 18:11:43", "link": "http://arxiv.org/abs/2410.17337v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "AMUSD: Asynchronous Multi-Device Speculative Decoding for LLM\n  Acceleration", "abstract": "Large language models typically generate tokens autoregressively, using each\ntoken as input for the next. Recent work on Speculative Decoding has sought to\naccelerate this process by employing a smaller, faster draft model to more\nquickly generate candidate tokens. These candidates are then verified in\nparallel by the larger (original) verify model, resulting in overall speedup\ncompared to using the larger model by itself in an autoregressive fashion. In\nthis work, we introduce AMUSD (Asynchronous Multi-device Speculative Decoding),\na system that further accelerates generation by decoupling the draft and verify\nphases into a continuous, asynchronous approach. Unlike conventional\nspeculative decoding, where only one model (draft or verify) performs token\ngeneration at a time, AMUSD enables both models to perform predictions\nindependently on separate devices (e.g., GPUs). We evaluate our approach over\nmultiple datasets and show that AMUSD achieves an average 29% improvement over\nspeculative decoding and up to 1.96$\\times$ speedup over conventional\nautoregressive decoding, while achieving identical output quality. Our system\nis open-source and available at https://github.com/BradMcDanel/AMUSD/.", "published": "2024-10-22 19:15:35", "link": "http://arxiv.org/abs/2410.17375v1", "categories": ["cs.CL", "cs.DC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Artificial Intelligence in Brazilian News: A Mixed-Methods Analysis", "abstract": "The current surge in Artificial Intelligence (AI) interest, reflected in\nheightened media coverage since 2009, has sparked significant debate on AI's\nimplications for privacy, social justice, workers' rights, and democracy. The\nmedia plays a crucial role in shaping public perception and acceptance of AI\ntechnologies. However, research into how AI appears in media has primarily\nfocused on anglophone contexts, leaving a gap in understanding how AI is\nrepresented globally. This study addresses this gap by analyzing 3,560 news\narticles from Brazilian media published between July 1, 2023, and February 29,\n2024, from 13 popular online news outlets. Using Computational Grounded Theory\n(CGT), the study applies Latent Dirichlet Allocation (LDA), BERTopic, and\nNamed-Entity Recognition to investigate the main topics in AI coverage and the\nentities represented. The findings reveal that Brazilian news coverage of AI is\ndominated by topics related to applications in the workplace and product\nlaunches, with limited space for societal concerns, which mostly focus on\ndeepfakes and electoral integrity. The analysis also highlights a significant\npresence of industry-related entities, indicating a strong influence of\ncorporate agendas in the country's news. This study underscores the need for a\nmore critical and nuanced discussion of AI's societal impacts in Brazilian\nmedia.", "published": "2024-10-22 20:52:51", "link": "http://arxiv.org/abs/2410.17423v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Do Robot Snakes Dream like Electric Sheep? Investigating the Effects of\n  Architectural Inductive Biases on Hallucination", "abstract": "The growth in prominence of large language models (LLMs) in everyday life can\nbe largely attributed to their generative abilities, yet some of this is also\nowed to the risks and costs associated with their use. On one front is their\ntendency to hallucinate false or misleading information, limiting their\nreliability. On another is the increasing focus on the computational\nlimitations associated with traditional self-attention based LLMs, which has\nbrought about new alternatives, in particular recurrent models, meant to\novercome them. Yet it remains uncommon to consider these two concerns\nsimultaneously. Do changes in architecture exacerbate/alleviate existing\nconcerns about hallucinations? Do they affect how and where they occur? Through\nan extensive evaluation, we study how these architecture-based inductive biases\naffect the propensity to hallucinate. While hallucination remains a general\nphenomenon not limited to specific architectures, the situations in which they\noccur and the ease with which specific types of hallucinations can be induced\ncan significantly differ based on the model architecture. These findings\nhighlight the need for better understanding both these problems in conjunction\nwith each other, as well as consider how to design more universal techniques\nfor handling hallucinations.", "published": "2024-10-22 23:24:15", "link": "http://arxiv.org/abs/2410.17477v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Tethering Broken Themes: Aligning Neural Topic Models with Labels and\n  Authors", "abstract": "Topic models are a popular approach for extracting semantic information from\nlarge document collections. However, recent studies suggest that the topics\ngenerated by these models often do not align well with human intentions.\nAlthough metadata such as labels and authorship information are available, it\nhas not yet been effectively incorporated into neural topic models. To address\nthis gap, we introduce FANToM, a novel method to align neural topic models with\nboth labels and authorship information. FANToM allows for the inclusion of this\nmetadata when available, producing interpretable topics and author\ndistributions for each topic. Our approach demonstrates greater expressiveness\nthan conventional topic models by learning the alignment between labels,\ntopics, and authors. Experimental results show that FANToM improves existing\nmodels in terms of both topic quality and alignment. Additionally, it\nidentifies author interests and similarities.", "published": "2024-10-22 11:20:47", "link": "http://arxiv.org/abs/2410.18140v2", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "SmartRAG: Jointly Learn RAG-Related Tasks From the Environment Feedback", "abstract": "RAG systems consist of multiple modules to work together. However, these\nmodules are usually separately trained. We argue that a system like RAG that\nincorporates multiple modules should be jointly optimized to achieve optimal\nperformance. To demonstrate this, we design a specific pipeline called\n\\textbf{SmartRAG} that includes a policy network and a retriever. The policy\nnetwork can serve as 1) a decision maker that decides when to retrieve, 2) a\nquery rewriter to generate a query most suited to the retriever, and 3) an\nanswer generator that produces the final response with/without the\nobservations. We then propose to jointly optimize the whole system using a\nreinforcement learning algorithm, with the reward designed to encourage the\nsystem to achieve the best performance with minimal retrieval cost. When\njointly optimized, all the modules can be aware of how other modules are\nworking and thus find the best way to work together as a complete system.\nEmpirical results demonstrate that the jointly optimized SmartRAG can achieve\nbetter performance than separately optimized counterparts.", "published": "2024-10-22 11:23:11", "link": "http://arxiv.org/abs/2410.18141v2", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Meaning Typed Prompting: A Technique for Efficient, Reliable Structured\n  Output Generation", "abstract": "Extending Large Language Models (LLMs) to advanced applications requires\nreliable structured output generation. Existing methods which often rely on\nrigid JSON schemas, can lead to unreliable outputs, diminished reasoning\ncapabilities, and increased computational overhead, limiting LLMs' adaptability\nfor complex tasks. We introduce Meaning Typed Prompting (MTP), a technique for\nefficient structured output generation that integrates types, meanings, and\nabstractions, such as variables and classes, into the prompting process. By\nutilizing expressive type definitions, MTP enhances output clarity and reduces\ndependence on complex abstractions, simplifying development, and improving\nimplementation efficiency. This enables LLMs to understand relationships and\ngenerate structured data more effectively. Empirical evaluations on multiple\nbenchmarks demonstrate that MTP outperforms existing frameworks in accuracy,\nreliability, consistency, and token efficiency. We present Semantix, a\nframework that implements MTP, providing practical insights into its\napplication.", "published": "2024-10-22 20:43:50", "link": "http://arxiv.org/abs/2410.18146v1", "categories": ["cs.CL", "cs.AI", "cs.PL"], "primary_category": "cs.CL"}
{"title": "Can a Machine Distinguish High and Low Amount of Social Creak in Speech?", "abstract": "Objectives: ncreased prevalence of social creak particularly among female\nspeakers has been reported in several studies. The study of social creak has\nbeen previously conducted by combining perceptual evaluation of speech with\nconventional acoustical parameters such as the harmonic-to-noise ratio and\ncepstral peak prominence. In the current study, machine learning (ML) was used\nto automatically distinguish speech of low amount of social creak from speech\nof high amount of social creak.\n  Methods: The amount of creak in continuous speech samples produced in Finnish\nby 90 female speakers was first perceptually assessed by two voice specialists.\nBased on their assessments, the speech samples were divided into two categories\n(low $vs$. high amount of creak). Using the speech signals and their creak\nlabels, seven different ML models were trained. Three spectral representations\nwere used as feature for each model.\n  Results: The results show that the best performance (accuracy of 71.1\\%) was\nobtained by the following two systems: an Adaboost classifier using the\nmel-spectrogram feature and a decision tree classifier using the mel-frequency\ncepstral coefficient feature.\n  Conclusions: The study of social creak is becoming increasingly popular in\nsociolinguistic and vocological research. The conventional human perceptual\nassessment of the amount of creak is laborious and therefore ML technology\ncould be used to assist researchers studying social creak. The classification\nsystems reported in this study could be considered as baselines in future\nML-based studies on social creak.", "published": "2024-10-22 13:52:51", "link": "http://arxiv.org/abs/2410.17028v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improving Automatic Speech Recognition with Decoder-Centric\n  Regularisation in Encoder-Decoder Models", "abstract": "This paper proposes a simple yet effective way of regularising the\nencoder-decoder-based automatic speech recognition (ASR) models that enhance\nthe robustness of the model and improve the generalisation to out-of-domain\nscenarios. The proposed approach is dubbed as\n$\\textbf{De}$coder-$\\textbf{C}$entric $\\textbf{R}$egularisation in\n$\\textbf{E}$ncoder-$\\textbf{D}$ecoder (DeCRED) architecture for ASR, where\nauxiliary classifier(s) is introduced in layers of the decoder module.\nLeveraging these classifiers, we propose two decoding strategies that\nre-estimate the next token probabilities. Using the recent E-branchformer\narchitecture, we build strong ASR systems that obtained competitive WERs as\ncompared to Whisper-medium and outperformed OWSM v3; while relying only on a\nfraction of training data and model size. On top of such a strong baseline, we\nshow that DeCRED can further improve the results and, moreover, generalise much\nbetter to out-of-domain scenarios, where we show an absolute reduction of 2.7\nand 2.9 WERs on AMI and Gigaspeech datasets, respectively. We provide extensive\nanalysis and accompanying experiments that support the benefits of the proposed\nregularisation scheme.", "published": "2024-10-22 21:27:30", "link": "http://arxiv.org/abs/2410.17437v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Prototype and Instance Contrastive Learning for Unsupervised Domain\n  Adaptation in Speaker Verification", "abstract": "Speaker verification system trained on one domain usually suffers performance\ndegradation when applied to another domain. To address this challenge,\nresearchers commonly use feature distribution matching-based methods in\nunsupervised domain adaptation scenarios where some unlabeled target domain\ndata is available. However, these methods often have limited performance\nimprovement and lack generalization in various mismatch situations. In this\npaper, we propose Prototype and Instance Contrastive Learning (PICL), a novel\nmethod for unsupervised domain adaptation in speaker verification through\ndual-level contrastive learning. For prototype contrastive learning, we\ngenerate pseudo labels via clustering to create dynamically updated prototype\nrepresentations, aligning instances with their corresponding class or cluster\nprototypes. For instance contrastive learning, we minimize the distance between\ndifferent views or augmentations of the same instance, ensuring robust and\ninvariant representations resilient to variations like noise. This dual-level\napproach provides both high-level and low-level supervision, leading to\nimproved generalization and robustness of the speaker verification model.\nUnlike previous studies that only evaluated mismatches in one situation, we\nhave conducted relevant explorations on various datasets and achieved\nstate-of-the-art performance currently, which also proves the generalization of\nour method.", "published": "2024-10-22 13:59:34", "link": "http://arxiv.org/abs/2410.17033v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Discogs-VI: A Musical Version Identification Dataset Based on Public\n  Editorial Metadata", "abstract": "Current version identification (VI) datasets often lack sufficient size and\nmusical diversity to train robust neural networks (NNs). Additionally, their\nnon-representative clique size distributions prevent realistic system\nevaluations. To address these challenges, we explore the untapped potential of\nthe rich editorial metadata in the Discogs music database and create a large\ndataset of musical versions containing about 1,900,000 versions across 348,000\ncliques. Utilizing a high-precision search algorithm, we map this dataset to\nofficial music uploads on YouTube, resulting in a dataset of approximately\n493,000 versions across 98,000 cliques. This dataset offers over nine times the\nnumber of cliques and over four times the number of versions than existing\ndatasets. We demonstrate the utility of our dataset by training a baseline NN\nwithout extensive model complexities or data augmentations, which achieves\ncompetitive results on the SHS100K and Da-TACOS datasets. Our dataset, along\nwith the tools used for its creation, the extracted audio features, and a\ntrained model, are all publicly available online.", "published": "2024-10-22 20:18:02", "link": "http://arxiv.org/abs/2410.17400v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "mmWave-Whisper: Phone Call Eavesdropping and Transcription Using\n  Millimeter-Wave Radar", "abstract": "This paper introduces mmWave-Whisper, a system that demonstrates the\nfeasibility of full-corpus automated speech recognition (ASR) on phone calls\neavesdropped remotely using off-the-shelf frequency modulated continuous wave\n(FMCW) millimeter-wave radars. Operating in the 77-81 GHz range, mmWave-Whisper\ncaptures earpiece vibrations from smartphones, converts them into audio, and\nprocesses the audio to produce speech transcriptions automatically. Unlike\nprevious work that focused on loudspeakers or limited vocabulary, this is the\nfirst work to perform such a speech recognition by handling large vocabulary\nand full sentences on earpiece vibrations from smartphones. This approach\nexpands the potential of radar-audio eavesdropping. mmWave-Whisper addresses\nchallenges such as the lack of large scale training datasets, low SNR, and\nlimited frequency information in radar data through a systematic pipeline\ndesigned to leverage synthetic training data, domain adaptation, and inference\nby incorporating OpenAI's Whisper automatic speech recognition model. The\nsystem achieves a word accuracy rate of 44.74% and a character accuracy rate of\n62.52% over a range of 25 cm to 125 cm. The paper highlights emerging misuse\nmodalities of AI as the technology evolves rapidly.", "published": "2024-10-22 22:21:39", "link": "http://arxiv.org/abs/2410.17457v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "GE2E-KWS: Generalized End-to-End Training and Evaluation for Zero-shot\n  Keyword Spotting", "abstract": "We propose GE2E-KWS -- a generalized end-to-end training and evaluation\nframework for customized keyword spotting. Specifically, enrollment utterances\nare separated and grouped by keywords from the training batch and their\nembedding centroids are compared to all other test utterance embeddings to\ncompute the loss. This simulates runtime enrollment and verification stages,\nand improves convergence stability and training speed by optimizing matrix\noperations compared to SOTA triplet loss approaches. To benchmark different\nmodels reliably, we propose an evaluation process that mimics the production\nenvironment and compute metrics that directly measure keyword matching\naccuracy. Trained with GE2E loss, our 419KB quantized conformer model beats a\n7.5GB ASR encoder by 23.6% relative AUC, and beats a same size triplet loss\nmodel by 60.7% AUC. Our KWS models are natively streamable with low memory\nfootprints, and designed to continuously run on-device with no retraining\nneeded for new keywords (zero-shot).", "published": "2024-10-22 02:45:59", "link": "http://arxiv.org/abs/2410.16647v1", "categories": ["eess.AS", "cs.AI", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Annotation-Free MIDI-to-Audio Synthesis via Concatenative Synthesis and\n  Generative Refinement", "abstract": "Recent MIDI-to-audio synthesis methods have employed deep neural networks to\nsuccessfully generate high-quality and expressive instrumental tracks. However,\nthese methods require MIDI annotations for supervised training, limiting the\ndiversity of the output audio in terms of instrument timbres, and expression\nstyles. We propose CoSaRef, a MIDI-to-audio synthesis method that can be\ndeveloped without MIDI-audio paired datasets. CoSaRef first performs\nconcatenative synthesis based on MIDI inputs and then refines the resulting\naudio into realistic tracks using a diffusion-based deep generative model\ntrained on audio-only datasets. This approach enhances the diversity of audio\ntimbres and expression styles. It also allows for control over the output\ntimbre based on audio sample selection, similar to traditional functions in\ndigital audio workstations. Experiments show that while inherently capable of\ngenerating general tracks with high control over timbre, CoSaRef can also\nperform comparably to conventional methods in generating realistic audio.", "published": "2024-10-22 08:01:40", "link": "http://arxiv.org/abs/2410.16785v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Variational autoencoders stabilise TCN performance when classifying\n  weakly labelled bioacoustics data", "abstract": "Passive acoustic monitoring (PAM) data is often weakly labelled, audited at\nthe scale of detection presence or absence on timescales of minutes to hours.\nMoreover, this data exhibits great variability from one deployment to the next,\ndue to differences in ambient noise and the signals across sources and\ngeographies. This study proposes a two-step solution to leverage weakly\nannotated data for training Deep Learning (DL) detection models. Our case study\ninvolves binary classification of the presence/absence of sperm whale\n(\\textit{Physeter macrocephalus}) click trains in 4-minute-long recordings from\na dataset comprising diverse sources and deployment conditions to maximise\ngeneralisability. We tested methods for extracting acoustic features from\nlengthy audio segments and integrated Temporal Convolutional Networks (TCNs)\ntrained on the extracted features for sequence classification. For feature\nextraction, we introduced a new approach using Variational AutoEncoders (VAEs)\nto extract information from both waveforms and spectrograms, which eliminates\nthe necessity for manual threshold setting or time-consuming strong labelling.\nFor classification, TCNs were trained separately on sequences of either VAE\nembeddings or handpicked acoustic features extracted from the waveform and\nspectrogram representations using classical methods, to compare the efficacy of\nthe two approaches. The TCN demonstrated robust classification capabilities on\na validation set, achieving accuracies exceeding 85\\% when applied to 4-minute\nacoustic recordings. Notably, TCNs trained on handpicked acoustic features\nexhibited greater variability in performance across recordings from diverse\ndeployment conditions, whereas those trained on VAEs showed a more consistent\nperformance, highlighting the robust transferability of VAEs for feature\nextraction across different deployment conditions.", "published": "2024-10-22 13:25:59", "link": "http://arxiv.org/abs/2410.17006v1", "categories": ["cs.SD", "eess.AS", "q-bio.QM"], "primary_category": "cs.SD"}
