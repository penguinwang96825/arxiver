{"title": "Dual Attention Network for Product Compatibility and Function\n  Satisfiability Analysis", "abstract": "Product compatibility and their functionality are of utmost importance to\ncustomers when they purchase products, and to sellers and manufacturers when\nthey sell products. Due to the huge number of products available online, it is\ninfeasible to enumerate and test the compatibility and functionality of every\nproduct. In this paper, we address two closely related problems: product\ncompatibility analysis and function satisfiability analysis, where the second\nproblem is a generalization of the first problem (e.g., whether a product works\nwith another product can be considered as a special function). We first\nidentify a novel question and answering corpus that is up-to-date regarding\nproduct compatibility and functionality information. To allow automatic\ndiscovery product compatibility and functionality, we then propose a deep\nlearning model called Dual Attention Network (DAN). Given a QA pair for a\nto-be-purchased product, DAN learns to 1) discover complementary products (or\nfunctions), and 2) accurately predict the actual compatibility (or\nsatisfiability) of the discovered products (or functions). The challenges\naddressed by the model include the briefness of QAs, linguistic patterns\nindicating compatibility, and the appropriate fusion of questions and answers.\nWe conduct experiments to quantitatively and qualitatively show that the\nidentified products and functions have both high coverage and accuracy,\ncompared with a wide spectrum of baselines.", "published": "2017-12-06 03:11:51", "link": "http://arxiv.org/abs/1712.02016v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-channel Encoder for Neural Machine Translation", "abstract": "Attention-based Encoder-Decoder has the effective architecture for neural\nmachine translation (NMT), which typically relies on recurrent neural networks\n(RNN) to build the blocks that will be lately called by attentive reader during\nthe decoding process. This design of encoder yields relatively uniform\ncomposition on source sentence, despite the gating mechanism employed in\nencoding RNN. On the other hand, we often hope the decoder to take pieces of\nsource sentence at varying levels suiting its own linguistic structure: for\nexample, we may want to take the entity name in its raw form while taking an\nidiom as a perfectly composed unit. Motivated by this demand, we propose\nMulti-channel Encoder (MCE), which enhances encoding components with different\nlevels of composition. More specifically, in addition to the hidden state of\nencoding RNN, MCE takes 1) the original word embedding for raw encoding with no\ncomposition, and 2) a particular design of external memory in Neural Turing\nMachine (NTM) for more complex composition, while all three encoding strategies\nare properly blended during decoding. Empirical study on Chinese-English\ntranslation shows that our model can improve by 6.52 BLEU points upon a strong\nopen source NMT system: DL4MT1. On the WMT14 English- French task, our single\nshallow system achieves BLEU=38.8, comparable with the state-of-the-art deep\nmodels.", "published": "2017-12-06 09:59:43", "link": "http://arxiv.org/abs/1712.02109v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Novel Embedding Model for Knowledge Base Completion Based on\n  Convolutional Neural Network", "abstract": "In this paper, we propose a novel embedding model, named ConvKB, for\nknowledge base completion. Our model ConvKB advances state-of-the-art models by\nemploying a convolutional neural network, so that it can capture global\nrelationships and transitional characteristics between entities and relations\nin knowledge bases. In ConvKB, each triple (head entity, relation, tail entity)\nis represented as a 3-column matrix where each column vector represents a\ntriple element. This 3-column matrix is then fed to a convolution layer where\nmultiple filters are operated on the matrix to generate different feature maps.\nThese feature maps are then concatenated into a single feature vector\nrepresenting the input triple. The feature vector is multiplied with a weight\nvector via a dot product to return a score. This score is then used to predict\nwhether the triple is valid or not. Experiments show that ConvKB achieves\nbetter link prediction performance than previous state-of-the-art embedding\nmodels on two benchmark datasets WN18RR and FB15k-237.", "published": "2017-12-06 10:41:47", "link": "http://arxiv.org/abs/1712.02121v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Product Function Need Recognition via Semi-supervised Attention Network", "abstract": "Functionality is of utmost importance to customers when they purchase\nproducts. However, it is unclear to customers whether a product can really\nsatisfy their needs on functions. Further, missing functions may be\nintentionally hidden by the manufacturers or the sellers. As a result, a\ncustomer needs to spend a fair amount of time before purchasing or just\npurchase the product on his/her own risk. In this paper, we first identify a\nnovel QA corpus that is dense on product functionality information\n\\footnote{The annotated corpus can be found at\n\\url{https://www.cs.uic.edu/~hxu/}.}. We then design a neural network called\nSemi-supervised Attention Network (SAN) to discover product functions from\nquestions. This model leverages unlabeled data as contextual information to\nperform semi-supervised sequence labeling. We conduct experiments to show that\nthe extracted function have both high coverage and accuracy, compared with a\nwide spectrum of baselines.", "published": "2017-12-06 13:48:57", "link": "http://arxiv.org/abs/1712.02186v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Convolutional Neural Networks for Medical Diagnosis from Admission Notes", "abstract": "$\\textbf{Objective}$ Develop an automatic diagnostic system which only uses\ntextual admission information from Electronic Health Records (EHRs) and assist\nclinicians with a timely and statistically proved decision tool. The hope is\nthat the tool can be used to reduce mis-diagnosis.\n  $\\textbf{Materials and Methods}$ We use the real-world clinical notes from\nMIMIC-III, a freely available dataset consisting of clinical data of more than\nforty thousand patients who stayed in intensive care units of the Beth Israel\nDeaconess Medical Center between 2001 and 2012. We proposed a Convolutional\nNeural Network model to learn semantic features from unstructured textual input\nand automatically predict primary discharge diagnosis.\n  $\\textbf{Results}$ The proposed model achieved an overall 96.11% accuracy and\n80.48% weighted F1 score values on 10 most frequent disease classes,\nsignificantly outperforming four strong baseline models by at least 12.7% in\nweighted F1 score.\n  $\\textbf{Discussion}$ Experimental results imply that the CNN model is\nsuitable for supporting diagnosis decision making in the presence of complex,\nnoisy and unstructured clinical data while at the same time using fewer layers\nand parameters that other traditional Deep Network models.\n  $\\textbf{Conclusion}$ Our model demonstrated capability of representing\ncomplex medical meaningful features from unstructured clinical notes and\nprediction power for commonly misdiagnosed frequent diseases. It can use easily\nadopted in clinical setting to provide timely and statistically proved decision\nsupport.\n  $\\textbf{Keywords}$ Convolutional neural network, text classification,\ndischarge diagnosis prediction, admission information from EHRs.", "published": "2017-12-06 08:39:29", "link": "http://arxiv.org/abs/1712.02768v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Distance-based Self-Attention Network for Natural Language Inference", "abstract": "Attention mechanism has been used as an ancillary means to help RNN or CNN.\nHowever, the Transformer (Vaswani et al., 2017) recently recorded the\nstate-of-the-art performance in machine translation with a dramatic reduction\nin training time by solely using attention. Motivated by the Transformer,\nDirectional Self Attention Network (Shen et al., 2017), a fully attention-based\nsentence encoder, was proposed. It showed good performance with various data by\nusing forward and backward directional information in a sentence. But in their\nstudy, not considered at all was the distance between words, an important\nfeature when learning the local dependency to help understand the context of\ninput text. We propose Distance-based Self-Attention Network, which considers\nthe word distance by using a simple distance mask in order to model the local\ndependency without losing the ability of modeling global dependency which\nattention has inherent. Our model shows good performance with NLI data, and it\nrecords the new state-of-the-art result with SNLI data. Additionally, we show\nthat our model has a strength in long sentences or documents.", "published": "2017-12-06 05:38:29", "link": "http://arxiv.org/abs/1712.02047v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Discourse-Aware Rumour Stance Classification in Social Media Using\n  Sequential Classifiers", "abstract": "Rumour stance classification, defined as classifying the stance of specific\nsocial media posts into one of supporting, denying, querying or commenting on\nan earlier post, is becoming of increasing interest to researchers. While most\nprevious work has focused on using individual tweets as classifier inputs, here\nwe report on the performance of sequential classifiers that exploit the\ndiscourse features inherent in social media interactions or 'conversational\nthreads'. Testing the effectiveness of four sequential classifiers -- Hawkes\nProcesses, Linear-Chain Conditional Random Fields (Linear CRF), Tree-Structured\nConditional Random Fields (Tree CRF) and Long Short Term Memory networks (LSTM)\n-- on eight datasets associated with breaking news stories, and looking at\ndifferent types of local and contextual features, our work sheds new light on\nthe development of accurate stance classifiers. We show that sequential\nclassifiers that exploit the use of discourse properties in social media\nconversations while using only local features, outperform non-sequential\nclassifiers. Furthermore, we show that LSTM using a reduced set of features can\noutperform the other sequential classifiers; this performance is consistent\nacross datasets and across types of stances. To conclude, our work also\nanalyses the different features under study, identifying those that best help\ncharacterise and distinguish between stances, such as supporting tweets being\nmore likely to be accompanied by evidence than denying tweets. We also set\nforth a number of directions for future research.", "published": "2017-12-06 15:15:21", "link": "http://arxiv.org/abs/1712.02223v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Why Do Neural Dialog Systems Generate Short and Meaningless Replies? A\n  Comparison between Dialog and Translation", "abstract": "This paper addresses the question: Why do neural dialog systems generate\nshort and meaningless replies? We conjecture that, in a dialog system, an\nutterance may have multiple equally plausible replies, causing the deficiency\nof neural networks in the dialog application. We propose a systematic way to\nmimic the dialog scenario in a machine translation system, and manage to\nreproduce the phenomenon of generating short and less meaningful sentences in\nthe translation setting, showing evidence of our conjecture.", "published": "2017-12-06 16:00:45", "link": "http://arxiv.org/abs/1712.02250v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An innovative solution for breast cancer textual big data analysis", "abstract": "The digitalization of stored information in hospitals now allows for the\nexploitation of medical data in text format, as electronic health records\n(EHRs), initially gathered for other purposes than epidemiology. Manual search\nand analysis operations on such data become tedious. In recent years, the use\nof natural language processing (NLP) tools was highlighted to automatize the\nextraction of information contained in EHRs, structure it and perform\nstatistical analysis on this structured information. The main difficulties with\nthe existing approaches is the requirement of synonyms or ontology\ndictionaries, that are mostly available in English only and do not include\nlocal or custom notations. In this work, a team composed of oncologists as\ndomain experts and data scientists develop a custom NLP-based system to process\nand structure textual clinical reports of patients suffering from breast\ncancer. The tool relies on the combination of standard text mining techniques\nand an advanced synonym detection method. It allows for a global analysis by\nretrieval of indicators such as medical history, tumor characteristics,\ntherapeutic responses, recurrences and prognosis. The versatility of the method\nallows to obtain easily new indicators, thus opening up the way for\nretrospective studies with a substantial reduction of the amount of manual\nwork. With no need for biomedical annotators or pre-defined ontologies, this\nlanguage-agnostic method reached an good extraction accuracy for several\nconcepts of interest, according to a comparison with a manually structured\nfile, without requiring any existing corpus with local or new notations.", "published": "2017-12-06 16:18:31", "link": "http://arxiv.org/abs/1712.02259v1", "categories": ["stat.ML", "cs.CL"], "primary_category": "stat.ML"}
{"title": "An analysis of incorporating an external language model into a\n  sequence-to-sequence model", "abstract": "Attention-based sequence-to-sequence models for automatic speech recognition\njointly train an acoustic model, language model, and alignment mechanism. Thus,\nthe language model component is only trained on transcribed audio-text pairs.\nThis leads to the use of shallow fusion with an external language model at\ninference time. Shallow fusion refers to log-linear interpolation with a\nseparately trained language model at each step of the beam search. In this\nwork, we investigate the behavior of shallow fusion across a range of\nconditions: different types of language models, different decoding units, and\ndifferent tasks. On Google Voice Search, we demonstrate that the use of shallow\nfusion with a neural LM with wordpieces yields a 9.1% relative word error rate\nreduction (WERR) over our competitive attention-based sequence-to-sequence\nmodel, obviating the need for second-pass rescoring.", "published": "2017-12-06 01:30:54", "link": "http://arxiv.org/abs/1712.01996v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SMILES2Vec: An Interpretable General-Purpose Deep Neural Network for\n  Predicting Chemical Properties", "abstract": "Chemical databases store information in text representations, and the SMILES\nformat is a universal standard used in many cheminformatics software. Encoded\nin each SMILES string is structural information that can be used to predict\ncomplex chemical properties. In this work, we develop SMILES2vec, a deep RNN\nthat automatically learns features from SMILES to predict chemical properties,\nwithout the need for additional explicit feature engineering. Using Bayesian\noptimization methods to tune the network architecture, we show that an\noptimized SMILES2vec model can serve as a general-purpose neural network for\npredicting distinct chemical properties including toxicity, activity,\nsolubility and solvation energy, while also outperforming contemporary MLP\nneural networks that uses engineered features. Furthermore, we demonstrate\nproof-of-concept of interpretability by developing an explanation mask that\nlocalizes on the most important characters used in making a prediction. When\ntested on the solubility dataset, it identified specific parts of a chemical\nthat is consistent with established first-principles knowledge with an accuracy\nof 88%. Our work demonstrates that neural networks can learn technically\naccurate chemical concept and provide state-of-the-art accuracy, making\ninterpretable deep neural networks a useful tool of relevance to the chemical\nindustry.", "published": "2017-12-06 04:29:28", "link": "http://arxiv.org/abs/1712.02034v2", "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Enabling Early Audio Event Detection with Neural Networks", "abstract": "This paper presents a methodology for early detection of audio events from\naudio streams. Early detection is the ability to infer an ongoing event during\nits initial stage. The proposed system consists of a novel inference step\ncoupled with dual parallel tailored-loss deep neural networks (DNNs). The DNNs\nshare a similar architecture except for their loss functions, i.e. weighted\nloss and multitask loss, which are designed to efficiently cope with issues\ncommon to audio event detection. The inference step is newly introduced to make\nuse of the network outputs for recognizing ongoing events. The monotonicity of\nthe detection function is required for reliable early detection, and will also\nbe proved. Experiments on the ITC-Irst database show that the proposed system\nachieves state-of-the-art detection performance. Furthermore, even partial\nevents are sufficient to achieve good performance similar to that obtained when\nan entire event is observed, enabling early event detection.", "published": "2017-12-06 10:12:46", "link": "http://arxiv.org/abs/1712.02116v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
