{"title": "Misconfidence-based Demonstration Selection for LLM In-Context Learning", "abstract": "In-context learning with large language models (LLMs) excels at adapting to\nvarious tasks rapidly. However, its success hinges on carefully selecting\ndemonstrations, which remains an obstacle in practice. Current approaches to\nthis problem either rely on hard-to-acquire external supervision or require\nfrequent interactions with LLMs, resulting in high costs. We propose a new\nmethod called In-Context Reflection (ICR) to overcome these challenges. ICR\nstrategically selects demonstrations to reduce the discrepancy between the\nLLM's outputs and the actual input-output mappings. Specifically, ICR starts\nwith a random set of initial demonstrations, then iteratively refines it. In\neach step, it analyzes a pool of candidate examples and identifies the ones\nmost likely to challenge the LLM's current understanding, measured by a new\nmetric called misconfidence. These most confusing examples are then selected to\nreplace the less informative demonstrations in the current set. Our\ncomprehensive evaluation across five diverse datasets encompassing 13 subtasks\nshows the efficacy of ICR. Compared to existing methods, ICR achieves an\naverage performance boost of 4%, while demonstrating remarkable cross-task\ngeneralization capabilities.", "published": "2024-01-12 00:11:24", "link": "http://arxiv.org/abs/2401.06301v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Task Learning for Front-End Text Processing in TTS", "abstract": "We propose a multi-task learning (MTL) model for jointly performing three\ntasks that are commonly solved in a text-to-speech (TTS) front-end: text\nnormalization (TN), part-of-speech (POS) tagging, and homograph disambiguation\n(HD). Our framework utilizes a tree-like structure with a trunk that learns\nshared representations, followed by separate task-specific heads. We further\nincorporate a pre-trained language model to utilize its built-in lexical and\ncontextual knowledge, and study how to best use its embeddings so as to most\neffectively benefit our multi-task model. Through task-wise ablations, we show\nthat our full model trained on all three tasks achieves the strongest overall\nperformance compared to models trained on individual or sub-combinations of\ntasks, confirming the advantages of our MTL framework. Finally, we introduce a\nnew HD dataset containing a balanced number of sentences in diverse contexts\nfor a variety of homographs and their pronunciations. We demonstrate that\nincorporating this dataset into training significantly improves HD performance\nover only using a commonly used, but imbalanced, pre-existing dataset.", "published": "2024-01-12 02:13:21", "link": "http://arxiv.org/abs/2401.06321v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning from Semi-Factuals: A Debiased and Semantic-Aware Framework for\n  Generalized Relation Discovery", "abstract": "We introduce a novel task, called Generalized Relation Discovery (GRD), for\nopen-world relation extraction. GRD aims to identify unlabeled instances in\nexisting pre-defined relations or discover novel relations by assigning\ninstances to clusters as well as providing specific meanings for these\nclusters. The key challenges of GRD are how to mitigate the serious model\nbiases caused by labeled pre-defined relations to learn effective relational\nrepresentations and how to determine the specific semantics of novel relations\nduring classifying or clustering unlabeled instances. We then propose a novel\nframework, SFGRD, for this task to solve the above issues by learning from\nsemi-factuals in two stages. The first stage is semi-factual generation\nimplemented by a tri-view debiased relation representation module, in which we\ntake each original sentence as the main view and design two debiased views to\ngenerate semi-factual examples for this sentence. The second stage is\nsemi-factual thinking executed by a dual-space tri-view collaborative relation\nlearning module, where we design a cluster-semantic space and a class-index\nspace to learn relational semantics and relation label indices, respectively.\nIn addition, we devise alignment and selection strategies to integrate two\nspaces and establish a self-supervised learning loop for unlabeled data by\ndoing semi-factual thinking across three views. Extensive experimental results\nshow that SFGRD surpasses state-of-the-art models in terms of accuracy by\n2.36\\% $\\sim$5.78\\% and cosine similarity by 32.19\\%$\\sim$ 84.45\\% for relation\nlabel index and relation semantic quality, respectively. To the best of our\nknowledge, we are the first to exploit the efficacy of semi-factuals in\nrelation extraction.", "published": "2024-01-12 02:38:55", "link": "http://arxiv.org/abs/2401.06327v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An approach for mistranslation removal from popular dataset for Indic MT\n  Task", "abstract": "The conversion of content from one language to another utilizing a computer\nsystem is known as Machine Translation (MT). Various techniques have come up to\nensure effective translations that retain the contextual and lexical\ninterpretation of the source language. End-to-end Neural Machine Translation\n(NMT) is a popular technique and it is now widely used in real-world MT\nsystems. Massive amounts of parallel datasets (sentences in one language\nalongside translations in another) are required for MT systems. These datasets\nare crucial for an MT system to learn linguistic structures and patterns of\nboth languages during the training phase. One such dataset is Samanantar, the\nlargest publicly accessible parallel dataset for Indian languages (ILs). Since\nthe corpus has been gathered from various sources, it contains many incorrect\ntranslations. Hence, the MT systems built using this dataset cannot perform to\ntheir usual potential. In this paper, we propose an algorithm to remove\nmistranslations from the training corpus and evaluate its performance and\nefficiency. Two Indic languages (ILs), namely, Hindi (HIN) and Odia (ODI) are\nchosen for the experiment. A baseline NMT system is built for these two ILs,\nand the effect of different dataset sizes is also investigated. The quality of\nthe translations in the experiment is evaluated using standard metrics such as\nBLEU, METEOR, and RIBES. From the results, it is observed that removing the\nincorrect translation from the dataset makes the translation quality better. It\nis also noticed that, despite the fact that the ILs-English and English-ILs\nsystems are trained using the same corpus, ILs-English works more effectively\nacross all the evaluation metrics.", "published": "2024-01-12 06:37:19", "link": "http://arxiv.org/abs/2401.06398v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AboutMe: Using Self-Descriptions in Webpages to Document the Effects of\n  English Pretraining Data Filters", "abstract": "Large language models' (LLMs) abilities are drawn from their pretraining\ndata, and model development begins with data curation. However, decisions\naround what data is retained or removed during this initial stage are\nunder-scrutinized. In our work, we ground web text, which is a popular\npretraining data source, to its social and geographic contexts. We create a new\ndataset of 10.3 million self-descriptions of website creators, and extract\ninformation about who they are and where they are from: their topical\ninterests, social roles, and geographic affiliations. Then, we conduct the\nfirst study investigating how ten \"quality\" and English language identification\n(langID) filters affect webpages that vary along these social dimensions. Our\nexperiments illuminate a range of implicit preferences in data curation: we\nshow that some quality classifiers act like topical domain filters, and langID\ncan overlook English content from some regions of the world. Overall, we hope\nthat our work will encourage a new line of research on pretraining data\ncuration practices and its social implications.", "published": "2024-01-12 07:10:10", "link": "http://arxiv.org/abs/2401.06408v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BOK-VQA: Bilingual outside Knowledge-Based Visual Question Answering via\n  Graph Representation Pretraining", "abstract": "The current research direction in generative models, such as the recently\ndeveloped GPT4, aims to find relevant knowledge information for multimodal and\nmultilingual inputs to provide answers. Under these research circumstances, the\ndemand for multilingual evaluation of visual question answering (VQA) tasks, a\nrepresentative task of multimodal systems, has increased. Accordingly, we\npropose a bilingual outside-knowledge VQA (BOK-VQA) dataset in this study that\ncan be extended to multilingualism. The proposed data include 17K images, 17K\nquestion-answer pairs for both Korean and English and 280K instances of\nknowledge information related to question-answer content. We also present a\nframework that can effectively inject knowledge information into a VQA system\nby pretraining the knowledge information of BOK-VQA data in the form of graph\nembeddings. Finally, through in-depth analysis, we demonstrated the actual\neffect of the knowledge information contained in the constructed training data\non VQA.", "published": "2024-01-12 08:31:42", "link": "http://arxiv.org/abs/2401.06443v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adapting Large Language Models for Document-Level Machine Translation", "abstract": "Large language models (LLMs) have significantly advanced various natural\nlanguage processing (NLP) tasks. Recent research indicates that\nmoderately-sized LLMs often outperform larger ones after task-specific\nfine-tuning. This study focuses on adapting LLMs for document-level machine\ntranslation (DocMT) for specific language pairs. We first investigate the\nimpact of prompt strategies on translation performance and then conduct\nextensive experiments using two fine-tuning methods, three LLM backbones, and\n18 translation tasks across nine language pairs. Our results show that\nspecialized models can sometimes surpass GPT-4 in translation performance but\nstill face issues like off-target translation due to error propagation in\ndecoding. We provide an in-depth analysis of these LLMs tailored for DocMT,\nexamining translation errors, discourse phenomena, strategies for training and\ninference, the data efficiency of parallel documents, recent test set\nevaluations, and zero-shot crosslingual transfer. Our findings highlight the\nstrengths and limitations of LLM-based DocMT models and provide a foundation\nfor future research.", "published": "2024-01-12 09:29:13", "link": "http://arxiv.org/abs/2401.06468v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AntEval: Evaluation of Social Interaction Competencies in LLM-Driven\n  Agents", "abstract": "Large Language Models (LLMs) have demonstrated their ability to replicate\nhuman behaviors across a wide range of scenarios. However, their capability in\nhandling complex, multi-character social interactions has yet to be fully\nexplored, primarily due to the absence of robust, quantitative evaluation\nmethods. This gap has slowed the development of agents proficient in more\nnuanced interactions beyond simple exchanges, for example, small talk. To\naddress this challenge, we introduce the Multi-Agent Interaction Evaluation\nFramework (AntEval), encompassing a novel interaction framework and evaluation\nmethods. The interaction framework aims to foster an complex interaction\nenvironment that bolsters information exchange and intention expression within\nsocial interactions. Furthermore, we introduce evaluation methods, including\ntwo metrics: Information Exchanging Precision (IEP) and Interaction\nExpressiveness Gap (IEG), designed for the quantitative and objective\nassessment of agents' interaction competencies. Our findings highlight the\nutility of these evaluative methods and show significant potential for\nimproving LLMs' ability to construct agents that interact in a more natural\nmanner with human-like intricacy.", "published": "2024-01-12 11:18:00", "link": "http://arxiv.org/abs/2401.06509v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Intention Analysis Makes LLMs A Good Jailbreak Defender", "abstract": "Aligning large language models (LLMs) with human values, particularly when\nfacing complex and stealthy jailbreak attacks, presents a formidable challenge.\nUnfortunately, existing methods often overlook this intrinsic nature of\njailbreaks, which limits their effectiveness in such complex scenarios. In this\nstudy, we present a simple yet highly effective defense strategy, i.e.,\nIntention Analysis ($\\mathbb{IA}$). $\\mathbb{IA}$ works by triggering LLMs'\ninherent self-correct and improve ability through a two-stage process: 1)\nanalyzing the essential intention of the user input, and 2) providing final\npolicy-aligned responses based on the first round conversation. Notably,\n$\\mathbb{IA}$ is an inference-only method, thus could enhance LLM safety\nwithout compromising their helpfulness. Extensive experiments on varying\njailbreak benchmarks across a wide range of LLMs show that $\\mathbb{IA}$ could\nconsistently and significantly reduce the harmfulness in responses (averagely\n-48.2% attack success rate). Encouragingly, with our $\\mathbb{IA}$, Vicuna-7B\neven outperforms GPT-3.5 regarding attack success rate. We empirically\ndemonstrate that, to some extent, $\\mathbb{IA}$ is robust to errors in\ngenerated intentions. Further analyses reveal the underlying principle of\n$\\mathbb{IA}$: suppressing LLM's tendency to follow jailbreak prompts, thereby\nenhancing safety.", "published": "2024-01-12 13:15:05", "link": "http://arxiv.org/abs/2401.06561v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prometheus-Vision: Vision-Language Model as a Judge for Fine-Grained\n  Evaluation", "abstract": "Assessing long-form responses generated by Vision-Language Models (VLMs) is\nchallenging. It not only requires checking whether the VLM follows the given\ninstruction but also verifying whether the text output is properly grounded on\nthe given image. Inspired by the recent approach of evaluating LMs with LMs, in\nthis work, we propose to evaluate VLMs with VLMs. For this purpose, we present\na new feedback dataset called the Perception Collection, encompassing 15K\ncustomized score rubrics that users might care about during assessment. Using\nthe Perception Collection, we train Prometheus-Vision, the first open-source\nVLM evaluator model that can understand the user-defined score criteria during\nevaluation. Prometheus-Vision shows the highest Pearson correlation with human\nevaluators and GPT-4V among open-source models, showing its effectiveness for\ntransparent and accessible evaluation of VLMs. We open-source our code,\ndataset, and model at https://github.com/kaistAI/prometheus-vision", "published": "2024-01-12 14:19:23", "link": "http://arxiv.org/abs/2401.06591v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mutual Enhancement of Large Language and Reinforcement Learning Models\n  through Bi-Directional Feedback Mechanisms: A Planning Case Study", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities for\nreinforcement learning (RL) models, such as planning and reasoning\ncapabilities. However, the problems of LLMs and RL model collaboration still\nneed to be solved. In this study, we employ a teacher-student learning\nframework to tackle these problems, specifically by offering feedback for LLMs\nusing RL models and providing high-level information for RL models with LLMs in\na cooperative multi-agent setting. Within this framework, the LLM acts as a\nteacher, while the RL model acts as a student. The two agents cooperatively\nassist each other through a process of recursive help, such as \"I help you help\nI help.\" The LLM agent supplies abstract information to the RL agent, enabling\nefficient exploration and policy improvement. In turn, the RL agent offers\nfeedback to the LLM agent, providing valuable, real-time information that helps\ngenerate more useful tokens. This bi-directional feedback loop promotes\noptimization, exploration, and mutual improvement for both agents, enabling\nthem to accomplish increasingly challenging tasks. Remarkably, we propose a\npractical algorithm to address the problem and conduct empirical experiments to\nevaluate the effectiveness of our method.", "published": "2024-01-12 14:35:57", "link": "http://arxiv.org/abs/2401.06603v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TransliCo: A Contrastive Learning Framework to Address the Script\n  Barrier in Multilingual Pretrained Language Models", "abstract": "The world's more than 7000 languages are written in at least 293 scripts. Due\nto various reasons, many closely related languages use different scripts, which\nposes a difficulty for multilingual pretrained language models (mPLMs) in\nlearning crosslingual knowledge through lexical overlap. As a consequence,\nmPLMs are faced with a script barrier: representations from different scripts\nare located in different subspaces, which can result in crosslingual transfer\ninvolving languages of different scripts performing suboptimally. To address\nthis problem, we propose TransliCo, a framework that optimizes the\nTransliteration Contrastive Modeling (TCM) objective to fine-tune an mPLM by\ncontrasting sentences in its training data and their transliterations in a\nunified script (in our case Latin), which enhances uniformity in the\nrepresentation space for different scripts. Using Glot500-m, an mPLM pretrained\non over 500 languages, as our source model, we fine-tune it on a small portion\n(5%) of its training data, and refer to the resulting model as Furina. We show\nthat Furina not only better aligns representations from distinct scripts but\nalso outperforms the original Glot500-m on various zero-shot crosslingual\ntransfer tasks. Additionally, we achieve consistent improvement in a case study\non the Indic group where the languages exhibit areal features but use different\nscripts. We make our code and models publicly available.", "published": "2024-01-12 15:12:48", "link": "http://arxiv.org/abs/2401.06620v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OOP: Object-Oriented Programming Evaluation Benchmark for Large Language\n  Models", "abstract": "Advancing automated programming necessitates robust and comprehensive code\ngeneration benchmarks, yet current evaluation frameworks largely neglect\nobject-oriented programming (OOP) in favor of functional programming (FP),\ne.g., HumanEval and MBPP. To address this, our study introduces a pioneering\nOOP-focused benchmark, featuring 431 Python programs that encompass essential\nOOP concepts and features like classes and encapsulation methods. We propose a\nnovel evaluation metric, pass@o, tailored for OOP, enhancing traditional pass@k\nmeasures. Our evaluation of 23 leading large language models (LLMs), including\nboth general and code-specialized models, reveals three key insights: 1) pass@o\noffers a more relevant and comprehensive assessment for OOP code generation; 2)\nDespite excelling in FP, code-specialized LLMs like WizardCoder lag in OOP\ncompared to models like ChatGPT; 3) The poor performance of all advanced LLMs\non our OOP benchmark highlights a critical need for improvements in this field.\nOur benchmark and scripts are publicly released at:\nhttps://github.com/alphadl/OOP-eval.", "published": "2024-01-12 15:21:36", "link": "http://arxiv.org/abs/2401.06628v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Effects of diversity incentives on sample diversity and downstream model\n  performance in LLM-based text augmentation", "abstract": "The latest generative large language models (LLMs) have found their\napplication in data augmentation tasks, where small numbers of text samples are\nLLM-paraphrased and then used to fine-tune downstream models. However, more\nresearch is needed to assess how different prompts, seed data selection\nstrategies, filtering methods, or model settings affect the quality of\nparaphrased data (and downstream models). In this study, we investigate three\ntext diversity incentive methods well established in crowdsourcing: taboo\nwords, hints by previous outlier solutions, and chaining on previous outlier\nsolutions. Using these incentive methods as part of instructions to LLMs\naugmenting text datasets, we measure their effects on generated texts lexical\ndiversity and downstream model performance. We compare the effects over 5\ndifferent LLMs, 6 datasets and 2 downstream models. We show that diversity is\nmost increased by taboo words, but downstream model performance is highest with\nhints.", "published": "2024-01-12 15:46:43", "link": "http://arxiv.org/abs/2401.06643v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WisdoM: Improving Multimodal Sentiment Analysis by Fusing Contextual\n  World Knowledge", "abstract": "Sentiment analysis is rapidly advancing by utilizing various data modalities\n(e.g., text, image). However, most previous works relied on superficial\ninformation, neglecting the incorporation of contextual world knowledge (e.g.,\nbackground information derived from but beyond the given image and text pairs)\nand thereby restricting their ability to achieve better multimodal sentiment\nanalysis (MSA). In this paper, we proposed a plug-in framework named WisdoM, to\nleverage the contextual world knowledge induced from the large vision-language\nmodels (LVLMs) for enhanced MSA. WisdoM utilizes LVLMs to comprehensively\nanalyze both images and corresponding texts, simultaneously generating\npertinent context. To reduce the noise in the context, we also introduce a\ntraining-free contextual fusion mechanism. Experiments across diverse\ngranularities of MSA tasks consistently demonstrate that our approach has\nsubstantial improvements (brings an average +1.96% F1 score among five advanced\nmethods) over several state-of-the-art methods.", "published": "2024-01-12 16:08:07", "link": "http://arxiv.org/abs/2401.06659v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Candidate Speculative Decoding", "abstract": "Large language models have shown impressive capabilities across a variety of\nNLP tasks, yet their generating text autoregressively is time-consuming. One\nway to speed them up is speculative decoding, which generates candidate\nsegments (a sequence of tokens) from a fast draft model that is then verified\nin parallel by the target model. However, the acceptance rate of candidate\ntokens receives limitations from several factors, such as the model, the\ndataset, and the decoding setup. This paper proposes sampling multiple\ncandidates from a draft model and then organising them in batches for\nverification. We design algorithms for efficient multi-candidate verification\nwhile maintaining the distribution of the target model. Our approach shows\nsignificant improvements in acceptance rates on multiple datasets and models,\nconsistently outperforming standard speculative decoding.", "published": "2024-01-12 17:15:23", "link": "http://arxiv.org/abs/2401.06706v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Stylometry Analysis of Multi-authored Documents for Authorship and\n  Author Style Change Detection", "abstract": "In recent years, the increasing use of Artificial Intelligence based text\ngeneration tools has posed new challenges in document provenance,\nauthentication, and authorship detection. However, advancements in stylometry\nhave provided opportunities for automatic authorship and author change\ndetection in multi-authored documents using style analysis techniques. Style\nanalysis can serve as a primary step toward document provenance and\nauthentication through authorship detection. This paper investigates three key\ntasks of style analysis: (i) classification of single and multi-authored\ndocuments, (ii) single change detection, which involves identifying the point\nwhere the author switches, and (iii) multiple author-switching detection in\nmulti-authored documents. We formulate all three tasks as classification\nproblems and propose a merit-based fusion framework that integrates several\nstate-of-the-art natural language processing (NLP) algorithms and weight\noptimization techniques. We also explore the potential of special characters,\nwhich are typically removed during pre-processing in NLP applications, on the\nperformance of the proposed methods for these tasks by conducting extensive\nexperiments on both cleaned and raw datasets. Experimental results demonstrate\nsignificant improvements over existing solutions for all three tasks on a\nbenchmark dataset.", "published": "2024-01-12 18:36:41", "link": "http://arxiv.org/abs/2401.06752v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Navigating the Metrics Maze: Reconciling Score Magnitudes and Accuracies", "abstract": "Ten years ago a single metric, BLEU, governed progress in machine translation\nresearch. For better or worse, there is no such consensus today, and\nconsequently it is difficult for researchers to develop and retain the kinds of\nheuristic intuitions about metric deltas that drove earlier research and\ndeployment decisions. This paper investigates the \"dynamic range\" of a number\nof modern metrics in an effort to provide a collective understanding of the\nmeaning of differences in scores both within and among metrics; in other words,\nwe ask what point difference X in metric Y is required between two systems for\nhumans to notice? We conduct our evaluation on a new large dataset, ToShip23,\nusing it to discover deltas at which metrics achieve system-level differences\nthat are meaningful to humans, which we measure by pairwise system accuracy. We\nadditionally show that this method of establishing delta-accuracy is more\nstable than the standard use of statistical p-values in regards to testset\nsize. Where data size permits, we also explore the effect of metric deltas and\naccuracy across finer-grained features such as translation direction, domain,\nand system closeness.", "published": "2024-01-12 18:47:40", "link": "http://arxiv.org/abs/2401.06760v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "APAR: LLMs Can Do Auto-Parallel Auto-Regressive Decoding", "abstract": "The massive adoption of large language models (LLMs) demands efficient\ndeployment strategies. However, the auto-regressive decoding process, which is\nfundamental to how most LLMs generate text, poses challenges to achieve\nefficient serving. In this work, we introduce a parallel auto-regressive\ngeneration method. By instruct-tuning on general domain data that contains\nhierarchical structures, we enable LLMs to independently plan their generation\nprocess and perform auto-parallel auto-regressive (APAR) generation,\nsignificantly reducing the number of generation steps. APAR alone can achieve\nup to 2x speed-up, and when combined with speculative decoding, the speed-up\ncan reach up to 4x. In addition, APAR reduces the key-value cache consumption\nand attention computation during generation. This leads to a throughput\nincrease of 20-70% and a latency reduce of 20-35% in high-throughput scenarios,\ncompared to state-of-the-art serving frameworks.", "published": "2024-01-12 18:50:36", "link": "http://arxiv.org/abs/2401.06761v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mind Your Format: Towards Consistent Evaluation of In-Context Learning\n  Improvements", "abstract": "Large language models demonstrate a remarkable capability for learning to\nsolve new tasks from a few examples. The prompt template, or the way the input\nexamples are formatted to obtain the prompt, is an important yet often\noverlooked aspect of in-context learning. In this work, we conduct a\ncomprehensive study of the template format's influence on the in-context\nlearning performance. We evaluate the impact of the prompt template across 21\nmodels (from 770M to 70B parameters) and 4 standard classification datasets. We\nshow that a poor choice of the template can reduce the performance of the\nstrongest models and inference methods to a random guess level. More\nimportantly, the best templates do not transfer between different setups and\neven between models of the same family. Our findings show that the currently\nprevalent approach to evaluation, which ignores template selection, may give\nmisleading results due to different templates in different works. As a first\nstep towards mitigating this issue, we propose Template Ensembles that\naggregate model predictions across several templates. This simple test-time\naugmentation boosts average performance while being robust to the choice of\nrandom set of templates.", "published": "2024-01-12 18:58:26", "link": "http://arxiv.org/abs/2401.06766v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Machine Translation Models are Zero-Shot Detectors of Translation\n  Direction", "abstract": "Detecting the translation direction of parallel text has applications for\nmachine translation training and evaluation, but also has forensic applications\nsuch as resolving plagiarism or forgery allegations. In this work, we explore\nan unsupervised approach to translation direction detection based on the simple\nhypothesis that\n$p(\\text{translation}|\\text{original})>p(\\text{original}|\\text{translation})$,\nmotivated by the well-known simplification effect in translationese or\nmachine-translationese. In experiments with massively multilingual machine\ntranslation models across 20 translation directions, we confirm the\neffectiveness of the approach for high-resource language pairs, achieving\ndocument-level accuracies of 82--96% for NMT-produced translations, and 60--81%\nfor human translations, depending on the model used. Code and demo are\navailable at https://github.com/ZurichNLP/translation-direction-detection", "published": "2024-01-12 18:59:02", "link": "http://arxiv.org/abs/2401.06769v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MAPO: Advancing Multilingual Reasoning through Multilingual\n  Alignment-as-Preference Optimization", "abstract": "Though reasoning abilities are considered language-agnostic, existing LLMs\nexhibit inconsistent reasoning abilities across different languages, e.g.,\nreasoning in the dominant language like English is superior to other languages\ndue to the imbalance of multilingual training data. To enhance reasoning\nabilities in non-dominant languages, we propose a\nMultilingual-Alignment-as-Preference Optimization framework (MAPO), aiming to\nalign the reasoning processes in other languages with the dominant language.\nSpecifically, we harness an off-the-shelf translation model for the consistency\nbetween answers in non-dominant and dominant languages, which we adopt as the\npreference for optimization, e.g., Direct Preference Optimization (DPO) or\nProximal Policy Optimization (PPO). Experiments show that MAPO stably achieves\nsignificant improvements in the multilingual reasoning of various models on all\nthree benchmarks (MSVAMP +16.2%, MGSM +6.1%, and MNumGLUESub +13.3%), with\nimproved reasoning consistency across languages.", "published": "2024-01-12 18:03:54", "link": "http://arxiv.org/abs/2401.06838v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models Can Learn Temporal Reasoning", "abstract": "While large language models (LLMs) have demonstrated remarkable reasoning\ncapabilities, they are not without their flaws and inaccuracies. Recent studies\nhave introduced various methods to mitigate these limitations. Temporal\nreasoning (TR), in particular, presents a significant challenge for LLMs due to\nits reliance on diverse temporal concepts and intricate temporal logic. In this\npaper, we propose TG-LLM, a novel framework towards language-based TR. Instead\nof reasoning over the original context, we adopt a latent representation,\ntemporal graph (TG) that enhances the learning of TR. A synthetic dataset\n(TGQA), which is fully controllable and requires minimal supervision, is\nconstructed for fine-tuning LLMs on this text-to-TG translation task. We\nconfirmed in experiments that the capability of TG translation learned on our\ndataset can be transferred to other TR tasks and benchmarks. On top of that, we\nteach LLM to perform deliberate reasoning over the TGs via Chain-of-Thought\n(CoT) bootstrapping and graph data augmentation. We observed that those\nstrategies, which maintain a balance between usefulness and diversity, bring\nmore reliable CoTs and final results than the vanilla CoT distillation.", "published": "2024-01-12 19:00:26", "link": "http://arxiv.org/abs/2401.06853v6", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-grained Hallucination Detection and Editing for Language Models", "abstract": "Large language models (LMs) are prone to generate factual errors, which are\noften called hallucinations. In this paper, we introduce a comprehensive\ntaxonomy of hallucinations and argue that hallucinations manifest in diverse\nforms, each requiring varying degrees of careful assessments to verify\nfactuality. We propose a novel task of automatic fine-grained hallucination\ndetection and construct a new evaluation benchmark, FavaBench, that includes\nabout one thousand fine-grained human judgments on three LM outputs across\nvarious domains. Our analysis reveals that ChatGPT and Llama2-Chat (70B, 7B)\nexhibit diverse types of hallucinations in the majority of their outputs in\ninformation-seeking scenarios. We train FAVA, a retrieval-augmented LM by\ncarefully creating synthetic data to detect and correct fine-grained\nhallucinations. On our benchmark, our automatic and human evaluations show that\nFAVA significantly outperforms ChatGPT and GPT-4 on fine-grained hallucination\ndetection, and edits suggested by FAVA improve the factuality of LM-generated\ntext.", "published": "2024-01-12 19:02:48", "link": "http://arxiv.org/abs/2401.06855v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Promptly Predicting Structures: The Return of Inference", "abstract": "Prompt-based methods have been used extensively across NLP to build zero- and\nfew-shot label predictors. Many NLP tasks are naturally structured: that is,\ntheir outputs consist of multiple labels which constrain each other. Annotating\ndata for such tasks can be cumbersome. Can the promise of the prompt-based\nparadigm be extended to such structured outputs? In this paper, we present a\nframework for constructing zero- and few-shot linguistic structure predictors.\nOur key insight is that we can use structural constraints -- and combinatorial\ninference derived from them -- to filter out inconsistent structures predicted\nby large language models. We instantiated this framework on two structured\nprediction tasks, and five datasets. Across all cases, our results show that\nenforcing consistency not only constructs structurally valid outputs, but also\nimproves performance over the unconstrained variants.", "published": "2024-01-12 20:08:39", "link": "http://arxiv.org/abs/2401.06877v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparing GPT-4 and Open-Source Language Models in Misinformation\n  Mitigation", "abstract": "Recent large language models (LLMs) have been shown to be effective for\nmisinformation detection. However, the choice of LLMs for experiments varies\nwidely, leading to uncertain conclusions. In particular, GPT-4 is known to be\nstrong in this domain, but it is closed source, potentially expensive, and can\nshow instability between different versions. Meanwhile, alternative LLMs have\ngiven mixed results. In this work, we show that Zephyr-7b presents a\nconsistently viable alternative, overcoming key limitations of commonly used\napproaches like Llama-2 and GPT-3.5. This provides the research community with\na solid open-source option and shows open-source models are gradually catching\nup on this task. We then highlight how GPT-3.5 exhibits unstable performance,\nsuch that this very widely used model could provide misleading results in\nmisinformation detection. Finally, we validate new tools including approaches\nto structured output and the latest version of GPT-4 (Turbo), showing they do\nnot compromise performance, thus unlocking them for future research and\npotentially enabling more complex pipelines for misinformation mitigation.", "published": "2024-01-12 22:27:25", "link": "http://arxiv.org/abs/2401.06920v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PizzaCommonSense: Learning to Model Commonsense Reasoning about\n  Intermediate Steps in Cooking Recipes", "abstract": "Understanding procedural texts, such as cooking recipes, is essential for\nenabling machines to follow instructions and reason about tasks, a key aspect\nof intelligent reasoning. In cooking, these instructions can be interpreted as\na series of modifications to a food preparation. For a model to effectively\nreason about cooking recipes, it must accurately discern and understand the\ninputs and outputs of intermediate steps within the recipe. We present a new\ncorpus of cooking recipes enriched with descriptions of intermediate steps that\ndescribe the input and output for each step. PizzaCommonsense serves as a\nbenchmark for the reasoning capabilities of LLMs because it demands rigorous\nexplicit input-output descriptions to demonstrate the acquisition of implicit\ncommonsense knowledge, which is unlikely to be easily memorized. GPT-4 achieves\nonly 26\\% human-evaluated preference for generations, leaving room for future\nimprovements.", "published": "2024-01-12 23:33:01", "link": "http://arxiv.org/abs/2401.06930v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-shot Generative Large Language Models for Systematic Review\n  Screening Automation", "abstract": "Systematic reviews are crucial for evidence-based medicine as they\ncomprehensively analyse published research findings on specific questions.\nConducting such reviews is often resource- and time-intensive, especially in\nthe screening phase, where abstracts of publications are assessed for inclusion\nin a review. This study investigates the effectiveness of using zero-shot large\nlanguage models~(LLMs) for automatic screening. We evaluate the effectiveness\nof eight different LLMs and investigate a calibration technique that uses a\npredefined recall threshold to determine whether a publication should be\nincluded in a systematic review. Our comprehensive evaluation using five\nstandard test collections shows that instruction fine-tuning plays an important\nrole in screening, that calibration renders LLMs practical for achieving a\ntargeted recall, and that combining both with an ensemble of zero-shot models\nsaves significant screening time compared to state-of-the-art approaches.", "published": "2024-01-12 01:54:08", "link": "http://arxiv.org/abs/2401.06320v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to\n  Challenge AI Safety by Humanizing LLMs", "abstract": "Most traditional AI safety research has approached AI models as machines and\ncentered on algorithm-focused attacks developed by security experts. As large\nlanguage models (LLMs) become increasingly common and competent, non-expert\nusers can also impose risks during daily interactions. This paper introduces a\nnew perspective to jailbreak LLMs as human-like communicators, to explore this\noverlooked intersection between everyday language interaction and AI safety.\nSpecifically, we study how to persuade LLMs to jailbreak them. First, we\npropose a persuasion taxonomy derived from decades of social science research.\nThen, we apply the taxonomy to automatically generate interpretable persuasive\nadversarial prompts (PAP) to jailbreak LLMs. Results show that persuasion\nsignificantly increases the jailbreak performance across all risk categories:\nPAP consistently achieves an attack success rate of over $92\\%$ on Llama 2-7b\nChat, GPT-3.5, and GPT-4 in $10$ trials, surpassing recent algorithm-focused\nattacks. On the defense side, we explore various mechanisms against PAP and,\nfound a significant gap in existing defenses, and advocate for more fundamental\nmitigation for highly interactive LLMs", "published": "2024-01-12 16:13:24", "link": "http://arxiv.org/abs/2401.06373v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Adaptive Data Augmentation for Aspect Sentiment Quad Prediction", "abstract": "Aspect sentiment quad prediction (ASQP) aims to predict the quad sentiment\nelements for a given sentence, which is a critical task in the field of\naspect-based sentiment analysis. However, the data imbalance issue has not\nreceived sufficient attention in ASQP task. In this paper, we divide the issue\ninto two-folds, quad-pattern imbalance and aspect-category imbalance, and\npropose an Adaptive Data Augmentation (ADA) framework to tackle the imbalance\nissue. Specifically, a data augmentation process with a condition function\nadaptively enhances the tail quad patterns and aspect categories, alleviating\nthe data imbalance in ASQP. Following previous studies, we also further explore\nthe generative framework for extracting complete quads by introducing the\ncategory prior knowledge and syntax-guided decoding target. Experimental\nresults demonstrate that data augmentation for imbalance in ASQP task can\nimprove the performance, and the proposed ADA method is superior to naive data\noversampling.", "published": "2024-01-12 06:20:56", "link": "http://arxiv.org/abs/2401.06394v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Generalizing Visual Question Answering from Synthetic to Human-Written\n  Questions via a Chain of QA with a Large Language Model", "abstract": "Visual question answering (VQA) is a task where an image is given, and a\nseries of questions are asked about the image. To build an efficient VQA\nalgorithm, a large amount of QA data is required which is very expensive.\nGenerating synthetic QA pairs based on templates is a practical way to obtain\ndata. However, VQA models trained on those data do not perform well on complex,\nhuman-written questions. To address this issue, we propose a new method called\n{\\it chain of QA for human-written questions} (CoQAH). CoQAH utilizes a\nsequence of QA interactions between a large language model and a VQA model\ntrained on synthetic data to reason and derive logical answers for\nhuman-written questions. We tested the effectiveness of CoQAH on two types of\nhuman-written VQA datasets for 3D-rendered and chest X-ray images and found\nthat it achieved state-of-the-art accuracy in both types of data. Notably,\nCoQAH outperformed general vision-language models, VQA models, and medical\nfoundation models with no finetuning.", "published": "2024-01-12 06:49:49", "link": "http://arxiv.org/abs/2401.06400v3", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Human-AI Collaborative Essay Scoring: A Dual-Process Framework with LLMs", "abstract": "Receiving timely and personalized feedback is essential for second-language\nlearners, especially when human instructors are unavailable. This study\nexplores the effectiveness of Large Language Models (LLMs), including both\nproprietary and open-source models, for Automated Essay Scoring (AES). Through\nextensive experiments with public and private datasets, we find that while LLMs\ndo not surpass conventional state-of-the-art (SOTA) grading models in\nperformance, they exhibit notable consistency, generalizability, and\nexplainability. We propose an open-source LLM-based AES system, inspired by the\ndual-process theory. Our system offers accurate grading and high-quality\nfeedback, at least comparable to that of fine-tuned proprietary LLMs, in\naddition to its ability to alleviate misgrading. Furthermore, we conduct\nhuman-AI co-grading experiments with both novice and expert graders. We find\nthat our system not only automates the grading process but also enhances the\nperformance and efficiency of human graders, particularly for essays where the\nmodel has lower confidence. These results highlight the potential of LLMs to\nfacilitate effective human-AI collaboration in the educational context,\npotentially transforming learning experiences through AI-generated feedback.", "published": "2024-01-12 07:50:10", "link": "http://arxiv.org/abs/2401.06431v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PersianMind: A Cross-Lingual Persian-English Large Language Model", "abstract": "Large language models demonstrate remarkable proficiency in various\nlinguistic tasks and have extensive knowledge across various domains. Although\nthey perform best in English, their ability in other languages is notable too.\nIn contrast, open-source models, such as LLaMa, are primarily trained on\nEnglish datasets, resulting in poor performance in non-English languages. In\nthis paper, we introduce PersianMind, an open-source bilingual large language\nmodel which demonstrates comparable performance to closed-source GPT-3.5-turbo\nin the Persian language. By expanding LLaMa2's vocabulary with 10,000 Persian\ntokens and training it on a dataset comprising nearly 2 billion Persian tokens,\nwe show that our approach preserves the model's English knowledge and employs\ntransfer learning to excel at transferring task knowledge from one language to\nanother.", "published": "2024-01-12 09:24:10", "link": "http://arxiv.org/abs/2401.06466v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Batch-ICL: Effective, Efficient, and Order-Agnostic In-Context Learning", "abstract": "In this paper, by treating in-context learning (ICL) as a meta-optimization\nprocess, we explain why LLMs are sensitive to the order of ICL examples. This\nunderstanding leads us to the development of Batch-ICL, an effective,\nefficient, and order-agnostic inference algorithm for ICL. Differing from the\nstandard N-shot learning approach, Batch-ICL employs $N$ separate 1-shot\nforward computations and aggregates the resulting meta-gradients. These\naggregated meta-gradients are then applied to the forward computation of a\nzero-shot query to generate the final prediction. This batch processing\napproach renders the LLM agnostic to the order of ICL examples. Through\nextensive experiments and analysis, we demonstrate that Batch-ICL consistently\noutperforms most permutations of ICL examples. In some cases, it even exceeds\nthe performance of the best order for standard ICL, all while reducing the\ncomputational resources required. Furthermore, we develop a novel variant of\nBatch-ICL featuring multiple \"epochs\" of meta-optimization. This variant\nimplicitly explores permutations of ICL examples, further enhancing ICL\nperformance.", "published": "2024-01-12 09:31:17", "link": "http://arxiv.org/abs/2401.06469v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Kun: Answer Polishment for Chinese Self-Alignment with Instruction\n  Back-Translation", "abstract": "In this paper, we introduce Kun, a novel approach for creating high-quality\ninstruction-tuning datasets for large language models (LLMs) without relying on\nmanual annotations. Adapting a self-training algorithm based on instruction\nback-translation and answer polishment, Kun leverages unlabelled data from\ndiverse sources such as Wudao, Wanjuan, and SkyPile to generate a substantial\ndataset of over a million Chinese instructional data points. This approach\nsignificantly deviates from traditional methods by using a self-curation\nprocess to refine and select the most effective instruction-output pairs. Our\nexperiments with the 6B-parameter Yi model across various benchmarks\ndemonstrate Kun's robustness and scalability. Our method's core contributions\nlie in its algorithmic advancement, which enhances data retention and clarity,\nand its innovative data generation approach that substantially reduces the\nreliance on costly and time-consuming manual annotations. This methodology\npresents a scalable and efficient solution for improving the\ninstruction-following capabilities of LLMs, with significant implications for\ntheir application across diverse fields. The code and dataset can be found at\nhttps://github.com/Zheng0428/COIG-Kun", "published": "2024-01-12 09:56:57", "link": "http://arxiv.org/abs/2401.06477v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MetaHate: A Dataset for Unifying Efforts on Hate Speech Detection", "abstract": "Hate speech represents a pervasive and detrimental form of online discourse,\noften manifested through an array of slurs, from hateful tweets to defamatory\nposts. As such speech proliferates, it connects people globally and poses\nsignificant social, psychological, and occasionally physical threats to\ntargeted individuals and communities. Current computational linguistic\napproaches for tackling this phenomenon rely on labelled social media datasets\nfor training. For unifying efforts, our study advances in the critical need for\na comprehensive meta-collection, advocating for an extensive dataset to help\ncounteract this problem effectively. We scrutinized over 60 datasets,\nselectively integrating those pertinent into MetaHate. This paper offers a\ndetailed examination of existing collections, highlighting their strengths and\nlimitations. Our findings contribute to a deeper understanding of the existing\ndatasets, paving the way for training more robust and adaptable models. These\nenhanced models are essential for effectively combating the dynamic and complex\nnature of hate speech in the digital realm.", "published": "2024-01-12 11:54:53", "link": "http://arxiv.org/abs/2401.06526v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "INTERS: Unlocking the Power of Large Language Models in Search with\n  Instruction Tuning", "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in\nvarious natural language processing tasks. Despite this, their application to\ninformation retrieval (IR) tasks is still challenging due to the infrequent\noccurrence of many IR-specific concepts in natural language. While prompt-based\nmethods can provide task descriptions to LLMs, they often fall short in\nfacilitating a comprehensive understanding and execution of IR tasks, thereby\nlimiting LLMs' applicability. To address this gap, in this work, we explore the\npotential of instruction tuning to enhance LLMs' proficiency in IR tasks. We\nintroduce a novel instruction tuning dataset, INTERS, encompassing 20 tasks\nacross three fundamental IR categories: query understanding, document\nunderstanding, and query-document relationship understanding. The data are\nderived from 43 distinct datasets with manually written templates. Our\nempirical results reveal that INTERS significantly boosts the performance of\nvarious publicly available LLMs, such as LLaMA, Mistral, and Phi, in IR tasks.\nFurthermore, we conduct extensive experiments to analyze the effects of\ninstruction design, template diversity, few-shot demonstrations, and the volume\nof instructions on performance. We make our dataset and the fine-tuned models\npublicly accessible at https://github.com/DaoD/INTERS.", "published": "2024-01-12 12:10:28", "link": "http://arxiv.org/abs/2401.06532v3", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Medical Dialogue Generation via Intuitive-then-Analytical Differential\n  Diagnosis", "abstract": "Medical dialogue systems have attracted growing research attention as they\nhave the potential to provide rapid diagnoses, treatment plans, and health\nconsultations. In medical dialogues, a proper diagnosis is crucial as it\nestablishes the foundation for future consultations. Clinicians typically\nemploy both intuitive and analytic reasoning to formulate a differential\ndiagnosis. This reasoning process hypothesizes and verifies a variety of\npossible diseases and strives to generate a comprehensive and rigorous\ndiagnosis. However, recent studies on medical dialogue generation have\noverlooked the significance of modeling a differential diagnosis, which hinders\nthe practical application of these systems. To address the above issue, we\npropose a medical dialogue generation framework with the\nIntuitive-then-Analytic Differential Diagnosis (IADDx). Our method starts with\na differential diagnosis via retrieval-based intuitive association and\nsubsequently refines it through a graph-enhanced analytic procedure. The\nresulting differential diagnosis is then used to retrieve medical knowledge and\nguide response generation. Experimental results on two datasets validate the\nefficacy of our method. Besides, we demonstrate how our framework assists both\nclinicians and patients in understanding the diagnostic process, for instance,\nby producing intermediate results and graph-based diagnosis paths.", "published": "2024-01-12 12:35:19", "link": "http://arxiv.org/abs/2401.06541v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Lost in the Source Language: How Large Language Models Evaluate the\n  Quality of Machine Translation", "abstract": "This study investigates how Large Language Models (LLMs) leverage source and\nreference data in machine translation evaluation task, aiming to better\nunderstand the mechanisms behind their remarkable performance in this task. We\ndesign the controlled experiments across various input modes and model types,\nand employ both coarse-grained and fine-grained prompts to discern the utility\nof source versus reference information. We find that reference information\nsignificantly enhances the evaluation accuracy, while surprisingly, source\ninformation sometimes is counterproductive, indicating LLMs' inability to fully\nleverage the cross-lingual capability when evaluating translations. Further\nanalysis of the fine-grained evaluation and fine-tuning experiments show\nsimilar results. These findings also suggest a potential research direction for\nLLMs that fully exploits the cross-lingual capability of LLMs to achieve better\nperformance in machine translation evaluation tasks.", "published": "2024-01-12 13:23:21", "link": "http://arxiv.org/abs/2401.06568v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Experimental Contexts Can Facilitate Robust Semantic Property Inference\n  in Language Models, but Inconsistently", "abstract": "Recent zero-shot evaluations have highlighted important limitations in the\nabilities of language models (LMs) to perform meaning extraction. However, it\nis now well known that LMs can demonstrate radical improvements in the presence\nof experimental contexts such as in-context examples and instructions. How well\ndoes this translate to previously studied meaning-sensitive tasks? We present a\ncase-study on the extent to which experimental contexts can improve LMs'\nrobustness in performing property inheritance -- predicting semantic properties\nof novel concepts, a task that they have been previously shown to fail on. Upon\ncarefully controlling the nature of the in-context examples and the\ninstructions, our work reveals that they can indeed lead to non-trivial\nproperty inheritance behavior in LMs. However, this ability is inconsistent:\nwith a minimal reformulation of the task, some LMs were found to pick up on\nshallow, non-semantic heuristics from their inputs, suggesting that the\ncomputational principles of semantic property inference are yet to be mastered\nby LMs.", "published": "2024-01-12 15:40:31", "link": "http://arxiv.org/abs/2401.06640v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Don't Rank, Combine! Combining Machine Translation Hypotheses Using\n  Quality Estimation", "abstract": "Neural machine translation systems estimate probabilities of target sentences\ngiven source sentences, yet these estimates may not align with human\npreferences. This work introduces QE-fusion, a method that synthesizes\ntranslations using a quality estimation metric (QE), which correlates better\nwith human judgments. QE-fusion leverages a pool of candidates sampled from a\nmodel, combining spans from different candidates using a QE metric such as\nCometKiwi. We compare QE-fusion against beam search and recent reranking\ntechniques, such as Minimum Bayes Risk decoding or QE-reranking. Our method\nconsistently improves translation quality in terms of COMET and BLEURT scores\nwhen applied to large language models (LLMs) used for translation (PolyLM,\nXGLM, Llama2, Mistral, ALMA, and Tower) and to multilingual translation models\n(NLLB), over five language pairs. Notably, QE-fusion exhibits larger\nimprovements for LLMs due to their ability to generate diverse outputs. We\ndemonstrate that our approach generates novel translations in over half of the\ncases and consistently outperforms other methods across varying numbers of\ncandidates (5-200). Furthermore, we empirically establish that QE-fusion scales\nlinearly with the number of candidates in the pool.", "published": "2024-01-12 16:52:41", "link": "http://arxiv.org/abs/2401.06688v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Reliability Analysis of Psychological Concept Extraction and\n  Classification in User-penned Text", "abstract": "The social NLP research community witness a recent surge in the computational\nadvancements of mental health analysis to build responsible AI models for a\ncomplex interplay between language use and self-perception. Such responsible AI\nmodels aid in quantifying the psychological concepts from user-penned texts on\nsocial media. On thinking beyond the low-level (classification) task, we\nadvance the existing binary classification dataset, towards a higher-level task\nof reliability analysis through the lens of explanations, posing it as one of\nthe safety measures. We annotate the LoST dataset to capture nuanced textual\ncues that suggest the presence of low self-esteem in the posts of Reddit users.\nWe further state that the NLP models developed for determining the presence of\nlow self-esteem, focus more on three types of textual cues: (i) Trigger: words\nthat triggers mental disturbance, (ii) LoST indicators: text indicators\nemphasizing low self-esteem, and (iii) Consequences: words describing the\nconsequences of mental disturbance. We implement existing classifiers to\nexamine the attention mechanism in pre-trained language models (PLMs) for a\ndomain-specific psychology-grounded task. Our findings suggest the need of\nshifting the focus of PLMs from Trigger and Consequences to a more\ncomprehensive explanation, emphasizing LoST indicators while determining low\nself-esteem in Reddit posts.", "published": "2024-01-12 17:19:14", "link": "http://arxiv.org/abs/2401.06709v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Few-Shot Detection of Machine-Generated Text using Style Representations", "abstract": "The advent of instruction-tuned language models that convincingly mimic human\nwriting poses a significant risk of abuse. However, such abuse may be\ncounteracted with the ability to detect whether a piece of text was composed by\na language model rather than a human author. Some previous approaches to this\nproblem have relied on supervised methods by training on corpora of confirmed\nhuman- and machine- written documents. Unfortunately, model under-specification\nposes an unavoidable challenge for neural network-based detectors, making them\nbrittle in the face of data shifts, such as the release of newer language\nmodels producing still more fluent text than the models used to train the\ndetectors. Other approaches require access to the models that may have\ngenerated a document in question, which is often impractical. In light of these\nchallenges, we pursue a fundamentally different approach not relying on samples\nfrom language models of concern at training time. Instead, we propose to\nleverage representations of writing style estimated from human-authored text.\nIndeed, we find that features effective at distinguishing among human authors\nare also effective at distinguishing human from machine authors, including\nstate-of-the-art large language models like Llama-2, ChatGPT, and GPT-4.\nFurthermore, given a handful of examples composed by each of several specific\nlanguage models of interest, our approach affords the ability to predict which\nmodel generated a given document. The code and data to reproduce our\nexperiments are available at\nhttps://github.com/LLNL/LUAR/tree/main/fewshot_iclr2024.", "published": "2024-01-12 17:26:51", "link": "http://arxiv.org/abs/2401.06712v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Reframing Tax Law Entailment as Analogical Reasoning", "abstract": "Statutory reasoning refers to the application of legislative provisions to a\nseries of case facts described in natural language. We re-frame statutory\nreasoning as an analogy task, where each instance of the analogy task involves\na combination of two instances of statutory reasoning. This increases the\ndataset size by two orders of magnitude, and introduces an element of\ninterpretability. We show that this task is roughly as difficult to Natural\nLanguage Processing models as the original task. Finally, we come back to\nstatutory reasoning, solving it with a combination of a retrieval mechanism and\nanalogy models, and showing some progress on prior comparable work.", "published": "2024-01-12 17:37:07", "link": "http://arxiv.org/abs/2401.06715v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Using Natural Language Inference to Improve Persona Extraction from\n  Dialogue in a New Domain", "abstract": "While valuable datasets such as PersonaChat provide a foundation for training\npersona-grounded dialogue agents, they lack diversity in conversational and\nnarrative settings, primarily existing in the \"real\" world. To develop dialogue\nagents with unique personas, models are trained to converse given a specific\npersona, but hand-crafting these persona can be time-consuming, thus methods\nexist to automatically extract persona information from existing\ncharacter-specific dialogue. However, these persona-extraction models are also\ntrained on datasets derived from PersonaChat and struggle to provide\nhigh-quality persona information from conversational settings that do not take\nplace in the real world, such as the fantasy-focused dataset, LIGHT. Creating\nnew data to train models on a specific setting is human-intensive, thus\nprohibitively expensive. To address both these issues, we introduce a natural\nlanguage inference method for post-hoc adapting a trained persona extraction\nmodel to a new setting. We draw inspiration from the literature of dialog\nnatural language inference (NLI), and devise NLI-reranking methods to extract\nstructured persona information from dialogue. Compared to existing persona\nextraction models, our method returns higher-quality extracted persona and\nrequires less human annotation.", "published": "2024-01-12 18:25:03", "link": "http://arxiv.org/abs/2401.06742v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Revisiting Jailbreaking for Large Language Models: A Representation\n  Engineering Perspective", "abstract": "The recent surge in jailbreaking attacks has revealed significant\nvulnerabilities in Large Language Models (LLMs) when exposed to malicious\ninputs. While various defense strategies have been proposed to mitigate these\nthreats, there has been limited research into the underlying mechanisms that\nmake LLMs vulnerable to such attacks. In this study, we suggest that the\nself-safeguarding capability of LLMs is linked to specific activity patterns\nwithin their representation space. Although these patterns have little impact\non the semantic content of the generated text, they play a crucial role in\nshaping LLM behavior under jailbreaking attacks. Our findings demonstrate that\nthese patterns can be detected with just a few pairs of contrastive queries.\nExtensive experimentation shows that the robustness of LLMs against\njailbreaking can be manipulated by weakening or strengthening these patterns.\nFurther visual analysis provides additional evidence for our conclusions,\nproviding new insights into the jailbreaking phenomenon. These findings\nhighlight the importance of addressing the potential misuse of open-source LLMs\nwithin the community.", "published": "2024-01-12 00:50:04", "link": "http://arxiv.org/abs/2401.06824v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Cross-Attention Watermarking of Large Language Models", "abstract": "A new approach to linguistic watermarking of language models is presented in\nwhich information is imperceptibly inserted into the output text while\npreserving its readability and original meaning. A cross-attention mechanism is\nused to embed watermarks in the text during inference. Two methods using\ncross-attention are presented that minimize the effect of watermarking on the\nperformance of a pretrained model. Exploration of different training strategies\nfor optimizing the watermarking and of the challenges and implications of\napplying this approach in real-world scenarios clarified the tradeoff between\nwatermark robustness and text quality. Watermark selection substantially\naffects the generated output for high entropy sentences. This proactive\nwatermarking approach has potential application in future model development.", "published": "2024-01-12 09:39:50", "link": "http://arxiv.org/abs/2401.06829v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Survey on the Applications of Frontier AI, Foundation Models, and\n  Large Language Models to Intelligent Transportation Systems", "abstract": "This survey paper explores the transformative influence of frontier AI,\nfoundation models, and Large Language Models (LLMs) in the realm of Intelligent\nTransportation Systems (ITS), emphasizing their integral role in advancing\ntransportation intelligence, optimizing traffic management, and contributing to\nthe realization of smart cities. Frontier AI refers to the forefront of AI\ntechnology, encompassing the latest advancements, innovations, and experimental\ntechniques in the field, especially AI foundation models and LLMs. Foundation\nmodels, like GPT-4, are large, general-purpose AI models that provide a base\nfor a wide range of applications. They are characterized by their versatility\nand scalability. LLMs are obtained from finetuning foundation models with a\nspecific focus on processing and generating natural language. They excel in\ntasks like language understanding, text generation, translation, and\nsummarization. By leveraging vast textual data, including traffic reports and\nsocial media interactions, LLMs extract critical insights, fostering the\nevolution of ITS. The survey navigates the dynamic synergy between LLMs and\nITS, delving into applications in traffic management, integration into\nautonomous vehicles, and their role in shaping smart cities. It provides\ninsights into ongoing research, innovations, and emerging trends, aiming to\ninspire collaboration at the intersection of language, intelligence, and\nmobility for safer, more efficient, and sustainable transportation systems. The\npaper further surveys interactions between LLMs and various aspects of ITS,\nexploring roles in traffic management, facilitating autonomous vehicles, and\ncontributing to smart city development, while addressing challenges brought by\nfrontier AI and foundation models. This paper offers valuable inspiration for\nfuture research and innovation in the transformative domain of intelligent\ntransportation.", "published": "2024-01-12 10:29:48", "link": "http://arxiv.org/abs/2401.06831v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing Emotional Generation Capability of Large Language Models via\n  Emotional Chain-of-Thought", "abstract": "Large Language Models (LLMs) have shown remarkable performance in various\nemotion recognition tasks, thereby piquing the research community's curiosity\nfor exploring their potential in emotional intelligence. However, several\nissues in the field of emotional generation tasks remain unresolved, including\nhuman preference alignment and emotional generation assessment. In this paper,\nwe propose the Emotional Chain-of-Thought (ECoT), a plug-and-play prompting\nmethod that enhances the performance of LLMs on various emotional generation\ntasks by aligning with human emotional intelligence guidelines. To assess the\nreliability of ECoT, we propose an automated model-based evaluation method\ncalled Emotional Generation Score (EGS). EGS incorporates Goleman's Emotional\nIntelligence Theory as a consensus of human experts, providing a new\nperspective on the evaluation of emotional generation tasks. Extensive\nexperimental results demonstrate the effectiveness of ECoT and EGS. Further, we\ndiscuss the promise of LLMs in the field of emotional intelligence and present\nkey insights into the LLMs with the ECoT in emotional generation tasks.", "published": "2024-01-12 16:42:10", "link": "http://arxiv.org/abs/2401.06836v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Structsum Generation for Faster Text Comprehension", "abstract": "We consider the task of generating structured representations of text using\nlarge language models (LLMs). We focus on tables and mind maps as\nrepresentative modalities. Tables are more organized way of representing data,\nwhile mind maps provide a visually dynamic and flexible approach, particularly\nsuitable for sparse content. Despite the effectiveness of LLMs on different\ntasks, we show that current models struggle with generating structured outputs.\nIn response, we present effective prompting strategies for both of these tasks.\nWe introduce a taxonomy of problems around factuality, global and local\nstructure, common to both modalities and propose a set of critiques to tackle\nthese issues resulting in an absolute improvement in accuracy of +37pp (79%)\nfor mind maps and +15pp (78%) for tables. To evaluate semantic coverage of\ngenerated structured representations we propose Auto-QA, and we verify the\nadequacy of Auto-QA using SQuAD dataset. We further evaluate the usefulness of\nstructured representations via a text comprehension user study. The results\nshow a significant reduction in comprehension time compared to text when using\ntable (42.9%) and mind map (31.9%), without loss in accuracy.", "published": "2024-01-12 17:43:51", "link": "http://arxiv.org/abs/2401.06837v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DocFinQA: A Long-Context Financial Reasoning Dataset", "abstract": "For large language models (LLMs) to be effective in the financial domain --\nwhere each decision can have a significant impact -- it is necessary to\ninvestigate realistic tasks and data. Financial professionals often interact\nwith documents that are hundreds of pages long, but most financial research\ndatasets only deal with short excerpts from these documents. To address this,\nwe introduce a long-document financial QA task. We augment 7,437 questions from\nthe existing FinQA dataset with the full-document context, extending the\naverage context length from under 700 words in FinQA to 123k words in DocFinQA.\nWe conduct extensive experiments over retrieval-based QA pipelines and\nlong-context language models. DocFinQA proves a significant challenge for even\nstate-of-the-art systems. We also provide a case-study on the longest documents\nin DocFinQA and find that models particularly struggle on these documents.\nAddressing these challenges may have a wide reaching impact across applications\nwhere specificity and long-range contexts are critical, like gene sequences and\nlegal document contract analysis.", "published": "2024-01-12 22:19:22", "link": "http://arxiv.org/abs/2401.06915v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ViSAGe: A Global-Scale Analysis of Visual Stereotypes in Text-to-Image\n  Generation", "abstract": "Recent studies have shown that Text-to-Image (T2I) model generations can\nreflect social stereotypes present in the real world. However, existing\napproaches for evaluating stereotypes have a noticeable lack of coverage of\nglobal identity groups and their associated stereotypes. To address this gap,\nwe introduce the ViSAGe (Visual Stereotypes Around the Globe) dataset to enable\nthe evaluation of known nationality-based stereotypes in T2I models, across 135\nnationalities. We enrich an existing textual stereotype resource by\ndistinguishing between stereotypical associations that are more likely to have\nvisual depictions, such as `sombrero', from those that are less visually\nconcrete, such as 'attractive'. We demonstrate ViSAGe's utility through a\nmulti-faceted evaluation of T2I generations. First, we show that stereotypical\nattributes in ViSAGe are thrice as likely to be present in generated images of\ncorresponding identities as compared to other attributes, and that the\noffensiveness of these depictions is especially higher for identities from\nAfrica, South America, and South East Asia. Second, we assess the stereotypical\npull of visual depictions of identity groups, which reveals how the 'default'\nrepresentations of all identity groups in ViSAGe have a pull towards\nstereotypical depictions, and that this pull is even more prominent for\nidentity groups from the Global South. CONTENT WARNING: Some examples contain\noffensive stereotypes.", "published": "2024-01-12 00:43:57", "link": "http://arxiv.org/abs/2401.06310v3", "categories": ["cs.CV", "cs.CL", "cs.CY"], "primary_category": "cs.CV"}
{"title": "What should I say? -- Interacting with AI and Natural Language\n  Interfaces", "abstract": "As Artificial Intelligence (AI) technology becomes more and more prevalent,\nit becomes increasingly important to explore how we as humans interact with AI.\nThe Human-AI Interaction (HAI) sub-field has emerged from the Human-Computer\nInteraction (HCI) field and aims to examine this very notion. Many interaction\npatterns have been implemented without fully understanding the changes in\nrequired cognition as well as the cognitive science implications of using these\nalternative interfaces that aim to be more human-like in nature. Prior research\nsuggests that theory of mind representations are crucial to successful and\neffortless communication, however very little is understood when it comes to\nhow theory of mind representations are established when interacting with AI.", "published": "2024-01-12 05:10:23", "link": "http://arxiv.org/abs/2401.06382v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "I.2.m; J.4; B.4.2"], "primary_category": "cs.HC"}
{"title": "DevEval: Evaluating Code Generation in Practical Software Projects", "abstract": "How to evaluate Large Language Models (LLMs) in code generation is an open\nquestion. Many benchmarks have been proposed but are inconsistent with\npractical software projects, e.g., unreal program distributions, insufficient\ndependencies, and small-scale project contexts. Thus, the capabilities of LLMs\nin practical projects are still unclear. In this paper, we propose a new\nbenchmark named DevEval, aligned with Developers' experiences in practical\nprojects. DevEval is collected through a rigorous pipeline, containing 2,690\nsamples from 119 practical projects and covering 10 domains. Compared to\nprevious benchmarks, DevEval aligns to practical projects in multiple\ndimensions, e.g., real program distributions, sufficient dependencies, and\nenough-scale project contexts. We assess five popular LLMs on DevEval (e.g.,\ngpt-4, gpt-3.5-turbo, CodeLLaMa, and StarCoder) and reveal their actual\nabilities in code generation. For instance, the highest Pass@1 of gpt-3.5-turbo\nonly is 42 in our experiments. We also discuss the challenges and future\ndirections of code generation in practical projects. We open-source DevEval and\nhope it can facilitate the development of code generation in practical\nprojects.", "published": "2024-01-12 06:51:30", "link": "http://arxiv.org/abs/2401.06401v4", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Mission: Impossible Language Models", "abstract": "Chomsky and others have very directly claimed that large language models\n(LLMs) are equally capable of learning languages that are possible and\nimpossible for humans to learn. However, there is very little published\nexperimental evidence to support such a claim. Here, we develop a set of\nsynthetic impossible languages of differing complexity, each designed by\nsystematically altering English data with unnatural word orders and grammar\nrules. These languages lie on an impossibility continuum: at one end are\nlanguages that are inherently impossible, such as random and irreversible\nshuffles of English words, and on the other, languages that may not be\nintuitively impossible but are often considered so in linguistics, particularly\nthose with rules based on counting word positions. We report on a wide range of\nevaluations to assess the capacity of GPT-2 small models to learn these\nuncontroversially impossible languages, and crucially, we perform these\nassessments at various stages throughout training to compare the learning\nprocess for each language. Our core finding is that GPT-2 struggles to learn\nimpossible languages when compared to English as a control, challenging the\ncore claim. More importantly, we hope our approach opens up a productive line\nof inquiry in which different LLM architectures are tested on a variety of\nimpossible languages in an effort to learn more about how LLMs can be used as\ntools for these cognitive and typological investigations.", "published": "2024-01-12 07:24:26", "link": "http://arxiv.org/abs/2401.06416v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "3D-PreMise: Can Large Language Models Generate 3D Shapes with Sharp\n  Features and Parametric Control?", "abstract": "Recent advancements in implicit 3D representations and generative models have\nmarkedly propelled the field of 3D object generation forward. However, it\nremains a significant challenge to accurately model geometries with defined\nsharp features under parametric controls, which is crucial in fields like\nindustrial design and manufacturing. To bridge this gap, we introduce a\nframework that employs Large Language Models (LLMs) to generate text-driven 3D\nshapes, manipulating 3D software via program synthesis. We present 3D-PreMise,\na dataset specifically tailored for 3D parametric modeling of industrial\nshapes, designed to explore state-of-the-art LLMs within our proposed pipeline.\nOur work reveals effective generation strategies and delves into the\nself-correction capabilities of LLMs using a visual interface. Our work\nhighlights both the potential and limitations of LLMs in 3D parametric modeling\nfor industrial applications.", "published": "2024-01-12 08:07:52", "link": "http://arxiv.org/abs/2401.06437v1", "categories": ["cs.GR", "cs.AI", "cs.CL"], "primary_category": "cs.GR"}
{"title": "Between Lines of Code: Unraveling the Distinct Patterns of Machine and\n  Human Programmers", "abstract": "Large language models have catalyzed an unprecedented wave in code\ngeneration. While achieving significant advances, they blur the distinctions\nbetween machine- and human-authored source code, causing integrity and\nauthenticity issues of software artifacts. Previous methods such as DetectGPT\nhave proven effective in discerning machine-generated texts, but they do not\nidentify and harness the unique patterns of machine-generated code. Thus, its\napplicability falters when applied to code. In this paper, we carefully study\nthe specific patterns that characterize machine- and human-authored code.\nThrough a rigorous analysis of code attributes such as lexical diversity,\nconciseness, and naturalness, we expose unique patterns inherent to each\nsource. We particularly notice that the syntactic segmentation of code is a\ncritical factor in identifying its provenance. Based on our findings, we\npropose DetectCodeGPT, a novel method for detecting machine-generated code,\nwhich improves DetectGPT by capturing the distinct stylized patterns of code.\nDiverging from conventional techniques that depend on external LLMs for\nperturbations, DetectCodeGPT perturbs the code corpus by strategically\ninserting spaces and newlines, ensuring both efficacy and efficiency.\nExperiment results show that our approach significantly outperforms\nstate-of-the-art techniques in detecting machine-generated code.", "published": "2024-01-12 09:15:20", "link": "http://arxiv.org/abs/2401.06461v5", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "An investigation of structures responsible for gender bias in BERT and\n  DistilBERT", "abstract": "In recent years, large Transformer-based Pre-trained Language Models (PLM)\nhave changed the Natural Language Processing (NLP) landscape, by pushing the\nperformance boundaries of the state-of-the-art on a wide variety of tasks.\nHowever, this performance gain goes along with an increase in complexity, and\nas a result, the size of such models (up to billions of parameters) represents\na constraint for their deployment on embedded devices or short-inference time\ntasks. To cope with this situation, compressed models emerged (e.g.\nDistilBERT), democratizing their usage in a growing number of applications that\nimpact our daily lives. A crucial issue is the fairness of the predictions made\nby both PLMs and their distilled counterparts. In this paper, we propose an\nempirical exploration of this problem by formalizing two questions: (1) Can we\nidentify the neural mechanism(s) responsible for gender bias in BERT (and by\nextension DistilBERT)? (2) Does distillation tend to accentuate or mitigate\ngender bias (e.g. is DistilBERT more prone to gender bias than its uncompressed\nversion, BERT)? Our findings are the following: (I) one cannot identify a\nspecific layer that produces bias; (II) every attention head uniformly encodes\nbias; except in the context of underrepresented classes with a high imbalance\nof the sensitive attribute; (III) this subset of heads is different as we\nre-fine tune the network; (IV) bias is more homogeneously produced by the heads\nin the distilled model.", "published": "2024-01-12 10:42:20", "link": "http://arxiv.org/abs/2401.06495v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mapping Transformer Leveraged Embeddings for Cross-Lingual Document\n  Representation", "abstract": "Recommendation systems, for documents, have become tools to find relevant\ncontent on the Web. However, these systems have limitations when it comes to\nrecommending documents in languages different from the query language, which\nmeans they might overlook resources in non-native languages. This research\nfocuses on representing documents across languages by using Transformer\nLeveraged Document Representations (TLDRs) that are mapped to a cross-lingual\ndomain. Four multilingual pre-trained transformer models (mBERT, mT5 XLM\nRoBERTa, ErnieM) were evaluated using three mapping methods across 20 language\npairs representing combinations of five selected languages of the European\nUnion. Metrics like Mate Retrieval Rate and Reciprocal Rank were used to\nmeasure the effectiveness of mapped TLDRs compared to non-mapped ones. The\nresults highlight the power of cross-lingual representations achieved through\npre-trained transformers and mapping approaches suggesting a promising\ndirection for expanding beyond language connections, between two specific\nlanguages.", "published": "2024-01-12 14:01:15", "link": "http://arxiv.org/abs/2401.06583v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PolyTOPS: Reconfigurable and Flexible Polyhedral Scheduler", "abstract": "Polyhedral techniques have been widely used for automatic code optimization\nin low-level compilers and higher-level processes. Loop optimization is central\nto this technique, and several polyhedral schedulers like Feautrier, Pluto, isl\nand Tensor Scheduler have been proposed, each of them targeting a different\narchitecture, parallelism model, or application scenario. The need for\nscenario-specific optimization is growing due to the heterogeneity of\narchitectures. One of the most critical cases is represented by NPUs (Neural\nProcessing Units) used for AI, which may require loop optimization with\ndifferent objectives. Another factor to be considered is the framework or\ncompiler in which polyhedral optimization takes place. Different scenarios,\ndepending on the target architecture, compilation environment, and application\ndomain, may require different kinds of optimization to best exploit the\narchitecture feature set.\n  We introduce a new configurable polyhedral scheduler, PolyTOPS, that can be\nadjusted to various scenarios with straightforward, high-level configurations.\nThis scheduler allows the creation of diverse scheduling strategies that can be\nboth scenario-specific (like state-of-the-art schedulers) and kernel-specific,\nbreaking the concept of a one-size-fits-all scheduler approach. PolyTOPS has\nbeen used with isl and CLooG as code generators and has been integrated in\nMindSpore AKG deep learning compiler. Experimental results in different\nscenarios show good performance: a geomean speedup of 7.66x on MindSpore (for\nthe NPU Ascend architecture) hybrid custom operators over isl scheduling, a\ngeomean speedup up to 1.80x on PolyBench on different multicore architectures\nover Pluto scheduling. Finally, some comparisons with different\nstate-of-the-art tools are presented in the PolyMage scenario.", "published": "2024-01-12 16:11:27", "link": "http://arxiv.org/abs/2401.06665v1", "categories": ["cs.DC", "cs.CL", "cs.PF"], "primary_category": "cs.DC"}
{"title": "DQNC2S: DQN-based Cross-stream Crisis event Summarizer", "abstract": "Summarizing multiple disaster-relevant data streams simultaneously is\nparticularly challenging as existing Retrieve&Re-ranking strategies suffer from\nthe inherent redundancy of multi-stream data and limited scalability in a\nmulti-query setting. This work proposes an online approach to crisis timeline\ngeneration based on weak annotation with Deep Q-Networks. It selects on-the-fly\nthe relevant pieces of text without requiring neither human annotations nor\ncontent re-ranking. This makes the inference time independent of the number of\ninput queries. The proposed approach also incorporates a redundancy filter into\nthe reward function to effectively handle cross-stream content overlaps. The\nachieved ROUGE and BERTScore results are superior to those of best-performing\nmodels on the CrisisFACTS 2022 benchmark.", "published": "2024-01-12 16:43:28", "link": "http://arxiv.org/abs/2401.06683v2", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Proximal Causal Inference With Text Data", "abstract": "Recent text-based causal methods attempt to mitigate confounding bias by\nestimating proxies of confounding variables that are partially or imperfectly\nmeasured from unstructured text data. These approaches, however, assume\nanalysts have supervised labels of the confounders given text for a subset of\ninstances, a constraint that is sometimes infeasible due to data privacy or\nannotation costs. In this work, we address settings in which an important\nconfounding variable is completely unobserved. We propose a new causal\ninference method that uses two instances of pre-treatment text data, infers two\nproxies using two zero-shot models on the separate instances, and applies these\nproxies in the proximal g-formula. We prove, under certain assumptions about\nthe instances of text and accuracy of the zero-shot predictions, that our\nmethod of inferring text-based proxies satisfies identification conditions of\nthe proximal g-formula while other seemingly reasonable proposals do not. To\naddress untestable assumptions associated with our method and the proximal\ng-formula, we further propose an odds ratio falsification heuristic that flags\nwhen to proceed with downstream effect estimation using the inferred proxies.\nWe evaluate our method in synthetic and semi-synthetic settings -- the latter\nwith real-world clinical notes from MIMIC-III and open large language models\nfor zero-shot prediction -- and find that our method produces estimates with\nlow bias. We believe that this text-based design of proxies allows for the use\nof proximal causal inference in a wider range of scenarios, particularly those\nfor which obtaining suitable proxies from structured data is difficult.", "published": "2024-01-12 16:51:02", "link": "http://arxiv.org/abs/2401.06687v3", "categories": ["cs.CL", "cs.LG", "stat.ME"], "primary_category": "cs.CL"}
{"title": "An Experimental Design Framework for Label-Efficient Supervised\n  Finetuning of Large Language Models", "abstract": "Supervised finetuning (SFT) on instruction datasets has played a crucial role\nin achieving the remarkable zero-shot generalization capabilities observed in\nmodern large language models (LLMs). However, the annotation efforts required\nto produce high quality responses for instructions are becoming prohibitively\nexpensive, especially as the number of tasks spanned by instruction datasets\ncontinues to increase. Active learning is effective in identifying useful\nsubsets of samples to annotate from an unlabeled pool, but its high\ncomputational cost remains a barrier to its widespread applicability in the\ncontext of LLMs. To mitigate the annotation cost of SFT and circumvent the\ncomputational bottlenecks of active learning, we propose using experimental\ndesign. Experimental design techniques select the most informative samples to\nlabel, and typically maximize some notion of uncertainty and/or diversity. In\nour work, we implement a framework that evaluates several existing and novel\nexperimental design techniques and find that these methods consistently yield\nsignificant gains in label efficiency with little computational overhead. On\ngenerative tasks, our methods achieve the same generalization performance with\nonly $50\\%$ of annotation cost required by random sampling.", "published": "2024-01-12 16:56:54", "link": "http://arxiv.org/abs/2401.06692v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Relying on the Unreliable: The Impact of Language Models' Reluctance to\n  Express Uncertainty", "abstract": "As natural language becomes the default interface for human-AI interaction,\nthere is a need for LMs to appropriately communicate uncertainties in\ndownstream applications. In this work, we investigate how LMs incorporate\nconfidence in responses via natural language and how downstream users behave in\nresponse to LM-articulated uncertainties. We examine publicly deployed models\nand find that LMs are reluctant to express uncertainties when answering\nquestions even when they produce incorrect responses. LMs can be explicitly\nprompted to express confidences, but tend to be overconfident, resulting in\nhigh error rates (an average of 47%) among confident responses. We test the\nrisks of LM overconfidence by conducting human experiments and show that users\nrely heavily on LM generations, whether or not they are marked by certainty.\nLastly, we investigate the preference-annotated datasets used in post training\nalignment and find that humans are biased against texts with uncertainty. Our\nwork highlights new safety harms facing human-LM interactions and proposes\ndesign recommendations and mitigating strategies moving forward.", "published": "2024-01-12 18:03:30", "link": "http://arxiv.org/abs/2401.06730v2", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "The Unreasonable Effectiveness of Easy Training Data for Hard Tasks", "abstract": "How can we train models to perform well on hard test data when hard training\ndata is by definition difficult to label correctly? This question has been\ntermed the scalable oversight problem and has drawn increasing attention as\nlanguage models have continually improved. In this paper, we present the\nsurprising conclusion that current pretrained language models often generalize\nrelatively well from easy to hard data, even performing as well as oracle\nmodels finetuned on hard data. We demonstrate this kind of easy-to-hard\ngeneralization using simple finetuning methods like in-context learning, linear\nclassifier heads, and QLoRA for seven different measures of datapoint hardness,\nincluding six empirically diverse human hardness measures (like grade level)\nand one model-based measure (loss-based). Furthermore, we show that even if one\ncares most about model performance on hard data, it can be better to collect\neasy data rather than hard data for finetuning, since hard data is generally\nnoisier and costlier to collect. Our experiments use open models up to 70b in\nsize and four publicly available question-answering datasets with questions\nranging in difficulty from 3rd grade science questions to college level STEM\nquestions and general-knowledge trivia. We conclude that easy-to-hard\ngeneralization in LMs is surprisingly strong for the tasks studied. Our code is\navailable at: https://github.com/allenai/easy-to-hard-generalization", "published": "2024-01-12 18:36:29", "link": "http://arxiv.org/abs/2401.06751v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "APLe: Token-Wise Adaptive for Multi-Modal Prompt Learning", "abstract": "Pre-trained Vision-Language (V-L) models set the benchmark for generalization\nto downstream tasks among the noteworthy contenders. Many characteristics of\nthe V-L model have been explored in existing research including the challenge\nof the sensitivity to text input and the tuning process across multi-modal\nprompts. With the advanced utilization of the V-L model like CLIP, recent\napproaches deploy learnable prompts instead of hand-craft prompts to boost the\ngeneralization performance and address the aforementioned challenges. Inspired\nby layer-wise training, which is wildly used in image fusion, we note that\nusing a sequential training process to adapt different modalities branches of\nCLIP efficiently facilitates the improvement of generalization. In the context\nof addressing the multi-modal prompting challenge, we propose Token-wise\nAdaptive for Multi-modal Prompt Learning (APLe) for tuning both modalities\nprompts, vision and language, as tokens in a sequential manner. APLe addresses\nthe challenges in V-L models to promote prompt learning across both modalities,\nwhich indicates a competitive generalization performance in line with the\nstate-of-the-art. Preeminently, APLe shows robustness and favourable\nperformance in prompt-length experiments with an absolute advantage in adopting\nthe V-L models.", "published": "2024-01-12 04:54:01", "link": "http://arxiv.org/abs/2401.06827v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "XLS-R Deep Learning Model for Multilingual ASR on Low- Resource\n  Languages: Indonesian, Javanese, and Sundanese", "abstract": "This research paper focuses on the development and evaluation of Automatic\nSpeech Recognition (ASR) technology using the XLS-R 300m model. The study aims\nto improve ASR performance in converting spoken language into written text,\nspecifically for Indonesian, Javanese, and Sundanese languages. The paper\ndiscusses the testing procedures, datasets used, and methodology employed in\ntraining and evaluating the ASR systems. The results show that the XLS-R 300m\nmodel achieves competitive Word Error Rate (WER) measurements, with a slight\ncompromise in performance for Javanese and Sundanese languages. The integration\nof a 5-gram KenLM language model significantly reduces WER and enhances ASR\naccuracy. The research contributes to the advancement of ASR technology by\naddressing linguistic diversity and improving performance across various\nlanguages. The findings provide insights into optimizing ASR accuracy and\napplicability for diverse linguistic contexts.", "published": "2024-01-12 13:44:48", "link": "http://arxiv.org/abs/2401.06832v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Health-LLM: Large Language Models for Health Prediction via Wearable\n  Sensor Data", "abstract": "Large language models (LLMs) are capable of many natural language tasks, yet\nthey are far from perfect. In health applications, grounding and interpreting\ndomain-specific and non-linguistic data is crucial. This paper investigates the\ncapacity of LLMs to make inferences about health based on contextual\ninformation (e.g. user demographics, health knowledge) and physiological data\n(e.g. resting heart rate, sleep minutes). We present a comprehensive evaluation\nof 12 state-of-the-art LLMs with prompting and fine-tuning techniques on four\npublic health datasets (PMData, LifeSnaps, GLOBEM and AW_FB). Our experiments\ncover 10 consumer health prediction tasks in mental health, activity,\nmetabolic, and sleep assessment. Our fine-tuned model, HealthAlpaca exhibits\ncomparable performance to much larger models (GPT-3.5, GPT-4 and Gemini-Pro),\nachieving the best performance in 8 out of 10 tasks. Ablation studies highlight\nthe effectiveness of context enhancement strategies. Notably, we observe that\nour context enhancement can yield up to 23.8% improvement in performance. While\nconstructing contextually rich prompts (combining user context, health\nknowledge and temporal information) exhibits synergistic improvement, the\ninclusion of health knowledge context in prompts significantly enhances overall\nperformance.", "published": "2024-01-12 19:40:11", "link": "http://arxiv.org/abs/2401.06866v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A systematic review of geospatial location embedding approaches in large\n  language models: A path to spatial AI systems", "abstract": "Geospatial Location Embedding (GLE) helps a Large Language Model (LLM)\nassimilate and analyze spatial data. GLE emergence in Geospatial Artificial\nIntelligence (GeoAI) is precipitated by the need for deeper geospatial\nawareness in our complex contemporary spaces and the success of LLMs in\nextracting deep meaning in Generative AI. We searched Google Scholar, Science\nDirect, and arXiv for papers on geospatial location embedding and LLM and\nreviewed articles focused on gaining deeper spatial \"knowing\" through LLMs. We\nscreened 304 titles, 30 abstracts, and 18 full-text papers that reveal four GLE\nthemes - Entity Location Embedding (ELE), Document Location Embedding (DLE),\nSequence Location Embedding (SLE), and Token Location Embedding (TLE).\nSynthesis is tabular and narrative, including a dialogic conversation between\n\"Space\" and \"LLM.\" Though GLEs aid spatial understanding by superimposing\nspatial data, they emphasize the need to advance in the intricacies of spatial\nmodalities and generalized reasoning. GLEs signal the need for a Spatial\nFoundation/Language Model (SLM) that embeds spatial knowing within the model\narchitecture. The SLM framework advances Spatial Artificial Intelligence\nSystems (SPAIS), establishing a Spatial Vector Space (SVS) that maps to\nphysical space. The resulting spatially imbued Language Model is unique. It\nsimultaneously represents actual space and an AI-capable space, paving the way\nfor AI native geo storage, analysis, and multi-modality as the basis for\nSpatial Artificial Intelligence Systems (SPAIS).", "published": "2024-01-12 12:43:33", "link": "http://arxiv.org/abs/2401.10279v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Uncertainty Awareness of Large Language Models Under Code Distribution\n  Shifts: A Benchmark Study", "abstract": "Large Language Models (LLMs) have been widely employed in programming\nlanguage analysis to enhance human productivity. Yet, their reliability can be\ncompromised by various code distribution shifts, leading to inconsistent\noutputs. While probabilistic methods are known to mitigate such impact through\nuncertainty calibration and estimation, their efficacy in the language domain\nremains underexplored compared to their application in image-based tasks. In\nthis work, we first introduce a large-scale benchmark dataset, incorporating\nthree realistic patterns of code distribution shifts at varying intensities.\nThen we thoroughly investigate state-of-the-art probabilistic methods applied\nto CodeLlama using these shifted code snippets. We observe that these methods\ngenerally improve the uncertainty awareness of CodeLlama, with increased\ncalibration quality and higher uncertainty estimation~(UE) precision. However,\nour study further reveals varied performance dynamics across different criteria\n(e.g., calibration error vs misclassification detection) and trade-off between\nefficacy and efficiency, highlighting necessary methodological selection\ntailored to specific contexts.", "published": "2024-01-12 00:00:32", "link": "http://arxiv.org/abs/2402.05939v1", "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "LLM-Assisted Crisis Management: Building Advanced LLM Platforms for\n  Effective Emergency Response and Public Collaboration", "abstract": "Emergencies and critical incidents often unfold rapidly, necessitating a\nswift and effective response. In this research, we introduce a novel approach\nto identify and classify emergency situations from social media posts and\ndirect emergency messages using an open source Large Language Model, LLAMA2.\nThe goal is to harness the power of natural language processing and machine\nlearning to assist public safety telecommunicators and huge crowds during\ncountrywide emergencies. Our research focuses on developing a language model\nthat can understand users describe their situation in the 911 call, enabling\nLLAMA2 to analyze the content and offer relevant instructions to the\ntelecommunicator, while also creating workflows to notify government agencies\nwith the caller's information when necessary. Another benefit this language\nmodel provides is its ability to assist people during a significant emergency\nincident when the 911 system is overwhelmed, by assisting the users with simple\ninstructions and informing authorities with their location and emergency\ninformation.", "published": "2024-01-12 17:50:35", "link": "http://arxiv.org/abs/2402.10908v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG", "68T50 68T50 68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Maximum-Entropy Adversarial Audio Augmentation for Keyword Spotting", "abstract": "Data augmentation is a key tool for improving the performance of deep\nnetworks, particularly when there is limited labeled data. In some fields, such\nas computer vision, augmentation methods have been extensively studied;\nhowever, for speech and audio data, there are relatively fewer methods\ndeveloped. Using adversarial learning as a starting point, we develop a simple\nand effective augmentation strategy based on taking the gradient of the entropy\nof the outputs with respect to the inputs and then creating new data points by\nmoving in the direction of the gradient to maximize the entropy. We validate\nits efficacy on several keyword spotting tasks as well as standard audio\nbenchmarks. Our method is straightforward to implement, offering greater\ncomputational efficiency than more complex adversarial schemes like GANs.\nDespite its simplicity, it proves robust and effective, especially when\ncombined with the established SpecAugment technique, leading to enhanced\nperformance.", "published": "2024-01-12 21:27:44", "link": "http://arxiv.org/abs/2401.06897v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Contrastive Learning With Audio Discrimination For Customizable Keyword\n  Spotting In Continuous Speech", "abstract": "Customizable keyword spotting (KWS) in continuous speech has attracted\nincreasing attention due to its real-world application potential. While\ncontrastive learning (CL) has been widely used to extract keyword\nrepresentations, previous CL approaches all operate on pre-segmented isolated\nwords and employ only audio-text representations matching strategy. However,\nfor KWS in continuous speech, co-articulation and streaming word segmentation\ncan easily yield similar audio patterns for different texts, which may\nconsequently trigger false alarms. To address this issue, we propose a novel CL\nwith Audio Discrimination (CLAD) approach to learning keyword representation\nwith both audio-text matching and audio-audio discrimination ability. Here, an\nInfoNCE loss considering both audio-audio and audio-text CL data pairs is\nemployed for each sliding window during training. Evaluations on the\nopen-source LibriPhrase dataset show that the use of sliding-window level\nInfoNCE loss yields comparable performance compared to previous CL approaches.\nFurthermore, experiments on the continuous speech dataset LibriSpeech\ndemonstrate that, by incorporating audio discrimination, CLAD achieves\nsignificant performance gain over CL without audio discrimination. Meanwhile,\ncompared to two-stage KWS approaches, the end-to-end KWS with CLAD achieves not\nonly better performance, but also significant speed-up.", "published": "2024-01-12 10:08:34", "link": "http://arxiv.org/abs/2401.06485v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Towards High-Quality and Efficient Speech Bandwidth Extension with\n  Parallel Amplitude and Phase Prediction", "abstract": "Speech bandwidth extension (BWE) refers to widening the frequency bandwidth\nrange of speech signals, enhancing the speech quality towards brighter and\nfuller. This paper proposes a generative adversarial network (GAN) based BWE\nmodel with parallel prediction of Amplitude and Phase spectra, named AP-BWE,\nwhich achieves both high-quality and efficient wideband speech waveform\ngeneration. The proposed AP-BWE generator is entirely based on convolutional\nneural networks (CNNs). It features a dual-stream architecture with mutual\ninteraction, where the amplitude stream and the phase stream communicate with\neach other and respectively extend the high-frequency components from the input\nnarrowband amplitude and phase spectra. To improve the naturalness of the\nextended speech signals, we employ a multi-period discriminator at the waveform\nlevel and design a pair of multi-resolution amplitude and phase discriminators\nat the spectral level, respectively. Experimental results demonstrate that our\nproposed AP-BWE achieves state-of-the-art performance in terms of speech\nquality for BWE tasks targeting sampling rates of both 16 kHz and 48 kHz. In\nterms of generation efficiency, due to the all-convolutional architecture and\nall-frame-level operations, the proposed AP-BWE can generate 48 kHz waveform\nsamples 292.3 times faster than real-time on a single RTX 4090 GPU and 18.1\ntimes faster than real-time on a single CPU. Notably, to our knowledge, AP-BWE\nis the first to achieve the direct extension of the high-frequency phase\nspectrum, which is beneficial for improving the effectiveness of existing BWE\nmethods.", "published": "2024-01-12 05:40:33", "link": "http://arxiv.org/abs/2401.06387v2", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "LCB-net: Long-Context Biasing for Audio-Visual Speech Recognition", "abstract": "The growing prevalence of online conferences and courses presents a new\nchallenge in improving automatic speech recognition (ASR) with enriched textual\ninformation from video slides. In contrast to rare phrase lists, the slides\nwithin videos are synchronized in real-time with the speech, enabling the\nextraction of long contextual bias. Therefore, we propose a novel long-context\nbiasing network (LCB-net) for audio-visual speech recognition (AVSR) to\nleverage the long-context information available in videos effectively.\nSpecifically, we adopt a bi-encoder architecture to simultaneously model audio\nand long-context biasing. Besides, we also propose a biasing prediction module\nthat utilizes binary cross entropy (BCE) loss to explicitly determine biased\nphrases in the long-context biasing. Furthermore, we introduce a dynamic\ncontextual phrases simulation to enhance the generalization and robustness of\nour LCB-net. Experiments on the SlideSpeech, a large-scale audio-visual corpus\nenriched with slides, reveal that our proposed LCB-net outperforms general ASR\nmodel by 9.4%/9.1%/10.9% relative WER/U-WER/B-WER reduction on test set, which\nenjoys high unbiased and biased performance. Moreover, we also evaluate our\nmodel on LibriSpeech corpus, leading to 23.8%/19.2%/35.4% relative\nWER/U-WER/B-WER reduction over the ASR model.", "published": "2024-01-12 06:03:39", "link": "http://arxiv.org/abs/2401.06390v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Microphone Conversion: Mitigating Device Variability in Sound Event\n  Classification", "abstract": "In this study, we introduce a new augmentation technique to enhance the\nresilience of sound event classification (SEC) systems against device\nvariability through the use of CycleGAN. We also present a unique dataset to\nevaluate this method. As SEC systems become increasingly common, it is crucial\nthat they work well with audio from diverse recording devices. Our method\naddresses limited device diversity in training data by enabling unpaired\ntraining to transform input spectrograms as if they are recorded on a different\ndevice. Our experiments show that our approach outperforms existing methods in\ngeneralization by 5.2% - 11.5% in weighted f1 score. Additionally, it surpasses\nthe current methods in adaptability across diverse recording devices by\nachieving a 6.5% - 12.8% improvement in weighted f1 score.", "published": "2024-01-12 21:59:01", "link": "http://arxiv.org/abs/2401.06913v1", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Transcending Controlled Environments Assessing the Transferability of\n  ASRRobust NLU Models to Real-World Applications", "abstract": "This research investigates the transferability of Automatic Speech\nRecognition (ASR)-robust Natural Language Understanding (NLU) models from\ncontrolled experimental conditions to practical, real-world applications.\nFocused on smart home automation commands in Urdu, the study assesses model\nperformance under diverse noise profiles, linguistic variations, and ASR error\nscenarios. Leveraging the UrduBERT model, the research employs a systematic\nmethodology involving real-world data collection, cross-validation, transfer\nlearning, noise variation studies, and domain adaptation. Evaluation metrics\nencompass task-specific accuracy, latency, user satisfaction, and robustness to\nASR errors. The findings contribute insights into the challenges and\nadaptability of ASR-robust NLU models in transcending controlled environments.", "published": "2024-01-12 16:10:04", "link": "http://arxiv.org/abs/2401.09354v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Dynamic Behaviour of Connectionist Speech Recognition with Strong\n  Latency Constraints", "abstract": "This paper describes the use of connectionist techniques in phonetic speech\nrecognition with strong latency constraints. The constraints are imposed by the\ntask of deriving the lip movements of a synthetic face in real time from the\nspeech signal, by feeding the phonetic string into an articulatory synthesiser.\nParticular attention has been paid to analysing the interaction between the\ntime evolution model learnt by the multi-layer perceptrons and the transition\nmodel imposed by the Viterbi decoder, in different latency conditions. Two\nexperiments were conducted in which the time dependencies in the language model\n(LM) were controlled by a parameter. The results show a strong interaction\nbetween the three factors involved, namely the neural network topology, the\nlength of time dependencies in the LM and the decoder latency.", "published": "2024-01-12 14:10:28", "link": "http://arxiv.org/abs/2401.06588v1", "categories": ["eess.AS", "cs.AI", "cs.CV", "cs.LG", "cs.SD", "I.5.0; I.2.7; E.4"], "primary_category": "eess.AS"}
