{"title": "Exploiting Word Semantics to Enrich Character Representations of Chinese\n  Pre-trained Models", "abstract": "Most of the Chinese pre-trained models adopt characters as basic units for\ndownstream tasks. However, these models ignore the information carried by words\nand thus lead to the loss of some important semantics. In this paper, we\npropose a new method to exploit word structure and integrate lexical semantics\ninto character representations of pre-trained models. Specifically, we project\na word's embedding into its internal characters' embeddings according to the\nsimilarity weight. To strengthen the word boundary information, we mix the\nrepresentations of the internal characters within a word. After that, we apply\na word-to-character alignment attention mechanism to emphasize important\ncharacters by masking unimportant ones. Moreover, in order to reduce the error\npropagation caused by word segmentation, we present an ensemble approach to\ncombine segmentation results given by different tokenizers. The experimental\nresults show that our approach achieves superior performance over the basic\npre-trained models BERT, BERT-wwm and ERNIE on different Chinese NLP tasks:\nsentiment classification, sentence pair matching, natural language inference\nand machine reading comprehension. We make further analysis to prove the\neffectiveness of each component of our model.", "published": "2022-07-13 02:28:08", "link": "http://arxiv.org/abs/2207.05928v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A General Contextualized Rewriting Framework for Text Summarization", "abstract": "The rewriting method for text summarization combines extractive and\nabstractive approaches, improving the conciseness and readability of extractive\nsummaries using an abstractive model. Exiting rewriting systems take each\nextractive sentence as the only input, which is relatively focused but can lose\nnecessary background knowledge and discourse context. In this paper, we\ninvestigate contextualized rewriting, which consumes the entire document and\nconsiders the summary context. We formalize contextualized rewriting as a\nseq2seq with group-tag alignments, introducing group-tag as a solution to model\nthe alignments, identifying extractive sentences through content-based\naddressing. Results show that our approach significantly outperforms\nnon-contextualized rewriting systems without requiring reinforcement learning,\nachieving strong improvements on ROUGE scores upon multiple extractors.", "published": "2022-07-13 03:55:57", "link": "http://arxiv.org/abs/2207.05948v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Developing a Component Comment Extractor from Product Reviews on\n  E-Commerce Sites", "abstract": "Consumers often read product reviews to inform their buying decision, as some\nconsumers want to know a specific component of a product. However, because\ntypical sentences on product reviews contain various details, users must\nidentify sentences about components they want to know amongst the many reviews.\nTherefore, we aimed to develop a system that identifies and collects component\nand aspect information of products in sentences. Our BERT-based classifiers\nassign labels referring to components and aspects to sentences in reviews and\nextract sentences with comments on specific components and aspects. We\ndetermined proper labels based for the words identified through pattern\nmatching from product reviews to create the training data. Because we could not\nuse the words as labels, we carefully created labels covering the meanings of\nthe words. However, the training data was imbalanced on component and aspect\npairs. We introduced a data augmentation method using WordNet to reduce the\nbias. Our evaluation demonstrates that the system can determine labels for road\nbikes using pattern matching, covering more than 88\\% of the indicators of\ncomponents and aspects on e-commerce sites. Moreover, our data augmentation\nmethod can improve the-F1-measure on insufficient data from 0.66 to 0.76.", "published": "2022-07-13 06:25:55", "link": "http://arxiv.org/abs/2207.05979v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fuse It More Deeply! A Variational Transformer with Layer-Wise Latent\n  Variable Inference for Text Generation", "abstract": "The past several years have witnessed Variational Auto-Encoder's superiority\nin various text generation tasks. However, due to the sequential nature of the\ntext, auto-regressive decoders tend to ignore latent variables and then reduce\nto simple language models, known as the KL vanishing problem, which would\nfurther deteriorate when VAE is combined with Transformer-based structures. To\nameliorate this problem, we propose DELLA, a novel variational Transformer\nframework. DELLA learns a series of layer-wise latent variables with each\ninferred from those of lower layers and tightly coupled with the hidden states\nby low-rank tensor product. In this way, DELLA forces these posterior latent\nvariables to be fused deeply with the whole computation path and hence\nincorporate more information. We theoretically demonstrate that our method can\nbe regarded as entangling latent variables to avoid posterior information\ndecrease through layers, enabling DELLA to get higher non-zero KL values even\nwithout any annealing or thresholding tricks. Experiments on four unconditional\nand three conditional generation tasks show that DELLA could better alleviate\nKL vanishing and improve both quality and diversity compared to several strong\nbaselines.", "published": "2022-07-13 11:27:46", "link": "http://arxiv.org/abs/2207.06130v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Reinforcement Learning-based Offensive semantics Censorship System for\n  Chatbots", "abstract": "The rapid development of artificial intelligence (AI) technology has enabled\nlarge-scale AI applications to land in the market and practice. However, while\nAI technology has brought many conveniences to people in the productization\nprocess, it has also exposed many security issues. Especially, attacks against\nonline learning vulnerabilities of chatbots occur frequently. Therefore, this\npaper proposes a semantics censorship chatbot system based on reinforcement\nlearning, which is mainly composed of two parts: the Offensive semantics\ncensorship model and the semantics purification model. Offensive semantics\nreview can combine the context of user input sentences to detect the rapid\nevolution of Offensive semantics and respond to Offensive semantics responses.\nThe semantics purification model For the case of chatting robot models, it has\nbeen contaminated by large numbers of offensive semantics, by strengthening the\noffensive reply learned by the learning algorithm, rather than rolling back to\nthe early versions. In addition, by integrating a once-through learning\napproach, the speed of semantics purification is accelerated while reducing the\nimpact on the quality of replies. The experimental results show that our\nproposed approach reduces the probability of the chat model generating\noffensive replies and that the integration of the few-shot learning algorithm\nimproves the training speed rapidly while effectively slowing down the decline\nin BLEU values.", "published": "2022-07-13 10:10:30", "link": "http://arxiv.org/abs/2207.10569v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "O-Dang! The Ontology of Dangerous Speech Messages", "abstract": "Inside the NLP community there is a considerable amount of language resources\ncreated, annotated and released every day with the aim of studying specific\nlinguistic phenomena. Despite a variety of attempts in order to organize such\nresources has been carried on, a lack of systematic methods and of possible\ninteroperability between resources are still present. Furthermore, when storing\nlinguistic information, still nowadays, the most common practice is the concept\nof \"gold standard\", which is in contrast with recent trends in NLP that aim at\nstressing the importance of different subjectivities and points of view when\ntraining machine learning and deep learning methods. In this paper we present\nO-Dang!: The Ontology of Dangerous Speech Messages, a systematic and\ninteroperable Knowledge Graph (KG) for the collection of linguistic annotated\ndata. O-Dang! is designed to gather and organize Italian datasets into a\nstructured KG, according to the principles shared within the Linguistic Linked\nOpen Data community. The ontology has also been designed to account for a\nperspectivist approach, since it provides a model for encoding both gold\nstandard and single-annotator labels in the KG. The paper is structured as\nfollows. In Section 1 the motivations of our work are outlined. Section 2\ndescribes the O-Dang! Ontology, that provides a common semantic model for the\nintegration of datasets in the KG. The Ontology Population stage with\ninformation about corpora, users, and annotations is presented in Section 3.\nFinally, in Section 4 an analysis of offensiveness across corpora is provided\nas a first case study for the resource.", "published": "2022-07-13 11:50:05", "link": "http://arxiv.org/abs/2207.10652v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "N-Grammer: Augmenting Transformers with latent n-grams", "abstract": "Transformer models have recently emerged as one of the foundational models in\nnatural language processing, and as a byproduct, there is significant recent\ninterest and investment in scaling these models. However, the training and\ninference costs of these large Transformer language models are prohibitive,\nthus necessitating more research in identifying more efficient variants. In\nthis work, we propose a simple yet effective modification to the Transformer\narchitecture inspired by the literature in statistical language modeling, by\naugmenting the model with n-grams that are constructed from a discrete latent\nrepresentation of the text sequence. We evaluate our model, the N-Grammer on\nlanguage modeling on the C4 data-set as well as text classification on the\nSuperGLUE data-set, and find that it outperforms several strong baselines such\nas the Transformer and the Primer. We open-source our model for reproducibility\npurposes in Jax.", "published": "2022-07-13 17:18:02", "link": "http://arxiv.org/abs/2207.06366v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Wide & Deep Learning for Judging Student Performance in Online\n  One-on-one Math Classes", "abstract": "In this paper, we investigate the opportunities of automating the judgment\nprocess in online one-on-one math classes. We build a Wide & Deep framework to\nlearn fine-grained predictive representations from a limited amount of noisy\nclassroom conversation data that perform better student judgments. We conducted\nexperiments on the task of predicting students' levels of mastery of example\nquestions and the results demonstrate the superiority and availability of our\nmodel in terms of various evaluation metrics.", "published": "2022-07-13 01:38:57", "link": "http://arxiv.org/abs/2207.10645v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DocPrompting: Generating Code by Retrieving the Docs", "abstract": "Publicly available source-code libraries are continuously growing and\nchanging. This makes it impossible for models of code to keep current with all\navailable APIs by simply training these models on existing code repositories.\nThus, existing models inherently cannot generalize to using unseen functions\nand libraries, because these would never appear in the training data. In\ncontrast, when human programmers use functions and libraries for the first\ntime, they frequently refer to textual resources such as code manuals and\ndocumentation, to explore and understand the available functionality. Inspired\nby this observation, we introduce DocPrompting: a natural-language-to-code\ngeneration approach that explicitly leverages documentation by (1) retrieving\nthe relevant documentation pieces given an NL intent, and (2) generating code\nbased on the NL intent and the retrieved documentation. DocPrompting is\ngeneral: it can be applied to any programming language and is agnostic to the\nunderlying neural model. We demonstrate that DocPrompting consistently improves\nNL-to-code models: DocPrompting improves strong base models such as CodeT5 by\n2.85% in pass@1 (52% relative gain) and 4.39% in pass@10 (30% relative gain) in\nexecution-based evaluation on the popular Python CoNaLa benchmark; on a new\nBash dataset tldr, DocPrompting improves CodeT5 and GPT-Neo1.3B by up to\nabsolute 6.9% exact match.", "published": "2022-07-13 06:47:51", "link": "http://arxiv.org/abs/2207.05987v3", "categories": ["cs.CL", "cs.AI", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Text-driven Emotional Style Control and Cross-speaker Style Transfer in\n  Neural TTS", "abstract": "Expressive text-to-speech has shown improved performance in recent years.\nHowever, the style control of synthetic speech is often restricted to discrete\nemotion categories and requires training data recorded by the target speaker in\nthe target style. In many practical situations, users may not have reference\nspeech recorded in target emotion but still be interested in controlling speech\nstyle just by typing text description of desired emotional style. In this\npaper, we propose a text-based interface for emotional style control and\ncross-speaker style transfer in multi-speaker TTS. We propose the bi-modal\nstyle encoder which models the semantic relationship between text description\nembedding and speech style embedding with a pretrained language model. To\nfurther improve cross-speaker style transfer on disjoint, multi-style datasets,\nwe propose the novel style loss. The experimental results show that our model\ncan generate high-quality expressive speech even in unseen style.", "published": "2022-07-13 07:05:44", "link": "http://arxiv.org/abs/2207.06000v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Transfer Learning Based Model for Text Readability Assessment in\n  German", "abstract": "Text readability assessment has a wide range of applications for different\ntarget people, from language learners to people with disabilities. The fast\npace of textual content production on the web makes it impossible to measure\ntext complexity without the benefit of machine learning and natural language\nprocessing techniques. Although various research addressed the readability\nassessment of English text in recent years, there is still room for improvement\nof the models for other languages. In this paper, we proposed a new model for\ntext complexity assessment for German text based on transfer learning. Our\nresults show that the model outperforms more classical solutions based on\nlinguistic features extraction from input text. The best model is based on the\nBERT pre-trained language model achieved the Root Mean Square Error (RMSE) of\n0.483.", "published": "2022-07-13 15:15:44", "link": "http://arxiv.org/abs/2207.06265v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Re2G: Retrieve, Rerank, Generate", "abstract": "As demonstrated by GPT-3 and T5, transformers grow in capability as parameter\nspaces become larger and larger. However, for tasks that require a large amount\nof knowledge, non-parametric memory allows models to grow dramatically with a\nsub-linear increase in computational cost and GPU memory requirements. Recent\nmodels such as RAG and REALM have introduced retrieval into conditional\ngeneration. These models incorporate neural initial retrieval from a corpus of\npassages. We build on this line of research, proposing Re2G, which combines\nboth neural initial retrieval and reranking into a BART-based\nsequence-to-sequence generation. Our reranking approach also permits merging\nretrieval results from sources with incomparable scores, enabling an ensemble\nof BM25 and neural initial retrieval. To train our system end-to-end, we\nintroduce a novel variation of knowledge distillation to train the initial\nretrieval, reranker, and generation using only ground truth on the target\nsequence output. We find large gains in four diverse tasks: zero-shot slot\nfilling, question answering, fact-checking, and dialog, with relative gains of\n9% to 34% over the previous state-of-the-art on the KILT leaderboard. We make\nour code available as open source at\nhttps://github.com/IBM/kgi-slot-filling/tree/re2g.", "published": "2022-07-13 15:51:40", "link": "http://arxiv.org/abs/2207.06300v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Modeling Long-term Dependencies and Short-term Correlations in Patient\n  Journey Data with Temporal Attention Networks for Health Prediction", "abstract": "Building models for health prediction based on Electronic Health Records\n(EHR) has become an active research area. EHR patient journey data consists of\npatient time-ordered clinical events/visits from patients. Most existing\nstudies focus on modeling long-term dependencies between visits, without\nexplicitly taking short-term correlations between consecutive visits into\naccount, where irregular time intervals, incorporated as auxiliary information,\nare fed into health prediction models to capture latent progressive patterns of\npatient journeys. We present a novel deep neural network with four modules to\ntake into account the contributions of various variables for health prediction:\ni) the Stacked Attention module strengthens the deep semantics in clinical\nevents within each patient journey and generates visit embeddings, ii) the\nShort-Term Temporal Attention module models short-term correlations between\nconsecutive visit embeddings while capturing the impact of time intervals\nwithin those visit embeddings, iii) the Long-Term Temporal Attention module\nmodels long-term dependencies between visit embeddings while capturing the\nimpact of time intervals within those visit embeddings, iv) and finally, the\nCoupled Attention module adaptively aggregates the outputs of Short-Term\nTemporal Attention and Long-Term Temporal Attention modules to make health\npredictions. Experimental results on MIMIC-III demonstrate superior predictive\naccuracy of our model compared to existing state-of-the-art methods, as well as\nthe interpretability and robustness of this approach. Furthermore, we found\nthat modeling short-term correlations contributes to local priors generation,\nleading to improved predictive modeling of patient journeys.", "published": "2022-07-13 09:15:26", "link": "http://arxiv.org/abs/2207.06414v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "3D Concept Grounding on Neural Fields", "abstract": "In this paper, we address the challenging problem of 3D concept grounding\n(i.e. segmenting and learning visual concepts) by looking at RGBD images and\nreasoning about paired questions and answers. Existing visual reasoning\napproaches typically utilize supervised methods to extract 2D segmentation\nmasks on which concepts are grounded. In contrast, humans are capable of\ngrounding concepts on the underlying 3D representation of images. However,\ntraditionally inferred 3D representations (e.g., point clouds, voxelgrids, and\nmeshes) cannot capture continuous 3D features flexibly, thus making it\nchallenging to ground concepts to 3D regions based on the language description\nof the object being referred to. To address both issues, we propose to leverage\nthe continuous, differentiable nature of neural fields to segment and learn\nconcepts. Specifically, each 3D coordinate in a scene is represented as a\nhigh-dimensional descriptor. Concept grounding can then be performed by\ncomputing the similarity between the descriptor vector of a 3D coordinate and\nthe vector embedding of a language concept, which enables segmentations and\nconcept learning to be jointly learned on neural fields in a differentiable\nfashion. As a result, both 3D semantic and instance segmentations can emerge\ndirectly from question answering supervision using a set of defined neural\noperators on top of neural fields (e.g., filtering and counting). Experimental\nresults show that our proposed framework outperforms\nunsupervised/language-mediated segmentation models on semantic and instance\nsegmentation tasks, as well as outperforms existing models on the challenging\n3D aware visual reasoning tasks. Furthermore, our framework can generalize well\nto unseen shape categories and real scans.", "published": "2022-07-13 17:59:33", "link": "http://arxiv.org/abs/2207.06403v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.GR", "cs.LG"], "primary_category": "cs.CV"}
{"title": "A Cyclical Approach to Synthetic and Natural Speech Mismatch Refinement\n  of Neural Post-filter for Low-cost Text-to-speech System", "abstract": "Neural-based text-to-speech (TTS) systems achieve very high-fidelity speech\ngeneration because of the rapid neural network developments. However, the huge\nlabeled corpus and high computation cost requirements limit the possibility of\ndeveloping a high-fidelity TTS system by small companies or individuals. On the\nother hand, a neural vocoder, which has been widely adopted for the speech\ngeneration in neural-based TTS systems, can be trained with a relatively small\nunlabeled corpus. Therefore, in this paper, we explore a general framework to\ndevelop a neural post-filter (NPF) for low-cost TTS systems using neural\nvocoders. A cyclical approach is proposed to tackle the acoustic and temporal\nmismatches (AM and TM) of developing an NPF. Both objective and subjective\nevaluations have been conducted to demonstrate the AM and TM problems and the\neffectiveness of the proposed framework.", "published": "2022-07-13 01:40:59", "link": "http://arxiv.org/abs/2207.05913v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Cross-Age Speaker Verification: Learning Age-Invariant Speaker\n  Embeddings", "abstract": "Automatic speaker verification has achieved remarkable progress in recent\nyears. However, there is little research on cross-age speaker verification\n(CASV) due to insufficient relevant data. In this paper, we mine cross-age test\nsets based on the VoxCeleb dataset and propose our age-invariant speaker\nrepresentation(AISR) learning method. Since the VoxCeleb is collected from the\nYouTube platform, the dataset consists of cross-age data inherently. However,\nthe meta-data does not contain the speaker age label. Therefore, we adopt the\nface age estimation method to predict the speaker age value from the associated\nvisual data, then label the audio recording with the estimated age. We\nconstruct multiple Cross-Age test sets on VoxCeleb (Vox-CA), which deliberately\nselect the positive trials with large age-gap. Also, the effect of nationality\nand gender is considered in selecting negative pairs to align with Vox-H cases.\nThe baseline system performance drops from 1.939\\% EER on the Vox-H test set to\n10.419\\% on the Vox-CA20 test set, which indicates how difficult the cross-age\nscenario is. Consequently, we propose an age-decoupling adversarial learning\n(ADAL) method to alleviate the negative effect of the age gap and reduce\nintra-class variance. Our method outperforms the baseline system by over 10\\%\nrelated EER reduction on the Vox-CA20 test set. The source code and trial\nresources are available on\nhttps://github.com/qinxiaoyi/Cross-Age_Speaker_Verification", "published": "2022-07-13 02:28:50", "link": "http://arxiv.org/abs/2207.05929v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SATTS: Speaker Attractor Text to Speech, Learning to Speak by Learning\n  to Separate", "abstract": "The mapping of text to speech (TTS) is non-deterministic, letters may be\npronounced differently based on context, or phonemes can vary depending on\nvarious physiological and stylistic factors like gender, age, accent, emotions,\netc. Neural speaker embeddings, trained to identify or verify speakers are\ntypically used to represent and transfer such characteristics from reference\nspeech to synthesized speech. Speech separation on the other hand is the\nchallenging task of separating individual speakers from an overlapping mixed\nsignal of various speakers. Speaker attractors are high-dimensional embedding\nvectors that pull the time-frequency bins of each speaker's speech towards\nthemselves while repelling those belonging to other speakers. In this work, we\nexplore the possibility of using these powerful speaker attractors for\nzero-shot speaker adaptation in multi-speaker TTS synthesis and propose speaker\nattractor text to speech (SATTS). Through various experiments, we show that\nSATTS can synthesize natural speech from text from an unseen target speaker's\nreference signal which might have less than ideal recording conditions, i.e.\nreverberations or mixed with other speakers.", "published": "2022-07-13 07:35:23", "link": "http://arxiv.org/abs/2207.06011v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Controllable and Lossless Non-Autoregressive End-to-End Text-to-Speech", "abstract": "Some recent studies have demonstrated the feasibility of single-stage neural\ntext-to-speech, which does not need to generate mel-spectrograms but generates\nthe raw waveforms directly from the text. Single-stage text-to-speech often\nfaces two problems: a) the one-to-many mapping problem due to multiple speech\nvariations and b) insufficiency of high frequency reconstruction due to the\nlack of supervision of ground-truth acoustic features during training. To solve\nthe a) problem and generate more expressive speech, we propose a novel\nphoneme-level prosody modeling method based on a variational autoencoder with\nnormalizing flows to model underlying prosodic information in speech. We also\nuse the prosody predictor to support end-to-end expressive speech synthesis.\nFurthermore, we propose the dual parallel autoencoder to introduce supervision\nof the ground-truth acoustic features during training to solve the b) problem\nenabling our model to generate high-quality speech. We compare the synthesis\nquality with state-of-the-art text-to-speech systems on an internal expressive\nEnglish dataset. Both qualitative and quantitative evaluations demonstrate the\nsuperiority and robustness of our method for lossless speech generation while\nalso showing a strong capability in prosody modeling.", "published": "2022-07-13 09:57:06", "link": "http://arxiv.org/abs/2207.06088v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Polyphonic sound event detection for highly dense birdsong scenes", "abstract": "One hour before sunrise, one can experience the dawn chorus where birds from\ndifferent species sing together. In this scenario, high levels of polyphony, as\nin the number of overlapping sound sources, are prone to happen resulting in a\ncomplex acoustic outcome. Sound Event Detection (SED) tasks analyze acoustic\nscenarios in order to identify the occurring events and their respective\ntemporal information. However, highly dense scenarios can be hard to process\nand have not been studied in depth. Here we show, using a Convolutional\nRecurrent Neural Network (CRNN), how birdsong polyphonic scenarios can be\ndetected when dealing with higher polyphony and how effectively this type of\nmodel can face a very dense scene with up to 10 overlapping birds. We found\nthat models trained with denser examples (i.e., higher polyphony) learn at a\nsimilar rate as models that used simpler samples in their training set.\nAdditionally, the model trained with the densest samples maintained a\nconsistent score for all polyphonies, while the model trained with the least\ndense samples degraded as the polyphony increased. Our results demonstrate that\nhighly dense acoustic scenarios can be dealt with using CRNNs. We expect that\nthis study serves as a starting point for working on highly populated bird\nscenarios such as dawn chorus or other dense acoustic problems.", "published": "2022-07-13 17:02:29", "link": "http://arxiv.org/abs/2207.06349v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Online Target Speaker Voice Activity Detection for Speaker Diarization", "abstract": "This paper proposes an online target speaker voice activity detection system\nfor speaker diarization tasks, which does not require a priori knowledge from\nthe clustering-based diarization system to obtain the target speaker\nembeddings. First, we employ a ResNet-based front-end model to extract the\nframe-level speaker embeddings for each coming block of a signal. Next, we\npredict the detection state of each speaker based on these frame-level speaker\nembeddings and the previously estimated target speaker embedding. Then, the\ntarget speaker embeddings are updated by aggregating these frame-level speaker\nembeddings according to the predictions in the current block. We iteratively\nextract the results for each block and update the target speaker embedding\nuntil reaching the end of the signal. Experimental results show that the\nproposed method is better than the offline clustering-based diarization system\non the AliMeeting dataset.", "published": "2022-07-13 01:56:31", "link": "http://arxiv.org/abs/2207.05920v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Subband-based Generative Adversarial Network for Non-parallel\n  Many-to-many Voice Conversion", "abstract": "Voice conversion is to generate a new speech with the source content and a\ntarget voice style. In this paper, we focus on one general setting, i.e.,\nnon-parallel many-to-many voice conversion, which is close to the real-world\nscenario. As the name implies, non-parallel many-to-many voice conversion does\nnot require the paired source and reference speeches and can be applied to\narbitrary voice transfer. In recent years, Generative Adversarial Networks\n(GANs) and other techniques such as Conditional Variational Autoencoders\n(CVAEs) have made considerable progress in this field. However, due to the\nsophistication of voice conversion, the style similarity of the converted\nspeech is still unsatisfactory. Inspired by the inherent structure of\nmel-spectrogram, we propose a new voice conversion framework, i.e.,\nSubband-based Generative Adversarial Network for Voice Conversion (SGAN-VC).\nSGAN-VC converts each subband content of the source speech separately by\nexplicitly utilizing the spatial characteristics between different subbands.\nSGAN-VC contains one style encoder, one content encoder, and one decoder. In\nparticular, the style encoder network is designed to learn style codes for\ndifferent subbands of the target speaker. The content encoder network can\ncapture the content information on the source speech. Finally, the decoder\ngenerates particular subband content. In addition, we propose a pitch-shift\nmodule to fine-tune the pitch of the source speaker, making the converted tone\nmore accurate and explainable. Extensive experiments demonstrate that the\nproposed approach achieves state-of-the-art performance on VCTK Corpus and\nAISHELL3 datasets both qualitatively and quantitatively, whether on seen or\nunseen data. Furthermore, the content intelligibility of SGAN-VC on unseen data\neven exceeds that of StarGANv2-VC with ASR network assistance.", "published": "2022-07-13 09:03:28", "link": "http://arxiv.org/abs/2207.06057v2", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MM-ALT: A Multimodal Automatic Lyric Transcription System", "abstract": "Automatic lyric transcription (ALT) is a nascent field of study attracting\nincreasing interest from both the speech and music information retrieval\ncommunities, given its significant application potential. However, ALT with\naudio data alone is a notoriously difficult task due to instrumental\naccompaniment and musical constraints resulting in degradation of both the\nphonetic cues and the intelligibility of sung lyrics. To tackle this challenge,\nwe propose the MultiModal Automatic Lyric Transcription system (MM-ALT),\ntogether with a new dataset, N20EM, which consists of audio recordings, videos\nof lip movements, and inertial measurement unit (IMU) data of an earbud worn by\nthe performing singer. We first adapt the wav2vec 2.0 framework from automatic\nspeech recognition (ASR) to the ALT task. We then propose a video-based ALT\nmethod and an IMU-based voice activity detection (VAD) method. In addition, we\nput forward the Residual Cross Attention (RCA) mechanism to fuse data from the\nthree modalities (i.e., audio, video, and IMU). Experiments show the\neffectiveness of our proposed MM-ALT system, especially in terms of noise\nrobustness. Project page is at https://n20em.github.io.", "published": "2022-07-13 11:23:34", "link": "http://arxiv.org/abs/2207.06127v3", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "ProDiff: Progressive Fast Diffusion Model For High-Quality\n  Text-to-Speech", "abstract": "Denoising diffusion probabilistic models (DDPMs) have recently achieved\nleading performances in many generative tasks. However, the inherited iterative\nsampling process costs hinder their applications to text-to-speech deployment.\nThrough the preliminary study on diffusion model parameterization, we find that\nprevious gradient-based TTS models require hundreds or thousands of iterations\nto guarantee high sample quality, which poses a challenge for accelerating\nsampling. In this work, we propose ProDiff, on progressive fast diffusion model\nfor high-quality text-to-speech. Unlike previous work estimating the gradient\nfor data density, ProDiff parameterizes the denoising model by directly\npredicting clean data to avoid distinct quality degradation in accelerating\nsampling. To tackle the model convergence challenge with decreased diffusion\niterations, ProDiff reduces the data variance in the target site via knowledge\ndistillation. Specifically, the denoising model uses the generated\nmel-spectrogram from an N-step DDIM teacher as the training target and distills\nthe behavior into a new model with N/2 steps. As such, it allows the TTS model\nto make sharp predictions and further reduces the sampling time by orders of\nmagnitude. Our evaluation demonstrates that ProDiff needs only 2 iterations to\nsynthesize high-fidelity mel-spectrograms, while it maintains sample quality\nand diversity competitive with state-of-the-art models using hundreds of steps.\nProDiff enables a sampling speed of 24x faster than real-time on a single\nNVIDIA 2080Ti GPU, making diffusion models practically applicable to\ntext-to-speech synthesis deployment for the first time. Our extensive ablation\nstudies demonstrate that each design in ProDiff is effective, and we further\nshow that ProDiff can be easily extended to the multi-speaker setting. Audio\nsamples are available at \\url{https://ProDiff.github.io/.}", "published": "2022-07-13 17:45:43", "link": "http://arxiv.org/abs/2207.06389v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Masked Autoencoders that Listen", "abstract": "This paper studies a simple extension of image-based Masked Autoencoders\n(MAE) to self-supervised representation learning from audio spectrograms.\nFollowing the Transformer encoder-decoder design in MAE, our Audio-MAE first\nencodes audio spectrogram patches with a high masking ratio, feeding only the\nnon-masked tokens through encoder layers. The decoder then re-orders and\ndecodes the encoded context padded with mask tokens, in order to reconstruct\nthe input spectrogram. We find it beneficial to incorporate local window\nattention in the decoder, as audio spectrograms are highly correlated in local\ntime and frequency bands. We then fine-tune the encoder with a lower masking\nratio on target datasets. Empirically, Audio-MAE sets new state-of-the-art\nperformance on six audio and speech classification tasks, outperforming other\nrecent models that use external supervised pre-training. The code and models\nwill be at https://github.com/facebookresearch/AudioMAE.", "published": "2022-07-13 17:59:55", "link": "http://arxiv.org/abs/2207.06405v3", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Wakeword Detection under Distribution Shifts", "abstract": "We propose a novel approach for semi-supervised learning (SSL) designed to\novercome distribution shifts between training and real-world data arising in\nthe keyword spotting (KWS) task. Shifts from training data distribution are a\nkey challenge for real-world KWS tasks: when a new model is deployed on device,\nthe gating of the accepted data undergoes a shift in distribution, making the\nproblem of timely updates via subsequent deployments hard. Despite the shift,\nwe assume that the marginal distributions on labels do not change. We utilize a\nmodified teacher/student training framework, where labeled training data is\naugmented with unlabeled data. Note that the teacher does not have access to\nthe new distribution as well. To train effectively with a mix of human and\nteacher labeled data, we develop a teacher labeling strategy based on\nconfidence heuristics to reduce entropy on the label distribution from the\nteacher model; the data is then sampled to match the marginal distribution on\nthe labels. Large scale experimental results show that a convolutional neural\nnetwork (CNN) trained on far-field audio, and evaluated on far-field audio\ndrawn from a different distribution, obtains a 14.3% relative improvement in\nfalse discovery rate (FDR) at equal false reject rate (FRR), while yielding a\n5% improvement in FDR under no distribution shift. Under a more severe\ndistribution shift from far-field to near-field audio with a smaller fully\nconnected network (FCN) our approach achieves a 52% relative improvement in FDR\nat equal FRR, while yielding a 20% relative improvement in FDR on the original\ndistribution.", "published": "2022-07-13 17:35:08", "link": "http://arxiv.org/abs/2207.06423v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Sub 8-Bit Quantization of Streaming Keyword Spotting Models for Embedded\n  Chipsets", "abstract": "We propose a novel 2-stage sub 8-bit quantization aware training algorithm\nfor all components of a 250K parameter feedforward, streaming, state-free\nkeyword spotting model. For the 1st-stage, we adapt a recently proposed\nquantization technique using a non-linear transformation with tanh(.) on dense\nlayer weights. In the 2nd-stage, we use linear quantization methods on the rest\nof the network, including other parameters (bias, gain, batchnorm), inputs, and\nactivations. We conduct large scale experiments, training on 26,000 hours of\nde-identified production, far-field and near-field audio data (evaluating on\n4,000 hours of data). We organize our results in two embedded chipset settings:\na) with commodity ARM NEON instruction set and 8-bit containers, we present\naccuracy, CPU, and memory results using sub 8-bit weights (4, 5, 8-bit) and\n8-bit quantization of rest of the network; b) with off-the-shelf neural network\naccelerators, for a range of weight bit widths (1 and 5-bit), while presenting\naccuracy results, we project reduction in memory utilization. In both\nconfigurations, our results show that the proposed algorithm can achieve: a)\nparity with a full floating point model's operating point on a detection error\ntradeoff (DET) curve in terms of false detection rate (FDR) at false rejection\nrate (FRR); b) significant reduction in compute and memory, yielding up to 3\ntimes improvement in CPU consumption and more than 4 times improvement in\nmemory consumption.", "published": "2022-07-13 17:46:08", "link": "http://arxiv.org/abs/2207.06920v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Visual Context-driven Audio Feature Enhancement for Robust End-to-End\n  Audio-Visual Speech Recognition", "abstract": "This paper focuses on designing a noise-robust end-to-end Audio-Visual Speech\nRecognition (AVSR) system. To this end, we propose Visual Context-driven Audio\nFeature Enhancement module (V-CAFE) to enhance the input noisy audio speech\nwith a help of audio-visual correspondence. The proposed V-CAFE is designed to\ncapture the transition of lip movements, namely visual context and to generate\na noise reduction mask by considering the obtained visual context. Through\ncontext-dependent modeling, the ambiguity in viseme-to-phoneme mapping can be\nrefined for mask generation. The noisy representations are masked out with the\nnoise reduction mask resulting in enhanced audio features. The enhanced audio\nfeatures are fused with the visual features and taken to an encoder-decoder\nmodel composed of Conformer and Transformer for speech recognition. We show the\nproposed end-to-end AVSR with the V-CAFE can further improve the\nnoise-robustness of AVSR. The effectiveness of the proposed method is evaluated\nin noisy speech recognition and overlapped speech recognition experiments using\nthe two largest audio-visual datasets, LRS2 and LRS3.", "published": "2022-07-13 08:07:19", "link": "http://arxiv.org/abs/2207.06020v1", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.MM", "eess.AS", "eess.IV"], "primary_category": "cs.SD"}
