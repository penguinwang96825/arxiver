{"title": "SummaRuNNer: A Recurrent Neural Network based Sequence Model for\n  Extractive Summarization of Documents", "abstract": "We present SummaRuNNer, a Recurrent Neural Network (RNN) based sequence model\nfor extractive summarization of documents and show that it achieves performance\nbetter than or comparable to state-of-the-art. Our model has the additional\nadvantage of being very interpretable, since it allows visualization of its\npredictions broken up by abstract features such as information content,\nsalience and novelty. Another novel contribution of our work is abstractive\ntraining of our extractive model that can train on human generated reference\nsummaries alone, eliminating the need for sentence-level extractive labels.", "published": "2016-11-14 02:44:14", "link": "http://arxiv.org/abs/1611.04230v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A New Recurrent Neural CRF for Learning Non-linear Edge Features", "abstract": "Conditional Random Field (CRF) and recurrent neural models have achieved\nsuccess in structured prediction. More recently, there is a marriage of CRF and\nrecurrent neural models, so that we can gain from both non-linear dense\nfeatures and globally normalized CRF objective. These recurrent neural CRF\nmodels mainly focus on encode node features in CRF undirected graphs. However,\nedge features prove important to CRF in structured prediction. In this work, we\nintroduce a new recurrent neural CRF model, which learns non-linear edge\nfeatures, and thus makes non-linear features encoded completely. We compare our\nmodel with different neural models in well-known structured prediction tasks.\nExperiments show that our model outperforms state-of-the-art methods in NP\nchunking, shallow parsing, Chinese word segmentation and POS tagging.", "published": "2016-11-14 02:48:46", "link": "http://arxiv.org/abs/1611.04233v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "F-Score Driven Max Margin Neural Network for Named Entity Recognition in\n  Chinese Social Media", "abstract": "We focus on named entity recognition (NER) for Chinese social media. With\nmassive unlabeled text and quite limited labelled corpus, we propose a\nsemi-supervised learning model based on B-LSTM neural network. To take\nadvantage of traditional methods in NER such as CRF, we combine transition\nprobability with deep learning in our model. To bridge the gap between label\naccuracy and F-score of NER, we construct a model which can be directly trained\non F-score. When considering the instability of F-score driven method and\nmeaningful information provided by label accuracy, we propose an integrated\nmethod to train on both F-score and label accuracy. Our integrated model yields\n7.44\\% improvement over previous state-of-the-art result.", "published": "2016-11-14 02:50:33", "link": "http://arxiv.org/abs/1611.04234v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Classify or Select: Neural Architectures for Extractive Document\n  Summarization", "abstract": "We present two novel and contrasting Recurrent Neural Network (RNN) based\narchitectures for extractive summarization of documents. The Classifier based\narchitecture sequentially accepts or rejects each sentence in the original\ndocument order for its membership in the final summary. The Selector\narchitecture, on the other hand, is free to pick one sentence at a time in any\narbitrary order to piece together the summary. Our models under both\narchitectures jointly capture the notions of salience and redundancy of\nsentences. In addition, these models have the advantage of being very\ninterpretable, since they allow visualization of their predictions broken up by\nabstract features such as information content, salience and redundancy. We show\nthat our models reach or outperform state-of-the-art supervised models on two\ndifferent corpora. We also recommend the conditions under which one\narchitecture is superior to the other based on experimental evidence.", "published": "2016-11-14 03:54:10", "link": "http://arxiv.org/abs/1611.04244v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "`Who would have thought of that!': A Hierarchical Topic Model for\n  Extraction of Sarcasm-prevalent Topics and Sarcasm Detection", "abstract": "Topic Models have been reported to be beneficial for aspect-based sentiment\nanalysis. This paper reports a simple topic model for sarcasm detection, a\nfirst, to the best of our knowledge. Designed on the basis of the intuition\nthat sarcastic tweets are likely to have a mixture of words of both sentiments\nas against tweets with literal sentiment (either positive or negative), our\nhierarchical topic model discovers sarcasm-prevalent topics and topic-level\nsentiment. Using a dataset of tweets labeled using hashtags, the model\nestimates topic-level, and sentiment-level distributions. Our evaluation shows\nthat topics such as `work', `gun laws', `weather' are sarcasm-prevalent topics.\nOur model is also able to discover the mixture of sentiment-bearing words that\nexist in a text of a given sentiment-related label. Finally, we apply our model\nto predict sarcasm in tweets. We outperform two prior work based on statistical\nclassifiers with specific features, by around 25\\%.", "published": "2016-11-14 10:40:44", "link": "http://arxiv.org/abs/1611.04326v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Character-level Convolutional Network for Text Classification Applied to\n  Chinese Corpus", "abstract": "This article provides an interesting exploration of character-level\nconvolutional neural network solving Chinese corpus text classification\nproblem. We constructed a large-scale Chinese language dataset, and the result\nshows that character-level convolutional neural network works better on Chinese\ncorpus than its corresponding pinyin format dataset. This is the first time\nthat character-level convolutional neural network applied to text\nclassification problem.", "published": "2016-11-14 12:24:27", "link": "http://arxiv.org/abs/1611.04358v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ranking medical jargon in electronic health record notes by adapted\n  distant supervision", "abstract": "Objective: Allowing patients to access their own electronic health record\n(EHR) notes through online patient portals has the potential to improve\npatient-centered care. However, medical jargon, which abounds in EHR notes, has\nbeen shown to be a barrier for patient EHR comprehension. Existing knowledge\nbases that link medical jargon to lay terms or definitions play an important\nrole in alleviating this problem but have low coverage of medical jargon in\nEHRs. We developed a data-driven approach that mines EHRs to identify and rank\nmedical jargon based on its importance to patients, to support the building of\nEHR-centric lay language resources.\n  Methods: We developed an innovative adapted distant supervision (ADS) model\nbased on support vector machines to rank medical jargon from EHRs. For distant\nsupervision, we utilized the open-access, collaborative consumer health\nvocabulary, a large, publicly available resource that links lay terms to\nmedical jargon. We explored both knowledge-based features from the Unified\nMedical Language System and distributed word representations learned from\nunlabeled large corpora. We evaluated the ADS model using physician-identified\nimportant medical terms.\n  Results: Our ADS model significantly surpassed two state-of-the-art automatic\nterm recognition methods, TF*IDF and C-Value, yielding 0.810 ROC-AUC versus\n0.710 and 0.667, respectively. Our model identified 10K important medical\njargon terms after ranking over 100K candidate terms mined from over 7,500 EHR\nnarratives.\n  Conclusion: Our work is an important step towards enriching lexical resources\nthat link medical jargon to lay terms/definitions to support patient EHR\ncomprehension. The identified medical jargon terms and their rankings are\navailable upon request.", "published": "2016-11-14 17:36:05", "link": "http://arxiv.org/abs/1611.04491v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Multi-view Recurrent Neural Acoustic Word Embeddings", "abstract": "Recent work has begun exploring neural acoustic word\nembeddings---fixed-dimensional vector representations of arbitrary-length\nspeech segments corresponding to words. Such embeddings are applicable to\nspeech retrieval and recognition tasks, where reasoning about whole words may\nmake it possible to avoid ambiguous sub-word representations. The main idea is\nto map acoustic sequences to fixed-dimensional vectors such that examples of\nthe same word are mapped to similar vectors, while different-word examples are\nmapped to very different vectors. In this work we take a multi-view approach to\nlearning acoustic word embeddings, in which we jointly learn to embed acoustic\nsequences and their corresponding character sequences. We use deep\nbidirectional LSTM embedding models and multi-view contrastive losses. We study\nthe effect of different loss variants, including fixed-margin and\ncost-sensitive losses. Our acoustic word embeddings improve over previous\napproaches for the task of word discrimination. We also present results on\nother tasks that are enabled by the multi-view approach, including cross-view\nword discrimination and word similarity.", "published": "2016-11-14 17:47:03", "link": "http://arxiv.org/abs/1611.04496v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lost in Space: Geolocation in Event Data", "abstract": "Extracting the \"correct\" location information from text data, i.e.,\ndetermining the place of event, has long been a goal for automated text\nprocessing. To approximate human-like coding schema, we introduce a supervised\nmachine learning algorithm that classifies each location word to be either\ncorrect or incorrect. We use news articles collected from around the world\n(Integrated Crisis Early Warning System [ICEWS] data and Open Event Data\nAlliance [OEDA] data) to test our algorithm that consists of two stages. In the\nfeature selection stage, we extract contextual information from texts, namely,\nthe N-gram patterns for location words, the frequency of mention, and the\ncontext of the sentences containing location words. In the classification\nstage, we use three classifiers to estimate the model parameters in the\ntraining set and then to predict whether a location word in the test set news\narticles is the place of the event. The validation results show that our\nalgorithm improves the accuracy rate of the current geolocation methods of\ndictionary approach by as much as 25%.", "published": "2016-11-14 20:50:03", "link": "http://arxiv.org/abs/1611.04837v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Google's Multilingual Neural Machine Translation System: Enabling\n  Zero-Shot Translation", "abstract": "We propose a simple solution to use a single Neural Machine Translation (NMT)\nmodel to translate between multiple languages. Our solution requires no change\nin the model architecture from our base system but instead introduces an\nartificial token at the beginning of the input sentence to specify the required\ntarget language. The rest of the model, which includes encoder, decoder and\nattention, remains unchanged and is shared across all languages. Using a shared\nwordpiece vocabulary, our approach enables Multilingual NMT using a single\nmodel without any increase in parameters, which is significantly simpler than\nprevious proposals for Multilingual NMT. Our method often improves the\ntranslation quality of all involved language pairs, even while keeping the\ntotal number of model parameters constant. On the WMT'14 benchmarks, a single\nmultilingual model achieves comparable performance for\nEnglish$\\rightarrow$French and surpasses state-of-the-art results for\nEnglish$\\rightarrow$German. Similarly, a single multilingual model surpasses\nstate-of-the-art results for French$\\rightarrow$English and\nGerman$\\rightarrow$English on WMT'14 and WMT'15 benchmarks respectively. On\nproduction corpora, multilingual models of up to twelve language pairs allow\nfor better translation of many individual pairs. In addition to improving the\ntranslation quality of language pairs that the model was trained with, our\nmodels can also learn to perform implicit bridging between language pairs never\nseen explicitly during training, showing that transfer learning and zero-shot\ntranslation is possible for neural translation. Finally, we show analyses that\nhints at a universal interlingua representation in our models and show some\ninteresting examples when mixing languages.", "published": "2016-11-14 20:24:39", "link": "http://arxiv.org/abs/1611.04558v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Quantitative Entropy Study of Language Complexity", "abstract": "We study the entropy of Chinese and English texts, based on characters in\ncase of Chinese texts and based on words for both languages. Significant\ndifferences are found between the languages and between different personal\nstyles of debating partners. The entropy analysis points in the direction of\nlower entropy, that is of higher complexity. Such a text analysis would be\napplied for individuals of different styles, a single individual at different\nage, as well as different groups of the population.", "published": "2016-11-14 10:53:40", "link": "http://arxiv.org/abs/1611.04841v2", "categories": ["cs.CL", "physics.soc-ph"], "primary_category": "cs.CL"}
{"title": "Attending to Characters in Neural Sequence Labeling Models", "abstract": "Sequence labeling architectures use word embeddings for capturing similarity,\nbut suffer when handling previously unseen or rare words. We investigate\ncharacter-level extensions to such models and propose a novel architecture for\ncombining alternative word representations. By using an attention mechanism,\nthe model is able to dynamically decide how much information to use from a\nword- or character-level component. We evaluated different architectures on a\nrange of sequence labeling datasets, and character-level extensions were found\nto improve performance on every benchmark. In addition, the proposed\nattention-based architecture delivered the best results even with a smaller\nnumber of trainable parameters.", "published": "2016-11-14 12:36:07", "link": "http://arxiv.org/abs/1611.04361v1", "categories": ["cs.CL", "cs.LG", "cs.NE", "I.5.1; I.2.6; I.2.7"], "primary_category": "cs.CL"}
{"title": "Zero-resource Machine Translation by Multimodal Encoder-decoder Network\n  with Multimedia Pivot", "abstract": "We propose an approach to build a neural machine translation system with no\nsupervised resources (i.e., no parallel corpora) using multimodal embedded\nrepresentation over texts and images. Based on the assumption that text\ndocuments are often likely to be described with other multimedia information\n(e.g., images) somewhat related to the content, we try to indirectly estimate\nthe relevance between two languages. Using multimedia as the \"pivot\", we\nproject all modalities into one common hidden space where samples belonging to\nsimilar semantic concepts should come close to each other, whatever the\nobserved space of each sample is. This modality-agnostic representation is the\nkey to bridging the gap between different modalities. Putting a decoder on top\nof it, our network can flexibly draw the outputs from any input modality.\nNotably, in the testing phase, we need only source language texts as the input\nfor translation. In experiments, we tested our method on two benchmarks to show\nthat it can achieve reasonable translation performance. We compared and\ninvestigated several possible implementations and found that an end-to-end\nmodel that simultaneously optimized both rank loss in multimodal encoders and\ncross-entropy loss in decoders performed the best.", "published": "2016-11-14 18:07:54", "link": "http://arxiv.org/abs/1611.04503v3", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Link Prediction using Embedded Knowledge Graphs", "abstract": "Since large knowledge bases are typically incomplete, missing facts need to\nbe inferred from observed facts in a task called knowledge base completion. The\nmost successful approaches to this task have typically explored explicit paths\nthrough sequences of triples. These approaches have usually resorted to\nhuman-designed sampling procedures, since large knowledge graphs produce\nprohibitively large numbers of possible paths, most of which are uninformative.\nAs an alternative approach, we propose performing a single, short sequence of\ninteractive lookup operations on an embedded knowledge graph which has been\ntrained through end-to-end backpropagation to be an optimized and compressed\nversion of the initial knowledge base. Our proposed model, called Embedded\nKnowledge Graph Network (EKGN), achieves new state-of-the-art results on\npopular knowledge base completion benchmarks.", "published": "2016-11-14 22:54:45", "link": "http://arxiv.org/abs/1611.04642v5", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
