{"title": "On Faithfulness and Factuality in Abstractive Summarization", "abstract": "It is well known that the standard likelihood training and approximate\ndecoding objectives in neural text generation models lead to less human-like\nresponses for open-ended tasks such as language modeling and story generation.\nIn this paper we have analyzed limitations of these models for abstractive\ndocument summarization and found that these models are highly prone to\nhallucinate content that is unfaithful to the input document. We conducted a\nlarge scale human evaluation of several neural abstractive summarization\nsystems to better understand the types of hallucinations they produce. Our\nhuman annotators found substantial amounts of hallucinated content in all model\ngenerated summaries. However, our analysis does show that pretrained models are\nbetter summarizers not only in terms of raw metrics, i.e., ROUGE, but also in\ngenerating faithful and factual summaries as evaluated by humans. Furthermore,\nwe show that textual entailment measures better correlate with faithfulness\nthan standard metrics, potentially leading the way to automatic evaluation\nmetrics as well as training and decoding criteria.", "published": "2020-05-02 00:09:16", "link": "http://arxiv.org/abs/2005.00661v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Benchmarking Multimodal Regex Synthesis with Complex Structures", "abstract": "Existing datasets for regular expression (regex) generation from natural\nlanguage are limited in complexity; compared to regex tasks that users post on\nStackOverflow, the regexes in these datasets are simple, and the language used\nto describe them is not diverse. We introduce StructuredRegex, a new regex\nsynthesis dataset differing from prior ones in three aspects. First, to obtain\nstructurally complex and realistic regexes, we generate the regexes using a\nprobabilistic grammar with pre-defined macros observed from real-world\nStackOverflow posts. Second, to obtain linguistically diverse natural language\ndescriptions, we show crowdworkers abstract depictions of the underlying regex\nand ask them to describe the pattern they see, rather than having them\nparaphrase synthetic language. Third, we augment each regex example with a\ncollection of strings that are and are not matched by the ground truth regex,\nsimilar to how real users give examples. Our quantitative and qualitative\nanalysis demonstrates the advantages of StructuredRegex over prior datasets.\nFurther experimental results using various multimodal synthesis techniques\nhighlight the challenge presented by our dataset, including non-local\nconstraints and multi-modal inputs.", "published": "2020-05-02 00:16:09", "link": "http://arxiv.org/abs/2005.00663v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DagoBERT: Generating Derivational Morphology with a Pretrained Language\n  Model", "abstract": "Can pretrained language models (PLMs) generate derivationally complex words?\nWe present the first study investigating this question, taking BERT as the\nexample PLM. We examine BERT's derivational capabilities in different settings,\nranging from using the unmodified pretrained model to full finetuning. Our best\nmodel, DagoBERT (Derivationally and generatively optimized BERT), clearly\noutperforms the previous state of the art in derivation generation (DG).\nFurthermore, our experiments show that the input segmentation crucially impacts\nBERT's derivational knowledge, suggesting that the performance of PLMs could be\nfurther improved if a morphologically informed vocabulary of units were used.", "published": "2020-05-02 01:26:46", "link": "http://arxiv.org/abs/2005.00672v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Opportunistic Decoding with Timely Correction for Simultaneous\n  Translation", "abstract": "Simultaneous translation has many important application scenarios and\nattracts much attention from both academia and industry recently. Most existing\nframeworks, however, have difficulties in balancing between the translation\nquality and latency, i.e., the decoding policy is usually either too aggressive\nor too conservative. We propose an opportunistic decoding technique with timely\ncorrection ability, which always (over-)generates a certain mount of extra\nwords at each step to keep the audience on track with the latest information.\nAt the same time, it also corrects, in a timely fashion, the mistakes in the\nformer overgenerated words when observing more source context to ensure high\ntranslation quality. Experiments show our technique achieves substantial\nreduction in latency and up to +3.1 increase in BLEU, with revision rate under\n8% in Chinese-to-English and English-to-Chinese translation.", "published": "2020-05-02 01:41:02", "link": "http://arxiv.org/abs/2005.00675v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Design Challenges in Low-resource Cross-lingual Entity Linking", "abstract": "Cross-lingual Entity Linking (XEL), the problem of grounding mentions of\nentities in a foreign language text into an English knowledge base such as\nWikipedia, has seen a lot of research in recent years, with a range of\npromising techniques. However, current techniques do not rise to the challenges\nintroduced by text in low-resource languages (LRL) and, surprisingly, fail to\ngeneralize to text not taken from Wikipedia, on which they are usually trained.\n  This paper provides a thorough analysis of low-resource XEL techniques,\nfocusing on the key step of identifying candidate English Wikipedia titles that\ncorrespond to a given foreign language mention. Our analysis indicates that\ncurrent methods are limited by their reliance on Wikipedia's interlanguage\nlinks and thus suffer when the foreign language's Wikipedia is small. We\nconclude that the LRL setting requires the use of outside-Wikipedia\ncross-lingual resources and present a simple yet effective zero-shot XEL\nsystem, QuEL, that utilizes search engines query logs. With experiments on 25\nlanguages, QuEL~shows an average increase of 25\\% in gold candidate recall and\nof 13\\% in end-to-end linking accuracy over state-of-the-art baselines.", "published": "2020-05-02 04:00:26", "link": "http://arxiv.org/abs/2005.00692v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Robust and Interpretable Grounding of Spatial References with Relation\n  Networks", "abstract": "Learning representations of spatial references in natural language is a key\nchallenge in tasks like autonomous navigation and robotic manipulation. Recent\nwork has investigated various neural architectures for learning multi-modal\nrepresentations for spatial concepts. However, the lack of explicit reasoning\nover entities makes such approaches vulnerable to noise in input text or state\nobservations. In this paper, we develop effective models for understanding\nspatial references in text that are robust and interpretable, without\nsacrificing performance. We design a text-conditioned \\textit{relation network}\nwhose parameters are dynamically computed with a cross-modal attention module\nto capture fine-grained spatial relations between entities. This design choice\nprovides interpretability of learned intermediate outputs. Experiments across\nthree tasks demonstrate that our model achieves superior performance, with a\n17\\% improvement in predicting goal locations and a 15\\% improvement in\nrobustness compared to state-of-the-art systems.", "published": "2020-05-02 04:11:33", "link": "http://arxiv.org/abs/2005.00696v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Expertise Style Transfer: A New Task Towards Better Communication\n  between Experts and Laymen", "abstract": "The curse of knowledge can impede communication between experts and laymen.\nWe propose a new task of expertise style transfer and contribute a manually\nannotated dataset with the goal of alleviating such cognitive biases. Solving\nthis task not only simplifies the professional language, but also improves the\naccuracy and expertise level of laymen descriptions using simple words. This is\na challenging task, unaddressed in previous work, as it requires the models to\nhave expert intelligence in order to modify text with a deep understanding of\ndomain knowledge and structures. We establish the benchmark performance of five\nstate-of-the-art models for style transfer and text simplification. The results\ndemonstrate a significant gap between machine and human performance. We also\ndiscuss the challenges of automatic evaluation, to provide insights into future\nresearch directions. The dataset is publicly available at\nhttps://srhthu.github.io/expertise-style-transfer.", "published": "2020-05-02 04:50:20", "link": "http://arxiv.org/abs/2005.00701v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Probing the Probing Paradigm: Does Probing Accuracy Entail Task\n  Relevance?", "abstract": "Although neural models have achieved impressive results on several NLP\nbenchmarks, little is understood about the mechanisms they use to perform\nlanguage tasks. Thus, much recent attention has been devoted to analyzing the\nsentence representations learned by neural encoders, through the lens of\n`probing' tasks. However, to what extent was the information encoded in\nsentence representations, as discovered through a probe, actually used by the\nmodel to perform its task? In this work, we examine this probing paradigm\nthrough a case study in Natural Language Inference, showing that models can\nlearn to encode linguistic properties even if they are not needed for the task\non which the model was trained. We further identify that pretrained word\nembeddings play a considerable role in encoding these properties rather than\nthe training task itself, highlighting the importance of careful controls when\ndesigning probing experiments. Finally, through a set of controlled synthetic\ntasks, we demonstrate models can encode these properties considerably above\nchance-level even when distributed in the data as random noise, calling into\nquestion the interpretation of absolute claims on probing tasks.", "published": "2020-05-02 06:19:20", "link": "http://arxiv.org/abs/2005.00719v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hard-Coded Gaussian Attention for Neural Machine Translation", "abstract": "Recent work has questioned the importance of the Transformer's multi-headed\nattention for achieving high translation quality. We push further in this\ndirection by developing a \"hard-coded\" attention variant without any learned\nparameters. Surprisingly, replacing all learned self-attention heads in the\nencoder and decoder with fixed, input-agnostic Gaussian distributions minimally\nimpacts BLEU scores across four different language pairs. However, additionally\nhard-coding cross attention (which connects the decoder to the encoder)\nsignificantly lowers BLEU, suggesting that it is more important than\nself-attention. Much of this BLEU drop can be recovered by adding just a single\nlearned cross attention head to an otherwise hard-coded Transformer. Taken as a\nwhole, our results offer insight into which components of the Transformer are\nactually important, which we hope will guide future work into the development\nof simpler and more efficient attention-based models.", "published": "2020-05-02 08:16:13", "link": "http://arxiv.org/abs/2005.00742v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BERT-kNN: Adding a kNN Search Component to Pretrained Language Models\n  for Better QA", "abstract": "Khandelwal et al. (2020) use a k-nearest-neighbor (kNN) component to improve\nlanguage model performance. We show that this idea is beneficial for\nopen-domain question answering (QA). To improve the recall of facts encountered\nduring training, we combine BERT (Devlin et al., 2019) with a traditional\ninformation retrieval step (IR) and a kNN search over a large datastore of an\nembedded text collection. Our contributions are as follows: i) BERT-kNN\noutperforms BERT on cloze-style QA by large margins without any further\ntraining. ii) We show that BERT often identifies the correct response category\n(e.g., US city), but only kNN recovers the factually correct answer (e.g.,\n\"Miami\"). iii) Compared to BERT, BERT-kNN excels for rare facts. iv) BERT-kNN\ncan easily handle facts not covered by BERT's training set, e.g., recent\nevents.", "published": "2020-05-02 09:34:42", "link": "http://arxiv.org/abs/2005.00766v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring and Predicting Transferability across NLP Tasks", "abstract": "Recent advances in NLP demonstrate the effectiveness of training large-scale\nlanguage models and transferring them to downstream tasks. Can fine-tuning\nthese models on tasks other than language modeling further improve performance?\nIn this paper, we conduct an extensive study of the transferability between 33\nNLP tasks across three broad classes of problems (text classification, question\nanswering, and sequence labeling). Our results show that transfer learning is\nmore beneficial than previously thought, especially when target task data is\nscarce, and can improve performance even when the source task is small or\ndiffers substantially from the target task (e.g., part-of-speech tagging\ntransfers well to the DROP QA dataset). We also develop task embeddings that\ncan be used to predict the most transferable source tasks for a given target\ntask, and we validate their effectiveness in experiments controlled for source\nand target data size. Overall, our experiments reveal that factors such as\nsource data size, task and domain similarity, and task complexity all play a\nrole in determining transferability.", "published": "2020-05-02 09:39:36", "link": "http://arxiv.org/abs/2005.00770v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ProtoQA: A Question Answering Dataset for Prototypical Common-Sense\n  Reasoning", "abstract": "Given questions regarding some prototypical situation such as Name something\nthat people usually do before they leave the house for work? a human can easily\nanswer them via acquired experiences. There can be multiple right answers for\nsuch questions, with some more common for a situation than others. This paper\nintroduces a new question answering dataset for training and evaluating common\nsense reasoning capabilities of artificial intelligence systems in such\nprototypical situations. The training set is gathered from an existing set of\nquestions played in a long-running international game show FAMILY- FEUD. The\nhidden evaluation set is created by gathering answers for each question from\n100 crowd-workers. We also propose a generative evaluation task where a model\nhas to output a ranked list of answers, ideally covering all prototypical\nanswers for a question. After presenting multiple competitive baseline models,\nwe find that human performance still exceeds model scores on all evaluation\nmetrics with a meaningful gap, supporting the challenging nature of the task.", "published": "2020-05-02 09:40:05", "link": "http://arxiv.org/abs/2005.00771v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Visually Grounded Continual Learning of Compositional Phrases", "abstract": "Humans acquire language continually with much more limited access to data\nsamples at a time, as compared to contemporary NLP systems. To study this\nhuman-like language acquisition ability, we present VisCOLL, a visually\ngrounded language learning task, which simulates the continual acquisition of\ncompositional phrases from streaming visual scenes. In the task, models are\ntrained on a paired image-caption stream which has shifting object\ndistribution; while being constantly evaluated by a visually-grounded masked\nlanguage prediction task on held-out test sets. VisCOLL compounds the\nchallenges of continual learning (i.e., learning from continuously shifting\ndata distribution) and compositional generalization (i.e., generalizing to\nnovel compositions). To facilitate research on VisCOLL, we construct two\ndatasets, COCO-shift and Flickr-shift, and benchmark them using different\ncontinual learning methods. Results reveal that SoTA continual learning\napproaches provide little to no improvements on VisCOLL, since storing examples\nof all possible compositions is infeasible. We conduct further ablations and\nanalysis to guide future work.", "published": "2020-05-02 10:45:30", "link": "http://arxiv.org/abs/2005.00785v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KinGDOM: Knowledge-Guided DOMain adaptation for sentiment analysis", "abstract": "Cross-domain sentiment analysis has received significant attention in recent\nyears, prompted by the need to combat the domain gap between different\napplications that make use of sentiment analysis. In this paper, we take a\nnovel perspective on this task by exploring the role of external commonsense\nknowledge. We introduce a new framework, KinGDOM, which utilizes the ConceptNet\nknowledge graph to enrich the semantics of a document by providing both\ndomain-specific and domain-general background concepts. These concepts are\nlearned by training a graph convolutional autoencoder that leverages\ninter-domain concepts in a domain-invariant manner. Conditioning a popular\ndomain-adversarial baseline method with these learned concepts helps improve\nits performance over state-of-the-art approaches, demonstrating the efficacy of\nour proposed framework.", "published": "2020-05-02 11:03:25", "link": "http://arxiv.org/abs/2005.00791v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Simple Language Model for Task-Oriented Dialogue", "abstract": "Task-oriented dialogue is often decomposed into three tasks: understanding\nuser input, deciding actions, and generating a response. While such\ndecomposition might suggest a dedicated model for each sub-task, we find a\nsimple, unified approach leads to state-of-the-art performance on the MultiWOZ\ndataset. SimpleTOD is a simple approach to task-oriented dialogue that uses a\nsingle, causal language model trained on all sub-tasks recast as a single\nsequence prediction problem. This allows SimpleTOD to fully leverage transfer\nlearning from pre-trained, open domain, causal language models such as GPT-2.\nSimpleTOD improves over the prior state-of-the-art in joint goal accuracy for\ndialogue state tracking, and our analysis reveals robustness to noisy\nannotations in this setting. SimpleTOD also improves the main metrics used to\nevaluate action decisions and response generation in an end-to-end setting:\ninform rate by 8.1 points, success rate by 9.7 points, and combined score by\n7.2 points.", "published": "2020-05-02 11:09:27", "link": "http://arxiv.org/abs/2005.00796v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Treebank Embedding Vectors for Out-of-domain Dependency Parsing", "abstract": "A recent advance in monolingual dependency parsing is the idea of a treebank\nembedding vector, which allows all treebanks for a particular language to be\nused as training data while at the same time allowing the model to prefer\ntraining data from one treebank over others and to select the preferred\ntreebank at test time. We build on this idea by 1) introducing a method to\npredict a treebank vector for sentences that do not come from a treebank used\nin training, and 2) exploring what happens when we move away from predefined\ntreebank embedding vectors during test time and instead devise tailored\ninterpolations. We show that 1) there are interpolated vectors that are\nsuperior to the predefined ones, and 2) treebank vectors can be predicted with\nsufficient accuracy, for nine out of ten test languages, to match the\nperformance of an oracle approach that knows the most suitable predefined\ntreebank embedding for the test set.", "published": "2020-05-02 11:33:41", "link": "http://arxiv.org/abs/2005.00800v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DQI: Measuring Data Quality in NLP", "abstract": "Neural language models have achieved human level performance across several\nNLP datasets. However, recent studies have shown that these models are not\ntruly learning the desired task; rather, their high performance is attributed\nto overfitting using spurious biases, which suggests that the capabilities of\nAI systems have been over-estimated. We introduce a generic formula for Data\nQuality Index (DQI) to help dataset creators create datasets free of such\nunwanted biases. We evaluate this formula using a recently proposed approach\nfor adversarial filtering, AFLite. We propose a new data creation paradigm\nusing DQI to create higher quality data. The data creation paradigm consists of\nseveral data visualizations to help data creators (i) understand the quality of\ndata and (ii) visualize the impact of the created data instance on the overall\nquality. It also has a couple of automation methods to (i) assist data creators\nand (ii) make the model more robust to adversarial attacks. We use DQI along\nwith these automation methods to renovate biased examples in SNLI. We show that\nmodels trained on the renovated SNLI dataset generalize better to out of\ndistribution tasks. Renovation results in reduced model performance, exposing a\nlarge gap with respect to human performance. DQI systematically helps in\ncreating harder benchmarks using active learning. Our work takes the process of\ndynamic dataset creation forward, wherein datasets evolve together with the\nevolving state of the art, therefore serving as a means of benchmarking the\ntrue progress of AI.", "published": "2020-05-02 12:34:17", "link": "http://arxiv.org/abs/2005.00816v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Models as an Alternative Evaluator of Word Order Hypotheses: A\n  Case Study in Japanese", "abstract": "We examine a methodology using neural language models (LMs) for analyzing the\nword order of language. This LM-based method has the potential to overcome the\ndifficulties existing methods face, such as the propagation of preprocessor\nerrors in count-based methods. In this study, we explore whether the LM-based\nmethod is valid for analyzing the word order. As a case study, this study\nfocuses on Japanese due to its complex and flexible word order. To validate the\nLM-based method, we test (i) parallels between LMs and human word order\npreference, and (ii) consistency of the results obtained using the LM-based\nmethod with previous linguistic studies. Through our experiments, we\ntentatively conclude that LMs display sufficient word order knowledge for usage\nas an analysis tool. Finally, using the LM-based method, we demonstrate the\nrelationship between the canonical word order and topicalization, which had yet\nto be analyzed by large-scale experiments.", "published": "2020-05-02 14:32:40", "link": "http://arxiv.org/abs/2005.00842v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sources of Transfer in Multilingual Named Entity Recognition", "abstract": "Named-entities are inherently multilingual, and annotations in any given\nlanguage may be limited. This motivates us to consider polyglot named-entity\nrecognition (NER), where one model is trained using annotated data drawn from\nmore than one language. However, a straightforward implementation of this\nsimple idea does not always work in practice: naive training of NER models\nusing annotated data drawn from multiple languages consistently underperforms\nmodels trained on monolingual data alone, despite having access to more\ntraining data. The starting point of this paper is a simple solution to this\nproblem, in which polyglot models are fine-tuned on monolingual data to\nconsistently and significantly outperform their monolingual counterparts. To\nexplain this phenomena, we explore the sources of multilingual transfer in\npolyglot NER models and examine the weight structure of polyglot models\ncompared to their monolingual counterparts. We find that polyglot models\nefficiently share many parameters across languages and that fine-tuning may\nutilize a large number of those parameters.", "published": "2020-05-02 15:00:02", "link": "http://arxiv.org/abs/2005.00847v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A language score based output selection method for multilingual speech\n  recognition", "abstract": "The quality of a multilingual speech recognition system can be improved by\nadaptation methods if the input language is specified. For systems that can\naccept multilingual inputs, the popular approach is to apply a language\nidentifier to the input then switch or configure decoders in the next step, or\nuse one more subsequence model to select the output from a set of candidates.\nMotivated by the goal of reducing the latency for real-time applications, in\nthis paper, a language model rescoring method is firstly applied to produce all\npossible candidates for target languages, then a simple score is proposed to\nautomatically select the output without any identifier model or language\nspecification of the input language. The main point is that this score can be\nsimply and automatically estimated on-the-fly so that the whole decoding\npipeline is more simple and compact. Experimental results showed that this\nmethod can achieve the same quality as when the input language is specified. In\naddition, we present to design an English and Vietnamese End-to-End model to\ndeal with not only the problem of cross-lingual speakers but also as a solution\nto improve the accuracy of borrowed words of English in Vietnamese.", "published": "2020-05-02 15:07:14", "link": "http://arxiv.org/abs/2005.00851v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Predicting Performance for Natural Language Processing Tasks", "abstract": "Given the complexity of combinations of tasks, languages, and domains in\nnatural language processing (NLP) research, it is computationally prohibitive\nto exhaustively test newly proposed models on each possible experimental\nsetting. In this work, we attempt to explore the possibility of gaining\nplausible judgments of how well an NLP model can perform under an experimental\nsetting, without actually training or testing the model. To do so, we build\nregression models to predict the evaluation score of an NLP experiment given\nthe experimental settings as input. Experimenting on 9 different NLP tasks, we\nfind that our predictors can produce meaningful predictions over unseen\nlanguages and different modeling architectures, outperforming reasonable\nbaselines as well as human experts. Going further, we outline how our predictor\ncan be used to find a small subset of representative experiments that should be\nrun in order to obtain plausible predictions for all other experimental\nsettings.", "published": "2020-05-02 16:02:18", "link": "http://arxiv.org/abs/2005.00870v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Truthfulness of Headline Generation", "abstract": "Most studies on abstractive summarization report ROUGE scores between system\nand reference summaries. However, we have a concern about the truthfulness of\ngenerated summaries: whether all facts of a generated summary are mentioned in\nthe source text. This paper explores improving the truthfulness in headline\ngeneration on two popular datasets. Analyzing headlines generated by the\nstate-of-the-art encoder-decoder model, we show that the model sometimes\ngenerates untruthful headlines. We conjecture that one of the reasons lies in\nuntruthful supervision data used for training the model. In order to quantify\nthe truthfulness of article-headline pairs, we consider the textual entailment\nof whether an article entails its headline. After confirming quite a few\nuntruthful instances in the datasets, this study hypothesizes that removing\nuntruthful instances from the supervision data may remedy the problem of the\nuntruthful behaviors of the model. Building a binary classifier that predicts\nan entailment relation between an article and its headline, we filter out\nuntruthful instances from the supervision data. Experimental results\ndemonstrate that the headline generation model trained on filtered supervision\ndata shows no clear difference in ROUGE scores but remarkable improvements in\nautomatic and manual evaluations of the generated headlines.", "published": "2020-05-02 16:33:37", "link": "http://arxiv.org/abs/2005.00882v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rationalizing Medical Relation Prediction from Corpus-level Statistics", "abstract": "Nowadays, the interpretability of machine learning models is becoming\nincreasingly important, especially in the medical domain. Aiming to shed some\nlight on how to rationalize medical relation prediction, we present a new\ninterpretable framework inspired by existing theories on how human memory\nworks, e.g., theories of recall and recognition. Given the corpus-level\nstatistics, i.e., a global co-occurrence graph of a clinical text corpus, to\npredict the relations between two entities, we first recall rich contexts\nassociated with the target entities, and then recognize relational interactions\nbetween these contexts to form model rationales, which will contribute to the\nfinal prediction. We conduct experiments on a real-world public clinical\ndataset and show that our framework can not only achieve competitive predictive\nperformance against a comprehensive list of neural baseline models, but also\npresent rationales to justify its prediction. We further collaborate with\nmedical experts deeply to verify the usefulness of our model rationales for\nclinical decision making.", "published": "2020-05-02 17:39:40", "link": "http://arxiv.org/abs/2005.00889v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Transfer Learning with Synthesized Data for Multi-Domain\n  Dialogue State Tracking", "abstract": "Zero-shot transfer learning for multi-domain dialogue state tracking can\nallow us to handle new domains without incurring the high cost of data\nacquisition. This paper proposes new zero-short transfer learning technique for\ndialogue state tracking where the in-domain training data are all synthesized\nfrom an abstract dialogue model and the ontology of the domain. We show that\ndata augmentation through synthesized data can improve the accuracy of\nzero-shot learning for both the TRADE model and the BERT-based SUMBT model on\nthe MultiWOZ 2.1 dataset. We show training with only synthesized in-domain data\non the SUMBT model can reach about 2/3 of the accuracy obtained with the full\ntraining dataset. We improve the zero-shot learning state of the art on average\nacross domains by 21%.", "published": "2020-05-02 18:00:48", "link": "http://arxiv.org/abs/2005.00891v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Practical Perspectives on Quality Estimation for Machine Translation", "abstract": "Sentence level quality estimation (QE) for machine translation (MT) attempts\nto predict the translation edit rate (TER) cost of post-editing work required\nto correct MT output. We describe our view on sentence-level QE as dictated by\nseveral practical setups encountered in the industry. We find consumers of MT\noutput---whether human or algorithmic ones---to be primarily interested in a\nbinary quality metric: is the translated sentence adequate as-is or does it\nneed post-editing? Motivated by this we propose a quality classification (QC)\nview on sentence-level QE whereby we focus on maximizing recall at precision\nabove a given threshold. We demonstrate that, while classical QE regression\nmodels fare poorly on this task, they can be re-purposed by replacing the\noutput regression layer with a binary classification one, achieving 50-60\\%\nrecall at 90\\% precision. For a high-quality MT system producing 75-80\\%\ncorrect translations, this promises a significant reduction in post-editing\nwork indeed.", "published": "2020-05-02 01:50:10", "link": "http://arxiv.org/abs/2005.03519v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GenericsKB: A Knowledge Base of Generic Statements", "abstract": "We present a new resource for the NLP community, namely a large (3.5M+\nsentence) knowledge base of *generic statements*, e.g., \"Trees remove carbon\ndioxide from the atmosphere\", collected from multiple corpora. This is the\nfirst large resource to contain *naturally occurring* generic sentences, as\nopposed to extracted or crowdsourced triples, and thus is rich in high-quality,\ngeneral, semantically complete statements. All GenericsKB sentences are\nannotated with their topical term, surrounding context (sentences), and a\n(learned) confidence. We also release GenericsKB-Best (1M+ sentences),\ncontaining the best-quality generics in GenericsKB augmented with selected,\nsynthesized generics from WordNet and ConceptNet. In tests on two existing\ndatasets requiring multihop reasoning (OBQA and QASC), we find using GenericsKB\ncan result in higher scores and better explanations than using a much larger\ncorpus. This demonstrates that GenericsKB can be a useful resource for NLP\napplications, as well as providing data for linguistic studies of generics and\ntheir semantics. GenericsKB is available at\nhttps://allenai.org/data/genericskb.", "published": "2020-05-02 00:08:42", "link": "http://arxiv.org/abs/2005.00660v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Contrastive Self-Supervised Learning for Commonsense Reasoning", "abstract": "We propose a self-supervised method to solve Pronoun Disambiguation and\nWinograd Schema Challenge problems. Our approach exploits the characteristic\nstructure of training corpora related to so-called \"trigger\" words, which are\nresponsible for flipping the answer in pronoun disambiguation. We achieve such\ncommonsense reasoning by constructing pair-wise contrastive auxiliary\npredictions. To this end, we leverage a mutual exclusive loss regularized by a\ncontrastive margin. Our architecture is based on the recently introduced\ntransformer networks, BERT, that exhibits strong performance on many NLP\nbenchmarks. Empirical results show that our method alleviates the limitation of\ncurrent supervised approaches for commonsense reasoning. This study opens up\navenues for exploiting inexpensive self-supervision to achieve performance gain\nin commonsense reasoning tasks.", "published": "2020-05-02 00:39:09", "link": "http://arxiv.org/abs/2005.00669v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Birds have four legs?! NumerSense: Probing Numerical Commonsense\n  Knowledge of Pre-trained Language Models", "abstract": "Recent works show that pre-trained language models (PTLMs), such as BERT,\npossess certain commonsense and factual knowledge. They suggest that it is\npromising to use PTLMs as \"neural knowledge bases\" via predicting masked words.\nSurprisingly, we find that this may not work for numerical commonsense\nknowledge (e.g., a bird usually has two legs). In this paper, we investigate\nwhether and to what extent we can induce numerical commonsense knowledge from\nPTLMs as well as the robustness of this process. To study this, we introduce a\nnovel probing task with a diagnostic dataset, NumerSense, containing 13.6k\nmasked-word-prediction probes (10.5k for fine-tuning and 3.1k for testing). Our\nanalysis reveals that: (1) BERT and its stronger variant RoBERTa perform poorly\non the diagnostic dataset prior to any fine-tuning; (2) fine-tuning with\ndistant supervision brings some improvement; (3) the best supervised model\nstill performs poorly as compared to human performance (54.06% vs 96.3% in\naccuracy).", "published": "2020-05-02 02:47:02", "link": "http://arxiv.org/abs/2005.00683v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Imitation Game for Learning Semantic Parsers from User Interaction", "abstract": "Despite the widely successful applications, bootstrapping and fine-tuning\nsemantic parsers are still a tedious process with challenges such as costly\ndata annotation and privacy risks. In this paper, we suggest an alternative,\nhuman-in-the-loop methodology for learning semantic parsers directly from\nusers. A semantic parser should be introspective of its uncertainties and\nprompt for user demonstration when uncertain. In doing so it also gets to\nimitate the user behavior and continue improving itself autonomously with the\nhope that eventually it may become as good as the user in interpreting their\nquestions. To combat the sparsity of demonstration, we propose a novel\nannotation-efficient imitation learning algorithm, which iteratively collects\nnew datasets by mixing demonstrated states and confident predictions and\nre-trains the semantic parser in a Dataset Aggregation fashion (Ross et al.,\n2011). We provide a theoretical analysis of its cost bound and also empirically\ndemonstrate its promising performance on the text-to-SQL problem. Code will be\navailable at https://github.com/sunlab-osu/MISP.", "published": "2020-05-02 03:30:49", "link": "http://arxiv.org/abs/2005.00689v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Connecting the Dots: A Knowledgeable Path Generator for Commonsense\n  Question Answering", "abstract": "Commonsense question answering (QA) requires background knowledge which is\nnot explicitly stated in a given context. Prior works use commonsense knowledge\ngraphs (KGs) to obtain this knowledge for reasoning. However, relying entirely\non these KGs may not suffice, considering their limited coverage and the\ncontextual dependence of their knowledge. In this paper, we augment a general\ncommonsense QA framework with a knowledgeable path generator. By extrapolating\nover existing paths in a KG with a state-of-the-art language model, our\ngenerator learns to connect a pair of entities in text with a dynamic, and\npotentially novel, multi-hop relational path. Such paths can provide structured\nevidence for solving commonsense questions without fine-tuning the path\ngenerator. Experiments on two datasets show the superiority of our method over\nprevious works which fully rely on knowledge from KGs (with up to 6%\nimprovement in accuracy), across various amounts of training data. Further\nevaluation suggests that the generated paths are typically interpretable,\nnovel, and relevant to the task.", "published": "2020-05-02 03:53:21", "link": "http://arxiv.org/abs/2005.00691v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "UnifiedQA: Crossing Format Boundaries With a Single QA System", "abstract": "Question answering (QA) tasks have been posed using a variety of formats,\nsuch as extractive span selection, multiple choice, etc. This has led to\nformat-specialized models, and even to an implicit division in the QA\ncommunity. We argue that such boundaries are artificial and perhaps\nunnecessary, given the reasoning abilities we seek to teach are not governed by\nthe format. As evidence, we use the latest advances in language modeling to\nbuild a single pre-trained QA model, UnifiedQA, that performs surprisingly well\nacross 17 QA datasets spanning 4 diverse formats. UnifiedQA performs on par\nwith 9 different models that were trained on individual datasets themselves.\nEven when faced with 12 unseen datasets of observed formats, UnifiedQA performs\nsurprisingly well, showing strong generalization from its out-of-format\ntraining data. Finally, simply fine-tuning this pre-trained QA model into\nspecialized models results in a new state of the art on 6 datasets,\nestablishing UnifiedQA as a strong starting point for building QA systems.", "published": "2020-05-02 04:42:14", "link": "http://arxiv.org/abs/2005.00700v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Benchmark for Structured Procedural Knowledge Extraction from Cooking\n  Videos", "abstract": "Watching instructional videos are often used to learn about procedures. Video\ncaptioning is one way of automatically collecting such knowledge. However, it\nprovides only an indirect, overall evaluation of multimodal models with no\nfiner-grained quantitative measure of what they have learned. We propose\ninstead, a benchmark of structured procedural knowledge extracted from cooking\nvideos. This work is complementary to existing tasks, but requires models to\nproduce interpretable structured knowledge in the form of verb-argument tuples.\nOur manually annotated open-vocabulary resource includes 356 instructional\ncooking videos and 15,523 video clip/sentence-level annotations. Our analysis\nshows that the proposed task is challenging and standard modeling approaches\nlike unsupervised segmentation, semantic role labeling, and visual action\ndetection perform poorly when forced to predict every action of a procedure in\na structured form.", "published": "2020-05-02 05:15:20", "link": "http://arxiv.org/abs/2005.00706v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "ESPRIT: Explaining Solutions to Physical Reasoning Tasks", "abstract": "Neural networks lack the ability to reason about qualitative physics and so\ncannot generalize to scenarios and tasks unseen during training. We propose\nESPRIT, a framework for commonsense reasoning about qualitative physics in\nnatural language that generates interpretable descriptions of physical events.\nWe use a two-step approach of first identifying the pivotal physical events in\nan environment and then generating natural language descriptions of those\nevents using a data-to-text approach. Our framework learns to generate\nexplanations of how the physical simulation will causally evolve so that an\nagent or a human can easily reason about a solution using those interpretable\ndescriptions. Human evaluations indicate that ESPRIT produces crucial\nfine-grained details and has high coverage of physical concepts compared to\neven human annotations. Dataset, code and documentation are available at\nhttps://github.com/salesforce/esprit.", "published": "2020-05-02 07:03:06", "link": "http://arxiv.org/abs/2005.00730v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Teaching Machine Comprehension with Compositional Explanations", "abstract": "Advances in machine reading comprehension (MRC) rely heavily on the\ncollection of large scale human-annotated examples in the form of (question,\nparagraph, answer) triples. In contrast, humans are typically able to\ngeneralize with only a few examples, relying on deeper underlying world\nknowledge, linguistic sophistication, and/or simply superior deductive powers.\nIn this paper, we focus on \"teaching\" machines reading comprehension, using a\nsmall number of semi-structured explanations that explicitly inform machines\nwhy answer spans are correct. We extract structured variables and rules from\nexplanations and compose neural module teachers that annotate instances for\ntraining downstream MRC models. We use learnable neural modules and soft logic\nto handle linguistic variation and overcome sparse coverage; the modules are\njointly optimized with the MRC model to improve final performance. On the SQuAD\ndataset, our proposed method achieves 70.14% F1 score with supervision from 26\nexplanations, comparable to plain supervised learning using 1,100 labeled\ninstances, yielding a 12x speed up.", "published": "2020-05-02 11:54:34", "link": "http://arxiv.org/abs/2005.00806v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Generalized Entropy Regularization or: There's Nothing Special about\n  Label Smoothing", "abstract": "Prior work has explored directly regularizing the output distributions of\nprobabilistic models to alleviate peaky (i.e. over-confident) predictions, a\ncommon sign of overfitting. This class of techniques, of which label smoothing\nis one, has a connection to entropy regularization. Despite the consistent\nsuccess of label smoothing across architectures and data sets in language\ngeneration tasks, two problems remain open: (1) there is little understanding\nof the underlying effects entropy regularizers have on models, and (2) the full\nspace of entropy regularization techniques is largely unexplored. We introduce\na parametric family of entropy regularizers, which includes label smoothing as\na special case, and use it to gain a better understanding of the relationship\nbetween the entropy of a model and its performance on language generation\ntasks. We also find that variance in model performance can be explained largely\nby the resulting entropy of the model. Lastly, we find that label smoothing\nprovably does not allow for sparsity in an output distribution, an undesirable\nproperty for language generation models, and therefore advise the use of other\nentropy regularization methods in its place.", "published": "2020-05-02 12:46:28", "link": "http://arxiv.org/abs/2005.00820v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ENGINE: Energy-Based Inference Networks for Non-Autoregressive Machine\n  Translation", "abstract": "We propose to train a non-autoregressive machine translation model to\nminimize the energy defined by a pretrained autoregressive model. In\nparticular, we view our non-autoregressive translation system as an inference\nnetwork (Tu and Gimpel, 2018) trained to minimize the autoregressive teacher\nenergy. This contrasts with the popular approach of training a\nnon-autoregressive model on a distilled corpus consisting of the beam-searched\noutputs of such a teacher model. Our approach, which we call ENGINE\n(ENerGy-based Inference NEtworks), achieves state-of-the-art non-autoregressive\nresults on the IWSLT 2014 DE-EN and WMT 2016 RO-EN datasets, approaching the\nperformance of autoregressive models.", "published": "2020-05-02 15:06:47", "link": "http://arxiv.org/abs/2005.00850v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Single Model Ensemble using Pseudo-Tags and Distinct Vectors", "abstract": "Model ensemble techniques often increase task performance in neural networks;\nhowever, they require increased time, memory, and management effort. In this\nstudy, we propose a novel method that replicates the effects of a model\nensemble with a single model. Our approach creates K-virtual models within a\nsingle parameter space using K-distinct pseudo-tags and K-distinct vectors.\nExperiments on text classification and sequence labeling tasks on several\ndatasets demonstrate that our method emulates or outperforms a traditional\nmodel ensemble with 1/K-times fewer parameters.", "published": "2020-05-02 16:23:47", "link": "http://arxiv.org/abs/2005.00879v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Clue: Cross-modal Coherence Modeling for Caption Generation", "abstract": "We use coherence relations inspired by computational models of discourse to\nstudy the information needs and goals of image captioning. Using an annotation\nprotocol specifically devised for capturing image--caption coherence relations,\nwe annotate 10,000 instances from publicly-available image--caption pairs. We\nintroduce a new task for learning inferences in imagery and text, coherence\nrelation prediction, and show that these coherence annotations can be exploited\nto learn relation classifiers as an intermediary step, and also train\ncoherence-aware, controllable image captioning models. The results show a\ndramatic improvement in the consistency and quality of the generated captions\nwith respect to information needs specified via coherence relations.", "published": "2020-05-02 19:28:52", "link": "http://arxiv.org/abs/2005.00908v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Examining Citations of Natural Language Processing Literature", "abstract": "We extracted information from the ACL Anthology (AA) and Google Scholar (GS)\nto examine trends in citations of NLP papers. We explore questions such as: how\nwell cited are papers of different types (journal articles, conference papers,\ndemo papers, etc.)? how well cited are papers from different areas of within\nNLP? etc. Notably, we show that only about 56\\% of the papers in AA are cited\nten or more times. CL Journal has the most cited papers, but its citation\ndominance has lessened in recent years. On average, long papers get almost\nthree times as many citations as short papers; and papers on sentiment\nclassification, anaphora resolution, and entity recognition have the highest\nmedian citations. The analyses presented here, and the associated dataset of\nNLP papers mapped to citations, have a number of uses including: understanding\nhow the field is growing and quantifying the impact of different types of\npapers.", "published": "2020-05-02 20:01:59", "link": "http://arxiv.org/abs/2005.00912v1", "categories": ["cs.DL", "cs.CL"], "primary_category": "cs.DL"}
{"title": "Improving Non-autoregressive Neural Machine Translation with Monolingual\n  Data", "abstract": "Non-autoregressive (NAR) neural machine translation is usually done via\nknowledge distillation from an autoregressive (AR) model. Under this framework,\nwe leverage large monolingual corpora to improve the NAR model's performance,\nwith the goal of transferring the AR model's generalization ability while\npreventing overfitting. On top of a strong NAR baseline, our experimental\nresults on the WMT14 En-De and WMT16 En-Ro news translation tasks confirm that\nmonolingual data augmentation consistently improves the performance of the NAR\nmodel to approach the teacher AR model's performance, yields comparable or\nbetter results than the best non-iterative NAR methods in the literature and\nhelps reduce overfitting in the training process.", "published": "2020-05-02 22:24:52", "link": "http://arxiv.org/abs/2005.00932v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Understanding and Improving Information Transfer in Multi-Task Learning", "abstract": "We investigate multi-task learning approaches that use a shared feature\nrepresentation for all tasks. To better understand the transfer of task\ninformation, we study an architecture with a shared module for all tasks and a\nseparate output module for each task. We study the theory of this setting on\nlinear and ReLU-activated models. Our key observation is that whether or not\ntasks' data are well-aligned can significantly affect the performance of\nmulti-task learning. We show that misalignment between task data can cause\nnegative transfer (or hurt performance) and provide sufficient conditions for\npositive transfer. Inspired by the theoretical insights, we show that aligning\ntasks' embedding layers leads to performance gains for multi-task training and\ntransfer learning on the GLUE benchmark and sentiment analysis tasks; for\nexample, we obtain a 2.35% GLUE score average improvement on 5 GLUE tasks over\nBERT-LARGE using our alignment method. We also design an SVD-based task\nreweighting scheme and show that it improves the robustness of multi-task\ntraining on a multi-label image dataset.", "published": "2020-05-02 23:43:52", "link": "http://arxiv.org/abs/2005.00944v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Neural Computing for Online Arabic Handwriting Character Recognition\n  using Hard Stroke Features Mining", "abstract": "Online Arabic cursive character recognition is still a big challenge due to\nthe existing complexities including Arabic cursive script styles, writing\nspeed, writer mood and so forth. Due to these unavoidable constraints, the\naccuracy of online Arabic character's recognition is still low and retain space\nfor improvement. In this research, an enhanced method of detecting the desired\ncritical points from vertical and horizontal direction-length of handwriting\nstroke features of online Arabic script recognition is proposed. Each extracted\nstroke feature divides every isolated character into some meaningful pattern\nknown as tokens. A minimum feature set is extracted from these tokens for\nclassification of characters using a multilayer perceptron with a\nback-propagation learning algorithm and modified sigmoid function-based\nactivation function. In this work, two milestones are achieved; firstly, attain\na fixed number of tokens, secondly, minimize the number of the most repetitive\ntokens. For experiments, handwritten Arabic characters are selected from the\nOHASD benchmark dataset to test and evaluate the proposed method. The proposed\nmethod achieves an average accuracy of 98.6% comparable in state of art\ncharacter recognition techniques.", "published": "2020-05-02 23:17:08", "link": "http://arxiv.org/abs/2005.02171v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "IsoBN: Fine-Tuning BERT with Isotropic Batch Normalization", "abstract": "Fine-tuning pre-trained language models (PTLMs), such as BERT and its better\nvariant RoBERTa, has been a common practice for advancing performance in\nnatural language understanding (NLU) tasks. Recent advance in representation\nlearning shows that isotropic (i.e., unit-variance and uncorrelated) embeddings\ncan significantly improve performance on downstream tasks with faster\nconvergence and better generalization. The isotropy of the pre-trained\nembeddings in PTLMs, however, is relatively under-explored. In this paper, we\nanalyze the isotropy of the pre-trained [CLS] embeddings of PTLMs with\nstraightforward visualization, and point out two major issues: high variance in\ntheir standard deviation, and high correlation between different dimensions. We\nalso propose a new network regularization method, isotropic batch normalization\n(IsoBN) to address the issues, towards learning more isotropic representations\nin fine-tuning by dynamically penalizing dominating principal components. This\nsimple yet effective fine-tuning method yields about 1.0 absolute increment on\nthe average of seven NLU tasks.", "published": "2020-05-02 11:49:09", "link": "http://arxiv.org/abs/2005.02178v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mega-COV: A Billion-Scale Dataset of 100+ Languages for COVID-19", "abstract": "We describe Mega-COV, a billion-scale dataset from Twitter for studying\nCOVID-19. The dataset is diverse (covers 268 countries), longitudinal (goes as\nback as 2007), multilingual (comes in 100+ languages), and has a significant\nnumber of location-tagged tweets (~169M tweets). We release tweet IDs from the\ndataset. We also develop and release two powerful models, one for identifying\nwhether or not a tweet is related to the pandemic (best F1=97%) and another for\ndetecting misinformation about COVID-19 (best F1=92%). A human annotation study\nreveals the utility of our models on a subset of Mega-COV. Our data and models\ncan be useful for studying a wide host of phenomena related to the pandemic.\nMega-COV and our models are publicly available.", "published": "2020-05-02 10:23:27", "link": "http://arxiv.org/abs/2005.06012v4", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "A Comprehensive Survey of Grammar Error Correction", "abstract": "Grammar error correction (GEC) is an important application aspect of natural\nlanguage processing techniques. The past decade has witnessed significant\nprogress achieved in GEC for the sake of increasing popularity of machine\nlearning and deep learning, especially in late 2010s when near human-level GEC\nsystems are available. However, there is no prior work focusing on the whole\nrecapitulation of the progress. We present the first survey in GEC for a\ncomprehensive retrospect of the literature in this area. We first give the\nintroduction of five public datasets, data annotation schema, two important\nshared tasks and four standard evaluation metrics. More importantly, we discuss\nfour kinds of basic approaches, including statistical machine translation based\napproach, neural machine translation based approach, classification based\napproach and language model based approach, six commonly applied performance\nboosting techniques for GEC systems and two data augmentation methods. Since\nGEC is typically viewed as a sister task of machine translation, many GEC\nsystems are based on neural machine translation (NMT) approaches, where the\nneural sequence-to-sequence model is applied. Similarly, some performance\nboosting techniques are adapted from machine translation and are successfully\ncombined with GEC systems for enhancement on the final performance.\nFurthermore, we conduct an analysis in level of basic approaches, performance\nboosting techniques and integrated GEC systems based on their experiment\nresults respectively for more clear patterns and conclusions. Finally, we\ndiscuss five prospective directions for future GEC researches.", "published": "2020-05-02 04:46:52", "link": "http://arxiv.org/abs/2005.06600v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Are Emojis Emotional? A Study to Understand the Association between\n  Emojis and Emotions", "abstract": "Given the growing ubiquity of emojis in language, there is a need for methods\nand resources that shed light on their meaning and communicative role. One\nconspicuous aspect of emojis is their use to convey affect in ways that may\notherwise be non-trivial to achieve. In this paper, we seek to explore the\nconnection between emojis and emotions by means of a new dataset consisting of\nhuman-solicited association ratings. We additionally conduct experiments to\nassess to what extent such associations can be inferred from existing data,\nsuch that similar associations can be predicted for a larger set of emojis. Our\nexperiments show that this succeeds when high-quality word-level information is\navailable.", "published": "2020-05-02 04:04:42", "link": "http://arxiv.org/abs/2005.00693v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "DeFormer: Decomposing Pre-trained Transformers for Faster Question\n  Answering", "abstract": "Transformer-based QA models use input-wide self-attention -- i.e. across both\nthe question and the input passage -- at all layers, causing them to be slow\nand memory-intensive. It turns out that we can get by without input-wide\nself-attention at all layers, especially in the lower layers. We introduce\nDeFormer, a decomposed transformer, which substitutes the full self-attention\nwith question-wide and passage-wide self-attentions in the lower layers. This\nallows for question-independent processing of the input text representations,\nwhich in turn enables pre-computing passage representations reducing runtime\ncompute drastically. Furthermore, because DeFormer is largely similar to the\noriginal model, we can initialize DeFormer with the pre-training weights of a\nstandard transformer, and directly fine-tune on the target QA dataset. We show\nDeFormer versions of BERT and XLNet can be used to speed up QA by over 4.3x and\nwith simple distillation-based losses they incur only a 1% drop in accuracy. We\nopen source the code at https://github.com/StonyBrookNLP/deformer.", "published": "2020-05-02 04:28:22", "link": "http://arxiv.org/abs/2005.00697v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Gender Bias in Multilingual Embeddings and Cross-Lingual Transfer", "abstract": "Multilingual representations embed words from many languages into a single\nsemantic space such that words with similar meanings are close to each other\nregardless of the language. These embeddings have been widely used in various\nsettings, such as cross-lingual transfer, where a natural language processing\n(NLP) model trained on one language is deployed to another language. While the\ncross-lingual transfer techniques are powerful, they carry gender bias from the\nsource to target languages. In this paper, we study gender bias in multilingual\nembeddings and how it affects transfer learning for NLP applications. We create\na multilingual dataset for bias analysis and propose several ways for\nquantifying bias in multilingual representations from both the intrinsic and\nextrinsic perspectives. Experimental results show that the magnitude of bias in\nthe multilingual representations changes differently when we align the\nembeddings to different target spaces and that the alignment direction can also\nhave an influence on the bias in transfer learning. We further provide\nrecommendations for using the multilingual word representations for downstream\ntasks.", "published": "2020-05-02 04:34:37", "link": "http://arxiv.org/abs/2005.00699v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Girl Has A Name: Detecting Authorship Obfuscation", "abstract": "Authorship attribution aims to identify the author of a text based on the\nstylometric analysis. Authorship obfuscation, on the other hand, aims to\nprotect against authorship attribution by modifying a text's style. In this\npaper, we evaluate the stealthiness of state-of-the-art authorship obfuscation\nmethods under an adversarial threat model. An obfuscator is stealthy to the\nextent an adversary finds it challenging to detect whether or not a text\nmodified by the obfuscator is obfuscated - a decision that is key to the\nadversary interested in authorship attribution. We show that the existing\nauthorship obfuscation methods are not stealthy as their obfuscated texts can\nbe identified with an average F1 score of 0.87. The reason for the lack of\nstealthiness is that these obfuscators degrade text smoothness, as ascertained\nby neural language models, in a detectable manner. Our results highlight the\nneed to develop stealthy authorship obfuscation methods that can better protect\nthe identity of an author seeking anonymity.", "published": "2020-05-02 04:52:55", "link": "http://arxiv.org/abs/2005.00702v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AVA: an Automatic eValuation Approach to Question Answering Systems", "abstract": "We introduce AVA, an automatic evaluation approach for Question Answering,\nwhich given a set of questions associated with Gold Standard answers, can\nestimate system Accuracy. AVA uses Transformer-based language models to encode\nquestion, answer, and reference text. This allows for effectively measuring the\nsimilarity between the reference and an automatic answer, biased towards the\nquestion semantics. To design, train and test AVA, we built multiple large\ntraining, development, and test sets on both public and industrial benchmarks.\nOur innovative solutions achieve up to 74.7% in F1 score in predicting human\njudgement for single answers. Additionally, AVA can be used to evaluate the\noverall system Accuracy with an RMSE, ranging from 0.02 to 0.09, depending on\nthe availability of multiple references.", "published": "2020-05-02 05:00:16", "link": "http://arxiv.org/abs/2005.00705v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Obtaining Faithful Interpretations from Compositional Neural Networks", "abstract": "Neural module networks (NMNs) are a popular approach for modeling\ncompositionality: they achieve high accuracy when applied to problems in\nlanguage and vision, while reflecting the compositional structure of the\nproblem in the network architecture. However, prior work implicitly assumed\nthat the structure of the network modules, describing the abstract reasoning\nprocess, provides a faithful explanation of the model's reasoning; that is,\nthat all modules perform their intended behaviour. In this work, we propose and\nconduct a systematic evaluation of the intermediate outputs of NMNs on NLVR2\nand DROP, two datasets which require composing multiple reasoning steps. We\nfind that the intermediate outputs differ from the expected output,\nillustrating that the network structure does not provide a faithful explanation\nof model behaviour. To remedy that, we train the model with auxiliary\nsupervision and propose particular choices for module architecture that yield\nmuch better faithfulness, at a minimal cost to accuracy.", "published": "2020-05-02 06:50:35", "link": "http://arxiv.org/abs/2005.00724v2", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Synthesizer: Rethinking Self-Attention in Transformer Models", "abstract": "The dot product self-attention is known to be central and indispensable to\nstate-of-the-art Transformer models. But is it really required? This paper\ninvestigates the true importance and contribution of the dot product-based\nself-attention mechanism on the performance of Transformer models. Via\nextensive experiments, we find that (1) random alignment matrices surprisingly\nperform quite competitively and (2) learning attention weights from token-token\n(query-key) interactions is useful but not that important after all. To this\nend, we propose \\textsc{Synthesizer}, a model that learns synthetic attention\nweights without token-token interactions. In our experiments, we first show\nthat simple Synthesizers achieve highly competitive performance when compared\nagainst vanilla Transformer models across a range of tasks, including machine\ntranslation, language modeling, text generation and GLUE/SuperGLUE benchmarks.\nWhen composed with dot product attention, we find that Synthesizers\nconsistently outperform Transformers. Moreover, we conduct additional\ncomparisons of Synthesizers against Dynamic Convolutions, showing that simple\nRandom Synthesizer is not only $60\\%$ faster but also improves perplexity by a\nrelative $3.5\\%$. Finally, we show that simple factorized Synthesizers can\noutperform Linformers on encoding only tasks.", "published": "2020-05-02 08:16:19", "link": "http://arxiv.org/abs/2005.00743v3", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "RICA: Evaluating Robust Inference Capabilities Based on Commonsense\n  Axioms", "abstract": "Pre-trained language models (PTLMs) have achieved impressive performance on\ncommonsense inference benchmarks, but their ability to employ commonsense to\nmake robust inferences, which is crucial for effective communications with\nhumans, is debated. In the pursuit of advancing fluid human-AI communication,\nwe propose a new challenge, RICA: Robust Inference capability based on\nCommonsense Axioms, that evaluates robust commonsense inference despite textual\nperturbations. To generate data for this challenge, we develop a systematic and\nscalable procedure using commonsense knowledge bases and probe PTLMs across two\ndifferent evaluation settings. Extensive experiments on our generated probe\nsets with more than 10k statements show that PTLMs perform no better than\nrandom guessing on the zero-shot setting, are heavily impacted by statistical\nbiases, and are not robust to perturbation attacks. We also find that\nfine-tuning on similar statements offer limited gains, as PTLMs still fail to\ngeneralize to unseen inferences. Our new large-scale benchmark exposes a\nsignificant gap between PTLMs and human-level language understanding and offers\na new challenge for PTLMs to demonstrate commonsense.", "published": "2020-05-02 10:36:55", "link": "http://arxiv.org/abs/2005.00782v4", "categories": ["cs.CL", "cs.AI", "cs.LO"], "primary_category": "cs.CL"}
{"title": "Is Multihop QA in DiRe Condition? Measuring and Reducing Disconnected\n  Reasoning", "abstract": "Has there been real progress in multi-hop question-answering? Models often\nexploit dataset artifacts to produce correct answers, without connecting\ninformation across multiple supporting facts. This limits our ability to\nmeasure true progress and defeats the purpose of building multi-hop QA\ndatasets. We make three contributions towards addressing this. First, we\nformalize such undesirable behavior as disconnected reasoning across subsets of\nsupporting facts. This allows developing a model-agnostic probe for measuring\nhow much any model can cheat via disconnected reasoning. Second, using a notion\nof \\emph{contrastive support sufficiency}, we introduce an automatic\ntransformation of existing datasets that reduces the amount of disconnected\nreasoning. Third, our experiments suggest that there hasn't been much progress\nin multi-hop QA in the reading comprehension setting. For a recent large-scale\nmodel (XLNet), we show that only 18 points out of its answer F1 score of 72 on\nHotpotQA are obtained through multifact reasoning, roughly the same as that of\na simpler RNN baseline. Our transformation substantially reduces disconnected\nreasoning (19 points in answer F1). It is complementary to adversarial\napproaches, yielding further reductions in conjunction.", "published": "2020-05-02 11:01:07", "link": "http://arxiv.org/abs/2005.00789v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Enhancing Text-based Reinforcement Learning Agents with Commonsense\n  Knowledge", "abstract": "In this paper, we consider the recent trend of evaluating progress on\nreinforcement learning technology by using text-based environments and games as\nevaluation environments. This reliance on text brings advances in natural\nlanguage processing into the ambit of these agents, with a recurring thread\nbeing the use of external knowledge to mimic and better human-level\nperformance. We present one such instantiation of agents that use commonsense\nknowledge from ConceptNet to show promising performance on two text-based\nenvironments.", "published": "2020-05-02 12:07:02", "link": "http://arxiv.org/abs/2005.00811v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "MultiQT: Multimodal Learning for Real-Time Question Tracking in Speech", "abstract": "We address a challenging and practical task of labeling questions in speech\nin real time during telephone calls to emergency medical services in English,\nwhich embeds within a broader decision support system for emergency\ncall-takers. We propose a novel multimodal approach to real-time sequence\nlabeling in speech. Our model treats speech and its own textual representation\nas two separate modalities or views, as it jointly learns from streamed audio\nand its noisy transcription into text via automatic speech recognition. Our\nresults show significant gains of jointly learning from the two modalities when\ncompared to text or audio only, under adverse noise and limited volume of\ntraining data. The results generalize to medical symptoms detection where we\nobserve a similar pattern of improvements with multimodal learning.", "published": "2020-05-02 12:16:14", "link": "http://arxiv.org/abs/2005.00812v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Social Biases in NLP Models as Barriers for Persons with Disabilities", "abstract": "Building equitable and inclusive NLP technologies demands consideration of\nwhether and how social attitudes are represented in ML models. In particular,\nrepresentations encoded in models often inadvertently perpetuate undesirable\nsocial biases from the data on which they are trained. In this paper, we\npresent evidence of such undesirable biases towards mentions of disability in\ntwo different English language models: toxicity prediction and sentiment\nanalysis. Next, we demonstrate that the neural embeddings that are the critical\nfirst step in most NLP pipelines similarly contain undesirable biases towards\nmentions of disability. We end by highlighting topical biases in the discourse\nabout disability which may contribute to the observed model biases; for\ninstance, gun violence, homelessness, and drug addiction are over-represented\nin texts discussing mental illness.", "published": "2020-05-02 12:16:54", "link": "http://arxiv.org/abs/2005.00813v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SEEK: Segmented Embedding of Knowledge Graphs", "abstract": "In recent years, knowledge graph embedding becomes a pretty hot research\ntopic of artificial intelligence and plays increasingly vital roles in various\ndownstream applications, such as recommendation and question answering.\nHowever, existing methods for knowledge graph embedding can not make a proper\ntrade-off between the model complexity and the model expressiveness, which\nmakes them still far from satisfactory. To mitigate this problem, we propose a\nlightweight modeling framework that can achieve highly competitive relational\nexpressiveness without increasing the model complexity. Our framework focuses\non the design of scoring functions and highlights two critical characteristics:\n1) facilitating sufficient feature interactions; 2) preserving both symmetry\nand antisymmetry properties of relations. It is noteworthy that owing to the\ngeneral and elegant design of scoring functions, our framework can incorporate\nmany famous existing methods as special cases. Moreover, extensive experiments\non public benchmarks demonstrate the efficiency and effectiveness of our\nframework. Source codes and data can be found at\n\\url{https://github.com/Wentao-Xu/SEEK}.", "published": "2020-05-02 15:15:50", "link": "http://arxiv.org/abs/2005.00856v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Quantifying Attention Flow in Transformers", "abstract": "In the Transformer model, \"self-attention\" combines information from attended\nembeddings into the representation of the focal embedding in the next layer.\nThus, across layers of the Transformer, information originating from different\ntokens gets increasingly mixed. This makes attention weights unreliable as\nexplanations probes. In this paper, we consider the problem of quantifying this\nflow of information through self-attention. We propose two methods for\napproximating the attention to input tokens given attention weights, attention\nrollout and attention flow, as post hoc methods when we use attention weights\nas the relative relevance of the input tokens. We show that these methods give\ncomplementary views on the flow of information, and compared to raw attention,\nboth yield higher correlations with importance scores of input tokens obtained\nusing an ablation method and input gradients.", "published": "2020-05-02 21:45:27", "link": "http://arxiv.org/abs/2005.00928v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "POSNoise: An Effective Countermeasure Against Topic Biases in Authorship\n  Analysis", "abstract": "Authorship verification (AV) is a fundamental research task in digital text\nforensics, which addresses the problem of whether two texts were written by the\nsame person. In recent years, a variety of AV methods have been proposed that\nfocus on this problem and can be divided into two categories: The first\ncategory refers to such methods that are based on explicitly defined features,\nwhere one has full control over which features are considered and what they\nactually represent. The second category, on the other hand, relates to such AV\nmethods that are based on implicitly defined features, where no control\nmechanism is involved, so that any character sequence in a text can serve as a\npotential feature. However, AV methods belonging to the second category bear\nthe risk that the topic of the texts may bias their classification predictions,\nwhich in turn may lead to misleading conclusions regarding their results. To\ntackle this problem, we propose a preprocessing technique called POSNoise,\nwhich effectively masks topic-related content in a given text. In this way, AV\nmethods are forced to focus on such text units that are more related to the\nwriting style. Our empirical evaluation based on six AV methods (falling into\nthe second category) and seven corpora shows that POSNoise leads to better\nresults compared to a well-known topic masking approach in 34 out of 42 cases,\nwith an increase in accuracy of up to 10%.", "published": "2020-05-02 21:10:24", "link": "http://arxiv.org/abs/2005.06605v2", "categories": ["cs.CL", "cs.DL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Stochastic Neighbor Embedding of Multimodal Relational Data for\n  Image-Text Simultaneous Visualization", "abstract": "Multimodal relational data analysis has become of increasing importance in\nrecent years, for exploring across different domains of data, such as images\nand their text tags obtained from social networking services (e.g., Flickr). A\nvariety of data analysis methods have been developed for visualization; to give\nan example, t-Stochastic Neighbor Embedding (t-SNE) computes low-dimensional\nfeature vectors so that their similarities keep those of the observed data\nvectors. However, t-SNE is designed only for a single domain of data but not\nfor multimodal data; this paper aims at visualizing multimodal relational data\nconsisting of data vectors in multiple domains with relations across these\nvectors. By extending t-SNE, we herein propose Multimodal Relational Stochastic\nNeighbor Embedding (MR-SNE), that (1) first computes augmented relations, where\nwe observe the relations across domains and compute those within each of\ndomains via the observed data vectors, and (2) jointly embeds the augmented\nrelations to a low-dimensional space. Through visualization of Flickr and\nAnimal with Attributes 2 datasets, proposed MR-SNE is compared with other graph\nembedding-based approaches; MR-SNE demonstrates the promising performance.", "published": "2020-05-02 00:39:29", "link": "http://arxiv.org/abs/2005.00670v1", "categories": ["cs.LG", "cs.CL", "cs.CV", "cs.HC", "stat.ML"], "primary_category": "cs.LG"}
{"title": "RMM: A Recursive Mental Model for Dialog Navigation", "abstract": "Language-guided robots must be able to both ask humans questions and\nunderstand answers. Much existing work focuses only on the latter. In this\npaper, we go beyond instruction following and introduce a two-agent task where\none agent navigates and asks questions that a second, guiding agent answers.\nInspired by theory of mind, we propose the Recursive Mental Model (RMM). The\nnavigating agent models the guiding agent to simulate answers given candidate\ngenerated questions. The guiding agent in turn models the navigating agent to\nsimulate navigation steps it would take to generate answers. We use the\nprogress agents make towards the goal as a reinforcement learning reward signal\nto directly inform not only navigation actions, but also both question and\nanswer generation. We demonstrate that RMM enables better generalization to\nnovel environments. Interlocutor modelling may be a way forward for human-agent\ndialogue where robots need to both ask and answer questions.", "published": "2020-05-02 06:57:14", "link": "http://arxiv.org/abs/2005.00728v2", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.RO"], "primary_category": "cs.CL"}
{"title": "Addressing Missing Labels in Large-Scale Sound Event Recognition Using a\n  Teacher-Student Framework With Loss Masking", "abstract": "The study of label noise in sound event recognition has recently gained\nattention with the advent of larger and noisier datasets. This work addresses\nthe problem of missing labels, one of the big weaknesses of large audio\ndatasets, and one of the most conspicuous issues for AudioSet. We propose a\nsimple and model-agnostic method based on a teacher-student framework with loss\nmasking to first identify the most critical missing label candidates, and then\nignore their contribution during the learning process. We find that a simple\noptimisation of the training label set improves recognition performance without\nadditional computation. We discover that most of the improvement comes from\nignoring a critical tiny portion of the missing labels. We also show that the\ndamage done by missing labels is larger as the training set gets smaller, yet\nit can still be observed even when training with massive amounts of audio. We\nbelieve these insights can generalize to other large-scale datasets.", "published": "2020-05-02 16:21:20", "link": "http://arxiv.org/abs/2005.00878v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
