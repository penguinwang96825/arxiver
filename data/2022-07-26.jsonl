{"title": "Controllable User Dialogue Act Augmentation for Dialogue State Tracking", "abstract": "Prior work has demonstrated that data augmentation is useful for improving\ndialogue state tracking. However, there are many types of user utterances,\nwhile the prior method only considered the simplest one for augmentation,\nraising the concern about poor generalization capability. In order to better\ncover diverse dialogue acts and control the generation quality, this paper\nproposes controllable user dialogue act augmentation (CUDA-DST) to augment user\nutterances with diverse behaviors. With the augmented data, different state\ntrackers gain improvement and show better robustness, achieving the\nstate-of-the-art performance on MultiWOZ 2.1", "published": "2022-07-26 09:04:48", "link": "http://arxiv.org/abs/2207.12757v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Training Effective Neural Sentence Encoders from Automatically Mined\n  Paraphrases", "abstract": "Sentence embeddings are commonly used in text clustering and semantic\nretrieval tasks. State-of-the-art sentence representation methods are based on\nartificial neural networks fine-tuned on large collections of manually labeled\nsentence pairs. Sufficient amount of annotated data is available for\nhigh-resource languages such as English or Chinese. In less popular languages,\nmultilingual models have to be used, which offer lower performance. In this\npublication, we address this problem by proposing a method for training\neffective language-specific sentence encoders without manually labeled data.\nOur approach is to automatically construct a dataset of paraphrase pairs from\nsentence-aligned bilingual text corpora. We then use the collected data to\nfine-tune a Transformer language model with an additional recurrent pooling\nlayer. Our sentence encoder can be trained in less than a day on a single\ngraphics card, achieving high performance on a diverse set of sentence-level\ntasks. We evaluate our method on eight linguistic tasks in Polish, comparing it\nwith the best available multilingual sentence encoders.", "published": "2022-07-26 09:08:56", "link": "http://arxiv.org/abs/2207.12759v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Equivariant and Invariant Grounding for Video Question Answering", "abstract": "Video Question Answering (VideoQA) is the task of answering the natural\nlanguage questions about a video. Producing an answer requires understanding\nthe interplay across visual scenes in video and linguistic semantics in\nquestion. However, most leading VideoQA models work as black boxes, which make\nthe visual-linguistic alignment behind the answering process obscure. Such\nblack-box nature calls for visual explainability that reveals ``What part of\nthe video should the model look at to answer the question?''. Only a few works\npresent the visual explanations in a post-hoc fashion, which emulates the\ntarget model's answering process via an additional method. Nonetheless, the\nemulation struggles to faithfully exhibit the visual-linguistic alignment\nduring answering.\n  Instead of post-hoc explainability, we focus on intrinsic interpretability to\nmake the answering process transparent. At its core is grounding the\nquestion-critical cues as the causal scene to yield answers, while rolling out\nthe question-irrelevant information as the environment scene. Taking a causal\nlook at VideoQA, we devise a self-interpretable framework, Equivariant and\nInvariant Grounding for Interpretable VideoQA (EIGV). Specifically, the\nequivariant grounding encourages the answering to be sensitive to the semantic\nchanges in the causal scene and question; in contrast, the invariant grounding\nenforces the answering to be insensitive to the changes in the environment\nscene. By imposing them on the answering process, EIGV is able to distinguish\nthe causal scene from the environment information, and explicitly present the\nvisual-linguistic alignment. Extensive experiments on three benchmark datasets\njustify the superiority of EIGV in terms of accuracy and visual\ninterpretability over the leading baselines.", "published": "2022-07-26 10:01:02", "link": "http://arxiv.org/abs/2207.12783v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hansel: A Chinese Few-Shot and Zero-Shot Entity Linking Benchmark", "abstract": "Modern Entity Linking (EL) systems entrench a popularity bias, yet there is\nno dataset focusing on tail and emerging entities in languages other than\nEnglish. We present Hansel, a new benchmark in Chinese that fills the vacancy\nof non-English few-shot and zero-shot EL challenges. The test set of Hansel is\nhuman annotated and reviewed, created with a novel method for collecting\nzero-shot EL datasets. It covers 10K diverse documents in news, social media\nposts and other web articles, with Wikidata as its target Knowledge Base. We\ndemonstrate that the existing state-of-the-art EL system performs poorly on\nHansel (R@1 of 36.6% on Few-Shot). We then establish a strong baseline that\nscores a R@1 of 46.2% on Few-Shot and 76.6% on Zero-Shot on our dataset. We\nalso show that our baseline achieves competitive results on TAC-KBP2015 Chinese\nEntity Linking task.", "published": "2022-07-26 16:09:07", "link": "http://arxiv.org/abs/2207.13005v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey of Intent Classification and Slot-Filling Datasets for\n  Task-Oriented Dialog", "abstract": "Interest in dialog systems has grown substantially in the past decade. By\nextension, so too has interest in developing and improving intent\nclassification and slot-filling models, which are two components that are\ncommonly used in task-oriented dialog systems. Moreover, good evaluation\nbenchmarks are important in helping to compare and analyze systems that\nincorporate such models. Unfortunately, much of the literature in the field is\nlimited to analysis of relatively few benchmark datasets. In an effort to\npromote more robust analyses of task-oriented dialog systems, we have conducted\na survey of publicly available datasets for the tasks of intent classification\nand slot-filling. We catalog the important characteristics of each dataset, and\noffer discussion on the applicability, strengths, and weaknesses of each. Our\ngoal is that this survey aids in increasing the accessibility of these\ndatasets, which we hope will enable their use in future evaluations of intent\nclassification and slot-filling models for task-oriented dialog systems.", "published": "2022-07-26 23:20:03", "link": "http://arxiv.org/abs/2207.13211v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Visual Representation from Modality-Shared Contrastive\n  Language-Image Pre-training", "abstract": "Large-scale multi-modal contrastive pre-training has demonstrated great\nutility to learn transferable features for a range of downstream tasks by\nmapping multiple modalities into a shared embedding space. Typically, this has\nemployed separate encoders for each modality. However, recent work suggests\nthat transformers can support learning across multiple modalities and allow\nknowledge sharing. Inspired by this, we investigate a variety of\nModality-Shared Contrastive Language-Image Pre-training (MS-CLIP) frameworks.\nMore specifically, we question how many parameters of a transformer model can\nbe shared across modalities during contrastive pre-training, and rigorously\nexamine architectural design choices that position the proportion of parameters\nshared along a spectrum. In studied conditions, we observe that a mostly\nunified encoder for vision and language signals outperforms all other\nvariations that separate more parameters. Additionally, we find that\nlight-weight modality-specific parallel modules further improve performance.\nExperimental results show that the proposed MS-CLIP approach outperforms\nvanilla CLIP by up to 13\\% relative in zero-shot ImageNet classification\n(pre-trained on YFCC-100M), while simultaneously supporting a reduction of\nparameters. In addition, our approach outperforms vanilla CLIP by 1.6 points in\nlinear probing on a collection of 24 downstream vision tasks. Furthermore, we\ndiscover that sharing parameters leads to semantic concepts from different\nmodalities being encoded more closely in the embedding space, facilitating the\ntransferring of common semantic structure (e.g., attention patterns) from\nlanguage to vision. Code is available at\n\\href{https://github.com/Hxyou/MSCLIP}{URL}.", "published": "2022-07-26 05:19:16", "link": "http://arxiv.org/abs/2207.12661v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Advanced Conditional Variational Autoencoders (A-CVAE): Towards\n  interpreting open-domain conversation generation via disentangling latent\n  feature representation", "abstract": "Currently end-to-end deep learning based open-domain dialogue systems remain\nblack box models, making it easy to generate irrelevant contents with\ndata-driven models. Specifically, latent variables are highly entangled with\ndifferent semantics in the latent space due to the lack of priori knowledge to\nguide the training. To address this problem, this paper proposes to harness the\ngenerative model with a priori knowledge through a cognitive approach involving\nmesoscopic scale feature disentanglement. Particularly, the model integrates\nthe macro-level guided-category knowledge and micro-level open-domain dialogue\ndata for the training, leveraging the priori knowledge into the latent space,\nwhich enables the model to disentangle the latent variables within the\nmesoscopic scale. Besides, we propose a new metric for open-domain dialogues,\nwhich can objectively evaluate the interpretability of the latent space\ndistribution. Finally, we validate our model on different datasets and\nexperimentally demonstrate that our model is able to generate higher quality\nand more interpretable dialogues than other models.", "published": "2022-07-26 07:39:36", "link": "http://arxiv.org/abs/2207.12696v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Automated News Bias Classifier Using Caenorhabditis Elegans Inspired\n  Recursive Feedback Network Architecture", "abstract": "Traditional approaches to classify the political bias of news articles have\nfailed to generate accurate, generalizable results. Existing networks premised\non CNNs and DNNs lack a model to identify and extrapolate subtle indicators of\nbias like word choice, context, and presentation. In this paper, we propose a\nnetwork architecture that achieves human-level accuracy in assigning bias\nclassifications to articles. The underlying model is based on a novel Mesh\nNeural Network (MNN),this structure enables feedback and feedforward synaptic\nconnections between any two neurons in the mesh. The MNN ontains six network\nconfigurations that utilize Bernoulli based random sampling, pre-trained DNNs,\nand a network modelled after the C. Elegans nematode. The model is trained on\nover ten-thousand articles scraped from AllSides.com which are labelled to\nindicate political bias. The parameters of the network are then evolved using a\ngenetic algorithm suited to the feedback neural structure. Finally, the best\nperforming model is applied to five popular news sources in the United States\nover a fifty-day trial to quantify political biases in the articles they\ndisplay. We hope our project can spur research into biological solutions for\nNLP tasks and provide accurate tools for citizens to understand subtle biases\nin the articles they consume.", "published": "2022-07-26 08:26:26", "link": "http://arxiv.org/abs/2207.12724v1", "categories": ["cs.NE", "cs.CL"], "primary_category": "cs.NE"}
{"title": "Learning structures of the French clinical language:development and\n  validation of word embedding models using 21 million clinical reports from\n  electronic health records", "abstract": "Background\n  Clinical studies using real-world data may benefit from exploiting clinical\nreports, a particularly rich albeit unstructured medium. To that end, natural\nlanguage processing can extract relevant information. Methods based on transfer\nlearning using pre-trained language models have achieved state-of-the-art\nresults in most NLP applications; however, publicly available models lack\nexposure to speciality-languages, especially in the medical field.\n  Objective\n  We aimed to evaluate the impact of adapting a language model to French\nclinical reports on downstream medical NLP tasks.\n  Methods\n  We leveraged a corpus of 21M clinical reports collected from August 2017 to\nJuly 2021 at the Greater Paris University Hospitals (APHP) to produce two\nCamemBERT architectures on speciality language: one retrained from scratch and\nthe other using CamemBERT as its initialisation. We used two French annotated\nmedical datasets to compare our language models to the original CamemBERT\nnetwork, evaluating the statistical significance of improvement with the\nWilcoxon test.\n  Results\n  Our models pretrained on clinical reports increased the average F1-score on\nAPMed (an APHP-specific task) by 3 percentage points to 91%, a statistically\nsignificant improvement. They also achieved performance comparable to the\noriginal CamemBERT on QUAERO. These results hold true for the fine-tuned and\nfrom-scratch versions alike, starting from very few pre-training samples.\n  Conclusions\n  We confirm previous literature showing that adapting generalist pre-train\nlanguage models such as CamenBERT on speciality corpora improves their\nperformance for downstream clinical NLP tasks. Our results suggest that\nretraining from scratch does not induce a statistically significant performance\ngain compared to fine-tuning.", "published": "2022-07-26 14:46:34", "link": "http://arxiv.org/abs/2207.12940v1", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "When BERT Fails -- The Limits of EHR Classification", "abstract": "Transformers are powerful text representation learners, useful for all kinds\nof clinical decision support tasks. Although they outperform baselines on\nreadmission prediction, they are not infallible. Here, we look into one such\nfailure case, and report patterns that lead to inferior predictive performance.", "published": "2022-07-26 17:18:24", "link": "http://arxiv.org/abs/2208.10245v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "NewsStories: Illustrating articles with visual summaries", "abstract": "Recent self-supervised approaches have used large-scale image-text datasets\nto learn powerful representations that transfer to many tasks without\nfinetuning. These methods often assume that there is one-to-one correspondence\nbetween its images and their (short) captions. However, many tasks require\nreasoning about multiple images and long text narratives, such as describing\nnews articles with visual summaries. Thus, we explore a novel setting where the\ngoal is to learn a self-supervised visual-language representation that is\nrobust to varying text length and the number of images. In addition, unlike\nprior work which assumed captions have a literal relation to the image, we\nassume images only contain loose illustrative correspondence with the text. To\nexplore this problem, we introduce a large-scale multimodal dataset containing\nover 31M articles, 22M images and 1M videos. We show that state-of-the-art\nimage-text alignment methods are not robust to longer narratives with multiple\nimages. Finally, we introduce an intuitive baseline that outperforms these\nmethods on zero-shot image-set retrieval by 10% on the GoodNews dataset.", "published": "2022-07-26 17:34:11", "link": "http://arxiv.org/abs/2207.13061v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Retrieval-Augmented Transformer for Image Captioning", "abstract": "Image captioning models aim at connecting Vision and Language by providing\nnatural language descriptions of input images. In the past few years, the task\nhas been tackled by learning parametric models and proposing visual feature\nextraction advancements or by modeling better multi-modal connections. In this\npaper, we investigate the development of an image captioning approach with a\nkNN memory, with which knowledge can be retrieved from an external corpus to\naid the generation process. Our architecture combines a knowledge retriever\nbased on visual similarities, a differentiable encoder, and a kNN-augmented\nattention layer to predict tokens based on the past context and on text\nretrieved from the external memory. Experimental results, conducted on the COCO\ndataset, demonstrate that employing an explicit external memory can aid the\ngeneration process and increase caption quality. Our work opens up new avenues\nfor improving image captioning models at larger scale.", "published": "2022-07-26 19:35:49", "link": "http://arxiv.org/abs/2207.13162v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Multimodal Neural Machine Translation with Search Engine Based Image\n  Retrieval", "abstract": "Recently, numbers of works shows that the performance of neural machine\ntranslation (NMT) can be improved to a certain extent with using visual\ninformation. However, most of these conclusions are drawn from the analysis of\nexperimental results based on a limited set of bilingual sentence-image pairs,\nsuch as Multi30K. In these kinds of datasets, the content of one bilingual\nparallel sentence pair must be well represented by a manually annotated image,\nwhich is different with the actual translation situation. Some previous works\nare proposed to addressed the problem by retrieving images from exiting\nsentence-image pairs with topic model. However, because of the limited\ncollection of sentence-image pairs they used, their image retrieval method is\ndifficult to deal with the out-of-vocabulary words, and can hardly prove that\nvisual information enhance NMT rather than the co-occurrence of images and\nsentences. In this paper, we propose an open-vocabulary image retrieval methods\nto collect descriptive images for bilingual parallel corpus using image search\nengine. Next, we propose text-aware attentive visual encoder to filter\nincorrectly collected noise images. Experiment results on Multi30K and other\ntwo translation datasets show that our proposed method achieves significant\nimprovements over strong baselines.", "published": "2022-07-26 08:42:06", "link": "http://arxiv.org/abs/2208.00767v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.CV"}
{"title": "MEG-MASC: a high-quality magneto-encephalography dataset for evaluating\n  natural speech processing", "abstract": "The \"MEG-MASC\" dataset provides a curated set of raw magnetoencephalography\n(MEG) recordings of 27 English speakers who listened to two hours of\nnaturalistic stories. Each participant performed two identical sessions,\ninvolving listening to four fictional stories from the Manually Annotated\nSub-Corpus (MASC) intermixed with random word lists and comprehension\nquestions. We time-stamp the onset and offset of each word and phoneme in the\nmetadata of the recording, and organize the dataset according to the 'Brain\nImaging Data Structure' (BIDS). This data collection provides a suitable\nbenchmark to large-scale encoding and decoding analyses of temporally-resolved\nbrain responses to speech. We provide the Python code to replicate several\nvalidations analyses of the MEG evoked related fields such as the temporal\ndecoding of phonetic features and word frequency. All code and MEG, audio and\ntext data are publicly available to keep with best practices in transparent and\nreproducible research.", "published": "2022-07-26 19:17:01", "link": "http://arxiv.org/abs/2208.11488v1", "categories": ["q-bio.QM", "cs.CL", "eess.AS"], "primary_category": "q-bio.QM"}
{"title": "Distinguishing between pre- and post-treatment in the speech of patients\n  with chronic obstructive pulmonary disease", "abstract": "Chronic obstructive pulmonary disease (COPD) causes lung inflammation and\nairflow blockage leading to a variety of respiratory symptoms; it is also a\nleading cause of death and affects millions of individuals around the world.\nPatients often require treatment and hospitalisation, while no cure is\ncurrently available. As COPD predominantly affects the respiratory system,\nspeech and non-linguistic vocalisations present a major avenue for measuring\nthe effect of treatment. In this work, we present results on a new COPD dataset\nof 20 patients, showing that, by employing personalisation through\nspeaker-level feature normalisation, we can distinguish between pre- and\npost-treatment speech with an unweighted average recall (UAR) of up to 82\\,\\%\nin (nested) leave-one-speaker-out cross-validation. We further identify the\nmost important features and link them to pathological voice properties, thus\nenabling an auditory interpretation of treatment effects. Monitoring tools\nbased on such approaches may help objectivise the clinical status of COPD\npatients and facilitate personalised treatment plans.", "published": "2022-07-26 10:01:07", "link": "http://arxiv.org/abs/2207.12784v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multimodal Speech Emotion Recognition using Cross Attention with Aligned\n  Audio and Text", "abstract": "In this paper, we propose a novel speech emotion recognition model called\nCross Attention Network (CAN) that uses aligned audio and text signals as\ninputs. It is inspired by the fact that humans recognize speech as a\ncombination of simultaneously produced acoustic and textual signals. First, our\nmethod segments the audio and the underlying text signals into equal number of\nsteps in an aligned way so that the same time steps of the sequential signals\ncover the same time span in the signals. Together with this technique, we apply\nthe cross attention to aggregate the sequential information from the aligned\nsignals. In the cross attention, each modality is aggregated independently by\napplying the global attention mechanism onto each modality. Then, the attention\nweights of each modality are applied directly to the other modality in a\ncrossed way, so that the CAN gathers the audio and text information from the\nsame time steps based on each modality. In the experiments conducted on the\nstandard IEMOCAP dataset, our model outperforms the state-of-the-art systems by\n2.66% and 3.18% relatively in terms of the weighted and unweighted accuracy.", "published": "2022-07-26 13:44:07", "link": "http://arxiv.org/abs/2207.12895v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Generative Extraction of Audio Classifiers for Speaker Identification", "abstract": "It is perhaps no longer surprising that machine learning models, especially\ndeep neural networks, are particularly vulnerable to attacks. One such\nvulnerability that has been well studied is model extraction: a phenomenon in\nwhich the attacker attempts to steal a victim's model by training a surrogate\nmodel to mimic the decision boundaries of the victim model. Previous works have\ndemonstrated the effectiveness of such an attack and its devastating\nconsequences, but much of this work has been done primarily for image and text\nprocessing tasks. Our work is the first attempt to perform model extraction on\n{\\em audio classification models}. We are motivated by an attacker whose goal\nis to mimic the behavior of the victim's model trained to identify a speaker.\nThis is particularly problematic in security-sensitive domains such as\nbiometric authentication. We find that prior model extraction techniques, where\nthe attacker \\textit{naively} uses a proxy dataset to attack a potential\nvictim's model, fail. We therefore propose the use of a generative model to\ncreate a sufficiently large and diverse pool of synthetic attack queries. We\nfind that our approach is able to extract a victim's model trained on\n\\texttt{LibriSpeech} using queries synthesized with a proxy dataset based off\nof \\texttt{VoxCeleb}; we achieve a test accuracy of 84.41\\% with a budget of 3\nmillion queries.", "published": "2022-07-26 11:21:35", "link": "http://arxiv.org/abs/2207.12816v1", "categories": ["cs.CR", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
{"title": "An exhaustive variable selection study for linear models of soundscape\n  emotions: rankings and Gibbs analysis", "abstract": "In the last decade, soundscapes have become one of the most active topics in\nAcoustics, providing a holistic approach to the acoustic environment, which\ninvolves human perception and context. Soundscapes-elicited emotions are\ncentral and substantially subtle and unnoticed (compared to speech or music).\nCurrently, soundscape emotion recognition is a very active topic in the\nliterature. We provide an exhaustive variable selection study (i.e., a\nselection of the soundscapes indicators) to a well-known dataset\n(emo-soundscapes). We consider linear soundscape emotion models for two\nsoundscapes descriptors: arousal and valence.\n  Several ranking schemes and procedures for selecting the number of variables\nare applied. We have also performed an alternating optimization scheme for\nobtaining the best sequences keeping fixed a certain number of features.\nFurthermore, we have designed a novel technique based on Gibbs sampling, which\nprovides a more complete and clear view of the relevance of each variable.\nFinally, we have also compared our results with the analysis obtained by the\nclassical methods based on p-values. As a result of our study, we suggest two\nsimple and parsimonious linear models of only 7 and 16 variables (within the\n122 possible features) for the two outputs (arousal and valence), respectively.\nThe suggested linear models provide very good and competitive performance, with\n$R^2>0.86$ and $R^2>0.63$ (values obtained after a cross-validation procedure),\nrespectively.", "published": "2022-07-26 08:49:26", "link": "http://arxiv.org/abs/2207.12743v1", "categories": ["cs.SD", "eess.AS", "eess.SP", "stat.CO", "stat.ME"], "primary_category": "cs.SD"}
{"title": "Perception-Aware Attack: Creating Adversarial Music via\n  Reverse-Engineering Human Perception", "abstract": "Recently, adversarial machine learning attacks have posed serious security\nthreats against practical audio signal classification systems, including speech\nrecognition, speaker recognition, and music copyright detection. Previous\nstudies have mainly focused on ensuring the effectiveness of attacking an audio\nsignal classifier via creating a small noise-like perturbation on the original\nsignal. It is still unclear if an attacker is able to create audio signal\nperturbations that can be well perceived by human beings in addition to its\nattack effectiveness. This is particularly important for music signals as they\nare carefully crafted with human-enjoyable audio characteristics.\n  In this work, we formulate the adversarial attack against music signals as a\nnew perception-aware attack framework, which integrates human study into\nadversarial attack design. Specifically, we conduct a human study to quantify\nthe human perception with respect to a change of a music signal. We invite\nhuman participants to rate their perceived deviation based on pairs of original\nand perturbed music signals, and reverse-engineer the human perception process\nby regression analysis to predict the human-perceived deviation given a\nperturbed signal. The perception-aware attack is then formulated as an\noptimization problem that finds an optimal perturbation signal to minimize the\nprediction of perceived deviation from the regressed human perception model. We\nuse the perception-aware framework to design a realistic adversarial music\nattack against YouTube's copyright detector. Experiments show that the\nperception-aware attack produces adversarial music with significantly better\nperceptual quality than prior work.", "published": "2022-07-26 21:40:47", "link": "http://arxiv.org/abs/2207.13192v1", "categories": ["cs.SD", "cs.AI", "cs.CR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
