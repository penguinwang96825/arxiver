{"title": "Legal entity recognition in an agglutinating language and document\n  connection network for EU Legislation and EU/Hungarian Case Law", "abstract": "We have developed an application aiming at federated search for EU and\nHungarian legislation and jurisdiction. It now contains above 1 million\ndocuments, with daily updates. The database holds documents downloaded from the\nEU sources EUR-Lex and Curia Online as well as public jurisdiction documents\nfrom the Constitutional Court of Hungary and The National Office for The\nJudiciary. The application is termed Justeus. Justeus provides comprehensible\nsearch possibilities. Besides free text and metadata (dropdown list) searches,\nit features hierarchical data structures (concept hierarchy trees) of directory\ncodes and classification as well as subject terms. Justeus collects all links\nof a particular document to other documents (court judgements citing other case\nlaw documents as well as legislation, national court decisions referring to EU\nregulation etc.) as tables and directed graph networks. Choosing a document,\nits relations to other documents are visualized in real time as a network.\nNetwork graphs help in identifying key documents influencing or referred by\nmany other documents (legislative and/or jurisdictive) and sets of documents\npredominantly referring to each other (citation networks).", "published": "2019-07-29 08:54:02", "link": "http://arxiv.org/abs/1907.12280v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hierarchical Multi-Label Dialog Act Recognition on Spanish Data", "abstract": "Dialog acts reveal the intention behind the uttered words. Thus, their\nautomatic recognition is important for a dialog system trying to understand its\nconversational partner. The study presented in this article approaches that\ntask on the DIHANA corpus, whose three-level dialog act annotation scheme poses\nproblems which have not been explored in recent studies. In addition to the\nhierarchical problem, the two lower levels pose multi-label classification\nproblems. Furthermore, each level in the hierarchy refers to a different aspect\nconcerning the intention of the speaker both in terms of the structure of the\ndialog and the task. Also, since its dialogs are in Spanish, it allows us to\nassess whether the state-of-the-art approaches on English data generalize to a\ndifferent language. More specifically, we compare the performance of different\nsegment representation approaches focusing on both sequences and patterns of\nwords and assess the importance of the dialog history and the relations between\nthe multiple levels of the hierarchy. Concerning the single-label\nclassification problem posed by the top level, we show that the conclusions\ndrawn on English data also hold on Spanish data. Furthermore, we show that the\napproaches can be adapted to multi-label scenarios. Finally, by hierarchically\ncombining the best classifiers for each level, we achieve the best results\nreported for this corpus.", "published": "2019-07-29 10:12:18", "link": "http://arxiv.org/abs/1907.12316v1", "categories": ["cs.CL", "H.1.2; H.3.1; I.2.7"], "primary_category": "cs.CL"}
{"title": "ERNIE 2.0: A Continual Pre-training Framework for Language Understanding", "abstract": "Recently, pre-trained models have achieved state-of-the-art results in\nvarious language understanding tasks, which indicates that pre-training on\nlarge-scale corpora may play a crucial role in natural language processing.\nCurrent pre-training procedures usually focus on training the model with\nseveral simple tasks to grasp the co-occurrence of words or sentences. However,\nbesides co-occurring, there exists other valuable lexical, syntactic and\nsemantic information in training corpora, such as named entity, semantic\ncloseness and discourse relations. In order to extract to the fullest extent,\nthe lexical, syntactic and semantic information from training corpora, we\npropose a continual pre-training framework named ERNIE 2.0 which builds and\nlearns incrementally pre-training tasks through constant multi-task learning.\nExperimental results demonstrate that ERNIE 2.0 outperforms BERT and XLNet on\n16 tasks including English tasks on GLUE benchmarks and several common tasks in\nChinese. The source codes and pre-trained models have been released at\nhttps://github.com/PaddlePaddle/ERNIE.", "published": "2019-07-29 13:25:37", "link": "http://arxiv.org/abs/1907.12412v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Baseline Neural Machine Translation System for Indian Languages", "abstract": "We present a simple, yet effective, Neural Machine Translation system for\nIndian languages. We demonstrate the feasibility for multiple language pairs,\nand establish a strong baseline for further research.", "published": "2019-07-29 13:59:02", "link": "http://arxiv.org/abs/1907.12437v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Pre-trained Checkpoints for Sequence Generation Tasks", "abstract": "Unsupervised pre-training of large neural models has recently revolutionized\nNatural Language Processing. By warm-starting from the publicly released\ncheckpoints, NLP practitioners have pushed the state-of-the-art on multiple\nbenchmarks while saving significant amounts of compute time. So far the focus\nhas been mainly on the Natural Language Understanding tasks. In this paper, we\ndemonstrate the efficacy of pre-trained checkpoints for Sequence Generation. We\ndeveloped a Transformer-based sequence-to-sequence model that is compatible\nwith publicly available pre-trained BERT, GPT-2 and RoBERTa checkpoints and\nconducted an extensive empirical study on the utility of initializing our\nmodel, both encoder and decoder, with these checkpoints. Our models result in\nnew state-of-the-art results on Machine Translation, Text Summarization,\nSentence Splitting, and Sentence Fusion.", "published": "2019-07-29 14:42:30", "link": "http://arxiv.org/abs/1907.12461v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Mention Detection", "abstract": "Mention detection is an important preprocessing step for annotation and\ninterpretation in applications such as NER and coreference resolution, but few\nstand-alone neural models have been proposed able to handle the full range of\nmentions. In this work, we propose and compare three neural network-based\napproaches to mention detection. The first approach is based on the mention\ndetection part of a state of the art coreference resolution system; the second\nuses ELMO embeddings together with a bidirectional LSTM and a biaffine\nclassifier; the third approach uses the recently introduced BERT model. Our\nbest model (using a biaffine classifier) achieves gains of up to 1.8 percentage\npoints on mention recall when compared with a strong baseline in a HIGH RECALL\ncoreference annotation setting. The same model achieves improvements of up to\n5.3 and 6.2 p.p. when compared with the best-reported mention detection F1 on\nthe CONLL and CRAC coreference data sets respectively in a HIGH F1 annotation\nsetting. We then evaluate our models for coreference resolution by using\nmentions predicted by our best model in start-of-the-art coreference systems.\nThe enhanced model achieved absolute improvements of up to 1.7 and 0.7 p.p.\nwhen compared with our strong baseline systems (pipeline system and end-to-end\nsystem) respectively. For nested NER, the evaluation of our model on the GENIA\ncorpora shows that our model matches or outperforms state-of-the-art models\ndespite not being specifically designed for this task.", "published": "2019-07-29 16:59:53", "link": "http://arxiv.org/abs/1907.12524v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CUNI Systems for the Unsupervised News Translation Task in WMT 2019", "abstract": "In this paper we describe the CUNI translation system used for the\nunsupervised news shared task of the ACL 2019 Fourth Conference on Machine\nTranslation (WMT19). We follow the strategy of Artexte et al. (2018b), creating\na seed phrase-based system where the phrase table is initialized from\ncross-lingual embedding mappings trained on monolingual data, followed by a\nneural machine translation system trained on synthetic parallel data. The\nsynthetic corpus was produced from a monolingual corpus by a tuned PBMT model\nrefined through iterative back-translation. We further focus on the handling of\nnamed entities, i.e. the part of vocabulary where the cross-lingual embedding\nmapping suffers most. Our system reaches a BLEU score of 15.3 on the\nGerman-Czech WMT19 shared task.", "published": "2019-07-29 21:44:50", "link": "http://arxiv.org/abs/1907.12664v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "One-to-X analogical reasoning on word embeddings: a case for diachronic\n  armed conflict prediction from news texts", "abstract": "We extend the well-known word analogy task to a one-to-X formulation,\nincluding one-to-none cases, when no correct answer exists. The task is cast as\na relation discovery problem and applied to historical armed conflicts\ndatasets, attempting to predict new relations of type `location:armed-group'\nbased on data about past events. As the source of semantic information, we use\ndiachronic word embedding models trained on English news texts. A simple\ntechnique to improve diachronic performance in such task is demonstrated, using\na threshold based on a function of cosine distance to decrease the number of\nfalse positives; this approach is shown to be beneficial on two different\ncorpora. Finally, we publish a ready-to-use test set for one-to-X analogy\nevaluation on historical armed conflicts data.", "published": "2019-07-29 22:26:51", "link": "http://arxiv.org/abs/1907.12674v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Machine Translation Evaluation with BERT Regressor", "abstract": "We introduce the metric using BERT (Bidirectional Encoder Representations\nfrom Transformers) (Devlin et al., 2019) for automatic machine translation\nevaluation. The experimental results of the WMT-2017 Metrics Shared Task\ndataset show that our metric achieves state-of-the-art performance in\nsegment-level metrics task for all to-English language pairs.", "published": "2019-07-29 22:53:59", "link": "http://arxiv.org/abs/1907.12679v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "VIANA: Visual Interactive Annotation of Argumentation", "abstract": "Argumentation Mining addresses the challenging tasks of identifying\nboundaries of argumentative text fragments and extracting their relationships.\nFully automated solutions do not reach satisfactory accuracy due to their\ninsufficient incorporation of semantics and domain knowledge. Therefore,\nexperts currently rely on time-consuming manual annotations. In this paper, we\npresent a visual analytics system that augments the manual annotation process\nby automatically suggesting which text fragments to annotate next. The accuracy\nof those suggestions is improved over time by incorporating linguistic\nknowledge and language modeling to learn a measure of argument similarity from\nuser interactions. Based on a long-term collaboration with domain experts, we\nidentify and model five high-level analysis tasks. We enable close reading and\nnote-taking, annotation of arguments, argument reconstruction, extraction of\nargument relations, and exploration of argument graphs. To avoid context\nswitches, we transition between all views through seamless morphing, visually\nanchoring all text- and graph-based layers. We evaluate our system with a\ntwo-stage expert user study based on a corpus of presidential debates. The\nresults show that experts prefer our system over existing solutions due to the\nspeedup provided by the automatic suggestions and the tight integration between\ntext and graph views.", "published": "2019-07-29 13:26:03", "link": "http://arxiv.org/abs/1907.12413v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Joey NMT: A Minimalist NMT Toolkit for Novices", "abstract": "We present Joey NMT, a minimalist neural machine translation toolkit based on\nPyTorch that is specifically designed for novices. Joey NMT provides many\npopular NMT features in a small and simple code base, so that novices can\neasily and quickly learn to use it and adapt it to their needs. Despite its\nfocus on simplicity, Joey NMT supports classic architectures (RNNs,\ntransformers), fast beam search, weight tying, and more, and achieves\nperformance comparable to more complex toolkits on standard benchmarks. We\nevaluate the accessibility of our toolkit in a user study where novices with\ngeneral knowledge about Pytorch and NMT and experts work through a\nself-contained Joey NMT tutorial, showing that novices perform almost as well\nas experts in a subsequent code quiz. Joey NMT is available at\nhttps://github.com/joeynmt/joeynmt .", "published": "2019-07-29 15:35:13", "link": "http://arxiv.org/abs/1907.12484v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Reinforced Dynamic Reasoning for Conversational Question Generation", "abstract": "This paper investigates a new task named Conversational Question Generation\n(CQG) which is to generate a question based on a passage and a conversation\nhistory (i.e., previous turns of question-answer pairs). CQG is a crucial task\nfor developing intelligent agents that can drive question-answering style\nconversations or test user understanding of a given passage. Towards that end,\nwe propose a new approach named Reinforced Dynamic Reasoning (ReDR) network,\nwhich is based on the general encoder-decoder framework but incorporates a\nreasoning procedure in a dynamic manner to better understand what has been\nasked and what to ask next about the passage. To encourage producing meaningful\nquestions, we leverage a popular question answering (QA) model to provide\nfeedback and fine-tune the question generator using a reinforcement learning\nmechanism. Empirical results on the recently released CoQA dataset demonstrate\nthe effectiveness of our method in comparison with various baselines and model\nvariants. Moreover, to show the applicability of our method, we also apply it\nto create multi-turn question-answering conversations for passages in SQuAD.", "published": "2019-07-29 21:56:35", "link": "http://arxiv.org/abs/1907.12667v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A mathematical model for universal semantics", "abstract": "We characterize the meaning of words with language-independent numerical\nfingerprints, through a mathematical analysis of recurring patterns in texts.\nApproximating texts by Markov processes on a long-range time scale, we are able\nto extract topics, discover synonyms, and sketch semantic fields from a\nparticular document of moderate length, without consulting external\nknowledge-base or thesaurus. Our Markov semantic model allows us to represent\neach topical concept by a low-dimensional vector, interpretable as algebraic\ninvariants in succinct statistical operations on the document, targeting local\nenvironments of individual words. These language-independent semantic\nrepresentations enable a robot reader to both understand short texts in a given\nlanguage (automated question-answering) and match medium-length texts across\ndifferent languages (automated word translation). Our semantic fingerprints\nquantify local meaning of words in 14 representative languages across 5 major\nlanguage families, suggesting a universal and cost-effective mechanism by which\nhuman languages are processed at the semantic level. Our protocols and source\ncodes are publicly available on\nhttps://github.com/yajun-zhou/linguae-naturalis-principia-mathematica", "published": "2019-07-29 09:25:49", "link": "http://arxiv.org/abs/1907.12293v7", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Semantic RL with Action Grammars: Data-Efficient Learning of\n  Hierarchical Task Abstractions", "abstract": "Hierarchical Reinforcement Learning algorithms have successfully been applied\nto temporal credit assignment problems with sparse reward signals. However,\nstate-of-the-art algorithms require manual specification of sub-task\nstructures, a sample inefficient exploration phase or lack semantic\ninterpretability. Humans, on the other hand, efficiently detect hierarchical\nsub-structures induced by their surroundings. It has been argued that this\ninference process universally applies to language, logical reasoning as well as\nmotor control. Therefore, we propose a cognitive-inspired Reinforcement\nLearning architecture which uses grammar induction to identify sub-goal\npolicies. By treating an on-policy trajectory as a sentence sampled from the\npolicy-conditioned language of the environment, we identify hierarchical\nconstituents with the help of unsupervised grammatical inference. The resulting\nset of temporal abstractions is called action grammar (Pastra & Aloimonos,\n2012) and unifies symbolic and connectionist approaches to Reinforcement\nLearning. It can be used to facilitate efficient imitation, transfer and online\nlearning.", "published": "2019-07-29 15:27:50", "link": "http://arxiv.org/abs/1907.12477v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Multi-Frame Cross-Entropy Training for Convolutional Neural Networks in\n  Speech Recognition", "abstract": "We introduce Multi-Frame Cross-Entropy training (MFCE) for convolutional\nneural network acoustic models. Recognizing that similar to RNNs, CNNs are in\nnature sequence models that take variable length inputs, we propose to take as\ninput to the CNN a part of an utterance long enough that multiple labels are\npredicted at once, therefore getting cross-entropy loss signal from multiple\nadjacent frames. This increases the amount of label information drastically for\nsmall marginal computational cost. We show large WER improvements on hub5 and\nrt02 after training on the 2000-hour Switchboard benchmark.", "published": "2019-07-29 19:56:46", "link": "http://arxiv.org/abs/1907.13121v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MIRaGe: Multichannel Database Of Room Impulse Responses Measured On\n  High-Resolution Cube-Shaped Grid In Multiple Acoustic Conditions", "abstract": "We introduce a database of multi-channel recordings performed in an acoustic\nlab with adjustable reverberation time. The recordings provide information\nabout room impulse responses (RIR) for various positions of a loudspeaker. In\nparticular, the main positions correspond to 4104 vertices of a cube-shaped\ndense grid within a 46x36x32 cm volume. The database thus provides a tool for\ndetailed analyses of beampatterns of spatial processing methods as well as for\ntraining and testing of mathematical models of the acoustic field.", "published": "2019-07-29 13:35:49", "link": "http://arxiv.org/abs/1907.12421v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Fast and Robust 3-D Sound Source Localization with DSVD-PHAT", "abstract": "This paper introduces a variant of the Singular Value Decomposition with\nPhase Transform (SVD-PHAT), named Difference SVD-PHAT (DSVD-PHAT), to achieve\nrobust Sound Source Localization (SSL) in noisy conditions. Experiments are\nperformed on a Baxter robot with a four-microphone planar array mounted on its\nhead. Results show that this method offers similar robustness to noise as the\nstate-of-the-art Multiple Signal Classification based on Generalized Singular\nValue Decomposition (GSVD-MUSIC) method, and considerably reduces the\ncomputational load by a factor of 250. This performance gain thus makes\nDSVD-PHAT appealing for real-time application on robots with limited on-board\ncomputing power.", "published": "2019-07-29 20:04:14", "link": "http://arxiv.org/abs/1907.12621v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "StarGAN-VC2: Rethinking Conditional Methods for StarGAN-Based Voice\n  Conversion", "abstract": "Non-parallel multi-domain voice conversion (VC) is a technique for learning\nmappings among multiple domains without relying on parallel data. This is\nimportant but challenging owing to the requirement of learning multiple\nmappings and the non-availability of explicit supervision. Recently, StarGAN-VC\nhas garnered attention owing to its ability to solve this problem only using a\nsingle generator. However, there is still a gap between real and converted\nspeech. To bridge this gap, we rethink conditional methods of StarGAN-VC, which\nare key components for achieving non-parallel multi-domain VC in a single\nmodel, and propose an improved variant called StarGAN-VC2. Particularly, we\nrethink conditional methods in two aspects: training objectives and network\narchitectures. For the former, we propose a source-and-target conditional\nadversarial loss that allows all source domain data to be convertible to the\ntarget domain data. For the latter, we introduce a modulation-based conditional\nmethod that can transform the modulation of the acoustic feature in a\ndomain-specific manner. We evaluated our methods on non-parallel multi-speaker\nVC. An objective evaluation demonstrates that our proposed methods improve\nspeech quality in terms of both global and local structure measures.\nFurthermore, a subjective evaluation shows that StarGAN-VC2 outperforms\nStarGAN-VC in terms of naturalness and speaker similarity. The converted speech\nsamples are provided at\nhttp://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/stargan-vc2/index.html.", "published": "2019-07-29 08:51:52", "link": "http://arxiv.org/abs/1907.12279v2", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
