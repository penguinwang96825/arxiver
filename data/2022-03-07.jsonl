{"title": "Towards Robust Online Dialogue Response Generation", "abstract": "Although pre-trained sequence-to-sequence models have achieved great success\nin dialogue response generation, chatbots still suffer from generating\ninconsistent responses in real-world practice, especially in multi-turn\nsettings. We argue that this can be caused by a discrepancy between training\nand real-world testing. At training time, chatbot generates the response with\nthe golden context, while it has to generate based on the context consisting of\nboth user utterances and the model predicted utterances during real-world\ntesting. With the growth of the number of utterances, this discrepancy becomes\nmore serious in the multi-turn settings. In this paper, we propose a\nhierarchical sampling-based method consisting of both utterance-level sampling\nand semi-utterance-level sampling, to alleviate the discrepancy, which\nimplicitly increases the dialogue coherence. We further adopt reinforcement\nlearning and re-ranking methods to explicitly optimize the dialogue coherence\nduring training and inference, respectively. Empirical experiments show the\neffectiveness of the proposed methods for improving the robustness of chatbots\nin real practice.", "published": "2022-03-07 06:51:41", "link": "http://arxiv.org/abs/2203.03168v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "USTC-NELSLIP at SemEval-2022 Task 11: Gazetteer-Adapted Integration\n  Network for Multilingual Complex Named Entity Recognition", "abstract": "This paper describes the system developed by the USTC-NELSLIP team for\nSemEval-2022 Task 11 Multilingual Complex Named Entity Recognition\n(MultiCoNER). We propose a gazetteer-adapted integration network (GAIN) to\nimprove the performance of language models for recognizing complex named\nentities. The method first adapts the representations of gazetteer networks to\nthose of language models by minimizing the KL divergence between them. After\nadaptation, these two networks are then integrated for backend supervised named\nentity recognition (NER) training. The proposed method is applied to several\nstate-of-the-art Transformer-based NER models with a gazetteer built from\nWikidata, and shows great generalization ability across them. The final\npredictions are derived from an ensemble of these trained models. Experimental\nresults and detailed analysis verify the effectiveness of the proposed method.\nThe official results show that our system ranked 1st on three tracks (Chinese,\nCode-mixed and Bangla) and 2nd on the other ten tracks in this task.", "published": "2022-03-07 09:05:37", "link": "http://arxiv.org/abs/2203.03216v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Precognition in Task-oriented Dialogue Understanding: Posterior\n  Regularization by Future Context", "abstract": "Task-oriented dialogue systems have become overwhelmingly popular in recent\nresearches. Dialogue understanding is widely used to comprehend users' intent,\nemotion and dialogue state in task-oriented dialogue systems. Most previous\nworks on such discriminative tasks only models current query or historical\nconversations. Even if in some work the entire dialogue flow was modeled, it is\nnot suitable for the real-world task-oriented conversations as the future\ncontexts are not visible in such cases. In this paper, we propose to jointly\nmodel historical and future information through the posterior regularization\nmethod. More specifically, by modeling the current utterance and past contexts\nas prior, and the entire dialogue flow as posterior, we optimize the KL\ndistance between these distributions to regularize our model during training.\nAnd only historical information is used for inference. Extensive experiments on\ntwo dialogue datasets validate the effectiveness of our proposed method,\nachieving superior results compared with all baseline models.", "published": "2022-03-07 09:58:50", "link": "http://arxiv.org/abs/2203.03244v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Building and curating conversational corpora for diversity-aware\n  language science and technology", "abstract": "We present an analysis pipeline and best practice guidelines for building and\ncurating corpora of everyday conversation in diverse languages. Surveying\nlanguage documentation corpora and other resources that cover 67 languages and\nvarieties from 28 phyla, we describe the compilation and curation process,\nspecify minimal properties of a unified format for interactional data, and\ndevelop methods for quality control that take into account turn-taking and\ntiming. Two case studies show the broad utility of conversational data for (i)\ncharting human interactional infrastructure and (ii) tracing challenges and\nopportunities for current ASR solutions. Linguistically diverse conversational\ncorpora can provide new insights for the language sciences and stronger\nempirical foundations for language technology.", "published": "2022-03-07 13:51:19", "link": "http://arxiv.org/abs/2203.03399v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What Did You Say? Task-Oriented Dialog Datasets Are Not Conversational!?", "abstract": "High-quality datasets for task-oriented dialog are crucial for the\ndevelopment of virtual assistants. Yet three of the most relevant large scale\ndialog datasets suffer from one common flaw: the dialog state update can be\ntracked, to a great extent, by a model that only considers the current user\nutterance, ignoring the dialog history. In this work, we outline a taxonomy of\nconversational and contextual effects, which we use to examine MultiWOZ, SGD\nand SMCalFlow, among the most recent and widely used task-oriented dialog\ndatasets. We analyze the datasets in a model-independent fashion and\ncorroborate these findings experimentally using a strong text-to-text baseline\n(T5). We find that less than 4% of MultiWOZ's turns and 10% of SGD's turns are\nconversational, while SMCalFlow is not conversational at all in its current\nrelease: its dialog state tracking task can be reduced to single exchange\nsemantic parsing. We conclude by outlining desiderata for truly conversational\ndialog datasets.", "published": "2022-03-07 14:26:23", "link": "http://arxiv.org/abs/2203.03431v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SOCIOFILLMORE: A Tool for Discovering Perspectives", "abstract": "SOCIOFILLMORE is a multilingual tool which helps to bring to the fore the\nfocus or the perspective that a text expresses in depicting an event. Our tool,\nwhose rationale we also support through a large collection of human judgements,\nis theoretically grounded on frame semantics and cognitive linguistics, and\nimplemented using the LOME frame semantic parser. We describe SOCIOFILLMORE's\ndevelopment and functionalities, show how non-NLP researchers can easily\ninteract with the tool, and present some example case studies which are already\nincorporated in the system, together with the kind of analysis that can be\nvisualised.", "published": "2022-03-07 14:42:22", "link": "http://arxiv.org/abs/2203.03438v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hierarchical Sketch Induction for Paraphrase Generation", "abstract": "We propose a generative model of paraphrase generation, that encourages\nsyntactic diversity by conditioning on an explicit syntactic sketch. We\nintroduce Hierarchical Refinement Quantized Variational Autoencoders (HRQ-VAE),\na method for learning decompositions of dense encodings as a sequence of\ndiscrete latent variables that make iterative refinements of increasing\ngranularity. This hierarchy of codes is learned through end-to-end training,\nand represents fine-to-coarse grained information about the input. We use\nHRQ-VAE to encode the syntactic form of an input sentence as a path through the\nhierarchy, allowing us to more easily predict syntactic sketches at test time.\nExtensive experiments, including a human evaluation, confirm that HRQ-VAE\nlearns a hierarchical representation of the input space, and generates\nparaphrases of higher quality than previous systems.", "published": "2022-03-07 15:28:36", "link": "http://arxiv.org/abs/2203.03463v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Creating Speech-to-Speech Corpus from Dubbed Series", "abstract": "Dubbed series are gaining a lot of popularity in recent years with strong\nsupport from major media service providers. Such popularity is fueled by\nstudies that showed that dubbed versions of TV shows are more popular than\ntheir subtitled equivalents. We propose an unsupervised approach to construct\nspeech-to-speech corpus, aligned on short segment levels, to produce a parallel\nspeech corpus in the source- and target- languages. Our methodology exploits\nvideo frames, speech recognition, machine translation, and noisy frames removal\nalgorithms to match segments in both languages. To verify the performance of\nthe proposed method, we apply it on long and short dubbed clips. Out of 36\nhours TR-AR dubbed series, our pipeline was able to generate 17 hours of paired\nsegments, which is about 47% of the corpus. We applied our method on another\nlanguage pair, EN-AR, to ensure it is robust enough and not tuned for a\nspecific language or a specific corpus. Regardless of the language pairs, the\naccuracy of the paired segments was around 70% when evaluated using human\nsubjective evaluation. The corpus will be freely available for the research\ncommunity.", "published": "2022-03-07 18:52:48", "link": "http://arxiv.org/abs/2203.03601v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IT5: Text-to-text Pretraining for Italian Language Understanding and\n  Generation", "abstract": "We introduce IT5, the first family of encoder-decoder transformer models\npretrained specifically on Italian. We document and perform a thorough cleaning\nprocedure for a large Italian corpus and use it to pretrain four IT5 model\nsizes. We then introduce the ItaGen benchmark, which includes a broad range of\nnatural language understanding and generation tasks for Italian, and use it to\nevaluate the performance of IT5 models and multilingual baselines. We find\nmonolingual IT5 models to provide the best scale-to-performance ratio across\ntested models, consistently outperforming their multilingual counterparts and\nsetting a new state-of-the-art for Italian language generation.", "published": "2022-03-07 22:39:01", "link": "http://arxiv.org/abs/2203.03759v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mismatch between Multi-turn Dialogue and its Evaluation Metric in\n  Dialogue State Tracking", "abstract": "Dialogue state tracking (DST) aims to extract essential information from\nmulti-turn dialogue situations and take appropriate actions. A belief state,\none of the core pieces of information, refers to the subject and its specific\ncontent, and appears in the form of domain-slot-value. The trained model\npredicts \"accumulated\" belief states in every turn, and joint goal accuracy and\nslot accuracy are mainly used to evaluate the prediction; however, we specify\nthat the current evaluation metrics have a critical limitation when evaluating\nbelief states accumulated as the dialogue proceeds, especially in the most used\nMultiWOZ dataset. Additionally, we propose relative slot accuracy to complement\nexisting metrics. Relative slot accuracy does not depend on the number of\npredefined slots, and allows intuitive evaluation by assigning relative scores\naccording to the turn of each dialogue. This study also encourages not solely\nthe reporting of joint goal accuracy, but also various complementary metrics in\nDST tasks for the sake of a realistic evaluation.", "published": "2022-03-07 04:07:36", "link": "http://arxiv.org/abs/2203.03123v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Input-Tuning: Adapting Unfamiliar Inputs to Frozen Pretrained Models", "abstract": "Recently the prompt-tuning paradigm has attracted significant attention. By\nonly tuning continuous prompts with a frozen pre-trained language model (PLM),\nprompt-tuning takes a step towards deploying a shared frozen PLM to serve\nnumerous downstream tasks. Although prompt-tuning shows good performance on\ncertain natural language understanding (NLU) tasks, its effectiveness on\nnatural language generation (NLG) tasks is still under-explored. In this paper,\nwe argue that one of the factors hindering the development of prompt-tuning on\nNLG tasks is the unfamiliar inputs (i.e., inputs are linguistically different\nfrom the pretraining corpus). For example, our preliminary exploration reveals\na large performance gap between prompt-tuning and fine-tuning when unfamiliar\ninputs occur frequently in NLG tasks. This motivates us to propose\ninput-tuning, which fine-tunes both the continuous prompts and the input\nrepresentations, leading to a more effective way to adapt unfamiliar inputs to\nfrozen PLMs. Our proposed input-tuning is conceptually simple and empirically\npowerful. Experimental results on seven NLG tasks demonstrate that input-tuning\nis significantly and consistently better than prompt-tuning. Furthermore, on\nthree of these tasks, input-tuning can achieve a comparable or even better\nperformance than fine-tuning.", "published": "2022-03-07 05:04:32", "link": "http://arxiv.org/abs/2203.03131v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unpaired Image Captioning by Image-level Weakly-Supervised Visual\n  Concept Recognition", "abstract": "The goal of unpaired image captioning (UIC) is to describe images without\nusing image-caption pairs in the training phase. Although challenging, we\nexcept the task can be accomplished by leveraging a training set of images\naligned with visual concepts. Most existing studies use off-the-shelf\nalgorithms to obtain the visual concepts because the Bounding Box (BBox) labels\nor relationship-triplet labels used for the training are expensive to acquire.\nIn order to resolve the problem in expensive annotations, we propose a novel\napproach to achieve cost-effective UIC. Specifically, we adopt image-level\nlabels for the optimization of the UIC model in a weakly-supervised manner. For\neach image, we assume that only the image-level labels are available without\nspecific locations and numbers. The image-level labels are utilized to train a\nweakly-supervised object recognition model to extract object information (e.g.,\ninstance) in an image, and the extracted instances are adopted to infer the\nrelationships among different objects based on an enhanced graph neural network\n(GNN). The proposed approach achieves comparable or even better performance\ncompared with previous methods without the expensive cost of annotations.\nFurthermore, we design an unrecognized object (UnO) loss combined with a visual\nconcept reward to improve the alignment of the inferred object and relationship\ninformation with the images. It can effectively alleviate the issue encountered\nby existing UIC models about generating sentences with nonexistent objects. To\nthe best of our knowledge, this is the first attempt to solve the problem of\nWeakly-Supervised visual concept recognition for UIC (WS-UIC) based only on\nimage-level labels. Extensive experiments have been carried out to demonstrate\nthat the proposed WS-UIC model achieves inspiring results on the COCO dataset\nwhile significantly reducing the cost of labeling.", "published": "2022-03-07 08:02:23", "link": "http://arxiv.org/abs/2203.03195v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Pre-trained Token-replaced Detection Model as Few-shot Learner", "abstract": "Pre-trained masked language models have demonstrated remarkable ability as\nfew-shot learners. In this paper, as an alternative, we propose a novel\napproach to few-shot learning with pre-trained token-replaced detection models\nlike ELECTRA. In this approach, we reformulate a classification or a regression\ntask as a token-replaced detection problem. Specifically, we first define a\ntemplate and label description words for each task and put them into the input\nto form a natural language prompt. Then, we employ the pre-trained\ntoken-replaced detection model to predict which label description word is the\nmost original (i.e., least replaced) among all label description words in the\nprompt. A systematic evaluation on 16 datasets demonstrates that our approach\noutperforms few-shot learners with pre-trained masked language models in both\none-sentence and two-sentence learning tasks.", "published": "2022-03-07 09:47:53", "link": "http://arxiv.org/abs/2203.03235v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi-CPR: A Multi Domain Chinese Dataset for Passage Retrieval", "abstract": "Passage retrieval is a fundamental task in information retrieval (IR)\nresearch, which has drawn much attention recently. In the English field, the\navailability of large-scale annotated dataset (e.g, MS MARCO) and the emergence\nof deep pre-trained language models (e.g, BERT) has resulted in a substantial\nimprovement of existing passage retrieval systems. However, in the Chinese\nfield, especially for specific domains, passage retrieval systems are still\nimmature due to quality-annotated dataset being limited by scale. Therefore, in\nthis paper, we present a novel multi-domain Chinese dataset for passage\nretrieval (Multi-CPR). The dataset is collected from three different domains,\nincluding E-commerce, Entertainment video and Medical. Each dataset contains\nmillions of passages and a certain amount of human annotated query-passage\nrelated pairs. We implement various representative passage retrieval methods as\nbaselines. We find that the performance of retrieval models trained on dataset\nfrom general domain will inevitably decrease on specific domain. Nevertheless,\na passage retrieval system built on in-domain annotated dataset can achieve\nsignificant improvement, which indeed demonstrates the necessity of domain\nlabeled data for further optimization. We hope the release of the Multi-CPR\ndataset could benchmark Chinese passage retrieval task in specific domain and\nalso make advances for future studies.", "published": "2022-03-07 13:20:46", "link": "http://arxiv.org/abs/2203.03367v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Towards Automated Real-time Evaluation in Text-based Counseling", "abstract": "Automated real-time evaluation of counselor-client interaction is important\nfor ensuring quality counseling but the rules are difficult to articulate.\nRecent advancements in machine learning methods show the possibility of\nlearning such rules automatically. However, these methods often demand large\nscale and high quality counseling data, which are difficult to collect. To\naddress this issue, we build an online counseling platform, which allows\nprofessional psychotherapists to provide free counseling services to those are\nin need. In exchange, we collect the counseling transcripts. Within a year of\nits operation, we manage to get one of the largest set of (675) transcripts of\ncounseling sessions. To further leverage the valuable data we have, we label\nour dataset using both coarse- and fine-grained labels and use a set of\npretraining techniques. In the end, we are able to achieve practically useful\naccuracy in both labeling system.", "published": "2022-03-07 14:53:49", "link": "http://arxiv.org/abs/2203.03442v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Who Should Review Your Proposal? Interdisciplinary Topic Path Detection\n  for Research Proposals", "abstract": "The peer merit review of research proposals has been the major mechanism to\ndecide grant awards. Nowadays, research proposals have become increasingly\ninterdisciplinary. It has been a longstanding challenge to assign proposals to\nappropriate reviewers. One of the critical steps in reviewer assignment is to\ngenerate accurate interdisciplinary topic labels for proposals. Existing\nsystems mainly collect topic labels manually reported by discipline\ninvestigators. However, such human-reported labels can be non-accurate and\nincomplete. What role can AI play in developing a fair and precise proposal\nreview system? In this evidential study, we collaborate with the National\nScience Foundation of China to address the task of automated interdisciplinary\ntopic path detection. For this purpose, we develop a deep Hierarchical\nInterdisciplinary Research Proposal Classification Network (HIRPCN). We first\npropose a hierarchical transformer to extract the textual semantic information\nof proposals. We then design an interdisciplinary graph and leverage GNNs to\nlearn representations of each discipline in order to extract interdisciplinary\nknowledge. After extracting the semantic and interdisciplinary knowledge, we\ndesign a level-wise prediction component to fuse the two types of knowledge\nrepresentations and detect interdisciplinary topic paths for each proposal. We\nconduct extensive experiments and expert evaluations on three real-world\ndatasets to demonstrate the effectiveness of our proposed model.", "published": "2022-03-07 03:30:50", "link": "http://arxiv.org/abs/2203.10922v1", "categories": ["cs.CL", "cs.LG", "I.2.7; F.2.2"], "primary_category": "cs.CL"}
{"title": "ILDAE: Instance-Level Difficulty Analysis of Evaluation Data", "abstract": "Knowledge of questions' difficulty level helps a teacher in several ways,\nsuch as estimating students' potential quickly by asking carefully selected\nquestions and improving quality of examination by modifying trivial and hard\nquestions. Can we extract such benefits of instance difficulty in NLP? To this\nend, we conduct Instance-Level Difficulty Analysis of Evaluation data (ILDAE)\nin a large-scale setup of 23 datasets and demonstrate its five novel\napplications: 1) conducting efficient-yet-accurate evaluations with fewer\ninstances saving computational cost and time, 2) improving quality of existing\nevaluation datasets by repairing erroneous and trivial instances, 3) selecting\nthe best model based on application requirements, 4) analyzing dataset\ncharacteristics for guiding future data creation, 5) estimating Out-of-Domain\nperformance reliably. Comprehensive experiments for these applications result\nin several interesting findings, such as evaluation using just 5% instances\n(selected via ILDAE) achieves as high as 0.93 Kendall correlation with\nevaluation using complete dataset and computing weighted accuracy using\ndifficulty scores leads to 5.2% higher correlation with Out-of-Domain\nperformance. We release the difficulty scores and hope our analyses and\nfindings will bring more attention to this important yet understudied field of\nleveraging instance difficulty in evaluations.", "published": "2022-03-07 00:02:11", "link": "http://arxiv.org/abs/2203.03073v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Language-Agnostic Meta-Learning for Low-Resource Text-to-Speech with\n  Articulatory Features", "abstract": "While neural text-to-speech systems perform remarkably well in high-resource\nscenarios, they cannot be applied to the majority of the over 6,000 spoken\nlanguages in the world due to a lack of appropriate training data. In this\nwork, we use embeddings derived from articulatory vectors rather than\nembeddings derived from phoneme identities to learn phoneme representations\nthat hold across languages. In conjunction with language agnostic meta\nlearning, this enables us to fine-tune a high-quality text-to-speech model on\njust 30 minutes of data in a previously unseen language spoken by a previously\nunseen speaker.", "published": "2022-03-07 07:58:01", "link": "http://arxiv.org/abs/2203.03191v1", "categories": ["cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Enhance Language Identification using Dual-mode Model with Knowledge\n  Distillation", "abstract": "In this paper, we propose to employ a dual-mode framework on the x-vector\nself-attention (XSA-LID) model with knowledge distillation (KD) to enhance its\nlanguage identification (LID) performance for both long and short utterances.\nThe dual-mode XSA-LID model is trained by jointly optimizing both the full and\nshort modes with their respective inputs being the full-length speech and its\nshort clip extracted by a specific Boolean mask, and KD is applied to further\nboost the performance on short utterances. In addition, we investigate the\nimpact of clip-wise linguistic variability and lexical integrity for LID by\nanalyzing the variation of LID performance in terms of the lengths and\npositions of the mimicked speech clips. We evaluated our approach on the MLS14\ndata from the NIST 2017 LRE. With the 3~s random-location Boolean mask, our\nproposed method achieved 19.23%, 21.52% and 8.37% relative improvement in\naverage cost compared with the XSA-LID model on 3s, 10s, and 30s speech,\nrespectively.", "published": "2022-03-07 09:12:29", "link": "http://arxiv.org/abs/2203.03218v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SkillNet-NLU: A Sparsely Activated Model for General-Purpose Natural\n  Language Understanding", "abstract": "Prevailing deep models are single-purpose and overspecialize at individual\ntasks. However, when being extended to new tasks, they typically forget\npreviously learned skills and learn from scratch. We address this issue by\nintroducing SkillNet-NLU, a general-purpose model that stitches together\nexisting skills to learn new tasks more effectively. The key feature of our\napproach is that it is sparsely activated guided by predefined skills.\nDifferent from traditional dense models that always activate all the model\nparameters, SkillNet-NLU only activates parts of the model parameters whose\nskills are relevant to the target task. When learning for a new task, our\napproach precisely activates required skills and also provides an option to add\nnew skills. We evaluate on natural language understandings tasks and have the\nfollowing findings. First, with only one model checkpoint, SkillNet-NLU\nperforms better than task-specific fine-tuning and two multi-task learning\nbaselines (i.e., dense model and Mixture-of-Experts model) on six tasks.\nSecond, sparsely activated pre-training further improves the overall\nperformance. Third, SkillNet-NLU significantly outperforms baseline systems\nwhen being extended to new tasks.", "published": "2022-03-07 11:48:09", "link": "http://arxiv.org/abs/2203.03312v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multi-Modal Attribute Extraction for E-Commerce", "abstract": "To improve users' experience as they navigate the myriad of options offered\nby online marketplaces, it is essential to have well-organized product\ncatalogs. One key ingredient to that is the availability of product attributes\nsuch as color or material. However, on some marketplaces such as\nRakuten-Ichiba, which we focus on, attribute information is often incomplete or\neven missing. One promising solution to this problem is to rely on deep models\npre-trained on large corpora to predict attributes from unstructured data, such\nas product descriptive texts and images (referred to as modalities in this\npaper). However, we find that achieving satisfactory performance with this\napproach is not straightforward but rather the result of several refinements,\nwhich we discuss in this paper. We provide a detailed description of our\napproach to attribute extraction, from investigating strong single-modality\nmethods, to building a solid multimodal model combining textual and visual\ninformation. One key component of our multimodal architecture is a novel\napproach to seamlessly combine modalities, which is inspired by our\nsingle-modality investigations. In practice, we notice that this new\nmodality-merging method may suffer from a modality collapse issue, i.e., it\nneglects one modality. Hence, we further propose a mitigation to this problem\nbased on a principled regularization scheme. Experiments on Rakuten-Ichiba data\nprovide empirical evidence for the benefits of our approach, which has been\nalso successfully deployed to Rakuten-Ichiba. We also report results on\npublicly available datasets showing that our model is competitive compared to\nseveral recent multimodal and unimodal baselines.", "published": "2022-03-07 14:48:44", "link": "http://arxiv.org/abs/2203.03441v1", "categories": ["cs.CV", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CV"}
{"title": "Audio-visual Generalised Zero-shot Learning with Cross-modal Attention\n  and Language", "abstract": "Learning to classify video data from classes not included in the training\ndata, i.e. video-based zero-shot learning, is challenging. We conjecture that\nthe natural alignment between the audio and visual modalities in video data\nprovides a rich training signal for learning discriminative multi-modal\nrepresentations. Focusing on the relatively underexplored task of audio-visual\nzero-shot learning, we propose to learn multi-modal representations from\naudio-visual data using cross-modal attention and exploit textual label\nembeddings for transferring knowledge from seen classes to unseen classes.\nTaking this one step further, in our generalised audio-visual zero-shot\nlearning setting, we include all the training classes in the test-time search\nspace which act as distractors and increase the difficulty while making the\nsetting more realistic. Due to the lack of a unified benchmark in this domain,\nwe introduce a (generalised) zero-shot learning benchmark on three audio-visual\ndatasets of varying sizes and difficulty, VGGSound, UCF, and ActivityNet,\nensuring that the unseen test classes do not appear in the dataset used for\nsupervised training of the backbone deep models. Comparing multiple relevant\nand recent methods, we demonstrate that our proposed AVCA model achieves\nstate-of-the-art performance on all three datasets. Code and data are available\nat \\url{https://github.com/ExplainableML/AVCA-GZSL}.", "published": "2022-03-07 18:52:13", "link": "http://arxiv.org/abs/2203.03598v2", "categories": ["cs.CV", "cs.CL", "eess.AS"], "primary_category": "cs.CV"}
{"title": "HyperMixer: An MLP-based Low Cost Alternative to Transformers", "abstract": "Transformer-based architectures are the model of choice for natural language\nunderstanding, but they come at a significant cost, as they have quadratic\ncomplexity in the input length, require a lot of training data, and can be\ndifficult to tune. In the pursuit of lower costs, we investigate simple\nMLP-based architectures. We find that existing architectures such as MLPMixer,\nwhich achieves token mixing through a static MLP applied to each feature\nindependently, are too detached from the inductive biases required for natural\nlanguage understanding. In this paper, we propose a simple variant, HyperMixer,\nwhich forms the token mixing MLP dynamically using hypernetworks. Empirically,\nwe demonstrate that our model performs better than alternative MLP-based\nmodels, and on par with Transformers. In contrast to Transformers, HyperMixer\nachieves these results at substantially lower costs in terms of processing\ntime, training data, and hyperparameter tuning.", "published": "2022-03-07 20:23:46", "link": "http://arxiv.org/abs/2203.03691v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "HRTF measurement for accurate sound localization cues", "abstract": "A new database of head-related transfer functions (HRTFs) for accurate sound\nsource localization is presented through precise measurement and\npost-processing in terms of improved frequency bandwidth and causality of\nhead-related impulse responses (HRIRs) for accurate spectral cue (SC) and\ninteraural time difference (ITD), respectively. The improvement effects of the\nproposed methods on binaural sound localization cues were investigated. To\nachieve sufficient frequency bandwidth with a single source, a one-way sealed\nspeaker module was designed to obtain wide band frequency response based on\nelectro-acoustics, whereas most existing HRTF databases rely on a two-way\nvented loudspeaker that has multiple sources. The origin transfer function at\nthe head center was obtained by the proposed measurement scheme using a 0\ndegree on-axis microphone to ensure accurate spectral cue pattern of HRTFs,\nwhereas in the previous measurements with a 90 degree off-axis microphone, the\nmagnitude response of the origin transfer function fluctuated and decreased\nwith increasing frequency, causing erroneous SCs of HRTFs. To prevent\ndiscontinuity of ITD due to non-causality of ipsilateral HRTFs, obtained HRIRs\nwere circularly shifted by time delay considering the head radius of the\nmeasurement subject. Finally, various sound localization cues such as ITD,\ninteraural level difference (ILD), SC, and horizontal plane directivity (HPD)\nwere derived from the presented HRTFs, and improvements on binaural sound\nlocalization cues were examined. As a result, accurate SC patterns of HRTFs\nwere confirmed through the proposed measurement scheme using the 0 degree\non-axis microphone, and continuous ITD patterns were obtained due to the\nnon-causality compensation. Source codes and presented HRTF database are\navailable to relevant research groups at GitHub\n(https://github.com/han-saram/HRTF-HATS-KAIST).", "published": "2022-03-07 06:48:43", "link": "http://arxiv.org/abs/2203.03166v2", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Speaker recognition by means of a combination of linear and nonlinear\n  predictive models", "abstract": "This paper deals the combination of nonlinear predictive models with\nclassical LPCC parameterization for speaker recognition. It is shown that the\ncombination of both a measure defined over LPCC coefficients and a measure\ndefined over predictive analysis residual signal gives rise to an improvement\nover the classical method that considers only the LPCC coefficients. If the\nresidual signal is obtained from a linear prediction analysis, the improvement\nis 2.63% (error rate drops from 6.31% to 3.68%) and if it is computed through a\nnonlinear predictive neural nets based model, the improvement is 3.68%. An\nefficient algorithm for reducing the computational burden is also proposed.", "published": "2022-03-07 07:57:54", "link": "http://arxiv.org/abs/2203.03190v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Visually Supervised Speaker Detection and Localization via Microphone\n  Array", "abstract": "Active speaker detection (ASD) is a multi-modal task that aims to identify\nwho, if anyone, is speaking from a set of candidates. Current audio-visual\napproaches for ASD typically rely on visually pre-extracted face tracks\n(sequences of consecutive face crops) and the respective monaural audio.\nHowever, their recall rate is often low as only the visible faces are included\nin the set of candidates. Monaural audio may successfully detect the presence\nof speech activity but fails in localizing the speaker due to the lack of\nspatial cues. Our solution extends the audio front-end using a microphone\narray. We train an audio convolutional neural network (CNN) in combination with\nbeamforming techniques to regress the speaker's horizontal position directly in\nthe video frames. We propose to generate weak labels using a pre-trained active\nspeaker detector on pre-extracted face tracks. Our pipeline embraces the\n\"student-teacher\" paradigm, where a trained \"teacher\" network is used to\nproduce pseudo-labels visually. The \"student\" network is an audio network\ntrained to generate the same results. At inference, the student network can\nindependently localize the speaker in the visual frames directly from the audio\ninput. Experimental results on newly collected data prove that our approach\nsignificantly outperforms a variety of other baselines as well as the teacher\nnetwork itself. It results in an excellent speech activity detector too.", "published": "2022-03-07 11:12:39", "link": "http://arxiv.org/abs/2203.03291v1", "categories": ["eess.AS", "cs.SD", "eess.IV"], "primary_category": "eess.AS"}
{"title": "Deep Neural Decision Forest for Acoustic Scene Classification", "abstract": "Acoustic scene classification (ASC) aims to classify an audio clip based on\nthe characteristic of the recording environment. In this regard, deep learning\nbased approaches have emerged as a useful tool for ASC problems. Conventional\napproaches to improving the classification accuracy include integrating\nauxiliary methods such as attention mechanism, pre-trained models and ensemble\nmultiple sub-networks. However, due to the complexity of audio clips captured\nfrom different environments, it is difficult to distinguish their categories\nwithout using any auxiliary methods for existing deep learning models using\nonly a single classifier. In this paper, we propose a novel approach for ASC\nusing deep neural decision forest (DNDF). DNDF combines a fixed number of\nconvolutional layers and a decision forest as the final classifier. The\ndecision forest consists of a fixed number of decision tree classifiers, which\nhave been shown to offer better classification performance than a single\nclassifier in some datasets. In particular, the decision forest differs\nsubstantially from traditional random forests as it is stochastic,\ndifferentiable, and capable of using the back-propagation to update and learn\nfeature representations in neural network. Experimental results on the\nDCASE2019 and ESC-50 datasets demonstrate that our proposed DNDF method\nimproves the ASC performance in terms of classification accuracy and shows\ncompetitive performance as compared with state-of-the-art baselines.", "published": "2022-03-07 14:39:42", "link": "http://arxiv.org/abs/2203.03436v1", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Detection of AI Synthesized Hindi Speech", "abstract": "The recent advancements in generative artificial speech models have made\npossible the generation of highly realistic speech signals. At first, it seems\nexciting to obtain these artificially synthesized signals such as speech clones\nor deep fakes but if left unchecked, it may lead us to digital dystopia. One of\nthe primary focus in audio forensics is validating the authenticity of a\nspeech. Though some solutions are proposed for English speeches but the\ndetection of synthetic Hindi speeches have not gained much attention. Here, we\npropose an approach for discrimination of AI synthesized Hindi speech from an\nactual human speech. We have exploited the Bicoherence Phase, Bicoherence\nMagnitude, Mel Frequency Cepstral Coefficient (MFCC), Delta Cepstral, and Delta\nSquare Cepstral as the discriminating features for machine learning models.\nAlso, we extend the study to using deep neural networks for extensive\nexperiments, specifically VGG16 and homemade CNN as the architecture models. We\nobtained an accuracy of 99.83% with VGG16 and 99.99% with homemade CNN models.", "published": "2022-03-07 21:13:54", "link": "http://arxiv.org/abs/2203.03706v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A study on joint modeling and data augmentation of multi-modalities for\n  audio-visual scene classification", "abstract": "In this paper, we propose two techniques, namely joint modeling and data\naugmentation, to improve system performances for audio-visual scene\nclassification (AVSC). We employ pre-trained networks trained only on image\ndata sets to extract video embedding; whereas for audio embedding models, we\ndecide to train them from scratch. We explore different neural network\narchitectures for joint modeling to effectively combine the video and audio\nmodalities. Moreover, data augmentation strategies are investigated to increase\naudio-visual training set size. For the video modality the effectiveness of\nseveral operations in RandAugment is verified. An audio-video joint mixup\nscheme is proposed to further improve AVSC performances. Evaluated on the\ndevelopment set of TAU Urban Audio Visual Scenes 2021, our final system can\nachieve the best accuracy of 94.2% among all single AVSC systems submitted to\nDCASE 2021 Task 1b.", "published": "2022-03-07 07:29:55", "link": "http://arxiv.org/abs/2203.04114v3", "categories": ["cs.MM", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
