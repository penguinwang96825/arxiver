{"title": "Building an Evaluation Scale using Item Response Theory", "abstract": "Evaluation of NLP methods requires testing against a previously vetted\ngold-standard test set and reporting standard metrics\n(accuracy/precision/recall/F1). The current assumption is that all items in a\ngiven test set are equal with regards to difficulty and discriminating power.\nWe propose Item Response Theory (IRT) from psychometrics as an alternative\nmeans for gold-standard test-set generation and NLP system evaluation. IRT is\nable to describe characteristics of individual items - their difficulty and\ndiscriminating power - and can account for these characteristics in its\nestimation of human intelligence or ability for an NLP task. In this paper, we\ndemonstrate IRT by generating a gold-standard test set for Recognizing Textual\nEntailment. By collecting a large number of human responses and fitting our IRT\nmodel, we show that our IRT model compares NLP systems with the performance in\na human population and is able to provide more insight into system performance\nthan standard evaluation metrics. We show that a high accuracy score does not\nalways imply a high IRT score, which depends on the item characteristics and\nthe response pattern.", "published": "2016-05-28 13:19:15", "link": "http://arxiv.org/abs/1605.08889v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Aspect Level Sentiment Classification with Deep Memory Network", "abstract": "We introduce a deep memory network for aspect level sentiment classification.\nUnlike feature-based SVM and sequential neural models such as LSTM, this\napproach explicitly captures the importance of each context word when inferring\nthe sentiment polarity of an aspect. Such importance degree and text\nrepresentation are calculated with multiple computational layers, each of which\nis a neural attention model over an external memory. Experiments on laptop and\nrestaurant datasets demonstrate that our approach performs comparable to\nstate-of-art feature based SVM system, and substantially better than LSTM and\nattention-based LSTM architectures. On both datasets we show that multiple\ncomputational layers could improve the performance. Moreover, our approach is\nalso fast. The deep memory network with 9 layers is 15 times faster than LSTM\nwith a CPU implementation.", "published": "2016-05-28 14:47:49", "link": "http://arxiv.org/abs/1605.08900v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
