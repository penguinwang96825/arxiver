{"title": "Exploring the Limits of Language Modeling", "abstract": "In this work we explore recent advances in Recurrent Neural Networks for\nlarge scale Language Modeling, a task central to language understanding. We\nextend current models to deal with two key challenges present in this task:\ncorpora and vocabulary sizes, and complex, long term structure of language. We\nperform an exhaustive study on techniques such as character Convolutional\nNeural Networks or Long-Short Term Memory, on the One Billion Word Benchmark.\nOur best single model significantly improves state-of-the-art perplexity from\n51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20),\nwhile an ensemble of models sets a new record by improving perplexity from 41.0\ndown to 23.7. We also release these models for the NLP and ML community to\nstudy and improve upon.", "published": "2016-02-07 19:11:17", "link": "http://arxiv.org/abs/1602.02410v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Scalable Text Mining with Sparse Generative Models", "abstract": "The information age has brought a deluge of data. Much of this is in text\nform, insurmountable in scope for humans and incomprehensible in structure for\ncomputers. Text mining is an expanding field of research that seeks to utilize\nthe information contained in vast document collections. General data mining\nmethods based on machine learning face challenges with the scale of text data,\nposing a need for scalable text mining methods.\n  This thesis proposes a solution to scalable text mining: generative models\ncombined with sparse computation. A unifying formalization for generative text\nmodels is defined, bringing together research traditions that have used\nformally equivalent models, but ignored parallel developments. This framework\nallows the use of methods developed in different processing tasks such as\nretrieval and classification, yielding effective solutions across different\ntext mining tasks. Sparse computation using inverted indices is proposed for\ninference on probabilistic models. This reduces the computational complexity of\nthe common text mining operations according to sparsity, yielding probabilistic\nmodels with the scalability of modern search engines.\n  The proposed combination provides sparse generative models: a solution for\ntext mining that is general, effective, and scalable. Extensive experimentation\non text classification and ranked retrieval datasets are conducted, showing\nthat the proposed solution matches or outperforms the leading task-specific\nmethods in effectiveness, with a order of magnitude decrease in classification\ntimes for Wikipedia article categorization with a million classes. The\ndeveloped methods were further applied in two 2014 Kaggle data mining prize\ncompetitions with over a hundred competing teams, earning first and second\nplaces.", "published": "2016-02-07 02:49:27", "link": "http://arxiv.org/abs/1602.02332v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Supervised and Semi-Supervised Text Categorization using LSTM for Region\n  Embeddings", "abstract": "One-hot CNN (convolutional neural network) has been shown to be effective for\ntext categorization (Johnson & Zhang, 2015). We view it as a special case of a\ngeneral framework which jointly trains a linear model with a non-linear feature\ngenerator consisting of `text region embedding + pooling'. Under this\nframework, we explore a more sophisticated region embedding method using Long\nShort-Term Memory (LSTM). LSTM can embed text regions of variable (and possibly\nlarge) sizes, whereas the region size needs to be fixed in a CNN. We seek\neffective and efficient use of LSTM for this purpose in the supervised and\nsemi-supervised settings. The best results were obtained by combining region\nembeddings in the form of LSTM and convolution layers trained on unlabeled\ndata. The results indicate that on this task, embeddings of text regions, which\ncan convey complex concepts, are more useful than embeddings of single words in\nisolation. We report performances exceeding the previous best results on four\nbenchmark datasets.", "published": "2016-02-07 14:05:58", "link": "http://arxiv.org/abs/1602.02373v2", "categories": ["stat.ML", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
