{"title": "Thresh: A Unified, Customizable and Deployable Platform for Fine-Grained\n  Text Evaluation", "abstract": "Fine-grained, span-level human evaluation has emerged as a reliable and\nrobust method for evaluating text generation tasks such as summarization,\nsimplification, machine translation and news generation, and the derived\nannotations have been useful for training automatic metrics and improving\nlanguage models. However, existing annotation tools implemented for these\nevaluation frameworks lack the adaptability to be extended to different domains\nor languages, or modify annotation settings according to user needs; and, the\nabsence of a unified annotated data format inhibits the research in multi-task\nlearning. In this paper, we introduce Thresh, a unified, customizable and\ndeployable platform for fine-grained evaluation. With a single YAML\nconfiguration file, users can build and test an annotation interface for any\nframework within minutes -- all in one web browser window. To facilitate\ncollaboration and sharing, Thresh provides a community hub that hosts a\ncollection of fine-grained frameworks and corresponding annotations made and\ncollected by the community, covering a wide range of NLP tasks. For deployment,\nThresh offers multiple options for any scale of annotation projects from small\nmanual inspections to large crowdsourcing ones. Additionally, we introduce a\nPython library to streamline the entire process from typology design and\ndeployment to annotation processing. Thresh is publicly accessible at\nhttps://thresh.tools.", "published": "2023-08-14 06:09:51", "link": "http://arxiv.org/abs/2308.06953v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EcomGPT: Instruction-tuning Large Language Models with Chain-of-Task\n  Tasks for E-commerce", "abstract": "Recently, instruction-following Large Language Models (LLMs) , represented by\nChatGPT, have exhibited exceptional performance in general Natural Language\nProcessing (NLP) tasks. However, the unique characteristics of E-commerce data\npose significant challenges to general LLMs. An LLM tailored specifically for\nE-commerce scenarios, possessing robust cross-dataset/task generalization\ncapabilities, is a pressing necessity. To solve this issue, in this work, we\nproposed the first e-commerce instruction dataset EcomInstruct, with a total of\n2.5 million instruction data. EcomInstruct scales up the data size and task\ndiversity by constructing atomic tasks with E-commerce basic data types, such\nas product information, user reviews. Atomic tasks are defined as intermediate\ntasks implicitly involved in solving a final task, which we also call\nChain-of-Task tasks. We developed EcomGPT with different parameter scales by\ntraining the backbone model BLOOMZ with the EcomInstruct. Benefiting from the\nfundamental semantic understanding capabilities acquired from the Chain-of-Task\ntasks, EcomGPT exhibits excellent zero-shot generalization capabilities.\nExtensive experiments and human evaluations demonstrate that EcomGPT\noutperforms ChatGPT in term of cross-dataset/task generalization on E-commerce\ntasks.", "published": "2023-08-14 06:49:53", "link": "http://arxiv.org/abs/2308.06966v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Knowledge Graphs Simplify Text?", "abstract": "Knowledge Graph (KG)-to-Text Generation has seen recent improvements in\ngenerating fluent and informative sentences which describe a given KG. As KGs\nare widespread across multiple domains and contain important entity-relation\ninformation, and as text simplification aims to reduce the complexity of a text\nwhile preserving the meaning of the original text, we propose KGSimple, a novel\napproach to unsupervised text simplification which infuses KG-established\ntechniques in order to construct a simplified KG path and generate a concise\ntext which preserves the original input's meaning. Through an iterative and\nsampling KG-first approach, our model is capable of simplifying text when\nstarting from a KG by learning to keep important information while harnessing\nKG-to-text generation to output fluent and descriptive sentences. We evaluate\nvarious settings of the KGSimple model on currently-available KG-to-text\ndatasets, demonstrating its effectiveness compared to unsupervised text\nsimplification models which start with a given complex text. Our code is\navailable on GitHub.", "published": "2023-08-14 07:20:49", "link": "http://arxiv.org/abs/2308.06975v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Aesthetics of Sanskrit Poetry from the Perspective of Computational\n  Linguistics: A Case Study Analysis on Siksastaka", "abstract": "Sanskrit poetry has played a significant role in shaping the literary and\ncultural landscape of the Indian subcontinent for centuries. However, not much\nattention has been devoted to uncovering the hidden beauty of Sanskrit poetry\nin computational linguistics. This article explores the intersection of\nSanskrit poetry and computational linguistics by proposing a roadmap of an\ninterpretable framework to analyze and classify the qualities and\ncharacteristics of fine Sanskrit poetry. We discuss the rich tradition of\nSanskrit poetry and the significance of computational linguistics in\nautomatically identifying the characteristics of fine poetry. The proposed\nframework involves a human-in-the-loop approach that combines deterministic\naspects delegated to machines and deep semantics left to human experts. We\nprovide a deep analysis of Siksastaka, a Sanskrit poem, from the perspective of\n6 prominent kavyashastra schools, to illustrate the proposed framework.\nAdditionally, we provide compound, dependency, anvaya (prose order linearised\nform), meter, rasa (mood), alankar (figure of speech), and riti (writing style)\nannotations for Siksastaka and a web application to illustrate the poem's\nanalysis and annotations. Our key contributions include the proposed framework,\nthe analysis of Siksastaka, the annotations and the web application for future\nresearch. Link for interactive analysis:\nhttps://sanskritshala.github.io/shikshastakam/", "published": "2023-08-14 11:26:25", "link": "http://arxiv.org/abs/2308.07081v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Position: Key Claims in LLM Research Have a Long Tail of Footnotes", "abstract": "Much of the recent discourse within the ML community has been centered around\nLarge Language Models (LLMs), their functionality and potential -- yet not only\ndo we not have a working definition of LLMs, but much of this discourse relies\non claims and assumptions that are worth re-examining. We contribute a\ndefinition of LLMs, critically examine five common claims regarding their\nproperties (including 'emergent properties'), and conclude with suggestions for\nfuture research directions and their framing.", "published": "2023-08-14 13:00:53", "link": "http://arxiv.org/abs/2308.07120v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Incorporating Annotator Uncertainty into Representations of Discourse\n  Relations", "abstract": "Annotation of discourse relations is a known difficult task, especially for\nnon-expert annotators. In this paper, we investigate novice annotators'\nuncertainty on the annotation of discourse relations on spoken conversational\ndata. We find that dialogue context (single turn, pair of turns within speaker,\nand pair of turns across speakers) is a significant predictor of confidence\nscores. We compute distributed representations of discourse relations from\nco-occurrence statistics that incorporate information about confidence scores\nand dialogue context. We perform a hierarchical clustering analysis using these\nrepresentations and show that weighting discourse relation representations with\ninformation about confidence and dialogue context coherently models our\nannotators' uncertainty about discourse relation labels.", "published": "2023-08-14 14:39:02", "link": "http://arxiv.org/abs/2308.07179v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate", "abstract": "Text evaluation has historically posed significant challenges, often\ndemanding substantial labor and time cost. With the emergence of large language\nmodels (LLMs), researchers have explored LLMs' potential as alternatives for\nhuman evaluation. While these single-agent-based approaches show promise,\nexperimental results suggest that further advancements are needed to bridge the\ngap between their current effectiveness and human-level evaluation quality.\nRecognizing that best practices of human evaluation processes often involve\nmultiple human annotators collaborating in the evaluation, we resort to a\nmulti-agent debate framework, moving beyond single-agent prompting strategies.\nThe multi-agent-based approach enables a group of LLMs to synergize with an\narray of intelligent counterparts, harnessing their distinct capabilities and\nexpertise to enhance efficiency and effectiveness in handling intricate tasks.\nIn this paper, we construct a multi-agent referee team called ChatEval to\nautonomously discuss and evaluate the quality of generated responses from\ndifferent models on open-ended questions and traditional natural language\ngeneration (NLG) tasks. Our analysis shows that ChatEval transcends mere\ntextual scoring, offering a human-mimicking evaluation process for reliable\nassessments. Our code is available at https://github.com/chanchimin/ChatEval.", "published": "2023-08-14 15:13:04", "link": "http://arxiv.org/abs/2308.07201v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparison between parameter-efficient techniques and full fine-tuning:\n  A case study on multilingual news article classification", "abstract": "Adapters and Low-Rank Adaptation (LoRA) are parameter-efficient fine-tuning\ntechniques designed to make the training of language models more efficient.\nPrevious results demonstrated that these methods can even improve performance\non some classification tasks. This paper complements the existing research by\ninvestigating how these techniques influence the classification performance and\ncomputation costs compared to full fine-tuning when applied to multilingual\ntext classification tasks (genre, framing, and persuasion techniques detection;\nwith different input lengths, number of predicted classes and classification\ndifficulty), some of which have limited training data. In addition, we conduct\nin-depth analyses of their efficacy across different training scenarios\n(training on the original multilingual data; on the translations into English;\nand on a subset of English-only data) and different languages. Our findings\nprovide valuable insights into the applicability of the parameter-efficient\nfine-tuning techniques, particularly to complex multilingual and multilabel\nclassification tasks.", "published": "2023-08-14 17:12:43", "link": "http://arxiv.org/abs/2308.07282v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Platypus: Quick, Cheap, and Powerful Refinement of LLMs", "abstract": "We present $\\textbf{Platypus}$, a family of fine-tuned and merged Large\nLanguage Models (LLMs) that achieves the strongest performance and currently\nstands at first place in HuggingFace's Open LLM Leaderboard as of the release\ndate of this work. In this work we describe (1) our curated dataset\n$\\textbf{Open-Platypus}$, that is a subset of other open datasets and which\n$\\textit{we release to the public}$ (2) our process of fine-tuning and merging\nLoRA modules in order to conserve the strong prior of pretrained LLMs, while\nbringing specific domain knowledge to the surface (3) our efforts in checking\nfor test data leaks and contamination in the training data, which can inform\nfuture research. Specifically, the Platypus family achieves strong performance\nin quantitative LLM metrics across model sizes, topping the global Open LLM\nleaderboard while using just a fraction of the fine-tuning data and overall\ncompute that are required for other state-of-the-art fine-tuned LLMs. In\nparticular, a 13B Platypus model can be trained on $\\textit{a single}$ A100 GPU\nusing 25k questions in 5 hours. This is a testament of the quality of our\nOpen-Platypus dataset, and opens opportunities for more improvements in the\nfield. Project page: https://platypus-llm.github.io", "published": "2023-08-14 17:59:56", "link": "http://arxiv.org/abs/2308.07317v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SOTASTREAM: A Streaming Approach to Machine Translation Training", "abstract": "Many machine translation toolkits make use of a data preparation step wherein\nraw data is transformed into a tensor format that can be used directly by the\ntrainer. This preparation step is increasingly at odds with modern research and\ndevelopment practices because this process produces a static, unchangeable\nversion of the training data, making common training-time needs difficult\n(e.g., subword sampling), time-consuming (preprocessing with large data can\ntake days), expensive (e.g., disk space), and cumbersome (managing experiment\ncombinatorics). We propose an alternative approach that separates the\ngeneration of data from the consumption of that data. In this approach, there\nis no separate pre-processing step; data generation produces an infinite stream\nof permutations of the raw training data, which the trainer tensorizes and\nbatches as it is consumed. Additionally, this data stream can be manipulated by\na set of user-definable operators that provide on-the-fly modifications, such\nas data normalization, augmentation or filtering. We release an open-source\ntoolkit, SOTASTREAM, that implements this approach:\nhttps://github.com/marian-nmt/sotastream. We show that it cuts training time,\nadds flexibility, reduces experiment management complexity, and reduces disk\nspace, all without affecting the accuracy of the trained models.", "published": "2023-08-14 22:47:19", "link": "http://arxiv.org/abs/2308.07489v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CausalLM is not optimal for in-context learning", "abstract": "Recent empirical evidence indicates that transformer based in-context\nlearning performs better when using a prefix language model (prefixLM), in\nwhich in-context samples can all attend to each other, compared to causal\nlanguage models (causalLM), which use auto-regressive attention that prohibits\nin-context samples to attend to future samples. While this result is intuitive,\nit is not understood from a theoretical perspective. In this paper we take a\ntheoretical approach and analyze the convergence behavior of prefixLM and\ncausalLM under a certain parameter construction. Our analysis shows that both\nLM types converge to their stationary points at a linear rate, but that while\nprefixLM converges to the optimal solution of linear regression, causalLM\nconvergence dynamics follows that of an online gradient descent algorithm,\nwhich is not guaranteed to be optimal even as the number of samples grows\ninfinitely. We supplement our theoretical claims with empirical experiments\nover synthetic and real tasks and using various types of transformers. Our\nexperiments verify that causalLM consistently underperforms prefixLM in all\nsettings.", "published": "2023-08-14 03:14:38", "link": "http://arxiv.org/abs/2308.06912v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Large Language Models for Information Retrieval: A Survey", "abstract": "As a primary means of information acquisition, information retrieval (IR)\nsystems, such as search engines, have integrated themselves into our daily\nlives. These systems also serve as components of dialogue, question-answering,\nand recommender systems. The trajectory of IR has evolved dynamically from its\norigins in term-based methods to its integration with advanced neural models.\nWhile the neural models excel at capturing complex contextual signals and\nsemantic nuances, thereby reshaping the IR landscape, they still face\nchallenges such as data scarcity, interpretability, and the generation of\ncontextually plausible yet potentially inaccurate responses. This evolution\nrequires a combination of both traditional methods (such as term-based sparse\nretrieval methods with rapid response) and modern neural architectures (such as\nlanguage models with powerful language understanding capacity). Meanwhile, the\nemergence of large language models (LLMs), typified by ChatGPT and GPT-4, has\nrevolutionized natural language processing due to their remarkable language\nunderstanding, generation, generalization, and reasoning abilities.\nConsequently, recent research has sought to leverage LLMs to improve IR\nsystems. Given the rapid evolution of this research trajectory, it is necessary\nto consolidate existing methodologies and provide nuanced insights through a\ncomprehensive overview. In this survey, we delve into the confluence of LLMs\nand IR systems, including crucial aspects such as query rewriters, retrievers,\nrerankers, and readers. Additionally, we explore promising directions, such as\nsearch agents, within this expanding field.", "published": "2023-08-14 12:47:22", "link": "http://arxiv.org/abs/2308.07107v4", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "OctoPack: Instruction Tuning Code Large Language Models", "abstract": "Finetuning large language models (LLMs) on instructions leads to vast\nperformance improvements on natural language tasks. We apply instruction tuning\nusing code, leveraging the natural structure of Git commits, which pair code\nchanges with human instructions. We compile CommitPack: 4 terabytes of Git\ncommits across 350 programming languages. We benchmark CommitPack against other\nnatural and synthetic code instructions (xP3x, Self-Instruct, OASST) on the 16B\nparameter StarCoder model, and achieve state-of-the-art performance among\nmodels not trained on OpenAI outputs, on the HumanEval Python benchmark (46.2%\npass@1). We further introduce HumanEvalPack, expanding the HumanEval benchmark\nto a total of 3 coding tasks (Code Repair, Code Explanation, Code Synthesis)\nacross 6 languages (Python, JavaScript, Java, Go, C++, Rust). Our models,\nOctoCoder and OctoGeeX, achieve the best performance across HumanEvalPack among\nall permissive models, demonstrating CommitPack's benefits in generalizing to a\nwider set of languages and natural coding tasks. Code, models and data are\nfreely available at https://github.com/bigcode-project/octopack.", "published": "2023-08-14 13:53:54", "link": "http://arxiv.org/abs/2308.07124v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dialogue for Prompting: a Policy-Gradient-Based Discrete Prompt\n  Generation for Few-shot Learning", "abstract": "Prompt-based pre-trained language models (PLMs) paradigm have succeeded\nsubstantially in few-shot natural language processing (NLP) tasks. However,\nprior discrete prompt optimization methods require expert knowledge to design\nthe base prompt set and identify high-quality prompts, which is costly,\ninefficient, and subjective. Meanwhile, existing continuous prompt optimization\nmethods improve the performance by learning the ideal prompts through the\ngradient information of PLMs, whose high computational cost, and low\nreadability and generalizability are often concerning. To address the research\ngap, we propose a Dialogue-comprised Policy-gradient-based Discrete Prompt\nOptimization ($DP_2O$) method. We first design a multi-round dialogue alignment\nstrategy for readability prompt set generation based on GPT-4. Furthermore, we\npropose an efficient prompt screening metric to identify high-quality prompts\nwith linear complexity. Finally, we construct a reinforcement learning (RL)\nframework based on policy gradients to match the prompts to inputs optimally.\nBy training a policy network with only 0.67% of the PLM parameter size on the\ntasks in the few-shot setting, $DP_2O$ outperforms the state-of-the-art (SOTA)\nmethod by 1.52% in accuracy on average on four open-source datasets. Moreover,\nsubsequent experiments also demonstrate that $DP_2O$ has good universality,\nrobustness, and generalization ability.", "published": "2023-08-14 16:58:50", "link": "http://arxiv.org/abs/2308.07272v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "The Devil is in the Errors: Leveraging Large Language Models for\n  Fine-grained Machine Translation Evaluation", "abstract": "Automatic evaluation of machine translation (MT) is a critical tool driving\nthe rapid iterative development of MT systems. While considerable progress has\nbeen made on estimating a single scalar quality score, current metrics lack the\ninformativeness of more detailed schemes that annotate individual errors, such\nas Multidimensional Quality Metrics (MQM). In this paper, we help fill this gap\nby proposing AutoMQM, a prompting technique which leverages the reasoning and\nin-context learning capabilities of large language models (LLMs) and asks them\nto identify and categorize errors in translations. We start by evaluating\nrecent LLMs, such as PaLM and PaLM-2, through simple score prediction\nprompting, and we study the impact of labeled data through in-context learning\nand finetuning. We then evaluate AutoMQM with PaLM-2 models, and we find that\nit improves performance compared to just prompting for scores (with\nparticularly large gains for larger models) while providing interpretability\nthrough error spans that align with human annotations.", "published": "2023-08-14 17:17:21", "link": "http://arxiv.org/abs/2308.07286v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Neural Authorship Attribution: Stylometric Analysis on Large Language\n  Models", "abstract": "Large language models (LLMs) such as GPT-4, PaLM, and Llama have\nsignificantly propelled the generation of AI-crafted text. With rising concerns\nabout their potential misuse, there is a pressing need for AI-generated-text\nforensics. Neural authorship attribution is a forensic effort, seeking to trace\nAI-generated text back to its originating LLM. The LLM landscape can be divided\ninto two primary categories: proprietary and open-source. In this work, we\ndelve into these emerging categories of LLMs, focusing on the nuances of neural\nauthorship attribution. To enrich our understanding, we carry out an empirical\nanalysis of LLM writing signatures, highlighting the contrasts between\nproprietary and open-source models, and scrutinizing variations within each\ngroup. By integrating stylometric features across lexical, syntactic, and\nstructural aspects of language, we explore their potential to yield\ninterpretable results and augment pre-trained language model-based classifiers\nutilized in neural authorship attribution. Our findings, based on a range of\nstate-of-the-art LLMs, provide empirical insights into neural authorship\nattribution, paving the way for future investigations aimed at mitigating the\nthreats posed by AI-generated misinformation.", "published": "2023-08-14 17:46:52", "link": "http://arxiv.org/abs/2308.07305v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked", "abstract": "Large language models (LLMs) are popular for high-quality text generation but\ncan produce harmful content, even when aligned with human values through\nreinforcement learning. Adversarial prompts can bypass their safety measures.\nWe propose LLM Self Defense, a simple approach to defend against these attacks\nby having an LLM screen the induced responses. Our method does not require any\nfine-tuning, input preprocessing, or iterative output generation. Instead, we\nincorporate the generated content into a pre-defined prompt and employ another\ninstance of an LLM to analyze the text and predict whether it is harmful. We\ntest LLM Self Defense on GPT 3.5 and Llama 2, two of the current most prominent\nLLMs against various types of attacks, such as forcefully inducing affirmative\nresponses to prompts and prompt engineering attacks. Notably, LLM Self Defense\nsucceeds in reducing the attack success rate to virtually 0 using both GPT 3.5\nand Llama 2. The code is publicly available at\nhttps://github.com/poloclub/llm-self-defense", "published": "2023-08-14 17:54:10", "link": "http://arxiv.org/abs/2308.07308v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Development and Evaluation of Three Chatbots for Postpartum Mood and\n  Anxiety Disorders", "abstract": "In collaboration with Postpartum Support International (PSI), a non-profit\norganization dedicated to supporting caregivers with postpartum mood and\nanxiety disorders, we developed three chatbots to provide context-specific\nempathetic support to postpartum caregivers, leveraging both rule-based and\ngenerative models. We present and evaluate the performance of our chatbots\nusing both machine-based metrics and human-based questionnaires. Overall, our\nrule-based model achieves the best performance, with outputs that are close to\nground truth reference and contain the highest levels of empathy. Human users\nprefer the rule-based chatbot over the generative chatbot for its\ncontext-specific and human-like replies. Our generative chatbot also produced\nempathetic responses and was described by human users as engaging. However,\nlimitations in the training dataset often result in confusing or nonsensical\nresponses. We conclude by discussing practical benefits of rule-based vs.\ngenerative models for supporting individuals with mental health challenges. In\nlight of the recent surge of ChatGPT and BARD, we also discuss the\npossibilities and pitfalls of large language models for digital mental\nhealthcare.", "published": "2023-08-14 18:52:03", "link": "http://arxiv.org/abs/2308.07407v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Playing with words: Comparing the vocabulary and lexical diversity of\n  ChatGPT and humans", "abstract": "The introduction of Artificial Intelligence (AI) generative language models\nsuch as GPT (Generative Pre-trained Transformer) and tools such as ChatGPT has\ntriggered a revolution that can transform how text is generated. This has many\nimplications, for example, as AI-generated text becomes a significant fraction\nof the text, would this have an effect on the language capabilities of readers\nand also on the training of newer AI tools? Would it affect the evolution of\nlanguages? Focusing on one specific aspect of the language: words; will the use\nof tools such as ChatGPT increase or reduce the vocabulary used or the lexical\nrichness? This has implications for words, as those not included in\nAI-generated content will tend to be less and less popular and may eventually\nbe lost. In this work, we perform an initial comparison of the vocabulary and\nlexical richness of ChatGPT and humans when performing the same tasks. In more\ndetail, two datasets containing the answers to different types of questions\nanswered by ChatGPT and humans, and a third dataset in which ChatGPT\nparaphrases sentences and questions are used. The analysis shows that ChatGPT\ntends to use fewer distinct words and lower lexical richness than humans. These\nresults are very preliminary and additional datasets and ChatGPT configurations\nhave to be evaluated to extract more general conclusions. Therefore, further\nresearch is needed to understand how the use of ChatGPT and more broadly\ngenerative AI tools will affect the vocabulary and lexical richness in\ndifferent types of text and languages.", "published": "2023-08-14 21:19:44", "link": "http://arxiv.org/abs/2308.07462v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Automated Testing and Improvement of Named Entity Recognition Systems", "abstract": "Named entity recognition (NER) systems have seen rapid progress in recent\nyears due to the development of deep neural networks. These systems are widely\nused in various natural language processing applications, such as information\nextraction, question answering, and sentiment analysis. However, the complexity\nand intractability of deep neural networks can make NER systems unreliable in\ncertain circumstances, resulting in incorrect predictions. For example, NER\nsystems may misidentify female names as chemicals or fail to recognize the\nnames of minority groups, leading to user dissatisfaction. To tackle this\nproblem, we introduce TIN, a novel, widely applicable approach for\nautomatically testing and repairing various NER systems. The key idea for\nautomated testing is that the NER predictions of the same named entities under\nsimilar contexts should be identical. The core idea for automated repairing is\nthat similar named entities should have the same NER prediction under the same\ncontext. We use TIN to test two SOTA NER models and two commercial NER APIs,\ni.e., Azure NER and AWS NER. We manually verify 784 of the suspicious issues\nreported by TIN and find that 702 are erroneous issues, leading to high\nprecision (85.0%-93.4%) across four categories of NER errors: omission,\nover-labeling, incorrect category, and range error. For automated repairing,\nTIN achieves a high error reduction rate (26.8%-50.6%) over the four systems\nunder test, which successfully repairs 1,056 out of the 1,877 reported NER\nerrors.", "published": "2023-08-14 03:17:24", "link": "http://arxiv.org/abs/2308.07937v1", "categories": ["cs.CL", "cs.SE", "D.2.5; I.2.7"], "primary_category": "cs.CL"}
{"title": "SpeechX: Neural Codec Language Model as a Versatile Speech Transformer", "abstract": "Recent advancements in generative speech models based on audio-text prompts\nhave enabled remarkable innovations like high-quality zero-shot text-to-speech.\nHowever, existing models still face limitations in handling diverse audio-text\nspeech generation tasks involving transforming input speech and processing\naudio captured in adverse acoustic conditions. This paper introduces SpeechX, a\nversatile speech generation model capable of zero-shot TTS and various speech\ntransformation tasks, dealing with both clean and noisy signals. SpeechX\ncombines neural codec language modeling with multi-task learning using\ntask-dependent prompting, enabling unified and extensible modeling and\nproviding a consistent way for leveraging textual input in speech enhancement\nand transformation tasks. Experimental results show SpeechX's efficacy in\nvarious tasks, including zero-shot TTS, noise suppression, target speaker\nextraction, speech removal, and speech editing with or without background\nnoise, achieving comparable or superior performance to specialized models\nacross tasks. See https://aka.ms/speechx for demo samples.", "published": "2023-08-14 01:01:19", "link": "http://arxiv.org/abs/2308.06873v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "GIT-Mol: A Multi-modal Large Language Model for Molecular Science with\n  Graph, Image, and Text", "abstract": "Large language models have made significant strides in natural language\nprocessing, enabling innovative applications in molecular science by processing\ntextual representations of molecules. However, most existing language models\ncannot capture the rich information with complex molecular structures or\nimages. In this paper, we introduce GIT-Mol, a multi-modal large language model\nthat integrates the Graph, Image, and Text information. To facilitate the\nintegration of multi-modal molecular data, we propose GIT-Former, a novel\narchitecture that is capable of aligning all modalities into a unified latent\nspace. We achieve a 5%-10% accuracy increase in properties prediction and a\n20.2% boost in molecule generation validity compared to the baselines. With the\nany-to-language molecular translation strategy, our model has the potential to\nperform more downstream tasks, such as compound name recognition and chemical\nreaction prediction.", "published": "2023-08-14 03:12:29", "link": "http://arxiv.org/abs/2308.06911v3", "categories": ["cs.LG", "cs.CL", "q-bio.BM"], "primary_category": "cs.LG"}
{"title": "Approximating Human-Like Few-shot Learning with GPT-based Compression", "abstract": "In this work, we conceptualize the learning process as information\ncompression. We seek to equip generative pre-trained models with human-like\nlearning capabilities that enable data compression during inference. We present\na novel approach that utilizes the Generative Pre-trained Transformer (GPT) to\napproximate Kolmogorov complexity, with the aim of estimating the optimal\nInformation Distance for few-shot learning. We first propose using GPT as a\nprior for lossless text compression, achieving a noteworthy compression ratio.\nExperiment with LLAMA2-7B backbone achieves a compression ratio of 15.5 on\nenwik9. We justify the pre-training objective of GPT models by demonstrating\nits equivalence to the compression length, and, consequently, its ability to\napproximate the information distance for texts. Leveraging the approximated\ninformation distance, our method allows the direct application of GPT models in\nquantitative text similarity measurements. Experiment results show that our\nmethod overall achieves superior performance compared to embedding and prompt\nbaselines on challenging NLP tasks, including semantic similarity, zero and\none-shot text classification, and zero-shot text ranking.", "published": "2023-08-14 05:22:33", "link": "http://arxiv.org/abs/2308.06942v1", "categories": ["cs.AI", "cs.CL", "cs.IT", "math.IT"], "primary_category": "cs.AI"}
{"title": "#InsTag: Instruction Tagging for Analyzing Supervised Fine-tuning of\n  Large Language Models", "abstract": "Foundation language models obtain the instruction-following ability through\nsupervised fine-tuning (SFT). Diversity and complexity are considered critical\nfactors of a successful SFT dataset, while their definitions remain obscure and\nlack quantitative analyses. In this work, we propose InsTag, an open-set\nfine-grained tagger, to tag samples within SFT datasets based on semantics and\nintentions and define instruction diversity and complexity regarding tags. We\nobtain 6.6K tags to describe comprehensive user queries. Then we analyze\npopular open-sourced SFT datasets and find that the model ability grows with\nmore diverse and complex data. Based on this observation, we propose a data\nselector based on InsTag to select 6K diverse and complex samples from\nopen-source datasets and fine-tune models on InsTag-selected data. The\nresulting models, TagLM, outperform open-source models based on considerably\nlarger SFT data evaluated by MT-Bench, echoing the importance of query\ndiversity and complexity. We open-source InsTag in\nhttps://github.com/OFA-Sys/InsTag.", "published": "2023-08-14 11:16:28", "link": "http://arxiv.org/abs/2308.07074v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Temporal Sentence Grounding in Streaming Videos", "abstract": "This paper aims to tackle a novel task - Temporal Sentence Grounding in\nStreaming Videos (TSGSV). The goal of TSGSV is to evaluate the relevance\nbetween a video stream and a given sentence query. Unlike regular videos,\nstreaming videos are acquired continuously from a particular source, and are\nalways desired to be processed on-the-fly in many applications such as\nsurveillance and live-stream analysis. Thus, TSGSV is challenging since it\nrequires the model to infer without future frames and process long historical\nframes effectively, which is untouched in the early methods. To specifically\naddress the above challenges, we propose two novel methods: (1) a TwinNet\nstructure that enables the model to learn about upcoming events; and (2) a\nlanguage-guided feature compressor that eliminates redundant visual frames and\nreinforces the frames that are relevant to the query. We conduct extensive\nexperiments using ActivityNet Captions, TACoS, and MAD datasets. The results\ndemonstrate the superiority of our proposed methods. A systematic ablation\nstudy also confirms their effectiveness.", "published": "2023-08-14 12:30:58", "link": "http://arxiv.org/abs/2308.07102v1", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Language is All a Graph Needs", "abstract": "The emergence of large-scale pre-trained language models has revolutionized\nvarious AI research domains. Transformers-based Large Language Models (LLMs)\nhave gradually replaced CNNs and RNNs to unify fields of computer vision and\nnatural language processing. Compared with independent data samples such as\nimages, videos or texts, graphs usually contain rich structural and relational\ninformation. Meanwhile, language, especially natural language, being one of the\nmost expressive mediums, excels in describing complex structures. However,\nexisting work on incorporating graph problems into the generative language\nmodeling framework remains very limited. Considering the rising prominence of\nLLMs, it becomes essential to explore whether LLMs can also replace GNNs as the\nfoundation model for graphs. In this paper, we propose InstructGLM\n(Instruction-finetuned Graph Language Model) with highly scalable prompts based\non natural language instructions. We use natural language to describe\nmulti-scale geometric structure of the graph and then instruction finetune an\nLLM to perform graph tasks, which enables Generative Graph Learning. Our method\nsurpasses all GNN baselines on ogbn-arxiv, Cora and PubMed datasets,\nunderscoring its effectiveness and sheds light on generative LLMs as new\nfoundation model for graph machine learning. Our code is open-sourced at\nhttps://github.com/agiresearch/InstructGLM.", "published": "2023-08-14 13:41:09", "link": "http://arxiv.org/abs/2308.07134v5", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Human-centered NLP Fact-checking: Co-Designing with Fact-checkers using\n  Matchmaking for AI", "abstract": "While many Natural Language Processing (NLP) techniques have been proposed\nfor fact-checking, both academic research and fact-checking organizations\nreport limited adoption of such NLP work due to poor alignment with\nfact-checker practices, values, and needs. To address this, we investigate a\nco-design method, Matchmaking for AI, to enable fact-checkers, designers, and\nNLP researchers to collaboratively identify what fact-checker needs should be\naddressed by technology, and to brainstorm ideas for potential solutions.\nCo-design sessions we conducted with 22 professional fact-checkers yielded a\nset of 11 design ideas that offer a \"north star\", integrating fact-checker\ncriteria into novel NLP design concepts. These concepts range from pre-bunking\nmisinformation, efficient and personalized monitoring misinformation,\nproactively reducing fact-checker potential biases, and collaborative writing\nfact-check reports. Our work provides new insights into both human-centered\nfact-checking research and practice and AI co-design research.", "published": "2023-08-14 15:31:32", "link": "http://arxiv.org/abs/2308.07213v3", "categories": ["cs.HC", "cs.CL", "cs.CY"], "primary_category": "cs.HC"}
{"title": "Using Text Injection to Improve Recognition of Personal Identifiers in\n  Speech", "abstract": "Accurate recognition of specific categories, such as persons' names, dates or\nother identifiers is critical in many Automatic Speech Recognition (ASR)\napplications. As these categories represent personal information, ethical use\nof this data including collection, transcription, training and evaluation\ndemands special care. One way of ensuring the security and privacy of\nindividuals is to redact or eliminate Personally Identifiable Information (PII)\nfrom collection altogether. However, this results in ASR models that tend to\nhave lower recognition accuracy of these categories. We use text-injection to\nimprove the recognition of PII categories by including fake textual substitutes\nof PII categories in the training data using a text injection method. We\ndemonstrate substantial improvement to Recall of Names and Dates in medical\nnotes while improving overall WER. For alphanumeric digit sequences we show\nimprovements to Character Error Rate and Sentence Accuracy.", "published": "2023-08-14 18:26:27", "link": "http://arxiv.org/abs/2308.07393v1", "categories": ["cs.CL", "cs.SD", "eess.AS", "68T10", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Text Injection for Capitalization and Turn-Taking Prediction in Speech\n  Models", "abstract": "Text injection for automatic speech recognition (ASR), wherein unpaired\ntext-only data is used to supplement paired audio-text data, has shown\npromising improvements for word error rate. This study examines the use of text\ninjection for auxiliary tasks, which are the non-ASR tasks often performed by\nan E2E model. In this work, we use joint end-to-end and internal language model\ntraining (JEIT) as our text injection algorithm to train an ASR model which\nperforms two auxiliary tasks. The first is capitalization, which is a\nde-normalization task. The second is turn-taking prediction, which attempts to\nidentify whether a user has completed their conversation turn in a digital\nassistant interaction. We show results demonstrating that our text injection\nmethod boosts capitalization performance for long-tail data, and improves\nturn-taking detection recall.", "published": "2023-08-14 18:28:04", "link": "http://arxiv.org/abs/2308.07395v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "O-1: Self-training with Oracle and 1-best Hypothesis", "abstract": "We introduce O-1, a new self-training objective to reduce training bias and\nunify training and evaluation metrics for speech recognition. O-1 is a faster\nvariant of Expected Minimum Bayes Risk (EMBR), that boosts the oracle\nhypothesis and can accommodate both supervised and unsupervised data. We\ndemonstrate the effectiveness of our approach in terms of recognition on\npublicly available SpeechStew datasets and a large-scale, in-house data set. On\nSpeechstew, the O-1 objective closes the gap between the actual and oracle\nperformance by 80\\% relative compared to EMBR which bridges the gap by 43\\%\nrelative. O-1 achieves 13\\% to 25\\% relative improvement over EMBR on the\nvarious datasets that SpeechStew comprises of, and a 12\\% relative gap\nreduction with respect to the oracle WER over EMBR training on the in-house\ndataset. Overall, O-1 results in a 9\\% relative improvement in WER over EMBR,\nthereby speaking to the scalability of the proposed objective for large-scale\ndatasets.", "published": "2023-08-14 22:36:27", "link": "http://arxiv.org/abs/2308.07486v1", "categories": ["cs.LG", "cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Improving Audio-Visual Speech Recognition by Lip-Subword Correlation\n  Based Visual Pre-training and Cross-Modal Fusion Encoder", "abstract": "In recent research, slight performance improvement is observed from automatic\nspeech recognition systems to audio-visual speech recognition systems in the\nend-to-end framework with low-quality videos. Unmatching convergence rates and\nspecialized input representations between audio and visual modalities are\nconsidered to cause the problem. In this paper, we propose two novel techniques\nto improve audio-visual speech recognition (AVSR) under a pre-training and\nfine-tuning training framework. First, we explore the correlation between lip\nshapes and syllable-level subword units in Mandarin to establish good\nframe-level syllable boundaries from lip shapes. This enables accurate\nalignment of video and audio streams during visual model pre-training and\ncross-modal fusion. Next, we propose an audio-guided cross-modal fusion encoder\n(CMFE) neural network to utilize main training parameters for multiple\ncross-modal attention layers to make full use of modality complementarity.\nExperiments on the MISP2021-AVSR data set show the effectiveness of the two\nproposed techniques. Together, using only a relatively small amount of training\ndata, the final system achieves better performances than state-of-the-art\nsystems with more complex front-ends and back-ends.", "published": "2023-08-14 08:19:24", "link": "http://arxiv.org/abs/2308.08488v2", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Novel Ehanced Move Recognition Algorithm Based on Pre-trained Models\n  with Positional Embeddings", "abstract": "The recognition of abstracts is crucial for effectively locating the content\nand clarifying the article. Existing move recognition algorithms lack the\nability to learn word position information to obtain contextual semantics. This\npaper proposes a novel enhanced move recognition algorithm with an improved\npre-trained model and a gated network with attention mechanism for unstructured\nabstracts of Chinese scientific and technological papers. The proposed\nalgorithm first performs summary data segmentation and vocabulary training. The\nEP-ERNIE$\\_$AT-GRU framework is leveraged to incorporate word positional\ninformation, facilitating deep semantic learning and targeted feature\nextraction. Experimental results demonstrate that the proposed algorithm\nachieves 13.37$\\%$ higher accuracy on the split dataset than on the original\ndataset and a 7.55$\\%$ improvement in accuracy over the basic comparison model.", "published": "2023-08-14 03:20:28", "link": "http://arxiv.org/abs/2308.10822v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Generative Interpretation", "abstract": "We introduce generative interpretation, a new approach to estimating\ncontractual meaning using large language models. As AI triumphalism is the\norder of the day, we proceed by way of grounded case studies, each illustrating\nthe capabilities of these novel tools in distinct ways. Taking well-known\ncontracts opinions, and sourcing the actual agreements that they adjudicated,\nwe show that AI models can help factfinders ascertain ordinary meaning in\ncontext, quantify ambiguity, and fill gaps in parties' agreements. We also\nillustrate how models can calculate the probative value of individual pieces of\nextrinsic evidence. After offering best practices for the use of these models\ngiven their limitations, we consider their implications for judicial practice\nand contract theory. Using LLMs permits courts to estimate what the parties\nintended cheaply and accurately, and as such generative interpretation\nunsettles the current interpretative stalemate. Their use responds to\nefficiency-minded textualists and justice-oriented contextualists, who argue\nabout whether parties will prefer cost and certainty or accuracy and fairness.\nParties--and courts--would prefer a middle path, in which adjudicators strive\nto predict what the contract really meant, admitting just enough context to\napproximate reality while avoiding unguided and biased assimilation of\nevidence. As generative interpretation offers this possibility, we argue it can\nbecome the new workhorse of contractual interpretation.", "published": "2023-08-14 02:59:27", "link": "http://arxiv.org/abs/2308.06907v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "econ.TH"], "primary_category": "cs.CL"}
{"title": "EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language\n  Models", "abstract": "Large Language Models (LLMs) usually suffer from knowledge cutoff or fallacy\nissues, which means they are unaware of unseen events or generate text with\nincorrect facts owing to outdated/noisy data. To this end, many knowledge\nediting approaches for LLMs have emerged -- aiming to subtly inject/edit\nupdated knowledge or adjust undesired behavior while minimizing the impact on\nunrelated inputs. Nevertheless, due to significant differences among various\nknowledge editing methods and the variations in task setups, there is no\nstandard implementation framework available for the community, which hinders\npractitioners from applying knowledge editing to applications. To address these\nissues, we propose EasyEdit, an easy-to-use knowledge editing framework for\nLLMs. It supports various cutting-edge knowledge editing approaches and can be\nreadily applied to many well-known LLMs such as T5, GPT-J, LlaMA, etc.\nEmpirically, we report the knowledge editing results on LlaMA-2 with EasyEdit,\ndemonstrating that knowledge editing surpasses traditional fine-tuning in terms\nof reliability and generalization. We have released the source code on GitHub,\nalong with Google Colab tutorials and comprehensive documentation for beginners\nto get started. Besides, we present an online system for real-time knowledge\nediting, and a demo video.", "published": "2023-08-14 16:52:42", "link": "http://arxiv.org/abs/2308.07269v3", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Sound Demixing Challenge 2023 $\\unicode{x2013}$ Music Demixing Track", "abstract": "This paper summarizes the music demixing (MDX) track of the Sound Demixing\nChallenge (SDX'23). We provide a summary of the challenge setup and introduce\nthe task of robust music source separation (MSS), i.e., training MSS models in\nthe presence of errors in the training data. We propose a formalization of the\nerrors that can occur in the design of a training dataset for MSS systems and\nintroduce two new datasets that simulate such errors: SDXDB23_LabelNoise and\nSDXDB23_Bleeding. We describe the methods that achieved the highest scores in\nthe competition. Moreover, we present a direct comparison with the previous\nedition of the challenge (the Music Demixing Challenge 2021): the best\nperforming system achieved an improvement of over 1.6dB in signal-to-distortion\nratio over the winner of the previous competition, when evaluated on MDXDB21.\nBesides relying on the signal-to-distortion ratio as objective metric, we also\nperformed a listening test with renowned producers and musicians to study the\nperceptual quality of the systems and report here the results. Finally, we\nprovide our insights into the organization of the competition and our prospects\nfor future editions.", "published": "2023-08-14 07:32:03", "link": "http://arxiv.org/abs/2308.06979v4", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The Sound Demixing Challenge 2023 $\\unicode{x2013}$ Cinematic Demixing\n  Track", "abstract": "This paper summarizes the cinematic demixing (CDX) track of the Sound\nDemixing Challenge 2023 (SDX'23). We provide a comprehensive summary of the\nchallenge setup, detailing the structure of the competition and the datasets\nused. Especially, we detail CDXDB23, a new hidden dataset constructed from real\nmovies that was used to rank the submissions. The paper also offers insights\ninto the most successful approaches employed by participants. Compared to the\ncocktail-fork baseline, the best-performing system trained exclusively on the\nsimulated Divide and Remaster (DnR) dataset achieved an improvement of 1.8 dB\nin SDR, whereas the top-performing system on the open leaderboard, where any\ndata could be used for training, saw a significant improvement of 5.7 dB. A\nsignificant source of this improvement was making the simulated data better\nmatch real cinematic audio, which we further investigate in detail.", "published": "2023-08-14 07:34:00", "link": "http://arxiv.org/abs/2308.06981v4", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Integrating Emotion Recognition with Speech Recognition and Speaker\n  Diarisation for Conversations", "abstract": "Although automatic emotion recognition (AER) has recently drawn significant\nresearch interest, most current AER studies use manually segmented utterances,\nwhich are usually unavailable for dialogue systems. This paper proposes\nintegrating AER with automatic speech recognition (ASR) and speaker diarisation\n(SD) in a jointly-trained system. Distinct output layers are built for four\nsub-tasks including AER, ASR, voice activity detection and speaker\nclassification based on a shared encoder. Taking the audio of a conversation as\ninput, the integrated system finds all speech segments and transcribes the\ncorresponding emotion classes, word sequences, and speaker identities. Two\nmetrics are proposed to evaluate AER performance with automatic segmentation\nbased on time-weighted emotion and speaker classification errors. Results on\nthe IEMOCAP dataset show that the proposed system consistently outperforms two\nbaselines with separately trained single-task systems on AER, ASR and SD.", "published": "2023-08-14 13:50:47", "link": "http://arxiv.org/abs/2308.07145v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Localization of DOA trajectories -- Beyond the grid", "abstract": "The direction of arrival (DOA) estimation algorithms are crucial in\nlocalizing acoustic sources. Traditional localization methods rely on\nblock-level processing to extract the directional information from multiple\nmeasurements processed together. However, these methods assume that DOA remains\nconstant throughout the block, which may not be true in practical scenarios.\nAlso, the performance of localization methods is limited when the true\nparameters do not lie on the parameter search grid. In this paper we propose\ntwo trajectory models, namely the polynomial and bandlimited trajectory models,\nto capture the DOA dynamics. To estimate trajectory parameters, we adopt two\ngridless algorithms: i) Sliding Frank-Wolfe (SFW), which solves the Beurling\nLASSO problem and ii) Newtonized Orthogonal Matching Pursuit (NOMP), which\nimproves over OMP using cyclic refinement. Furthermore, we extend our analysis\nto include wideband processing. The simulation results indicate that the\nproposed trajectory localization algorithms exhibit improved performance\ncompared to grid-based methods in terms of resolution, robustness to noise, and\ncomputational efficiency.", "published": "2023-08-14 16:50:00", "link": "http://arxiv.org/abs/2308.07265v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "VoxBlink: A Large Scale Speaker Verification Dataset on Camera", "abstract": "In this paper, we introduce a large-scale and high-quality audio-visual\nspeaker verification dataset, named VoxBlink. We propose an innovative and\nrobust automatic audio-visual data mining pipeline to curate this dataset,\nwhich contains 1.45M utterances from 38K speakers. Due to the inherent nature\nof automated data collection, introducing noisy data is inevitable. Therefore,\nwe also utilize a multi-modal purification step to generate a cleaner version\nof the VoxBlink, named VoxBlink-clean, comprising 18K identities and 1.02M\nutterances. In contrast to the VoxCeleb, the VoxBlink sources from short videos\nof ordinary users, and the covered scenarios can better align with real-life\nsituations. To our best knowledge, the VoxBlink dataset is one of the largest\npublicly available speaker verification datasets. Leveraging the VoxCeleb and\nVoxBlink-clean datasets together, we employ diverse speaker verification models\nwith multiple architectural backbones to conduct comprehensive evaluations on\nthe VoxCeleb test sets. Experimental results indicate a substantial enhancement\nin performance,ranging from 12% to 30% relatively, across various backbone\narchitectures upon incorporating the VoxBlink-clean into the training process.\nThe details of the dataset can be found on http://voxblink.github.io", "published": "2023-08-14 10:31:29", "link": "http://arxiv.org/abs/2308.07056v7", "categories": ["eess.AS", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
{"title": "iSTFTNet2: Faster and More Lightweight iSTFT-Based Neural Vocoder Using\n  1D-2D CNN", "abstract": "The inverse short-time Fourier transform network (iSTFTNet) has garnered\nattention owing to its fast, lightweight, and high-fidelity speech synthesis.\nIt obtains these characteristics using a fast and lightweight 1D CNN as the\nbackbone and replacing some neural processes with iSTFT. Owing to the\ndifficulty of a 1D CNN to model high-dimensional spectrograms, the frequency\ndimension is reduced via temporal upsampling. However, this strategy\ncompromises the potential to enhance the speed. Therefore, we propose\niSTFTNet2, an improved variant of iSTFTNet with a 1D-2D CNN that employs 1D and\n2D CNNs to model temporal and spectrogram structures, respectively. We designed\na 2D CNN that performs frequency upsampling after conversion in a few-frequency\nspace. This design facilitates the modeling of high-dimensional spectrograms\nwithout compromising the speed. The results demonstrated that iSTFTNet2 made\niSTFTNet faster and more lightweight with comparable speech quality. Audio\nsamples are available at\nhttps://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/istftnet2/.", "published": "2023-08-14 12:56:31", "link": "http://arxiv.org/abs/2308.07117v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Active Bird2Vec: Towards End-to-End Bird Sound Monitoring with\n  Transformers", "abstract": "We propose a shift towards end-to-end learning in bird sound monitoring by\ncombining self-supervised (SSL) and deep active learning (DAL). Leveraging\ntransformer models, we aim to bypass traditional spectrogram conversions,\nenabling direct raw audio processing. ActiveBird2Vec is set to generate\nhigh-quality bird sound representations through SSL, potentially accelerating\nthe assessment of environmental changes and decision-making processes for wind\nfarms. Additionally, we seek to utilize the wide variety of bird vocalizations\nthrough DAL, reducing the reliance on extensively labeled datasets by human\nexperts. We plan to curate a comprehensive set of tasks through Huggingface\nDatasets, enhancing future comparability and reproducibility of bioacoustic\nresearch. A comparative analysis between various transformer models will be\nconducted to evaluate their proficiency in bird sound recognition tasks. We aim\nto accelerate the progression of avian bioacoustic research and contribute to\nmore effective conservation strategies.", "published": "2023-08-14 13:06:10", "link": "http://arxiv.org/abs/2308.07121v2", "categories": ["cs.SD", "cs.HC", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Human Voice Pitch Estimation: A Convolutional Network with Auto-Labeled\n  and Synthetic Data", "abstract": "In the domain of music and sound processing, pitch extraction plays a pivotal\nrole. Our research presents a specialized convolutional neural network designed\nfor pitch extraction, particularly from the human singing voice in acapella\nperformances. Notably, our approach combines synthetic data with auto-labeled\nacapella sung audio, creating a robust training environment. Evaluation across\ndatasets comprising synthetic sounds, opera recordings, and time-stretched\nvowels demonstrates its efficacy. This work paves the way for enhanced pitch\nextraction in both music and voice settings.", "published": "2023-08-14 14:26:52", "link": "http://arxiv.org/abs/2308.07170v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AudioFormer: Audio Transformer learns audio feature representations from\n  discrete acoustic codes", "abstract": "We propose a method named AudioFormer,which learns audio feature\nrepresentations through the acquisition of discrete acoustic codes and\nsubsequently fine-tunes them for audio classification tasks. Initially,we\nintroduce a novel perspective by considering the audio classification task as a\nform of natural language understanding (NLU). Leveraging an existing neural\naudio codec model,we generate discrete acoustic codes and utilize them to train\na masked language model (MLM),thereby obtaining audio feature representations.\nFurthermore,we pioneer the integration of a Multi-Positive sample Contrastive\n(MPC) learning approach. This method enables the learning of joint\nrepresentations among multiple discrete acoustic codes within the same audio\ninput. In our experiments,we treat discrete acoustic codes as textual data and\ntrain a masked language model using a cloze-like methodology,ultimately\nderiving high-quality audio representations. Notably,the MPC learning technique\neffectively captures collaborative representations among distinct positive\nsamples. Our research outcomes demonstrate that AudioFormer attains\nsignificantly improved performance compared to prevailing monomodal audio\nclassification models across multiple datasets,and even outperforms\naudio-visual multimodal classification models on select datasets.\nSpecifically,our approach achieves remarkable results on datasets including\nAudioSet (2M,20K),and FSD50K,with performance scores of 53.9,45.1,and\n65.6,respectively. We have openly shared both the code and models:\nhttps://github.com/LZH-0225/AudioFormer.git.", "published": "2023-08-14 15:47:25", "link": "http://arxiv.org/abs/2308.07221v6", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DiffSED: Sound Event Detection with Denoising Diffusion", "abstract": "Sound Event Detection (SED) aims to predict the temporal boundaries of all\nthe events of interest and their class labels, given an unconstrained audio\nsample. Taking either the splitand-classify (i.e., frame-level) strategy or the\nmore principled event-level modeling approach, all existing methods consider\nthe SED problem from the discriminative learning perspective. In this work, we\nreformulate the SED problem by taking a generative learning perspective.\nSpecifically, we aim to generate sound temporal boundaries from noisy proposals\nin a denoising diffusion process, conditioned on a target audio sample. During\ntraining, our model learns to reverse the noising process by converting noisy\nlatent queries to the groundtruth versions in the elegant Transformer decoder\nframework. Doing so enables the model generate accurate event boundaries from\neven noisy queries during inference. Extensive experiments on the Urban-SED and\nEPIC-Sounds datasets demonstrate that our model significantly outperforms\nexisting alternatives, with 40+% faster convergence in training.", "published": "2023-08-14 17:29:41", "link": "http://arxiv.org/abs/2308.07293v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Compositional nonlinear audio signal processing with Volterra series", "abstract": "We present a compositional theory of nonlinear audio signal processing based\non a categorification of the Volterra series. We begin by augmenting the\nclassical definition of the Volterra series so that it is functorial with\nrespect to a base category whose objects are temperate distributions and whose\nmorphisms are certain linear transformations. This motivates the derivation of\nformulae describing how the outcomes of nonlinear transformations are affected\nif their input signals are linearly processed--e.g., translated, modulated,\nsampled, or periodized. We then consider how nonlinear systems, themselves,\nchange, and introduce as a model thereof the notion of morphism of Volterra\nseries, which we exhibit as both a type of lens map and natural transformation.\nWe show how morphisms can be parameterized and used to generate indexed\nfamilies of Volterra series, which are well-suited to model nonstationary or\ntime-varying nonlinear phenomena. We then describe how Volterra series and\ntheir morphisms organize into a category, which we call Volt. We exhibit the\noperations of sum, product, and series composition of Volterra series as\nmonoidal products on Volt, and identify, for each in turn, its corresponding\nuniversal property. In particular, we show that the series composition of\nVolterra series is associative. We then bridge between our framework and the\nsubject at the heart of audio signal processing: time-frequency analysis.\nSpecifically, we show that a known equivalence, between a class of second-order\nVolterra series and the bilinear time-frequency distributions, can be extended\nto one between certain higher-order Volterra series and the so-called\npolynomial TFDs. We end by outlining potential avenues for future work,\nincluding the incorporation of system identification techniques and the\npotential extension of our theory to the settings of graph and topological\naudio signal processing.", "published": "2023-08-14 16:05:38", "link": "http://arxiv.org/abs/2308.07229v4", "categories": ["eess.AS", "cs.SD", "cs.SY", "eess.SP", "eess.SY"], "primary_category": "eess.AS"}
