{"title": "Coarse-grain Fine-grain Coattention Network for Multi-evidence Question\n  Answering", "abstract": "End-to-end neural models have made significant progress in question\nanswering, however recent studies show that these models implicitly assume that\nthe answer and evidence appear close together in a single document. In this\nwork, we propose the Coarse-grain Fine-grain Coattention Network (CFC), a new\nquestion answering model that combines information from evidence across\nmultiple documents. The CFC consists of a coarse-grain module that interprets\ndocuments with respect to the query then finds a relevant answer, and a\nfine-grain module which scores each candidate answer by comparing its\noccurrences across all of the documents with the query. We design these modules\nusing hierarchies of coattention and self-attention, which learn to emphasize\ndifferent parts of the input. On the Qangaroo WikiHop multi-evidence question\nanswering task, the CFC obtains a new state-of-the-art result of 70.6% on the\nblind test set, outperforming the previous best by 3% accuracy despite not\nusing pretrained contextual encoders.", "published": "2019-01-03 03:55:49", "link": "http://arxiv.org/abs/1901.00603v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Event detection in Twitter: A keyword volume approach", "abstract": "Event detection using social media streams needs a set of informative\nfeatures with strong signals that need minimal preprocessing and are highly\nassociated with events of interest. Identifying these informative features as\nkeywords from Twitter is challenging, as people use informal language to\nexpress their thoughts and feelings. This informality includes acronyms,\nmisspelled words, synonyms, transliteration and ambiguous terms. In this paper,\nwe propose an efficient method to select the keywords frequently used in\nTwitter that are mostly associated with events of interest such as protests.\nThe volume of these keywords is tracked in real time to identify the events of\ninterest in a binary classification scheme. We use keywords within word-pairs\nto capture the context. The proposed method is to binarize vectors of daily\ncounts for each word-pair by applying a spike detection temporal filter, then\nuse the Jaccard metric to measure the similarity of the binary vector for each\nword-pair with the binary vector describing event occurrence. The top n\nword-pairs are used as features to classify any day to be an event or non-event\nday. The selected features are tested using multiple classifiers such as Naive\nBayes, SVM, Logistic Regression, KNN and decision trees. They all produced AUC\nROC scores up to 0.91 and F1 scores up to 0.79. The experiment is performed\nusing the English language in multiple cities such as Melbourne, Sydney and\nBrisbane as well as the Indonesian language in Jakarta. The two experiments,\ncomprising different languages and locations, yielded similar results.", "published": "2019-01-03 01:06:55", "link": "http://arxiv.org/abs/1901.00570v1", "categories": ["cs.SI", "cs.CL", "cs.LG"], "primary_category": "cs.SI"}
{"title": "Feature reinforcement with word embedding and parsing information in\n  neural TTS", "abstract": "In this paper, we propose a feature reinforcement method under the\nsequence-to-sequence neural text-to-speech (TTS) synthesis framework. The\nproposed method utilizes the multiple input encoder to take three levels of\ntext information, i.e., phoneme sequence, pre-trained word embedding, and\ngrammatical structure of sentences from parser as the input feature for the\nneural TTS system. The added word and sentence level information can be viewed\nas the feature based pre-training strategy, which clearly enhances the model\ngeneralization ability. The proposed method not only improves the system\nrobustness significantly but also improves the synthesized speech to near\nrecording quality in our experiments for out-of-domain text.", "published": "2019-01-03 13:15:19", "link": "http://arxiv.org/abs/1901.00707v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "CLEVR-Ref+: Diagnosing Visual Reasoning with Referring Expressions", "abstract": "Referring object detection and referring image segmentation are important\ntasks that require joint understanding of visual information and natural\nlanguage. Yet there has been evidence that current benchmark datasets suffer\nfrom bias, and current state-of-the-art models cannot be easily evaluated on\ntheir intermediate reasoning process. To address these issues and complement\nsimilar efforts in visual question answering, we build CLEVR-Ref+, a synthetic\ndiagnostic dataset for referring expression comprehension. The precise\nlocations and attributes of the objects are readily available, and the\nreferring expressions are automatically associated with functional programs.\nThe synthetic nature allows control over dataset bias (through sampling\nstrategy), and the modular programs enable intermediate reasoning ground truth\nwithout human annotators.\n  In addition to evaluating several state-of-the-art models on CLEVR-Ref+, we\nalso propose IEP-Ref, a module network approach that significantly outperforms\nother models on our dataset. In particular, we present two interesting and\nimportant findings using IEP-Ref: (1) the module trained to transform feature\nmaps into segmentation masks can be attached to any intermediate module to\nreveal the entire reasoning process step-by-step; (2) even if all training data\nhas at least one object referred, IEP-Ref can correctly predict no-foreground\nwhen presented with false-premise referring expressions. To the best of our\nknowledge, this is the first direct and quantitative proof that neural modules\nbehave in the way they are intended.", "published": "2019-01-03 18:58:06", "link": "http://arxiv.org/abs/1901.00850v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
