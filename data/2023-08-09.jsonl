{"title": "Cross-Lingual Constituency Parsing for Middle High German: A\n  Delexicalized Approach", "abstract": "Constituency parsing plays a fundamental role in advancing natural language\nprocessing (NLP) tasks. However, training an automatic syntactic analysis\nsystem for ancient languages solely relying on annotated parse data is a\nformidable task due to the inherent challenges in building treebanks for such\nlanguages. It demands extensive linguistic expertise, leading to a scarcity of\navailable resources. To overcome this hurdle, cross-lingual transfer techniques\nwhich require minimal or even no annotated data for low-resource target\nlanguages offer a promising solution. In this study, we focus on building a\nconstituency parser for $\\mathbf{M}$iddle $\\mathbf{H}$igh $\\mathbf{G}$erman\n($\\mathbf{MHG}$) under realistic conditions, where no annotated MHG treebank is\navailable for training. In our approach, we leverage the linguistic continuity\nand structural similarity between MHG and $\\mathbf{M}$odern $\\mathbf{G}$erman\n($\\mathbf{MG}$), along with the abundance of MG treebank resources.\nSpecifically, by employing the $\\mathit{delexicalization}$ method, we train a\nconstituency parser on MG parse datasets and perform cross-lingual transfer to\nMHG parsing. Our delexicalized constituency parser demonstrates remarkable\nperformance on the MHG test set, achieving an F1-score of 67.3%. It outperforms\nthe best zero-shot cross-lingual baseline by a margin of 28.6% points. These\nencouraging results underscore the practicality and potential for automatic\nsyntactic analysis in other ancient languages that face similar challenges as\nMHG.", "published": "2023-08-09 01:02:06", "link": "http://arxiv.org/abs/2308.04645v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sudowoodo: a Chinese Lyric Imitation System with Source Lyrics", "abstract": "Lyrics generation is a well-known application in natural language generation\nresearch, with several previous studies focusing on generating accurate lyrics\nusing precise control such as keywords, rhymes, etc. However, lyrics imitation,\nwhich involves writing new lyrics by imitating the style and content of the\nsource lyrics, remains a challenging task due to the lack of a parallel corpus.\nIn this paper, we introduce \\textbf{\\textit{Sudowoodo}}, a Chinese lyrics\nimitation system that can generate new lyrics based on the text of source\nlyrics. To address the issue of lacking a parallel training corpus for lyrics\nimitation, we propose a novel framework to construct a parallel corpus based on\na keyword-based lyrics model from source lyrics. Then the pairs \\textit{(new\nlyrics, source lyrics)} are used to train the lyrics imitation model. During\nthe inference process, we utilize a post-processing module to filter and rank\nthe generated lyrics, selecting the highest-quality ones. We incorporated audio\ninformation and aligned the lyrics with the audio to form the songs as a bonus.\nThe human evaluation results show that our framework can perform better lyric\nimitation. Meanwhile, the \\textit{Sudowoodo} system and demo video of the\nsystem is available at\n\\href{https://Sudowoodo.apps-hp.danlu.netease.com/}{Sudowoodo} and\n\\href{https://youtu.be/u5BBT_j1L5M}{https://youtu.be/u5BBT\\_j1L5M}.", "published": "2023-08-09 02:12:04", "link": "http://arxiv.org/abs/2308.04665v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Comparative Study of Open-Source Large Language Models, GPT-4 and\n  Claude 2: Multiple-Choice Test Taking in Nephrology", "abstract": "In recent years, there have been significant breakthroughs in the field of\nnatural language processing, particularly with the development of large\nlanguage models (LLMs). These LLMs have showcased remarkable capabilities on\nvarious benchmarks. In the healthcare field, the exact role LLMs and other\nfuture AI models will play remains unclear. There is a potential for these\nmodels in the future to be used as part of adaptive physician training, medical\nco-pilot applications, and digital patient interaction scenarios. The ability\nof AI models to participate in medical training and patient care will depend in\npart on their mastery of the knowledge content of specific medical fields. This\nstudy investigated the medical knowledge capability of LLMs, specifically in\nthe context of internal medicine subspecialty multiple-choice test-taking\nability. We compared the performance of several open-source LLMs (Koala 7B,\nFalcon 7B, Stable-Vicuna 13B, and Orca Mini 13B), to GPT-4 and Claude 2 on\nmultiple-choice questions in the field of Nephrology. Nephrology was chosen as\nan example of a particularly conceptually complex subspecialty field within\ninternal medicine. The study was conducted to evaluate the ability of LLM\nmodels to provide correct answers to nephSAP (Nephrology Self-Assessment\nProgram) multiple-choice questions. The overall success of open-sourced LLMs in\nanswering the 858 nephSAP multiple-choice questions correctly was 17.1% -\n25.5%. In contrast, Claude 2 answered 54.4% of the questions correctly, whereas\nGPT-4 achieved a score of 73.3%. We show that current widely used open-sourced\nLLMs do poorly in their ability for zero-shot reasoning when compared to GPT-4\nand Claude 2. The findings of this study potentially have significant\nimplications for the future of subspecialty medical training and patient care.", "published": "2023-08-09 05:01:28", "link": "http://arxiv.org/abs/2308.04709v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Answering Unseen Questions With Smaller Language Models Using Rationale\n  Generation and Dense Retrieval", "abstract": "When provided with sufficient explanatory context, smaller Language Models\nhave been shown to exhibit strong reasoning ability on challenging short-answer\nquestion-answering tasks where the questions are unseen in training. We\nevaluate two methods for further improvement in this setting. Both methods\nfocus on combining rationales generated by a larger Language Model with longer\ncontexts created from a multi-hop dense retrieval system. The first method\n($\\textit{RR}$) involves training a Rationale Ranking model to score both\ngenerated rationales and retrieved contexts with respect to relevance and\ntruthfulness. We then use the scores to derive combined contexts from both\nknowledge sources using a number of combinatory strategies. For the second\nmethod ($\\textit{RATD}$) we utilise retrieval-augmented training datasets\ndeveloped by Hartill et al. 2023 to train a smaller Reasoning model such that\nit becomes proficient at utilising relevant information from longer text\nsequences that may be only partially evidential and frequently contain many\nirrelevant sentences. We find that both methods significantly improve results.\nOur single best Reasoning model materially improves upon strong comparable\nprior baselines for unseen evaluation datasets (StrategyQA 58.9 $\\rightarrow$\n61.7 acc., CommonsenseQA 63.6 $\\rightarrow$ 72.7 acc., ARC-DA 31.6\n$\\rightarrow$ 52.1 F1, IIRC 25.5 $\\rightarrow$ 27.3 F1) and a version utilising\nour prior knowledge of each type of question in selecting a context combination\nstrategy does even better. Our proposed models also generally outperform direct\nprompts against much larger models (BLOOM 175B and StableVicuna 13B) in both\nfew-shot chain-of-thought and standard few-shot settings.", "published": "2023-08-09 05:06:39", "link": "http://arxiv.org/abs/2308.04711v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatically measuring speech fluency in people with aphasia: first\n  achievements using read-speech data", "abstract": "Background: Speech and language pathologists (SLPs) often relyon judgements\nof speech fluency for diagnosing or monitoringpatients with aphasia. However,\nsuch subjective methods havebeen criticised for their lack of reliability and\ntheir clinical cost interms of time. Aims: This study aims at assessing the\nrelevance of a signalprocessingalgorithm, initially developed in the field of\nlanguage acquisition, for the automatic measurement of speech fluency in people\nwith aphasia (PWA). Methods & Procedures: Twenty-nine PWA and five control\nparticipantswere recruited via non-profit organizations and SLP networks. All\nparticipants were recorded while reading out loud a set ofsentences taken from\nthe French version of the Boston Diagnostic Aphasia Examination. Three trained\nSLPs assessed the fluency of each sentence on a five-point qualitative scale. A\nforward-backward divergence segmentation and a clustering algorithm were used\nto compute, for each sentence, four automatic predictors of speech fluency:\npseudo-syllable rate, speech ratio, rate of silent breaks, and standard\ndeviation of pseudo-syllable length. The four predictors were finally combined\ninto multivariate regression models (a multiplelinear regression - MLR, and two\nnon-linear models) to predict the average SLP ratings of speech fluency, using\na leave-one speaker-out validation scheme. Outcomes & Results: All models\nachieved accurate predictions of speech fluency ratings, with average\nroot-mean-square errors as low as 0.5. The MLR yielded a correlation\ncoefficient of 0.87 with reference ratings at the sentence level, and of 0.93\nwhen aggregating the data for each participant. The inclusion of an additional\npredictor sensitive to repetitions improved further the predictions with a\ncorrelation coefficient of 0.91 at the sentence level, and of 0.96 at the\nparticipant level. Conclusions: The algorithms used in this study can\nconstitute a cost-effective and reliable tool for the assessment of the speech\nfluency of patients with aphasia in read-aloud tasks. Perspectives for the\nassessment of spontaneous speech are discussed.", "published": "2023-08-09 07:51:40", "link": "http://arxiv.org/abs/2308.04763v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ADMUS: A Progressive Question Answering Framework Adaptable to Multiple\n  Knowledge Sources", "abstract": "With the introduction of deep learning models, semantic parsingbased\nknowledge base question answering (KBQA) systems have achieved high performance\nin handling complex questions. However, most existing approaches primarily\nfocus on enhancing the model's effectiveness on individual benchmark datasets,\ndisregarding the high costs of adapting the system to disparate datasets in\nreal-world scenarios (e.g., multi-tenant platform). Therefore, we present\nADMUS, a progressive knowledge base question answering framework designed to\naccommodate a wide variety of datasets, including multiple languages, diverse\nbackbone knowledge bases, and disparate question answering datasets. To\naccomplish the purpose, we decouple the architecture of conventional KBQA\nsystems and propose this dataset-independent framework. Our framework supports\nthe seamless integration of new datasets with minimal effort, only requiring\ncreating a dataset-related micro-service at a negligible cost. To enhance the\nusability of ADUMS, we design a progressive framework consisting of three\nstages, ranges from executing exact queries, generating approximate queries and\nretrieving open-domain knowledge referring from large language models. An\nonline demonstration of ADUMS is available at:\nhttps://answer.gstore.cn/pc/index.html", "published": "2023-08-09 08:46:39", "link": "http://arxiv.org/abs/2308.04800v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Bipartite Graph is All We Need for Enhancing Emotional Reasoning with\n  Commonsense Knowledge", "abstract": "The context-aware emotional reasoning ability of AI systems, especially in\nconversations, is of vital importance in applications such as online opinion\nmining from social media and empathetic dialogue systems. Due to the implicit\nnature of conveying emotions in many scenarios, commonsense knowledge is widely\nutilized to enrich utterance semantics and enhance conversation modeling.\nHowever, most previous knowledge infusion methods perform empirical knowledge\nfiltering and design highly customized architectures for knowledge interaction\nwith the utterances, which can discard useful knowledge aspects and limit their\ngeneralizability to different knowledge sources. Based on these observations,\nwe propose a Bipartite Heterogeneous Graph (BHG) method for enhancing emotional\nreasoning with commonsense knowledge. In BHG, the extracted context-aware\nutterance representations and knowledge representations are modeled as\nheterogeneous nodes. Two more knowledge aggregation node types are proposed to\nperform automatic knowledge filtering and interaction. BHG-based knowledge\ninfusion can be directly generalized to multi-type and multi-grained knowledge\nsources. In addition, we propose a Multi-dimensional Heterogeneous Graph\nTransformer (MHGT) to perform graph reasoning, which can retain unchanged\nfeature spaces and unequal dimensions for heterogeneous node types during\ninference to prevent unnecessary loss of information. Experiments show that\nBHG-based methods significantly outperform state-of-the-art knowledge infusion\nmethods and show generalized knowledge infusion ability with higher efficiency.\nFurther analysis proves that previous empirical knowledge filtering methods do\nnot guarantee to provide the most useful knowledge information. Our code is\navailable at: https://github.com/SteveKGYang/BHG.", "published": "2023-08-09 09:09:17", "link": "http://arxiv.org/abs/2308.04811v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CLEVA: Chinese Language Models EVAluation Platform", "abstract": "With the continuous emergence of Chinese Large Language Models (LLMs), how to\nevaluate a model's capabilities has become an increasingly significant issue.\nThe absence of a comprehensive Chinese benchmark that thoroughly assesses a\nmodel's performance, the unstandardized and incomparable prompting procedure,\nand the prevalent risk of contamination pose major challenges in the current\nevaluation of Chinese LLMs. We present CLEVA, a user-friendly platform crafted\nto holistically evaluate Chinese LLMs. Our platform employs a standardized\nworkflow to assess LLMs' performance across various dimensions, regularly\nupdating a competitive leaderboard. To alleviate contamination, CLEVA curates a\nsignificant proportion of new data and develops a sampling strategy that\nguarantees a unique subset for each leaderboard round. Empowered by an\neasy-to-use interface that requires just a few mouse clicks and a model API,\nusers can conduct a thorough evaluation with minimal coding. Large-scale\nexperiments featuring 23 Chinese LLMs have validated CLEVA's efficacy.", "published": "2023-08-09 09:11:31", "link": "http://arxiv.org/abs/2308.04813v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating the Generation Capabilities of Large Chinese Language Models", "abstract": "This paper unveils CG-Eval, the first-ever comprehensive and automated\nevaluation framework designed for assessing the generative capabilities of\nlarge Chinese language models across a spectrum of academic disciplines.\nCG-Eval stands out for its automated process, which critically assesses models\nbased on their proficiency in generating precise and contextually relevant\nresponses to a diverse array of questions within six key domains: Science and\nEngineering, Humanities and Social Sciences, Mathematical Calculations, Medical\nPractitioner Qualification Examination, Judicial Examination, and Certified\nPublic Accountant Examination. Alongside this, we introduce Gscore, an\ninnovative composite index developed from a weighted sum of multiple metrics.\nGscore uniquely automates the quality measurement of a model's text generation\nagainst reference standards, providing a detailed and nuanced assessment of\nmodel performance. This automation not only enhances the efficiency and\nscalability of the evaluation process but also ensures objective and consistent\nassessment across various models. The detailed test data and results,\nhighlighting the robust capabilities and comparative performance of the\nevaluated models, are accessible at http://cgeval.besteasy.com/.", "published": "2023-08-09 09:22:56", "link": "http://arxiv.org/abs/2308.04823v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Emotion-Conditioned Text Generation through Automatic Prompt\n  Optimization", "abstract": "Conditional natural language generation methods often require either\nexpensive fine-tuning or training a large language model from scratch. Both are\nunlikely to lead to good results without a substantial amount of data and\ncomputational resources. Prompt learning without changing the parameters of a\nlarge language model presents a promising alternative. It is a cost-effective\napproach, while still achieving competitive results. While this procedure is\nnow established for zero- and few-shot text classification and structured\nprediction, it has received limited attention in conditional text generation.\nWe present the first automatic prompt optimization approach for\nemotion-conditioned text generation with instruction-fine-tuned models. Our\nmethod uses an iterative optimization procedure that changes the prompt by\nadding, removing, or replacing tokens. As objective function, we only require a\ntext classifier that measures the realization of the conditional variable in\nthe generated text. We evaluate the method on emotion-conditioned text\ngeneration with a focus on event reports and compare it to manually designed\nprompts that also act as the seed for the optimization procedure. The optimized\nprompts achieve 0.75 macro-average F1 to fulfill the emotion condition in\ncontrast to manually designed seed prompts with only 0.22 macro-average F1.", "published": "2023-08-09 10:42:38", "link": "http://arxiv.org/abs/2308.04857v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Information-Theoretic Characterization of Vowel Harmony: A\n  Cross-Linguistic Study on Word Lists", "abstract": "We present a cross-linguistic study that aims to quantify vowel harmony using\ndata-driven computational modeling. Concretely, we define an\ninformation-theoretic measure of harmonicity based on the predictability of\nvowels in a natural language lexicon, which we estimate using phoneme-level\nlanguage models (PLMs). Prior quantitative studies have relied heavily on\ninflected word-forms in the analysis of vowel harmony. We instead train our\nmodels using cross-linguistically comparable lemma forms with little or no\ninflection, which enables us to cover more under-studied languages. Training\ndata for our PLMs consists of word lists with a maximum of 1000 entries per\nlanguage. Despite the fact that the data we employ are substantially smaller\nthan previously used corpora, our experiments demonstrate the neural PLMs\ncapture vowel harmony patterns in a set of languages that exhibit this\nphenomenon. Our work also demonstrates that word lists are a valuable resource\nfor typological research, and offers new possibilities for future studies on\nlow-resource, under-studied languages.", "published": "2023-08-09 11:32:16", "link": "http://arxiv.org/abs/2308.04885v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extrapolating Large Language Models to Non-English by Aligning Languages", "abstract": "Existing large language models show disparate capability across different\nlanguages, due to the imbalance in the training data. Their performances on\nEnglish tasks are often stronger than on tasks of other languages. In this\npaper, we empower pre-trained LLMs on non-English languages by building\nsemantic alignment across languages. We start from targeting individual\nlanguages by performing cross-lingual instruction-tuning (CoIT) on LLaMA, i.e.\ntuning it with translation task data and cross-lingual general task data to\nobtain cross-lingual models (x-LLaMAs), and formulate underlying scaling laws\nto investigate the advantages of using scalable translation data. Then we\nperform multilingual instruction-tuning (MuIT) with mixed resources to build\nmultilingual m-LLaMA. We also illustrate how we leverage the scaling laws to\noptimize data allocation in a resource-constrained setting. Experiment results\non cross-lingual benchmarks XQUAD and MLQA show that x-LLaMAs surpass the\nEnglish instruction-tuned counterpart (Alpaca) by an average of 27.83% across\nsix non-English languages. Evaluation results on translation dataset Flores-101\nshow that x-LLaMAs outperform previous LLaMA-based models by an average of\n18.89%. Encouragingly, m-LLaMA achieves comparable performance to x-LLaMAs on\nindividual languages and demonstrates the ability to follow multilingual\ninstructions. Further analysis on response content and representation space\nreveals the alignment of the multilingual semantic space within the middle\nlayers of m-LLaMA.", "published": "2023-08-09 13:32:06", "link": "http://arxiv.org/abs/2308.04948v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sci-CoT: Leveraging Large Language Models for Enhanced Knowledge\n  Distillation in Small Models for Scientific QA", "abstract": "Large Language Models (LLMs) have shown outstanding performance across wide\nrange of downstream tasks. This competency is attributed to their substantial\nparameter size and pre-training on extensive corpus. Moreover, LLMs have\nexhibited enhanced reasoning capabilities in tackling complex reasoning tasks,\nowing to the utilization of a method named ``Chain-of-Thought (CoT)\nprompting''. This method is designed to generate intermediate reasoning steps\nthat guide the inference of the final answer. However, it is essential to\nhighlight that these advanced reasoning abilities appear to emerge in models\nwith a minimum of 10 billion parameters, thereby limiting its efficacy in\nsituations where computational resources are constrained. In this paper, we\ninvestigate the possibility of transferring the reasoning capabilities of LLMs\nto smaller models via knowledge distillation. Specifically, we propose Sci-CoT,\na two-stage framework that separates the processes of generating rationales and\ninferring answers. This method enables a more efficient use of rationales\nduring the answer inference stage, leading to improved performance on\nscientific question-answering tasks. Utilizing Sci-CoT, our 80-million\nparameter model is able to exceed the performance of BLOOM-176B in the ARC-Easy\ndataset under the few shot setting.", "published": "2023-08-09 03:18:07", "link": "http://arxiv.org/abs/2308.04679v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Generating News-Centric Crossword Puzzles As A Constraint Satisfaction\n  and Optimization Problem", "abstract": "Crossword puzzles have traditionally served not only as entertainment but\nalso as an educational tool that can be used to acquire vocabulary and language\nproficiency. One strategy to enhance the educational purpose is\npersonalization, such as including more words on a particular topic. This paper\nfocuses on the case of encouraging people's interest in news and proposes a\nframework for automatically generating news-centric crossword puzzles. We\ndesigned possible scenarios and built a prototype as a constraint satisfaction\nand optimization problem, that is, containing as many news-derived words as\npossible. Our experiments reported the generation probabilities and time\nrequired under several conditions. The results showed that news-centric\ncrossword puzzles can be generated even with few news-derived words. We\nsummarize the current issues and future research directions through a\nqualitative evaluation of the prototype. This is the first proposal that a\nformulation of a constraint satisfaction and optimization problem can be\nbeneficial as an educational application.", "published": "2023-08-09 03:50:26", "link": "http://arxiv.org/abs/2308.04688v1", "categories": ["cs.CL", "cs.DS"], "primary_category": "cs.CL"}
{"title": "Slot Induction via Pre-trained Language Model Probing and Multi-level\n  Contrastive Learning", "abstract": "Recent advanced methods in Natural Language Understanding for Task-oriented\nDialogue (TOD) Systems (e.g., intent detection and slot filling) require a\nlarge amount of annotated data to achieve competitive performance. In reality,\ntoken-level annotations (slot labels) are time-consuming and difficult to\nacquire. In this work, we study the Slot Induction (SI) task whose objective is\nto induce slot boundaries without explicit knowledge of token-level slot\nannotations. We propose leveraging Unsupervised Pre-trained Language Model\n(PLM) Probing and Contrastive Learning mechanism to exploit (1) unsupervised\nsemantic knowledge extracted from PLM, and (2) additional sentence-level intent\nlabel signals available from TOD. Our approach is shown to be effective in SI\ntask and capable of bridging the gaps with token-level supervised models on two\nNLU benchmark datasets. When generalized to emerging intents, our SI objectives\nalso provide enhanced slot label representations, leading to improved\nperformance on the Slot Filling tasks.", "published": "2023-08-09 05:08:57", "link": "http://arxiv.org/abs/2308.04712v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Building Interpretable and Reliable Open Information Retriever for New\n  Domains Overnight", "abstract": "Information retrieval (IR) or knowledge retrieval, is a critical component\nfor many down-stream tasks such as open-domain question answering (QA). It is\nalso very challenging, as it requires succinctness, completeness, and\ncorrectness. In recent works, dense retrieval models have achieved\nstate-of-the-art (SOTA) performance on in-domain IR and QA benchmarks by\nrepresenting queries and knowledge passages with dense vectors and learning the\nlexical and semantic similarity. However, using single dense vectors and\nend-to-end supervision are not always optimal because queries may require\nattention to multiple aspects and event implicit knowledge. In this work, we\npropose an information retrieval pipeline that uses entity/event linking model\nand query decomposition model to focus more accurately on different information\nunits of the query. We show that, while being more interpretable and reliable,\nour proposed pipeline significantly improves passage coverages and denotation\naccuracies across five IR and QA benchmarks. It will be the go-to system to use\nfor applications that need to perform IR on a new domain without much dedicated\neffort, because of its superior interpretability and cross-domain performance.", "published": "2023-08-09 07:47:17", "link": "http://arxiv.org/abs/2308.04756v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Comparing How a Chatbot References User Utterances from Previous\n  Chatting Sessions: An Investigation of Users' Privacy Concerns and\n  Perceptions", "abstract": "Chatbots are capable of remembering and referencing previous conversations,\nbut does this enhance user engagement or infringe on privacy? To explore this\ntrade-off, we investigated the format of how a chatbot references previous\nconversations with a user and its effects on a user's perceptions and privacy\nconcerns. In a three-week longitudinal between-subjects study, 169 participants\ntalked about their dental flossing habits to a chatbot that either, (1-None):\ndid not explicitly reference previous user utterances, (2-Verbatim): referenced\nprevious utterances verbatim, or (3-Paraphrase): used paraphrases to reference\nprevious utterances. Participants perceived Verbatim and Paraphrase chatbots as\nmore intelligent and engaging. However, the Verbatim chatbot also raised\nprivacy concerns with participants. To gain insights as to why people prefer\ncertain conditions or had privacy concerns, we conducted semi-structured\ninterviews with 15 participants. We discuss implications from our findings that\ncan help designers choose an appropriate format to reference previous user\nutterances and inform in the design of longitudinal dialogue scripting.", "published": "2023-08-09 11:21:51", "link": "http://arxiv.org/abs/2308.04879v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Integrating large language models and active inference to understand eye\n  movements in reading and dyslexia", "abstract": "We present a novel computational model employing hierarchical active\ninference to simulate reading and eye movements. The model characterizes\nlinguistic processing as inference over a hierarchical generative model,\nfacilitating predictions and inferences at various levels of granularity, from\nsyllables to sentences.\n  Our approach combines the strengths of large language models for realistic\ntextual predictions and active inference for guiding eye movements to\ninformative textual information, enabling the testing of predictions. The model\nexhibits proficiency in reading both known and unknown words and sentences,\nadhering to the distinction between lexical and nonlexical routes in dual-route\ntheories of reading. Notably, our model permits the exploration of maladaptive\ninference effects on eye movements during reading, such as in dyslexia. To\nsimulate this condition, we attenuate the contribution of priors during the\nreading process, leading to incorrect inferences and a more fragmented reading\nstyle, characterized by a greater number of shorter saccades. This alignment\nwith empirical findings regarding eye movements in dyslexic individuals\nhighlights the model's potential to aid in understanding the cognitive\nprocesses underlying reading and eye movements, as well as how reading deficits\nassociated with dyslexia may emerge from maladaptive predictive processing.\n  In summary, our model represents a significant advancement in comprehending\nthe intricate cognitive processes involved in reading and eye movements, with\npotential implications for understanding and addressing dyslexia through the\nsimulation of maladaptive inference. It may offer valuable insights into this\ncondition and contribute to the development of more effective interventions for\ntreatment.", "published": "2023-08-09 13:16:30", "link": "http://arxiv.org/abs/2308.04941v2", "categories": ["q-bio.NC", "cs.CL"], "primary_category": "q-bio.NC"}
{"title": "LLMeBench: A Flexible Framework for Accelerating LLMs Benchmarking", "abstract": "The recent development and success of Large Language Models (LLMs)\nnecessitate an evaluation of their performance across diverse NLP tasks in\ndifferent languages. Although several frameworks have been developed and made\npublicly available, their customization capabilities for specific tasks and\ndatasets are often complex for different users. In this study, we introduce the\nLLMeBench framework, which can be seamlessly customized to evaluate LLMs for\nany NLP task, regardless of language. The framework features generic dataset\nloaders, several model providers, and pre-implements most standard evaluation\nmetrics. It supports in-context learning with zero- and few-shot settings. A\nspecific dataset and task can be evaluated for a given LLM in less than 20\nlines of code while allowing full flexibility to extend the framework for\ncustom datasets, models, or tasks. The framework has been tested on 31 unique\nNLP tasks using 53 publicly available datasets within 90 experimental setups,\ninvolving approximately 296K data points. We open-sourced LLMeBench for the\ncommunity (https://github.com/qcri/LLMeBench/) and a video demonstrating the\nframework is available online. (https://youtu.be/9cC2m_abk3A)", "published": "2023-08-09 13:22:37", "link": "http://arxiv.org/abs/2308.04945v2", "categories": ["cs.CL", "cs.AI", "68T50", "F.2.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "Exploring Multilingual Text Data Distillation", "abstract": "With the rise of deep learning, large datasets and complex models have become\ncommon, requiring significant computing power. To address this, data\ndistillation has emerged as a technique to quickly train models with lower\nmemory and time requirements. However, data distillation on text-based datasets\nhasn't been explored much because of the challenges rising due to its discrete\nnature. Additionally, existing dataset distillation methods often struggle to\ngeneralize to new architectures. In the paper, we propose several data\ndistillation techniques for multilingual text classification datasets using\nlanguage-model-based learning methods. We conduct experiments to analyze their\nperformance in terms of classification strength, and cross-architecture\ngeneralization. Furthermore, we investigate the language-specific fairness of\nthe data summaries generated by these methods. Our approach builds upon\nexisting techniques, enhancing cross-architecture generalization in the text\ndata distillation domain.", "published": "2023-08-09 14:31:57", "link": "http://arxiv.org/abs/2308.04982v1", "categories": ["cs.CL", "cs.AI", "F.2.2, I.2.7"], "primary_category": "cs.CL"}
{"title": "AspectMMKG: A Multi-modal Knowledge Graph with Aspect-aware Entities", "abstract": "Multi-modal knowledge graphs (MMKGs) combine different modal data (e.g., text\nand image) for a comprehensive understanding of entities. Despite the recent\nprogress of large-scale MMKGs, existing MMKGs neglect the multi-aspect nature\nof entities, limiting the ability to comprehend entities from various\nperspectives. In this paper, we construct AspectMMKG, the first MMKG with\naspect-related images by matching images to different entity aspects.\nSpecifically, we collect aspect-related images from a knowledge base, and\nfurther extract aspect-related sentences from the knowledge base as queries to\nretrieve a large number of aspect-related images via an online image search\nengine. Finally, AspectMMKG contains 2,380 entities, 18,139 entity aspects, and\n645,383 aspect-related images. We demonstrate the usability of AspectMMKG in\nentity aspect linking (EAL) downstream task and show that previous EAL models\nachieve a new state-of-the-art performance with the help of AspectMMKG. To\nfacilitate the research on aspect-related MMKG, we further propose an\naspect-related image retrieval (AIR) model, that aims to correct and expand\naspect-related images in AspectMMKG. We train an AIR model to learn the\nrelationship between entity image and entity aspect-related images by\nincorporating entity image, aspect, and aspect image information. Experimental\nresults indicate that the AIR model could retrieve suitable images for a given\nentity w.r.t different aspects.", "published": "2023-08-09 14:45:13", "link": "http://arxiv.org/abs/2308.04992v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RadGraph2: Modeling Disease Progression in Radiology Reports via\n  Hierarchical Information Extraction", "abstract": "We present RadGraph2, a novel dataset for extracting information from\nradiology reports that focuses on capturing changes in disease state and device\nplacement over time. We introduce a hierarchical schema that organizes entities\nbased on their relationships and show that using this hierarchy during training\nimproves the performance of an information extraction model. Specifically, we\npropose a modification to the DyGIE++ framework, resulting in our model HGIE,\nwhich outperforms previous models in entity and relation extraction tasks. We\ndemonstrate that RadGraph2 enables models to capture a wider variety of\nfindings and perform better at relation extraction compared to those trained on\nthe original RadGraph dataset. Our work provides the foundation for developing\nautomated systems that can track disease progression over time and develop\ninformation extraction models that leverage the natural hierarchy of labels in\nthe medical domain.", "published": "2023-08-09 16:19:43", "link": "http://arxiv.org/abs/2308.05046v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Constructing Holistic Spatio-Temporal Scene Graph for Video Semantic\n  Role Labeling", "abstract": "Video Semantic Role Labeling (VidSRL) aims to detect the salient events from\ngiven videos, by recognizing the predict-argument event structures and the\ninterrelationships between events. While recent endeavors have put forth\nmethods for VidSRL, they can be mostly subject to two key drawbacks, including\nthe lack of fine-grained spatial scene perception and the insufficiently\nmodeling of video temporality. Towards this end, this work explores a novel\nholistic spatio-temporal scene graph (namely HostSG) representation based on\nthe existing dynamic scene graph structures, which well model both the\nfine-grained spatial semantics and temporal dynamics of videos for VidSRL.\nBuilt upon the HostSG, we present a nichetargeting VidSRL framework. A\nscene-event mapping mechanism is first designed to bridge the gap between the\nunderlying scene structure and the high-level event semantic structure,\nresulting in an overall hierarchical scene-event (termed ICE) graph structure.\nWe further perform iterative structure refinement to optimize the ICE graph,\nsuch that the overall structure representation can best coincide with end task\ndemand. Finally, three subtask predictions of VidSRL are jointly decoded, where\nthe end-to-end paradigm effectively avoids error propagation. On the benchmark\ndataset, our framework boosts significantly over the current best-performing\nmodel. Further analyses are shown for a better understanding of the advances of\nour methods.", "published": "2023-08-09 17:20:14", "link": "http://arxiv.org/abs/2308.05081v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Decoding Layer Saliency in Language Transformers", "abstract": "In this paper, we introduce a strategy for identifying textual saliency in\nlarge-scale language models applied to classification tasks. In visual networks\nwhere saliency is more well-studied, saliency is naturally localized through\nthe convolutional layers of the network; however, the same is not true in\nmodern transformer-stack networks used to process natural language. We adapt\ngradient-based saliency methods for these networks, propose a method for\nevaluating the degree of semantic coherence of each layer, and demonstrate\nconsistent improvement over numerous other methods for textual saliency on\nmultiple benchmark classification datasets. Our approach requires no additional\ntraining or access to labelled data, and is comparatively very computationally\nefficient.", "published": "2023-08-09 20:53:22", "link": "http://arxiv.org/abs/2308.05219v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TSSR: A Truncated and Signed Square Root Activation Function for Neural\n  Networks", "abstract": "Activation functions are essential components of neural networks. In this\npaper, we introduce a new activation function called the Truncated and Signed\nSquare Root (TSSR) function. This function is distinctive because it is odd,\nnonlinear, monotone and differentiable. Its gradient is continuous and always\npositive. Thanks to these properties, it has the potential to improve the\nnumerical stability of neural networks. Several experiments confirm that the\nproposed TSSR has better performance than other stat-of-the-art activation\nfunctions. The proposed function has significant implications for the\ndevelopment of neural network models and can be applied to a wide range of\napplications in fields such as computer vision, natural language processing,\nand speech recognition.", "published": "2023-08-09 09:40:34", "link": "http://arxiv.org/abs/2308.04832v1", "categories": ["cs.CV", "cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CV"}
{"title": "Unsupervised Out-of-Distribution Dialect Detection with Mahalanobis\n  Distance", "abstract": "Dialect classification is used in a variety of applications, such as machine\ntranslation and speech recognition, to improve the overall performance of the\nsystem. In a real-world scenario, a deployed dialect classification model can\nencounter anomalous inputs that differ from the training data distribution,\nalso called out-of-distribution (OOD) samples. Those OOD samples can lead to\nunexpected outputs, as dialects of those samples are unseen during model\ntraining. Out-of-distribution detection is a new research area that has\nreceived little attention in the context of dialect classification. Towards\nthis, we proposed a simple yet effective unsupervised Mahalanobis distance\nfeature-based method to detect out-of-distribution samples. We utilize the\nlatent embeddings from all intermediate layers of a wav2vec 2.0\ntransformer-based dialect classifier model for multi-task learning. Our\nproposed approach outperforms other state-of-the-art OOD detection methods\nsignificantly.", "published": "2023-08-09 11:33:53", "link": "http://arxiv.org/abs/2308.04886v1", "categories": ["cs.CL", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "LLaMA-E: Empowering E-commerce Authoring with Object-Interleaved\n  Instruction Following", "abstract": "E-commerce authoring entails creating engaging, diverse, and targeted content\nto enhance preference elicitation and retrieval experience. While Large\nLanguage Models (LLMs) have revolutionized content generation, they often fall\nshort in e-commerce applications due to their limited memorization of\ndomain-specific features. This paper proposes LLaMA-E, the unified e-commerce\nauthoring models that address the contextual preferences of customers, sellers,\nand platforms, the essential objects in e-commerce operation. We design the\ninstruction set derived from tasks of ads generation, query-enhanced product\ntitle rewriting, product classification, purchase intent speculation, and\ngeneral e-commerce Q&A. The instruction formulation ensures the interleaved\ncover of the presented and required object features, allowing the alignment of\nbase models to parameterise e-commerce knowledge comprehensively. The proposed\nLLaMA-E models achieve state-of-the-art evaluation performance and exhibit the\nadvantage in zero-shot practical applications. To our knowledge, this is the\nfirst LLM tailored to empower authoring applications with comprehensive\nscenario understanding by integrating features focused on participated objects.", "published": "2023-08-09 12:26:37", "link": "http://arxiv.org/abs/2308.04913v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Performance Analysis of Transformer Based Models (BERT, ALBERT and\n  RoBERTa) in Fake News Detection", "abstract": "Fake news is fake material in a news media format but is not processed\nproperly by news agencies. The fake material can provoke or defame significant\nentities or individuals or potentially even for the personal interests of the\ncreators, causing problems for society. Distinguishing fake news and real news\nis challenging due to limited of domain knowledge and time constraints.\nAccording to the survey, the top three areas most exposed to hoaxes and\nmisinformation by residents are in Banten, DKI Jakarta and West Java. The model\nof transformers is referring to an approach in the field of artificial\nintelligence (AI) in natural language processing utilizing the deep learning\narchitectures. Transformers exercise a powerful attention mechanism to process\ntext in parallel and produce rich and contextual word representations. A\nprevious study indicates a superior performance of a transformer model known as\nBERT over and above non transformer approach. However, some studies suggest the\nperformance can be improved with the use of improved BERT models known as\nALBERT and RoBERTa. However, the modified BERT models are not well explored for\ndetecting fake news in Bahasa Indonesia. In this research, we explore those\ntransformer models and found that ALBERT outperformed other models with 87.6%\naccuracy, 86.9% precision, 86.9% F1-score, and 174.5 run-time (s/epoch)\nrespectively. Source code available at:\nhttps://github.com/Shafna81/fakenewsdetection.git", "published": "2023-08-09 13:33:27", "link": "http://arxiv.org/abs/2308.04950v1", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TBIN: Modeling Long Textual Behavior Data for CTR Prediction", "abstract": "Click-through rate (CTR) prediction plays a pivotal role in the success of\nrecommendations. Inspired by the recent thriving of language models (LMs), a\nsurge of works improve prediction by organizing user behavior data in a\n\\textbf{textual} format and using LMs to understand user interest at a semantic\nlevel. While promising, these works have to truncate the textual data to reduce\nthe quadratic computational overhead of self-attention in LMs. However, it has\nbeen studied that long user behavior data can significantly benefit CTR\nprediction. In addition, these works typically condense user diverse interests\ninto a single feature vector, which hinders the expressive capability of the\nmodel. In this paper, we propose a \\textbf{T}extual \\textbf{B}ehavior-based\n\\textbf{I}nterest Chunking \\textbf{N}etwork (TBIN), which tackles the above\nlimitations by combining an efficient locality-sensitive hashing algorithm and\na shifted chunk-based self-attention. The resulting user diverse interests are\ndynamically activated, producing user interest representation towards the\ntarget item. Finally, the results of both offline and online experiments on\nreal-world food recommendation platform demonstrate the effectiveness of TBIN.", "published": "2023-08-09 03:48:41", "link": "http://arxiv.org/abs/2308.08483v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Conceptualizing Machine Learning for Dynamic Information Retrieval of\n  Electronic Health Record Notes", "abstract": "The large amount of time clinicians spend sifting through patient notes and\ndocumenting in electronic health records (EHRs) is a leading cause of clinician\nburnout. By proactively and dynamically retrieving relevant notes during the\ndocumentation process, we can reduce the effort required to find relevant\npatient history. In this work, we conceptualize the use of EHR audit logs for\nmachine learning as a source of supervision of note relevance in a specific\nclinical context, at a particular point in time. Our evaluation focuses on the\ndynamic retrieval in the emergency department, a high acuity setting with\nunique patterns of information retrieval and note writing. We show that our\nmethods can achieve an AUC of 0.963 for predicting which notes will be read in\nan individual note writing session. We additionally conduct a user study with\nseveral clinicians and find that our framework can help clinicians retrieve\nrelevant information more efficiently. Demonstrating that our framework and\nmethods can perform well in this demanding setting is a promising proof of\nconcept that they will translate to other clinical settings and data modalities\n(e.g., labs, medications, imaging).", "published": "2023-08-09 21:04:19", "link": "http://arxiv.org/abs/2308.08494v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "On the Unexpected Abilities of Large Language Models", "abstract": "Large Language Models (LLMs) are capable of displaying a wide range of\nabilities that are not directly connected with the task for which they are\ntrained: predicting the next words of human-written texts. In this article, I\nreview recent research investigating the cognitive abilities developed by LLMs\nand their relation to human cognition. I discuss the nature of the indirect\nprocess that leads to the acquisition of these cognitive abilities, their\nrelation to other indirect processes, and the implications for the acquisition\nof integrated abilities. Moreover, I propose the factors that enable the\ndevelopment of abilities that are related only very indirectly to the proximal\nobjective of the training task. Finally, I discuss whether the full set of\ncapabilities that LLMs could possibly develop is predictable.", "published": "2023-08-09 09:15:07", "link": "http://arxiv.org/abs/2308.09720v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Speaker Recognition Using Isomorphic Graph Attention Network Based\n  Pooling on Self-Supervised Representation", "abstract": "The emergence of self-supervised representation (i.e., wav2vec 2.0) allows\nspeaker-recognition approaches to process spoken signals through foundation\nmodels built on speech data. Nevertheless, effective fusion on the\nrepresentation requires further investigating, due to the inclusion of fixed or\nsub-optimal temporal pooling strategies. Despite of improved strategies\nconsidering graph learning and graph attention factors, non-injective\naggregation still exists in the approaches, which may influence the performance\nfor speaker recognition. In this regard, we propose a speaker recognition\napproach using Isomorphic Graph ATtention network (IsoGAT) on self-supervised\nrepresentation. The proposed approach contains three modules of representation\nlearning, graph attention, and aggregation, jointly considering learning on the\nself-supervised representation and the IsoGAT. Then, we perform experiments for\nspeaker recognition tasks on VoxCeleb1\\&2 datasets, with the corresponding\nexperimental results demonstrating the recognition performance for the proposed\napproach, compared with existing pooling approaches on the self-supervised\nrepresentation.", "published": "2023-08-09 02:14:10", "link": "http://arxiv.org/abs/2308.04666v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DiVa: An Iterative Framework to Harvest More Diverse and Valid Labels\n  from User Comments for Music", "abstract": "Towards sufficient music searching, it is vital to form a complete set of\nlabels for each song. However, current solutions fail to resolve it as they\ncannot produce diverse enough mappings to make up for the information missed by\nthe gold labels. Based on the observation that such missing information may\nalready be presented in user comments, we propose to study the automated music\nlabeling in an essential but under-explored setting, where the model is\nrequired to harvest more diverse and valid labels from the users' comments\ngiven limited gold labels. To this end, we design an iterative framework (DiVa)\nto harvest more $\\underline{\\text{Di}}$verse and $\\underline{\\text{Va}}$lid\nlabels from user comments for music. The framework makes a classifier able to\nform complete sets of labels for songs via pseudo-labels inferred from\npre-trained classifiers and a novel joint score function. The experiment on a\ndensely annotated testing set reveals the superiority of the Diva over\nstate-of-the-art solutions in producing more diverse labels missed by the gold\nlabels. We hope our work can inspire future research on automated music\nlabeling.", "published": "2023-08-09 08:59:48", "link": "http://arxiv.org/abs/2308.04805v1", "categories": ["cs.IR", "cs.SD", "eess.AS"], "primary_category": "cs.IR"}
{"title": "Representation Learning for Audio Privacy Preservation using Source\n  Separation and Robust Adversarial Learning", "abstract": "Privacy preservation has long been a concern in smart acoustic monitoring\nsystems, where speech can be passively recorded along with a target signal in\nthe system's operating environment. In this study, we propose the integration\nof two commonly used approaches in privacy preservation: source separation and\nadversarial representation learning. The proposed system learns the latent\nrepresentation of audio recordings such that it prevents differentiating\nbetween speech and non-speech recordings. Initially, the source separation\nnetwork filters out some of the privacy-sensitive data, and during the\nadversarial learning process, the system will learn privacy-preserving\nrepresentation on the filtered signal. We demonstrate the effectiveness of our\nproposed method by comparing our method against systems without source\nseparation, without adversarial learning, and without both. Overall, our\nresults suggest that the proposed system can significantly improve speech\nprivacy preservation compared to that of using source separation or adversarial\nlearning solely while maintaining good performance in the acoustic monitoring\ntask.", "published": "2023-08-09 13:50:00", "link": "http://arxiv.org/abs/2308.04960v1", "categories": ["cs.SD", "cs.CR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Transferable Models for Bioacoustics with Human Language Supervision", "abstract": "Passive acoustic monitoring offers a scalable, non-invasive method for\ntracking global biodiversity and anthropogenic impacts on species. Although\ndeep learning has become a vital tool for processing this data, current models\nare inflexible, typically cover only a handful of species, and are limited by\ndata scarcity. In this work, we propose BioLingual, a new model for\nbioacoustics based on contrastive language-audio pretraining. We first\naggregate bioacoustic archives into a language-audio dataset, called\nAnimalSpeak, with over a million audio-caption pairs holding information on\nspecies, vocalization context, and animal behavior. After training on this\ndataset to connect language and audio representations, our model can identify\nover a thousand species' calls across taxa, complete bioacoustic tasks\nzero-shot, and retrieve animal vocalization recordings from natural text\nqueries. When fine-tuned, BioLingual sets a new state-of-the-art on nine tasks\nin the Benchmark of Animal Sounds. Given its broad taxa coverage and ability to\nbe flexibly queried in human language, we believe this model opens new\nparadigms in ecological monitoring and research, including free-text search on\nthe world's acoustic monitoring archives. We open-source our models, dataset,\nand code.", "published": "2023-08-09 14:22:18", "link": "http://arxiv.org/abs/2308.04978v1", "categories": ["cs.LG", "cs.SD", "eess.AS", "q-bio.QM"], "primary_category": "cs.LG"}
{"title": "Separate Anything You Describe", "abstract": "Language-queried audio source separation (LASS) is a new paradigm for\ncomputational auditory scene analysis (CASA). LASS aims to separate a target\nsound from an audio mixture given a natural language query, which provides a\nnatural and scalable interface for digital audio applications. Recent works on\nLASS, despite attaining promising separation performance on specific sources\n(e.g., musical instruments, limited classes of audio events), are unable to\nseparate audio concepts in the open domain. In this work, we introduce\nAudioSep, a foundation model for open-domain audio source separation with\nnatural language queries. We train AudioSep on large-scale multimodal datasets\nand extensively evaluate its capabilities on numerous tasks including audio\nevent separation, musical instrument separation, and speech enhancement.\nAudioSep demonstrates strong separation performance and impressive zero-shot\ngeneralization ability using audio captions or text labels as queries,\nsubstantially outperforming previous audio-queried and language-queried sound\nseparation models. For reproducibility of this work, we will release the source\ncode, evaluation benchmark and pre-trained model at:\nhttps://github.com/Audio-AGI/AudioSep.", "published": "2023-08-09 16:09:44", "link": "http://arxiv.org/abs/2308.05037v3", "categories": ["eess.AS", "cs.AI", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Analyzing the Effect of Data Impurity on the Detection Performances of\n  Mental Disorders", "abstract": "The primary method for identifying mental disorders automatically has\ntraditionally involved using binary classifiers. These classifiers are trained\nusing behavioral data obtained from an interview setup. In this training\nprocess, data from individuals with the specific disorder under consideration\nare categorized as the positive class, while data from all other participants\nconstitute the negative class. In practice, it is widely recognized that\ncertain mental disorders share similar symptoms, causing the collected\nbehavioral data to encompass a variety of attributes associated with multiple\ndisorders. Consequently, attributes linked to the targeted mental disorder\nmight also be present within the negative class. This data impurity may lead to\nsub-optimal training of the classifier for a mental disorder of interest. In\nthis study, we investigate this hypothesis in the context of major depressive\ndisorder (MDD) and post-traumatic stress disorder detection (PTSD). The results\nshow that upon removal of such data impurity, MDD and PTSD detection\nperformances are significantly improved.", "published": "2023-08-09 13:13:26", "link": "http://arxiv.org/abs/2308.05133v1", "categories": ["q-bio.NC", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "q-bio.NC"}
{"title": "Sound propagation in realistic interactive 3D scenes with parameterized\n  sources using deep neural operators", "abstract": "We address the challenge of sound propagation simulations in 3D virtual rooms\nwith moving sources, which have applications in virtual/augmented reality, game\naudio, and spatial computing. Solutions to the wave equation can describe wave\nphenomena such as diffraction and interference. However, simulating them using\nconventional numerical discretization methods with hundreds of source and\nreceiver positions is intractable, making stimulating a sound field with moving\nsources impractical. To overcome this limitation, we propose using deep\noperator networks to approximate linear wave-equation operators. This enables\nthe rapid prediction of sound propagation in realistic 3D acoustic scenes with\nmoving sources, achieving millisecond-scale computations. By learning a compact\nsurrogate model, we avoid the offline calculation and storage of impulse\nresponses for all relevant source/listener pairs. Our experiments, including\nvarious complex scene geometries, show good agreement with reference solutions,\nwith root mean squared errors ranging from 0.02 Pa to 0.10 Pa. Notably, our\nmethod signifies a paradigm shift as no prior machine learning approach has\nachieved precise predictions of complete wave fields within realistic domains.\nWe anticipate that our findings will drive further exploration of deep neural\noperator methods, advancing research in immersive user experiences within\nvirtual environments.$", "published": "2023-08-09 16:32:51", "link": "http://arxiv.org/abs/2308.05141v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Conformer-based Target-Speaker Automatic Speech Recognition for\n  Single-Channel Audio", "abstract": "We propose CONF-TSASR, a non-autoregressive end-to-end time-frequency domain\narchitecture for single-channel target-speaker automatic speech recognition\n(TS-ASR). The model consists of a TitaNet based speaker embedding module, a\nConformer based masking as well as ASR modules. These modules are jointly\noptimized to transcribe a target-speaker, while ignoring speech from other\nspeakers. For training we use Connectionist Temporal Classification (CTC) loss\nand introduce a scale-invariant spectrogram reconstruction loss to encourage\nthe model better separate the target-speaker's spectrogram from mixture. We\nobtain state-of-the-art target-speaker word error rate (TS-WER) on\nWSJ0-2mix-extr (4.2%). Further, we report for the first time TS-WER on\nWSJ0-3mix-extr (12.4%), LibriSpeech2Mix (4.2%) and LibriSpeech3Mix (7.6%)\ndatasets, establishing new benchmarks for TS-ASR. The proposed model will be\nopen-sourced through NVIDIA NeMo toolkit.", "published": "2023-08-09 20:51:54", "link": "http://arxiv.org/abs/2308.05218v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "JEN-1: Text-Guided Universal Music Generation with Omnidirectional\n  Diffusion Models", "abstract": "Music generation has attracted growing interest with the advancement of deep\ngenerative models. However, generating music conditioned on textual\ndescriptions, known as text-to-music, remains challenging due to the complexity\nof musical structures and high sampling rate requirements. Despite the task's\nsignificance, prevailing generative models exhibit limitations in music\nquality, computational efficiency, and generalization. This paper introduces\nJEN-1, a universal high-fidelity model for text-to-music generation. JEN-1 is a\ndiffusion model incorporating both autoregressive and non-autoregressive\ntraining. Through in-context learning, JEN-1 performs various generation tasks\nincluding text-guided music generation, music inpainting, and continuation.\nEvaluations demonstrate JEN-1's superior performance over state-of-the-art\nmethods in text-music alignment and music quality while maintaining\ncomputational efficiency. Our demos are available at\nhttp://futureverse.com/research/jen/demos/jen1", "published": "2023-08-09 06:27:24", "link": "http://arxiv.org/abs/2308.04729v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Induction Network: Audio-Visual Modality Gap-Bridging for\n  Self-Supervised Sound Source Localization", "abstract": "Self-supervised sound source localization is usually challenged by the\nmodality inconsistency. In recent studies, contrastive learning based\nstrategies have shown promising to establish such a consistent correspondence\nbetween audio and sound sources in visual scenarios. Unfortunately, the\ninsufficient attention to the heterogeneity influence in the different modality\nfeatures still limits this scheme to be further improved, which also becomes\nthe motivation of our work. In this study, an Induction Network is proposed to\nbridge the modality gap more effectively. By decoupling the gradients of visual\nand audio modalities, the discriminative visual representations of sound\nsources can be learned with the designed Induction Vector in a bootstrap\nmanner, which also enables the audio modality to be aligned with the visual\nmodality consistently. In addition to a visual weighted contrastive loss, an\nadaptive threshold selection strategy is introduced to enhance the robustness\nof the Induction Network. Substantial experiments conducted on SoundNet-Flickr\nand VGG-Sound Source datasets have demonstrated a superior performance compared\nto other state-of-the-art works in different challenging scenarios. The code is\navailable at https://github.com/Tahy1/AVIN", "published": "2023-08-09 07:55:12", "link": "http://arxiv.org/abs/2308.04767v1", "categories": ["cs.CV", "cs.AI", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
