{"title": "Deep Residual Learning for Small-Footprint Keyword Spotting", "abstract": "We explore the application of deep residual learning and dilated convolutions\nto the keyword spotting task, using the recently-released Google Speech\nCommands Dataset as our benchmark. Our best residual network (ResNet)\nimplementation significantly outperforms Google's previous convolutional neural\nnetworks in terms of accuracy. By varying model depth and width, we can achieve\ncompact models that also outperform previous small-footprint variants. To our\nknowledge, we are the first to examine these approaches for keyword spotting,\nand our results establish an open-source state-of-the-art reference to support\nthe development of future speech-based interfaces.", "published": "2017-10-28 00:43:01", "link": "http://arxiv.org/abs/1710.10361v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Study of All-Convolutional Encoders for Connectionist Temporal\n  Classification", "abstract": "Connectionist temporal classification (CTC) is a popular sequence prediction\napproach for automatic speech recognition that is typically used with models\nbased on recurrent neural networks (RNNs). We explore whether deep\nconvolutional neural networks (CNNs) can be used effectively instead of RNNs as\nthe \"encoder\" in CTC. CNNs lack an explicit representation of the entire\nsequence, but have the advantage that they are much faster to train. We present\nan exploration of CNNs as encoders for CTC models, in the context of\ncharacter-based (lexicon-free) automatic speech recognition. In particular, we\nexplore a range of one-dimensional convolutional layers, which are particularly\nefficient. We compare the performance of our CNN-based models against typical\nRNNbased models in terms of training time, decoding time, model size and word\nerror rate (WER) on the Switchboard Eval2000 corpus. We find that our CNN-based\nmodels are close in performance to LSTMs, while not matching them, and are much\nfaster to train and decode.", "published": "2017-10-28 06:24:36", "link": "http://arxiv.org/abs/1710.10398v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Inducing Regular Grammars Using Recurrent Neural Networks", "abstract": "Grammar induction is the task of learning a grammar from a set of examples.\nRecently, neural networks have been shown to be powerful learning machines that\ncan identify patterns in streams of data. In this work we investigate their\neffectiveness in inducing a regular grammar from data, without any assumptions\nabout the grammar. We train a recurrent neural network to distinguish between\nstrings that are in or outside a regular language, and utilize an algorithm for\nextracting the learned finite-state automaton. We apply this method to several\nregular languages and find unexpected results regarding the connections between\nthe network's states that may be regarded as evidence for generalization.", "published": "2017-10-28 12:00:09", "link": "http://arxiv.org/abs/1710.10453v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Phase Conductor on Multi-layered Attentions for Machine Comprehension", "abstract": "Attention models have been intensively studied to improve NLP tasks such as\nmachine comprehension via both question-aware passage attention model and\nself-matching attention model. Our research proposes phase conductor\n(PhaseCond) for attention models in two meaningful ways. First, PhaseCond, an\narchitecture of multi-layered attention models, consists of multiple phases\neach implementing a stack of attention layers producing passage representations\nand a stack of inner or outer fusion layers regulating the information flow.\nSecond, we extend and improve the dot-product attention function for PhaseCond\nby simultaneously encoding multiple question and passage embedding layers from\ndifferent perspectives. We demonstrate the effectiveness of our proposed model\nPhaseCond on the SQuAD dataset, showing that our model significantly\noutperforms both state-of-the-art single-layered and multiple-layered attention\nmodels. We deepen our results with new findings via both detailed qualitative\nanalysis and visualized examples showing the dynamic changes through\nmulti-layered attention models.", "published": "2017-10-28 17:28:04", "link": "http://arxiv.org/abs/1710.10504v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Dual Encoder Sequence to Sequence Model for Open-Domain Dialogue\n  Modeling", "abstract": "Ever since the successful application of sequence to sequence learning for\nneural machine translation systems, interest has surged in its applicability\ntowards language generation in other problem domains. Recent work has\ninvestigated the use of these neural architectures towards modeling open-domain\nconversational dialogue, where it has been found that although these models are\ncapable of learning a good distributional language model, dialogue coherence is\nstill of concern. Unlike translation, conversation is much more a one-to-many\nmapping from utterance to a response, and it is even more pressing that the\nmodel be aware of the preceding flow of conversation. In this paper we propose\nto tackle this problem by introducing previous conversational context in terms\nof latent representations of dialogue acts over time. We inject the latent\ncontext representations into a sequence to sequence neural network in the form\nof dialog acts using a second encoder to enhance the quality and the coherence\nof the conversations generated. The main task of this research work is to show\nthat adding latent variables that capture discourse relations does indeed\nresult in more coherent responses when compared to conventional sequence to\nsequence models.", "published": "2017-10-28 19:40:47", "link": "http://arxiv.org/abs/1710.10520v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "JSUT corpus: free large-scale Japanese speech corpus for end-to-end\n  speech synthesis", "abstract": "Thanks to improvements in machine learning techniques including deep\nlearning, a free large-scale speech corpus that can be shared between academic\ninstitutions and commercial companies has an important role. However, such a\ncorpus for Japanese speech synthesis does not exist. In this paper, we designed\na novel Japanese speech corpus, named the \"JSUT corpus,\" that is aimed at\nachieving end-to-end speech synthesis. The corpus consists of 10 hours of\nreading-style speech data and its transcription and covers all of the main\npronunciations of daily-use Japanese characters. In this paper, we describe how\nwe designed and analyzed the corpus. The corpus is freely available online.", "published": "2017-10-28 05:28:01", "link": "http://arxiv.org/abs/1711.00354v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Topic Based Sentiment Analysis Using Deep Learning", "abstract": "In this paper , we tackle Sentiment Analysis conditioned on a Topic in\nTwitter data using Deep Learning . We propose a 2-tier approach : In the first\nphase we create our own Word Embeddings and see that they do perform better\nthan state-of-the-art embeddings when used with standard classifiers. We then\nperform inference on these embeddings to learn more about a word with respect\nto all the topics being considered, and also the top n-influencing words for\neach topic. In the second phase we use these embeddings to predict the\nsentiment of the tweet with respect to a given topic, and all other topics\nunder discussion.", "published": "2017-10-28 17:13:49", "link": "http://arxiv.org/abs/1710.10498v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Speeding up Context-based Sentence Representation Learning with\n  Non-autoregressive Convolutional Decoding", "abstract": "Context plays an important role in human language understanding, thus it may\nalso be useful for machines learning vector representations of language. In\nthis paper, we explore an asymmetric encoder-decoder structure for unsupervised\ncontext-based sentence representation learning. We carefully designed\nexperiments to show that neither an autoregressive decoder nor an RNN decoder\nis required. After that, we designed a model which still keeps an RNN as the\nencoder, while using a non-autoregressive convolutional decoder. We further\ncombine a suite of effective designs to significantly improve model efficiency\nwhile also achieving better performance. Our model is trained on two different\nlarge unlabelled corpora, and in both cases the transferability is evaluated on\na set of downstream NLP tasks. We empirically show that our model is simple and\nfast while producing rich sentence representations that excel in downstream\ntasks.", "published": "2017-10-28 03:18:12", "link": "http://arxiv.org/abs/1710.10380v3", "categories": ["cs.NE", "cs.CL", "cs.LG"], "primary_category": "cs.NE"}
{"title": "Label Embedding Network: Learning Label Representation for Soft Training\n  of Deep Networks", "abstract": "We propose a method, called Label Embedding Network, which can learn label\nrepresentation (label embedding) during the training process of deep networks.\nWith the proposed method, the label embedding is adaptively and automatically\nlearned through back propagation. The original one-hot represented loss\nfunction is converted into a new loss function with soft distributions, such\nthat the originally unrelated labels have continuous interactions with each\nother during the training process. As a result, the trained model can achieve\nsubstantially higher accuracy and with faster convergence speed. Experimental\nresults based on competitive tasks demonstrate the effectiveness of the\nproposed method, and the learned label embedding is reasonable and\ninterpretable. The proposed method achieves comparable or even better results\nthan the state-of-the-art systems. The source code is available at\n\\url{https://github.com/lancopku/LabelEmb}.", "published": "2017-10-28 05:42:19", "link": "http://arxiv.org/abs/1710.10393v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Generalized End-to-End Loss for Speaker Verification", "abstract": "In this paper, we propose a new loss function called generalized end-to-end\n(GE2E) loss, which makes the training of speaker verification models more\nefficient than our previous tuple-based end-to-end (TE2E) loss function. Unlike\nTE2E, the GE2E loss function updates the network in a way that emphasizes\nexamples that are difficult to verify at each step of the training process.\nAdditionally, the GE2E loss does not require an initial stage of example\nselection. With these properties, our model with the new loss function\ndecreases speaker verification EER by more than 10%, while reducing the\ntraining time by 60% at the same time. We also introduce the MultiReader\ntechnique, which allows us to do domain adaptation - training a more accurate\nmodel that supports multiple keywords (i.e. \"OK Google\" and \"Hey Google\") as\nwell as multiple dialects.", "published": "2017-10-28 13:51:51", "link": "http://arxiv.org/abs/1710.10467v5", "categories": ["eess.AS", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Jointly Tracking and Separating Speech Sources Using Multiple Features\n  and the generalized labeled multi-Bernoulli Framework", "abstract": "This paper proposes a novel joint multi-speaker tracking-and-separation\nmethod based on the generalized labeled multi-Bernoulli (GLMB) multi-target\ntracking filter, using sound mixtures recorded by microphones. Standard\nmulti-speaker tracking algorithms usually only track speaker locations, and\nambiguity occurs when speakers are spatially close. The proposed multi-feature\nGLMB tracking filter treats the set of vectors of associated speaker features\n(location, pitch and sound) as the multi-target multi-feature observation,\ncharacterizes transitioning features with corresponding transition models and\noverall likelihood function, thus jointly tracks and separates each\nmulti-feature speaker, and addresses the spatial ambiguity problem. Numerical\nevaluation verifies that the proposed method can correctly track locations of\nmultiple speakers and meanwhile separate speech signals.", "published": "2017-10-28 09:28:56", "link": "http://arxiv.org/abs/1710.10432v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Investigation of Frame Alignments for GMM-based Digit-prompted Speaker\n  Verification", "abstract": "Frame alignments can be computed by different methods in GMM-based speaker\nverification. By incorporating a phonetic Gaussian mixture model (PGMM), we are\nable to compare the performance using alignments extracted from the deep neural\nnetworks (DNN) and the conventional hidden Markov model (HMM) in digit-prompted\nspeaker verification. Based on the different characteristics of these two\nalignments, we present a novel content verification method to improve the\nsystem security without much computational overhead. Our experiments on the\nRSR2015 Part-3 digit-prompted task show that, the DNN based alignment performs\non par with the HMM alignment. The results also demonstrate the effectiveness\nof the proposed Kullback-Leibler (KL) divergence based scoring to reject speech\nwith incorrect pass-phrases.", "published": "2017-10-28 09:46:03", "link": "http://arxiv.org/abs/1710.10436v4", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speaker Diarization with LSTM", "abstract": "For many years, i-vector based audio embedding techniques were the dominant\napproach for speaker verification and speaker diarization applications.\nHowever, mirroring the rise of deep learning in various domains, neural network\nbased audio embeddings, also known as d-vectors, have consistently demonstrated\nsuperior speaker verification performance. In this paper, we build on the\nsuccess of d-vector based speaker verification systems to develop a new\nd-vector based approach to speaker diarization. Specifically, we combine\nLSTM-based d-vector audio embeddings with recent work in non-parametric\nclustering to obtain a state-of-the-art speaker diarization system. Our system\nis evaluated on three standard public datasets, suggesting that d-vector based\ndiarization systems offer significant advantages over traditional i-vector\nbased systems. We achieved a 12.0% diarization error rate on NIST SRE 2000\nCALLHOME, while our model is trained with out-of-domain data from voice search\nlogs.", "published": "2017-10-28 13:59:17", "link": "http://arxiv.org/abs/1710.10468v7", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Attention-Based Models for Text-Dependent Speaker Verification", "abstract": "Attention-based models have recently shown great performance on a range of\ntasks, such as speech recognition, machine translation, and image captioning\ndue to their ability to summarize relevant information that expands through the\nentire length of an input sequence. In this paper, we analyze the usage of\nattention mechanisms to the problem of sequence summarization in our end-to-end\ntext-dependent speaker recognition system. We explore different topologies and\ntheir variants of the attention layer, and compare different pooling methods on\nthe attention weights. Ultimately, we show that attention-based models can\nimproves the Equal Error Rate (EER) of our speaker verification system by\nrelatively 14% compared to our non-attention LSTM baseline model.", "published": "2017-10-28 14:12:29", "link": "http://arxiv.org/abs/1710.10470v3", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Multi-Resolution Fully Convolutional Neural Networks for Monaural Audio\n  Source Separation", "abstract": "In deep neural networks with convolutional layers, each layer typically has\nfixed-size/single-resolution receptive field (RF). Convolutional layers with a\nlarge RF capture global information from the input features, while layers with\nsmall RF size capture local details with high resolution from the input\nfeatures. In this work, we introduce novel deep multi-resolution fully\nconvolutional neural networks (MR-FCNN), where each layer has different RF\nsizes to extract multi-resolution features that capture the global and local\ndetails information from its input features. The proposed MR-FCNN is applied to\nseparate a target audio source from a mixture of many audio sources.\nExperimental results show that using MR-FCNN improves the performance compared\nto feedforward deep neural networks (DNNs) and single resolution deep fully\nconvolutional neural networks (FCNNs) on the audio source separation problem.", "published": "2017-10-28 22:12:08", "link": "http://arxiv.org/abs/1710.11473v1", "categories": ["cs.SD", "cs.CV", "cs.LG", "eess.AS", "68T01", "H.5.5; I.5; I.2.6; I.4.3; I.4; I.2"], "primary_category": "cs.SD"}
{"title": "Sample-level CNN Architectures for Music Auto-tagging Using Raw\n  Waveforms", "abstract": "Recent work has shown that the end-to-end approach using convolutional neural\nnetwork (CNN) is effective in various types of machine learning tasks. For\naudio signals, the approach takes raw waveforms as input using an 1-D\nconvolution layer. In this paper, we improve the 1-D CNN architecture for music\nauto-tagging by adopting building blocks from state-of-the-art image\nclassification models, ResNets and SENets, and adding multi-level feature\naggregation to it. We compare different combinations of the modules in building\nCNN architectures. The results show that they achieve significant improvements\nover previous state-of-the-art models on the MagnaTagATune dataset and\ncomparable results on Million Song Dataset. Furthermore, we analyze and\nvisualize our model to show how the 1-D CNN operates.", "published": "2017-10-28 11:55:50", "link": "http://arxiv.org/abs/1710.10451v2", "categories": ["cs.SD", "cs.LG", "cs.MM", "cs.NE", "eess.AS"], "primary_category": "cs.SD"}
