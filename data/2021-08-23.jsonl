{"title": "A Unified Transformer-based Framework for Duplex Text Normalization", "abstract": "Text normalization (TN) and inverse text normalization (ITN) are essential\npreprocessing and postprocessing steps for text-to-speech synthesis and\nautomatic speech recognition, respectively. Many methods have been proposed for\neither TN or ITN, ranging from weighted finite-state transducers to neural\nnetworks. Despite their impressive performance, these methods aim to tackle\nonly one of the two tasks but not both. As a result, in a complete spoken\ndialog system, two separate models for TN and ITN need to be built. This\nheterogeneity increases the technical complexity of the system, which in turn\nincreases the cost of maintenance in a production setting. Motivated by this\nobservation, we propose a unified framework for building a single neural duplex\nsystem that can simultaneously handle TN and ITN. Combined with a simple but\neffective data augmentation method, our systems achieve state-of-the-art\nresults on the Google TN dataset for English and Russian. They can also reach\nover 95% sentence-level accuracy on an internal English TN dataset without any\nadditional fine-tuning. In addition, we also create a cleaned dataset from the\nSpoken Wikipedia Corpora for German and report the performance of our systems\non the dataset. Overall, experimental results demonstrate the proposed duplex\ntext normalization framework is highly effective and applicable to a range of\ndomains and languages", "published": "2021-08-23 01:55:03", "link": "http://arxiv.org/abs/2108.09889v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analyzing the Granularity and Cost of Annotation in Clinical Sequence\n  Labeling", "abstract": "Well-annotated datasets, as shown in recent top studies, are becoming more\nimportant for researchers than ever before in supervised machine learning (ML).\nHowever, the dataset annotation process and its related human labor costs\nremain overlooked. In this work, we analyze the relationship between the\nannotation granularity and ML performance in sequence labeling, using clinical\nrecords from nursing shift-change handover. We first study a model derived from\ntextual language features alone, without additional information based on\nnursing knowledge. We find that this sequence tagger performs well in most\ncategories under this granularity. Then, we further include the additional\nmanual annotations by a nurse, and find the sequence tagging performance\nremaining nearly the same. Finally, we give a guideline and reference to the\ncommunity arguing it is not necessary and even not recommended to annotate in\ndetailed granularity because of a low Return on Investment. Therefore we\nrecommend emphasizing other features, like textual knowledge, for researchers\nand practitioners as a cost-effective source for increasing the sequence\nlabeling performance.", "published": "2021-08-23 03:48:27", "link": "http://arxiv.org/abs/2108.09913v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Event Extraction by Associating Event Types and Argument Roles", "abstract": "Event extraction (EE), which acquires structural event knowledge from texts,\ncan be divided into two sub-tasks: event type classification and element\nextraction (namely identifying triggers and arguments under different role\npatterns). As different event types always own distinct extraction schemas\n(i.e., role patterns), previous work on EE usually follows an isolated learning\nparadigm, performing element extraction independently for different event\ntypes. It ignores meaningful associations among event types and argument roles,\nleading to relatively poor performance for less frequent types/roles. This\npaper proposes a novel neural association framework for the EE task. Given a\ndocument, it first performs type classification via constructing a\ndocument-level graph to associate sentence nodes of different types, and\nadopting a graph attention network to learn sentence embeddings. Then, element\nextraction is achieved by building a universal schema of argument roles, with a\nparameter inheritance mechanism to enhance role preference for extracted\nelements. As such, our model takes into account type and role associations\nduring EE, enabling implicit information sharing among them. Experimental\nresults show that our approach consistently outperforms most state-of-the-art\nEE methods in both sub-tasks. Particularly, for types/roles with less training\ndata, the performance is superior to the existing methods.", "published": "2021-08-23 10:09:39", "link": "http://arxiv.org/abs/2108.10038v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Legal Search in Case Law and Statute Law", "abstract": "In this work we describe a method to identify document pairwise relevance in\nthe context of a typical legal document collection: limited resources, long\nqueries and long documents. We review the usage of generalized language models,\nincluding supervised and unsupervised learning. We observe how our method,\nwhile using text summaries, overperforms existing baselines based on full text,\nand motivate potential improvement directions for future work.", "published": "2021-08-23 12:51:24", "link": "http://arxiv.org/abs/2108.10127v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Examining Covert Gender Bias: A Case Study in Turkish and English\n  Machine Translation Models", "abstract": "As Machine Translation (MT) has become increasingly more powerful,\naccessible, and widespread, the potential for the perpetuation of bias has\ngrown alongside its advances. While overt indicators of bias have been studied\nin machine translation, we argue that covert biases expose a problem that is\nfurther entrenched. Through the use of the gender-neutral language Turkish and\nthe gendered language English, we examine cases of both overt and covert gender\nbias in MT models. Specifically, we introduce a method to investigate\nasymmetrical gender markings. We also assess bias in the attribution of\npersonhood and examine occupational and personality stereotypes through overt\nbias indicators in MT models. Our work explores a deeper layer of bias in MT\nmodels and demonstrates the continued need for language-specific,\ninterdisciplinary methodology in MT model development.", "published": "2021-08-23 19:25:56", "link": "http://arxiv.org/abs/2108.10379v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sarcasm Detection in Twitter -- Performance Impact while using Data\n  Augmentation: Word Embeddings", "abstract": "Sarcasm is the use of words usually used to either mock or annoy someone, or\nfor humorous purposes. Sarcasm is largely used in social networks and\nmicroblogging websites, where people mock or censure in a way that makes it\ndifficult even for humans to tell if what is said is what is meant. Failure to\nidentify sarcastic utterances in Natural Language Processing applications such\nas sentiment analysis and opinion mining will confuse classification algorithms\nand generate false results. Several studies on sarcasm detection have utilized\ndifferent learning algorithms. However, most of these learning models have\nalways focused on the contents of expression only, leaving the contextual\ninformation in isolation. As a result, they failed to capture the contextual\ninformation in the sarcastic expression. Moreover, some datasets used in\nseveral studies have an unbalanced dataset which impacting the model result. In\nthis paper, we propose a contextual model for sarcasm identification in twitter\nusing RoBERTa, and augmenting the dataset by applying Global Vector\nrepresentation (GloVe) for the construction of word embedding and context\nlearning to generate more data and balancing the dataset. The effectiveness of\nthis technique is tested with various datasets and data augmentation settings.\nIn particular, we achieve performance gain by 3.2% in the iSarcasm dataset when\nusing data augmentation to increase 20% of data labeled as sarcastic, resulting\nF-score of 40.4% compared to 37.2% without data augmentation.", "published": "2021-08-23 04:24:12", "link": "http://arxiv.org/abs/2108.09924v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Semantic-Preserving Adversarial Text Attacks", "abstract": "Deep neural networks (DNNs) are known to be vulnerable to adversarial images,\nwhile their robustness in text classification is rarely studied. Several lines\nof text attack methods have been proposed in the literature, including\ncharacter-level, word-level, and sentence-level attacks. However, it is still a\nchallenge to minimize the number of word changes necessary to induce\nmisclassification, while simultaneously ensuring lexical correctness, syntactic\nsoundness, and semantic similarity. In this paper, we propose a Bigram and\nUnigram based adaptive Semantic Preservation Optimization (BU-SPO) method to\nexamine the vulnerability of deep models. Our method has four major merits.\nFirstly, we propose to attack text documents not only at the unigram word level\nbut also at the bigram level which better keeps semantics and avoids producing\nmeaningless outputs. Secondly, we propose a hybrid method to replace the input\nwords with options among both their synonyms candidates and sememe candidates,\nwhich greatly enriches the potential substitutions compared to only using\nsynonyms. Thirdly, we design an optimization algorithm, i.e., Semantic\nPreservation Optimization (SPO), to determine the priority of word\nreplacements, aiming to reduce the modification cost. Finally, we further\nimprove the SPO with a semantic Filter (named SPOF) to find the adversarial\nexample with the highest semantic similarity. We evaluate the effectiveness of\nour BU-SPO and BU-SPOF on IMDB, AG's News, and Yahoo! Answers text datasets by\nattacking four popular DNNs models. Results show that our methods achieve the\nhighest attack success rates and semantics rates by changing the smallest\nnumber of words compared with existing methods.", "published": "2021-08-23 09:05:18", "link": "http://arxiv.org/abs/2108.10015v2", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "VerbCL: A Dataset of Verbatim Quotes for Highlight Extraction in Case\n  Law", "abstract": "Citing legal opinions is a key part of legal argumentation, an expert task\nthat requires retrieval, extraction and summarization of information from court\ndecisions. The identification of legally salient parts in an opinion for the\npurpose of citation may be seen as a domain-specific formulation of a highlight\nextraction or passage retrieval task. As similar tasks in other domains such as\nweb search show significant attention and improvement, progress in the legal\ndomain is hindered by the lack of resources for training and evaluation.\n  This paper presents a new dataset that consists of the citation graph of\ncourt opinions, which cite previously published court opinions in support of\ntheir arguments. In particular, we focus on the verbatim quotes, i.e., where\nthe text of the original opinion is directly reused.\n  With this approach, we explain the relative importance of different text\nspans of a court opinion by showcasing their usage in citations, and measuring\ntheir contribution to the relations between opinions in the citation graph.\n  We release VerbCL, a large-scale dataset derived from CourtListener and\nintroduce the task of highlight extraction as a single-document summarization\ntask based on the citation graph establishing the first baseline results for\nthis task on the VerbCL dataset.", "published": "2021-08-23 12:41:41", "link": "http://arxiv.org/abs/2108.10120v1", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "Deploying a BERT-based Query-Title Relevance Classifier in a Production\n  System: a View from the Trenches", "abstract": "The Bidirectional Encoder Representations from Transformers (BERT) model has\nbeen radically improving the performance of many Natural Language Processing\n(NLP) tasks such as Text Classification and Named Entity Recognition (NER)\napplications. However, it is challenging to scale BERT for low-latency and\nhigh-throughput industrial use cases due to its enormous size. We successfully\noptimize a Query-Title Relevance (QTR) classifier for deployment via a compact\nmodel, which we name BERT Bidirectional Long Short-Term Memory (BertBiLSTM).\nThe model is capable of inferring an input in at most 0.2ms on CPU. BertBiLSTM\nexceeds the off-the-shelf BERT model's performance in terms of accuracy and\nefficiency for the aforementioned real-world production task. We achieve this\nresult in two phases. First, we create a pre-trained model, called eBERT, which\nis the original BERT architecture trained with our unique item title corpus. We\nthen fine-tune eBERT for the QTR task. Second, we train the BertBiLSTM model to\nmimic the eBERT model's performance through a process called Knowledge\nDistillation (KD) and show the effect of data augmentation to achieve the\nresembling goal. Experimental results show that the proposed model outperforms\nother compact and production-ready models.", "published": "2021-08-23 14:28:23", "link": "http://arxiv.org/abs/2108.10197v1", "categories": ["cs.CL", "stat.CO"], "primary_category": "cs.CL"}
{"title": "Towards Explainable Fact Checking", "abstract": "The past decade has seen a substantial rise in the amount of mis- and\ndisinformation online, from targeted disinformation campaigns to influence\npolitics, to the unintentional spreading of misinformation about public health.\nThis development has spurred research in the area of automatic fact checking,\nfrom approaches to detect check-worthy claims and determining the stance of\ntweets towards claims, to methods to determine the veracity of claims given\nevidence documents. These automatic methods are often content-based, using\nnatural language processing methods, which in turn utilise deep neural networks\nto learn higher-order features from text in order to make predictions. As deep\nneural networks are black-box models, their inner workings cannot be easily\nexplained. At the same time, it is desirable to explain how they arrive at\ncertain decisions, especially if they are to be used for decision making. While\nthis has been known for some time, the issues this raises have been exacerbated\nby models increasing in size, and by EU legislation requiring models to be used\nfor decision making to provide explanations, and, very recently, by legislation\nrequiring online platforms operating in the EU to provide transparent reporting\non their services. Despite this, current solutions for explainability are still\nlacking in the area of fact checking. This thesis presents my research on\nautomatic fact checking, including claim check-worthiness detection, stance\ndetection and veracity prediction. Its contributions go beyond fact checking,\nwith the thesis proposing more general machine learning solutions for natural\nlanguage processing in the area of learning with limited labelled data.\nFinally, the thesis presents some first solutions for explainable fact\nchecking.", "published": "2021-08-23 16:22:50", "link": "http://arxiv.org/abs/2108.10274v2", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Recurrent multiple shared layers in Depth for Neural Machine Translation", "abstract": "Learning deeper models is usually a simple and effective approach to improve\nmodel performance, but deeper models have larger model parameters and are more\ndifficult to train. To get a deeper model, simply stacking more layers of the\nmodel seems to work well, but previous works have claimed that it cannot\nbenefit the model. We propose to train a deeper model with recurrent mechanism,\nwhich loops the encoder and decoder blocks of Transformer in the depth\ndirection. To address the increasing of model parameters, we choose to share\nparameters in different recursive moments. We conduct our experiments on WMT16\nEnglish-to-German and WMT14 English-to-France translation tasks, our model\noutperforms the shallow Transformer-Base/Big baseline by 0.35, 1.45 BLEU\npoints, which is 27.23% of Transformer-Big model parameters. Compared to the\ndeep Transformer(20-layer encoder, 6-layer decoder), our model has similar\nmodel performance and infer speed, but our model parameters are 54.72% of the\nformer.", "published": "2021-08-23 21:21:45", "link": "http://arxiv.org/abs/2108.10417v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Regularizing Transformers With Deep Probabilistic Layers", "abstract": "Language models (LM) have grown with non-stop in the last decade, from\nsequence-to-sequence architectures to the state-of-the-art and utter\nattention-based Transformers. In this work, we demonstrate how the inclusion of\ndeep generative models within BERT can bring more versatile models, able to\nimpute missing/noisy words with richer text or even improve BLEU score. More\nprecisely, we use a Gaussian Mixture Variational Autoencoder (GMVAE) as a\nregularizer layer and prove its effectiveness not only in Transformers but also\nin the most relevant encoder-decoder based LM, seq2seq with and without\nattention.", "published": "2021-08-23 10:17:02", "link": "http://arxiv.org/abs/2108.10764v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Fluent: An AI Augmented Writing Tool for People who Stutter", "abstract": "Stuttering is a speech disorder which impacts the personal and professional\nlives of millions of people worldwide. To save themselves from stigma and\ndiscrimination, people who stutter (PWS) may adopt different strategies to\nconceal their stuttering. One of the common strategies is word substitution\nwhere an individual avoids saying a word they might stutter on and use an\nalternative instead. This process itself can cause stress and add more burden.\nIn this work, we present Fluent, an AI augmented writing tool which assists PWS\nin writing scripts which they can speak more fluently. Fluent embodies a novel\nactive learning based method of identifying words an individual might struggle\npronouncing. Such words are highlighted in the interface. On hovering over any\nsuch word, Fluent presents a set of alternative words which have similar\nmeaning but are easier to speak. The user is free to accept or ignore these\nsuggestions. Based on such user interaction (feedback), Fluent continuously\nevolves its classifier to better suit the personalized needs of each user. We\nevaluated our tool by measuring its ability to identify difficult words for 10\nsimulated users. We found that our tool can identify difficult words with a\nmean accuracy of over 80% in under 20 interactions and it keeps improving with\nmore feedback. Our tool can be beneficial for certain important life situations\nlike giving a talk, presentation, etc. The source code for this tool has been\nmade publicly accessible at github.com/bhavyaghai/Fluent.", "published": "2021-08-23 04:08:27", "link": "http://arxiv.org/abs/2108.09918v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Modeling chronic pain experiences from online reports using the Reddit\n  Reports of Chronic Pain dataset", "abstract": "Objective: Reveal and quantify qualities of reported experiences of chronic\npain on social media, from multiple pathological backgrounds, by means of the\nnovel Reddit Reports of Chronic Pain (RRCP) dataset, using Natural Language\nProcessing techniques. Materials and Methods: Define and validate the RRCP\ndataset for a set of subreddits related to chronic pain. Identify the main\nconcerns discussed in each subreddit. Model each subreddit according to their\nmain concerns. Compare subreddit models. Results: The RRCP dataset comprises\n86,537 Reddit submissions from 12 subreddits related to chronic pain (each\nrelated to one pathological background). Each RRCP subreddit has various main\nconcerns. Some of these concerns are shared between multiple subreddits (e.g.,\nthe subreddit Sciatica semantically entails the subreddit backpain in their\nvarious concerns, but not the other way around), whilst some concerns are\nexclusive to specific subreddits (e.g., Interstitialcystitis and\nCrohnsDisease). Discussion: These results suggest that the reported experience\nof chronic pain, from multiple pathologies (i.e., subreddits), has concerns\nrelevant to all, and concerns exclusive to certain pathologies. Our analysis\ndetails each of these concerns and their similarity relations. Conclusion:\nAlthough limited by intrinsic qualities of the Reddit platform, to the best of\nour knowledge, this is the first research work attempting to model the\nlinguistic expression of various chronic pain-inducing pathologies and\ncomparing these models to identify and quantify the similarities and\ndifferences between the corresponding emergent chronic pain experiences.", "published": "2021-08-23 14:53:03", "link": "http://arxiv.org/abs/2108.10218v4", "categories": ["cs.CL", "cs.IR", "cs.SI", "q-bio.QM", "I.2.7; I.5.3; I.5.4; J.3; J.4"], "primary_category": "cs.CL"}
{"title": "End-to-End Open Vocabulary Keyword Search", "abstract": "Recently, neural approaches to spoken content retrieval have become popular.\nHowever, they tend to be restricted in their vocabulary or in their ability to\ndeal with imbalanced test settings. These restrictions limit their\napplicability in keyword search, where the set of queries is not known\nbeforehand, and where the system should return not just whether an utterance\ncontains a query but the exact location of any such occurrences. In this work,\nwe propose a model directly optimized for keyword search. The model takes a\nquery and an utterance as input and returns a sequence of probabilities for\neach frame of the utterance of the query having occurred in that frame.\nExperiments show that the proposed model not only outperforms similar\nend-to-end models on a task where the ratio of positive and negative trials is\nartificially balanced, but it is also able to deal with the far more\nchallenging task of keyword search with its inherent imbalance. Furthermore,\nusing our system to rescore the outputs an LVCSR-based keyword search system\nleads to significant improvements on the latter.", "published": "2021-08-23 18:34:53", "link": "http://arxiv.org/abs/2108.10357v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "One TTS Alignment To Rule Them All", "abstract": "Speech-to-text alignment is a critical component of neural textto-speech\n(TTS) models. Autoregressive TTS models typically use an attention mechanism to\nlearn these alignments on-line. However, these alignments tend to be brittle\nand often fail to generalize to long utterances and out-of-domain text, leading\nto missing or repeating words. Most non-autoregressive endto-end TTS models\nrely on durations extracted from external sources. In this paper we leverage\nthe alignment mechanism proposed in RAD-TTS as a generic alignment learning\nframework, easily applicable to a variety of neural TTS models. The framework\ncombines forward-sum algorithm, the Viterbi algorithm, and a simple and\nefficient static prior. In our experiments, the alignment learning framework\nimproves all tested TTS architectures, both autoregressive (Flowtron, Tacotron\n2) and non-autoregressive (FastPitch, FastSpeech 2, RAD-TTS). Specifically, it\nimproves alignment convergence speed of existing attention-based mechanisms,\nsimplifies the training pipeline, and makes the models more robust to errors on\nlong utterances. Most importantly, the framework improves the perceived speech\nsynthesis quality, as judged by human evaluators.", "published": "2021-08-23 23:45:48", "link": "http://arxiv.org/abs/2108.10447v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ComSum: Commit Messages Summarization and Meaning Preservation", "abstract": "We present ComSum, a data set of 7 million commit messages for text\nsummarization. When documenting commits, software code changes, both a message\nand its summary are posted. We gather and filter those to curate developers'\nwork summarization data set. Along with its growing size, practicality and\nchallenging language domain, the data set benefits from the living field of\nempirical software engineering. As commits follow a typology, we propose to not\nonly evaluate outputs by Rouge, but by their meaning preservation.", "published": "2021-08-23 07:43:48", "link": "http://arxiv.org/abs/2108.10763v1", "categories": ["cs.CL", "cs.LG", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Subject Envelope based Multitype Reconstruction Algorithm of Speech\n  Samples of Parkinson's Disease", "abstract": "The risk of Parkinson's disease (PD) is extremely serious, and PD speech\nrecognition is an effective method of diagnosis nowadays. However, due to the\ninfluence of the disease stage, corpus, and other factors on data collection,\nthe ability of every samples within one subject to reflect the status of PD\nvary. No samples are useless totally, and not samples are 100% perfect. This\ncharacteristic means that it is not suitable just to remove some samples or\nkeep some samples. It is necessary to consider the sample transformation for\nobtaining high quality new samples. Unfortunately, existing PD speech\nrecognition methods focus mainly on feature learning and classifier design\nrather than sample learning, and few methods consider the sample\ntransformation. To solve the problem above, a PD speech sample transformation\nalgorithm based on multitype reconstruction operators is proposed in this\npaper. The algorithm is divided into four major steps. Three types of\nreconstruction operators are designed in the algorithm: types A, B and C.\nConcerning the type A operator, the original dataset is directly reconstructed\nby designing a linear transformation to obtain the first dataset. The type B\noperator is designed for clustering and linear transformation of the dataset to\nobtain the second new dataset. The third operator, namely, the type C operator,\nreconstructs the dataset by clustering and convolution to obtain the third\ndataset. Finally, the base classifier is trained based on the three new\ndatasets, and then the classification results are fused by decision weighting.\nIn the experimental section, two representative PD speech datasets are used for\nverification. The results show that the proposed algorithm is effective.\nCompared with other algorithms, the proposed algorithm achieves apparent\nimprovements in terms of classification accuracy.", "published": "2021-08-23 04:20:51", "link": "http://arxiv.org/abs/2108.09922v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Automatic Speech Recognition And Limited Vocabulary: A Survey", "abstract": "Automatic Speech Recognition (ASR) is an active field of research due to its\nlarge number of applications and the proliferation of interfaces or computing\ndevices that can support speech processing. However, the bulk of applications\nare based on well-resourced languages that overshadow under-resourced ones.\nYet, ASR represents an undeniable means to promote such languages, especially\nwhen designing human-to-human or human-to-machine systems involving illiterate\npeople. An approach to design an ASR system targeting under-resourced languages\nis to start with a limited vocabulary. ASR using a limited vocabulary is a\nsubset of the speech recognition problem that focuses on the recognition of a\nsmall number of words or sentences. This paper aims to provide a comprehensive\nview of mechanisms behind ASR systems as well as techniques, tools, projects,\nrecent contributions, and possible future directions in ASR using a limited\nvocabulary. This work consequently provides a way forward when designing an ASR\nsystem using limited vocabulary. Although an emphasis is put on limited\nvocabulary, most of the tools and techniques reported in this survey can be\napplied to ASR systems in general.", "published": "2021-08-23 15:51:41", "link": "http://arxiv.org/abs/2108.10254v2", "categories": ["cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.AI"}
{"title": "Learning Sparse Analytic Filters for Piano Transcription", "abstract": "In recent years, filterbank learning has become an increasingly popular\nstrategy for various audio-related machine learning tasks. This is partly due\nto its ability to discover task-specific audio characteristics which can be\nleveraged in downstream processing. It is also a natural extension of the\nnearly ubiquitous deep learning methods employed to tackle a diverse array of\naudio applications. In this work, several variations of a frontend filterbank\nlearning module are investigated for piano transcription, a challenging\nlow-level music information retrieval task. We build upon a standard piano\ntranscription model, modifying only the feature extraction stage. The\nfilterbank module is designed such that its complex filters are unconstrained\n1D convolutional kernels with long receptive fields. Additional variations\nemploy the Hilbert transform to render the filters intrinsically analytic and\napply variational dropout to promote filterbank sparsity. Transcription results\nare compared across all experiments, and we offer visualization and analysis of\nthe filterbanks.", "published": "2021-08-23 19:41:11", "link": "http://arxiv.org/abs/2108.10382v3", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Differential Music: Automated Music Generation Using LSTM Networks with\n  Representation Based on Melodic and Harmonic Intervals", "abstract": "This paper presents a generative AI model for automated music composition\nwith LSTM networks that takes a novel approach at encoding musical information\nwhich is based on movement in music rather than absolute pitch. Melodies are\nencoded as a series of intervals rather than a series of pitches, and chords\nare encoded as the set of intervals that each chord note makes with the melody\nat each timestep. Experimental results show promise as they sound musical and\ntonal. There are also weaknesses to this method, mainly excessive modulations\nin the compositions, but that is expected from the nature of the encoding. This\nissue is discussed later in the paper and is a potential topic for future work.", "published": "2021-08-23 23:51:08", "link": "http://arxiv.org/abs/2108.10449v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Determining the origin of impulsive noise events using paired wireless\n  sound sensors", "abstract": "This work investigates how to identify the source of impulsive noise events\nusing a pair of wireless noise sensors. One sensor is placed at a known noise\nsource, and another sensor is placed at the noise receiver. Machine learning\nmodels receive data from the two sensors and estimate whether a given noise\nevent originates from the known noise source or another source. To avoid\nprivacy issues, the approach uses on-edge preprocessing that converts the sound\ninto privacy compatible spectrograms. The system was evaluated at a shooting\nrange and explosives training facility, using data collected during noise\nemission testing. The combination of convolutional neural networks with\ncross-correlation achieved the best results. We created multiple alternative\nmodels using different spectrogram representations. The best model detected\n70.8\\% of the impulsive noise events and correctly predicted 90.3\\% of the\nnoise events in the optimal trade-off between recall and precision.", "published": "2021-08-23 14:19:42", "link": "http://arxiv.org/abs/2108.11758v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
