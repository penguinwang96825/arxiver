{"title": "Aspect and Opinion Terms Extraction Using Double Embeddings and\n  Attention Mechanism for Indonesian Hotel Reviews", "abstract": "Aspect and opinion terms extraction from review texts is one of the key tasks\nin aspect-based sentiment analysis. In order to extract aspect and opinion\nterms for Indonesian hotel reviews, we adapt double embeddings feature and\nattention mechanism that outperform the best system at SemEval 2015 and 2016.\nWe conduct experiments using 4000 reviews to find the best configuration and\nshow the influences of double embeddings and attention mechanism toward model\nperformance. Using 1000 reviews for evaluation, we achieved F1-measure of 0.914\nand 0.90 for aspect and opinion terms extraction in token and entity (term)\nlevel respectively.", "published": "2019-08-14 00:38:13", "link": "http://arxiv.org/abs/1908.04899v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reinforcement Learning Based Graph-to-Sequence Model for Natural\n  Question Generation", "abstract": "Natural question generation (QG) aims to generate questions from a passage\nand an answer. Previous works on QG either (i) ignore the rich structure\ninformation hidden in text, (ii) solely rely on cross-entropy loss that leads\nto issues like exposure bias and inconsistency between train/test measurement,\nor (iii) fail to fully exploit the answer information. To address these\nlimitations, in this paper, we propose a reinforcement learning (RL) based\ngraph-to-sequence (Graph2Seq) model for QG. Our model consists of a Graph2Seq\ngenerator with a novel Bidirectional Gated Graph Neural Network based encoder\nto embed the passage, and a hybrid evaluator with a mixed objective combining\nboth cross-entropy and RL losses to ensure the generation of syntactically and\nsemantically valid text. We also introduce an effective Deep Alignment Network\nfor incorporating the answer information into the passage at both the word and\ncontextual levels. Our model is end-to-end trainable and achieves new\nstate-of-the-art scores, outperforming existing methods by a significant margin\non the standard SQuAD benchmark.", "published": "2019-08-14 03:40:04", "link": "http://arxiv.org/abs/1908.04942v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Establishing Strong Baselines for the New Decade: Sequence Tagging,\n  Syntactic and Semantic Parsing with BERT", "abstract": "This paper presents new state-of-the-art models for three tasks,\npart-of-speech tagging, syntactic parsing, and semantic parsing, using the\ncutting-edge contextualized embedding framework known as BERT. For each task,\nwe first replicate and simplify the current state-of-the-art approach to\nenhance its model efficiency. We then evaluate our simplified approaches on\nthose three tasks using token embeddings generated by BERT. 12 datasets in both\nEnglish and Chinese are used for our experiments. The BERT models outperform\nthe previously best-performing models by 2.5% on average (7.5% for the most\nsignificant case). Moreover, an in-depth analysis on the impact of BERT\nembeddings is provided using self-attention, which helps understanding in this\nrich yet representation. All models and source codes are available in public so\nthat researchers can improve upon and utilize them to establish strong\nbaselines for the next decade.", "published": "2019-08-14 03:45:15", "link": "http://arxiv.org/abs/1908.04943v4", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "FlexNER: A Flexible LSTM-CNN Stack Framework for Named Entity\n  Recognition", "abstract": "Named entity recognition (NER) is a foundational technology for information\nextraction. This paper presents a flexible NER framework compatible with\ndifferent languages and domains. Inspired by the idea of distant supervision\n(DS), this paper enhances the representation by increasing the entity-context\ndiversity without relying on external resources. We choose different layer\nstacks and sub-network combinations to construct the bilateral networks. This\nstrategy can generally improve model performance on different datasets. We\nconduct experiments on five languages, such as English, German, Spanish, Dutch\nand Chinese, and biomedical fields, such as identifying the chemicals and\ngene/protein terms from scientific works. Experimental results demonstrate the\ngood performance of this framework.", "published": "2019-08-14 08:06:14", "link": "http://arxiv.org/abs/1908.05009v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "X-WikiRE: A Large, Multilingual Resource for Relation Extraction as\n  Machine Comprehension", "abstract": "Although the vast majority of knowledge bases KBs are heavily biased towards\nEnglish, Wikipedias do cover very different topics in different languages.\nExploiting this, we introduce a new multilingual dataset (X-WikiRE), framing\nrelation extraction as a multilingual machine reading problem. We show that by\nleveraging this resource it is possible to robustly transfer models\ncross-lingually and that multilingual support significantly improves\n(zero-shot) relation extraction, enabling the population of low-resourced KBs\nfrom their well-populated counterparts.", "published": "2019-08-14 13:15:24", "link": "http://arxiv.org/abs/1908.05111v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SG-Net: Syntax-Guided Machine Reading Comprehension", "abstract": "For machine reading comprehension, the capacity of effectively modeling the\nlinguistic knowledge from the detail-riddled and lengthy passages and getting\nride of the noises is essential to improve its performance. Traditional\nattentive models attend to all words without explicit constraint, which results\nin inaccurate concentration on some dispensable words. In this work, we propose\nusing syntax to guide the text modeling by incorporating explicit syntactic\nconstraints into attention mechanism for better linguistically motivated word\nrepresentations. In detail, for self-attention network (SAN) sponsored\nTransformer-based encoder, we introduce syntactic dependency of interest (SDOI)\ndesign into the SAN to form an SDOI-SAN with syntax-guided self-attention.\nSyntax-guided network (SG-Net) is then composed of this extra SDOI-SAN and the\nSAN from the original Transformer encoder through a dual contextual\narchitecture for better linguistics inspired representation. To verify its\neffectiveness, the proposed SG-Net is applied to typical pre-trained language\nmodel BERT which is right based on a Transformer encoder. Extensive experiments\non popular benchmarks including SQuAD 2.0 and RACE show that the proposed\nSG-Net design helps achieve substantial performance improvement over strong\nbaselines.", "published": "2019-08-14 14:28:07", "link": "http://arxiv.org/abs/1908.05147v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On The Evaluation of Machine Translation Systems Trained With\n  Back-Translation", "abstract": "Back-translation is a widely used data augmentation technique which leverages\ntarget monolingual data. However, its effectiveness has been challenged since\nautomatic metrics such as BLEU only show significant improvements for test\nexamples where the source itself is a translation, or translationese. This is\nbelieved to be due to translationese inputs better matching the back-translated\ntraining data. In this work, we show that this conjecture is not empirically\nsupported and that back-translation improves translation quality of both\nnaturally occurring text as well as translationese according to professional\nhuman translators. We provide empirical evidence to support the view that\nback-translation is preferred by humans because it produces more fluent\noutputs. BLEU cannot capture human preferences because references are\ntranslationese when source sentences are natural text. We recommend\ncomplementing BLEU with a language model score to measure fluency.", "published": "2019-08-14 16:24:56", "link": "http://arxiv.org/abs/1908.05204v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The lexical and grammatical sources of neg-raising inferences", "abstract": "We investigate neg(ation)-raising inferences, wherein negation on a predicate\ncan be interpreted as though in that predicate's subordinate clause. To do\nthis, we collect a large-scale dataset of neg-raising judgments for effectively\nall English clause-embedding verbs and develop a model to jointly induce the\nsemantic types of verbs and their subordinate clauses and the relationship of\nthese types to neg-raising inferences. We find that some neg-raising inferences\nare attributable to properties of particular predicates, while others are\nattributable to subordinate clause structure.", "published": "2019-08-14 17:32:40", "link": "http://arxiv.org/abs/1908.05253v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Debiasing Fact Verification Models", "abstract": "Fact verification requires validating a claim in the context of evidence. We\nshow, however, that in the popular FEVER dataset this might not necessarily be\nthe case. Claim-only classifiers perform competitively with top evidence-aware\nmodels. In this paper, we investigate the cause of this phenomenon, identifying\nstrong cues for predicting labels solely based on the claim, without\nconsidering any evidence. We create an evaluation set that avoids those\nidiosyncrasies. The performance of FEVER-trained models significantly drops\nwhen evaluated on this test set. Therefore, we introduce a regularization\nmethod which alleviates the effect of bias in the training data, obtaining\nimprovements on the newly created test set. This work is a step towards a more\nsound evaluation of reasoning capabilities in fact verification models.", "published": "2019-08-14 17:47:02", "link": "http://arxiv.org/abs/1908.05267v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Debiasing Personal Identities in Toxicity Classification", "abstract": "As Machine Learning models continue to be relied upon for making automated\ndecisions, the issue of model bias becomes more and more prevalent. In this\npaper, we approach training a text classifica-tion model and optimize on bias\nminimization by measuring not only the models performance on our dataset as a\nwhole, but also how it performs across different subgroups. This requires\nmeasuring per-formance independently for different demographic subgroups and\nmeasuring bias by comparing them to results from the rest of our data. We show\nhow unintended bias can be detected using these metrics and how removing bias\nfrom a dataset completely can result in worse results.", "published": "2019-08-14 17:37:31", "link": "http://arxiv.org/abs/1908.05757v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HyperKG: Hyperbolic Knowledge Graph Embeddings for Knowledge Base\n  Completion", "abstract": "Learning embeddings of entities and relations existing in knowledge bases\nallows the discovery of hidden patterns in data. In this work, we examine the\ngeometrical space's contribution to the task of knowledge base completion. We\nfocus on the family of translational models, whose performance has been\nlagging, and propose a model, dubbed HyperKG, which exploits the hyperbolic\nspace in order to better reflect the topological properties of knowledge bases.\nWe investigate the type of regularities that our model can capture and we show\nthat it is a prominent candidate for effectively representing a subset of\nDatalog rules. We empirically show, using a variety of link prediction\ndatasets, that hyperbolic space allows to narrow down significantly the\nperformance gap between translational and bilinear models.", "published": "2019-08-14 00:24:54", "link": "http://arxiv.org/abs/1908.04895v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Cascade Sequence-to-Sequence Model for Chinese Mandarin Lip Reading", "abstract": "Lip reading aims at decoding texts from the movement of a speaker's mouth. In\nrecent years, lip reading methods have made great progress for English, at both\nword-level and sentence-level. Unlike English, however, Chinese Mandarin is a\ntone-based language and relies on pitches to distinguish lexical or grammatical\nmeaning, which significantly increases the ambiguity for the lip reading task.\nIn this paper, we propose a Cascade Sequence-to-Sequence Model for Chinese\nMandarin (CSSMCM) lip reading, which explicitly models tones when predicting\nsentence. Tones are modeled based on visual information and syntactic\nstructure, and are used to predict sentence along with visual information and\nsyntactic structure. In order to evaluate CSSMCM, a dataset called CMLR\n(Chinese Mandarin Lip Reading) is collected and released, consisting of over\n100,000 natural sentences from China Network Television website. When trained\non CMLR dataset, the proposed CSSMCM surpasses the performance of\nstate-of-the-art lip reading frameworks, which confirms the effectiveness of\nexplicit modeling of tones for Chinese Mandarin lip reading.", "published": "2019-08-14 01:49:32", "link": "http://arxiv.org/abs/1908.04917v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Towards Diverse and Accurate Image Captions via Reinforcing\n  Determinantal Point Process", "abstract": "Although significant progress has been made in the field of automatic image\ncaptioning, it is still a challenging task. Previous works normally pay much\nattention to improving the quality of the generated captions but ignore the\ndiversity of captions. In this paper, we combine determinantal point process\n(DPP) and reinforcement learning (RL) and propose a novel reinforcing DPP\n(R-DPP) approach to generate a set of captions with high quality and diversity\nfor an image. We show that R-DPP performs better on accuracy and diversity than\nusing noise as a control signal (GANs, VAEs). Moreover, R-DPP is able to\npreserve the modes of the learned distribution. Hence, beam search algorithm\ncan be applied to generate a single accurate caption, which performs better\nthan other RL-based models.", "published": "2019-08-14 01:51:49", "link": "http://arxiv.org/abs/1908.04919v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Reasoning-Driven Question-Answering for Natural Language Understanding", "abstract": "Natural language understanding (NLU) of text is a fundamental challenge in\nAI, and it has received significant attention throughout the history of NLP\nresearch. This primary goal has been studied under different tasks, such as\nQuestion Answering (QA) and Textual Entailment (TE). In this thesis, we\ninvestigate the NLU problem through the QA task and focus on the aspects that\nmake it a challenge for the current state-of-the-art technology. This thesis is\norganized into three main parts:\n  In the first part, we explore multiple formalisms to improve existing machine\ncomprehension systems. We propose a formulation for abductive reasoning in\nnatural language and show its effectiveness, especially in domains with limited\ntraining data. Additionally, to help reasoning systems cope with irrelevant or\nredundant information, we create a supervised approach to learn and detect the\nessential terms in questions.\n  In the second part, we propose two new challenge datasets. In particular, we\ncreate two datasets of natural language questions where (i) the first one\nrequires reasoning over multiple sentences; (ii) the second one requires\ntemporal common sense reasoning. We hope that the two proposed datasets will\nmotivate the field to address more complex problems.\n  In the final part, we present the first formal framework for multi-step\nreasoning algorithms, in the presence of a few important properties of language\nuse, such as incompleteness, ambiguity, etc. We apply this framework to prove\nfundamental limitations for reasoning algorithms. These theoretical results\nprovide extra intuition into the existing empirical evidence in the field.", "published": "2019-08-14 02:07:02", "link": "http://arxiv.org/abs/1908.04926v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Reactive Multi-Stage Feature Fusion for Multimodal Dialogue Modeling", "abstract": "Visual question answering and visual dialogue tasks have been increasingly\nstudied in the multimodal field towards more practical real-world scenarios. A\nmore challenging task, audio visual scene-aware dialogue (AVSD), is proposed to\nfurther advance the technologies that connect audio, vision, and language,\nwhich introduces temporal video information and dialogue interactions between a\nquestioner and an answerer. This paper proposes an intuitive mechanism that\nfuses features and attention in multiple stages in order to well integrate\nmultimodal features, and the results demonstrate its capability in the\nexperiments. Also, we apply several state-of-the-art models in other tasks to\nthe AVSD task, and further analyze their generalization across different tasks.", "published": "2019-08-14 10:58:14", "link": "http://arxiv.org/abs/1908.05067v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Towards Optimisation of Collaborative Question Answering over Knowledge\n  Graphs", "abstract": "Collaborative Question Answering (CQA) frameworks for knowledge graphs aim at\nintegrating existing question answering (QA) components for implementing\nsequences of QA tasks (i.e. QA pipelines). The research community has paid\nsubstantial attention to CQAs since they support reusability and scalability of\nthe available components in addition to the flexibility of pipelines. CQA\nframeworks attempt to build such pipelines automatically by solving two\noptimisation problems: 1) local collective performance of QA components per QA\ntask and 2) global performance of QA pipelines. In spite offering several\nadvantages over monolithic QA systems, the effectiveness and efficiency of CQA\nframeworks in answering questions is limited. In this paper, we tackle the\nproblem of local optimisation of CQA frameworks and propose a three fold\napproach, which applies feature selection techniques with supervised machine\nlearning approaches in order to identify the best performing components\nefficiently. We have empirically evaluated our approach over existing\nbenchmarks and compared to existing automatic CQA frameworks. The observed\nresults provide evidence that our approach answers a higher number of questions\nthan the state of the art while reducing: i) the number of used features by 50%\nand ii) the number of components used by 76%.", "published": "2019-08-14 12:42:48", "link": "http://arxiv.org/abs/1908.05098v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "FlowDelta: Modeling Flow Information Gain in Reasoning for\n  Conversational Machine Comprehension", "abstract": "Conversational machine comprehension requires deep understanding of the\ndialogue flow, and the prior work proposed FlowQA to implicitly model the\ncontext representations in reasoning for better understanding. This paper\nproposes to explicitly model the information gain through dialogue reasoning in\norder to allow the model to focus on more informative cues. The proposed model\nachieves state-of-the-art performance in a conversational QA dataset QuAC and\nsequential instruction understanding dataset SCONE, which shows the\neffectiveness of the proposed mechanism and demonstrates its capability of\ngeneralization to different QA models and tasks.", "published": "2019-08-14 13:34:40", "link": "http://arxiv.org/abs/1908.05117v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MemeFaceGenerator: Adversarial Synthesis of Chinese Meme-face from\n  Natural Sentences", "abstract": "Chinese meme-face is a special kind of internet subculture widely spread in\nChinese Social Community Networks. It usually consists of a template image\nmodified by some amusing details and a text caption. In this paper, we present\nMemeFaceGenerator, a Generative Adversarial Network with the attention module\nand template information as supplementary signals, to automatically generate\nmeme-faces from text inputs. We also develop a web service as system\ndemonstration of meme-face synthesis. MemeFaceGenerator has been shown to be\ncapable of generating high-quality meme-faces from random text inputs.", "published": "2019-08-14 14:15:24", "link": "http://arxiv.org/abs/1908.05138v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Raw-to-End Name Entity Recognition in Social Media", "abstract": "Taking word sequences as the input, typical named entity recognition (NER)\nmodels neglect errors from pre-processing (e.g., tokenization). However, these\nerrors can influence the model performance greatly, especially for noisy texts\nlike tweets. Here, we introduce Neural-Char-CRF, a raw-to-end framework that is\nmore robust to pre-processing errors. It takes raw character sequences as\ninputs and makes end-to-end predictions. Word embedding and contextualized\nrepresentation models are further tailored to capture textual signals for each\ncharacter instead of each word. Our model neither requires the conversion from\ncharacter sequences to word sequences, nor assumes tokenizer can correctly\ndetect all word boundaries. Moreover, we observe our model performance remains\nunchanged after replacing tokenization with string matching, which demonstrates\nits potential to be tokenization-free. Extensive experimental results on two\npublic datasets demonstrate the superiority of our proposed method over the\nstate of the art. The implementations and datasets are made available at:\nhttps://github.com/LiyuanLucasLiu/Raw-to-End.", "published": "2019-08-14 20:50:14", "link": "http://arxiv.org/abs/1908.05344v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On-Device Text Representations Robust To Misspellings via Projections", "abstract": "Recently, there has been a strong interest in developing natural language\napplications that live on personal devices such as mobile phones, watches and\nIoT with the objective to preserve user privacy and have low memory. Advances\nin Locality-Sensitive Hashing (LSH)-based projection networks have demonstrated\nstate-of-the-art performance in various classification tasks without explicit\nword (or word-piece) embedding lookup tables by computing on-the-fly text\nrepresentations. In this paper, we show that the projection based neural\nclassifiers are inherently robust to misspellings and perturbations of the\ninput text. We empirically demonstrate that the LSH projection based\nclassifiers are more robust to common misspellings compared to BiLSTMs (with\nboth word-piece & word-only tokenization) and fine-tuned BERT based methods.\nWhen subject to misspelling attacks, LSH projection based classifiers had a\nsmall average accuracy drop of 2.94% across multiple classifications tasks,\nwhile the fine-tuned BERT model accuracy had a significant drop of 11.44%.", "published": "2019-08-14 14:48:07", "link": "http://arxiv.org/abs/1908.05763v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Adabot: Fault-Tolerant Java Decompiler", "abstract": "Reverse Engineering(RE) has been a fundamental task in software engineering.\nHowever, most of the traditional Java reverse engineering tools are strictly\nrule defined, thus are not fault-tolerant, which pose serious problem when\nnoise and interference were introduced into the system. In this paper, we view\nreverse engineering as a statistical machine translation task instead of\nrule-based task, and propose a fault-tolerant Java decompiler based on machine\ntranslation models. Our model is based on attention-based Neural Machine\nTranslation (NMT) and Transformer architectures. First, we measure the\ntranslation quality on both the redundant and purified datasets. Next, we\nevaluate the fault-tolerance(anti-noise ability) of our framework on test sets\nwith different unit error probability (UEP). In addition, we compare the\nsuitability of different word segmentation algorithms for decompilation task.\nExperimental results demonstrate that our model is more robust and\nfault-tolerant compared to traditional Abstract Syntax Tree (AST) based\ndecompilers. Specifically, in terms of BLEU-4 and Word Error Rate (WER), our\nperformance has reached 94.50% and 2.65% on the redundant test set; 92.30% and\n3.48% on the purified test set.", "published": "2019-08-14 05:17:04", "link": "http://arxiv.org/abs/1908.06748v2", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Architecture and evolution of semantic networks in mathematics texts", "abstract": "Knowledge is a network of interconnected concepts. Yet, precisely how the\ntopological structure of knowledge constrains its acquisition remains unknown,\nhampering the development of learning enhancement strategies. Here we study the\ntopological structure of semantic networks reflecting mathematical concepts and\ntheir relations in college-level linear algebra texts. We hypothesize that\nthese networks will exhibit structural order, reflecting the logical sequence\nof topics that ensures accessibility. We find that the networks exhibit strong\ncore-periphery architecture, where a dense core of concepts presented early is\ncomplemented with a sparse periphery presented evenly throughout the\nexposition; the latter is composed of many small modules each reflecting more\nnarrow domains. Using tools from applied topology, we find that the\nexpositional evolution of the semantic networks produces and subsequently fills\nknowledge gaps, and that the density of these gaps tracks negatively with\ncommunity ratings of each textbook. Broadly, our study lays the groundwork for\nfuture efforts developing optimal design principles for textbook exposition and\nteaching in a classroom setting.", "published": "2019-08-14 01:38:07", "link": "http://arxiv.org/abs/1908.04911v2", "categories": ["cs.CL", "physics.soc-ph", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "VideoNavQA: Bridging the Gap between Visual and Embodied Question\n  Answering", "abstract": "Embodied Question Answering (EQA) is a recently proposed task, where an agent\nis placed in a rich 3D environment and must act based solely on its egocentric\ninput to answer a given question. The desired outcome is that the agent learns\nto combine capabilities such as scene understanding, navigation and language\nunderstanding in order to perform complex reasoning in the visual world.\nHowever, initial advancements combining standard vision and language methods\nwith imitation and reinforcement learning algorithms have shown EQA might be\ntoo complex and challenging for these techniques. In order to investigate the\nfeasibility of EQA-type tasks, we build the VideoNavQA dataset that contains\npairs of questions and videos generated in the House3D environment. The goal of\nthis dataset is to assess question-answering performance from nearly-ideal\nnavigation paths, while considering a much more complete variety of questions\nthan current instantiations of the EQA task. We investigate several models,\nadapted from popular VQA methods, on this new benchmark. This establishes an\ninitial understanding of how well VQA-style methods can perform within this\nnovel EQA paradigm.", "published": "2019-08-14 04:44:26", "link": "http://arxiv.org/abs/1908.04950v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Fusion of Detected Objects in Text for Visual Question Answering", "abstract": "To advance models of multimodal context, we introduce a simple yet powerful\nneural architecture for data that combines vision and natural language. The\n\"Bounding Boxes in Text Transformer\" (B2T2) also leverages referential\ninformation binding words to portions of the image in a single unified\narchitecture. B2T2 is highly effective on the Visual Commonsense Reasoning\nbenchmark (https://visualcommonsense.com), achieving a new state-of-the-art\nwith a 25% relative reduction in error rate compared to published baselines and\nobtaining the best performance to date on the public leaderboard (as of May 22,\n2019). A detailed ablation analysis shows that the early integration of the\nvisual features into the text analysis is key to the effectiveness of the new\narchitecture. A reference implementation of our models is provided\n(https://github.com/google-research/language/tree/master/language/question_answering/b2t2).", "published": "2019-08-14 10:03:12", "link": "http://arxiv.org/abs/1908.05054v2", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mastering emergent language: learning to guide in simulated navigation", "abstract": "To cooperate with humans effectively, virtual agents need to be able to\nunderstand and execute language instructions. A typical setup to achieve this\nis with a scripted teacher which guides a virtual agent using language\ninstructions. However, such setup has clear limitations in scalability and,\nmore importantly, it is not interactive. Here, we introduce an autonomous agent\nthat uses discrete communication to interactively guide other agents to\nnavigate and act on a simulated environment. The developed communication\nprotocol is trainable, emergent and requires no additional supervision. The\nemergent language speeds up learning of new agents, it generalizes across\nincrementally more difficult tasks and, contrary to most other emergent\nlanguages, it is highly interpretable. We demonstrate how the emitted messages\ncorrelate with particular actions and observations, and how new agents become\nless dependent on this guidance as training progresses. By exploiting the\ncorrelations identified in our analysis, we manage to successfully address the\nagents in their own language.", "published": "2019-08-14 14:10:21", "link": "http://arxiv.org/abs/1908.05135v1", "categories": ["cs.CL", "cs.AI", "cs.MA"], "primary_category": "cs.CL"}
{"title": "Scalable Attentive Sentence-Pair Modeling via Distilled Sentence\n  Embedding", "abstract": "Recent state-of-the-art natural language understanding models, such as BERT\nand XLNet, score a pair of sentences (A and B) using multiple cross-attention\noperations - a process in which each word in sentence A attends to all words in\nsentence B and vice versa. As a result, computing the similarity between a\nquery sentence and a set of candidate sentences, requires the propagation of\nall query-candidate sentence-pairs throughout a stack of cross-attention\nlayers. This exhaustive process becomes computationally prohibitive when the\nnumber of candidate sentences is large. In contrast, sentence embedding\ntechniques learn a sentence-to-vector mapping and compute the similarity\nbetween the sentence vectors via simple elementary operations. In this paper,\nwe introduce Distilled Sentence Embedding (DSE) - a model that is based on\nknowledge distillation from cross-attentive models, focusing on sentence-pair\ntasks. The outline of DSE is as follows: Given a cross-attentive teacher model\n(e.g. a fine-tuned BERT), we train a sentence embedding based student model to\nreconstruct the sentence-pair scores obtained by the teacher model. We\nempirically demonstrate the effectiveness of DSE on five GLUE sentence-pair\ntasks. DSE significantly outperforms several ELMO variants and other sentence\nembedding methods, while accelerating computation of the query-candidate\nsentence-pairs similarities by several orders of magnitude, with an average\nrelative degradation of 4.6% compared to BERT. Furthermore, we show that DSE\nproduces sentence embeddings that reach state-of-the-art performance on\nuniversal sentence representation benchmarks. Our code is made publicly\navailable at https://github.com/microsoft/Distilled-Sentence-Embedding.", "published": "2019-08-14 15:06:48", "link": "http://arxiv.org/abs/1908.05161v3", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Two-stage Federated Phenotyping and Patient Representation Learning", "abstract": "A large percentage of medical information is in unstructured text format in\nelectronic medical record systems. Manual extraction of information from\nclinical notes is extremely time consuming. Natural language processing has\nbeen widely used in recent years for automatic information extraction from\nmedical texts. However, algorithms trained on data from a single healthcare\nprovider are not generalizable and error-prone due to the heterogeneity and\nuniqueness of medical documents. We develop a two-stage federated natural\nlanguage processing method that enables utilization of clinical notes from\ndifferent hospitals or clinics without moving the data, and demonstrate its\nperformance using obesity and comorbities phenotyping as medical task. This\napproach not only improves the quality of a specific clinical task but also\nfacilitates knowledge progression in the whole healthcare system, which is an\nessential part of learning health system. To the best of our knowledge, this is\nthe first application of federated machine learning in clinical NLP.", "published": "2019-08-14 14:06:11", "link": "http://arxiv.org/abs/1908.05596v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Entity-aware ELMo: Learning Contextual Entity Representation for Entity\n  Disambiguation", "abstract": "We present a new local entity disambiguation system. The key to our system is\na novel approach for learning entity representations. In our approach we learn\nan entity aware extension of Embedding for Language Model (ELMo) which we call\nEntity-ELMo (E-ELMo). Given a paragraph containing one or more named entity\nmentions, each mention is first defined as a function of the entire paragraph\n(including other mentions), then they predict the referent entities. Utilizing\nE-ELMo for local entity disambiguation, we outperform all of the\nstate-of-the-art local and global models on the popular benchmarks by improving\nabout 0.5\\% on micro average accuracy for AIDA test-b with Yago candidate set.\nThe evaluation setup of the training data and candidate set are the same as our\nbaselines for fair comparison.", "published": "2019-08-14 03:51:25", "link": "http://arxiv.org/abs/1908.05762v2", "categories": ["cs.CL", "cs.IR", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Components Loss for Neural Networks in Mask-Based Speech Enhancement", "abstract": "Estimating time-frequency domain masks for single-channel speech enhancement\nusing deep learning methods has recently become a popular research field with\npromising results. In this paper, we propose a novel components loss (CL) for\nthe training of neural networks for mask-based speech enhancement. During the\ntraining process, the proposed CL offers separate control over preservation of\nthe speech component quality, suppression of the residual noise component, and\npreservation of a naturally sounding residual noise component. We illustrate\nthe potential of the proposed CL by evaluating a standard convolutional neural\nnetwork (CNN) for mask-based speech enhancement. The new CL obtains a better\nand more balanced performance in almost all employed instrumental quality\nmetrics over the baseline losses, the latter comprising the conventional mean\nsquared error (MSE) loss and also auditory-related loss functions, such as the\nperceptual evaluation of speech quality (PESQ) loss and the recently proposed\nperceptual weighting filter loss. Particularly, applying the CL offers better\nspeech component quality, better overall enhanced speech perceptual quality, as\nwell as a more naturally sounding residual noise. On average, an at least 0.1\npoints higher PESQ score on the enhanced speech is obtained while also\nobtaining a higher SNR improvement by more than 0.5 dB, for seen noise types.\nThis improvement is stronger for unseen noise types, where an about 0.2 points\nhigher PESQ score on the enhanced speech is obtained, while also the output SNR\nis ahead by more than 0.5 dB. The new proposed CL is easy to implement and code\nis provided at https://github.com/ifnspaml/Components-Loss.", "published": "2019-08-14 12:03:27", "link": "http://arxiv.org/abs/1908.05087v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "State-of-the-art Speech Recognition using EEG and Towards Decoding of\n  Speech Spectrum From EEG", "abstract": "In this paper we first demonstrate continuous noisy speech recognition using\nelectroencephalography (EEG) signals on English vocabulary using different\ntypes of state of the art end-to-end automatic speech recognition (ASR) models,\nwe further provide results obtained using EEG data recorded under different\nexperimental conditions. We finally demonstrate decoding of speech spectrum\nfrom EEG signals using a long short term memory (LSTM) based regression model\nand Generative Adversarial Network (GAN) based model. Our results demonstrate\nthe feasibility of using EEG signals for continuous noisy speech recognition\nunder different experimental conditions and we provide preliminary results for\nsynthesis of speech from EEG features.", "published": "2019-08-14 16:03:37", "link": "http://arxiv.org/abs/1908.05743v5", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Interleaved Multitask Learning for Audio Source Separation with\n  Independent Databases", "abstract": "Deep Neural Network-based source separation methods usually train independent\nmodels to optimize for the separation of individual sources. Although this can\nlead to good performance for well-defined targets, it can also be\ncomputationally expensive. The multitask alternative of a single network\njointly optimizing for all targets simultaneously usually requires the\navailability of all target sources for each input. This requirement hampers the\nability to create large training databases. In this paper, we present a model\nthat decomposes the learnable parameters into a shared parametric model\n(encoder) and independent components (decoders) specific to each source. We\npropose an interleaved training procedure that optimizes the sub-task decoders\nindependently and thus does not require each sample to possess a ground truth\nfor all of its composing sources. Experimental results on MUSDB18 with the\nproposed method show comparable performance to independently trained models,\nwith less trainable parameters, more efficient inference, and an encoder\ntransferable to future target objectives. The results also show that using the\nproposed interleaved training procedure leads to better Source-to-Interference\nenergy ratios when compared to the simultaneous optimization of all training\nobjectives, even when all composing sources are available.", "published": "2019-08-14 15:56:34", "link": "http://arxiv.org/abs/1908.05182v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
