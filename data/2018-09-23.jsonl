{"title": "Learning and Evaluating Sparse Interpretable Sentence Embeddings", "abstract": "Previous research on word embeddings has shown that sparse representations,\nwhich can be either learned on top of existing dense embeddings or obtained\nthrough model constraints during training time, have the benefit of increased\ninterpretability properties: to some degree, each dimension can be understood\nby a human and associated with a recognizable feature in the data. In this\npaper, we transfer this idea to sentence embeddings and explore several\napproaches to obtain a sparse representation. We further introduce a novel,\nquantitative and automated evaluation metric for sentence embedding\ninterpretability, based on topic coherence methods. We observe an increase in\ninterpretability compared to dense models, on a dataset of movie dialogs and on\nthe scene descriptions from the MS COCO dataset.", "published": "2018-09-23 16:02:03", "link": "http://arxiv.org/abs/1809.08621v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Hate Speech and Offensive Language on Twitter using Machine\n  Learning: An N-gram and TFIDF based Approach", "abstract": "Toxic online content has become a major issue in today's world due to an\nexponential increase in the use of internet by people of different cultures and\neducational background. Differentiating hate speech and offensive language is a\nkey challenge in automatic detection of toxic text content. In this paper, we\npropose an approach to automatically classify tweets on Twitter into three\nclasses: hateful, offensive and clean. Using Twitter dataset, we perform\nexperiments considering n-grams as features and passing their term\nfrequency-inverse document frequency (TFIDF) values to multiple machine\nlearning models. We perform comparative analysis of the models considering\nseveral values of n in n-grams and TFIDF normalization methods. After tuning\nthe model giving the best results, we achieve 95.6% accuracy upon evaluating it\non test data. We also create a module which serves as an intermediate between\nuser and Twitter.", "published": "2018-09-23 18:19:03", "link": "http://arxiv.org/abs/1809.08651v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mind Your Language: Abuse and Offense Detection for Code-Switched\n  Languages", "abstract": "In multilingual societies like the Indian subcontinent, use of code-switched\nlanguages is much popular and convenient for the users. In this paper, we study\noffense and abuse detection in the code-switched pair of Hindi and English\n(i.e. Hinglish), the pair that is the most spoken. The task is made difficult\ndue to non-fixed grammar, vocabulary, semantics and spellings of Hinglish\nlanguage. We apply transfer learning and make a LSTM based model for hate\nspeech classification. This model surpasses the performance shown by the\ncurrent best models to establish itself as the state-of-the-art in the\nunexplored domain of Hinglish offensive text classification.We also release our\nmodel and the embeddings trained for research purposes", "published": "2018-09-23 18:19:46", "link": "http://arxiv.org/abs/1809.08652v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Textually Enriched Neural Module Networks for Visual Question Answering", "abstract": "Problems at the intersection of language and vision, like visual question\nanswering, have recently been gaining a lot of attention in the field of\nmulti-modal machine learning as computer vision research moves beyond\ntraditional recognition tasks. There has been recent success in visual question\nanswering using deep neural network models which use the linguistic structure\nof the questions to dynamically instantiate network layouts. In the process of\nconverting the question to a network layout, the question is simplified, which\nresults in loss of information in the model. In this paper, we enrich the image\ninformation with textual data using image captions and external knowledge bases\nto generate more coherent answers. We achieve 57.1% overall accuracy on the\ntest-dev open-ended questions from the visual question answering (VQA 1.0) real\nimage dataset.", "published": "2018-09-23 23:45:54", "link": "http://arxiv.org/abs/1809.08697v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Towards Language Agnostic Universal Representations", "abstract": "When a bilingual student learns to solve word problems in math, we expect the\nstudent to be able to solve these problem in both languages the student is\nfluent in,even if the math lessons were only taught in one language. However,\ncurrent representations in machine learning are language dependent. In this\nwork, we present a method to decouple the language from the problem by learning\nlanguage agnostic representations and therefore allowing training a model in\none language and applying to a different one in a zero shot fashion. We learn\nthese representations by taking inspiration from linguistics and formalizing\nUniversal Grammar as an optimization process (Chomsky, 2014; Montague, 1970).\nWe demonstrate the capabilities of these representations by showing that the\nmodels trained on a single language using language agnostic representations\nachieve very similar accuracies in other languages.", "published": "2018-09-23 01:55:46", "link": "http://arxiv.org/abs/1809.08510v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
