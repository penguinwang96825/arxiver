{"title": "Experiments with POS Tagging Code-mixed Indian Social Media Text", "abstract": "This paper presents Centre for Development of Advanced Computing Mumbai's\n(CDACM) submission to the NLP Tools Contest on Part-Of-Speech (POS) Tagging For\nCode-mixed Indian Social Media Text (POSCMISMT) 2015 (collocated with ICON\n2015). We submitted results for Hindi (hi), Bengali (bn), and Telugu (te)\nlanguages mixed with English (en). In this paper, we have described our\napproaches to the POS tagging techniques, we exploited for this task. Machine\nlearning has been used to POS tag the mixed language text. For POS tagging,\ndistributed representations of words in vector space (word2vec) for feature\nextraction and Log-linear models have been tried. We report our work on all\nthree languages hi, bn, and te mixed with en.", "published": "2016-10-31 06:13:31", "link": "http://arxiv.org/abs/1610.09799v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Named Entity Recognition for Novel Types by Transfer Learning", "abstract": "In named entity recognition, we often don't have a large in-domain training\ncorpus or a knowledge base with adequate coverage to train a model directly. In\nthis paper, we propose a method where, given training data in a related domain\nwith similar (but not identical) named entity (NE) types and a small amount of\nin-domain training data, we use transfer learning to learn a domain-specific NE\nmodel. That is, the novelty in the task setup is that we assume not just domain\nmismatch, but also label mismatch.", "published": "2016-10-31 13:36:35", "link": "http://arxiv.org/abs/1610.09914v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge Questions from Knowledge Graphs", "abstract": "We address the novel problem of automatically generating quiz-style knowledge\nquestions from a knowledge graph such as DBpedia. Questions of this kind have\nample applications, for instance, to educate users about or to evaluate their\nknowledge in a specific domain. To solve the problem, we propose an end-to-end\napproach. The approach first selects a named entity from the knowledge graph as\nan answer. It then generates a structured triple-pattern query, which yields\nthe answer as its sole result. If a multiple-choice question is desired, the\napproach selects alternative answer options. Finally, our approach uses a\ntemplate-based method to verbalize the structured query and yield a natural\nlanguage question. A key challenge is estimating how difficult the generated\nquestion is to human users. To do this, we make use of historical data from the\nJeopardy! quiz show and a semantically annotated Web-scale document collection,\nengineer suitable features, and train a logistic regression classifier to\npredict question difficulty. Experiments demonstrate the viability of our\noverall approach.", "published": "2016-10-31 14:27:07", "link": "http://arxiv.org/abs/1610.09935v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Sentiment Lexicons for German Twitter", "abstract": "Despite a substantial progress made in developing new sentiment lexicon\ngeneration (SLG) methods for English, the task of transferring these approaches\nto other languages and domains in a sound way still remains open. In this\npaper, we contribute to the solution of this problem by systematically\ncomparing semi-automatic translations of common English polarity lists with the\nresults of the original automatic SLG algorithms, which were applied directly\nto German data. We evaluate these lexicons on a corpus of 7,992 manually\nannotated tweets. In addition to that, we also collate the results of\ndictionary- and corpus-based SLG methods in order to find out which of these\nparadigms is better suited for the inherently noisy domain of social media. Our\nexperiments show that semi-automatic translations notably outperform automatic\nsystems (reaching a macro-averaged F1-score of 0.589), and that\ndictionary-based techniques produce much better polarity lists as compared to\ncorpus-based approaches (whose best F1-scores run up to 0.479 and 0.419\nrespectively) even for the non-standard Twitter genre.", "published": "2016-10-31 16:12:16", "link": "http://arxiv.org/abs/1610.09995v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "End-to-End Answer Chunk Extraction and Ranking for Reading Comprehension", "abstract": "This paper proposes dynamic chunk reader (DCR), an end-to-end neural reading\ncomprehension (RC) model that is able to extract and rank a set of answer\ncandidates from a given document to answer questions. DCR is able to predict\nanswers of variable lengths, whereas previous neural RC models primarily\nfocused on predicting single tokens or entities. DCR encodes a document and an\ninput question with recurrent neural networks, and then applies a word-by-word\nattention mechanism to acquire question-aware representations for the document,\nfollowed by the generation of chunk representations and a ranking module to\npropose the top-ranked chunk as the answer. Experimental results show that DCR\nachieves state-of-the-art exact match and F1 scores on the SQuAD dataset.", "published": "2016-10-31 16:14:08", "link": "http://arxiv.org/abs/1610.09996v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RNN Approaches to Text Normalization: A Challenge", "abstract": "This paper presents a challenge to the community: given a large corpus of\nwritten text aligned to its normalized spoken form, train an RNN to learn the\ncorrect normalization function. We present a data set of general text where the\nnormalizations were generated using an existing text normalization component of\na text-to-speech system. This data set will be released open-source in the near\nfuture.\n  We also present our own experiments with this data set with a variety of\ndifferent RNN architectures. While some of the architectures do in fact produce\nvery good results when measured in terms of overall accuracy, the errors that\nare produced are problematic, since they would convey completely the wrong\nmessage if such a system were deployed in a speech application. On the other\nhand, we show that a simple FST-based filter can mitigate those errors, and\nachieve a level of accuracy not achievable by the RNN alone.\n  Though our conclusions are largely negative on this point, we are actually\nnot arguing that the text normalization problem is intractable using an pure\nRNN approach, merely that it is not going to be something that can be solved\nmerely by having huge amounts of annotated text data and feeding that to a\ngeneral RNN model. And when we open-source our data, we will be providing a\nnovel data set for sequence-to-sequence modeling in the hopes that the the\ncommunity can find better solutions.\n  The data used in this work have been released and are available at:\nhttps://github.com/rwsproat/text-normalization-data", "published": "2016-10-31 22:42:02", "link": "http://arxiv.org/abs/1611.00068v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Deep Learning in Hindi NER: An approach to tackle the Labelled\n  Data Scarcity", "abstract": "In this paper we describe an end to end Neural Model for Named Entity\nRecognition NER) which is based on Bi-Directional RNN-LSTM. Almost all NER\nsystems for Hindi use Language Specific features and handcrafted rules with\ngazetteers. Our model is language independent and uses no domain specific\nfeatures or any handcrafted rules. Our models rely on semantic information in\nthe form of word vectors which are learnt by an unsupervised learning algorithm\non an unannotated corpus. Our model attained state of the art performance in\nboth English and Hindi without the use of any morphological analysis or without\nusing gazetteers of any sort.", "published": "2016-10-31 01:31:52", "link": "http://arxiv.org/abs/1610.09756v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Chinese Poetry Generation with Planning based Neural Network", "abstract": "Chinese poetry generation is a very challenging task in natural language\nprocessing. In this paper, we propose a novel two-stage poetry generating\nmethod which first plans the sub-topics of the poem according to the user's\nwriting intent, and then generates each line of the poem sequentially, using a\nmodified recurrent neural network encoder-decoder framework. The proposed\nplanning-based method can ensure that the generated poem is coherent and\nsemantically consistent with the user's intent. A comprehensive evaluation with\nhuman judgments demonstrates that our proposed approach outperforms the\nstate-of-the-art poetry generating methods and the poem quality is somehow\ncomparable to human poets.", "published": "2016-10-31 12:16:39", "link": "http://arxiv.org/abs/1610.09889v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LightRNN: Memory and Computation-Efficient Recurrent Neural Networks", "abstract": "Recurrent neural networks (RNNs) have achieved state-of-the-art performances\nin many natural language processing tasks, such as language modeling and\nmachine translation. However, when the vocabulary is large, the RNN model will\nbecome very big (e.g., possibly beyond the memory capacity of a GPU device) and\nits training will become very inefficient. In this work, we propose a novel\ntechnique to tackle this challenge. The key idea is to use 2-Component (2C)\nshared embedding for word representations. We allocate every word in the\nvocabulary into a table, each row of which is associated with a vector, and\neach column associated with another vector. Depending on its position in the\ntable, a word is jointly represented by two components: a row vector and a\ncolumn vector. Since the words in the same row share the row vector and the\nwords in the same column share the column vector, we only need $2 \\sqrt{|V|}$\nvectors to represent a vocabulary of $|V|$ unique words, which are far less\nthan the $|V|$ vectors required by existing approaches. Based on the\n2-Component shared embedding, we design a new RNN algorithm and evaluate it\nusing the language modeling task on several benchmark datasets. The results\nshow that our algorithm significantly reduces the model size and speeds up the\ntraining process, without sacrifice of accuracy (it achieves similar, if not\nbetter, perplexity as compared to state-of-the-art language models).\nRemarkably, on the One-Billion-Word benchmark Dataset, our algorithm achieves\ncomparable perplexity to previous language models, whilst reducing the model\nsize by a factor of 40-100, and speeding up the training process by a factor of\n2. We name our proposed algorithm \\emph{LightRNN} to reflect its very small\nmodel size and very high training speed.", "published": "2016-10-31 12:24:13", "link": "http://arxiv.org/abs/1610.09893v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Ontology Verbalization using Semantic-Refinement", "abstract": "We propose a rule-based technique to generate redundancy-free NL descriptions\nof OWL entities.The existing approaches which address the problem of\nverbalizing OWL ontologies generate NL text segments which are close to their\ncounterpart OWL statements.Some of these approaches also perform grouping and\naggregating of these NL text segments to generate a more fluent and\ncomprehensive form of the content.Restricting our attention to description of\nindividuals and concepts, we find that the approach currently followed in the\navailable tools is that of determining the set of all logical conditions that\nare satisfied by the given individual/concept name and translate these\nconditions verbatim into corresponding NL descriptions.Human-understandability\nof such descriptions is affected by the presence of repetitions and\nredundancies, as they have high fidelity to their OWL representation.In the\nliterature, no efforts had been taken to remove redundancies and repetitions at\nthe logical-level before generating the NL descriptions of entities and we find\nthis to be the main reason for lack of readability of the generated\ntext.Herein, we propose a technique called semantic-refinement(SR) to generate\nmeaningful and easily-understandable descriptions of individuals and concepts\nof a given OWLontology.We identify the combinations of OWL/DL constructs that\nlead to repetitive/redundant descriptions and propose a series of refinement\nrules to rewrite the conditions that are satisfied by an individual/concept in\na meaning-preserving manner.The reduced set of conditions are then employed for\ngenerating NL descriptions.Our experiments show that, SR leads to significantly\nimproved descriptions of ontology entities.We also test the effectiveness and\nusefulness of the the generated descriptions for the purpose of validating the\nontologies and find that the proposed technique is indeed helpful in the\ncontext.", "published": "2016-10-31 15:20:14", "link": "http://arxiv.org/abs/1610.09964v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Sentiment Analysis of Review Datasets Using Naive Bayes and K-NN\n  Classifier", "abstract": "The advent of Web 2.0 has led to an increase in the amount of sentimental\ncontent available in the Web. Such content is often found in social media web\nsites in the form of movie or product reviews, user comments, testimonials,\nmessages in discussion forums etc. Timely discovery of the sentimental or\nopinionated web content has a number of advantages, the most important of all\nbeing monetization. Understanding of the sentiments of human masses towards\ndifferent entities and products enables better services for contextual\nadvertisements, recommendation systems and analysis of market trends. The focus\nof our project is sentiment focussed web crawling framework to facilitate the\nquick discovery of sentimental contents of movie reviews and hotel reviews and\nanalysis of the same. We use statistical methods to capture elements of\nsubjective style and the sentence polarity. The paper elaborately discusses two\nsupervised machine learning algorithms: K-Nearest Neighbour(K-NN) and Naive\nBayes and compares their overall accuracy, precisions as well as recall values.\nIt was seen that in case of movie reviews Naive Bayes gave far better results\nthan K-NN but for hotel reviews these algorithms gave lesser, almost same\naccuracies.", "published": "2016-10-31 15:45:41", "link": "http://arxiv.org/abs/1610.09982v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Neural Machine Translation in Linear Time", "abstract": "We present a novel neural network for processing sequences. The ByteNet is a\none-dimensional convolutional neural network that is composed of two parts, one\nto encode the source sequence and the other to decode the target sequence. The\ntwo network parts are connected by stacking the decoder on top of the encoder\nand preserving the temporal resolution of the sequences. To address the\ndiffering lengths of the source and the target, we introduce an efficient\nmechanism by which the decoder is dynamically unfolded over the representation\nof the encoder. The ByteNet uses dilation in the convolutional layers to\nincrease its receptive field. The resulting network has two core properties: it\nruns in time that is linear in the length of the sequences and it sidesteps the\nneed for excessive memorization. The ByteNet decoder attains state-of-the-art\nperformance on character-level language modelling and outperforms the previous\nbest results obtained with recurrent networks. The ByteNet also achieves\nstate-of-the-art performance on character-to-character machine translation on\nthe English-to-German WMT translation task, surpassing comparable neural\ntranslation models that are based on recurrent networks with attentional\npooling and run in quadratic time. We find that the latent alignment structure\ncontained in the representations reflects the expected alignment between the\ntokens.", "published": "2016-10-31 19:56:39", "link": "http://arxiv.org/abs/1610.10099v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Neural Speech Recognizer: Acoustic-to-Word LSTM Model for Large\n  Vocabulary Speech Recognition", "abstract": "We present results that show it is possible to build a competitive, greatly\nsimplified, large vocabulary continuous speech recognition system with whole\nwords as acoustic units. We model the output vocabulary of about 100,000 words\ndirectly using deep bi-directional LSTM RNNs with CTC loss. The model is\ntrained on 125,000 hours of semi-supervised acoustic training data, which\nenables us to alleviate the data sparsity problem for word models. We show that\nthe CTC word models work very well as an end-to-end all-neural speech\nrecognition model without the use of traditional context-dependent sub-word\nphone units that require a pronunciation lexicon, and without any language\nmodel removing the need to decode. We demonstrate that the CTC word models\nperform better than a strong, more complex, state-of-the-art baseline with\nsub-word units.", "published": "2016-10-31 15:36:42", "link": "http://arxiv.org/abs/1610.09975v1", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Neural Symbolic Machines: Learning Semantic Parsers on Freebase with\n  Weak Supervision", "abstract": "Harnessing the statistical power of neural networks to perform language\nunderstanding and symbolic reasoning is difficult, when it requires executing\nefficient discrete operations against a large knowledge-base. In this work, we\nintroduce a Neural Symbolic Machine, which contains (a) a neural \"programmer\",\ni.e., a sequence-to-sequence model that maps language utterances to programs\nand utilizes a key-variable memory to handle compositionality (b) a symbolic\n\"computer\", i.e., a Lisp interpreter that performs program execution, and helps\nfind good programs by pruning the search space. We apply REINFORCE to directly\noptimize the task reward of this structured prediction problem. To train with\nweak supervision and improve the stability of REINFORCE, we augment it with an\niterative maximum-likelihood training process. NSM outperforms the\nstate-of-the-art on the WebQuestionsSP dataset when trained from\nquestion-answer pairs only, without requiring any feature engineering or\ndomain-specific knowledge.", "published": "2016-10-31 20:07:23", "link": "http://arxiv.org/abs/1611.00020v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
