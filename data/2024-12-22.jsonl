{"title": "Ask-Before-Detection: Identifying and Mitigating Conformity Bias in\n  LLM-Powered Error Detector for Math Word Problem Solutions", "abstract": "The rise of large language models (LLMs) offers new opportunities for\nautomatic error detection in education, particularly for math word problems\n(MWPs). While prior studies demonstrate the promise of LLMs as error detectors,\nthey overlook the presence of multiple valid solutions for a single MWP. Our\npreliminary analysis reveals a significant performance gap between conventional\nand alternative solutions in MWPs, a phenomenon we term conformity bias in this\nwork. To mitigate this bias, we introduce the Ask-Before-Detect (AskBD)\nframework, which generates adaptive reference solutions using LLMs to enhance\nerror detection. Experiments on 200 examples of GSM8K show that AskBD\neffectively mitigates bias and improves performance, especially when combined\nwith reasoning-enhancing techniques like chain-of-thought prompting.", "published": "2024-12-22 03:08:36", "link": "http://arxiv.org/abs/2412.16838v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Teaching LLMs to Refine with Tools", "abstract": "Large language models (LLMs) can refine their responses based on feedback,\nenabling self-improvement through iterative training or test-time refinement.\nHowever, existing methods predominantly focus on refinement within the same\nreasoning format, which may lead to non-correcting behaviors. We propose CaP, a\nnovel approach that uses external tools to refine chain-of-thought (CoT)\nresponses generated by the same or other LLMs. CaP employs a two-stage training\nprocess: supervised fine-tuning followed by preference optimization with DPO\nvariants. Our observations highlight the critical role of preference\noptimization in enabling effective refinement. Additionally, we compare several\nsampling strategies to leverage CoT and tools at inference time. Experimental\nresults demonstrate CaP's potential for effective cross-reasoning refinement\nand efficient inference.", "published": "2024-12-22 05:43:50", "link": "http://arxiv.org/abs/2412.16871v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reconsidering SMT Over NMT for Closely Related Languages: A Case Study\n  of Persian-Hindi Pair", "abstract": "This paper demonstrates that Phrase-Based Statistical Machine Translation\n(PBSMT) can outperform Transformer-based Neural Machine Translation (NMT) in\nmoderate-resource scenarios, specifically for structurally similar languages,\nlike the Persian-Hindi pair. Despite the Transformer architecture's typical\npreference for large parallel corpora, our results show that PBSMT achieves a\nBLEU score of 66.32, significantly exceeding the Transformer-NMT score of 53.7\non the same dataset. Additionally, we explore variations of the SMT\narchitecture, including training on Romanized text and modifying the word order\nof Persian sentences to match the left-to-right (LTR) structure of Hindi. Our\nfindings highlight the importance of choosing the right architecture based on\nlanguage pair characteristics and advocate for SMT as a high-performing\nalternative, even in contexts commonly dominated by NMT.", "published": "2024-12-22 06:12:46", "link": "http://arxiv.org/abs/2412.16877v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Bilingual Lexicon Induction for Low Resource Languages", "abstract": "Bilingual lexicons play a crucial role in various Natural Language Processing\ntasks. However, many low-resource languages (LRLs) do not have such lexicons,\nand due to the same reason, cannot benefit from the supervised Bilingual\nLexicon Induction (BLI) techniques. To address this, unsupervised BLI (UBLI)\ntechniques were introduced. A prominent technique in this line is\nstructure-based UBLI. It is an iterative method, where a seed lexicon, which is\ninitially learned from monolingual embeddings is iteratively improved. There\nhave been numerous improvements to this core idea, however they have been\nexperimented with independently of each other. In this paper, we investigate\nwhether using these techniques simultaneously would lead to equal gains. We use\nthe unsupervised version of VecMap, a commonly used structure-based UBLI\nframework, and carry out a comprehensive set of experiments using the LRL\npairs, English-Sinhala, English-Tamil, and English-Punjabi. These experiments\nhelped us to identify the best combination of the extensions. We also release\nbilingual dictionaries for English-Sinhala and English-Punjabi.", "published": "2024-12-22 07:07:09", "link": "http://arxiv.org/abs/2412.16894v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Career Interview Dialogue System using Large Language Model-based\n  Dynamic Slot Generation", "abstract": "This study aims to improve the efficiency and quality of career interviews\nconducted by nursing managers. To this end, we have been developing a\nslot-filling dialogue system that engages in pre-interviews to collect\ninformation on staff careers as a preparatory step before the actual\ninterviews. Conventional slot-filling-based interview dialogue systems have\nlimitations in the flexibility of information collection because the dialogue\nprogresses based on predefined slot sets. We therefore propose a method that\nleverages large language models (LLMs) to dynamically generate new slots\naccording to the flow of the dialogue, achieving more natural conversations.\nFurthermore, we incorporate abduction into the slot generation process to\nenable more appropriate and effective slot generation. To validate the\neffectiveness of the proposed method, we conducted experiments using a user\nsimulator. The results suggest that the proposed method using abduction is\neffective in enhancing both information-collecting capabilities and the\nnaturalness of the dialogue.", "published": "2024-12-22 09:25:02", "link": "http://arxiv.org/abs/2412.16943v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Aristotle: Mastering Logical Reasoning with A Logic-Complete\n  Decompose-Search-Resolve Framework", "abstract": "In the context of large language models (LLMs), current advanced reasoning\nmethods have made impressive strides in various reasoning tasks. However, when\nit comes to logical reasoning tasks, major challenges remain in both efficacy\nand efficiency. This is rooted in the fact that these systems fail to fully\nleverage the inherent structure of logical tasks throughout the reasoning\nprocesses such as decomposition, search, and resolution. To address this, we\npropose a logic-complete reasoning framework, Aristotle, with three key\ncomponents: Logical Decomposer, Logical Search Router, and Logical Resolver. In\nour framework, symbolic expressions and logical rules are comprehensively\nintegrated into the entire reasoning process, significantly alleviating the\nbottlenecks of logical reasoning, i.e., reducing sub-task complexity,\nminimizing search errors, and resolving logical contradictions. The\nexperimental results on several datasets demonstrate that Aristotle\nconsistently outperforms state-of-the-art reasoning frameworks in both accuracy\nand efficiency, particularly excelling in complex logical reasoning scenarios.\nWe will open-source all our code at https://github.com/Aiden0526/Aristotle.", "published": "2024-12-22 10:14:09", "link": "http://arxiv.org/abs/2412.16953v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LH-Mix: Local Hierarchy Correlation Guided Mixup over Hierarchical\n  Prompt Tuning", "abstract": "Hierarchical text classification (HTC) aims to assign one or more labels in\nthe hierarchy for each text. Many methods represent this structure as a global\nhierarchy, leading to redundant graph structures. To address this,\nincorporating a text-specific local hierarchy is essential. However, existing\napproaches often model this local hierarchy as a sequence, focusing on explicit\nparent-child relationships while ignoring implicit correlations among\nsibling/peer relationships. In this paper, we first integrate local hierarchies\ninto a manual depth-level prompt to capture parent-child relationships. We then\napply Mixup to this hierarchical prompt tuning scheme to improve the latent\ncorrelation within sibling/peer relationships. Notably, we propose a novel\nMixup ratio guided by local hierarchy correlation to effectively capture\nintrinsic correlations. This Local Hierarchy Mixup (LH-Mix) model demonstrates\nremarkable performance across three widely-used datasets.", "published": "2024-12-22 10:47:18", "link": "http://arxiv.org/abs/2412.16963v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Part-Of-Speech Sensitivity of Routers in Mixture of Experts Models", "abstract": "This study investigates the behavior of model-integrated routers in Mixture\nof Experts (MoE) models, focusing on how tokens are routed based on their\nlinguistic features, specifically Part-of-Speech (POS) tags. The goal is to\nexplore across different MoE architectures whether experts specialize in\nprocessing tokens with similar linguistic traits. By analyzing token\ntrajectories across experts and layers, we aim to uncover how MoE models handle\nlinguistic information. Findings from six popular MoE models reveal expert\nspecialization for specific POS categories, with routing paths showing high\npredictive accuracy for POS, highlighting the value of routing paths in\ncharacterizing tokens.", "published": "2024-12-22 11:03:41", "link": "http://arxiv.org/abs/2412.16971v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Robustness of Large Language Models Against Adversarial Attacks", "abstract": "The increasing deployment of Large Language Models (LLMs) in various\napplications necessitates a rigorous evaluation of their robustness against\nadversarial attacks. In this paper, we present a comprehensive study on the\nrobustness of GPT LLM family. We employ two distinct evaluation methods to\nassess their resilience. The first method introduce character-level text attack\nin input prompts, testing the models on three sentiment classification\ndatasets: StanfordNLP/IMDB, Yelp Reviews, and SST-2. The second method involves\nusing jailbreak prompts to challenge the safety mechanisms of the LLMs. Our\nexperiments reveal significant variations in the robustness of these models,\ndemonstrating their varying degrees of vulnerability to both character-level\nand semantic-level adversarial attacks. These findings underscore the necessity\nfor improved adversarial training and enhanced safety mechanisms to bolster the\nrobustness of LLMs.", "published": "2024-12-22 13:21:15", "link": "http://arxiv.org/abs/2412.17011v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reversed Attention: On The Gradient Descent Of Attention Layers In GPT", "abstract": "The success of Transformer-based Language Models (LMs) stems from their\nattention mechanism. While this mechanism has been extensively studied in\nexplainability research, particularly through the attention values obtained\nduring the forward pass of LMs, the backward pass of attention has been largely\noverlooked. In this work, we study the mathematics of the backward pass of\nattention, revealing that it implicitly calculates an attention matrix we refer\nto as \"Reversed Attention\". We examine the properties of Reversed Attention and\ndemonstrate its ability to elucidate the models' behavior and edit dynamics. In\nan experimental setup, we showcase the ability of Reversed Attention to\ndirectly alter the forward pass of attention, without modifying the model's\nweights, using a novel method called \"attention patching\". In addition to\nenhancing the comprehension of how LM configure attention layers during\nbackpropagation, Reversed Attention maps contribute to a more interpretable\nbackward pass.", "published": "2024-12-22 13:48:04", "link": "http://arxiv.org/abs/2412.17019v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MINTQA: A Multi-Hop Question Answering Benchmark for Evaluating LLMs on\n  New and Tail Knowledge", "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in\nvarious reasoning tasks but face significant challenges with complex,\nknowledge-intensive multi-hop queries, particularly those involving new or\nlong-tail knowledge. Existing benchmarks often fail to fully address these\nchallenges. To bridge this gap, we introduce MINTQA (Multi-hop Question\nAnswering on New and Tail Knowledge), a comprehensive benchmark to evaluate\nLLMs' capabilities in multi-hop reasoning across four critical dimensions:\nquestion handling strategy, sub-question generation, retrieval-augmented\ngeneration, and iterative or dynamic decomposition and retrieval. MINTQA\ncomprises 10,479 question-answer pairs for evaluating new knowledge and 17,887\npairs for assessing long-tail knowledge, with each question equipped with\ncorresponding sub-questions and answers. Our systematic evaluation of 22\nstate-of-the-art LLMs on MINTQA reveals significant limitations in their\nability to handle complex knowledge base queries, particularly in handling new\nor unpopular knowledge. Our findings highlight critical challenges and offer\ninsights for advancing multi-hop reasoning capabilities. The MINTQA benchmark\nis available at https://github.com/probe2/multi-hop/.", "published": "2024-12-22 14:17:12", "link": "http://arxiv.org/abs/2412.17032v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Shaping the Safety Boundaries: Understanding and Defending Against\n  Jailbreaks in Large Language Models", "abstract": "Jailbreaking in Large Language Models (LLMs) is a major security concern as\nit can deceive LLMs to generate harmful text. Yet, there is still insufficient\nunderstanding of how jailbreaking works, which makes it hard to develop\neffective defense strategies. We aim to shed more light into this issue: we\nconduct a detailed large-scale analysis of seven different jailbreak methods\nand find that these disagreements stem from insufficient observation samples.\nIn particular, we introduce \\textit{safety boundary}, and we find that\njailbreaks shift harmful activations outside that safety boundary, where LLMs\nare less sensitive to harmful information. We also find that the low and the\nmiddle layers are critical in such shifts, while deeper layers have less\nimpact. Leveraging on these insights, we propose a novel defense called\n\\textbf{Activation Boundary Defense} (ABD), which adaptively constrains the\nactivations within the safety boundary. We further use Bayesian optimization to\nselectively apply the defense method to the low and the middle layers. Our\nexperiments on several benchmarks show that ABD achieves an average DSR of over\n98\\% against various forms of jailbreak attacks, with less than 2\\% impact on\nthe model's general capabilities.", "published": "2024-12-22 14:18:39", "link": "http://arxiv.org/abs/2412.17034v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Agent Sampling: Scaling Inference Compute for Data Synthesis with\n  Tree Search-Based Agentic Collaboration", "abstract": "Scaling laws for inference compute in multi-agent systems remain\nunder-explored compared to single-agent scenarios. This work aims to bridge\nthis gap by investigating the problem of data synthesis through multi-agent\nsampling, where synthetic responses are generated by sampling from multiple\ndistinct language models. Effective model coordination is crucial for\nsuccessful multi-agent collaboration. Unlike previous approaches that rely on\nfixed workflows, we treat model coordination as a multi-step decision-making\nprocess, optimizing generation structures dynamically for each input question.\nWe introduce Tree Search-based Orchestrated Agents~(TOA), where the workflow\nevolves iteratively during the sequential sampling process. To achieve this, we\nleverage Monte Carlo Tree Search (MCTS), integrating a reward model to provide\nreal-time feedback and accelerate exploration. Our experiments on alignment,\nmachine translation, and mathematical reasoning demonstrate that multi-agent\nsampling significantly outperforms single-agent sampling as inference compute\nscales. TOA is the most compute-efficient approach, achieving SOTA performance\non WMT and a 71.8\\% LC win rate on AlpacaEval. Moreover, fine-tuning with our\nsynthesized alignment data surpasses strong preference learning methods on\nchallenging benchmarks such as Arena-Hard and AlpacaEval.", "published": "2024-12-22 15:16:44", "link": "http://arxiv.org/abs/2412.17061v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Computational Analysis of Character Development in Holocaust Testimonies", "abstract": "This work presents a computational approach to analyze character development\nalong the narrative timeline. The analysis characterizes the inner and outer\nchanges the protagonist undergoes within a narrative, and the interplay between\nthem. We consider transcripts of Holocaust survivor testimonies as a test case,\neach telling the story of an individual in first-person terms. We focus on the\nsurvivor's religious trajectory, examining the evolution of their disposition\ntoward religious belief and practice along the testimony. Clustering the\nresulting trajectories in the dataset, we identify common sequences in the\ndata. Our findings highlight multiple common structures of religiosity across\nthe narratives: in terms of belief, most present a constant disposition, while\nfor practice, most present an oscillating structure, serving as valuable\nmaterial for historical and sociological research. This work demonstrates the\npotential of natural language processing techniques for analyzing character\nevolution through thematic trajectories in narratives.", "published": "2024-12-22 15:20:53", "link": "http://arxiv.org/abs/2412.17063v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Adapt to Low-Resource Paraphrase Generation", "abstract": "Paraphrase generation is a longstanding NLP task and achieves great success\nwith the aid of large corpora. However, transferring a paraphrasing model to\nanother domain encounters the problem of domain shifting especially when the\ndata is sparse. At the same time, widely using large pre-trained language\nmodels (PLMs) faces the overfitting problem when training on scarce labeled\ndata. To mitigate these two issues, we propose, LAPA, an effective adapter for\nPLMs optimized by meta-learning. LAPA has three-stage training on three types\nof related resources to solve this problem: 1. pre-training PLMs on\nunsupervised corpora, 2. inserting an adapter layer and meta-training on source\ndomain labeled data, and 3. fine-tuning adapters on a small amount of target\ndomain labeled data. This method enables paraphrase generation models to learn\nbasic language knowledge first, then learn the paraphrasing task itself later,\nand finally adapt to the target task. Our experimental results demonstrate that\nLAPA achieves state-of-the-art in supervised, unsupervised, and low-resource\nsettings on three benchmark datasets. With only 2\\% of trainable parameters and\n1\\% labeled data of the target task, our approach can achieve a competitive\nperformance with previous work.", "published": "2024-12-22 17:55:52", "link": "http://arxiv.org/abs/2412.17111v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLMsAgainstHate @ NLU of Devanagari Script Languages 2025: Hate Speech\n  Detection and Target Identification in Devanagari Languages via Parameter\n  Efficient Fine-Tuning of LLMs", "abstract": "The detection of hate speech has become increasingly important in combating\nonline hostility and its real-world consequences. Despite recent advancements,\nthere is limited research addressing hate speech detection in\nDevanagari-scripted languages, where resources and tools are scarce. While\nlarge language models (LLMs) have shown promise in language-related tasks,\ntraditional fine-tuning approaches are often infeasible given the size of the\nmodels. In this paper, we propose a Parameter Efficient Fine tuning (PEFT)\nbased solution for hate speech detection and target identification. We evaluate\nmultiple LLMs on the Devanagari dataset provided by (Thapa et al., 2025), which\ncontains annotated instances in 2 languages - Hindi and Nepali. The results\ndemonstrate the efficacy of our approach in handling Devanagari-scripted\ncontent.", "published": "2024-12-22 18:38:24", "link": "http://arxiv.org/abs/2412.17131v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sim911: Towards Effective and Equitable 9-1-1 Dispatcher Training with\n  an LLM-Enabled Simulation", "abstract": "Emergency response services are vital for enhancing public safety by\nsafeguarding the environment, property, and human lives. As frontline members\nof these services, 9-1-1 dispatchers have a direct impact on response times and\nthe overall effectiveness of emergency operations. However, traditional\ndispatcher training methods, which rely on role-playing by experienced\npersonnel, are labor-intensive, time-consuming, and often neglect the specific\nneeds of underserved communities. To address these challenges, we introduce\nSim911, the first training simulation for 9-1-1 dispatchers powered by Large\nLanguage Models (LLMs). Sim911 enhances training through three key technical\ninnovations: (1) knowledge construction, which utilizes archived 9-1-1 call\ndata to generate simulations that closely mirror real-world scenarios; (2)\ncontext-aware controlled generation, which employs dynamic prompts and vector\nbases to ensure that LLM behavior aligns with training objectives; and (3)\nvalidation with looped correction, which filters out low-quality responses and\nrefines the system performance.", "published": "2024-12-22 03:43:51", "link": "http://arxiv.org/abs/2412.16844v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GME: Improving Universal Multimodal Retrieval by Multimodal LLMs", "abstract": "Universal Multimodal Retrieval (UMR) aims to enable search across various\nmodalities using a unified model, where queries and candidates can consist of\npure text, images, or a combination of both. Previous work has attempted to\nadopt multimodal large language models (MLLMs) to realize UMR using only text\ndata. However, our preliminary experiments demonstrate that more diverse\nmultimodal training data can further unlock the potential of MLLMs. Despite its\neffectiveness, the existing multimodal training data is highly imbalanced in\nterms of modality, which motivates us to develop a training data synthesis\npipeline and construct a large-scale, high-quality fused-modal training\ndataset. Based on the synthetic training data, we develop the General\nMultimodal Embedder (GME), an MLLM-based dense retriever designed for UMR.\nFurthermore, we construct a comprehensive UMR Benchmark (UMRB) to evaluate the\neffectiveness of our approach. Experimental results show that our method\nachieves state-of-the-art performance among existing UMR methods. Last, we\nprovide in-depth analyses of model scaling and training strategies, and perform\nablation studies on both the model and synthetic data.", "published": "2024-12-22 04:40:24", "link": "http://arxiv.org/abs/2412.16855v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "PsychAdapter: Adapting LLM Transformers to Reflect Traits, Personality\n  and Mental Health", "abstract": "Artificial intelligence-based language generators are now a part of most\npeople's lives. However, by default, they tend to generate \"average\" language\nwithout reflecting the ways in which people differ. Here, we propose a\nlightweight modification to the standard language model transformer\narchitecture - \"PsychAdapter\" - that uses empirically derived trait-language\npatterns to generate natural language for specified personality, demographic,\nand mental health characteristics (with or without prompting). We applied\nPsychAdapters to modify OpenAI's GPT-2, Google's Gemma, and Meta's Llama 3 and\nfound generated text to reflect the desired traits. For example, expert raters\nevaluated PsychAdapter's generated text output and found it matched intended\ntrait levels with 87.3% average accuracy for Big Five personalities, and 96.7%\nfor depression and life satisfaction. PsychAdapter is a novel method to\nintroduce psychological behavior patterns into language models at the\nfoundation level, independent of prompting, by influencing every transformer\nlayer. This approach can create chatbots with specific personality profiles,\nclinical training tools that mirror language associated with psychological\nconditionals, and machine translations that match an authors reading or\neducation level without taking up LLM context windows. PsychAdapter also allows\nfor the exploration psychological constructs through natural language\nexpression, extending the natural language processing toolkit to study human\npsychology.", "published": "2024-12-22 06:22:40", "link": "http://arxiv.org/abs/2412.16882v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Speech-Based Depression Prediction Using Encoder-Weight-Only Transfer\n  Learning and a Large Corpus", "abstract": "Speech-based algorithms have gained interest for the management of behavioral\nhealth conditions such as depression. We explore a speech-based transfer\nlearning approach that uses a lightweight encoder and that transfers only the\nencoder weights, enabling a simplified run-time model. Our study uses a large\ndata set containing roughly two orders of magnitude more speakers and sessions\nthan used in prior work. The large data set enables reliable estimation of\nimprovement from transfer learning. Results for the prediction of PHQ-8 labels\nshow up to 27% relative performance gains for binary classification; these\ngains are statistically significant with a p-value close to zero. Improvements\nwere also found for regression. Additionally, the gain from transfer learning\ndoes not appear to require strong source task performance. Results suggest that\nthis approach is flexible and offers promise for efficient implementation.", "published": "2024-12-22 07:21:51", "link": "http://arxiv.org/abs/2412.16900v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Prompting Large Language Models with Rationale Heuristics for\n  Knowledge-based Visual Question Answering", "abstract": "Recently, Large Language Models (LLMs) have been used for knowledge-based\nVisual Question Answering (VQA). Despite the encouraging results of previous\nstudies, prior methods prompt LLMs to predict answers directly, neglecting\nintermediate thought processes. We argue that prior methods do not sufficiently\nactivate the capacities of LLMs. We propose a framework called PLRH that\nPrompts LLMs with Rationale Heuristics for knowledge-based VQA. The PLRH\nprompts LLMs with Chain of Thought (CoT) to generate rationale heuristics,\ni.e., intermediate thought processes, and then leverages the rationale\nheuristics to inspire LLMs to predict answers. Experiments show that our\napproach outperforms the existing baselines by more than 2.2 and 2.1 on OK-VQA\nand A-OKVQA, respectively.", "published": "2024-12-22 09:14:35", "link": "http://arxiv.org/abs/2412.16936v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "System-2 Mathematical Reasoning via Enriched Instruction Tuning", "abstract": "Solving complex mathematical problems via system-2 reasoning is a natural\nhuman skill, yet it remains a significant challenge for current large language\nmodels (LLMs). We identify the scarcity of deliberate multi-step reasoning data\nas a primary limiting factor. To this end, we introduce Enriched Instruction\nTuning (EIT), a method that enriches existing human-annotated mathematical\ndatasets by synergizing human and AI feedback to create fine-grained reasoning\ntrajectories. These datasets are then used to fine-tune open-source LLMs,\nenhancing their mathematical reasoning abilities without reliance on any\nsymbolic verification program. Concretely, EIT is composed of two critical\nsteps: Enriching with Reasoning Plan (ERP) and Enriching with Reasoning Step\n(ERS). The former generates a high-level plan that breaks down complex\ninstructions into a sequence of simpler objectives, while ERS fills in\nreasoning contexts often overlooked by human annotators, creating a smoother\nreasoning trajectory for LLM fine-tuning. Unlike existing CoT prompting methods\nthat generate reasoning chains only depending on LLM's internal knowledge, our\nmethod leverages human-annotated initial answers as ``meta-knowledge'' to help\nLLMs generate more detailed and precise reasoning processes, leading to a more\ntrustworthy LLM expert for complex mathematical problems. In experiments, EIT\nachieves an accuracy of 84.1% on GSM8K and 32.5% on MATH, surpassing\nstate-of-the-art fine-tuning and prompting methods, and even matching the\nperformance of tool-augmented methods.", "published": "2024-12-22 10:49:27", "link": "http://arxiv.org/abs/2412.16964v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Cannot or Should Not? Automatic Analysis of Refusal Composition in\n  IFT/RLHF Datasets and Refusal Behavior of Black-Box LLMs", "abstract": "Refusals - instances where large language models (LLMs) decline or fail to\nfully execute user instructions - are crucial for both AI safety and AI\ncapabilities and the reduction of hallucinations in particular. These behaviors\nare learned during post-training, especially in instruction fine-tuning (IFT)\nand reinforcement learning from human feedback (RLHF). However, existing\ntaxonomies and evaluation datasets for refusals are inadequate, often focusing\nsolely on should-not-related (instead of cannot-related) categories, and\nlacking tools for auditing refusal content in black-box LLM outputs.\n  We present a comprehensive framework for classifying LLM refusals: (a) a\ntaxonomy of 16 refusal categories, (b) a human-annotated dataset of over 8,600\ninstances from publicly available IFT and RLHF datasets, (c) a synthetic\ndataset with 8,000 examples for each refusal category, and (d) classifiers\ntrained for refusal classification.\n  Our work enables precise auditing of refusal behaviors in black-box LLMs and\nautomatic analyses of refusal patterns in large IFT and RLHF datasets. This\nfacilitates the strategic adjustment of LLM refusals, contributing to the\ndevelopment of more safe and reliable LLMs.", "published": "2024-12-22 11:16:53", "link": "http://arxiv.org/abs/2412.16974v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "On Fusing ChatGPT and Ensemble Learning in Discon-tinuous Named Entity\n  Recognition in Health Corpora", "abstract": "Named Entity Recognition has traditionally been a key task in natural\nlanguage processing, aiming to identify and extract important terms from\nunstructured text data. However, a notable challenge for contemporary\ndeep-learning NER models has been identifying discontinuous entities, which are\noften fragmented within the text. To date, methods to address Discontinuous\nNamed Entity Recognition have not been explored using ensemble learning to the\nbest of our knowledge. Furthermore, the rise of large language models, such as\nChatGPT in recent years, has shown significant effectiveness across many NLP\ntasks. Most existing approaches, however, have primarily utilized ChatGPT as a\nproblem-solving tool rather than exploring its potential as an integrative\nelement within ensemble learning algorithms. In this study, we investigated the\nintegration of ChatGPT as an arbitrator within an ensemble method, aiming to\nenhance performance on DNER tasks. Our method combines five state-of-the-art\nNER models with ChatGPT using custom prompt engineering to assess the\nrobustness and generalization capabilities of the ensemble algorithm. We\nconducted experiments on three benchmark medical datasets, comparing our method\nagainst the five SOTA models, individual applications of GPT-3.5 and GPT-4, and\na voting ensemble method. The results indicate that our proposed fusion of\nChatGPT with the ensemble learning algorithm outperforms the SOTA results in\nthe CADEC, ShARe13, and ShARe14 datasets, showcasing its potential to enhance\nNLP applications in the healthcare domain.", "published": "2024-12-22 11:26:49", "link": "http://arxiv.org/abs/2412.16976v1", "categories": ["cs.CL", "cs.AI", "I.2.7; J.3"], "primary_category": "cs.CL"}
{"title": "A Reality Check on Context Utilisation for Retrieval-Augmented\n  Generation", "abstract": "Retrieval-augmented generation (RAG) helps address the limitations of the\nparametric knowledge embedded within a language model (LM). However,\ninvestigations of how LMs utilise retrieved information of varying complexity\nin real-world scenarios have been limited to synthetic contexts. We introduce\nDRUID (Dataset of Retrieved Unreliable, Insufficient and\nDifficult-to-understand contexts) with real-world queries and contexts manually\nannotated for stance. The dataset is based on the prototypical task of\nautomated claim verification, for which automated retrieval of real-world\nevidence is crucial. We compare DRUID to synthetic datasets (CounterFact,\nConflictQA) and find that artificial datasets often fail to represent the\ncomplex and diverse real-world context settings. We show that synthetic\ndatasets exaggerate context characteristics rare in real retrieved data, which\nleads to inflated context utilisation results, as measured by our novel ACU\nscore. Moreover, while previous work has mainly focused on singleton context\ncharacteristics to explain context utilisation, correlations between singleton\ncontext properties and ACU on DRUID are surprisingly small compared to other\nproperties related to context source. Overall, our work underscores the need\nfor real-world aligned context utilisation studies to represent and improve\nperformance in real-world RAG settings.", "published": "2024-12-22 14:16:38", "link": "http://arxiv.org/abs/2412.17031v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The HalluRAG Dataset: Detecting Closed-Domain Hallucinations in RAG\n  Applications Using an LLM's Internal States", "abstract": "Detecting hallucinations in large language models (LLMs) is critical for\nenhancing their reliability and trustworthiness. Most research focuses on\nhallucinations as deviations from information seen during training. However,\nthe opaque nature of an LLM's parametric knowledge complicates the\nunderstanding of why generated texts appear ungrounded: The LLM might not have\npicked up the necessary knowledge from large and often inaccessible datasets,\nor the information might have been changed or contradicted during further\ntraining. Our focus is on hallucinations involving information not used in\ntraining, which we determine by using recency to ensure the information emerged\nafter a cut-off date. This study investigates these hallucinations by detecting\nthem at sentence level using different internal states of various LLMs. We\npresent HalluRAG, a dataset designed to train classifiers on these\nhallucinations. Depending on the model and quantization, MLPs trained on\nHalluRAG detect hallucinations with test accuracies ranging up to 75 %, with\nMistral-7B-Instruct-v0.1 achieving the highest test accuracies. Our results\nshow that IAVs detect hallucinations as effectively as CEVs and reveal that\nanswerable and unanswerable prompts are encoded differently as separate\nclassifiers for these categories improved accuracy. However, HalluRAG showed\nsome limited generalizability, advocating for more diversity in datasets on\nhallucinations.", "published": "2024-12-22 15:08:24", "link": "http://arxiv.org/abs/2412.17056v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Iterative NLP Query Refinement for Enhancing Domain-Specific Information\n  Retrieval: A Case Study in Career Services", "abstract": "Retrieving semantically relevant documents in niche domains poses significant\nchallenges for traditional TF-IDF-based systems, often resulting in low\nsimilarity scores and suboptimal retrieval performance. This paper addresses\nthese challenges by introducing an iterative and semi-automated query\nrefinement methodology tailored to Humber College's career services webpages.\nInitially, generic queries related to interview preparation yield low\ntop-document similarities (approximately 0.2--0.3). To enhance retrieval\neffectiveness, we implement a two-fold approach: first, domain-aware query\nrefinement by incorporating specialized terms such as\nresources-online-learning, student-online-services, and career-advising;\nsecond, the integration of structured educational descriptors like \"online\nresume and interview improvement tools.\" Additionally, we automate the\nextraction of domain-specific keywords from top-ranked documents to suggest\nrelevant terms for query expansion. Through experiments conducted on five\nbaseline queries, our semi-automated iterative refinement process elevates the\naverage top similarity score from approximately 0.18 to 0.42, marking a\nsubstantial improvement in retrieval performance. The implementation details,\nincluding reproducible code and experimental setups, are made available in our\nGitHub repositories \\url{https://github.com/Elipei88/HumberChatbotBackend} and\n\\url{https://github.com/Nisarg851/HumberChatbot}. We also discuss the\nlimitations of our approach and propose future directions, including the\nintegration of advanced neural retrieval models.", "published": "2024-12-22 15:57:35", "link": "http://arxiv.org/abs/2412.17075v1", "categories": ["cs.IR", "cs.CL", "I.7.3; H.3.3"], "primary_category": "cs.IR"}
{"title": "SAIL: Sample-Centric In-Context Learning for Document Information\n  Extraction", "abstract": "Document Information Extraction (DIE) aims to extract structured information\nfrom Visually Rich Documents (VRDs). Previous full-training approaches have\ndemonstrated strong performance but may struggle with generalization to unseen\ndata. In contrast, training-free methods leverage powerful pre-trained models\nlike Large Language Models (LLMs) to address various downstream tasks with only\na few examples. Nonetheless, training-free methods for DIE encounter two\nprimary challenges: (1) understanding the complex relationship between layout\nand textual elements in VRDs, and (2) providing accurate guidance to\npre-trained models. To address these challenges, we propose Sample-centric\nIn-context Learning (SAIL) for DIE. SAIL introduces a fine-grained entity-level\ntextual similarity to facilitate in-depth text analysis by LLMs and\nincorporates layout similarity to enhance the analysis of layouts in VRDs.\nAdditionally, SAIL formulates a unified In-Context Learning (ICL) prompt\ntemplate for various sample-centric examples, enabling tailored prompts that\ndeliver precise guidance to pre-trained models for each sample. Extensive\nexperiments on FUNSD, CORD, and SROIE benchmarks with various base models\n(e.g., LLMs) indicate that our method outperforms training-free baselines, even\ncloser to the full-training methods. The results show the superiority and\ngeneralization of our method.", "published": "2024-12-22 16:58:59", "link": "http://arxiv.org/abs/2412.17092v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluating LLM Reasoning in the Operations Research Domain with ORQA", "abstract": "In this paper, we introduce and apply Operations Research Question Answering\n(ORQA), a new benchmark designed to assess the generalization capabilities of\nLarge Language Models (LLMs) in the specialized technical domain of Operations\nResearch (OR). This benchmark evaluates whether LLMs can emulate the knowledge\nand reasoning skills of OR experts when confronted with diverse and complex\noptimization problems. The dataset, developed by OR experts, features\nreal-world optimization problems that demand multistep reasoning to construct\ntheir mathematical models. Our evaluations of various open source LLMs, such as\nLLaMA 3.1, DeepSeek, and Mixtral, reveal their modest performance, highlighting\na gap in their ability to generalize to specialized technical domains. This\nwork contributes to the ongoing discourse on LLMs generalization capabilities,\noffering valuable insights for future research in this area. The dataset and\nevaluation code are publicly available.", "published": "2024-12-22 09:10:34", "link": "http://arxiv.org/abs/2412.17874v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Autoregressive Speech Synthesis with Next-Distribution Prediction", "abstract": "We introduce KALL-E, a novel autoregressive (AR) language modeling approach\nwith next-distribution prediction for text-to-speech (TTS) synthesis. Unlike\nexisting methods, KALL-E directly models and predicts the continuous speech\ndistribution conditioned on text without relying on VAE- or diffusion-based\ncomponents. Specifically, we use WaveVAE to extract continuous speech\ndistributions from waveforms instead of using discrete speech tokens. A single\nAR language model predicts these continuous speech distributions from text,\nwith a Kullback-Leibler divergence loss as the constraint. Experimental results\nshow that KALL-E outperforms open-source implementations of YourTTS, VALL-E,\nNaturalSpeech 2, and CosyVoice in terms of naturalness and speaker similarity\nin zero-shot TTS scenarios. Moreover, KALL-E demonstrates exceptional zero-shot\ncapabilities in emotion and accent cloning. Importantly, KALL-E presents a more\nstraightforward and effective paradigm for using continuous speech\nrepresentations in TTS. Audio samples are available at:\n\\url{https://zxf-icpc.github.io/kalle/}.", "published": "2024-12-22 04:03:24", "link": "http://arxiv.org/abs/2412.16846v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Revisiting In-Context Learning with Long Context Language Models", "abstract": "In-Context Learning (ICL) is a technique by which language models make\npredictions based on examples provided in their input context. Previously,\ntheir context window size imposed a limit on the number of examples that can be\nshown, making example selection techniques crucial for identifying the\nmaximally effective set of examples. However, the recent advent of Long Context\nLanguage Models (LCLMs) has significantly increased the number of examples that\ncan be included in context, raising an important question of whether ICL\nperformance in a many-shot regime is still sensitive to the method of sample\nselection. To answer this, we revisit these approaches in the context of LCLMs\nthrough extensive experiments on 18 datasets spanning 4 tasks. Surprisingly, we\nobserve that sophisticated example selection techniques do not yield\nsignificant improvements over a simple random sample selection method. Instead,\nwe find that the advent of LCLMs has fundamentally shifted the challenge of ICL\nfrom that of selecting the most effective examples to that of collecting\nsufficient examples to fill the context window. Specifically, in certain\ndatasets, including all available examples does not fully utilize the context\nwindow; however, by augmenting the examples in context with a simple data\naugmentation approach, we substantially improve ICL performance by 5%.", "published": "2024-12-22 08:55:19", "link": "http://arxiv.org/abs/2412.16926v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards a Unified Paradigm: Integrating Recommendation Systems as a New\n  Language in Large Models", "abstract": "This paper explores the use of Large Language Models (LLMs) for sequential\nrecommendation, which predicts users' future interactions based on their past\nbehavior. We introduce a new concept, \"Integrating Recommendation Systems as a\nNew Language in Large Models\" (RSLLM), which combines the strengths of\ntraditional recommenders and LLMs. RSLLM uses a unique prompting method that\ncombines ID-based item embeddings from conventional recommendation models with\ntextual item features. It treats users' sequential behaviors as a distinct\nlanguage and aligns the ID embeddings with the LLM's input space using a\nprojector. We also propose a two-stage LLM fine-tuning framework that refines a\npretrained LLM using a combination of two contrastive losses and a language\nmodeling loss. The LLM is first fine-tuned using text-only prompts, followed by\ntarget domain fine-tuning with unified prompts. This trains the model to\nincorporate behavioral knowledge from the traditional sequential recommender\ninto the LLM. Our empirical results validate the effectiveness of our proposed\nframework.", "published": "2024-12-22 09:08:46", "link": "http://arxiv.org/abs/2412.16933v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Why Do Speech Language Models Fail to Generate Semantically Coherent\n  Outputs? A Modality Evolving Perspective", "abstract": "Although text-based large language models exhibit human-level writing ability\nand remarkable intelligence, speech language models (SLMs) still struggle to\ngenerate semantically coherent outputs. There are several potential reasons for\nthis performance degradation: (A) speech tokens mainly provide phonetic\ninformation rather than semantic information, (B) the length of speech\nsequences is much longer than that of text sequences, and (C) paralinguistic\ninformation, such as prosody, introduces additional complexity and variability.\nIn this paper, we explore the influence of three key factors separately by\ntransiting the modality from text to speech in an evolving manner. Our findings\nreveal that the impact of the three factors varies. Factor A has a relatively\nminor impact, factor B influences syntactical and semantic modeling more\nobviously, and factor C exerts the most significant impact, particularly in the\nbasic lexical modeling. Based on these findings, we provide insights into the\nunique challenges of training SLMs and highlight pathways to develop more\neffective end-to-end SLMs.", "published": "2024-12-22 14:59:19", "link": "http://arxiv.org/abs/2412.17048v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Modular Conversational Agents for Surveys and Interviews", "abstract": "Surveys and interviews are widely used for collecting insights on emerging or\nhypothetical scenarios. Traditional human-led methods often face challenges\nrelated to cost, scalability, and consistency. Recently, various domains have\nbegun to explore the use of conversational agents (chatbots) powered by\ngenerative artificial intelligence (AI) technologies. However, considering\ndecisions in transportation investments and policies often carry significant\npublic and environmental stakes, surveys and interviews face unique challenges\nin integrating AI agents, underscoring the need for a rigorous,\nresource-efficient approach that enhances participant engagement and ensures\nprivacy. This paper addresses this gap by introducing a modular approach and\nits resulting parameterized process for designing AI agents. We detail the\nsystem architecture, integrating engineered prompts, specialized knowledge\nbases, and customizable, goal-oriented conversational logic. We demonstrate the\nadaptability, generalizability, and efficacy of our modular approach through\nthree empirical studies: (1) travel preference surveys, highlighting\nconditional logic and multimodal (voice, text, and image generation)\ncapabilities; (2) public opinion elicitation on a newly constructed, novel\ninfrastructure project, showcasing question customization and multilingual\n(English and French) capabilities; and (3) expert consultation about the impact\nof technologies on future transportation systems, highlighting real-time,\nclarification request capabilities for open-ended questions, resilience in\nhandling erratic inputs, and efficient transcript postprocessing. The results\nsuggest that the AI agent increases completion rates and response quality.\nFurthermore, the modular approach demonstrates controllability, flexibility,\nand robustness while addressing key ethical, privacy, security, and token\nconsumption concerns.", "published": "2024-12-22 15:00:16", "link": "http://arxiv.org/abs/2412.17049v2", "categories": ["cs.HC", "cs.CL", "cs.CY", "cs.MM"], "primary_category": "cs.HC"}
{"title": "Lies, Damned Lies, and Distributional Language Statistics: Persuasion\n  and Deception with Large Language Models", "abstract": "Large Language Models (LLMs) can generate content that is as persuasive as\nhuman-written text and appear capable of selectively producing deceptive\noutputs. These capabilities raise concerns about potential misuse and\nunintended consequences as these systems become more widely deployed. This\nreview synthesizes recent empirical work examining LLMs' capacity and\nproclivity for persuasion and deception, analyzes theoretical risks that could\narise from these capabilities, and evaluates proposed mitigations. While\ncurrent persuasive effects are relatively small, various mechanisms could\nincrease their impact, including fine-tuning, multimodality, and social\nfactors. We outline key open questions for future research, including how\npersuasive AI systems might become, whether truth enjoys an inherent advantage\nover falsehoods, and how effective different mitigation strategies may be in\npractice.", "published": "2024-12-22 18:34:10", "link": "http://arxiv.org/abs/2412.17128v1", "categories": ["cs.CL", "cs.CY", "cs.HC", "68T50", "K.4.0; I.2.7; H.5.2"], "primary_category": "cs.CL"}
{"title": "Analysis of Speech Temporal Dynamics in the Context of Speaker\n  Verification and Voice Anonymization", "abstract": "In this paper, we investigate the impact of speech temporal dynamics in\napplication to automatic speaker verification and speaker voice anonymization\ntasks. We propose several metrics to perform automatic speaker verification\nbased only on phoneme durations. Experimental results demonstrate that phoneme\ndurations leak some speaker information and can reveal speaker identity from\nboth original and anonymized speech. Thus, this work emphasizes the importance\nof taking into account the speaker's speech rate and, more importantly, the\nspeaker's phonetic duration characteristics, as well as the need to modify them\nin order to develop anonymization systems with strong privacy protection\ncapacity.", "published": "2024-12-22 21:18:08", "link": "http://arxiv.org/abs/2412.17164v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Joint Knowledge Editing for Information Enrichment and Probability\n  Promotion", "abstract": "Knowledge stored in large language models requires timely updates to reflect\nthe dynamic nature of real-world information. To update the knowledge, most\nknowledge editing methods focus on the low layers, since recent probes into the\nknowledge recall process reveal that the answer information is enriched in low\nlayers. However, these probes only and could only reveal critical recall stages\nfor the original answers, while the goal of editing is to rectify model's\nprediction for the target answers. This inconsistency indicates that both the\nprobe approaches and the associated editing methods are deficient. To mitigate\nthe inconsistency and identify critical editing regions, we propose a\ncontrast-based probe approach, and locate two crucial stages where the model\nbehavior diverges between the original and target answers: Information\nEnrichment in low layers and Probability Promotion in high layers. Building\nupon the insights, we develop the Joint knowledge Editing for information\nEnrichment and probability Promotion (JEEP) method, which jointly edits both\nthe low and high layers to modify the two critical recall stages. Considering\nthe mutual interference and growing forgetting due to dual modifications, JEEP\nis designed to ensure that updates to distinct regions share the same\nobjectives and are complementary. We rigorously evaluate JEEP by editing up to\nthousands of facts on various models, i.e., GPT-J (6B) and LLaMA (7B), and\naddressing diverse editing objectives, i.e., adding factual and counterfactual\nknowledge. In all tested scenarios, JEEP achieves best performances, validating\nthe effectiveness of the revealings of our probe approach and the designs of\nour editing method. Our code and data are available at\nhttps://github.com/Eric8932/JEEP.", "published": "2024-12-22 03:16:49", "link": "http://arxiv.org/abs/2412.17872v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Quantifying Public Response to COVID-19 Events: Introducing the\n  Community Sentiment and Engagement Index", "abstract": "This study introduces the Community Sentiment and Engagement Index (CSEI),\ndeveloped to capture nuanced public sentiment and engagement variations on\nsocial media, particularly in response to major events related to COVID-19.\nConstructed with diverse sentiment indicators, CSEI integrates features like\nengagement, daily post count, compound sentiment, fine-grain sentiments (fear,\nsurprise, joy, sadness, anger, disgust, and neutral), readability,\noffensiveness, and domain diversity. Each component is systematically weighted\nthrough a multi-step Principal Component Analysis (PCA)-based framework,\nprioritizing features according to their variance contributions across temporal\nsentiment shifts. This approach dynamically adjusts component importance,\nenabling CSEI to precisely capture high-sensitivity shifts in public sentiment.\nThe development of CSEI showed statistically significant correlations with its\nconstituent features, underscoring internal consistency and sensitivity to\nspecific sentiment dimensions. CSEI's responsiveness was validated using a\ndataset of 4,510,178 Reddit posts about COVID-19. The analysis focused on 15\nmajor events, including the WHO's declaration of COVID-19 as a pandemic, the\nfirst reported cases of COVID-19 across different countries, national\nlockdowns, vaccine developments, and crucial public health measures. Cumulative\nchanges in CSEI revealed prominent peaks and valleys aligned with these events,\nindicating significant patterns in public sentiment across different phases of\nthe pandemic. Pearson correlation analysis further confirmed a statistically\nsignificant relationship between CSEI daily fluctuations and these events (p =\n0.0428), highlighting the capacity of CSEI to infer and interpret shifts in\npublic sentiment and engagement in response to major events related to\nCOVID-19.", "published": "2024-12-22 08:52:12", "link": "http://arxiv.org/abs/2412.16925v1", "categories": ["cs.SI", "cs.AI", "cs.CL", "cs.CY", "cs.LG", "I.2.7; I.2.8; I.5.4; K.4.2; H.2.8; I.2.6"], "primary_category": "cs.SI"}
{"title": "A Multi-AI Agent System for Autonomous Optimization of Agentic AI\n  Solutions via Iterative Refinement and LLM-Driven Feedback Loops", "abstract": "Agentic AI systems use specialized agents to handle tasks within complex\nworkflows, enabling automation and efficiency. However, optimizing these\nsystems often requires labor-intensive, manual adjustments to refine roles,\ntasks, and interactions. This paper introduces a framework for autonomously\noptimizing Agentic AI solutions across industries, such as NLP-driven\nenterprise applications. The system employs agents for Refinement, Execution,\nEvaluation, Modification, and Documentation, leveraging iterative feedback\nloops powered by an LLM (Llama 3.2-3B). The framework achieves optimal\nperformance without human input by autonomously generating and testing\nhypotheses to improve system configurations. This approach enhances scalability\nand adaptability, offering a robust solution for real-world applications in\ndynamic environments. Case studies across diverse domains illustrate the\ntransformative impact of this framework, showcasing significant improvements in\noutput quality, relevance, and actionability. All data for these case studies,\nincluding original and evolved agent codes, along with their outputs, are here:\nhttps://anonymous.4open.science/r/evolver-1D11/", "published": "2024-12-22 20:08:04", "link": "http://arxiv.org/abs/2412.17149v1", "categories": ["cs.CL", "cs.AI", "cs.ET", "cs.MA", "cs.NE"], "primary_category": "cs.CL"}
{"title": "COVID-19 on YouTube: A Data-Driven Analysis of Sentiment, Toxicity, and\n  Content Recommendations", "abstract": "This study presents a data-driven analysis of COVID-19 discourse on YouTube,\nexamining the sentiment, toxicity, and thematic patterns of video content\npublished between January 2023 and October 2024. The analysis involved applying\nadvanced natural language processing (NLP) techniques: sentiment analysis with\nVADER, toxicity detection with Detoxify, and topic modeling using Latent\nDirichlet Allocation (LDA). The sentiment analysis revealed that 49.32% of\nvideo descriptions were positive, 36.63% were neutral, and 14.05% were\nnegative, indicating a generally informative and supportive tone in\npandemic-related content. Toxicity analysis identified only 0.91% of content as\ntoxic, suggesting minimal exposure to toxic content. Topic modeling revealed\ntwo main themes, with 66.74% of the videos covering general health information\nand pandemic-related impacts and 33.26% focused on news and real-time updates,\nhighlighting the dual informational role of YouTube. A recommendation system\nwas also developed using TF-IDF vectorization and cosine similarity, refined by\nsentiment, toxicity, and topic filters to ensure relevant and context-aligned\nvideo recommendations. This system achieved 69% aggregate coverage, with\nmonthly coverage rates consistently above 85%, demonstrating robust performance\nand adaptability over time. Evaluation across recommendation sizes showed\ncoverage reaching 69% for five video recommendations and 79% for ten video\nrecommendations per video. In summary, this work presents a framework for\nunderstanding COVID-19 discourse on YouTube and a recommendation system that\nsupports user engagement while promoting responsible and relevant content\nrelated to COVID-19.", "published": "2024-12-22 22:43:36", "link": "http://arxiv.org/abs/2412.17180v1", "categories": ["cs.SI", "cs.AI", "cs.CL", "cs.CY", "cs.LG", "I.2.7; I.2.8; I.5.4; K.4.2; H.2.8; I.2.6"], "primary_category": "cs.SI"}
{"title": "Bridging Auditory Perception and Language Comprehension through\n  MEG-Driven Encoding Models", "abstract": "Understanding the neural mechanisms behind auditory and linguistic processing\nis key to advancing cognitive neuroscience. In this study, we use\nMagnetoencephalography (MEG) data to analyze brain responses to spoken language\nstimuli. We develop two distinct encoding models: an audio-to-MEG encoder,\nwhich uses time-frequency decompositions (TFD) and wav2vec2 latent space\nrepresentations, and a text-to-MEG encoder, which leverages CLIP and GPT-2\nembeddings. Both models successfully predict neural activity, demonstrating\nsignificant correlations between estimated and observed MEG signals. However,\nthe text-to-MEG model outperforms the audio-based model, achieving higher\nPearson Correlation (PC) score. Spatially, we identify that auditory-based\nembeddings (TFD and wav2vec2) predominantly activate lateral temporal regions,\nwhich are responsible for primary auditory processing and the integration of\nauditory signals. In contrast, textual embeddings (CLIP and GPT-2) primarily\nengage the frontal cortex, particularly Broca's area, which is associated with\nhigher-order language processing, including semantic integration and language\nproduction, especially in the 8-30 Hz frequency range. The strong involvement\nof these regions suggests that auditory stimuli are processed through more\ndirect sensory pathways, while linguistic information is encoded via networks\nthat integrate meaning and cognitive control. Our results reveal distinct\nneural pathways for auditory and linguistic information processing, with higher\nencoding accuracy for text representations in the frontal regions. These\ninsights refine our understanding of the brain's functional architecture in\nprocessing auditory and textual information, offering quantitative advancements\nin the modelling of neural responses to complex language stimuli.", "published": "2024-12-22 19:41:54", "link": "http://arxiv.org/abs/2501.03246v1", "categories": ["q-bio.NC", "cs.CL", "cs.LG", "cs.SD", "eess.AS", "eess.SP"], "primary_category": "q-bio.NC"}
{"title": "Time-Graph Frequency Representation with Singular Value Decomposition\n  for Neural Speech Enhancement", "abstract": "Time-frequency (T-F) domain methods for monaural speech enhancement have\nbenefited from the success of deep learning. Recently, focus has been put on\ndesigning two-stream network models to predict amplitude mask and phase\nseparately, or, coupling the amplitude and phase into Cartesian coordinates and\nconstructing real and imaginary pairs. However, most methods suffer from the\nalignment modeling of amplitude and phase (real and imaginary pairs) in a\ntwo-stream network framework, which inevitably incurs performance restrictions.\nIn this paper, we introduce a graph Fourier transform defined with the singular\nvalue decomposition (GFT-SVD), resulting in real-valued time-graph\nrepresentation for neural speech enhancement. This real-valued\nrepresentation-based GFT-SVD provides an ability to align the modeling of\namplitude and phase, leading to avoiding recovering the target speech phase\ninformation. Our findings demonstrate the effects of real-valued time-graph\nrepresentation based on GFT-SVD for neutral speech enhancement. The extensive\nspeech enhancement experiments establish that the combination of GFT-SVD and\nDNN outperforms the combination of GFT with the eigenvector decomposition\n(GFT-EVD) and magnitude estimation UNet, and outperforms the short-time Fourier\ntransform (STFT) and DNN, regarding objective intelligibility and perceptual\nquality. We release our source code at:\nhttps://github.com/Wangfighting0015/GFT\\_project.", "published": "2024-12-22 02:05:21", "link": "http://arxiv.org/abs/2412.16823v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Incremental Disentanglement for Environment-Aware Zero-Shot\n  Text-to-Speech Synthesis", "abstract": "This paper proposes an Incremental Disentanglement-based Environment-Aware\nzero-shot text-to-speech (TTS) method, dubbed IDEA-TTS, that can synthesize\nspeech for unseen speakers while preserving the acoustic characteristics of a\ngiven environment reference speech. IDEA-TTS adopts VITS as the TTS backbone.\nTo effectively disentangle the environment, speaker, and text factors, we\npropose an incremental disentanglement process, where an environment estimator\nis designed to first decompose the environmental spectrogram into an\nenvironment mask and an enhanced spectrogram. The environment mask is then\nprocessed by an environment encoder to extract environment embeddings, while\nthe enhanced spectrogram facilitates the subsequent disentanglement of the\nspeaker and text factors with the condition of the speaker embeddings, which\nare extracted from the environmental speech using a pretrained\nenvironment-robust speaker encoder. Finally, both the speaker and environment\nembeddings are conditioned into the decoder for environment-aware speech\ngeneration. Experimental results demonstrate that IDEA-TTS achieves superior\nperformance in the environment-aware TTS task, excelling in speech quality,\nspeaker similarity, and environmental similarity. Additionally, IDEA-TTS is\nalso capable of the acoustic environment conversion task and achieves\nstate-of-the-art performance.", "published": "2024-12-22 11:26:58", "link": "http://arxiv.org/abs/2412.16977v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Tandem spoofing-robust automatic speaker verification based on\n  time-domain embeddings", "abstract": "Spoofing-robust automatic speaker verification (SASV) systems are a crucial\ntechnology for the protection against spoofed speech. In this study, we focus\non logical access attacks and introduce a novel approach to SASV tasks. A novel\nrepresentation of genuine and spoofed speech is employed, based on the\nprobability mass function (PMF) of waveform amplitudes in the time domain. This\nmethodology generates novel time embeddings derived from the PMF of selected\ngroups within the training set. This paper highlights the role of gender\nsegregation and its positive impact on performance. We propose a countermeasure\n(CM) system that employs time-domain embeddings derived from the PMF of spoofed\nand genuine speech, as well as gender recognition based on male and female\ntime-based embeddings. The method exhibits notable gender recognition\ncapabilities, with mismatch rates of 0.94% and 1.79% for males and females,\nrespectively. The male and female CM systems achieve an equal error rate (EER)\nof 8.67% and 10.12%, respectively. By integrating this approach with\ntraditional speaker verification systems, we demonstrate improved\ngeneralization ability and tandem detection cost function evaluation using the\nASVspoof2019 challenge database. Furthermore, we investigate the impact of\nfusing the time embedding approach with traditional CM and illustrate how this\nfusion enhances generalization in SASV architectures.", "published": "2024-12-22 18:41:53", "link": "http://arxiv.org/abs/2412.17133v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "A Multi-modal Approach to Dysarthria Detection and Severity Assessment\n  Using Speech and Text Information", "abstract": "Automatic detection and severity assessment of dysarthria are crucial for\ndelivering targeted therapeutic interventions to patients. While most existing\nresearch focuses primarily on speech modality, this study introduces a novel\napproach that leverages both speech and text modalities. By employing\ncross-attention mechanism, our method learns the acoustic and linguistic\nsimilarities between speech and text representations. This approach assesses\nspecifically the pronunciation deviations across different severity levels,\nthereby enhancing the accuracy of dysarthric detection and severity assessment.\nAll the experiments have been performed using UA-Speech dysarthric database.\nImproved accuracies of 99.53% and 93.20% in detection, and 98.12% and 51.97%\nfor severity assessment have been achieved when speaker-dependent and\nspeaker-independent, unseen and seen words settings are used. These findings\nsuggest that by integrating text information, which provides a reference\nlinguistic knowledge, a more robust framework has been developed for dysarthric\ndetection and assessment, thereby potentially leading to more effective\ndiagnoses.", "published": "2024-12-22 06:08:35", "link": "http://arxiv.org/abs/2412.16874v3", "categories": ["cs.AI", "eess.AS"], "primary_category": "cs.AI"}
{"title": "Temporal-Frequency State Space Duality: An Efficient Paradigm for Speech\n  Emotion Recognition", "abstract": "Speech Emotion Recognition (SER) plays a critical role in enhancing user\nexperience within human-computer interaction. However, existing methods are\noverwhelmed by temporal domain analysis, overlooking the valuable envelope\nstructures of the frequency domain that are equally important for robust\nemotion recognition. To overcome this limitation, we propose TF-Mamba, a novel\nmulti-domain framework that captures emotional expressions in both temporal and\nfrequency dimensions.Concretely, we propose a temporal-frequency mamba block to\nextract temporal- and frequency-aware emotional features, achieving an optimal\nbalance between computational efficiency and model expressiveness. Besides, we\ndesign a Complex Metric-Distance Triplet (CMDT) loss to enable the model to\ncapture representative emotional clues for SER. Extensive experiments on the\nIEMOCAP and MELD datasets show that TF-Mamba surpasses existing methods in\nterms of model size and latency, providing a more practical solution for future\nSER applications.", "published": "2024-12-22 07:37:13", "link": "http://arxiv.org/abs/2412.16904v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Uncovering the Visual Contribution in Audio-Visual Speech Recognition", "abstract": "Audio-Visual Speech Recognition (AVSR) combines auditory and visual speech\ncues to enhance the accuracy and robustness of speech recognition systems.\nRecent advancements in AVSR have improved performance in noisy environments\ncompared to audio-only counterparts. However, the true extent of the visual\ncontribution, and whether AVSR systems fully exploit the available cues in the\nvisual domain, remains unclear. This paper assesses AVSR systems from a\ndifferent perspective, by considering human speech perception. We use three\nsystems: Auto-AVSR, AVEC and AV-RelScore. We first quantify the visual\ncontribution using effective SNR gains at 0 dB and then investigate the use of\nvisual information in terms of its temporal distribution and word-level\ninformativeness. We show that low WER does not guarantee high SNR gains. Our\nresults suggest that current methods do not fully exploit visual information,\nand we recommend future research to report effective SNR gains alongside WERs.", "published": "2024-12-22 18:34:58", "link": "http://arxiv.org/abs/2412.17129v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SoundLoc3D: Invisible 3D Sound Source Localization and Classification\n  Using a Multimodal RGB-D Acoustic Camera", "abstract": "Accurately localizing 3D sound sources and estimating their semantic labels\n-- where the sources may not be visible, but are assumed to lie on the physical\nsurface of objects in the scene -- have many real applications, including\ndetecting gas leak and machinery malfunction. The audio-visual weak-correlation\nin such setting poses new challenges in deriving innovative methods to answer\nif or how we can use cross-modal information to solve the task. Towards this\nend, we propose to use an acoustic-camera rig consisting of a pinhole RGB-D\ncamera and a coplanar four-channel microphone array~(Mic-Array). By using this\nrig to record audio-visual signals from multiviews, we can use the cross-modal\ncues to estimate the sound sources 3D locations. Specifically, our framework\nSoundLoc3D treats the task as a set prediction problem, each element in the set\ncorresponds to a potential sound source. Given the audio-visual\nweak-correlation, the set representation is initially learned from a single\nview microphone array signal, and then refined by actively incorporating\nphysical surface cues revealed from multiview RGB-D images. We demonstrate the\nefficiency and superiority of SoundLoc3D on large-scale simulated dataset, and\nfurther show its robustness to RGB-D measurement inaccuracy and ambient noise\ninterference.", "published": "2024-12-22 05:04:17", "link": "http://arxiv.org/abs/2412.16861v2", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AV-DTEC: Self-Supervised Audio-Visual Fusion for Drone Trajectory\n  Estimation and Classification", "abstract": "The increasing use of compact UAVs has created significant threats to public\nsafety, while traditional drone detection systems are often bulky and costly.\nTo address these challenges, we propose AV-DTEC, a lightweight self-supervised\naudio-visual fusion-based anti-UAV system. AV-DTEC is trained using\nself-supervised learning with labels generated by LiDAR, and it simultaneously\nlearns audio and visual features through a parallel selective state-space\nmodel. With the learned features, a specially designed plug-and-play\nprimary-auxiliary feature enhancement module integrates visual features into\naudio features for better robustness in cross-lighting conditions. To reduce\nreliance on auxiliary features and align modalities, we propose a\nteacher-student model that adaptively adjusts the weighting of visual features.\nAV-DTEC demonstrates exceptional accuracy and effectiveness in real-world\nmulti-modality data. The code and trained models are publicly accessible on\nGitHub\n  \\url{https://github.com/AmazingDay1/AV-DETC}.", "published": "2024-12-22 08:58:15", "link": "http://arxiv.org/abs/2412.16928v1", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Scalable Speech Enhancement with Dynamic Channel Pruning", "abstract": "Speech Enhancement (SE) is essential for improving productivity in remote\ncollaborative environments. Although deep learning models are highly effective\nat SE, their computational demands make them impractical for embedded systems.\nFurthermore, acoustic conditions can change significantly in terms of\ndifficulty, whereas neural networks are usually static with regard to the\namount of computation performed. To this end, we introduce Dynamic Channel\nPruning to the audio domain for the first time and apply it to a custom\nconvolutional architecture for SE. Our approach works by identifying\nunnecessary convolutional channels at runtime and saving computational\nresources by not computing the activations for these channels and retrieving\ntheir filters. When trained to only use 25% of channels, we save 29.6% of MACs\nwhile only causing a 0.75% drop in PESQ. Thus, DynCP offers a promising path\ntoward deploying larger and more powerful SE solutions on resource-constrained\ndevices.", "published": "2024-12-22 18:21:08", "link": "http://arxiv.org/abs/2412.17121v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "FADA: Fast Diffusion Avatar Synthesis with Mixed-Supervised Multi-CFG\n  Distillation", "abstract": "Diffusion-based audio-driven talking avatar methods have recently gained\nattention for their high-fidelity, vivid, and expressive results. However,\ntheir slow inference speed limits practical applications. Despite the\ndevelopment of various distillation techniques for diffusion models, we found\nthat naive diffusion distillation methods do not yield satisfactory results.\nDistilled models exhibit reduced robustness with open-set input images and a\ndecreased correlation between audio and video compared to teacher models,\nundermining the advantages of diffusion models. To address this, we propose\nFADA (Fast Diffusion Avatar Synthesis with Mixed-Supervised Multi-CFG\nDistillation). We first designed a mixed-supervised loss to leverage data of\nvarying quality and enhance the overall model capability as well as robustness.\nAdditionally, we propose a multi-CFG distillation with learnable tokens to\nutilize the correlation between audio and reference image conditions, reducing\nthe threefold inference runs caused by multi-CFG with acceptable quality\ndegradation. Extensive experiments across multiple datasets show that FADA\ngenerates vivid videos comparable to recent diffusion model-based methods while\nachieving an NFE speedup of 4.17-12.5 times. Demos are available at our webpage\nhttp://fadavatar.github.io.", "published": "2024-12-22 08:19:22", "link": "http://arxiv.org/abs/2412.16915v2", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "InterDance:Reactive 3D Dance Generation with Realistic Duet Interactions", "abstract": "Humans perform a variety of interactive motions, among which duet dance is\none of the most challenging interactions. However, in terms of human motion\ngenerative models, existing works are still unable to generate high-quality\ninteractive motions, especially in the field of duet dance. On the one hand, it\nis due to the lack of large-scale high-quality datasets. On the other hand, it\narises from the incomplete representation of interactive motion and the lack of\nfine-grained optimization of interactions. To address these challenges, we\npropose, InterDance, a large-scale duet dance dataset that significantly\nenhances motion quality, data scale, and the variety of dance genres. Built\nupon this dataset, we propose a new motion representation that can accurately\nand comprehensively describe interactive motion. We further introduce a\ndiffusion-based framework with an interaction refinement guidance strategy to\noptimize the realism of interactions progressively. Extensive experiments\ndemonstrate the effectiveness of our dataset and algorithm.", "published": "2024-12-22 11:53:51", "link": "http://arxiv.org/abs/2412.16982v1", "categories": ["cs.CV", "cs.GR", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
