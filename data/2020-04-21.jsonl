{"title": "Learning Goal-oriented Dialogue Policy with Opposite Agent Awareness", "abstract": "Most existing approaches for goal-oriented dialogue policy learning used\nreinforcement learning, which focuses on the target agent policy and simply\ntreat the opposite agent policy as part of the environment. While in real-world\nscenarios, the behavior of an opposite agent often exhibits certain patterns or\nunderlies hidden policies, which can be inferred and utilized by the target\nagent to facilitate its own decision making. This strategy is common in human\nmental simulation by first imaging a specific action and the probable results\nbefore really acting it. We therefore propose an opposite behavior aware\nframework for policy learning in goal-oriented dialogues. We estimate the\nopposite agent's policy from its behavior and use this estimation to improve\nthe target agent by regarding it as part of the target policy. We evaluate our\nmodel on both cooperative and competitive dialogue tasks, showing superior\nperformance over state-of-the-art baselines.", "published": "2020-04-21 03:13:44", "link": "http://arxiv.org/abs/2004.09731v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Train No Evil: Selective Masking for Task-Guided Pre-Training", "abstract": "Recently, pre-trained language models mostly follow the\npre-train-then-fine-tuning paradigm and have achieved great performance on\nvarious downstream tasks. However, since the pre-training stage is typically\ntask-agnostic and the fine-tuning stage usually suffers from insufficient\nsupervised data, the models cannot always well capture the domain-specific and\ntask-specific patterns. In this paper, we propose a three-stage framework by\nadding a task-guided pre-training stage with selective masking between general\npre-training and fine-tuning. In this stage, the model is trained by masked\nlanguage modeling on in-domain unsupervised data to learn domain-specific\npatterns and we propose a novel selective masking strategy to learn\ntask-specific patterns. Specifically, we design a method to measure the\nimportance of each token in sequences and selectively mask the important\ntokens. Experimental results on two sentiment analysis tasks show that our\nmethod can achieve comparable or even better performance with less than 50% of\ncomputation cost, which indicates our method is both effective and efficient.\nThe source code of this paper can be obtained from\nhttps://github.com/thunlp/SelectiveMasking.", "published": "2020-04-21 03:14:22", "link": "http://arxiv.org/abs/2004.09733v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Keyphrase Generation with Cross-Document Attention", "abstract": "Keyphrase generation aims to produce a set of phrases summarizing the\nessentials of a given document. Conventional methods normally apply an\nencoder-decoder architecture to generate the output keyphrases for an input\ndocument, where they are designed to focus on each current document so they\ninevitably omit crucial corpus-level information carried by other similar\ndocuments, i.e., the cross-document dependency and latent topics. In this\npaper, we propose CDKGen, a Transformer-based keyphrase generator, which\nexpands the Transformer to global attention with cross-document attention\nnetworks to incorporate available documents as references so as to generate\nbetter keyphrases with the guidance of topic information. On top of the\nproposed Transformer + cross-document attention architecture, we also adopt a\ncopy mechanism to enhance our model via selecting appropriate words from\ndocuments to deal with out-of-vocabulary words in keyphrases. Experiment\nresults on five benchmark datasets illustrate the validity and effectiveness of\nour model, which achieves the state-of-the-art performance on all datasets.\nFurther analyses confirm that the proposed model is able to generate keyphrases\nconsistent with references while keeping sufficient diversity. The code of\nCDKGen is available at https://github.com/SVAIGBA/CDKGen.", "published": "2020-04-21 07:58:27", "link": "http://arxiv.org/abs/2004.09800v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Making Monolingual Sentence Embeddings Multilingual using Knowledge\n  Distillation", "abstract": "We present an easy and efficient method to extend existing sentence embedding\nmodels to new languages. This allows to create multilingual versions from\npreviously monolingual models. The training is based on the idea that a\ntranslated sentence should be mapped to the same location in the vector space\nas the original sentence. We use the original (monolingual) model to generate\nsentence embeddings for the source language and then train a new system on\ntranslated sentences to mimic the original model. Compared to other methods for\ntraining multilingual sentence embeddings, this approach has several\nadvantages: It is easy to extend existing models with relatively few samples to\nnew languages, it is easier to ensure desired properties for the vector space,\nand the hardware requirements for training is lower. We demonstrate the\neffectiveness of our approach for 50+ languages from various language families.\nCode to extend sentence embeddings models to more than 400 languages is\npublicly available.", "published": "2020-04-21 08:20:25", "link": "http://arxiv.org/abs/2004.09813v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contextual Neural Machine Translation Improves Translation of Cataphoric\n  Pronouns", "abstract": "The advent of context-aware NMT has resulted in promising improvements in the\noverall translation quality and specifically in the translation of discourse\nphenomena such as pronouns. Previous works have mainly focused on the use of\npast sentences as context with a focus on anaphora translation. In this work,\nwe investigate the effect of future sentences as context by comparing the\nperformance of a contextual NMT model trained with the future context to the\none trained with the past context. Our experiments and evaluation, using\ngeneric and pronoun-focused automatic metrics, show that the use of future\ncontext not only achieves significant improvements over the context-agnostic\nTransformer, but also demonstrates comparable and in some cases improved\nperformance over its counterpart trained on past context. We also perform an\nevaluation on a targeted cataphora test suite and report significant gains over\nthe context-agnostic Transformer in terms of BLEU.", "published": "2020-04-21 10:45:48", "link": "http://arxiv.org/abs/2004.09894v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Relabel the Noise: Joint Extraction of Entities and Relations via\n  Cooperative Multiagents", "abstract": "Distant supervision based methods for entity and relation extraction have\nreceived increasing popularity due to the fact that these methods require light\nhuman annotation efforts. In this paper, we consider the problem of\n\\textit{shifted label distribution}, which is caused by the inconsistency\nbetween the noisy-labeled training set subject to external knowledge graph and\nthe human-annotated test set, and exacerbated by the pipelined\nentity-then-relation extraction manner with noise propagation. We propose a\njoint extraction approach to address this problem by re-labeling noisy\ninstances with a group of cooperative multiagents. To handle noisy instances in\na fine-grained manner, each agent in the cooperative group evaluates the\ninstance by calculating a continuous confidence score from its own perspective;\nTo leverage the correlations between these two extraction tasks, a confidence\nconsensus module is designed to gather the wisdom of all agents and\nre-distribute the noisy training set with confidence-scored labels. Further,\nthe confidences are used to adjust the training losses of extractors.\nExperimental results on two real-world datasets verify the benefits of\nre-labeling noisy instance, and show that the proposed model significantly\noutperforms the state-of-the-art entity and relation extraction methods.", "published": "2020-04-21 12:03:04", "link": "http://arxiv.org/abs/2004.09930v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DIET: Lightweight Language Understanding for Dialogue Systems", "abstract": "Large-scale pre-trained language models have shown impressive results on\nlanguage understanding benchmarks like GLUE and SuperGLUE, improving\nconsiderably over other pre-training methods like distributed representations\n(GloVe) and purely supervised approaches. We introduce the Dual Intent and\nEntity Transformer (DIET) architecture, and study the effectiveness of\ndifferent pre-trained representations on intent and entity prediction, two\ncommon dialogue language understanding tasks. DIET advances the state of the\nart on a complex multi-domain NLU dataset and achieves similarly high\nperformance on other simpler datasets. Surprisingly, we show that there is no\nclear benefit to using large pre-trained models for this task, and in fact DIET\nimproves upon the current state of the art even in a purely supervised setup\nwithout any pre-trained embeddings. Our best performing model outperforms\nfine-tuning BERT and is about six times faster to train.", "published": "2020-04-21 12:10:48", "link": "http://arxiv.org/abs/2004.09936v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BERT-ATTACK: Adversarial Attack Against BERT Using BERT", "abstract": "Adversarial attacks for discrete data (such as texts) have been proved\nsignificantly more challenging than continuous data (such as images) since it\nis difficult to generate adversarial samples with gradient-based methods.\nCurrent successful attack methods for texts usually adopt heuristic replacement\nstrategies on the character or word level, which remains challenging to find\nthe optimal solution in the massive space of possible combinations of\nreplacements while preserving semantic consistency and language fluency. In\nthis paper, we propose \\textbf{BERT-Attack}, a high-quality and effective\nmethod to generate adversarial samples using pre-trained masked language models\nexemplified by BERT. We turn BERT against its fine-tuned models and other deep\nneural models in downstream tasks so that we can successfully mislead the\ntarget models to predict incorrectly. Our method outperforms state-of-the-art\nattack strategies in both success rate and perturb percentage, while the\ngenerated adversarial samples are fluent and semantically preserved. Also, the\ncost of calculation is low, thus possible for large-scale generations. The code\nis available at https://github.com/LinyangLee/BERT-Attack.", "published": "2020-04-21 13:30:02", "link": "http://arxiv.org/abs/2004.09984v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Relation Ties with a Force-Directed Graph in Distant Supervised\n  Relation Extraction", "abstract": "Relation ties, defined as the correlation and mutual exclusion between\ndifferent relations, are critical for distant supervised relation extraction.\nExisting approaches model this property by greedily learning local\ndependencies. However, they are essentially limited by failing to capture the\nglobal topology structure of relation ties. As a result, they may easily fall\ninto a locally optimal solution. To solve this problem, in this paper, we\npropose a novel force-directed graph based relation extraction model to\ncomprehensively learn relation ties. Specifically, we first build a graph\naccording to the global co-occurrence of relations. Then, we borrow the idea of\nCoulomb's Law from physics and introduce the concept of attractive force and\nrepulsive force to this graph to learn correlation and mutual exclusion between\nrelations. Finally, the obtained relation representations are applied as an\ninter-dependent relation classifier. Experimental results on a large scale\nbenchmark dataset demonstrate that our model is capable of modeling global\nrelation ties and significantly outperforms other baselines. Furthermore, the\nproposed force-directed graph can be used as a module to augment existing\nrelation extraction systems and improve their performance.", "published": "2020-04-21 14:41:38", "link": "http://arxiv.org/abs/2004.10051v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Attention is Not Only a Weight: Analyzing Transformers with Vector Norms", "abstract": "Attention is a key component of Transformers, which have recently achieved\nconsiderable success in natural language processing. Hence, attention is being\nextensively studied to investigate various linguistic capabilities of\nTransformers, focusing on analyzing the parallels between attention weights and\nspecific linguistic phenomena. This paper shows that attention weights alone\nare only one of the two factors that determine the output of attention and\nproposes a norm-based analysis that incorporates the second factor, the norm of\nthe transformed input vectors. The findings of our norm-based analyses of BERT\nand a Transformer-based neural machine translation system include the\nfollowing: (i) contrary to previous studies, BERT pays poor attention to\nspecial tokens, and (ii) reasonable word alignment can be extracted from\nattention mechanisms of Transformer. These findings provide insights into the\ninner workings of Transformers.", "published": "2020-04-21 15:22:27", "link": "http://arxiv.org/abs/2004.10102v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Opinion Summarization with Noising and Denoising", "abstract": "The supervised training of high-capacity models on large datasets containing\nhundreds of thousands of document-summary pairs is critical to the recent\nsuccess of deep learning techniques for abstractive summarization.\nUnfortunately, in most domains (other than news) such training data is not\navailable and cannot be easily sourced. In this paper we enable the use of\nsupervised learning for the setting where there are only documents available\n(e.g.,~product or business reviews) without ground truth summaries. We create a\nsynthetic dataset from a corpus of user reviews by sampling a review,\npretending it is a summary, and generating noisy versions thereof which we\ntreat as pseudo-review input. We introduce several linguistically motivated\nnoise generation functions and a summarization model which learns to denoise\nthe input and generate the original review. At test time, the model accepts\ngenuine reviews and generates a summary containing salient opinions, treating\nthose that do not reach consensus as noise. Extensive automatic and human\nevaluation shows that our model brings substantial improvements over both\nabstractive and extractive baselines.", "published": "2020-04-21 16:54:57", "link": "http://arxiv.org/abs/2004.10150v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Logic-Guided Data Augmentation and Regularization for Consistent\n  Question Answering", "abstract": "Many natural language questions require qualitative, quantitative or logical\ncomparisons between two entities or events. This paper addresses the problem of\nimproving the accuracy and consistency of responses to comparison questions by\nintegrating logic rules and neural models. Our method leverages logical and\nlinguistic knowledge to augment labeled training data and then uses a\nconsistency-based regularizer to train the model. Improving the global\nconsistency of predictions, our approach achieves large improvements over\nprevious methods in a variety of question answering (QA) tasks including\nmultiple-choice qualitative reasoning, cause-effect reasoning, and extractive\nmachine reading comprehension. In particular, our method significantly improves\nthe performance of RoBERTa-based models by 1-5% across datasets. We advance the\nstate of the art by around 5-8% on WIQA and QuaRel and reduce consistency\nviolations by 58% on HotpotQA. We further demonstrate that our approach can\nlearn effectively from limited data.", "published": "2020-04-21 17:03:08", "link": "http://arxiv.org/abs/2004.10157v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge Distillation for Multilingual Unsupervised Neural Machine\n  Translation", "abstract": "Unsupervised neural machine translation (UNMT) has recently achieved\nremarkable results for several language pairs. However, it can only translate\nbetween a single language pair and cannot produce translation results for\nmultiple language pairs at the same time. That is, research on multilingual\nUNMT has been limited. In this paper, we empirically introduce a simple method\nto translate between thirteen languages using a single encoder and a single\ndecoder, making use of multilingual data to improve UNMT for all language\npairs. On the basis of the empirical findings, we propose two knowledge\ndistillation methods to further enhance multilingual UNMT performance. Our\nexperiments on a dataset with English translated to and from twelve other\nlanguages (including three language families and six language branches) show\nremarkable results, surpassing strong unsupervised individual baselines while\nachieving promising performance between non-English language pairs in zero-shot\ntranslation scenarios and alleviating poor performance in low-resource language\npairs.", "published": "2020-04-21 17:26:16", "link": "http://arxiv.org/abs/2004.10171v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Observations on Annotations", "abstract": "The annotation of textual information is a fundamental activity in\nLinguistics and Computational Linguistics. This article presents various\nobservations on annotations. It approaches the topic from several angles\nincluding Hypertext, Computational Linguistics and Language Technology,\nArtificial Intelligence and Open Science. Annotations can be examined along\ndifferent dimensions. In terms of complexity, they can range from trivial to\nhighly sophisticated, in terms of maturity from experimental to standardised.\nAnnotations can be annotated themselves using more abstract annotations.\nPrimary research data such as, e.g., text documents can be annotated on\ndifferent layers concurrently, which are independent but can be exploited using\nmulti-layer querying. Standards guarantee interoperability and reusability of\ndata sets. The chapter concludes with four final observations, formulated as\nresearch questions or rather provocative remarks on the current state of\nannotation research.", "published": "2020-04-21 20:29:50", "link": "http://arxiv.org/abs/2004.10283v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Deep Learning System for Sentiment Analysis of Service Calls", "abstract": "Sentiment analysis is crucial for the advancement of artificial intelligence\n(AI). Sentiment understanding can help AI to replicate human language and\ndiscourse. Studying the formation and response of sentiment state from\nwell-trained Customer Service Representatives (CSRs) can help make the\ninteraction between humans and AI more intelligent. In this paper, a sentiment\nanalysis pipeline is first carried out with respect to real-world multi-party\nconversations - that is, service calls. Based on the acoustic and linguistic\nfeatures extracted from the source information, a novel aggregated method for\nvoice sentiment recognition framework is built. Each party's sentiment pattern\nduring the communication is investigated along with the interaction sentiment\npattern between all parties.", "published": "2020-04-21 22:02:43", "link": "http://arxiv.org/abs/2004.10320v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Word Embedding-based Text Processing for Comprehensive Summarization and\n  Distinct Information Extraction", "abstract": "In this paper, we propose two automated text processing frameworks\nspecifically designed to analyze online reviews. The objective of the first\nframework is to summarize the reviews dataset by extracting essential sentence.\nThis is performed by converting sentences into numerical vectors and clustering\nthem using a community detection algorithm based on their similarity levels.\nAfterwards, a correlation score is measured for each sentence to determine its\nimportance level in each cluster and assign it as a tag for that community. The\nsecond framework is based on a question-answering neural network model trained\nto extract answers to multiple different questions. The collected answers are\neffectively clustered to find multiple distinct answers to a single question\nthat might be asked by a customer. The proposed frameworks are shown to be more\ncomprehensive than existing reviews processing solutions.", "published": "2020-04-21 02:43:31", "link": "http://arxiv.org/abs/2004.09719v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Neural Abstractive Summarization with Structural Attention", "abstract": "Attentional, RNN-based encoder-decoder architectures have achieved impressive\nperformance on abstractive summarization of news articles. However, these\nmethods fail to account for long term dependencies within the sentences of a\ndocument. This problem is exacerbated in multi-document summarization tasks\nsuch as summarizing the popular opinion in threads present in community\nquestion answering (CQA) websites such as Yahoo! Answers and Quora. These\nthreads contain answers which often overlap or contradict each other. In this\nwork, we present a hierarchical encoder based on structural attention to model\nsuch inter-sentence and inter-document dependencies. We set the popular\npointer-generator architecture and some of the architectures derived from it as\nour baselines and show that they fail to generate good summaries in a\nmulti-document setting. We further illustrate that our proposed model achieves\nsignificant improvement over the baselines in both single and multi-document\nsummarization settings -- in the former setting, it beats the best baseline by\n1.31 and 7.8 ROUGE-1 points on CNN and CQA datasets, respectively; in the\nlatter setting, the performance is further improved by 1.6 ROUGE-1 points on\nthe CQA dataset.", "published": "2020-04-21 03:39:15", "link": "http://arxiv.org/abs/2004.09739v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Knowledge-Driven Distractor Generation for Cloze-style Multiple Choice\n  Questions", "abstract": "In this paper, we propose a novel configurable framework to automatically\ngenerate distractive choices for open-domain cloze-style multiple-choice\nquestions, which incorporates a general-purpose knowledge base to effectively\ncreate a small distractor candidate set, and a feature-rich learning-to-rank\nmodel to select distractors that are both plausible and reliable. Experimental\nresults on datasets across four domains show that our framework yields\ndistractors that are more plausible and reliable than previous methods. This\ndataset can also be used as a benchmark for distractor generation in the\nfuture.", "published": "2020-04-21 09:29:50", "link": "http://arxiv.org/abs/2004.09853v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Considering Likelihood in NLP Classification Explanations with Occlusion\n  and Language Modeling", "abstract": "Recently, state-of-the-art NLP models gained an increasing syntactic and\nsemantic understanding of language, and explanation methods are crucial to\nunderstand their decisions. Occlusion is a well established method that\nprovides explanations on discrete language data, e.g. by removing a language\nunit from an input and measuring the impact on a model's decision. We argue\nthat current occlusion-based methods often produce invalid or syntactically\nincorrect language data, neglecting the improved abilities of recent NLP\nmodels. Furthermore, gradient-based explanation methods disregard the discrete\ndistribution of data in NLP. Thus, we propose OLM: a novel explanation method\nthat combines occlusion and language models to sample valid and syntactically\ncorrect replacements with high likelihood, given the context of the original\ninput. We lay out a theoretical foundation that alleviates these weaknesses of\nother explanation methods in NLP and provide results that underline the\nimportance of considering data likelihood in occlusion-based explanation.", "published": "2020-04-21 10:37:44", "link": "http://arxiv.org/abs/2004.09890v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning to Encode Evolutionary Knowledge for Automatic Commenting Long\n  Novels", "abstract": "Static knowledge graph has been incorporated extensively into\nsequence-to-sequence framework for text generation. While effectively\nrepresenting structured context, static knowledge graph failed to represent\nknowledge evolution, which is required in modeling dynamic events. In this\npaper, an automatic commenting task is proposed for long novels, which involves\nunderstanding context of more than tens of thousands of words. To model the\ndynamic storyline, especially the transitions of the characters and their\nrelations, Evolutionary Knowledge Graph(EKG) is proposed and learned within a\nmulti-task framework. Given a specific passage to comment, sequential modeling\nis used to incorporate historical and future embedding for context\nrepresentation. Further, a graph-to-sequence model is designed to utilize the\nEKG for comment generation. Extensive experimental results show that our\nEKG-based method is superior to several strong baselines on both automatic and\nhuman evaluations.", "published": "2020-04-21 13:09:50", "link": "http://arxiv.org/abs/2004.09974v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Leveraging Cognitive Search Patterns to Enhance Automated Natural\n  Language Retrieval Performance", "abstract": "The search of information in large text repositories has been plagued by the\nso-called document-query vocabulary gap, i.e. the semantic discordance between\nthe contents in the stored document entities on the one hand and the human\nquery on the other hand. Over the past two decades, a significant body of works\nhas advanced technical retrieval prowess while several studies have shed light\non issues pertaining to human search behavior. We believe that these efforts\nshould be conjoined, in the sense that automated retrieval systems have to\nfully emulate human search behavior and thus consider the procedure according\nto which users incrementally enhance their initial query. To this end,\ncognitive reformulation patterns that mimic user search behaviour are\nhighlighted and enhancement terms which are statistically collocated with or\nlexical-semantically related to the original terms adopted in the retrieval\nprocess. We formalize the application of these patterns by considering a query\nconceptual representation and introducing a set of operations allowing to\noperate modifications on the initial query. A genetic algorithm-based weighting\nprocess allows placing emphasis on terms according to their conceptual\nrole-type. An experimental evaluation on real-world datasets against relevance,\nlanguage, conceptual and knowledge-based models is conducted. We also show,\nwhen compared to language and relevance models, a better performance in terms\nof mean average precision than a word embedding-based model instantiation.", "published": "2020-04-21 14:13:33", "link": "http://arxiv.org/abs/2004.10035v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "LineaRE: Simple but Powerful Knowledge Graph Embedding for Link\n  Prediction", "abstract": "The task of link prediction for knowledge graphs is to predict missing\nrelationships between entities. Knowledge graph embedding, which aims to\nrepresent entities and relations of a knowledge graph as low dimensional\nvectors in a continuous vector space, has achieved promising predictive\nperformance. If an embedding model can cover different types of connectivity\npatterns and mapping properties of relations as many as possible, it will\npotentially bring more benefits for link prediction tasks. In this paper, we\npropose a novel embedding model, namely LineaRE, which is capable of modeling\nfour connectivity patterns (i.e., symmetry, antisymmetry, inversion, and\ncomposition) and four mapping properties (i.e., one-to-one, one-to-many,\nmany-to-one, and many-to-many) of relations. Specifically, we regard knowledge\ngraph embedding as a simple linear regression task, where a relation is modeled\nas a linear function of two low-dimensional vector-presented entities with two\nweight vectors and a bias vector. Since the vectors are defined in a real\nnumber space and the scoring function of the model is linear, our model is\nsimple and scalable to large knowledge graphs. Experimental results on multiple\nwidely used real-world datasets show that the proposed LineaRE model\nsignificantly outperforms existing state-of-the-art models for link prediction\ntasks.", "published": "2020-04-21 14:19:43", "link": "http://arxiv.org/abs/2004.10037v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Learnings from Technological Interventions in a Low Resource Language: A\n  Case-Study on Gondi", "abstract": "The primary obstacle to developing technologies for low-resource languages is\nthe lack of usable data. In this paper, we report the adoption and deployment\nof 4 technology-driven methods of data collection for Gondi, a low-resource\nvulnerable language spoken by around 2.3 million tribal people in south and\ncentral India. In the process of data collection, we also help in its revival\nby expanding access to information in Gondi through the creation of linguistic\nresources that can be used by the community, such as a dictionary, children's\nstories, an app with Gondi content from multiple sources and an Interactive\nVoice Response (IVR) based mass awareness platform. At the end of these\ninterventions, we collected a little less than 12,000 translated words and/or\nsentences and identified more than 650 community members whose help can be\nsolicited for future translation efforts. The larger goal of the project is\ncollecting enough data in Gondi to build and deploy viable language\ntechnologies like machine translation and speech to text systems that can help\ntake the language onto the internet.", "published": "2020-04-21 20:03:57", "link": "http://arxiv.org/abs/2004.10270v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Textual Visual Semantic Dataset for Text Spotting", "abstract": "Text Spotting in the wild consists of detecting and recognizing text\nappearing in images (e.g. signboards, traffic signals or brands in clothing or\nobjects). This is a challenging problem due to the complexity of the context\nwhere texts appear (uneven backgrounds, shading, occlusions, perspective\ndistortions, etc.). Only a few approaches try to exploit the relation between\ntext and its surrounding environment to better recognize text in the scene. In\nthis paper, we propose a visual context dataset for Text Spotting in the wild,\nwhere the publicly available dataset COCO-text [Veit et al. 2016] has been\nextended with information about the scene (such as objects and places appearing\nin the image) to enable researchers to include semantic relations between texts\nand scene in their Text Spotting systems, and to offer a common framework for\nsuch approaches. For each text in an image, we extract three kinds of context\ninformation: objects in the scene, image location label and a textual image\ndescription (caption). We use state-of-the-art out-of-the-box available tools\nto extract this additional information. Since this information has textual\nform, it can be used to leverage text similarity or semantic relation methods\ninto Text Spotting systems, either as a post-processing or in an end-to-end\ntraining strategy. Our data is publicly available at https://git.io/JeZTb.", "published": "2020-04-21 23:58:16", "link": "http://arxiv.org/abs/2004.10349v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Leveraging Personal Navigation Assistant Systems Using Automated Social\n  Media Traffic Reporting", "abstract": "Modern urbanization is demanding smarter technologies to improve a variety of\napplications in intelligent transportation systems to relieve the increasing\namount of vehicular traffic congestion and incidents. Existing incident\ndetection techniques are limited to the use of sensors in the transportation\nnetwork and hang on human-inputs. Despite of its data abundance, social media\nis not well-exploited in such context. In this paper, we develop an automated\ntraffic alert system based on Natural Language Processing (NLP) that filters\nthis flood of information and extract important traffic-related bullets. To\nthis end, we employ the fine-tuning Bidirectional Encoder Representations from\nTransformers (BERT) language embedding model to filter the related traffic\ninformation from social media. Then, we apply a question-answering model to\nextract necessary information characterizing the report event such as its exact\nlocation, occurrence time, and nature of the events. We demonstrate the adopted\nNLP approaches outperform other existing approach and, after effectively\ntraining them, we focus on real-world situation and show how the developed\napproach can, in real-time, extract traffic-related information and\nautomatically convert them into alerts for navigation assistance applications\nsuch as navigation apps.", "published": "2020-04-21 02:26:06", "link": "http://arxiv.org/abs/2004.13823v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Discrete Variational Attention Models for Language Generation", "abstract": "Variational autoencoders have been widely applied for natural language\ngeneration, however, there are two long-standing problems: information\nunder-representation and posterior collapse. The former arises from the fact\nthat only the last hidden state from the encoder is transformed to the latent\nspace, which is insufficient to summarize data. The latter comes as a result of\nthe imbalanced scale between the reconstruction loss and the KL divergence in\nthe objective function. To tackle these issues, in this paper we propose the\ndiscrete variational attention model with categorical distribution over the\nattention mechanism owing to the discrete nature in languages. Our approach is\ncombined with an auto-regressive prior to capture the sequential dependency\nfrom observations, which can enhance the latent space for language generation.\nMoreover, thanks to the property of discreteness, the training of our proposed\napproach does not suffer from posterior collapse. Furthermore, we carefully\nanalyze the superiority of discrete latent space over the continuous space with\nthe common Gaussian distribution. Extensive experiments on language generation\ndemonstrate superior advantages of our proposed approach in comparison with the\nstate-of-the-art counterparts.", "published": "2020-04-21 05:49:04", "link": "http://arxiv.org/abs/2004.09764v4", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "The Ivory Tower Lost: How College Students Respond Differently than the\n  General Public to the COVID-19 Pandemic", "abstract": "Recently, the pandemic of the novel Coronavirus Disease-2019 (COVID-19) has\npresented governments with ultimate challenges. In the United States, the\ncountry with the highest confirmed COVID-19 infection cases, a nationwide\nsocial distancing protocol has been implemented by the President. For the first\ntime in a hundred years since the 1918 flu pandemic, the US population is\nmandated to stay in their households and avoid public contact. As a result, the\nmajority of public venues and services have ceased their operations. Following\nthe closure of the University of Washington on March 7th, more than a thousand\ncolleges and universities in the United States have cancelled in-person classes\nand campus activities, impacting millions of students. This paper aims to\ndiscover the social implications of this unprecedented disruption in our\ninteractive society regarding both the general public and higher education\npopulations by mining people's opinions on social media. We discover several\ntopics embedded in a large number of COVID-19 tweets that represent the most\ncentral issues related to the pandemic, which are of great concerns for both\ncollege students and the general public. Moreover, we find significant\ndifferences between these two groups of Twitter users with respect to the\nsentiments they expressed towards the COVID-19 issues. To our best knowledge,\nthis is the first social media-based study which focuses on the college student\ncommunity's demographics and responses to prevalent social issues during a\nmajor crisis.", "published": "2020-04-21 13:02:38", "link": "http://arxiv.org/abs/2004.09968v1", "categories": ["cs.SI", "cs.CL", "cs.LG"], "primary_category": "cs.SI"}
{"title": "Beyond Optimizing for Clicks: Incorporating Editorial Values in News\n  Recommendation", "abstract": "With the uptake of algorithmic personalization in the news domain, news\norganizations increasingly trust automated systems with previously considered\neditorial responsibilities, e.g., prioritizing news to readers. In this paper\nwe study an automated news recommender system in the context of a news\norganization's editorial values. We conduct and present two online studies with\na news recommender system, which span one and a half months and involve over\n1,200 users. In our first study we explore how our news recommender steers\nreading behavior in the context of editorial values such as serendipity,\ndynamism, diversity, and coverage. Next, we present an intervention study where\nwe extend our news recommender to steer our readers to more dynamic reading\nbehavior. We find that (i) our recommender system yields more diverse reading\nbehavior and yields a higher coverage of articles compared to non-personalized\neditorial rankings, and (ii) we can successfully incorporate dynamism in our\nrecommender system as a re-ranking method, effectively steering our readers to\nmore dynamic articles without hurting our recommender system's accuracy.", "published": "2020-04-21 13:24:49", "link": "http://arxiv.org/abs/2004.09980v1", "categories": ["cs.IR", "cs.CL", "cs.HC", "cs.SI"], "primary_category": "cs.IR"}
{"title": "Adaptive Interaction Fusion Networks for Fake News Detection", "abstract": "The majority of existing methods for fake news detection universally focus on\nlearning and fusing various features for detection. However, the learning of\nvarious features is independent, which leads to a lack of cross-interaction\nfusion between features on social media, especially between posts and comments.\nGenerally, in fake news, there are emotional associations and semantic\nconflicts between posts and comments. How to represent and fuse the\ncross-interaction between both is a key challenge. In this paper, we propose\nAdaptive Interaction Fusion Networks (AIFN) to fulfill cross-interaction fusion\namong features for fake news detection. In AIFN, to discover semantic\nconflicts, we design gated adaptive interaction networks (GAIN) to capture\nadaptively similar semantics and conflicting semantics between posts and\ncomments. To establish feature associations, we devise semantic-level fusion\nself-attention networks (SFSN) to enhance semantic correlations and fusion\namong features. Extensive experiments on two real-world datasets, i.e.,\nRumourEval and PHEME, demonstrate that AIFN achieves the state-of-the-art\nperformance and boosts accuracy by more than 2.05% and 1.90%, respectively.", "published": "2020-04-21 13:51:03", "link": "http://arxiv.org/abs/2004.10009v1", "categories": ["cs.CL", "cs.CY", "cs.IR"], "primary_category": "cs.CL"}
{"title": "AGIF: An Adaptive Graph-Interactive Framework for Joint Multiple Intent\n  Detection and Slot Filling", "abstract": "In real-world scenarios, users usually have multiple intents in the same\nutterance. Unfortunately, most spoken language understanding (SLU) models\neither mainly focused on the single intent scenario, or simply incorporated an\noverall intent context vector for all tokens, ignoring the fine-grained\nmultiple intents information integration for token-level slot prediction. In\nthis paper, we propose an Adaptive Graph-Interactive Framework (AGIF) for joint\nmultiple intent detection and slot filling, where we introduce an intent-slot\ngraph interaction layer to model the strong correlation between the slot and\nintents. Such an interaction layer is applied to each token adaptively, which\nhas the advantage to automatically extract the relevant intents information,\nmaking a fine-grained intent information integration for the token-level slot\nprediction. Experimental results on three multi-intent datasets show that our\nframework obtains substantial improvement and achieves the state-of-the-art\nperformance. In addition, our framework achieves new state-of-the-art\nperformance on two single-intent datasets.", "published": "2020-04-21 15:07:34", "link": "http://arxiv.org/abs/2004.10087v4", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Curriculum Pre-training for End-to-End Speech Translation", "abstract": "End-to-end speech translation poses a heavy burden on the encoder, because it\nhas to transcribe, understand, and learn cross-lingual semantics\nsimultaneously. To obtain a powerful encoder, traditional methods pre-train it\non ASR data to capture speech features. However, we argue that pre-training the\nencoder only through simple speech recognition is not enough and high-level\nlinguistic knowledge should be considered. Inspired by this, we propose a\ncurriculum pre-training method that includes an elementary course for\ntranscription learning and two advanced courses for understanding the utterance\nand mapping words in two languages. The difficulty of these courses is\ngradually increasing. Experiments show that our curriculum pre-training method\nleads to significant improvements on En-De and En-Fr speech translation\nbenchmarks.", "published": "2020-04-21 15:12:07", "link": "http://arxiv.org/abs/2004.10093v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Experience Grounds Language", "abstract": "Language understanding research is held back by a failure to relate language\nto the physical world it describes and to the social interactions it\nfacilitates. Despite the incredible effectiveness of language processing models\nto tackle tasks after being trained on text alone, successful linguistic\ncommunication relies on a shared experience of the world. It is this shared\nexperience that makes utterances meaningful.\n  Natural language processing is a diverse field, and progress throughout its\ndevelopment has come from new representational theories, modeling techniques,\ndata collection paradigms, and tasks. We posit that the present success of\nrepresentation learning approaches trained on large, text-only corpora requires\nthe parallel tradition of research on the broader physical and social context\nof language to address the deeper questions of communication.", "published": "2020-04-21 16:56:27", "link": "http://arxiv.org/abs/2004.10151v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Domain-Guided Task Decomposition with Self-Training for Detecting\n  Personal Events in Social Media", "abstract": "Mining social media content for tasks such as detecting personal experiences\nor events, suffer from lexical sparsity, insufficient training data, and\ninventive lexicons. To reduce the burden of creating extensive labeled data and\nimprove classification performance, we propose to perform these tasks in two\nsteps: 1. Decomposing the task into domain-specific sub-tasks by identifying\nkey concepts, thus utilizing human domain understanding; and 2. Combining the\nresults of learners for each key concept using co-training to reduce the\nrequirements for labeled training data. We empirically show the effectiveness\nand generality of our approach, Co-Decomp, using three representative social\nmedia mining tasks, namely Personal Health Mention detection, Crisis Report\ndetection, and Adverse Drug Reaction monitoring. The experiments show that our\nmodel is able to outperform the state-of-the-art text classification\nmodels--including those using the recently introduced BERT model--when small\namounts of training data are available.", "published": "2020-04-21 14:50:31", "link": "http://arxiv.org/abs/2004.10201v1", "categories": ["cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "MT-Clinical BERT: Scaling Clinical Information Extraction with Multitask\n  Learning", "abstract": "Clinical notes contain an abundance of important but not-readily accessible\ninformation about patients. Systems to automatically extract this information\nrely on large amounts of training data for which their exists limited resources\nto create. Furthermore, they are developed dis-jointly; meaning that no\ninformation can be shared amongst task-specific systems. This bottle-neck\nunnecessarily complicates practical application, reduces the performance\ncapabilities of each individual solution and associates the engineering debt of\nmanaging multiple information extraction systems. We address these challenges\nby developing Multitask-Clinical BERT: a single deep learning model that\nsimultaneously performs eight clinical tasks spanning entity extraction, PHI\nidentification, language entailment and similarity by sharing representations\namongst tasks. We find our single system performs competitively with all\nstate-the-art task-specific systems while also benefiting from massive\ncomputational benefits at inference.", "published": "2020-04-21 18:04:08", "link": "http://arxiv.org/abs/2004.10220v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ESPnet-ST: All-in-One Speech Translation Toolkit", "abstract": "We present ESPnet-ST, which is designed for the quick development of\nspeech-to-speech translation systems in a single framework. ESPnet-ST is a new\nproject inside end-to-end speech processing toolkit, ESPnet, which integrates\nor newly implements automatic speech recognition, machine translation, and\ntext-to-speech functions for speech translation. We provide all-in-one recipes\nincluding data pre-processing, feature extraction, training, and decoding\npipelines for a wide range of benchmark datasets. Our reproducible results can\nmatch or even outperform the current state-of-the-art performances; these\npre-trained models are downloadable. The toolkit is publicly available at\nhttps://github.com/espnet/espnet.", "published": "2020-04-21 18:38:38", "link": "http://arxiv.org/abs/2004.10234v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Investigating the Effectiveness of Representations Based on Pretrained\n  Transformer-based Language Models in Active Learning for Labelling Text\n  Datasets", "abstract": "Active learning has been shown to be an effective way to alleviate some of\nthe effort required in utilising large collections of unlabelled data for\nmachine learning tasks without needing to fully label them. The representation\nmechanism used to represent text documents when performing active learning,\nhowever, has a significant influence on how effective the process will be.\nWhile simple vector representations such as bag-of-words and embedding-based\nrepresentations based on techniques such as word2vec have been shown to be an\neffective way to represent documents during active learning, the emergence of\nrepresentation mechanisms based on the pre-trained transformer-based neural\nnetwork models popular in natural language processing research (e.g. BERT)\noffer a promising, and as yet not fully explored, alternative. This paper\ndescribes a comprehensive evaluation of the effectiveness of representations\nbased on pre-trained transformer-based language models for active learning.\nThis evaluation shows that transformer-based models, especially BERT-like\nmodels, that have not yet been widely used in active learning, achieve a\nsignificant improvement over more commonly used vector representations like\nbag-of-words or other classical word embeddings like word2vec. This paper also\ninvestigates the effectiveness of representations based on variants of BERT\nsuch as Roberta, Albert as well as comparing the effectiveness of the [CLS]\ntoken representation and the aggregated representation that can be generated\nusing BERT-like models. Finally, we propose an approach Adaptive Tuning Active\nLearning. Our experiments show that the limited label information acquired in\nactive learning can not only be used for training a classifier but can also\nadaptively improve the embeddings generated by the BERT-like language models as\nwell.", "published": "2020-04-21 02:37:44", "link": "http://arxiv.org/abs/2004.13138v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Vector Quantized Contrastive Predictive Coding for Template-based Music\n  Generation", "abstract": "In this work, we propose a flexible method for generating variations of\ndiscrete sequences in which tokens can be grouped into basic units, like\nsentences in a text or bars in music. More precisely, given a template\nsequence, we aim at producing novel sequences sharing perceptible similarities\nwith the original template without relying on any annotation; so our problem of\ngenerating variations is intimately linked to the problem of learning relevant\nhigh-level representations without supervision. Our contribution is two-fold:\nFirst, we propose a self-supervised encoding technique, named Vector Quantized\nContrastive Predictive Coding which allows to learn a meaningful assignment of\nthe basic units over a discrete set of codes, together with mechanisms allowing\nto control the information content of these learnt discrete representations.\nSecondly, we show how these compressed representations can be used to generate\nvariations of a template sequence by using an appropriate attention pattern in\nthe Transformer architecture. We illustrate our approach on the corpus of J.S.\nBach chorales where we discuss the musical meaning of the learnt discrete codes\nand show that our proposed method allows to generate coherent and high-quality\nvariations of a given template.", "published": "2020-04-21 15:58:17", "link": "http://arxiv.org/abs/2004.10120v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Music Generation with Temporal Structure Augmentation", "abstract": "In this paper we introduce a novel feature augmentation approach for\ngenerating structured musical compositions comprising melodies and harmonies.\nThe proposed method augments a connectionist generation model with count-down\nto song conclusion and meter markers as extra input features to study whether\nneural networks can learn to produce more aesthetically pleasing and structured\nmusical output as a consequence of augmenting the input data with structural\nfeatures. An RNN architecture with LSTM cells is trained on the Nottingham folk\nmusic dataset in a supervised sequence learning setup, following a Music\nLanguage Modelling approach, and then applied to generation of harmonies and\nmelodies. Our experiments show an improved prediction performance for both\ntypes of annotation. The generated music was also subjectively evaluated using\nan on-line Turing style listening test which confirms a substantial improvement\nin the aesthetic quality and in the perceived structure of the music generated\nusing the temporal structure.", "published": "2020-04-21 19:19:58", "link": "http://arxiv.org/abs/2004.10246v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MIDI-Sheet Music Alignment Using Bootleg Score Synthesis", "abstract": "MIDI-sheet music alignment is the task of finding correspondences between a\nMIDI representation of a piece and its corresponding sheet music images. Rather\nthan using optical music recognition to bridge the gap between sheet music and\nMIDI, we explore an alternative approach: projecting the MIDI data into pixel\nspace and performing alignment in the image domain. Our method converts the\nMIDI data into a crude representation of the score that only contains\nrectangular floating notehead blobs, a process we call bootleg score synthesis.\nFurthermore, we project sheet music images into the same bootleg space by\napplying a deep watershed notehead detector and filling in the bounding boxes\naround each detected notehead. Finally, we align the bootleg representations\nusing a simple variant of dynamic time warping. On a dataset of 68 real scanned\npiano scores from IMSLP and corresponding MIDI performances, our method\nachieves a 97.3% accuracy at an error tolerance of one second, outperforming\nseveral baseline systems that employ optical music recognition.", "published": "2020-04-21 23:45:18", "link": "http://arxiv.org/abs/2004.10345v1", "categories": ["cs.MM", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.MM"}
{"title": "MIDI Passage Retrieval Using Cell Phone Pictures of Sheet Music", "abstract": "This paper investigates a cross-modal retrieval problem in which a user would\nlike to retrieve a passage of music from a MIDI file by taking a cell phone\npicture of a physical page of sheet music. While audio-sheet music retrieval\nhas been explored by a number of works, this scenario is novel in that the\nquery is a cell phone picture rather than a digital scan. To solve this\nproblem, we introduce a mid-level feature representation called a bootleg score\nwhich explicitly encodes the rules of Western musical notation. We convert both\nthe MIDI and the sheet music into bootleg scores using deterministic rules of\nmusic and classical computer vision techniques for detecting simple geometric\nshapes. Once the MIDI and cell phone image have been converted into bootleg\nscores, we estimate the alignment using dynamic programming. The most notable\ncharacteristic of our system is that it does test-time adaptation and has no\ntrainable weights at all -- only a set of about 30 hyperparameters. On a\ndataset containing 1000 cell phone pictures taken of 100 scores of classical\npiano music, our system achieves an F measure score of .869 and outperforms\nbaseline systems based on commercial optical music recognition software.", "published": "2020-04-21 23:56:53", "link": "http://arxiv.org/abs/2004.10347v1", "categories": ["cs.MM", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.MM"}
