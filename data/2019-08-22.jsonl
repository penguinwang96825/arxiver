{"title": "Entropy-Enhanced Multimodal Attention Model for Scene-Aware Dialogue\n  Generation", "abstract": "With increasing information from social media, there are more and more videos\navailable. Therefore, the ability to reason on a video is important and\ndeserves to be discussed. TheDialog System Technology Challenge (DSTC7)\n(Yoshino et al. 2018) proposed an Audio Visual Scene-aware Dialog (AVSD) task,\nwhich contains five modalities including video, dialogue history, summary, and\ncaption, as a scene-aware environment. In this paper, we propose the\nentropy-enhanced dynamic memory network (DMN) to effectively model video\nmodality. The attention-based GRU in the proposed model can improve the model's\nability to comprehend and memorize sequential information. The entropy\nmechanism can control the attention distribution higher, so each to-be-answered\nquestion can focus more specifically on a small set of video segments. After\nthe entropy-enhanced DMN secures the video context, we apply an attention model\nthat in-corporates summary and caption to generate an accurate answer given the\nquestion about the video. In the official evaluation, our system can achieve\nimproved performance against the released baseline model for both subjective\nand objective evaluation metrics.", "published": "2019-08-22 03:53:36", "link": "http://arxiv.org/abs/1908.08191v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Denoising based Sequence-to-Sequence Pre-training for Text Generation", "abstract": "This paper presents a new sequence-to-sequence (seq2seq) pre-training method\nPoDA (Pre-training of Denoising Autoencoders), which learns representations\nsuitable for text generation tasks. Unlike encoder-only (e.g., BERT) or\ndecoder-only (e.g., OpenAI GPT) pre-training approaches, PoDA jointly\npre-trains both the encoder and decoder by denoising the noise-corrupted text,\nand it also has the advantage of keeping the network architecture unchanged in\nthe subsequent fine-tuning stage. Meanwhile, we design a hybrid model of\nTransformer and pointer-generator networks as the backbone architecture for\nPoDA. We conduct experiments on two text generation tasks: abstractive\nsummarization, and grammatical error correction. Results on four datasets show\nthat PoDA can improve model performance over strong baselines without using any\ntask-specific techniques and significantly speed up convergence.", "published": "2019-08-22 05:26:25", "link": "http://arxiv.org/abs/1908.08206v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Relation-Aware Entity Alignment for Heterogeneous Knowledge Graphs", "abstract": "Entity alignment is the task of linking entities with the same real-world\nidentity from different knowledge graphs (KGs), which has been recently\ndominated by embedding-based methods. Such approaches work by learning KG\nrepresentations so that entity alignment can be performed by measuring the\nsimilarities between entity embeddings. While promising, prior works in the\nfield often fail to properly capture complex relation information that commonly\nexists in multi-relational KGs, leaving much room for improvement. In this\npaper, we propose a novel Relation-aware Dual-Graph Convolutional Network\n(RDGCN) to incorporate relation information via attentive interactions between\nthe knowledge graph and its dual relation counterpart, and further capture\nneighboring structures to learn better entity representations. Experiments on\nthree real-world cross-lingual datasets show that our approach delivers better\nand more robust results over the state-of-the-art alignment methods by learning\nbetter KG representations.", "published": "2019-08-22 05:45:30", "link": "http://arxiv.org/abs/1908.08210v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting Semantic Representation and Tree Search for Similar Question\n  Retrieval", "abstract": "This paper studies the performances of BERT combined with tree structure in\nshort sentence ranking task. In retrieval-based question answering system, we\nretrieve the most similar question of the query question by ranking all the\nquestions in datasets. If we want to rank all the sentences by neural rankers,\nwe need to score all the sentence pairs. However it consumes large amount of\ntime. So we design a specific tree for searching and combine deep model to\nsolve this problem. We fine-tune BERT on the training data to get semantic\nvector or sentence embeddings on the test data. We use all the sentence\nembeddings of test data to build our tree based on k-means and do beam search\nat predicting time when given a sentence as query. We do the experiments on the\nsemantic textual similarity dataset, Quora Question Pairs, and process the\ndataset for sentence ranking. Experimental results show that our methods\noutperform the strong baseline. Our tree accelerate the predicting speed by\n500%-1000% without losing too much ranking accuracy.", "published": "2019-08-22 11:44:12", "link": "http://arxiv.org/abs/1908.08326v8", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Argument Invention from First Principles", "abstract": "Competitive debaters often find themselves facing a challenging task -- how\nto debate a topic they know very little about, with only minutes to prepare,\nand without access to books or the Internet? What they often do is rely on\n\"first principles\", commonplace arguments which are relevant to many topics,\nand which they have refined in past debates.\n  In this work we aim to explicitly define a taxonomy of such principled\nrecurring arguments, and, given a controversial topic, to automatically\nidentify which of these arguments are relevant to the topic.\n  As far as we know, this is the first time that this approach to argument\ninvention is formalized and made explicit in the context of NLP.\n  The main goal of this work is to show that it is possible to define such a\ntaxonomy. While the taxonomy suggested here should be thought of as a \"first\nattempt\" it is nonetheless coherent, covers well the relevant topics and\ncoincides with what professional debaters actually argue in their speeches, and\nfacilitates automatic argument invention for new topics.", "published": "2019-08-22 12:36:58", "link": "http://arxiv.org/abs/1908.08336v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Controllable Dual Skew Divergence Loss for Neural Machine Translation", "abstract": "In sequence prediction tasks like neural machine translation, training with\ncross-entropy loss often leads to models that overgeneralize and plunge into\nlocal optima. In this paper, we propose an extended loss function called\n\\emph{dual skew divergence} (DSD) that integrates two symmetric terms on KL\ndivergences with a balanced weight. We empirically discovered that such a\nbalanced weight plays a crucial role in applying the proposed DSD loss into\ndeep models. Thus we eventually develop a controllable DSD loss for\ngeneral-purpose scenarios. Our experiments indicate that switching to the DSD\nloss after the convergence of ML training helps models escape local optima and\nstimulates stable performance improvements. Our evaluations on the WMT 2014\nEnglish-German and English-French translation tasks demonstrate that the\nproposed loss as a general and convenient mean for NMT training indeed brings\nperformance improvement in comparison to strong baselines.", "published": "2019-08-22 14:16:20", "link": "http://arxiv.org/abs/1908.08399v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NE-LP: Normalized Entropy and Loss Prediction based Sampling for Active\n  Learning in Chinese Word Segmentation on EHRs", "abstract": "Electronic Health Records (EHRs) in hospital information systems contain\npatients' diagnosis and treatments, so EHRs are essential to clinical data\nmining. Of all the tasks in the mining process, Chinese Word Segmentation (CWS)\nis a fundamental and important one, and most state-of-the-art methods greatly\nrely on large-scale of manually-annotated data. Since annotation is\ntime-consuming and expensive, efforts have been devoted to techniques, such as\nactive learning, to locate the most informative samples for modeling. In this\npaper, we follow the trend and present an active learning method for CWS in\nEHRs. Specically, a new sampling strategy combining Normalized Entropy with\nLoss Prediction (NE-LP) is proposed to select the most representative data.\nMeanwhile, to minimize the computational cost of learning, we propose a joint\nmodel including a word segmenter and a loss prediction model. Furthermore, to\ncapture interactions between adjacent characters, bigram features are also\napplied in the joint model. To illustrate the effectiveness of NE-LP, we\nconducted experiments on EHRs collected from the Shuguang Hospital Affiliated\nto Shanghai University of Traditional Chinese Medicine. The results demonstrate\nthat NE-LP consistently outperforms conventional uncertainty-based sampling\nstrategies for active learning in CWS.", "published": "2019-08-22 14:51:44", "link": "http://arxiv.org/abs/1908.08419v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dialogue Coherence Assessment Without Explicit Dialogue Act Labels", "abstract": "Recent dialogue coherence models use the coherence features designed for\nmonologue texts, e.g. nominal entities, to represent utterances and then\nexplicitly augment them with dialogue-relevant features, e.g., dialogue act\nlabels. It indicates two drawbacks, (a) semantics of utterances is limited to\nentity mentions, and (b) the performance of coherence models strongly relies on\nthe quality of the input dialogue act labels. We address these issues by\nintroducing a novel approach to dialogue coherence assessment. We use dialogue\nact prediction as an auxiliary task in a multi-task learning scenario to obtain\ninformative utterance representations for coherence assessment. Our approach\nalleviates the need for explicit dialogue act labels during evaluation. The\nresults of our experiments show that our model substantially (more than 20\naccuracy points) outperforms its strong competitors on the DailyDialogue\ncorpus, and performs on par with them on the SwitchBoard corpus for ranking\ndialogues concerning their coherence.", "published": "2019-08-22 16:41:57", "link": "http://arxiv.org/abs/1908.08486v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Lemmatization as Embeddings-Based Word Clustering", "abstract": "We focus on the task of unsupervised lemmatization, i.e. grouping together\ninflected forms of one word under one label (a lemma) without the use of\nannotated training data. We propose to perform agglomerative clustering of word\nforms with a novel distance measure. Our distance measure is based on the\nobservation that inflections of the same word tend to be similar both\nstring-wise and in meaning. We therefore combine word embedding cosine\nsimilarity, serving as a proxy to the meaning similarity, with Jaro-Winkler\nedit distance. Our experiments on 23 languages show our approach to be\npromising, surpassing the baseline on 23 of the 28 evaluation datasets.", "published": "2019-08-22 17:58:55", "link": "http://arxiv.org/abs/1908.08528v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Text Summarization via Mixed Model Back-Translation", "abstract": "Back-translation based approaches have recently lead to significant progress\nin unsupervised sequence-to-sequence tasks such as machine translation or style\ntransfer. In this work, we extend the paradigm to the problem of learning a\nsentence summarization system from unaligned data. We present several initial\nmodels which rely on the asymmetrical nature of the task to perform the first\nback-translation step, and demonstrate the value of combining the data created\nby these diverse initialization methods. Our system outperforms the current\nstate-of-the-art for unsupervised sentence summarization from fully unaligned\ndata by over 2 ROUGE, and matches the performance of recent semi-supervised\napproaches.", "published": "2019-08-22 19:07:34", "link": "http://arxiv.org/abs/1908.08566v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-passage BERT: A Globally Normalized BERT Model for Open-domain\n  Question Answering", "abstract": "BERT model has been successfully applied to open-domain QA tasks. However,\nprevious work trains BERT by viewing passages corresponding to the same\nquestion as independent training instances, which may cause incomparable scores\nfor answers from different passages. To tackle this issue, we propose a\nmulti-passage BERT model to globally normalize answer scores across all\npassages of the same question, and this change enables our QA model find better\nanswers by utilizing more passages. In addition, we find that splitting\narticles into passages with the length of 100 words by sliding window improves\nperformance by 4%. By leveraging a passage ranker to select high-quality\npassages, multi-passage BERT gains additional 2%. Experiments on four standard\nbenchmarks showed that our multi-passage BERT outperforms all state-of-the-art\nmodels on all benchmarks. In particular, on the OpenSQuAD dataset, our model\ngains 21.4% EM and 21.5% $F_1$ over all non-BERT models, and 5.8% EM and 6.5%\n$F_1$ over BERT-based models.", "published": "2019-08-22 02:00:53", "link": "http://arxiv.org/abs/1908.08167v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Text Summarization with Pretrained Encoders", "abstract": "Bidirectional Encoder Representations from Transformers (BERT) represents the\nlatest incarnation of pretrained language models which have recently advanced a\nwide range of natural language processing tasks. In this paper, we showcase how\nBERT can be usefully applied in text summarization and propose a general\nframework for both extractive and abstractive models. We introduce a novel\ndocument-level encoder based on BERT which is able to express the semantics of\na document and obtain representations for its sentences. Our extractive model\nis built on top of this encoder by stacking several inter-sentence Transformer\nlayers. For abstractive summarization, we propose a new fine-tuning schedule\nwhich adopts different optimizers for the encoder and the decoder as a means of\nalleviating the mismatch between the two (the former is pretrained while the\nlatter is not). We also demonstrate that a two-staged fine-tuning approach can\nfurther boost the quality of the generated summaries. Experiments on three\ndatasets show that our model achieves state-of-the-art results across the board\nin both extractive and abstractive settings. Our code is available at\nhttps://github.com/nlpyang/PreSumm", "published": "2019-08-22 12:59:40", "link": "http://arxiv.org/abs/1908.08345v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ViCo: Word Embeddings from Visual Co-occurrences", "abstract": "We propose to learn word embeddings from visual co-occurrences. Two words\nco-occur visually if both words apply to the same image or image region.\nSpecifically, we extract four types of visual co-occurrences between object and\nattribute words from large-scale, textually-annotated visual databases like\nVisualGenome and ImageNet. We then train a multi-task log-bilinear model that\ncompactly encodes word \"meanings\" represented by each co-occurrence type into a\nsingle visual word-vector. Through unsupervised clustering, supervised\npartitioning, and a zero-shot-like generalization analysis we show that our\nword embeddings complement text-only embeddings like GloVe by better\nrepresenting similarities and differences between visual concepts that are\ndifficult to obtain from text corpora alone. We further evaluate our embeddings\non five downstream applications, four of which are vision-language tasks.\nAugmenting GloVe with our embeddings yields gains on all tasks. We also find\nthat random embeddings perform comparably to learned embeddings on all\nsupervised vision-language tasks, contrary to conventional wisdom.", "published": "2019-08-22 17:58:52", "link": "http://arxiv.org/abs/1908.08527v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "When Low Resource NLP Meets Unsupervised Language Model:\n  Meta-pretraining Then Meta-learning for Few-shot Text Classification", "abstract": "Text classification tends to be difficult when data are deficient or when it\nis required to adapt to unseen classes. In such challenging scenarios, recent\nstudies have often used meta-learning to simulate the few-shot task, thus\nnegating implicit common linguistic features across tasks. This paper addresses\nsuch problems using meta-learning and unsupervised language models. Our\napproach is based on the insight that having a good generalization from a few\nexamples relies on both a generic model initialization and an effective\nstrategy for adapting this model to newly arising tasks. We show that our\napproach is not only simple but also produces a state-of-the-art performance on\na well-studied sentiment classification dataset. It can thus be further\nsuggested that pretraining could be a promising solution for few-shot learning\nof many other NLP tasks. The code and the dataset to replicate the experiments\nare made available at https://github.com/zxlzr/FewShotNLP.", "published": "2019-08-22 17:23:29", "link": "http://arxiv.org/abs/1908.08788v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Compositionality decomposed: how do neural networks generalise?", "abstract": "Despite a multitude of empirical studies, little consensus exists on whether\nneural networks are able to generalise compositionally, a controversy that, in\npart, stems from a lack of agreement about what it means for a neural model to\nbe compositional. As a response to this controversy, we present a set of tests\nthat provide a bridge between, on the one hand, the vast amount of linguistic\nand philosophical theory about compositionality of language and, on the other,\nthe successful neural models of language. We collect different interpretations\nof compositionality and translate them into five theoretically grounded tests\nfor models that are formulated on a task-independent level. In particular, we\nprovide tests to investigate (i) if models systematically recombine known parts\nand rules (ii) if models can extend their predictions beyond the length they\nhave seen in the training data (iii) if models' composition operations are\nlocal or global (iv) if models' predictions are robust to synonym substitutions\nand (v) if models favour rules or exceptions during training. To demonstrate\nthe usefulness of this evaluation paradigm, we instantiate these five tests on\na highly compositional data set which we dub PCFG SET and apply the resulting\ntests to three popular sequence-to-sequence models: a recurrent, a\nconvolution-based and a transformer model. We provide an in-depth analysis of\nthe results, which uncover the strengths and weaknesses of these three\narchitectures and point to potential areas of improvement.", "published": "2019-08-22 13:08:26", "link": "http://arxiv.org/abs/1908.08351v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Transfer Learning for Relation Extraction via Relation-Gated Adversarial\n  Learning", "abstract": "Relation extraction aims to extract relational facts from sentences. Previous\nmodels mainly rely on manually labeled datasets, seed instances or\nhuman-crafted patterns, and distant supervision. However, the human annotation\nis expensive, while human-crafted patterns suffer from semantic drift and\ndistant supervision samples are usually noisy. Domain adaptation methods enable\nleveraging labeled data from a different but related domain. However, different\ndomains usually have various textual relation descriptions and different label\nspace (the source label space is usually a superset of the target label space).\nTo solve these problems, we propose a novel model of relation-gated adversarial\nlearning for relation extraction, which extends the adversarial based domain\nadaptation. Experimental results have shown that the proposed approach\noutperforms previous domain adaptation methods regarding partial domain\nadaptation and can improve the accuracy of distance supervised relation\nextraction through fine-tuning.", "published": "2019-08-22 17:27:54", "link": "http://arxiv.org/abs/1908.08507v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Sequential Latent Spaces for Modeling the Intention During Diverse Image\n  Captioning", "abstract": "Diverse and accurate vision+language modeling is an important goal to retain\ncreative freedom and maintain user engagement. However, adequately capturing\nthe intricacies of diversity in language models is challenging. Recent works\ncommonly resort to latent variable models augmented with more or less\nsupervision from object detectors or part-of-speech tags. Common to all those\nmethods is the fact that the latent variable either only initializes the\nsentence generation process or is identical across the steps of generation.\nBoth methods offer no fine-grained control. To address this concern, we propose\nSeq-CVAE which learns a latent space for every word position. We encourage this\ntemporal latent space to capture the 'intention' about how to complete the\nsentence by mimicking a representation which summarizes the future. We\nillustrate the efficacy of the proposed approach to anticipate the sentence\ncontinuation on the challenging MSCOCO dataset, significantly improving\ndiversity metrics compared to baselines while performing on par w.r.t sentence\nquality.", "published": "2019-08-22 17:59:08", "link": "http://arxiv.org/abs/1908.08529v1", "categories": ["cs.CV", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CV"}
{"title": "VL-BERT: Pre-training of Generic Visual-Linguistic Representations", "abstract": "We introduce a new pre-trainable generic representation for visual-linguistic\ntasks, called Visual-Linguistic BERT (VL-BERT for short). VL-BERT adopts the\nsimple yet powerful Transformer model as the backbone, and extends it to take\nboth visual and linguistic embedded features as input. In it, each element of\nthe input is either of a word from the input sentence, or a region-of-interest\n(RoI) from the input image. It is designed to fit for most of the\nvisual-linguistic downstream tasks. To better exploit the generic\nrepresentation, we pre-train VL-BERT on the massive-scale Conceptual Captions\ndataset, together with text-only corpus. Extensive empirical analysis\ndemonstrates that the pre-training procedure can better align the\nvisual-linguistic clues and benefit the downstream tasks, such as visual\ncommonsense reasoning, visual question answering and referring expression\ncomprehension. It is worth noting that VL-BERT achieved the first place of\nsingle model on the leaderboard of the VCR benchmark. Code is released at\n\\url{https://github.com/jackroos/VL-BERT}.", "published": "2019-08-22 17:59:30", "link": "http://arxiv.org/abs/1908.08530v4", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Feedbackward Decoding for Semantic Segmentation", "abstract": "We propose a novel approach for semantic segmentation that uses an encoder in\nthe reverse direction to decode. Many semantic segmentation networks adopt a\nfeedforward encoder-decoder architecture. Typically, an input is first\ndownsampled by the encoder to extract high-level semantic features and\ncontinues to be fed forward through the decoder module to recover low-level\nspatial clues. Our method works in an alternative direction that lets\ninformation flow backward from the last layer of the encoder towards the first.\nThe encoder performs encoding in the forward pass and the same network performs\ndecoding in the backward pass. Therefore, the encoder itself is also the\ndecoder. Compared to conventional encoder-decoder architectures, ours doesn't\nrequire additional layers for decoding and further reuses the encoder weights\nthereby reducing the total number of parameters required for processing. We\nshow by using only the 13 convolutional layers from VGG-16 plus one tiny\nclassification layer, our model significantly outperforms other frequently\ncited models that are also adapted from VGG-16. On the Cityscapes semantic\nsegmentation benchmark, our model uses 50.0% less parameters than SegNet and\nachieves an 18.1% higher \"IoU class\" score; it uses 28.3% less parameters than\nDeepLab LargeFOV and the achieved \"IoU class\" score is 3.9% higher; it uses\n89.1% fewer parameters than FCN-8s and the achieved \"IoU class\" score is 3.1%\nhigher. Our code will be publicly available on Github later.", "published": "2019-08-22 20:29:05", "link": "http://arxiv.org/abs/1908.08584v1", "categories": ["cs.CV", "cs.CL", "eess.IV", "stat.ML"], "primary_category": "cs.CV"}
{"title": "Gender Prediction from Tweets: Improving Neural Representations with\n  Hand-Crafted Features", "abstract": "Author profiling is the characterization of an author through some key\nattributes such as gender, age, and language. In this paper, a RNN model with\nAttention (RNNwA) is proposed to predict the gender of a twitter user using\ntheir tweets. Both word level and tweet level attentions are utilized to learn\n'where to look'. This model\n(https://github.com/Darg-Iztech/gender-prediction-from-tweets) is improved by\nconcatenating LSA-reduced n-gram features with the learned neural\nrepresentation of a user. Both models are tested on three languages: English,\nSpanish, Arabic. The improved version of the proposed model (RNNwA + n-gram)\nachieves state-of-the-art performance on English and has competitive results on\nSpanish and Arabic.", "published": "2019-08-22 07:36:48", "link": "http://arxiv.org/abs/1908.09919v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Sign Language Recognition, Generation, and Translation: An\n  Interdisciplinary Perspective", "abstract": "Developing successful sign language recognition, generation, and translation\nsystems requires expertise in a wide range of fields, including computer\nvision, computer graphics, natural language processing, human-computer\ninteraction, linguistics, and Deaf culture. Despite the need for deep\ninterdisciplinary knowledge, existing research occurs in separate disciplinary\nsilos, and tackles separate portions of the sign language processing pipeline.\nThis leads to three key questions: 1) What does an interdisciplinary view of\nthe current landscape reveal? 2) What are the biggest challenges facing the\nfield? and 3) What are the calls to action for people working in the field? To\nhelp answer these questions, we brought together a diverse group of experts for\na two-day workshop. This paper presents the results of that interdisciplinary\nworkshop, providing key background that is often overlooked by computer\nscientists, a review of the state-of-the-art, a set of pressing challenges, and\na call to action for the research community.", "published": "2019-08-22 21:05:17", "link": "http://arxiv.org/abs/1908.08597v1", "categories": ["cs.CV", "cs.CL", "cs.CY", "cs.GR", "cs.HC"], "primary_category": "cs.CV"}
{"title": "Sound Localization and Separation in Three-dimensional Space Using a\n  Single Microphone with a Metamaterial Enclosure", "abstract": "Conventional approaches to sound localization and separation are based on\nmicrophone arrays in artificial systems. Inspired by the selective perception\nof human auditory system, we design a multi-source listening system which can\nseparate simultaneous overlapping sounds and localize the sound sources in\nthree-dimensional space, using only a single microphone with a metamaterial\nenclosure. The enclosure modifies the frequency response of the microphone in a\ndirection-dependent way by giving each direction a signature. Thus, the\ninformation about the location and audio content of sound sources can be\nexperimentally reconstructed from the modulated mixed signals using compressive\nsensing algorithm. Owing to the low computational complexity of the proposed\nreconstruction algorithm, the designed system can also be applied in source\nidentification and tracking. The effectiveness of the system in multiple real\nscenarios has been proved through multiple random listening tests. The proposed\nmetamaterial-based single-sensor listening system opens a new way of sound\nlocalization and separation, which can be applied to intelligent scene\nmonitoring and robot audition.", "published": "2019-08-22 01:20:31", "link": "http://arxiv.org/abs/1908.08160v2", "categories": ["cs.SD", "eess.AS", "physics.app-ph"], "primary_category": "cs.SD"}
