{"title": "From Machine Translation to Code-Switching: Generating High-Quality\n  Code-Switched Text", "abstract": "Generating code-switched text is a problem of growing interest, especially\ngiven the scarcity of corpora containing large volumes of real code-switched\ntext. In this work, we adapt a state-of-the-art neural machine translation\nmodel to generate Hindi-English code-switched sentences starting from\nmonolingual Hindi sentences. We outline a carefully designed curriculum of\npretraining steps, including the use of synthetic code-switched text, that\nenable the model to generate high-quality code-switched text. Using text\ngenerated from our model as data augmentation, we show significant reductions\nin perplexity on a language modeling task, compared to using text from other\ngenerative models of CS text. We also show improvements using our text for a\ndownstream code-switched natural language inference task. Our generated text is\nfurther subjected to a rigorous evaluation using a human evaluation study and a\nrange of objective metrics, where we show performance comparable (and sometimes\neven superior) to code-switched text obtained via crowd workers who are native\nHindi speakers.", "published": "2021-07-14 04:46:39", "link": "http://arxiv.org/abs/2107.06483v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Importance-based Neuron Allocation for Multilingual Neural Machine\n  Translation", "abstract": "Multilingual neural machine translation with a single model has drawn much\nattention due to its capability to deal with multiple languages. However, the\ncurrent multilingual translation paradigm often makes the model tend to\npreserve the general knowledge, but ignore the language-specific knowledge.\nSome previous works try to solve this problem by adding various kinds of\nlanguage-specific modules to the model, but they suffer from the parameter\nexplosion problem and require specialized manual design. To solve these\nproblems, we propose to divide the model neurons into general and\nlanguage-specific parts based on their importance across languages. The general\npart is responsible for preserving the general knowledge and participating in\nthe translation of all the languages, while the language-specific part is\nresponsible for preserving the language-specific knowledge and participating in\nthe translation of some specific languages. Experimental results on several\nlanguage pairs, covering IWSLT and Europarl corpus datasets, demonstrate the\neffectiveness and universality of the proposed method.", "published": "2021-07-14 09:15:05", "link": "http://arxiv.org/abs/2107.06569v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ParCourE: A Parallel Corpus Explorer for a Massively Multilingual Corpus", "abstract": "With more than 7000 languages worldwide, multilingual natural language\nprocessing (NLP) is essential both from an academic and commercial perspective.\nResearching typological properties of languages is fundamental for progress in\nmultilingual NLP. Examples include assessing language similarity for effective\ntransfer learning, injecting inductive biases into machine learning models or\ncreating resources such as dictionaries and inflection tables. We provide\nParCourE, an online tool that allows to browse a word-aligned parallel corpus,\ncovering 1334 languages. We give evidence that this is useful for typological\nresearch. ParCourE can be set up for any parallel corpus and can thus be used\nfor typological research on other corpora as well as for exploring their\nquality and properties.", "published": "2021-07-14 12:16:21", "link": "http://arxiv.org/abs/2107.06632v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transition-based Bubble Parsing: Improvements on Coordination Structure\n  Prediction", "abstract": "We propose a transition-based bubble parser to perform coordination structure\nidentification and dependency-based syntactic analysis simultaneously. Bubble\nrepresentations were proposed in the formal linguistics literature decades ago;\nthey enhance dependency trees by encoding coordination boundaries and internal\nrelationships within coordination structures explicitly. In this paper, we\nintroduce a transition system and neural models for parsing these\nbubble-enhanced structures. Experimental results on the English Penn Treebank\nand the English GENIA corpus show that our parsers beat previous\nstate-of-the-art approaches on the task of coordination structure prediction,\nespecially for the subset of sentences with complex coordination structures.", "published": "2021-07-14 18:00:05", "link": "http://arxiv.org/abs/2107.06905v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "TGIF: Tree-Graph Integrated-Format Parser for Enhanced UD with Two-Stage\n  Generic- to Individual-Language Finetuning", "abstract": "We present our contribution to the IWPT 2021 shared task on parsing into\nenhanced Universal Dependencies. Our main system component is a hybrid\ntree-graph parser that integrates (a) predictions of spanning trees for the\nenhanced graphs with (b) additional graph edges not present in the spanning\ntrees. We also adopt a finetuning strategy where we first train a\nlanguage-generic parser on the concatenation of data from all available\nlanguages, and then, in a second step, finetune on each individual language\nseparately. Additionally, we develop our own complete set of pre-processing\nmodules relevant to the shared task, including tokenization, sentence\nsegmentation, and multiword token expansion, based on pre-trained XLM-R models\nand our own pre-training of character-level language models. Our submission\nreaches a macro-average ELAS of 89.24 on the test set. It ranks top among all\nteams, with a margin of more than 2 absolute ELAS over the next best-performing\nsubmission, and best score on 16 out of 17 languages.", "published": "2021-07-14 18:00:08", "link": "http://arxiv.org/abs/2107.06907v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Increasing Faithfulness in Knowledge-Grounded Dialogue with Controllable\n  Features", "abstract": "Knowledge-grounded dialogue systems are intended to convey information that\nis based on evidence provided in a given source text. We discuss the challenges\nof training a generative neural dialogue model for such systems that is\ncontrolled to stay faithful to the evidence. Existing datasets contain a mix of\nconversational responses that are faithful to selected evidence as well as more\nsubjective or chit-chat style responses. We propose different evaluation\nmeasures to disentangle these different styles of responses by quantifying the\ninformativeness and objectivity. At training time, additional inputs based on\nthese evaluation measures are given to the dialogue model. At generation time,\nthese additional inputs act as stylistic controls that encourage the model to\ngenerate responses that are faithful to the provided evidence. We also\ninvestigate the usage of additional controls at decoding time using resampling\ntechniques. In addition to automatic metrics, we perform a human evaluation\nstudy where raters judge the output of these controlled generation models to be\ngenerally more objective and faithful to the evidence compared to baseline\ndialogue systems.", "published": "2021-07-14 19:52:12", "link": "http://arxiv.org/abs/2107.06963v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deduplicating Training Data Makes Language Models Better", "abstract": "We find that existing language modeling datasets contain many near-duplicate\nexamples and long repetitive substrings. As a result, over 1% of the unprompted\noutput of language models trained on these datasets is copied verbatim from the\ntraining data. We develop two tools that allow us to deduplicate training\ndatasets -- for example removing from C4 a single 61 word English sentence that\nis repeated over 60,000 times. Deduplication allows us to train models that\nemit memorized text ten times less frequently and require fewer train steps to\nachieve the same or better accuracy. We can also reduce train-test overlap,\nwhich affects over 4% of the validation set of standard datasets, thus allowing\nfor more accurate evaluation. We release code for reproducing our work and\nperforming dataset deduplication at\nhttps://github.com/google-research/deduplicate-text-datasets.", "published": "2021-07-14 06:06:52", "link": "http://arxiv.org/abs/2107.06499v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning Algebraic Recombination for Compositional Generalization", "abstract": "Neural sequence models exhibit limited compositional generalization ability\nin semantic parsing tasks. Compositional generalization requires algebraic\nrecombination, i.e., dynamically recombining structured expressions in a\nrecursive manner. However, most previous studies mainly concentrate on\nrecombining lexical units, which is an important but not sufficient part of\nalgebraic recombination. In this paper, we propose LeAR, an end-to-end neural\nmodel to learn algebraic recombination for compositional generalization. The\nkey insight is to model the semantic parsing task as a homomorphism between a\nlatent syntactic algebra and a semantic algebra, thus encouraging algebraic\nrecombination. Specifically, we learn two modules jointly: a Composer for\nproducing latent syntax, and an Interpreter for assigning semantic operations.\nExperiments on two realistic and comprehensive compositional generalization\nbenchmarks demonstrate the effectiveness of our model. The source code is\npublicly available at https://github.com/microsoft/ContextualSP.", "published": "2021-07-14 07:23:46", "link": "http://arxiv.org/abs/2107.06516v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Indonesia's Fake News Detection using Transformer Network", "abstract": "Fake news is a problem faced by society in this era. It is not rare for fake\nnews to cause provocation and problem for the people. Indonesia, as a country\nwith the 4th largest population, has a problem in dealing with fake news. More\nthan 30% of rural and urban population are deceived by this fake news problem.\nAs we have been studying, there is only few literatures on preventing the\nspread of fake news in Bahasa Indonesia. So, this research is conducted to\nprevent these problems. The dataset used in this research was obtained from a\nnews portal that identifies fake news, turnbackhoax.id. Using Web Scrapping on\nthis page, we got 1116 data consisting of valid news and fake news. The dataset\ncan be accessed at https://github.com/JibranFawaid/turnbackhoax-dataset. This\ndataset will be combined with other available datasets. The methods used are\nCNN, BiLSTM, Hybrid CNN-BiLSTM, and BERT with Transformer Network. This\nresearch shows that the BERT method with Transformer Network has the best\nresults with an accuracy of up to 90%.", "published": "2021-07-14 15:52:15", "link": "http://arxiv.org/abs/2107.06796v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BERT Fine-Tuning for Sentiment Analysis on Indonesian Mobile Apps\n  Reviews", "abstract": "User reviews have an essential role in the success of the developed mobile\napps. User reviews in the textual form are unstructured data, creating a very\nhigh complexity when processed for sentiment analysis. Previous approaches that\nhave been used often ignore the context of reviews. In addition, the relatively\nsmall data makes the model overfitting. A new approach, BERT, has been\nintroduced as a transfer learning model with a pre-trained model that has\npreviously been trained to have a better context representation. This study\nexamines the effectiveness of fine-tuning BERT for sentiment analysis using two\ndifferent pre-trained models. Besides the multilingual pre-trained model, we\nuse the pre-trained model that only has been trained in Indonesian. The dataset\nused is Indonesian user reviews of the ten best apps in 2020 in Google Play\nsites. We also perform hyper-parameter tuning to find the optimum trained\nmodel. Two training data labeling approaches were also tested to determine the\neffectiveness of the model, which is score-based and lexicon-based. The\nexperimental results show that pre-trained models trained in Indonesian have\nbetter average accuracy on lexicon-based data. The pre-trained Indonesian model\nhighest accuracy is 84%, with 25 epochs and a training time of 24 minutes.\nThese results are better than all of the machine learning and multilingual\npre-trained models.", "published": "2021-07-14 16:00:15", "link": "http://arxiv.org/abs/2107.06802v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Composing Conversational Negation", "abstract": "Negation in natural language does not follow Boolean logic and is therefore\ninherently difficult to model. In particular, it takes into account the broader\nunderstanding of what is being negated. In previous work, we proposed a\nframework for the negation of words that accounts for 'worldly context'. This\npaper extends that proposal now accounting for the compositional structure\ninherent in language within the DisCoCirc framework. We compose the negations\nof single words to capture the negation of sentences. We also describe how to\nmodel the negation of words whose meanings evolve in the text.", "published": "2021-07-14 16:24:41", "link": "http://arxiv.org/abs/2107.06820v2", "categories": ["cs.CL", "math.CT"], "primary_category": "cs.CL"}
{"title": "\"How to best say it?\" : Translating Directives in Machine Language into\n  Natural Language in the Blocks World", "abstract": "We propose a method to generate optimal natural language for block placement\ndirectives generated by a machine's planner during human-agent interactions in\nthe blocks world. A non user-friendly machine directive, e.g., move(ObjId,\ntoPos), is transformed into visually and contextually grounded referring\nexpressions that are much easier for the user to comprehend. We describe an\nalgorithm that progressively and generatively transforms the machine's\ndirective in ECI (Elementary Composable Ideas)-space, generating many\nalternative versions of the directive. We then define a cost function to\nevaluate the ease of comprehension of these alternatives and select the best\noption. The parameters for this cost function were derived empirically from a\nuser study that measured utterance-to-action timings.", "published": "2021-07-14 17:59:08", "link": "http://arxiv.org/abs/2107.06886v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "From Show to Tell: A Survey on Deep Learning-based Image Captioning", "abstract": "Connecting Vision and Language plays an essential role in Generative\nIntelligence. For this reason, large research efforts have been devoted to\nimage captioning, i.e. describing images with syntactically and semantically\nmeaningful sentences. Starting from 2015 the task has generally been addressed\nwith pipelines composed of a visual encoder and a language model for text\ngeneration. During these years, both components have evolved considerably\nthrough the exploitation of object regions, attributes, the introduction of\nmulti-modal connections, fully-attentive approaches, and BERT-like early-fusion\nstrategies. However, regardless of the impressive results, research in image\ncaptioning has not reached a conclusive answer yet. This work aims at providing\na comprehensive overview of image captioning approaches, from visual encoding\nand text generation to training strategies, datasets, and evaluation metrics.\nIn this respect, we quantitatively compare many relevant state-of-the-art\napproaches to identify the most impactful technical innovations in\narchitectures and training strategies. Moreover, many variants of the problem\nand its open challenges are discussed. The final goal of this work is to serve\nas a tool for understanding the existing literature and highlighting the future\ndirections for a research area where Computer Vision and Natural Language\nProcessing can find an optimal synergy.", "published": "2021-07-14 18:00:54", "link": "http://arxiv.org/abs/2107.06912v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "HTLM: Hyper-Text Pre-Training and Prompting of Language Models", "abstract": "We introduce HTLM, a hyper-text language model trained on a large-scale web\ncrawl. Modeling hyper-text has a number of advantages: (1) it is easily\ngathered at scale, (2) it provides rich document-level and end-task-adjacent\nsupervision (e.g. class and id attributes often encode document category\ninformation), and (3) it allows for new structured prompting that follows the\nestablished semantics of HTML (e.g. to do zero-shot summarization by infilling\ntitle tags for a webpage that contains the input text). We show that\npretraining with a BART-style denoising loss directly on simplified HTML\nprovides highly effective transfer for a wide range of end tasks and\nsupervision levels. HTLM matches or exceeds the performance of comparably sized\ntext-only LMs for zero-shot prompting and fine-tuning for classification\nbenchmarks, while also setting new state-of-the-art performance levels for\nzero-shot summarization. We also find that hyper-text prompts provide more\nvalue to HTLM, in terms of data efficiency, than plain text prompts do for\nexisting LMs, and that HTLM is highly effective at auto-prompting itself, by\nsimply generating the most likely hyper-text formatting for any available\ntraining data. We will release all code and models to support future HTLM\nresearch.", "published": "2021-07-14 19:39:31", "link": "http://arxiv.org/abs/2107.06955v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Linking Health News to Research Literature", "abstract": "Accurately linking news articles to scientific research works is a critical\ncomponent in a number of applications, such as measuring the social impact of a\nresearch work and detecting inaccuracies or distortions in science news.\nAlthough the lack of links between news and literature has been a challenge in\nthese applications, it is a relatively unexplored research problem. In this\npaper we designed and evaluated a new approach that consists of (1) augmenting\nlatest named-entity recognition techniques to extract various metadata, and (2)\ndesigning a new elastic search engine that can facilitate the use of enriched\nmetadata queries. To evaluate our approach, we constructed two datasets of\npaired news articles and research papers: one is used for training models to\nextract metadata, and the other for evaluation. Our experiments showed that the\nnew approach performed significantly better than a baseline approach used by\naltmetric.com (0.89 vs 0.32 in terms of top-1 accuracy). To further demonstrate\nthe effectiveness of the approach, we also conducted a study on 37,600\nhealth-related press releases published on EurekAlert!, which showed that our\napproach was able to identify the corresponding research papers with a top-1\naccuracy of at least 0.97.", "published": "2021-07-14 03:50:51", "link": "http://arxiv.org/abs/2107.06472v1", "categories": ["cs.IR", "cs.CL", "cs.DL"], "primary_category": "cs.IR"}
{"title": "Serialized Multi-Layer Multi-Head Attention for Neural Speaker Embedding", "abstract": "This paper proposes a serialized multi-layer multi-head attention for neural\nspeaker embedding in text-independent speaker verification. In prior works,\nframe-level features from one layer are aggregated to form an utterance-level\nrepresentation. Inspired by the Transformer network, our proposed method\nutilizes the hierarchical architecture of stacked self-attention mechanisms to\nderive refined features that are more correlated with speakers. Serialized\nattention mechanism contains a stack of self-attention modules to create\nfixed-dimensional representations of speakers. Instead of utilizing multi-head\nattention in parallel, the proposed serialized multi-layer multi-head attention\nis designed to aggregate and propagate attentive statistics from one layer to\nthe next in a serialized manner. In addition, we employ an input-aware query\nfor each utterance with the statistics pooling. With more layers stacked, the\nneural network can learn more discriminative speaker embeddings. Experiment\nresults on VoxCeleb1 dataset and SITW dataset show that our proposed method\noutperforms other baseline methods, including x-vectors and other x-vectors +\nconventional attentive pooling approaches by 9.7% in EER and 8.1% in DCF0.01.", "published": "2021-07-14 05:38:48", "link": "http://arxiv.org/abs/2107.06493v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ZR-2021VG: Zero-Resource Speech Challenge, Visually-Grounded Language\n  Modelling track, 2021 edition", "abstract": "We present the visually-grounded language modelling track that was introduced\nin the Zero-Resource Speech challenge, 2021 edition, 2nd round. We motivate the\nnew track and discuss participation rules in detail. We also present the two\nbaseline systems that were developed for this track.", "published": "2021-07-14 08:29:07", "link": "http://arxiv.org/abs/2107.06546v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "MMGCN: Multimodal Fusion via Deep Graph Convolution Network for Emotion\n  Recognition in Conversation", "abstract": "Emotion recognition in conversation (ERC) is a crucial component in affective\ndialogue systems, which helps the system understand users' emotions and\ngenerate empathetic responses. However, most works focus on modeling speaker\nand contextual information primarily on the textual modality or simply\nleveraging multimodal information through feature concatenation. In order to\nexplore a more effective way of utilizing both multimodal and long-distance\ncontextual information, we propose a new model based on multimodal fused graph\nconvolutional network, MMGCN, in this work. MMGCN can not only make use of\nmultimodal dependencies effectively, but also leverage speaker information to\nmodel inter-speaker and intra-speaker dependency. We evaluate our proposed\nmodel on two public benchmark datasets, IEMOCAP and MELD, and the results prove\nthe effectiveness of MMGCN, which outperforms other SOTA methods by a\nsignificant margin under the multimodal conversation setting.", "published": "2021-07-14 15:37:02", "link": "http://arxiv.org/abs/2107.06779v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Large-Scale News Classification using BERT Language Model: Spark NLP\n  Approach", "abstract": "The rise of big data analytics on top of NLP increases the computational\nburden for text processing at scale. The problems faced in NLP are very high\ndimensional text, so it takes a high computation resource. The MapReduce allows\nparallelization of large computations and can improve the efficiency of text\nprocessing. This research aims to study the effect of big data processing on\nNLP tasks based on a deep learning approach. We classify a big text of news\ntopics with fine-tuning BERT used pre-trained models. Five pre-trained models\nwith a different number of parameters were used in this study. To measure the\nefficiency of this method, we compared the performance of the BERT with the\npipelines from Spark NLP. The result shows that BERT without Spark NLP gives\nhigher accuracy compared to BERT with Spark NLP. The accuracy average and\ntraining time of all models using BERT is 0.9187 and 35 minutes while using\nBERT with Spark NLP pipeline is 0.8444 and 9 minutes. The bigger model will\ntake more computation resources and need a longer time to complete the tasks.\nHowever, the accuracy of BERT with Spark NLP only decreased by an average of\n5.7%, while the training time was reduced significantly by 62.9% compared to\nBERT without Spark NLP.", "published": "2021-07-14 15:42:15", "link": "http://arxiv.org/abs/2107.06785v2", "categories": ["cs.CL", "cs.CC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FST: the FAIR Speech Translation System for the IWSLT21 Multilingual\n  Shared Task", "abstract": "In this paper, we describe our end-to-end multilingual speech translation\nsystem submitted to the IWSLT 2021 evaluation campaign on the Multilingual\nSpeech Translation shared task. Our system is built by leveraging transfer\nlearning across modalities, tasks and languages. First, we leverage\ngeneral-purpose multilingual modules pretrained with large amounts of\nunlabelled and labelled data. We further enable knowledge transfer from the\ntext task to the speech task by training two tasks jointly. Finally, our\nmultilingual model is finetuned on speech translation task-specific data to\nachieve the best translation results. Experimental results show our system\noutperforms the reported systems, including both end-to-end and cascaded based\napproaches, by a large margin.\n  In some translation directions, our speech translation results evaluated on\nthe public Multilingual TEDx test set are even comparable with the ones from a\nstrong text-to-text translation system, which uses the oracle speech\ntranscripts as input.", "published": "2021-07-14 19:43:44", "link": "http://arxiv.org/abs/2107.06959v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Annotation and Classification of Evidence and Reasoning Revisions in\n  Argumentative Writing", "abstract": "Automated writing evaluation systems can improve students' writing insofar as\nstudents attend to the feedback provided and revise their essay drafts in ways\naligned with such feedback. Existing research on revision of argumentative\nwriting in such systems, however, has focused on the types of revisions\nstudents make (e.g., surface vs. content) rather than the extent to which\nrevisions actually respond to the feedback provided and improve the essay. We\nintroduce an annotation scheme to capture the nature of sentence-level\nrevisions of evidence use and reasoning (the `RER' scheme) and apply it to 5th-\nand 6th-grade students' argumentative essays. We show that reliable manual\nannotation can be achieved and that revision annotations correlate with a\nholistic assessment of essay improvement in line with the feedback provided.\nFurthermore, we explore the feasibility of automatically classifying revisions\naccording to our scheme.", "published": "2021-07-14 20:58:26", "link": "http://arxiv.org/abs/2107.06990v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Efficient DP-SGD Mechanism for Large Scale NLP Models", "abstract": "Recent advances in deep learning have drastically improved performance on\nmany Natural Language Understanding (NLU) tasks. However, the data used to\ntrain NLU models may contain private information such as addresses or phone\nnumbers, particularly when drawn from human subjects. It is desirable that\nunderlying models do not expose private information contained in the training\ndata. Differentially Private Stochastic Gradient Descent (DP-SGD) has been\nproposed as a mechanism to build privacy-preserving models. However, DP-SGD can\nbe prohibitively slow to train. In this work, we propose a more efficient\nDP-SGD for training using a GPU infrastructure and apply it to fine-tuning\nmodels based on LSTM and transformer architectures. We report faster training\ntimes, alongside accuracy, theoretical privacy guarantees and success of\nMembership inference attacks for our models and observe that fine-tuning with\nproposed variant of DP-SGD can yield competitive models without significant\ndegradation in training time and improvement in privacy protection. We also\nmake observations such as looser theoretical $\\epsilon, \\delta$ can translate\ninto significant practical privacy gains.", "published": "2021-07-14 15:23:27", "link": "http://arxiv.org/abs/2107.14586v3", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Scalable Optimal Transport in High Dimensions for Graph Distances,\n  Embedding Alignment, and More", "abstract": "The current best practice for computing optimal transport (OT) is via entropy\nregularization and Sinkhorn iterations. This algorithm runs in quadratic time\nas it requires the full pairwise cost matrix, which is prohibitively expensive\nfor large sets of objects. In this work we propose two effective log-linear\ntime approximations of the cost matrix: First, a sparse approximation based on\nlocality-sensitive hashing (LSH) and, second, a Nystr\\\"om approximation with\nLSH-based sparse corrections, which we call locally corrected Nystr\\\"om (LCN).\nThese approximations enable general log-linear time algorithms for\nentropy-regularized OT that perform well even for the complex, high-dimensional\nspaces common in deep learning. We analyse these approximations theoretically\nand evaluate them experimentally both directly and end-to-end as a component\nfor real-world applications. Using our approximations for unsupervised word\nembedding alignment enables us to speed up a state-of-the-art method by a\nfactor of 3 while also improving the accuracy by 3.1 percentage points without\nany additional model changes. For graph distance regression we propose the\ngraph transport network (GTN), which combines graph neural networks (GNNs) with\nenhanced Sinkhorn. GTN outcompetes previous models by 48% and still scales\nlog-linearly in the number of nodes.", "published": "2021-07-14 17:40:08", "link": "http://arxiv.org/abs/2107.06876v2", "categories": ["cs.LG", "cs.CL", "cs.DS", "cs.SI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "The Benchmark Lottery", "abstract": "The world of empirical machine learning (ML) strongly relies on benchmarks in\norder to determine the relative effectiveness of different algorithms and\nmethods. This paper proposes the notion of \"a benchmark lottery\" that describes\nthe overall fragility of the ML benchmarking process. The benchmark lottery\npostulates that many factors, other than fundamental algorithmic superiority,\nmay lead to a method being perceived as superior. On multiple benchmark setups\nthat are prevalent in the ML community, we show that the relative performance\nof algorithms may be altered significantly simply by choosing different\nbenchmark tasks, highlighting the fragility of the current paradigms and\npotential fallacious interpretation derived from benchmarking ML methods. Given\nthat every benchmark makes a statement about what it perceives to be important,\nwe argue that this might lead to biased progress in the community. We discuss\nthe implications of the observed phenomena and provide recommendations on\nmitigating them using multiple machine learning domains and communities as use\ncases, including natural language processing, computer vision, information\nretrieval, recommender systems, and reinforcement learning.", "published": "2021-07-14 21:08:30", "link": "http://arxiv.org/abs/2107.07002v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.IR"], "primary_category": "cs.LG"}
{"title": "Low complexity online convolutional beamforming", "abstract": "Convolutional beamformers integrate the multichannel linear prediction model\ninto beamformers, which provide good performance and optimality for joint\ndereverberation and noise reduction tasks. While longer filters are required to\nmodel long reverberation times, the computational burden of current online\nsolutions grows fast with the filter length and number of microphones. In this\nwork, we propose a low complexity convolutional beamformer using a Kalman\nfilter derived affine projection algorithm to solve the adaptive filtering\nproblem. The proposed solution is several orders of magnitude less complex than\ncomparable existing solutions while slightly outperforming them on the REVERB\nchallenge dataset.", "published": "2021-07-14 15:35:23", "link": "http://arxiv.org/abs/2107.06775v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Multi-Task Audio Source Separation", "abstract": "The audio source separation tasks, such as speech enhancement, speech\nseparation, and music source separation, have achieved impressive performance\nin recent studies. The powerful modeling capabilities of deep neural networks\ngive us hope for more challenging tasks. This paper launches a new multi-task\naudio source separation (MTASS) challenge to separate the speech, music, and\nnoise signals from the monaural mixture. First, we introduce the details of\nthis task and generate a dataset of mixtures containing speech, music, and\nbackground noises. Then, we propose an MTASS model in the complex domain to\nfully utilize the differences in spectral characteristics of the three audio\nsignals. In detail, the proposed model follows a two-stage pipeline, which\nseparates the three types of audio signals and then performs signal\ncompensation separately. After comparing different training targets, the\ncomplex ratio mask is selected as a more suitable target for the MTASS. The\nexperimental results also indicate that the residual signal compensation module\nhelps to recover the signals further. The proposed model shows significant\nadvantages in separation performance over several well-known separation models.", "published": "2021-07-14 03:04:33", "link": "http://arxiv.org/abs/2107.06467v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Localization Based Sequential Grouping for Continuous Speech Separation", "abstract": "This study investigates robust speaker localization for con-tinuous speech\nseparation and speaker diarization, where we use speaker directions to group\nnon-contiguous segments of the same speaker. Assuming that speakers do not move\nand are located in different directions, the direction of arrival (DOA)\ninformation provides an informative cue for accurate sequential grouping and\nspeaker diarization. Our system is block-online in the following sense. Given a\nblock of frames with at most two speakers, we apply a two-speaker separa-tion\nmodel to separate (and enhance) the speakers, estimate the DOA of each\nseparated speaker, and group the separation results across blocks based on the\nDOA estimates. Speaker diarization and speaker-attributed speech recognition\nresults on the LibriCSS corpus demonstrate the effectiveness of the proposed\nalgorithm.", "published": "2021-07-14 17:18:40", "link": "http://arxiv.org/abs/2107.06853v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Is Someone Speaking? Exploring Long-term Temporal Features for\n  Audio-visual Active Speaker Detection", "abstract": "Active speaker detection (ASD) seeks to detect who is speaking in a visual\nscene of one or more speakers. The successful ASD depends on accurate\ninterpretation of short-term and long-term audio and visual information, as\nwell as audio-visual interaction. Unlike the prior work where systems make\ndecision instantaneously using short-term features, we propose a novel\nframework, named TalkNet, that makes decision by taking both short-term and\nlong-term features into consideration. TalkNet consists of audio and visual\ntemporal encoders for feature representation, audio-visual cross-attention\nmechanism for inter-modality interaction, and a self-attention mechanism to\ncapture long-term speaking evidence. The experiments demonstrate that TalkNet\nachieves 3.5% and 2.2% improvement over the state-of-the-art systems on the\nAVA-ActiveSpeaker dataset and Columbia ASD dataset, respectively. Code has been\nmade available at: https://github.com/TaoRuijie/TalkNet_ASD.", "published": "2021-07-14 10:30:34", "link": "http://arxiv.org/abs/2107.06592v2", "categories": ["eess.AS", "cs.SD", "eess.IV"], "primary_category": "eess.AS"}
{"title": "The Period-Modulated Harmonic Locked Loop (PM-HLL): A low-effort\n  algorithm for rapid time-domain multi-periodicity estimation", "abstract": "Many speech and music analysis and processing schemes rely on an estimate of\nthe fundamental frequency $f_0$ of periodic signal components. Most established\nschemes apply rather unspecific signal models such as sinusoidal models to the\nestimation problem, which may limit time resolution and estimation accuracy.\nThis study proposes a novel time-domain locked-loop algorithm with low\ncomputational effort and low memory footprint for $f_0$ estimation. The loop\ncontrol signal is directly derived from the input time signal, using a harmonic\nsignal model. Theoretically, this allows for a noise-robust and rapid $f_0$\nestimation for periodic signals of arbitrary waveform, and without the\nrequirement of a prior frequency analysis. Several simulations with short\nsignals employing different types of periodicity and with added wide-band noise\nwere performed to demonstrate and evaluate the basic properties of the proposed\nalgorithm. Depending on the Signal-to-Noise Ratio (SNR), the estimator was\nfound to converge within 3-4 signal repetitions, even at SNR close to or below\n0dB. Furthermore, it was found to follow fundamental frequency sweeps with a\ndelay of less than one period and to track all tones of a three-tone musical\nchord signal simultaneously. Quasi-periodic sounds with shifted harmonics as\nwell as signals with stochastic periodicity were robustly tracked. Mean and\nstandard deviation of the estimation error, i.e., the difference between true\nand estimated $f_0$, were at or below 1 Hz in most cases. The results suggest\nthat the proposed algorithm may be applicable to low-delay speech and music\nanalysis and processing.", "published": "2021-07-14 12:33:16", "link": "http://arxiv.org/abs/2107.06645v2", "categories": ["cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "Federated Self-Training for Semi-Supervised Audio Recognition", "abstract": "Federated Learning is a distributed machine learning paradigm dealing with\ndecentralized and personal datasets. Since data reside on devices like\nsmartphones and virtual assistants, labeling is entrusted to the clients, or\nlabels are extracted in an automated way. Specifically, in the case of audio\ndata, acquiring semantic annotations can be prohibitively expensive and\ntime-consuming. As a result, an abundance of audio data remains unlabeled and\nunexploited on users' devices. Most existing federated learning approaches\nfocus on supervised learning without harnessing the unlabeled data. In this\nwork, we study the problem of semi-supervised learning of audio models via\nself-training in conjunction with federated learning. We propose FedSTAR to\nexploit large-scale on-device unlabeled data to improve the generalization of\naudio recognition models. We further demonstrate that self-supervised\npre-trained models can accelerate the training of on-device models,\nsignificantly improving convergence to within fewer training rounds. We conduct\nexperiments on diverse public audio classification datasets and investigate the\nperformance of our models under varying percentages of labeled and unlabeled\ndata. Notably, we show that with as little as 3% labeled data available,\nFedSTAR on average can improve the recognition rate by 13.28% compared to the\nfully supervised federated model.", "published": "2021-07-14 17:40:10", "link": "http://arxiv.org/abs/2107.06877v2", "categories": ["cs.LG", "cs.DC", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Leveraging Hierarchical Structures for Few-Shot Musical Instrument\n  Recognition", "abstract": "Deep learning work on musical instrument recognition has generally focused on\ninstrument classes for which we have abundant data. In this work, we exploit\nhierarchical relationships between instruments in a few-shot learning setup to\nenable classification of a wider set of musical instruments, given a few\nexamples at inference. We apply a hierarchical loss function to the training of\nprototypical networks, combined with a method to aggregate prototypes\nhierarchically, mirroring the structure of a predefined musical instrument\nhierarchy. These extensions require no changes to the network architecture and\nnew levels can be easily added or removed. Compared to a non-hierarchical\nfew-shot baseline, our method leads to a significant increase in classification\naccuracy and significant decrease mistake severity on instrument classes unseen\nin training.", "published": "2021-07-14 22:50:24", "link": "http://arxiv.org/abs/2107.07029v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
