{"title": "Automatic Severity Classification of Coronary Artery Disease via\n  Recurrent Capsule Network", "abstract": "Coronary artery disease (CAD) is one of the leading causes of cardiovascular\ndisease deaths. CAD condition progresses rapidly, if not diagnosed and treated\nat an early stage may eventually lead to an irreversible state of the heart\nmuscle death. Invasive coronary arteriography is the gold standard technique\nfor CAD diagnosis. Coronary arteriography texts describe which part has\nstenosis and how much stenosis is in details. It is crucial to conduct the\nseverity classification of CAD. In this paper, we employ a recurrent capsule\nnetwork (RCN) to extract semantic relations between clinical named entities in\nChinese coronary arteriography texts, through which we can automatically find\nout the maximal stenosis for each lumen to inference how severe CAD is\naccording to the improved method of Gensini. Experimental results on the corpus\ncollected from Shanghai Shuguang Hospital show that our proposed method\nachieves an accuracy of 97.0\\% in the severity classification of CAD.", "published": "2018-07-18 00:38:47", "link": "http://arxiv.org/abs/1807.06718v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Online Multitask Learning of Behavioral Sentence Embeddings", "abstract": "Unsupervised learning has been an attractive method for easily deriving\nmeaningful data representations from vast amounts of unlabeled data. These\nrepresentations, or embeddings, often yield superior results in many tasks,\nwhether used directly or as features in subsequent training stages. However,\nthe quality of the embeddings is highly dependent on the assumed knowledge in\nthe unlabeled data and how the system extracts information without supervision.\nDomain portability is also very limited in unsupervised learning, often\nrequiring re-training on other in-domain corpora to achieve robustness. In this\nwork we present a multitask paradigm for unsupervised contextual learning of\nbehavioral interactions which addresses unsupervised domain adaption. We\nintroduce an online multitask objective into unsupervised learning and show\nthat sentence embeddings generated through this process increases performance\nof affective tasks.", "published": "2018-07-18 06:39:07", "link": "http://arxiv.org/abs/1807.06792v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Distinct patterns of syntactic agreement errors in recurrent networks\n  and humans", "abstract": "Determining the correct form of a verb in context requires an understanding\nof the syntactic structure of the sentence. Recurrent neural networks have been\nshown to perform this task with an error rate comparable to humans, despite the\nfact that they are not designed with explicit syntactic representations. To\nexamine the extent to which the syntactic representations of these networks are\nsimilar to those used by humans when processing sentences, we compare the\ndetailed pattern of errors that RNNs and humans make on this task. Despite\nsignificant similarities (attraction errors, asymmetry between singular and\nplural subjects), the error patterns differed in important ways. In particular,\nin complex sentences with relative clauses error rates increased in RNNs but\ndecreased in humans. Furthermore, RNNs showed a cumulative effect of attractors\nbut humans did not. We conclude that at least in some respects the syntactic\nrepresentations acquired by RNNs are fundamentally different from those used by\nhumans.", "published": "2018-07-18 11:58:59", "link": "http://arxiv.org/abs/1807.06882v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is it worth it? Budget-related evaluation metrics for model selection", "abstract": "Creating a linguistic resource is often done by using a machine learning\nmodel that filters the content that goes through to a human annotator, before\ngoing into the final resource. However, budgets are often limited, and the\namount of available data exceeds the amount of affordable annotation. In order\nto optimize the benefit from the invested human work, we argue that deciding on\nwhich model one should employ depends not only on generalized evaluation\nmetrics such as F-score, but also on the gain metric. Because the model with\nthe highest F-score may not necessarily have the best sequencing of predicted\nclasses, this may lead to wasting funds on annotating false positives, yielding\nzero improvement of the linguistic resource. We exemplify our point with a case\nstudy, using real data from a task of building a verb-noun idiom dictionary. We\nshow that, given the choice of three systems with varying F-scores, the system\nwith the highest F-score does not yield the highest profits. In other words, in\nour case the cost-benefit trade off is more favorable for a system with a lower\nF-score.", "published": "2018-07-18 15:37:58", "link": "http://arxiv.org/abs/1807.06998v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hierarchical Multi Task Learning With CTC", "abstract": "In Automatic Speech Recognition it is still challenging to learn useful\nintermediate representations when using high-level (or abstract) target units\nsuch as words. For that reason, character or phoneme based systems tend to\noutperform word-based systems when just few hundreds of hours of training data\nare being used. In this paper, we first show how hierarchical multi-task\ntraining can encourage the formation of useful intermediate representations. We\nachieve this by performing Connectionist Temporal Classification at different\nlevels of the network with targets of different granularity. Our model thus\nperforms predictions in multiple scales for the same input. On the standard\n300h Switchboard training setup, our hierarchical multi-task architecture\nexhibits improvements over single-task architectures with the same number of\nparameters. Our model obtains 14.0% Word Error Rate on the Eval2000 Switchboard\nsubset without any decoder or language model, outperforming the current\nstate-of-the-art on acoustic-to-word models.", "published": "2018-07-18 18:57:37", "link": "http://arxiv.org/abs/1807.07104v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic Parsing: Syntactic assurance to target sentence using LSTM\n  Encoder CFG-Decoder", "abstract": "Semantic parsing can be defined as the process of mapping natural language\nsentences into a machine interpretable, formal representation of its meaning.\nSemantic parsing using LSTM encoder-decoder neural networks have become\npromising approach. However, human automated translation of natural language\ndoes not provide grammaticality guarantees for the sentences generate such a\nguarantee is particularly important for practical cases where a data base query\ncan cause critical errors if the sentence is ungrammatical. In this work, we\npropose an neural architecture called Encoder CFG-Decoder, whose output\nconforms to a given context-free grammar. Results are show for any\nimplementation of such architecture display its correctness and providing\nbenchmark accuracy levels better than the literature.", "published": "2018-07-18 19:10:45", "link": "http://arxiv.org/abs/1807.07108v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fake news as we feel it: perception and conceptualization of the term\n  \"fake news\" in the media", "abstract": "In this article, we quantitatively analyze how the term \"fake news\" is being\nshaped in news media in recent years. We study the perception and the\nconceptualization of this term in the traditional media using eight years of\ndata collected from news outlets based in 20 countries. Our results not only\ncorroborate previous indications of a high increase in the usage of the\nexpression \"fake news\", but also show contextual changes around this expression\nafter the United States presidential election of 2016. Among other results, we\nfound changes in the related vocabulary, in the mentioned entities, in the\nsurrounding topics and in the contextual polarity around the term \"fake news\",\nsuggesting that this expression underwent a change in perception and\nconceptualization after 2016. These outcomes expand the understandings on the\nusage of the term \"fake news\", helping to comprehend and more accurately\ncharacterize this relevant social phenomenon linked to misinformation and\nmanipulation.", "published": "2018-07-18 13:41:57", "link": "http://arxiv.org/abs/1807.06926v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Evaluating Word Embeddings in Multi-label Classification Using\n  Fine-grained Name Typing", "abstract": "Embedding models typically associate each word with a single real-valued\nvector, representing its different properties. Evaluation methods, therefore,\nneed to analyze the accuracy and completeness of these properties in\nembeddings. This requires fine-grained analysis of embedding subspaces.\nMulti-label classification is an appropriate way to do so. We propose a new\nevaluation method for word embeddings based on multi-label classification given\na word embedding. The task we use is fine-grained name typing: given a large\ncorpus, find all types that a name can refer to based on the name embedding.\nGiven the scale of entities in knowledge bases, we can build datasets for this\ntask that are complementary to the current embedding evaluation datasets in:\nthey are very large, contain fine-grained classes, and allow the direct\nevaluation of embeddings without confounding factors like sentence context", "published": "2018-07-18 23:38:08", "link": "http://arxiv.org/abs/1807.07186v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Forward Attention in Sequence-to-sequence Acoustic Modelling for Speech\n  Synthesis", "abstract": "This paper proposes a forward attention method for the sequenceto- sequence\nacoustic modeling of speech synthesis. This method is motivated by the nature\nof the monotonic alignment from phone sequences to acoustic sequences. Only the\nalignment paths that satisfy the monotonic condition are taken into\nconsideration at each decoder timestep. The modified attention probabilities at\neach timestep are computed recursively using a forward algorithm. A transition\nagent for forward attention is further proposed, which helps the attention\nmechanism to make decisions whether to move forward or stay at each decoder\ntimestep. Experimental results show that the proposed forward attention method\nachieves faster convergence speed and higher stability than the baseline\nattention method. Besides, the method of forward attention with transition\nagent can also help improve the naturalness of synthetic speech and control the\nspeed of synthetic speech effectively.", "published": "2018-07-18 01:59:26", "link": "http://arxiv.org/abs/1807.06736v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Improving Explainable Recommendations with Synthetic Reviews", "abstract": "An important task for a recommender system to provide interpretable\nexplanations for the user. This is important for the credibility of the system.\nCurrent interpretable recommender systems tend to focus on certain features\nknown to be important to the user and offer their explanations in a structured\nform. It is well known that user generated reviews and feedback from reviewers\nhave strong leverage over the users' decisions. On the other hand, recent text\ngeneration works have been shown to generate text of similar quality to human\nwritten text, and we aim to show that generated text can be successfully used\nto explain recommendations.\n  In this paper, we propose a framework consisting of popular review-oriented\ngeneration models aiming to create personalised explanations for\nrecommendations. The interpretations are generated at both character and word\nlevels. We build a dataset containing reviewers' feedback from the Amazon books\nreview dataset. Our cross-domain experiments are designed to bridge from\nnatural language processing to the recommender system domain. Besides language\nmodel evaluation methods, we employ DeepCoNN, a novel review-oriented\nrecommender system using a deep neural network, to evaluate the recommendation\nperformance of generated reviews by root mean square error (RMSE). We\ndemonstrate that the synthetic personalised reviews have better recommendation\nperformance than human written reviews. To our knowledge, this presents the\nfirst machine-generated natural language explanations for rating prediction.", "published": "2018-07-18 14:42:35", "link": "http://arxiv.org/abs/1807.06978v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Efficient Training on Very Large Corpora via Gramian Estimation", "abstract": "We study the problem of learning similarity functions over very large corpora\nusing neural network embedding models. These models are typically trained using\nSGD with sampling of random observed and unobserved pairs, with a number of\nsamples that grows quadratically with the corpus size, making it expensive to\nscale to very large corpora. We propose new efficient methods to train these\nmodels without having to sample unobserved pairs. Inspired by matrix\nfactorization, our approach relies on adding a global quadratic penalty to all\npairs of examples and expressing this term as the matrix-inner-product of two\ngeneralized Gramians. We show that the gradient of this term can be efficiently\ncomputed by maintaining estimates of the Gramians, and develop variance\nreduction schemes to improve the quality of the estimates. We conduct\nlarge-scale experiments that show a significant improvement in training time\nand generalization quality compared to traditional sampling methods.", "published": "2018-07-18 23:45:33", "link": "http://arxiv.org/abs/1807.07187v1", "categories": ["stat.ML", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Deep neural network based speech separation optimizing an objective\n  estimator of intelligibility for low latency applications", "abstract": "Mean square error (MSE) has been the preferred choice as loss function in the\ncurrent deep neural network (DNN) based speech separation techniques. In this\npaper, we propose a new cost function with the aim of optimizing the extended\nshort time objective intelligibility (ESTOI) measure. We focus on applications\nwhere low algorithmic latency ($\\leq 10$ ms) is important. We use long\nshort-term memory networks (LSTM) and evaluate our proposed approach on four\nsets of two-speaker mixtures from extended Danish hearing in noise (HINT)\ndataset. We show that the proposed loss function can offer improved or at par\nobjective intelligibility (in terms of ESTOI) compared to an MSE optimized\nbaseline while resulting in lower objective separation performance (in terms of\nthe source to distortion ratio (SDR)). We then proceed to propose an approach\nwhere the network is first initialized with weights optimized for MSE criterion\nand then trained with the proposed ESTOI loss criterion. This approach\nmitigates some of the losses in objective separation performance while\npreserving the gains in objective intelligibility.", "published": "2018-07-18 12:55:59", "link": "http://arxiv.org/abs/1807.06899v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
