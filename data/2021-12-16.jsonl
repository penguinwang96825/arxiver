{"title": "QAFactEval: Improved QA-Based Factual Consistency Evaluation for\n  Summarization", "abstract": "Factual consistency is an essential quality of text summarization models in\npractical settings. Existing work in evaluating this dimension can be broadly\ncategorized into two lines of research, entailment-based and question answering\n(QA)-based metrics, and different experimental setups often lead to contrasting\nconclusions as to which paradigm performs the best. In this work, we conduct an\nextensive comparison of entailment and QA-based metrics, demonstrating that\ncarefully choosing the components of a QA-based metric, especially question\ngeneration and answerability classification, is critical to performance.\nBuilding on those insights, we propose an optimized metric, which we call\nQAFactEval, that leads to a 14% average improvement over previous QA-based\nmetrics on the SummaC factual consistency benchmark, and also outperforms the\nbest-performing entailment-based metric. Moreover, we find that QA-based and\nentailment-based metrics can offer complementary signals and be combined into a\nsingle metric for a further performance boost.", "published": "2021-12-16 00:38:35", "link": "http://arxiv.org/abs/2112.08542v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Isochrony-Aware Neural Machine Translation for Automatic Dubbing", "abstract": "We introduce the task of isochrony-aware machine translation which aims at\ngenerating translations suitable for dubbing. Dubbing of a spoken sentence\nrequires transferring the content as well as the speech-pause structure of the\nsource into the target language to achieve audiovisual coherence. Practically,\nthis implies correctly projecting pauses from the source to the target and\nensuring that target speech segments have roughly the same duration of the\ncorresponding source speech segments. In this work, we propose implicit and\nexplicit modeling approaches to integrate isochrony information into neural\nmachine translation. Experiments on English-German/French language pairs with\nautomatic metrics show that the simplest of the considered approaches works\nbest. Results are confirmed by human evaluations of translations and dubbed\nvideos.", "published": "2021-12-16 01:11:08", "link": "http://arxiv.org/abs/2112.08548v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Content Extraction for Poster Generation of Scientific Papers", "abstract": "The problem of poster generation for scientific papers is under-investigated.\nPosters often present the most important information of papers, and the task\ncan be considered as a special form of document summarization. Previous studies\nfocus mainly on poster layout and panel composition, while neglecting the\nimportance of content extraction. Besides, their datasets are not publicly\navailable, which hinders further research. In this paper, we construct a\nbenchmark dataset from scratch for this task. Then we propose a three-step\nframework to tackle this task and focus on the content extraction step in this\nstudy. To get both textual and visual elements of a poster panel, a neural\nextractive model is proposed to extract text, figures and tables of a paper\nsection simultaneously. We conduct experiments on the dataset and also perform\nablation study. Results demonstrate the efficacy of our proposed model. The\ndataset and code will be released.", "published": "2021-12-16 01:19:37", "link": "http://arxiv.org/abs/2112.08550v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Deep Learning Approach for Ontology Enrichment from Unstructured Text", "abstract": "Information Security in the cyber world is a major cause for concern, with a\nsignificant increase in the number of attack surfaces. Existing information on\nvulnerabilities, attacks, controls, and advisories available on the web\nprovides an opportunity to represent knowledge and perform security analytics\nto mitigate some of the concerns. Representing security knowledge in the form\nof ontology facilitates anomaly detection, threat intelligence, reasoning and\nrelevance attribution of attacks, and many more. This necessitates dynamic and\nautomated enrichment of information security ontologies. However, existing\nontology enrichment algorithms based on natural language processing and ML\nmodels have issues with contextual extraction of concepts in words, phrases,\nand sentences. This motivates the need for sequential Deep Learning\narchitectures that traverse through dependency paths in text and extract\nembedded vulnerabilities, threats, controls, products, and other\nsecurity-related concepts and instances from learned path representations. In\nthe proposed approach, Bidirectional LSTMs trained on a large DBpedia dataset\nand Wikipedia corpus of 2.8 GB along with Universal Sentence Encoder is\ndeployed to enrich ISO 27001-based information security ontology. The model is\ntrained and tested on a high-performance computing (HPC) environment to handle\nWiki text dimensionality. The approach yielded a test accuracy of over 80% when\ntested with knocked-out concepts from ontology and web page instances to\nvalidate the robustness.", "published": "2021-12-16 01:32:21", "link": "http://arxiv.org/abs/2112.08554v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CONQRR: Conversational Query Rewriting for Retrieval with Reinforcement\n  Learning", "abstract": "Compared to standard retrieval tasks, passage retrieval for conversational\nquestion answering (CQA) poses new challenges in understanding the current user\nquestion, as each question needs to be interpreted within the dialogue context.\nMoreover, it can be expensive to re-train well-established retrievers such as\nsearch engines that are originally developed for non-conversational queries. To\nfacilitate their use, we develop a query rewriting model CONQRR that rewrites a\nconversational question in the context into a standalone question. It is\ntrained with a novel reward function to directly optimize towards retrieval\nusing reinforcement learning and can be adapted to any off-the-shelf retriever.\nCONQRR achieves state-of-the-art results on a recent open-domain CQA dataset\ncontaining conversations from three different sources, and is effective for two\ndifferent off-the-shelf retrievers. Our extensive analysis also shows the\nrobustness of CONQRR to out-of-domain dialogues as well as to zero query\nrewriting supervision.", "published": "2021-12-16 01:40:30", "link": "http://arxiv.org/abs/2112.08558v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Block-Skim: Efficient Question Answering for Transformer", "abstract": "Transformer models have achieved promising results on natural language\nprocessing (NLP) tasks including extractive question answering (QA). Common\nTransformer encoders used in NLP tasks process the hidden states of all input\ntokens in the context paragraph throughout all layers. However, different from\nother tasks such as sequence classification, answering the raised question does\nnot necessarily need all the tokens in the context paragraph. Following this\nmotivation, we propose Block-skim, which learns to skim unnecessary context in\nhigher hidden layers to improve and accelerate the Transformer performance. The\nkey idea of Block-Skim is to identify the context that must be further\nprocessed and those that could be safely discarded early on during inference.\nCritically, we find that such information could be sufficiently derived from\nthe self-attention weights inside the Transformer model. We further prune the\nhidden states corresponding to the unnecessary positions early in lower layers,\nachieving significant inference-time speedup. To our surprise, we observe that\nmodels pruned in this way outperform their full-size counterparts. Block-Skim\nimproves QA models' accuracy on different datasets and achieves 3 times speedup\non BERT-base model.", "published": "2021-12-16 01:45:33", "link": "http://arxiv.org/abs/2112.08560v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Multilinguality benefit Non-autoregressive Machine Translation?", "abstract": "Non-autoregressive (NAR) machine translation has recently achieved\nsignificant improvements, and now outperforms autoregressive (AR) models on\nsome benchmarks, providing an efficient alternative to AR inference. However,\nwhile AR translation is often implemented using multilingual models that\nbenefit from transfer between languages and from improved serving efficiency,\nmultilingual NAR models remain relatively unexplored. Taking Connectionist\nTemporal Classification (CTC) as an example NAR model and Imputer as a semi-NAR\nmodel, we present a comprehensive empirical study of multilingual NAR. We test\nits capabilities with respect to positive transfer between related languages\nand negative transfer under capacity constraints. As NAR models require\ndistilled training sets, we carefully study the impact of bilingual versus\nmultilingual teachers. Finally, we fit a scaling law for multilingual NAR,\nwhich quantifies its performance relative to the AR model as model scale\nincreases.", "published": "2021-12-16 02:20:59", "link": "http://arxiv.org/abs/2112.08570v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CLICKER: A Computational LInguistics Classification Scheme for\n  Educational Resources", "abstract": "A classification scheme of a scientific subject gives an overview of its body\nof knowledge. It can also be used to facilitate access to research articles and\nother materials related to the subject. For example, the ACM Computing\nClassification System (CCS) is used in the ACM Digital Library search interface\nand also for indexing computer science papers. We observed that a comprehensive\nclassification system like CCS or Mathematics Subject Classification (MSC) does\nnot exist for Computational Linguistics (CL) and Natural Language Processing\n(NLP). We propose a classification scheme -- CLICKER for CL/NLP based on the\nanalysis of online lectures from 77 university courses on this subject. The\ncurrently proposed taxonomy includes 334 topics and focuses on educational\naspects of CL/NLP; it is based primarily, but not exclusively, on lecture notes\nfrom NLP courses. We discuss how such a taxonomy can help in various real-world\napplications, including tutoring platforms, resource retrieval, resource\nrecommendation, prerequisite chain learning, and survey generation.", "published": "2021-12-16 02:40:43", "link": "http://arxiv.org/abs/2112.08578v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Does Pre-training Induce Systematic Inference? How Masked Language\n  Models Acquire Commonsense Knowledge", "abstract": "Transformer models pre-trained with a masked-language-modeling objective\n(e.g., BERT) encode commonsense knowledge as evidenced by behavioral probes;\nhowever, the extent to which this knowledge is acquired by systematic inference\nover the semantics of the pre-training corpora is an open question. To answer\nthis question, we selectively inject verbalized knowledge into the minibatches\nof a BERT model during pre-training and evaluate how well the model generalizes\nto supported inferences. We find generalization does not improve over the\ncourse of pre-training, suggesting that commonsense knowledge is acquired from\nsurface-level, co-occurrence patterns rather than induced, systematic\nreasoning.", "published": "2021-12-16 03:13:04", "link": "http://arxiv.org/abs/2112.08583v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Idiomatic Expression Paraphrasing without Strong Supervision", "abstract": "Idiomatic expressions (IEs) play an essential role in natural language. In\nthis paper, we study the task of idiomatic sentence paraphrasing (ISP), which\naims to paraphrase a sentence with an IE by replacing the IE with its literal\nparaphrase. The lack of large-scale corpora with idiomatic-literal parallel\nsentences is a primary challenge for this task, for which we consider two\nseparate solutions. First, we propose an unsupervised approach to ISP, which\nleverages an IE's contextual information and definition and does not require a\nparallel sentence training set. Second, we propose a weakly supervised approach\nusing back-translation to jointly perform paraphrasing and generation of\nsentences with IEs to enlarge the small-scale parallel sentence training\ndataset. Other significant derivatives of the study include a model that\nreplaces a literal phrase in a sentence with an IE to generate an idiomatic\nexpression and a large scale parallel dataset with idiomatic/literal sentence\npairs. The effectiveness of the proposed solutions compared to competitive\nbaselines is seen in the relative gains of over 5.16 points in BLEU, over 8.75\npoints in METEOR, and over 19.57 points in SARI when the generated sentences\nare empirically validated on a parallel dataset using automatic and manual\nevaluations. We demonstrate the practical utility of ISP as a preprocessing\nstep in En-De machine translation.", "published": "2021-12-16 03:29:31", "link": "http://arxiv.org/abs/2112.08592v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Guiding Neural Story Generation with Reader Models", "abstract": "Automated storytelling has long captured the attention of researchers for the\nubiquity of narratives in everyday life. However, it is challenging to maintain\ncoherence and stay on-topic toward a specific ending when generating narratives\nwith neural language models. In this paper, we introduce Story generation with\nReader Models (StoRM), a framework in which a reader model is used to reason\nabout the story should progress. A reader model infers what a human reader\nbelieves about the concepts, entities, and relations about the fictional story\nworld. We show how an explicit reader model represented as a knowledge graph\naffords story coherence and provides controllability in the form of achieving a\ngiven story world state goal. Experiments show that our model produces\nsignificantly more coherent and on-topic stories, outperforming baselines in\ndimensions including plot plausibility and staying on topic.", "published": "2021-12-16 03:44:01", "link": "http://arxiv.org/abs/2112.08596v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "QuALITY: Question Answering with Long Input Texts, Yes!", "abstract": "To enable building and testing models on long-document comprehension, we\nintroduce QuALITY, a multiple-choice QA dataset with context passages in\nEnglish that have an average length of about 5,000 tokens, much longer than\ntypical current models can process. Unlike in prior work with passages, our\nquestions are written and validated by contributors who have read the entire\npassage, rather than relying on summaries or excerpts. In addition, only half\nof the questions are answerable by annotators working under tight time\nconstraints, indicating that skimming and simple search are not enough to\nconsistently perform well. Our baseline models perform poorly on this task\n(55.4%) and significantly lag behind human performance (93.5%).", "published": "2021-12-16 04:14:38", "link": "http://arxiv.org/abs/2112.08608v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DuQM: A Chinese Dataset of Linguistically Perturbed Natural Questions\n  for Evaluating the Robustness of Question Matching Models", "abstract": "In this paper, we focus on studying robustness evaluation of Chinese question\nmatching. Most of the previous work on analyzing robustness issue focus on just\none or a few types of artificial adversarial examples. Instead, we argue that\nit is necessary to formulate a comprehensive evaluation about the linguistic\ncapabilities of models on natural texts. For this purpose, we create a Chinese\ndataset namely DuQM which contains natural questions with linguistic\nperturbations to evaluate the robustness of question matching models. DuQM\ncontains 3 categories and 13 subcategories with 32 linguistic perturbations.\nThe extensive experiments demonstrate that DuQM has a better ability to\ndistinguish different models. Importantly, the detailed breakdown of evaluation\nby linguistic phenomenon in DuQM helps us easily diagnose the strength and\nweakness of different models. Additionally, our experiment results show that\nthe effect of artificial adversarial examples does not work on the natural\ntexts.", "published": "2021-12-16 04:16:39", "link": "http://arxiv.org/abs/2112.08609v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KAT: A Knowledge Augmented Transformer for Vision-and-Language", "abstract": "The primary focus of recent work with largescale transformers has been on\noptimizing the amount of information packed into the model's parameters. In\nthis work, we ask a different question: Can multimodal transformers leverage\nexplicit knowledge in their reasoning? Existing, primarily unimodal, methods\nhave explored approaches under the paradigm of knowledge retrieval followed by\nanswer prediction, but leave open questions about the quality and relevance of\nthe retrieved knowledge used, and how the reasoning processes over implicit and\nexplicit knowledge should be integrated. To address these challenges, we\npropose a novel model - Knowledge Augmented Transformer (KAT) - which achieves\na strong state-of-the-art result (+6 points absolute) on the open-domain\nmultimodal task of OK-VQA. Our approach integrates implicit and explicit\nknowledge in an end to end encoder-decoder architecture, while still jointly\nreasoning over both knowledge sources during answer generation. An additional\nbenefit of explicit knowledge integration is seen in improved interpretability\nof model predictions in our analysis.", "published": "2021-12-16 04:37:10", "link": "http://arxiv.org/abs/2112.08614v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge-Augmented Language Models for Cause-Effect Relation\n  Classification", "abstract": "Previous studies have shown the efficacy of knowledge augmentation methods in\npretrained language models. However, these methods behave differently across\ndomains and downstream tasks. In this work, we investigate the augmentation of\npretrained language models with commonsense knowledge in the cause-effect\nrelation classification and commonsense causal reasoning tasks. After\nautomatically verbalizing ATOMIC2020, a wide coverage commonsense reasoning\nknowledge graph, and GLUCOSE, a dataset of implicit commonsense causal\nknowledge, we continually pretrain BERT and RoBERTa with the verbalized data.\nThen we evaluate the resulting models on cause-effect pair classification and\nanswering commonsense causal reasoning questions. Our results show that\ncontinually pretrained language models augmented with commonsense knowledge\noutperform our baselines on two commonsense causal reasoning benchmarks, COPA\nand BCOPA-CE, and the Temporal and Causal Reasoning (TCR) dataset, without\nadditional improvement in model architecture or using quality-enhanced data for\nfine-tuning.", "published": "2021-12-16 04:38:40", "link": "http://arxiv.org/abs/2112.08615v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FRUIT: Faithfully Reflecting Updated Information in Text", "abstract": "Textual knowledge bases such as Wikipedia require considerable effort to keep\nup to date and consistent. While automated writing assistants could potentially\nease this burden, the problem of suggesting edits grounded in external\nknowledge has been under-explored. In this paper, we introduce the novel\ngeneration task of *faithfully reflecting updated information in text* (FRUIT)\nwhere the goal is to update an existing article given new evidence. We release\nthe FRUIT-WIKI dataset, a collection of over 170K distantly supervised data\nproduced from pairs of Wikipedia snapshots, along with our data generation\npipeline and a gold evaluation set of 914 instances whose edits are guaranteed\nto be supported by the evidence. We provide benchmark results for popular\ngeneration systems as well as EDIT5 -- a T5-based approach tailored to editing\nwe introduce that establishes the state of the art. Our analysis shows that\ndeveloping models that can update articles faithfully requires new capabilities\nfor neural generation models, and opens doors to many new applications.", "published": "2021-12-16 05:21:24", "link": "http://arxiv.org/abs/2112.08634v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reconsidering the Past: Optimizing Hidden States in Language Models", "abstract": "We present Hidden-State Optimization (HSO), a gradient-based method for\nimproving the performance of transformer language models at inference time.\nSimilar to dynamic evaluation (Krause et al., 2018), HSO computes the gradient\nof the log-probability the language model assigns to an evaluation text, but\nuses it to update the cached hidden states rather than the model parameters. We\ntest HSO with pretrained Transformer-XL and GPT-2 language models, finding\nimprovement on the WikiText103 and PG-19 datasets in terms of perplexity,\nespecially when evaluating a model outside of its training distribution. We\nalso demonstrate downstream applicability by showing gains in the recently\ndeveloped prompt-based few-shot evaluation setting, again with no extra\nparameters or training data.", "published": "2021-12-16 06:14:37", "link": "http://arxiv.org/abs/2112.08653v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Taming Repetition in Dialogue Generation", "abstract": "The wave of pre-training language models has been continuously improving the\nquality of the machine-generated conversations, however, some of the generated\nresponses still suffer from excessive repetition, sometimes repeating words\nfrom utterance, sometimes repeating words within self-generated responses, or\nboth. Inappropriate repetition of words can significantly degrade the quality\nof the generated texts. Penalized sampling is one popular solution, reducing\nthe sampling probability of existing words during inference, however, it is\nhighly vulnerable to the inappropriate setting of the static weight. Setting it\ntoo high can yield strange and unrealistic sentences while setting it too low\nmakes the task of suppressing repetition trivial. To remedy the shortcomings of\nthe above methods, we design a context-aware classifier to explicitly decide\nwhen to allow repetition and when to employ penalized sampling. Such a\nclassifier can be easily integrated with existing decoding methods, reducing\nrepetitions where appropriate while preserving the diversity of the text.\nExperimental results demonstrate that our method can generate higher quality\nand more authentic dialogues.", "published": "2021-12-16 06:25:46", "link": "http://arxiv.org/abs/2112.08657v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reframing Human-AI Collaboration for Generating Free-Text Explanations", "abstract": "Large language models are increasingly capable of generating fluent-appearing\ntext with relatively little task-specific supervision. But can these models\naccurately explain classification decisions? We consider the task of generating\nfree-text explanations using human-written examples in a few-shot manner. We\nfind that (1) authoring higher quality prompts results in higher quality\ngenerations; and (2) surprisingly, in a head-to-head comparison, crowdworkers\noften prefer explanations generated by GPT-3 to crowdsourced explanations in\nexisting datasets. Our human studies also show, however, that while models\noften produce factual, grammatical, and sufficient explanations, they have room\nto improve along axes such as providing novel information and supporting the\nlabel. We create a pipeline that combines GPT-3 with a supervised filter that\nincorporates binary acceptability judgments from humans in the loop. Despite\nthe intrinsic subjectivity of acceptability judgments, we demonstrate that\nacceptability is partially correlated with various fine-grained attributes of\nexplanations. Our approach is able to consistently filter GPT-3-generated\nexplanations deemed acceptable by humans.", "published": "2021-12-16 07:31:37", "link": "http://arxiv.org/abs/2112.08674v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evidentiality-guided Generation for Knowledge-Intensive NLP Tasks", "abstract": "Retrieval-augmented generation models have shown state-of-the-art performance\nacross many knowledge-intensive NLP tasks such as open question answering and\nfact verification. These models are trained to generate the final output given\nthe retrieved passages, which can be irrelevant to the original query, leading\nto learning spurious cues or answer memorization. This work introduces a method\nto incorporate the evidentiality of passages -- whether a passage contains\ncorrect evidence to support the output -- into training the generator. We\nintroduce a multi-task learning framework to jointly generate the final output\nand predict the evidentiality of each passage, leveraging a new task-agnostic\nmethod to obtain silver evidentiality labels for supervision. Our experiments\non five datasets across three knowledge-intensive tasks show that our new\nevidentiality-guided generator significantly outperforms its direct counterpart\nwith the same-size model and advances the state of the art on FaVIQ-Ambig. We\nattribute these improvements to both the auxiliary multi-task learning and\nsilver evidentiality mining techniques.", "published": "2021-12-16 08:18:47", "link": "http://arxiv.org/abs/2112.08688v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Few-Shot Semantic Parsing with Language Models Trained On Code", "abstract": "Large language models can perform semantic parsing with little training data,\nwhen prompted with in-context examples. It has been shown that this can be\nimproved by formulating the problem as paraphrasing into canonical utterances,\nwhich casts the underlying meaning representation into a controlled natural\nlanguage-like representation. Intuitively, such models can more easily output\ncanonical utterances as they are closer to the natural language used for\npre-training. Recently, models also pre-trained on code, like OpenAI Codex,\nhave risen in prominence. For semantic parsing tasks where we map natural\nlanguage into code, such models may prove more adept at it. In this paper, we\ntest this hypothesis and find that Codex performs better on such tasks than\nequivalent GPT-3 models. We evaluate on Overnight and SMCalFlow and find that\nunlike GPT-3, Codex performs similarly when targeting meaning representations\ndirectly, perhaps because meaning representations are structured similar to\ncode in these datasets.", "published": "2021-12-16 08:34:06", "link": "http://arxiv.org/abs/2112.08696v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DOCmT5: Document-Level Pretraining of Multilingual Language Models", "abstract": "In this paper, we introduce DOCmT5, a multilingual sequence-to-sequence\nlanguage model pretrained with large scale parallel documents. While previous\napproaches have focused on leveraging sentence-level parallel data, we try to\nbuild a general-purpose pretrained model that can understand and generate long\ndocuments. We propose a simple and effective pretraining objective - Document\nreordering Machine Translation (DrMT), in which the input documents that are\nshuffled and masked need to be translated. DrMT brings consistent improvements\nover strong baselines on a variety of document-level generation tasks,\nincluding over 12 BLEU points for seen-language-pair document-level MT, over 7\nBLEU points for unseen-language-pair document-level MT and over 3 ROUGE-1\npoints for seen-language-pair cross-lingual summarization. We achieve\nstate-of-the-art (SOTA) on WMT20 De-En and IWSLT15 Zh-En document translation\ntasks. We also conduct extensive analysis on various factors for document\npretraining, including (1) The effects of pretraining data quality and (2) The\neffects of combining mono-lingual and cross-lingual pretraining. We plan to\nmake our model checkpoints publicly available.", "published": "2021-12-16 08:58:52", "link": "http://arxiv.org/abs/2112.08709v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CONFIT: Toward Faithful Dialogue Summarization with\n  Linguistically-Informed Contrastive Fine-tuning", "abstract": "Factual inconsistencies in generated summaries severely limit the practical\napplications of abstractive dialogue summarization. Although significant\nprogress has been achieved by using pre-trained models, substantial amounts of\nhallucinated content are found during the human evaluation. Pre-trained models\nare most commonly fine-tuned with cross-entropy loss for text summarization,\nwhich may not be an optimal strategy. In this work, we provide a typology of\nfactual errors with annotation data to highlight the types of errors and move\naway from a binary understanding of factuality. We further propose a training\nstrategy that improves the factual consistency and overall quality of summaries\nvia a novel contrastive fine-tuning, called ConFiT. Based on our\nlinguistically-informed typology of errors, we design different modular\nobjectives that each target a specific type. Specifically, we utilize hard\nnegative samples with errors to reduce the generation of factual inconsistency.\nIn order to capture the key information between speakers, we also design a\ndialogue-specific loss. Using human evaluation and automatic faithfulness\nmetrics, we show that our model significantly reduces all kinds of factual\nerrors on the dialogue summarization, SAMSum corpus. Moreover, our model could\nbe generalized to the meeting summarization, AMI corpus, and it produces\nsignificantly higher scores than most of the baselines on both datasets\nregarding word-overlap metrics.", "published": "2021-12-16 09:08:40", "link": "http://arxiv.org/abs/2112.08713v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NeuroLogic A*esque Decoding: Constrained Text Generation with Lookahead\n  Heuristics", "abstract": "The dominant paradigm for neural text generation is left-to-right decoding\nfrom autoregressive language models. Constrained or controllable generation\nunder complex lexical constraints, however, requires foresight to plan ahead\nfeasible future paths.\n  Drawing inspiration from the A* search algorithm, we propose NeuroLogic\nA*esque, a decoding algorithm that incorporates heuristic estimates of future\ncost. We develop efficient lookahead heuristics that are efficient for\nlarge-scale language models, making our method a drop-in replacement for common\ntechniques such as beam search and top-k sampling. To enable constrained\ngeneration, we build on NeuroLogic decoding (Lu et al., 2021), combining its\nflexibility in incorporating logical constraints with A*esque estimates of\nfuture constraint satisfaction.\n  Our approach outperforms competitive baselines on five generation tasks, and\nachieves new state-of-the-art performance on table-to-text generation,\nconstrained machine translation, and keyword-constrained generation. The\nimprovements are particularly notable on tasks that require complex constraint\nsatisfaction or in few-shot or zero-shot settings. NeuroLogic A*esque\nillustrates the power of decoding for improving and enabling new capabilities\nof large-scale language models.", "published": "2021-12-16 09:22:54", "link": "http://arxiv.org/abs/2112.08726v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Hierarchical Domain Adaptation for Pretrained Language Models", "abstract": "The remarkable success of large language models has been driven by dense\nmodels trained on massive unlabeled, unstructured corpora. These corpora\ntypically contain text from diverse, heterogeneous sources, but information\nabout the source of the text is rarely used during training. Transferring their\nknowledge to a target domain is typically done by continuing training\nin-domain. In this paper, we introduce a method to permit domain adaptation to\nmany diverse domains using a computationally efficient adapter approach. Our\nmethod is based on the observation that textual domains are partially\noverlapping, and we represent domains as a hierarchical tree structure where\neach node in the tree is associated with a set of adapter weights. When\ncombined with a frozen pretrained language model, this approach enables\nparameter sharing among related domains, while avoiding negative interference\nbetween unrelated ones. Experimental results with GPT-2 and a large fraction of\nthe 100 most represented websites in C4 show across-the-board improvements\nin-domain. We additionally provide an inference time algorithm for a held-out\ndomain and show that averaging over multiple paths through the tree enables\nfurther gains in generalization, while adding only a marginal cost to\ninference.", "published": "2021-12-16 11:09:29", "link": "http://arxiv.org/abs/2112.08786v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AcTune: Uncertainty-aware Active Self-Training for Semi-Supervised\n  Active Learning with Pretrained Language Models", "abstract": "While pre-trained language model (PLM) fine-tuning has achieved strong\nperformance in many NLP tasks, the fine-tuning stage can be still demanding in\nlabeled data. Recent works have resorted to active fine-tuning to improve the\nlabel efficiency of PLM fine-tuning, but none of them investigate the potential\nof unlabeled data. We propose {\\ours}, a new framework that leverages unlabeled\ndata to improve the label efficiency of active PLM fine-tuning. AcTune switches\nbetween data annotation and model self-training based on uncertainty: it\nselects high-uncertainty unlabeled samples for active annotation and\nlow-uncertainty ones for model self-training. Under this framework, we design\n(1) a region-aware sampling strategy that reduces redundancy when actively\nquerying for annotations and (2) a momentum-based memory bank that dynamically\naggregates the model's pseudo labels to suppress label noise in self-training.\nExperiments on 6 text classification datasets show that AcTune outperforms the\nstrongest active learning and self-training baselines and improves the label\nefficiency of PLM fine-tuning by 56.2\\% on average. Our implementation will be\navailable at \\url{https://github.com/yueyu1030/actune}.", "published": "2021-12-16 11:09:48", "link": "http://arxiv.org/abs/2112.08787v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Harnessing Cross-lingual Features to Improve Cognate Detection for\n  Low-resource Languages", "abstract": "Cognates are variants of the same lexical form across different languages;\nfor example 'fonema' in Spanish and 'phoneme' in English are cognates, both of\nwhich mean 'a unit of sound'. The task of automatic detection of cognates among\nany two languages can help downstream NLP tasks such as Cross-lingual\nInformation Retrieval, Computational Phylogenetics, and Machine Translation. In\nthis paper, we demonstrate the use of cross-lingual word embeddings for\ndetecting cognates among fourteen Indian Languages. Our approach introduces the\nuse of context from a knowledge graph to generate improved feature\nrepresentations for cognate detection. We, then, evaluate the impact of our\ncognate detection mechanism on neural machine translation (NMT), as a\ndownstream task. We evaluate our methods to detect cognates on a challenging\ndataset of twelve Indian languages, namely, Sanskrit, Hindi, Assamese, Oriya,\nKannada, Gujarati, Tamil, Telugu, Punjabi, Bengali, Marathi, and Malayalam.\nAdditionally, we create evaluation datasets for two more Indian languages,\nKonkani and Nepali. We observe an improvement of up to 18% points, in terms of\nF-score, for cognate detection. Furthermore, we observe that cognates extracted\nusing our method help improve NMT quality by up to 2.76 BLEU. We also release\nour code, newly constructed datasets and cross-lingual models publicly.", "published": "2021-12-16 11:17:58", "link": "http://arxiv.org/abs/2112.08789v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CrossSum: Beyond English-Centric Cross-Lingual Summarization for 1,500+\n  Language Pairs", "abstract": "We present CrossSum, a large-scale cross-lingual summarization dataset\ncomprising 1.68 million article-summary samples in 1,500+ language pairs. We\ncreate CrossSum by aligning parallel articles written in different languages\nvia cross-lingual retrieval from a multilingual abstractive summarization\ndataset and perform a controlled human evaluation to validate its quality. We\npropose a multistage data sampling algorithm to effectively train a\ncross-lingual summarization model capable of summarizing an article in any\ntarget language. We also introduce LaSE, an embedding-based metric for\nautomatically evaluating model-generated summaries. LaSE is strongly correlated\nwith ROUGE and, unlike ROUGE, can be reliably measured even in the absence of\nreferences in the target language. Performance on ROUGE and LaSE indicate that\nour proposed model consistently outperforms baseline models. To the best of our\nknowledge, CrossSum is the largest cross-lingual summarization dataset and the\nfirst ever that is not centered around English. We are releasing the dataset,\ntraining and evaluation scripts, and models to spur future research on\ncross-lingual summarization. The resources can be found at\nhttps://github.com/csebuetnlp/CrossSum", "published": "2021-12-16 11:40:36", "link": "http://arxiv.org/abs/2112.08804v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Simple Questions Generate Named Entity Recognition Datasets", "abstract": "Recent named entity recognition (NER) models often rely on human-annotated\ndatasets, requiring the significant engagement of professional knowledge on the\ntarget domain and entities. This research introduces an ask-to-generate\napproach that automatically generates NER datasets by asking questions in\nsimple natural language to an open-domain question answering system (e.g.,\n\"Which disease?\"). Despite using fewer in-domain resources, our models, solely\ntrained on the generated datasets, largely outperform strong low-resource\nmodels by an average F1 score of 19.4 for six popular NER benchmarks.\nFurthermore, our models provide competitive performance with rich-resource\nmodels that additionally leverage in-domain dictionaries provided by domain\nexperts. In few-shot NER, we outperform the previous best model by an F1 score\nof 5.2 on three benchmarks and achieve new state-of-the-art performance.", "published": "2021-12-16 11:44:38", "link": "http://arxiv.org/abs/2112.08808v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Khmer Word Search: Challenges, Solutions, and Semantic-Aware Search", "abstract": "Search is one of the key functionalities in digital platforms and\napplications such as an electronic dictionary, a search engine, and an\ne-commerce platform. While the search function in some languages is trivial,\nKhmer word search is challenging given its complex writing system. Multiple\norders of characters and different spelling realizations of words impose a\nconstraint on Khmer word search functionality. Additionally, spelling mistakes\nare common since robust spellcheckers are not commonly available across the\ninput device platforms. These challenges hinder the use of Khmer language in\nsearch-embedded applications. Moreover, due to the absence of WordNet-like\nlexical databases for Khmer language, it is impossible to establish semantic\nrelation between words, enabling semantic search. In this paper, we propose a\nset of robust solutions to the above challenges associated with Khmer word\nsearch. The proposed solutions include character order normalization, grapheme\nand phoneme-based spellcheckers, and Khmer word semantic model. The semantic\nmodel is based on the word embedding model that is trained on a 30-million-word\ncorpus and is used to capture the semantic similarities between words.", "published": "2021-12-16 14:37:41", "link": "http://arxiv.org/abs/2112.08918v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pushing the Limits of Rule Reasoning in Transformers through Natural\n  Language Satisfiability", "abstract": "Investigating the reasoning abilities of transformer models, and discovering\nnew challenging tasks for them, has been a topic of much interest. Recent\nstudies have found these models to be surprisingly strong at performing\ndeductive reasoning over formal logical theories expressed in natural language.\nA shortcoming of these studies, however, is that they do not take into account\nthat logical theories, when sampled uniformly at random, do not necessarily\nlead to hard instances. We propose a new methodology for creating challenging\nalgorithmic reasoning datasets that focus on natural language satisfiability\n(NLSat) problems. The key idea is to draw insights from empirical sampling of\nhard propositional SAT problems and from complexity-theoretic studies of\nlanguage. This methodology allows us to distinguish easy from hard instances,\nand to systematically increase the complexity of existing reasoning benchmarks\nsuch as RuleTaker. We find that current transformers, given sufficient training\ndata, are surprisingly robust at solving the resulting NLSat problems of\nsubstantially increased difficulty. They also exhibit some degree of\nscale-invariance - the ability to generalize to problems of larger size and\nscope. Our results, however, reveal important limitations too: a careful\nsampling of training data is crucial for building models that generalize to\nlarger problems, and transformer models' limited scale-invariance suggests they\nare far from learning robust deductive reasoning algorithms.", "published": "2021-12-16 17:47:20", "link": "http://arxiv.org/abs/2112.09054v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Models in the Loop: Aiding Crowdworkers with Generative Annotation\n  Assistants", "abstract": "In Dynamic Adversarial Data Collection (DADC), human annotators are tasked\nwith finding examples that models struggle to predict correctly. Models trained\non DADC-collected training data have been shown to be more robust in\nadversarial and out-of-domain settings, and are considerably harder for humans\nto fool. However, DADC is more time-consuming than traditional data collection\nand thus more costly per annotated example. In this work, we examine whether we\ncan maintain the advantages of DADC, without incurring the additional cost. To\nthat end, we introduce Generative Annotation Assistants (GAAs),\ngenerator-in-the-loop models that provide real-time suggestions that annotators\ncan either approve, modify, or reject entirely. We collect training datasets in\ntwenty experimental settings and perform a detailed analysis of this approach\nfor the task of extractive question answering (QA) for both standard and\nadversarial data collection. We demonstrate that GAAs provide significant\nefficiency benefits with over a 30% annotation speed-up, while leading to over\na 5x improvement in model fooling rates. In addition, we find that using\nGAA-assisted training data leads to higher downstream model performance on a\nvariety of question answering tasks over adversarial data collection.", "published": "2021-12-16 17:59:39", "link": "http://arxiv.org/abs/2112.09062v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Bounded Context-Free-Grammar via LSTM and the\n  Transformer:Difference and Explanations", "abstract": "Long Short-Term Memory (LSTM) and Transformers are two popular neural\narchitectures used for natural language processing tasks. Theoretical results\nshow that both are Turing-complete and can represent any context-free language\n(CFL).In practice, it is often observed that Transformer models have better\nrepresentation power than LSTM. But the reason is barely understood. We study\nsuch practical differences between LSTM and Transformer and propose an\nexplanation based on their latent space decomposition patterns. To achieve this\ngoal, we introduce an oracle training paradigm, which forces the decomposition\nof the latent representation of LSTM and the Transformer and supervises with\nthe transitions of the Pushdown Automaton (PDA) of the corresponding CFL. With\nthe forced decomposition, we show that the performance upper bounds of LSTM and\nTransformer in learning CFL are close: both of them can simulate a stack and\nperform stack operation along with state transitions. However, the absence of\nforced decomposition leads to the failure of LSTM models to capture the stack\nand stack operations, while having a marginal impact on the Transformer model.\nLastly, we connect the experiment on the prototypical PDA to a real-world\nparsing task to re-verify the conclusions", "published": "2021-12-16 19:56:44", "link": "http://arxiv.org/abs/2112.09174v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PECO: Examining Single Sentence Label Leakage in Natural Language\n  Inference Datasets through Progressive Evaluation of Cluster Outliers", "abstract": "Building natural language inference (NLI) benchmarks that are both\nchallenging for modern techniques, and free from shortcut biases is difficult.\nChief among these biases is \"single sentence label leakage,\" where\nannotator-introduced spurious correlations yield datasets where the logical\nrelation between (premise, hypothesis) pairs can be accurately predicted from\nonly a single sentence, something that should in principle be impossible. We\ndemonstrate that despite efforts to reduce this leakage, it persists in modern\ndatasets that have been introduced since its 2018 discovery. To enable future\namelioration efforts, introduce a novel model-driven technique, the progressive\nevaluation of cluster outliers (PECO) which enables both the objective\nmeasurement of leakage, and the automated detection of subpopulations in the\ndata which maximally exhibit it.", "published": "2021-12-16 22:49:01", "link": "http://arxiv.org/abs/2112.09237v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NewsClaims: A New Benchmark for Claim Detection from News with Attribute\n  Knowledge", "abstract": "Claim detection and verification are crucial for news understanding and have\nemerged as promising technologies for mitigating misinformation and\ndisinformation in the news. However, most existing work has focused on claim\nsentence analysis while overlooking additional crucial attributes (e.g., the\nclaimer and the main object associated with the claim). In this work, we\npresent NewsClaims, a new benchmark for attribute-aware claim detection in the\nnews domain. We extend the claim detection problem to include extraction of\nadditional attributes related to each claim and release 889 claims annotated\nover 143 news articles. NewsClaims aims to benchmark claim detection systems in\nemerging scenarios, comprising unseen topics with little or no training data.\nTo this end, we see that zero-shot and prompt-based baselines show promising\nperformance on this benchmark, while still considerably behind human\nperformance.", "published": "2021-12-16 00:50:24", "link": "http://arxiv.org/abs/2112.08544v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Goal-Directed Story Generation: Augmenting Generative Language Models\n  with Reinforcement Learning", "abstract": "The advent of large pre-trained generative language models has provided a\ncommon framework for AI story generation via sampling the model to create\nsequences that continue the story. However, sampling alone is insufficient for\nstory generation. In particular, it is hard to direct a language model to\ncreate stories to reach a specific goal event. We present two automated\ntechniques grounded in deep reinforcement learning and reward shaping to\ncontrol the plot of computer-generated stories. The first utilizes proximal\npolicy optimization to fine-tune an existing transformer-based language model\nto generate text continuations but also be goal-seeking. The second extracts a\nknowledge graph from the unfolding story, which is used by a policy network\nwith graph attention to select a candidate continuation generated by a language\nmodel. We report on automated metrics pertaining to how often stories achieve a\ngiven goal event as well as human participant rankings of coherence and overall\nstory quality compared to baselines and ablations.", "published": "2021-12-16 03:34:14", "link": "http://arxiv.org/abs/2112.08593v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Twitter-COMMs: Detecting Climate, COVID, and Military Multimodal\n  Misinformation", "abstract": "Detecting out-of-context media, such as \"mis-captioned\" images on Twitter, is\na relevant problem, especially in domains of high public significance. In this\nwork we aim to develop defenses against such misinformation for the topics of\nClimate Change, COVID-19, and Military Vehicles. We first present a large-scale\nmultimodal dataset with over 884k tweets relevant to these topics. Next, we\npropose a detection method, based on the state-of-the-art CLIP model, that\nleverages automatically generated hard image-text mismatches. While this\napproach works well on our automatically constructed out-of-context tweets, we\naim to validate its usefulness on data representative of the real world. Thus,\nwe test it on a set of human-generated fakes created by mimicking in-the-wild\nmisinformation. We achieve an 11% detection improvement in a high precision\nregime over a strong baseline. Finally, we share insights about our best model\ndesign and analyze the challenges of this emerging threat.", "published": "2021-12-16 03:37:20", "link": "http://arxiv.org/abs/2112.08594v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Masked Measurement Prediction: Learning to Jointly Predict Quantities\n  and Units from Textual Context", "abstract": "Physical measurements constitute a large portion of numbers in academic\npapers, engineering reports, and web tables. Current benchmarks fall short of\nproperly evaluating numeracy of pretrained language models on measurements,\nhindering research on developing new methods and applying them to numerical\ntasks. To that end, we introduce a novel task, Masked Measurement Prediction\n(MMP), where a model learns to reconstruct a number together with its\nassociated unit given masked text. MMP is useful for both training new\nnumerically informed models as well as evaluating numeracy of existing systems.\nIn order to address this task, we introduce a new Generative Masked Measurement\n(GeMM) model that jointly learns to predict numbers along with their units. We\nperform fine-grained analyses comparing our model with various ablations and\nbaselines. We use linear probing of traditional pretrained transformer models\n(RoBERTa) to show that they significantly underperform jointly trained\nnumber-unit models, highlighting the difficulty of this new task and the\nbenefits of our proposed pretraining approach. We hope this framework\naccelerates the progress towards building more robust numerical reasoning\nsystems in the future.", "published": "2021-12-16 04:42:13", "link": "http://arxiv.org/abs/2112.08616v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Call for Customized Conversation: Customized Conversation Grounding\n  Persona and Knowledge", "abstract": "Humans usually have conversations by making use of prior knowledge about a\ntopic and background information of the people whom they are talking to.\nHowever, existing conversational agents and datasets do not consider such\ncomprehensive information, and thus they have a limitation in generating the\nutterances where the knowledge and persona are fused properly. To address this\nissue, we introduce a call For Customized conversation (FoCus) dataset where\nthe customized answers are built with the user's persona and Wikipedia\nknowledge. To evaluate the abilities to make informative and customized\nutterances of pre-trained language models, we utilize BART and GPT-2 as well as\ntransformer-based models. We assess their generation abilities with automatic\nscores and conduct human evaluations for qualitative results. We examine\nwhether the model reflects adequate persona and knowledge with our proposed two\nsub-tasks, persona grounding (PG) and knowledge grounding (KG). Moreover, we\nshow that the utterances of our data are constructed with the proper knowledge\nand persona through grounding quality assessment.", "published": "2021-12-16 04:44:27", "link": "http://arxiv.org/abs/2112.08619v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning To Retrieve Prompts for In-Context Learning", "abstract": "In-context learning is a recent paradigm in natural language understanding,\nwhere a large pre-trained language model (LM) observes a test instance and a\nfew training examples as its input, and directly decodes the output without any\nupdate to its parameters. However, performance has been shown to strongly\ndepend on the selected training examples (termed prompt). In this work, we\npropose an efficient method for retrieving prompts for in-context learning\nusing annotated data and a LM. Given an input-output pair, we estimate the\nprobability of the output given the input and a candidate training example as\nthe prompt, and label training examples as positive or negative based on this\nprobability. We then train an efficient dense retriever from this data, which\nis used to retrieve training examples as prompts at test time. We evaluate our\napproach on three sequence-to-sequence tasks where language utterances are\nmapped to meaning representations, and find that it substantially outperforms\nprior work and multiple baselines across the board.", "published": "2021-12-16 05:17:56", "link": "http://arxiv.org/abs/2112.08633v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Analyzing the Limits of Self-Supervision in Handling Bias in Language", "abstract": "Prompting inputs with natural language task descriptions has emerged as a\npopular mechanism to elicit reasonably accurate outputs from large-scale\ngenerative language models with little to no in-context supervision. This also\nhelps gain insight into how well language models capture the semantics of a\nwide range of downstream tasks purely from self-supervised pre-training on\nmassive corpora of unlabeled text. Such models have naturally also been exposed\nto a lot of undesirable content like racist and sexist language and there is\nlimited work on awareness of models along these dimensions. In this paper, we\ndefine and comprehensively evaluate how well such language models capture the\nsemantics of four tasks for bias: diagnosis, identification, extraction and\nrephrasing. We define three broad classes of task descriptions for these tasks:\nstatement, question, and completion, with numerous lexical variants within each\nclass. We study the efficacy of prompting for each task using these classes and\nthe null task description across several decoding methods and few-shot\nexamples. Our analyses indicate that language models are capable of performing\nthese tasks to widely varying degrees across different bias dimensions, such as\ngender and political affiliation. We believe our work is an important step\ntowards unbiased language models by quantifying the limits of current\nself-supervision objectives at accomplishing such sociologically challenging\ntasks.", "published": "2021-12-16 05:36:08", "link": "http://arxiv.org/abs/2112.08637v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Extreme Zero-Shot Learning for Extreme Text Classification", "abstract": "The eXtreme Multi-label text Classification (XMC) problem concerns finding\nmost relevant labels for an input text instance from a large label set.\nHowever, the XMC setup faces two challenges: (1) it is not generalizable to\npredict unseen labels in dynamic environments, and (2) it requires a large\namount of supervised (instance, label) pairs, which can be difficult to obtain\nfor emerging domains. Recently, the generalized zero-shot XMC (GZ-XMC) setup\nhas been studied and ZestXML is proposed accordingly to handle the unseen\nlabels, which still requires a large number of annotated (instance, label)\npairs. In this paper, we consider a more practical scenario called Extreme\nZero-Shot XMC (EZ-XMC), in which no supervision is needed and merely raw text\nof instances and labels are accessible. Few-Shot XMC (FS-XMC), an extension to\nEZ-XMC with limited supervision is also investigated. To learn the semantic\nembeddings of instances and labels with raw text, we propose to pre-train\nTransformer-based encoders with self-supervised contrastive losses.\nSpecifically, we develop a pre-training method MACLR, which thoroughly\nleverages the raw text with techniques including Multi-scale Adaptive\nClustering, Label Regularization, and self-training with pseudo positive pairs.\nExperimental results on four public EZ-XMC datasets demonstrate that MACLR\nachieves superior performance compared to all other leading baseline methods,\nin particular with approximately 5-10% improvement in precision and recall on\naverage. Moreover, we also show that our pre-trained encoder can be further\nimproved on FS-XMC when there are a limited number of ground-truth positive\npairs in training. By fine-tuning the encoder on such a few-shot subset, MACLR\nstill outperforms other extreme classifiers significantly.", "published": "2021-12-16 06:06:42", "link": "http://arxiv.org/abs/2112.08652v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "DREAM: Improving Situational QA by First Elaborating the Situation", "abstract": "When people answer questions about a specific situation, e.g., \"I cheated on\nmy mid-term exam last week. Was that wrong?\", cognitive science suggests that\nthey form a mental picture of that situation before answering. While we do not\nknow how language models (LMs) answer such questions, we conjecture that they\nmay answer more accurately if they are also provided with additional details\nabout the question situation, elaborating the \"scene\". To test this conjecture,\nwe train a new model, DREAM, to answer questions that elaborate the scenes that\nsituated questions are about, and then provide those elaborations as additional\ncontext to a question-answering (QA) model. We find that DREAM is able to\ncreate better scene elaborations (more accurate, useful, and consistent) than a\nrepresentative state-of-the-art, zero-shot model (Macaw). We also find that\nusing the scene elaborations as additional context improves the answer accuracy\nof a downstream QA system, including beyond that obtainable by simply further\nfinetuning the QA system on DREAM's training data. These results suggest that\nadding focused elaborations about a situation can improve a system's reasoning\nabout it, and may serve as an effective way of injecting new scenario based\nknowledge into QA models. Finally, our approach is dataset-neutral; we observe\nimproved QA performance across different models, with even bigger gains on\nmodels with fewer parameters. We make our dataset and model publicly available\nat https://github.com/allenai/dream.", "published": "2021-12-16 06:22:47", "link": "http://arxiv.org/abs/2112.08656v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MAVE: A Product Dataset for Multi-source Attribute Value Extraction", "abstract": "Attribute value extraction refers to the task of identifying values of an\nattribute of interest from product information. Product attribute values are\nessential in many e-commerce scenarios, such as customer service robots,\nproduct ranking, retrieval and recommendations. While in the real world, the\nattribute values of a product are usually incomplete and vary over time, which\ngreatly hinders the practical applications. In this paper, we introduce MAVE, a\nnew dataset to better facilitate research on product attribute value\nextraction. MAVE is composed of a curated set of 2.2 million products from\nAmazon pages, with 3 million attribute-value annotations across 1257 unique\ncategories. MAVE has four main and unique advantages: First, MAVE is the\nlargest product attribute value extraction dataset by the number of\nattribute-value examples. Second, MAVE includes multi-source representations\nfrom the product, which captures the full product information with high\nattribute coverage. Third, MAVE represents a more diverse set of attributes and\nvalues relative to what previous datasets cover. Lastly, MAVE provides a very\nchallenging zero-shot test set, as we empirically illustrate in the\nexperiments. We further propose a novel approach that effectively extracts the\nattribute value from the multi-source product information. We conduct extensive\nexperiments with several baselines and show that MAVE is an effective dataset\nfor attribute value extraction task. It is also a very challenging task on\nzero-shot attribute extraction. Data is available at {\\it\n\\url{https://github.com/google-research-datasets/MAVE}}.", "published": "2021-12-16 06:48:31", "link": "http://arxiv.org/abs/2112.08663v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Amortized Noisy Channel Neural Machine Translation", "abstract": "Noisy channel models have been especially effective in neural machine\ntranslation (NMT). However, recent approaches like \"beam search and rerank\"\n(BSR) incur significant computation overhead during inference, making\nreal-world application infeasible. We aim to study if it is possible to build\nan amortized noisy channel NMT model such that when we do greedy decoding\nduring inference, the translation accuracy matches that of BSR in terms of\nreward (based on the source-to-target log probability and the target-to-source\nlog probability) and quality (based on BLEU and BLEURT). We attempt three\napproaches to train the new model: knowledge distillation, one-step-deviation\nimitation learning, and Q learning. The first approach obtains the noisy\nchannel signal from a pseudo-corpus, and the latter two approaches aim to\noptimize toward a noisy-channel MT reward directly. For all three approaches,\nthe generated translations fail to achieve rewards comparable to BSR, but the\ntranslation quality approximated by BLEU and BLEURT is similar to the quality\nof BSR-produced translations. Additionally, all three approaches speed up\ninference by 1-2 orders of magnitude.", "published": "2021-12-16 07:10:02", "link": "http://arxiv.org/abs/2112.08670v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Isometric MT: Neural Machine Translation for Automatic Dubbing", "abstract": "Automatic dubbing (AD) is among the machine translation (MT) use cases where\ntranslations should match a given length to allow for synchronicity between\nsource and target speech. For neural MT, generating translations of length\nclose to the source length (e.g. within +-10% in character count), while\npreserving quality is a challenging task. Controlling MT output length comes at\na cost to translation quality, which is usually mitigated with a two step\napproach of generating N-best hypotheses and then re-ranking based on length\nand quality. This work introduces a self-learning approach that allows a\ntransformer model to directly learn to generate outputs that closely match the\nsource length, in short Isometric MT. In particular, our approach does not\nrequire to generate multiple hypotheses nor any auxiliary ranking function. We\nreport results on four language pairs (English - French, Italian, German,\nSpanish) with a publicly available benchmark. Automatic and manual evaluations\nshow that our method for Isometric MT outperforms more complex approaches\nproposed in the literature.", "published": "2021-12-16 08:03:20", "link": "http://arxiv.org/abs/2112.08682v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Prompt Tuning GPT-2 language model for parameter-efficient domain\n  adaptation of ASR systems", "abstract": "Automatic Speech Recognition (ASR) systems have found their use in numerous\nindustrial applications in very diverse domains creating a need to adapt to new\ndomains with small memory and deployment overhead. In this work, we introduce\ndomain-prompts, a methodology that involves training a small number of domain\nembedding parameters to prime a Transformer-based Language Model (LM) to a\nparticular domain. Using this domain-adapted LM for rescoring ASR hypotheses\ncan achieve 7-13% WER reduction for a new domain with just 1000 unlabeled\ntextual domain-specific sentences. This improvement is comparable or even\nbetter than fully fine-tuned models even though just 0.02% of the parameters of\nthe base LM are updated. Additionally, our method is deployment-friendly as the\nlearnt domain embeddings are prefixed to the input to the model rather than\nchanging the base model architecture. Therefore, our method is an ideal choice\nfor on-the-fly adaptation of LMs used in ASR systems to progressively scale it\nto new domains.", "published": "2021-12-16 09:13:04", "link": "http://arxiv.org/abs/2112.08718v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Distilled Dual-Encoder Model for Vision-Language Understanding", "abstract": "We propose a cross-modal attention distillation framework to train a\ndual-encoder model for vision-language understanding tasks, such as visual\nreasoning and visual question answering. Dual-encoder models have a faster\ninference speed than fusion-encoder models and enable the pre-computation of\nimages and text during inference. However, the shallow interaction module used\nin dual-encoder models is insufficient to handle complex vision-language\nunderstanding tasks. In order to learn deep interactions of images and text, we\nintroduce cross-modal attention distillation, which uses the image-to-text and\ntext-to-image attention distributions of a fusion-encoder model to guide the\ntraining of our dual-encoder model. In addition, we show that applying the\ncross-modal attention distillation for both pre-training and fine-tuning stages\nachieves further improvements. Experimental results demonstrate that the\ndistilled dual-encoder model achieves competitive performance for visual\nreasoning, visual entailment and visual question answering tasks while enjoying\na much faster inference speed than fusion-encoder models. Our code and models\nwill be publicly available at https://github.com/kugwzk/Distilled-DualEncoder.", "published": "2021-12-16 09:21:18", "link": "http://arxiv.org/abs/2112.08723v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Pay More Attention to History: A Context Modelling Strategy for\n  Conversational Text-to-SQL", "abstract": "Conversational text-to-SQL aims at converting multi-turn natural language\nqueries into their corresponding SQL (Structured Query Language)\nrepresentations. One of the most intractable problems of conversational\ntext-to-SQL is modelling the semantics of multi-turn queries and gathering the\nproper information required for the current query. This paper shows that\nexplicitly modelling the semantic changes by adding each turn and the\nsummarization of the whole context can bring better performance on converting\nconversational queries into SQLs. In particular, we propose two conversational\nmodelling tasks in both turn grain and conversation grain. These two tasks\nsimply work as auxiliary training tasks to help with multi-turn conversational\nsemantic parsing. We conducted empirical studies and achieved new\nstate-of-the-art results on the large-scale open-domain conversational\ntext-to-SQL dataset. The results demonstrate that the proposed mechanism\nsignificantly improves the performance of multi-turn semantic parsing.", "published": "2021-12-16 09:41:04", "link": "http://arxiv.org/abs/2112.08735v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CLIN-X: pre-trained language models and a study on cross-task transfer\n  for concept extraction in the clinical domain", "abstract": "The field of natural language processing (NLP) has recently seen a large\nchange towards using pre-trained language models for solving almost any task.\nDespite showing great improvements in benchmark datasets for various tasks,\nthese models often perform sub-optimal in non-standard domains like the\nclinical domain where a large gap between pre-training documents and target\ndocuments is observed. In this paper, we aim at closing this gap with\ndomain-specific training of the language model and we investigate its effect on\na diverse set of downstream tasks and settings. We introduce the pre-trained\nCLIN-X (Clinical XLM-R) language models and show how CLIN-X outperforms other\npre-trained transformer models by a large margin for ten clinical concept\nextraction tasks from two languages. In addition, we demonstrate how the\ntransformer model can be further improved with our proposed task- and\nlanguage-agnostic model architecture based on ensembles over random splits and\ncross-sentence context. Our studies in low-resource and transfer settings\nreveal stable model performance despite a lack of annotated data with\nimprovements of up to 47 F1 points when only 250 labeled sentences are\navailable. Our results highlight the importance of specialized language models\nas CLIN-X for concept extraction in non-standard domains, but also show that\nour task-agnostic model architecture is robust across the tested tasks and\nlanguages so that domain- or task-specific adaptations are not required.", "published": "2021-12-16 10:07:39", "link": "http://arxiv.org/abs/2112.08754v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Proposition-Level Clustering for Multi-Document Summarization", "abstract": "Text clustering methods were traditionally incorporated into multi-document\nsummarization (MDS) as a means for coping with considerable information\nrepetition. Particularly, clusters were leveraged to indicate information\nsaliency as well as to avoid redundancy. Such prior methods focused on\nclustering sentences, even though closely related sentences usually contain\nalso non-aligned parts. In this work, we revisit the clustering approach,\ngrouping together sub-sentential propositions, aiming at more precise\ninformation alignment. Specifically, our method detects salient propositions,\nclusters them into paraphrastic clusters, and generates a representative\nsentence for each cluster via text fusion. Our summarization method improves\nover the previous state-of-the-art MDS method in the DUC 2004 and TAC 2011\ndatasets, both in automatic ROUGE scores and human preference.", "published": "2021-12-16 10:34:22", "link": "http://arxiv.org/abs/2112.08770v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sharpness-Aware Minimization with Dynamic Reweighting", "abstract": "Deep neural networks are often overparameterized and may not easily achieve\nmodel generalization. Adversarial training has shown effectiveness in improving\ngeneralization by regularizing the change of loss on top of adversarially\nchosen perturbations. The recently proposed sharpness-aware minimization (SAM)\nalgorithm conducts adversarial weight perturbation, encouraging the model to\nconverge to a flat minima. SAM finds a common adversarial weight perturbation\nper-batch. Although per-instance adversarial weight perturbations are stronger\nadversaries and can potentially lead to better generalization performance,\ntheir computational cost is very high and thus it is impossible to use\nper-instance perturbations efficiently in SAM. In this paper, we tackle this\nefficiency bottleneck and propose sharpness-aware minimization with dynamic\nreweighting (delta-SAM). Our theoretical analysis motivates that it is possible\nto approach the stronger, per-instance adversarial weight perturbations using\nreweighted per-batch weight perturbations. delta-SAM dynamically reweights\nperturbation within each batch according to the theoretically principled\nweighting factors, serving as a good approximation to per-instance\nperturbation. Experiments on various natural language understanding tasks\ndemonstrate the effectiveness of delta-SAM.", "published": "2021-12-16 10:36:35", "link": "http://arxiv.org/abs/2112.08772v4", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Long Context Question Answering via Supervised Contrastive Learning", "abstract": "Long-context question answering (QA) tasks require reasoning over a long\ndocument or multiple documents. Addressing these tasks often benefits from\nidentifying a set of evidence spans (e.g., sentences), which provide supporting\nevidence for answering the question. In this work, we propose a novel method\nfor equipping long-context QA models with an additional sequence-level\nobjective for better identification of the supporting evidence. We achieve this\nvia an additional contrastive supervision signal in finetuning, where the model\nis encouraged to explicitly discriminate supporting evidence sentences from\nnegative ones by maximizing question-evidence similarity. The proposed\nadditional loss exhibits consistent improvements on three different strong\nlong-context transformer models, across two challenging question answering\nbenchmarks -- HotpotQA and QAsper.", "published": "2021-12-16 10:40:49", "link": "http://arxiv.org/abs/2112.08777v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Self-Supervised Learning for speech recognition with Intermediate layer\n  supervision", "abstract": "Recently, pioneer work finds that speech pre-trained models can solve\nfull-stack speech processing tasks, because the model utilizes bottom layers to\nlearn speaker-related information and top layers to encode content-related\ninformation. Since the network capacity is limited, we believe the speech\nrecognition performance could be further improved if the model is dedicated to\naudio content information learning. To this end, we propose Intermediate Layer\nSupervision for Self-Supervised Learning (ILS-SSL), which forces the model to\nconcentrate on content information as much as possible by adding an additional\nSSL loss on the intermediate layers. Experiments on LibriSpeech test-other set\nshow that our method outperforms HuBERT significantly, which achieves a\n23.5%/11.6% relative word error rate reduction in the w/o language model\nsetting for base/large models. Detailed analysis shows the bottom layers of our\nmodel have a better correlation with phonetic units, which is consistent with\nour intuition and explains the success of our method for ASR.", "published": "2021-12-16 10:45:05", "link": "http://arxiv.org/abs/2112.08778v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Ditch the Gold Standard: Re-evaluating Conversational Question Answering", "abstract": "Conversational question answering aims to provide natural-language answers to\nusers in information-seeking conversations. Existing conversational QA\nbenchmarks compare models with pre-collected human-human conversations, using\nground-truth answers provided in conversational history. It remains unclear\nwhether we can rely on this static evaluation for model development and whether\ncurrent systems can well generalize to real-world human-machine conversations.\nIn this work, we conduct the first large-scale human evaluation of\nstate-of-the-art conversational QA systems, where human evaluators converse\nwith models and judge the correctness of their answers. We find that the\ndistribution of human machine conversations differs drastically from that of\nhuman-human conversations, and there is a disagreement between human and\ngold-history evaluation in terms of model ranking. We further investigate how\nto improve automatic evaluations, and propose a question rewriting mechanism\nbased on predicted history, which better correlates with human judgments.\nFinally, we analyze the impact of various modeling strategies and discuss\nfuture directions towards building better conversational question answering\nsystems.", "published": "2021-12-16 11:57:56", "link": "http://arxiv.org/abs/2112.08812v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Bridging between Cognitive Processing Signals and Linguistic Features\n  via a Unified Attentional Network", "abstract": "Cognitive processing signals can be used to improve natural language\nprocessing (NLP) tasks. However, it is not clear how these signals correlate\nwith linguistic information. Bridging between human language processing and\nlinguistic features has been widely studied in neurolinguistics, usually via\nsingle-variable controlled experiments with highly-controlled stimuli. Such\nmethods not only compromises the authenticity of natural reading, but also are\ntime-consuming and expensive. In this paper, we propose a data-driven method to\ninvestigate the relationship between cognitive processing signals and\nlinguistic features. Specifically, we present a unified attentional framework\nthat is composed of embedding, attention, encoding and predicting layers to\nselectively map cognitive processing signals to linguistic features. We define\nthe mapping procedure as a bridging task and develop 12 bridging tasks for\nlexical, syntactic and semantic features. The proposed framework only requires\ncognitive processing signals recorded under natural reading as inputs, and can\nbe used to detect a wide range of linguistic features with a single cognitive\ndataset. Observations from experiment results resonate with previous\nneuroscience findings. In addition to this, our experiments also reveal a\nnumber of interesting findings, such as the correlation between contextual\neye-tracking features and tense of sentence.", "published": "2021-12-16 12:25:11", "link": "http://arxiv.org/abs/2112.08831v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Bottom Up Top Down Detection Transformers for Language Grounding in\n  Images and Point Clouds", "abstract": "Most models tasked to ground referential utterances in 2D and 3D scenes learn\nto select the referred object from a pool of object proposals provided by a\npre-trained detector. This is limiting because an utterance may refer to visual\nentities at various levels of granularity, such as the chair, the leg of the\nchair, or the tip of the front leg of the chair, which may be missed by the\ndetector. We propose a language grounding model that attends on the referential\nutterance and on the object proposal pool computed from a pre-trained detector\nto decode referenced objects with a detection head, without selecting them from\nthe pool. In this way, it is helped by powerful pre-trained object detectors\nwithout being restricted by their misses. We call our model Bottom Up Top Down\nDEtection TRansformers (BUTD-DETR) because it uses both language guidance (top\ndown) and objectness guidance (bottom-up) to ground referential utterances in\nimages and point clouds. Moreover, BUTD-DETR casts object detection as\nreferential grounding and uses object labels as language prompts to be grounded\nin the visual scene, augmenting supervision for the referential grounding task\nin this way. The proposed model sets a new state-of-the-art across popular 3D\nlanguage grounding benchmarks with significant performance gains over previous\n3D approaches (12.6% on SR3D, 11.6% on NR3D and 6.3% on ScanRefer). When\napplied in 2D images, it performs on par with the previous state of the art. We\nablate the design choices of our model and quantify their contribution to\nperformance. Our code and checkpoints can be found at the project website\nhttps://butd-detr.github.io.", "published": "2021-12-16 13:50:23", "link": "http://arxiv.org/abs/2112.08879v5", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Degendering Resumes for Fair Algorithmic Resume Screening", "abstract": "We investigate whether it is feasible to remove gendered information from\nresumes to mitigate potential bias in algorithmic resume screening. Using a\ncorpus of 709k resumes from IT firms, we first train a series of models to\nclassify the self-reported gender of the applicant, thereby measuring the\nextent and nature of gendered information encoded in resumes. We then conduct a\nseries of gender obfuscation experiments, where we iteratively remove gendered\ninformation from resumes. Finally, we train a resume screening algorithm and\ninvestigate the trade-off between gender obfuscation and screening algorithm\nperformance. Results show: (1) There is a significant amount of gendered\ninformation in resumes. (2) Lexicon-based gender obfuscation method (i.e.\nremoving tokens that are predictive of gender) can reduce the amount of\ngendered information to a large extent. However, after a certain point, the\nperformance of the resume screening algorithm starts suffering. (3)\nGeneral-purpose gender debiasing methods for NLP models such as removing gender\nsubspace from embeddings are not effective in obfuscating gender.", "published": "2021-12-16 14:26:36", "link": "http://arxiv.org/abs/2112.08910v3", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Characterizing and addressing the issue of oversmoothing in neural\n  autoregressive sequence modeling", "abstract": "Neural autoregressive sequence models smear the probability among many\npossible sequences including degenerate ones, such as empty or repetitive\nsequences. In this work, we tackle one specific case where the model assigns a\nhigh probability to unreasonably short sequences. We define the oversmoothing\nrate to quantify this issue. After confirming the high degree of oversmoothing\nin neural machine translation, we propose to explicitly minimize the\noversmoothing rate during training. We conduct a set of experiments to study\nthe effect of the proposed regularization on both model distribution and\ndecoding performance. We use a neural machine translation task as the testbed\nand consider three different datasets of varying size. Our experiments reveal\nthree major findings. First, we can control the oversmoothing rate of the model\nby tuning the strength of the regularization. Second, by enhancing the\noversmoothing loss contribution, the probability and the rank of <eos> token\ndecrease heavily at positions where it is not supposed to be. Third, the\nproposed regularization impacts the outcome of beam search especially when a\nlarge beam is used. The degradation of translation quality (measured in BLEU)\nwith a large beam significantly lessens with lower oversmoothing rate, but the\ndegradation compared to smaller beam sizes remains to exist. From these\nobservations, we conclude that the high degree of oversmoothing is the main\nreason behind the degenerate case of overly probable short sequences in a\nneural autoregressive model.", "published": "2021-12-16 14:33:12", "link": "http://arxiv.org/abs/2112.08914v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Hyperbolic Disentangled Representation for Fine-Grained Aspect\n  Extraction", "abstract": "Automatic identification of salient aspects from user reviews is especially\nuseful for opinion analysis. There has been significant progress in utilizing\nweakly supervised approaches, which require only a small set of seed words for\ntraining aspect classifiers. However, there is always room for improvement.\nFirst, no weakly supervised approaches fully utilize latent hierarchies between\nwords. Second, each seed words representation should have different latent\nsemantics and be distinct when it represents a different aspect. In this paper,\nwe propose HDAE, a hyperbolic disentangled aspect extractor in which a\nhyperbolic aspect classifier captures words latent hierarchies, and\naspect-disentangled representation models the distinct latent semantics of each\nseed word. Compared to previous baselines, HDAE achieves average F1 performance\ngains of 18.2% and 24.1% on Amazon product review and restaurant review\ndatasets, respectively. In addition, the em-bedding visualization experience\ndemonstrates that HDAE is a more effective approach to leveraging seed words.\nAn ablation study and a case study further attest to the effectiveness of the\nproposed components", "published": "2021-12-16 21:47:28", "link": "http://arxiv.org/abs/2112.09215v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning to Repair: Repairing model output errors after deployment using\n  a dynamic memory of feedback", "abstract": "Large language models (LMs), while powerful, are not immune to mistakes, but\ncan be difficult to retrain. Our goal is for an LM to continue to improve after\ndeployment, without retraining, using feedback from the user. Our approach\npairs an LM with (i) a growing memory of cases where the user identified an\noutput error and provided general feedback on how to correct it (ii) a\ncorrector model, trained to translate this general feedback into specific edits\nto repair the model output. Given a new, unseen input, our model can then use\nfeedback from similar, past cases to repair output errors that may occur. We\ninstantiate our approach using an existing, fixed model for script generation,\nthat takes a goal (e.g., \"bake a cake\") and generates a partially ordered\nsequence of actions to achieve that goal, sometimes containing errors. Our\nmemory-enhanced system, FBNet, learns to apply user feedback to repair such\nerrors (up to 30 points improvement), while making a start at avoiding similar\npast mistakes on new, unseen examples (up to 7 points improvement in a\ncontrolled setting). This is a first step towards strengthening deployed\nmodels, potentially broadening their utility. Our code and data is available at\nhttps://github.com/allenai/interscript/.", "published": "2021-12-16 07:01:28", "link": "http://arxiv.org/abs/2112.09737v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Trees in transformers: a theoretical analysis of the Transformer's\n  ability to represent trees", "abstract": "Transformer networks are the de facto standard architecture in natural\nlanguage processing. To date, there are no theoretical analyses of the\nTransformer's ability to capture tree structures. We focus on the ability of\nTransformer networks to learn tree structures that are important for tree\ntransduction problems. We first analyze the theoretical capability of the\nstandard Transformer architecture to learn tree structures given enumeration of\nall possible tree backbones, which we define as trees without labels. We then\nprove that two linear layers with ReLU activation function can recover any tree\nbackbone from any two nonzero, linearly independent starting backbones. This\nimplies that a Transformer can learn tree structures well in theory. We conduct\nexperiments with synthetic data and find that the standard Transformer achieves\nsimilar accuracy compared to a Transformer where tree position information is\nexplicitly encoded, albeit with slower convergence. This confirms empirically\nthat Transformers can learn tree structures.", "published": "2021-12-16 00:02:02", "link": "http://arxiv.org/abs/2112.11913v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning Rich Representation of Keyphrases from Text", "abstract": "In this work, we explore how to train task-specific language models aimed\ntowards learning rich representation of keyphrases from text documents. We\nexperiment with different masking strategies for pre-training transformer\nlanguage models (LMs) in discriminative as well as generative settings. In the\ndiscriminative setting, we introduce a new pre-training objective - Keyphrase\nBoundary Infilling with Replacement (KBIR), showing large gains in performance\n(upto 8.16 points in F1) over SOTA, when the LM pre-trained using KBIR is\nfine-tuned for the task of keyphrase extraction. In the generative setting, we\nintroduce a new pre-training setup for BART - KeyBART, that reproduces the\nkeyphrases related to the input text in the CatSeq format, instead of the\ndenoised original input. This also led to gains in performance (upto 4.33\npoints in F1@M) over SOTA for keyphrase generation. Additionally, we also\nfine-tune the pre-trained language models on named entity recognition (NER),\nquestion answering (QA), relation extraction (RE), abstractive summarization\nand achieve comparable performance with that of the SOTA, showing that learning\nrich representation of keyphrases is indeed beneficial for many other\nfundamental NLP tasks.", "published": "2021-12-16 01:09:51", "link": "http://arxiv.org/abs/2112.08547v2", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Lacuna Reconstruction: Self-supervised Pre-training for Low-Resource\n  Historical Document Transcription", "abstract": "We present a self-supervised pre-training approach for learning rich visual\nlanguage representations for both handwritten and printed historical document\ntranscription. After supervised fine-tuning of our pre-trained encoder\nrepresentations for low-resource document transcription on two languages, (1) a\nheterogeneous set of handwritten Islamicate manuscript images and (2) early\nmodern English printed documents, we show a meaningful improvement in\nrecognition accuracy over the same supervised model trained from scratch with\nas few as 30 line image transcriptions for training. Our masked language\nmodel-style pre-training strategy, where the model is trained to be able to\nidentify the true masked visual representation from distractors sampled from\nwithin the same line, encourages learning robust contextualized language\nrepresentations invariant to scribal writing style and printing noise present\nacross documents.", "published": "2021-12-16 08:28:26", "link": "http://arxiv.org/abs/2112.08692v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "UNIREX: A Unified Learning Framework for Language Model Rationale\n  Extraction", "abstract": "An extractive rationale explains a language model's (LM's) prediction on a\ngiven task instance by highlighting the text inputs that most influenced the\nprediction. Ideally, rationale extraction should be faithful (reflective of\nLM's actual behavior) and plausible (convincing to humans), without\ncompromising the LM's (i.e., task model's) task performance. Although\nattribution algorithms and select-predict pipelines are commonly used in\nrationale extraction, they both rely on certain heuristics that hinder them\nfrom satisfying all three desiderata. In light of this, we propose UNIREX, a\nflexible learning framework that generalizes rationale extractor optimization\nas follows: (1) specify architecture for a learned rationale extractor; (2)\nselect explainability objectives (i.e., faithfulness and plausibility\ncriteria); and (3) jointly the train task model and rationale extractor on the\ntask using the selected objectives. UNIREX enables replacing prior works'\nheuristic design choices with a generic learned rationale extractor in (1) and\noptimizing it for all three desiderata in (2)-(3). To facilitate comparison\nbetween methods with respect to multiple desiderata, we introduce the\nNormalized Relative Gain (NRG) metric. Across five text classification\ndatasets, our best UNIREX configuration outperforms baselines by an average of\n32.9% NRG. Plus, we find that UNIREX-trained rationale extractors can even\ngeneralize to unseen datasets and tasks.", "published": "2021-12-16 11:39:21", "link": "http://arxiv.org/abs/2112.08802v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Adapting Document-Grounded Dialog Systems to Spoken Conversations using\n  Data Augmentation and a Noisy Channel Model", "abstract": "This paper summarizes our submission to Task 2 of the second track of the\n10th Dialog System Technology Challenge (DSTC10) \"Knowledge-grounded\nTask-oriented Dialogue Modeling on Spoken Conversations\". Similar to the\nprevious year's iteration, the task consists of three subtasks: detecting\nwhether a turn is knowledge seeking, selecting the relevant knowledge document\nand finally generating a grounded response. This year, the focus lies on\nadapting the system to noisy ASR transcripts. We explore different approaches\nto make the models more robust to this type of input and to adapt the generated\nresponses to the style of spoken conversations. For the latter, we get the best\nresults with a noisy channel model that additionally reduces the number of\nshort and generic responses. Our best system achieved the 1st rank in the\nautomatic and the 3rd rank in the human evaluation of the challenge.", "published": "2021-12-16 12:51:52", "link": "http://arxiv.org/abs/2112.08844v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Inherently Explainable Reinforcement Learning in Natural Language", "abstract": "We focus on the task of creating a reinforcement learning agent that is\ninherently explainable -- with the ability to produce immediate local\nexplanations by thinking out loud while performing a task and analyzing entire\ntrajectories post-hoc to produce causal explanations. This Hierarchically\nExplainable Reinforcement Learning agent (HEX-RL), operates in Interactive\nFictions, text-based game environments in which an agent perceives and acts\nupon the world using textual natural language. These games are usually\nstructured as puzzles or quests with long-term dependencies in which an agent\nmust complete a sequence of actions to succeed -- providing ideal environments\nin which to test an agent's ability to explain its actions. Our agent is\ndesigned to treat explainability as a first-class citizen, using an extracted\nsymbolic knowledge graph-based state representation coupled with a Hierarchical\nGraph Attention mechanism that points to the facts in the internal graph\nrepresentation that most influenced the choice of actions. Experiments show\nthat this agent provides significantly improved explanations over strong\nbaselines, as rated by human participants generally unfamiliar with the\nenvironment, while also matching state-of-the-art task performance.", "published": "2021-12-16 14:24:35", "link": "http://arxiv.org/abs/2112.08907v3", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Connecting the Dots between Audio and Text without Parallel Data through\n  Visual Knowledge Transfer", "abstract": "Machines that can represent and describe environmental soundscapes have\npractical potential, e.g., for audio tagging and captioning systems. Prevailing\nlearning paradigms have been relying on parallel audio-text data, which is,\nhowever, scarcely available on the web. We propose VIP-ANT that induces\n\\textbf{A}udio-\\textbf{T}ext alignment without using any parallel audio-text\ndata. Our key idea is to share the image modality between bi-modal image-text\nrepresentations and bi-modal image-audio representations; the image modality\nfunctions as a pivot and connects audio and text in a tri-modal embedding space\nimplicitly.\n  In a difficult zero-shot setting with no paired audio-text data, our model\ndemonstrates state-of-the-art zero-shot performance on the ESC50 and US8K audio\nclassification tasks, and even surpasses the supervised state of the art for\nClotho caption retrieval (with audio queries) by 2.2\\% R@1. We further\ninvestigate cases of minimal audio-text supervision, finding that, e.g., just a\nfew hundred supervised audio-text pairs increase the zero-shot audio\nclassification accuracy by 8\\% on US8K. However, to match human parity on some\nzero-shot tasks, our empirical scaling experiments suggest that we would need\nabout $2^{21} \\approx 2M$ supervised audio-caption pairs. Our work opens up new\navenues for learning audio-text connections with little to no parallel\naudio-text data.", "published": "2021-12-16 16:22:10", "link": "http://arxiv.org/abs/2112.08995v2", "categories": ["cs.SD", "cs.CL", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Learning and Analyzing Generation Order for Undirected Sequence Models", "abstract": "Undirected neural sequence models have achieved performance competitive with\nthe state-of-the-art directed sequence models that generate monotonically from\nleft to right in machine translation tasks. In this work, we train a policy\nthat learns the generation order for a pre-trained, undirected translation\nmodel via reinforcement learning. We show that the translations decoded by our\nlearned orders achieve higher BLEU scores than the outputs decoded from left to\nright or decoded by the learned order from Mansimov et al. (2019) on the WMT'14\nGerman-English translation task. On examples with a maximum source and target\nlength of 30 from De-En, WMT'16 English-Romanian, and WMT'21 English-Chinese\ntranslation tasks, our learned order outperforms all heuristic generation\norders on four out of six tasks. We next carefully analyze the learned order\npatterns via qualitative and quantitative analysis. We show that our policy\ngenerally follows an outer-to-inner order, predicting the left-most and\nright-most positions first, and then moving toward the middle while skipping\nless important words at the beginning. Furthermore, the policy usually predicts\npositions for a single syntactic constituent structure in consecutive steps. We\nbelieve our findings could provide more insights on the mechanism of undirected\ngeneration models and encourage further research in this direction. Our code is\npublicly available at https://github.com/jiangycTarheel/undirected-generation", "published": "2021-12-16 18:29:07", "link": "http://arxiv.org/abs/2112.09097v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unsupervised Dense Information Retrieval with Contrastive Learning", "abstract": "Recently, information retrieval has seen the emergence of dense retrievers,\nusing neural networks, as an alternative to classical sparse methods based on\nterm-frequency. These models have obtained state-of-the-art results on datasets\nand tasks where large training sets are available. However, they do not\ntransfer well to new applications with no training data, and are outperformed\nby unsupervised term-frequency methods such as BM25. In this work, we explore\nthe limits of contrastive learning as a way to train unsupervised dense\nretrievers and show that it leads to strong performance in various retrieval\nsettings. On the BEIR benchmark our unsupervised model outperforms BM25 on 11\nout of 15 datasets for the Recall@100. When used as pre-training before\nfine-tuning, either on a few thousands in-domain examples or on the large\nMS~MARCO dataset, our contrastive model leads to improvements on the BEIR\nbenchmark. Finally, we evaluate our approach for multi-lingual retrieval, where\ntraining data is even scarcer than for English, and show that our approach\nleads to strong unsupervised performance. Our model also exhibits strong\ncross-lingual transfer when fine-tuned on supervised English data only and\nevaluated on low resources language such as Swahili. We show that our\nunsupervised models can perform cross-lingual retrieval between different\nscripts, such as retrieving English documents from Arabic queries, which would\nnot be possible with term matching methods.", "published": "2021-12-16 18:57:37", "link": "http://arxiv.org/abs/2112.09118v4", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "An Empirical Investigation of the Role of Pre-training in Lifelong\n  Learning", "abstract": "The lifelong learning paradigm in machine learning is an attractive\nalternative to the more prominent isolated learning scheme not only due to its\nresemblance to biological learning but also its potential to reduce energy\nwaste by obviating excessive model re-training. A key challenge to this\nparadigm is the phenomenon of catastrophic forgetting. With the increasing\npopularity and success of pre-trained models in machine learning, we pose the\nquestion: What role does pre-training play in lifelong learning, specifically\nwith respect to catastrophic forgetting? We investigate existing methods in the\ncontext of large, pre-trained models and evaluate their performance on a\nvariety of text and image classification tasks, including a large-scale study\nusing a novel data set of 15 diverse NLP tasks. Across all settings, we observe\nthat generic pre-training implicitly alleviates the effects of catastrophic\nforgetting when learning multiple tasks sequentially compared to randomly\ninitialized models. We then further investigate why pre-training alleviates\nforgetting in this setting. We study this phenomenon by analyzing the loss\nlandscape, finding that pre-trained weights appear to ease forgetting by\nleading to wider minima. Based on this insight, we propose jointly optimizing\nfor current task loss and loss basin sharpness to explicitly encourage wider\nbasins during sequential fine-tuning. We show that this optimization approach\noutperforms several state-of-the-art task-sequential continual learning\nalgorithms across multiple settings, occasionally even without retaining a\nmemory that scales in size with the number of tasks.", "published": "2021-12-16 19:00:55", "link": "http://arxiv.org/abs/2112.09153v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Two-view Graph Neural Networks for Knowledge Graph Completion", "abstract": "We present an effective graph neural network (GNN)-based knowledge graph\nembedding model, which we name WGE, to capture entity- and relation-focused\ngraph structures. Given a knowledge graph, WGE builds a single undirected\nentity-focused graph that views entities as nodes. WGE also constructs another\nsingle undirected graph from relation-focused constraints, which views entities\nand relations as nodes. WGE then proposes a GNN-based architecture to better\nlearn vector representations of entities and relations from these two single\nentity- and relation-focused graphs. WGE feeds the learned entity and relation\nrepresentations into a weighted score function to return the triple scores for\nknowledge graph completion. Experimental results show that WGE outperforms\nstrong baselines on seven benchmark datasets for knowledge graph completion.", "published": "2021-12-16 22:36:17", "link": "http://arxiv.org/abs/2112.09231v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Logically at Factify 2022: Multimodal Fact Verification", "abstract": "This paper describes our participant system for the multi-modal fact\nverification (Factify) challenge at AAAI 2022. Despite the recent advance in\ntext based verification techniques and large pre-trained multimodal models\ncross vision and language, very limited work has been done in applying\nmultimodal techniques to automate fact checking process, particularly\nconsidering the increasing prevalence of claims and fake news about images and\nvideos on social media. In our work, the challenge is treated as multimodal\nentailment task and framed as multi-class classification. Two baseline\napproaches are proposed and explored including an ensemble model (combining two\nuni-modal models) and a multi-modal attention network (modeling the interaction\nbetween image and text pair from claim and evidence document). We conduct\nseveral experiments investigating and benchmarking different SoTA pre-trained\ntransformers and vision models in this work. Our best model is ranked first in\nleaderboard which obtains a weighted average F-measure of 0.77 on both\nvalidation and test set. Exploratory analysis of dataset is also carried out on\nthe Factify data set and uncovers salient patterns and issues (e.g., word\noverlapping, visual entailment correlation, source bias) that motivates our\nhypothesis. Finally, we highlight challenges of the task and multimodal dataset\nfor future research.", "published": "2021-12-16 23:34:07", "link": "http://arxiv.org/abs/2112.09253v2", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Improving Ethical Outcomes with Machine-in-the-Loop: Broadening Human\n  Understanding of Data Annotations", "abstract": "We introduce a machine-in-the-loop pipeline that aims to address root causes\nof unwanted bias in natural language based supervised machine learning tasks in\nthe education domain. Learning from the experiences of students is foundational\nfor education researchers, and academic administrators. 21st-century skills\nlearned from experience are becoming a core part of college and career\nreadiness as well as the hiring process in the new knowledge economy.\nMinoritized students demonstrate these skills in their daily lives, but\ndocumenting, assessing, and validating these skills is a huge problem for\neducational institutions. As an equity focused online platform, LivedX\ntranslates minoritized students' lived experiences into the 21st century\nskills, issues micro-credentials, and creates personal 21st century skills\nportfolio. To automate the micro credential mining from the natural language\ntexts received from the students' submitted essays, we employed a bag-of-word\nmodel to construct a multi-output classifier. Despite our goal, our model\ninitially exacerbated disparate impact on minoritized students. We used a\nmachine-in-the-loop model development pipeline to address the problem and\nrefine the aforementioned model to ensure fairness in its prediction.", "published": "2021-12-16 03:21:01", "link": "http://arxiv.org/abs/2112.09738v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Intelligent Online Selling Point Extraction for E-Commerce\n  Recommendation", "abstract": "In the past decade, automatic product description generation for e-commerce\nhave witnessed significant advancement. As the services provided by e-commerce\nplatforms become diverse, it is necessary to dynamically adapt the patterns of\ndescriptions generated. The selling point of products is an important type of\nproduct description for which the length should be as short as possible while\nstill conveying key information. In addition, this kind of product description\nshould be eye-catching to the readers. Currently, product selling points are\nnormally written by human experts. Thus, the creation and maintenance of these\ncontents incur high costs. These costs can be significantly reduced if product\nselling points can be automatically generated by machines. In this paper, we\nreport our experience developing and deploying the Intelligent Online Selling\nPoint Extraction (IOSPE) system to serve the recommendation system in the\nJD.com e-commerce platform. Since July 2020, IOSPE has become a core service\nfor 62 key categories of products (covering more than 4 million products). So\nfar, it has generated more than 0.1 billion selling points, thereby\nsignificantly scaling up the selling point creation operation and saving human\nlabour. These IOSPE generated selling points have increased the click-through\nrate (CTR) by 1.89\\% and the average duration the customers spent on the\nproducts by more than 2.03\\% compared to the previous practice, which are\nsignificant improvements for such a large-scale e-commerce platform.", "published": "2021-12-16 00:32:06", "link": "http://arxiv.org/abs/2112.10613v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "ALP: Data Augmentation using Lexicalized PCFGs for Few-Shot Text\n  Classification", "abstract": "Data augmentation has been an important ingredient for boosting performances\nof learned models. Prior data augmentation methods for few-shot text\nclassification have led to great performance boosts. However, they have not\nbeen designed to capture the intricate compositional structure of natural\nlanguage. As a result, they fail to generate samples with plausible and diverse\nsentence structures. Motivated by this, we present the data Augmentation using\nLexicalized Probabilistic context-free grammars (ALP) that generates augmented\nsamples with diverse syntactic structures with plausible grammar. The\nlexicalized PCFG parse trees consider both the constituents and dependencies to\nproduce a syntactic frame that maximizes a variety of word choices in a\nsyntactically preservable manner without specific domain experts. Experiments\non few-shot text classification tasks demonstrate that ALP enhances many\nstate-of-the-art classification methods. As a second contribution, we delve\ninto the train-val splitting methodologies when a data augmentation method\ncomes into play. We argue empirically that the traditional splitting of\ntraining and validation sets is sub-optimal compared to our novel\naugmentation-based splitting strategies that further expand the training split\nwith the same number of labeled data. Taken together, our contributions on the\ndata augmentation strategies yield a strong training recipe for few-shot text\nclassification tasks.", "published": "2021-12-16 09:56:35", "link": "http://arxiv.org/abs/2112.11916v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Hierarchical Cross-Modality Semantic Correlation Learning Model for\n  Multimodal Summarization", "abstract": "Multimodal summarization with multimodal output (MSMO) generates a summary\nwith both textual and visual content. Multimodal news report contains\nheterogeneous contents, which makes MSMO nontrivial. Moreover, it is observed\nthat different modalities of data in the news report correlate hierarchically.\nTraditional MSMO methods indistinguishably handle different modalities of data\nby learning a representation for the whole data, which is not directly\nadaptable to the heterogeneous contents and hierarchical correlation. In this\npaper, we propose a hierarchical cross-modality semantic correlation learning\nmodel (HCSCL) to learn the intra- and inter-modal correlation existing in the\nmultimodal data. HCSCL adopts a graph network to encode the intra-modal\ncorrelation. Then, a hierarchical fusion framework is proposed to learn the\nhierarchical correlation between text and images. Furthermore, we construct a\nnew dataset with relevant image annotation and image object label information\nto provide the supervision information for the learning procedure. Extensive\nexperiments on the dataset show that HCSCL significantly outperforms the\nbaseline methods in automatic summarization metrics and fine-grained diversity\ntests.", "published": "2021-12-16 01:46:30", "link": "http://arxiv.org/abs/2112.12072v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "SGEITL: Scene Graph Enhanced Image-Text Learning for Visual Commonsense\n  Reasoning", "abstract": "Answering complex questions about images is an ambitious goal for machine\nintelligence, which requires a joint understanding of images, text, and\ncommonsense knowledge, as well as a strong reasoning ability. Recently,\nmultimodal Transformers have made great progress in the task of Visual\nCommonsense Reasoning (VCR), by jointly understanding visual objects and text\ntokens through layers of cross-modality attention. However, these approaches do\nnot utilize the rich structure of the scene and the interactions between\nobjects which are essential in answering complex commonsense questions. We\npropose a Scene Graph Enhanced Image-Text Learning (SGEITL) framework to\nincorporate visual scene graphs in commonsense reasoning. To exploit the scene\ngraph structure, at the model structure level, we propose a multihop graph\ntransformer for regularizing attention interaction among hops. As for\npre-training, a scene-graph-aware pre-training method is proposed to leverage\nstructure knowledge extracted in the visual scene graph. Moreover, we introduce\na method to train and generate domain-relevant visual scene graphs using\ntextual annotations in a weakly-supervised manner. Extensive experiments on VCR\nand other tasks show a significant performance boost compared with the\nstate-of-the-art methods and prove the efficacy of each proposed component.", "published": "2021-12-16 03:16:30", "link": "http://arxiv.org/abs/2112.08587v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Explainable Natural Language Processing with Matrix Product States", "abstract": "Despite empirical successes of recurrent neural networks (RNNs) in natural\nlanguage processing (NLP), theoretical understanding of RNNs is still limited\ndue to intrinsically complex non-linear computations. We systematically analyze\nRNNs' behaviors in a ubiquitous NLP task, the sentiment analysis of movie\nreviews, via the mapping between a class of RNNs called recurrent arithmetic\ncircuits (RACs) and a matrix product state (MPS). Using the von-Neumann\nentanglement entropy (EE) as a proxy for information propagation, we show that\nsingle-layer RACs possess a maximum information propagation capacity, reflected\nby the saturation of the EE. Enlarging the bond dimension beyond the EE\nsaturation threshold does not increase model prediction accuracies, so a\nminimal model that best estimates the data statistics can be inferred. Although\nthe saturated EE is smaller than the maximum EE allowed by the area law, our\nminimal model still achieves ~99% training accuracies in realistic sentiment\nanalysis data sets. Thus, low EE is not a warrant against the adoption of\nsingle-layer RACs for NLP. Contrary to a common belief that long-range\ninformation propagation is the main source of RNNs' successes, we show that\nsingle-layer RACs harness high expressiveness from the subtle interplay between\nthe information propagation and the word vector embeddings. Our work sheds\nlight on the phenomenology of learning in RACs, and more generally on the\nexplainability of RNNs for NLP, using tools from many-body quantum physics.", "published": "2021-12-16 05:10:32", "link": "http://arxiv.org/abs/2112.08628v2", "categories": ["cond-mat.dis-nn", "cond-mat.stat-mech", "cs.CL", "cs.LG", "quant-ph", "15A69", "I.2.7"], "primary_category": "cond-mat.dis-nn"}
{"title": "Bioacoustic Event Detection with prototypical networks and data\n  augmentation", "abstract": "This report presents deep learning and data augmentation techniques used by a\nsystem entered into the Few-Shot Bioacoustic Event Detection for the DCASE2021\nChallenge. The remit was to develop a few-shot learning system for animal\n(mammal and bird) vocalisations. Participants were tasked with developing a\nmethod that can extract information from five exemplar vocalisations, or shots,\nof mammals or birds and detect and classify sounds in field recordings. In the\nsystem described in this report, prototypical networks are used to learn a\nmetric space, from which classification is performed by computing the distance\nof a query point to class prototypes, classifying based on shortest distance.\nWe describe the architecture of this network, feature extraction methods, and\ndata augmentation performed on the given dataset and compare our work to the\nchallenge's baseline networks.", "published": "2021-12-16 16:40:37", "link": "http://arxiv.org/abs/2112.09006v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Low Resource Species Agnostic Bird Activity Detection", "abstract": "This paper explores low resource classifiers and features for the detection\nof bird activity, suitable for embedded Automatic Recording Units which are\ntypically deployed for long term remote monitoring of bird populations.\nFeatures include low-level spectral parameters, statistical moments on pitch\nsamples, and features derived from amplitude modulation. Performance is\nevaluated on several lightweight classifiers using the NIPS4Bplus dataset. Our\nexperiments show that random forest classifiers perform best on this task,\nachieving an accuracy of 0.721 and an F1-Score of 0.604. We compare the results\nof our system against both a Convolutional Neural Network based detector, and\nstandard MFCC features. Our experiments show that we can achieve equal or\nbetter performance in most metrics using features and models with a smaller\ncomputational cost and which are suitable for edge deployment.", "published": "2021-12-16 17:34:36", "link": "http://arxiv.org/abs/2112.09042v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "EmotionBox: a music-element-driven emotional music generation system\n  using Recurrent Neural Network", "abstract": "With the development of deep neural networks, automatic music composition has\nmade great progress. Although emotional music can evoke listeners' different\nemotions and it is important for artistic expression, only few researches have\nfocused on generating emotional music. This paper presents EmotionBox -an\nmusic-element-driven emotional music generator that is capable of composing\nmusic given a specific emotion, where this model does not require a music\ndataset labeled with emotions. Instead, pitch histogram and note density are\nextracted as features that represent mode and tempo respectively to control\nmusic emotions. The subjective listening tests show that the Emotionbox has a\nmore competitive and balanced performance in arousing a specified emotion than\nthe emotion-label-based method.", "published": "2021-12-16 01:50:07", "link": "http://arxiv.org/abs/2112.08561v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Knowledge Distillation Leveraging Alternative Soft Targets from\n  Non-Parallel Qualified Speech Data", "abstract": "This paper describes a novel knowledge distillation framework that leverages\nacoustically qualified speech data included in an existing training data pool\nas privileged information. In our proposed framework, a student network is\ntrained with multiple soft targets for each utterance that consist of main soft\ntargets from original speakers' utterance and alternative targets from other\nspeakers' utterances spoken under better acoustic conditions as a secondary\nview. These qualified utterances from other speakers, used to generate better\nsoft targets, are collected from a qualified data pool by using strict\nconstraints in terms of word/phone/state durations. Our proposed method is a\nform of target-side data augmentation that creates multiple copies of data with\ncorresponding better soft targets obtained from a qualified data pool. We show\nin our experiments under acoustic model adaptation settings that the proposed\nmethod, exploiting better soft targets obtained from various speakers, can\nfurther improve recognition accuracy compared with conventional methods using\nonly soft targets from original speakers.", "published": "2021-12-16 13:43:54", "link": "http://arxiv.org/abs/2112.08878v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Bootstrap Equilibrium and Probabilistic Speaker Representation Learning\n  for Self-supervised Speaker Verification", "abstract": "In this paper, we propose self-supervised speaker representation learning\nstrategies, which comprise of a bootstrap equilibrium speaker representation\nlearning in the front-end and an uncertainty-aware probabilistic speaker\nembedding training in the back-end. In the front-end stage, we learn the\nspeaker representations via the bootstrap training scheme with the uniformity\nregularization term. In the back-end stage, the probabilistic speaker\nembeddings are estimated by maximizing the mutual likelihood score between the\nspeech samples belonging to the same speaker, which provide not only speaker\nrepresentations but also data uncertainty. Experimental results show that the\nproposed bootstrap equilibrium training strategy can effectively help learn the\nspeaker representations and outperforms the conventional methods based on\ncontrastive learning. Also, we demonstrate that the integrated two-stage\nframework further improves the speaker verification performance on the\nVoxCeleb1 test set in terms of EER and MinDCF.", "published": "2021-12-16 14:55:44", "link": "http://arxiv.org/abs/2112.08929v2", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Object-based synthesis of scraping and rolling sounds based on\n  non-linear physical constraints", "abstract": "Sustained contact interactions like scraping and rolling produce a wide\nvariety of sounds. Previous studies have explored ways to synthesize these\nsounds efficiently and intuitively but could not fully mimic the rich structure\nof real instances of these sounds. We present a novel source-filter model for\nrealistic synthesis of scraping and rolling sounds with physically and\nperceptually relevant controllable parameters constrained by principles of\nmechanics. Key features of our model include non-linearities to constrain the\ncontact force, naturalistic normal force variation for different motions, and a\nmethod for morphing impulse responses within a material to achieve\nlocation-dependence. Perceptual experiments show that the presented model is\nable to synthesize realistic scraping and rolling sounds while conveying\nphysical information similar to that in recorded sounds.", "published": "2021-12-16 15:58:02", "link": "http://arxiv.org/abs/2112.08984v1", "categories": ["eess.AS", "cs.SD", "eess.SP", "physics.app-ph"], "primary_category": "eess.AS"}
{"title": "Towards Robust Real-time Audio-Visual Speech Enhancement", "abstract": "The human brain contextually exploits heterogeneous sensory information to\nefficiently perform cognitive tasks including vision and hearing. For example,\nduring the cocktail party situation, the human auditory cortex contextually\nintegrates audio-visual (AV) cues in order to better perceive speech. Recent\nstudies have shown that AV speech enhancement (SE) models can significantly\nimprove speech quality and intelligibility in very low signal to noise ratio\n(SNR) environments as compared to audio-only SE models. However, despite\nsignificant research in the area of AV SE, development of real-time processing\nmodels with low latency remains a formidable technical challenge. In this\npaper, we present a novel framework for low latency speaker-independent AV SE\nthat can generalise on a range of visual and acoustic noises. In particular, a\ngenerative adversarial networks (GAN) is proposed to address the practical\nissue of visual imperfections in AV SE. In addition, we propose a deep neural\nnetwork based real-time AV SE model that takes into account the cleaned visual\nspeech output from GAN to deliver more robust SE. The proposed framework is\nevaluated on synthetic and real noisy AV corpora using objective speech quality\nand intelligibility metrics and subjective listing tests. Comparative\nsimulation results show that our real time AV SE framework outperforms\nstate-of-the-art SE approaches, including recent DNN based SE models.", "published": "2021-12-16 17:54:45", "link": "http://arxiv.org/abs/2112.09060v1", "categories": ["cs.SD", "cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
