{"title": "RuDSI: graph-based word sense induction dataset for Russian", "abstract": "We present RuDSI, a new benchmark for word sense induction (WSI) in Russian.\nThe dataset was created using manual annotation and semi-automatic clustering\nof Word Usage Graphs (WUGs). Unlike prior WSI datasets for Russian, RuDSI is\ncompletely data-driven (based on texts from Russian National Corpus), with no\nexternal word senses imposed on annotators. Depending on the parameters of\ngraph clustering, different derivative datasets can be produced from raw\nannotation. We report the performance that several baseline WSI methods obtain\non RuDSI and discuss possibilities for improving these scores.", "published": "2022-09-28 00:08:24", "link": "http://arxiv.org/abs/2209.13750v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Structured Summarization: Unified Text Segmentation and Segment Labeling\n  as a Generation Task", "abstract": "Text segmentation aims to divide text into contiguous, semantically coherent\nsegments, while segment labeling deals with producing labels for each segment.\nPast work has shown success in tackling segmentation and labeling for documents\nand conversations. This has been possible with a combination of task-specific\npipelines, supervised and unsupervised learning objectives. In this work, we\npropose a single encoder-decoder neural network that can handle long documents\nand conversations, trained simultaneously for both segmentation and segment\nlabeling using only standard supervision. We successfully show a way to solve\nthe combined task as a pure generation task, which we refer to as structured\nsummarization. We apply the same technique to both document and conversational\ndata, and we show state of the art performance across datasets for both\nsegmentation and labeling, under both high- and low-resource settings. Our\nresults establish a strong case for considering text segmentation and segment\nlabeling as a whole, and moving towards general-purpose techniques that don't\ndepend on domain expertise or task-specific components.", "published": "2022-09-28 01:08:50", "link": "http://arxiv.org/abs/2209.13759v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "METS-CoV: A Dataset of Medical Entity and Targeted Sentiment on COVID-19\n  Related Tweets", "abstract": "The COVID-19 pandemic continues to bring up various topics discussed or\ndebated on social media. In order to explore the impact of pandemics on\npeople's lives, it is crucial to understand the public's concerns and attitudes\ntowards pandemic-related entities (e.g., drugs, vaccines) on social media.\nHowever, models trained on existing named entity recognition (NER) or targeted\nsentiment analysis (TSA) datasets have limited ability to understand\nCOVID-19-related social media texts because these datasets are not designed or\nannotated from a medical perspective. This paper releases METS-CoV, a dataset\ncontaining medical entities and targeted sentiments from COVID-19-related\ntweets. METS-CoV contains 10,000 tweets with 7 types of entities, including 4\nmedical entity types (Disease, Drug, Symptom, and Vaccine) and 3 general entity\ntypes (Person, Location, and Organization). To further investigate tweet users'\nattitudes toward specific entities, 4 types of entities (Person, Organization,\nDrug, and Vaccine) are selected and annotated with user sentiments, resulting\nin a targeted sentiment dataset with 9,101 entities (in 5,278 tweets). To the\nbest of our knowledge, METS-CoV is the first dataset to collect medical\nentities and corresponding sentiments of COVID-19-related tweets. We benchmark\nthe performance of classical machine learning models and state-of-the-art deep\nlearning models on NER and TSA tasks with extensive experiments. Results show\nthat the dataset has vast room for improvement for both NER and TSA tasks.\nMETS-CoV is an important resource for developing better medical social media\ntools and facilitating computational social science research, especially in\nepidemiology. Our data, annotation guidelines, benchmark models, and source\ncode are publicly available (https://github.com/YLab-Open/METS-CoV) to ensure\nreproducibility.", "published": "2022-09-28 01:55:14", "link": "http://arxiv.org/abs/2209.13773v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data-driven Parsing Evaluation for Child-Parent Interactions", "abstract": "We present a syntactic dependency treebank for naturalistic child and\nchild-directed speech in English (MacWhinney, 2000). Our annotations largely\nfollowed the guidelines of the Universal Dependencies project (UD (Zeman et\nal., 2022)), with detailed extensions to lexical/syntactic structures unique to\nconversational speech (in opposition to written texts). Compared to existing\nUD-style spoken treebanks as well as other dependency corpora of child-parent\ninteractions specifically, our dataset is of (much) larger size (N of\nutterances = 44,744; N of words = 233, 907) and contains speech from a total of\n10 children covering a wide age range (18-66 months). With this dataset, we\nask: (1) How well would state-of-the-art dependency parsers, tailored for the\nwritten domain, perform for speech of different interlocutors in spontaneous\nconversations? (2) What is the relationship between parser performance and the\ndevelopmental stage of the child? To address these questions, in ongoing work,\nwe are conducting thorough dependency parser evaluations using both graph-based\nand transition-based parsers with different hyperparameterization, trained from\nthree different types of out-of-domain written texts: news, tweets, and learner\ndata.", "published": "2022-09-28 02:13:26", "link": "http://arxiv.org/abs/2209.13778v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "YATO: Yet Another deep learning based Text analysis Open toolkit", "abstract": "We introduce YATO, an open-source, easy-to-use toolkit for text analysis with\ndeep learning. Different from existing heavily engineered toolkits and\nplatforms, YATO is lightweight and user-friendly for researchers from\ncross-disciplinary areas. Designed in a hierarchical structure, YATO supports\nfree combinations of three types of widely used features including 1)\ntraditional neural networks (CNN, RNN, etc.); 2) pre-trained language models\n(BERT, RoBERTa, ELECTRA, etc.); and 3) user-customized neural features via a\nsimple configurable file. Benefiting from the advantages of flexibility and\nease of use, YATO can facilitate fast reproduction and refinement of\nstate-of-the-art NLP models, and promote the cross-disciplinary applications of\nNLP techniques. The code, examples, and documentation are publicly available at\nhttps://github.com/jiesutd/YATO. A demo video is also available at\nhttps://www.youtube.com/playlist?list=PLJ0mhzMcRuDUlTkzBfAftOqiJRxYTTjXH.", "published": "2022-09-28 07:25:04", "link": "http://arxiv.org/abs/2209.13877v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revamping Multilingual Agreement Bidirectionally via Switched\n  Back-translation for Multilingual Neural Machine Translation", "abstract": "Despite the fact that multilingual agreement (MA) has shown its importance\nfor multilingual neural machine translation (MNMT), current methodologies in\nthe field have two shortages: (i) require parallel data between multiple\nlanguage pairs, which is not always realistic and (ii) optimize the agreement\nin an ambiguous direction, which hampers the translation performance. We\npresent \\textbf{B}idirectional \\textbf{M}ultilingual \\textbf{A}greement via\n\\textbf{S}witched \\textbf{B}ack-\\textbf{t}ranslation (\\textbf{BMA-SBT}), a\nnovel and universal multilingual agreement framework for fine-tuning\npre-trained MNMT models, which (i) exempts the need for aforementioned parallel\ndata by using a novel method called switched BT that creates synthetic text\nwritten in another source language using the translation target and (ii)\noptimizes the agreement bidirectionally with the Kullback-Leibler Divergence\nloss. Experiments indicate that BMA-SBT clearly improves the strong baselines\non the task of MNMT with three benchmarks: TED Talks, News, and Europarl.\nIn-depth analyzes indicate that BMA-SBT brings additive improvements to the\nconventional BT method.", "published": "2022-09-28 09:14:58", "link": "http://arxiv.org/abs/2209.13940v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CEFER: A Four Facets Framework based on Context and Emotion embedded\n  features for Implicit and Explicit Emotion Recognition", "abstract": "People's conduct and reactions are driven by their emotions. Online social\nmedia is becoming a great instrument for expressing emotions in written form.\nPaying attention to the context and the entire sentence help us to detect\nemotion from texts. However, this perspective inhibits us from noticing some\nemotional words or phrases in the text, particularly when the words express an\nemotion implicitly rather than explicitly. On the other hand, focusing only on\nthe words and ignoring the context results in a distorted understanding of the\nsentence meaning and feeling. In this paper, we propose a framework that\nanalyses text at both the sentence and word levels. We name it CEFER (Context\nand Emotion embedded Framework for Emotion Recognition). Our four approach\nfacets are to extracting data by considering the entire sentence and each\nindividual word simultaneously, as well as implicit and explicit emotions. The\nknowledge gained from these data not only mitigates the impact of flaws in the\npreceding approaches but also it strengthens the feature vector. We evaluate\nseveral feature spaces using BERT family and design the CEFER based on them.\nCEFER combines the emotional vector of each word, including explicit and\nimplicit emotions, with the feature vector of each word based on context. CEFER\nperforms better than the BERT family. The experimental results demonstrate that\nidentifying implicit emotions are more challenging than detecting explicit\nemotions. CEFER, improves the accuracy of implicit emotion recognition.\nAccording to the results, CEFER perform 5% better than the BERT family in\nrecognizing explicit emotions and 3% in implicit.", "published": "2022-09-28 11:16:32", "link": "http://arxiv.org/abs/2209.13999v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Keyword Extraction from Short Texts with a Text-To-Text Transfer\n  Transformer", "abstract": "The paper explores the relevance of the Text-To-Text Transfer Transformer\nlanguage model (T5) for Polish (plT5) to the task of intrinsic and extrinsic\nkeyword extraction from short text passages. The evaluation is carried out on\nthe new Polish Open Science Metadata Corpus (POSMAC), which is released with\nthis paper: a collection of 216,214 abstracts of scientific publications\ncompiled in the CURLICAT project. We compare the results obtained by four\ndifferent methods, i.e. plT5kw, extremeText, TermoPL, KeyBERT and conclude that\nthe plT5kw model yields particularly promising results for both frequent and\nsparsely represented keywords. Furthermore, a plT5kw keyword generation model\ntrained on the POSMAC also seems to produce highly useful results in\ncross-domain text labelling scenarios. We discuss the performance of the model\non news stories and phone-based dialog transcripts which represent text genres\nand domains extrinsic to the dataset of scientific abstracts. Finally, we also\nattempt to characterize the challenges of evaluating a text-to-text model on\nboth intrinsic and extrinsic keyword extraction.", "published": "2022-09-28 11:31:43", "link": "http://arxiv.org/abs/2209.14008v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Effective General-Domain Data Inclusion for the Machine Translation Task\n  by Vanilla Transformers", "abstract": "One of the vital breakthroughs in the history of machine translation is the\ndevelopment of the Transformer model. Not only it is revolutionary for various\ntranslation tasks, but also for a majority of other NLP tasks. In this paper,\nwe aim at a Transformer-based system that is able to translate a source\nsentence in German to its counterpart target sentence in English. We perform\nthe experiments on the news commentary German-English parallel sentences from\nthe WMT'13 dataset. In addition, we investigate the effect of the inclusion of\nadditional general-domain data in training from the IWSLT'16 dataset to improve\nthe Transformer model performance. We find that including the IWSLT'16 dataset\nin training helps achieve a gain of 2 BLEU score points on the test set of the\nWMT'13 dataset. Qualitative analysis is introduced to analyze how the usage of\ngeneral-domain data helps improve the quality of the produced translation\nsentences.", "published": "2022-09-28 13:14:49", "link": "http://arxiv.org/abs/2209.14073v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Zero to Production: Baltic-Ukrainian Machine Translation Systems to\n  Aid Refugees", "abstract": "In this paper, we examine the development and usage of six low-resource\nmachine translation systems translating between the Ukrainian language and each\nof the official languages of the Baltic states. We developed these systems in\nreaction to the escalating Ukrainian refugee crisis caused by the Russian\nmilitary aggression in Ukraine in the hope that they might be helpful for\nrefugees and public administrations. Now, two months after MT systems were made\npublic, we analyze their usage patterns and statistics. Our findings show that\nthe Latvian-Ukrainian and Lithuanian-Ukrainian systems are integrated into the\npublic services of Baltic states, leading to more than 127 million translated\nsentences for the Lithuanian-Ukrainian system. Motivated by these findings, we\nfurther enhance our MT systems by better Ukrainian toponym translation and\npublish an improved version of the Lithuanian-Ukrainian system.", "published": "2022-09-28 14:46:01", "link": "http://arxiv.org/abs/2209.14142v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Supervised Contrastive Learning as Multi-Objective Optimization for\n  Fine-Tuning Large Pre-trained Language Models", "abstract": "Recently, Supervised Contrastive Learning (SCL) has been shown to achieve\nexcellent performance in most classification tasks. In SCL, a neural network is\ntrained to optimize two objectives: pull an anchor and positive samples\ntogether in the embedding space, and push the anchor apart from the negatives.\nHowever, these two different objectives may conflict, requiring trade-offs\nbetween them during optimization. In this work, we formulate the SCL problem as\na Multi-Objective Optimization problem for the fine-tuning phase of RoBERTa\nlanguage model. Two methods are utilized to solve the optimization problem: (i)\nthe linear scalarization (LS) method, which minimizes a weighted linear\ncombination of pertask losses; and (ii) the Exact Pareto Optimal (EPO) method\nwhich finds the intersection of the Pareto front with a given preference\nvector. We evaluate our approach on several GLUE benchmark tasks, without using\ndata augmentations, memory banks, or generating adversarial examples. The\nempirical results show that the proposed learning strategy significantly\noutperforms a strong competitive contrastive learning baseline", "published": "2022-09-28 15:13:58", "link": "http://arxiv.org/abs/2209.14161v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Automatic Evaluation of the WMT22 General Machine Translation Task", "abstract": "This report presents an automatic evaluation of the general machine\ntranslation task of the Seventh Conference on Machine Translation (WMT22). It\nevaluates a total of 185 systems for 21 translation directions including\nhigh-resource to low-resource language pairs and from closely related to\ndistant languages. This large-scale automatic evaluation highlights some of the\ncurrent limits of state-of-the-art machine translation systems. It also shows\nhow automatic metrics, namely chrF, BLEU, and COMET, can complement themselves\nto mitigate their own limits in terms of interpretability and accuracy.", "published": "2022-09-28 15:31:57", "link": "http://arxiv.org/abs/2209.14172v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Causal Proxy Models for Concept-Based Model Explanations", "abstract": "Explainability methods for NLP systems encounter a version of the fundamental\nproblem of causal inference: for a given ground-truth input text, we never\ntruly observe the counterfactual texts necessary for isolating the causal\neffects of model representations on outputs. In response, many explainability\nmethods make no use of counterfactual texts, assuming they will be unavailable.\nIn this paper, we show that robust causal explainability methods can be created\nusing approximate counterfactuals, which can be written by humans to\napproximate a specific counterfactual or simply sampled using metadata-guided\nheuristics. The core of our proposal is the Causal Proxy Model (CPM). A CPM\nexplains a black-box model $\\mathcal{N}$ because it is trained to have the same\nactual input/output behavior as $\\mathcal{N}$ while creating neural\nrepresentations that can be intervened upon to simulate the counterfactual\ninput/output behavior of $\\mathcal{N}$. Furthermore, we show that the best CPM\nfor $\\mathcal{N}$ performs comparably to $\\mathcal{N}$ in making factual\npredictions, which means that the CPM can simply replace $\\mathcal{N}$, leading\nto more explainable deployed models. Our code is available at\nhttps://github.com/frankaging/Causal-Proxy-Model.", "published": "2022-09-28 17:45:07", "link": "http://arxiv.org/abs/2209.14279v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Who is GPT-3? An Exploration of Personality, Values and Demographics", "abstract": "Language models such as GPT-3 have caused a furore in the research community.\nSome studies found that GPT-3 has some creative abilities and makes mistakes\nthat are on par with human behaviour. This paper answers a related question:\nWho is GPT-3? We administered two validated measurement tools to GPT-3 to\nassess its personality, the values it holds and its self-reported demographics.\nOur results show that GPT-3 scores similarly to human samples in terms of\npersonality and - when provided with a model response memory - in terms of the\nvalues it holds. We provide the first evidence of psychological assessment of\nthe GPT-3 model and thereby add to our understanding of this language model. We\nclose with suggestions for future research that moves social science closer to\nlanguage models and vice versa.", "published": "2022-09-28 18:07:02", "link": "http://arxiv.org/abs/2209.14338v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Happy or grumpy? A Machine Learning Approach to Analyze the Sentiment of\n  Airline Passengers' Tweets", "abstract": "As one of the most extensive social networking services, Twitter has more\nthan 300 million active users as of 2022. Among its many functions, Twitter is\nnow one of the go-to platforms for consumers to share their opinions about\nproducts or experiences, including flight services provided by commercial\nairlines. This study aims to measure customer satisfaction by analyzing\nsentiments of Tweets that mention airlines using a machine learning approach.\nRelevant Tweets are retrieved from Twitter's API and processed through\ntokenization and vectorization. After that, these processed vectors are passed\ninto a pre-trained machine learning classifier to predict the sentiments. In\naddition to sentiment analysis, we also perform lexical analysis on the\ncollected Tweets to model keywords' frequencies, which provide meaningful\ncontexts to facilitate the interpretation of sentiments. We then apply time\nseries methods such as Bollinger Bands to detect abnormalities in sentiment\ndata. Using historical records from January to July 2022, our approach is\nproven to be capable of capturing sudden and significant changes in passengers'\nsentiment. This study has the potential to be developed into an application\nthat can help airlines, along with several other customer-facing businesses,\nefficiently detect abrupt changes in customers' sentiments and take adequate\nmeasures to counteract them.", "published": "2022-09-28 18:50:11", "link": "http://arxiv.org/abs/2209.14363v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Clinical Language Understanding Evaluation (CLUE)", "abstract": "Clinical language processing has received a lot of attention in recent years,\nresulting in new models or methods for disease phenotyping, mortality\nprediction, and other tasks. Unfortunately, many of these approaches are tested\nunder different experimental settings (e.g., data sources, training and testing\nsplits, metrics, evaluation criteria, etc.) making it difficult to compare\napproaches and determine state-of-the-art. To address these issues and\nfacilitate reproducibility and comparison, we present the Clinical Language\nUnderstanding Evaluation (CLUE) benchmark with a set of four clinical language\nunderstanding tasks, standard training, development, validation and testing\nsets derived from MIMIC data, as well as a software toolkit. It is our hope\nthat these data will enable direct comparison between approaches, improve\nreproducibility, and reduce the barrier-to-entry for developing novel models or\nmethods for these clinical language understanding tasks.", "published": "2022-09-28 19:14:08", "link": "http://arxiv.org/abs/2209.14377v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Text-to-SQL Semantic Parsing with Fine-grained Query\n  Understanding", "abstract": "Most recent research on Text-to-SQL semantic parsing relies on either parser\nitself or simple heuristic based approach to understand natural language query\n(NLQ). When synthesizing a SQL query, there is no explicit semantic information\nof NLQ available to the parser which leads to undesirable generalization\nperformance. In addition, without lexical-level fine-grained query\nunderstanding, linking between query and database can only rely on fuzzy string\nmatch which leads to suboptimal performance in real applications. In view of\nthis, in this paper we present a general-purpose, modular neural semantic\nparsing framework that is based on token-level fine-grained query\nunderstanding. Our framework consists of three modules: named entity recognizer\n(NER), neural entity linker (NEL) and neural semantic parser (NSP). By jointly\nmodeling query and database, NER model analyzes user intents and identifies\nentities in the query. NEL model links typed entities to schema and cell values\nin database. Parser model leverages available semantic information and linking\nresults and synthesizes tree-structured SQL queries based on dynamically\ngenerated grammar. Experiments on SQUALL, a newly released semantic parsing\ndataset, show that we can achieve 56.8% execution accuracy on\nWikiTableQuestions (WTQ) test set, which outperforms the state-of-the-art model\nby 2.7%.", "published": "2022-09-28 21:00:30", "link": "http://arxiv.org/abs/2209.14415v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using contradictions improves question answering systems", "abstract": "This work examines the use of contradiction in natural language inference\n(NLI) for question answering (QA). Typically, NLI systems help answer questions\nby determining if a potential answer is \\emph{entailed} (supported) by some\nbackground context. But is it useful to also determine if an answer contradicts\nthe context? We test this in two settings, multiple choice and extractive QA,\nand find that systems that incorporate contradiction can do slightly better\nthan entailment-only systems on certain datasets. However, the best\nperformances come from using contradiction, entailment, and QA model confidence\nscores together. This has implications for the deployment of QA systems in\ndomains such as medicine and science where safety is an issue.", "published": "2022-09-28 02:44:01", "link": "http://arxiv.org/abs/2211.05598v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Natural Language Processing Methods to Identify Oncology Patients at\n  High Risk for Acute Care with Clinical Notes", "abstract": "Clinical notes are an essential component of a health record. This paper\nevaluates how natural language processing (NLP) can be used to identify the\nrisk of acute care use (ACU) in oncology patients, once chemotherapy starts.\nRisk prediction using structured health data (SHD) is now standard, but\npredictions using free-text formats are complex. This paper explores the use of\nfree-text notes for the prediction of ACU instead of SHD. Deep Learning models\nwere compared to manually engineered language features. Results show that SHD\nmodels minimally outperform NLP models; an l1-penalised logistic regression\nwith SHD achieved a C-statistic of 0.748 (95%-CI: 0.735, 0.762), while the same\nmodel with language features achieved 0.730 (95%-CI: 0.717, 0.745) and a\ntransformer-based model achieved 0.702 (95%-CI: 0.688, 0.717). This paper shows\nhow language models can be used in clinical applications and underlines how\nrisk bias is different for diverse patient groups, even using only free-text\ndata.", "published": "2022-09-28 06:31:19", "link": "http://arxiv.org/abs/2209.13860v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "UCEpic: Unifying Aspect Planning and Lexical Constraints for Generating\n  Explanations in Recommendation", "abstract": "Personalized natural language generation for explainable recommendations\nplays a key role in justifying why a recommendation might match a user's\ninterests. Existing models usually control the generation process by aspect\nplanning. While promising, these aspect-planning methods struggle to generate\nspecific information correctly, which prevents generated explanations from\nbeing convincing. In this paper, we claim that introducing lexical constraints\ncan alleviate the above issues. We propose a model, UCEpic, that generates\nhigh-quality personalized explanations for recommendation results by unifying\naspect planning and lexical constraints in an insertion-based generation\nmanner.\n  Methodologically, to ensure text generation quality and robustness to various\nlexical constraints, we pre-train a non-personalized text generator via our\nproposed robust insertion process. Then, to obtain personalized explanations\nunder this framework of insertion-based generation, we design a method of\nincorporating aspect planning and personalized references into the insertion\nprocess. Hence, UCEpic unifies aspect planning and lexical constraints into one\nframework and generates explanations for recommendations under different\nsettings. Compared to previous recommendation explanation generators controlled\nby only aspects, UCEpic incorporates specific information from keyphrases and\nthen largely improves the diversity and informativeness of generated\nexplanations for recommendations on datasets such as RateBeer and Yelp.", "published": "2022-09-28 07:33:50", "link": "http://arxiv.org/abs/2209.13885v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Hierarchical MixUp Multi-label Classification with Imbalanced\n  Interdisciplinary Research Proposals", "abstract": "Funding agencies are largely relied on a topic matching between domain\nexperts and research proposals to assign proposal reviewers. As proposals are\nincreasingly interdisciplinary, it is challenging to profile the\ninterdisciplinary nature of a proposal, and, thereafter, find expert reviewers\nwith an appropriate set of expertise. An essential step in solving this\nchallenge is to accurately model and classify the interdisciplinary labels of a\nproposal. Existing methodological and application-related literature, such as\ntextual classification and proposal classification, are insufficient in jointly\naddressing the three key unique issues introduced by interdisciplinary proposal\ndata: 1) the hierarchical structure of discipline labels of a proposal from\ncoarse-grain to fine-grain, e.g., from information science to AI to\nfundamentals of AI. 2) the heterogeneous semantics of various main textual\nparts that play different roles in a proposal; 3) the number of proposals is\nimbalanced between non-interdisciplinary and interdisciplinary research. Can we\nsimultaneously address the three issues in understanding the proposal's\ninterdisciplinary nature? In response to this question, we propose a\nhierarchical mixup multiple-label classification framework, which we called\nH-MixUp. H-MixUp leverages a transformer-based semantic information extractor\nand a GCN-based interdisciplinary knowledge extractor for the first and second\nissues. H-MixUp develops a fused training method of Wold-level MixUp,\nWord-level CutMix, Manifold MixUp, and Document-level MixUp to address the\nthird issue.", "published": "2022-09-28 08:27:52", "link": "http://arxiv.org/abs/2209.13912v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Explaining Autonomy with Verbalised Decision Tree States", "abstract": "The development of new AUV technology increased the range of tasks that AUVs\ncan tackle and the length of their operations. As a result, AUVs are capable of\nhandling highly complex operations. However, these missions do not fit easily\ninto the traditional method of defining a mission as a series of pre-planned\nwaypoints because it is not possible to know, in advance, everything that might\noccur during the mission. This results in a gap between the operator's\nexpectations and actual operational performance. Consequently, this can create\na diminished level of trust between the operators and AUVs, resulting in\nunnecessary mission interruptions. To bridge this gap between in-mission\nrobotic behaviours and operators' expectations, this work aims to provide a\nframework to explain decisions and actions taken by an autonomous vehicle\nduring the mission, in an easy-to-understand manner. Additionally, the\nobjective is to have an autonomy-agnostic system that can be added as an\nadditional layer on top of any autonomy architecture. To make the approach\napplicable across different autonomous systems equipped with different\nautonomies, this work decouples the inner workings of the autonomy from the\ndecision points and the resulting executed actions applying Knowledge\nDistillation. Finally, to present the explanations to the operators in a more\nnatural way, the output of the distilled decision tree is combined with natural\nlanguage explanations and reported to the operators as sentences. For this\nreason, an additional step known as Concept2Text Generation is added at the end\nof the explanation pipeline.", "published": "2022-09-28 10:28:01", "link": "http://arxiv.org/abs/2209.13985v1", "categories": ["cs.RO", "cs.CL"], "primary_category": "cs.RO"}
{"title": "Offensive Language Detection on Twitter", "abstract": "Detection of offensive language in social media is one of the key challenges\nfor social media. Researchers have proposed many advanced methods to accomplish\nthis task. In this report, we try to use the learnings from their approach and\nincorporate our ideas to improve upon them. We have successfully achieved an\naccuracy of 74% in classifying offensive tweets. We also list upcoming\nchallenges in the abusive content detection in the social media world.", "published": "2022-09-28 13:35:22", "link": "http://arxiv.org/abs/2209.14091v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FiD-Light: Efficient and Effective Retrieval-Augmented Text Generation", "abstract": "Retrieval-augmented generation models offer many benefits over standalone\nlanguage models: besides a textual answer to a given query they provide\nprovenance items retrieved from an updateable knowledge base. However, they are\nalso more complex systems and need to handle long inputs. In this work, we\nintroduce FiD-Light to strongly increase the efficiency of the state-of-the-art\nretrieval-augmented FiD model, while maintaining the same level of\neffectiveness. Our FiD-Light model constrains the information flow from the\nencoder (which encodes passages separately) to the decoder (using concatenated\nencoded representations). Furthermore, we adapt FiD-Light with re-ranking\ncapabilities through textual source pointers, to improve the top-ranked\nprovenance precision. Our experiments on a diverse set of seven knowledge\nintensive tasks (KILT) show FiD-Light consistently improves the Pareto frontier\nbetween query latency and effectiveness. FiD-Light with source pointing sets\nsubstantial new state-of-the-art results on six KILT tasks for combined text\ngeneration and provenance retrieval evaluation, while maintaining reasonable\nefficiency.", "published": "2022-09-28 17:54:55", "link": "http://arxiv.org/abs/2209.14290v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Improving alignment of dialogue agents via targeted human judgements", "abstract": "We present Sparrow, an information-seeking dialogue agent trained to be more\nhelpful, correct, and harmless compared to prompted language model baselines.\nWe use reinforcement learning from human feedback to train our models with two\nnew additions to help human raters judge agent behaviour. First, to make our\nagent more helpful and harmless, we break down the requirements for good\ndialogue into natural language rules the agent should follow, and ask raters\nabout each rule separately. We demonstrate that this breakdown enables us to\ncollect more targeted human judgements of agent behaviour and allows for more\nefficient rule-conditional reward models. Second, our agent provides evidence\nfrom sources supporting factual claims when collecting preference judgements\nover model statements. For factual questions, evidence provided by Sparrow\nsupports the sampled response 78% of the time. Sparrow is preferred more often\nthan baselines while being more resilient to adversarial probing by humans,\nviolating our rules only 8% of the time when probed. Finally, we conduct\nextensive analyses showing that though our model learns to follow our rules it\ncan exhibit distributional biases.", "published": "2022-09-28 19:04:43", "link": "http://arxiv.org/abs/2209.14375v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Downstream Datasets Make Surprisingly Good Pretraining Corpora", "abstract": "For most natural language processing tasks, the dominant practice is to\nfinetune large pretrained transformer models (e.g., BERT) using smaller\ndownstream datasets. Despite the success of this approach, it remains unclear\nto what extent these gains are attributable to the massive background corpora\nemployed for pretraining versus to the pretraining objectives themselves. This\npaper introduces a large-scale study of self-pretraining, where the same\n(downstream) training data is used for both pretraining and finetuning. In\nexperiments addressing both ELECTRA and RoBERTa models and 10 distinct\ndownstream classification datasets, we observe that self-pretraining rivals\nstandard pretraining on the BookWiki corpus (despite using around\n$10\\times$--$500\\times$ less data), outperforming the latter on $7$ and $5$\ndatasets, respectively. Surprisingly, these task-specific pretrained models\noften perform well on other tasks, including the GLUE benchmark. Besides\nclassification tasks, self-pretraining also provides benefits on structured\noutput prediction tasks such as span based question answering and commonsense\ninference, often providing more than $50\\%$ of the performance boosts provided\nby pretraining on the BookWiki corpus. Our results hint that in many scenarios,\nperformance gains attributable to pretraining are driven primarily by the\npretraining objective itself and are not always attributable to the use of\nexternal pretraining data in massive amounts. These findings are especially\nrelevant in light of concerns about intellectual property and offensive content\nin web-scale pretraining data.", "published": "2022-09-28 19:28:43", "link": "http://arxiv.org/abs/2209.14389v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ArNLI: Arabic Natural Language Inference for Entailment and\n  Contradiction Detection", "abstract": "Natural Language Inference (NLI) is a hot topic research in natural language\nprocessing, contradiction detection between sentences is a special case of NLI.\nThis is considered a difficult NLP task which has a big influence when added as\na component in many NLP applications, such as Question Answering Systems, text\nSummarization. Arabic Language is one of the most challenging low-resources\nlanguages in detecting contradictions due to its rich lexical, semantics\nambiguity. We have created a data set of more than 12k sentences and named\nArNLI, that will be publicly available. Moreover, we have applied a new model\ninspired by Stanford contradiction detection proposed solutions on English\nlanguage. We proposed an approach to detect contradictions between pairs of\nsentences in Arabic language using contradiction vector combined with language\nmodel vector as an input to machine learning model. We analyzed results of\ndifferent traditional machine learning classifiers and compared their results\non our created data set (ArNLI) and on an automatic translation of both PHEME,\nSICK English data sets. Best results achieved using Random Forest classifier\nwith an accuracy of 99%, 60%, 75% on PHEME, SICK and ArNLI respectively.", "published": "2022-09-28 09:37:16", "link": "http://arxiv.org/abs/2209.13953v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Monitoring ROS2: from Requirements to Autonomous Robots", "abstract": "Runtime verification (RV) has the potential to enable the safe operation of\nsafety-critical systems that are too complex to formally verify, such as Robot\nOperating System 2 (ROS2) applications. Writing correct monitors can itself be\ncomplex, and errors in the monitoring subsystem threaten the mission as a\nwhole. This paper provides an overview of a formal approach to generating\nruntime monitors for autonomous robots from requirements written in a\nstructured natural language. Our approach integrates the Formal Requirement\nElicitation Tool (FRET) with Copilot, a runtime verification framework, through\nthe Ogma integration tool. FRET is used to specify requirements with\nunambiguous semantics, which are then automatically translated into temporal\nlogic formulae. Ogma generates monitor specifications from the FRET output,\nwhich are compiled into hard-real time C99. To facilitate integration of the\nmonitors in ROS2, we have extended Ogma to generate ROS2 packages defining\nmonitoring nodes, which run the monitors when new data becomes available, and\npublish the results of any violations. The goal of our approach is to treat the\ngenerated ROS2 packages as black boxes and integrate them into larger ROS2\nsystems with minimal effort.", "published": "2022-09-28 12:19:13", "link": "http://arxiv.org/abs/2209.14030v1", "categories": ["cs.RO", "cs.CL", "cs.FL", "D.2.1; D.2.4; I.2.9;"], "primary_category": "cs.RO"}
{"title": "TVLT: Textless Vision-Language Transformer", "abstract": "In this work, we present the Textless Vision-Language Transformer (TVLT),\nwhere homogeneous transformer blocks take raw visual and audio inputs for\nvision-and-language representation learning with minimal modality-specific\ndesign, and do not use text-specific modules such as tokenization or automatic\nspeech recognition (ASR). TVLT is trained by reconstructing masked patches of\ncontinuous video frames and audio spectrograms (masked autoencoding) and\ncontrastive modeling to align video and audio. TVLT attains performance\ncomparable to its text-based counterpart on various multimodal tasks, such as\nvisual question answering, image retrieval, video retrieval, and multimodal\nsentiment analysis, with 28x faster inference speed and only 1/3 of the\nparameters. Our findings suggest the possibility of learning compact and\nefficient visual-linguistic representations from low-level visual and audio\nsignals without assuming the prior existence of text. Our code and checkpoints\nare available at: https://github.com/zinengtang/TVLT", "published": "2022-09-28 15:08:03", "link": "http://arxiv.org/abs/2209.14156v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Multilingual Search with Subword TF-IDF", "abstract": "Multilingual search can be achieved with subword tokenization. The accuracy\nof traditional TF-IDF approaches depend on manually curated tokenization, stop\nwords and stemming rules, whereas subword TF-IDF (STF-IDF) can offer higher\naccuracy without such heuristics. Moreover, multilingual support can be\nincorporated inherently as part of the subword tokenization model training.\nXQuAD evaluation demonstrates the advantages of STF-IDF: superior information\nretrieval accuracy of 85.4% for English and over 80% for 10 other languages\nwithout any heuristics-based preprocessing. The software to reproduce these\nresults are open-sourced as a part of Text2Text:\nhttps://github.com/artitw/text2text", "published": "2022-09-28 17:49:37", "link": "http://arxiv.org/abs/2209.14281v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cross-Domain Neural Entity Linking", "abstract": "Entity Linking is the task of matching a mention to an entity in a given\nknowledge base (KB). It contributes to annotating a massive amount of documents\nexisting on the Web to harness new facts about their matched entities. However,\nexisting Entity Linking systems focus on developing models that are typically\ndomain-dependent and robust only to a particular knowledge base on which they\nhave been trained. The performance is not as adequate when being evaluated on\ndocuments and knowledge bases from different domains.\n  Approaches based on pre-trained language models, such as Wu et al. (2020),\nattempt to solve the problem using a zero-shot setup, illustrating some\npotential when evaluated on a general-domain KB. Nevertheless, the performance\nis not equivalent when evaluated on a domain-specific KB. To allow for more\naccurate Entity Linking across different domains, we propose our framework:\nCross-Domain Neural Entity Linking (CDNEL). Our objective is to have a single\nsystem that enables simultaneous linking to both the general-domain KB and the\ndomain-specific KB. CDNEL works by learning a joint representation space for\nthese knowledge bases from different domains. It is evaluated using the\nexternal Entity Linking dataset (Zeshel) constructed by Logeswaran et al.\n(2019) and the Reddit dataset collected by Botzer et al. (2021), to compare our\nproposed method with the state-of-the-art results. The proposed framework uses\ndifferent types of datasets for fine-tuning, resulting in different model\nvariants of CDNEL. When evaluated on four domains included in the Zeshel\ndataset, these variants achieve an average precision gain of 9%.", "published": "2022-09-28 15:22:31", "link": "http://arxiv.org/abs/2210.15616v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Automatic Analysis of Available Source Code of Top Artificial\n  Intelligence Conference Papers", "abstract": "Source code is essential for researchers to reproduce the methods and\nreplicate the results of artificial intelligence (AI) papers. Some\norganizations and researchers manually collect AI papers with available source\ncode to contribute to the AI community. However, manual collection is a\nlabor-intensive and time-consuming task. To address this issue, we propose a\nmethod to automatically identify papers with available source code and extract\ntheir source code repository URLs. With this method, we find that 20.5% of\nregular papers of 10 top AI conferences published from 2010 to 2019 are\nidentified as papers with available source code and that 8.1% of these source\ncode repositories are no longer accessible. We also create the XMU NLP Lab\nREADME Dataset, the largest dataset of labeled README files for source code\ndocument research. Through this dataset, we have discovered that quite a few\nREADME files have no installation instructions or usage tutorials provided.\nFurther, a large-scale comprehensive statistical analysis is made for a general\npicture of the source code of AI conference papers. The proposed solution can\nalso go beyond AI conference papers to analyze other scientific papers from\nboth journals and conferences to shed light on more domains.", "published": "2022-09-28 15:05:58", "link": "http://arxiv.org/abs/2209.14155v1", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.DL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Towards Multimodal Prediction of Spontaneous Humour: A Novel Dataset and\n  First Results", "abstract": "Humor is a substantial element of human social behavior, affect, and\ncognition. Its automatic understanding can facilitate a more naturalistic\nhuman-AI interaction. Current methods of humor detection have been exclusively\nbased on staged data, making them inadequate for \"real-world\" applications. We\ncontribute to addressing this deficiency by introducing the novel\nPassau-Spontaneous Football Coach Humor (Passau-SFCH) dataset, comprising about\n11 hours of recordings. The Passau-SFCH dataset is annotated for the presence\nof humor and its dimensions (sentiment and direction) as proposed in Martin's\nHumor Style Questionnaire. We conduct a series of experiments employing\npretrained Transformers, convolutional neural networks, and expert-designed\nfeatures. The performance of each modality (text, audio, video) for spontaneous\nhumor recognition is analyzed and their complementarity is investigated. Our\nfindings suggest that for the automatic analysis of humor and its sentiment,\nfacial expressions are most promising, while humor direction can be best\nmodeled via text-based features. Further, we experiment with different\nmultimodal approaches to humor recognition, including decision-level fusion and\nMulT, a multimodal Transformer approach. In this context, we propose a novel\nmultimodal architecture that yields the best overall results. Finally, we make\nour code publicly available at https://www.github.com/lc0197/passau-sfch. The\nPassau-SFCH dataset is available upon request.", "published": "2022-09-28 17:36:47", "link": "http://arxiv.org/abs/2209.14272v3", "categories": ["cs.LG", "cs.CL", "cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "MeWEHV: Mel and Wave Embeddings for Human Voice Tasks", "abstract": "A recent trend in speech processing is the use of embeddings created through\nmachine learning models trained on a specific task with large datasets. By\nleveraging the knowledge already acquired, these models can be reused in new\ntasks where the amount of available data is small. This paper proposes a\npipeline to create a new model, called Mel and Wave Embeddings for Human Voice\nTasks (MeWEHV), capable of generating robust embeddings for speech processing.\nMeWEHV combines the embeddings generated by a pre-trained raw audio waveform\nencoder model, and deep features extracted from Mel Frequency Cepstral\nCoefficients (MFCCs) using Convolutional Neural Networks (CNNs). We evaluate\nthe performance of MeWEHV on three tasks: speaker, language, and accent\nidentification. For the first one, we use the VoxCeleb1 dataset and present\nYouSpeakers204, a new and publicly available dataset for English speaker\nidentification that contains 19607 audio clips from 204 persons speaking in six\ndifferent accents, allowing other researchers to work with a very balanced\ndataset, and to create new models that are robust to multiple accents. For\nevaluating the language identification task, we use the VoxForge and Common\nLanguage datasets. Finally, for accent identification, we use the Latin\nAmerican Spanish Corpora (LASC) and Common Voice datasets. Our approach allows\na significant increase in the performance of state-of-the-art models on all the\ntested datasets, with a low additional computational cost.", "published": "2022-09-28 13:20:16", "link": "http://arxiv.org/abs/2209.14078v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speech Enhancement Using Self-Supervised Pre-Trained Model and Vector\n  Quantization", "abstract": "With the development of deep learning, neural network-based speech\nenhancement (SE) models have shown excellent performance. Meanwhile, it was\nshown that the development of self-supervised pre-trained models can be applied\nto various downstream tasks. In this paper, we will consider the application of\nthe pre-trained model to the real-time SE problem. Specifically, the encoder\nand bottleneck layer of the DEMUCS model are initialized using the\nself-supervised pretrained WavLM model, the convolution in the encoder is\nreplaced by causal convolution, and the transformer encoder in the bottleneck\nlayer is based on causal attention mask. In addition, as discretizing the noisy\nspeech representations is more beneficial for denoising, we utilize a\nquantization module to discretize the representation output from the bottleneck\nlayer, which is then fed into the decoder to reconstruct the clean speech\nwaveform. Experimental results on the Valentini dataset and an internal dataset\nshow that the pre-trained model based initialization can improve the SE\nperformance and the discretization operation suppresses the noise component in\nthe representations to some extent, which can further improve the performance.", "published": "2022-09-28 15:00:39", "link": "http://arxiv.org/abs/2209.14150v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Audio Retrieval with WavText5K and CLAP Training", "abstract": "Audio-Text retrieval takes a natural language query to retrieve relevant\naudio files in a database. Conversely, Text-Audio retrieval takes an audio file\nas a query to retrieve relevant natural language descriptions. Most of the\nliterature train retrieval systems with one audio captioning dataset, but\nevaluating the benefit of training with multiple datasets is underexplored.\nMoreover, retrieval systems have to learn the alignment between elaborated\nsentences describing audio content of variable length ranging from a few\nseconds to several minutes. In this work, we propose a new collection of web\naudio-text pairs and a new framework for retrieval. First, we provide a new\ncollection of about five thousand web audio-text pairs that we refer to as\nWavText5K. When used to train our retrieval system, WavText5K improved\nperformance more than other audio captioning datasets. Second, our framework\nlearns to connect language and audio content by using a text encoder, two audio\nencoders, and a contrastive learning objective. Combining both audio encoders\nhelps to process variable length audio. The two contributions beat state of the\nart performance for AudioCaps and Clotho on Text-Audio retrieval by a relative\n2% and 16%, and Audio-Text retrieval by 6% and 23%.", "published": "2022-09-28 17:39:26", "link": "http://arxiv.org/abs/2209.14275v1", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "An Efficient Multitask Learning Architecture for Affective Vocal Burst\n  Analysis", "abstract": "Affective speech analysis is an ongoing topic of research. A relatively new\nproblem in this field is the analysis of vocal bursts, which are nonverbal\nvocalisations such as laughs or sighs. Current state-of-the-art approaches to\naddress affective vocal burst analysis are mostly based on wav2vec2 or HuBERT\nfeatures. In this paper, we investigate the use of the wav2vec successor\ndata2vec in combination with a multitask learning pipeline to tackle different\nanalysis problems at once. To assess the performance of our efficient multitask\nlearning architecture, we participate in the 2022 ACII Affective Vocal Burst\nChallenge, showing that our approach substantially outperforms the baseline\nestablished there in three different subtasks.", "published": "2022-09-28 08:32:08", "link": "http://arxiv.org/abs/2209.13914v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Entangling Practice with Artistic and Educational Aims: Interviews on\n  Technology-based Movement Sound Interactions", "abstract": "Movement-sound interactive systems are at the interface of different artistic\nand educational practices. Within this multiplicity of uses, we examine common\ndenominators in terms of learning, appropriation and relationship to\ntechnological systems. While these topics have been previously reported at\nNIME, we wanted to investigate how practitioners, coming from different\nperspectives, relate to these questions. We conducted interviews with 6 artists\nwho are engaged in movementsound interactions: 1 performer, 1\nperformer/composer, 1 composer, 1 teacher/composer, 1 dancer/teacher, 1 dancer.\nThrough a thematic analysis of the transcripts we identified three main themes\nrelated to (1) the mediating role of technological tools (2) usability and\nnormativity, and (3) learning and practice. These results provide ground for\ndiscussion about the design and study of movement-sound interactive systems.", "published": "2022-09-28 08:52:51", "link": "http://arxiv.org/abs/2209.13921v1", "categories": ["cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
{"title": "Deepfake audio detection by speaker verification", "abstract": "Thanks to recent advances in deep learning, sophisticated generation tools\nexist, nowadays, that produce extremely realistic synthetic speech. However,\nmalicious uses of such tools are possible and likely, posing a serious threat\nto our society. Hence, synthetic voice detection has become a pressing research\ntopic, and a large variety of detection methods have been recently proposed.\nUnfortunately, they hardly generalize to synthetic audios generated by tools\nnever seen in the training phase, which makes them unfit to face real-world\nscenarios. In this work, we aim at overcoming this issue by proposing a new\ndetection approach that leverages only the biometric characteristics of the\nspeaker, with no reference to specific manipulations. Since the detector is\ntrained only on real data, generalization is automatically ensured. The\nproposed approach can be implemented based on off-the-shelf speaker\nverification tools. We test several such solutions on three popular test sets,\nobtaining good performance, high generalization ability, and high robustness to\naudio impairment.", "published": "2022-09-28 13:46:29", "link": "http://arxiv.org/abs/2209.14098v1", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio Barlow Twins: Self-Supervised Audio Representation Learning", "abstract": "The Barlow Twins self-supervised learning objective requires neither negative\nsamples or asymmetric learning updates, achieving results on a par with the\ncurrent state-of-the-art within Computer Vision. As such, we present Audio\nBarlow Twins, a novel self-supervised audio representation learning approach,\nadapting Barlow Twins to the audio domain. We pre-train on the large-scale\naudio dataset AudioSet, and evaluate the quality of the learnt representations\non 18 tasks from the HEAR 2021 Challenge, achieving results which outperform,\nor otherwise are on a par with, the current state-of-the-art for instance\ndiscrimination self-supervised learning approaches to audio representation\nlearning. Code at https://github.com/jonahanton/SSL_audio.", "published": "2022-09-28 18:17:11", "link": "http://arxiv.org/abs/2209.14345v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The Chamber Ensemble Generator: Limitless High-Quality MIR Data via\n  Generative Modeling", "abstract": "Data is the lifeblood of modern machine learning systems, including for those\nin Music Information Retrieval (MIR). However, MIR has long been mired by small\ndatasets and unreliable labels. In this work, we propose to break this\nbottleneck using generative modeling. By pipelining a generative model of notes\n(Coconet trained on Bach Chorales) with a structured synthesis model of chamber\nensembles (MIDI-DDSP trained on URMP), we demonstrate a system capable of\nproducing unlimited amounts of realistic chorale music with rich annotations\nincluding mixes, stems, MIDI, note-level performance attributes (staccato,\nvibrato, etc.), and even fine-grained synthesis parameters (pitch, amplitude,\netc.). We call this system the Chamber Ensemble Generator (CEG), and use it to\ngenerate a large dataset of chorales from four different chamber ensembles\n(CocoChorales). We demonstrate that data generated using our approach improves\nstate-of-the-art models for music transcription and source separation, and we\nrelease both the system and the dataset as an open-source foundation for future\nwork in the MIR community.", "published": "2022-09-28 22:55:15", "link": "http://arxiv.org/abs/2209.14458v1", "categories": ["cs.SD", "cs.IR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
