{"title": "Point or Generate Dialogue State Tracker", "abstract": "Dialogue state tracking is a key part of a task-oriented dialogue system,\nwhich estimates the user's goal at each turn of the dialogue. In this paper, we\npropose the Point-Or-Generate Dialogue State Tracker (POGD). POGD solves the\ndialogue state tracking task in two perspectives: 1) point out explicitly\nexpressed slot values from the user's utterance, and 2) generate implicitly\nexpressed ones based on slot-specific contexts. It also shares parameters\nacross all slots, which achieves knowledge sharing and gains scalability to\nlarge-scale across-domain dialogues. Moreover, the training process of its\nsubmodules is formulated as a multi-task learning procedure to further promote\nits capability of generalization. Experiments show that POGD not only obtains\nstate-of-the-art results on both WoZ 2.0 and MultiWoZ 2.0 datasets but also has\ngood generalization on unseen values and new slots.", "published": "2020-08-08 02:15:25", "link": "http://arxiv.org/abs/2008.03417v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Word Error Rate Estimation Without ASR Output: e-WER2", "abstract": "Measuring the performance of automatic speech recognition (ASR) systems\nrequires manually transcribed data in order to compute the word error rate\n(WER), which is often time-consuming and expensive. In this paper, we continue\nour effort in estimating WER using acoustic, lexical and phonotactic features.\nOur novel approach to estimate the WER uses a multistream end-to-end\narchitecture. We report results for systems using internal speech decoder\nfeatures (glass-box), systems without speech decoder features (black-box), and\nfor systems without having access to the ASR system (no-box). The no-box system\nlearns joint acoustic-lexical representation from phoneme recognition results\nalong with MFCC acoustic features to estimate WER. Considering WER per\nsentence, our no-box system achieves 0.56 Pearson correlation with the\nreference evaluation and 0.24 root mean square error (RMSE) across 1,400\nsentences. The estimated overall WER by e-WER2 is 30.9% for a three hours test\nset, while the WER computed using the reference transcriptions was 28.5%.", "published": "2020-08-08 00:19:09", "link": "http://arxiv.org/abs/2008.03403v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Learning to Detect Bipolar Disorder and Borderline Personality Disorder\n  with Language and Speech in Non-Clinical Interviews", "abstract": "Bipolar disorder (BD) and borderline personality disorder (BPD) are both\nchronic psychiatric disorders. However, their overlapping symptoms and common\ncomorbidity make it challenging for the clinicians to distinguish the two\nconditions on the basis of a clinical interview. In this work, we first present\na new multi-modal dataset containing interviews involving individuals with BD\nor BPD being interviewed about a non-clinical topic . We investigate the\nautomatic detection of the two conditions, and demonstrate a good linear\nclassifier that can be learnt using a down-selected set of features from the\ndifferent aspects of the interviews and a novel approach of summarising these\nfeatures. Finally, we find that different sets of features characterise BD and\nBPD, thus providing insights into the difference between the automatic\nscreening of the two conditions.", "published": "2020-08-08 00:48:59", "link": "http://arxiv.org/abs/2008.03408v2", "categories": ["cs.LG", "cs.CL", "eess.AS", "stat.ML", "60L10"], "primary_category": "cs.LG"}
{"title": "Assessing Demographic Bias in Named Entity Recognition", "abstract": "Named Entity Recognition (NER) is often the first step towards automated\nKnowledge Base (KB) generation from raw text. In this work, we assess the bias\nin various Named Entity Recognition (NER) systems for English across different\ndemographic groups with synthetically generated corpora. Our analysis reveals\nthat models perform better at identifying names from specific demographic\ngroups across two datasets. We also identify that debiased embeddings do not\nhelp in resolving this issue. Finally, we observe that character-based\ncontextualized word representation models such as ELMo results in the least\nbias across demographics. Our work can shed light on potential biases in\nautomated KB generation due to systematic exclusion of named entities belonging\nto certain demographics.", "published": "2020-08-08 02:01:25", "link": "http://arxiv.org/abs/2008.03415v1", "categories": ["cs.CL", "cs.CY", "cs.IR", "cs.LG", "68T50 (Primary), 68T30 (Secondary), 68U15", "I.2.7; I.2.1; I.2.6; H.3.1; H.3.3; H.1.2; K.4.2"], "primary_category": "cs.CL"}
{"title": "Deep F-measure Maximization for End-to-End Speech Understanding", "abstract": "Spoken language understanding (SLU) datasets, like many other machine\nlearning datasets, usually suffer from the label imbalance problem. Label\nimbalance usually causes the learned model to replicate similar biases at the\noutput which raises the issue of unfairness to the minority classes in the\ndataset. In this work, we approach the fairness problem by maximizing the\nF-measure instead of accuracy in neural network model training. We propose a\ndifferentiable approximation to the F-measure and train the network with this\nobjective using standard backpropagation. We perform experiments on two\nstandard fairness datasets, Adult, and Communities and Crime, and also on\nspeech-to-intent detection on the ATIS dataset and speech-to-image concept\nclassification on the Speech-COCO dataset. In all four of these tasks,\nF-measure maximization results in improved micro-F1 scores, with absolute\nimprovements of up to 8% absolute, as compared to models trained with the\ncross-entropy loss function. In the two multi-class SLU tasks, the proposed\napproach significantly improves class coverage, i.e., the number of classes\nwith positive recall.", "published": "2020-08-08 03:02:27", "link": "http://arxiv.org/abs/2008.03425v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Context Dependent RNNLM for Automatic Transcription of Conversations", "abstract": "Conversational speech, while being unstructured at an utterance level,\ntypically has a macro topic which provides larger context spanning multiple\nutterances. The current language models in speech recognition systems using\nrecurrent neural networks (RNNLM) rely mainly on the local context and exclude\nthe larger context. In order to model the long term dependencies of words\nacross multiple sentences, we propose a novel architecture where the words from\nprior utterances are converted to an embedding. The relevance of these\nembeddings for the prediction of next word in the current sentence is found\nusing a gating network. The relevance weighted context embedding vector is\ncombined in the language model to improve the next word prediction, and the\nentire model including the context embedding and the relevance weighting layers\nis jointly learned for a conversational language modeling task. Experiments are\nperformed on two conversational datasets - AMI corpus and the Switchboard\ncorpus. In these tasks, we illustrate that the proposed approach yields\nsignificant improvements in language model perplexity over the RNNLM baseline.\nIn addition, the use of proposed conversational LM for ASR rescoring results in\nabsolute WER reduction of $1.2$\\% on Switchboard dataset and $1.0$\\% on AMI\ndataset over the RNNLM based ASR baseline.", "published": "2020-08-08 13:14:45", "link": "http://arxiv.org/abs/2008.03517v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Stacked 1D convolutional networks for end-to-end small footprint voice\n  trigger detection", "abstract": "We propose a stacked 1D convolutional neural network (S1DCNN) for end-to-end\nsmall footprint voice trigger detection in a streaming scenario. Voice trigger\ndetection is an important speech application, with which users can activate\ntheir devices by simply saying a keyword or phrase. Due to privacy and latency\nreasons, a voice trigger detection system should run on an always-on processor\non device. Therefore, having small memory and compute cost is crucial for a\nvoice trigger detection system. Recently, singular value decomposition filters\n(SVDFs) has been used for end-to-end voice trigger detection. The SVDFs\napproximate a fully-connected layer with a low rank approximation, which\nreduces the number of model parameters. In this work, we propose S1DCNN as an\nalternative approach for end-to-end small-footprint voice trigger detection. An\nS1DCNN layer consists of a 1D convolution layer followed by a depth-wise 1D\nconvolution layer. We show that the SVDF can be expressed as a special case of\nthe S1DCNN layer. Experimental results show that the S1DCNN achieve 19.0%\nrelative false reject ratio (FRR) reduction with a similar model size and a\nsimilar time delay compared to the SVDF. By using longer time delays, the\nS1DCNN further improve the FRR up to 12.2% relative.", "published": "2020-08-08 00:32:55", "link": "http://arxiv.org/abs/2008.03405v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Symbolic Music Playing Techniques Generation as a Tagging Problem", "abstract": "Music generation has always been a hot topic. When discussing symbolic music,\nmelody or harmonies are usually seen as the only generating targets. But in\nfact, playing techniques are also quite an important part of the music. In this\npaper, we discuss the playing techniques generation problem by seeing it as a\ntagging problem. We propose a model that can use both the current data and\nexternal knowledge. Experiments were carried out by applying the proposed model\nin Chinese bamboo flute music, and results show that our method can make\ngenerated music more lively.", "published": "2020-08-08 03:44:28", "link": "http://arxiv.org/abs/2008.03436v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio Spoofing Verification using Deep Convolutional Neural Networks by\n  Transfer Learning", "abstract": "Automatic Speaker Verification systems are gaining popularity these days;\nspoofing attacks are of prime concern as they make these systems vulnerable.\nSome spoofing attacks like Replay attacks are easier to implement but are very\nhard to detect thus creating the need for suitable countermeasures. In this\npaper, we propose a speech classifier based on deep-convolutional neural\nnetwork to detect spoofing attacks. Our proposed methodology uses acoustic\ntime-frequency representation of power spectral densities on Mel frequency\nscale (Mel-spectrogram), via deep residual learning (an adaptation of ResNet-34\narchitecture). Using a single model system, we have achieved an equal error\nrate (EER) of 0.9056% on the development and 5.32% on the evaluation dataset of\nlogical access scenario and an equal error rate (EER) of 5.87% on the\ndevelopment and 5.74% on the evaluation dataset of physical access scenario of\nASVspoof 2019.", "published": "2020-08-08 07:14:40", "link": "http://arxiv.org/abs/2008.03464v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "NPU Speaker Verification System for INTERSPEECH 2020 Far-Field Speaker\n  Verification Challenge", "abstract": "This paper describes the NPU system submitted to Interspeech 2020 Far-Field\nSpeaker Verification Challenge (FFSVC). We particularly focus on far-field\ntext-dependent SV from single (task1) and multiple microphone arrays (task3).\nThe major challenges in such scenarios are short utterance and cross-channel\nand distance mismatch for enrollment and test. With the belief that better\nspeaker embedding can alleviate the effects from short utterance, we introduce\na new speaker embedding architecture - ResNet-BAM, which integrates a\nbottleneck attention module with ResNet as a simple and efficient way to\nfurther improve the representation power of ResNet. This contribution brings up\nto 1% EER reduction. We further address the mismatch problem in three\ndirections. First, domain adversarial training, which aims to learn\ndomain-invariant features, can yield to 0.8% EER reduction. Second, front-end\nsignal processing, including WPE and beamforming, has no obvious contribution,\nbut together with data selection and domain adversarial training, can further\ncontribute to 0.5% EER reduction. Finally, data augmentation, which works with\na specifically-designed data selection strategy, can lead to 2% EER reduction.\nTogether with the above contributions, in the middle challenge results, our\nsingle submission system (without multi-system fusion) achieves the first and\nsecond place on task 1 and task 3, respectively.", "published": "2020-08-08 13:32:48", "link": "http://arxiv.org/abs/2008.03521v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Exploring the Use of an Unsupervised Autoregressive Model as a Shared\n  Encoder for Text-Dependent Speaker Verification", "abstract": "In this paper, we propose a novel way of addressing text-dependent automatic\nspeaker verification (TD-ASV) by using a shared-encoder with task-specific\ndecoders. An autoregressive predictive coding (APC) encoder is pre-trained in\nan unsupervised manner using both out-of-domain (LibriSpeech, VoxCeleb) and\nin-domain (DeepMine) unlabeled datasets to learn generic, high-level feature\nrepresentation that encapsulates speaker and phonetic content. Two\ntask-specific decoders were trained using labeled datasets to classify speakers\n(SID) and phrases (PID). Speaker embeddings extracted from the SID decoder were\nscored using a PLDA. SID and PID systems were fused at the score level. There\nis a 51.9% relative improvement in minDCF for our system compared to the fully\nsupervised x-vector baseline on the cross-lingual DeepMine dataset. However,\nthe i-vector/HMM method outperformed the proposed APC encoder-decoder system. A\nfusion of the x-vector/PLDA baseline and the SID/PLDA scores prior to PID\nfusion further improved performance by 15% indicating complementarity of the\nproposed approach to the x-vector system. We show that the proposed approach\ncan leverage from large, unlabeled, data-rich domains, and learn speech\npatterns independent of downstream tasks. Such a system can provide competitive\nperformance in domain-mismatched scenarios where test data is from data-scarce\ndomains.", "published": "2020-08-08 22:47:10", "link": "http://arxiv.org/abs/2008.03615v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "JukeBox: A Multilingual Singer Recognition Dataset", "abstract": "A text-independent speaker recognition system relies on successfully encoding\nspeech factors such as vocal pitch, intensity, and timbre to achieve good\nperformance. A majority of such systems are trained and evaluated using spoken\nvoice or everyday conversational voice data. Spoken voice, however, exhibits a\nlimited range of possible speaker dynamics, thus constraining the utility of\nthe derived speaker recognition models. Singing voice, on the other hand,\ncovers a broader range of vocal and ambient factors and can, therefore, be used\nto evaluate the robustness of a speaker recognition system. However, a majority\nof existing speaker recognition datasets only focus on the spoken voice. In\ncomparison, there is a significant shortage of labeled singing voice data\nsuitable for speaker recognition research. To address this issue, we assemble\n\\textit{JukeBox} - a speaker recognition dataset with multilingual singing\nvoice audio annotated with singer identity, gender, and language labels. We use\nthe current state-of-the-art methods to demonstrate the difficulty of\nperforming speaker recognition on singing voice using models trained on spoken\nvoice alone. We also evaluate the effect of gender and language on speaker\nrecognition performance, both in spoken and singing voice data. The complete\n\\textit{JukeBox} dataset can be accessed at\nhttp://iprobe.cse.msu.edu/datasets/jukebox.html.", "published": "2020-08-08 12:22:51", "link": "http://arxiv.org/abs/2008.03507v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Novel Method for Obtaining Diffuse Field Measurements for Microphone\n  Calibration", "abstract": "We propose a straightforward and cost-effective method to perform diffuse\nsoundfield measurements for calibrating the magnitude response of a microphone\narray. Typically, such calibration is performed in a diffuse soundfield created\nin reverberation chambers, an expensive and time-consuming process. A method is\nproposed for obtaining diffuse field measurements in untreated environments.\nFirst, a closed-form expression for the spatial correlation of a wideband\nsignal in a diffuse field is derived. Next, we describe a practical procedure\nfor obtaining the diffuse field response of a microphone array in the presence\nof a non-diffuse soundfield by the introduction of random perturbations in the\nmicrophone location. Experimental spatial correlation data obtained is compared\nwith the theoretical model, confirming that it is possible to obtain diffuse\nfield measurements in untreated environments with relatively few loudspeakers.\nA 30 second test signal played from 4-8 loudspeakers is shown to be sufficient\nin obtaining a diffuse field measurement using the proposed method. An\nEigenmike is then successfully calibrated at two different geographical\nlocations.", "published": "2020-08-08 12:48:47", "link": "http://arxiv.org/abs/2008.03513v1", "categories": ["cs.SD", "cs.IT", "eess.AS", "math.IT"], "primary_category": "cs.SD"}
{"title": "Extrapolating false alarm rates in automatic speaker verification", "abstract": "Automatic speaker verification (ASV) vendors and corpus providers would both\nbenefit from tools to reliably extrapolate performance metrics for large\nspeaker populations without collecting new speakers. We address false alarm\nrate extrapolation under a worst-case model whereby an adversary identifies the\nclosest impostor for a given target speaker from a large population. Our models\nare generative and allow sampling new speakers. The models are formulated in\nthe ASV detection score space to facilitate analysis of arbitrary ASV systems.", "published": "2020-08-08 20:31:57", "link": "http://arxiv.org/abs/2008.03590v1", "categories": ["eess.AS", "cs.LG", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Speech Driven Talking Face Generation from a Single Image and an Emotion\n  Condition", "abstract": "Visual emotion expression plays an important role in audiovisual speech\ncommunication. In this work, we propose a novel approach to rendering visual\nemotion expression in speech-driven talking face generation. Specifically, we\ndesign an end-to-end talking face generation system that takes a speech\nutterance, a single face image, and a categorical emotion label as input to\nrender a talking face video synchronized with the speech and expressing the\nconditioned emotion. Objective evaluation on image quality, audiovisual\nsynchronization, and visual emotion expression shows that the proposed system\noutperforms a state-of-the-art baseline system. Subjective evaluation of visual\nemotion expression and video realness also demonstrates the superiority of the\nproposed system. Furthermore, we conduct a human emotion recognition pilot\nstudy using generated videos with mismatched emotions among the audio and\nvisual modalities. Results show that humans respond to the visual modality more\nsignificantly than the audio modality on this task.", "published": "2020-08-08 20:46:31", "link": "http://arxiv.org/abs/2008.03592v2", "categories": ["eess.AS", "cs.CV", "cs.LG", "cs.MM"], "primary_category": "eess.AS"}
{"title": "Variable frame rate-based data augmentation to handle speaking-style\n  variability for automatic speaker verification", "abstract": "The effects of speaking-style variability on automatic speaker verification\nwere investigated using the UCLA Speaker Variability database which comprises\nmultiple speaking styles per speaker. An x-vector/PLDA (probabilistic linear\ndiscriminant analysis) system was trained with the SRE and Switchboard\ndatabases with standard augmentation techniques and evaluated with utterances\nfrom the UCLA database. The equal error rate (EER) was low when enrollment and\ntest utterances were of the same style (e.g., 0.98% and 0.57% for read and\nconversational speech, respectively), but it increased substantially when\nstyles were mismatched between enrollment and test utterances. For instance,\nwhen enrolled with conversation utterances, the EER increased to 3.03%, 2.96%\nand 22.12% when tested on read, narrative, and pet-directed speech,\nrespectively. To reduce the effect of style mismatch, we propose an\nentropy-based variable frame rate technique to artificially generate\nstyle-normalized representations for PLDA adaptation. The proposed system\nsignificantly improved performance. In the aforementioned conditions, the EERs\nimproved to 2.69% (conversation -- read), 2.27% (conversation -- narrative),\nand 18.75% (pet-directed -- read). Overall, the proposed technique performed\ncomparably to multi-style PLDA adaptation without the need for training data in\ndifferent speaking styles per speaker.", "published": "2020-08-08 22:47:12", "link": "http://arxiv.org/abs/2008.03616v1", "categories": ["eess.AS", "cs.LG", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Speaker discrimination in humans and machines: Effects of speaking style\n  variability", "abstract": "Does speaking style variation affect humans' ability to distinguish\nindividuals from their voices? How do humans compare with automatic systems\ndesigned to discriminate between voices? In this paper, we attempt to answer\nthese questions by comparing human and machine speaker discrimination\nperformance for read speech versus casual conversations. Thirty listeners were\nasked to perform a same versus different speaker task. Their performance was\ncompared to a state-of-the-art x-vector/PLDA-based automatic speaker\nverification system. Results showed that both humans and machines performed\nbetter with style-matched stimuli, and human performance was better when\nlisteners were native speakers of American English. Native listeners performed\nbetter than machines in the style-matched conditions (EERs of 6.96% versus\n14.35% for read speech, and 15.12% versus 19.87%, for conversations), but for\nstyle-mismatched conditions, there was no significant difference between native\nlisteners and machines. In all conditions, fusing human responses with machine\nresults showed improvements compared to each alone, suggesting that humans and\nmachines have different approaches to speaker discrimination tasks. Differences\nin the approaches were further confirmed by examining results for individual\nspeakers which showed that the perception of distinct and confused speakers\ndiffered between human listeners and machines.", "published": "2020-08-08 22:59:46", "link": "http://arxiv.org/abs/2008.03617v1", "categories": ["eess.AS", "cs.LG", "eess.SP"], "primary_category": "eess.AS"}
