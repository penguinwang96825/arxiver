{"title": "SentiLSTM: A Deep Learning Approach for Sentiment Analysis of Restaurant\n  Reviews", "abstract": "The amount of textual data generation has increased enormously due to the\neffortless access of the Internet and the evolution of various web 2.0\napplications. These textual data productions resulted because of the people\nexpress their opinion, emotion or sentiment about any product or service in the\nform of tweets, Facebook post or status, blog write up, and reviews. Sentiment\nanalysis deals with the process of computationally identifying and categorizing\nopinions expressed in a piece of text, especially in order to determine whether\nthe writer's attitude toward a particular topic is positive, negative, or\nneutral. The impact of customer review is significant to perceive the customer\nattitude towards a restaurant. Thus, the automatic detection of sentiment from\nreviews is advantageous for the restaurant owners, or service providers and\ncustomers to make their decisions or services more satisfactory. This paper\nproposes, a deep learning-based technique (i.e., BiLSTM) to classify the\nreviews provided by the clients of the restaurant into positive and negative\npolarities. A corpus consists of 8435 reviews is constructed to evaluate the\nproposed technique. In addition, a comparative analysis of the proposed\ntechnique with other machine learning algorithms presented. The results of the\nevaluation on test dataset show that BiLSTM technique produced in the highest\naccuracy of 91.35%.", "published": "2020-11-19 06:24:42", "link": "http://arxiv.org/abs/2011.09684v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are Pre-trained Language Models Knowledgeable to Ground Open Domain\n  Dialogues?", "abstract": "We study knowledge-grounded dialogue generation with pre-trained language\nmodels. Instead of pursuing new state-of-the-art on benchmarks, we try to\nunderstand if the knowledge stored in parameters of the pre-trained models is\nalready enough to ground open domain dialogues, and thus allows us to get rid\nof the dependency on external knowledge sources in generation. Through\nextensive experiments on benchmarks, we find that by fine-tuning with a few\ndialogues containing knowledge, the pre-trained language models can outperform\nthe state-of-the-art model that requires external knowledge in automatic\nevaluation and human judgment, suggesting a positive answer to the question we\nraised.", "published": "2020-11-19 08:22:49", "link": "http://arxiv.org/abs/2011.09708v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fact-level Extractive Summarization with Hierarchical Graph Mask on BERT", "abstract": "Most current extractive summarization models generate summaries by selecting\nsalient sentences. However, one of the problems with sentence-level extractive\nsummarization is that there exists a gap between the human-written gold summary\nand the oracle sentence labels. In this paper, we propose to extract fact-level\nsemantic units for better extractive summarization. We also introduce a\nhierarchical structure, which incorporates the multi-level of granularities of\nthe textual information into the model. In addition, we incorporate our model\nwith BERT using a hierarchical graph mask. This allows us to combine BERT's\nability in natural language understanding and the structural information\nwithout increasing the scale of the model. Experiments on the CNN/DaliyMail\ndataset show that our model achieves state-of-the-art results.", "published": "2020-11-19 09:29:51", "link": "http://arxiv.org/abs/2011.09739v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Entity Recognition and Relation Extraction from Scientific and Technical\n  Texts in Russian", "abstract": "This paper is devoted to the study of methods for information extraction\n(entity recognition and relation classification) from scientific texts on\ninformation technology. Scientific publications provide valuable information\ninto cutting-edge scientific advances, but efficient processing of increasing\namounts of data is a time-consuming task. In this paper, several modifications\nof methods for the Russian language are proposed. It also includes the results\nof experiments comparing a keyword extraction method, vocabulary method, and\nsome methods based on neural networks. Text collections for these tasks exist\nfor the English language and are actively used by the scientific community, but\nat present, such datasets in Russian are not publicly available. In this paper,\nwe present a corpus of scientific texts in Russian, RuSERRC. This dataset\nconsists of 1600 unlabeled documents and 80 labeled with entities and semantic\nrelations (6 relation types were considered). The dataset and models are\navailable at https://github.com/iis-research-team. We hope they can be useful\nfor research purposes and development of information extraction systems.", "published": "2020-11-19 13:40:03", "link": "http://arxiv.org/abs/2011.09817v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do We Need Online NLU Tools?", "abstract": "The intent recognition is an essential algorithm of any conversational AI\napplication. It is responsible for the classification of an input message into\nmeaningful classes. In many bot development platforms, we can configure the NLU\npipeline. Several intent recognition services are currently available as an\nAPI, or we choose from many open-source alternatives. However, there is no\ncomparison of intent recognition services and open-source algorithms. Many\nfactors make the selection of the right approach to the intent recognition\nchallenging in practice. In this paper, we suggest criteria to choose the best\nintent recognition algorithm for an application. We present a dataset for\nevaluation. Finally, we compare selected public NLU services with selected\nopen-source algorithms for intent recognition.", "published": "2020-11-19 13:58:47", "link": "http://arxiv.org/abs/2011.09825v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Text Specific and Blackbox Fairness Algorithms in Multimodal\n  Clinical NLP", "abstract": "Clinical machine learning is increasingly multimodal, collected in both\nstructured tabular formats and unstructured forms such as freetext. We propose\na novel task of exploring fairness on a multimodal clinical dataset, adopting\nequalized odds for the downstream medical prediction tasks. To this end, we\ninvestigate a modality-agnostic fairness algorithm - equalized odds post\nprocessing - and compare it to a text-specific fairness algorithm: debiased\nclinical word embeddings. Despite the fact that debiased word embeddings do not\nexplicitly address equalized odds of protected groups, we show that a\ntext-specific approach to fairness may simultaneously achieve a good balance of\nperformance and classical notions of fairness. We hope that our paper inspires\nfuture contributions at the critical intersection of clinical NLP and fairness.\nThe full source code is available here:\nhttps://github.com/johntiger1/multimodal_fairness", "published": "2020-11-19 03:11:24", "link": "http://arxiv.org/abs/2011.09625v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Relation Extraction with Contextualized Relation Embedding (CRE)", "abstract": "Relation extraction is the task of identifying relation instance between two\nentities given a corpus whereas Knowledge base modeling is the task of\nrepresenting a knowledge base, in terms of relations between entities. This\npaper proposes an architecture for the relation extraction task that integrates\nsemantic information with knowledge base modeling in a novel manner. Existing\napproaches for relation extraction either do not utilize knowledge base\nmodelling or use separately trained KB models for the RE task. We present a\nmodel architecture that internalizes KB modeling in relation extraction. This\nmodel applies a novel approach to encode sentences into contextualized relation\nembeddings, which can then be used together with parameterized entity\nembeddings to score relation instances. The proposed CRE model achieves state\nof the art performance on datasets derived from The New York Times Annotated\nCorpus and FreeBase. The source code has been made available.", "published": "2020-11-19 05:19:46", "link": "http://arxiv.org/abs/2011.09658v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Integrated Approach for Improving Brand Consistency of Web Content:\n  Modeling, Analysis and Recommendation", "abstract": "A consumer-dependent (business-to-consumer) organization tends to present\nitself as possessing a set of human qualities, which is termed as the brand\npersonality of the company. The perception is impressed upon the consumer\nthrough the content, be it in the form of advertisement, blogs or magazines,\nproduced by the organization. A consistent brand will generate trust and retain\ncustomers over time as they develop an affinity towards regularity and common\npatterns. However, maintaining a consistent messaging tone for a brand has\nbecome more challenging with the virtual explosion in the amount of content\nwhich needs to be authored and pushed to the Internet to maintain an edge in\nthe era of digital marketing. To understand the depth of the problem, we\ncollect around 300K web page content from around 650 companies. We develop\ntrait-specific classification models by considering the linguistic features of\nthe content. The classifier automatically identifies the web articles which are\nnot consistent with the mission and vision of a company and further helps us to\ndiscover the conditions under which the consistency cannot be maintained. To\naddress the brand inconsistency issue, we then develop a sentence ranking\nsystem that outputs the top three sentences that need to be changed for making\na web article more consistent with the company's brand personality.", "published": "2020-11-19 10:18:47", "link": "http://arxiv.org/abs/2011.09754v3", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Persuasive Dialogue Understanding: the Baselines and Negative Results", "abstract": "Persuasion aims at forming one's opinion and action via a series of\npersuasive messages containing persuader's strategies. Due to its potential\napplication in persuasive dialogue systems, the task of persuasive strategy\nrecognition has gained much attention lately. Previous methods on user intent\nrecognition in dialogue systems adopt recurrent neural network (RNN) or\nconvolutional neural network (CNN) to model context in conversational history,\nneglecting the tactic history and intra-speaker relation. In this paper, we\ndemonstrate the limitations of a Transformer-based approach coupled with\nConditional Random Field (CRF) for the task of persuasive strategy recognition.\nIn this model, we leverage inter- and intra-speaker contextual semantic\nfeatures, as well as label dependencies to improve the recognition. Despite\nextensive hyper-parameter optimizations, this architecture fails to outperform\nthe baseline methods. We observe two negative results. Firstly, CRF cannot\ncapture persuasive label dependencies, possibly as strategies in persuasive\ndialogues do not follow any strict grammar or rules as the cases in Named\nEntity Recognition (NER) or part-of-speech (POS) tagging. Secondly, the\nTransformer encoder trained from scratch is less capable of capturing\nsequential information in persuasive dialogues than Long Short-Term Memory\n(LSTM). We attribute this to the reason that the vanilla Transformer encoder\ndoes not efficiently consider relative position information of sequence\nelements.", "published": "2020-11-19 16:52:43", "link": "http://arxiv.org/abs/2011.09954v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "VLG-Net: Video-Language Graph Matching Network for Video Grounding", "abstract": "Grounding language queries in videos aims at identifying the time interval\n(or moment) semantically relevant to a language query. The solution to this\nchallenging task demands understanding videos' and queries' semantic content\nand the fine-grained reasoning about their multi-modal interactions. Our key\nidea is to recast this challenge into an algorithmic graph matching problem.\nFueled by recent advances in Graph Neural Networks, we propose to leverage\nGraph Convolutional Networks to model video and textual information as well as\ntheir semantic alignment. To enable the mutual exchange of information across\nthe modalities, we design a novel Video-Language Graph Matching Network\n(VLG-Net) to match video and query graphs. Core ingredients include\nrepresentation graphs built atop video snippets and query tokens separately and\nused to model intra-modality relationships. A Graph Matching layer is adopted\nfor cross-modal context modeling and multi-modal fusion. Finally, moment\ncandidates are created using masked moment attention pooling by fusing the\nmoment's enriched snippet features. We demonstrate superior performance over\nstate-of-the-art grounding methods on three widely used datasets for temporal\nlocalization of moments in videos with language queries: ActivityNet-Captions,\nTACoS, and DiDeMo.", "published": "2020-11-19 22:32:03", "link": "http://arxiv.org/abs/2011.10132v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Predicting Early Indicators of Cognitive Decline from Verbal Utterances", "abstract": "Dementia is a group of irreversible, chronic, and progressive\nneurodegenerative disorders resulting in impaired memory, communication, and\nthought processes. In recent years, clinical research advances in brain aging\nhave focused on the earliest clinically detectable stage of incipient dementia,\ncommonly known as mild cognitive impairment (MCI). Currently, these disorders\nare diagnosed using a manual analysis of neuropsychological examinations. We\nmeasure the feasibility of using the linguistic characteristics of verbal\nutterances elicited during neuropsychological exams of elderly subjects to\ndistinguish between elderly control groups, people with MCI, people diagnosed\nwith possible Alzheimer's disease (AD), and probable AD. We investigated the\nperformance of both theory-driven psycholinguistic features and data-driven\ncontextual language embeddings in identifying different clinically diagnosed\ngroups. Our experiments show that a combination of contextual and\npsycholinguistic features extracted by a Support Vector Machine improved\ndistinguishing the verbal utterances of elderly controls, people with MCI,\npossible AD, and probable AD. This is the first work to identify four clinical\ndiagnosis groups of dementia in a highly imbalanced dataset. Our work shows\nthat machine learning algorithms built on contextual and psycholinguistic\nfeatures can learn the linguistic biomarkers from verbal utterances and assist\nclinical diagnosis of different stages and types of dementia, even with limited\ndata.", "published": "2020-11-19 02:24:11", "link": "http://arxiv.org/abs/2012.02029v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Universal MelGAN: A Robust Neural Vocoder for High-Fidelity Waveform\n  Generation in Multiple Domains", "abstract": "We propose Universal MelGAN, a vocoder that synthesizes high-fidelity speech\nin multiple domains. To preserve sound quality when the MelGAN-based structure\nis trained with a dataset of hundreds of speakers, we added multi-resolution\nspectrogram discriminators to sharpen the spectral resolution of the generated\nwaveforms. This enables the model to generate realistic waveforms of\nmulti-speakers, by alleviating the over-smoothing problem in the high frequency\nband of the large footprint model. Our structure generates signals close to\nground-truth data without reducing the inference speed, by discriminating the\nwaveform and spectrogram during training. The model achieved the best mean\nopinion score (MOS) in most scenarios using ground-truth mel-spectrogram as an\ninput. Especially, it showed superior performance in unseen domains with regard\nof speaker, emotion, and language. Moreover, in a multi-speaker text-to-speech\nscenario using mel-spectrogram generated by a transformer model, it synthesized\nhigh-fidelity speech of 4.22 MOS. These results, achieved without external\ndomain information, highlight the potential of the proposed model as a\nuniversal vocoder.", "published": "2020-11-19 03:35:45", "link": "http://arxiv.org/abs/2011.09631v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Deep Residual Local Feature Learning for Speech Emotion Recognition", "abstract": "Speech Emotion Recognition (SER) is becoming a key role in global business\ntoday to improve service efficiency, like call center services. Recent SERs\nwere based on a deep learning approach. However, the efficiency of deep\nlearning depends on the number of layers, i.e., the deeper layers, the higher\nefficiency. On the other hand, the deeper layers are causes of a vanishing\ngradient problem, a low learning rate, and high time-consuming. Therefore, this\npaper proposed a redesign of existing local feature learning block (LFLB). The\nnew design is called a deep residual local feature learning block\n(DeepResLFLB). DeepResLFLB consists of three cascade blocks: LFLB, residual\nlocal feature learning block (ResLFLB), and multilayer perceptron (MLP). LFLB\nis built for learning local correlations along with extracting hierarchical\ncorrelations; DeepResLFLB can take advantage of repeatedly learning to explain\nmore detail in deeper layers using residual learning for solving vanishing\ngradient and reducing overfitting; and MLP is adopted to find the relationship\nof learning and discover probability for predicted speech emotions and gender\ntypes. Based on two available published datasets: EMODB and RAVDESS, the\nproposed DeepResLFLB can significantly improve performance when evaluated by\nstandard metrics: accuracy, precision, recall, and F1-score.", "published": "2020-11-19 11:04:31", "link": "http://arxiv.org/abs/2011.09767v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Everybody Sign Now: Translating Spoken Language to Photo Realistic Sign\n  Language Video", "abstract": "To be truly understandable and accepted by Deaf communities, an automatic\nSign Language Production (SLP) system must generate a photo-realistic signer.\nPrior approaches based on graphical avatars have proven unpopular, whereas\nrecent neural SLP works that produce skeleton pose sequences have been shown to\nbe not understandable to Deaf viewers.\n  In this paper, we propose SignGAN, the first SLP model to produce\nphoto-realistic continuous sign language videos directly from spoken language.\nWe employ a transformer architecture with a Mixture Density Network (MDN)\nformulation to handle the translation from spoken language to skeletal pose. A\npose-conditioned human synthesis model is then introduced to generate a\nphoto-realistic sign language video from the skeletal pose sequence. This\nallows the photo-realistic production of sign videos directly translated from\nwritten text.\n  We further propose a novel keypoint-based loss function, which significantly\nimproves the quality of synthesized hand images, operating in the keypoint\nspace to avoid issues caused by motion blur. In addition, we introduce a method\nfor controllable video generation, enabling training on large, diverse sign\nlanguage datasets and providing the ability to control the signer appearance at\ninference.\n  Using a dataset of eight different sign language interpreters extracted from\nbroadcast footage, we show that SignGAN significantly outperforms all baseline\nmethods for quantitative metrics and human perceptual studies.", "published": "2020-11-19 14:31:06", "link": "http://arxiv.org/abs/2011.09846v4", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Sentiment Classification in Bangla Textual Content: A Comparative Study", "abstract": "Sentiment analysis has been widely used to understand our views on social and\npolitical agendas or user experiences over a product. It is one of the cores\nand well-researched areas in NLP. However, for low-resource languages, like\nBangla, one of the prominent challenge is the lack of resources. Another\nimportant limitation, in the current literature for Bangla, is the absence of\ncomparable results due to the lack of a well-defined train/test split. In this\nstudy, we explore several publicly available sentiment labeled datasets and\ndesigned classifiers using both classical and deep learning algorithms. In our\nstudy, the classical algorithms include SVM and Random Forest, and deep\nlearning algorithms include CNN, FastText, and transformer-based models. We\ncompare these models in terms of model performance and time-resource\ncomplexity. Our finding suggests transformer-based models, which have not been\nexplored earlier for Bangla, outperform all other models. Furthermore, we\ncreated a weighted list of lexicon content based on the valence score per\nclass. We then analyzed the content for high significance entries per class, in\nthe datasets. For reproducibility, we make publicly available data splits and\nthe ranked lexicon list. The presented results can be used for future studies\nas a benchmark.", "published": "2020-11-19 21:06:28", "link": "http://arxiv.org/abs/2011.10106v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "TaL: a synchronised multi-speaker corpus of ultrasound tongue imaging,\n  audio, and lip videos", "abstract": "We present the Tongue and Lips corpus (TaL), a multi-speaker corpus of audio,\nultrasound tongue imaging, and lip videos. TaL consists of two parts: TaL1 is a\nset of six recording sessions of one professional voice talent, a male native\nspeaker of English; TaL80 is a set of recording sessions of 81 native speakers\nof English without voice talent experience. Overall, the corpus contains 24\nhours of parallel ultrasound, video, and audio data, of which approximately\n13.5 hours are speech. This paper describes the corpus and presents benchmark\nresults for the tasks of speech recognition, speech synthesis\n(articulatory-to-acoustic mapping), and automatic synchronisation of ultrasound\nto audio. The TaL corpus is publicly available under the CC BY-NC 4.0 license.", "published": "2020-11-19 13:11:46", "link": "http://arxiv.org/abs/2011.09804v1", "categories": ["eess.AS", "cs.CL", "cs.CV", "cs.SD", "eess.IV"], "primary_category": "eess.AS"}
{"title": "Multi-stage Speaker Extraction with Utterance and Frame-Level Reference\n  Signals", "abstract": "Speaker extraction requires a sample speech from the target speaker as the\nreference. However, enrolling a speaker with a long speech is not practical. We\npropose a speaker extraction technique, that performs in multiple stages to\ntake full advantage of short reference speech sample. The extracted speech in\nearly stages is used as the reference speech for late stages. For the first\ntime, we use frame-level sequential speech embedding as the reference for\ntarget speaker. This is a departure from the traditional utterance-based\nspeaker embedding reference. In addition, a signal fusion scheme is proposed to\ncombine the decoded signals in multiple scales with automatically learned\nweights. Experiments on WSJ0-2mix and its noisy versions (WHAM! and WHAMR!)\nshow that SpEx++ consistently outperforms other state-of-the-art baselines.", "published": "2020-11-19 03:08:04", "link": "http://arxiv.org/abs/2011.09624v2", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "End-To-End Dilated Variational Autoencoder with Bottleneck\n  Discriminative Loss for Sound Morphing -- A Preliminary Study", "abstract": "We present a preliminary study on an end-to-end variational autoencoder (VAE)\nfor sound morphing. Two VAE variants are compared: VAE with dilation layers\n(DC-VAE) and VAE only with regular convolutional layers (CC-VAE). We combine\nthe following loss functions: 1) the time-domain mean-squared error for\nreconstructing the input signal, 2) the Kullback-Leibler divergence to the\nstandard normal distribution in the bottleneck layer, and 3) the classification\nloss calculated from the bottleneck representation. On a database of spoken\ndigits, we use 1-nearest neighbor classification to show that the sound classes\nseparate in the bottleneck layer. We introduce the Mel-frequency cepstrum\ncoefficient dynamic time warping (MFCC-DTW) deviation as a measure of how well\nthe VAE decoder projects the class center in the latent (bottleneck) layer to\nthe center of the sounds of that class in the audio domain. In terms of\nMFCC-DTW deviation and 1-NN classification, DC-VAE outperforms CC-VAE. These\nresults for our parametrization and our dataset indicate that DC-VAE is more\nsuitable for sound morphing than CC-VAE, since the DC-VAE decoder better\npreserves the topology when mapping from the audio domain to the latent space.\nExamples are given both for morphing spoken digits and drum sounds.", "published": "2020-11-19 09:47:13", "link": "http://arxiv.org/abs/2011.09744v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
