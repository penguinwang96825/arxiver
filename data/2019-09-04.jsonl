{"title": "Towards Realistic Practices In Low-Resource Natural Language Processing:\n  The Development Set", "abstract": "Development sets are impractical to obtain for real low-resource languages,\nsince using all available data for training is often more effective. However,\ndevelopment sets are widely used in research papers that purport to deal with\nlow-resource natural language processing (NLP). Here, we aim to answer the\nfollowing questions: Does using a development set for early stopping in the\nlow-resource setting influence results as compared to a more realistic\nalternative, where the number of training epochs is tuned on development\nlanguages? And does it lead to overestimation or underestimation of\nperformance? We repeat multiple experiments from recent work on neural models\nfor low-resource NLP and compare results for models obtained by training with\nand without development sets. On average over languages, absolute accuracy\ndiffers by up to 1.4%. However, for some languages and tasks, differences are\nas big as 18.0% accuracy. Our results highlight the importance of realistic\nexperimental setups in the publication of low-resource NLP research results.", "published": "2019-09-04 02:20:54", "link": "http://arxiv.org/abs/1909.01522v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Simpler and Faster Learning of Adaptive Policies for Simultaneous\n  Translation", "abstract": "Simultaneous translation is widely useful but remains challenging. Previous\nwork falls into two main categories: (a) fixed-latency policies such as Ma et\nal. (2019) and (b) adaptive policies such as Gu et al. (2017). The former are\nsimple and effective, but have to aggressively predict future content due to\ndiverging source-target word order; the latter do not anticipate, but suffer\nfrom unstable and inefficient training. To combine the merits of both\napproaches, we propose a simple supervised-learning framework to learn an\nadaptive policy from oracle READ/WRITE sequences generated from parallel text.\nAt each step, such an oracle sequence chooses to WRITE the next target word if\nthe available source sentence context provides enough information to do so,\notherwise READ the next source word. Experiments on German<->English show that\nour method, without retraining the underlying NMT model, can learn flexible\npolicies with better BLEU scores and similar latencies compared to previous\nwork.", "published": "2019-09-04 05:37:36", "link": "http://arxiv.org/abs/1909.01559v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Better Modeling Hierarchical Structure for Self-Attention with\n  Ordered Neurons", "abstract": "Recent studies have shown that a hybrid of self-attention networks (SANs) and\nrecurrent neural networks (RNNs) outperforms both individual architectures,\nwhile not much is known about why the hybrid models work. With the belief that\nmodeling hierarchical structure is an essential complementary between SANs and\nRNNs, we propose to further enhance the strength of hybrid models with an\nadvanced variant of RNNs - Ordered Neurons LSTM (ON-LSTM), which introduces a\nsyntax-oriented inductive bias to perform tree-like composition. Experimental\nresults on the benchmark machine translation task show that the proposed\napproach outperforms both individual architectures and a standard hybrid model.\nFurther analyses on targeted linguistic evaluation and logical inference tasks\ndemonstrate that the proposed approach indeed benefits from a better modeling\nof hierarchical structure.", "published": "2019-09-04 05:59:03", "link": "http://arxiv.org/abs/1909.01562v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AMR Normalization for Fairer Evaluation", "abstract": "Meaning Representation (AMR; Banarescu et al., 2013) encodes the meaning of\nsentences as a directed graph and Smatch (Cai and Knight, 2013) is the primary\nmetric for evaluating AMR graphs. Smatch, however, is unaware of some\nmeaning-equivalent variations in graph structure allowed by the AMR\nSpecification and gives different scores for AMRs exhibiting these variations.\nIn this paper I propose four normalization methods for helping to ensure that\nconceptually equivalent AMRs are evaluated as equivalent. Equivalent AMRs with\nand without normalization can look quite different---comparing a gold corpus to\nitself with relation reification alone yields a difference of 25 Smatch points,\nsuggesting that the outputs of two systems may not be directly comparable\nwithout normalization. The algorithms described in this paper are implemented\non top of an existing open-source Python toolkit for AMR and will be released\nunder the same license.", "published": "2019-09-04 06:29:35", "link": "http://arxiv.org/abs/1909.01568v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Discovering Hypernymy in Text-Rich Heterogeneous Information Network by\n  Exploiting Context Granularity", "abstract": "Text-rich heterogeneous information networks (text-rich HINs) are ubiquitous\nin real-world applications. Hypernymy, also known as is-a relation or\nsubclass-of relation, lays in the core of many knowledge graphs and benefits\nmany downstream applications. Existing methods of hypernymy discovery either\nleverage textual patterns to extract explicitly mentioned hypernym-hyponym\npairs, or learn a distributional representation for each term of interest based\nits context. These approaches rely on statistical signals from the textual\ncorpus, and their effectiveness would therefore be hindered when the signals\nfrom the corpus are not sufficient for all terms of interest. In this work, we\npropose to discover hypernymy in text-rich HINs, which can introduce additional\nhigh-quality signals. We develop a new framework, named HyperMine, that\nexploits multi-granular contexts and combines signals from both text and\nnetwork without human labeled data. HyperMine extends the definition of context\nto the scenario of text-rich HIN. For example, we can define typed nodes and\ncommunities as contexts. These contexts encode signals of different\ngranularities and we feed them into a hypernymy inference model. HyperMine\nlearns this model using weak supervision acquired based on high-precision\ntextual patterns. Extensive experiments on two large real-world datasets\ndemonstrate the effectiveness of HyperMine and the utility of modeling context\ngranularity. We further show a case study that a high-quality taxonomy can be\ngenerated solely based on the hypernymy discovered by HyperMine.", "published": "2019-09-04 07:23:06", "link": "http://arxiv.org/abs/1909.01584v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do We Really Need Fully Unsupervised Cross-Lingual Embeddings?", "abstract": "Recent efforts in cross-lingual word embedding (CLWE) learning have\npredominantly focused on fully unsupervised approaches that project monolingual\nembeddings into a shared cross-lingual space without any cross-lingual signal.\nThe lack of any supervision makes such approaches conceptually attractive. Yet,\ntheir only core difference from (weakly) supervised projection-based CLWE\nmethods is in the way they obtain a seed dictionary used to initialize an\niterative self-learning procedure. The fully unsupervised methods have arguably\nbecome more robust, and their primary use case is CLWE induction for pairs of\nresource-poor and distant languages. In this paper, we question the ability of\neven the most robust unsupervised CLWE approaches to induce meaningful CLWEs in\nthese more challenging settings. A series of bilingual lexicon induction (BLI)\nexperiments with 15 diverse languages (210 language pairs) show that fully\nunsupervised CLWE methods still fail for a large number of language pairs\n(e.g., they yield zero BLI performance for 87/210 pairs). Even when they\nsucceed, they never surpass the performance of weakly supervised methods\n(seeded with 500-1,000 translation pairs) using the same self-learning\nprocedure in any BLI setup, and the gaps are often substantial. These findings\ncall for revisiting the main motivations behind fully unsupervised CLWE\nmethods.", "published": "2019-09-04 09:17:21", "link": "http://arxiv.org/abs/1909.01638v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ParaQG: A System for Generating Questions and Answers from Paragraphs", "abstract": "Generating syntactically and semantically valid and relevant questions from\nparagraphs is useful with many applications. Manual generation is a\nlabour-intensive task, as it requires the reading, parsing and understanding of\nlong passages of text. A number of question generation models based on\nsequence-to-sequence techniques have recently been proposed. Most of them\ngenerate questions from sentences only, and none of them is publicly available\nas an easy-to-use service. In this paper, we demonstrate ParaQG, a Web-based\nsystem for generating questions from sentences and paragraphs. ParaQG\nincorporates a number of novel functionalities to make the question generation\nprocess user-friendly. It provides an interactive interface for a user to\nselect answers with visual insights on generation of questions. It also employs\nvarious faceted views to group similar questions as well as filtering\ntechniques to eliminate unanswerable questions", "published": "2019-09-04 09:23:50", "link": "http://arxiv.org/abs/1909.01642v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SAO WMT19 Test Suite: Machine Translation of Audit Reports", "abstract": "This paper describes a machine translation test set of documents from the\nauditing domain and its use as one of the \"test suites\" in the WMT19 News\nTranslation Task for translation directions involving Czech, English and\nGerman.\n  Our evaluation suggests that current MT systems optimized for the general\nnews domain can perform quite well even in the particular domain of audit\nreports. The detailed manual evaluation however indicates that deep factual\nknowledge of the domain is necessary. For the naked eye of a non-expert,\ntranslations by many systems seem almost perfect and automatic MT evaluation\nwith one reference is practically useless for considering these details.\n  Furthermore, we show on a sample document from the domain of agreements that\neven the best systems completely fail in preserving the semantics of the\nagreement, namely the identity of the parties.", "published": "2019-09-04 11:37:26", "link": "http://arxiv.org/abs/1909.01701v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mogrifier LSTM", "abstract": "Many advances in Natural Language Processing have been based upon more\nexpressive models for how inputs interact with the context in which they occur.\nRecurrent networks, which have enjoyed a modicum of success, still lack the\ngeneralization and systematicity ultimately required for modelling language. In\nthis work, we propose an extension to the venerable Long Short-Term Memory in\nthe form of mutual gating of the current input and the previous output. This\nmechanism affords the modelling of a richer space of interactions between\ninputs and their context. Equivalently, our model can be viewed as making the\ntransition function given by the LSTM context-dependent. Experiments\ndemonstrate markedly improved generalization on language modelling in the range\nof 3-4 perplexity points on Penn Treebank and Wikitext-2, and 0.01-0.05 bpc on\nfour character-based datasets. We establish a new state of the art on all\ndatasets with the exception of Enwik8, where we close a large gap between the\nLSTM and Transformer models.", "published": "2019-09-04 13:32:23", "link": "http://arxiv.org/abs/1909.01792v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extracting Aspects Hierarchies using Rhetorical Structure Theory", "abstract": "We propose a novel approach to generate aspect hierarchies that proved to be\nconsistently correct compared with human-generated hierarchies. We present an\nunsupervised technique using Rhetorical Structure Theory and graph analysis. We\nevaluated our approach based on 100,000 reviews from Amazon and achieved an\nastonishing 80% coverage compared with human-generated hierarchies coded in\nConceptNet. The method could be easily extended with a sentiment analysis model\nand used to describe sentiment on different levels of aspect granularity.\nHence, besides the flat aspect structure, we can differentiate between aspects\nand describe if the charging aspect is related to battery or price.", "published": "2019-09-04 13:48:27", "link": "http://arxiv.org/abs/1909.01800v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ICDM 2019 Knowledge Graph Contest: Team UWA", "abstract": "We present an overview of our triple extraction system for the ICDM 2019\nKnowledge Graph Contest. Our system uses a pipeline-based approach to extract a\nset of triples from a given document. It offers a simple and effective solution\nto the challenge of knowledge graph construction from domain-specific text. It\nalso provides the facility to visualise useful information about each triple\nsuch as the degree, betweenness, structured relation type(s), and named entity\ntypes.", "published": "2019-09-04 13:56:52", "link": "http://arxiv.org/abs/1909.01807v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Empirical Study of Diachronic Word Embeddings for Scarce Data", "abstract": "Word meaning change can be inferred from drifts of time-varying word\nembeddings. However, temporal data may be too sparse to build robust word\nembeddings and to discriminate significant drifts from noise. In this paper, we\ncompare three models to learn diachronic word embeddings on scarce data:\nincremental updating of a Skip-Gram from Kim et al. (2014), dynamic filtering\nfrom Bamler and Mandt (2017), and dynamic Bernoulli embeddings from Rudolph and\nBlei (2018). In particular, we study the performance of different\ninitialisation schemes and emphasise what characteristics of each model are\nmore suitable to data scarcity, relying on the distribution of detected drifts.\nFinally, we regularise the loss of these models to better adapt to scarce data.", "published": "2019-09-04 15:11:32", "link": "http://arxiv.org/abs/1909.01863v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mixture Content Selection for Diverse Sequence Generation", "abstract": "Generating diverse sequences is important in many NLP applications such as\nquestion generation or summarization that exhibit semantically one-to-many\nrelationships between source and the target sequences. We present a method to\nexplicitly separate diversification from generation using a general\nplug-and-play module (called SELECTOR) that wraps around and guides an existing\nencoder-decoder model. The diversification stage uses a mixture of experts to\nsample different binary masks on the source sequence for diverse content\nselection. The generation stage uses a standard encoder-decoder model given\neach selected content from the source sequence. Due to the non-differentiable\nnature of discrete sampling and the lack of ground truth labels for binary\nmask, we leverage a proxy for ground truth mask and adopt stochastic hard-EM\nfor training. In question generation (SQuAD) and abstractive summarization\n(CNN-DM), our method demonstrates significant improvements in accuracy,\ndiversity and training efficiency, including state-of-the-art top-1 accuracy in\nboth datasets, 6% gain in top-5 accuracy, and 3.7 times faster training over a\nstate of the art model. Our code is publicly available at\nhttps://github.com/clovaai/FocusSeq2Seq.", "published": "2019-09-04 17:23:54", "link": "http://arxiv.org/abs/1909.01953v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Jointly Learning to Align and Translate with Transformer Models", "abstract": "The state of the art in machine translation (MT) is governed by neural\napproaches, which typically provide superior translation accuracy over\nstatistical approaches. However, on the closely related task of word alignment,\ntraditional statistical word alignment models often remain the go-to solution.\nIn this paper, we present an approach to train a Transformer model to produce\nboth accurate translations and alignments. We extract discrete alignments from\nthe attention probabilities learnt during regular neural machine translation\nmodel training and leverage them in a multi-task framework to optimize towards\ntranslation and alignment objectives. We demonstrate that our approach produces\ncompetitive results compared to GIZA++ trained IBM alignment models without\nsacrificing translation accuracy and outperforms previous attempts on\nTransformer model based word alignment. Finally, by incorporating IBM model\nalignments into our multi-task training, we report significantly better\nalignment accuracies compared to GIZA++ on three publicly available data sets.", "published": "2019-09-04 19:54:06", "link": "http://arxiv.org/abs/1909.02074v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PaLM: A Hybrid Parser and Language Model", "abstract": "We present PaLM, a hybrid parser and neural language model. Building on an\nRNN language model, PaLM adds an attention layer over text spans in the left\ncontext. An unsupervised constituency parser can be derived from its attention\nweights, using a greedy decoding algorithm. We evaluate PaLM on language\nmodeling, and empirically show that it outperforms strong baselines. If\nsyntactic annotations are available, the attention component can be trained in\na supervised manner, providing syntactically-informed representations of the\ncontext, and further improving language modeling performance.", "published": "2019-09-04 22:10:41", "link": "http://arxiv.org/abs/1909.02134v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Meta Relational Learning for Few-Shot Link Prediction in Knowledge\n  Graphs", "abstract": "Link prediction is an important way to complete knowledge graphs (KGs), while\nembedding-based methods, effective for link prediction in KGs, perform poorly\non relations that only have a few associative triples. In this work, we propose\na Meta Relational Learning (MetaR) framework to do the common but challenging\nfew-shot link prediction in KGs, namely predicting new triples about a relation\nby only observing a few associative triples. We solve few-shot link prediction\nby focusing on transferring relation-specific meta information to make model\nlearn the most important knowledge and learn faster, corresponding to relation\nmeta and gradient meta respectively in MetaR. Empirically, our model achieves\nstate-of-the-art results on few-shot link prediction KG benchmarks.", "published": "2019-09-04 01:35:47", "link": "http://arxiv.org/abs/1909.01515v1", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Referring Expression Generation Using Entity Profiles", "abstract": "Referring Expression Generation (REG) is the task of generating contextually\nappropriate references to entities. A limitation of existing REG systems is\nthat they rely on entity-specific supervised training, which means that they\ncannot handle entities not seen during training. In this study, we address this\nin two ways. First, we propose task setups in which we specifically test a REG\nsystem's ability to generalize to entities not seen during training. Second, we\npropose a profile-based deep neural network model, ProfileREG, which encodes\nboth the local context and an external profile of the entity to generate\nreference realizations. Our model generates tokens by learning to choose\nbetween generating pronouns, generating from a fixed vocabulary, or copying a\nword from the profile. We evaluate our model on three different splits of the\nWebNLG dataset, and show that it outperforms competitive baselines in all\nsettings according to automatic and human evaluations.", "published": "2019-09-04 02:42:07", "link": "http://arxiv.org/abs/1909.01528v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Non-commutative Bilinear Model for Answering Path Queries in Knowledge\n  Graphs", "abstract": "Bilinear diagonal models for knowledge graph embedding (KGE), such as\nDistMult and ComplEx, balance expressiveness and computational efficiency by\nrepresenting relations as diagonal matrices. Although they perform well in\npredicting atomic relations, composite relations (relation paths) cannot be\nmodeled naturally by the product of relation matrices, as the product of\ndiagonal matrices is commutative and hence invariant with the order of\nrelations. In this paper, we propose a new bilinear KGE model, called\nBlockHolE, based on block circulant matrices. In BlockHolE, relation matrices\ncan be non-commutative, allowing composite relations to be modeled by matrix\nproduct. The model is parameterized in a way that covers a spectrum ranging\nfrom diagonal to full relation matrices. A fast computation technique is\ndeveloped on the basis of the duality of the Fourier transform of circulant\nmatrices.", "published": "2019-09-04 06:26:05", "link": "http://arxiv.org/abs/1909.01567v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Different Absorption from the Same Sharing: Sifted Multi-task Learning\n  for Fake News Detection", "abstract": "Recently, neural networks based on multi-task learning have achieved\npromising performance on fake news detection, which focus on learning shared\nfeatures among tasks as complementary features to serve different tasks.\nHowever, in most of the existing approaches, the shared features are completely\nassigned to different tasks without selection, which may lead to some useless\nand even adverse features integrated into specific tasks. In this paper, we\ndesign a sifted multi-task learning method with a selected sharing layer for\nfake news detection. The selected sharing layer adopts gate mechanism and\nattention mechanism to filter and select shared feature flows between tasks.\nExperiments on two public and widely used competition datasets, i.e. RumourEval\nand PHEME, demonstrate that our proposed method achieves the state-of-the-art\nperformance and boosts the F1-score by more than 0.87%, 1.31%, respectively.", "published": "2019-09-04 12:15:02", "link": "http://arxiv.org/abs/1909.01720v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Affect Enriched Word Embeddings for News Information Retrieval", "abstract": "Distributed representations of words have shown to be useful to improve the\neffectiveness of IR systems in many sub-tasks like query expansion, retrieval\nand ranking. Algorithms like word2vec, GloVe and others are also key factors in\nmany improvements in different NLP tasks. One common issue with such embedding\nmodels is that words like happy and sad appear in similar contexts and hence\nare wrongly clustered close in the embedding space. In this paper we leverage\nAff2Vec, a set of word embeddings models which include affect information, in\norder to better capture the affect aspect in news text to achieve better\nresults in information retrieval tasks, also such embeddings are less hit by\nthe synonym/antonym issue. We evaluate their effectiveness on two IR related\ntasks (query expansion and ranking) over the New York Times dataset (TREC-core\n'17) comparing them against other word embeddings based models and classic\nranking models.", "published": "2019-09-04 13:12:30", "link": "http://arxiv.org/abs/1909.01772v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "From 'F' to 'A' on the N.Y. Regents Science Exams: An Overview of the\n  Aristo Project", "abstract": "AI has achieved remarkable mastery over games such as Chess, Go, and Poker,\nand even Jeopardy, but the rich variety of standardized exams has remained a\nlandmark challenge. Even in 2016, the best AI system achieved merely 59.3% on\nan 8th Grade science exam challenge. This paper reports unprecedented success\non the Grade 8 New York Regents Science Exam, where for the first time a system\nscores more than 90% on the exam's non-diagram, multiple choice (NDMC)\nquestions. In addition, our Aristo system, building upon the success of recent\nlanguage models, exceeded 83% on the corresponding Grade 12 Science Exam NDMC\nquestions. The results, on unseen test questions, are robust across different\ntest years and different variations of this kind of test. They demonstrate that\nmodern NLP methods can result in mastery on this task. While not a full\nsolution to general question-answering (the questions are multiple choice, and\nthe domain is restricted to 8th Grade science), it represents a significant\nmilestone for the field.", "published": "2019-09-04 17:33:42", "link": "http://arxiv.org/abs/1909.01958v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TIGEr: Text-to-Image Grounding for Image Caption Evaluation", "abstract": "This paper presents a new metric called TIGEr for the automatic evaluation of\nimage captioning systems. Popular metrics, such as BLEU and CIDEr, are based\nsolely on text matching between reference captions and machine-generated\ncaptions, potentially leading to biased evaluations because references may not\nfully cover the image content and natural language is inherently ambiguous.\nBuilding upon a machine-learned text-image grounding model, TIGEr allows to\nevaluate caption quality not only based on how well a caption represents image\ncontent, but also on how well machine-generated captions match human-generated\ncaptions. Our empirical tests show that TIGEr has a higher consistency with\nhuman judgments than alternative existing metrics. We also comprehensively\nassess the metric's effectiveness in caption evaluation by measuring the\ncorrelation between human judgments and metric scores.", "published": "2019-09-04 18:43:04", "link": "http://arxiv.org/abs/1909.02050v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Decoupled Box Proposal and Featurization with Ultrafine-Grained Semantic\n  Labels Improve Image Captioning and Visual Question Answering", "abstract": "Object detection plays an important role in current solutions to vision and\nlanguage tasks like image captioning and visual question answering. However,\npopular models like Faster R-CNN rely on a costly process of annotating\nground-truths for both the bounding boxes and their corresponding semantic\nlabels, making it less amenable as a primitive task for transfer learning. In\nthis paper, we examine the effect of decoupling box proposal and featurization\nfor down-stream tasks. The key insight is that this allows us to leverage a\nlarge amount of labeled annotations that were previously unavailable for\nstandard object detection benchmarks. Empirically, we demonstrate that this\nleads to effective transfer learning and improved image captioning and visual\nquestion answering models, as measured on publicly available benchmarks.", "published": "2019-09-04 20:37:30", "link": "http://arxiv.org/abs/1909.02097v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Learning Dynamic Context Augmentation for Global Entity Linking", "abstract": "Despite of the recent success of collective entity linking (EL) methods,\nthese \"global\" inference methods may yield sub-optimal results when the\n\"all-mention coherence\" assumption breaks, and often suffer from high\ncomputational cost at the inference stage, due to the complex search space. In\nthis paper, we propose a simple yet effective solution, called Dynamic Context\nAugmentation (DCA), for collective EL, which requires only one pass through the\nmentions in a document. DCA sequentially accumulates context information to\nmake efficient, collective inference, and can cope with different local EL\nmodels as a plug-and-enhance module. We explore both supervised and\nreinforcement learning strategies for learning the DCA model. Extensive\nexperiments show the effectiveness of our model with different learning\nsettings, base models, decision orders and attention mechanisms.", "published": "2019-09-04 21:13:23", "link": "http://arxiv.org/abs/1909.02117v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Reporting the Unreported: Event Extraction for Analyzing the Local\n  Representation of Hate Crimes", "abstract": "Official reports of hate crimes in the US are under-reported relative to the\nactual number of such incidents. Further, despite statistical approximations,\nthere are no official reports from a large number of US cities regarding\nincidents of hate. Here, we first demonstrate that event extraction and\nmulti-instance learning, applied to a corpus of local news articles, can be\nused to predict instances of hate crime. We then use the trained model to\ndetect incidents of hate in cities for which the FBI lacks statistics. Lastly,\nwe train models on predicting homicide and kidnapping, compare the predictions\nto FBI reports, and establish that incidents of hate are indeed under-reported,\ncompared to other types of crimes, in local press.", "published": "2019-09-04 21:45:51", "link": "http://arxiv.org/abs/1909.02126v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "KagNet: Knowledge-Aware Graph Networks for Commonsense Reasoning", "abstract": "Commonsense reasoning aims to empower machines with the human ability to make\npresumptions about ordinary situations in our daily life. In this paper, we\npropose a textual inference framework for answering commonsense questions,\nwhich effectively utilizes external, structured commonsense knowledge graphs to\nperform explainable inferences. The framework first grounds a question-answer\npair from the semantic space to the knowledge-based symbolic space as a schema\ngraph, a related sub-graph of external knowledge graphs. It represents schema\ngraphs with a novel knowledge-aware graph network module named KagNet, and\nfinally scores answers with graph representations. Our model is based on graph\nconvolutional networks and LSTMs, with a hierarchical path-based attention\nmechanism. The intermediate attention scores make it transparent and\ninterpretable, which thus produce trustworthy inferences. Using ConceptNet as\nthe only external resource for Bert-based models, we achieved state-of-the-art\nperformance on the CommonsenseQA, a large-scale dataset for commonsense\nreasoning.", "published": "2019-09-04 23:37:25", "link": "http://arxiv.org/abs/1909.02151v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Answers Unite! Unsupervised Metrics for Reinforced Summarization Models", "abstract": "Abstractive summarization approaches based on Reinforcement Learning (RL)\nhave recently been proposed to overcome classical likelihood maximization. RL\nenables to consider complex, possibly non-differentiable, metrics that globally\nassess the quality and relevance of the generated outputs. ROUGE, the most used\nsummarization metric, is known to suffer from bias towards lexical similarity\nas well as from suboptimal accounting for fluency and readability of the\ngenerated abstracts. We thus explore and propose alternative evaluation\nmeasures: the reported human-evaluation analysis shows that the proposed\nmetrics, based on Question Answering, favorably compares to ROUGE -- with the\nadditional property of not requiring reference summaries. Training a RL-based\nmodel on these metrics leads to improvements (both in terms of human or\nautomated metrics) over current approaches that use ROUGE as a reward.", "published": "2019-09-04 08:20:31", "link": "http://arxiv.org/abs/1909.01610v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "On the k-synchronizability of systems", "abstract": "In this paper, we work on the notion of k-synchronizability: a system is\nk-synchronizable if any of its executions, up to reordering causally\nindependent actions, can be divided into a succession of k-bounded interaction\nphases. We show two results (both for mailbox and peer-to-peer automata):\nfirst, the reachability problem is decidable for k-synchronizable systems;\nsecond, the membership problem (whether a given system is k-synchronizable) is\ndecidable as well. Our proofs fix several important issues in previous attempts\nto prove these two results for mailbox automata.", "published": "2019-09-04 08:58:53", "link": "http://arxiv.org/abs/1909.01627v2", "categories": ["cs.FL", "cs.CL", "cs.SC", "cs.SE"], "primary_category": "cs.FL"}
{"title": "DurIAN: Duration Informed Attention Network For Multimodal Synthesis", "abstract": "In this paper, we present a generic and robust multimodal synthesis system\nthat produces highly natural speech and facial expression simultaneously. The\nkey component of this system is the Duration Informed Attention Network\n(DurIAN), an autoregressive model in which the alignments between the input\ntext and the output acoustic features are inferred from a duration model. This\nis different from the end-to-end attention mechanism used, and accounts for\nvarious unavoidable artifacts, in existing end-to-end speech synthesis systems\nsuch as Tacotron. Furthermore, DurIAN can be used to generate high quality\nfacial expression which can be synchronized with generated speech with/without\nparallel speech and face data. To improve the efficiency of speech generation,\nwe also propose a multi-band parallel generation strategy on top of the WaveRNN\nmodel. The proposed Multi-band WaveRNN effectively reduces the total\ncomputational complexity from 9.8 to 5.5 GFLOPS, and is able to generate audio\nthat is 6 times faster than real time on a single CPU core. We show that DurIAN\ncould generate highly natural speech that is on par with current state of the\nart end-to-end systems, while at the same time avoid word skipping/repeating\nerrors in those systems. Finally, a simple yet effective approach for\nfine-grained control of expressiveness of speech and facial expression is\nintroduced.", "published": "2019-09-04 11:35:48", "link": "http://arxiv.org/abs/1909.01700v2", "categories": ["cs.CL", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "ScisummNet: A Large Annotated Corpus and Content-Impact Models for\n  Scientific Paper Summarization with Citation Networks", "abstract": "Scientific article summarization is challenging: large, annotated corpora are\nnot available, and the summary should ideally include the article's impacts on\nresearch community. This paper provides novel solutions to these two\nchallenges. We 1) develop and release the first large-scale manually-annotated\ncorpus for scientific papers (on computational linguistics) by enabling faster\nannotation, and 2) propose summarization methods that integrate the authors'\noriginal highlights (abstract) and the article's actual impacts on the\ncommunity (citations), to create comprehensive, hybrid summaries. We conduct\nexperiments to demonstrate the efficacy of our corpus in training data-driven\nmodels for scientific paper summarization and the advantage of our hybrid\nsummaries over abstracts and traditional citation-based summaries. Our large\nannotated corpus and hybrid methods provide a new framework for scientific\npaper summarization research.", "published": "2019-09-04 12:04:48", "link": "http://arxiv.org/abs/1909.01716v3", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Evaluation Dataset for Intent Classification and Out-of-Scope\n  Prediction", "abstract": "Task-oriented dialog systems need to know when a query falls outside their\nrange of supported intents, but current text classification corpora only define\nlabel sets that cover every example. We introduce a new dataset that includes\nqueries that are out-of-scope---i.e., queries that do not fall into any of the\nsystem's supported intents. This poses a new challenge because models cannot\nassume that every query at inference time belongs to a system-supported intent\nclass. Our dataset also covers 150 intent classes over 10 domains, capturing\nthe breadth that a production task-oriented agent must handle. We evaluate a\nrange of benchmark classifiers on our dataset along with several different\nout-of-scope identification schemes. We find that while the classifiers perform\nwell on in-scope intent classification, they struggle to identify out-of-scope\nqueries. Our dataset and evaluation fill an important gap in the field,\noffering a way of more rigorously and realistically benchmarking text\nclassification in task-driven dialog systems.", "published": "2019-09-04 18:04:56", "link": "http://arxiv.org/abs/1909.02027v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Entity-Driven Framework for Abstractive Summarization", "abstract": "Abstractive summarization systems aim to produce more coherent and concise\nsummaries than their extractive counterparts. Popular neural models have\nachieved impressive results for single-document summarization, yet their\noutputs are often incoherent and unfaithful to the input. In this paper, we\nintroduce SENECA, a novel System for ENtity-drivEn Coherent Abstractive\nsummarization framework that leverages entity information to generate\ninformative and coherent abstracts. Our framework takes a two-step approach:\n(1) an entity-aware content selection module first identifies salient sentences\nfrom the input, then (2) an abstract generation module conducts cross-sentence\ninformation compression and abstraction to generate the final summary, which is\ntrained with rewards to promote coherence, conciseness, and clarity. The two\ncomponents are further connected using reinforcement learning. Automatic\nevaluation shows that our model significantly outperforms previous\nstate-of-the-art on ROUGE and our proposed coherence measures on New York Times\nand CNN/Daily Mail datasets. Human judges further rate our system summaries as\nmore informative and coherent than those by popular summarization models.", "published": "2019-09-04 19:07:29", "link": "http://arxiv.org/abs/1909.02059v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Distributionally Robust Language Modeling", "abstract": "Language models are generally trained on data spanning a wide range of topics\n(e.g., news, reviews, fiction), but they might be applied to an a priori\nunknown target distribution (e.g., restaurant reviews). In this paper, we first\nshow that training on text outside the test distribution can degrade test\nperformance when using standard maximum likelihood (MLE) training. To remedy\nthis without the knowledge of the test distribution, we propose an approach\nwhich trains a model that performs well over a wide range of potential test\ndistributions. In particular, we derive a new distributionally robust\noptimization (DRO) procedure which minimizes the loss of the model over the\nworst-case mixture of topics with sufficient overlap with the training\ndistribution. Our approach, called topic conditional value at risk (topic\nCVaR), obtains a 5.5 point perplexity reduction over MLE when the language\nmodels are trained on a mixture of Yelp reviews and news and tested only on\nreviews.", "published": "2019-09-04 19:07:33", "link": "http://arxiv.org/abs/1909.02060v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "The Diversity-Innovation Paradox in Science", "abstract": "Prior work finds a diversity paradox: diversity breeds innovation, and yet,\nunderrepresented groups that diversify organizations have less successful\ncareers within them. Does the diversity paradox hold for scientists as well? We\nstudy this by utilizing a near-population of ~1.2 million US doctoral\nrecipients from 1977-2015 and following their careers into publishing and\nfaculty positions. We use text analysis and machine learning to answer a series\nof questions: How do we detect scientific innovations? Are underrepresented\ngroups more likely to generate scientific innovations? And are the innovations\nof underrepresented groups adopted and rewarded? Our analyses show that\nunderrepresented groups produce higher rates of scientific novelty. However,\ntheir novel contributions are devalued and discounted: e.g., novel\ncontributions by gender and racial minorities are taken up by other scholars at\nlower rates than novel contributions by gender and racial majorities, and\nequally impactful contributions of gender and racial minorities are less likely\nto result in successful scientific careers than for majority groups. These\nresults suggest there may be unwarranted reproduction of stratification in\nacademic careers that discounts diversity's role in innovation and partly\nexplains the underrepresentation of some groups in academia.", "published": "2019-09-04 19:10:20", "link": "http://arxiv.org/abs/1909.02063v2", "categories": ["cs.SI", "cs.CL", "stat.AP", "stat.ML"], "primary_category": "cs.SI"}
{"title": "Problems with automating translation of movie/TV show subtitles", "abstract": "We present 27 problems encountered in automating the translation of movie/TV\nshow subtitles. We categorize each problem in one of the three categories viz.\nproblems directly related to textual translation, problems related to subtitle\ncreation guidelines, and problems due to adaptability of machine translation\n(MT) engines. We also present the findings of a translation quality evaluation\nexperiment where we share the frequency of 16 key problems. We show that the\nsystems working at the frontiers of Natural Language Processing do not perform\nwell for subtitles and require some post-processing solutions for redressal of\nthese problems", "published": "2019-09-04 06:45:51", "link": "http://arxiv.org/abs/1909.05362v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Help, Anna! Visual Navigation with Natural Multimodal Assistance via\n  Retrospective Curiosity-Encouraging Imitation Learning", "abstract": "Mobile agents that can leverage help from humans can potentially accomplish\nmore complex tasks than they could entirely on their own. We develop \"Help,\nAnna!\" (HANNA), an interactive photo-realistic simulator in which an agent\nfulfills object-finding tasks by requesting and interpreting natural\nlanguage-and-vision assistance. An agent solving tasks in a HANNA environment\ncan leverage simulated human assistants, called ANNA (Automatic Natural\nNavigation Assistants), which, upon request, provide natural language and\nvisual instructions to direct the agent towards the goals. To address the HANNA\nproblem, we develop a memory-augmented neural agent that hierarchically models\nmultiple levels of decision-making, and an imitation learning algorithm that\nteaches the agent to avoid repeating past mistakes while simultaneously\npredicting its own chances of making future progress. Empirically, our approach\nis able to ask for help more effectively than competitive baselines and, thus,\nattains higher task success rate on both previously seen and previously unseen\nenvironments. We publicly release code and data at\nhttps://github.com/khanhptnk/hanna . A video demo is available at\nhttps://youtu.be/18P94aaaLKg .", "published": "2019-09-04 15:20:01", "link": "http://arxiv.org/abs/1909.01871v6", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.HC"}
{"title": "Towards Interpretable Polyphonic Transcription with Invertible Neural\n  Networks", "abstract": "We explore a novel way of conceptualising the task of polyphonic music\ntranscription, using so-called invertible neural networks. Invertible models\nunify both discriminative and generative aspects in one function, sharing one\nset of parameters. Introducing invertibility enables the practitioner to\ndirectly inspect what the discriminative model has learned, and exactly\ndetermine which inputs lead to which outputs. For the task of transcribing\npolyphonic audio into symbolic form, these models may be especially useful as\nthey allow us to observe, for instance, to what extent the concept of single\nnotes could be learned from a corpus of polyphonic music alone (which has been\nidentified as a serious problem in recent research). This is an entirely new\napproach to audio transcription, which first of all necessitates some\ngroundwork. In this paper, we begin by looking at the simplest possible\ninvertible transcription model, and then thoroughly investigate its properties.\nFinally, we will take first steps towards a more sophisticated and capable\nversion. We use the task of piano transcription, and specifically the MAPS\ndataset, as a basis for these investigations.", "published": "2019-09-04 08:46:14", "link": "http://arxiv.org/abs/1909.01622v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "VoIPLoc: Passive VoIP call provenance via acoustic side-channels", "abstract": "We propose VoIPLoc, a novel location fingerprinting technique and apply it to\nthe VoIP call provenance problem. It exploits echo-location information\nembedded within VoIP audio to support fine-grained location inference. We found\nconsistent statistical features induced by the echo-reflection characteristics\nof the location into recorded speech. These features are discernible within\ntraces received at the VoIP destination, enabling location inference. We\nevaluated VoIPLoc by developing a dataset of audio traces received through VoIP\nchannels over the Tor network. We show that recording locations can be\nfingerprinted and detected remotely with a low false-positive rate, even when a\nmajority of the audio samples are unlabelled. Finally, we note that the\ntechnique is fully passive and thus undetectable, unlike prior art. VoIPLoc is\nrobust to the impact of environmental noise and background sounds, as well as\nthe impact of compressive codecs and network jitter. The technique is also\nhighly scalable and offers several degrees of freedom terms of the\nfingerprintable space.", "published": "2019-09-04 15:55:46", "link": "http://arxiv.org/abs/1909.01904v2", "categories": ["cs.CR", "cs.SD", "eess.AS", "68M99", "C.2.0; C.2.2"], "primary_category": "cs.CR"}
{"title": "Exploiting Parallel Audio Recordings to Enforce Device Invariance in\n  CNN-based Acoustic Scene Classification", "abstract": "Distribution mismatches between the data seen at training and at application\ntime remain a major challenge in all application areas of machine learning. We\nstudy this problem in the context of machine listening (Task 1b of the DCASE\n2019 Challenge). We propose a novel approach to learn domain-invariant\nclassifiers in an end-to-end fashion by enforcing equal hidden layer\nrepresentations for domain-parallel samples, i.e. time-aligned recordings from\ndifferent recording devices. No classification labels are needed for our domain\nadaptation (DA) method, which makes the data collection process cheaper.", "published": "2019-09-04 16:19:50", "link": "http://arxiv.org/abs/1909.02869v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
