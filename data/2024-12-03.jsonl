{"title": "Unsupervised learning-based calibration scheme for Rough Bergomi model", "abstract": "Current deep learning-based calibration schemes for rough volatility models\nare based on the supervised learning framework, which can be costly due to a\nlarge amount of training data being generated. In this work, we propose a novel\nunsupervised learning-based scheme for the rough Bergomi (rBergomi) model which\ndoes not require accessing training data. The main idea is to use the backward\nstochastic differential equation (BSDE) derived in [Bayer, Qiu and Yao, {SIAM\nJ. Financial Math.}, 2022] and simultaneously learn the BSDE solutions with the\nmodel parameters. We establish that the mean squares error between the option\nprices under the learned model parameters and the historical data is bounded by\nthe loss function. Moreover, the loss can be made arbitrarily small under\nsuitable conditions on the fitting ability of the rBergomi model to the market\nand the universal approximation capability of neural networks. Numerical\nexperiments for both simulated and historical data confirm the efficiency of\nscheme.", "published": "2024-12-03 03:48:09", "link": "http://arxiv.org/abs/2412.02135v2", "categories": ["q-fin.CP", "91G20, 60H15, 91G60, 60H35, 91G80"], "primary_category": "q-fin.CP"}
{"title": "An Integral Equation in Portfolio Selection with Time-Inconsistent Preferences", "abstract": "This paper discusses a nonlinear integral equation arising from portfolio\nselection with a class of time-inconsistent preferences. We propose a unified\nframework requiring minimal assumptions, such as right-continuity of market\ncoefficients and square-integrability of the market price of risk. Our main\ncontribution is proving the existence and uniqueness of the square-integrable\nsolution for the integral equation under mild conditions. Illustrative\napplications include the mean-variance portfolio selection and the utility\nmaximization with random risk aversion.", "published": "2024-12-03 13:30:05", "link": "http://arxiv.org/abs/2412.02446v2", "categories": ["q-fin.MF", "91G10, 91B70"], "primary_category": "q-fin.MF"}
{"title": "Uncertain Regulations, Definite Impacts: The Impact of the US Securities and Exchange Commission's Regulatory Interventions on Crypto Assets", "abstract": "This study employs an event study methodology to investigate the market\nimpact of the U.S. Securities and Exchange Commission's (SEC) classification of\ncrypto assets as securities. It explores how SEC interventions influence asset\nreturns and trading volumes, focusing on explicitly named crypto assets. The\nempirical analysis highlights significant adverse market reactions, notably\nreturns plummeting 12% over one week post-announcement, persisting for a month.\nWe demonstrate that the severity of market reaction depends on sentiment and\nasset characteristics such as market size, age, volatility, and illiquidity.\nFurther, we identify significant ex-ante trading volume effects indicative of\npre-announcement informed trading.", "published": "2024-12-03 13:38:02", "link": "http://arxiv.org/abs/2412.02452v1", "categories": ["q-fin.GN", "q-fin.RM", "q-fin.TR", "91G15, 91G30, 91G45, 91G70, 62P20", "K.4.1; I.6.5"], "primary_category": "q-fin.GN"}
{"title": "A Multi-way Parallel Named Entity Annotated Corpus for English, Tamil\n  and Sinhala", "abstract": "This paper presents a multi-way parallel English-Tamil-Sinhala corpus\nannotated with Named Entities (NEs), where Sinhala and Tamil are low-resource\nlanguages. Using pre-trained multilingual Language Models (mLMs), we establish\nnew benchmark Named Entity Recognition (NER) results on this dataset for\nSinhala and Tamil. We also carry out a detailed investigation on the NER\ncapabilities of different types of mLMs. Finally, we demonstrate the utility of\nour NER system on a low-resource Neural Machine Translation (NMT) task. Our\ndataset is publicly released: https://github.com/suralk/multiNER.", "published": "2024-12-03 00:28:31", "link": "http://arxiv.org/abs/2412.02056v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Let's Think Var-by-Var: Large Language Models Enable Ad Hoc\n  Probabilistic Reasoning", "abstract": "A hallmark of intelligence is the ability to flesh out underspecified\nsituations using \"common sense.\" We propose to extract that common sense from\nlarge language models (LLMs), in a form that can feed into probabilistic\ninference. We focus our investigation on $\\textit{guesstimation}$ questions\nsuch as \"How much are Airbnb listings in Newark, NJ?\" Formulating a sensible\nanswer without access to data requires drawing on, and integrating, bits of\ncommon knowledge about how $\\texttt{Price}$ and $\\texttt{Location}$ may relate\nto other variables, such as $\\texttt{Property Type}$. Our framework answers\nsuch a question by synthesizing an $\\textit{ad hoc}$ probabilistic model. First\nwe prompt an LLM to propose a set of random variables relevant to the question,\nfollowed by moment constraints on their joint distribution. We then optimize\nthe joint distribution $p$ within a log-linear family to maximize the overall\nconstraint satisfaction. Our experiments show that LLMs can successfully be\nprompted to propose reasonable variables, and while the proposed numerical\nconstraints can be noisy, jointly optimizing for their satisfaction reconciles\nthem. When evaluated on probabilistic questions derived from three real-world\ntabular datasets, we find that our framework performs comparably to a direct\nprompting baseline in terms of total variation distance from the dataset\ndistribution, and is similarly robust to noise.", "published": "2024-12-03 01:53:06", "link": "http://arxiv.org/abs/2412.02081v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Language Transfer Capability of Decoder-only Architecture in\n  Multilingual Neural Machine Translation", "abstract": "Existing multilingual neural machine translation (MNMT) approaches mainly\nfocus on improving models with the encoder-decoder architecture to translate\nmultiple languages. However, decoder-only architecture has been explored less\nin MNMT due to its underperformance when trained on parallel data solely. In\nthis work, we attribute the issue of the decoder-only architecture to its lack\nof language transfer capability. Specifically, the decoder-only architecture is\ninsufficient in encoding source tokens with the target language features. We\npropose dividing the decoding process into two stages so that target tokens are\nexplicitly excluded in the first stage to implicitly boost the transfer\ncapability across languages. Additionally, we impose contrastive learning on\ntranslation instructions, resulting in improved performance in zero-shot\ntranslation. We conduct experiments on TED-19 and OPUS-100 datasets,\nconsidering both training from scratch and fine-tuning scenarios. Experimental\nresults show that, compared to the encoder-decoder architecture, our methods\nnot only perform competitively in supervised translations but also achieve\nimprovements of up to 3.39 BLEU, 6.99 chrF++, 3.22 BERTScore, and 4.81 COMET in\nzero-shot translations.", "published": "2024-12-03 02:52:14", "link": "http://arxiv.org/abs/2412.02101v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Explainable and Interpretable Multimodal Large Language Models: A\n  Comprehensive Survey", "abstract": "The rapid development of Artificial Intelligence (AI) has revolutionized\nnumerous fields, with large language models (LLMs) and computer vision (CV)\nsystems driving advancements in natural language understanding and visual\nprocessing, respectively. The convergence of these technologies has catalyzed\nthe rise of multimodal AI, enabling richer, cross-modal understanding that\nspans text, vision, audio, and video modalities. Multimodal large language\nmodels (MLLMs), in particular, have emerged as a powerful framework,\ndemonstrating impressive capabilities in tasks like image-text generation,\nvisual question answering, and cross-modal retrieval. Despite these\nadvancements, the complexity and scale of MLLMs introduce significant\nchallenges in interpretability and explainability, essential for establishing\ntransparency, trustworthiness, and reliability in high-stakes applications.\nThis paper provides a comprehensive survey on the interpretability and\nexplainability of MLLMs, proposing a novel framework that categorizes existing\nresearch across three perspectives: (I) Data, (II) Model, (III) Training \\&\nInference. We systematically analyze interpretability from token-level to\nembedding-level representations, assess approaches related to both architecture\nanalysis and design, and explore training and inference strategies that enhance\ntransparency. By comparing various methodologies, we identify their strengths\nand limitations and propose future research directions to address unresolved\nchallenges in multimodal explainability. This survey offers a foundational\nresource for advancing interpretability and transparency in MLLMs, guiding\nresearchers and practitioners toward developing more accountable and robust\nmultimodal AI systems.", "published": "2024-12-03 02:54:31", "link": "http://arxiv.org/abs/2412.02104v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Misalignment of Semantic Relation Knowledge between WordNet and Human\n  Intuition", "abstract": "WordNet provides a carefully constructed repository of semantic relations,\ncreated by specialists. But there is another source of information on semantic\nrelations, the intuition of language users. We present the first systematic\nstudy of the degree to which these two sources are aligned. Investigating the\ncases of misalignment could make proper use of WordNet and facilitate its\nimprovement. Our analysis which uses templates to elicit responses from human\nparticipants, reveals a general misalignment of semantic relation knowledge\nbetween WordNet and human intuition. Further analyses find a systematic pattern\nof mismatch among synonymy and taxonomic relations~(hypernymy and hyponymy),\ntogether with the fact that WordNet path length does not serve as a reliable\nindicator of human intuition regarding hypernymy or hyponymy relations.", "published": "2024-12-03 03:51:31", "link": "http://arxiv.org/abs/2412.02138v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Compressing KV Cache for Long-Context LLM Inference with Inter-Layer\n  Attention Similarity", "abstract": "The increasing context window size in Large Language Models (LLMs), such as\nthe GPT and LLaMA series, has improved their ability to tackle complex,\nlong-text tasks, but at the cost of inference efficiency, particularly\nregarding memory and computational complexity. Existing methods, including\nselective token retention and window-based attention, improve efficiency but\nrisk discarding important tokens needed for future text generation. In this\npaper, we propose an approach that enhances LLM efficiency without token loss\nby reducing the memory and computational load of less important tokens, rather\nthan discarding them.We address two challenges: 1) investigating the\ndistribution of important tokens in the context, discovering recent tokens are\nmore important than distant tokens in context, and 2) optimizing resources for\ndistant tokens by sharing attention scores across layers. The experiments show\nthat our method saves $35\\%$ KV cache without compromising the performance.", "published": "2024-12-03 08:29:27", "link": "http://arxiv.org/abs/2412.02252v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MediaSpin: Exploring Media Bias Through Fine-Grained Analysis of News\n  Headlines", "abstract": "In this paper, we introduce the MediaSpin dataset aiming to help in the\ndevelopment of models that can detect different forms of media bias present in\nnews headlines, developed through human-supervised and -validated Large\nLanguage Model (LLM) labeling of media bias. This corpus comprises 78,910 pairs\nof news headlines and annotations with explanations of the 13 distinct types of\nmedia bias categories assigned. We demonstrate the usefulness of our dataset\nfor automated bias detection in news edits.", "published": "2024-12-03 08:41:13", "link": "http://arxiv.org/abs/2412.02271v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can ChatGPT capture swearing nuances? Evidence from translating Arabic\n  oaths", "abstract": "This study sets out to answer one major question: Can ChatGPT capture\nswearing nuances? It presents an empirical study on the ability of ChatGPT to\ntranslate Arabic oath expressions into English. 30 Arabic oath expressions were\ncollected from the literature. These 30 oaths were first translated via ChatGPT\nand then analyzed and compared to the human translation in terms of types of\ngaps left unfulfilled by ChatGPT. Specifically, the gaps involved are:\nreligious gap, cultural gap, both religious and cultural gaps, no gap, using\nnon-oath particles, redundancy and noncapturing of Arabic script diacritics. It\nconcludes that ChatGPT translation of oaths is still much unsatisfactory,\nunveiling the need of further developments of ChatGPT, and the inclusion of\nArabic data on which ChatGPT should be trained including oath expressions, oath\nnuances, rituals, and practices.", "published": "2024-12-03 14:09:40", "link": "http://arxiv.org/abs/2412.02466v2", "categories": ["cs.CL", "cs-CL", "F.2.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "Patent-CR: A Dataset for Patent Claim Revision", "abstract": "This paper presents Patent-CR, the first dataset created for the patent claim\nrevision task in English. It includes both initial patent applications rejected\nby patent examiners and the final granted versions. Unlike normal text revision\ntasks that predominantly focus on enhancing sentence quality, such as grammar\ncorrection and coherence improvement, patent claim revision aims at ensuring\nthe claims meet stringent legal criteria. These criteria are beyond novelty and\ninventiveness, including clarity of scope, technical accuracy, language\nprecision, and legal robustness. We assess various large language models (LLMs)\nthrough professional human evaluation, including general LLMs with different\nsizes and architectures, text revision models, and domain-specific models. Our\nresults indicate that LLMs often bring ineffective edits that deviate from the\ntarget revisions. In addition, domain-specific models and the method of\nfine-tuning show promising results. Notably, GPT-4 outperforms other tested\nLLMs, but further revisions are still necessary to reach the examination\nstandard. Furthermore, we demonstrate the inconsistency between automated and\nhuman evaluation results, suggesting that GPT-4-based automated evaluation has\nthe highest correlation with human judgment. This dataset, along with our\npreliminary empirical research, offers invaluable insights for further\nexploration in patent claim revision.", "published": "2024-12-03 16:43:42", "link": "http://arxiv.org/abs/2412.02549v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Nemotron-CC: Transforming Common Crawl into a Refined Long-Horizon\n  Pretraining Dataset", "abstract": "Recent English Common Crawl datasets like FineWeb-Edu and DCLM achieved\nsignificant benchmark gains via aggressive model-based filtering, but at the\ncost of removing 90% of data. This limits their suitability for long token\nhorizon training, such as 15T tokens for Llama 3.1. In this paper, we show how\nto achieve better trade-offs between accuracy and data quantity by a\ncombination of classifier ensembling, synthetic data rephrasing, and reduced\nreliance on heuristic filters. When training 8B parameter models for 1T tokens,\nusing a high-quality subset of our data improves MMLU by 5.6 over DCLM,\ndemonstrating the efficacy of our methods for boosting accuracies over a\nrelatively short token horizon. Furthermore, our full 6.3T token dataset\nmatches DCLM on MMLU, but contains four times more unique real tokens than\nDCLM. This unlocks state-of-the-art training over a long token horizon: an 8B\nparameter model trained for 15T tokens, of which 7.2T came from our dataset, is\nbetter than the Llama 3.1 8B model: +5 on MMLU, +3.1 on ARC-Challenge, and +0.5\non average across ten diverse tasks. The dataset is available at\nhttps://data.commoncrawl.org/contrib/Nemotron/Nemotron-CC/index.html", "published": "2024-12-03 17:28:50", "link": "http://arxiv.org/abs/2412.02595v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RARE: Retrieval-Augmented Reasoning Enhancement for Large Language\n  Models", "abstract": "This work introduces RARE (Retrieval-Augmented Reasoning Enhancement), a\nversatile extension to the mutual reasoning framework (rStar), aimed at\nenhancing reasoning accuracy and factual integrity across large language models\n(LLMs) for complex, knowledge-intensive tasks such as commonsense and medical\nreasoning. RARE incorporates two innovative actions within the Monte Carlo Tree\nSearch (MCTS) framework: A6, which generates search queries based on the\ninitial problem statement, performs information retrieval using those queries,\nand augments reasoning with the retrieved data to formulate the final answer;\nand A7, which leverages information retrieval specifically for generated\nsub-questions and re-answers these sub-questions with the relevant contextual\ninformation. Additionally, a Retrieval-Augmented Factuality Scorer is proposed\nto replace the original discriminator, prioritizing reasoning paths that meet\nhigh standards of factuality. Experimental results with LLaMA 3.1 show that\nRARE enables open-source LLMs to achieve competitive performance with top\nopen-source models like GPT-4 and GPT-4o. This research establishes RARE as a\nscalable solution for improving LLMs in domains where logical coherence and\nfactual integrity are critical.", "published": "2024-12-03 20:52:35", "link": "http://arxiv.org/abs/2412.02830v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BN-AuthProf: Benchmarking Machine Learning for Bangla Author Profiling\n  on Social Media Texts", "abstract": "Author profiling, the analysis of texts to uncover attributes such as gender\nand age of the author, has become essential with the widespread use of social\nmedia platforms. This paper focuses on author profiling in the Bangla language,\naiming to extract valuable insights about anonymous authors based on their\nwriting style on social media. The primary objective is to introduce and\nbenchmark the performance of machine learning approaches on a newly created\nBangla Author Profiling dataset, BN-AuthProf. The dataset comprises 30,131\nsocial media posts from 300 authors, labeled by their age and gender. Authors'\nidentities and sensitive information were anonymized to ensure privacy. Various\nclassical machine learning and deep learning techniques were employed to\nevaluate the dataset. For gender classification, the best accuracy achieved was\n80% using Support Vector Machine (SVM), while a Multinomial Naive Bayes (MNB)\nclassifier achieved the best F1 score of 0.756. For age classification, MNB\nattained a maximum accuracy score of 91% with an F1 score of 0.905. This\nresearch highlights the effectiveness of machine learning in gender and age\nclassification for Bangla author profiling, with practical implications\nspanning marketing, security, forensic linguistics, education, and criminal\ninvestigations, considering privacy and biases.", "published": "2024-12-03 00:32:32", "link": "http://arxiv.org/abs/2412.02058v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "WSI-LLaVA: A Multimodal Large Language Model for Whole Slide Image", "abstract": "Recent advancements in computational pathology have produced patch-level\nMulti-modal Large Language Models (MLLMs), but these models are limited by\ntheir inability to analyze whole slide images (WSIs) comprehensively and their\ntendency to bypass crucial morphological features that pathologists rely on for\ndiagnosis. To address these challenges, we first introduce WSI-Bench, a\nlarge-scale morphology-aware benchmark containing 180k VQA pairs from 9,850\nWSIs across 30 cancer types, designed to evaluate MLLMs' understanding of\nmorphological characteristics crucial for accurate diagnosis. Building upon\nthis benchmark, we present WSI-LLaVA, a novel framework for gigapixel WSI\nunderstanding that employs a three-stage training approach: WSI-text alignment,\nfeature space alignment, and task-specific instruction tuning. To better assess\nmodel performance in pathological contexts, we develop two specialized WSI\nmetrics: WSI-Precision and WSI-Relevance. Experimental results demonstrate that\nWSI-LLaVA outperforms existing models across all capability dimensions, with a\nsignificant improvement in morphological analysis, establishing a clear\ncorrelation between morphological understanding and diagnostic accuracy.", "published": "2024-12-03 03:57:24", "link": "http://arxiv.org/abs/2412.02141v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Leveraging Large Language Models for Comparative Literature\n  Summarization with Reflective Incremental Mechanisms", "abstract": "In this paper, we introduce ChatCite, a novel method leveraging large\nlanguage models (LLMs) for generating comparative literature summaries. The\nability to summarize research papers with a focus on key comparisons between\nstudies is an essential task in academic research. Existing summarization\nmodels, while effective at generating concise summaries, fail to provide deep\ncomparative insights. ChatCite addresses this limitation by incorporating a\nmulti-step reasoning mechanism that extracts critical elements from papers,\nincrementally builds a comparative summary, and refines the output through a\nreflective memory process. We evaluate ChatCite on a custom dataset,\nCompLit-LongContext, consisting of 1000 research papers with annotated\ncomparative summaries. Experimental results show that ChatCite outperforms\nseveral baseline methods, including GPT-4, BART, T5, and CoT, across various\nautomatic evaluation metrics such as ROUGE and the newly proposed G-Score.\nHuman evaluation further confirms that ChatCite generates more coherent,\ninsightful, and fluent summaries compared to these baseline models. Our method\nprovides a significant advancement in automatic literature review generation,\noffering researchers a powerful tool for efficiently comparing and synthesizing\nscientific research.", "published": "2024-12-03 04:09:36", "link": "http://arxiv.org/abs/2412.02149v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "A Comprehensive Evaluation of Large Language Models on Aspect-Based\n  Sentiment Analysis", "abstract": "Recently, Large Language Models (LLMs) have garnered increasing attention in\nthe field of natural language processing, revolutionizing numerous downstream\ntasks with powerful reasoning and generation abilities. For example, In-Context\nLearning (ICL) introduces a fine-tuning-free paradigm, allowing out-of-the-box\nLLMs to execute downstream tasks by analogy learning without any fine-tuning.\nBesides, in a fine-tuning-dependent paradigm where substantial training data\nexists, Parameter-Efficient Fine-Tuning (PEFT), as the cost-effective methods,\nenable LLMs to achieve excellent performance comparable to full fine-tuning.\n  However, these fascinating techniques employed by LLMs have not been fully\nexploited in the ABSA field. Previous works probe LLMs in ABSA by merely using\nrandomly selected input-output pairs as demonstrations in ICL, resulting in an\nincomplete and superficial evaluation. In this paper, we shed light on a\ncomprehensive evaluation of LLMs in the ABSA field, involving 13 datasets, 8\nABSA subtasks, and 6 LLMs. Specifically, we design a unified task formulation\nto unify ``multiple LLMs for multiple ABSA subtasks in multiple paradigms.''\nFor the fine-tuning-dependent paradigm, we efficiently fine-tune LLMs using\ninstruction-based multi-task learning. For the fine-tuning-free paradigm, we\npropose 3 demonstration selection strategies to stimulate the few-shot\nabilities of LLMs. Our extensive experiments demonstrate that LLMs achieve a\nnew state-of-the-art performance compared to fine-tuned Small Language Models\n(SLMs) in the fine-tuning-dependent paradigm. More importantly, in the\nfine-tuning-free paradigm where SLMs are ineffective, LLMs with ICL still\nshowcase impressive potential and even compete with fine-tuned SLMs on some\nABSA subtasks.", "published": "2024-12-03 08:54:17", "link": "http://arxiv.org/abs/2412.02279v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Pay Attention to the Robustness of Chinese Minority Language Models!\n  Syllable-level Textual Adversarial Attack on Tibetan Script", "abstract": "The textual adversarial attack refers to an attack method in which the\nattacker adds imperceptible perturbations to the original texts by elaborate\ndesign so that the NLP (natural language processing) model produces false\njudgments. This method is also used to evaluate the robustness of NLP models.\nCurrently, most of the research in this field focuses on English, and there is\nalso a certain amount of research on Chinese. However, to the best of our\nknowledge, there is little research targeting Chinese minority languages.\nTextual adversarial attacks are a new challenge for the information processing\nof Chinese minority languages. In response to this situation, we propose a\nTibetan syllable-level black-box textual adversarial attack called TSAttacker\nbased on syllable cosine distance and scoring mechanism. And then, we conduct\nTSAttacker on six models generated by fine-tuning two PLMs (pre-trained\nlanguage models) for three downstream tasks. The experiment results show that\nTSAttacker is effective and generates high-quality adversarial samples. In\naddition, the robustness of the involved models still has much room for\nimprovement.", "published": "2024-12-03 09:38:22", "link": "http://arxiv.org/abs/2412.02323v2", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Multi-Granularity Tibetan Textual Adversarial Attack Method Based on\n  Masked Language Model", "abstract": "In social media, neural network models have been applied to hate speech\ndetection, sentiment analysis, etc., but neural network models are susceptible\nto adversarial attacks. For instance, in a text classification task, the\nattacker elaborately introduces perturbations to the original texts that hardly\nalter the original semantics in order to trick the model into making different\npredictions. By studying textual adversarial attack methods, the robustness of\nlanguage models can be evaluated and then improved. Currently, most of the\nresearch in this field focuses on English, and there is also a certain amount\nof research on Chinese. However, there is little research targeting Chinese\nminority languages. With the rapid development of artificial intelligence\ntechnology and the emergence of Chinese minority language models, textual\nadversarial attacks become a new challenge for the information processing of\nChinese minority languages. In response to this situation, we propose a\nmulti-granularity Tibetan textual adversarial attack method based on masked\nlanguage models called TSTricker. We utilize the masked language models to\ngenerate candidate substitution syllables or words, adopt the scoring mechanism\nto determine the substitution order, and then conduct the attack method on\nseveral fine-tuned victim models. The experimental results show that TSTricker\nreduces the accuracy of the classification models by more than 28.70% and makes\nthe classification models change the predictions of more than 90.60% of the\nsamples, which has an evidently higher attack effect than the baseline method.", "published": "2024-12-03 10:03:52", "link": "http://arxiv.org/abs/2412.02343v1", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "The Impact of Featuring Comments in Online Discussions", "abstract": "A widespread moderation strategy by online news platforms is to feature what\nthe platform deems high quality comments, usually called editor picks or\nfeatured comments. In this paper, we compare online discussions of news\narticles in which certain comments are featured, versus discussions in which no\ncomments are featured. We measure the impact of featuring comments on the\ndiscussion, by estimating and comparing the quality of discussions from the\nperspective of the user base and the platform itself. Our analysis shows that\nthe impact on discussion quality is limited. However, we do observe an increase\nin discussion activity after the first comments are featured by moderators,\nsuggesting that the moderation strategy might be used to increase user\nengagement and to postpone the natural decline in user activity over time.", "published": "2024-12-03 10:53:22", "link": "http://arxiv.org/abs/2412.02369v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "TSCheater: Generating High-Quality Tibetan Adversarial Texts via Visual\n  Similarity", "abstract": "Language models based on deep neural networks are vulnerable to textual\nadversarial attacks. While rich-resource languages like English are receiving\nfocused attention, Tibetan, a cross-border language, is gradually being studied\ndue to its abundant ancient literature and critical language strategy.\nCurrently, there are several Tibetan adversarial text generation methods, but\nthey do not fully consider the textual features of Tibetan script and\noverestimate the quality of generated adversarial texts. To address this issue,\nwe propose a novel Tibetan adversarial text generation method called TSCheater,\nwhich considers the characteristic of Tibetan encoding and the feature that\nvisually similar syllables have similar semantics. This method can also be\ntransferred to other abugidas, such as Devanagari script. We utilize a\nself-constructed Tibetan syllable visual similarity database called TSVSDB to\ngenerate substitution candidates and adopt a greedy algorithm-based scoring\nmechanism to determine substitution order. After that, we conduct the method on\neight victim language models. Experimentally, TSCheater outperforms existing\nmethods in attack effectiveness, perturbation magnitude, semantic similarity,\nvisual similarity, and human acceptance. Finally, we construct the first\nTibetan adversarial robustness evaluation benchmark called AdvTS, which is\ngenerated by existing methods and proofread by humans.", "published": "2024-12-03 10:57:19", "link": "http://arxiv.org/abs/2412.02371v3", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Four Guiding Principles for Modeling Causal Domain Knowledge: A Case\n  Study on Brainstorming Approaches for Urban Blight Analysis", "abstract": "Urban blight is a problem of high interest for planning and policy making.\nResearchers frequently propose theories about the relationships between urban\nblight indicators, focusing on relationships reflecting causality. In this\npaper, we improve on the integration of domain knowledge in the analysis of\nurban blight by introducing four rules for effective modeling of causal domain\nknowledge. The findings of this study reveal significant deviation from causal\nmodeling guidelines by investigating cognitive maps developed for urban blight\nanalysis. These findings provide valuable insights that will inform future work\non urban blight, ultimately enhancing our understanding of urban blight complex\ninteractions.", "published": "2024-12-03 11:49:34", "link": "http://arxiv.org/abs/2412.02400v1", "categories": ["cs.CE", "cs.CL"], "primary_category": "cs.CE"}
{"title": "GerPS-Compare: Comparing NER methods for legal norm analysis", "abstract": "We apply NER to a particular sub-genre of legal texts in German: the genre of\nlegal norms regulating administrative processes in public service\nadministration. The analysis of such texts involves identifying stretches of\ntext that instantiate one of ten classes identified by public service\nadministration professionals. We investigate and compare three methods for\nperforming Named Entity Recognition (NER) to detect these classes: a Rule-based\nsystem, deep discriminative models, and a deep generative model. Our results\nshow that Deep Discriminative models outperform both the Rule-based system as\nwell as the Deep Generative model, the latter two roughly performing equally\nwell, outperforming each other in different classes. The main cause for this\nsomewhat surprising result is arguably the fact that the classes used in the\nanalysis are semantically and syntactically heterogeneous, in contrast to the\nclasses used in more standard NER tasks. Deep Discriminative models appear to\nbe better equipped for dealing with this heterogenerity than both generic LLMs\nand human linguists designing rule-based NER systems.", "published": "2024-12-03 12:46:06", "link": "http://arxiv.org/abs/2412.02427v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLMForecaster: Improving Seasonal Event Forecasts with Unstructured\n  Textual Data", "abstract": "Modern time-series forecasting models often fail to make full use of rich\nunstructured information about the time series themselves. This lack of proper\nconditioning can lead to obvious model failures; for example, models may be\nunaware of the details of a particular product, and hence fail to anticipate\nseasonal surges in customer demand in the lead up to major exogenous events\nlike holidays for clearly relevant products. To address this shortcoming, this\npaper introduces a novel forecast post-processor -- which we call LLMForecaster\n-- that fine-tunes large language models (LLMs) to incorporate unstructured\nsemantic and contextual information and historical data to improve the\nforecasts from an existing demand forecasting pipeline. In an industry-scale\nretail application, we demonstrate that our technique yields statistically\nsignificantly forecast improvements across several sets of products subject to\nholiday-driven demand surges.", "published": "2024-12-03 16:18:42", "link": "http://arxiv.org/abs/2412.02525v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Semantic Tokens in Retrieval Augmented Generation", "abstract": "Retrieval-Augmented Generation (RAG) architectures have recently garnered\nsignificant attention for their ability to improve truth grounding and\ncoherence in natural language processing tasks. However, the reliability of RAG\nsystems in producing accurate answers diminishes as the volume of data they\naccess increases. Even with smaller datasets, these systems occasionally fail\nto address simple queries. This issue arises from their dependence on\nstate-of-the-art large language models (LLMs), which can introduce uncertainty\ninto the system's outputs. In this work, I propose a novel Comparative RAG\nsystem that introduces an evaluator module to bridge the gap between\nprobabilistic RAG systems and deterministically verifiable responses. The\nevaluator compares external recommendations with the retrieved document chunks,\nadding a decision-making layer that enhances the system's reliability. This\napproach ensures that the chunks retrieved are both semantically relevant and\nlogically consistent with deterministic insights, thereby improving the\naccuracy and overall efficiency of RAG systems. This framework paves the way\nfor more reliable and scalable question-answering applications in domains\nrequiring high precision and verifiability.", "published": "2024-12-03 16:52:06", "link": "http://arxiv.org/abs/2412.02563v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Time-Reversal Provides Unsupervised Feedback to LLMs", "abstract": "Large Language Models (LLMs) are typically trained to predict in the forward\ndirection of time. However, recent works have shown that prompting these models\nto look back and critique their own generations can produce useful feedback.\nMotivated by this, we explore the question of whether LLMs can be empowered to\nthink (predict and score) backwards to provide unsupervised feedback that\ncomplements forward LLMs. Towards this, we introduce Time Reversed Language\nModels (TRLMs), which can score and generate queries when conditioned on\nresponses, effectively functioning in the reverse direction of time. Further,\nto effectively infer in the response to query direction, we pre-train and\nfine-tune a language model (TRLM-Ba) in the reverse token order from scratch.\nWe show empirically (and theoretically in a stylized setting) that\ntime-reversed models can indeed complement forward model predictions when used\nto score the query given response for re-ranking multiple forward generations.\nWe obtain up to 5\\% improvement on the widely used AlpacaEval Leaderboard over\nthe competent baseline of best-of-N re-ranking using self log-perplexity\nscores. We further show that TRLM scoring outperforms conventional forward\nscoring of response given query, resulting in significant gains in applications\nsuch as citation generation and passage retrieval. We next leverage the\ngenerative ability of TRLM to augment or provide unsupervised feedback to input\nsafety filters of LLMs, demonstrating a drastic reduction in false negative\nrate with negligible impact on false positive rates against several attacks\npublished on the popular JailbreakBench leaderboard.", "published": "2024-12-03 17:54:12", "link": "http://arxiv.org/abs/2412.02626v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Words and Action: Modeling Linguistic Leadership in #BlackLivesMatter\n  Communities", "abstract": "In this project, we describe a method of modeling semantic leadership across\na set of communities associated with the #BlackLivesMatter movement, which has\nbeen informed by qualitative research on the structure of social media and\nBlack Twitter in particular. We describe our bespoke approaches to\ntime-binning, community clustering, and connecting communities over time, as\nwell as our adaptation of state-of-the-art approaches to semantic change\ndetection and semantic leadership induction. We find substantial evidence of\nthe leadership role of BLM activists and progressives, as well as Black\ncelebrities. We also find evidence of the sustained engagement of the\nconservative community with this discourse, suggesting an alternative\nexplanation for how we arrived at the present moment, in which \"anti-woke\" and\n\"anti-CRT\" bills are being enacted nationwide.", "published": "2024-12-03 18:10:28", "link": "http://arxiv.org/abs/2412.02637v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "QA-TOOLBOX: Conversational Question-Answering for process task guidance\n  in manufacturing", "abstract": "In this work we explore utilizing LLMs for data augmentation for\nmanufacturing task guidance system. The dataset consists of representative\nsamples of interactions with technicians working in an advanced manufacturing\nsetting. The purpose of this work to explore the task, data augmentation for\nthe supported tasks and evaluating the performance of the existing LLMs. We\nobserve that that task is complex requiring understanding from procedure\nspecification documents, actions and objects sequenced temporally. The dataset\nconsists of 200,000+ question/answer pairs that refer to the spec document and\nare grounded in narrations and/or video demonstrations. We compared the\nperformance of several popular open-sourced LLMs by developing a baseline using\neach LLM and then compared the responses in a reference-free setting using\nLLM-as-a-judge and compared the ratings with crowd-workers whilst validating\nthe ratings with experts.", "published": "2024-12-03 18:10:31", "link": "http://arxiv.org/abs/2412.02638v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Probing the statistical properties of enriched co-occurrence networks", "abstract": "Recent studies have explored the addition of virtual edges to word\nco-occurrence networks using word embeddings to enhance graph representations,\nparticularly for short texts. While these enriched networks have demonstrated\nsome success, the impact of incorporating semantic edges into traditional\nco-occurrence networks remains uncertain. This study investigates two key\nstatistical properties of text-based network models. First, we assess whether\nnetwork metrics can effectively distinguish between meaningless and meaningful\ntexts. Second, we analyze whether these metrics are more sensitive to syntactic\nor semantic aspects of the text. Our results show that incorporating virtual\nedges can have positive and negative effects, depending on the specific network\nmetric. For instance, the informativeness of the average shortest path and\ncloseness centrality improves in short texts, while the clustering\ncoefficient's informativeness decreases as more virtual edges are added.\nAdditionally, we found that including stopwords affects the statistical\nproperties of enriched networks. Our results can serve as a guideline for\ndetermining which network metrics are most appropriate for specific\napplications, depending on the typical text size and the nature of the problem.", "published": "2024-12-03 18:38:14", "link": "http://arxiv.org/abs/2412.02664v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Mind the Gap: Examining the Self-Improvement Capabilities of Large\n  Language Models", "abstract": "Self-improvement is a mechanism in Large Language Model (LLM) pre-training,\npost-training and test-time inference. We explore a framework where the model\nverifies its own outputs, filters or reweights data based on this verification,\nand distills the filtered data. Despite several empirical successes, a\nfundamental understanding is still lacking. In this work, we initiate a\ncomprehensive, modular and controlled study on LLM self-improvement. We provide\na mathematical formulation for self-improvement, which is largely governed by a\nquantity which we formalize as the generation-verification gap. Through\nexperiments with various model families and tasks, we discover a scaling\nphenomenon of self-improvement -- a variant of the generation-verification gap\nscales monotonically with the model pre-training flops. We also examine when\nself-improvement is possible, an iterative self-improvement procedure, and ways\nto improve its performance. Our findings not only advance understanding of LLM\nself-improvement with practical implications, but also open numerous avenues\nfor future research into its capabilities and boundaries.", "published": "2024-12-03 18:47:26", "link": "http://arxiv.org/abs/2412.02674v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Hybrid-SQuAD: Hybrid Scholarly Question Answering Dataset", "abstract": "Existing Scholarly Question Answering (QA) methods typically target\nhomogeneous data sources, relying solely on either text or Knowledge Graphs\n(KGs). However, scholarly information often spans heterogeneous sources,\nnecessitating the development of QA systems that integrate information from\nmultiple heterogeneous data sources. To address this challenge, we introduce\nHybrid-SQuAD (Hybrid Scholarly Question Answering Dataset), a novel large-scale\nQA dataset designed to facilitate answering questions incorporating both text\nand KG facts. The dataset consists of 10.5K question-answer pairs generated by\na large language model, leveraging the KGs DBLP and SemOpenAlex alongside\ncorresponding text from Wikipedia. In addition, we propose a RAG-based baseline\nhybrid QA model, achieving an exact match score of 69.65 on the Hybrid-SQuAD\ntest set.", "published": "2024-12-03 19:37:00", "link": "http://arxiv.org/abs/2412.02788v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Evolutionary Large Language Model for Hallucination Mitigation", "abstract": "The emergence of LLMs, like ChatGPT and Gemini, has marked the modern era of\nartificial intelligence applications characterized by high-impact applications\ngenerating text, images, and videos. However, these models usually ensue with\none critical challenge called hallucination: confident presentation of\ninaccurate or fabricated information. This problem attracts serious concern\nwhen these models are applied to specialized domains, including healthcare and\nlaw, where the accuracy and preciseness of information are absolute conditions.\nIn this paper, we propose EvoLLMs, an innovative framework inspired by\nEvolutionary Computation, which automates the generation of high-quality\nQuestion-answering (QA) datasets while minimizing hallucinations. EvoLLMs\nemploys genetic algorithms, mimicking evolutionary processes like selection,\nvariation, and mutation, to guide LLMs in generating accurate, contextually\nrelevant question-answer pairs. Comparative analysis shows that EvoLLMs\nconsistently outperforms human-generated datasets in key metrics such as Depth,\nRelevance, and Coverage, while nearly matching human performance in mitigating\nhallucinations. These results highlight EvoLLMs as a robust and efficient\nsolution for QA dataset generation, significantly reducing the time and\nresources required for manual curation.", "published": "2024-12-03 19:40:13", "link": "http://arxiv.org/abs/2412.02790v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CNNSum: Exploring Long-Context Summarization with Large Language Models\n  in Chinese Novels", "abstract": "Large Language Models (LLMs) have been well-researched in various\nlong-context tasks. However, the scarcity of high-quality long-context\nsummarization datasets has hindered further advancements in this area. To\naddress this, we introduce CNNSum, a multi-scale long-context summarization\nbenchmark based on Chinese novels, featuring human-driven annotations, which\ncomprises four subsets totaling 695 samples, with lengths ranging from 16k to\n128k. We evaluate numerous LLMs and conduct detailed case analyses.\nFurthermore, we conduct extensive fine-tuning experiments to explore and\nimprove long-context summarization. In our study: (1) Advanced LLMs like GPT-4o\nmay still generate subjective commentary, leading to vague summaries. (2)\nCurrently, long-context summarization mainly relies on memory ability afforded\nby longer context lengths. The advantages of Large LLMs are hard to utilize,\nthus small LLMs are the most cost-effective. (3) Different prompt templates\npaired with various version models may cause large performance gaps. In further\nfine-tuning, these can be mitigated, and the Base version models perform\nbetter. (4) LLMs with RoPE-base scaled exhibit strong extrapolation potential;\nusing short-context data can significantly improve long-context summarization\nperformance. However, further applying other interpolation methods requires\ncareful selection. (5) CNNSum provides more reliable and insightful evaluation\nresults than other benchmarks. We release CNNSum to advance future research in\nthis field. https://github.com/CxsGhost/CNNSum", "published": "2024-12-03 20:35:57", "link": "http://arxiv.org/abs/2412.02819v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Minimization of Boolean Complexity in In-Context Concept Learning", "abstract": "What factors contribute to the relative success and corresponding\ndifficulties of in-context learning for Large Language Models (LLMs)? Drawing\non insights from the literature on human concept learning, we test LLMs on\ncarefully designed concept learning tasks, and show that task performance\nhighly correlates with the Boolean complexity of the concept. This suggests\nthat in-context learning exhibits a learning bias for simplicity in a way\nsimilar to humans.", "published": "2024-12-03 20:41:37", "link": "http://arxiv.org/abs/2412.02823v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MLD-EA: Check and Complete Narrative Coherence by Introducing Emotions\n  and Actions", "abstract": "Narrative understanding and story generation are critical challenges in\nnatural language processing (NLP), with much of the existing research focused\non summarization and question-answering tasks. While previous studies have\nexplored predicting plot endings and generating extended narratives, they often\nneglect the logical coherence within stories, leaving a significant gap in the\nfield. To address this, we introduce the Missing Logic Detector by Emotion and\nAction (MLD-EA) model, which leverages large language models (LLMs) to identify\nnarrative gaps and generate coherent sentences that integrate seamlessly with\nthe story's emotional and logical flow. The experimental results demonstrate\nthat the MLD-EA model enhances narrative understanding and story generation,\nhighlighting LLMs' potential as effective logic checkers in story writing with\nlogical coherence and emotional consistency. This work fills a gap in NLP\nresearch and advances border goals of creating more sophisticated and reliable\nstory-generation systems.", "published": "2024-12-03 23:01:21", "link": "http://arxiv.org/abs/2412.02897v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Single-Cell Omics Arena: A Benchmark Study for Large Language Models on\n  Cell Type Annotation Using Single-Cell Data", "abstract": "Over the past decade, the revolution in single-cell sequencing has enabled\nthe simultaneous molecular profiling of various modalities across thousands of\nindividual cells, allowing scientists to investigate the diverse functions of\ncomplex tissues and uncover underlying disease mechanisms. Among all the\nanalytical steps, assigning individual cells to specific types is fundamental\nfor understanding cellular heterogeneity. However, this process is usually\nlabor-intensive and requires extensive expert knowledge. Recent advances in\nlarge language models (LLMs) have demonstrated their ability to efficiently\nprocess and synthesize vast corpora of text to automatically extract essential\nbiological knowledge, such as marker genes, potentially promoting more\nefficient and automated cell type annotations. To thoroughly evaluate the\ncapability of modern instruction-tuned LLMs in automating the cell type\nidentification process, we introduce SOAR, a comprehensive benchmarking study\nof LLMs for cell type annotation tasks in single-cell genomics. Specifically,\nwe assess the performance of 8 instruction-tuned LLMs across 11 datasets,\nspanning multiple cell types and species. Our study explores the potential of\nLLMs to accurately classify and annotate cell types in single-cell RNA\nsequencing (scRNA-seq) data, while extending their application to multiomics\ndata through cross-modality translation. Additionally, we evaluate the\neffectiveness of chain-of-thought (CoT) prompting techniques in generating\ndetailed biological insights during the annotation process. The results\ndemonstrate that LLMs can provide robust interpretations of single-cell data\nwithout requiring additional fine-tuning, advancing the automation of cell type\nannotation in genomics research.", "published": "2024-12-03 23:58:35", "link": "http://arxiv.org/abs/2412.02915v1", "categories": ["cs.CL", "q-bio.GN"], "primary_category": "cs.CL"}
{"title": "CPTQuant - A Novel Mixed Precision Post-Training Quantization Techniques\n  for Large Language Models", "abstract": "Large language models have transformed the comprehension and generation of\nnatural language tasks, but they come with substantial memory and computational\nrequirements. Quantization techniques have emerged as a promising avenue for\naddressing these challenges while preserving accuracy and making energy\nefficient. We propose CPTQuant, a comprehensive strategy that introduces\ncorrelation-based (CMPQ), pruning-based (PMPQ), and Taylor decomposition-based\n(TDMPQ) mixed precision techniques. CMPQ adapts the precision level based on\ncanonical correlation analysis of different layers. PMPQ optimizes precision\nlayer-wise based on their sensitivity to sparsity. TDMPQ modifies precision\nusing Taylor decomposition to assess each layer's sensitivity to input\nperturbation. These strategies allocate higher precision to more sensitive\nlayers while diminishing precision to robust layers. CPTQuant assesses the\nperformance across BERT, OPT-125M, OPT-350M, OPT-1.3B, and OPT-2.7B. We\ndemonstrate up to 4x compression and a 2x-fold increase in efficiency with\nminimal accuracy drop compared to Hugging Face FP16. PMPQ stands out for\nachieving a considerably higher model compression. Sensitivity analyses across\nvarious LLMs show that the initial and final 30% of layers exhibit higher\nsensitivities than the remaining layers. PMPQ demonstrates an 11% higher\ncompression ratio than other methods for classification tasks, while TDMPQ\nachieves a 30% greater compression ratio for language modeling tasks.", "published": "2024-12-03 04:52:41", "link": "http://arxiv.org/abs/2412.03599v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Primer on Large Language Models and their Limitations", "abstract": "This paper provides a primer on Large Language Models (LLMs) and identifies\ntheir strengths, limitations, applications and research directions. It is\nintended to be useful to those in academia and industry who are interested in\ngaining an understanding of the key LLM concepts and technologies, and in\nutilising this knowledge in both day to day tasks and in more complex scenarios\nwhere this technology can enhance current practices and processes.", "published": "2024-12-03 02:45:02", "link": "http://arxiv.org/abs/2412.04503v1", "categories": ["cs.CL", "cs.AI", "68T50"], "primary_category": "cs.CL"}
{"title": "Impact of Data Snooping on Deep Learning Models for Locating\n  Vulnerabilities in Lifted Code", "abstract": "This study examines the impact of data snooping on neural networks used to\ndetect vulnerabilities in lifted code, and builds on previous research that\nused word2vec and unidirectional and bidirectional transformer-based\nembeddings. The research specifically focuses on how model performance is\naffected when embedding models are trained with datasets, which include samples\nused for neural network training and validation. The results show that\nintroducing data snooping did not significantly alter model performance,\nsuggesting that data snooping had a minimal impact or that samples randomly\ndropped as part of the methodology contained hidden features critical to\nachieving optimal performance. In addition, the findings reinforce the\nconclusions of previous research, which found that models trained with GPT-2\nembeddings consistently outperformed neural networks trained with other\nembeddings. The fact that this holds even when data snooping is introduced into\nthe embedding model indicates GPT-2's robustness in representing complex code\nfeatures, even under less-than-ideal conditions.", "published": "2024-12-03 00:08:01", "link": "http://arxiv.org/abs/2412.02048v2", "categories": ["cs.CR", "cs.CL", "cs.LG", "cs.SE", "D.4.6; I.2.6; I.5.1"], "primary_category": "cs.CR"}
{"title": "Personalized Multimodal Large Language Models: A Survey", "abstract": "Multimodal Large Language Models (MLLMs) have become increasingly important\ndue to their state-of-the-art performance and ability to integrate multiple\ndata modalities, such as text, images, and audio, to perform complex tasks with\nhigh accuracy. This paper presents a comprehensive survey on personalized\nmultimodal large language models, focusing on their architecture, training\nmethods, and applications. We propose an intuitive taxonomy for categorizing\nthe techniques used to personalize MLLMs to individual users, and discuss the\ntechniques accordingly. Furthermore, we discuss how such techniques can be\ncombined or adapted when appropriate, highlighting their advantages and\nunderlying rationale. We also provide a succinct summary of personalization\ntasks investigated in existing research, along with the evaluation metrics\ncommonly used. Additionally, we summarize the datasets that are useful for\nbenchmarking personalized MLLMs. Finally, we outline critical open challenges.\nThis survey aims to serve as a valuable resource for researchers and\npractitioners seeking to understand and advance the development of personalized\nmultimodal large language models.", "published": "2024-12-03 03:59:03", "link": "http://arxiv.org/abs/2412.02142v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.CV"}
{"title": "Jailbreak Defense in a Narrow Domain: Limitations of Existing Methods\n  and a New Transcript-Classifier Approach", "abstract": "Defending large language models against jailbreaks so that they never engage\nin a broadly-defined set of forbidden behaviors is an open problem. In this\npaper, we investigate the difficulty of jailbreak-defense when we only want to\nforbid a narrowly-defined set of behaviors. As a case study, we focus on\npreventing an LLM from helping a user make a bomb. We find that popular\ndefenses such as safety training, adversarial training, and input/output\nclassifiers are unable to fully solve this problem. In pursuit of a better\nsolution, we develop a transcript-classifier defense which outperforms the\nbaseline defenses we test. However, our classifier defense still fails in some\ncircumstances, which highlights the difficulty of jailbreak-defense even in a\nnarrow domain.", "published": "2024-12-03 04:34:58", "link": "http://arxiv.org/abs/2412.02159v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "A Theoretical Framework for Acoustic Neighbor Embeddings", "abstract": "This paper provides a theoretical framework for interpreting acoustic\nneighbor embeddings, which are representations of the phonetic content of\nvariable-width audio or text in a fixed-dimensional embedding space. A\nprobabilistic interpretation of the distances between embeddings is proposed,\nbased on a general quantitative definition of phonetic similarity between\nwords. This provides us a framework for understanding and applying the\nembeddings in a principled manner. Theoretical and empirical evidence to\nsupport an approximation of uniform cluster-wise isotropy are shown, which\nallows us to reduce the distances to simple Euclidean distances. Four\nexperiments that validate the framework and demonstrate how it can be applied\nto diverse problems are described. Nearest-neighbor search between audio and\ntext embeddings can give isolated word classification accuracy that is\nidentical to that of finite state transducers (FSTs) for vocabularies as large\nas 500k. Embedding distances give accuracy with 0.5% point difference compared\nto phone edit distances in out-of-vocabulary word recovery, as well as\nproducing clustering hierarchies identical to those derived from human\nlistening experiments in English dialect clustering. The theoretical framework\nalso allows us to use the embeddings to predict the expected confusion of\ndevice wake-up words. All source code and pretrained models are provided.", "published": "2024-12-03 04:48:59", "link": "http://arxiv.org/abs/2412.02164v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "VISCO: Benchmarking Fine-Grained Critique and Correction Towards\n  Self-Improvement in Visual Reasoning", "abstract": "The ability of large vision-language models (LVLMs) to critique and correct\ntheir reasoning is an essential building block towards their self-improvement.\nHowever, a systematic analysis of such capabilities in LVLMs is still lacking.\nWe propose VISCO, the first benchmark to extensively analyze the fine-grained\ncritique and correction capabilities of LVLMs. Compared to existing work that\nuses a single scalar value to critique the entire reasoning [4], VISCO features\ndense and fine-grained critique, requiring LVLMs to evaluate the correctness of\neach step in the chain-of-thought and provide natural language explanations to\nsupport their judgments. Extensive evaluation of 24 LVLMs demonstrates that\nhuman-written critiques significantly enhance the performance after correction,\nshowcasing the potential of the self-improvement strategy. However, the\nmodel-generated critiques are less helpful and sometimes detrimental to the\nperformance, suggesting that critique is the crucial bottleneck. We identified\nthree common patterns in critique failures: failure to critique visual\nperception, reluctance to \"say no\", and exaggerated assumption of error\npropagation. To address these issues, we propose an effective LookBack strategy\nthat revisits the image to verify each piece of information in the initial\nreasoning. LookBack significantly improves critique and correction performance\nby up to 13.5%.", "published": "2024-12-03 05:04:49", "link": "http://arxiv.org/abs/2412.02172v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "DataLab: A Unified Platform for LLM-Powered Business Intelligence", "abstract": "Business intelligence (BI) transforms large volumes of data within modern\norganizations into actionable insights for informed decision-making. Recently,\nlarge language model (LLM)-based agents have streamlined the BI workflow by\nautomatically performing task planning, reasoning, and actions in executable\nenvironments based on natural language (NL) queries. However, existing\napproaches primarily focus on individual BI tasks such as NL2SQL and NL2VIS.\nThe fragmentation of tasks across different data roles and tools lead to\ninefficiencies and potential errors due to the iterative and collaborative\nnature of BI. In this paper, we introduce DataLab, a unified BI platform that\nintegrates a one-stop LLM-based agent framework with an augmented computational\nnotebook interface. DataLab supports various BI tasks for different data roles\nin data preparation, analysis, and visualization by seamlessly combining LLM\nassistance with user customization within a single environment. To achieve this\nunification, we design a domain knowledge incorporation module tailored for\nenterprise-specific BI tasks, an inter-agent communication mechanism to\nfacilitate information sharing across the BI workflow, and a cell-based context\nmanagement strategy to enhance context utilization efficiency in BI notebooks.\nExtensive experiments demonstrate that DataLab achieves state-of-the-art\nperformance on various BI tasks across popular research benchmarks. Moreover,\nDataLab maintains high effectiveness and efficiency on real-world datasets from\nTencent, achieving up to a 58.58% increase in accuracy and a 61.65% reduction\nin token cost on enterprise-specific BI tasks.", "published": "2024-12-03 06:47:15", "link": "http://arxiv.org/abs/2412.02205v3", "categories": ["cs.DB", "cs.AI", "cs.CL"], "primary_category": "cs.DB"}
{"title": "BANER: Boundary-Aware LLMs for Few-Shot Named Entity Recognition", "abstract": "Despite the recent success of two-stage prototypical networks in few-shot\nnamed entity recognition (NER), challenges such as over/under-detected false\nspans in the span detection stage and unaligned entity prototypes in the type\nclassification stage persist. Additionally, LLMs have not proven to be\neffective few-shot information extractors in general. In this paper, we propose\nan approach called Boundary-Aware LLMs for Few-Shot Named Entity Recognition to\naddress these issues. We introduce a boundary-aware contrastive learning\nstrategy to enhance the LLM's ability to perceive entity boundaries for\ngeneralized entity spans. Additionally, we utilize LoRAHub to align information\nfrom the target domain to the source domain, thereby enhancing adaptive\ncross-domain classification capabilities. Extensive experiments across various\nbenchmarks demonstrate that our framework outperforms prior methods, validating\nits effectiveness. In particular, the proposed strategies demonstrate\neffectiveness across a range of LLM architectures. The code and data are\nreleased on https://github.com/UESTC-GQJ/BANER.", "published": "2024-12-03 07:51:14", "link": "http://arxiv.org/abs/2412.02228v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Characterizing Information Shared by Participants to Coding Challenges:\n  The Case of Advent of Code", "abstract": "Advent of Code (AoC from now on) is a popular coding challenge requiring to\nsolve programming puzzles for a variety of skill sets and levels. AoC follows\nthe advent calendar, therefore it is an annual challenge that lasts for 25\ndays. AoC participants usually post their solutions on social networks and\ndiscuss them online. These challenges are interesting to study since they could\nhighlight the adoption of new tools, the evolution of the developer community,\nor the technological requirements of well-known companies. For these reasons,\nwe first create a dataset of the 2019-2021 AoC editions containing the\ndiscussion threads made on the subreddit {\\tt /r/adventofcode}. Then, we\npropose a model based on stream graphs to best study this context, where we\nrepresent its most important actors through time: participants, comments, and\nprogramming languages. Thanks to our model, we investigate user participation,\nadoption of new programming languages during a challenge and between two of\nthem, and resiliency of programming languages based on a Stack Overflow survey.\nWe find that the top-used programming languages are almost the same in the\nthree years, pointing out their importance. Moreover, participants tend to keep\nthe same programming language for the whole challenge, while the ones attending\ntwo AoCs usually change it in the next one. Finally, we observe interesting\nresults about the programming languages that are ``Popular'' or ``Loved''\naccording to the Stack Overflow survey. Firstly, these are the ones adopted for\nthe longest time in an AoC edition, thanks to which users have a high chance of\nreaching the end of the challenge. Secondly, they are the most chosen when a\nparticipant decides to change programming language during the same challenge.", "published": "2024-12-03 09:07:13", "link": "http://arxiv.org/abs/2412.02290v1", "categories": ["cs.SI", "cs.CL", "cs.IR"], "primary_category": "cs.SI"}
{"title": "Large Multimodal Agents for Accurate Phishing Detection with Enhanced\n  Token Optimization and Cost Reduction", "abstract": "With the rise of sophisticated phishing attacks, there is a growing need for\neffective and economical detection solutions. This paper explores the use of\nlarge multimodal agents, specifically Gemini 1.5 Flash and GPT-4o mini, to\nanalyze both URLs and webpage screenshots via APIs, thus avoiding the\ncomplexities of training and maintaining AI systems. Our findings indicate that\nintegrating these two data types substantially enhances detection performance\nover using either type alone. However, API usage incurs costs per query that\ndepend on the number of input and output tokens. To address this, we propose a\ntwo-tiered agentic approach: initially, one agent assesses the URL, and if\ninconclusive, a second agent evaluates both the URL and the screenshot. This\nmethod not only maintains robust detection performance but also significantly\nreduces API costs by minimizing unnecessary multi-input queries. Cost analysis\nshows that with the agentic approach, GPT-4o mini can process about 4.2 times\nas many websites per $100 compared to the multimodal approach (107,440 vs.\n25,626), and Gemini 1.5 Flash can process about 2.6 times more websites\n(2,232,142 vs. 862,068). These findings underscore the significant economic\nbenefits of the agentic approach over the multimodal method, providing a viable\nsolution for organizations aiming to leverage advanced AI for phishing\ndetection while controlling expenses.", "published": "2024-12-03 09:13:52", "link": "http://arxiv.org/abs/2412.02301v1", "categories": ["cs.AI", "cs.CL", "cs.CR"], "primary_category": "cs.AI"}
{"title": "ScImage: How Good Are Multimodal Large Language Models at Scientific\n  Text-to-Image Generation?", "abstract": "Multimodal large language models (LLMs) have demonstrated impressive\ncapabilities in generating high-quality images from textual instructions.\nHowever, their performance in generating scientific images--a critical\napplication for accelerating scientific progress--remains underexplored. In\nthis work, we address this gap by introducing ScImage, a benchmark designed to\nevaluate the multimodal capabilities of LLMs in generating scientific images\nfrom textual descriptions. ScImage assesses three key dimensions of\nunderstanding: spatial, numeric, and attribute comprehension, as well as their\ncombinations, focusing on the relationships between scientific objects (e.g.,\nsquares, circles). We evaluate five models, GPT-4o, Llama, AutomaTikZ, Dall-E,\nand StableDiffusion, using two modes of output generation: code-based outputs\n(Python, TikZ) and direct raster image generation. Additionally, we examine\nfour different input languages: English, German, Farsi, and Chinese. Our\nevaluation, conducted with 11 scientists across three criteria (correctness,\nrelevance, and scientific accuracy), reveals that while GPT-4o produces outputs\nof decent quality for simpler prompts involving individual dimensions such as\nspatial, numeric, or attribute understanding in isolation, all models face\nchallenges in this task, especially for more complex prompts.", "published": "2024-12-03 10:52:06", "link": "http://arxiv.org/abs/2412.02368v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "Artificial Expert Intelligence through PAC-reasoning", "abstract": "Artificial Expert Intelligence (AEI) seeks to transcend the limitations of\nboth Artificial General Intelligence (AGI) and narrow AI by integrating\ndomain-specific expertise with critical, precise reasoning capabilities akin to\nthose of top human experts. Existing AI systems often excel at predefined tasks\nbut struggle with adaptability and precision in novel problem-solving. To\novercome this, AEI introduces a framework for ``Probably Approximately Correct\n(PAC) Reasoning\". This paradigm provides robust theoretical guarantees for\nreliably decomposing complex problems, with a practical mechanism for\ncontrolling reasoning precision. In reference to the division of human thought\ninto System 1 for intuitive thinking and System 2 for reflective\nreasoning~\\citep{tversky1974judgment}, we refer to this new type of reasoning\nas System 3 for precise reasoning, inspired by the rigor of the scientific\nmethod. AEI thus establishes a foundation for error-bounded, inference-time\nlearning.", "published": "2024-12-03 13:25:18", "link": "http://arxiv.org/abs/2412.02441v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Gracefully Filtering Backdoor Samples for Generative Large Language\n  Models without Retraining", "abstract": "Backdoor attacks remain significant security threats to generative large\nlanguage models (LLMs). Since generative LLMs output sequences of\nhigh-dimensional token logits instead of low-dimensional classification logits,\nmost existing backdoor defense methods designed for discriminative models like\nBERT are ineffective for generative LLMs. Inspired by the observed differences\nin learning behavior between backdoor and clean mapping in the frequency space,\nwe transform gradients of each training sample, directly influencing parameter\nupdates, into the frequency space. Our findings reveal a distinct separation\nbetween the gradients of backdoor and clean samples in the frequency space.\nBased on this phenomenon, we propose Gradient Clustering in the Frequency Space\nfor Backdoor Sample Filtering (GraCeFul), which leverages sample-wise gradients\nin the frequency space to effectively identify backdoor samples without\nrequiring retraining LLMs. Experimental results show that GraCeFul outperforms\nbaselines significantly. Notably, GraCeFul exhibits remarkable computational\nefficiency, achieving nearly 100% recall and F1 scores in identifying backdoor\nsamples, reducing the average success rate of various backdoor attacks to 0%\nwith negligible drops in clean accuracy across multiple free-style question\nanswering datasets. Additionally, GraCeFul generalizes to Llama-2 and Vicuna.\nThe codes are publicly available at https://github.com/ZrW00/GraceFul.", "published": "2024-12-03 13:43:36", "link": "http://arxiv.org/abs/2412.02454v1", "categories": ["cs.CL", "cs.AI", "cs.CR"], "primary_category": "cs.CL"}
{"title": "DP-2Stage: Adapting Language Models as Differentially Private Tabular\n  Data Generators", "abstract": "Generating tabular data under differential privacy (DP) protection ensures\ntheoretical privacy guarantees but poses challenges for training machine\nlearning models, primarily due to the need to capture complex structures under\nnoisy supervision signals. Recently, pre-trained Large Language Models (LLMs)\n-- even those at the scale of GPT-2 -- have demonstrated great potential in\nsynthesizing tabular data. However, their applications under DP constraints\nremain largely unexplored. In this work, we address this gap by applying DP\ntechniques to the generation of synthetic tabular data. Our findings shows that\nLLMs face difficulties in generating coherent text when fine-tuned with DP, as\nprivacy budgets are inefficiently allocated to non-private elements like table\nstructures. To overcome this, we propose \\ours, a two-stage fine-tuning\nframework for differentially private tabular data generation. The first stage\ninvolves non-private fine-tuning on a pseudo dataset, followed by DP\nfine-tuning on a private dataset. Our empirical results show that this approach\nimproves performance across various settings and metrics compared to directly\nfine-tuned LLMs in DP contexts. We release our code and setup at\nhttps://github.com/tejuafonja/DP-2Stage.", "published": "2024-12-03 14:10:09", "link": "http://arxiv.org/abs/2412.02467v1", "categories": ["cs.LG", "cs.CL", "cs.CR", "D.4.6; G.3; I.2.7"], "primary_category": "cs.LG"}
{"title": "CEGI: Measuring the trade-off between efficiency and carbon emissions\n  for SLMs and VLMs", "abstract": "This paper analyzes the performance of Small Language Models (SLMs) and\nVision Language Models (VLMs) and evaluates the trade-off between model\nperformance and carbon emissions across 4 essential tasks: Image Captioning,\nVisual Question Answering (VQA), Dialogue Summarization and Text-to-SQL\nconversion. Various SLMs and VLMs belonging to the Qwen and LLaMA architecture\nfamily are chosen and variants based on model size in terms of the number of\nparameters, quantization level and fine-tuning parameters are evaluated. The\nmodel variant's performance and carbon emissions are calculated. To quantify\nthe trade-off between model performance and carbon emissions, we introduce a\nnovel metric called CEGI (Carbon Efficient Gain Index). This metric represents\nthe carbon emission per unit percentage gain per million trainable parameters .\nThis metric provides a normalized measure to compare model's efficiency in\nterms of performance improvement relative to their environmental cost. The\nexperiment's outcome demonstrates that fine-tuning SLMs and VLMs can achieve\nperformance levels comparable to Large Language Models (LLMs) while producing\nsignificantly less carbon emissions. Our findings suggest that the marginal\ngains in accuracy from larger models do not justify the substantial increase in\ncarbon emissions. Leveraging lower-bit quantization levels, the proposed metric\nfurther enhances energy efficiency without compromising performance. This study\nhighlights balancing high performance and environmental sustainability. It\noffers a valuable metric for selecting models suitable for\nenvironmentally-friendly AI development.", "published": "2024-12-03 17:32:47", "link": "http://arxiv.org/abs/2412.02602v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Interpretable Company Similarity with Sparse Autoencoders", "abstract": "Determining company similarity is a vital task in finance, underpinning\nhedging, risk management, portfolio diversification, and more. Practitioners\noften rely on sector and industry classifications to gauge similarity, such as\nSIC-codes and GICS-codes - the former being used by the U.S. Securities and\nExchange Commission (SEC), and the latter widely used by the investment\ncommunity. Since these classifications can lack granularity and often need to\nbe updated, using clusters of embeddings of company descriptions has been\nproposed as a potential alternative, but the lack of interpretability in token\nembeddings poses a significant barrier to adoption in high-stakes contexts.\nSparse Autoencoders (SAEs) have shown promise in enhancing the interpretability\nof Large Language Models (LLMs) by decomposing LLM activations into\ninterpretable features. We apply SAEs to company descriptions, obtaining\nmeaningful clusters of equities in the process. We benchmark SAE features\nagainst SIC-codes, Major Group codes, and Embeddings. Our results demonstrate\nthat SAE features not only replicate but often surpass sector classifications\nand embeddings in capturing fundamental company characteristics. This is\nevidenced by their superior performance in correlating monthly returns - a\nproxy for similarity - and generating higher Sharpe ratio co-integration\nstrategies, which underscores deeper fundamental similarities among companies.", "published": "2024-12-03 17:34:50", "link": "http://arxiv.org/abs/2412.02605v2", "categories": ["cs.CL", "cs.LG", "econ.GN", "q-fin.EC"], "primary_category": "cs.CL"}
{"title": "GLM-4-Voice: Towards Intelligent and Human-Like End-to-End Spoken\n  Chatbot", "abstract": "We introduce GLM-4-Voice, an intelligent and human-like end-to-end spoken\nchatbot. It supports both Chinese and English, engages in real-time voice\nconversations, and varies vocal nuances such as emotion, intonation, speech\nrate, and dialect according to user instructions. GLM-4-Voice uses an ultra-low\nbitrate (175bps), single-codebook speech tokenizer with 12.5Hz frame rate\nderived from an automatic speech recognition (ASR) model by incorporating a\nvector-quantized bottleneck into the encoder. To efficiently transfer knowledge\nfrom text to speech modalities, we synthesize speech-text interleaved data from\nexisting text pre-training corpora using a text-to-token model. We continue\npre-training from the pre-trained text language model GLM-4-9B with a\ncombination of unsupervised speech data, interleaved speech-text data, and\nsupervised speech-text data, scaling up to 1 trillion tokens, achieving\nstate-of-the-art performance in both speech language modeling and spoken\nquestion answering. We then fine-tune the pre-trained model with high-quality\nconversational speech data, achieving superior performance compared to existing\nbaselines in both conversational ability and speech quality. The open models\ncan be accessed through https://github.com/THUDM/GLM-4-Voice and\nhttps://huggingface.co/THUDM/glm-4-voice-9b.", "published": "2024-12-03 17:41:24", "link": "http://arxiv.org/abs/2412.02612v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "T-REG: Preference Optimization with Token-Level Reward Regularization", "abstract": "Reinforcement learning from human feedback (RLHF) has been crucial in\naligning large language models (LLMs) with human values. Traditionally, RLHF\ninvolves generating responses to a query and using a reward model to assign a\nreward to the entire response. However, this approach faces challenges due to\nits reliance on a single, sparse reward, which makes it challenging for the\nmodel to identify which parts of the sequence contribute most significantly to\nthe final reward. Recent methods have attempted to address this limitation by\nintroducing token-level rewards. However, these methods often rely on either a\ntrained credit assignment model or AI annotators, raising concerns about the\nquality and reliability of the rewards. In this paper, we propose token-level\nreward regularization (T-REG), a novel approach that leverages both\nsequence-level and token-level rewards for preference optimization. Harnessing\nthe self-refinement capabilities of LLMs, our method uses contrastive prompting\nto enable LLMs to self-generate token-level rewards. These self-generated\nrewards then act as reward regularization, guiding the model to more\neffectively distribute sequence-level rewards across tokens. This facilitates\nbetter token-level credit assignment and enhances alignment performance.\nExperiments on the instruction following benchmarks, including Alpaca Eval 2\nand Arena-Hard, show that our method consistently outperforms baseline methods\nby up to 3.8% and 4.4%, respectively. We will release the code and models at\nhttps://github.com/wzhouad/T-REG.", "published": "2024-12-03 18:56:07", "link": "http://arxiv.org/abs/2412.02685v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Scaling BERT Models for Turkish Automatic Punctuation and Capitalization\n  Correction", "abstract": "This paper investigates the effectiveness of BERT based models for automated\npunctuation and capitalization corrections in Turkish texts across five\ndistinct model sizes. The models are designated as Tiny, Mini, Small, Medium,\nand Base. The design and capabilities of each model are tailored to address the\nspecific challenges of the Turkish language, with a focus on optimizing\nperformance while minimizing computational overhead. The study presents a\nsystematic comparison of the performance metrics precision, recall, and F1\nscore of each model, offering insights into their applicability in diverse\noperational contexts. The results demonstrate a significant improvement in text\nreadability and accuracy as model size increases, with the Base model achieving\nthe highest correction precision. This research provides a comprehensive guide\nfor selecting the appropriate model size based on specific user needs and\ncomputational resources, establishing a framework for deploying these models in\nreal-world applications to enhance the quality of written Turkish.", "published": "2024-12-03 18:59:51", "link": "http://arxiv.org/abs/2412.02698v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cosmos-LLaVA: Chatting with the Visual Cosmos-LLaVA: G\u00f6rselle Sohbet\n  Etmek", "abstract": "In this study, a Turkish visual instruction model was developed and various\nmodel architectures and dataset combinations were analysed to improve the\nperformance of this model. The Cosmos-LLaVA model, which is built by combining\ndifferent large language models and image coders, is designed to overcome the\ndeficiencies in the Turkish language. In the experiments, the effects of\nfine-tuning with various datasets on the model performance are analysed in\ndetail. The results show that model architecture and dataset selection have a\nsignificant impact on performance.\n  Bu \\c{c}al{\\i}\\c{s}mada bir T\\\"urk\\c{c}e g\\\"orsel talimat modeli\ngeli\\c{s}tirilerek bu modelin performans{\\i}n{\\i} art{\\i}rmaya y\\\"onelik\n\\c{c}e\\c{s}itli model mimarileri ve veri k\\\"umesi kombinasyonlar{\\i}\nderinlemesine incelenmi\\c{s}tir. Farkl{\\i} b\\\"uy\\\"uk dil modelleri ve\ng\\\"or\\\"unt\\\"u kodlay{\\i}c{\\i}lar{\\i}n{\\i}n bir araya getirilmesiyle\nolu\\c{s}turulan Cosmos-LLaVA modeli, T\\\"urk\\c{c}e dilindeki eksiklikleri\ngidermeye y\\\"onelik olarak tasarlanm{\\i}\\c{s}t{\\i}r. Yap{\\i}lan deneylerde,\n\\c{c}e\\c{s}itli veri k\\\"umeleri ile yap{\\i}lan ince ayarlar{\\i}n model\nperformans{\\i}n{\\i} nas{\\i}l etkiledi\\u{g}i detayl{\\i} olarak ele\nal{\\i}nm{\\i}\\c{s}t{\\i}r. Sonu\\c{c}lar, model mimarisi ve veri k\\\"umesi\nse\\c{c}iminin performans \\\"uzerinde \\\"onemli bir etkiye sahip oldu\\u{g}unu\ng\\\"ostermektedir.", "published": "2024-12-03 19:01:00", "link": "http://arxiv.org/abs/2412.02760v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Optimizing Large Language Models for Turkish: New Methodologies in\n  Corpus Selection and Training", "abstract": "In this study, we develop and assess new corpus selection and training\nmethodologies to improve the effectiveness of Turkish language models.\nSpecifically, we adapted Large Language Model generated datasets and translated\nEnglish datasets into Turkish, integrating these resources into the training\nprocess. This approach led to substantial enhancements in model accuracy for\nboth few-shot and zero-shot learning scenarios. Furthermore, the merging of\nthese adapted models was found to markedly improve their performance. Human\nevaluative metrics, including task-specific performance assessments, further\ndemonstrated that these adapted models possess a greater aptitude for\ncomprehending the Turkish language and addressing logic-based queries. This\nresearch underscores the importance of refining corpus selection strategies to\noptimize the performance of multilingual models, particularly for\nunder-resourced languages like Turkish.", "published": "2024-12-03 19:17:18", "link": "http://arxiv.org/abs/2412.02775v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CAISSON: Concept-Augmented Inference Suite of Self-Organizing Neural\n  Networks", "abstract": "We present CAISSON, a novel hierarchical approach to Retrieval-Augmented\nGeneration (RAG) that transforms traditional single-vector search into a\nmulti-view clustering framework. At its core, CAISSON leverages dual\nSelf-Organizing Maps (SOMs) to create complementary organizational views of the\ndocument space, where each view captures different aspects of document\nrelationships through specialized embeddings. The first view processes combined\ntext and metadata embeddings, while the second operates on metadata enriched\nwith concept embeddings, enabling a comprehensive multi-view analysis that\ncaptures both fine-grained semantic relationships and high-level conceptual\npatterns. This dual-view approach enables more nuanced document discovery by\ncombining evidence from different organizational perspectives. To evaluate\nCAISSON, we develop SynFAQA, a framework for generating synthetic financial\nanalyst notes and question-answer pairs that systematically tests different\naspects of information retrieval capabilities. Drawing on HotPotQA's\nmethodology for constructing multi-step reasoning questions, SynFAQA generates\ncontrolled test cases where each question is paired with the set of notes\ncontaining its ground-truth answer, progressing from simple single-entity\nqueries to complex multi-hop retrieval tasks involving multiple entities and\nconcepts. Our experimental results demonstrate substantial improvements over\nboth basic and enhanced RAG implementations, particularly for complex\nmulti-entity queries, while maintaining practical response times suitable for\ninteractive applications.", "published": "2024-12-03 21:00:10", "link": "http://arxiv.org/abs/2412.02835v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TDD-Bench Verified: Can LLMs Generate Tests for Issues Before They Get\n  Resolved?", "abstract": "Test-driven development (TDD) is the practice of writing tests first and\ncoding later, and the proponents of TDD expound its numerous benefits. For\ninstance, given an issue on a source code repository, tests can clarify the\ndesired behavior among stake-holders before anyone writes code for the\nagreed-upon fix. Although there has been a lot of work on automated test\ngeneration for the practice \"write code first, test later\", there has been\nlittle such automation for TDD. Ideally, tests for TDD should be fail-to-pass\n(i.e., fail before the issue is resolved and pass after) and have good adequacy\nwith respect to covering the code changed during issue resolution. This paper\nintroduces TDD-Bench Verified, a high-quality benchmark suite of 449 issues\nmined from real-world GitHub code repositories. The benchmark's evaluation\nharness runs only relevant tests in isolation for simple yet accurate coverage\nmeasurements, and the benchmark's dataset is filtered both by human judges and\nby execution in the harness. This paper also presents Auto-TDD, an LLM-based\nsolution that takes as input an issue description and a codebase (prior to\nissue resolution) and returns as output a test that can be used to validate the\nchanges made for resolving the issue. Our evaluation shows that Auto-TDD yields\na better fail-to-pass rate than the strongest prior work while also yielding\nhigh coverage adequacy. Overall, we hope that this work helps make developers\nmore productive at resolving issues while simultaneously leading to more robust\nfixes.", "published": "2024-12-03 22:38:05", "link": "http://arxiv.org/abs/2412.02883v1", "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Enhancing Trust in Large Language Models with Uncertainty-Aware\n  Fine-Tuning", "abstract": "Large language models (LLMs) have revolutionized the field of natural\nlanguage processing with their impressive reasoning and question-answering\ncapabilities. However, these models are sometimes prone to generating\ncredible-sounding but incorrect information, a phenomenon known as LLM\nhallucinations. Reliable uncertainty estimation in LLMs is essential for\nfostering trust in their generated responses and serves as a critical tool for\nthe detection and prevention of erroneous or hallucinated outputs. To achieve\nreliable and well-calibrated uncertainty quantification in open-ended and\nfree-form natural language generation, we propose an uncertainty-aware\nfine-tuning approach for LLMs. This approach enhances the model's ability to\nprovide reliable uncertainty estimates without compromising accuracy, thereby\nguiding them to produce more trustworthy responses. We introduce a novel\nuncertainty-aware causal language modeling loss function, grounded in the\nprinciples of decision theory. Through rigorous evaluation on multiple\nfree-form question-answering datasets and models, we demonstrate that our\nuncertainty-aware fine-tuning approach yields better calibrated uncertainty\nestimates in natural language generation tasks than fine-tuning with the\nstandard causal language modeling loss. Furthermore, the experimental results\nshow that the proposed method significantly improves the model's ability to\ndetect hallucinations and identify out-of-domain prompts.", "published": "2024-12-03 23:14:47", "link": "http://arxiv.org/abs/2412.02904v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Does Few-Shot Learning Help LLM Performance in Code Synthesis?", "abstract": "Large language models (LLMs) have made significant strides at code generation\nthrough improved model design, training, and chain-of-thought. However,\nprompt-level optimizations remain an important yet under-explored aspect of\nLLMs for coding. This work focuses on the few-shot examples present in most\ncode generation prompts, offering a systematic study on whether few-shot\nexamples improve LLM's coding capabilities, which few-shot examples have the\nlargest impact, and how to select impactful examples. Our work offers 2\napproaches for selecting few-shot examples, a model-free method,\nCODEEXEMPLAR-FREE, and a model-based method, CODEEXEMPLAR-BASED. The 2 methods\noffer a trade-off between improved performance and reliance on training data\nand interpretability. Both methods significantly improve CodeLlama's coding\nability across the popular HumanEval+ coding benchmark. In summary, our work\nprovides valuable insights into how to pick few-shot examples in code\ngeneration prompts to improve LLM code generation capabilities.", "published": "2024-12-03 23:19:40", "link": "http://arxiv.org/abs/2412.02906v1", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Achieving Semantic Consistency: Contextualized Word Representations for\n  Political Text Analysis", "abstract": "Accurately interpreting words is vital in political science text analysis;\nsome tasks require assuming semantic stability, while others aim to trace\nsemantic shifts. Traditional static embeddings, like Word2Vec effectively\ncapture long-term semantic changes but often lack stability in short-term\ncontexts due to embedding fluctuations caused by unbalanced training data.\nBERT, which features transformer-based architecture and contextual embeddings,\noffers greater semantic consistency, making it suitable for analyses in which\nstability is crucial. This study compares Word2Vec and BERT using 20 years of\nPeople's Daily articles to evaluate their performance in semantic\nrepresentations across different timeframes. The results indicate that BERT\noutperforms Word2Vec in maintaining semantic stability and still recognizes\nsubtle semantic variations. These findings support BERT's use in text analysis\ntasks that require stability, where semantic changes are not assumed, offering\na more reliable foundation than static alternatives.", "published": "2024-12-03 15:51:37", "link": "http://arxiv.org/abs/2412.04505v2", "categories": ["cs.CL", "econ.GN", "q-fin.EC"], "primary_category": "cs.CL"}
{"title": "Arctic-Embed 2.0: Multilingual Retrieval Without Compromise", "abstract": "This paper presents the training methodology of Arctic-Embed 2.0, a set of\nopen-source text embedding models built for accurate and efficient multilingual\nretrieval. While prior works have suffered from degraded English retrieval\nquality, Arctic-Embed 2.0 delivers competitive retrieval quality on\nmultilingual and English-only benchmarks, and supports Matryoshka\nRepresentation Learning (MRL) for efficient embedding storage with\nsignificantly lower compressed quality degradation compared to alternatives. We\ndetail the design and implementation, presenting several important open\nresearch questions that arose during model development. We conduct experiments\nexploring these research questions and include extensive discussion aimed at\nfostering further discussion in this field.", "published": "2024-12-03 22:59:36", "link": "http://arxiv.org/abs/2412.04506v2", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Adaptive Two-Phase Finetuning LLMs for Japanese Legal Text Retrieval", "abstract": "Text Retrieval (TR) involves finding and retrieving text-based content\nrelevant to a user's query from a large repository, with applications in\nreal-world scenarios such as legal document retrieval. While most existing\nstudies focus on English, limited work addresses Japanese contexts. In this\npaper, we introduce a new dataset specifically designed for Japanese legal\ncontexts and propose a novel two-phase pipeline tailored to this domain.\n  In the first phase, the model learns a broad understanding of global\ncontexts, enhancing its generalization and adaptability to diverse queries. In\nthe second phase, the model is fine-tuned to address complex queries specific\nto legal scenarios. Extensive experiments are conducted to demonstrate the\nsuperior performance of our method, which outperforms existing baselines.\n  Furthermore, our pipeline proves effective in English contexts, surpassing\ncomparable baselines on the MS MARCO dataset. We have made our code publicly\navailable on GitHub, and the model checkpoints are accessible via HuggingFace.", "published": "2024-12-03 10:52:49", "link": "http://arxiv.org/abs/2412.13205v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Removing Spurious Correlation from Neural Network Interpretations", "abstract": "The existing algorithms for identification of neurons responsible for\nundesired and harmful behaviors do not consider the effects of confounders such\nas topic of the conversation. In this work, we show that confounders can create\nspurious correlations and propose a new causal mediation approach that controls\nthe impact of the topic. In experiments with two large language models, we\nstudy the localization hypothesis and show that adjusting for the effect of\nconversation topic, toxicity becomes less localized.", "published": "2024-12-03 22:58:21", "link": "http://arxiv.org/abs/2412.02893v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.AP", "stat.ME"], "primary_category": "cs.CL"}
{"title": "Multi-Bin Batching for Increasing LLM Inference Throughput", "abstract": "As large language models (LLMs) grow in popularity for their diverse\ncapabilities, improving the efficiency of their inference systems has become\nincreasingly critical. Batching LLM requests is a critical step in scheduling\nthe inference jobs on servers (e.g. GPUs), enabling the system to maximize\nthroughput by allowing multiple requests to be processed in parallel. However,\nrequests often have varying generation lengths, causing resource\nunderutilization, as hardware must wait for the longest-running request in the\nbatch to complete before moving to the next batch. We formalize this problem\nfrom a queueing-theoretic perspective, and aim to design a control policy which\nis throughput-optimal. We propose Multi-Bin Batching, a simple yet effective\nmethod that can provably improve LLM inference throughput by grouping requests\nwith similar (predicted) execution times into predetermined bins. Through a\ncombination of theoretical analysis and experiments, including real-world LLM\ninference scenarios, we demonstrate significant throughput gains compared to\nstandard batching approaches.", "published": "2024-12-03 03:16:12", "link": "http://arxiv.org/abs/2412.04504v1", "categories": ["cs.CL", "cs.DC", "cs.LG", "cs.SY", "eess.SY"], "primary_category": "cs.CL"}
{"title": "AV-Odyssey Bench: Can Your Multimodal LLMs Really Understand\n  Audio-Visual Information?", "abstract": "Recently, multimodal large language models (MLLMs), such as GPT-4o, Gemini\n1.5 Pro, and Reka Core, have expanded their capabilities to include vision and\naudio modalities. While these models demonstrate impressive performance across\na wide range of audio-visual applications, our proposed DeafTest reveals that\nMLLMs often struggle with simple tasks humans find trivial: 1) determining\nwhich of two sounds is louder, and 2) determining which of two sounds has a\nhigher pitch. Motivated by these observations, we introduce AV-Odyssey Bench, a\ncomprehensive audio-visual benchmark designed to assess whether those MLLMs can\ntruly understand the audio-visual information. This benchmark encompasses 4,555\ncarefully crafted problems, each incorporating text, visual, and audio\ncomponents. To successfully infer answers, models must effectively leverage\nclues from both visual and audio inputs. To ensure precise and objective\nevaluation of MLLM responses, we have structured the questions as\nmultiple-choice, eliminating the need for human evaluation or LLM-assisted\nassessment. We benchmark a series of closed-source and open-source models and\nsummarize the observations. By revealing the limitations of current models, we\naim to provide useful insight for future dataset collection and model\ndevelopment.", "published": "2024-12-03 17:41:23", "link": "http://arxiv.org/abs/2412.02611v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Switchable deep beamformer for high-quality and real-time passive\n  acoustic mapping", "abstract": "Passive acoustic mapping (PAM) is a promising tool for monitoring acoustic\ncavitation activities in the applications of ultrasound therapy. Data-adaptive\nbeamformers for PAM have better image quality compared to the time exposure\nacoustics (TEA) algorithms. However, the computational cost of data-adaptive\nbeamformers is considerably expensive. In this work, we develop a deep\nbeamformer based on a generative adversarial network, which can switch between\ndifferent transducer arrays and reconstruct high-quality PAM images directly\nfrom radio frequency ultrasound signals with low computational cost. The deep\nbeamformer was trained on the dataset consisting of simulated and experimental\ncavitation signals of single and multiple microbubble clouds measured by\ndifferent (linear and phased) arrays covering 1-15 MHz. We compared the\nperformance of the deep beamformer to TEA and three different data-adaptive\nbeamformers using the simulated and experimental test dataset. Compared with\nTEA, the deep beamformer reduced the energy spread area by 18.9%-65.0% and\nimproved the image signal-to-noise ratio by 9.3-22.9 dB in average for the\ndifferent arrays in our data. Compared to the data-adaptive beamformers, the\ndeep beamformer reduced the computational cost by three orders of magnitude\nachieving 10.5 ms image reconstruction speed in our data, while the image\nquality was as good as that of the data-adaptive beamformers. These results\ndemonstrated the potential of the deep beamformer for high-resolution\nmonitoring of microbubble cavitation activities for ultrasound therapy.", "published": "2024-12-03 09:40:59", "link": "http://arxiv.org/abs/2412.02327v1", "categories": ["cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.AI"}
{"title": "SyncFlow: Toward Temporally Aligned Joint Audio-Video Generation from\n  Text", "abstract": "Video and audio are closely correlated modalities that humans naturally\nperceive together. While recent advancements have enabled the generation of\naudio or video from text, producing both modalities simultaneously still\ntypically relies on either a cascaded process or multi-modal contrastive\nencoders. These approaches, however, often lead to suboptimal results due to\ninherent information losses during inference and conditioning. In this paper,\nwe introduce SyncFlow, a system that is capable of simultaneously generating\ntemporally synchronized audio and video from text. The core of SyncFlow is the\nproposed dual-diffusion-transformer (d-DiT) architecture, which enables joint\nvideo and audio modelling with proper information fusion. To efficiently manage\nthe computational cost of joint audio and video modelling, SyncFlow utilizes a\nmulti-stage training strategy that separates video and audio learning before\njoint fine-tuning. Our empirical evaluations demonstrate that SyncFlow produces\naudio and video outputs that are more correlated than baseline methods with\nsignificantly enhanced audio quality and audio-visual correspondence. Moreover,\nwe demonstrate strong zero-shot capabilities of SyncFlow, including zero-shot\nvideo-to-audio generation and adaptation to novel video resolutions without\nfurther training.", "published": "2024-12-03 21:48:08", "link": "http://arxiv.org/abs/2412.15220v1", "categories": ["cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "It Takes Two: Real-time Co-Speech Two-person's Interaction Generation\n  via Reactive Auto-regressive Diffusion Model", "abstract": "Conversational scenarios are very common in real-world settings, yet existing\nco-speech motion synthesis approaches often fall short in these contexts, where\none person's audio and gestures will influence the other's responses.\nAdditionally, most existing methods rely on offline sequence-to-sequence\nframeworks, which are unsuitable for online applications. In this work, we\nintroduce an audio-driven, auto-regressive system designed to synthesize\ndynamic movements for two characters during a conversation. At the core of our\napproach is a diffusion-based full-body motion synthesis model, which is\nconditioned on the past states of both characters, speech audio, and a\ntask-oriented motion trajectory input, allowing for flexible spatial control.\nTo enhance the model's ability to learn diverse interactions, we have enriched\nexisting two-person conversational motion datasets with more dynamic and\ninteractive motions. We evaluate our system through multiple experiments to\nshow it outperforms across a variety of tasks, including single and two-person\nco-speech motion generation, as well as interactive motion generation. To the\nbest of our knowledge, this is the first system capable of generating\ninteractive full-body motions for two characters from speech in an online\nmanner.", "published": "2024-12-03 12:31:44", "link": "http://arxiv.org/abs/2412.02419v1", "categories": ["cs.SD", "cs.CV", "cs.GR", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
