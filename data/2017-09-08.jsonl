{"title": "A Statistical Comparison of Some Theories of NP Word Order", "abstract": "A frequent object of study in linguistic typology is the order of elements\n{demonstrative, adjective, numeral, noun} in the noun phrase. The goal is to\npredict the relative frequencies of these orders across languages. Here we use\nPoisson regression to statistically compare some prominent accounts of this\nvariation. We compare feature systems derived from Cinque (2005) to feature\nsystems given in Cysouw (2010) and Dryer (in prep). In this setting, we do not\nfind clear reasons to prefer the model of Cinque (2005) or Dryer (in prep), but\nwe find both of these models have substantially better fit to the typological\ndata than the model from Cysouw (2010).", "published": "2017-09-08 17:12:16", "link": "http://arxiv.org/abs/1709.02783v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Globally Normalized Reader", "abstract": "Rapid progress has been made towards question answering (QA) systems that can\nextract answers from text. Existing neural approaches make use of expensive\nbi-directional attention mechanisms or score all possible answer spans,\nlimiting scalability. We propose instead to cast extractive QA as an iterative\nsearch problem: select the answer's sentence, start word, and end word. This\nrepresentation reduces the space of each search step and allows computation to\nbe conditionally allocated to promising search paths. We show that globally\nnormalizing the decision process and back-propagating through beam search makes\nthis representation viable and learning efficient. We empirically demonstrate\nthe benefits of this approach using our model, Globally Normalized Reader\n(GNR), which achieves the second highest single model performance on the\nStanford Question Answering Dataset (68.4 EM, 76.21 F1 dev) and is 24.7x faster\nthan bi-attention-flow. We also introduce a data-augmentation method to produce\nsemantically valid examples by aligning named entities to a knowledge base and\nswapping them with new entities of the same type. This method improves the\nperformance of all models considered in this work and is of independent\ninterest for a variety of NLP tasks.", "published": "2017-09-08 18:27:50", "link": "http://arxiv.org/abs/1709.02828v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Combining LSTM and Latent Topic Modeling for Mortality Prediction", "abstract": "There is a great need for technologies that can predict the mortality of\npatients in intensive care units with both high accuracy and accountability. We\npresent joint end-to-end neural network architectures that combine long\nshort-term memory (LSTM) and a latent topic model to simultaneously train a\nclassifier for mortality prediction and learn latent topics indicative of\nmortality from textual clinical notes. For topic interpretability, the topic\nmodeling layer has been carefully designed as a single-layer network with\nconstraints inspired by LDA. Experiments on the MIMIC-III dataset show that our\nmodels significantly outperform prior models that are based on LDA topics in\nmortality prediction. However, we achieve limited success with our method for\ninterpreting topics from the trained models by looking at the neural network\nweights.", "published": "2017-09-08 19:30:09", "link": "http://arxiv.org/abs/1709.02842v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CLaC at SemEval-2016 Task 11: Exploring linguistic and psycho-linguistic\n  Features for Complex Word Identification", "abstract": "This paper describes the system deployed by the CLaC-EDLK team to the\n\"SemEval 2016, Complex Word Identification task\". The goal of the task is to\nidentify if a given word in a given context is \"simple\" or \"complex\". Our\nsystem relies on linguistic features and cognitive complexity. We used several\nsupervised models, however the Random Forest model outperformed the others.\nOverall our best configuration achieved a G-score of 68.8% in the task, ranking\nour system 21 out of 45.", "published": "2017-09-08 19:34:02", "link": "http://arxiv.org/abs/1709.02843v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Simple Recurrent Units for Highly Parallelizable Recurrence", "abstract": "Common recurrent neural architectures scale poorly due to the intrinsic\ndifficulty in parallelizing their state computations. In this work, we propose\nthe Simple Recurrent Unit (SRU), a light recurrent unit that balances model\ncapacity and scalability. SRU is designed to provide expressive recurrence,\nenable highly parallelized implementation, and comes with careful\ninitialization to facilitate training of deep models. We demonstrate the\neffectiveness of SRU on multiple NLP tasks. SRU achieves 5--9x speed-up over\ncuDNN-optimized LSTM on classification and question answering datasets, and\ndelivers stronger results than LSTM and convolutional models. We also obtain an\naverage of 0.7 BLEU improvement over the Transformer model on translation by\nincorporating SRU into the architecture.", "published": "2017-09-08 16:02:30", "link": "http://arxiv.org/abs/1709.02755v5", "categories": ["cs.CL", "cs.NE"], "primary_category": "cs.CL"}
