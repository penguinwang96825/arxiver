{"title": "OmniTab: Pretraining with Natural and Synthetic Data for Few-shot\n  Table-based Question Answering", "abstract": "The information in tables can be an important complement to text, making\ntable-based question answering (QA) systems of great value. The intrinsic\ncomplexity of handling tables often adds an extra burden to both model design\nand data annotation. In this paper, we aim to develop a simple table-based QA\nmodel with minimal annotation effort. Motivated by the fact that table-based QA\nrequires both alignment between questions and tables and the ability to perform\ncomplicated reasoning over multiple table elements, we propose an omnivorous\npretraining approach that consumes both natural and synthetic data to endow\nmodels with these respective abilities. Specifically, given freely available\ntables, we leverage retrieval to pair them with relevant natural sentences for\nmask-based pretraining, and synthesize NL questions by converting SQL sampled\nfrom tables for pretraining with a QA loss. We perform extensive experiments in\nboth few-shot and full settings, and the results clearly demonstrate the\nsuperiority of our model OmniTab, with the best multitasking approach achieving\nan absolute gain of 16.2% and 2.7% in 128-shot and full settings respectively,\nalso establishing a new state-of-the-art on WikiTableQuestions. Detailed\nablations and analyses reveal different characteristics of natural and\nsynthetic data, shedding light on future directions in omnivorous pretraining.\nCode, pretraining data, and pretrained models are available at\nhttps://github.com/jzbjyb/OmniTab.", "published": "2022-07-08 01:23:45", "link": "http://arxiv.org/abs/2207.03637v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Crake: Causal-Enhanced Table-Filler for Question Answering over Large\n  Scale Knowledge Base", "abstract": "Semantic parsing solves knowledge base (KB) question answering (KBQA) by\ncomposing a KB query, which generally involves node extraction (NE) and graph\ncomposition (GC) to detect and connect related nodes in a query. Despite the\nstrong causal effects between NE and GC, previous works fail to directly model\nsuch causalities in their pipeline, hindering the learning of subtask\ncorrelations. Also, the sequence-generation process for GC in previous works\ninduces ambiguity and exposure bias, which further harms accuracy. In this\nwork, we formalize semantic parsing into two stages. In the first stage (graph\nstructure generation), we propose a causal-enhanced table-filler to overcome\nthe issues in sequence-modelling and to learn the internal causalities. In the\nsecond stage (relation extraction), an efficient beam-search algorithm is\npresented to scale complex queries on large-scale KBs. Experiments on LC-QuAD\n1.0 indicate that our method surpasses previous state-of-the-arts by a large\nmargin (17%) while remaining time and space efficiency. The code and models are\navailable at https://github.com/AOZMH/Crake.", "published": "2022-07-08 04:21:26", "link": "http://arxiv.org/abs/2207.03680v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DSTEA: Improving Dialogue State Tracking via Entity Adaptive\n  Pre-training", "abstract": "Dialogue State Tracking (DST) is critical for comprehensively interpreting\nuser and system utterances, thereby forming the cornerstone of efficient\ndialogue systems. Despite past research efforts focused on enhancing DST\nperformance through alterations to the model structure or integrating\nadditional features like graph relations, they often require additional\npre-training with external dialogue corpora. In this study, we propose DSTEA,\nimproving Dialogue State Tracking via Entity Adaptive pre-training, which can\nenhance the encoder through by intensively training key entities in dialogue\nutterances. DSTEA identifies these pivotal entities from input dialogues\nutilizing four different methods: ontology information, named-entity\nrecognition, the spaCy, and the flair library. Subsequently, it employs\nselective knowledge masking to train the model effectively. Remarkably, DSTEA\nonly requires pre-training without the direct infusion of extra knowledge into\nthe DST model. This approach resulted in substantial performance improvements\nof four robust DST models on MultiWOZ 2.0, 2.1, and 2.2, with joint goal\naccuracy witnessing an increase of up to 2.69% (from 52.41% to 55.10%). Further\nvalidation of DSTEA's efficacy was provided through comparative experiments\nconsidering various entity types and different entity adaptive pre-training\nconfigurations such as masking strategy and masking rate.", "published": "2022-07-08 12:27:19", "link": "http://arxiv.org/abs/2207.03858v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Medical Information Extraction Workbench to Process German Clinical\n  Text", "abstract": "Background: In the information extraction and natural language processing\ndomain, accessible datasets are crucial to reproduce and compare results.\nPublicly available implementations and tools can serve as benchmark and\nfacilitate the development of more complex applications. However, in the\ncontext of clinical text processing the number of accessible datasets is scarce\n-- and so is the number of existing tools. One of the main reasons is the\nsensitivity of the data. This problem is even more evident for non-English\nlanguages.\n  Approach: In order to address this situation, we introduce a workbench: a\ncollection of German clinical text processing models. The models are trained on\na de-identified corpus of German nephrology reports.\n  Result: The presented models provide promising results on in-domain data.\nMoreover, we show that our models can be also successfully applied to other\nbiomedical text in German. Our workbench is made publicly available so it can\nbe used out of the box, as a benchmark or transferred to related problems.", "published": "2022-07-08 13:19:19", "link": "http://arxiv.org/abs/2207.03885v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "No Time Like the Present: Effects of Language Change on Automated\n  Comment Moderation", "abstract": "The spread of online hate has become a significant problem for newspapers\nthat host comment sections. As a result, there is growing interest in using\nmachine learning and natural language processing for (semi-) automated abusive\nlanguage detection to avoid manual comment moderation costs or having to shut\ndown comment sections altogether. However, much of the past work on abusive\nlanguage detection assumes that classifiers operate in a static language\nenvironment, despite language and news being in a state of constant flux. In\nthis paper, we show using a new German newspaper comments dataset that the\nclassifiers trained with naive ML techniques like a random-test train split\nwill underperform on future data, and that a time stratified evaluation split\nis more appropriate. We also show that classifier performance rapidly degrades\nwhen evaluated on data from a different period than the training data. Our\nfindings suggest that it is necessary to consider the temporal dynamics of\nlanguage when developing an abusive language detection system or risk deploying\na model that will quickly become defunct.", "published": "2022-07-08 16:39:21", "link": "http://arxiv.org/abs/2207.04003v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ABB-BERT: A BERT model for disambiguating abbreviations and contractions", "abstract": "Abbreviations and contractions are commonly found in text across different\ndomains. For example, doctors' notes contain many contractions that can be\npersonalized based on their choices. Existing spelling correction models are\nnot suitable to handle expansions because of many reductions of characters in\nwords. In this work, we propose ABB-BERT, a BERT-based model, which deals with\nan ambiguous language containing abbreviations and contractions. ABB-BERT can\nrank them from thousands of options and is designed for scale. It is trained on\nWikipedia text, and the algorithm allows it to be fine-tuned with little\ncompute to get better performance for a domain or person. We are publicly\nreleasing the training dataset for abbreviations and contractions derived from\nWikipedia.", "published": "2022-07-08 16:54:57", "link": "http://arxiv.org/abs/2207.04008v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ASL-Homework-RGBD Dataset: An annotated dataset of 45 fluent and\n  non-fluent signers performing American Sign Language homeworks", "abstract": "We are releasing a dataset containing videos of both fluent and non-fluent\nsigners using American Sign Language (ASL), which were collected using a Kinect\nv2 sensor. This dataset was collected as a part of a project to develop and\nevaluate computer vision algorithms to support new technologies for automatic\ndetection of ASL fluency attributes. A total of 45 fluent and non-fluent\nparticipants were asked to perform signing homework assignments that are\nsimilar to the assignments used in introductory or intermediate level ASL\ncourses. The data is annotated to identify several aspects of signing including\ngrammatical features and non-manual markers. Sign language recognition is\ncurrently very data-driven and this dataset can support the design of\nrecognition technologies, especially technologies that can benefit ASL\nlearners. This dataset might also be interesting to ASL education researchers\nwho want to contrast fluent and non-fluent signing.", "published": "2022-07-08 17:18:49", "link": "http://arxiv.org/abs/2207.04021v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Entity Disambiguation by Reasoning over a Knowledge Base", "abstract": "Recent work in entity disambiguation (ED) has typically neglected structured\nknowledge base (KB) facts, and instead relied on a limited subset of KB\ninformation, such as entity descriptions or types. This limits the range of\ncontexts in which entities can be disambiguated. To allow the use of all KB\nfacts, as well as descriptions and types, we introduce an ED model which links\nentities by reasoning over a symbolic knowledge base in a fully differentiable\nfashion. Our model surpasses state-of-the-art baselines on six well-established\nED datasets by 1.3 F1 on average. By allowing access to all KB information, our\nmodel is less reliant on popularity-based entity priors, and improves\nperformance on the challenging ShadowLink dataset (which emphasises infrequent\nand ambiguous entities) by 12.7 F1.", "published": "2022-07-08 19:13:53", "link": "http://arxiv.org/abs/2207.04106v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ReFinED: An Efficient Zero-shot-capable Approach to End-to-End Entity\n  Linking", "abstract": "We introduce ReFinED, an efficient end-to-end entity linking model which uses\nfine-grained entity types and entity descriptions to perform linking. The model\nperforms mention detection, fine-grained entity typing, and entity\ndisambiguation for all mentions within a document in a single forward pass,\nmaking it more than 60 times faster than competitive existing approaches.\nReFinED also surpasses state-of-the-art performance on standard entity linking\ndatasets by an average of 3.7 F1. The model is capable of generalising to\nlarge-scale knowledge bases such as Wikidata (which has 15 times more entities\nthan Wikipedia) and of zero-shot entity linking. The combination of speed,\naccuracy and scale makes ReFinED an effective and cost-efficient system for\nextracting entities from web-scale datasets, for which the model has been\nsuccessfully deployed. Our code and pre-trained models are available at\nhttps://github.com/alexa/ReFinED", "published": "2022-07-08 19:20:42", "link": "http://arxiv.org/abs/2207.04108v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SETSum: Summarization and Visualization of Student Evaluations of\n  Teaching", "abstract": "Student Evaluations of Teaching (SETs) are widely used in colleges and\nuniversities. Typically SET results are summarized for instructors in a static\nPDF report. The report often includes summary statistics for quantitative\nratings and an unsorted list of open-ended student comments. The lack of\norganization and summarization of the raw comments hinders those interpreting\nthe reports from fully utilizing informative feedback, making accurate\ninferences, and designing appropriate instructional improvements. In this work,\nwe introduce a novel system, SETSum, that leverages sentiment analysis, aspect\nextraction, summarization, and visualization techniques to provide organized\nillustrations of SET findings to instructors and other reviewers. Ten\nuniversity professors from diverse departments serve as evaluators of the\nsystem and all agree that SETSum helps them interpret SET results more\nefficiently; and 6 out of 10 instructors prefer our system over the standard\nstatic PDF report (while the remaining 4 would like to have both). This\ndemonstrates that our work holds the potential to reform the SET reporting\nconventions in the future. Our code is available at\nhttps://github.com/evahuyn/SETSum", "published": "2022-07-08 01:40:11", "link": "http://arxiv.org/abs/2207.03640v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Getting BART to Ride the Idiomatic Train: Learning to Represent\n  Idiomatic Expressions", "abstract": "Idiomatic expressions (IEs), characterized by their non-compositionality, are\nan important part of natural language. They have been a classical challenge to\nNLP, including pre-trained language models that drive today's state-of-the-art.\nPrior work has identified deficiencies in their contextualized representation\nstemming from the underlying compositional paradigm of representation. In this\nwork, we take a first-principles approach to build idiomaticity into BART using\nan adapter as a lightweight non-compositional language expert trained on\nidiomatic sentences. The improved capability over baselines (e.g., BART) is\nseen via intrinsic and extrinsic methods, where idiom embeddings score 0.19\npoints higher in homogeneity score for embedding clustering, and up to 25%\nhigher sequence accuracy on the idiom processing tasks of IE sense\ndisambiguation and span detection.", "published": "2022-07-08 04:05:19", "link": "http://arxiv.org/abs/2207.03679v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Hidden Schema Networks", "abstract": "Large, pretrained language models infer powerful representations that encode\nrich semantic and syntactic content, albeit implicitly. In this work we\nintroduce a novel neural language model that enforces, via inductive biases,\nexplicit relational structures which allow for compositionality onto the output\nrepresentations of pretrained language models. Specifically, the model encodes\nsentences into sequences of symbols (composed representations), which\ncorrespond to the nodes visited by biased random walkers on a global latent\ngraph, and infers the posterior distribution of the latter. We first\ndemonstrate that the model is able to uncover ground-truth graphs from\nartificially generated datasets of random token sequences. Next, we leverage\npretrained BERT and GPT-2 language models as encoder and decoder, respectively,\nto infer networks of symbols (schemata) from natural language datasets. Our\nexperiments show that (i) the inferred symbols can be interpreted as encoding\ndifferent aspects of language, as e.g. topics or sentiments, and that (ii)\nGPT-like models can effectively be conditioned on symbolic representations.\nFinally, we explore training autoregressive, random walk ``reasoning\" models on\nschema networks inferred from commonsense knowledge databases, and using the\nsampled paths to enhance the performance of pretrained language models on\ncommonsense If-Then reasoning tasks.", "published": "2022-07-08 09:26:19", "link": "http://arxiv.org/abs/2207.03777v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Efficiency Study for SPLADE Models", "abstract": "Latency and efficiency issues are often overlooked when evaluating IR models\nbased on Pretrained Language Models (PLMs) in reason of multiple hardware and\nsoftware testing scenarios. Nevertheless, efficiency is an important part of\nsuch systems and should not be overlooked.\n  In this paper, we focus on improving the efficiency of the SPLADE model since\nit has achieved state-of-the-art zero-shot performance and competitive results\non TREC collections. SPLADE efficiency can be controlled via a regularization\nfactor, but solely controlling this regularization has been shown to not be\nefficient enough. In order to reduce the latency gap between SPLADE and\ntraditional retrieval systems, we propose several techniques including L1\nregularization for queries, a separation of document/query encoders, a\nFLOPS-regularized middle-training, and the use of faster query encoders. Our\nbenchmark demonstrates that we can drastically improve the efficiency of these\nmodels while increasing the performance metrics on in-domain data. To our\nknowledge, {we propose the first neural models that, under the same computing\nconstraints, \\textit{achieve similar latency (less than 4ms difference) as\ntraditional BM25}, while having \\textit{similar performance (less than 10\\%\nMRR@10 reduction)} as the state-of-the-art single-stage neural rankers on\nin-domain data}.", "published": "2022-07-08 11:42:05", "link": "http://arxiv.org/abs/2207.03834v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Probing Classifiers are Unreliable for Concept Removal and Detection", "abstract": "Neural network models trained on text data have been found to encode\nundesirable linguistic or sensitive concepts in their representation. Removing\nsuch concepts is non-trivial because of a complex relationship between the\nconcept, text input, and the learnt representation. Recent work has proposed\npost-hoc and adversarial methods to remove such unwanted concepts from a\nmodel's representation. Through an extensive theoretical and empirical\nanalysis, we show that these methods can be counter-productive: they are unable\nto remove the concepts entirely, and in the worst case may end up destroying\nall task-relevant features. The reason is the methods' reliance on a probing\nclassifier as a proxy for the concept. Even under the most favorable conditions\nfor learning a probing classifier when a concept's relevant features in\nrepresentation space alone can provide 100% accuracy, we prove that a probing\nclassifier is likely to use non-concept features and thus post-hoc or\nadversarial methods will fail to remove the concept correctly. These\ntheoretical implications are confirmed by experiments on models trained on\nsynthetic, Multi-NLI, and Twitter datasets. For sensitive applications of\nconcept removal such as fairness, we recommend caution against using these\nmethods and propose a spuriousness metric to gauge the quality of the final\nclassifier.", "published": "2022-07-08 23:15:26", "link": "http://arxiv.org/abs/2207.04153v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Emotion detection of social data: APIs comparative study", "abstract": "The development of emotion detection technology has emerged as a highly\nvaluable possibility in the corporate sector due to the nearly limitless uses\nof this new discipline, particularly with the unceasing propagation of social\ndata. In recent years, the electronic marketplace has witnessed the\nestablishment of a large number of start-up businesses with an almost sole\nfocus on building new commercial and open-source tools and APIs for emotion\ndetection and recognition. Yet, these tools and APIs must be continuously\nreviewed and evaluated, and their performances should be reported and\ndiscussed. There is a lack of research to empirically compare current emotion\ndetection technologies in terms of the results obtained from each model using\nthe same textual dataset. Also, there is a lack of comparative studies that\napply benchmark comparison to social data. This study compares eight\ntechnologies; IBM Watson NLU, ParallelDots, Symanto-Ekman, Crystalfeel, Text to\nEmotion, Senpy, Textprobe, and NLP Cloud. The comparison was undertaken using\ntwo different datasets. The emotions from the chosen datasets were then derived\nusing the incorporated APIs. The performance of these APIs was assessed using\nthe aggregated scores that they delivered as well as the theoretically proven\nevaluation metrics such as the micro-average of accuracy, classification error,\nprecision, recall, and f1-score. Lastly, the assessment of these APIs\nincorporating the evaluation measures is reported and discussed.", "published": "2022-07-08 08:47:31", "link": "http://arxiv.org/abs/2207.10654v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Approach to Ensure Fairness in News Articles", "abstract": "Recommender systems, information retrieval, and other information access\nsystems present unique challenges for examining and applying concepts of\nfairness and bias mitigation in unstructured text. This paper introduces Dbias,\nwhich is a Python package to ensure fairness in news articles. Dbias is a\ntrained Machine Learning (ML) pipeline that can take a text (e.g., a paragraph\nor news story) and detects if the text is biased or not. Then, it detects the\nbiased words in the text, masks them, and recommends a set of sentences with\nnew words that are bias-free or at least less biased. We incorporate the\nelements of data science best practices to ensure that this pipeline is\nreproducible and usable. We show in experiments that this pipeline can be\neffective for mitigating biases and outperforms the common neural network\narchitectures in ensuring fairness in the news articles.", "published": "2022-07-08 14:43:56", "link": "http://arxiv.org/abs/2207.03938v1", "categories": ["cs.IR", "cs.CL", "cs.CY"], "primary_category": "cs.IR"}
{"title": "CoSIm: Commonsense Reasoning for Counterfactual Scene Imagination", "abstract": "As humans, we can modify our assumptions about a scene by imagining\nalternative objects or concepts in our minds. For example, we can easily\nanticipate the implications of the sun being overcast by rain clouds (e.g., the\nstreet will get wet) and accordingly prepare for that. In this paper, we\nintroduce a new task/dataset called Commonsense Reasoning for Counterfactual\nScene Imagination (CoSIm) which is designed to evaluate the ability of AI\nsystems to reason about scene change imagination. In this task/dataset, models\nare given an image and an initial question-response pair about the image. Next,\na counterfactual imagined scene change (in textual form) is applied, and the\nmodel has to predict the new response to the initial question based on this\nscene change. We collect 3.5K high-quality and challenging data instances, with\neach instance consisting of an image, a commonsense question with a response, a\ndescription of a counterfactual change, a new response to the question, and\nthree distractor responses. Our dataset contains various complex scene change\ntypes (such as object addition/removal/state change, event description,\nenvironment change, etc.) that require models to imagine many different\nscenarios and reason about the changed scenes. We present a baseline model\nbased on a vision-language Transformer (i.e., LXMERT) and ablation studies.\nThrough human evaluation, we demonstrate a large human-model performance gap,\nsuggesting room for promising future work on this challenging counterfactual,\nscene imagination task. Our code and dataset are publicly available at:\nhttps://github.com/hyounghk/CoSIm", "published": "2022-07-08 15:28:23", "link": "http://arxiv.org/abs/2207.03961v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "The Harvard USPTO Patent Dataset: A Large-Scale, Well-Structured, and\n  Multi-Purpose Corpus of Patent Applications", "abstract": "Innovation is a major driver of economic and social development, and\ninformation about many kinds of innovation is embedded in semi-structured data\nfrom patents and patent applications. Although the impact and novelty of\ninnovations expressed in patent data are difficult to measure through\ntraditional means, ML offers a promising set of techniques for evaluating\nnovelty, summarizing contributions, and embedding semantics. In this paper, we\nintroduce the Harvard USPTO Patent Dataset (HUPD), a large-scale,\nwell-structured, and multi-purpose corpus of English-language patent\napplications filed to the United States Patent and Trademark Office (USPTO)\nbetween 2004 and 2018. With more than 4.5 million patent documents, HUPD is two\nto three times larger than comparable corpora. Unlike previously proposed\npatent datasets in NLP, HUPD contains the inventor-submitted versions of patent\napplications--not the final versions of granted patents--thereby allowing us to\nstudy patentability at the time of filing using NLP methods for the first time.\nIt is also novel in its inclusion of rich structured metadata alongside the\ntext of patent filings: By providing each application's metadata along with all\nof its text fields, the dataset enables researchers to perform new sets of NLP\ntasks that leverage variation in structured covariates. As a case study on the\ntypes of research HUPD makes possible, we introduce a new task to the NLP\ncommunity--namely, binary classification of patent decisions. We additionally\nshow the structured metadata provided in the dataset enables us to conduct\nexplicit studies of concept shifts for this task. Finally, we demonstrate how\nHUPD can be used for three additional tasks: multi-class classification of\npatent subject areas, language modeling, and summarization.", "published": "2022-07-08 17:57:15", "link": "http://arxiv.org/abs/2207.04043v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TalkToModel: Explaining Machine Learning Models with Interactive Natural\n  Language Conversations", "abstract": "Machine Learning (ML) models are increasingly used to make critical decisions\nin real-world applications, yet they have become more complex, making them\nharder to understand. To this end, researchers have proposed several techniques\nto explain model predictions. However, practitioners struggle to use these\nexplainability techniques because they often do not know which one to choose\nand how to interpret the results of the explanations. In this work, we address\nthese challenges by introducing TalkToModel: an interactive dialogue system for\nexplaining machine learning models through conversations. Specifically,\nTalkToModel comprises of three key components: 1) a natural language interface\nfor engaging in conversations, making ML model explainability highly\naccessible, 2) a dialogue engine that adapts to any tabular model and dataset,\ninterprets natural language, maps it to appropriate explanations, and generates\ntext responses, and 3) an execution component that constructs the explanations.\nWe carried out extensive quantitative and human subject evaluations of\nTalkToModel. Overall, we found the conversational system understands user\ninputs on novel datasets and models with high accuracy, demonstrating the\nsystem's capacity to generalize to new situations. In real-world evaluations\nwith humans, 73% of healthcare workers (e.g., doctors and nurses) agreed they\nwould use TalkToModel over baseline point-and-click systems for explainability\nin a disease prediction task, and 85% of ML professionals agreed TalkToModel\nwas easier to use for computing explanations. Our findings demonstrate that\nTalkToModel is more effective for model explainability than existing systems,\nintroducing a new category of explainability tools for practitioners. Code &\ndemo released here: https://github.com/dylan-slack/TalkToModel.", "published": "2022-07-08 23:42:56", "link": "http://arxiv.org/abs/2207.04154v4", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Automated Audio Captioning and Language-Based Audio Retrieval", "abstract": "This project involved participation in the DCASE 2022 Competition (Task 6)\nwhich had two subtasks: (1) Automated Audio Captioning and (2) Language-Based\nAudio Retrieval. The first subtask involved the generation of a textual\ndescription for audio samples, while the goal of the second was to find audio\nsamples within a fixed dataset that match a given description. For both\nsubtasks, the Clotho dataset was used. The models were evaluated on BLEU1,\nBLEU2, BLEU3, ROUGEL, METEOR, CIDEr, SPICE, and SPIDEr scores for audio\ncaptioning and R1, R5, R10 and mARP10 scores for audio retrieval. We have\nconducted a handful of experiments that modify the baseline models for these\ntasks. Our final architecture for Automated Audio Captioning is close to the\nbaseline performance, while our model for Language-Based Audio Retrieval has\nsurpassed its counterpart.", "published": "2022-07-08 23:48:52", "link": "http://arxiv.org/abs/2207.04156v2", "categories": ["cs.SD", "cs.CL", "cs.IR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Twitmo: A Twitter Data Topic Modeling and Visualization Package for R", "abstract": "We present Twitmo, a package that provides a broad range of methods to\ncollect, pre-process, analyze and visualize geo-tagged Twitter data. Twitmo\nenables the user to collect geo-tagged Tweets from Twitter and and provides a\ncomprehensive and user-friendly toolbox to generate topic distributions from\nLatent Dirichlet Allocations (LDA), correlated topic models (CTM) and\nstructural topic models (STM). Functions are included for pre-processing of\ntext, model building and prediction. In addition, one of the innovations of the\npackage is the automatic pooling of Tweets into longer pseudo-documents using\nhashtags and cosine similarities for better topic coherence. The package\nadditionally comes with functionality to visualize collected data sets and\nfitted models in static as well as interactive ways and offers built-in support\nfor model visualizations via LDAvis providing great convenience for researchers\nin this area. The Twitmo package is an innovative toolbox that can be used to\nanalyze public discourse of various topics, political parties or persons of\ninterest in space and time.", "published": "2022-07-08 12:23:20", "link": "http://arxiv.org/abs/2207.11236v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "stat.ML", "68N30 (Primary) 62P25, 97K80 (Secondary)"], "primary_category": "cs.IR"}
{"title": "Computationally Identifying Funneling and Focusing Questions in\n  Classroom Discourse", "abstract": "Responsive teaching is a highly effective strategy that promotes student\nlearning. In math classrooms, teachers might \"funnel\" students towards a\nnormative answer or \"focus\" students to reflect on their own thinking,\ndeepening their understanding of math concepts. When teachers focus, they treat\nstudents' contributions as resources for collective sensemaking, and thereby\nsignificantly improve students' achievement and confidence in mathematics. We\npropose the task of computationally detecting funneling and focusing questions\nin classroom discourse. We do so by creating and releasing an annotated dataset\nof 2,348 teacher utterances labeled for funneling and focusing questions, or\nneither. We introduce supervised and unsupervised approaches to differentiating\nthese questions. Our best model, a supervised RoBERTa model fine-tuned on our\ndataset, has a strong linear correlation of .76 with human expert labels and\nwith positive educational outcomes, including math instruction quality and\nstudent achievement, showing the model's potential for use in automated teacher\nfeedback tools. Our unsupervised measures show significant but weaker\ncorrelations with human labels and outcomes, and they highlight interesting\nlinguistic patterns of funneling and focusing questions. The high performance\nof the supervised measure indicates its promise for supporting teachers in\ntheir instruction.", "published": "2022-07-08 01:28:29", "link": "http://arxiv.org/abs/2208.04715v1", "categories": ["cs.CY", "cs.CL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "FastLTS: Non-Autoregressive End-to-End Unconstrained Lip-to-Speech\n  Synthesis", "abstract": "Unconstrained lip-to-speech synthesis aims to generate corresponding speeches\nfrom silent videos of talking faces with no restriction on head poses or\nvocabulary. Current works mainly use sequence-to-sequence models to solve this\nproblem, either in an autoregressive architecture or a flow-based\nnon-autoregressive architecture. However, these models suffer from several\ndrawbacks: 1) Instead of directly generating audios, they use a two-stage\npipeline that first generates mel-spectrograms and then reconstructs audios\nfrom the spectrograms. This causes cumbersome deployment and degradation of\nspeech quality due to error propagation; 2) The audio reconstruction algorithm\nused by these models limits the inference speed and audio quality, while neural\nvocoders are not available for these models since their output spectrograms are\nnot accurate enough; 3) The autoregressive model suffers from high inference\nlatency, while the flow-based model has high memory occupancy: neither of them\nis efficient enough in both time and memory usage. To tackle these problems, we\npropose FastLTS, a non-autoregressive end-to-end model which can directly\nsynthesize high-quality speech audios from unconstrained talking videos with\nlow latency, and has a relatively small model size. Besides, different from the\nwidely used 3D-CNN visual frontend for lip movement encoding, we for the first\ntime propose a transformer-based visual frontend for this task. Experiments\nshow that our model achieves $19.76\\times$ speedup for audio waveform\ngeneration compared with the current autoregressive model on input sequences of\n3 seconds, and obtains superior audio quality.", "published": "2022-07-08 10:10:39", "link": "http://arxiv.org/abs/2207.03800v2", "categories": ["cs.SD", "cs.CL", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Graph-based Multi-View Fusion and Local Adaptation: Mitigating\n  Within-Household Confusability for Speaker Identification", "abstract": "Speaker identification (SID) in the household scenario (e.g., for smart\nspeakers) is an important but challenging problem due to limited number of\nlabeled (enrollment) utterances, confusable voices, and demographic imbalances.\nConventional speaker recognition systems generalize from a large random sample\nof speakers, causing the recognition to underperform for households drawn from\nspecific cohorts or otherwise exhibiting high confusability. In this work, we\npropose a graph-based semi-supervised learning approach to improve\nhousehold-level SID accuracy and robustness with locally adapted graph\nnormalization and multi-signal fusion with multi-view graphs. Unlike other work\non household SID, fairness, and signal fusion, this work focuses on speaker\nlabel inference (scoring) and provides a simple solution to realize\nhousehold-specific adaptation and multi-signal fusion without tuning the\nembeddings or training a fusion network. Experiments on the VoxCeleb dataset\ndemonstrate that our approach consistently improves the performance across\nhouseholds with different customer cohorts and degrees of confusability.", "published": "2022-07-08 18:12:25", "link": "http://arxiv.org/abs/2207.04081v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD", "eess.IV"], "primary_category": "eess.AS"}
{"title": "Tandem Multitask Training of Speaker Diarisation and Speech Recognition\n  for Meeting Transcription", "abstract": "Self-supervised-learning-based pre-trained models for speech data, such as\nWav2Vec 2.0 (W2V2), have become the backbone of many speech tasks. In this\npaper, to achieve speaker diarisation and speech recognition using a single\nmodel, a tandem multitask training (TMT) method is proposed to fine-tune W2V2.\nFor speaker diarisation, the tasks of voice activity detection (VAD) and\nspeaker classification (SC) are required, and connectionist temporal\nclassification (CTC) is used for ASR. The multitask framework implements VAD,\nSC, and ASR using an early layer, middle layer, and late layer of W2V2, which\ncoincides with the order of segmenting the audio with VAD, clustering the\nsegments based on speaker embeddings, and transcribing each segment with ASR.\nExperimental results on the augmented multi-party (AMI) dataset showed that\nusing different W2V2 layers for VAD, SC, and ASR from the earlier to later\nlayers for TMT not only saves computational cost, but also reduces diarisation\nerror rates (DERs). Joint fine-tuning of VAD, SC, and ASR yielded 16%/17%\nrelative reductions of DER with manual/automatic segmentation respectively, and\nconsistent reductions in speaker attributed word error rate, compared to the\nbaseline with separately fine-tuned models.", "published": "2022-07-08 12:06:52", "link": "http://arxiv.org/abs/2207.03852v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "End-to-End Binaural Speech Synthesis", "abstract": "In this work, we present an end-to-end binaural speech synthesis system that\ncombines a low-bitrate audio codec with a powerful binaural decoder that is\ncapable of accurate speech binauralization while faithfully reconstructing\nenvironmental factors like ambient noise or reverb. The network is a modified\nvector-quantized variational autoencoder, trained with several carefully\ndesigned objectives, including an adversarial loss. We evaluate the proposed\nsystem on an internal binaural dataset with objective metrics and a perceptual\nstudy. Results show that the proposed approach matches the ground truth data\nmore closely than previous methods. In particular, we demonstrate the\ncapability of the adversarial loss in capturing environment effects needed to\ncreate an authentic auditory scene.", "published": "2022-07-08 05:18:36", "link": "http://arxiv.org/abs/2207.03697v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "BAST: Binaural Audio Spectrogram Transformer for Binaural Sound\n  Localization", "abstract": "Accurate sound localization in a reverberation environment is essential for\nhuman auditory perception. Recently, Convolutional Neural Networks (CNNs) have\nbeen utilized to model the binaural human auditory pathway. However, CNN shows\nbarriers in capturing the global acoustic features. To address this issue, we\npropose a novel end-to-end Binaural Audio Spectrogram Transformer (BAST) model\nto predict the sound azimuth in both anechoic and reverberation environments.\nTwo modes of implementation, i.e. BAST-SP and BAST-NSP corresponding to BAST\nmodel with shared and non-shared parameters respectively, are explored. Our\nmodel with subtraction interaural integration and hybrid loss achieves an\nangular distance of 1.29 degrees and a Mean Square Error of 1e-3 at all\nazimuths, significantly surpassing CNN based model. The exploratory analysis of\nthe BAST's performance on the left-right hemifields and anechoic and\nreverberation environments shows its generalization ability as well as the\nfeasibility of binaural Transformers in sound localization. Furthermore, the\nanalysis of the attention maps is provided to give additional insights on the\ninterpretation of the localization process in a natural reverberant\nenvironment.", "published": "2022-07-08 14:27:52", "link": "http://arxiv.org/abs/2207.03927v2", "categories": ["cs.SD", "cs.LG", "eess.AS", "I.2; I.5"], "primary_category": "cs.SD"}
{"title": "A Multi-tasking Model of Speaker-Keyword Classification for Keeping\n  Human in the Loop of Drone-assisted Inspection", "abstract": "Audio commands are a preferred communication medium to keep inspectors in the\nloop of civil infrastructure inspection performed by a semi-autonomous drone.\nTo understand job-specific commands from a group of heterogeneous and dynamic\ninspectors, a model must be developed cost-effectively for the group and easily\nadapted when the group changes. This paper is motivated to build a\nmulti-tasking deep learning model that possesses a Share-Split-Collaborate\narchitecture. This architecture allows the two classification tasks to share\nthe feature extractor and then split subject-specific and keyword-specific\nfeatures intertwined in the extracted features through feature projection and\ncollaborative training. A base model for a group of five authorized subjects is\ntrained and tested on the inspection keyword dataset collected by this study.\nThe model achieved a 95.3% or higher mean accuracy in classifying the keywords\nof any authorized inspectors. Its mean accuracy in speaker classification is\n99.2%. Due to the richer keyword representations that the model learns from the\npooled training data, adapting the base model to a new inspector requires only\na little training data from that inspector, like five utterances per keyword.\nUsing the speaker classification scores for inspector verification can achieve\na success rate of at least 93.9% in verifying authorized inspectors and 76.1%\nin detecting unauthorized ones. Further, the paper demonstrates the\napplicability of the proposed model to larger-size groups on a public dataset.\nThis paper provides a solution to addressing challenges facing AI-assisted\nhuman-robot interaction, including worker heterogeneity, worker dynamics, and\njob heterogeneity.", "published": "2022-07-08 17:32:51", "link": "http://arxiv.org/abs/2207.04027v3", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
