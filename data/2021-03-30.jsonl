{"title": "NaijaNER : Comprehensive Named Entity Recognition for 5 Nigerian\n  Languages", "abstract": "Most of the common applications of Named Entity Recognition (NER) is on\nEnglish and other highly available languages. In this work, we present our\nfindings on Named Entity Recognition for 5 Nigerian Languages (Nigerian\nEnglish, Nigerian Pidgin English, Igbo, Yoruba and Hausa). These languages are\nconsidered low-resourced, and very little openly available Natural Language\nProcessing work has been done in most of them. In this work, individual NER\nmodels were trained and metrics recorded for each of the languages. We also\nworked on a combined model that can handle Named Entity Recognition (NER) for\nany of the five languages. The combined model works well for Named Entity\nRecognition(NER) on each of the languages and with better performance compared\nto individual NER models trained specifically on annotated data for the\nspecific language. The aim of this work is to share our learning on how\ninformation extraction using Named Entity Recognition can be optimized for the\nlisted Nigerian Languages for inclusion, ease of deployment in production and\nreusability of models. Models developed during this project are available on\nGitHub https://git.io/JY0kk and an interactive web app\nhttps://nigner.herokuapp.com/.", "published": "2021-03-30 22:10:54", "link": "http://arxiv.org/abs/2105.00810v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Autocorrect in the Process of Translation -- Multi-task Learning\n  Improves Dialogue Machine Translation", "abstract": "Automatic translation of dialogue texts is a much needed demand in many real\nlife scenarios. However, the currently existing neural machine translation\ndelivers unsatisfying results. In this paper, we conduct a deep analysis of a\ndialogue corpus and summarize three major issues on dialogue translation,\nincluding pronoun dropping (\\droppro), punctuation dropping (\\droppun), and\ntypos (\\typo). In response to these challenges, we propose a joint learning\nmethod to identify omission and typo, and utilize context to translate dialogue\nutterances. To properly evaluate the performance, we propose a manually\nannotated dataset with 1,931 Chinese-English parallel utterances from 300\ndialogues as a benchmark testbed for dialogue translation. Our experiments show\nthat the proposed method improves translation quality by 3.2 BLEU over the\nbaselines. It also elevates the recovery rate of omitted pronouns from 26.09%\nto 47.16%. We will publish the code and dataset publicly at\nhttps://github.com/rgwt123/DialogueMT.", "published": "2021-03-30 09:12:47", "link": "http://arxiv.org/abs/2103.16189v2", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Grounding Dialogue Systems via Knowledge Graph Aware Decoding with\n  Pre-trained Transformers", "abstract": "Generating knowledge grounded responses in both goal and non-goal oriented\ndialogue systems is an important research challenge. Knowledge Graphs (KG) can\nbe viewed as an abstraction of the real world, which can potentially facilitate\na dialogue system to produce knowledge grounded responses. However, integrating\nKGs into the dialogue generation process in an end-to-end manner is a\nnon-trivial task. This paper proposes a novel architecture for integrating KGs\ninto the response generation process by training a BERT model that learns to\nanswer using the elements of the KG (entities and relations) in a multi-task,\nend-to-end setting. The k-hop subgraph of the KG is incorporated into the model\nduring training and inference using Graph Laplacian. Empirical evaluation\nsuggests that the model achieves better knowledge groundedness (measured via\nEntity F1 score) compared to other state-of-the-art models for both goal and\nnon-goal oriented dialogues.", "published": "2021-03-30 12:36:00", "link": "http://arxiv.org/abs/2103.16289v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Representing ELMo embeddings as two-dimensional text online", "abstract": "We describe a new addition to the WebVectors toolkit which is used to serve\nword embedding models over the Web. The new ELMoViz module adds support for\ncontextualized embedding architectures, in particular for ELMo models. The\nprovided visualizations follow the metaphor of `two-dimensional text' by\nshowing lexical substitutes: words which are most semantically similar in\ncontext to the words of the input sentence. The system allows the user to\nchange the ELMo layers from which token embeddings are inferred. It also\nconveys corpus information about the query words and their lexical substitutes\n(namely their frequency tiers and parts of speech). The module is well\nintegrated into the rest of the WebVectors toolkit, providing lexical\nhyperlinks to word representations in static embedding models. Two web services\nhave already implemented the new functionality with pre-trained ELMo models for\nRussian, Norwegian and English.", "published": "2021-03-30 15:12:29", "link": "http://arxiv.org/abs/2103.16414v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating the Morphosyntactic Well-formedness of Generated Texts", "abstract": "Text generation systems are ubiquitous in natural language processing\napplications. However, evaluation of these systems remains a challenge,\nespecially in multilingual settings. In this paper, we propose L'AMBRE -- a\nmetric to evaluate the morphosyntactic well-formedness of text using its\ndependency parse and morphosyntactic rules of the language. We present a way to\nautomatically extract various rules governing morphosyntax directly from\ndependency treebanks. To tackle the noisy outputs from text generation systems,\nwe propose a simple methodology to train robust parsers. We show the\neffectiveness of our metric on the task of machine translation through a\ndiachronic study of systems translating into morphologically-rich languages.", "published": "2021-03-30 18:02:58", "link": "http://arxiv.org/abs/2103.16590v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Collaborative construction of lexicographic and parallel datasets for\n  African languages: first assessment", "abstract": "Faced with a considerable lack of resources in African languages to carry out\nwork in Natural Language Processing (NLP), Natural Language Understanding (NLU)\nand artificial intelligence, the research teams of NTeALan association has set\nitself the objective of building open-source platforms for the collaborative\nconstruction of lexicographic data in African languages. In this article, we\npresent our first reports after 2 years of collaborative construction of\nlexicographic resources useful for African NLP tools.", "published": "2021-03-30 22:43:13", "link": "http://arxiv.org/abs/2103.16712v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BASE Layers: Simplifying Training of Large, Sparse Models", "abstract": "We introduce a new balanced assignment of experts (BASE) layer for large\nlanguage models that greatly simplifies existing high capacity sparse layers.\nSparse layers can dramatically improve the efficiency of training and inference\nby routing each token to specialized expert modules that contain only a small\nfraction of the model parameters. However, it can be difficult to learn\nbalanced routing functions that make full use of the available experts;\nexisting approaches typically use routing heuristics or auxiliary\nexpert-balancing loss functions. In contrast, we formulate token-to-expert\nallocation as a linear assignment problem, allowing an optimal assignment in\nwhich each expert receives an equal number of tokens. This optimal assignment\nscheme improves efficiency by guaranteeing balanced compute loads, and also\nsimplifies training by not requiring any new hyperparameters or auxiliary\nlosses. Code is publicly released at https://github.com/pytorch/fairseq/", "published": "2021-03-30 23:08:32", "link": "http://arxiv.org/abs/2103.16716v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AGQA: A Benchmark for Compositional Spatio-Temporal Reasoning", "abstract": "Visual events are a composition of temporal actions involving actors\nspatially interacting with objects. When developing computer vision models that\ncan reason about compositional spatio-temporal events, we need benchmarks that\ncan analyze progress and uncover shortcomings. Existing video question\nanswering benchmarks are useful, but they often conflate multiple sources of\nerror into one accuracy metric and have strong biases that models can exploit,\nmaking it difficult to pinpoint model weaknesses. We present Action Genome\nQuestion Answering (AGQA), a new benchmark for compositional spatio-temporal\nreasoning. AGQA contains $192M$ unbalanced question answer pairs for $9.6K$\nvideos. We also provide a balanced subset of $3.9M$ question answer pairs, $3$\norders of magnitude larger than existing benchmarks, that minimizes bias by\nbalancing the answer distributions and types of question structures. Although\nhuman evaluators marked $86.02\\%$ of our question-answer pairs as correct, the\nbest model achieves only $47.74\\%$ accuracy. In addition, AGQA introduces\nmultiple training/test splits to test for various reasoning abilities,\nincluding generalization to novel compositions, to indirect references, and to\nmore compositional steps. Using AGQA, we evaluate modern visual reasoning\nsystems, demonstrating that the best models barely perform better than\nnon-visual baselines exploiting linguistic biases and that none of the existing\nmodels generalize to novel compositions unseen during training.", "published": "2021-03-30 00:24:01", "link": "http://arxiv.org/abs/2103.16002v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Grounding Open-Domain Instructions to Automate Web Support Tasks", "abstract": "Grounding natural language instructions on the web to perform previously\nunseen tasks enables accessibility and automation. We introduce a task and\ndataset to train AI agents from open-domain, step-by-step instructions\noriginally written for people. We build RUSS (Rapid Universal Support Service)\nto tackle this problem. RUSS consists of two models: First, a BERT-LSTM with\npointers parses instructions to ThingTalk, a domain-specific language we design\nfor grounding natural language on the web. Then, a grounding model retrieves\nthe unique IDs of any webpage elements requested in ThingTalk. RUSS may\ninteract with the user through a dialogue (e.g. ask for an address) or execute\na web operation (e.g. click a button) inside the web runtime. To augment\ntraining, we synthesize natural language instructions mapped to ThingTalk. Our\ndataset consists of 80 different customer service problems from help websites,\nwith a total of 741 step-by-step instructions and their corresponding actions.\nRUSS achieves 76.7% end-to-end accuracy predicting agent actions from single\ninstructions. It outperforms state-of-the-art models that directly map\ninstructions to actions without ThingTalk. Our user study shows that RUSS is\npreferred by actual users over web navigation.", "published": "2021-03-30 04:02:34", "link": "http://arxiv.org/abs/2103.16057v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "XRJL-HKUST at SemEval-2021 Task 4: WordNet-Enhanced Dual Multi-head\n  Co-Attention for Reading Comprehension of Abstract Meaning", "abstract": "This paper presents our submitted system to SemEval 2021 Task 4: Reading\nComprehension of Abstract Meaning. Our system uses a large pre-trained language\nmodel as the encoder and an additional dual multi-head co-attention layer to\nstrengthen the relationship between passages and question-answer pairs,\nfollowing the current state-of-the-art model DUMA. The main difference is that\nwe stack the passage-question and question-passage attention modules instead of\ncalculating parallelly to simulate re-considering process. We also add a layer\nnormalization module to improve the performance of our model. Furthermore, to\nincorporate our known knowledge about abstract concepts, we retrieve the\ndefinitions of candidate answers from WordNet and feed them to the model as\nextra inputs. Our system, called WordNet-enhanced DUal Multi-head Co-Attention\n(WN-DUMA), achieves 86.67% and 89.99% accuracy on the official blind test set\nof subtask 1 and subtask 2 respectively.", "published": "2021-03-30 06:22:58", "link": "http://arxiv.org/abs/2103.16102v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AfriKI: Machine-in-the-Loop Afrikaans Poetry Generation", "abstract": "This paper proposes a generative language model called AfriKI. Our approach\nis based on an LSTM architecture trained on a small corpus of contemporary\nfiction. With the aim of promoting human creativity, we use the model as an\nauthoring tool to explore machine-in-the-loop Afrikaans poetry generation. To\nour knowledge, this is the first study to attempt creative text generation in\nAfrikaans.", "published": "2021-03-30 09:17:56", "link": "http://arxiv.org/abs/2103.16190v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Locally-Contextual Nonlinear CRFs for Sequence Labeling", "abstract": "Linear chain conditional random fields (CRFs) combined with contextual word\nembeddings have achieved state of the art performance on sequence labeling\ntasks. In many of these tasks, the identity of the neighboring words is often\nthe most useful contextual information when predicting the label of a given\nword. However, contextual embeddings are usually trained in a task-agnostic\nmanner. This means that although they may encode information about the\nneighboring words, it is not guaranteed. It can therefore be beneficial to\ndesign the sequence labeling architecture to directly extract this information\nfrom the embeddings. We propose locally-contextual nonlinear CRFs for sequence\nlabeling. Our approach directly incorporates information from the neighboring\nembeddings when predicting the label for a given word, and parametrizes the\npotential functions using deep neural networks. Our model serves as a drop-in\nreplacement for the linear chain CRF, consistently outperforming it in our\nablation study. On a variety of tasks, our results are competitive with those\nof the best published methods. In particular, we outperform the previous state\nof the art on chunking on CoNLL 2000 and named entity recognition on OntoNotes\n5.0 English.", "published": "2021-03-30 09:43:25", "link": "http://arxiv.org/abs/2103.16210v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Put Chatbot into Its Interlocutor's Shoes: New Framework to Learn\n  Chatbot Responding with Intention", "abstract": "Most chatbot literature that focuses on improving the fluency and coherence\nof a chatbot, is dedicated to making chatbots more human-like. However, very\nlittle work delves into what really separates humans from chatbots -- humans\nintrinsically understand the effect their responses have on the interlocutor\nand often respond with an intention such as proposing an optimistic view to\nmake the interlocutor feel better. This paper proposes an innovative framework\nto train chatbots to possess human-like intentions. Our framework includes a\nguiding chatbot and an interlocutor model that plays the role of humans. The\nguiding chatbot is assigned an intention and learns to induce the interlocutor\nto reply with responses matching the intention, for example, long responses,\njoyful responses, responses with specific words, etc. We examined our framework\nusing three experimental setups and evaluated the guiding chatbot with four\ndifferent metrics to demonstrate flexibility and performance advantages.\nAdditionally, we performed trials with human interlocutors to substantiate the\nguiding chatbot's effectiveness in influencing the responses of humans to a\ncertain extent. Code will be made available to the public.", "published": "2021-03-30 15:24:37", "link": "http://arxiv.org/abs/2103.16429v5", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Diagnosing Vision-and-Language Navigation: What Really Matters", "abstract": "Vision-and-language navigation (VLN) is a multimodal task where an agent\nfollows natural language instructions and navigates in visual environments.\nMultiple setups have been proposed, and researchers apply new model\narchitectures or training techniques to boost navigation performance. However,\nthere still exist non-negligible gaps between machines' performance and human\nbenchmarks. Moreover, the agents' inner mechanisms for navigation decisions\nremain unclear. To the best of our knowledge, how the agents perceive the\nmultimodal input is under-studied and needs investigation. In this work, we\nconduct a series of diagnostic experiments to unveil agents' focus during\nnavigation. Results show that indoor navigation agents refer to both object and\ndirection tokens when making decisions. In contrast, outdoor navigation agents\nheavily rely on direction tokens and poorly understand the object tokens.\nTransformer-based agents acquire a better cross-modal understanding of objects\nand display strong numerical reasoning ability than non-Transformer-based\nagents. When it comes to vision-and-language alignments, many models claim that\nthey can align object tokens with specific visual targets. We find unbalanced\nattention on the vision and text input and doubt the reliability of such\ncross-modal alignments.", "published": "2021-03-30 17:59:07", "link": "http://arxiv.org/abs/2103.16561v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Text Classification Using Hybrid Machine Learning Algorithms on Big Data", "abstract": "Recently, there are unprecedented data growth originating from different\nonline platforms which contribute to big data in terms of volume, velocity,\nvariety and veracity (4Vs). Given this nature of big data which is\nunstructured, performing analytics to extract meaningful information is\ncurrently a great challenge to big data analytics. Collecting and analyzing\nunstructured textual data allows decision makers to study the escalation of\ncomments/posts on our social media platforms. Hence, there is need for\nautomatic big data analysis to overcome the noise and the non-reliability of\nthese unstructured dataset from the digital media platforms. However, current\nmachine learning algorithms used are performance driven focusing on the\nclassification/prediction accuracy based on known properties learned from the\ntraining samples. With the learning task in a large dataset, most machine\nlearning models are known to require high computational cost which eventually\nleads to computational complexity. In this work, two supervised machine\nlearning algorithms are combined with text mining techniques to produce a\nhybrid model which consists of Na\\\"ive Bayes and support vector machines (SVM).\nThis is to increase the efficiency and accuracy of the results obtained and\nalso to reduce the computational cost and complexity. The system also provides\nan open platform where a group of persons with a common interest can share\ntheir comments/messages and these comments classified automatically as legal or\nillegal. This improves the quality of conversation among users. The hybrid\nmodel was developed using WEKA tools and Java programming language. The result\nshows that the hybrid model gave 96.76% accuracy as against the 61.45% and\n69.21% of the Na\\\"ive Bayes and SVM models respectively.", "published": "2021-03-30 19:02:48", "link": "http://arxiv.org/abs/2103.16624v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Pre-training for low resource speech-to-intent applications", "abstract": "Designing a speech-to-intent (S2I) agent which maps the users' spoken\ncommands to the agents' desired task actions can be challenging due to the\ndiverse grammatical and lexical preference of different users. As a remedy, we\ndiscuss a user-taught S2I system in this paper. The user-taught system learns\nfrom scratch from the users' spoken input with action demonstration, which\nensure it is fully matched to the users' way of formulating intents and their\narticulation habits. The main issue is the scarce training data due to the user\neffort involved. Existing state-of-art approaches in this setting are based on\nnon-negative matrix factorization (NMF) and capsule networks. In this paper we\ncombine the encoder of an end-to-end ASR system with the prior NMF/capsule\nnetwork-based user-taught decoder, and investigate whether pre-training\nmethodology can reduce training data requirements for the NMF and capsule\nnetwork. Experimental results show the pre-trained ASR-NMF framework\nsignificantly outperforms other models, and also, we discuss limitations of\npre-training with different types of command-and-control(C&C) applications.", "published": "2021-03-30 20:44:29", "link": "http://arxiv.org/abs/2103.16674v1", "categories": ["eess.AS", "cs.CL", "cs.HC", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A study of latent monotonic attention variants", "abstract": "End-to-end models reach state-of-the-art performance for speech recognition,\nbut global soft attention is not monotonic, which might lead to convergence\nproblems, to instability, to bad generalisation, cannot be used for online\nstreaming, and is also inefficient in calculation. Monotonicity can potentially\nfix all of this. There are several ad-hoc solutions or heuristics to introduce\nmonotonicity, but a principled introduction is rarely found in literature so\nfar. In this paper, we present a mathematically clean solution to introduce\nmonotonicity, by introducing a new latent variable which represents the audio\nposition or segment boundaries. We compare several monotonic latent models to\nour global soft attention baseline such as a hard attention model, a local\nwindowed soft attention model, and a segmental soft attention model. We can\nshow that our monotonic models perform as good as the global soft attention\nmodel. We perform our experiments on Switchboard 300h. We carefully outline the\ndetails of our training and release our code and configs.", "published": "2021-03-30 22:35:56", "link": "http://arxiv.org/abs/2103.16710v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Grounding Physical Concepts of Objects and Events Through Dynamic Visual\n  Reasoning", "abstract": "We study the problem of dynamic visual reasoning on raw videos. This is a\nchallenging problem; currently, state-of-the-art models often require dense\nsupervision on physical object properties and events from simulation, which are\nimpractical to obtain in real life. In this paper, we present the Dynamic\nConcept Learner (DCL), a unified framework that grounds physical objects and\nevents from video and language. DCL first adopts a trajectory extractor to\ntrack each object over time and to represent it as a latent, object-centric\nfeature vector. Building upon this object-centric representation, DCL learns to\napproximate the dynamic interaction among objects using graph networks. DCL\nfurther incorporates a semantic parser to parse questions into semantic\nprograms and, finally, a program executor to run the program to answer the\nquestion, levering the learned dynamics model. After training, DCL can detect\nand associate objects across the frames, ground visual properties, and physical\nevents, understand the causal relationship between events, make future and\ncounterfactual predictions, and leverage these extracted presentations for\nanswering queries. DCL achieves state-of-the-art performance on CLEVRER, a\nchallenging causal video reasoning dataset, even without using ground-truth\nattributes and collision labels from simulations for training. We further test\nDCL on a newly proposed video-retrieval and event localization dataset derived\nfrom CLEVRER, showing its strong generalization capacity.", "published": "2021-03-30 17:59:48", "link": "http://arxiv.org/abs/2103.16564v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.SC"], "primary_category": "cs.CV"}
{"title": "Audio classification of the content of food containers and drinking\n  glasses", "abstract": "Food containers, drinking glasses and cups handled by a person generate\nsounds that vary with the type and amount of their content. In this paper, we\npropose a new model for sound-based classification of the type and amount of\ncontent in a container. The proposed model is based on the decomposition of the\nproblem into two steps, namely action recognition and content classification.\nWe use the scenario of the recent CORSMAL Containers Manipulation dataset and\nconsider two actions (shaking and pouring), and seven combinations of material\nand filling level. The first step identifies the action performed by a person\nwith the container. The second step determines the amount and type of content\nusing an action-specific classifier. Experiments show that the proposed model\nachieves 76.02, 78.24, and 41.89 weighted average F1 score on the three test\nsets, respectively, and outperforms baselines and existing approaches that\nclassify the content amount and type either independently or jointly.", "published": "2021-03-30 00:18:34", "link": "http://arxiv.org/abs/2103.15999v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MediaSpeech: Multilanguage ASR Benchmark and Dataset", "abstract": "The performance of automated speech recognition (ASR) systems is well known\nto differ for varied application domains. At the same time, vendors and\nresearch groups typically report ASR quality results either for limited use\nsimplistic domains (audiobooks, TED talks), or proprietary datasets. To fill\nthis gap, we provide an open-source 10-hour ASR system evaluation dataset NTR\nMediaSpeech for 4 languages: Spanish, French, Turkish and Arabic. The dataset\nwas collected from the official youtube channels of media in the respective\nlanguages, and manually transcribed. We estimate that the WER of the dataset is\nunder 5%. We have benchmarked many ASR systems available both commercially and\nfreely, and provide the benchmark results. We also open-source baseline\nQuartzNet models for each language.", "published": "2021-03-30 09:21:44", "link": "http://arxiv.org/abs/2103.16193v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Enhancing Segment-Based Speech Emotion Recognition by Deep Self-Learning", "abstract": "Despite the widespread utilization of deep neural networks (DNNs) for speech\nemotion recognition (SER), they are severely restricted due to the paucity of\nlabeled data for training. Recently, segment-based approaches for SER have been\nevolving, which train backbone networks on shorter segments instead of whole\nutterances, and thus naturally augments training examples without additional\nresources. However, one core challenge remains for segment-based approaches:\nmost emotional corpora do not provide ground-truth labels at the segment level.\nTo supervisely train a segment-based emotion model on such datasets, the most\ncommon way assigns each segment the corresponding utterance's emotion label.\nHowever, this practice typically introduces noisy (incorrect) labels as\nemotional information is not uniformly distributed across the whole utterance.\nOn the other hand, DNNs have been shown to easily over-fit a dataset when being\ntrained with noisy labels. To this end, this work proposes a simple and\neffective deep self-learning (DSL) framework, which comprises a procedure to\nprogressively correct segment-level labels in an iterative learning manner. The\nDSL method produces dynamically-generated and soft emotion labels, leading to\nsignificant performance improvements. Experiments on three well-known emotional\ncorpora demonstrate noticeable gains using the proposed method.", "published": "2021-03-30 16:02:31", "link": "http://arxiv.org/abs/2103.16456v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Environmental sound analysis with mixup based multitask learning and\n  cross-task fusion", "abstract": "Environmental sound analysis is currently getting more and more attentions.\nIn the domain, acoustic scene classification and acoustic event classification\nare two closely related tasks. In this letter, a two-stage method is proposed\nfor the above tasks. In the first stage, a mixup based MTL solution is proposed\nto classify both tasks in one single convolutional neural network. Artificial\nmulti-label samples are used in the training of the MTL model, which are mixed\nup using existing single-task datasets. The multi-task model obtained can\neffectively recognize both the acoustic scenes and events. Compared with other\nmethods such as re-annotation or synthesis, the mixup based MTL is low-cost,\nflexible and effective. In the second stage, the MTL model is modified into a\nsingle-task model which is fine-tuned using the original dataset corresponding\nto the specific task. By controlling the frozen layers carefully, the\ntask-specific high level features are fused and the performance of the single\nclassification task is further improved. The proposed method has confirmed the\ncomplementary characteristics of acoustic scene and acoustic event\nclassifications. Finally, enhanced by ensemble learning, a satisfactory\naccuracy of 84.5 percent on TUT acoustic scene 2017 dataset and an accuracy of\n77.5 percent on ESC-50 dataset are achieved respectively.", "published": "2021-03-30 05:11:53", "link": "http://arxiv.org/abs/2103.16079v1", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Symbolic Music Generation with Diffusion Models", "abstract": "Score-based generative models and diffusion probabilistic models have been\nsuccessful at generating high-quality samples in continuous domains such as\nimages and audio. However, due to their Langevin-inspired sampling mechanisms,\ntheir application to discrete and sequential data has been limited. In this\nwork, we present a technique for training diffusion models on sequential data\nby parameterizing the discrete domain in the continuous latent space of a\npre-trained variational autoencoder. Our method is non-autoregressive and\nlearns to generate sequences of latent embeddings through the reverse process\nand offers parallel generation with a constant number of iterative refinement\nsteps. We apply this technique to modeling symbolic music and show strong\nunconditional generation and post-hoc conditional infilling results compared to\nautoregressive language models operating over the same continuous embeddings.", "published": "2021-03-30 05:48:05", "link": "http://arxiv.org/abs/2103.16091v2", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Time-domain Speech Enhancement with Generative Adversarial Learning", "abstract": "Speech enhancement aims to obtain speech signals with high intelligibility\nand quality from noisy speech. Recent work has demonstrated the excellent\nperformance of time-domain deep learning methods, such as Conv-TasNet. However,\nthese methods can be degraded by the arbitrary scales of the waveform induced\nby the scale-invariant signal-to-noise ratio (SI-SNR) loss. This paper proposes\na new framework called Time-domain Speech Enhancement Generative Adversarial\nNetwork (TSEGAN), which is an extension of the generative adversarial network\n(GAN) in time-domain with metric evaluation to mitigate the scaling problem,\nand provide model training stability, thus achieving performance improvement.\nIn addition, we provide a new method based on objective function mapping for\nthe theoretical analysis of the performance of Metric GAN, and explain why it\nis better than the Wasserstein GAN. Experiments conducted demonstrate the\neffectiveness of our proposed method, and illustrate the advantage of Metric\nGAN.", "published": "2021-03-30 08:09:49", "link": "http://arxiv.org/abs/2103.16149v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Target Speaker Verification with Selective Auditory Attention for Single\n  and Multi-talker Speech", "abstract": "Speaker verification has been studied mostly under the single-talker\ncondition. It is adversely affected in the presence of interference speakers.\nInspired by the study on target speaker extraction, e.g., SpEx, we propose a\nunified speaker verification framework for both single- and multi-talker\nspeech, that is able to pay selective auditory attention to the target speaker.\nThis target speaker verification (tSV) framework jointly optimizes a speaker\nattention module and a speaker representation module via multi-task learning.\nWe study four different target speaker embedding schemes under the tSV\nframework. The experimental results show that all four target speaker embedding\nschemes significantly outperform other competitive solutions for multi-talker\nspeech. Notably, the best tSV speaker embedding scheme achieves 76.0% and 55.3%\nrelative improvements over the baseline system on the WSJ0-2mix-extr and\nLibri2Mix corpora in terms of equal-error-rate for 2-talker speech, while the\nperformance of tSV for single-talker speech is on par with that of traditional\nspeaker verification system, that is trained and evaluated under the same\nsingle-talker condition.", "published": "2021-03-30 11:40:35", "link": "http://arxiv.org/abs/2103.16269v2", "categories": ["eess.AS", "cs.HC", "cs.SD"], "primary_category": "eess.AS"}
