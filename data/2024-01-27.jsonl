{"title": "How We Refute Claims: Automatic Fact-Checking through Flaw\n  Identification and Explanation", "abstract": "Automated fact-checking is a crucial task in the governance of internet\ncontent. Although various studies utilize advanced models to tackle this issue,\na significant gap persists in addressing complex real-world rumors and\ndeceptive claims. To address this challenge, this paper explores the novel task\nof flaw-oriented fact-checking, including aspect generation and flaw\nidentification. We also introduce RefuteClaim, a new framework designed\nspecifically for this task. Given the absence of an existing dataset, we\npresent FlawCheck, a dataset created by extracting and transforming insights\nfrom expert reviews into relevant aspects and identified flaws. The\nexperimental results underscore the efficacy of RefuteClaim, particularly in\nclassifying and elucidating false claims.", "published": "2024-01-27 06:06:16", "link": "http://arxiv.org/abs/2401.15312v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UNSEE: Unsupervised Non-contrastive Sentence Embeddings", "abstract": "We present UNSEE: Unsupervised Non-Contrastive Sentence Embeddings, a novel\napproach that outperforms SimCSE in the Massive Text Embedding benchmark. Our\nexploration begins by addressing the challenge of representation collapse, a\nphenomenon observed when contrastive objectives in SimCSE are replaced with\nnon-contrastive objectives. To counter this issue, we propose a straightforward\nsolution known as the target network, effectively mitigating representation\ncollapse. The introduction of the target network allows us to leverage\nnon-contrastive objectives, maintaining training stability while achieving\nperformance improvements comparable to contrastive objectives. Our method has\nachieved peak performance in non-contrastive sentence embeddings through\nmeticulous fine-tuning and optimization. This comprehensive effort has yielded\nsuperior sentence representation models, showcasing the effectiveness of our\napproach.", "published": "2024-01-27 06:29:07", "link": "http://arxiv.org/abs/2401.15316v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Equipping Language Models with Tool Use Capability for Tabular Data\n  Analysis in Finance", "abstract": "Large language models (LLMs) have exhibited an array of reasoning\ncapabilities but face challenges like error propagation and hallucination,\nparticularly in specialised areas like finance, where data is heterogeneous,\nand precision is paramount. We explore the potential of language model\naugmentation with external tools to mitigate these limitations and offload\ncertain reasoning steps to external tools that are more suited for the task,\ninstead of solely depending on the LLM's inherent abilities. More concretely,\nusing financial domain question-answering datasets, we apply supervised\nfine-tuning on a LLaMA-2 13B Chat model to act both as a 'task router' and\n'task solver'. The 'task router' dynamically directs a question to either be\nanswered internally by the LLM or externally via the right tool from the tool\nset. Our tool-equipped SFT model, Raven, demonstrates an improvement of 35.2%\nand 5.06% over the base model and SFT-only baselines, respectively, and is\nhighly competitive with strong GPT-3.5 results. To the best of our knowledge,\nour work is the first that investigates tool augmentation of language models\nfor the finance domain.", "published": "2024-01-27 07:08:37", "link": "http://arxiv.org/abs/2401.15328v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Importance-Aware Data Augmentation for Document-Level Neural Machine\n  Translation", "abstract": "Document-level neural machine translation (DocNMT) aims to generate\ntranslations that are both coherent and cohesive, in contrast to its\nsentence-level counterpart. However, due to its longer input length and limited\navailability of training data, DocNMT often faces the challenge of data\nsparsity. To overcome this issue, we propose a novel Importance-Aware Data\nAugmentation (IADA) algorithm for DocNMT that augments the training data based\non token importance information estimated by the norm of hidden states and\ntraining gradients. We conduct comprehensive experiments on three widely-used\nDocNMT benchmarks. Our empirical results show that our proposed IADA\noutperforms strong DocNMT baselines as well as several data augmentation\napproaches, with statistical significance on both sentence-level and\ndocument-level BLEU.", "published": "2024-01-27 09:27:47", "link": "http://arxiv.org/abs/2401.15360v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LegalDuet: Learning Effective Representations for Legal Judgment\n  Prediction through a Dual-View Legal Clue Reasoning", "abstract": "Most existing Legal Judgment Prediction (LJP) models focus on discovering the\nlegal triggers in the criminal fact description. However, in real-world\nscenarios, a professional judge not only needs to assimilate the law case\nexperience that thrives on past sentenced legal judgments but also depends on\nthe professional legal grounded reasoning that learned from professional legal\nknowledge. In this paper, we propose a LegalDuet model, which pretrains\nlanguage models to learn a tailored embedding space for making legal judgments.\nIt proposes a dual-view legal clue reasoning mechanism, which derives from two\nreasoning chains of judges: 1) Law Case Reasoning, which makes legal judgments\naccording to the judgment experiences learned from analogy/confusing legal\ncases; 2) Legal Ground Reasoning, which lies in matching the legal clues\nbetween criminal cases and legal decisions. Our experiments show that LegalDuet\nachieves state-of-the-art performance on the CAIL2018 dataset and outperforms\nbaselines with about 4% improvements on average. Our dual-view reasoning based\npretraining can capture critical legal clues to learn a tailored embedding\nspace to distinguish criminal cases. It reduces LegalDuet's uncertainty during\nprediction and brings pretraining advances to the confusing/low frequent\ncharges. All codes are available at https://github.com/NEUIR/LegalDuet.", "published": "2024-01-27 10:28:27", "link": "http://arxiv.org/abs/2401.15371v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop\n  Queries", "abstract": "Retrieval-augmented generation (RAG) augments large language models (LLM) by\nretrieving relevant knowledge, showing promising potential in mitigating LLM\nhallucinations and enhancing response quality, thereby facilitating the great\nadoption of LLMs in practice. However, we find that existing RAG systems are\ninadequate in answering multi-hop queries, which require retrieving and\nreasoning over multiple pieces of supporting evidence. Furthermore, to our\nknowledge, no existing RAG benchmarking dataset focuses on multi-hop queries.\nIn this paper, we develop a novel dataset, MultiHop-RAG, which consists of a\nknowledge base, a large collection of multi-hop queries, their ground-truth\nanswers, and the associated supporting evidence. We detail the procedure of\nbuilding the dataset, utilizing an English news article dataset as the\nunderlying RAG knowledge base. We demonstrate the benchmarking utility of\nMultiHop-RAG in two experiments. The first experiment compares different\nembedding models for retrieving evidence for multi-hop queries. In the second\nexperiment, we examine the capabilities of various state-of-the-art LLMs,\nincluding GPT-4, PaLM, and Llama2-70B, in reasoning and answering multi-hop\nqueries given the evidence. Both experiments reveal that existing RAG methods\nperform unsatisfactorily in retrieving and answering multi-hop queries. We hope\nMultiHop-RAG will be a valuable resource for the community in developing\neffective RAG systems, thereby facilitating greater adoption of LLMs in\npractice. The MultiHop-RAG and implemented RAG system is publicly available at\nhttps://github.com/yixuantt/MultiHop-RAG/.", "published": "2024-01-27 11:41:48", "link": "http://arxiv.org/abs/2401.15391v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantics of Multiword Expressions in Transformer-Based Models: A Survey", "abstract": "Multiword expressions (MWEs) are composed of multiple words and exhibit\nvariable degrees of compositionality. As such, their meanings are notoriously\ndifficult to model, and it is unclear to what extent this issue affects\ntransformer architectures. Addressing this gap, we provide the first in-depth\nsurvey of MWE processing with transformer models. We overall find that they\ncapture MWE semantics inconsistently, as shown by reliance on surface patterns\nand memorized information. MWE meaning is also strongly localized,\npredominantly in early layers of the architecture. Representations benefit from\nspecific linguistic properties, such as lower semantic idiosyncrasy and\nambiguity of target expressions. Our findings overall question the ability of\ntransformer models to robustly capture fine-grained semantics. Furthermore, we\nhighlight the need for more directly comparable evaluation setups.", "published": "2024-01-27 11:51:11", "link": "http://arxiv.org/abs/2401.15393v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pre-training and Diagnosing Knowledge Base Completion Models", "abstract": "In this work, we introduce and analyze an approach to knowledge transfer from\none collection of facts to another without the need for entity or relation\nmatching. The method works for both canonicalized knowledge bases and\nuncanonicalized or open knowledge bases, i.e., knowledge bases where more than\none copy of a real-world entity or relation may exist. The main contribution is\na method that can make use of large-scale pre-training on facts, which were\ncollected from unstructured text, to improve predictions on structured data\nfrom a specific domain. The introduced method is most impactful on small\ndatasets such as ReVerb20k, where a 6% absolute increase of mean reciprocal\nrank and 65% relative decrease of mean rank over the previously best method was\nachieved, despite not relying on large pre-trained models like Bert. To\nunderstand the obtained pre-trained models better, we then introduce a novel\ndataset for the analysis of pre-trained models for Open Knowledge Base\nCompletion, called Doge (Diagnostics of Open knowledge Graph Embeddings). It\nconsists of 6 subsets and is designed to measure multiple properties of a\npre-trained model: robustness against synonyms, ability to perform deductive\nreasoning, presence of gender stereotypes, consistency with reverse relations,\nand coverage of different areas of general knowledge. Using the introduced\ndataset, we show that the existing OKBC models lack consistency in the presence\nof synonyms and inverse relations and are unable to perform deductive\nreasoning. Moreover, their predictions often align with gender stereotypes,\nwhich persist even when presented with counterevidence. We additionally\ninvestigate the role of pre-trained word embeddings and demonstrate that\navoiding biased word embeddings is not a sufficient measure to prevent biased\nbehavior of OKBC models.", "published": "2024-01-27 15:20:43", "link": "http://arxiv.org/abs/2401.15439v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Trust Your Feelings: Leveraging Self-awareness in LLMs for\n  Hallucination Mitigation", "abstract": "We evaluate the ability of Large Language Models (LLMs) to discern and\nexpress their internal knowledge state, a key factor in countering factual\nhallucination and ensuring reliable application of LLMs. We observe a robust\nself-awareness of internal knowledge state in LLMs, evidenced by over 85%\naccuracy in knowledge probing. However, LLMs often fail to express their\ninternal knowledge during generation, leading to factual hallucinations. We\ndevelop an automated hallucination annotation tool, Dreamcatcher, which merges\nknowledge probing and consistency checking methods to rank factual preference\ndata. Using knowledge preference as reward, We propose a Reinforcement Learning\nfrom Knowledge Feedback (RLKF) training framework, leveraging reinforcement\nlearning to enhance the factuality and honesty of LLMs. Our experiments across\nmultiple models show that RLKF training effectively enhances the ability of\nmodels to utilize their internal knowledge state, boosting performance in a\nvariety of knowledge-based and honesty-related tasks.", "published": "2024-01-27 16:19:30", "link": "http://arxiv.org/abs/2401.15449v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ConvoSense: Overcoming Monotonous Commonsense Inferences for\n  Conversational AI", "abstract": "Mastering commonsense understanding and reasoning is a pivotal skill\nessential for conducting engaging conversations. While there have been several\nattempts to create datasets that facilitate commonsense inferences in dialogue\ncontexts, existing datasets tend to lack in-depth details, restate information\nalready present in the conversation, and often fail to capture the multifaceted\nnature of commonsense reasoning. In response to these limitations, we compile a\nnew synthetic dataset for commonsense reasoning in dialogue contexts using GPT,\nConvoSense, that boasts greater contextual novelty, offers a higher volume of\ninferences per example, and substantially enriches the detail conveyed by the\ninferences. Our dataset contains over 500,000 inferences across 12,000\ndialogues with 10 popular inference types, which empowers the training of\ngenerative commonsense models for dialogue that are superior in producing\nplausible inferences with high novelty when compared to models trained on the\nprevious datasets. To the best of our knowledge, ConvoSense is the first of its\nkind to provide such a multitude of novel inferences at such a large scale.", "published": "2024-01-27 17:51:05", "link": "http://arxiv.org/abs/2401.15471v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "To Burst or Not to Burst: Generating and Quantifying Improbable Text", "abstract": "While large language models (LLMs) are extremely capable at text generation,\ntheir outputs are still distinguishable from human-authored text. We explore\nthis separation across many metrics over text, many sampling techniques, many\ntypes of text data, and across two popular LLMs, LLaMA and Vicuna. Along the\nway, we introduce a new metric, recoverability, to highlight differences\nbetween human and machine text; and we propose a new sampling technique, burst\nsampling, designed to close this gap. We find that LLaMA and Vicuna have\ndistinct distributions under many of the metrics, and that this influences our\nresults: Recoverability separates real from fake text better than any other\nmetric when using LLaMA. When using Vicuna, burst sampling produces text which\nis distributionally closer to real text compared to other sampling techniques.", "published": "2024-01-27 18:34:29", "link": "http://arxiv.org/abs/2401.15476v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do We Need Language-Specific Fact-Checking Models? The Case of Chinese", "abstract": "This paper investigates the potential benefits of language-specific\nfact-checking models, focusing on the case of Chinese. We first demonstrate the\nlimitations of translation-based methods and multilingual large language models\n(e.g., GPT-4), highlighting the need for language-specific systems. We further\npropose a Chinese fact-checking system that can better retrieve evidence from a\ndocument by incorporating context information. To better analyze token-level\nbiases in different systems, we construct an adversarial dataset based on the\nCHEF dataset, where each instance has large word overlap with the original one\nbut holds the opposite veracity label. Experimental results on the CHEF dataset\nand our adversarial dataset show that our proposed method outperforms\ntranslation-based methods and multilingual LLMs and is more robust toward\nbiases, while there is still large room for improvement, emphasizing the\nimportance of language-specific fact-checking systems.", "published": "2024-01-27 20:26:03", "link": "http://arxiv.org/abs/2401.15498v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic Properties of cosine based bias scores for word embeddings", "abstract": "Plenty of works have brought social biases in language models to attention\nand proposed methods to detect such biases. As a result, the literature\ncontains a great deal of different bias tests and scores, each introduced with\nthe premise to uncover yet more biases that other scores fail to detect. What\nseverely lacks in the literature, however, are comparative studies that analyse\nsuch bias scores and help researchers to understand the benefits or limitations\nof the existing methods. In this work, we aim to close this gap for cosine\nbased bias scores. By building on a geometric definition of bias, we propose\nrequirements for bias scores to be considered meaningful for quantifying\nbiases. Furthermore, we formally analyze cosine based scores from the\nliterature with regard to these requirements. We underline these findings with\nexperiments to show that the bias scores' limitations have an impact in the\napplication case.", "published": "2024-01-27 20:31:10", "link": "http://arxiv.org/abs/2401.15499v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Comprehensive Survey of Compression Algorithms for Language Models", "abstract": "How can we compress language models without sacrificing accuracy? The number\nof compression algorithms for language models is rapidly growing to benefit\nfrom remarkable advances of recent language models without side effects due to\nthe gigantic size of language models, such as increased carbon emissions and\nexpensive maintenance fees. While numerous compression algorithms have shown\nremarkable progress in compressing language models, it ironically becomes\nchallenging to capture emerging trends and identify the fundamental concepts\nunderlying them due to the excessive number of algorithms. In this paper, we\nsurvey and summarize diverse compression algorithms including pruning,\nquantization, knowledge distillation, low-rank approximation, parameter\nsharing, and efficient architecture design. We not only summarize the overall\ntrend of diverse compression algorithms but also select representative\nalgorithms and provide in-depth analyses of them. We discuss the value of each\ncategory of compression algorithms, and the desired properties of low-cost\ncompression algorithms which have a significant impact due to the emergence of\nlarge language models. Finally, we introduce promising future research topics\nbased on our survey results.", "published": "2024-01-27 08:38:56", "link": "http://arxiv.org/abs/2401.15347v1", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "A RAG-based Question Answering System Proposal for Understanding Islam:\n  MufassirQAS LLM", "abstract": "Challenges exist in learning and understanding religions, such as the\ncomplexity and depth of religious doctrines and teachings. Chatbots as\nquestion-answering systems can help in solving these challenges. LLM chatbots\nuse NLP techniques to establish connections between topics and accurately\nrespond to complex questions. These capabilities make it perfect for\nenlightenment on religion as a question-answering chatbot. However, LLMs also\ntend to generate false information, known as hallucination. Also, the chatbots'\nresponses can include content that insults personal religious beliefs,\ninterfaith conflicts, and controversial or sensitive topics. It must avoid such\ncases without promoting hate speech or offending certain groups of people or\ntheir beliefs. This study uses a vector database-based Retrieval Augmented\nGeneration (RAG) approach to enhance the accuracy and transparency of LLMs. Our\nquestion-answering system is called \"MufassirQAS\". We created a database\nconsisting of several open-access books that include Turkish context. These\nbooks contain Turkish translations and interpretations of Islam. This database\nis utilized to answer religion-related questions and ensure our answers are\ntrustworthy. The relevant part of the dataset, which LLM also uses, is\npresented along with the answer. We have put careful effort into creating\nsystem prompts that give instructions to prevent harmful, offensive, or\ndisrespectful responses to respect people's values and provide reliable\nresults. The system answers and shares additional information, such as the page\nnumber from the respective book and the articles referenced for obtaining the\ninformation. MufassirQAS and ChatGPT are also tested with sensitive questions.\nWe got better performance with our system. Study and enhancements are still in\nprogress. Results and future works are given.", "published": "2024-01-27 10:50:11", "link": "http://arxiv.org/abs/2401.15378v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Event Extraction from Speech with Contextual Clues", "abstract": "While text-based event extraction has been an active research area and has\nseen successful application in many domains, extracting semantic events from\nspeech directly is an under-explored problem. In this paper, we introduce the\nSpeech Event Extraction (SpeechEE) task and construct three synthetic training\nsets and one human-spoken test set. Compared to event extraction from text,\nSpeechEE poses greater challenges mainly due to complex speech signals that are\ncontinuous and have no word boundaries. Additionally, unlike perceptible sound\nevents, semantic events are more subtle and require a deeper understanding. To\ntackle these challenges, we introduce a sequence-to-structure generation\nparadigm that can produce events from speech signals in an end-to-end manner,\ntogether with a conditioned generation method that utilizes speech recognition\ntranscripts as the contextual clue. We further propose to represent events with\na flat format to make outputs more natural language-like. Our experimental\nresults show that our method brings significant improvements on all datasets,\nachieving a maximum F1 gain of 10.7%. The code and datasets are released on\nhttps://github.com/jodie-kang/SpeechEE.", "published": "2024-01-27 11:07:19", "link": "http://arxiv.org/abs/2401.15385v1", "categories": ["cs.CL", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Indexing Portuguese NLP Resources with PT-Pump-Up", "abstract": "The recent advances in natural language processing (NLP) are linked to\ntraining processes that require vast amounts of corpora. Access to this data is\ncommonly not a trivial process due to resource dispersion and the need to\nmaintain these infrastructures online and up-to-date. New developments in NLP\nare often compromised due to the scarcity of data or lack of a shared\nrepository that works as an entry point to the community. This is especially\ntrue in low and mid-resource languages, such as Portuguese, which lack data and\nproper resource management infrastructures. In this work, we propose\nPT-Pump-Up, a set of tools that aim to reduce resource dispersion and improve\nthe accessibility to Portuguese NLP resources. Our proposal is divided into\nfour software components: a) a web platform to list the available resources; b)\na client-side Python package to simplify the loading of Portuguese NLP\nresources; c) an administrative Python package to manage the platform and d) a\npublic GitHub repository to foster future collaboration and contributions. All\nfour components are accessible using: https://linktr.ee/pt_pump_up", "published": "2024-01-27 12:33:07", "link": "http://arxiv.org/abs/2401.15400v1", "categories": ["cs.CL", "cs.IR", "68P20", "I.7.1"], "primary_category": "cs.CL"}
{"title": "DataFrame QA: A Universal LLM Framework on DataFrame Question Answering\n  Without Data Exposure", "abstract": "This paper introduces DataFrame question answering (QA), a novel task that\nutilizes large language models (LLMs) to generate Pandas queries for\ninformation retrieval and data analysis on dataframes, emphasizing safe and\nnon-revealing data handling. Our method, which solely relies on dataframe\ncolumn names, not only ensures data privacy but also significantly reduces the\ncontext window in the prompt, streamlining information processing and\naddressing major challenges in LLM-based data analysis. We propose DataFrame QA\nas a comprehensive framework that includes safe Pandas query generation and\ncode execution. Various LLMs, notably GPT-4, are evaluated using the pass@1\nmetric on the renowned WikiSQL and our newly developed 'UCI-DataFrameQA',\ntailored for complex data analysis queries. Our findings indicate that GPT-4\nachieves pass@1 rates of 86% on WikiSQL and 97% on UCI-DataFrameQA,\nunderscoring its capability in securely retrieving and aggregating dataframe\nvalues and conducting sophisticated data analyses. This approach, deployable in\na zero-shot manner without prior training or adjustments, proves to be highly\nadaptable and secure for diverse applications.", "published": "2024-01-27 17:06:53", "link": "http://arxiv.org/abs/2401.15463v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing Large Language Model Performance To Answer Questions and\n  Extract Information More Accurately", "abstract": "Large Language Models (LLMs) generate responses to questions; however, their\neffectiveness is often hindered by sub-optimal quality of answers and\noccasional failures to provide accurate responses to questions. To address\nthese challenges, a fine-tuning process is employed, involving feedback and\nexamples to refine models. The objective is to enhance AI models through\ncontinuous feedback loops, utilizing metrics such as cosine similarity, LLM\nevaluation and Rouge-L scores to evaluate the models. Leveraging LLMs like\nGPT-3.5, GPT4ALL, and LLaMA2, and Claude, this approach is benchmarked on\nfinancial datasets, including the FinanceBench and RAG Instruct Benchmark\nTester Dataset, illustrating the necessity of fine-tuning. The results showcase\nthe capability of fine-tuned models to surpass the accuracy of zero-shot LLMs,\nproviding superior question and answering capabilities. Notably, the\ncombination of fine-tuning the LLM with a process known as Retrieval Augmented\nGeneration (RAG) proves to generate responses with improved accuracy.", "published": "2024-01-27 00:18:07", "link": "http://arxiv.org/abs/2402.01722v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Empirical Study on Large Language Models in Accuracy and Robustness\n  under Chinese Industrial Scenarios", "abstract": "Recent years have witnessed the rapid development of large language models\n(LLMs) in various domains. To better serve the large number of Chinese users,\nmany commercial vendors in China have adopted localization strategies, training\nand providing local LLMs specifically customized for Chinese users.\nFurthermore, looking ahead, one of the key future applications of LLMs will be\npractical deployment in industrial production by enterprises and users in those\nsectors. However, the accuracy and robustness of LLMs in industrial scenarios\nhave not been well studied. In this paper, we present a comprehensive empirical\nstudy on the accuracy and robustness of LLMs in the context of the Chinese\nindustrial production area. We manually collected 1,200 domain-specific\nproblems from 8 different industrial sectors to evaluate LLM accuracy.\nFurthermore, we designed a metamorphic testing framework containing four\nindustrial-specific stability categories with eight abilities, totaling 13,631\nquestions with variants to evaluate LLM robustness. In total, we evaluated 9\ndifferent LLMs developed by Chinese vendors, as well as four different LLMs\ndeveloped by global vendors. Our major findings include: (1) Current LLMs\nexhibit low accuracy in Chinese industrial contexts, with all LLMs scoring less\nthan 0.6. (2) The robustness scores vary across industrial sectors, and local\nLLMs overall perform worse than global ones. (3) LLM robustness differs\nsignificantly across abilities. Global LLMs are more robust under\nlogical-related variants, while advanced local LLMs perform better on problems\nrelated to understanding Chinese industrial terminology. Our study results\nprovide valuable guidance for understanding and promoting the industrial domain\ncapabilities of LLMs from both development and industrial enterprise\nperspectives. The results further motivate possible research directions and\ntooling support.", "published": "2024-01-27 03:37:55", "link": "http://arxiv.org/abs/2402.01723v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fortifying Ethical Boundaries in AI: Advanced Strategies for Enhancing\n  Security in Large Language Models", "abstract": "Recent advancements in large language models (LLMs) have significantly\nenhanced capabilities in natural language processing and artificial\nintelligence. These models, including GPT-3.5 and LLaMA-2, have revolutionized\ntext generation, translation, and question-answering tasks due to the\ntransformative Transformer model. Despite their widespread use, LLMs present\nchallenges such as ethical dilemmas when models are compelled to respond\ninappropriately, susceptibility to phishing attacks, and privacy violations.\nThis paper addresses these challenges by introducing a multi-pronged approach\nthat includes: 1) filtering sensitive vocabulary from user input to prevent\nunethical responses; 2) detecting role-playing to halt interactions that could\nlead to 'prison break' scenarios; 3) implementing custom rule engines to\nrestrict the generation of prohibited content; and 4) extending these\nmethodologies to various LLM derivatives like Multi-Model Large Language Models\n(MLLMs). Our approach not only fortifies models against unethical manipulations\nand privacy breaches but also maintains their high performance across tasks. We\ndemonstrate state-of-the-art performance under various attack prompts, without\ncompromising the model's core functionalities. Furthermore, the introduction of\ndifferentiated security levels empowers users to control their personal data\ndisclosure. Our methods contribute to reducing social risks and conflicts\narising from technological abuse, enhance data protection, and promote social\nequity. Collectively, this research provides a framework for balancing the\nefficiency of question-answering systems with user privacy and ethical\nstandards, ensuring a safer user experience and fostering trust in AI\ntechnology.", "published": "2024-01-27 08:09:33", "link": "http://arxiv.org/abs/2402.01725v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Medical Reasoning through Retrieval and Self-Reflection with\n  Retrieval-Augmented Large Language Models", "abstract": "Recent proprietary large language models (LLMs), such as GPT-4, have achieved\na milestone in tackling diverse challenges in the biomedical domain, ranging\nfrom multiple-choice questions to long-form generations. To address challenges\nthat still cannot be handled with the encoded knowledge of LLMs, various\nretrieval-augmented generation (RAG) methods have been developed by searching\ndocuments from the knowledge corpus and appending them unconditionally or\nselectively to the input of LLMs for generation. However, when applying\nexisting methods to different domain-specific problems, poor generalization\nbecomes apparent, leading to fetching incorrect documents or making inaccurate\njudgments. In this paper, we introduce Self-BioRAG, a framework reliable for\nbiomedical text that specializes in generating explanations, retrieving\ndomain-specific documents, and self-reflecting generated responses. We utilize\n84k filtered biomedical instruction sets to train Self-BioRAG that can assess\nits generated explanations with customized reflective tokens. Our work proves\nthat domain-specific components, such as a retriever, domain-related document\ncorpus, and instruction sets are necessary for adhering to domain-related\ninstructions. Using three major medical question-answering benchmark datasets,\nexperimental results of Self-BioRAG demonstrate significant performance gains\nby achieving a 7.2% absolute improvement on average over the state-of-the-art\nopen-foundation model with a parameter size of 7B or less. Overall, we analyze\nthat Self-BioRAG finds the clues in the question, retrieves relevant documents\nif needed, and understands how to answer with information from retrieved\ndocuments and encoded knowledge as a medical expert does. We release our data\nand code for training our framework components and model weights (7B and 13B)\nto enhance capabilities in biomedical and clinical domains.", "published": "2024-01-27 02:29:42", "link": "http://arxiv.org/abs/2401.15269v3", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "A Survey on Neural Topic Models: Methods, Applications, and Challenges", "abstract": "Topic models have been prevalent for decades to discover latent topics and\ninfer topic proportions of documents in an unsupervised fashion. They have been\nwidely used in various applications like text analysis and context\nrecommendation. Recently, the rise of neural networks has facilitated the\nemergence of a new research field -- Neural Topic Models (NTMs). Different from\nconventional topic models, NTMs directly optimize parameters without requiring\nmodel-specific derivations. This endows NTMs with better scalability and\nflexibility, resulting in significant research attention and plentiful new\nmethods and applications. In this paper, we present a comprehensive survey on\nneural topic models concerning methods, applications, and challenges.\nSpecifically, we systematically organize current NTM methods according to their\nnetwork structures and introduce the NTMs for various scenarios like short\ntexts and bilingual documents. We also discuss a wide range of popular\napplications built on NTMs. Finally, we highlight the challenges confronted by\nNTMs to inspire future research. We accompany this survey with a repository for\neasier access to the mentioned paper resources:\nhttps://github.com/bobxwu/Paper-Neural-Topic-Models.", "published": "2024-01-27 08:52:19", "link": "http://arxiv.org/abs/2401.15351v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "A Survey on Data Augmentation in Large Model Era", "abstract": "Large models, encompassing large language and diffusion models, have shown\nexceptional promise in approximating human-level intelligence, garnering\nsignificant interest from both academic and industrial spheres. However, the\ntraining of these large models necessitates vast quantities of high-quality\ndata, and with continuous updates to these models, the existing reservoir of\nhigh-quality data may soon be depleted. This challenge has catalyzed a surge in\nresearch focused on data augmentation methods. Leveraging large models, these\ndata augmentation techniques have outperformed traditional approaches. This\npaper offers an exhaustive review of large model-driven data augmentation\nmethods, adopting a comprehensive perspective. We begin by establishing a\nclassification of relevant studies into three main categories: image\naugmentation, text augmentation, and paired data augmentation. Following this,\nwe delve into various data post-processing techniques pertinent to large\nmodel-based data augmentation. Our discussion then expands to encompass the\narray of applications for these data augmentation methods within natural\nlanguage processing, computer vision, and audio signal processing. We proceed\nto evaluate the successes and limitations of large model-based data\naugmentation across different scenarios. Concluding our review, we highlight\nprospective challenges and avenues for future exploration in the field of data\naugmentation. Our objective is to furnish researchers with critical insights,\nultimately contributing to the advancement of more sophisticated large models.\nWe consistently maintain the related open-source materials at:\nhttps://github.com/MLGroup-JLU/LLM-data-aug-survey.", "published": "2024-01-27 14:19:33", "link": "http://arxiv.org/abs/2401.15422v2", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Navigating the Post-API Dilemma | Search Engine Results Pages Present a\n  Biased View of Social Media Data", "abstract": "Recent decisions to discontinue access to social media APIs are having\ndetrimental effects on Internet research and the field of computational social\nscience as a whole. This lack of access to data has been dubbed the Post-API\nera of Internet research. Fortunately, popular search engines have the means to\ncrawl, capture, and surface social media data on their Search Engine Results\nPages (SERP) if provided the proper search query, and may provide a solution to\nthis dilemma. In the present work we ask: does SERP provide a complete and\nunbiased sample of social media data? Is SERP a viable alternative to direct\nAPI-access? To answer these questions, we perform a comparative analysis\nbetween (Google) SERP results and nonsampled data from Reddit and Twitter/X. We\nfind that SERP results are highly biased in favor of popular posts; against\npolitical, pornographic, and vulgar posts; are more positive in their\nsentiment; and have large topical gaps. Overall, we conclude that SERP is not a\nviable alternative to social media API access.", "published": "2024-01-27 19:04:30", "link": "http://arxiv.org/abs/2401.15479v4", "categories": ["cs.IR", "cs.CL", "cs.SI"], "primary_category": "cs.IR"}
{"title": "Baichuan2-Sum: Instruction Finetune Baichuan2-7B Model for Dialogue\n  Summarization", "abstract": "Large language models (LLMs) like Llama, Baichuan and Bloom models show\nremarkable ability with instruction fine-tuning in many natural language tasks.\nNevertheless, for the dialogue summarization task, which aims to generate\nsummaries for different roles in dialogue, most of the state-of-the-art methods\nconduct on small models (e.g Bart and Bert). Existing methods try to add task\nspecified optimization on small models like adding global-local centrality\nscore to models. In this paper, we propose an instruction fine-tuning model:\nBaichuan2-Sum, for role-oriented diaglouge summarization. By setting different\ninstructions for different roles, the model can learn from the dialogue\ninteractions and output the expected summaries. Furthermore, we applied NEFTune\ntechnique to add suitable noise during training to improve the results. The\nexperiments demonstrate that the proposed model achieves the new\nstate-of-the-art results on two public dialogue summarization datasets: CSDS\nand SAMSUM. We release our model and related codes to facilitate future studies\non dialogue summarization task.", "published": "2024-01-27 20:20:39", "link": "http://arxiv.org/abs/2401.15496v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Style-News: Incorporating Stylized News Generation and Adversarial\n  Verification for Neural Fake News Detection", "abstract": "With the improvements in generative models, the issues of producing\nhallucinations in various domains (e.g., law, writing) have been brought to\npeople's attention due to concerns about misinformation. In this paper, we\nfocus on neural fake news, which refers to content generated by neural networks\naiming to mimic the style of real news to deceive people. To prevent harmful\ndisinformation spreading fallaciously from malicious social media (e.g.,\ncontent farms), we propose a novel verification framework, Style-News, using\npublisher metadata to imply a publisher's template with the corresponding text\ntypes, political stance, and credibility. Based on threat modeling aspects, a\nstyle-aware neural news generator is introduced as an adversary for generating\nnews content conditioning for a specific publisher, and style and source\ndiscriminators are trained to defend against this attack by identifying which\npublisher the style corresponds with, and discriminating whether the source of\nthe given news is human-written or machine-generated. To evaluate the quality\nof the generated content, we integrate various dimensional metrics (language\nfluency, content preservation, and style adherence) and demonstrate that\nStyle-News significantly outperforms the previous approaches by a margin of\n0.35 for fluency, 15.24 for content, and 0.38 for style at most. Moreover, our\ndiscriminative model outperforms state-of-the-art baselines in terms of\npublisher prediction (up to 4.64%) and neural fake news detection (+6.94%\n$\\sim$ 31.72%).", "published": "2024-01-27 21:35:29", "link": "http://arxiv.org/abs/2401.15509v1", "categories": ["cs.CL", "cs.AI", "cs.SI"], "primary_category": "cs.CL"}
{"title": "FaKnow: A Unified Library for Fake News Detection", "abstract": "Over the past years, a large number of fake news detection algorithms based\non deep learning have emerged. However, they are often developed under\ndifferent frameworks, each mandating distinct utilization methodologies,\nconsequently hindering reproducibility. Additionally, a substantial amount of\nredundancy characterizes the code development of such fake news detection\nmodels. To address these concerns, we propose FaKnow, a unified and\ncomprehensive fake news detection algorithm library. It encompasses a variety\nof widely used fake news detection models, categorized as content-based and\nsocial context-based approaches. This library covers the full spectrum of the\nmodel training and evaluation process, effectively organizing the data, models,\nand training procedures within a unified framework. Furthermore, it furnishes a\nseries of auxiliary functionalities and tools, including visualization, and\nlogging. Our work contributes to the standardization and unification of fake\nnews detection research, concurrently facilitating the endeavors of researchers\nin this field. The open-source code and documentation can be accessed at\nhttps://github.com/NPURG/FaKnow and https://faknow.readthedocs.io,\nrespectively.", "published": "2024-01-27 13:29:17", "link": "http://arxiv.org/abs/2401.16441v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "CERM: Context-aware Literature-based Discovery via Sentiment Analysis", "abstract": "Driven by the abundance of biomedical publications, we introduce a sentiment\nanalysis task to understand food-health relationship. Prior attempts to\nincorporate health into recipe recommendation and analysis systems have\nprimarily focused on ingredient nutritional components or utilized basic\ncomputational models trained on curated labeled data. Enhanced models that\ncapture the inherent relationship between food ingredients and biomedical\nconcepts can be more beneficial for food-related research, given the wealth of\ninformation in biomedical texts. Considering the costly data labeling process,\nthese models should effectively utilize both labeled and unlabeled data. This\npaper introduces Entity Relationship Sentiment Analysis (ERSA), a new task that\ncaptures the sentiment of a text based on an entity pair. ERSA extends the\nwidely studied Aspect Based Sentiment Analysis (ABSA) task. Specifically, our\nstudy concentrates on the ERSA task applied to biomedical texts, focusing on\n(entity-entity) pairs of biomedical and food concepts. ERSA poses a significant\nchallenge compared to traditional sentiment analysis tasks, as sentence\nsentiment may not align with entity relationship sentiment. Additionally, we\npropose CERM, a semi-supervised architecture that combines different word\nembeddings to enhance the encoding of the ERSA task. Experimental results\nshowcase the model's efficiency across diverse learning scenarios.", "published": "2024-01-27 06:40:08", "link": "http://arxiv.org/abs/2402.01724v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AI Does Not Alter Perceptions of Text Messages", "abstract": "For many people, anxiety, depression, and other social and mental factors can\nmake composing text messages an active challenge. To remedy this problem, large\nlanguage models (LLMs) may yet prove to be the perfect tool to assist users\nthat would otherwise find texting difficult or stressful. However, despite\nrapid uptake in LLM usage, considerations for their assistive usage in text\nmessage composition have not been explored. A primary concern regarding LLM\nusage is that poor public sentiment regarding AI introduces the possibility\nthat its usage may harm perceptions of AI-assisted text messages, making usage\ncounter-productive. To (in)validate this possibility, we explore how the belief\nthat a text message did or did not receive AI assistance in composition alters\nits perceived tone, clarity, and ability to convey intent. In this study, we\nsurvey the perceptions of 26 participants on 18 randomly labeled pre-composed\ntext messages. In analyzing the participants' ratings of message tone, clarity,\nand ability to convey intent, we find that there is no statistically\nsignificant evidence that the belief that AI is utilized alters recipient\nperceptions. This provides hopeful evidence that LLM-based text message\ncomposition assistance can be implemented without the risk of\ncounter-productive outcomes.", "published": "2024-01-27 14:32:12", "link": "http://arxiv.org/abs/2402.01726v2", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Hardware Phi-1.5B: A Large Language Model Encodes Hardware Domain\n  Specific Knowledge", "abstract": "In the rapidly evolving semiconductor industry, where research, design,\nverification, and manufacturing are intricately linked, the potential of Large\nLanguage Models to revolutionize hardware design and security verification is\nimmense. The primary challenge, however, lies in the complexity of hardware\nspecific issues that are not adequately addressed by the natural language or\nsoftware code knowledge typically acquired during the pretraining stage.\nAdditionally, the scarcity of datasets specific to the hardware domain poses a\nsignificant hurdle in developing a foundational model. Addressing these\nchallenges, this paper introduces Hardware Phi 1.5B, an innovative large\nlanguage model specifically tailored for the hardware domain of the\nsemiconductor industry. We have developed a specialized, tiered dataset\ncomprising small, medium, and large subsets and focused our efforts on\npretraining using the medium dataset. This approach harnesses the compact yet\nefficient architecture of the Phi 1.5B model. The creation of this first\npretrained, hardware domain specific large language model marks a significant\nadvancement, offering improved performance in hardware design and verification\ntasks and illustrating a promising path forward for AI applications in the\nsemiconductor sector.", "published": "2024-01-27 22:49:43", "link": "http://arxiv.org/abs/2402.01728v1", "categories": ["cs.CL", "cs.AI", "cs.AR"], "primary_category": "cs.CL"}
{"title": "ProtAgents: Protein discovery via large language model multi-agent\n  collaborations combining physics and machine learning", "abstract": "Designing de novo proteins beyond those found in nature holds significant\npromise for advancements in both scientific and engineering applications.\nCurrent methodologies for protein design often rely on AI-based models, such as\nsurrogate models that address end-to-end problems by linking protein structure\nto material properties or vice versa. However, these models frequently focus on\nspecific material objectives or structural properties, limiting their\nflexibility when incorporating out-of-domain knowledge into the design process\nor comprehensive data analysis is required. In this study, we introduce\nProtAgents, a platform for de novo protein design based on Large Language\nModels (LLMs), where multiple AI agents with distinct capabilities\ncollaboratively address complex tasks within a dynamic environment. The\nversatility in agent development allows for expertise in diverse domains,\nincluding knowledge retrieval, protein structure analysis, physics-based\nsimulations, and results analysis. The dynamic collaboration between agents,\nempowered by LLMs, provides a versatile approach to tackling protein design and\nanalysis problems, as demonstrated through diverse examples in this study. The\nproblems of interest encompass designing new proteins, analyzing protein\nstructures and obtaining new first-principles data -- natural vibrational\nfrequencies -- via physics simulations. The concerted effort of the system\nallows for powerful automated and synergistic design of de novo proteins with\ntargeted mechanical properties. The flexibility in designing the agents, on one\nhand, and their capacity in autonomous collaboration through the dynamic\nLLM-based multi-agent environment on the other hand, unleashes great potentials\nof LLMs in addressing multi-objective materials problems and opens up new\navenues for autonomous materials discovery and design.", "published": "2024-01-27 20:19:49", "link": "http://arxiv.org/abs/2402.04268v1", "categories": ["cond-mat.soft", "cs.AI", "cs.CL", "q-bio.BM"], "primary_category": "cond-mat.soft"}
{"title": "Music Auto-Tagging with Robust Music Representation Learned via Domain\n  Adversarial Training", "abstract": "Music auto-tagging is crucial for enhancing music discovery and\nrecommendation. Existing models in Music Information Retrieval (MIR) struggle\nwith real-world noise such as environmental and speech sounds in multimedia\ncontent. This study proposes a method inspired by speech-related tasks to\nenhance music auto-tagging performance in noisy settings. The approach\nintegrates Domain Adversarial Training (DAT) into the music domain, enabling\nrobust music representations that withstand noise. Unlike previous research,\nthis approach involves an additional pretraining phase for the domain\nclassifier, to avoid performance degradation in the subsequent phase. Adding\nvarious synthesized noisy music data improves the model's generalization across\ndifferent noise levels. The proposed architecture demonstrates enhanced\nperformance in music auto-tagging by effectively utilizing unlabeled noisy\nmusic data. Additional experiments with supplementary unlabeled data further\nimproves the model's performance, underscoring its robust generalization\ncapabilities and broad applicability.", "published": "2024-01-27 06:56:51", "link": "http://arxiv.org/abs/2401.15323v1", "categories": ["cs.SD", "cs.AI", "cs.IR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Validation of artificial neural networks to model the acoustic behaviour\n  of induction motors", "abstract": "In the last decade, the sound quality of electric induction motors is a hot\ntopic in the research field. Specially, due to its high number of applications,\nthe population is exposed to physical and psychological discomfort caused by\nthe noise emission. Therefore, it is necessary to minimise its psychological\nimpact on the population. In this way, the main goal of this work is to\nevaluate the use of multitask artificial neural networks as a modelling\ntechnique for simultaneously predicting psychoacoustic parameters of induction\nmotors. Several inputs are used, such as, the electrical magnitudes of the\nmotor power signal and the number of poles, instead of separating the noise of\nthe electric motor from the environmental noise. Two different kind of\nartificial neural networks are proposed to evaluate the acoustic quality of\ninduction motors, by using the equivalent sound pressure, the loudness, the\nroughness and the sharpness as outputs. Concretely, two different topologies\nhave been considered: simple models and more complex models. The former are\nmore interpretable, while the later lead to higher accuracy at the cost of\nhiding the cause-effect relationship. Focusing on the simple interpretable\nmodels, product unit neural networks achieved the best results: for MSE and for\nSEP. The main benefit of this product unit model is its simplicity, since only\n10 inputs variables are used, outlining the effective transfer mechanism of\nmultitask artificial neural networks to extract common features of multiple\ntasks. Finally, a deep analysis of the acoustic quality of induction motors in\ndone using the best product unit neural networks.", "published": "2024-01-27 10:49:33", "link": "http://arxiv.org/abs/2401.15377v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Sigma-lognormal modeling of speech", "abstract": "Human movement studies and analyses have been fundamental in many scientific\ndomains, ranging from neuroscience to education, pattern recognition to\nrobotics, health care to sports, and beyond. Previous speech motor models were\nproposed to understand how speech movement is produced and how the resulting\nspeech varies when some parameters are changed. However, the inverse approach,\nin which the muscular response parameters and the subject's age are derived\nfrom real continuous speech, is not possible with such models. Instead, in the\nhandwriting field, the kinematic theory of rapid human movements and its\nassociated Sigma-lognormal model have been applied successfully to obtain the\nmuscular response parameters. This work presents a speech kinematics based\nmodel that can be used to study, analyze, and reconstruct complex speech\nkinematics in a simplified manner. A method based on the kinematic theory of\nrapid human movements and its associated Sigma lognormal model are applied to\ndescribe and to parameterize the asymptotic impulse response of the\nneuromuscular networks involved in speech as a response to a neuromotor\ncommand. The method used to carry out transformations from formants to a\nmovement observation is also presented. Experiments carried out with the\n(English) VTR TIMIT database and the (German) Saarbrucken Voice Database,\nincluding people of different ages, with and without laryngeal pathologies,\ncorroborate the link between the extracted parameters and aging, on the one\nhand, and the proportion between the first and second formants required in\napplying the kinematic theory of rapid human movements, on the other. The\nresults should drive innovative developments in the modeling and understanding\nof speech kinematics.", "published": "2024-01-27 18:00:20", "link": "http://arxiv.org/abs/2401.17320v1", "categories": ["q-bio.NC", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "q-bio.NC"}
