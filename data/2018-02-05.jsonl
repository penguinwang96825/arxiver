{"title": "Semantic projection: recovering human knowledge of multiple, distinct\n  object features from word embeddings", "abstract": "The words of a language reflect the structure of the human mind, allowing us\nto transmit thoughts between individuals. However, language can represent only\na subset of our rich and detailed cognitive architecture. Here, we ask what\nkinds of common knowledge (semantic memory) are captured by word meanings\n(lexical semantics). We examine a prominent computational model that represents\nwords as vectors in a multidimensional space, such that proximity between\nword-vectors approximates semantic relatedness. Because related words appear in\nsimilar contexts, such spaces - called \"word embeddings\" - can be learned from\npatterns of lexical co-occurrences in natural language. Despite their\npopularity, a fundamental concern about word embeddings is that they appear to\nbe semantically \"rigid\": inter-word proximity captures only overall similarity,\nyet human judgments about object similarities are highly context-dependent and\ninvolve multiple, distinct semantic features. For example, dolphins and\nalligators appear similar in size, but differ in intelligence and\naggressiveness. Could such context-dependent relationships be recovered from\nword embeddings? To address this issue, we introduce a powerful, domain-general\nsolution: \"semantic projection\" of word-vectors onto lines that represent\nvarious object features, like size (the line extending from the word \"small\" to\n\"big\"), intelligence (from \"dumb\" to \"smart\"), or danger (from \"safe\" to\n\"dangerous\"). This method, which is intuitively analogous to placing objects\n\"on a mental scale\" between two extremes, recovers human judgments across a\nrange of object categories and properties. We thus show that word embeddings\ninherit a wealth of common knowledge from word co-occurrence statistics and can\nbe flexibly manipulated to express context-dependent meanings.", "published": "2018-02-05 02:42:40", "link": "http://arxiv.org/abs/1802.01241v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Chemical-protein relation extraction with ensembles of SVM, CNN, and RNN\n  models", "abstract": "Text mining the relations between chemicals and proteins is an increasingly\nimportant task. The CHEMPROT track at BioCreative VI aims to promote the\ndevelopment and evaluation of systems that can automatically detect the\nchemical-protein relations in running text (PubMed abstracts). This manuscript\ndescribes our submission, which is an ensemble of three systems, including a\nSupport Vector Machine, a Convolutional Neural Network, and a Recurrent Neural\nNetwork. Their output is combined using a decision based on majority voting or\nstacking. Our CHEMPROT system obtained 0.7266 in precision and 0.5735 in recall\nfor an f-score of 0.6410, demonstrating the effectiveness of machine\nlearning-based approaches for automatic relation extraction from biomedical\nliterature. Our submission achieved the highest performance in the task during\nthe 2017 challenge.", "published": "2018-02-05 03:42:36", "link": "http://arxiv.org/abs/1802.01255v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DP-GAN: Diversity-Promoting Generative Adversarial Network for\n  Generating Informative and Diversified Text", "abstract": "Existing text generation methods tend to produce repeated and \"boring\"\nexpressions. To tackle this problem, we propose a new text generation model,\ncalled Diversity-Promoting Generative Adversarial Network (DP-GAN). The\nproposed model assigns low reward for repeatedly generated text and high reward\nfor \"novel\" and fluent text, encouraging the generator to produce diverse and\ninformative text. Moreover, we propose a novel language-model based\ndiscriminator, which can better distinguish novel text from repeated text\nwithout the saturation problem compared with existing classifier-based\ndiscriminators. The experimental results on review generation and dialogue\ngeneration tasks demonstrate that our model can generate substantially more\ndiverse and informative text than existing baselines. The code is available at\nhttps://github.com/lancopku/DPGAN", "published": "2018-02-05 10:54:29", "link": "http://arxiv.org/abs/1802.01345v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Diverse Beam Search for Increased Novelty in Abstractive Summarization", "abstract": "Text summarization condenses a text to a shorter version while retaining the\nimportant informations. Abstractive summarization is a recent development that\ngenerates new phrases, rather than simply copying or rephrasing sentences\nwithin the original text. Recently neural sequence-to-sequence models have\nachieved good results in the field of abstractive summarization, which opens\nnew possibilities and applications for industrial purposes. However, most\npractitioners observe that these models still use large parts of the original\ntext in the output summaries, making them often similar to extractive\nframeworks. To address this drawback, we first introduce a new metric to\nmeasure how much of a summary is extracted from the input text. Secondly, we\npresent a novel method, that relies on a diversity factor in computing the\nneural network loss, to improve the diversity of the summaries generated by any\nneural abstractive model implementing beam search. Finally, we show that this\nmethod not only makes the system less extractive, but also improves the overall\nrouge score of state-of-the-art methods by at least 2 points.", "published": "2018-02-05 15:17:01", "link": "http://arxiv.org/abs/1802.01457v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
