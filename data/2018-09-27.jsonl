{"title": "Iterative Document Representation Learning Towards Summarization with\n  Polishing", "abstract": "In this paper, we introduce Iterative Text Summarization (ITS), an\niteration-based model for supervised extractive text summarization, inspired by\nthe observation that it is often necessary for a human to read an article\nmultiple times in order to fully understand and summarize its contents. Current\nsummarization approaches read through a document only once to generate a\ndocument representation, resulting in a sub-optimal representation. To address\nthis issue we introduce a model which iteratively polishes the document\nrepresentation on many passes through the document. As part of our model, we\nalso introduce a selective reading mechanism that decides more accurately the\nextent to which each sentence in the model should be updated. Experimental\nresults on the CNN/DailyMail and DUC2002 datasets demonstrate that our model\nsignificantly outperforms state-of-the-art extractive systems when evaluated by\nmachines and by humans.", "published": "2018-09-27 03:11:35", "link": "http://arxiv.org/abs/1809.10324v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Predictive Embeddings for Hate Speech Detection on Twitter", "abstract": "We present a neural-network based approach to classifying online hate speech\nin general, as well as racist and sexist speech in particular. Using\npre-trained word embeddings and max/mean pooling from simple, fully-connected\ntransformations of these embeddings, we are able to predict the occurrence of\nhate speech on three commonly used publicly available datasets. Our models\nmatch or outperform state of the art F1 performance on all three datasets using\nsignificantly fewer parameters and minimal feature preprocessing compared to\nprevious methods.", "published": "2018-09-27 17:12:24", "link": "http://arxiv.org/abs/1809.10644v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Qualitative Comparison of CoQA, SQuAD 2.0 and QuAC", "abstract": "We compare three new datasets for question answering: SQuAD 2.0, QuAC, and\nCoQA, along several of their new features: (1) unanswerable questions, (2)\nmulti-turn interactions, and (3) abstractive answers. We show that the datasets\nprovide complementary coverage of the first two aspects, but weak coverage of\nthe third. Because of the datasets' structural similarity, a single extractive\nmodel can be easily adapted to any of the datasets and we show improved\nbaseline results on both SQuAD 2.0 and CoQA. Despite the similarity, models\ntrained on one dataset are ineffective on another dataset, but we find moderate\nperformance improvement through pretraining. To encourage cross-evaluation, we\nrelease code for conversion between datasets at\nhttps://github.com/my89/co-squac .", "published": "2018-09-27 19:33:03", "link": "http://arxiv.org/abs/1809.10735v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Controllable Neural Story Plot Generation via Reward Shaping", "abstract": "Language-modeling--based approaches to story plot generation attempt to\nconstruct a plot by sampling from a language model (LM) to predict the next\ncharacter, word, or sentence to add to the story. LM techniques lack the\nability to receive guidance from the user to achieve a specific goal, resulting\nin stories that don't have a clear sense of progression and lack coherence. We\npresent a reward-shaping technique that analyzes a story corpus and produces\nintermediate rewards that are backpropagated into a pre-trained LM in order to\nguide the model towards a given goal. Automated evaluations show our technique\ncan create a model that generates story plots which consistently achieve a\nspecified goal. Human-subject studies show that the generated stories have more\nplausible event ordering than baseline plot generation techniques.", "published": "2018-09-27 19:33:54", "link": "http://arxiv.org/abs/1809.10736v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Building a Lemmatizer and a Spell-checker for Sorani Kurdish", "abstract": "The present paper aims at presenting a lemmatization and a word-level error\ncorrection system for Sorani Kurdish. We propose a hybrid approach based on the\nmorphological rules and a n-gram language model. We have called our\nlemmatization and error correction systems Peyv and R\\^en\\^us respectively,\nwhich are the first tools presented for Sorani Kurdish to the best of our\nknowledge. The Peyv lemmatizer has shown 86.7% accuracy. As for R\\^en\\^us,\nusing a lexicon, we have obtained 96.4% accuracy while without a lexicon, the\ncorrection system has 87% accuracy. As two fundamental text processing tools,\nthese tools can pave the way for further researches on more natural language\nprocessing applications for Sorani Kurdish.", "published": "2018-09-27 21:00:36", "link": "http://arxiv.org/abs/1809.10763v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Data Expansion for Customer-care Spoken Language Understanding", "abstract": "Spoken language understanding (SLU) systems are widely used in handling of\ncustomer-care calls.A traditional SLU system consists of an acoustic model (AM)\nand a language model (LM) that areused to decode the utterance and a natural\nlanguage understanding (NLU) model that predicts theintent. While AM can be\nshared across different domains, LM and NLU models need to be\ntrainedspecifically for every new task. However, preparing enough data to train\nthese models is prohibitivelyexpensive. In this paper, we introduce an\nefficient method to expand the limited in-domain data. Theprocess starts with\ntraining a preliminary NLU model based on logistic regression on the\nin-domaindata. Since the features are based onn= 1,2-grams, we can detect the\nmost informative n-gramsfor each intent class. Using these n-grams, we find the\nsamples in the out-of-domain corpus that1) contain the desired n-gram and/or 2)\nhave similar intent label. The ones which meet the firstconstraint are used to\ntrain a new LM model and the ones that meet both constraints are used to train\nanew NLU model. Our results on two divergent experimental setups show that the\nproposed approachreduces by 30% the absolute classification error rate (CER)\ncomparing to the preliminary modelsand it significantly outperforms the\ntraditional data expansion algorithms such as the ones based onsemi-supervised\nlearning, TF-IDF and embedding vectors.", "published": "2018-09-27 18:49:29", "link": "http://arxiv.org/abs/1810.00670v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adaptive Pruning of Neural Language Models for Mobile Devices", "abstract": "Neural language models (NLMs) exist in an accuracy-efficiency tradeoff space\nwhere better perplexity typically comes at the cost of greater computation\ncomplexity. In a software keyboard application on mobile devices, this\ntranslates into higher power consumption and shorter battery life. This paper\nrepresents the first attempt, to our knowledge, in exploring\naccuracy-efficiency tradeoffs for NLMs. Building on quasi-recurrent neural\nnetworks (QRNNs), we apply pruning techniques to provide a \"knob\" to select\ndifferent operating points. In addition, we propose a simple technique to\nrecover some perplexity using a negligible amount of memory. Our empirical\nevaluations consider both perplexity as well as energy consumption on a\nRaspberry Pi, where we demonstrate which methods provide the best\nperplexity-power consumption operating point. At one operating point, one of\nthe techniques is able to provide energy savings of 40% over the state of the\nart with only a 17% relative increase in perplexity.", "published": "2018-09-27 00:41:16", "link": "http://arxiv.org/abs/1809.10282v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards a classification of Lindenmayer systems", "abstract": "In this paper we will attempt to classify Lindenmayer systems based on\nproperties of sets of rules and the kind of strings those rules generate. This\nclassification will be referred to as a parametrization of the L-space: the\nL-space is the phase space in which all possible L-developments are\nrepresented. This space is infinite, because there is no halting algorithm for\nL-grammars; but it is also subjected to hard conditions, because there are\ngrammars and developments which are not possible states of an L-system: a very\nwell-known example is the space of normal grammars. Just as the space of normal\ngrammars is parametrized into Regular, Context-Free, Context-Sensitive, and\nUnrestricted (with proper containment relations holding among them; see\nChomsky, 1959: Theorem 1), we contend here that the L-space is a very rich\nlandscape of grammars which cluster into kinds that are not mutually\ntranslatable.", "published": "2018-09-27 14:36:00", "link": "http://arxiv.org/abs/1809.10542v1", "categories": ["cs.FL", "cs.CL"], "primary_category": "cs.FL"}
{"title": "Enabling FAIR Research in Earth Science through Research Objects", "abstract": "Data-intensive science communities are progressively adopting FAIR practices\nthat enhance the visibility of scientific breakthroughs and enable reuse. At\nthe core of this movement, research objects contain and describe scientific\ninformation and resources in a way compliant with the FAIR principles and\nsustain the development of key infrastructure and tools. This paper provides an\naccount of the challenges, experiences and solutions involved in the adoption\nof FAIR around research objects over several Earth Science disciplines. During\nthis journey, our work has been comprehensive, with outcomes including: an\nextended research object model adapted to the needs of earth scientists; the\nprovisioning of digital object identifiers (DOI) to enable persistent\nidentification and to give due credit to authors; the generation of\ncontent-based, semantically rich, research object metadata through natural\nlanguage processing, enhancing visibility and reuse through recommendation\nsystems and third-party search engines; and various types of checklists that\nprovide a compact representation of research object quality as a key enabler of\nscientific reuse. All these results have been integrated in ROHub, a platform\nthat provides research object management functionality to a wealth of\napplications and interfaces across different scientific communities. To monitor\nand quantify the community uptake of research objects, we have defined\nindicators and obtained measures via ROHub that are also discussed herein.", "published": "2018-09-27 16:28:18", "link": "http://arxiv.org/abs/1809.10617v1", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "NEXUS Network: Connecting the Preceding and the Following in Dialogue\n  Generation", "abstract": "Sequence-to-Sequence (seq2seq) models have become overwhelmingly popular in\nbuilding end-to-end trainable dialogue systems. Though highly efficient in\nlearning the backbone of human-computer communications, they suffer from the\nproblem of strongly favoring short generic responses. In this paper, we argue\nthat a good response should smoothly connect both the preceding dialogue\nhistory and the following conversations. We strengthen this connection through\nmutual information maximization. To sidestep the non-differentiability of\ndiscrete natural language tokens, we introduce an auxiliary continuous code\nspace and map such code space to a learnable prior distribution for generation\npurpose. Experiments on two dialogue datasets validate the effectiveness of our\nmodel, where the generated responses are closely related to the dialogue\ncontext and lead to more interactive conversations.", "published": "2018-09-27 10:09:44", "link": "http://arxiv.org/abs/1810.00671v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Semantically Invariant Text-to-Image Generation", "abstract": "Image captioning has demonstrated models that are capable of generating\nplausible text given input images or videos. Further, recent work in image\ngeneration has shown significant improvements in image quality when text is\nused as a prior. Our work ties these concepts together by creating an\narchitecture that can enable bidirectional generation of images and text. We\ncall this network Multi-Modal Vector Representation (MMVR). Along with MMVR, we\npropose two improvements to the text conditioned image generation. Firstly, a\nn-gram metric based cost function is introduced that generalizes the caption\nwith respect to the image. Secondly, multiple semantically similar sentences\nare shown to help in generating better images. Qualitative and quantitative\nevaluations demonstrate that MMVR improves upon existing text conditioned image\ngeneration results by over 20%, while integrating visual and text modalities.", "published": "2018-09-27 00:11:25", "link": "http://arxiv.org/abs/1809.10274v1", "categories": ["cs.LG", "cs.CL", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Consistency and Variation in Kernel Neural Ranking Model", "abstract": "This paper studies the consistency of the kernel-based neural ranking model\nK-NRM, a recent state-of-the-art neural IR model, which is important for\nreproducible research and deployment in the industry. We find that K-NRM has\nlow variance on relevance-based metrics across experimental trials. In spite of\nthis low variance in overall performance, different trials produce different\ndocument rankings for individual queries. The main source of variance in our\nexperiments was found to be different latent matching patterns captured by\nK-NRM. In the IR-customized word embeddings learned by K-NRM, the\nquery-document word pairs follow two different matching patterns that are\nequally effective, but align word pairs differently in the embedding space. The\ndifferent latent matching patterns enable a simple yet effective approach to\nconstruct ensemble rankers, which improve K-NRM's effectiveness and\ngeneralization abilities.", "published": "2018-09-27 14:01:19", "link": "http://arxiv.org/abs/1809.10522v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Acoustic Probing for Estimating the Storage Time and Firmness of\n  Tomatoes and Mandarin Oranges", "abstract": "This paper introduces an acoustic probing technique to estimate the storage\ntime and firmness of fruits; we emit an acoustic signal to fruit from a small\nspeaker and capture the reflected signal with a tiny microphone. We collect\nreflected signals for fruits with various storage times and firmness\nconditions, using them to train regressors for estimation. To evaluate the\nfeasibility of our acoustic probing, we performed experiments; we prepared 162\ntomatoes and 153 mandarin oranges, collected their reflected signals using our\ndeveloped device and measured their firmness with a fruit firmness tester, for\na period of 35 days for tomatoes and 60 days for mandarin oranges. We performed\ncross validation by using this data set. The average estimation errors of\nstorage time and firmness for tomatoes were 0.89 days and 9.47 g/mm2. Those for\nmandarin oranges were 1.67 days and 15.67 g/mm2. The estimation of storage time\nwas sufficiently accurate for casual users to select fruits in their favorite\ncondition at home. In the experiments, we tested four different acoustic probes\nand found that sweep signals provide highly accurate estimation results.", "published": "2018-09-27 15:40:52", "link": "http://arxiv.org/abs/1809.10581v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Lightweight Music Texture Transfer System", "abstract": "Deep learning researches on the transformation problems for image and text\nhave raised great attention. However, present methods for music feature\ntransfer using neural networks are far from practical application. In this\npaper, we initiate a novel system for transferring the texture of music, and\nrelease it as an open source project. Its core algorithm is composed of a\nconverter which represents sounds as texture spectra, a corresponding\nreconstructor and a feed-forward transfer network. We evaluate this system from\nmultiple perspectives, and experimental results reveal that it achieves\nconvincing results in both sound effects and computational performance.", "published": "2018-09-27 16:03:53", "link": "http://arxiv.org/abs/1810.01248v3", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
