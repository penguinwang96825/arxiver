{"title": "Adapting Deep Learning for Sentiment Classification of Code-Switched\n  Informal Short Text", "abstract": "Nowadays, an abundance of short text is being generated that uses nonstandard\nwriting styles influenced by regional languages. Such informal and\ncode-switched content are under-resourced in terms of labeled datasets and\nlanguage models even for popular tasks like sentiment classification. In this\nwork, we (1) present a labeled dataset called MultiSenti for sentiment\nclassification of code-switched informal short text, (2) explore the\nfeasibility of adapting resources from a resource-rich language for an informal\none, and (3) propose a deep learning-based model for sentiment classification\nof code-switched informal short text. We aim to achieve this without any\nlexical normalization, language translation, or code-switching indication. The\nperformance of the proposed models is compared with three existing multilingual\nsentiment classification models. The results show that the proposed model\nperforms better in general and adapting character-based embeddings yield\nequivalent performance while being computationally more efficient than training\nword-based domain-specific embeddings.", "published": "2020-01-04 06:31:15", "link": "http://arxiv.org/abs/2001.01047v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Explain and Improve: LRP-Inference Fine-Tuning for Image Captioning\n  Models", "abstract": "This paper analyzes the predictions of image captioning models with attention\nmechanisms beyond visualizing the attention itself. We develop variants of\nlayer-wise relevance propagation (LRP) and gradient-based explanation methods,\ntailored to image captioning models with attention mechanisms. We compare the\ninterpretability of attention heatmaps systematically against the explanations\nprovided by explanation methods such as LRP, Grad-CAM, and Guided Grad-CAM. We\nshow that explanation methods provide simultaneously pixel-wise image\nexplanations (supporting and opposing pixels of the input image) and linguistic\nexplanations (supporting and opposing words of the preceding sequence) for each\nword in the predicted captions. We demonstrate with extensive experiments that\nexplanation methods 1) can reveal additional evidence used by the model to make\ndecisions compared to attention; 2) correlate to object locations with high\nprecision; 3) are helpful to \"debug\" the model, e.g. by analyzing the reasons\nfor hallucinated object words. With the observed properties of explanations, we\nfurther design an LRP-inference fine-tuning strategy that reduces the issue of\nobject hallucination in image captioning models, and meanwhile, maintains the\nsentence fluency. We conduct experiments with two widely used attention\nmechanisms: the adaptive attention mechanism calculated with the additive\nattention and the multi-head attention mechanism calculated with the scaled dot\nproduct.", "published": "2020-01-04 05:15:11", "link": "http://arxiv.org/abs/2001.01037v5", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "A Comprehensive Survey of Multilingual Neural Machine Translation", "abstract": "We present a survey on multilingual neural machine translation (MNMT), which\nhas gained a lot of traction in the recent years. MNMT has been useful in\nimproving translation quality as a result of translation knowledge transfer\n(transfer learning). MNMT is more promising and interesting than its\nstatistical machine translation counterpart because end-to-end modeling and\ndistributed representations open new avenues for research on machine\ntranslation. Many approaches have been proposed in order to exploit\nmultilingual parallel corpora for improving translation quality. However, the\nlack of a comprehensive survey makes it difficult to determine which approaches\nare promising and hence deserve further exploration. In this paper, we present\nan in-depth survey of existing literature on MNMT. We first categorize various\napproaches based on their central use-case and then further categorize them\nbased on resource scenarios, underlying modeling principles, core-issues and\nchallenges. Wherever possible we address the strengths and weaknesses of\nseveral techniques by comparing them with each other. We also discuss the\nfuture directions that MNMT research might take. This paper is aimed towards\nboth, beginners and experts in NMT. We hope this paper will serve as a starting\npoint as well as a source of new ideas for researchers and engineers interested\nin MNMT.", "published": "2020-01-04 19:38:00", "link": "http://arxiv.org/abs/2001.01115v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Can x2vec Save Lives? Integrating Graph and Language Embeddings for\n  Automatic Mental Health Classification", "abstract": "Graph and language embedding models are becoming commonplace in large scale\nanalyses given their ability to represent complex sparse data densely in\nlow-dimensional space. Integrating these models' complementary relational and\ncommunicative data may be especially helpful if predicting rare events or\nclassifying members of hidden populations - tasks requiring huge and sparse\ndatasets for generalizable analyses. For example, due to social stigma and\ncomorbidities, mental health support groups often form in amorphous online\ngroups. Predicting suicidality among individuals in these settings using\nstandard network analyses is prohibitive due to resource limits (e.g., memory),\nand adding auxiliary data like text to such models exacerbates complexity- and\nsparsity-related issues. Here, I show how merging graph and language embedding\nmodels (metapath2vec and doc2vec) avoids these limits and extracts unsupervised\nclustering data without domain expertise or feature engineering. Graph and\nlanguage distances to a suicide support group have little correlation (\\r{ho} <\n0.23), implying the two models are not embedding redundant information. When\nused separately to predict suicidality among individuals, graph and language\ndata generate relatively accurate results (69% and 76%, respectively); however,\nwhen integrated, both data produce highly accurate predictions (90%, with 10%\nfalse-positives and 12% false-negatives). Visualizing graph embeddings\nannotated with predictions of potentially suicidal individuals shows the\nintegrated model could classify such individuals even if they are positioned\nfar from the support group. These results extend research on the importance of\nsimultaneously analyzing behavior and language in massive networks and efforts\nto integrate embedding models for different kinds of data when predicting and\nclassifying, particularly when they involve rare events.", "published": "2020-01-04 20:56:21", "link": "http://arxiv.org/abs/2001.01126v1", "categories": ["cs.SI", "cs.AI", "cs.CL", "cs.LG", "H.1.2; H.2.8; H.3.3; H.3.4; H.3.5; H.4.3; I.2.1; I.2.6; I.2.7; I.5"], "primary_category": "cs.SI"}
{"title": "Transformer-based language modeling and decoding for conversational\n  speech recognition", "abstract": "We propose a way to use a transformer-based language model in conversational\nspeech recognition. Specifically, we focus on decoding efficiently in a\nweighted finite-state transducer framework. We showcase an approach to lattice\nre-scoring that allows for longer range history captured by a transfomer-based\nlanguage model and takes advantage of a transformer's ability to avoid\ncomputing sequentially.", "published": "2020-01-04 23:27:59", "link": "http://arxiv.org/abs/2001.01140v1", "categories": ["cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
