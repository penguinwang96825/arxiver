{"title": "Kannada Spell Checker with Sandhi Splitter", "abstract": "Spelling errors are introduced in text either during typing, or when the user\ndoes not know the correct phoneme or grapheme. If a language contains complex\nwords like sandhi where two or more morphemes join based on some rules, spell\nchecking becomes very tedious. In such situations, having a spell checker with\nsandhi splitter which alerts the user by flagging the errors and providing\nsuggestions is very useful. A novel algorithm of sandhi splitting is proposed\nin this paper. The sandhi splitter can split about 7000 most common sandhi\nwords in Kannada language used as test samples. The sandhi splitter was\nintegrated with a Kannada spell checker and a mechanism for generating\nsuggestions was added. A comprehensive, platform independent, standalone spell\nchecker with sandhi splitter application software was thus developed and tested\nextensively for its efficiency and correctness. A comparative analysis of this\nspell checker with sandhi splitter was made and results concluded that the\nKannada spell checker with sandhi splitter has an improved performance. It is\ntwice as fast, 200 times more space efficient, and it is 90% accurate in case\nof complex nouns and 50% accurate for complex verbs. Such a spell checker with\nsandhi splitter will be of foremost significance in machine translation\nsystems, voice processing, etc. This is the first sandhi splitter in Kannada\nand the advantage of the novel algorithm is that, it can be extended to all\nIndian languages.", "published": "2016-11-25 06:18:29", "link": "http://arxiv.org/abs/1611.08358v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Machine Translation with Latent Semantic of Image and Text", "abstract": "Although attention-based Neural Machine Translation have achieved great\nsuccess, attention-mechanism cannot capture the entire meaning of the source\nsentence because the attention mechanism generates a target word depending\nheavily on the relevant parts of the source sentence. The report of earlier\nstudies has introduced a latent variable to capture the entire meaning of\nsentence and achieved improvement on attention-based Neural Machine\nTranslation. We follow this approach and we believe that the capturing meaning\nof sentence benefits from image information because human beings understand the\nmeaning of language not only from textual information but also from perceptual\ninformation such as that gained from vision. As described herein, we propose a\nneural machine translation model that introduces a continuous latent variable\ncontaining an underlying semantic extracted from texts and images. Our model,\nwhich can be trained end-to-end, requires image information only when training.\nExperiments conducted with an English--German translation task show that our\nmodel outperforms over the baseline.", "published": "2016-11-25 14:10:39", "link": "http://arxiv.org/abs/1611.08459v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Simple, Fast Diverse Decoding Algorithm for Neural Generation", "abstract": "In this paper, we propose a simple, fast decoding algorithm that fosters\ndiversity in neural generation. The algorithm modifies the standard beam search\nalgorithm by adding an inter-sibling ranking penalty, favoring choosing\nhypotheses from diverse parents. We evaluate the proposed model on the tasks of\ndialogue response generation, abstractive summarization and machine\ntranslation. We find that diverse decoding helps across all tasks, especially\nthose for which reranking is needed.\n  We further propose a variation that is capable of automatically adjusting its\ndiversity decoding rates for different inputs using reinforcement learning\n(RL). We observe a further performance boost from this RL technique. This paper\nincludes material from the unpublished script \"Mutual Information and Diverse\nDecoding Improve Neural Machine Translation\" (Li and Jurafsky, 2016).", "published": "2016-11-25 19:18:27", "link": "http://arxiv.org/abs/1611.08562v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bidirectional LSTM-CRF for Clinical Concept Extraction", "abstract": "Automated extraction of concepts from patient clinical records is an\nessential facilitator of clinical research. For this reason, the 2010 i2b2/VA\nNatural Language Processing Challenges for Clinical Records introduced a\nconcept extraction task aimed at identifying and classifying concepts into\npredefined categories (i.e., treatments, tests and problems). State-of-the-art\nconcept extraction approaches heavily rely on handcrafted features and\ndomain-specific resources which are hard to collect and define. For this\nreason, this paper proposes an alternative, streamlined approach: a recurrent\nneural network (the bidirectional LSTM with CRF decoding) initialized with\ngeneral-purpose, off-the-shelf word embeddings. The experimental results\nachieved on the 2010 i2b2/VA reference corpora using the proposed framework\noutperform all recent methods and ranks closely to the best submission from the\noriginal 2010 i2b2/VA challenge.", "published": "2016-11-25 08:11:23", "link": "http://arxiv.org/abs/1611.08373v1", "categories": ["stat.ML", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
