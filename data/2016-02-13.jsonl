{"title": "Attention-Based Convolutional Neural Network for Machine Comprehension", "abstract": "Understanding open-domain text is one of the primary challenges in natural\nlanguage processing (NLP). Machine comprehension benchmarks evaluate the\nsystem's ability to understand text based on the text content only. In this\nwork, we investigate machine comprehension on MCTest, a question answering (QA)\nbenchmark. Prior work is mainly based on feature engineering approaches. We\ncome up with a neural network framework, named hierarchical attention-based\nconvolutional neural network (HABCNN), to address this task without any\nmanually designed features. Specifically, we explore HABCNN for this task by\ntwo routes, one is through traditional joint modeling of passage, question and\nanswer, one is through textual entailment. HABCNN employs an attention\nmechanism to detect key phrases, key sentences and key snippets that are\nrelevant to answering the question. Experiments show that HABCNN outperforms\nprior deep learning approaches by a big margin.", "published": "2016-02-13 14:38:47", "link": "http://arxiv.org/abs/1602.04341v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Signer-independent Fingerspelling Recognition with Deep Neural Network\n  Adaptation", "abstract": "We study the problem of recognition of fingerspelled letter sequences in\nAmerican Sign Language in a signer-independent setting. Fingerspelled sequences\nare both challenging and important to recognize, as they are used for many\ncontent words such as proper nouns and technical terms. Previous work has shown\nthat it is possible to achieve almost 90% accuracies on fingerspelling\nrecognition in a signer-dependent setting. However, the more realistic\nsigner-independent setting presents challenges due to significant variations\namong signers, coupled with the dearth of available training data. We\ninvestigate this problem with approaches inspired by automatic speech\nrecognition. We start with the best-performing approaches from prior work,\nbased on tandem models and segmental conditional random fields (SCRFs), with\nfeatures based on deep neural network (DNN) classifiers of letters and\nphonological features. Using DNN adaptation, we find that it is possible to\nbridge a large part of the gap between signer-dependent and signer-independent\nperformance. Using only about 115 transcribed words for adaptation from the\ntarget signer, we obtain letter accuracies of up to 82.7% with frame-level\nadaptation labels and 69.7% with only word labels.", "published": "2016-02-13 03:30:34", "link": "http://arxiv.org/abs/1602.04278v1", "categories": ["cs.CL", "cs.CV", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Science Question Answering using Instructional Materials", "abstract": "We provide a solution for elementary science test using instructional\nmaterials. We posit that there is a hidden structure that explains the\ncorrectness of an answer given the question and instructional materials and\npresent a unified max-margin framework that learns to find these hidden\nstructures (given a corpus of question-answer pairs and instructional\nmaterials), and uses what it learns to answer novel elementary science\nquestions. Our evaluation shows that our framework outperforms several strong\nbaselines.", "published": "2016-02-13 20:13:48", "link": "http://arxiv.org/abs/1602.04375v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
