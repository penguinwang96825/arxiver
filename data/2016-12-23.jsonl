{"title": "A CRF Based POS Tagger for Code-mixed Indian Social Media Text", "abstract": "In this work, we describe a conditional random fields (CRF) based system for\nPart-Of- Speech (POS) tagging of code-mixed Indian social media text as part of\nour participation in the tool contest on POS tagging for codemixed Indian\nsocial media text, held in conjunction with the 2016 International Conference\non Natural Language Processing, IIT(BHU), India. We participated only in\nconstrained mode contest for all three language pairs, Bengali-English,\nHindi-English and Telegu-English. Our system achieves the overall average F1\nscore of 79.99, which is the highest overall average F1 score among all 16\nsystems participated in constrained mode contest.", "published": "2016-12-23 12:58:58", "link": "http://arxiv.org/abs/1612.07956v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Modeling with Gated Convolutional Networks", "abstract": "The pre-dominant approach to language modeling to date is based on recurrent\nneural networks. Their success on this task is often linked to their ability to\ncapture unbounded context. In this paper we develop a finite context approach\nthrough stacked convolutions, which can be more efficient since they allow\nparallelization over sequential tokens. We propose a novel simplified gating\nmechanism that outperforms Oord et al (2016) and investigate the impact of key\narchitectural decisions. The proposed approach achieves state-of-the-art on the\nWikiText-103 benchmark, even though it features long-term dependencies, as well\nas competitive results on the Google Billion Words benchmark. Our model reduces\nthe latency to score a sentence by an order of magnitude compared to a\nrecurrent baseline. To our knowledge, this is the first time a non-recurrent\napproach is competitive with strong recurrent models on these large scale\nlanguage tasks.", "published": "2016-12-23 20:32:33", "link": "http://arxiv.org/abs/1612.08083v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Supervised Opinion Aspect Extraction by Exploiting Past Extraction\n  Results", "abstract": "One of the key tasks of sentiment analysis of product reviews is to extract\nproduct aspects or features that users have expressed opinions on. In this\nwork, we focus on using supervised sequence labeling as the base approach to\nperforming the task. Although several extraction methods using sequence\nlabeling methods such as Conditional Random Fields (CRF) and Hidden Markov\nModels (HMM) have been proposed, we show that this supervised approach can be\nsignificantly improved by exploiting the idea of concept sharing across\nmultiple domains. For example, \"screen\" is an aspect in iPhone, but not only\niPhone has a screen, many electronic devices have screens too. When \"screen\"\nappears in a review of a new domain (or product), it is likely to be an aspect\ntoo. Knowing this information enables us to do much better extraction in the\nnew domain. This paper proposes a novel extraction method exploiting this idea\nin the context of supervised sequence labeling. Experimental results show that\nit produces markedly better results than without using the past information.", "published": "2016-12-23 11:32:37", "link": "http://arxiv.org/abs/1612.07940v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "\"What is Relevant in a Text Document?\": An Interpretable Machine\n  Learning Approach", "abstract": "Text documents can be described by a number of abstract concepts such as\nsemantic category, writing style, or sentiment. Machine learning (ML) models\nhave been trained to automatically map documents to these abstract concepts,\nallowing to annotate very large text collections, more than could be processed\nby a human in a lifetime. Besides predicting the text's category very\naccurately, it is also highly desirable to understand how and why the\ncategorization process takes place. In this paper, we demonstrate that such\nunderstanding can be achieved by tracing the classification decision back to\nindividual words using layer-wise relevance propagation (LRP), a recently\ndeveloped technique for explaining predictions of complex non-linear\nclassifiers. We train two word-based ML models, a convolutional neural network\n(CNN) and a bag-of-words SVM classifier, on a topic categorization task and\nadapt the LRP method to decompose the predictions of these models onto words.\nResulting scores indicate how much individual words contribute to the overall\nclassification decision. This enables one to distill relevant information from\ntext documents without an explicit semantic information extraction step. We\nfurther use the word-wise relevance scores for generating novel vector-based\ndocument representations which capture semantic information. Based on these\ndocument vectors, we introduce a measure of model explanatory power and show\nthat, although the SVM and CNN models perform similarly in terms of\nclassification accuracy, the latter exhibits a higher level of explainability\nwhich makes it more comprehensible for humans and potentially more useful for\nother applications.", "published": "2016-12-23 00:31:30", "link": "http://arxiv.org/abs/1612.07843v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
