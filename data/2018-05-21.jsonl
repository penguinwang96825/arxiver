{"title": "Knowledgeable Reader: Enhancing Cloze-Style Reading Comprehension with\n  External Commonsense Knowledge", "abstract": "We introduce a neural reading comprehension model that integrates external\ncommonsense knowledge, encoded as a key-value memory, in a cloze-style setting.\nInstead of relying only on document-to-question interaction or discrete\nfeatures as in prior work, our model attends to relevant external knowledge and\ncombines this knowledge with the context representation before inferring the\nanswer. This allows the model to attract and imply knowledge from an external\nknowledge source that is not explicitly stated in the text, but that is\nrelevant for inferring the answer. Our model improves results over a very\nstrong baseline on a hard Common Nouns dataset, making it a strong competitor\nof much more complex models. By including knowledge explicitly, our model can\nalso provide evidence about the background knowledge used in the RC process.", "published": "2018-05-21 01:13:42", "link": "http://arxiv.org/abs/1805.07858v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentence Modeling via Multiple Word Embeddings and Multi-level\n  Comparison for Semantic Textual Similarity", "abstract": "Different word embedding models capture different aspects of linguistic\nproperties. This inspired us to propose a model (M-MaxLSTM-CNN) for employing\nmultiple sets of word embeddings for evaluating sentence similarity/relation.\nRepresenting each word by multiple word embeddings, the MaxLSTM-CNN encoder\ngenerates a novel sentence embedding. We then learn the similarity/relation\nbetween our sentence embeddings via Multi-level comparison. Our method\nM-MaxLSTM-CNN consistently shows strong performances in several tasks (i.e.,\nmeasure textual similarity, identify paraphrase, recognize textual entailment).\nAccording to the experimental results on STS Benchmark dataset and SICK dataset\nfrom SemEval, M-MaxLSTM-CNN outperforms the state-of-the-art methods for\ntextual similarity tasks. Our model does not use hand-crafted features (e.g.,\nalignment features, Ngram overlaps, dependency features) as well as does not\nrequire pre-trained word embeddings to have the same dimension.", "published": "2018-05-21 03:54:39", "link": "http://arxiv.org/abs/1805.07882v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Aspect Term Extraction with Bidirectional Dependency Tree\n  Representation", "abstract": "Aspect term extraction is one of the important subtasks in aspect-based\nsentiment analysis. Previous studies have shown that using dependency tree\nstructure representation is promising for this task. However, most dependency\ntree structures involve only one directional propagation on the dependency\ntree. In this paper, we first propose a novel bidirectional dependency tree\nnetwork to extract dependency structure features from the given sentences. The\nkey idea is to explicitly incorporate both representations gained separately\nfrom the bottom-up and top-down propagation on the given dependency syntactic\ntree. An end-to-end framework is then developed to integrate the embedded\nrepresentations and BiLSTM plus CRF to learn both tree-structured and\nsequential features to solve the aspect term extraction problem. Experimental\nresults demonstrate that the proposed model outperforms state-of-the-art\nbaseline models on four benchmark SemEval datasets.", "published": "2018-05-21 04:49:53", "link": "http://arxiv.org/abs/1805.07889v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Morphological analysis using a sequence decoder", "abstract": "We introduce Morse, a recurrent encoder-decoder model that produces\nmorphological analyses of each word in a sentence. The encoder turns the\nrelevant information about the word and its context into a fixed size vector\nrepresentation and the decoder generates the sequence of characters for the\nlemma followed by a sequence of individual morphological features. We show that\ngenerating morphological features individually rather than as a combined tag\nallows the model to handle rare or unseen tags and outperform whole-tag models.\nIn addition, generating morphological features as a sequence rather than e.g.\\\nan unordered set allows our model to produce an arbitrary number of features\nthat represent multiple inflectional groups in morphologically complex\nlanguages. We obtain state-of-the art results in nine languages of different\nmorphological complexity under low-resource, high-resource and transfer\nlearning settings. We also introduce TrMor2018, a new high accuracy Turkish\nmorphology dataset. Our Morse implementation and the TrMor2018 dataset are\navailable online to support future research\\footnote{See\n\\url{https://github.com/ai-ku/Morse.jl} for a Morse implementation in\nJulia/Knet \\cite{knet2016mlsys} and \\url{https://github.com/ai-ku/TrMor2018}\nfor the new Turkish dataset.}.", "published": "2018-05-21 08:49:32", "link": "http://arxiv.org/abs/1805.07946v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A new dataset and model for learning to understand navigational\n  instructions", "abstract": "In this paper, we present a state-of-the-art model and introduce a new\ndataset for grounded language learning. Our goal is to develop a model that can\nlearn to follow new instructions given prior instruction-perception-action\nexamples. We based our work on the SAIL dataset which consists of navigational\ninstructions and actions in a maze-like environment. The new model we propose\nachieves the best results to date on the SAIL dataset by using an improved\nperceptual component that can represent relative positions of objects. We also\nanalyze the problems with the SAIL dataset regarding its size and balance. We\nargue that performance on a small, fixed-size dataset is no longer a good\nmeasure to differentiate state-of-the-art models. We introduce SAILx, a\nsynthetic dataset generator, and perform experiments where the size and balance\nof the dataset are controlled.", "published": "2018-05-21 09:01:31", "link": "http://arxiv.org/abs/1805.07952v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Incorporating Glosses into Neural Word Sense Disambiguation", "abstract": "Word Sense Disambiguation (WSD) aims to identify the correct meaning of\npolysemous words in the particular context. Lexical resources like WordNet\nwhich are proved to be of great help for WSD in the knowledge-based methods.\nHowever, previous neural networks for WSD always rely on massive labeled data\n(context), ignoring lexical resources like glosses (sense definitions). In this\npaper, we integrate the context and glosses of the target word into a unified\nframework in order to make full use of both labeled data and lexical knowledge.\nTherefore, we propose GAS: a gloss-augmented WSD neural network which jointly\nencodes the context and glosses of the target word. GAS models the semantic\nrelationship between the context and the gloss in an improved memory network\nframework, which breaks the barriers of the previous supervised methods and\nknowledge-based methods. We further extend the original gloss of word sense via\nits semantic relations in WordNet to enrich the gloss information. The\nexperimental results show that our model outperforms the state-of-theart\nsystems on several English all-words WSD datasets.", "published": "2018-05-21 12:59:17", "link": "http://arxiv.org/abs/1805.08028v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Talker Ensemble: the University of Wroc\u0142aw's Entry to the NIPS 2017\n  Conversational Intelligence Challenge", "abstract": "We present Poetwannabe, a chatbot submitted by the University of Wroc{\\l}aw\nto the NIPS 2017 Conversational Intelligence Challenge, in which it ranked\nfirst ex-aequo. It is able to conduct a conversation with a user in a natural\nlanguage. The primary functionality of our dialogue system is context-aware\nquestion answering (QA), while its secondary function is maintaining user\nengagement. The chatbot is composed of a number of sub-modules, which\nindependently prepare replies to user's prompts and assess their own\nconfidence. To answer questions, our dialogue system relies heavily on factual\ndata, sourced mostly from Wikipedia and DBpedia, data of real user interactions\nin public forums, as well as data concerning general literature. Where\napplicable, modules are trained on large datasets using GPUs. However, to\ncomply with the competition's requirements, the final system is compact and\nruns on commodity hardware.", "published": "2018-05-21 13:07:31", "link": "http://arxiv.org/abs/1805.08032v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient and Robust Question Answering from Minimal Context over\n  Documents", "abstract": "Neural models for question answering (QA) over documents have achieved\nsignificant performance improvements. Although effective, these models do not\nscale to large corpora due to their complex modeling of interactions between\nthe document and the question. Moreover, recent work has shown that such models\nare sensitive to adversarial inputs. In this paper, we study the minimal\ncontext required to answer the question, and find that most questions in\nexisting datasets can be answered with a small set of sentences. Inspired by\nthis observation, we propose a simple sentence selector to select the minimal\nset of sentences to feed into the QA model. Our overall system achieves\nsignificant reductions in training (up to 15 times) and inference times (up to\n13 times), with accuracy comparable to or better than the state-of-the-art on\nSQuAD, NewsQA, TriviaQA and SQuAD-Open. Furthermore, our experimental results\nand analyses show that our approach is more robust to adversarial inputs.", "published": "2018-05-21 14:48:08", "link": "http://arxiv.org/abs/1805.08092v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NeuralREG: An end-to-end approach to referring expression generation", "abstract": "Traditionally, Referring Expression Generation (REG) models first decide on\nthe form and then on the content of references to discourse entities in text,\ntypically relying on features such as salience and grammatical function. In\nthis paper, we present a new approach (NeuralREG), relying on deep neural\nnetworks, which makes decisions about form and content in one go without\nexplicit feature extraction. Using a delexicalized version of the WebNLG\ncorpus, we show that the neural model substantially improves over two strong\nbaselines. Data and models are publicly available.", "published": "2018-05-21 14:48:14", "link": "http://arxiv.org/abs/1805.08093v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Computational Historical Linguistics", "abstract": "Computational approaches to historical linguistics have been proposed since\nhalf a century. Within the last decade, this line of research has received a\nmajor boost, owing both to the transfer of ideas and software from\ncomputational biology and to the release of several large electronic data\nresources suitable for systematic comparative work.\n  In this article, some of the central research topic of this new wave of\ncomputational historical linguistics are introduced and discussed. These are\nautomatic assessment of genetic relatedness, automatic cognate detection,\nphylogenetic inference and ancestral state reconstruction. They will be\ndemonstrated by means of a case study of automatically reconstructing a\nProto-Romance word list from lexical data of 50 modern Romance languages and\ndialects.", "published": "2018-05-21 14:57:38", "link": "http://arxiv.org/abs/1805.08099v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Morphosyntactic Tagging with a Meta-BiLSTM Model over Context Sensitive\n  Token Encodings", "abstract": "The rise of neural networks, and particularly recurrent neural networks, has\nproduced significant advances in part-of-speech tagging accuracy. One\ncharacteristic common among these models is the presence of rich initial word\nencodings. These encodings typically are composed of a recurrent\ncharacter-based representation with learned and pre-trained word embeddings.\nHowever, these encodings do not consider a context wider than a single word and\nit is only through subsequent recurrent layers that word or sub-word\ninformation interacts. In this paper, we investigate models that use recurrent\nneural networks with sentence-level context for initial character and\nword-based representations. In particular we show that optimal results are\nobtained by integrating these context sensitive representations through\nsynchronized training with a meta-model that learns to combine their states. We\npresent results on part-of-speech and morphological tagging with\nstate-of-the-art performance on a number of languages.", "published": "2018-05-21 18:09:23", "link": "http://arxiv.org/abs/1805.08237v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sparse and Constrained Attention for Neural Machine Translation", "abstract": "In NMT, words are sometimes dropped from the source or generated repeatedly\nin the translation. We explore novel strategies to address the coverage problem\nthat change only the attention transformation. Our approach allocates\nfertilities to source words, used to bound the attention each word can receive.\nWe experiment with various sparse and constrained attention transformations and\npropose a new one, constrained sparsemax, shown to be differentiable and\nsparse. Empirical evaluation is provided in three languages pairs.", "published": "2018-05-21 18:14:35", "link": "http://arxiv.org/abs/1805.08241v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Halo: Learning Semantics-Aware Representations for Cross-Lingual\n  Information Extraction", "abstract": "Cross-lingual information extraction (CLIE) is an important and challenging\ntask, especially in low resource scenarios. To tackle this challenge, we\npropose a training method, called Halo, which enforces the local region of each\nhidden state of a neural model to only generate target tokens with the same\nsemantic structure tag. This simple but powerful technique enables a neural\nmodel to learn semantics-aware representations that are robust to noise,\nwithout introducing any extra parameter, thus yielding better generalization in\nboth high and low resource settings.", "published": "2018-05-21 19:57:23", "link": "http://arxiv.org/abs/1805.08271v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Character-based Neural Networks for Sentence Pair Modeling", "abstract": "Sentence pair modeling is critical for many NLP tasks, such as paraphrase\nidentification, semantic textual similarity, and natural language inference.\nMost state-of-the-art neural models for these tasks rely on pretrained word\nembedding and compose sentence-level semantics in varied ways; however, few\nworks have attempted to verify whether we really need pretrained embeddings in\nthese tasks. In this paper, we study how effective subword-level (character and\ncharacter n-gram) representations are in sentence pair modeling. Though it is\nwell-known that subword models are effective in tasks with single sentence\ninput, including language modeling and machine translation, they have not been\nsystematically studied in sentence pair modeling tasks where the semantic and\nstring similarities between texts matter. Our experiments show that subword\nmodels without any pretrained word embedding can achieve new state-of-the-art\nresults on two social media datasets and competitive results on news data for\nparaphrase identification.", "published": "2018-05-21 21:36:09", "link": "http://arxiv.org/abs/1805.08297v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Text Analysis of Federal Reserve meeting minutes", "abstract": "Recent developments in monetary policy by the Federal Reserve has created a\nneed for an objective method of communication analysis.Using methods developed\nfor text analysis, we present a novel technique of analysis which creates a\nsemantic space defined by various policymakers public comments and places the\ncommittee consensus in the appropriate location. Its then possible to determine\nwhich member of the committee is most closely aligned with the committee\nconsensus over time and create a foundation for further actionable research.", "published": "2018-05-21 00:27:05", "link": "http://arxiv.org/abs/1805.07851v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Aff2Vec: Affect--Enriched Distributional Word Representations", "abstract": "Human communication includes information, opinions, and reactions. Reactions\nare often captured by the affective-messages in written as well as verbal\ncommunications. While there has been work in affect modeling and to some extent\naffective content generation, the area of affective word distributions in not\nwell studied. Synsets and lexica capture semantic relationships across words.\nThese models however lack in encoding affective or emotional word\ninterpretations. Our proposed model, Aff2Vec provides a method for enriched\nword embeddings that are representative of affective interpretations of words.\nAff2Vec outperforms the state--of--the--art in intrinsic word-similarity tasks.\nFurther, the use of Aff2Vec representations outperforms baseline embeddings in\ndownstream natural language understanding tasks including sentiment analysis,\npersonality detection, and frustration prediction.", "published": "2018-05-21 10:10:16", "link": "http://arxiv.org/abs/1805.07966v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi-Perspective Relevance Matching with Hierarchical ConvNets for\n  Social Media Search", "abstract": "Despite substantial interest in applications of neural networks to\ninformation retrieval, neural ranking models have only been applied to standard\nad hoc retrieval tasks over web pages and newswire documents. This paper\nproposes MP-HCNN (Multi-Perspective Hierarchical Convolutional Neural Network)\na novel neural ranking model specifically designed for ranking short social\nmedia posts. We identify document length, informal language, and heterogeneous\nrelevance signals as features that distinguish documents in our domain, and\npresent a model specifically designed with these characteristics in mind. Our\nmodel uses hierarchical convolutional layers to learn latent semantic\nsoft-match relevance signals at the character, word, and phrase levels. A\npooling-based similarity measurement layer integrates evidence from multiple\ntypes of matches between the query, the social media post, as well as URLs\ncontained in the post. Extensive experiments using Twitter data from the TREC\nMicroblog Tracks 2011--2014 show that our model significantly outperforms prior\nfeature-based as well and existing neural ranking models. To our best\nknowledge, this paper presents the first substantial work tackling search over\nsocial media posts using neural ranking models.", "published": "2018-05-21 16:25:15", "link": "http://arxiv.org/abs/1805.08159v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Reproducibility Report for \"Learning To Count Objects In Natural Images\n  For Visual Question Answering\"", "abstract": "This is the reproducibility report for the paper \"Learning To Count Objects\nIn Natural Images For Visual QuestionAnswering\"", "published": "2018-05-21 16:50:55", "link": "http://arxiv.org/abs/1805.08174v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Party Matters: Enhancing Legislative Embeddings with Author Attributes\n  for Vote Prediction", "abstract": "Predicting how Congressional legislators will vote is important for\nunderstanding their past and future behavior. However, previous work on\nroll-call prediction has been limited to single session settings, thus did not\nconsider generalization across sessions. In this paper, we show that metadata\nis crucial for modeling voting outcomes in new contexts, as changes between\nsessions lead to changes in the underlying data generation process. We show how\naugmenting bill text with the sponsors' ideologies in a neural network model\ncan achieve an average of a 4% boost in accuracy over the previous\nstate-of-the-art.", "published": "2018-05-21 17:03:13", "link": "http://arxiv.org/abs/1805.08182v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Bilinear Attention Networks", "abstract": "Attention networks in multimodal learning provide an efficient way to utilize\ngiven visual information selectively. However, the computational cost to learn\nattention distributions for every pair of multimodal input channels is\nprohibitively expensive. To solve this problem, co-attention builds two\nseparate attention distributions for each modality neglecting the interaction\nbetween multimodal inputs. In this paper, we propose bilinear attention\nnetworks (BAN) that find bilinear attention distributions to utilize given\nvision-language information seamlessly. BAN considers bilinear interactions\namong two groups of input channels, while low-rank bilinear pooling extracts\nthe joint representations for each pair of channels. Furthermore, we propose a\nvariant of multimodal residual networks to exploit eight-attention maps of the\nBAN efficiently. We quantitatively and qualitatively evaluate our model on\nvisual question answering (VQA 2.0) and Flickr30k Entities datasets, showing\nthat BAN significantly outperforms previous methods and achieves new\nstate-of-the-arts on both datasets.", "published": "2018-05-21 07:58:31", "link": "http://arxiv.org/abs/1805.07932v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Numeracy for Language Models: Evaluating and Improving their Ability to\n  Predict Numbers", "abstract": "Numeracy is the ability to understand and work with numbers. It is a\nnecessary skill for composing and understanding documents in clinical,\nscientific, and other technical domains. In this paper, we explore different\nstrategies for modelling numerals with language models, such as memorisation\nand digit-by-digit composition, and propose a novel neural architecture that\nuses a continuous probability density function to model numerals from an open\nvocabulary. Our evaluation on clinical and scientific datasets shows that using\nhierarchical models to distinguish numerals from words improves a perplexity\nmetric on the subset of numerals by 2 and 4 orders of magnitude, respectively,\nover non-hierarchical models. A combination of strategies can further improve\nperplexity. Our continuous probability density function model reduces mean\nabsolute percentage errors by 18% and 54% in comparison to the second best\nstrategy for each dataset, respectively.", "published": "2018-05-21 16:18:41", "link": "http://arxiv.org/abs/1805.08154v1", "categories": ["cs.CL", "cs.NE", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Adversarial Learning of Raw Speech Features for Domain Invariant Speech\n  Recognition", "abstract": "Recent advances in neural network based acoustic modelling have shown\nsignificant improvements in automatic speech recognition (ASR) performance. In\norder for acoustic models to be able to handle large acoustic variability,\nlarge amounts of labeled data is necessary, which are often expensive to\nobtain. This paper explores the application of adversarial training to learn\nfeatures from raw speech that are invariant to acoustic variability. This\nacoustic variability is referred to as a domain shift in this paper. The\nexperimental study presented in this paper leverages the architecture of Domain\nAdversarial Neural Networks (DANNs) [1] which uses data from two different\ndomains. The DANN is a Y-shaped network that consists of a multi-layer CNN\nfeature extractor module that is common to a label (senone) classifier and a\nso-called domain classifier. The utility of DANNs is evaluated on multiple\ndatasets with domain shifts caused due to differences in gender and speaker\naccents. Promising empirical results indicate the strength of adversarial\ntraining for unsupervised domain adaptation in ASR, thereby emphasizing the\nability of DANNs to learn domain invariant features from raw speech.", "published": "2018-05-21 11:13:27", "link": "http://arxiv.org/abs/1805.08615v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speaker Clustering Using Dominant Sets", "abstract": "Speaker clustering is the task of forming speaker-specific groups based on a\nset of utterances. In this paper, we address this task by using Dominant Sets\n(DS). DS is a graph-based clustering algorithm with interesting properties that\nfits well to our problem and has never been applied before to speaker\nclustering. We report on a comprehensive set of experiments on the TIMIT\ndataset against standard clustering techniques and specific speaker clustering\nmethods. Moreover, we compare performances under different features by using\nones learned via deep neural network directly on TIMIT and other ones extracted\nfrom a pre-trained VGGVox net. To asses the stability, we perform a sensitivity\nanalysis on the free parameters of our method, showing that performance is\nstable under parameter changes. The extensive experimentation carried out\nconfirms the validity of the proposed method, reporting state-of-the-art\nresults under three different standard metrics. We also report reference\nbaseline results for speaker clustering on the entire TIMIT dataset for the\nfirst time.", "published": "2018-05-21 16:11:52", "link": "http://arxiv.org/abs/1805.08641v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
