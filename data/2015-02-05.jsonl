{"title": "Beyond Word-based Language Model in Statistical Machine Translation", "abstract": "Language model is one of the most important modules in statistical machine\ntranslation and currently the word-based language model dominants this\ncommunity. However, many translation models (e.g. phrase-based models) generate\nthe target language sentences by rendering and compositing the phrases rather\nthan the words. Thus, it is much more reasonable to model dependency between\nphrases, but few research work succeed in solving this problem. In this paper,\nwe tackle this problem by designing a novel phrase-based language model which\nattempts to solve three key sub-problems: 1, how to define a phrase in language\nmodel; 2, how to determine the phrase boundary in the large-scale monolingual\ndata in order to enlarge the training set; 3, how to alleviate the data\nsparsity problem due to the huge vocabulary size of phrases. By carefully\nhandling these issues, the extensive experiments on Chinese-to-English\ntranslation show that our phrase-based language model can significantly improve\nthe translation quality by up to +1.47 absolute BLEU score.", "published": "2015-02-05 07:42:18", "link": "http://arxiv.org/abs/1502.01446v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text Understanding from Scratch", "abstract": "This article demontrates that we can apply deep learning to text\nunderstanding from character-level inputs all the way up to abstract text\nconcepts, using temporal convolutional networks (ConvNets). We apply ConvNets\nto various large-scale datasets, including ontology classification, sentiment\nanalysis, and text categorization. We show that temporal ConvNets can achieve\nastonishing performance without the knowledge of words, phrases, sentences and\nany other syntactic or semantic structures with regards to a human language.\nEvidence shows that our models can work for both English and Chinese.", "published": "2015-02-05 20:45:19", "link": "http://arxiv.org/abs/1502.01710v5", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Use of Modality and Negation in Semantically-Informed Syntactic MT", "abstract": "This paper describes the resource- and system-building efforts of an\neight-week Johns Hopkins University Human Language Technology Center of\nExcellence Summer Camp for Applied Language Exploration (SCALE-2009) on\nSemantically-Informed Machine Translation (SIMT). We describe a new\nmodality/negation (MN) annotation scheme, the creation of a (publicly\navailable) MN lexicon, and two automated MN taggers that we built using the\nannotation scheme and lexicon. Our annotation scheme isolates three components\nof modality and negation: a trigger (a word that conveys modality or negation),\na target (an action associated with modality or negation) and a holder (an\nexperiencer of modality). We describe how our MN lexicon was semi-automatically\nproduced and we demonstrate that a structure-based MN tagger results in\nprecision around 86% (depending on genre) for tagging of a standard LDC data\nset.\n  We apply our MN annotation scheme to statistical machine translation using a\nsyntactic framework that supports the inclusion of semantic annotations.\nSyntactic tags enriched with semantic annotations are assigned to parse trees\nin the target-language training texts through a process of tree grafting. While\nthe focus of our work is modality and negation, the tree grafting procedure is\ngeneral and supports other types of semantic information. We exploit this\ncapability by including named entities, produced by a pre-existing tagger, in\naddition to the MN elements produced by the taggers described in this paper.\nThe resulting system significantly outperformed a linguistically naive baseline\nmodel (Hiero), and reached the highest scores yet reported on the NIST 2009\nUrdu-English test set. This finding supports the hypothesis that both syntactic\nand semantic information can improve translation quality.", "published": "2015-02-05 19:10:26", "link": "http://arxiv.org/abs/1502.01682v1", "categories": ["cs.CL", "cs.LG", "stat.ML", "I.2.7; I.2.6; I.5.1; I.5.4"], "primary_category": "cs.CL"}
{"title": "Monitoring Term Drift Based on Semantic Consistency in an Evolving\n  Vector Field", "abstract": "Based on the Aristotelian concept of potentiality vs. actuality allowing for\nthe study of energy and dynamics in language, we propose a field approach to\nlexical analysis. Falling back on the distributional hypothesis to\nstatistically model word meaning, we used evolving fields as a metaphor to\nexpress time-dependent changes in a vector space model by a combination of\nrandom indexing and evolving self-organizing maps (ESOM). To monitor semantic\ndrifts within the observation period, an experiment was carried out on the term\nspace of a collection of 12.8 million Amazon book reviews. For evaluation, the\nsemantic consistency of ESOM term clusters was compared with their respective\nneighbourhoods in WordNet, and contrasted with distances among term vectors by\nrandom indexing. We found that at 0.05 level of significance, the terms in the\nclusters showed a high level of semantic consistency. Tracking the drift of\ndistributional patterns in the term space across time periods, we found that\nconsistency decreased, but not at a statistically significant level. Our method\nis highly scalable, with interpretations in philosophy.", "published": "2015-02-05 22:51:45", "link": "http://arxiv.org/abs/1502.01753v1", "categories": ["cs.CL", "cs.LG", "cs.NE", "stat.ML"], "primary_category": "cs.CL"}
