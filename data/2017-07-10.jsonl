{"title": "Learning to Compose Task-Specific Tree Structures", "abstract": "For years, recursive neural networks (RvNNs) have been shown to be suitable\nfor representing text into fixed-length vectors and achieved good performance\non several natural language processing tasks. However, the main drawback of\nRvNNs is that they require structured input, which makes data preparation and\nmodel implementation hard. In this paper, we propose Gumbel Tree-LSTM, a novel\ntree-structured long short-term memory architecture that learns how to compose\ntask-specific tree structures only from plain text data efficiently. Our model\nuses Straight-Through Gumbel-Softmax estimator to decide the parent node among\ncandidates dynamically and to calculate gradients of the discrete decision. We\nevaluate the proposed model on natural language inference and sentiment\nanalysis, and show that our model outperforms or is at least comparable to\nprevious models. We also find that our model converges significantly faster\nthan other models.", "published": "2017-07-10 10:18:08", "link": "http://arxiv.org/abs/1707.02786v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Generalized Recurrent Neural Architecture for Text Classification with\n  Multi-Task Learning", "abstract": "Multi-task learning leverages potential correlations among related tasks to\nextract common features and yield performance gains. However, most previous\nworks only consider simple or weak interactions, thereby failing to model\ncomplex correlations among three or more tasks. In this paper, we propose a\nmulti-task learning architecture with four types of recurrent neural layers to\nfuse information across multiple related tasks. The architecture is\nstructurally flexible and considers various interactions among tasks, which can\nbe regarded as a generalized case of many previous works. Extensive experiments\non five benchmark datasets for text classification show that our model can\nsignificantly improve performances of related tasks with additional information\nfrom others.", "published": "2017-07-10 14:58:53", "link": "http://arxiv.org/abs/1707.02892v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Neural Parsing by Disentangling Model Combination and\n  Reranking Effects", "abstract": "Recent work has proposed several generative neural models for constituency\nparsing that achieve state-of-the-art results. Since direct search in these\ngenerative models is difficult, they have primarily been used to rescore\ncandidate outputs from base parsers in which decoding is more straightforward.\nWe first present an algorithm for direct search in these generative models. We\nthen demonstrate that the rescoring results are at least partly due to implicit\nmodel combination rather than reranking effects. Finally, we show that explicit\nmodel combination can improve performance even further, resulting in new\nstate-of-the-art numbers on the PTB of 94.25 F1 when training only on gold data\nand 94.66 F1 when using external data.", "published": "2017-07-10 20:47:33", "link": "http://arxiv.org/abs/1707.03058v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding State Preferences With Text As Data: Introducing the UN\n  General Debate Corpus", "abstract": "Every year at the United Nations, member states deliver statements during the\nGeneral Debate discussing major issues in world politics. These speeches\nprovide invaluable information on governments' perspectives and preferences on\na wide range of issues, but have largely been overlooked in the study of\ninternational politics. This paper introduces a new dataset consisting of over\n7,701 English-language country statements from 1970-2016. We demonstrate how\nthe UN General Debate Corpus (UNGDC) can be used to derive country positions on\ndifferent policy dimensions using text analytic methods. The paper provides\napplications of these estimates, demonstrating the contribution the UNGDC can\nmake to the study of international politics.", "published": "2017-07-10 09:40:12", "link": "http://arxiv.org/abs/1707.02774v1", "categories": ["cs.CL", "cs.AI", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Towards Crafting Text Adversarial Samples", "abstract": "Adversarial samples are strategically modified samples, which are crafted\nwith the purpose of fooling a classifier at hand. An attacker introduces\nspecially crafted adversarial samples to a deployed classifier, which are being\nmis-classified by the classifier. However, the samples are perceived to be\ndrawn from entirely different classes and thus it becomes hard to detect the\nadversarial samples. Most of the prior works have been focused on synthesizing\nadversarial samples in the image domain. In this paper, we propose a new method\nof crafting adversarial text samples by modification of the original samples.\nModifications of the original text samples are done by deleting or replacing\nthe important or salient words in the text or by introducing new words in the\ntext sample. Our algorithm works best for the datasets which have\nsub-categories within each of the classes of examples. While crafting\nadversarial samples, one of the key constraint is to generate meaningful\nsentences which can at pass off as legitimate from language (English)\nviewpoint. Experimental results on IMDB movie review dataset for sentiment\nanalysis and Twitter dataset for gender detection show the efficiency of our\nproposed method.", "published": "2017-07-10 11:58:08", "link": "http://arxiv.org/abs/1707.02812v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "A Brief Survey of Text Mining: Classification, Clustering and Extraction\n  Techniques", "abstract": "The amount of text that is generated every day is increasing dramatically.\nThis tremendous volume of mostly unstructured text cannot be simply processed\nand perceived by computers. Therefore, efficient and effective techniques and\nalgorithms are required to discover useful patterns. Text mining is the task of\nextracting meaningful information from text, which has gained significant\nattentions in recent years. In this paper, we describe several of the most\nfundamental text mining tasks and techniques including text pre-processing,\nclassification and clustering. Additionally, we briefly explain text mining in\nbiomedical and health care domains.", "published": "2017-07-10 16:02:44", "link": "http://arxiv.org/abs/1707.02919v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Learning Visual Reasoning Without Strong Priors", "abstract": "Achieving artificial visual reasoning - the ability to answer image-related\nquestions which require a multi-step, high-level process - is an important step\ntowards artificial general intelligence. This multi-modal task requires\nlearning a question-dependent, structured reasoning process over images from\nlanguage. Standard deep learning approaches tend to exploit biases in the data\nrather than learn this underlying structure, while leading methods learn to\nvisually reason successfully but are hand-crafted for reasoning. We show that a\ngeneral-purpose, Conditional Batch Normalization approach achieves\nstate-of-the-art results on the CLEVR Visual Reasoning benchmark with a 2.4%\nerror rate. We outperform the next best end-to-end method (4.5%) and even\nmethods that use extra supervision (3.1%). We probe our model to shed light on\nhow it reasons, showing it has learned a question-dependent, multi-step\nprocess. Previous work has operated under the assumption that visual reasoning\ncalls for a specialized architecture, but we show that a general architecture\nwith proper conditioning can learn to visually reason effectively.", "published": "2017-07-10 18:49:28", "link": "http://arxiv.org/abs/1707.03017v5", "categories": ["cs.CV", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.CV"}
