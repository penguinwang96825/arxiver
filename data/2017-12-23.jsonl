{"title": "Are words easier to learn from infant- than adult-directed speech? A\n  quantitative corpus-based investigation", "abstract": "We investigate whether infant-directed speech (IDS) could facilitate word\nform learning when compared to adult-directed speech (ADS). To study this, we\nexamine the distribution of word forms at two levels, acoustic and\nphonological, using a large database of spontaneous speech in Japanese. At the\nacoustic level we show that, as has been documented before for phonemes, the\nrealizations of words are more variable and less discriminable in IDS than in\nADS. At the phonological level, we find an effect in the opposite direction:\nthe IDS lexicon contains more distinctive words (such as onomatopoeias) than\nthe ADS counterpart. Combining the acoustic and phonological metrics together\nin a global discriminability score reveals that the bigger separation of\nlexical categories in the phonological space does not compensate for the\nopposite effect observed at the acoustic level. As a result, IDS word forms are\nstill globally less discriminable than ADS word forms, even though the effect\nis numerically small. We discuss the implication of these findings for the view\nthat the functional role of IDS is to improve language learnability.", "published": "2017-12-23 15:55:54", "link": "http://arxiv.org/abs/1712.08793v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Framework for Enriching Lexical Semantic Resources with Distributional\n  Semantics", "abstract": "We present an approach to combining distributional semantic representations\ninduced from text corpora with manually constructed lexical-semantic networks.\nWhile both kinds of semantic resources are available with high lexical\ncoverage, our aligned resource combines the domain specificity and availability\nof contextual information from distributional models with the conciseness and\nhigh quality of manually crafted lexical networks. We start with a\ndistributional representation of induced senses of vocabulary terms, which are\naccompanied with rich context information given by related lexical items. We\nthen automatically disambiguate such representations to obtain a full-fledged\nproto-conceptualization, i.e. a typed graph of induced word senses. In a final\nstep, this proto-conceptualization is aligned to a lexical ontology, resulting\nin a hybrid aligned resource. Moreover, unmapped induced senses are associated\nwith a semantic type in order to connect them to the core resource. Manual\nevaluations against ground-truth judgments for different stages of our method\nas well as an extrinsic evaluation on a knowledge-based Word Sense\nDisambiguation benchmark all indicate the high quality of the new hybrid\nresource. Additionally, we show the benefits of enriching top-down lexical\nknowledge resources with bottom-up distributional information from text for\naddressing high-end knowledge acquisition tasks such as cleaning hypernym\ngraphs and learning taxonomies from scratch.", "published": "2017-12-23 18:46:58", "link": "http://arxiv.org/abs/1712.08819v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dual Long Short-Term Memory Networks for Sub-Character Representation\n  Learning", "abstract": "Characters have commonly been regarded as the minimal processing unit in\nNatural Language Processing (NLP). But many non-latin languages have\nhieroglyphic writing systems, involving a big alphabet with thousands or\nmillions of characters. Each character is composed of even smaller parts, which\nare often ignored by the previous work. In this paper, we propose a novel\narchitecture employing two stacked Long Short-Term Memory Networks (LSTMs) to\nlearn sub-character level representation and capture deeper level of semantic\nmeanings. To build a concrete study and substantiate the efficiency of our\nneural architecture, we take Chinese Word Segmentation as a research case\nexample. Among those languages, Chinese is a typical case, for which every\ncharacter contains several components called radicals. Our networks employ a\nshared radical level embedding to solve both Simplified and Traditional Chinese\nWord Segmentation, without extra Traditional to Simplified Chinese conversion,\nin such a highly end-to-end way the word segmentation can be significantly\nsimplified compared to the previous work. Radical level embeddings can also\ncapture deeper semantic meaning below character level and improve the system\nperformance of learning. By tying radical and character embeddings together,\nthe parameter count is reduced whereas semantic knowledge is shared and\ntransferred between two levels, boosting the performance largely. On 3 out of 4\nBakeoff 2005 datasets, our method surpassed state-of-the-art results by up to\n0.4%. Our results are reproducible, source codes and corpora are available on\nGitHub.", "published": "2017-12-23 21:12:23", "link": "http://arxiv.org/abs/1712.08841v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Interpretable Counting for Visual Question Answering", "abstract": "Questions that require counting a variety of objects in images remain a major\nchallenge in visual question answering (VQA). The most common approaches to VQA\ninvolve either classifying answers based on fixed length representations of\nboth the image and question or summing fractional counts estimated from each\nsection of the image. In contrast, we treat counting as a sequential decision\nprocess and force our model to make discrete choices of what to count.\nSpecifically, the model sequentially selects from detected objects and learns\ninteractions between objects that influence subsequent selections. A\ndistinction of our approach is its intuitive and interpretable output, as\ndiscrete counts are automatically grounded in the image. Furthermore, our\nmethod outperforms the state of the art architecture for VQA on multiple\nmetrics that evaluate counting.", "published": "2017-12-23 01:44:45", "link": "http://arxiv.org/abs/1712.08697v2", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "Variational Autoencoders for Learning Latent Representations of Speech\n  Emotion: A Preliminary Study", "abstract": "Learning the latent representation of data in unsupervised fashion is a very\ninteresting process that provides relevant features for enhancing the\nperformance of a classifier. For speech emotion recognition tasks, generating\neffective features is crucial. Currently, handcrafted features are mostly used\nfor speech emotion recognition, however, features learned automatically using\ndeep learning have shown strong success in many problems, especially in image\nprocessing. In particular, deep generative models such as Variational\nAutoencoders (VAEs) have gained enormous success for generating features for\nnatural images. Inspired by this, we propose VAEs for deriving the latent\nrepresentation of speech signals and use this representation to classify\nemotions. To the best of our knowledge, we are the first to propose VAEs for\nspeech emotion classification. Evaluations on the IEMOCAP dataset demonstrate\nthat features learned by VAEs can produce state-of-the-art results for speech\nemotion classification.", "published": "2017-12-23 03:54:00", "link": "http://arxiv.org/abs/1712.08708v3", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
