{"title": "Probing of Quantitative Values in Abstractive Summarization Models", "abstract": "Abstractive text summarization has recently become a popular approach, but\ndata hallucination remains a serious problem, including with quantitative data.\nWe propose a set of probing tests to evaluate the efficacy of abstract\nsummarization models' modeling of quantitative values found in the input text.\nOur results show that in most cases, the encoders of recent SOTA-performing\nmodels struggle to provide embeddings that adequately represent quantitative\nvalues in the input compared to baselines, and in particular, they outperform\nrandom representations in some, but surprisingly not all, cases. Under our\nassumptions, this suggests that the encoder's performance contributes to the\nquantity hallucination problem. One model type in particular, DistilBART-CDM,\nwas observed to underperform randomly initialized representations for several\nexperiments, and performance versus BERT suggests that standard pretraining and\nfine-tuning approaches for the summarization task may play a role in\nunderperformance for some encoders.", "published": "2022-10-03 00:59:50", "link": "http://arxiv.org/abs/2210.00667v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lexical semantics enhanced neural word embeddings", "abstract": "Current breakthroughs in natural language processing have benefited\ndramatically from neural language models, through which distributional\nsemantics can leverage neural data representations to facilitate downstream\napplications. Since neural embeddings use context prediction on word\nco-occurrences to yield dense vectors, they are inevitably prone to capture\nmore semantic association than semantic similarity. To improve vector space\nmodels in deriving semantic similarity, we post-process neural word embeddings\nthrough deep metric learning, through which we can inject lexical-semantic\nrelations, including syn/antonymy and hypo/hypernymy, into a distributional\nspace. We introduce hierarchy-fitting, a novel semantic specialization approach\nto modelling semantic similarity nuances inherently stored in the IS-A\nhierarchies. Hierarchy-fitting attains state-of-the-art results on the common-\nand rare-word benchmark datasets for deriving semantic similarity from neural\nword embeddings. It also incorporates an asymmetric distance function to\nspecialize hypernymy's directionality explicitly, through which it\nsignificantly improves vanilla embeddings in multiple evaluation tasks of\ndetecting hypernymy and directionality without negative impacts on semantic\nsimilarity judgement. The results demonstrate the efficacy of hierarchy-fitting\nin specializing neural embeddings with semantic relations in late fusion,\npotentially expanding its applicability to aggregating heterogeneous data and\nvarious knowledge resources for learning multimodal semantic spaces.", "published": "2022-10-03 08:10:23", "link": "http://arxiv.org/abs/2210.00754v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Effectiveness of Masked Language Modeling and Adapters for Factual\n  Knowledge Injection", "abstract": "This paper studies the problem of injecting factual knowledge into large\npre-trained language models. We train adapter modules on parts of the\nConceptNet knowledge graph using the masked language modeling objective and\nevaluate the success of the method by a series of probing experiments on the\nLAMA probe. Mean P@K curves for different configurations indicate that the\ntechnique is effective, increasing the performance on subsets of the LAMA probe\nfor large values of k by adding as little as 2.1% additional parameters to the\noriginal models.", "published": "2022-10-03 13:08:09", "link": "http://arxiv.org/abs/2210.00907v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hypothesis Engineering for Zero-Shot Hate Speech Detection", "abstract": "Standard approaches to hate speech detection rely on sufficient available\nhate speech annotations. Extending previous work that repurposes natural\nlanguage inference (NLI) models for zero-shot text classification, we propose a\nsimple approach that combines multiple hypotheses to improve English NLI-based\nzero-shot hate speech detection. We first conduct an error analysis for vanilla\nNLI-based zero-shot hate speech detection and then develop four strategies\nbased on this analysis. The strategies use multiple hypotheses to predict\nvarious aspects of an input text and combine these predictions into a final\nverdict. We find that the zero-shot baseline used for the initial error\nanalysis already outperforms commercial systems and fine-tuned BERT-based hate\nspeech detection models on HateCheck. The combination of the proposed\nstrategies further increases the zero-shot accuracy of 79.4% on HateCheck by\n7.9 percentage points (pp), and the accuracy of 69.6% on ETHOS by 10.0pp.", "published": "2022-10-03 13:11:42", "link": "http://arxiv.org/abs/2210.00910v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The (In)Effectiveness of Intermediate Task Training For Domain\n  Adaptation and Cross-Lingual Transfer Learning", "abstract": "Transfer learning from large language models (LLMs) has emerged as a powerful\ntechnique to enable knowledge-based fine-tuning for a number of tasks,\nadaptation of models for different domains and even languages. However, it\nremains an open question, if and when transfer learning will work, i.e. leading\nto positive or negative transfer. In this paper, we analyze the knowledge\ntransfer across three natural language processing (NLP) tasks - text\nclassification, sentimental analysis, and sentence similarity, using three LLMs\n- BERT, RoBERTa, and XLNet - and analyzing their performance, by fine-tuning on\ntarget datasets for domain and cross-lingual adaptation tasks, with and without\nan intermediate task training on a larger dataset. Our experiments showed that\nfine-tuning without an intermediate task training can lead to a better\nperformance for most tasks, while more generalized tasks might necessitate a\npreceding intermediate task training step. We hope that this work will act as a\nguide on transfer learning to NLP practitioners.", "published": "2022-10-03 17:17:07", "link": "http://arxiv.org/abs/2210.01091v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ContraCLM: Contrastive Learning For Causal Language Model", "abstract": "Despite exciting progress in causal language models, the expressiveness of\nthe representations is largely limited due to poor discrimination ability. To\nremedy this issue, we present ContraCLM, a novel contrastive learning framework\nat both token-level and sequence-level. We assess ContraCLM on a variety of\ndownstream tasks. We show that ContraCLM enhances discrimination of the\nrepresentations and bridges the gap with the encoder-only models, which makes\ncausal language models better suited for tasks beyond language generation.\nSpecifically, we attain $44\\%$ relative improvement on the Semantic Textual\nSimilarity tasks and $34\\%$ on Code-to-Code Search tasks. Furthermore, by\nimproving the expressiveness of the representations, ContraCLM also boosts the\nsource code generation capability with $9\\%$ relative improvement on execution\naccuracy on the HumanEval benchmark.", "published": "2022-10-03 18:56:35", "link": "http://arxiv.org/abs/2210.01185v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of\n  Chain-of-Thought", "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities\ngiven chain-of-thought prompts (examples with intermediate reasoning steps).\nExisting benchmarks measure reasoning ability indirectly, by evaluating\naccuracy on downstream tasks such as mathematical reasoning. However, it is\nunclear how these models obtain the answers and whether they rely on simple\nheuristics rather than the generated chain-of-thought. To enable systematic\nexploration of the reasoning ability of LLMs, we present a new synthetic\nquestion-answering dataset called PrOntoQA, where each example is generated\nfrom a synthetic world model represented in first-order logic. This allows us\nto parse the generated chain-of-thought into symbolic proofs for formal\nanalysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite\ncapable of making correct individual deduction steps, and so are generally\ncapable of reasoning, even in fictional contexts. However, they have difficulty\nwith proof planning: When multiple valid deduction steps are available, they\nare not able to systematically explore the different options.", "published": "2022-10-03 21:34:32", "link": "http://arxiv.org/abs/2210.01240v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding Substructures in Commonsense Relations in ConceptNet", "abstract": "Acquiring commonsense knowledge and reasoning is an important goal in modern\nNLP research. Despite much progress, there is still a lack of understanding\n(especially at scale) of the nature of commonsense knowledge itself. A\npotential source of structured commonsense knowledge that could be used to\nderive insights is ConceptNet. In particular, ConceptNet contains several\ncoarse-grained relations, including HasContext, FormOf and SymbolOf, which can\nprove invaluable in understanding broad, but critically important, commonsense\nnotions such as 'context'. In this article, we present a methodology based on\nunsupervised knowledge graph representation learning and clustering to reveal\nand study substructures in three heavily used commonsense relations in\nConceptNet. Our results show that, despite having an 'official' definition in\nConceptNet, many of these commonsense relations exhibit considerable\nsub-structure. In the future, therefore, such relations could be sub-divided\ninto other relations with more refined definitions. We also supplement our core\nstudy with visualizations and qualitative analyses.", "published": "2022-10-03 22:59:07", "link": "http://arxiv.org/abs/2210.01263v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Embarrassingly Simple Approach for Intellectual Property Rights\n  Protection on Recurrent Neural Networks", "abstract": "Capitalise on deep learning models, offering Natural Language Processing\n(NLP) solutions as a part of the Machine Learning as a Service (MLaaS) has\ngenerated handsome revenues. At the same time, it is known that the creation of\nthese lucrative deep models is non-trivial. Therefore, protecting these\ninventions intellectual property rights (IPR) from being abused, stolen and\nplagiarized is vital. This paper proposes a practical approach for the IPR\nprotection on recurrent neural networks (RNN) without all the bells and\nwhistles of existing IPR solutions. Particularly, we introduce the Gatekeeper\nconcept that resembles the recurrent nature in RNN architecture to embed keys.\nAlso, we design the model training scheme in a way such that the protected RNN\nmodel will retain its original performance iff a genuine key is presented.\nExtensive experiments showed that our protection scheme is robust and effective\nagainst ambiguity and removal attacks in both white-box and black-box\nprotection schemes on different RNN variants. Code is available at\nhttps://github.com/zhiqin1998/RecurrentIPR", "published": "2022-10-03 07:25:59", "link": "http://arxiv.org/abs/2210.00743v2", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Unsupervised Search Algorithm Configuration using Query Performance\n  Prediction", "abstract": "Search engine configuration can be quite difficult for inexpert developers.\nInstead, an auto-configuration approach can be used to speed up development\ntime. Yet, such an automatic process usually requires relevance labels to train\na supervised model. In this work, we suggest a simple solution based on query\nperformance prediction that requires no relevance labels but only a sample of\nqueries in a given domain. Using two example usecases we demonstrate the merits\nof our solution.", "published": "2022-10-03 08:55:24", "link": "http://arxiv.org/abs/2210.00767v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Is Reinforcement Learning (Not) for Natural Language Processing:\n  Benchmarks, Baselines, and Building Blocks for Natural Language Policy\n  Optimization", "abstract": "We tackle the problem of aligning pre-trained large language models (LMs)\nwith human preferences. If we view text generation as a sequential\ndecision-making problem, reinforcement learning (RL) appears to be a natural\nconceptual framework. However, using RL for LM-based generation faces empirical\nchallenges, including training instability due to the combinatorial action\nspace, as well as a lack of open-source libraries and benchmarks customized for\nLM alignment. Thus, a question rises in the research community: is RL a\npractical paradigm for NLP?\n  To help answer this, we first introduce an open-source modular library,\nRL4LMs (Reinforcement Learning for Language Models), for optimizing language\ngenerators with RL. The library consists of on-policy RL algorithms that can be\nused to train any encoder or encoder-decoder LM in the HuggingFace library\n(Wolf et al. 2020) with an arbitrary reward function. Next, we present the GRUE\n(General Reinforced-language Understanding Evaluation) benchmark, a set of 6\nlanguage generation tasks which are supervised not by target strings, but by\nreward functions which capture automated measures of human preference. GRUE is\nthe first leaderboard-style evaluation of RL algorithms for NLP tasks. Finally,\nwe introduce an easy-to-use, performant RL algorithm, NLPO (Natural Language\nPolicy Optimization) that learns to effectively reduce the combinatorial action\nspace in language generation. We show 1) that RL techniques are generally\nbetter than supervised methods at aligning LMs to human preferences; and 2)\nthat NLPO exhibits greater stability and performance than previous policy\ngradient methods (e.g., PPO (Schulman et al. 2017)), based on both automatic\nand human evaluations.", "published": "2022-10-03 21:38:29", "link": "http://arxiv.org/abs/2210.01241v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Understanding Prior Bias and Choice Paralysis in Transformer-based\n  Language Representation Models through Four Experimental Probes", "abstract": "Recent work on transformer-based neural networks has led to impressive\nadvances on multiple-choice natural language understanding (NLU) problems, such\nas Question Answering (QA) and abductive reasoning. Despite these advances,\nthere is limited work still on understanding whether these models respond to\nperturbed multiple-choice instances in a sufficiently robust manner that would\nallow them to be trusted in real-world situations. We present four confusion\nprobes, inspired by similar phenomena first identified in the behavioral\nscience community, to test for problems such as prior bias and choice\nparalysis. Experimentally, we probe a widely used transformer-based\nmultiple-choice NLU system using four established benchmark datasets. Here we\nshow that the model exhibits significant prior bias and to a lesser, but still\nhighly significant degree, choice paralysis, in addition to other problems. Our\nresults suggest that stronger testing protocols and additional benchmarks may\nbe necessary before the language models are used in front-facing systems or\ndecision making with real world consequences.", "published": "2022-10-03 22:36:44", "link": "http://arxiv.org/abs/2210.01258v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Non-monotonic Self-terminating Language Model", "abstract": "Recent large-scale neural autoregressive sequence models have shown\nimpressive performances on a variety of natural language generation tasks.\nHowever, their generated sequences often exhibit degenerate properties such as\nnon-termination, undesirable repetition, and premature termination, when\ngenerated with decoding algorithms such as greedy search, beam search, top-$k$\nsampling, and nucleus sampling. In this paper, we focus on the problem of\nnon-terminating sequences resulting from an incomplete decoding algorithm. We\nfirst define an incomplete probable decoding algorithm which includes greedy\nsearch, top-$k$ sampling, and nucleus sampling, beyond the incomplete decoding\nalgorithm originally put forward by Welleck et al. (2020). We then propose a\nnon-monotonic self-terminating language model, which significantly relaxes the\nconstraint of monotonically increasing termination probability in the\noriginally proposed self-terminating language model by Welleck et al. (2020),\nto address the issue of non-terminating sequences when using incomplete\nprobable decoding algorithms. We prove that our proposed model prevents\nnon-terminating sequences when using not only incomplete probable decoding\nalgorithms but also beam search. We empirically validate our model on sequence\ncompletion tasks with various architectures.", "published": "2022-10-03 00:28:44", "link": "http://arxiv.org/abs/2210.00660v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Some pointwise and decidable properties of non-uniform cellular automata", "abstract": "For non-uniform cellular automata (NUCA) with finite memory over an arbitrary\nuniverse with multiple local transition rules, we show that pointwise\nnilpotency, pointwise periodicity, and pointwise eventual periodicity\nproperties are respectively equivalent to nilpotency, periodicity, and eventual\nperiodicity. Moreover, we prove that every linear NUCA which satisfies\npointwise a polynomial equation (which may depend on the configuration) must be\nan eventually periodic linear NUCA. Generalizing results for higher dimensional\ngroup and linear CA, we also establish the decidability results of the above\ndynamical properties as well as the injectivity for arbitrary NUCA with finite\nmemory which are local perturbations of higher dimensional linear and group CA.\nSome generalizations to the case of sparse global perturbations of higher\ndimensional linear and group CA are also obtained.", "published": "2022-10-03 01:43:08", "link": "http://arxiv.org/abs/2210.00676v1", "categories": ["math.DS", "cs.CL", "cs.DC", "nlin.CG", "05C25, 20F69, 37B10, 37B15, 37B51, 68Q80", "F.1.1"], "primary_category": "math.DS"}
{"title": "SpeechCLIP: Integrating Speech with Pre-Trained Vision and Language\n  Model", "abstract": "Data-driven speech processing models usually perform well with a large amount\nof text supervision, but collecting transcribed speech data is costly.\nTherefore, we propose SpeechCLIP, a novel framework bridging speech and text\nthrough images to enhance speech models without transcriptions. We leverage\nstate-of-the-art pre-trained HuBERT and CLIP, aligning them via paired images\nand spoken captions with minimal fine-tuning. SpeechCLIP outperforms prior\nstate-of-the-art on image-speech retrieval and performs zero-shot speech-text\nretrieval without direct supervision from transcriptions. Moreover, SpeechCLIP\ncan directly retrieve semantically related keywords from speech.", "published": "2022-10-03 04:15:36", "link": "http://arxiv.org/abs/2210.00705v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Complexity-Based Prompting for Multi-Step Reasoning", "abstract": "We study the task of prompting large-scale language models to perform\nmulti-step reasoning. Existing work shows that when prompted with a chain of\nthoughts (CoT), sequences of short sentences describing intermediate reasoning\nsteps towards a final answer, large language models can generate new reasoning\nchains and predict answers for new inputs. A central question is which\nreasoning examples make the most effective prompts. In this work, we propose\ncomplexity-based prompting, a simple and effective example selection scheme for\nmulti-step reasoning. We show that prompts with higher reasoning complexity,\ni.e., chains with more reasoning steps, achieve substantially better\nperformance on multi-step reasoning tasks over strong baselines. We further\nextend our complexity-based criteria from prompting (selecting inputs) to\ndecoding (selecting outputs), where we sample multiple reasoning chains from\nthe model, then choose the majority of generated answers from complex reasoning\nchains (over simple chains). When used to prompt GPT-3 and Codex, our approach\nsubstantially improves multi-step reasoning accuracy and achieves new\nstate-of-the-art (SOTA) performance on three math benchmarks (GSM8K,\nMultiArith, and MathQA) and two BigBenchHard tasks (Date Understanding and\nPenguins), with an average +5.3 and up to +18 accuracy improvements. Compared\nwith existing example selection schemes like manual tuning or retrieval-based\nselection, selection based on reasoning complexity is intuitive, easy to\nimplement, and annotation-efficient. Further results demonstrate the robustness\nof performance gains from complex prompts under format perturbation and\ndistribution shift.", "published": "2022-10-03 05:33:27", "link": "http://arxiv.org/abs/2210.00720v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "How Relevant is Selective Memory Population in Lifelong Language\n  Learning?", "abstract": "Lifelong language learning seeks to have models continuously learn multiple\ntasks in a sequential order without suffering from catastrophic forgetting.\nState-of-the-art approaches rely on sparse experience replay as the primary\napproach to prevent forgetting. Experience replay usually adopts sampling\nmethods for the memory population; however, the effect of the chosen sampling\nstrategy on model performance has not yet been studied. In this paper, we\ninvestigate how relevant the selective memory population is in the lifelong\nlearning process of text classification and question-answering tasks. We found\nthat methods that randomly store a uniform number of samples from the entire\ndata stream lead to high performances, especially for low memory size, which is\nconsistent with computer vision studies.", "published": "2022-10-03 13:52:54", "link": "http://arxiv.org/abs/2210.00940v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SemEval 2023 Task 9: Multilingual Tweet Intimacy Analysis", "abstract": "We propose MINT, a new Multilingual INTimacy analysis dataset covering 13,372\ntweets in 10 languages including English, French, Spanish, Italian, Portuguese,\nKorean, Dutch, Chinese, Hindi, and Arabic. We benchmarked a list of popular\nmultilingual pre-trained language models. The dataset is released along with\nthe SemEval 2023 Task 9: Multilingual Tweet Intimacy Analysis\n(https://sites.google.com/umich.edu/semeval-2023-tweet-intimacy).", "published": "2022-10-03 17:48:32", "link": "http://arxiv.org/abs/2210.01108v2", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Extending Compositional Attention Networks for Social Reasoning in\n  Videos", "abstract": "We propose a novel deep architecture for the task of reasoning about social\ninteractions in videos. We leverage the multi-step reasoning capabilities of\nCompositional Attention Networks (MAC), and propose a multimodal extension\n(MAC-X). MAC-X is based on a recurrent cell that performs iterative mid-level\nfusion of input modalities (visual, auditory, text) over multiple reasoning\nsteps, by use of a temporal attention mechanism. We then combine MAC-X with\nLSTMs for temporal input processing in an end-to-end architecture. Our ablation\nstudies show that the proposed MAC-X architecture can effectively leverage\nmultimodal input cues using mid-level fusion mechanisms. We apply MAC-X to the\ntask of Social Video Question Answering in the Social IQ dataset and obtain a\n2.5% absolute improvement in terms of binary accuracy over the current\nstate-of-the-art.", "published": "2022-10-03 19:03:01", "link": "http://arxiv.org/abs/2210.01191v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "PLOT: Prompt Learning with Optimal Transport for Vision-Language Models", "abstract": "With the increasing attention to large vision-language models such as CLIP,\nthere has been a significant amount of effort dedicated to building efficient\nprompts. Unlike conventional methods of only learning one single prompt, we\npropose to learn multiple comprehensive prompts to describe diverse\ncharacteristics of categories such as intrinsic attributes or extrinsic\ncontexts. However, directly matching each prompt to the same visual feature is\nproblematic, as it pushes the prompts to converge to one point. To solve this\nproblem, we propose to apply optimal transport to match the vision and text\nmodalities. Specifically, we first model images and the categories with visual\nand textual feature sets. Then, we apply a two-stage optimization strategy to\nlearn the prompts. In the inner loop, we optimize the optimal transport\ndistance to align visual features and prompts by the Sinkhorn algorithm, while\nin the outer loop, we learn the prompts by this distance from the supervised\ndata. Extensive experiments are conducted on the few-shot recognition task and\nthe improvement demonstrates the superiority of our method. The code is\navailable at https://github.com/CHENGY12/PLOT.", "published": "2022-10-03 22:21:07", "link": "http://arxiv.org/abs/2210.01253v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Russian Web Tables: A Public Corpus of Web Tables for Russian Language\n  Based on Wikipedia", "abstract": "Corpora that contain tabular data such as WebTables are a vital resource for\nthe academic community. Essentially, they are the backbone of any modern\nresearch in information management. They are used for various tasks of data\nextraction, knowledge base construction, question answering, column semantic\ntype detection and many other. Such corpora are useful not only as a source of\ndata, but also as a base for building test datasets. So far, there were no such\ncorpora for the Russian language and this seriously hindered research in the\naforementioned areas.\n  In this paper, we present the first corpus of Web tables created specifically\nout of Russian language material. It was built via a special toolkit we have\ndeveloped to crawl the Russian Wikipedia. Both the corpus and the toolkit are\nopen-source and publicly available. Finally, we present a short study that\ndescribes Russian Wikipedia tables and their statistics.", "published": "2022-10-03 16:15:48", "link": "http://arxiv.org/abs/2210.06353v1", "categories": ["cs.CL", "cs.DL", "cs.IR", "cs.LG", "H.3.0"], "primary_category": "cs.CL"}
{"title": "Text-to-Audio Grounding Based Novel Metric for Evaluating Audio Caption\n  Similarity", "abstract": "Automatic Audio Captioning (AAC) refers to the task of translating an audio\nsample into a natural language (NL) text that describes the audio events,\nsource of the events and their relationships. Unlike NL text generation tasks,\nwhich rely on metrics like BLEU, ROUGE, METEOR based on lexical semantics for\nevaluation, the AAC evaluation metric requires an ability to map NL text\n(phrases) that correspond to similar sounds in addition lexical semantics.\nCurrent metrics used for evaluation of AAC tasks lack an understanding of the\nperceived properties of sound represented by text. In this paper, wepropose a\nnovel metric based on Text-to-Audio Grounding (TAG), which is, useful for\nevaluating cross modal tasks like AAC. Experiments on publicly available AAC\ndata-set shows our evaluation metric to perform better compared to existing\nmetrics used in NL text and image captioning literature.", "published": "2022-10-03 11:58:59", "link": "http://arxiv.org/abs/2210.06354v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Contrastive Multimodal Learning for Emergence of Graphical Sensory-Motor\n  Communication", "abstract": "In this paper, we investigate whether artificial agents can develop a shared\nlanguage in an ecological setting where communication relies on a sensory-motor\nchannel. To this end, we introduce the Graphical Referential Game (GREG) where\na speaker must produce a graphical utterance to name a visual referent object\nwhile a listener has to select the corresponding object among distractor\nreferents, given the delivered message. The utterances are drawing images\nproduced using dynamical motor primitives combined with a sketching library. To\ntackle GREG we present CURVES: a multimodal contrastive deep learning mechanism\nthat represents the energy (alignment) between named referents and utterances\ngenerated through gradient ascent on the learned energy landscape. We\ndemonstrate that CURVES not only succeeds at solving the GREG but also enables\nagents to self-organize a language that generalizes to feature compositions\nnever seen during training. In addition to evaluating the communication\nperformance of our approach, we also explore the structure of the emerging\nlanguage. Specifically, we show that the resulting language forms a coherent\nlexicon shared between agents and that basic compositional rules on the\ngraphical productions could not explain the compositional generalization.", "published": "2022-10-03 17:11:18", "link": "http://arxiv.org/abs/2210.06468v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Learnable Acoustic Frontends in Bird Activity Detection", "abstract": "Autonomous recording units and passive acoustic monitoring present minimally\nintrusive methods of collecting bioacoustics data. Combining this data with\nspecies agnostic bird activity detection systems enables the monitoring of\nactivity levels of bird populations. Unfortunately, variability in ambient\nnoise levels and subject distance contribute to difficulties in accurately\ndetecting bird activity in recordings. The choice of acoustic frontend directly\naffects the impact these issues have on system performance. In this paper, we\nbenchmark traditional fixed-parameter acoustic frontends against the new\ngeneration of learnable frontends on a wide-ranging bird audio detection task\nusing data from the DCASE2018 BAD Challenge. We observe that Per-Channel Energy\nNormalization is the best overall performer, achieving an accuracy of 89.9%,\nand that in general learnable frontends significantly outperform traditional\nmethods. We also identify challenges in learning filterbanks for bird audio.", "published": "2022-10-03 12:54:26", "link": "http://arxiv.org/abs/2210.00889v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "An attention-based backend allowing efficient fine-tuning of transformer\n  models for speaker verification", "abstract": "In recent years, self-supervised learning paradigm has received extensive\nattention due to its great success in various down-stream tasks. However, the\nfine-tuning strategies for adapting those pre-trained models to speaker\nverification task have yet to be fully explored. In this paper, we analyze\nseveral feature extraction approaches built on top of a pre-trained model, as\nwell as regularization and learning rate schedule to stabilize the fine-tuning\nprocess and further boost performance: multi-head factorized attentive pooling\nis proposed to factorize the comparison of speaker representations into\nmultiple phonetic clusters. We regularize towards the parameters of the\npre-trained model and we set different learning rates for each layer of the\npre-trained model during fine-tuning. The experimental results show our method\ncan significantly shorten the training time to 4 hours and achieve SOTA\nperformance: 0.59%, 0.79% and 1.77% EER on Vox1-O, Vox1-E and Vox1-H,\nrespectively.", "published": "2022-10-03 23:46:11", "link": "http://arxiv.org/abs/2210.01273v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Efficient acoustic feature transformation in mismatched environments\n  using a Guided-GAN", "abstract": "We propose a new framework to improve automatic speech recognition (ASR)\nsystems in resource-scarce environments using a generative adversarial network\n(GAN) operating on acoustic input features. The GAN is used to enhance the\nfeatures of mismatched data prior to decoding, or can optionally be used to\nfine-tune the acoustic model. We achieve improvements that are comparable to\nmulti-style training (MTR), but at a lower computational cost. With less than\none hour of data, an ASR system trained on good quality data, and evaluated on\nmismatched audio is improved by between 11.5% and 19.7% relative word error\nrate (WER). Experiments demonstrate that the framework can be very useful in\nunder-resourced environments where training data and computational resources\nare limited. The GAN does not require parallel training data, because it\nutilises a baseline acoustic model to provide an additional loss term that\nguides the generator to create acoustic features that are better classified by\nthe baseline.", "published": "2022-10-03 05:33:28", "link": "http://arxiv.org/abs/2210.00721v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Push-Pull: Characterizing the Adversarial Robustness for Audio-Visual\n  Active Speaker Detection", "abstract": "Audio-visual active speaker detection (AVASD) is well-developed, and now is\nan indispensable front-end for several multi-modal applications. However, to\nthe best of our knowledge, the adversarial robustness of AVASD models hasn't\nbeen investigated, not to mention the effective defense against such attacks.\nIn this paper, we are the first to reveal the vulnerability of AVASD models\nunder audio-only, visual-only, and audio-visual adversarial attacks through\nextensive experiments. What's more, we also propose a novel audio-visual\ninteraction loss (AVIL) for making attackers difficult to find feasible\nadversarial examples under an allocated attack budget. The loss aims at pushing\nthe inter-class embeddings to be dispersed, namely non-speech and speech\nclusters, sufficiently disentangled, and pulling the intra-class embeddings as\nclose as possible to keep them compact. Experimental results show the AVIL\noutperforms the adversarial training by 33.14 mAP (%) under multi-modal\nattacks.", "published": "2022-10-03 08:10:12", "link": "http://arxiv.org/abs/2210.00753v1", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Simple Pooling Front-ends For Efficient Audio Classification", "abstract": "Recently, there has been increasing interest in building efficient audio\nneural networks for on-device scenarios. Most existing approaches are designed\nto reduce the size of audio neural networks using methods such as model\npruning. In this work, we show that instead of reducing model size using\ncomplex methods, eliminating the temporal redundancy in the input audio\nfeatures (e.g., mel-spectrogram) could be an effective approach for efficient\naudio classification. To do so, we proposed a family of simple pooling\nfront-ends (SimPFs) which use simple non-parametric pooling operations to\nreduce the redundant information within the mel-spectrogram. We perform\nextensive experiments on four audio classification tasks to evaluate the\nperformance of SimPFs. Experimental results show that SimPFs can achieve a\nreduction in more than half of the number of floating point operations (FLOPs)\nfor off-the-shelf audio neural networks, with negligible degradation or even\nsome improvements in audio classification performance.", "published": "2022-10-03 14:00:41", "link": "http://arxiv.org/abs/2210.00943v5", "categories": ["eess.AS", "cs.AI", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "WaveFit: An Iterative and Non-autoregressive Neural Vocoder based on\n  Fixed-Point Iteration", "abstract": "Denoising diffusion probabilistic models (DDPMs) and generative adversarial\nnetworks (GANs) are popular generative models for neural vocoders. The DDPMs\nand GANs can be characterized by the iterative denoising framework and\nadversarial training, respectively. This study proposes a fast and high-quality\nneural vocoder called \\textit{WaveFit}, which integrates the essence of GANs\ninto a DDPM-like iterative framework based on fixed-point iteration. WaveFit\niteratively denoises an input signal, and trains a deep neural network (DNN)\nfor minimizing an adversarial loss calculated from intermediate outputs at all\niterations. Subjective (side-by-side) listening tests showed no statistically\nsignificant differences in naturalness between human natural speech and those\nsynthesized by WaveFit with five iterations. Furthermore, the inference speed\nof WaveFit was more than 240 times faster than WaveRNN. Audio demos are\navailable at \\url{google.github.io/df-conformer/wavefit/}.", "published": "2022-10-03 15:45:05", "link": "http://arxiv.org/abs/2210.01029v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "That Sounds Right: Auditory Self-Supervision for Dynamic Robot\n  Manipulation", "abstract": "Learning to produce contact-rich, dynamic behaviors from raw sensory data has\nbeen a longstanding challenge in robotics. Prominent approaches primarily focus\non using visual or tactile sensing, where unfortunately one fails to capture\nhigh-frequency interaction, while the other can be too delicate for large-scale\ndata collection. In this work, we propose a data-centric approach to dynamic\nmanipulation that uses an often ignored source of information: sound. We first\ncollect a dataset of 25k interaction-sound pairs across five dynamic tasks\nusing commodity contact microphones. Then, given this data, we leverage\nself-supervised learning to accelerate behavior prediction from sound. Our\nexperiments indicate that this self-supervised 'pretraining' is crucial to\nachieving high performance, with a 34.5% lower MSE than plain supervised\nlearning and a 54.3% lower MSE over visual training. Importantly, we find that\nwhen asked to generate desired sound profiles, online rollouts of our models on\na UR10 robot can produce dynamic behavior that achieves an average of 11.5%\nimprovement over supervised learning on audio similarity metrics.", "published": "2022-10-03 17:57:09", "link": "http://arxiv.org/abs/2210.01116v1", "categories": ["cs.RO", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.RO"}
{"title": "Diagnosis of Parkinson's Disease Based on Voice Signals Using SHAP and\n  Hard Voting Ensemble Method", "abstract": "Background and Objective: Parkinson's disease (PD) is the second most common\nprogressive neurological condition after Alzheimer's, characterized by motor\nand non-motor symptoms. Developing a method to diagnose the condition in its\nbeginning phases is essential because of the significant number of individuals\nafflicting with this illness. PD is typically identified using motor symptoms\nor other Neuroimaging techniques, such as DATSCAN and SPECT. These methods are\nexpensive, time-consuming, and unavailable to the general public; furthermore,\nthey are not very accurate. These constraints encouraged us to develop a novel\ntechnique using SHAP and Hard Voting Ensemble Method based on voice signals.\nMethods: In this article, we used Pearson Correlation Coefficients to\nunderstand the relationship between input features and the output, and finally,\ninput features with high correlation were selected. These selected features\nwere classified by the Extreme Gradient Boosting (XGBoost), Light Gradient\nBoosting Machine (LightGBM), Gradient Boosting, and Bagging. Moreover, the Hard\nVoting Ensemble Method was determined based on the performance of the four\nclassifiers. At the final stage, we proposed Shapley Additive exPlanations\n(SHAP) to rank the features according to their significance in diagnosing\nParkinson's disease. Results and Conclusion: The proposed method achieved\n85.42% accuracy, 84.94% F1-score, 86.77% precision, 87.62% specificity, and\n83.20% sensitivity. The study's findings demonstrated that the proposed method\noutperformed state-of-the-art approaches and can assist physicians in\ndiagnosing Parkinson's cases.", "published": "2022-10-03 19:45:22", "link": "http://arxiv.org/abs/2210.01205v1", "categories": ["cs.LG", "eess.AS", "eess.SP"], "primary_category": "cs.LG"}
{"title": "And what if two musical versions don't share melody, harmony, rhythm, or\n  lyrics ?", "abstract": "Version identification (VI) has seen substantial progress over the past few\nyears. On the one hand, the introduction of the metric learning paradigm has\nfavored the emergence of scalable yet accurate VI systems. On the other hand,\nusing features focusing on specific aspects of musical pieces, such as melody,\nharmony, or lyrics, yielded interpretable and promising performances. In this\nwork, we build upon these recent advances and propose a metric learning-based\nsystem systematically leveraging four dimensions commonly admitted to convey\nmusical similarity between versions: melodic line, harmonic structure, rhythmic\npatterns, and lyrics. We describe our deliberately simple model architecture,\nand we show in particular that an approximated representation of the lyrics is\nan efficient proxy to discriminate between versions and non-versions. We then\ndescribe how these features complement each other and yield new\nstate-of-the-art performances on two publicly available datasets. We finally\nsuggest that a VI system using a combination of melodic, harmonic, rhythmic and\nlyrics features could theoretically reach the optimal performances obtainable\non these datasets.", "published": "2022-10-03 22:33:14", "link": "http://arxiv.org/abs/2210.01256v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
