{"title": "Input-length-shortening and text generation via attention values", "abstract": "Identifying words that impact a task's performance more than others is a\nchallenge in natural language processing. Transformers models have recently\naddressed this issue by incorporating an attention mechanism that assigns\ngreater attention (i.e., relevance) scores to some words than others. Because\nof the attention mechanism's high computational cost, transformer models\nusually have an input-length limitation caused by hardware constraints. This\nlimitation applies to many transformers, including the well-known bidirectional\nencoder representations of the transformer (BERT) model. In this paper, we\nexamined BERT's attention assignment mechanism, focusing on two questions: (1)\nHow can attention be employed to reduce input length? (2) How can attention be\nused as a control mechanism for conditional text generation? We investigated\nthese questions in the context of a text classification task. We discovered\nthat BERT's early layers assign more critical attention scores for text\nclassification tasks compared to later layers. We demonstrated that the first\nlayer's attention sums could be used to filter tokens in a given sequence,\nconsiderably decreasing the input length while maintaining good test accuracy.\nWe also applied filtering, which uses a compute-efficient semantic similarities\nalgorithm, and discovered that retaining approximately 6\\% of the original\nsequence is sufficient to obtain 86.5\\% accuracy. Finally, we showed that we\ncould generate data in a stable manner and indistinguishable from the original\none by only using a small percentage (10\\%) of the tokens with high attention\nscores according to BERT's first layer.", "published": "2023-03-14 02:11:24", "link": "http://arxiv.org/abs/2303.07585v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring ChatGPT's Ability to Rank Content: A Preliminary Study on\n  Consistency with Human Preferences", "abstract": "As a natural language assistant, ChatGPT is capable of performing various\ntasks, including but not limited to article generation, code completion, and\ndata analysis. Furthermore, ChatGPT has consistently demonstrated a remarkable\nlevel of accuracy and reliability in terms of content evaluation, exhibiting\nthe capability of mimicking human preferences. To further explore ChatGPT's\npotential in this regard, a study is conducted to assess its ability to rank\ncontent. In order to do so, a test set consisting of prompts is created,\ncovering a wide range of use cases, and five models are utilized to generate\ncorresponding responses. ChatGPT is then instructed to rank the responses\ngenerated by these models. The results on the test set show that ChatGPT's\nranking preferences are consistent with human to a certain extent. This\npreliminary experimental finding implies that ChatGPT's zero-shot ranking\ncapability could be used to reduce annotation pressure in a number of ranking\ntasks.", "published": "2023-03-14 03:13:02", "link": "http://arxiv.org/abs/2303.07610v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Life Cycle of Knowledge in Big Language Models: A Survey", "abstract": "Knowledge plays a critical role in artificial intelligence. Recently, the\nextensive success of pre-trained language models (PLMs) has raised significant\nattention about how knowledge can be acquired, maintained, updated and used by\nlanguage models. Despite the enormous amount of related studies, there still\nlacks a unified view of how knowledge circulates within language models\nthroughout the learning, tuning, and application processes, which may prevent\nus from further understanding the connections between current progress or\nrealizing existing limitations. In this survey, we revisit PLMs as\nknowledge-based systems by dividing the life circle of knowledge in PLMs into\nfive critical periods, and investigating how knowledge circulates when it is\nbuilt, maintained and used. To this end, we systematically review existing\nstudies of each period of the knowledge life cycle, summarize the main\nchallenges and current limitations, and discuss future directions.", "published": "2023-03-14 03:49:22", "link": "http://arxiv.org/abs/2303.07616v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RenewNAT: Renewing Potential Translation for Non-Autoregressive\n  Transformer", "abstract": "Non-autoregressive neural machine translation (NAT) models are proposed to\naccelerate the inference process while maintaining relatively high performance.\nHowever, existing NAT models are difficult to achieve the desired\nefficiency-quality trade-off. For one thing, fully NAT models with efficient\ninference perform inferior to their autoregressive counterparts. For another,\niterative NAT models can, though, achieve comparable performance while\ndiminishing the advantage of speed. In this paper, we propose RenewNAT, a\nflexible framework with high efficiency and effectiveness, to incorporate the\nmerits of fully and iterative NAT models. RenewNAT first generates the\npotential translation results and then renews them in a single pass. It can\nachieve significant performance improvements at the same expense as traditional\nNAT models (without introducing additional model parameters and decoding\nlatency). Experimental results on various translation benchmarks (e.g.,\n\\textbf{4} WMT) show that our framework consistently improves the performance\nof strong fully NAT methods (e.g., GLAT and DSLP) without additional speed\noverhead.", "published": "2023-03-14 07:10:03", "link": "http://arxiv.org/abs/2303.07665v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "X-ReCoSa: Multi-Scale Context Aggregation For Multi-Turn Dialogue\n  Generation", "abstract": "In multi-turn dialogue generation, responses are not only related to the\ntopic and background of the context but also related to words and phrases in\nthe sentences of the context. However, currently widely used hierarchical\ndialog models solely rely on context representations from the utterance-level\nencoder, ignoring the sentence representations output by the word-level\nencoder. This inevitably results in a loss of information while decoding and\ngenerating. In this paper, we propose a new dialog model X-ReCoSa to tackle\nthis problem which aggregates multi-scale context information for hierarchical\ndialog models. Specifically, we divide the generation decoder into upper and\nlower parts, namely the intention part and the generation part. Firstly, the\nintention part takes context representations as input to generate the intention\nof the response. Then the generation part generates words depending on sentence\nrepresentations. Therefore, the hierarchical information has been fused into\nresponse generation. we conduct experiments on the English dataset DailyDialog.\nExperimental results exhibit that our method outperforms baseline models on\nboth automatic metric-based and human-based evaluations.", "published": "2023-03-14 12:15:52", "link": "http://arxiv.org/abs/2303.07833v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Learnability of In-Context Learning", "abstract": "In-context learning is a surprising and important phenomenon that emerged\nwhen modern language models were scaled to billions of learned parameters.\nWithout modifying a large language model's weights, it can be tuned to perform\nvarious downstream natural language tasks simply by including concatenated\ntraining examples of these tasks in its input. Though disruptive for many\npractical applications of large language models, this emergent learning\nparadigm is not well understood from a theoretical perspective. In this paper,\nwe propose a first-of-its-kind PAC based framework for in-context learnability,\nand use it to provide the first finite sample complexity results for the\nin-context learning setup. Our framework includes an initial pretraining phase,\nwhich fits a function to the pretraining distribution, and then a second\nin-context learning phase, which keeps this function constant and concatenates\ntraining examples of the downstream task in its input. We use our framework in\norder to prove that, under mild assumptions, when the pretraining distribution\nis a mixture of latent tasks (a model often considered for natural language\npretraining), these tasks can be efficiently learned via in-context learning,\neven though the model's weights are unchanged and the input significantly\ndiverges from the pretraining distribution. Our theoretical analysis reveals\nthat in this setting, in-context learning is more about identifying the task\nthan about learning it, a result which is in line with a series of recent\nempirical findings. We hope that the in-context learnability framework\npresented in this paper will facilitate future progress towards a deeper\nunderstanding of this important new learning paradigm.", "published": "2023-03-14 13:28:39", "link": "http://arxiv.org/abs/2303.07895v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adapting Offline Speech Translation Models for Streaming with\n  Future-Aware Distillation and Inference", "abstract": "A popular approach to streaming speech translation is to employ a single\noffline model with a wait-k policy to support different latency requirements,\nwhich is simpler than training multiple online models with different latency\nconstraints. However, there is a mismatch problem in using a model trained with\ncomplete utterances for streaming inference with partial input. We demonstrate\nthat speech representations extracted at the end of a streaming input are\nsignificantly different from those extracted from a complete utterance. To\naddress this issue, we propose a new approach called Future-Aware Streaming\nTranslation (FAST) that adapts an offline ST model for streaming input. FAST\nincludes a Future-Aware Inference (FAI) strategy that incorporates future\ncontext through a trainable masked embedding, and a Future-Aware Distillation\n(FAD) framework that transfers future context from an approximation of full\nspeech to streaming input. Our experiments on the MuST-C EnDe, EnEs, and EnFr\nbenchmarks show that FAST achieves better trade-offs between translation\nquality and latency than strong baselines. Extensive analyses suggest that our\nmethods effectively alleviate the aforementioned mismatch problem between\noffline training and online inference.", "published": "2023-03-14 13:56:36", "link": "http://arxiv.org/abs/2303.07914v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can ChatGPT Replace Traditional KBQA Models? An In-depth Analysis of the\n  Question Answering Performance of the GPT LLM Family", "abstract": "ChatGPT is a powerful large language model (LLM) that covers knowledge\nresources such as Wikipedia and supports natural language question answering\nusing its own knowledge. Therefore, there is growing interest in exploring\nwhether ChatGPT can replace traditional knowledge-based question answering\n(KBQA) models. Although there have been some works analyzing the question\nanswering performance of ChatGPT, there is still a lack of large-scale,\ncomprehensive testing of various types of complex questions to analyze the\nlimitations of the model. In this paper, we present a framework that follows\nthe black-box testing specifications of CheckList proposed by Ribeiro et. al.\nWe evaluate ChatGPT and its family of LLMs on eight real-world KB-based complex\nquestion answering datasets, which include six English datasets and two\nmultilingual datasets. The total number of test cases is approximately 190,000.\nIn addition to the GPT family of LLMs, we also evaluate the well-known FLAN-T5\nto identify commonalities between the GPT family and other LLMs. The dataset\nand code are available at\nhttps://github.com/tan92hl/Complex-Question-Answering-Evaluation-of-GPT-family.git", "published": "2023-03-14 15:46:28", "link": "http://arxiv.org/abs/2303.07992v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Clinical Concept and Relation Extraction Using Prompt-based Machine\n  Reading Comprehension", "abstract": "Objective: To develop a natural language processing system that solves both\nclinical concept extraction and relation extraction in a unified prompt-based\nmachine reading comprehension (MRC) architecture with good generalizability for\ncross-institution applications.\n  Methods: We formulate both clinical concept extraction and relation\nextraction using a unified prompt-based MRC architecture and explore\nstate-of-the-art transformer models. We compare our MRC models with existing\ndeep learning models for concept extraction and end-to-end relation extraction\nusing two benchmark datasets developed by the 2018 National NLP Clinical\nChallenges (n2c2) challenge (medications and adverse drug events) and the 2022\nn2c2 challenge (relations of social determinants of health [SDoH]). We also\nevaluate the transfer learning ability of the proposed MRC models in a\ncross-institution setting. We perform error analyses and examine how different\nprompting strategies affect the performance of MRC models.\n  Results and Conclusion: The proposed MRC models achieve state-of-the-art\nperformance for clinical concept and relation extraction on the two benchmark\ndatasets, outperforming previous non-MRC transformer models. GatorTron-MRC\nachieves the best strict and lenient F1-scores for concept extraction,\noutperforming previous deep learning models on the two datasets by 1%~3% and\n0.7%~1.3%, respectively. For end-to-end relation extraction, GatorTron-MRC and\nBERT-MIMIC-MRC achieve the best F1-scores, outperforming previous deep learning\nmodels by 0.9%~2.4% and 10%-11%, respectively. For cross-institution\nevaluation, GatorTron-MRC outperforms traditional GatorTron by 6.4% and 16% for\nthe two datasets, respectively. The proposed method is better at handling\nnested/overlapped concepts, extracting relations, and has good portability for\ncross-institute applications.", "published": "2023-03-14 22:37:31", "link": "http://arxiv.org/abs/2303.08262v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Diffusion Models in NLP: A Survey", "abstract": "Diffusion models have become a powerful family of deep generative models,\nwith record-breaking performance in many applications. This paper first gives\nan overview and derivation of the basic theory of diffusion models, then\nreviews the research results of diffusion models in the field of natural\nlanguage processing, from text generation, text-driven image generation and\nother four aspects, and analyzes and summarizes the relevant literature\nmaterials sorted out, and finally records the experience and feelings of this\ntopic literature review research.", "published": "2023-03-14 01:53:49", "link": "http://arxiv.org/abs/2303.07576v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Query2doc: Query Expansion with Large Language Models", "abstract": "This paper introduces a simple yet effective query expansion approach,\ndenoted as query2doc, to improve both sparse and dense retrieval systems. The\nproposed method first generates pseudo-documents by few-shot prompting large\nlanguage models (LLMs), and then expands the query with generated\npseudo-documents. LLMs are trained on web-scale text corpora and are adept at\nknowledge memorization. The pseudo-documents from LLMs often contain highly\nrelevant information that can aid in query disambiguation and guide the\nretrievers. Experimental results demonstrate that query2doc boosts the\nperformance of BM25 by 3% to 15% on ad-hoc IR datasets, such as MS-MARCO and\nTREC DL, without any model fine-tuning. Furthermore, our method also benefits\nstate-of-the-art dense retrievers in terms of both in-domain and out-of-domain\nresults.", "published": "2023-03-14 07:27:30", "link": "http://arxiv.org/abs/2303.07678v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Dual-Attention Model for Aspect-Level Sentiment Classification", "abstract": "I propose a novel dual-attention model(DAM) for aspect-level sentiment\nclassification. Many methods have been proposed, such as support vector\nmachines for artificial design features, long short-term memory networks based\non attention mechanisms, and graph neural networks based on dependency parsing.\nWhile these methods all have decent performance, I think they all miss one\nimportant piece of syntactic information: dependency labels. Based on this\nidea, this paper proposes a model using dependency labels for the attention\nmechanism to do this task. We evaluate the proposed approach on three datasets:\nlaptop and restaurant are from SemEval 2014, and the last one is a twitter\ndataset. Experimental results show that the dual attention model has good\nperformance on all three datasets.", "published": "2023-03-14 08:04:38", "link": "http://arxiv.org/abs/2303.07689v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Efficient Image-Text Retrieval via Keyword-Guided Pre-Screening", "abstract": "Under the flourishing development in performance, current image-text\nretrieval methods suffer from $N$-related time complexity, which hinders their\napplication in practice. Targeting at efficiency improvement, this paper\npresents a simple and effective keyword-guided pre-screening framework for the\nimage-text retrieval. Specifically, we convert the image and text data into the\nkeywords and perform the keyword matching across modalities to exclude a large\nnumber of irrelevant gallery samples prior to the retrieval network. For the\nkeyword prediction, we transfer it into a multi-label classification problem\nand propose a multi-task learning scheme by appending the multi-label\nclassifiers to the image-text retrieval network to achieve a lightweight and\nhigh-performance keyword prediction. For the keyword matching, we introduce the\ninverted index in the search engine and create a win-win situation on both time\nand space complexities for the pre-screening. Extensive experiments on two\nwidely-used datasets, i.e., Flickr30K and MS-COCO, verify the effectiveness of\nthe proposed framework. The proposed framework equipped with only two embedding\nlayers achieves $O(1)$ querying time complexity, while improving the retrieval\nefficiency and keeping its performance, when applied prior to the common\nimage-text retrieval methods. Our code will be released.", "published": "2023-03-14 09:36:42", "link": "http://arxiv.org/abs/2303.07740v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "A Theory of Emergent In-Context Learning as Implicit Structure Induction", "abstract": "Scaling large language models (LLMs) leads to an emergent capacity to learn\nin-context from example demonstrations. Despite progress, theoretical\nunderstanding of this phenomenon remains limited. We argue that in-context\nlearning relies on recombination of compositional operations found in natural\nlanguage data. We derive an information-theoretic bound showing how in-context\nlearning abilities arise from generic next-token prediction when the\npretraining distribution has sufficient amounts of compositional structure,\nunder linguistically motivated assumptions. A second bound provides a\ntheoretical justification for the empirical success of prompting LLMs to output\nintermediate steps towards an answer. To validate theoretical predictions, we\nintroduce a controlled setup for inducing in-context learning; unlike previous\napproaches, it accounts for the compositional nature of language. Trained\ntransformers can perform in-context learning for a range of tasks, in a manner\nconsistent with the theoretical results. Mirroring real-world LLMs in a\nminiature setup, in-context learning emerges when scaling parameters and data,\nand models perform better when prompted to output intermediate steps. Probing\nshows that in-context learning is supported by a representation of the input's\ncompositional structure. Taken together, these results provide a step towards\ntheoretical understanding of emergent behavior in large language models.", "published": "2023-03-14 15:24:05", "link": "http://arxiv.org/abs/2303.07971v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Finding the Needle in a Haystack: Unsupervised Rationale Extraction from\n  Long Text Classifiers", "abstract": "Long-sequence transformers are designed to improve the representation of\nlonger texts by language models and their performance on downstream\ndocument-level tasks. However, not much is understood about the quality of\ntoken-level predictions in long-form models. We investigate the performance of\nsuch architectures in the context of document classification with unsupervised\nrationale extraction. We find standard soft attention methods to perform\nsignificantly worse when combined with the Longformer language model. We\npropose a compositional soft attention architecture that applies RoBERTa\nsentence-wise to extract plausible rationales at the token-level. We find this\nmethod to significantly outperform Longformer-driven baselines on sentiment\nclassification datasets, while also exhibiting significantly lower runtimes.", "published": "2023-03-14 15:45:35", "link": "http://arxiv.org/abs/2303.07991v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "OptBA: Optimizing Hyperparameters with the Bees Algorithm for Improved\n  Medical Text Classification", "abstract": "One of the main challenges in the field of deep learning is obtaining the\noptimal model hyperparameters. The search for optimal hyperparameters usually\nhinders the progress of solutions to real-world problems such as healthcare.\nPrevious solutions have been proposed, but they can still get stuck in local\noptima. To overcome this hurdle, we propose OptBA to automatically fine-tune\nthe hyperparameters of deep learning models by leveraging the Bees Algorithm,\nwhich is a recent promising swarm intelligence algorithm. In this paper, the\noptimization problem of OptBA is to maximize the accuracy in classifying\nailments using medical text, where initial hyperparameters are iteratively\nadjusted by specific criteria. Experimental results demonstrate a noteworthy\nenhancement in accuracy with approximately 1.4%. This outcome highlights the\neffectiveness of the proposed mechanism in addressing the critical issue of\nhyperparameter optimization and its potential impact on advancing solutions for\nhealthcare. The code is available publicly at\n\\url{https://github.com/Mai-CS/OptBA}.", "published": "2023-03-14 16:04:13", "link": "http://arxiv.org/abs/2303.08021v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Verifying the Robustness of Automatic Credibility Assessment", "abstract": "Text classification methods have been widely investigated as a way to detect\ncontent of low credibility: fake news, social media bots, propaganda, etc.\nQuite accurate models (likely based on deep neural networks) help in moderating\npublic electronic platforms and often cause content creators to face rejection\nof their submissions or removal of already published texts. Having the\nincentive to evade further detection, content creators try to come up with a\nslightly modified version of the text (known as an attack with an adversarial\nexample) that exploit the weaknesses of classifiers and result in a different\noutput. Here we systematically test the robustness of common text classifiers\nagainst available attacking techniques and discover that, indeed,\nmeaning-preserving changes in input text can mislead the models. The approaches\nwe test focus on finding vulnerable spans in text and replacing individual\ncharacters or words, taking into account the similarity between the original\nand replacement content. We also introduce BODEGA: a benchmark for testing both\nvictim models and attack methods on four misinformation detection tasks in an\nevaluation framework designed to simulate real use-cases of content moderation.\nThe attacked tasks include (1) fact checking and detection of (2) hyperpartisan\nnews, (3) propaganda and (4) rumours. Our experimental results show that modern\nlarge language models are often more vulnerable to attacks than previous,\nsmaller solutions, e.g. attacks on GEMMA being up to 27\\% more successful than\nthose on BERT. Finally, we manually analyse a subset adversarial examples and\ncheck what kinds of modifications are used in successful attacks.", "published": "2023-03-14 16:11:47", "link": "http://arxiv.org/abs/2303.08032v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Progress Note Understanding -- Assessment and Plan Reasoning: Overview\n  of the 2022 N2C2 Track 3 Shared Task", "abstract": "Daily progress notes are common types in the electronic health record (EHR)\nwhere healthcare providers document the patient's daily progress and treatment\nplans. The EHR is designed to document all the care provided to patients, but\nit also enables note bloat with extraneous information that distracts from the\ndiagnoses and treatment plans. Applications of natural language processing\n(NLP) in the EHR is a growing field with the majority of methods in information\nextraction. Few tasks use NLP methods for downstream diagnostic decision\nsupport. We introduced the 2022 National NLP Clinical Challenge (N2C2) Track 3:\nProgress Note Understanding - Assessment and Plan Reasoning as one step towards\na new suite of tasks. The Assessment and Plan Reasoning task focuses on the\nmost critical components of progress notes, Assessment and Plan subsections\nwhere health problems and diagnoses are contained. The goal of the task was to\ndevelop and evaluate NLP systems that automatically predict causal relations\nbetween the overall status of the patient contained in the Assessment section\nand its relation to each component of the Plan section which contains the\ndiagnoses and treatment plans. The goal of the task was to identify and\nprioritize diagnoses as the first steps in diagnostic decision support to find\nthe most relevant information in long documents like daily progress notes. We\npresent the results of 2022 n2c2 Track 3 and provide a description of the data,\nevaluation, participation and system performance.", "published": "2023-03-14 16:17:55", "link": "http://arxiv.org/abs/2303.08038v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Simfluence: Modeling the Influence of Individual Training Examples by\n  Simulating Training Runs", "abstract": "Training data attribution (TDA) methods offer to trace a model's prediction\non any given example back to specific influential training examples. Existing\napproaches do so by assigning a scalar influence score to each training\nexample, under a simplifying assumption that influence is additive. But in\nreality, we observe that training examples interact in highly non-additive ways\ndue to factors such as inter-example redundancy, training order, and curriculum\nlearning effects.\n  To study such interactions, we propose Simfluence, a new paradigm for TDA\nwhere the goal is not to produce a single influence score per example, but\ninstead a training run simulator: the user asks, ``If my model had trained on\nexample $z_1$, then $z_2$, ..., then $z_n$, how would it behave on\n$z_{test}$?''; the simulator should then output a simulated training run, which\nis a time series predicting the loss on $z_{test}$ at every step of the\nsimulated run. This enables users to answer counterfactual questions about what\ntheir model would have learned under different training curricula, and to\ndirectly see where in training that learning would occur.\n  We present a simulator, Simfluence-Linear, that captures non-additive\ninteractions and is often able to predict the spiky trajectory of individual\nexample losses with surprising fidelity. Furthermore, we show that existing TDA\nmethods such as TracIn and influence functions can be viewed as special cases\nof Simfluence-Linear. This enables us to directly compare methods in terms of\ntheir simulation accuracy, subsuming several prior TDA approaches to\nevaluation. In experiments on large language model (LLM) fine-tuning, we show\nthat our method predicts loss trajectories with much higher accuracy than\nexisting TDA methods (doubling Spearman's correlation and reducing mean-squared\nerror by 75%) across several tasks, models, and training methods.", "published": "2023-03-14 17:47:25", "link": "http://arxiv.org/abs/2303.08114v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Do Transformers Parse while Predicting the Masked Word?", "abstract": "Pre-trained language models have been shown to encode linguistic structures,\ne.g. dependency and constituency parse trees, in their embeddings while being\ntrained on unsupervised loss functions like masked language modeling. Some\ndoubts have been raised whether the models actually are doing parsing or only\nsome computation weakly correlated with it. We study questions: (a) Is it\npossible to explicitly describe transformers with realistic embedding\ndimension, number of heads, etc. that are capable of doing parsing -- or even\napproximate parsing? (b) Why do pre-trained models capture parsing structure?\nThis paper takes a step toward answering these questions in the context of\ngenerative modeling with PCFGs. We show that masked language models like BERT\nor RoBERTa of moderate sizes can approximately execute the Inside-Outside\nalgorithm for the English PCFG [Marcus et al, 1993]. We also show that the\nInside-Outside algorithm is optimal for masked language modeling loss on the\nPCFG-generated data. We also give a construction of transformers with $50$\nlayers, $15$ attention heads, and $1275$ dimensional embeddings in average such\nthat using its embeddings it is possible to do constituency parsing with\n$>70\\%$ F1 score on PTB dataset. We conduct probing experiments on models\npre-trained on PCFG-generated data to show that this not only allows recovery\nof approximate parse tree, but also recovers marginal span probabilities\ncomputed by the Inside-Outside algorithm, which suggests an implicit bias of\nmasked language modeling towards this algorithm.", "published": "2023-03-14 17:49:50", "link": "http://arxiv.org/abs/2303.08117v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MEDBERT.de: A Comprehensive German BERT Model for the Medical Domain", "abstract": "This paper presents medBERTde, a pre-trained German BERT model specifically\ndesigned for the German medical domain. The model has been trained on a large\ncorpus of 4.7 Million German medical documents and has been shown to achieve\nnew state-of-the-art performance on eight different medical benchmarks covering\na wide range of disciplines and medical document types. In addition to\nevaluating the overall performance of the model, this paper also conducts a\nmore in-depth analysis of its capabilities. We investigate the impact of data\ndeduplication on the model's performance, as well as the potential benefits of\nusing more efficient tokenization methods. Our results indicate that\ndomain-specific models such as medBERTde are particularly useful for longer\ntexts, and that deduplication of training data does not necessarily lead to\nimproved performance. Furthermore, we found that efficient tokenization plays\nonly a minor role in improving model performance, and attribute most of the\nimproved performance to the large amount of training data. To encourage further\nresearch, the pre-trained model weights and new benchmarks based on\nradiological data are made publicly available for use by the scientific\ncommunity.", "published": "2023-03-14 18:58:08", "link": "http://arxiv.org/abs/2303.08179v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "NL4Opt Competition: Formulating Optimization Problems Based on Their\n  Natural Language Descriptions", "abstract": "The Natural Language for Optimization (NL4Opt) Competition was created to\ninvestigate methods of extracting the meaning and formulation of an\noptimization problem based on its text description. Specifically, the goal of\nthe competition is to increase the accessibility and usability of optimization\nsolvers by allowing non-experts to interface with them using natural language.\nWe separate this challenging goal into two sub-tasks: (1) recognize and label\nthe semantic entities that correspond to the components of the optimization\nproblem; (2) generate a meaning representation (i.e., a logical form) of the\nproblem from its detected problem entities. The first task aims to reduce\nambiguity by detecting and tagging the entities of the optimization problems.\nThe second task creates an intermediate representation of the linear\nprogramming (LP) problem that is converted into a format that can be used by\ncommercial solvers. In this report, we present the LP word problem dataset and\nshared tasks for the NeurIPS 2022 competition. Furthermore, we investigate and\ncompare the performance of the ChatGPT large language model against the winning\nsolutions. Through this competition, we hope to bring interest towards the\ndevelopment of novel machine learning applications and datasets for\noptimization modeling.", "published": "2023-03-14 20:59:04", "link": "http://arxiv.org/abs/2303.08233v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Contextualized Medication Information Extraction Using Transformer-based\n  Deep Learning Architectures", "abstract": "Objective: To develop a natural language processing (NLP) system to extract\nmedications and contextual information that help understand drug changes. This\nproject is part of the 2022 n2c2 challenge.\n  Materials and methods: We developed NLP systems for medication mention\nextraction, event classification (indicating medication changes discussed or\nnot), and context classification to classify medication changes context into 5\northogonal dimensions related to drug changes. We explored 6 state-of-the-art\npretrained transformer models for the three subtasks, including GatorTron, a\nlarge language model pretrained using >90 billion words of text (including >80\nbillion words from >290 million clinical notes identified at the University of\nFlorida Health). We evaluated our NLP systems using annotated data and\nevaluation scripts provided by the 2022 n2c2 organizers.\n  Results:Our GatorTron models achieved the best F1-scores of 0.9828 for\nmedication extraction (ranked 3rd), 0.9379 for event classification (ranked\n2nd), and the best micro-average accuracy of 0.9126 for context classification.\nGatorTron outperformed existing transformer models pretrained using smaller\ngeneral English text and clinical text corpora, indicating the advantage of\nlarge language models.\n  Conclusion: This study demonstrated the advantage of using large transformer\nmodels for contextual medication information extraction from clinical\nnarratives.", "published": "2023-03-14 22:22:28", "link": "http://arxiv.org/abs/2303.08259v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "I3D: Transformer architectures with input-dependent dynamic depth for\n  speech recognition", "abstract": "Transformer-based end-to-end speech recognition has achieved great success.\nHowever, the large footprint and computational overhead make it difficult to\ndeploy these models in some real-world applications. Model compression\ntechniques can reduce the model size and speed up inference, but the compressed\nmodel has a fixed architecture which might be suboptimal. We propose a novel\nTransformer encoder with Input-Dependent Dynamic Depth (I3D) to achieve strong\nperformance-efficiency trade-offs. With a similar number of layers at inference\ntime, I3D-based models outperform the vanilla Transformer and the static pruned\nmodel via iterative layer pruning. We also present interesting analysis on the\ngate probabilities and the input-dependency, which helps us better understand\ndeep encoders.", "published": "2023-03-14 04:47:00", "link": "http://arxiv.org/abs/2303.07624v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Cross-lingual Alzheimer's Disease detection based on paralinguistic and\n  pre-trained features", "abstract": "We present our submission to the ICASSP-SPGC-2023 ADReSS-M Challenge Task,\nwhich aims to investigate which acoustic features can be generalized and\ntransferred across languages for Alzheimer's Disease (AD) prediction. The\nchallenge consists of two tasks: one is to classify the speech of AD patients\nand healthy individuals, and the other is to infer Mini Mental State\nExamination (MMSE) score based on speech only. The difficulty is mainly\nembodied in the mismatch of the dataset, in which the training set is in\nEnglish while the test set is in Greek. We extract paralinguistic features\nusing openSmile toolkit and acoustic features using XLSR-53. In addition, we\nextract linguistic features after transcribing the speech into text. These\nfeatures are used as indicators for AD detection in our method. Our method\nachieves an accuracy of 69.6% on the classification task and a root mean\nsquared error (RMSE) of 4.788 on the regression task. The results show that our\nproposed method is expected to achieve automatic multilingual Alzheimer's\nDisease detection through spontaneous speech.", "published": "2023-03-14 06:34:18", "link": "http://arxiv.org/abs/2303.07650v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "QI-TTS: Questioning Intonation Control for Emotional Speech Synthesis", "abstract": "Recent expressive text to speech (TTS) models focus on synthesizing emotional\nspeech, but some fine-grained styles such as intonation are neglected. In this\npaper, we propose QI-TTS which aims to better transfer and control intonation\nto further deliver the speaker's questioning intention while transferring\nemotion from reference speech. We propose a multi-style extractor to extract\nstyle embedding from two different levels. While the sentence level represents\nemotion, the final syllable level represents intonation. For fine-grained\nintonation control, we use relative attributes to represent intonation\nintensity at the syllable level.Experiments have validated the effectiveness of\nQI-TTS for improving intonation expressiveness in emotional speech synthesis.", "published": "2023-03-14 07:53:19", "link": "http://arxiv.org/abs/2303.07682v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Dynamic Alignment Mask CTC: Improved Mask-CTC with Aligned Cross Entropy", "abstract": "Because of predicting all the target tokens in parallel, the\nnon-autoregressive models greatly improve the decoding efficiency of speech\nrecognition compared with traditional autoregressive models. In this work, we\npresent dynamic alignment Mask CTC, introducing two methods: (1) Aligned Cross\nEntropy (AXE), finding the monotonic alignment that minimizes the cross-entropy\nloss through dynamic programming, (2) Dynamic Rectification, creating new\ntraining samples by replacing some masks with model predicted tokens. The AXE\nignores the absolute position alignment between prediction and ground truth\nsentence and focuses on tokens matching in relative order. The dynamic\nrectification method makes the model capable of simulating the non-mask but\npossible wrong tokens, even if they have high confidence. Our experiments on\nWSJ dataset demonstrated that not only AXE loss but also the rectification\nmethod could improve the WER performance of Mask CTC.", "published": "2023-03-14 08:01:21", "link": "http://arxiv.org/abs/2303.07687v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improving Prosody for Cross-Speaker Style Transfer by Semi-Supervised\n  Style Extractor and Hierarchical Modeling in Speech Synthesis", "abstract": "Cross-speaker style transfer in speech synthesis aims at transferring a style\nfrom source speaker to synthesized speech of a target speaker's timbre. In most\nprevious methods, the synthesized fine-grained prosody features often represent\nthe source speaker's average style, similar to the one-to-many problem(i.e.,\nmultiple prosody variations correspond to the same text). In response to this\nproblem, a strength-controlled semi-supervised style extractor is proposed to\ndisentangle the style from content and timbre, improving the representation and\ninterpretability of the global style embedding, which can alleviate the\none-to-many mapping and data imbalance problems in prosody prediction. A\nhierarchical prosody predictor is proposed to improve prosody modeling. We find\nthat better style transfer can be achieved by using the source speaker's\nprosody features that are easily predicted. Additionally, a\nspeaker-transfer-wise cycle consistency loss is proposed to assist the model in\nlearning unseen style-timbre combinations during the training phase.\nExperimental results show that the method outperforms the baseline. We provide\na website with audio samples.", "published": "2023-03-14 08:52:58", "link": "http://arxiv.org/abs/2303.07711v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Good Neighbors Are All You Need for Chinese Grapheme-to-Phoneme\n  Conversion", "abstract": "Most Chinese Grapheme-to-Phoneme (G2P) systems employ a three-stage framework\nthat first transforms input sequences into character embeddings, obtains\nlinguistic information using language models, and then predicts the phonemes\nbased on global context about the entire input sequence. However, linguistic\nknowledge alone is often inadequate. Language models frequently encode overly\ngeneral structures of a sentence and fail to cover specific cases needed to use\nphonetic knowledge. Also, a handcrafted post-processing system is needed to\naddress the problems relevant to the tone of the characters. However, the\nsystem exhibits inconsistency in the segmentation of word boundaries which\nconsequently degrades the performance of the G2P system. To address these\nissues, we propose the Reinforcer that provides strong inductive bias for\nlanguage models by emphasizing the phonological information between neighboring\ncharacters to help disambiguate pronunciations. Experimental results show that\nthe Reinforcer boosts the cutting-edge architectures by a large margin. We also\ncombine the Reinforcer with a large-scale pre-trained model and demonstrate the\nvalidity of using neighboring context in knowledge transfer scenarios.", "published": "2023-03-14 09:15:51", "link": "http://arxiv.org/abs/2303.07726v1", "categories": ["cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Predicting the Geolocation of Tweets Using transformer models on\n  Customized Data", "abstract": "This research is aimed to solve the tweet/user geolocation prediction task\nand provide a flexible methodology for the geotagging of textual big data. The\nsuggested approach implements neural networks for natural language processing\n(NLP) to estimate the location as coordinate pairs (longitude, latitude) and\ntwo-dimensional Gaussian Mixture Models (GMMs). The scope of proposed models\nhas been finetuned on a Twitter dataset using pretrained Bidirectional Encoder\nRepresentations from Transformers (BERT) as base models. Performance metrics\nshow a median error of fewer than 30 km on a worldwide-level, and fewer than 15\nkm on the US-level datasets for the models trained and evaluated on text\nfeatures of tweets' content and metadata context. Our source code and data are\navailable at https://github.com/K4TEL/geo-twitter.git", "published": "2023-03-14 12:56:47", "link": "http://arxiv.org/abs/2303.07865v6", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Improving Accented Speech Recognition with Multi-Domain Training", "abstract": "Thanks to the rise of self-supervised learning, automatic speech recognition\n(ASR) systems now achieve near-human performance on a wide variety of datasets.\nHowever, they still lack generalization capability and are not robust to domain\nshifts like accent variations. In this work, we use speech audio representing\nfour different French accents to create fine-tuning datasets that improve the\nrobustness of pre-trained ASR models. By incorporating various accents in the\ntraining set, we obtain both in-domain and out-of-domain improvements. Our\nnumerical experiments show that we can reduce error rates by up to 25%\n(relative) on African and Belgian accents compared to single-domain training\nwhile keeping a good performance on standard French.", "published": "2023-03-14 14:10:16", "link": "http://arxiv.org/abs/2303.07924v1", "categories": ["cs.LG", "cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Happy-GLL: modular, reusable and complete top-down parsers for\n  parameterized nonterminals", "abstract": "Parser generators and parser combinator libraries are the most popular tools\nfor producing parsers. Parser combinators use the host language to provide\nreusable components in the form of higher-order functions with parsers as\nparameters. Very few parser generators support this kind of reuse through\nabstraction and even fewer generate parsers that are as modular and reusable as\nthe parts of the grammar for which they are produced. This paper presents a\nstrategy for generating modular, reusable and complete top-down parsers from\nsyntax descriptions with parameterized nonterminals, based on the FUN-GLL\nvariant of the GLL algorithm.\n  The strategy is discussed and demonstrated as a novel back-end for the Happy\nparser generator. Happy grammars can contain `parameterized nonterminals' in\nwhich parameters abstract over grammar symbols, granting an abstraction\nmechanism to define reusable grammar operators. However, the existing Happy\nback-ends do not deliver on the full potential of parameterized nonterminals as\nparameterized nonterminals cannot be reused across grammars. Moreover, the\nparser generation process may fail to terminate or may result in exponentially\nlarge parsers generated in an exponential amount of time.\n  The GLL back-end presented in this paper implements parameterized\nnonterminals successfully by generating higher-order functions that resemble\nparser combinators, inheriting all the advantages of top-down parsing. The\nback-end is capable of generating parsers for the full class of context-free\ngrammars, generates parsers in linear time and generates parsers that find all\nderivations of the input string. To our knowledge, the presented GLL back-end\nmakes Happy the first parser generator that combines all these features.\n  This paper describes the translation procedure of the GLL back-end and\ncompares it to the LALR and GLR back-ends of Happy in several experiments.", "published": "2023-03-14 16:23:23", "link": "http://arxiv.org/abs/2303.08044v1", "categories": ["cs.CL", "cs.PL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "CB2: Collaborative Natural Language Interaction Research Platform", "abstract": "CB2 is a multi-agent platform to study collaborative natural language\ninteraction in a grounded task-oriented scenario. It includes a 3D game\nenvironment, a backend server designed to serve trained models to human agents,\nand various tools and processes to enable scalable studies. We deploy CB2 at\nhttps://cb2.ai as a system demonstration with a learned instruction following\nmodel.", "published": "2023-03-14 17:57:06", "link": "http://arxiv.org/abs/2303.08127v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.LG"}
{"title": "Distinguishing Cause from Effect on Categorical Data: The Uniform\n  Channel Model", "abstract": "Distinguishing cause from effect using observations of a pair of random\nvariables is a core problem in causal discovery. Most approaches proposed for\nthis task, namely additive noise models (ANM), are only adequate for\nquantitative data. We propose a criterion to address the cause-effect problem\nwith categorical variables (living in sets with no meaningful order), inspired\nby seeing a conditional probability mass function (pmf) as a discrete\nmemoryless channel. We select as the most likely causal direction the one in\nwhich the conditional pmf is closer to a uniform channel (UC). The rationale is\nthat, in a UC, as in an ANM, the conditional entropy (of the effect given the\ncause) is independent of the cause distribution, in agreement with the\nprinciple of independence of cause and mechanism. Our approach, which we call\nthe uniform channel model (UCM), thus extends the ANM rationale to categorical\nvariables. To assess how close a conditional pmf (estimated from data) is to a\nUC, we use statistical testing, supported by a closed-form estimate of a UC\nchannel. On the theoretical front, we prove identifiability of the UCM and show\nits equivalence with a structural causal model with a low-cardinality exogenous\nvariable. Finally, the proposed method compares favorably with recent\nstate-of-the-art alternatives in experiments on synthetic, benchmark, and real\ndata.", "published": "2023-03-14 13:54:11", "link": "http://arxiv.org/abs/2303.08572v1", "categories": ["cs.LG", "cs.CL", "cs.IT", "math.IT", "62D20"], "primary_category": "cs.LG"}
{"title": "Controllable Prosody Generation With Partial Inputs", "abstract": "We address the problem of human-in-the-loop control for generating prosody in\nthe context of text-to-speech synthesis. Controlling prosody is challenging\nbecause existing generative models lack an efficient interface through which\nusers can modify the output quickly and precisely. To solve this, we introduce\na novel framework whereby the user provides partial inputs and the generative\nmodel generates the missing features. We propose a model that is specifically\ndesigned to encode partial prosodic features and output complete audio. We show\nempirically that our model displays two essential qualities of a\nhuman-in-the-loop control mechanism: efficiency and robustness. With even a\nvery small number of input values (~4), our model enables users to improve the\nquality of the output significantly in terms of listener preference (4:1).", "published": "2023-03-14 09:47:23", "link": "http://arxiv.org/abs/2303.09446v2", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Features matching using natural language processing", "abstract": "The feature matching is a basic step in matching different datasets. This\narticle proposes shows a new hybrid model of a pretrained Natural Language\nProcessing (NLP) based model called BERT used in parallel with a statistical\nmodel based on Jaccard similarity to measure the similarity between list of\nfeatures from two different datasets. This reduces the time required to search\nfor correlations or manually match each feature from one dataset to another.", "published": "2023-03-14 13:31:19", "link": "http://arxiv.org/abs/2303.12804v1", "categories": ["cs.DB", "cs.CL", "cs.LG"], "primary_category": "cs.DB"}
{"title": "Learning Cross-lingual Visual Speech Representations", "abstract": "Cross-lingual self-supervised learning has been a growing research topic in\nthe last few years. However, current works only explored the use of audio\nsignals to create representations. In this work, we study cross-lingual\nself-supervised visual representation learning. We use the recently-proposed\nRaw Audio-Visual Speech Encoders (RAVEn) framework to pre-train an audio-visual\nmodel with unlabelled multilingual data, and then fine-tune the visual model on\nlabelled transcriptions. Our experiments show that: (1) multi-lingual models\nwith more data outperform monolingual ones, but, when keeping the amount of\ndata fixed, monolingual models tend to reach better performance; (2)\nmulti-lingual outperforms English-only pre-training; (3) using languages which\nare more similar yields better results; and (4) fine-tuning on unseen languages\nis competitive to using the target language in the pre-training set. We hope\nour study inspires future research on non-English-only speech representation\nlearning.", "published": "2023-03-14 17:05:08", "link": "http://arxiv.org/abs/2303.09455v1", "categories": ["cs.CL", "cs.CV", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Chat with the Environment: Interactive Multimodal Perception Using Large\n  Language Models", "abstract": "Programming robot behavior in a complex world faces challenges on multiple\nlevels, from dextrous low-level skills to high-level planning and reasoning.\nRecent pre-trained Large Language Models (LLMs) have shown remarkable reasoning\nability in few-shot robotic planning. However, it remains challenging to ground\nLLMs in multimodal sensory input and continuous action output, while enabling a\nrobot to interact with its environment and acquire novel information as its\npolicies unfold. We develop a robot interaction scenario with a partially\nobservable state, which necessitates a robot to decide on a range of epistemic\nactions in order to sample sensory information among multiple modalities,\nbefore being able to execute the task correctly. Matcha (Multimodal environment\nchatting) agent, an interactive perception framework, is therefore proposed\nwith an LLM as its backbone, whose ability is exploited to instruct epistemic\nactions and to reason over the resulting multimodal sensations (vision, sound,\nhaptics, proprioception), as well as to plan an entire task execution based on\nthe interactively acquired information. Our study demonstrates that LLMs can\nprovide high-level planning and reasoning skills and control interactive robot\nbehavior in a multimodal environment, while multimodal modules with the context\nof the environmental state help ground the LLMs and extend their processing\nability. The project website can be found at https://matcha-agent.github.io.", "published": "2023-03-14 23:01:27", "link": "http://arxiv.org/abs/2303.08268v3", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.RO"}
{"title": "Towards Real-Time Single-Channel Speech Separation in Noisy and\n  Reverberant Environments", "abstract": "Real-time single-channel speech separation aims to unmix an audio stream\ncaptured from a single microphone that contains multiple people talking at\nonce, environmental noise, and reverberation into multiple de-reverberated and\nnoise-free speech tracks, each track containing only one talker. While large\nstate-of-the-art DNNs can achieve excellent separation from anechoic mixtures\nof speech, the main challenge is to create compact and causal models that can\nseparate reverberant mixtures at inference time. In this paper, we explore\nlow-complexity, resource-efficient, causal DNN architectures for real-time\nseparation of two or more simultaneous speakers. A cascade of three neural\nnetwork modules are trained to sequentially perform noise-suppression,\nseparation, and de-reverberation. For comparison, a larger end-to-end model is\ntrained to output two anechoic speech signals directly from noisy reverberant\nspeech mixtures. We propose an efficient single-decoder architecture with\nsubtractive separation for real-time recursive speech separation for two or\nmore speakers. Evaluation on real monophonic recordings of speech mixtures,\naccording to speech separation measures like SI-SDR, perceptual measures like\nDNS-MOS, and a novel proposed channel separation metric, show that these\ncompact causal models can separate speech mixtures with low latency, and\nperform on par with large offline state-of-the-art models like SepFormer.", "published": "2023-03-14 01:40:36", "link": "http://arxiv.org/abs/2303.07569v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Lightweight feature encoder for wake-up word detection based on\n  self-supervised speech representation", "abstract": "Self-supervised learning method that provides generalized speech\nrepresentations has recently received increasing attention. Wav2vec 2.0 is the\nmost famous example, showing remarkable performance in numerous downstream\nspeech processing tasks. Despite its success, it is challenging to use it\ndirectly for wake-up word detection on mobile devices due to its expensive\ncomputational cost. In this work, we propose LiteFEW, a lightweight feature\nencoder for wake-up word detection that preserves the inherent ability of\nwav2vec 2.0 with a minimum scale. In the method, the knowledge of the\npre-trained wav2vec 2.0 is compressed by introducing an auto-encoder-based\ndimensionality reduction technique and distilled to LiteFEW. Experimental\nresults on the open-source \"Hey Snips\" dataset show that the proposed method\napplied to various model structures significantly improves the performance,\nachieving over 20% of relative improvements with only 64k parameters.", "published": "2023-03-14 02:31:44", "link": "http://arxiv.org/abs/2303.07592v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Two-stage Neural Network for ICASSP 2023 Speech Signal Improvement\n  Challenge", "abstract": "In ICASSP 2023 speech signal improvement challenge, we developed a dual-stage\nneural model which improves speech signal quality induced by different\ndistortions in a stage-wise divide-and-conquer fashion. Specifically, in the\nfirst stage, the speech improvement network focuses on recovering the missing\ncomponents of the spectrum, while in the second stage, our model aims to\nfurther suppress noise, reverberation, and artifacts introduced by the\nfirst-stage model. Achieving 0.446 in the final score and 0.517 in the P.835\nscore, our system ranks 4th in the non-real-time track.", "published": "2023-03-14 04:19:41", "link": "http://arxiv.org/abs/2303.07621v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "TEA-PSE 3.0: Tencent-Ethereal-Audio-Lab Personalized Speech Enhancement\n  System For ICASSP 2023 DNS Challenge", "abstract": "This paper introduces the Unbeatable Team's submission to the ICASSP 2023\nDeep Noise Suppression (DNS) Challenge. We expand our previous work, TEA-PSE,\nto its upgraded version -- TEA-PSE 3.0. Specifically, TEA-PSE 3.0 incorporates\na residual LSTM after squeezed temporal convolution network (S-TCN) to enhance\nsequence modeling capabilities. Additionally, the local-global representation\n(LGR) structure is introduced to boost speaker information extraction, and\nmulti-STFT resolution loss is used to effectively capture the time-frequency\ncharacteristics of the speech signals. Moreover, retraining methods are\nemployed based on the freeze training strategy to fine-tune the system.\nAccording to the official results, TEA-PSE 3.0 ranks 1st in both ICASSP 2023\nDNS-Challenge track 1 and track 2.", "published": "2023-03-14 08:41:58", "link": "http://arxiv.org/abs/2303.07704v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multi-Channel Masking with Learnable Filterbank for Sound Source\n  Separation", "abstract": "This work proposes a learnable filterbank based on a multi-channel masking\nframework for multi-channel source separation. The learnable filterbank is a 1D\nConv layer, which transforms the raw waveform into a 2D representation. In\ncontrast to the conventional single-channel masking method, we estimate a mask\nfor each individual microphone channel. The estimated masks are then applied to\nthe transformed waveform representation like in the traditional filter-and-sum\nbeamforming operation. Specifically, each mask is used to multiply the\ncorresponding channel's 2D representation, and the masked output of all\nchannels are then summed. At last, a 1D transposed Conv layer is used to\nconvert the summed masked signal into the waveform domain. The experimental\nresults show our method outperforms single-channel masking with a learnable\nfilterbank and can outperform multi-channel complex masking with STFT complex\nspectrum in the STGCSEN model if a learnable filterbank is transformed to a\nhigher feature dimension. The spatial response analysis also verifies that\nmulti-channel masking in the learnable filterbank domain has spatial\nselectivity.", "published": "2023-03-14 11:46:47", "link": "http://arxiv.org/abs/2303.07816v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "BLAT: Bootstrapping Language-Audio Pre-training based on AudioSet\n  Tag-guided Synthetic Data", "abstract": "Compared with ample visual-text pre-training research, few works explore\naudio-text pre-training, mostly due to the lack of sufficient parallel\naudio-text data. Most existing methods incorporate the visual modality as a\npivot for audio-text pre-training, which inevitably induces data noise. In this\npaper, we propose to utilize audio captioning to generate text directly from\naudio, without the aid of the visual modality so that potential noise from\nmodality mismatch is eliminated. Furthermore, we propose caption generation\nunder the guidance of AudioSet tags, leading to more accurate captions. With\nthe above two improvements, we curate high-quality, large-scale parallel\naudio-text data, based on which we perform audio-text pre-training. We\ncomprehensively demonstrate the performance of the pre-trained model on a\nseries of downstream audio-related tasks, including single-modality tasks like\naudio classification and tagging, as well as cross-modal tasks consisting of\naudio-text retrieval and audio-based text generation. Experimental results\nindicate that our approach achieves state-of-the-art zero-shot classification\nperformance on most datasets, suggesting the effectiveness of our synthetic\ndata. The audio encoder also serves as an efficient pattern recognition model\nby fine-tuning it on audio-related tasks. Synthetic data and pre-trained models\nare available online.", "published": "2023-03-14 13:42:26", "link": "http://arxiv.org/abs/2303.07902v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Native Multi-Band Audio Coding within Hyper-Autoencoded Reconstruction\n  Propagation Networks", "abstract": "Spectral sub-bands do not portray the same perceptual relevance. In audio\ncoding, it is therefore desirable to have independent control over each of the\nconstituent bands so that bitrate assignment and signal reconstruction can be\nachieved efficiently. In this work, we present a novel neural audio coding\nnetwork that natively supports a multi-band coding paradigm. Our model extends\nthe idea of compressed skip connections in the U-Net-based codec, allowing for\nindependent control over both core and high band-specific reconstructions and\nbit allocation. Our system reconstructs the full-band signal mainly from the\ncondensed core-band code, therefore exploiting and showcasing its bandwidth\nextension capabilities to its fullest. Meanwhile, the low-bitrate high-band\ncode helps the high-band reconstruction similarly to MPEG audio codecs'\nspectral bandwidth replication. MUSHRA tests show that the proposed model not\nonly improves the quality of the core band by explicitly assigning more bits to\nit but retains a good quality in the high-band as well.", "published": "2023-03-14 15:53:36", "link": "http://arxiv.org/abs/2303.08005v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Localizing Spatial Information in Neural Spatiospectral Filters", "abstract": "Beamforming for multichannel speech enhancement relies on the estimation of\nspatial characteristics of the acoustic scene. In its simplest form, the\ndelay-and-sum beamformer (DSB) introduces a time delay to all channels to align\nthe desired signal components for constructive superposition. Recent\ninvestigations of neural spatiospectral filtering revealed that these filters\ncan be characterized by a beampattern similar to one of traditional\nbeamformers, which shows that artificial neural networks can learn and\nexplicitly represent spatial structure. Using the Complex-valued Spatial\nAutoencoder (COSPA) as an exemplary neural spatiospectral filter for\nmultichannel speech enhancement, we investigate where and how such networks\nrepresent spatial information. We show via clustering that for COSPA the\nspatial information is represented by the features generated by a gated\nrecurrent unit (GRU) layer that has access to all channels simultaneously and\nthat these features are not source -- but only direction of arrival-dependent.", "published": "2023-03-14 16:32:26", "link": "http://arxiv.org/abs/2303.08052v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Facilitating deep acoustic phenotyping: A basic coding scheme of infant\n  vocalisations preluding computational analysis, machine learning and clinical\n  reasoning", "abstract": "Theoretical background: early verbal development is not yet fully understood,\nespecially in its formative phase. Research question: can a reliable,\neasy-to-use coding scheme for the classification of early infant vocalizations\nbe defined that is applicable as a basis for further analysis of language\ndevelopment? Methods: in a longitudinal study of 45 neurotypical infants, we\nanalyzed vocalizations of the first 4 months of life. Audio segments were\nassigned to 5 classes: (1) Voiced and (2) Voiceless vocalizations; (3) Defined\nsignal; (4) Non-target; (5) Nonassignable. Results: Two female coders with\ndifferent experience achieved high agreement without intensive training.\nDiscussion and Conclusion: The reliable scheme can be used in research and\nclinical settings for efficient coding of infant vocalizations, as a basis for\ndetailed manual and machine analyses.", "published": "2023-03-14 21:10:48", "link": "http://arxiv.org/abs/2303.08239v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "VANI: Very-lightweight Accent-controllable TTS for Native and Non-native\n  speakers with Identity Preservation", "abstract": "We introduce VANI, a very lightweight multi-lingual accent controllable\nspeech synthesis system. Our model builds upon disentanglement strategies\nproposed in RADMMM and supports explicit control of accent, language, speaker\nand fine-grained $F_0$ and energy features for speech synthesis. We utilize the\nIndic languages dataset, released for LIMMITS 2023 as part of ICASSP Signal\nProcessing Grand Challenge, to synthesize speech in 3 different languages. Our\nmodel supports transferring the language of a speaker while retaining their\nvoice and the native accent of the target language. We utilize the\nlarge-parameter RADMMM model for Track $1$ and lightweight VANI model for Track\n$2$ and $3$ of the competition.", "published": "2023-03-14 01:55:41", "link": "http://arxiv.org/abs/2303.07578v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "CAT: Causal Audio Transformer for Audio Classification", "abstract": "The attention-based Transformers have been increasingly applied to audio\nclassification because of their global receptive field and ability to handle\nlong-term dependency. However, the existing frameworks which are mainly\nextended from the Vision Transformers are not perfectly compatible with audio\nsignals. In this paper, we introduce a Causal Audio Transformer (CAT)\nconsisting of a Multi-Resolution Multi-Feature (MRMF) feature extraction with\nan acoustic attention block for more optimized audio modeling. In addition, we\npropose a causal module that alleviates over-fitting, helps with knowledge\ntransfer, and improves interpretability. CAT obtains higher or comparable\nstate-of-the-art classification performance on ESC50, AudioSet and UrbanSound8K\ndatasets, and can be easily generalized to other Transformer-based models.", "published": "2023-03-14 04:50:52", "link": "http://arxiv.org/abs/2303.07626v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Feature-Rich Audio Model Inversion for Data-Free Knowledge Distillation\n  Towards General Sound Classification", "abstract": "Data-Free Knowledge Distillation (DFKD) has recently attracted growing\nattention in the academic community, especially with major breakthroughs in\ncomputer vision. Despite promising results, the technique has not been well\napplied to audio and signal processing. Due to the variable duration of audio\nsignals, it has its own unique way of modeling. In this work, we propose\nfeature-rich audio model inversion (FRAMI), a data-free knowledge distillation\nframework for general sound classification tasks. It first generates\nhigh-quality and feature-rich Mel-spectrograms through a feature-invariant\ncontrastive loss. Then, the hidden states before and after the statistics\npooling layer are reused when knowledge distillation is performed on these\nfeature-rich samples. Experimental results on the Urbansound8k, ESC-50, and\naudioMNIST datasets demonstrate that FRAMI can generate feature-rich samples.\nMeanwhile, the accuracy of the student model is further improved by reusing the\nhidden state and significantly outperforms the baseline method.", "published": "2023-03-14 06:04:19", "link": "http://arxiv.org/abs/2303.07643v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Detecting post-stroke aphasia using EEG-based neural envelope tracking\n  of natural speech", "abstract": "[Objective]. After a stroke, one-third of patients suffer from aphasia, a\nlanguage disorder that impairs communication ability. The standard behavioral\ntests used to diagnose aphasia are time-consuming and have low ecological\nvalidity. Neural tracking of the speech envelope is a promising tool for\ninvestigating brain responses to natural speech. The speech envelope is crucial\nfor speech understanding, encompassing cues for processing linguistic units. In\nthis study, we aimed to test the potential of the neural envelope tracking\ntechnique for detecting language impairments in individuals with aphasia (IWA).\n[Approach]. We recorded EEG from 27 IWA in the chronic phase after stroke and\n22 controls while they listened to a story. We quantified neural envelope\ntracking in a broadband frequency range as well as in the delta, theta, alpha,\nbeta, and gamma frequency bands using mutual information analysis. Besides\ngroup differences in neural tracking measures, we also tested its suitability\nfor detecting aphasia using a Support Vector Machine (SVM) classifier. We\nfurther investigated the required recording length for the SVM to detect\naphasia and to obtain reliable outcomes. [Results]. IWA displayed decreased\nneural envelope tracking compared to controls in the broad, delta, theta, and\ngamma band. Neural tracking in these frequency bands effectively captured\naphasia at the individual level (SVM accuracy 84%, AUC 88%). High-accuracy and\nreliable detection could be obtained with 5-7 minutes of recording time.\n[Significance]. Our study shows that neural tracking of speech is an effective\nbiomarker for aphasia. We demonstrated its potential as a diagnostic tool with\nhigh reliability, individual-level detection of aphasia, and time-efficient\nassessment. This work represents a significant step towards more automatic,\nobjective, and ecologically valid assessments of language impairments in\naphasia.", "published": "2023-03-14 09:35:32", "link": "http://arxiv.org/abs/2303.07739v1", "categories": ["eess.SP", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
{"title": "DiffuseRoll: Multi-track multi-category music generation based on\n  diffusion model", "abstract": "Recent advancements in generative models have shown remarkable progress in\nmusic generation. However, most existing methods focus on generating monophonic\nor homophonic music, while the generation of polyphonic and multi-track music\nwith rich attributes is still a challenging task. In this paper, we propose a\nnovel approach for multi-track, multi-attribute symphonic music generation\nusing the diffusion model. Specifically, we generate piano-roll representations\nwith a diffusion model and map them to MIDI format for output. To capture rich\nattribute information, we introduce a color coding scheme to encode note\nsequences into color and position information that represents pitch,velocity,\nand instrument. This scheme enables a seamless mapping between discrete music\nsequences and continuous images. We also propose a post-processing method to\noptimize the generated scores for better performance. Experimental results show\nthat our method outperforms state-of-the-art methods in terms of polyphonic\nmusic generation with rich attribute information compared to the figure\nmethods.", "published": "2023-03-14 11:09:12", "link": "http://arxiv.org/abs/2303.07794v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Leveraging Pretrained Representations with Task-related Keywords for\n  Alzheimer's Disease Detection", "abstract": "With the global population aging rapidly, Alzheimer's disease (AD) is\nparticularly prominent in older adults, which has an insidious onset and leads\nto a gradual, irreversible deterioration in cognitive domains (memory,\ncommunication, etc.). Speech-based AD detection opens up the possibility of\nwidespread screening and timely disease intervention. Recent advances in\npre-trained models motivate AD detection modeling to shift from low-level\nfeatures to high-level representations. This paper presents several efficient\nmethods to extract better AD-related cues from high-level acoustic and\nlinguistic features. Based on these features, the paper also proposes a novel\ntask-oriented approach by modeling the relationship between the participants'\ndescription and the cognitive task. Experiments are carried out on the ADReSS\ndataset in a binary classification setup, and models are evaluated on the\nunseen test set. Results and comparison with recent literature demonstrate the\nefficiency and superior performance of proposed acoustic, linguistic and\ntask-oriented methods. The findings also show the importance of semantic and\nsyntactic information, and feasibility of automation and generalization with\nthe promising audio-only and task-oriented methods for the AD detection task.", "published": "2023-03-14 16:03:28", "link": "http://arxiv.org/abs/2303.08019v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "q-bio.QM"], "primary_category": "eess.AS"}
{"title": "A Study on Bias and Fairness In Deep Speaker Recognition", "abstract": "With the ubiquity of smart devices that use speaker recognition (SR) systems\nas a means of authenticating individuals and personalizing their services,\nfairness of SR systems has becomes an important point of focus. In this paper\nwe study the notion of fairness in recent SR systems based on 3 popular and\nrelevant definitions, namely Statistical Parity, Equalized Odds, and Equal\nOpportunity. We examine 5 popular neural architectures and 5 commonly used loss\nfunctions in training SR systems, while evaluating their fairness against\ngender and nationality groups. Our detailed experiments shed light on this\nconcept and demonstrate that more sophisticated encoder architectures better\nalign with the definitions of fairness. Additionally, we find that the choice\nof loss functions can significantly impact the bias of SR models.", "published": "2023-03-14 16:08:19", "link": "http://arxiv.org/abs/2303.08026v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Hierarchical Regression Chain Framework for Affective Vocal Burst\n  Recognition", "abstract": "As a common way of emotion signaling via non-linguistic vocalizations, vocal\nburst (VB) plays an important role in daily social interaction. Understanding\nand modeling human vocal bursts are indispensable for developing robust and\ngeneral artificial intelligence. Exploring computational approaches for\nunderstanding vocal bursts is attracting increasing research attention. In this\nwork, we propose a hierarchical framework, based on chain regression models,\nfor affective recognition from VBs, that explicitly considers multiple\nrelationships: (i) between emotional states and diverse cultures; (ii) between\nlow-dimensional (arousal & valence) and high-dimensional (10 emotion classes)\nemotion spaces; and (iii) between various emotion classes within the\nhigh-dimensional space. To address the challenge of data sparsity, we also use\nself-supervised learning (SSL) representations with layer-wise and temporal\naggregation modules. The proposed systems participated in the ACII Affective\nVocal Burst (A-VB) Challenge 2022 and ranked first in the \"TWO'' and \"CULTURE''\ntasks. Experimental results based on the ACII Challenge 2022 dataset\ndemonstrate the superior performance of the proposed system and the\neffectiveness of considering multiple relationships using hierarchical\nregression chain models.", "published": "2023-03-14 16:08:45", "link": "http://arxiv.org/abs/2303.08027v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
