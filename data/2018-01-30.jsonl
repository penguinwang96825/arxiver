{"title": "A State-of-the-Art of Semantic Change Computation", "abstract": "This paper reviews the state-of-the-art of semantic change computation, one\nemerging research field in computational linguistics, proposing a framework\nthat summarizes the literature by identifying and expounding five essential\ncomponents in the field: diachronic corpus, diachronic word sense\ncharacterization, change modelling, evaluation data and data visualization.\nDespite the potential of the field, the review shows that current studies are\nmainly focused on testifying hypotheses proposed in theoretical linguistics and\nthat several core issues remain to be solved: the need for diachronic corpora\nof languages other than English, the need for comprehensive evaluation data for\nevaluation, the comparison and construction of approaches to diachronic word\nsense characterization and change modelling, and further exploration of data\nvisualization techniques for hypothesis justification.", "published": "2018-01-30 07:17:40", "link": "http://arxiv.org/abs/1801.09872v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Attention-Based Word-Level Interaction Model: Relation Detection for\n  Knowledge Base Question Answering", "abstract": "Relation detection plays a crucial role in Knowledge Base Question Answering\n(KBQA) because of the high variance of relation expression in the question.\nTraditional deep learning methods follow an encoding-comparing paradigm, where\nthe question and the candidate relation are represented as vectors to compare\ntheir semantic similarity. Max- or average- pooling operation, which compresses\nthe sequence of words into fixed-dimensional vectors, becomes the bottleneck of\ninformation. In this paper, we propose to learn attention-based word-level\ninteractions between questions and relations to alleviate the bottleneck issue.\nSimilar to the traditional models, the question and relation are firstly\nrepresented as sequences of vectors. Then, instead of merging the sequence into\na single vector with pooling operation, soft alignments between words from the\nquestion and the relation are learned. The aligned words are subsequently\ncompared with the convolutional neural network (CNN) and the comparison results\nare merged finally. Through performing the comparison on low-level\nrepresentations, the attention-based word-level interaction model (ABWIM)\nrelieves the information loss issue caused by merging the sequence into a\nfixed-dimensional vector before the comparison. The experimental results of\nrelation detection on both SimpleQuestions and WebQuestions datasets show that\nABWIM achieves state-of-the-art accuracy, demonstrating its effectiveness.", "published": "2018-01-30 08:44:19", "link": "http://arxiv.org/abs/1801.09893v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pilot study for the COST Action \"Reassembling the Republic of Letters\":\n  language-driven network analysis of letters from the Hartlib's Papers", "abstract": "The present report summarizes an exploratory study which we carried out in\nthe context of the COST Action IS1310 \"Reassembling the Republic of Letters,\n1500-1800\", and which is relevant to the activities of Working Group 3 \"Texts\nand Topics\" and Working Group 2 \"People and Networks\". In this study we\ninvestigated the use of Natural Language Processing (NLP) and Network Text\nAnalysis on a small sample of seventeenth-century letters selected from Hartlib\nPapers, whose records are in one of the catalogues of Early Modern Letters\nOnline (EMLO) and whose online edition is available on the website of the\nHumanities Research Institute at the University of Sheffield\n(http://www.hrionline.ac.uk/hartlib/). We outline the NLP pipeline used to\nautomatically process the texts into a network representation, in order to\nidentify the texts' \"narrative centrality\", i.e. the most central entities in\nthe texts, and the relations between them.", "published": "2018-01-30 08:50:26", "link": "http://arxiv.org/abs/1801.09896v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PEYMA: A Tagged Corpus for Persian Named Entities", "abstract": "The goal in the NER task is to classify proper nouns of a text into classes\nsuch as person, location, and organization. This is an important preprocessing\nstep in many NLP tasks such as question-answering and summarization. Although\nmany research studies have been conducted in this area in English and the\nstate-of-the-art NER systems have reached performances of higher than 90\npercent in terms of F1 measure, there are very few research studies for this\ntask in Persian. One of the main important causes of this may be the lack of a\nstandard Persian NER dataset to train and test NER systems. In this research we\ncreate a standard, big-enough tagged Persian NER dataset which will be\ndistributed for free for research purposes. In order to construct such a\nstandard dataset, we studied standard NER datasets which are constructed for\nEnglish researches and found out that almost all of these datasets are\nconstructed using news texts. So we collected documents from ten news websites.\nLater, in order to provide annotators with some guidelines to tag these\ndocuments, after studying guidelines used for constructing CoNLL and MUC\nstandard English datasets, we set our own guidelines considering the Persian\nlinguistic rules.", "published": "2018-01-30 11:30:38", "link": "http://arxiv.org/abs/1801.09936v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Wikipedia by Summarizing Long Sequences", "abstract": "We show that generating English Wikipedia articles can be approached as a\nmulti- document summarization of source documents. We use extractive\nsummarization to coarsely identify salient information and a neural abstractive\nmodel to generate the article. For the abstractive model, we introduce a\ndecoder-only architecture that can scalably attend to very long sequences, much\nlonger than typical encoder- decoder architectures used in sequence\ntransduction. We show that this model can generate fluent, coherent\nmulti-sentence paragraphs and even whole Wikipedia articles. When given\nreference documents, we show it can extract relevant factual information as\nreflected in perplexity, ROUGE scores and human evaluations.", "published": "2018-01-30 20:07:01", "link": "http://arxiv.org/abs/1801.10198v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Manuscripts in Time and Space: Experiments in Scriptometrics on an Old\n  French Corpus", "abstract": "Witnesses of medieval literary texts, preserved in manuscript, are layered\nobjects , being almost exclusively copies of copies. This results in multiple\nand hard to distinguish linguistic strata -- the author's scripta interacting\nwith the scriptae of the various scribes -- in a context where literary written\nlanguage is already a dialectal hybrid. Moreover, no single linguistic\nphenomenon allows to distinguish between different scriptae, and only the\ncombination of multiple characteristics is likely to be significant [9] -- but\nwhich ones? The most common approach is to search for these features in a set\nof previously selected texts, that are supposed to be representative of a given\nscripta. This can induce a circularity, in which texts are used to select\nfeatures that in turn characterise them as belonging to a linguistic area. To\ncounter this issue, this paper offers an unsupervised and corpus-based\napproach, in which clustering methods are applied to an Old French corpus to\nidentify main divisions and groups. Ultimately, scriptometric profiles are\nbuilt for each of them.", "published": "2018-01-30 09:07:05", "link": "http://arxiv.org/abs/1802.01429v1", "categories": ["cs.CL", "stat.AP"], "primary_category": "cs.CL"}
{"title": "Accelerating recurrent neural network language model based online speech\n  recognition system", "abstract": "This paper presents methods to accelerate recurrent neural network based\nlanguage models (RNNLMs) for online speech recognition systems. Firstly, a\nlossy compression of the past hidden layer outputs (history vector) with\ncaching is introduced in order to reduce the number of LM queries. Next, RNNLM\ncomputations are deployed in a CPU-GPU hybrid manner, which computes each layer\nof the model on a more advantageous platform. The added overhead by data\nexchanges between CPU and GPU is compensated through a frame-wise batching\nstrategy. The performance of the proposed methods evaluated on LibriSpeech test\nsets indicates that the reduction in history vector precision improves the\naverage recognition speed by 1.23 times with minimum degradation in accuracy.\nOn the other hand, the CPU-GPU hybrid parallelization enables RNNLM based\nreal-time recognition with a four times improvement in speed.", "published": "2018-01-30 06:58:50", "link": "http://arxiv.org/abs/1801.09866v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Machine Learning Approach to Quantitative Prosopography", "abstract": "Prosopography is an investigation of the common characteristics of a group of\npeople in history, by a collective study of their lives. It involves a study of\nbiographies to solve historical problems. If such biographies are unavailable,\nsurviving documents and secondary biographical data are used. Quantitative\nprosopography involves analysis of information from a wide variety of sources\nabout \"ordinary people\". In this paper, we present a machine learning framework\nfor automatically designing a people gazetteer which forms the basis of\nquantitative prosopographical research. The gazetteer is learnt from the noisy\ntext of newspapers using a Named Entity Recognizer (NER). It is capable of\nidentifying influential people from it by making use of a custom designed\nInfluential Person Index (IPI). Our corpus comprises of 14020 articles from a\nlocal newspaper, \"The Sun\", published from New York in 1896. Some influential\npeople identified by our algorithm include Captain Donald Hankey (an English\nsoldier), Dame Nellie Melba (an Australian operatic soprano), Hugh Allan (a\nCanadian shipping magnate) and Sir Hugh John McDonald (the first Prime Minister\nof Canada).", "published": "2018-01-30 16:13:55", "link": "http://arxiv.org/abs/1801.10080v1", "categories": ["cs.DL", "cs.CL"], "primary_category": "cs.DL"}
{"title": "TransRev: Modeling Reviews as Translations from Users to Items", "abstract": "The text of a review expresses the sentiment a customer has towards a\nparticular product. This is exploited in sentiment analysis where machine\nlearning models are used to predict the review score from the text of the\nreview. Furthermore, the products costumers have purchased in the past are\nindicative of the products they will purchase in the future. This is what\nrecommender systems exploit by learning models from purchase information to\npredict the items a customer might be interested in. We propose TransRev, an\napproach to the product recommendation problem that integrates ideas from\nrecommender systems, sentiment analysis, and multi-relational learning into a\njoint learning objective. TransRev learns vector representations for users,\nitems, and reviews. The embedding of a review is learned such that (a) it\nperforms well as input feature of a regression model for sentiment prediction;\nand (b) it always translates the reviewer embedding to the embedding of the\nreviewed items. This allows TransRev to approximate a review embedding at test\ntime as the difference of the embedding of each item and the user embedding.\nThe approximated review embedding is then used with the regression model to\npredict the review score for each item. TransRev outperforms state of the art\nrecommender systems on a large number of benchmark data sets. Moreover, it is\nable to retrieve, for each user and item, the review text from the training set\nwhose embedding is most similar to the approximated review embedding.", "published": "2018-01-30 17:01:01", "link": "http://arxiv.org/abs/1801.10095v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Cross-type Biomedical Named Entity Recognition with Deep Multi-Task\n  Learning", "abstract": "Motivation: State-of-the-art biomedical named entity recognition (BioNER)\nsystems often require handcrafted features specific to each entity type, such\nas genes, chemicals and diseases. Although recent studies explored using neural\nnetwork models for BioNER to free experts from manual feature engineering, the\nperformance remains limited by the available training data for each entity\ntype. Results: We propose a multi-task learning framework for BioNER to\ncollectively use the training data of different types of entities and improve\nthe performance on each of them. In experiments on 15 benchmark BioNER\ndatasets, our multi-task model achieves substantially better performance\ncompared with state-of-the-art BioNER systems and baseline neural sequence\nlabeling models. Further analysis shows that the large performance gains come\nfrom sharing character- and word-level information among relevant biomedical\nentities across differently labeled corpora.", "published": "2018-01-30 04:44:14", "link": "http://arxiv.org/abs/1801.09851v4", "categories": ["cs.IR", "cs.CL", "stat.ML"], "primary_category": "cs.IR"}
{"title": "Preparation of Improved Turkish DataSet for Sentiment Analysis in Social\n  Media", "abstract": "A public dataset, with a variety of properties suitable for sentiment\nanalysis [1], event prediction, trend detection and other text mining\napplications, is needed in order to be able to successfully perform analysis\nstudies. The vast majority of data on social media is text-based and it is not\npossible to directly apply machine learning processes into these raw data,\nsince several different processes are required to prepare the data before the\nimplementation of the algorithms. For example, different misspellings of same\nword enlarge the word vector space unnecessarily, thereby it leads to reduce\nthe success of the algorithm and increase the computational power requirement.\nThis paper presents an improved Turkish dataset with an effective spelling\ncorrection algorithm based on Hadoop [2]. The collected data is recorded on the\nHadoop Distributed File System and the text based data is processed by\nMapReduce programming model. This method is suitable for the storage and\nprocessing of large sized text based social media data. In this study, movie\nreviews have been automatically recorded with Apache ManifoldCF (MCF) [3] and\ndata clusters have been created. Various methods compared such as Levenshtein\nand Fuzzy String Matching have been proposed to create a public dataset from\ncollected data. Experimental results show that the proposed algorithm, which\ncan be used as an open source dataset in sentiment analysis studies, have been\nperformed successfully to the detection and correction of spelling errors.", "published": "2018-01-30 13:18:51", "link": "http://arxiv.org/abs/1801.09975v2", "categories": ["cs.CL", "cs.IR", "cs.SI"], "primary_category": "cs.CL"}
{"title": "The New Modality: Emoji Challenges in Prediction, Anticipation, and\n  Retrieval", "abstract": "Over the past decade, emoji have emerged as a new and widespread form of\ndigital communication, spanning diverse social networks and spoken languages.\nWe propose to treat these ideograms as a new modality in their own right,\ndistinct in their semantic structure from both the text in which they are often\nembedded as well as the images which they resemble. As a new modality, emoji\npresent rich novel possibilities for representation and interaction. In this\npaper, we explore the challenges that arise naturally from considering the\nemoji modality through the lens of multimedia research. Specifically, the ways\nin which emoji can be related to other common modalities such as text and\nimages. To do so, we first present a large scale dataset of real-world emoji\nusage collected from Twitter. This dataset contains examples of both text-emoji\nand image-emoji relationships. We present baseline results on the challenge of\npredicting emoji from both text and images, using state-of-the-art neural\nnetworks. Further, we offer a first consideration into the problem of how to\naccount for new, unseen emoji - a relevant issue as the emoji vocabulary\ncontinues to expand on a yearly basis. Finally, we present results for\nmultimedia retrieval using emoji as queries.", "published": "2018-01-30 23:19:49", "link": "http://arxiv.org/abs/1801.10253v2", "categories": ["cs.CL", "cs.IR", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Information Measures for Microphone Arrays", "abstract": "We propose a novel information-theoretic approach for evaluating microphone\narrays that relies on the array physics and geometry rather than the underlying\nbeamforming algorithm. The analogy between Multiple-Input-Multiple-Output\n(MIMO) wireless communication channel and the acoustic channel of microphone\narrays is exploited to define information measures of microphone arrays, which\nprovide upper bounds of the information rate of the microphone array system.", "published": "2018-01-30 18:26:49", "link": "http://arxiv.org/abs/1801.10128v1", "categories": ["cs.IT", "eess.AS", "math.IT", "62B10, 94A12"], "primary_category": "cs.IT"}
