{"title": "Understanding Pre-Editing for Black-Box Neural Machine Translation", "abstract": "Pre-editing is the process of modifying the source text (ST) so that it can\nbe translated by machine translation (MT) in a better quality. Despite the\nunpredictability of black-box neural MT (NMT), pre-editing has been deployed in\nvarious practical MT use cases. Although many studies have demonstrated the\neffectiveness of pre-editing methods for particular settings, thus far, a deep\nunderstanding of what pre-editing is and how it works for black-box NMT is\nlacking. To elicit such understanding, we extensively investigated human\npre-editing practices. We first implemented a protocol to incrementally record\nthe minimum edits for each ST and collected 6,652 instances of pre-editing\nacross three translation directions, two MT systems, and four text domains. We\nthen analysed the instances from three perspectives: the characteristics of the\npre-edited ST, the diversity of pre-editing operations, and the impact of the\npre-editing operations on NMT outputs. Our findings include the following: (1)\nenhancing the explicitness of the meaning of an ST and its syntactic structure\nis more important for obtaining better translations than making the ST shorter\nand simpler, and (2) although the impact of pre-editing on NMT is generally\nunpredictable, there are some tendencies of changes in the NMT outputs\ndepending on the editing operation types.", "published": "2021-02-05 02:01:18", "link": "http://arxiv.org/abs/2102.02955v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GraphPlan: Story Generation by Planning with Event Graph", "abstract": "Story generation is a task that aims to automatically produce multiple\nsentences to make up a meaningful story. This task is challenging because it\nrequires high-level understanding of semantic meaning of sentences and\ncausality of story events. Naive sequence-to-sequence models generally fail to\nacquire such knowledge, as the logical correctness can hardly be guaranteed in\na text generation model without the strategic planning. In this paper, we focus\non planning a sequence of events assisted by event graphs, and use the events\nto guide the generator. Instead of using a sequence-to-sequence model to output\na storyline as in some existing works, we propose to generate an event sequence\nby walking on an event graph. The event graphs are built automatically based on\nthe corpus. To evaluate the proposed approach, we conduct human evaluation both\non event planning and story generation. Based on large-scale human annotation\nresults, our proposed approach is shown to produce more logically correct event\nsequences and stories.", "published": "2021-02-05 03:18:55", "link": "http://arxiv.org/abs/2102.02977v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Model Agnostic Answer Reranking System for Adversarial Question\n  Answering", "abstract": "While numerous methods have been proposed as defenses against adversarial\nexamples in question answering (QA), these techniques are often model specific,\nrequire retraining of the model, and give only marginal improvements in\nperformance over vanilla models. In this work, we present a simple\nmodel-agnostic approach to this problem that can be applied directly to any QA\nmodel without any retraining. Our method employs an explicit answer candidate\nreranking mechanism that scores candidate answers on the basis of their content\noverlap with the question before making the final prediction. Combined with a\nstrong base QAmodel, our method outperforms state-of-the-art defense\ntechniques, calling into question how well these techniques are actually doing\nand strong these adversarial testbeds are.", "published": "2021-02-05 06:18:12", "link": "http://arxiv.org/abs/2102.03016v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Commonsense Knowledge Aware Concept Selection For Diverse and\n  Informative Visual Storytelling", "abstract": "Visual storytelling is a task of generating relevant and interesting stories\nfor given image sequences. In this work we aim at increasing the diversity of\nthe generated stories while preserving the informative content from the images.\nWe propose to foster the diversity and informativeness of a generated story by\nusing a concept selection module that suggests a set of concept candidates.\nThen, we utilize a large scale pre-trained model to convert concepts and images\ninto full stories. To enrich the candidate concepts, a commonsense knowledge\ngraph is created for each image sequence from which the concept candidates are\nproposed. To obtain appropriate concepts from the graph, we propose two novel\nmodules that consider the correlation among candidate concepts and the\nimage-concept correlation. Extensive automatic and human evaluation results\ndemonstrate that our model can produce reasonable concepts. This enables our\nmodel to outperform the previous models by a large margin on the diversity and\ninformativeness of the story, while retaining the relevance of the story to the\nimage sequence.", "published": "2021-02-05 02:15:28", "link": "http://arxiv.org/abs/2102.02963v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "RpBERT: A Text-image Relation Propagation-based BERT Model for\n  Multimodal NER", "abstract": "Recently multimodal named entity recognition (MNER) has utilized images to\nimprove the accuracy of NER in tweets. However, most of the multimodal methods\nuse attention mechanisms to extract visual clues regardless of whether the text\nand image are relevant. Practically, the irrelevant text-image pairs account\nfor a large proportion in tweets. The visual clues that are unrelated to the\ntexts will exert uncertain or even negative effects on multimodal model\nlearning. In this paper, we introduce a method of text-image relation\npropagation into the multimodal BERT model. We integrate soft or hard gates to\nselect visual clues and propose a multitask algorithm to train on the MNER\ndatasets. In the experiments, we deeply analyze the changes in visual attention\nbefore and after the use of text-image relation propagation. Our model achieves\nstate-of-the-art performance on the MNER datasets.", "published": "2021-02-05 02:45:30", "link": "http://arxiv.org/abs/2102.02967v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Spell Correction for Azerbaijani Language using Deep Neural Networks", "abstract": "Spell correction is used to detect and correct orthographic mistakes in\ntexts. Most of the time, traditional dictionary lookup with string similarity\nmethods is suitable for the languages that have a less complex structure such\nas the English language. However, the Azerbaijani language has a more complex\nstructure and due to its morphological structure, the derivation of words is\nplenty that several words are derived from adding suffices, affixes to the\nwords. Therefore, in this paper sequence to sequence model with an attention\nmechanism is used to develop spelling correction for Azerbaijani. Total 12000\nwrong and correct sentence pairs used for training, and the model is tested on\n1000 real-world misspelled words and F1-score results are 75% for distance 0,\n90% for distance 1, and 96% for distance 2.", "published": "2021-02-05 15:02:35", "link": "http://arxiv.org/abs/2102.03218v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Think you have Solved Direct-Answer Question Answering? Try ARC-DA, the\n  Direct-Answer AI2 Reasoning Challenge", "abstract": "We present the ARC-DA dataset, a direct-answer (\"open response\", \"freeform\")\nversion of the ARC (AI2 Reasoning Challenge) multiple-choice dataset. While ARC\nhas been influential in the community, its multiple-choice format is\nunrepresentative of real-world questions, and multiple choice formats can be\nparticularly susceptible to artifacts. The ARC-DA dataset addresses these\nconcerns by converting questions to direct-answer format using a combination of\ncrowdsourcing and expert review. The resulting dataset contains 2985 questions\nwith a total of 8436 valid answers (questions typically have more than one\nvalid answer). ARC-DA is one of the first DA datasets of natural questions that\noften require reasoning, and where appropriate question decompositions are not\nevident from the questions themselves. We describe the conversion approach\ntaken, appropriate evaluation metrics, and several strong models. Although\nhigh, the best scores (81% GENIE, 61.4% F1, 63.2% ROUGE-L) still leave\nconsiderable room for improvement. In addition, the dataset provides a natural\nsetting for new research on explanation, as many questions require reasoning to\nconstruct answers. We hope the dataset spurs further advances in complex\nquestion-answering by the community. ARC-DA is available at\nhttps://allenai.org/data/arc-da", "published": "2021-02-05 17:41:43", "link": "http://arxiv.org/abs/2102.03315v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi-Label Annotation of Chest Abdomen Pelvis Computed Tomography Text\n  Reports Using Deep Learning", "abstract": "Purpose: To develop high throughput multi-label annotators for body (chest,\nabdomen, and pelvis) Computed Tomography (CT) reports that can be applied\nacross a variety of abnormalities, organs, and disease states.\n  Approach: We used a dictionary approach to develop rule-based algorithms\n(RBA) for extraction of disease labels from radiology text reports. We targeted\nthree organ systems (lungs/pleura, liver/gallbladder, kidneys/ureters) with\nfour diseases per system based on their prevalence in our dataset. To expand\nthe algorithms beyond pre-defined keywords, attention-guided recurrent neural\nnetworks (RNN) were trained using the RBA-extracted labels to classify reports\nas being positive for one or more diseases or normal for each organ system.\nConfounding effects on model performance were evaluated using random\ninitialization or pre-trained embedding as well as different sizes of training\ndatasets. Performance was evaluated using the receiver operating characteristic\n(ROC) area under the curve (AUC) against 2,158 manually obtained labels.\n  Results: Our models extracted disease labels from 261,229 radiology reports\nof 112,501 unique subjects. Pre-trained models outperformed random\ninitialization across all diseases. As the training dataset size was reduced,\nperformance was robust except for a few diseases with relatively small number\nof cases. Pre-trained classification AUCs achieved > 0.95 for all five disease\noutcomes across all three organ systems.\n  Conclusions: Our label-extracting pipeline was able to encompass a variety of\ncases and diseases by generalizing beyond strict rules with exceptional\naccuracy. This method can be easily adapted to enable automated labeling of\nhospital-scale medical data sets for training image-based disease classifiers.", "published": "2021-02-05 02:07:39", "link": "http://arxiv.org/abs/2102.02959v5", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Smart Proofs via Smart Contracts: Succinct and Informative Mathematical\n  Derivations via Decentralized Markets", "abstract": "Modern mathematics is built on the idea that proofs should be translatable\ninto formal proofs, whose validity is an objective question, decidable by a\ncomputer. Yet, in practice, proofs are informal and may omit many details. An\nagent considers a proof valid if they trust that it could be expanded into a\nmachine-verifiable proof. A proof's validity can thus become a subjective\nmatter and lead to a debate, which may be difficult to settle. Hence, while the\nconcept of valid proof is well-defined, the process to establish validity is\nitself a complex multi-agent problem.\n  We introduce the SPRIG protocol. SPRIG allows agents to propose and verify\nsuccinct and informative proofs in a decentralized fashion; the trust is\nestablished by agents being able to request more details in the proof steps;\ndebates, if they arise, must isolate details of proofs and, if they persist, go\ndown to machine-level details, where they are automatically settled. A\nstructure of bounties and stakes is set to incentivize agents to act in good\nfaith.\n  We propose a game-theoretic discussion of SPRIG, showing how agents with\nvarious types of information interact, leading to a proof tree with an\nappropriate level of detail and to the invalidation of wrong proofs, and we\ndiscuss resilience against various attacks. We then analyze a simplified model,\ncharacterize its equilibria and compute the agents' level of trust.\n  SPRIG is designed to run as a smart contract on a blockchain platform. This\nallows anonymous agents to participate in the verification debate, and to\ncontribute with their information. The smart contract mediates the\ninteractions, settles debates, and guarantees that bounties and stakes are paid\nas specified.\n  SPRIG enables new applications, such as the issuance of bounties for open\nproblems, and the creation of derivatives markets, allowing agents to inject\nmore information pertaining to proofs.", "published": "2021-02-05 08:00:19", "link": "http://arxiv.org/abs/2102.03044v4", "categories": ["cs.GT", "cs.CL", "cs.LO", "cs.SI", "03F07, 03F20, 91A05, 91A06, 91A07, 91A10, 91A11, 91A24, 91A26,\n  91A27, 91A28, 91A80, 68N17, 68P05, 68V15, 68V20, 68V30", "F.4"], "primary_category": "cs.GT"}
{"title": "Two-Stage Augmentation and Adaptive CTC Fusion for Improved Robustness\n  of Multi-Stream End-to-End ASR", "abstract": "Performance degradation of an Automatic Speech Recognition (ASR) system is\ncommonly observed when the test acoustic condition is different from training.\nHence, it is essential to make ASR systems robust against various environmental\ndistortions, such as background noises and reverberations. In a multi-stream\nparadigm, improving robustness takes account of handling a variety of unseen\nsingle-stream conditions and inter-stream dynamics. Previously, a practical\ntwo-stage training strategy was proposed within multi-stream end-to-end ASR,\nwhere Stage-2 formulates the multi-stream model with features from Stage-1\nUniversal Feature Extractor (UFE). In this paper, as an extension, we introduce\na two-stage augmentation scheme focusing on mismatch scenarios: Stage-1\nAugmentation aims to address single-stream input varieties with data\naugmentation techniques; Stage-2 Time Masking applies temporal masks on UFE\nfeatures of randomly selected streams to simulate diverse stream combinations.\nDuring inference, we also present adaptive Connectionist Temporal\nClassification (CTC) fusion with the help of hierarchical attention mechanisms.\nExperiments have been conducted on two datasets, DIRHA and AMI, as a\nmulti-stream scenario. Compared with the previous training strategy,\nsubstantial improvements are reported with relative word error rate reductions\nof 29.7-59.3% across several unseen stream combinations.", "published": "2021-02-05 08:36:58", "link": "http://arxiv.org/abs/2102.03055v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Understanding Emails and Drafting Responses -- An Approach Using GPT-3", "abstract": "Providing computer systems with the ability to understand and generate\nnatural language has long been a challenge of engineers. Recent progress in\nnatural language processing (NLP), like the GPT-3 language model released by\nOpenAI, has made both possible to an extent. In this paper, we explore the\npossibility of rationalising email communication using GPT-3. First, we\ndemonstrate the technical feasibility of understanding incoming emails and\ngenerating responses, drawing on literature from the disciplines of software\nengineering as well as data science. Second, we apply knowledge from both\nbusiness studies and, again, software engineering to identify ways to tackle\nchallenges we encountered. Third, we argue for the economic viability of such a\nsolution by analysing costs and market demand. We conclude that applying GPT-3\nto rationalising email communication is feasible both technically and\neconomically.", "published": "2021-02-05 08:56:42", "link": "http://arxiv.org/abs/2102.03062v3", "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.AI"}
{"title": "Intermediate Loss Regularization for CTC-based Speech Recognition", "abstract": "We present a simple and efficient auxiliary loss function for automatic\nspeech recognition (ASR) based on the connectionist temporal classification\n(CTC) objective. The proposed objective, an intermediate CTC loss, is attached\nto an intermediate layer in the CTC encoder network. This intermediate CTC loss\nwell regularizes CTC training and improves the performance requiring only small\nmodification of the code and small and no overhead during training and\ninference, respectively. In addition, we propose to combine this intermediate\nCTC loss with stochastic depth training, and apply this combination to a\nrecently proposed Conformer network. We evaluate the proposed method on various\ncorpora, reaching word error rate (WER) 9.9% on the WSJ corpus and character\nerror rate (CER) 5.2% on the AISHELL-1 corpus respectively, based on CTC greedy\nsearch without a language model. Especially, the AISHELL-1 task is comparable\nto other state-of-the-art ASR systems based on auto-regressive decoder with\nbeam search.", "published": "2021-02-05 15:01:03", "link": "http://arxiv.org/abs/2102.03216v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Minimum projective linearizations of trees in linear time", "abstract": "The Minimum Linear Arrangement problem (MLA) consists of finding a mapping\n$\\pi$ from vertices of a graph to distinct integers that minimizes\n$\\sum_{\\{u,v\\}\\in E}|\\pi(u) - \\pi(v)|$. In that setting, vertices are often\nassumed to lie on a horizontal line and edges are drawn as semicircles above\nsaid line. For trees, various algorithms are available to solve the problem in\npolynomial time in $n=|V|$. There exist variants of the MLA in which the\narrangements are constrained. Iordanskii, and later Hochberg and Stallmann\n(HS), put forward $O(n)$-time algorithms that solve the problem when\narrangements are constrained to be planar (also known as one-page book\nembeddings). We also consider linear arrangements of rooted trees that are\nconstrained to be projective (planar embeddings where the root is not covered\nby any edge). Gildea and Temperley (GT) sketched an algorithm for projective\narrangements which they claimed runs in $O(n)$ but did not provide any\njustification of its cost. In contrast, Park and Levy claimed that GT's\nalgorithm runs in $O(n \\log d_{max})$ where $d_{max}$ is the maximum degree but\ndid not provide sufficient detail. Here we correct an error in HS's algorithm\nfor the planar case, show its relationship with the projective case, and derive\nsimple algorithms for the projective and planar cases that run without a doubt\nin $O(n)$ time.", "published": "2021-02-05 16:35:38", "link": "http://arxiv.org/abs/2102.03277v6", "categories": ["cs.DS", "cs.CL", "cs.DM"], "primary_category": "cs.DS"}
{"title": "SkillBot: Identifying Risky Content for Children in Alexa Skills", "abstract": "Many households include children who use voice personal assistants (VPA) such\nas Amazon Alexa. Children benefit from the rich functionalities of VPAs and\nthird-party apps but are also exposed to new risks in the VPA ecosystem. In\nthis paper, we first investigate \"risky\" child-directed voice apps that contain\ninappropriate content or ask for personal information through voice\ninteractions. We build SkillBot - a natural language processing (NLP)-based\nsystem to automatically interact with VPA apps and analyze the resulting\nconversations. We find 28 risky child-directed apps and maintain a growing\ndataset of 31,966 non-overlapping app behaviors collected from 3,434 Alexa\napps. Our findings suggest that although child-directed VPA apps are subject to\nstricter policy requirements and more intensive vetting, children remain\nvulnerable to inappropriate content and privacy violations. We then conduct a\nuser study showing that parents are concerned about the identified risky apps.\nMany parents do not believe that these apps are available and designed for\nfamilies/kids, although these apps are actually published in Amazon's \"Kids\"\nproduct category. We also find that parents often neglect basic precautions\nsuch as enabling parental controls on Alexa devices. Finally, we identify a\nnovel risk in the VPA ecosystem: confounding utterances, or voice commands\nshared by multiple apps that may cause a user to interact with a different app\nthan intended. We identify 4,487 confounding utterances, including 581 shared\nby child-directed and non-child-directed apps. We find that 27% of these\nconfounding utterances prioritize invoking a non-child-directed app over a\nchild-directed app. This indicates that children are at real risk of\naccidentally invoking non-child-directed apps due to confounding utterances.", "published": "2021-02-05 19:07:39", "link": "http://arxiv.org/abs/2102.03382v2", "categories": ["cs.MA", "cs.CL", "cs.CR", "cs.HC"], "primary_category": "cs.MA"}
{"title": "Exploring the Limits of Few-Shot Link Prediction in Knowledge Graphs", "abstract": "Real-world knowledge graphs are often characterized by low-frequency\nrelations - a challenge that has prompted an increasing interest in few-shot\nlink prediction methods. These methods perform link prediction for a set of new\nrelations, unseen during training, given only a few example facts of each\nrelation at test time. In this work, we perform a systematic study on a\nspectrum of models derived by generalizing the current state of the art for\nfew-shot link prediction, with the goal of probing the limits of learning in\nthis few-shot setting. We find that a simple zero-shot baseline - which ignores\nany relation-specific information - achieves surprisingly strong performance.\nMoreover, experiments on carefully crafted synthetic datasets show that having\nonly a few examples of a relation fundamentally limits models from using\nfine-grained structural information and only allows for exploiting the\ncoarse-grained positional information of entities. Together, our findings\nchallenge the implicit assumptions and inductive biases of prior work and\nhighlight new directions for research in this area.", "published": "2021-02-05 21:04:31", "link": "http://arxiv.org/abs/2102.03419v1", "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.LG", "cs.SI"], "primary_category": "cs.AI"}
{"title": "Beam-Guided TasNet: An Iterative Speech Separation Framework with\n  Multi-Channel Output", "abstract": "Time-domain audio separation network (TasNet) has achieved remarkable\nperformance in blind source separation (BSS). Classic multi-channel speech\nprocessing framework employs signal estimation and beamforming. For example,\nBeam-TasNet links multi-channel convolutional TasNet (MC-Conv-TasNet) with\nminimum variance distortionless response (MVDR) beamforming, which leverages\nthe strong modeling ability of data-driven network and boosts the performance\nof beamforming with an accurate estimation of speech statistics. Such\nintegration can be viewed as a directed acyclic graph by accepting\nmulti-channel input and generating multi-source output. In this paper, we\ndesign a \"multi-channel input, multi-channel multi-source output\" (MIMMO)\nspeech separation system entitled \"Beam-Guided TasNet\", where MC-Conv-TasNet\nand MVDR can interact and promote each other more compactly under a directed\ncyclic flow. Specifically, the first stage uses Beam-TasNet to generate\nestimated single-speaker signals, which favors the separation in the second\nstage. The proposed framework facilitates iterative signal refinement with the\nguide of beamforming and seeks to reach the upper bound of the MVDR-based\nmethods. Experimental results on the spatialized WSJ0-2MIX demonstrate that the\nBeam-Guided TasNet has achieved an SDR of 21.5 dB, exceeding the baseline\nBeam-TasNet by 4.1 dB under the same model size and narrowing the gap with the\noracle signal-based MVDR to 2 dB.", "published": "2021-02-05 04:55:41", "link": "http://arxiv.org/abs/2102.02998v6", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Estimation of Microphone Clusters in Acoustic Sensor Networks using\n  Unsupervised Federated Learning", "abstract": "In this paper we present a privacy-aware method for estimating\nsource-dominated microphone clusters in the context of acoustic sensor networks\n(ASNs). The approach is based on clustered federated learning which we adapt to\nunsupervised scenarios by employing a light-weight autoencoder model. The model\nis further optimized for training on very scarce data. In order to best harness\nthe benefits of clustered microphone nodes in ASN applications, a method for\nthe computation of cluster membership values is introduced. We validate the\nperformance of the proposed approach using clustering-based measures and a\nnetwork-wide classification task.", "published": "2021-02-05 11:21:16", "link": "http://arxiv.org/abs/2102.03109v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Diversity-Robust Acoustic Feature Signatures Based on Multiscale Fractal\n  Dimension for Similarity Search of Environmental Sounds", "abstract": "This paper proposes new acoustic feature signatures based on the multiscale\nfractal dimension (MFD), which are robust against the diversity of\nenvironmental sounds, for the content-based similarity search. The diversity of\nsound sources and acoustic compositions is a typical feature of environmental\nsounds. Several acoustic features have been proposed for environmental sounds.\nAmong them is the widely-used Mel-Frequency Cepstral Coefficients (MFCCs),\nwhich describes frequency-domain features. However, in addition to these\nfeatures in the frequency domain, environmental sounds have other important\nfeatures in the time domain with various time scales. In our previous paper, we\nproposed enhanced multiscale fractal dimension signature (EMFD) for\nenvironmental sounds. This paper extends EMFD by using the kernel density\nestimation method, which results in better performance of the similarity search\ntasks. Furthermore, it newly proposes another acoustic feature signature based\non MFD, namely very-long-range multiscale fractal dimension signature (MFD-VL).\nThe MFD-VL signature describes several features of the time-varying envelope\nfor long periods of time. The MFD-VL signature has stability and robustness\nagainst background noise and small fluctuations in the parameters of sound\nsources, which are produced in field recordings. We discuss the effectiveness\nof these signatures in the similarity sound search by comparing with acoustic\nfeatures proposed in the DCASE 2018 challenges. Due to the unique\ndescriptiveness of our proposed signatures, we confirmed the signatures are\neffective when they are used with other acoustic features.", "published": "2021-02-05 02:37:21", "link": "http://arxiv.org/abs/2102.02964v2", "categories": ["cs.SD", "cs.IR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Benchmarking of eight recurrent neural network variants for breath phase\n  and adventitious sound detection on a self-developed open-access lung sound\n  database-HF_Lung_V1", "abstract": "A reliable, remote, and continuous real-time respiratory sound monitor with\nautomated respiratory sound analysis ability is urgently required in many\nclinical scenarios-such as in monitoring disease progression of coronavirus\ndisease 2019-to replace conventional auscultation with a handheld stethoscope.\nHowever, a robust computerized respiratory sound analysis algorithm has not yet\nbeen validated in practical applications. In this study, we developed a lung\nsound database (HF_Lung_V1) comprising 9,765 audio files of lung sounds\n(duration of 15 s each), 34,095 inhalation labels, 18,349 exhalation labels,\n13,883 continuous adventitious sound (CAS) labels (comprising 8,457 wheeze\nlabels, 686 stridor labels, and 4,740 rhonchi labels), and 15,606 discontinuous\nadventitious sound labels (all crackles). We conducted benchmark tests for long\nshort-term memory (LSTM), gated recurrent unit (GRU), bidirectional LSTM\n(BiLSTM), bidirectional GRU (BiGRU), convolutional neural network (CNN)-LSTM,\nCNN-GRU, CNN-BiLSTM, and CNN-BiGRU models for breath phase detection and\nadventitious sound detection. We also conducted a performance comparison\nbetween the LSTM-based and GRU-based models, between unidirectional and\nbidirectional models, and between models with and without a CNN. The results\nrevealed that these models exhibited adequate performance in lung sound\nanalysis. The GRU-based models outperformed, in terms of F1 scores and areas\nunder the receiver operating characteristic curves, the LSTM-based models in\nmost of the defined tasks. Furthermore, all bidirectional models outperformed\ntheir unidirectional counterparts. Finally, the addition of a CNN improved the\naccuracy of lung sound analysis, especially in the CAS detection tasks.", "published": "2021-02-05 08:21:28", "link": "http://arxiv.org/abs/2102.03049v3", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "White-box Audio VST Effect Programming", "abstract": "Learning to program an audio production VST plugin is a time consuming\nprocess, usually obtained through inefficient trial and error and only mastered\nafter extensive user experience. We propose a white-box, iterative system that\nprovides step-by-step instructions for applying audio effects to change a\nuser's audio signal towards a desired sound. We apply our system to Xfer\nRecords Serum: currently one of the most popular and complex VST synthesizers\nused by the audio production community. Our results indicate that our system is\nconsistently able to provide useful feedback for a variety of different audio\neffects and synthesizer presets.", "published": "2021-02-05 13:45:17", "link": "http://arxiv.org/abs/2102.03170v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Real-time Denoising and Dereverberation with Tiny Recurrent U-Net", "abstract": "Modern deep learning-based models have seen outstanding performance\nimprovement with speech enhancement tasks. The number of parameters of\nstate-of-the-art models, however, is often too large to be deployed on devices\nfor real-world applications. To this end, we propose Tiny Recurrent U-Net\n(TRU-Net), a lightweight online inference model that matches the performance of\ncurrent state-of-the-art models. The size of the quantized version of TRU-Net\nis 362 kilobytes, which is small enough to be deployed on edge devices. In\naddition, we combine the small-sized model with a new masking method called\nphase-aware $\\beta$-sigmoid mask, which enables simultaneous denoising and\ndereverberation. Results of both objective and subjective evaluations have\nshown that our model can achieve competitive performance with the current\nstate-of-the-art models on benchmark datasets using fewer parameters by orders\nof magnitude.", "published": "2021-02-05 14:46:41", "link": "http://arxiv.org/abs/2102.03207v3", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multi-Task Self-Supervised Pre-Training for Music Classification", "abstract": "Deep learning is very data hungry, and supervised learning especially\nrequires massive labeled data to work well. Machine listening research often\nsuffers from limited labeled data problem, as human annotations are costly to\nacquire, and annotations for audio are time consuming and less intuitive.\nBesides, models learned from labeled dataset often embed biases specific to\nthat particular dataset. Therefore, unsupervised learning techniques become\npopular approaches in solving machine listening problems. Particularly, a\nself-supervised learning technique utilizing reconstructions of multiple\nhand-crafted audio features has shown promising results when it is applied to\nspeech domain such as emotion recognition and automatic speech recognition\n(ASR). In this paper, we apply self-supervised and multi-task learning methods\nfor pre-training music encoders, and explore various design choices including\nencoder architectures, weighting mechanisms to combine losses from multiple\ntasks, and worker selections of pretext tasks. We investigate how these design\nchoices interact with various downstream music classification tasks. We find\nthat using various music specific workers altogether with weighting mechanisms\nto balance the losses during pre-training helps improve and generalize to the\ndownstream tasks.", "published": "2021-02-05 15:19:58", "link": "http://arxiv.org/abs/2102.03229v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Learning Audio-Visual Correlations from Variational Cross-Modal\n  Generation", "abstract": "People can easily imagine the potential sound while seeing an event. This\nnatural synchronization between audio and visual signals reveals their\nintrinsic correlations. To this end, we propose to learn the audio-visual\ncorrelations from the perspective of cross-modal generation in a\nself-supervised manner, the learned correlations can be then readily applied in\nmultiple downstream tasks such as the audio-visual cross-modal localization and\nretrieval. We introduce a novel Variational AutoEncoder (VAE) framework that\nconsists of Multiple encoders and a Shared decoder (MS-VAE) with an additional\nWasserstein distance constraint to tackle the problem. Extensive experiments\ndemonstrate that the optimized latent representation of the proposed MS-VAE can\neffectively learn the audio-visual correlations and can be readily applied in\nmultiple audio-visual downstream tasks to achieve competitive performance even\nwithout any given label information during training.", "published": "2021-02-05 21:27:00", "link": "http://arxiv.org/abs/2102.03424v2", "categories": ["cs.CV", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.CV"}
