{"title": "Deep-speare: A Joint Neural Model of Poetic Language, Meter and Rhyme", "abstract": "In this paper, we propose a joint architecture that captures language, rhyme\nand meter for sonnet modelling. We assess the quality of generated poems using\ncrowd and expert judgements. The stress and rhyme models perform very well, as\ngenerated poems are largely indistinguishable from human-written poems. Expert\nevaluation, however, reveals that a vanilla language model captures meter\nimplicitly, and that machine-generated poems still underperform in terms of\nreadability and emotion. Our research shows the importance expert evaluation\nfor poetry generation, and that future research should look beyond rhyme/meter\nand focus on poetic language.", "published": "2018-07-10 06:26:24", "link": "http://arxiv.org/abs/1807.03491v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-D Kneser-Ney Smoothing Preserving the Original Marginal\n  Distributions", "abstract": "Smoothing is an essential tool in many NLP tasks, therefore numerous\ntechniques have been developed for this purpose in the past. One of the most\nwidely used smoothing methods are the Kneser-Ney smoothing (KNS) and its\nvariants, including the Modified Kneser-Ney smoothing (MKNS), which are widely\nconsidered to be among the best smoothing methods available. Although when\ncreating the original KNS the intention of the authors was to develop such a\nsmoothing method that preserves the marginal distributions of the original\nmodel, this property was not maintained when developing the MKNS.\n  In this article I would like to overcome this and propose such a refined\nversion of the MKNS that preserves these marginal distributions while keeping\nthe advantages of both previous versions. Beside its advantageous properties,\nthis novel smoothing method is shown to achieve about the same results as the\nMKNS in a standard language modelling task.", "published": "2018-07-10 12:04:54", "link": "http://arxiv.org/abs/1807.03583v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Paired Comparison Sentiment Scores", "abstract": "The method of paired comparisons is an established method in psychology. In\nthis article, it is applied to obtain continuous sentiment scores for words\nfrom comparisons made by test persons. We created an initial lexicon with\n$n=199$ German words from a two-fold all-pair comparison experiment with ten\ndifferent test persons. From the probabilistic models taken into account, the\nlogistic model showed the best agreement with the results of the comparison\nexperiment. The initial lexicon can then be used in different ways. One is to\ncreate special purpose sentiment lexica through the addition of arbitrary words\nthat are compared with some of the initial words by test persons. A\ncross-validation experiment suggests that only about 18 two-fold comparisons\nare necessary to estimate the score of a new, yet unknown word, provided these\nwords are selected by a modification of a method by Silverstein & Farrell.\nAnother application of the initial lexicon is the evaluation of automatically\ncreated corpus-based lexica. By such an evaluation, we compared the\ncorpus-based lexica SentiWS, SenticNet, and SentiWordNet, of which SenticNet 4\nperformed best. This technical report is a corrected and extended version of a\npresentation made at the ICDM Sentire workshop in 2016.", "published": "2018-07-10 12:25:59", "link": "http://arxiv.org/abs/1807.03591v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting the Hierarchical Multiscale LSTM", "abstract": "Hierarchical Multiscale LSTM (Chung et al., 2016a) is a state-of-the-art\nlanguage model that learns interpretable structure from character-level input.\nSuch models can provide fertile ground for (cognitive) computational\nlinguistics studies. However, the high complexity of the architecture, training\nprocedure and implementations might hinder its applicability. We provide a\ndetailed reproduction and ablation study of the architecture, shedding light on\nsome of the potential caveats of re-purposing complex deep-learning\narchitectures. We further show that simplifying certain aspects of the\narchitecture can in fact improve its performance. We also investigate the\nlinguistic units (segments) learned by various levels of the model, and argue\nthat their quality does not correlate with the overall performance of the model\non language modeling.", "published": "2018-07-10 12:46:30", "link": "http://arxiv.org/abs/1807.03595v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Linguistic Characteristics of Censorable Language on SinaWeibo", "abstract": "This paper investigates censorship from a linguistic perspective. We collect\na corpus of censored and uncensored posts on a number of topics, build a\nclassifier that predicts censorship decisions independent of discussion topics.\nOur investigation reveals that the strongest linguistic indicator of censored\ncontent of our corpus is its readability.", "published": "2018-07-10 14:02:35", "link": "http://arxiv.org/abs/1807.03654v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enriching Knowledge Bases with Counting Quantifiers", "abstract": "Information extraction traditionally focuses on extracting relations between\nidentifiable entities, such as <Monterey, locatedIn, California>. Yet, texts\noften also contain Counting information, stating that a subject is in a\nspecific relation with a number of objects, without mentioning the objects\nthemselves, for example, \"California is divided into 58 counties\". Such\ncounting quantifiers can help in a variety of tasks such as query answering or\nknowledge base curation, but are neglected by prior work. This paper develops\nthe first full-fledged system for extracting counting information from text,\ncalled CINEX. We employ distant supervision using fact counts from a knowledge\nbase as training seeds, and develop novel techniques for dealing with several\nchallenges: (i) non-maximal training seeds due to the incompleteness of\nknowledge bases, (ii) sparse and skewed observations in text sources, and (iii)\nhigh diversity of linguistic patterns. Experiments with five human-evaluated\nrelations show that CINEX can achieve 60% average precision for extracting\ncounting information. In a large-scale experiment, we demonstrate the potential\nfor knowledge base enrichment by applying CINEX to 2,474 frequent relations in\nWikidata. CINEX can assert the existence of 2.5M facts for 110 distinct\nrelations, which is 28% more than the existing Wikidata facts for these\nrelations.", "published": "2018-07-10 14:03:23", "link": "http://arxiv.org/abs/1807.03656v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IAM at CLEF eHealth 2018: Concept Annotation and Coding in French Death\n  Certificates", "abstract": "In this paper, we describe the approach and results for our participation in\nthe task 1 (multilingual information extraction) of the CLEF eHealth 2018\nchallenge. We addressed the task of automatically assigning ICD-10 codes to\nFrench death certificates. We used a dictionary-based approach using materials\nprovided by the task organizers. The terms of the ICD-10 terminology were\nnormalized, tokenized and stored in a tree data structure. The Levenshtein\ndistance was used to detect typos. Frequent abbreviations were detected by\nmanually creating a small set of them. Our system achieved an F-score of 0.786\n(precision: 0.794, recall: 0.779). These scores were substantially higher than\nthe average score of the systems that participated in the challenge.", "published": "2018-07-10 14:31:25", "link": "http://arxiv.org/abs/1807.03674v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Difficulty Controllable Generation of Reading Comprehension Questions", "abstract": "We investigate the difficulty levels of questions in reading comprehension\ndatasets such as SQuAD, and propose a new question generation setting, named\nDifficulty-controllable Question Generation (DQG). Taking as input a sentence\nin the reading comprehension paragraph and some of its text fragments (i.e.,\nanswers) that we want to ask questions about, a DQG method needs to generate\nquestions each of which has a given text fragment as its answer, and meanwhile\nthe generation is under the control of specified difficulty labels---the output\nquestions should satisfy the specified difficulty as much as possible. To solve\nthis task, we propose an end-to-end framework to generate questions of\ndesignated difficulty levels by exploring a few important intuitions. For\nevaluation, we prepared the first dataset of reading comprehension questions\nwith difficulty labels. The results show that the question generated by our\nframework not only have better quality under the metrics like BLEU, but also\ncomply with the specified difficulty labels.", "published": "2018-07-10 12:10:16", "link": "http://arxiv.org/abs/1807.03586v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Latent Alignment and Variational Attention", "abstract": "Neural attention has become central to many state-of-the-art models in\nnatural language processing and related domains. Attention networks are an\neasy-to-train and effective method for softly simulating alignment; however,\nthe approach does not marginalize over latent alignments in a probabilistic\nsense. This property makes it difficult to compare attention to other alignment\napproaches, to compose it with probabilistic models, and to perform posterior\ninference conditioned on observed data. A related latent approach, hard\nattention, fixes these issues, but is generally harder to train and less\naccurate. This work considers variational attention networks, alternatives to\nsoft and hard attention for learning latent variable alignment models, with\ntighter approximation bounds based on amortized variational inference. We\nfurther propose methods for reducing the variance of gradients to make these\napproaches computationally feasible. Experiments show that for machine\ntranslation and visual question answering, inefficient exact latent variable\nmodels outperform standard neural attention, but these gains go away when using\nhard attention based training. On the other hand, variational attention retains\nmost of the performance gain but with training speed comparable to neural\nattention.", "published": "2018-07-10 16:59:12", "link": "http://arxiv.org/abs/1807.03756v2", "categories": ["stat.ML", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Universal Transformers", "abstract": "Recurrent neural networks (RNNs) sequentially process data by updating their\nstate with each new data point, and have long been the de facto choice for\nsequence modeling tasks. However, their inherently sequential computation makes\nthem slow to train. Feed-forward and convolutional architectures have recently\nbeen shown to achieve superior results on some sequence modeling tasks such as\nmachine translation, with the added advantage that they concurrently process\nall inputs in the sequence, leading to easy parallelization and faster training\ntimes. Despite these successes, however, popular feed-forward sequence models\nlike the Transformer fail to generalize in many simple tasks that recurrent\nmodels handle with ease, e.g. copying strings or even simple logical inference\nwhen the string or formula lengths exceed those observed at training time. We\npropose the Universal Transformer (UT), a parallel-in-time self-attentive\nrecurrent sequence model which can be cast as a generalization of the\nTransformer model and which addresses these issues. UTs combine the\nparallelizability and global receptive field of feed-forward sequence models\nlike the Transformer with the recurrent inductive bias of RNNs. We also add a\ndynamic per-position halting mechanism and find that it improves accuracy on\nseveral tasks. In contrast to the standard Transformer, under certain\nassumptions, UTs can be shown to be Turing-complete. Our experiments show that\nUTs outperform standard Transformers on a wide range of algorithmic and\nlanguage understanding tasks, including the challenging LAMBADA language\nmodeling task where UTs achieve a new state of the art, and machine translation\nwhere UTs achieve a 0.9 BLEU improvement over Transformers on the WMT14 En-De\ndataset.", "published": "2018-07-10 18:39:15", "link": "http://arxiv.org/abs/1807.03819v3", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Phase reconstruction from amplitude spectrograms based on\n  von-Mises-distribution deep neural network", "abstract": "This paper presents a deep neural network (DNN)-based phase reconstruction\nfrom amplitude spectrograms. In audio signal and speech processing, the\namplitude spectrogram is often used for processing, and the corresponding phase\nspectrogram is reconstructed from the amplitude spectrogram on the basis of the\nGriffin-Lim method. However, the Griffin-Lim method causes unnatural artifacts\nin synthetic speech. Addressing this problem, we introduce the\nvon-Mises-distribution DNN for phase reconstruction. The DNN is a generative\nmodel having the von Mises distribution that can model distributions of a\nperiodic variable such as a phase, and the model parameters of the DNN are\nestimated on the basis of the maximum likelihood criterion. Furthermore, we\npropose a group-delay loss for DNN training to make the predicted group delay\nclose to a natural group delay. The experimental results demonstrate that 1)\nthe trained DNN can predict group delay accurately more than phases themselves,\nand 2) our phase reconstruction methods achieve better speech quality than the\nconventional Griffin-Lim method.", "published": "2018-07-10 04:14:55", "link": "http://arxiv.org/abs/1807.03474v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Revisiting Synthesis Model of Sparse Audio Declipper", "abstract": "The state of the art in audio declipping has currently been achieved by SPADE\n(SParse Audio DEclipper) algorithm by Kiti\\'c et al. Until now, the\nsynthesis/sparse variant, S-SPADE, has been considered significantly slower\nthan its analysis/cosparse counterpart, A-SPADE. It turns out that the opposite\nis true: by exploiting a recent projection lemma, individual iterations of both\nalgorithms can be made equally computationally expensive, while S-SPADE tends\nto require considerably fewer iterations to converge. In this paper, the two\nalgorithms are compared across a range of parameters such as the window length,\nwindow overlap and redundancy of the transform. The experiments show that\nalthough S-SPADE typically converges faster, the average performance in terms\nof restoration quality is not superior to A-SPADE.", "published": "2018-07-10 13:18:20", "link": "http://arxiv.org/abs/1807.03612v2", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
