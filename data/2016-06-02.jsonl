{"title": "Single-Model Encoder-Decoder with Explicit Morphological Representation\n  for Reinflection", "abstract": "Morphological reinflection is the task of generating a target form given a\nsource form, a source tag and a target tag. We propose a new way of modeling\nthis task with neural encoder-decoder models. Our approach reduces the amount\nof required training data for this architecture and achieves state-of-the-art\nresults, making encoder-decoder models applicable to morphological reinflection\neven for low-resource languages. We further present a new automatic correction\nmethod for the outputs based on edit trees.", "published": "2016-06-02 08:57:14", "link": "http://arxiv.org/abs/1606.00589v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Matrix Factorization using Window Sampling and Negative Sampling for\n  Improved Word Representations", "abstract": "In this paper, we propose LexVec, a new method for generating distributed\nword representations that uses low-rank, weighted factorization of the Positive\nPoint-wise Mutual Information matrix via stochastic gradient descent, employing\na weighting scheme that assigns heavier penalties for errors on frequent\nco-occurrences while still accounting for negative co-occurrence. Evaluation on\nword similarity and analogy tasks shows that LexVec matches and often\noutperforms state-of-the-art methods on many of these tasks.", "published": "2016-06-02 19:35:46", "link": "http://arxiv.org/abs/1606.00819v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Source-LDA: Enhancing probabilistic topic models using prior knowledge\n  sources", "abstract": "A popular approach to topic modeling involves extracting co-occurring n-grams\nof a corpus into semantic themes. The set of n-grams in a theme represents an\nunderlying topic, but most topic modeling approaches are not able to label\nthese sets of words with a single n-gram. Such labels are useful for topic\nidentification in summarization systems. This paper introduces a novel approach\nto labeling a group of n-grams comprising an individual topic. The approach\ntaken is to complement the existing topic distributions over words with a known\ndistribution based on a predefined set of topics. This is done by integrating\nexisting labeled knowledge sources representing known potential topics into the\nprobabilistic topic model. These knowledge sources are translated into a\ndistribution and used to set the hyperparameters of the Dirichlet generated\ndistribution over words. In the inference these modified distributions guide\nthe convergence of the latent topics to conform with the complementary\ndistributions. This approach ensures that the topic inference process is\nconsistent with existing knowledge. The label assignment from the complementary\nknowledge sources are then transferred to the latent topics of the corpus. The\nresults show both accurate label assignment to topics as well as improved topic\ngeneration than those obtained using various labeling approaches based off\nLatent Dirichlet allocation (LDA).", "published": "2016-06-02 08:15:15", "link": "http://arxiv.org/abs/1606.00577v3", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Stochastic Structured Prediction under Bandit Feedback", "abstract": "Stochastic structured prediction under bandit feedback follows a learning\nprotocol where on each of a sequence of iterations, the learner receives an\ninput, predicts an output structure, and receives partial feedback in form of a\ntask loss evaluation of the predicted structure. We present applications of\nthis learning scenario to convex and non-convex objectives for structured\nprediction and analyze them as stochastic first-order methods. We present an\nexperimental evaluation on problems of natural language processing over\nexponential output spaces, and compare convergence speed across different\nobjectives under the practical criterion of optimal task performance on\ndevelopment data and the optimization-theoretic criterion of minimal squared\ngradient norm. Best results under both criteria are obtained for a non-convex\nobjective for pairwise preference learning under bandit feedback.", "published": "2016-06-02 16:06:29", "link": "http://arxiv.org/abs/1606.00739v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Multiresolution Recurrent Neural Networks: An Application to Dialogue\n  Response Generation", "abstract": "We introduce the multiresolution recurrent neural network, which extends the\nsequence-to-sequence framework to model natural language generation as two\nparallel discrete stochastic processes: a sequence of high-level coarse tokens,\nand a sequence of natural language tokens. There are many ways to estimate or\nlearn the high-level coarse tokens, but we argue that a simple extraction\nprocedure is sufficient to capture a wealth of high-level discourse semantics.\nSuch procedure allows training the multiresolution recurrent neural network by\nmaximizing the exact joint log-likelihood over both sequences. In contrast to\nthe standard log- likelihood objective w.r.t. natural language tokens (word\nperplexity), optimizing the joint log-likelihood biases the model towards\nmodeling high-level abstractions. We apply the proposed model to the task of\ndialogue response generation in two challenging domains: the Ubuntu technical\nsupport domain, and Twitter conversations. On Ubuntu, the model outperforms\ncompeting approaches by a substantial margin, achieving state-of-the-art\nresults according to both automatic evaluation metrics and a human evaluation\nstudy. On Twitter, the model appears to generate more relevant and on-topic\nresponses according to automatic evaluation metrics. Finally, our experiments\ndemonstrate that the proposed model is more adept at overcoming the sparsity of\nnatural language and is better able to capture long-term structure.", "published": "2016-06-02 17:37:31", "link": "http://arxiv.org/abs/1606.00776v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE", "stat.ML", "I.5.1; I.2.7"], "primary_category": "cs.CL"}
