{"title": "ScienceExamCER: A High-Density Fine-Grained Science-Domain Corpus for\n  Common Entity Recognition", "abstract": "Named entity recognition identifies common classes of entities in text, but\nthese entity labels are generally sparse, limiting utility to downstream tasks.\nIn this work we present ScienceExamCER, a densely-labeled semantic\nclassification corpus of 133k mentions in the science exam domain where nearly\nall (96%) of content words have been annotated with one or more fine-grained\nsemantic class labels including taxonomic groups, meronym groups, verb/action\ngroups, properties and values, and synonyms. Semantic class labels are drawn\nfrom a manually-constructed fine-grained typology of 601 classes generated\nthrough a data-driven analysis of 4,239 science exam questions. We show an\noff-the-shelf BERT-based named entity recognition model modified for\nmulti-label classification achieves an accuracy of 0.85 F1 on this task,\nsuggesting strong utility for downstream tasks in science domain question\nanswering requiring densely-labeled semantic classification.", "published": "2019-11-24 00:08:09", "link": "http://arxiv.org/abs/1911.10436v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Out-Of-Domain Utterance Detection with Data Augmentation Based\n  on Word Embeddings", "abstract": "For most intelligent assistant systems, it is essential to have a mechanism\nthat detects out-of-domain (OOD) utterances automatically to handle noisy input\nproperly. One typical approach would be introducing a separate class that\ncontains OOD utterance examples combined with in-domain text samples into the\nclassifier. However, since OOD utterances are usually unseen to the training\ndatasets, the detection performance largely depends on the quality of the\nattached OOD text data with restricted sizes of samples due to computing\nlimits. In this paper, we study how augmented OOD data based on sampling impact\nOOD utterance detection with a small sample size. We hypothesize that OOD\nutterance samples chosen randomly can increase the coverage of unknown OOD\nutterance space and enhance detection accuracy if they are more dispersed.\nExperiments show that given the same dataset with the same OOD sample size, the\nOOD utterance detection performance improves when OOD samples are more\nspread-out.", "published": "2019-11-24 00:30:30", "link": "http://arxiv.org/abs/1911.10439v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Retrieve Reasoning Paths over Wikipedia Graph for Question\n  Answering", "abstract": "Answering questions that require multi-hop reasoning at web-scale\nnecessitates retrieving multiple evidence documents, one of which often has\nlittle lexical or semantic relationship to the question. This paper introduces\na new graph-based recurrent retrieval approach that learns to retrieve\nreasoning paths over the Wikipedia graph to answer multi-hop open-domain\nquestions. Our retriever model trains a recurrent neural network that learns to\nsequentially retrieve evidence paragraphs in the reasoning path by conditioning\non the previously retrieved documents. Our reader model ranks the reasoning\npaths and extracts the answer span included in the best reasoning path.\nExperimental results show state-of-the-art results in three open-domain QA\ndatasets, showcasing the effectiveness and robustness of our method. Notably,\nour method achieves significant improvement in HotpotQA, outperforming the\nprevious best model by more than 14 points.", "published": "2019-11-24 08:27:42", "link": "http://arxiv.org/abs/1911.10470v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CopyMTL: Copy Mechanism for Joint Extraction of Entities and Relations\n  with Multi-Task Learning", "abstract": "Joint extraction of entities and relations has received significant attention\ndue to its potential of providing higher performance for both tasks. Among\nexisting methods, CopyRE is effective and novel, which uses a\nsequence-to-sequence framework and copy mechanism to directly generate the\nrelation triplets. However, it suffers from two fatal problems. The model is\nextremely weak at differing the head and tail entity, resulting in inaccurate\nentity extraction. It also cannot predict multi-token entities (e.g.\n\\textit{Steven Jobs}). To address these problems, we give a detailed analysis\nof the reasons behind the inaccurate entity extraction problem, and then\npropose a simple but extremely effective model structure to solve this problem.\nIn addition, we propose a multi-task learning framework equipped with copy\nmechanism, called CopyMTL, to allow the model to predict multi-token entities.\nExperiments reveal the problems of CopyRE and show that our model achieves\nsignificant improvement over the current state-of-the-art method by 9% in NYT\nand 16% in WebNLG (F1 score). Our code is available at\nhttps://github.com/WindChimeRan/CopyMTL", "published": "2019-11-24 00:24:32", "link": "http://arxiv.org/abs/1911.10438v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Task-Oriented Dialog Systems that Consider Multiple Appropriate\n  Responses under the Same Context", "abstract": "Conversations have an intrinsic one-to-many property, which means that\nmultiple responses can be appropriate for the same dialog context. In\ntask-oriented dialogs, this property leads to different valid dialog policies\ntowards task completion. However, none of the existing task-oriented dialog\ngeneration approaches takes this property into account. We propose a\nMulti-Action Data Augmentation (MADA) framework to utilize the one-to-many\nproperty to generate diverse appropriate dialog responses. Specifically, we\nfirst use dialog states to summarize the dialog history, and then discover all\npossible mappings from every dialog state to its different valid system\nactions. During dialog system training, we enable the current dialog state to\nmap to all valid system actions discovered in the previous process to create\nadditional state-action pairs. By incorporating these additional pairs, the\ndialog policy learns a balanced action distribution, which further guides the\ndialog model to generate diverse responses. Experimental results show that the\nproposed framework consistently improves dialog policy diversity, and results\nin improved response diversity and appropriateness. Our model obtains\nstate-of-the-art results on MultiWOZ.", "published": "2019-11-24 09:32:55", "link": "http://arxiv.org/abs/1911.10484v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Two Causal Principles for Improving Visual Dialog", "abstract": "This paper unravels the design tricks adopted by us, the champion team\nMReaL-BDAI, for Visual Dialog Challenge 2019: two causal principles for\nimproving Visual Dialog (VisDial). By \"improving\", we mean that they can\npromote almost every existing VisDial model to the state-of-the-art performance\non the leader-board. Such a major improvement is only due to our careful\ninspection on the causality behind the model and data, finding that the\ncommunity has overlooked two causalities in VisDial. Intuitively, Principle 1\nsuggests: we should remove the direct input of the dialog history to the answer\nmodel, otherwise a harmful shortcut bias will be introduced; Principle 2 says:\nthere is an unobserved confounder for history, question, and answer, leading to\nspurious correlations from training data. In particular, to remove the\nconfounder suggested in Principle 2, we propose several causal intervention\nalgorithms, which make the training fundamentally different from the\ntraditional likelihood estimation. Note that the two principles are\nmodel-agnostic, so they are applicable in any VisDial model. The code is\navailable at https://github.com/simpleshinobu/visdial-principles.", "published": "2019-11-24 10:35:35", "link": "http://arxiv.org/abs/1911.10496v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Causally Denoise Word Embeddings Using Half-Sibling Regression", "abstract": "Distributional representations of words, also known as word vectors, have\nbecome crucial for modern natural language processing tasks due to their wide\napplications. Recently, a growing body of word vector postprocessing algorithm\nhas emerged, aiming to render off-the-shelf word vectors even stronger. In line\nwith these investigations, we introduce a novel word vector postprocessing\nscheme under a causal inference framework. Concretely, the postprocessing\npipeline is realized by Half-Sibling Regression (HSR), which allows us to\nidentify and remove confounding noise contained in word vectors. Compared to\nprevious work, our proposed method has the advantages of interpretability and\ntransparency due to its causal inference grounding. Evaluated on a battery of\nstandard lexical-level evaluation tasks and downstream sentiment analysis\ntasks, our method reaches state-of-the-art performance.", "published": "2019-11-24 13:06:19", "link": "http://arxiv.org/abs/1911.10524v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Neural Storyboard Artist: Visualizing Stories with Coherent Image\n  Sequences", "abstract": "A storyboard is a sequence of images to illustrate a story containing\nmultiple sentences, which has been a key process to create different story\nproducts. In this paper, we tackle a new multimedia task of automatic\nstoryboard creation to facilitate this process and inspire human artists.\nInspired by the fact that our understanding of languages is based on our past\nexperience, we propose a novel inspire-and-create framework with a\nstory-to-image retriever that selects relevant cinematic images for inspiration\nand a storyboard creator that further refines and renders images to improve the\nrelevancy and visual consistency. The proposed retriever dynamically employs\ncontextual information in the story with hierarchical attentions and applies\ndense visual-semantic matching to accurately retrieve and ground images. The\ncreator then employs three rendering steps to increase the flexibility of\nretrieved images, which include erasing irrelevant regions, unifying styles of\nimages and substituting consistent characters. We carry out extensive\nexperiments on both in-domain and out-of-domain visual story datasets. The\nproposed model achieves better quantitative performance than the\nstate-of-the-art baselines for storyboard creation. Qualitative visualizations\nand user studies further verify that our approach can create high-quality\nstoryboards even for stories in the wild.", "published": "2019-11-24 05:06:41", "link": "http://arxiv.org/abs/1911.10460v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Improving EEG based Continuous Speech Recognition", "abstract": "In this paper we introduce various techniques to improve the performance of\nelectroencephalography (EEG) features based continuous speech recognition (CSR)\nsystems. A connectionist temporal classification (CTC) based automatic speech\nrecognition (ASR) system was implemented for performing recognition. We\nintroduce techniques to initialize the weights of the recurrent layers in the\nencoder of the CTC model with more meaningful weights rather than with random\nweights and we make use of an external language model to improve the beam\nsearch during decoding time.\n  We finally study the problem of predicting articulatory features from EEG\nfeatures in this paper.", "published": "2019-11-24 16:00:49", "link": "http://arxiv.org/abs/1911.11610v6", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
