{"title": "NoPropaganda at SemEval-2020 Task 11: A Borrowed Approach to Sequence\n  Tagging and Text Classification", "abstract": "This paper describes our contribution to SemEval-2020 Task 11: Detection Of\nPropaganda Techniques In News Articles. We start with simple LSTM baselines and\nmove to an autoregressive transformer decoder to predict long continuous\npropaganda spans for the first subtask. We also adopt an approach from relation\nextraction by enveloping spans mentioned above with special tokens for the\nsecond subtask of propaganda technique classification. Our models report an\nF-score of 44.6% and a micro-averaged F-score of 58.2% for those tasks\naccordingly.", "published": "2020-07-25 11:35:57", "link": "http://arxiv.org/abs/2007.12913v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bollyrics: Automatic Lyrics Generator for Romanised Hindi", "abstract": "Song lyrics convey a meaningful story in a creative manner with complex\nrhythmic patterns. Researchers have been successful in generating and analyisng\nlyrics for poetry and songs in English and Chinese. But there are no works\nwhich explore the Hindi language datasets. Given the popularity of Hindi songs\nacross the world and the ambiguous nature of romanized Hindi script, we propose\nBollyrics, an automatic lyric generator for romanized Hindi songs. We propose\nsimple techniques to capture rhyming patterns before and during the model\ntraining process in Hindi language. The dataset and codes are available\npublicly at https://github.com/lingo-iitgn/Bollyrics.", "published": "2020-07-25 12:02:26", "link": "http://arxiv.org/abs/2007.12916v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Duluth at SemEval-2020 Task 12: Offensive Tweet Identification in\n  English with Logistic Regression", "abstract": "This paper describes the Duluth systems that participated in SemEval--2020\nTask 12, Multilingual Offensive Language Identification in Social Media\n(OffensEval--2020). We participated in the three English language tasks. Our\nsystems provide a simple Machine Learning baseline using logistic regression.\nWe trained our models on the distantly supervised training data made available\nby the task organizers and used no other resources. As might be expected we did\nnot rank highly in the comparative evaluation: 79th of 85 in Task A, 34th of 43\nin Task B, and 24th of 39 in Task C. We carried out a qualitative analysis of\nour results and found that the class labels in the gold standard data are\nsomewhat noisy. We hypothesize that the extremely high accuracy (> 90%) of the\ntop ranked systems may reflect methods that learn the training data very well\nbut may not generalize to the task of identifying offensive language in\nEnglish. This analysis includes examples of tweets that despite being mildly\nredacted are still offensive.", "published": "2020-07-25 14:49:31", "link": "http://arxiv.org/abs/2007.12946v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Duluth at SemEval-2019 Task 6: Lexical Approaches to Identify and\n  Categorize Offensive Tweets", "abstract": "This paper describes the Duluth systems that participated in SemEval--2019\nTask 6, Identifying and Categorizing Offensive Language in Social Media\n(OffensEval). For the most part these systems took traditional Machine Learning\napproaches that built classifiers from lexical features found in manually\nlabeled training data. However, our most successful system for classifying a\ntweet as offensive (or not) was a rule-based black--list approach, and we also\nexperimented with combining the training data from two different but related\nSemEval tasks. Our best systems in each of the three OffensEval tasks placed in\nthe middle of the comparative evaluation, ranking 57th of 103 in task A, 39th\nof 75 in task B, and 44th of 65 in task C.", "published": "2020-07-25 14:56:10", "link": "http://arxiv.org/abs/2007.12949v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Constructing a Testbed for Psychometric Natural Language Processing", "abstract": "Psychometric measures of ability, attitudes, perceptions, and beliefs are\ncrucial for understanding user behaviors in various contexts including health,\nsecurity, e-commerce, and finance. Traditionally, psychometric dimensions have\nbeen measured and collected using survey-based methods. Inferring such\nconstructs from user-generated text could afford opportunities for timely,\nunobtrusive, collection and analysis. In this paper, we describe our efforts to\nconstruct a corpus for psychometric natural language processing (NLP). We\ndiscuss our multi-step process to align user text with their survey-based\nresponse items and provide an overview of the resulting testbed which\nencompasses survey-based psychometric measures and accompanying user-generated\ntext from over 8,500 respondents. We report preliminary results on the use of\nthe text to categorize/predict users' survey response labels. We also discuss\nthe important implications of our work and resulting testbed for future\npsychometric NLP research.", "published": "2020-07-25 16:29:24", "link": "http://arxiv.org/abs/2007.12969v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Insightful Assistant: AI-compatible Operation Graph Representations for\n  Enhancing Industrial Conversational Agents", "abstract": "Advances in voice-controlled assistants paved the way into the consumer\nmarket. For professional or industrial use, the capabilities of such assistants\nare too limited or too time-consuming to implement due to the higher complexity\nof data, possible AI-based operations, and requests. In the light of these\ndeficits, this paper presents Insightful Assistant---a pipeline concept based\non a novel operation graph representation resulting from the intents detected.\nUsing a predefined set of semantically annotated (executable) functions, each\nnode of the operation graph is assigned to a function for execution. Besides\nbasic operations, such functions can contain artificial intelligence (AI) based\noperations (e.g., anomaly detection). The result is then visualized to the user\naccording to type and extracted user preferences in an automated way. We\nfurther collected a unique crowd-sourced set of 869 requests, each with four\ndifferent variants expected visualization, for an industrial dataset. The\nevaluation of our proof-of-concept prototype on this dataset shows its\nfeasibility: it achieves an accuracy of up to 95.0% (74.5%) for simple\n(complex) request detection with different variants and a top3-accuracy up to\n95.4% for data-/user-adaptive visualization.", "published": "2020-07-25 13:46:58", "link": "http://arxiv.org/abs/2007.12929v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Storywrangler: A massive exploratorium for sociolinguistic, cultural,\n  socioeconomic, and political timelines using Twitter", "abstract": "In real-time, social media data strongly imprints world events, popular\nculture, and day-to-day conversations by millions of ordinary people at a scale\nthat is scarcely conventionalized and recorded. Vitally, and absent from many\nstandard corpora such as books and news archives, sharing and commenting\nmechanisms are native to social media platforms, enabling us to quantify social\namplification (i.e., popularity) of trending storylines and contemporary\ncultural phenomena. Here, we describe Storywrangler, a natural language\nprocessing instrument designed to carry out an ongoing, day-scale curation of\nover 100 billion tweets containing roughly 1 trillion 1-grams from 2008 to\n2021. For each day, we break tweets into unigrams, bigrams, and trigrams\nspanning over 100 languages. We track n-gram usage frequencies, and generate\nZipf distributions, for words, hashtags, handles, numerals, symbols, and\nemojis. We make the data set available through an interactive time series\nviewer, and as downloadable time series and daily distributions. Although\nStorywrangler leverages Twitter data, our method of extracting and tracking\ndynamic changes of n-grams can be extended to any similar social media\nplatform. We showcase a few examples of the many possible avenues of study we\naim to enable including how social amplification can be visualized through\n'contagiograms'. We also present some example case studies that bridge n-gram\ntime series with disparate data sources to explore sociotechnical dynamics of\nfamous individuals, box office success, and social unrest.", "published": "2020-07-25 18:09:22", "link": "http://arxiv.org/abs/2007.12988v5", "categories": ["cs.SI", "cs.CL", "physics.soc-ph"], "primary_category": "cs.SI"}
{"title": "Unsupervised Subword Modeling Using Autoregressive Pretraining and\n  Cross-Lingual Phone-Aware Modeling", "abstract": "This study addresses unsupervised subword modeling, i.e., learning feature\nrepresentations that can distinguish subword units of a language. The proposed\napproach adopts a two-stage bottleneck feature (BNF) learning framework,\nconsisting of autoregressive predictive coding (APC) as a front-end and a\nDNN-BNF model as a back-end. APC pretrained features are set as input features\nto a DNN-BNF model. A language-mismatched ASR system is used to provide\ncross-lingual phone labels for DNN-BNF model training. Finally, BNFs are\nextracted as the subword-discriminative feature representation. A second aim of\nthis work is to investigate the robustness of our approach's effectiveness to\ndifferent amounts of training data. The results on Libri-light and the\nZeroSpeech 2017 databases show that APC is effective in front-end feature\npretraining. Our whole system outperforms the state of the art on both\ndatabases. Cross-lingual phone labels for English data by a Dutch ASR\noutperform those by a Mandarin ASR, possibly linked to the larger similarity of\nDutch compared to Mandarin with English. Our system is less sensitive to\ntraining data amount when the training data is over 50 hours. APC pretraining\nleads to a reduction of needed training material from over 5,000 hours to\naround 200 hours with little performance degradation.", "published": "2020-07-25 19:41:41", "link": "http://arxiv.org/abs/2007.13002v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Effect of Text Processing Steps on Twitter Sentiment Classification\n  using Word Embedding", "abstract": "Processing of raw text is the crucial first step in text classification and\nsentiment analysis. However, text processing steps are often performed using\noff-the-shelf routines and pre-built word dictionaries without optimizing for\ndomain, application, and context. This paper investigates the effect of seven\ntext processing scenarios on a particular text domain (Twitter) and application\n(sentiment classification). Skip gram-based word embeddings are developed to\ninclude Twitter colloquial words, emojis, and hashtag keywords that are often\nremoved for being unavailable in conventional literature corpora. Our\nexperiments reveal negative effects on sentiment classification of two common\ntext processing steps: 1) stop word removal and 2) averaging of word vectors to\nrepresent individual tweets. New effective steps for 1) including non-ASCII\nemoji characters, 2) measuring word importance from word embedding, 3)\naggregating word vectors into a tweet embedding, and 4) developing linearly\nseparable feature space have been proposed to optimize the sentiment\nclassification pipeline. The best combination of text processing steps yields\nthe highest average area under the curve (AUC) of 88.4 (+/-0.4) in classifying\n14,640 tweets with three sentiment labels. Word selection from context-driven\nword embedding reveals that only the ten most important words in Tweets\ncumulatively yield over 98% of the maximum accuracy. Results demonstrate a\nmeans for data-driven selection of important words in tweet classification as\nopposed to using pre-built word dictionaries. The proposed tweet embedding is\nrobust to and alleviates the need for several text processing steps.", "published": "2020-07-25 22:44:00", "link": "http://arxiv.org/abs/2007.13027v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploring Deep Hybrid Tensor-to-Vector Network Architectures for\n  Regression Based Speech Enhancement", "abstract": "This paper investigates different trade-offs between the number of model\nparameters and enhanced speech qualities by employing several deep\ntensor-to-vector regression models for speech enhancement. We find that a\nhybrid architecture, namely CNN-TT, is capable of maintaining a good quality\nperformance with a reduced model parameter size. CNN-TT is composed of several\nconvolutional layers at the bottom for feature extraction to improve speech\nquality and a tensor-train (TT) output layer on the top to reduce model\nparameters. We first derive a new upper bound on the generalization power of\nthe convolutional neural network (CNN) based vector-to-vector regression\nmodels. Then, we provide experimental evidence on the Edinburgh noisy speech\ncorpus to demonstrate that, in single-channel speech enhancement, CNN\noutperforms DNN at the expense of a small increment of model sizes. Besides,\nCNN-TT slightly outperforms the CNN counterpart by utilizing only 32\\% of the\nCNN model parameters. Besides, further performance improvement can be attained\nif the number of CNN-TT parameters is increased to 44\\% of the CNN model size.\nFinally, our experiments of multi-channel speech enhancement on a simulated\nnoisy WSJ0 corpus demonstrate that our proposed hybrid CNN-TT architecture\nachieves better results than both DNN and CNN models in terms of\nbetter-enhanced speech qualities and smaller parameter sizes.", "published": "2020-07-25 22:21:05", "link": "http://arxiv.org/abs/2007.13024v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.NE", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Robust Front-End for Multi-Channel ASR using Flow-Based Density\n  Estimation", "abstract": "For multi-channel speech recognition, speech enhancement techniques such as\ndenoising or dereverberation are conventionally applied as a front-end\nprocessor. Deep learning-based front-ends using such techniques require aligned\nclean and noisy speech pairs which are generally obtained via data simulation.\nRecently, several joint optimization techniques have been proposed to train the\nfront-end without parallel data within an end-to-end automatic speech\nrecognition (ASR) scheme. However, the ASR objective is sub-optimal and\ninsufficient for fully training the front-end, which still leaves room for\nimprovement. In this paper, we propose a novel approach which incorporates\nflow-based density estimation for the robust front-end using non-parallel clean\nand noisy speech. Experimental results on the CHiME-4 dataset show that the\nproposed method outperforms the conventional techniques where the front-end is\ntrained only with ASR objective.", "published": "2020-07-25 10:35:54", "link": "http://arxiv.org/abs/2007.12903v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Quasi-Periodic Parallel WaveGAN: A Non-autoregressive Raw Waveform\n  Generative Model with Pitch-dependent Dilated Convolution Neural Network", "abstract": "In this paper, we propose a quasi-periodic parallel WaveGAN (QPPWG) waveform\ngenerative model, which applies a quasi-periodic (QP) structure to a parallel\nWaveGAN (PWG) model using pitch-dependent dilated convolution networks\n(PDCNNs). PWG is a small-footprint GAN-based raw waveform generative model,\nwhose generation time is much faster than real time because of its compact\nmodel and non-autoregressive (non-AR) and non-causal mechanisms. Although PWG\nachieves high-fidelity speech generation, the generic and simple network\narchitecture lacks pitch controllability for an unseen auxiliary fundamental\nfrequency ($F_{0}$) feature such as a scaled $F_{0}$. To improve the pitch\ncontrollability and speech modeling capability, we apply a QP structure with\nPDCNNs to PWG, which introduces pitch information to the network by dynamically\nchanging the network architecture corresponding to the auxiliary $F_{0}$\nfeature. Both objective and subjective experimental results show that QPPWG\noutperforms PWG when the auxiliary $F_{0}$ feature is scaled. Moreover,\nanalyses of the intermediate outputs of QPPWG also show better tractability and\ninterpretability of QPPWG, which respectively models spectral and\nexcitation-like signals using the cascaded fixed and adaptive blocks of the QP\nstructure.", "published": "2020-07-25 15:34:35", "link": "http://arxiv.org/abs/2007.12955v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DD-CNN: Depthwise Disout Convolutional Neural Network for Low-complexity\n  Acoustic Scene Classification", "abstract": "This paper presents a Depthwise Disout Convolutional Neural Network (DD-CNN)\nfor the detection and classification of urban acoustic scenes. Specifically, we\nuse log-mel as feature representations of acoustic signals for the inputs of\nour network. In the proposed DD-CNN, depthwise separable convolution is used to\nreduce the network complexity. Besides, SpecAugment and Disout are used for\nfurther performance boosting. Experimental results demonstrate that our DD-CNN\ncan learn discriminative acoustic characteristics from audio fragments and\neffectively reduce the network complexity. Our DD-CNN was used for the\nlow-complexity acoustic scene classification task of the DCASE2020 Challenge,\nwhich achieves 92.04% accuracy on the validation set.", "published": "2020-07-25 06:02:20", "link": "http://arxiv.org/abs/2007.12864v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MP3 Compression To Diminish Adversarial Noise in End-to-End Speech\n  Recognition", "abstract": "Audio Adversarial Examples (AAE) represent specially created inputs meant to\ntrick Automatic Speech Recognition (ASR) systems into misclassification. The\npresent work proposes MP3 compression as a means to decrease the impact of\nAdversarial Noise (AN) in audio samples transcribed by ASR systems. To this\nend, we generated AAEs with the Fast Gradient Sign Method for an end-to-end,\nhybrid CTC-attention ASR system. Our method is then validated by two objective\nindicators: (1) Character Error Rates (CER) that measure the speech decoding\nperformance of four ASR models trained on uncompressed, as well as\nMP3-compressed data sets and (2) Signal-to-Noise Ratio (SNR) estimated for both\nuncompressed and MP3-compressed AAEs that are reconstructed in the time domain\nby feature inversion. We found that MP3 compression applied to AAEs indeed\nreduces the CER when compared to uncompressed AAEs. Moreover, feature-inverted\n(reconstructed) AAEs had significantly higher SNRs after MP3 compression,\nindicating that AN was reduced. In contrast to AN, MP3 compression applied to\nutterances augmented with regular noise resulted in more transcription errors,\ngiving further evidence that MP3 encoding is effective in diminishing only AN.", "published": "2020-07-25 09:25:32", "link": "http://arxiv.org/abs/2007.12892v1", "categories": ["eess.AS", "cs.CR", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Non-parallel Emotion Conversion using a Deep-Generative Hybrid Network\n  and an Adversarial Pair Discriminator", "abstract": "We introduce a novel method for emotion conversion in speech that does not\nrequire parallel training data. Our approach loosely relies on a cycle-GAN\nschema to minimize the reconstruction error from converting back and forth\nbetween emotion pairs. However, unlike the conventional cycle-GAN, our\ndiscriminator classifies whether a pair of input real and generated samples\ncorresponds to the desired emotion conversion (e.g., A to B) or to its inverse\n(B to A). We will show that this setup, which we refer to as a variational\ncycle-GAN (VC-GAN), is equivalent to minimizing the empirical KL divergence\nbetween the source features and their cyclic counterpart. In addition, our\ngenerator combines a trainable deep network with a fixed generative block to\nimplement a smooth and invertible transformation on the input features, in our\ncase, the fundamental frequency (F0) contour. This hybrid architecture\nregularizes our adversarial training procedure. We use crowd sourcing to\nevaluate both the emotional saliency and the quality of synthesized speech.\nFinally, we show that our model generalizes to new speakers by modifying speech\nproduced by Wavenet.", "published": "2020-07-25 13:50:00", "link": "http://arxiv.org/abs/2007.12932v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multi-speaker Emotion Conversion via Latent Variable Regularization and\n  a Chained Encoder-Decoder-Predictor Network", "abstract": "We propose a novel method for emotion conversion in speech based on a chained\nencoder-decoder-predictor neural network architecture. The encoder constructs a\nlatent embedding of the fundamental frequency (F0) contour and the spectrum,\nwhich we regularize using the Large Diffeomorphic Metric Mapping (LDDMM)\nregistration framework. The decoder uses this embedding to predict the modified\nF0 contour in a target emotional class. Finally, the predictor uses the\noriginal spectrum and the modified F0 contour to generate a corresponding\ntarget spectrum. Our joint objective function simultaneously optimizes the\nparameters of three model blocks. We show that our method outperforms the\nexisting state-of-the-art approaches on both, the saliency of emotion\nconversion and the quality of resynthesized speech. In addition, the LDDMM\nregularization allows our model to convert phrases that were not present in\ntraining, thus providing evidence for out-of-sample generalization.", "published": "2020-07-25 13:59:22", "link": "http://arxiv.org/abs/2007.12937v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Nonlinear ISA with Auxiliary Variables for Learning Speech\n  Representations", "abstract": "This paper extends recent work on nonlinear Independent Component Analysis\n(ICA) by introducing a theoretical framework for nonlinear Independent Subspace\nAnalysis (ISA) in the presence of auxiliary variables. Observed high\ndimensional acoustic features like log Mel spectrograms can be considered as\nsurface level manifestations of nonlinear transformations over individual\nmultivariate sources of information like speaker characteristics, phonological\ncontent etc. Under assumptions of energy based models we use the theory of\nnonlinear ISA to propose an algorithm that learns unsupervised speech\nrepresentations whose subspaces are independent and potentially highly\ncorrelated with the original non-stationary multivariate sources. We show how\nnonlinear ICA with auxiliary variables can be extended to a generic\nidentifiable model for subspaces as well while also providing sufficient\nconditions for the identifiability of these high dimensional subspaces. Our\nproposed methodology is generic and can be integrated with standard\nunsupervised approaches to learn speech representations with subspaces that can\ntheoretically capture independent higher order speech signals. We evaluate the\ngains of our algorithm when integrated with the Autoregressive Predictive\nDecoding (APC) model by showing empirical results on the speaker verification\nand phoneme recognition tasks.", "published": "2020-07-25 14:53:09", "link": "http://arxiv.org/abs/2007.12948v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Few-Shot Keyword Spotting With Prototypical Networks", "abstract": "Recognizing a particular command or a keyword, keyword spotting has been\nwidely used in many voice interfaces such as Amazon's Alexa and Google Home. In\norder to recognize a set of keywords, most of the recent deep learning based\napproaches use a neural network trained with a large number of samples to\nidentify certain pre-defined keywords. This restricts the system from\nrecognizing new, user-defined keywords. Therefore, we first formulate this\nproblem as a few-shot keyword spotting and approach it using metric learning.\nTo enable this research, we also synthesize and publish a Few-shot Google\nSpeech Commands dataset. We then propose a solution to the few-shot keyword\nspotting problem using temporal and dilated convolutions on prototypical\nnetworks. Our comparative experimental results demonstrate keyword spotting of\nnew keywords using just a small number of samples.", "published": "2020-07-25 20:17:56", "link": "http://arxiv.org/abs/2007.14463v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "AutoClip: Adaptive Gradient Clipping for Source Separation Networks", "abstract": "Clipping the gradient is a known approach to improving gradient descent, but\nrequires hand selection of a clipping threshold hyperparameter. We present\nAutoClip, a simple method for automatically and adaptively choosing a gradient\nclipping threshold, based on the history of gradient norms observed during\ntraining. Experimental results show that applying AutoClip results in improved\ngeneralization performance for audio source separation networks. Observation of\nthe training dynamics of a separation network trained with and without AutoClip\nshow that AutoClip guides optimization into smoother parts of the loss\nlandscape. AutoClip is very simple to implement and can be integrated readily\ninto a variety of applications across multiple domains.", "published": "2020-07-25 20:59:39", "link": "http://arxiv.org/abs/2007.14469v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Adaptive music: Automated music composition and distribution", "abstract": "Creativity, or the ability to produce new useful ideas, is commonly\nassociated to the human being; but there are many other examples in nature\nwhere this phenomenon can be observed. Inspired by this fact, in engineering\nand particularly in computational sciences, many different models have been\ndeveloped to tackle a number of problems.\n  Composing music, a form of art broadly present along the human history, is\nthe main topic addressed in this thesis. Taking advantage of the kind of ideas\nthat bring diversity and creativity to nature and computation, we present\nMelomics: an algorithmic composition method based on evolutionary search. The\nsolutions have a genetic encoding based on formal grammars and these are\ninterpreted in a complex developmental process followed by a fitness\nassessment, to produce valid music compositions in standard formats.\n  The system has exhibited a high creative power and versatility to produce\nmusic of different types and it has been tested, proving on many occasions the\noutcome to be indistinguishable from the music made by human composers. The\nsystem has also enabled the emergence of a set of completely novel\napplications: from effective tools to help anyone to easily obtain the precise\nmusic that they need, to radically new uses, such as adaptive music for\ntherapy, exercise, amusement and many others. It seems clear that automated\ncomposition is an active research area and that countless new uses will be\ndiscovered.", "published": "2020-07-25 09:38:06", "link": "http://arxiv.org/abs/2008.04415v2", "categories": ["cs.SD", "cs.CY", "cs.NE", "eess.AS"], "primary_category": "cs.SD"}
