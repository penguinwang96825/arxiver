{"title": "Show, Write, and Retrieve: Entity-aware Article Generation and Retrieval", "abstract": "Article comprehension is an important challenge in natural language\nprocessing with many applications such as article generation or\nimage-to-article retrieval. Prior work typically encodes all tokens in articles\nuniformly using pretrained language models. However, in many applications, such\nas understanding news stories, these articles are based on real-world events\nand may reference many named entities that are difficult to accurately\nrecognize and predict by language models. To address this challenge, we propose\nan ENtity-aware article GeneratIoN and rEtrieval (ENGINE) framework, to\nexplicitly incorporate named entities into language models. ENGINE has two main\ncomponents: a named-entity extraction module to extract named entities from\nboth metadata and embedded images associated with articles, and an entity-aware\nmechanism that enhances the model's ability to recognize and predict entity\nnames. We conducted experiments on three public datasets: GoodNews, VisualNews,\nand WikiText, where our results demonstrate that our model can boost both\narticle generation and article retrieval performance, with a 4-5 perplexity\nimprovement in article generation and a 3-4% boost in recall@1 in article\nretrieval. We release our implementation at\nhttps://github.com/Zhongping-Zhang/ENGINE .", "published": "2021-12-11 05:32:09", "link": "http://arxiv.org/abs/2112.05917v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prosody Labelled Dataset for Hindi using Semi-Automated Approach", "abstract": "This study aims to develop a semi-automatically labelled prosody database for\nHindi, for enhancing the intonation component in ASR and TTS systems, which is\nalso helpful for building Speech to Speech Machine Translation systems.\nAlthough no single standard for prosody labelling exists in Hindi, researchers\nin the past have employed perceptual and statistical methods in literature to\ndraw inferences about the behaviour of prosody patterns in Hindi. Based on such\nexisting research and largely agreed upon theories of intonation in Hindi, this\nstudy attempts to first develop a manually annotated prosodic corpus of Hindi\nspeech data, which is then used for training prediction models for generating\nautomatic prosodic labels. A total of 5,000 sentences (23,500 words) for\ndeclarative and interrogative types have been labelled. The accuracy of the\ntrained models for pitch accent, intermediate phrase boundaries and accentual\nphrase boundaries is 73.40%, 93.20%, and 43% respectively.", "published": "2021-12-11 13:11:36", "link": "http://arxiv.org/abs/2112.05973v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Document-level Event Extraction via Pseudo-Trigger-aware\n  Pruned Complete Graph", "abstract": "Most previous studies of document-level event extraction mainly focus on\nbuilding argument chains in an autoregressive way, which achieves a certain\nsuccess but is inefficient in both training and inference. In contrast to the\nprevious studies, we propose a fast and lightweight model named as PTPCG. In\nour model, we design a novel strategy for event argument combination together\nwith a non-autoregressive decoding algorithm via pruned complete graphs, which\nare constructed under the guidance of the automatically selected pseudo\ntriggers. Compared to the previous systems, our system achieves competitive\nresults with 19.8\\% of parameters and much lower resource consumption, taking\nonly 3.8\\% GPU hours for training and up to 8.5 times faster for inference.\nBesides, our model shows superior compatibility for the datasets with (or\nwithout) triggers and the pseudo triggers can be the supplements for annotated\ntriggers to make further improvements. Codes are available at\nhttps://github.com/Spico197/DocEE .", "published": "2021-12-11 16:01:29", "link": "http://arxiv.org/abs/2112.06013v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Empirical Study on Relation Extraction in the Biomedical Domain", "abstract": "Relation extraction is a fundamental problem in natural language processing.\nMost existing models are defined for relation extraction in the general domain.\nHowever, their performance on specific domains (e.g., biomedicine) is yet\nunclear. To fill this gap, this paper carries out an empirical study on\nrelation extraction in biomedical research articles. Specifically, we consider\nboth sentence-level and document-level relation extraction, and run a few\nstate-of-the-art methods on several benchmark datasets. Our results show that\n(1) current document-level relation extraction methods have strong\ngeneralization ability; (2) existing methods require a large amount of labeled\ndata for model fine-tuning in biomedicine. Our observations may inspire people\nin this field to develop more effective models for biomedical relation\nextraction.", "published": "2021-12-11 03:36:38", "link": "http://arxiv.org/abs/2112.05910v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Selecting Parallel In-domain Sentences for Neural Machine Translation\n  Using Monolingual Texts", "abstract": "Continuously-growing data volumes lead to larger generic models. Specific\nuse-cases are usually left out, since generic models tend to perform poorly in\ndomain-specific cases. Our work addresses this gap with a method for selecting\nin-domain data from generic-domain (parallel text) corpora, for the task of\nmachine translation. The proposed method ranks sentences in parallel\ngeneral-domain data according to their cosine similarity with a monolingual\ndomain-specific data set. We then select the top K sentences with the highest\nsimilarity score to train a new machine translation system tuned to the\nspecific in-domain data. Our experimental results show that models trained on\nthis in-domain data outperform models trained on generic or a mixture of\ngeneric and domain data. That is, our method selects high-quality\ndomain-specific training instances at low computational cost and data size.", "published": "2021-12-11 23:29:26", "link": "http://arxiv.org/abs/2112.06096v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "U-shaped Transformer with Frequency-Band Aware Attention for Speech\n  Enhancement", "abstract": "The state-of-the-art speech enhancement has limited performance in speech\nestimation accuracy. Recently, in deep learning, the Transformer shows the\npotential to exploit the long-range dependency in speech by self-attention.\nTherefore, it is introduced in speech enhancement to improve the speech\nestimation accuracy from a noise mixture. However, to address the computational\ncost issue in Transformer with self-attention, the axial attention is the\noption i.e., to split a 2D attention into two 1D attentions. Inspired by the\naxial attention, in the proposed method we calculate the attention map along\nboth time- and frequency-axis to generate time and frequency sub-attention\nmaps. Moreover, different from the axial attention, the proposed method\nprovides two parallel multi-head attentions for time- and frequency-axis.\nFurthermore, it is proven in the literature that the lower frequency-band in\nspeech, generally, contains more desired information than the higher\nfrequency-band, in a noise mixture. Therefore, the frequency-band aware\nattention is proposed i.e., high frequency-band attention (HFA), and low\nfrequency-band attention (LFA). The U-shaped Transformer is also first time\nintroduced in the proposed method to further improve the speech estimation\naccuracy. The extensive evaluations over four public datasets, confirm the\nefficacy of the proposed method.", "published": "2021-12-11 19:18:34", "link": "http://arxiv.org/abs/2112.06052v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Perceptual Loss with Recognition Model for Single-Channel Enhancement\n  and Robust ASR", "abstract": "Single-channel speech enhancement approaches do not always improve automatic\nrecognition rates in the presence of noise, because they can introduce\ndistortions unhelpful for recognition. Following a trend towards end-to-end\ntraining of sequential neural network models, several research groups have\naddressed this problem with joint training of front-end enhancement module with\nback-end recognition module. While this approach ensures enhancement outputs\nare helpful for recognition, the enhancement model can overfit to the training\ndata, weakening the recognition model in the presence of unseen noise. To\naddress this, we used a pre-trained acoustic model to generate a perceptual\nloss that makes speech enhancement more aware of the phonetic properties of the\nsignal. This approach keeps some benefits of joint training, while alleviating\nthe overfitting problem. Experiments on Voicebank + DEMAND dataset for\nenhancement show that this approach achieves a new state of the art for some\nobjective enhancement scores. In combination with distortion-independent\ntraining, our approach gets a WER of 2.80\\% on the test set, which is more than\n20\\% relative better recognition performance than joint training, and 14\\%\nrelative better than distortion-independent mask training.", "published": "2021-12-11 20:44:26", "link": "http://arxiv.org/abs/2112.06068v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Hybrid Neural Networks for On-device Directional Hearing", "abstract": "On-device directional hearing requires audio source separation from a given\ndirection while achieving stringent human-imperceptible latency requirements.\nWhile neural nets can achieve significantly better performance than traditional\nbeamformers, all existing models fall short of supporting low-latency causal\ninference on computationally-constrained wearables. We present DeepBeam, a\nhybrid model that combines traditional beamformers with a custom lightweight\nneural net. The former reduces the computational burden of the latter and also\nimproves its generalizability, while the latter is designed to further reduce\nthe memory and computational overhead to enable real-time and low-latency\noperations. Our evaluation shows comparable performance to state-of-the-art\ncausal inference models on synthetic data while achieving a 5x reduction of\nmodel size, 4x reduction of computation per second, 5x reduction in processing\ntime and generalizing better to real hardware data. Further, our real-time\nhybrid model runs in 8 ms on mobile CPUs designed for low-power wearable\ndevices and achieves an end-to-end latency of 17.5 ms.", "published": "2021-12-11 01:29:12", "link": "http://arxiv.org/abs/2112.05893v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
