{"title": "Relevance Classification of Flood-related Twitter Posts via Multiple\n  Transformers", "abstract": "In recent years, social media has been widely explored as a potential source\nof communication and information in disasters and emergency situations. Several\ninteresting works and case studies of disaster analytics exploring different\naspects of natural disasters have been already conducted. Along with the great\npotential, disaster analytics comes with several challenges mainly due to the\nnature of social media content. In this paper, we explore one such challenge\nand propose a text classification framework to deal with Twitter noisy data.\nMore specifically, we employed several transformers both individually and in\ncombination, so as to differentiate between relevant and non-relevant Twitter\nposts, achieving the highest F1-score of 0.87.", "published": "2023-01-01 01:34:15", "link": "http://arxiv.org/abs/2301.00320v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Floods Relevancy and Identification of Location from Twitter Posts using\n  NLP Techniques", "abstract": "This paper presents our solutions for the MediaEval 2022 task on DisasterMM.\nThe task is composed of two subtasks, namely (i) Relevance Classification of\nTwitter Posts (RCTP), and (ii) Location Extraction from Twitter Texts (LETT).\nThe RCTP subtask aims at differentiating flood-related and non-relevant social\nposts while LETT is a Named Entity Recognition (NER) task and aims at the\nextraction of location information from the text. For RCTP, we proposed four\ndifferent solutions based on BERT, RoBERTa, Distil BERT, and ALBERT obtaining\nan F1-score of 0.7934, 0.7970, 0.7613, and 0.7924, respectively. For LETT, we\nused three models namely BERT, RoBERTa, and Distil BERTA obtaining an F1-score\nof 0.6256, 0.6744, and 0.6723, respectively.", "published": "2023-01-01 01:36:32", "link": "http://arxiv.org/abs/2301.00321v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Inflected Forms Are Redundant in Question Generation Models", "abstract": "Neural models with an encoder-decoder framework provide a feasible solution\nto Question Generation (QG). However, after analyzing the model vocabulary we\nfind that current models (both RNN-based and pre-training based) have more than\n23\\% inflected forms. As a result, the encoder will generate separate\nembeddings for the inflected forms, leading to a waste of training data and\nparameters. Even worse, in decoding these models are vulnerable to irrelevant\nnoise and they suffer from high computational costs. In this paper, we propose\nan approach to enhance the performance of QG by fusing word transformation.\nFirstly, we identify the inflected forms of words from the input of encoder,\nand replace them with the root words, letting the encoder pay more attention to\nthe repetitive root words. Secondly, we propose to adapt QG as a combination of\nthe following actions in the encode-decoder framework: generating a question\nword, copying a word from the source sequence or generating a word\ntransformation type. Such extension can greatly decrease the size of predicted\nwords in the decoder as well as noise. We apply our approach to a typical\nRNN-based model and \\textsc{UniLM} to get the improved versions. We conduct\nextensive experiments on SQuAD and MS MARCO datasets. The experimental results\nshow that the improved versions can significantly outperform the corresponding\nbaselines in terms of BLEU, ROUGE-L and METEOR as well as time cost.", "published": "2023-01-01 13:08:11", "link": "http://arxiv.org/abs/2301.00397v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic Operator Prediction and Applications", "abstract": "In the present paper, semantic parsing challenges are briefly introduced and\nQDMR formalism in semantic parsing is implemented using sequence to sequence\nmodel with attention but uses only part of speech(POS) as a representation of\nwords of a sentence to make the training as simple and as fast as possible and\nalso avoiding curse of dimensionality as well as overfitting. It is shown how\nsemantic operator prediction could be augmented with other models like the\nCopyNet model or the recursive neural net model.", "published": "2023-01-01 13:20:57", "link": "http://arxiv.org/abs/2301.00399v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is word segmentation necessary for Vietnamese sentiment classification?", "abstract": "To the best of our knowledge, this paper made the first attempt to answer\nwhether word segmentation is necessary for Vietnamese sentiment classification.\nTo do this, we presented five pre-trained monolingual S4- based language models\nfor Vietnamese, including one model without word segmentation, and four models\nusing RDRsegmenter, uitnlp, pyvi, or underthesea toolkits in the pre-processing\ndata phase. According to comprehensive experimental results on two corpora,\nincluding the VLSP2016-SA corpus of technical article reviews from the news and\nsocial media and the UIT-VSFC corpus of the educational survey, we have two\nsuggestions. Firstly, using traditional classifiers like Naive Bayes or Support\nVector Machines, word segmentation maybe not be necessary for the Vietnamese\nsentiment classification corpus, which comes from the social domain. Secondly,\nword segmentation is necessary for Vietnamese sentiment classification when\nword segmentation is used before using the BPE method and feeding into the deep\nlearning model. In this way, the RDRsegmenter is the stable toolkit for word\nsegmentation among the uitnlp, pyvi, and underthesea toolkits.", "published": "2023-01-01 15:04:47", "link": "http://arxiv.org/abs/2301.00418v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Semantic Representations Combined with Contextual Word\n  Representations for Recognizing Textual Entailment in Vietnamese", "abstract": "RTE is a significant problem and is a reasonably active research community.\nThe proposed research works on the approach to this problem are pretty diverse\nwith many different directions. For Vietnamese, the RTE problem is moderately\nnew, but this problem plays a vital role in natural language understanding\nsystems. Currently, methods to solve this problem based on contextual word\nrepresentation learning models have given outstanding results. However,\nVietnamese is a semantically rich language. Therefore, in this paper, we want\nto present an experiment combining semantic word representation through the SRL\ntask with context representation of BERT relative models for the RTE problem.\nThe experimental results give conclusions about the influence and role of\nsemantic representation on Vietnamese in understanding natural language. The\nexperimental results show that the semantic-aware contextual representation\nmodel has about 1% higher performance than the model that does not incorporate\nsemantic representation. In addition, the effects on the data domain in\nVietnamese are also higher than those in English. This result also shows the\npositive influence of SRL on RTE problem in Vietnamese.", "published": "2023-01-01 15:13:25", "link": "http://arxiv.org/abs/2301.00422v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Integrating Semantic Information into Sketchy Reading Module of\n  Retro-Reader for Vietnamese Machine Reading Comprehension", "abstract": "Machine Reading Comprehension has become one of the most advanced and popular\nresearch topics in the fields of Natural Language Processing in recent years.\nThe classification of answerability questions is a relatively significant\nsub-task in machine reading comprehension; however, there haven't been many\nstudies. Retro-Reader is one of the studies that has solved this problem\neffectively. However, the encoders of most traditional machine reading\ncomprehension models in general and Retro-Reader, in particular, have not been\nable to exploit the contextual semantic information of the context completely.\nInspired by SemBERT, we use semantic role labels from the SRL task to add\nsemantics to pre-trained language models such as mBERT, XLM-R, PhoBERT. This\nexperiment was conducted to compare the influence of semantics on the\nclassification of answerability for the Vietnamese machine reading\ncomprehension. Additionally, we hope this experiment will enhance the encoder\nfor the Retro-Reader model's Sketchy Reading Module. The improved Retro-Reader\nmodel's encoder with semantics was first applied to the Vietnamese Machine\nReading Comprehension task and obtained positive results.", "published": "2023-01-01 15:28:27", "link": "http://arxiv.org/abs/2301.00429v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Optimizing Readability Using Genetic Algorithms", "abstract": "This research presents ORUGA, a method that tries to automatically optimize\nthe readability of any text in English. The core idea behind the method is that\ncertain factors affect the readability of a text, some of which are\nquantifiable (number of words, syllables, presence or absence of adverbs, and\nso on). The nature of these factors allows us to implement a genetic learning\nstrategy to replace some existing words with their most suitable synonyms to\nfacilitate optimization. In addition, this research seeks to preserve both the\noriginal text's content and form through multi-objective optimization\ntechniques. In this way, neither the text's syntactic structure nor the\nsemantic content of the original message is significantly distorted. An\nexhaustive study on a substantial number and diversity of texts confirms that\nour method was able to optimize the degree of readability in all cases without\nsignificantly altering their form or meaning. The source code of this approach\nis available at https://github.com/jorge-martinez-gil/oruga.", "published": "2023-01-01 09:08:45", "link": "http://arxiv.org/abs/2301.00374v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Chatbots as Problem Solvers: Playing Twenty Questions with Role\n  Reversals", "abstract": "New chat AI applications like ChatGPT offer an advanced understanding of\nquestion context and memory across multi-step tasks, such that experiments can\ntest its deductive reasoning. This paper proposes a multi-role and multi-step\nchallenge, where ChatGPT plays the classic twenty-questions game but\ninnovatively switches roles from the questioner to the answerer. The main\nempirical result establishes that this generation of chat applications can\nguess random object names in fewer than twenty questions (average, 12) and\ncorrectly guess 94% of the time across sixteen different experimental setups.\nThe research introduces four novel cases where the chatbot fields the\nquestions, asks the questions, both question-answer roles, and finally tries to\nguess appropriate contextual emotions. One task that humans typically fail but\ntrained chat applications complete involves playing bilingual games of twenty\nquestions (English answers to Spanish questions). Future variations address\ndirect problem-solving using a similar inquisitive format to arrive at novel\noutcomes deductively, such as patentable inventions or combination thinking.\nFeatured applications of this dialogue format include complex protein designs,\nneuroscience metadata, and child development educational materials.", "published": "2023-01-01 03:04:04", "link": "http://arxiv.org/abs/2301.01743v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Second Thoughts are Best: Learning to Re-Align With Human Values from\n  Text Edits", "abstract": "We present Second Thought, a new learning paradigm that enables language\nmodels (LMs) to re-align with human values. By modeling the chain-of-edits\nbetween value-unaligned and value-aligned text, with LM fine-tuning and\nadditional refinement through reinforcement learning, Second Thought not only\nachieves superior performance in three value alignment benchmark datasets but\nalso shows strong human-value transfer learning ability in few-shot scenarios.\nThe generated editing steps also offer better interpretability and ease for\ninteractive error correction. Extensive human evaluations further confirm its\neffectiveness.", "published": "2023-01-01 05:56:21", "link": "http://arxiv.org/abs/2301.00355v2", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "CORGI-PM: A Chinese Corpus For Gender Bias Probing and Mitigation", "abstract": "As natural language processing (NLP) for gender bias becomes a significant\ninterdisciplinary topic, the prevalent data-driven techniques such as\nlarge-scale language models suffer from data inadequacy and biased corpus,\nespecially for languages with insufficient resources such as Chinese. To this\nend, we propose a Chinese cOrpus foR Gender bIas Probing and Mitigation\nCORGI-PM, which contains 32.9k sentences with high-quality labels derived by\nfollowing an annotation scheme specifically developed for gender bias in the\nChinese context. Moreover, we address three challenges for automatic textual\ngender bias mitigation, which requires the models to detect, classify, and\nmitigate textual gender bias. We also conduct experiments with state-of-the-art\nlanguage models to provide baselines. To our best knowledge, CORGI-PM is the\nfirst sentence-level Chinese corpus for gender bias probing and mitigation.", "published": "2023-01-01 12:48:12", "link": "http://arxiv.org/abs/2301.00395v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Design Principle of Blockchain: An Initiative for the SoK of SoKs", "abstract": "Blockchain, also coined as decentralized AI, has the potential to empower AI\nto be more trustworthy by creating a decentralized trust of privacy, security,\nand audibility. However, systematic studies on the design principle of\nblockchain as a trust engine for an integrated society of\ncyber-physical-social-system (CPSS) are still absent. In this article, we\nprovide an initiative for seeking the design principle of blockchain for a\nbetter digital world. Using a hybrid method of qualitative and quantitative\nstudies, we examine the past origin, the current development, and the future\ndirections of blockchain design principles. We have three findings. First, the\nanswer to whether blockchain lives up to its original design principle as a\ndistributed database is controversial. Second, the current development of the\nblockchain community reveals a taxonomy of 7 categories, namely, privacy and\nsecurity, scalability, decentralization, applicability, governance and\nregulation, system design, and cross-chain interoperability. Both research and\npractice are more centered around the first category of privacy and security\nand the fourth category of applicability. Future scholars, practitioners, and\npolicy-makers have vast opportunities in other, much less exploited facets and\nthe synthesis at the interface of multiple aspects. Finally, in\ncounter-examples, we conclude that a synthetic solution that crosses discipline\nboundaries is necessary to close the gaps between the current design of\nblockchain and the design principle of a trust engine for a truly intelligent\nworld.", "published": "2023-01-01 21:57:25", "link": "http://arxiv.org/abs/2301.00479v2", "categories": ["cs.CR", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.CR"}
{"title": "Unsupervised Acoustic Scene Mapping Based on Acoustic Features and\n  Dimensionality Reduction", "abstract": "Classical methods for acoustic scene mapping require the estimation of time\ndifference of arrival (TDOA) between microphones. Unfortunately, TDOA\nestimation is very sensitive to reverberation and additive noise. We introduce\nan unsupervised data-driven approach that exploits the natural structure of the\ndata. Our method builds upon local conformal autoencoders (LOCA) - an offline\ndeep learning scheme for learning standardized data coordinates from\nmeasurements. Our experimental setup includes a microphone array that measures\nthe transmitted sound source at multiple locations across the acoustic\nenclosure. We demonstrate that LOCA learns a representation that is isometric\nto the spatial locations of the microphones. The performance of our method is\nevaluated using a series of realistic simulations and compared with other\ndimensionality-reduction schemes. We further assess the influence of\nreverberation on the results of LOCA and show that it demonstrates considerable\nrobustness.", "published": "2023-01-01 17:46:09", "link": "http://arxiv.org/abs/2301.00448v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Quantum Representations of Sound: from mechanical waves to quantum\n  circuits", "abstract": "By the time of writing, quantum audio still is a very young area of study,\neven within the quantum signal processing community. This chapter introduces\nthe state of the art in quantum audio and discusses methods for the quantum\nrepresentation of audio signals. Currently, no quantum representation strategy\nclaims to be the best one for audio applications. Each one presents advantages\nand disadvantages. It can be argued that future quantum audio representation\nschemes will make use of multiple strategies aimed at specific applications.\nNOTE: This is an unedited abridged version of the pre-submission draft of a\nchapter, with the same title, published in the book Quantum Computer Music:\nFoundations, Methods and Advanced Concepts, by E. R. Miranda (pp. 223 - 274).\nPlease refer to the version in this book for application examples and a\ndiscussion on sound synthesis methods based on quantum audio representation and\ntheir potential for developing new types of musical instruments.\nhttps://link.springer.com/book/10.1007/978-3-031-13909-3", "published": "2023-01-01 17:10:30", "link": "http://arxiv.org/abs/2301.01595v1", "categories": ["quant-ph", "cs.SD", "eess.AS"], "primary_category": "quant-ph"}
