{"title": "Persian-WSD-Corpus: A Sense Annotated Corpus for Persian All-words Word\n  Sense Disambiguation", "abstract": "Word Sense Disambiguation (WSD) is a long-standing task in Natural Language\nProcessing(NLP) that aims to automatically identify the most relevant meaning\nof the words in a given context. Developing standard WSD test collections can\nbe mentioned as an important prerequisite for developing and evaluating\ndifferent WSD systems in the language of interest. Although many WSD test\ncollections have been developed for a variety of languages, no standard\nAll-words WSD benchmark is available for Persian. In this paper, we address\nthis shortage for the Persian language by introducing SBU-WSD-Corpus, as the\nfirst standard test set for the Persian All-words WSD task. SBU-WSD-Corpus is\nmanually annotated with senses from the Persian WordNet (FarsNet) sense\ninventory. To this end, three annotators used SAMP (a tool for sense annotation\nbased on FarsNet lexical graph) to perform the annotation task. SBU-WSD-Corpus\nconsists of 19 Persian documents in different domains such as Sports, Science,\nArts, etc. It includes 5892 content words of Persian running text and 3371\nmanually sense annotated words (2073 nouns, 566 verbs, 610 adjectives, and 122\nadverbs). Providing baselines for future studies on the Persian All-words WSD\ntask, we evaluate several WSD models on SBU-WSD-Corpus. The corpus is publicly\navailable at https://github.com/hrouhizadeh/SBU-WSD-Corpus.", "published": "2021-07-04 05:09:28", "link": "http://arxiv.org/abs/2107.01540v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Audio-Oriented Multimodal Machine Comprehension: Task, Dataset and Model", "abstract": "While Machine Comprehension (MC) has attracted extensive research interests\nin recent years, existing approaches mainly belong to the category of Machine\nReading Comprehension task which mines textual inputs (paragraphs and\nquestions) to predict the answers (choices or text spans). However, there are a\nlot of MC tasks that accept audio input in addition to the textual input, e.g.\nEnglish listening comprehension test. In this paper, we target the problem of\nAudio-Oriented Multimodal Machine Comprehension, and its goal is to answer\nquestions based on the given audio and textual information. To solve this\nproblem, we propose a Dynamic Inter- and Intra-modality Attention (DIIA) model\nto effectively fuse the two modalities (audio and textual). DIIA can work as an\nindependent component and thus be easily integrated into existing MC models.\nMoreover, we further develop a Multimodal Knowledge Distillation (MKD) module\nto enable our multimodal MC model to accurately predict the answers based only\non either the text or the audio. As a result, the proposed approach can handle\nvarious tasks including: Audio-Oriented Multimodal Machine Comprehension,\nMachine Reading Comprehension and Machine Listening Comprehension, in a single\nmodel, making fair comparisons possible between our model and the existing\nunimodal MC models. Experimental results and analysis prove the effectiveness\nof the proposed approaches. First, the proposed DIIA boosts the baseline models\nby up to 21.08% in terms of accuracy; Second, under the unimodal scenarios, the\nMKD module allows our multimodal MC model to significantly outperform the\nunimodal models by up to 18.87%, which are trained and tested with only audio\nor textual data.", "published": "2021-07-04 08:35:20", "link": "http://arxiv.org/abs/2107.01571v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CasEE: A Joint Learning Framework with Cascade Decoding for Overlapping\n  Event Extraction", "abstract": "Event extraction (EE) is a crucial information extraction task that aims to\nextract event information in texts. Most existing methods assume that events\nappear in sentences without overlaps, which are not applicable to the\ncomplicated overlapping event extraction. This work systematically studies the\nrealistic event overlapping problem, where a word may serve as triggers with\nseveral types or arguments with different roles. To tackle the above problem,\nwe propose a novel joint learning framework with cascade decoding for\noverlapping event extraction, termed as CasEE. Particularly, CasEE sequentially\nperforms type detection, trigger extraction and argument extraction, where the\noverlapped targets are extracted separately conditioned on the specific former\nprediction. All the subtasks are jointly learned in a framework to capture\ndependencies among the subtasks. The evaluation on a public event extraction\nbenchmark FewFC demonstrates that CasEE achieves significant improvements on\noverlapping event extraction over previous competitive methods.", "published": "2021-07-04 10:01:55", "link": "http://arxiv.org/abs/2107.01583v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Coarse-to-Careful: Seeking Semantic-related Knowledge for Open-domain\n  Commonsense Question Answering", "abstract": "It is prevalent to utilize external knowledge to help machine answer\nquestions that need background commonsense, which faces a problem that\nunlimited knowledge will transmit noisy and misleading information. Towards the\nissue of introducing related knowledge, we propose a semantic-driven\nknowledge-aware QA framework, which controls the knowledge injection in a\ncoarse-to-careful fashion. We devise a tailoring strategy to filter extracted\nknowledge under monitoring of the coarse semantic of question on the knowledge\nextraction stage. And we develop a semantic-aware knowledge fetching module\nthat engages structural knowledge information and fuses proper knowledge\naccording to the careful semantic of questions in a hierarchical way.\nExperiments demonstrate that the proposed approach promotes the performance on\nthe CommonsenseQA dataset comparing with strong baselines.", "published": "2021-07-04 10:56:36", "link": "http://arxiv.org/abs/2107.01592v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "End-to-end Neural Coreference Resolution Revisited: A Simple yet\n  Effective Baseline", "abstract": "Since the first end-to-end neural coreference resolution model was\nintroduced, many extensions to the model have been proposed, ranging from using\nhigher-order inference to directly optimizing evaluation metrics using\nreinforcement learning. Despite improving the coreference resolution\nperformance by a large margin, these extensions add substantial extra\ncomplexity to the original model. Motivated by this observation and the recent\nadvances in pre-trained Transformer language models, we propose a simple yet\neffective baseline for coreference resolution. Even though our model is a\nsimplified version of the original neural coreference resolution model, it\nachieves impressive performance, outperforming all recent extended works on the\npublic English OntoNotes benchmark. Our work provides evidence for the\nnecessity of carefully justifying the complexity of existing or newly proposed\nmodels, as introducing a conceptual or practical simplification to an existing\nmodel can still yield competitive results.", "published": "2021-07-04 18:12:24", "link": "http://arxiv.org/abs/2107.01700v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentence-level Online Handwritten Chinese Character Recognition", "abstract": "Single online handwritten Chinese character recognition~(single OLHCCR) has\nachieved prominent performance. However, in real application scenarios, users\nalways write multiple Chinese characters to form one complete sentence and the\ncontextual information within these characters holds the significant potential\nto improve the accuracy, robustness and efficiency of sentence-level OLHCCR. In\nthis work, we first propose a simple and straightforward end-to-end network,\nnamely vanilla compositional network~(VCN) to tackle the sentence-level OLHCCR.\nIt couples convolutional neural network with sequence modeling architecture to\nexploit the handwritten character's previous contextual information. Although\nVCN performs much better than the state-of-the-art single OLHCCR model, it\nexposes high fragility when confronting with not well written characters such\nas sloppy writing, missing or broken strokes. To improve the robustness of\nsentence-level OLHCCR, we further propose a novel deep spatial-temporal fusion\nnetwork~(DSTFN). It utilizes a pre-trained autoregresssive framework as the\nbackbone component, which projects each Chinese character into word embeddings,\nand integrates the spatial glyph features of handwritten characters and their\ncontextual information multiple times at multi-layer fusion module. We also\nconstruct a large-scale sentence-level handwriting dataset, named as CSOHD to\nevaluate models. Extensive experiment results demonstrate that DSTFN achieves\nthe state-of-the-art performance, which presents strong robustness compared\nwith VCN and exiting single OLHCCR models. The in-depth empirical analysis and\ncase studies indicate that DSTFN can significantly improve the efficiency of\nhandwriting input, with the handwritten Chinese character with incomplete\nstrokes being recognized precisely.", "published": "2021-07-04 14:26:06", "link": "http://arxiv.org/abs/2108.02561v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Cross-Modal Transformer-Based Neural Correction Models for Automatic\n  Speech Recognition", "abstract": "We propose a cross-modal transformer-based neural correction models that\nrefines the output of an automatic speech recognition (ASR) system so as to\nexclude ASR errors. Generally, neural correction models are composed of\nencoder-decoder networks, which can directly model sequence-to-sequence mapping\nproblems. The most successful method is to use both input speech and its ASR\noutput text as the input contexts for the encoder-decoder networks. However,\nthe conventional method cannot take into account the relationships between\nthese two different modal inputs because the input contexts are separately\nencoded for each modal. To effectively leverage the correlated information\nbetween the two different modal inputs, our proposed models encode two\ndifferent contexts jointly on the basis of cross-modal self-attention using a\ntransformer. We expect that cross-modal self-attention can effectively capture\nthe relationships between two different modals for refining ASR hypotheses. We\nalso introduce a shallow fusion technique to efficiently integrate the\nfirst-pass ASR model and our proposed neural correction model. Experiments on\nJapanese natural language ASR tasks demonstrated that our proposed models\nachieve better ASR performance than conventional neural correction models.", "published": "2021-07-04 07:58:31", "link": "http://arxiv.org/abs/2107.01569v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Domain Adaptation for Sentiment Analysis Using Increased Intraclass\n  Separation", "abstract": "Sentiment analysis is a costly yet necessary task for enterprises to study\nthe opinions of their customers to improve their products and to determine\noptimal marketing strategies. Due to the existence of a wide range of domains\nacross different products and services, cross-domain sentiment analysis methods\nhave received significant attention. These methods mitigate the domain gap\nbetween different applications by training cross-domain generalizable\nclassifiers which help to relax the need for data annotation for each domain.\nMost existing methods focus on learning domain-agnostic representations that\nare invariant with respect to both the source and the target domains. As a\nresult, a classifier that is trained using the source domain annotated data\nwould generalize well in a related target domain. We introduce a new domain\nadaptation method which induces large margins between different classes in an\nembedding space. This embedding space is trained to be domain-agnostic by\nmatching the data distributions across the domains. Large intraclass margins in\nthe source domain help to reduce the effect of \"domain shift\" on the classifier\nperformance in the target domain. Theoretical and empirical analysis are\nprovided to demonstrate that the proposed method is effective.", "published": "2021-07-04 11:39:12", "link": "http://arxiv.org/abs/2107.01598v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "IITP at WAT 2021: System description for English-Hindi Multimodal\n  Translation Task", "abstract": "Neural Machine Translation (NMT) is a predominant machine translation\ntechnology nowadays because of its end-to-end trainable flexibility. However,\nNMT still struggles to translate properly in low-resource settings specifically\non distant language pairs. One way to overcome this is to use the information\nfrom other modalities if available. The idea is that despite differences in\nlanguages, both the source and target language speakers see the same thing and\nthe visual representation of both the source and target is the same, which can\npositively assist the system. Multimodal information can help the NMT system to\nimprove the translation by removing ambiguity on some phrases or words. We\nparticipate in the 8th Workshop on Asian Translation (WAT - 2021) for\nEnglish-Hindi multimodal translation task and achieve 42.47 and 37.50 BLEU\npoints for Evaluation and Challenge subset, respectively.", "published": "2021-07-04 14:56:28", "link": "http://arxiv.org/abs/2107.01656v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DEAP-FAKED: Knowledge Graph based Approach for Fake News Detection", "abstract": "Fake News on social media platforms has attracted a lot of attention in\nrecent times, primarily for events related to politics (2016 US Presidential\nelections), healthcare (infodemic during COVID-19), to name a few. Various\nmethods have been proposed for detecting Fake News. The approaches span from\nexploiting techniques related to network analysis, Natural Language Processing\n(NLP), and the usage of Graph Neural Networks (GNNs). In this work, we propose\nDEAP-FAKED, a knowleDgE grAPh FAKe nEws Detection framework for identifying\nFake News. Our approach is a combination of the NLP -- where we encode the news\ncontent, and the GNN technique -- where we encode the Knowledge Graph (KG). A\nvariety of these encodings provides a complementary advantage to our detector.\nWe evaluate our framework using two publicly available datasets containing\narticles from domains such as politics, business, technology, and healthcare.\nAs part of dataset pre-processing, we also remove the bias, such as the source\nof the articles, which could impact the performance of the models. DEAP-FAKED\nobtains an F1-score of 88% and 78% for the two datasets, which is an\nimprovement of 21%, and 3% respectively, which shows the effectiveness of the\napproach.", "published": "2021-07-04 07:09:59", "link": "http://arxiv.org/abs/2107.10648v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Neural Diarization for Unlimited Numbers of Speakers Using\n  Global and Local Attractors", "abstract": "Attractor-based end-to-end diarization is achieving comparable accuracy to\nthe carefully tuned conventional clustering-based methods on challenging\ndatasets. However, the main drawback is that it cannot deal with the case where\nthe number of speakers is larger than the one observed during training. This is\nbecause its speaker counting relies on supervised learning. In this work, we\nintroduce an unsupervised clustering process embedded in the attractor-based\nend-to-end diarization. We first split a sequence of frame-wise embeddings into\nshort subsequences and then perform attractor-based diarization for each\nsubsequence. Given subsequence-wise diarization results, inter-subsequence\nspeaker correspondence is obtained by unsupervised clustering of the vectors\ncomputed from the attractors from all the subsequences. This makes it possible\nto produce diarization results of a large number of speakers for the whole\nrecording even if the number of output speakers for each subsequence is\nlimited. Experimental results showed that our method could produce accurate\ndiarization results of an unseen number of speakers. Our method achieved 11.84\n%, 28.33 %, and 19.49 % on the CALLHOME, DIHARD II, and DIHARD III datasets,\nrespectively, each of which is better than the conventional end-to-end\ndiarization methods.", "published": "2021-07-04 05:34:21", "link": "http://arxiv.org/abs/2107.01545v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Unified Autoregressive Modeling for Joint End-to-End Multi-Talker\n  Overlapped Speech Recognition and Speaker Attribute Estimation", "abstract": "In this paper, we present a novel modeling method for single-channel\nmulti-talker overlapped automatic speech recognition (ASR) systems. Fully\nneural network based end-to-end models have dramatically improved the\nperformance of multi-taker overlapped ASR tasks. One promising approach for\nend-to-end modeling is autoregressive modeling with serialized output training\nin which transcriptions of multiple speakers are recursively generated one\nafter another. This enables us to naturally capture relationships between\nspeakers. However, the conventional modeling method cannot explicitly take into\naccount the speaker attributes of individual utterances such as gender and age\ninformation. In fact, the performance deteriorates when each speaker is the\nsame gender or is close in age. To address this problem, we propose unified\nautoregressive modeling for joint end-to-end multi-talker overlapped ASR and\nspeaker attribute estimation. Our key idea is to handle gender and age\nestimation tasks within the unified autoregressive modeling. In the proposed\nmethod, transformer-based autoregressive model recursively generates not only\ntextual tokens but also attribute tokens of each speaker. This enables us to\neffectively utilize speaker attributes for improving multi-talker overlapped\nASR. Experiments on Japanese multi-talker overlapped ASR tasks demonstrate the\neffectiveness of the proposed method.", "published": "2021-07-04 05:47:18", "link": "http://arxiv.org/abs/2107.01549v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Arabic Code-Switching Speech Recognition using Monolingual Data", "abstract": "Code-switching in automatic speech recognition (ASR) is an important\nchallenge due to globalization. Recent research in multilingual ASR shows\npotential improvement over monolingual systems. We study key issues related to\nmultilingual modeling for ASR through a series of large-scale ASR experiments.\nOur innovative framework deploys a multi-graph approach in the weighted finite\nstate transducers (WFST) framework. We compare our WFST decoding strategies\nwith a transformer sequence to sequence system trained on the same data. Given\na code-switching scenario between Arabic and English languages, our results\nshow that the WFST decoding approaches were more suitable for the\nintersentential code-switching datasets. In addition, the transformer system\nperformed better for intrasentential code-switching task. With this study, we\nrelease an artificially generated development and test sets, along with\necological code-switching test set, to benchmark the ASR performance.", "published": "2021-07-04 08:40:49", "link": "http://arxiv.org/abs/2107.01573v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "EditSpeech: A Text Based Speech Editing System Using Partial Inference\n  and Bidirectional Fusion", "abstract": "This paper presents the design, implementation and evaluation of a speech\nediting system, named EditSpeech, which allows a user to perform deletion,\ninsertion and replacement of words in a given speech utterance, without causing\naudible degradation in speech quality and naturalness. The EditSpeech system is\ndeveloped upon a neural text-to-speech (NTTS) synthesis framework. Partial\ninference and bidirectional fusion are proposed to effectively incorporate the\ncontextual information related to the edited region and achieve smooth\ntransition at both left and right boundaries. Distortion introduced to the\nunmodified parts of the utterance is alleviated. The EditSpeech system is\ndeveloped and evaluated on English and Chinese in multi-speaker scenarios.\nObjective and subjective evaluation demonstrate that EditSpeech outperforms a\nfew baseline systems in terms of low spectral distortion and preferred speech\nquality. Audio samples are available online for demonstration\nhttps://daxintan-cuhk.github.io/EditSpeech/ .", "published": "2021-07-04 06:21:57", "link": "http://arxiv.org/abs/2107.01554v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "TENET: A Time-reversal Enhancement Network for Noise-robust ASR", "abstract": "Due to the unprecedented breakthroughs brought about by deep learning, speech\nenhancement (SE) techniques have been developed rapidly and play an important\nrole prior to acoustic modeling to mitigate noise effects on speech. To\nincrease the perceptual quality of speech, current state-of-the-art in the SE\nfield adopts adversarial training by connecting an objective metric to the\ndiscriminator. However, there is no guarantee that optimizing the perceptual\nquality of speech will necessarily lead to improved automatic speech\nrecognition (ASR) performance. In this study, we present TENET, a novel\nTime-reversal Enhancement NETwork, which leverages the transformation of an\ninput noisy signal itself, i.e., the time-reversed version, in conjunction with\nthe siamese network and complex dual-path transformer to promote SE performance\nfor noise-robust ASR. Extensive experiments conducted on the Voicebank-DEMAND\ndataset show that TENET can achieve state-of-the-art results compared to a few\ntop-of-the-line methods in terms of both SE and ASR evaluation metrics. To\ndemonstrate the model generalization ability, we further evaluate TENET on the\ntest set of scenarios contaminated with unseen noise, and the results also\nconfirm the superiority of this promising method.", "published": "2021-07-04 03:34:14", "link": "http://arxiv.org/abs/2107.01531v3", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
