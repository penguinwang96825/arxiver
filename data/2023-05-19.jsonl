{"title": "MD3: The Multi-Dialect Dataset of Dialogues", "abstract": "We introduce a new dataset of conversational speech representing English from\nIndia, Nigeria, and the United States. The Multi-Dialect Dataset of Dialogues\n(MD3) strikes a new balance between open-ended conversational speech and\ntask-oriented dialogue by prompting participants to perform a series of short\ninformation-sharing tasks. This facilitates quantitative cross-dialectal\ncomparison, while avoiding the imposition of a restrictive task structure that\nmight inhibit the expression of dialect features. Preliminary analysis of the\ndataset reveals significant differences in syntax and in the use of discourse\nmarkers. The dataset, which will be made publicly available with the\npublication of this paper, includes more than 20 hours of audio and more than\n200,000 orthographically-transcribed tokens.", "published": "2023-05-19 00:14:10", "link": "http://arxiv.org/abs/2305.11355v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AutoTrial: Prompting Language Models for Clinical Trial Design", "abstract": "Clinical trials are critical for drug development. Constructing the\nappropriate eligibility criteria (i.e., the inclusion/exclusion criteria for\npatient recruitment) is essential for the trial's success. Proper design of\nclinical trial protocols should consider similar precedent trials and their\neligibility criteria to ensure sufficient patient coverage. In this paper, we\npresent a method named AutoTrial to aid the design of clinical eligibility\ncriteria using language models. It allows (1) controllable generation under\ninstructions via a hybrid of discrete and neural prompting, (2) scalable\nknowledge incorporation via in-context learning, and (3) explicit reasoning\nchains to provide rationales for understanding the outputs. Experiments on over\n70K clinical trials verify that AutoTrial generates high-quality criteria texts\nthat are fluent and coherent and with high accuracy in capturing the relevant\nclinical concepts to the target trial. It is noteworthy that our method, with a\nmuch smaller parameter size, gains around 60% winning rate against the GPT-3.5\nbaselines via human evaluations.", "published": "2023-05-19 01:04:16", "link": "http://arxiv.org/abs/2305.11366v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Characterizing tradeoffs between teaching via language and\n  demonstrations in multi-agent systems", "abstract": "Humans teach others about the world through language and demonstration. When\nmight one of these modalities be more effective than the other? In this work,\nwe study the factors that modulate the effectiveness of language vs.\ndemonstration using multi-agent systems to model human communication.\nSpecifically, we train neural network agents to teach via language or\ndemonstration in a grounded communication task, manipulating 1) the inherent\ndifficulty of the task and 2) the competence of the teacher. We find that\nteaching by demonstration is more effective in the simplest settings, but\nlanguage is more effective as task difficulty increases, due to its ability to\ngeneralize more effectively to unseen scenarios. Overall, these results provide\nconverging evidence for a tradeoff between language and demonstration as\nteaching modalities in humans, and make the novel predictions that\ndemonstration may be optimal for easy tasks, while language enables\ngeneralization in more challenging settings.", "published": "2023-05-19 01:27:29", "link": "http://arxiv.org/abs/2305.11374v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analyzing and Reducing the Performance Gap in Cross-Lingual Transfer\n  with Fine-tuning Slow and Fast", "abstract": "Existing research has shown that a multilingual pre-trained language model\nfine-tuned with one (source) language also performs well on downstream tasks\nfor non-source languages, even though no fine-tuning is done on these\nlanguages. However, there is a clear gap between the performance of the source\nlanguage and that of the non-source languages. This paper analyzes the\nfine-tuning process, discovers when the performance gap changes and identifies\nwhich network weights affect the overall performance most. Additionally, the\npaper seeks to answer to what extent the gap can be reduced by reducing\nforgetting. Based on the analysis results, a method named Fine-tuning slow and\nfast with four training policies is proposed to address these issues.\nExperimental results show the proposed method outperforms baselines by a clear\nmargin.", "published": "2023-05-19 06:04:21", "link": "http://arxiv.org/abs/2305.11449v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extending Memory for Language Modelling", "abstract": "Breakthroughs in deep learning and memory networks have made major advances\nin natural language understanding. Language is sequential and information\ncarried through the sequence can be captured through memory networks. Learning\nthe sequence is one of the key aspects in learning the language. However,\nmemory networks are not capable of holding infinitely long sequences in their\nmemories and are limited by various constraints such as the vanishing or\nexploding gradient problem. Therefore, natural language understanding models\nare affected when presented with long sequential text. We introduce Long Term\nMemory network (LTM) to learn from infinitely long sequences. LTM gives\npriority to the current inputs to allow it to have a high impact. Language\nmodeling is an important factor in natural language understanding. LTM was\ntested in language modeling, which requires long term memory. LTM is tested on\nPenn Tree bank dataset, Google Billion Word dataset and WikiText-2 dataset. We\ncompare LTM with other language models which require long term memory.", "published": "2023-05-19 06:30:19", "link": "http://arxiv.org/abs/2305.11462v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Personalized Dialogue Generation with Contrastive Latent\n  Variables: Combining Sparse and Dense Persona", "abstract": "The personalized dialogue explores the consistent relationship between\ndialogue generation and personality. Existing personalized dialogue agents\nmodel persona profiles from three resources: sparse or dense persona\ndescriptions and dialogue histories. However, sparse structured persona\nattributes are explicit but uninformative, dense persona texts contain rich\npersona descriptions with much noise, and dialogue history query is both noisy\nand uninformative for persona modeling. In this work, we combine the advantages\nof the three resources to obtain a richer and more accurate persona. We design\na Contrastive Latent Variable-based model (CLV) that clusters the dense persona\ndescriptions into sparse categories, which are combined with the history query\nto generate personalized responses. Experimental results on Chinese and English\ndatasets demonstrate our model's superiority in personalization.", "published": "2023-05-19 07:24:27", "link": "http://arxiv.org/abs/2305.11482v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RCOT: Detecting and Rectifying Factual Inconsistency in Reasoning by\n  Reversing Chain-of-Thought", "abstract": "Large language Models (LLMs) have achieved promising performance on\narithmetic reasoning tasks by incorporating step-by-step chain-of-thought (CoT)\nprompting. However, LLMs face challenges in maintaining factual consistency\nduring reasoning, exhibiting tendencies to condition overlooking, question\nmisinterpretation, and condition hallucination over given problems. Existing\nmethods use coarse-grained feedback (e.g., whether the answer is correct) to\nimprove factual consistency. In this work, we propose RCoT (Reversing\nChain-of-Thought), a novel method to improve LLMs' reasoning abilities by\nautomatically detecting and rectifying factual inconsistency in LLMs, generated\nsolutions. To detect factual inconsistency, RCoT first asks LLMs to reconstruct\nthe problem based on generated solutions. Then fine-grained comparisons between\nthe original problem and the reconstructed problem expose the factual\ninconsistency in the original solutions. To rectify the solution, RCoT\nformulates detected factual inconsistency into fine-grained feedback to guide\nLLMs in revising solutions. Experimental results demonstrate improvements of\nRCoT over standard CoT, Self-Consistency and Self-Refine across seven\narithmetic datasets. Moreover, we find that manually written fine-grained\nfeedback can dramatically improve LLMs' reasoning abilities (e.g., ChatGPT\nreaches 94.6% accuracy on GSM8K), encouraging the community to further explore\nthe fine-grained feedback generation methods.", "published": "2023-05-19 08:02:52", "link": "http://arxiv.org/abs/2305.11499v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Alignment to Entailment: A Unified Textual Entailment Framework for\n  Entity Alignment", "abstract": "Entity Alignment (EA) aims to find the equivalent entities between two\nKnowledge Graphs (KGs). Existing methods usually encode the triples of entities\nas embeddings and learn to align the embeddings, which prevents the direct\ninteraction between the original information of the cross-KG entities.\nMoreover, they encode the relational triples and attribute triples of an entity\nin heterogeneous embedding spaces, which prevents them from helping each other.\nIn this paper, we transform both triples into unified textual sequences, and\nmodel the EA task as a bi-directional textual entailment task between the\nsequences of cross-KG entities. Specifically, we feed the sequences of two\nentities simultaneously into a pre-trained language model (PLM) and propose two\nkinds of PLM-based entity aligners that model the entailment probability\nbetween sequences as the similarity between entities. Our approach captures the\nunified correlation pattern of two kinds of information between entities, and\nexplicitly models the fine-grained interaction between original entity\ninformation. The experiments on five cross-lingual EA datasets show that our\napproach outperforms the state-of-the-art EA methods and enables the mutual\nenhancement of the heterogeneous information. Codes are available at\nhttps://github.com/OreOZhao/TEA.", "published": "2023-05-19 08:06:50", "link": "http://arxiv.org/abs/2305.11501v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Topic-aware Summarization Framework with Different Modal Side\n  Information", "abstract": "Automatic summarization plays an important role in the exponential document\ngrowth on the Web. On content websites such as CNN.com and WikiHow.com, there\noften exist various kinds of side information along with the main document for\nattention attraction and easier understanding, such as videos, images, and\nqueries. Such information can be used for better summarization, as they often\nexplicitly or implicitly mention the essence of the article. However, most of\nthe existing side-aware summarization methods are designed to incorporate\neither single-modal or multi-modal side information, and cannot effectively\nadapt to each other. In this paper, we propose a general summarization\nframework, which can flexibly incorporate various modalities of side\ninformation. The main challenges in designing a flexible summarization model\nwith side information include: (1) the side information can be in textual or\nvisual format, and the model needs to align and unify it with the document into\nthe same semantic space, (2) the side inputs can contain information from\nvarious aspects, and the model should recognize the aspects useful for\nsummarization. To address these two challenges, we first propose a unified\ntopic encoder, which jointly discovers latent topics from the document and\nvarious kinds of side information. The learned topics flexibly bridge and guide\nthe information flow between multiple inputs in a graph encoder through a\ntopic-aware interaction. We secondly propose a triplet contrastive learning\nmechanism to align the single-modal or multi-modal information into a unified\nsemantic space, where the summary quality is enhanced by better understanding\nthe document and side information. Results show that our model significantly\nsurpasses strong baselines on three public single-modal or multi-modal\nbenchmark summarization datasets.", "published": "2023-05-19 08:09:45", "link": "http://arxiv.org/abs/2305.11503v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contextualized Word Vector-based Methods for Discovering Semantic\n  Differences with No Training nor Word Alignment", "abstract": "In this paper, we propose methods for discovering semantic differences in\nwords appearing in two corpora based on the norms of contextualized word\nvectors. The key idea is that the coverage of meanings is reflected in the norm\nof its mean word vector. The proposed methods do not require the assumptions\nconcerning words and corpora for comparison that the previous methods do. All\nthey require are to compute the mean vector of contextualized word vectors and\nits norm for each word type. Nevertheless, they are (i) robust for the skew in\ncorpus size; (ii) capable of detecting semantic differences in infrequent\nwords; and (iii) effective in pinpointing word instances that have a meaning\nmissing in one of the two corpora for comparison. We show these advantages for\nnative and non-native English corpora and also for historical corpora.", "published": "2023-05-19 08:27:17", "link": "http://arxiv.org/abs/2305.11516v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Constructing Word-Context-Coupled Space Aligned with Associative\n  Knowledge Relations for Interpretable Language Modeling", "abstract": "As the foundation of current natural language processing methods, pre-trained\nlanguage model has achieved excellent performance. However, the black-box\nstructure of the deep neural network in pre-trained language models seriously\nlimits the interpretability of the language modeling process. After revisiting\nthe coupled requirement of deep neural representation and semantics logic of\nlanguage modeling, a Word-Context-Coupled Space (W2CSpace) is proposed by\nintroducing the alignment processing between uninterpretable neural\nrepresentation and interpretable statistical logic. Moreover, a clustering\nprocess is also designed to connect the word- and context-level semantics.\nSpecifically, an associative knowledge network (AKN), considered interpretable\nstatistical logic, is introduced in the alignment process for word-level\nsemantics. Furthermore, the context-relative distance is employed as the\nsemantic feature for the downstream classifier, which is greatly different from\nthe current uninterpretable semantic representations of pre-trained models. Our\nexperiments for performance evaluation and interpretable analysis are executed\non several types of datasets, including SIGHAN, Weibo, and ChnSenti. Wherein a\nnovel evaluation strategy for the interpretability of machine learning models\nis first proposed. According to the experimental results, our language model\ncan achieve better performance and highly credible interpretable ability\ncompared to related state-of-the-art methods.", "published": "2023-05-19 09:26:02", "link": "http://arxiv.org/abs/2305.11543v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Viewing Knowledge Transfer in Multilingual Machine Translation Through a\n  Representational Lens", "abstract": "We argue that translation quality alone is not a sufficient metric for\nmeasuring knowledge transfer in multilingual neural machine translation. To\nsupport this claim, we introduce Representational Transfer Potential (RTP),\nwhich measures representational similarities between languages. We show that\nRTP can measure both positive and negative transfer (interference), and find\nthat RTP is strongly correlated with changes in translation quality, indicating\nthat transfer does occur. Furthermore, we investigate data and language\ncharacteristics that are relevant for transfer, and find that multi-parallel\noverlap is an important yet under-explored feature. Based on this, we develop a\nnovel training scheme, which uses an auxiliary similarity loss that encourages\nrepresentations to be more invariant across languages by taking advantage of\nmulti-parallel data. We show that our method yields increased translation\nquality for low- and mid-resource languages across multiple data and model\nsetups.", "published": "2023-05-19 09:36:48", "link": "http://arxiv.org/abs/2305.11550v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Scientific Abstract Segmentation with Normalized Mutual\n  Information", "abstract": "The abstracts of scientific papers consist of premises and conclusions.\nStructured abstracts explicitly highlight the conclusion sentences, whereas\nnon-structured abstracts may have conclusion sentences at uncertain positions.\nThis implicit nature of conclusion positions makes the automatic segmentation\nof scientific abstracts into premises and conclusions a challenging task. In\nthis work, we empirically explore using Normalized Mutual Information (NMI) for\nabstract segmentation. We consider each abstract as a recurrent cycle of\nsentences and place segmentation boundaries by greedily optimizing the NMI\nscore between premises and conclusions. On non-structured abstracts, our\nproposed unsupervised approach GreedyCAS achieves the best performance across\nall evaluation metrics; on structured abstracts, GreedyCAS outperforms all\nbaseline methods measured by $P_k$. The strong correlation of NMI to our\nevaluation metrics reveals the effectiveness of NMI for abstract segmentation.", "published": "2023-05-19 09:53:45", "link": "http://arxiv.org/abs/2305.11553v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Attributable and Scalable Opinion Summarization", "abstract": "We propose a method for unsupervised opinion summarization that encodes\nsentences from customer reviews into a hierarchical discrete latent space, then\nidentifies common opinions based on the frequency of their encodings. We are\nable to generate both abstractive summaries by decoding these frequent\nencodings, and extractive summaries by selecting the sentences assigned to the\nsame frequent encodings. Our method is attributable, because the model\nidentifies sentences used to generate the summary as part of the summarization\nprocess. It scales easily to many hundreds of input reviews, because\naggregation is performed in the latent space rather than over long sequences of\ntokens. We also demonstrate that our appraoch enables a degree of control,\ngenerating aspect-specific summaries by restricting the model to parts of the\nencoding space that correspond to desired aspects (e.g., location or food).\nAutomatic and human evaluation on two datasets from different domains\ndemonstrates that our method generates summaries that are more informative than\nprior work and better grounded in the input reviews.", "published": "2023-05-19 11:30:37", "link": "http://arxiv.org/abs/2305.11603v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLM-Pruner: On the Structural Pruning of Large Language Models", "abstract": "Large language models (LLMs) have shown remarkable capabilities in language\nunderstanding and generation. However, such impressive capability typically\ncomes with a substantial model size, which presents significant challenges in\nboth the deployment, inference, and training stages. With LLM being a\ngeneral-purpose task solver, we explore its compression in a task-agnostic\nmanner, which aims to preserve the multi-task solving and language generation\nability of the original LLM. One challenge to achieving this is the enormous\nsize of the training corpus of LLM, which makes both data transfer and model\npost-training over-burdensome. Thus, we tackle the compression of LLMs within\nthe bound of two constraints: being task-agnostic and minimizing the reliance\non the original training dataset. Our method, named LLM-Pruner, adopts\nstructural pruning that selectively removes non-critical coupled structures\nbased on gradient information, maximally preserving the majority of the LLM's\nfunctionality. To this end, the performance of pruned models can be efficiently\nrecovered through tuning techniques, LoRA, in merely 3 hours, requiring only\n50K data. We validate the LLM-Pruner on three LLMs, including LLaMA, Vicuna,\nand ChatGLM, and demonstrate that the compressed models still exhibit\nsatisfactory capabilities in zero-shot classification and generation. The code\nis available at: https://github.com/horseee/LLM-Pruner", "published": "2023-05-19 12:10:53", "link": "http://arxiv.org/abs/2305.11627v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bias Beyond English: Counterfactual Tests for Bias in Sentiment Analysis\n  in Four Languages", "abstract": "Sentiment analysis (SA) systems are used in many products and hundreds of\nlanguages. Gender and racial biases are well-studied in English SA systems, but\nunderstudied in other languages, with few resources for such studies. To remedy\nthis, we build a counterfactual evaluation corpus for gender and racial/migrant\nbias in four languages. We demonstrate its usefulness by answering a simple but\nimportant question that an engineer might need to answer when deploying a\nsystem: What biases do systems import from pre-trained models when compared to\na baseline with no pre-training? Our evaluation corpus, by virtue of being\ncounterfactual, not only reveals which models have less bias, but also\npinpoints changes in model bias behaviour, which enables more targeted\nmitigation strategies. We release our code and evaluation corpora to facilitate\nfuture research.", "published": "2023-05-19 13:38:53", "link": "http://arxiv.org/abs/2305.11673v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set\n  Operations", "abstract": "Formulating selective information needs results in queries that implicitly\nspecify set operations, such as intersection, union, and difference. For\ninstance, one might search for \"shorebirds that are not sandpipers\" or\n\"science-fiction films shot in England\". To study the ability of retrieval\nsystems to meet such information needs, we construct QUEST, a dataset of 3357\nnatural language queries with implicit set operations, that map to a set of\nentities corresponding to Wikipedia documents. The dataset challenges models to\nmatch multiple constraints mentioned in queries with corresponding evidence in\ndocuments and correctly perform various set operations. The dataset is\nconstructed semi-automatically using Wikipedia category names. Queries are\nautomatically composed from individual categories, then paraphrased and further\nvalidated for naturalness and fluency by crowdworkers. Crowdworkers also assess\nthe relevance of entities based on their documents and highlight attribution of\nquery constraints to spans of document text. We analyze several modern\nretrieval systems, finding that they often struggle on such queries. Queries\ninvolving negation and conjunction are particularly challenging and systems are\nfurther challenged with combinations of these operations.", "published": "2023-05-19 14:19:32", "link": "http://arxiv.org/abs/2305.11694v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "S$^3$HQA: A Three-Stage Approach for Multi-hop Text-Table Hybrid\n  Question Answering", "abstract": "Answering multi-hop questions over hybrid factual knowledge from the given\ntext and table (TextTableQA) is a challenging task. Existing models mainly\nadopt a retriever-reader framework, which have several deficiencies, such as\nnoisy labeling in training retriever, insufficient utilization of heterogeneous\ninformation over text and table, and deficient ability for different reasoning\noperations. In this paper, we propose a three-stage TextTableQA framework\nS3HQA, which comprises of retriever, selector, and reasoner. We use a retriever\nwith refinement training to solve the noisy labeling problem. Then, a hybrid\nselector considers the linked relationships between heterogeneous data to\nselect the most relevant factual knowledge. For the final stage, instead of\nadapting a reading comprehension module like in previous methods, we employ a\ngeneration-based reasoner to obtain answers. This includes two approaches: a\nrow-wise generator and an LLM prompting generator~(first time used in this\ntask). The experimental results demonstrate that our method achieves\ncompetitive results in the few-shot setting. When trained on the full dataset,\nour approach outperforms all baseline methods, ranking first on the HybridQA\nleaderboard.", "published": "2023-05-19 15:01:48", "link": "http://arxiv.org/abs/2305.11725v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HalOmi: A Manually Annotated Benchmark for Multilingual Hallucination\n  and Omission Detection in Machine Translation", "abstract": "Hallucinations in machine translation are translations that contain\ninformation completely unrelated to the input. Omissions are translations that\ndo not include some of the input information. While both cases tend to be\ncatastrophic errors undermining user trust, annotated data with these types of\npathologies is extremely scarce and is limited to a few high-resource\nlanguages. In this work, we release an annotated dataset for the hallucination\nand omission phenomena covering 18 translation directions with varying resource\nlevels and scripts. Our annotation covers different levels of partial and full\nhallucinations as well as omissions both at the sentence and at the word level.\nAdditionally, we revisit previous methods for hallucination and omission\ndetection, show that conclusions made based on a single language pair largely\ndo not hold for a large-scale evaluation, and establish new solid baselines.", "published": "2023-05-19 15:33:50", "link": "http://arxiv.org/abs/2305.11746v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large\n  Language Models", "abstract": "Large language models (LLMs), such as ChatGPT, are prone to generate\nhallucinations, i.e., content that conflicts with the source or cannot be\nverified by the factual knowledge. To understand what types of content and to\nwhich extent LLMs are apt to hallucinate, we introduce the Hallucination\nEvaluation benchmark for Large Language Models (HaluEval), a large collection\nof generated and human-annotated hallucinated samples for evaluating the\nperformance of LLMs in recognizing hallucination. To generate these samples, we\npropose a ChatGPT-based two-step framework, i.e., sampling-then-filtering.\nBesides, we also hire some human labelers to annotate the hallucinations in\nChatGPT responses. The empirical results suggest that ChatGPT is likely to\ngenerate hallucinated content in specific topics by fabricating unverifiable\ninformation (i.e., about $19.5\\%$ responses). Moreover, existing LLMs face\ngreat challenges in recognizing the hallucinations in texts. However, our\nexperiments also prove that providing external knowledge or adding reasoning\nsteps can help LLMs recognize hallucinations. Our benchmark can be accessed at\nhttps://github.com/RUCAIBox/HaluEval.", "published": "2023-05-19 15:36:27", "link": "http://arxiv.org/abs/2305.11747v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ReSeTOX: Re-learning attention weights for toxicity mitigation in\n  machine translation", "abstract": "Our proposed method, ReSeTOX (REdo SEarch if TOXic), addresses the issue of\nNeural Machine Translation (NMT) generating translation outputs that contain\ntoxic words not present in the input. The objective is to mitigate the\nintroduction of toxic language without the need for re-training. In the case of\nidentified added toxicity during the inference process, ReSeTOX dynamically\nadjusts the key-value self-attention weights and re-evaluates the beam search\nhypotheses. Experimental results demonstrate that ReSeTOX achieves a remarkable\n57% reduction in added toxicity while maintaining an average translation\nquality of 99.5% across 164 languages.", "published": "2023-05-19 15:46:08", "link": "http://arxiv.org/abs/2305.11761v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Solving NLP Problems through Human-System Collaboration: A\n  Discussion-based Approach", "abstract": "Humans work together to solve common problems by having discussions,\nexplaining, and agreeing or disagreeing with each other. Similarly, if a system\ncan have discussions with humans when solving tasks, it can improve the\nsystem's performance and reliability. In previous research on explainability,\nit has only been possible for the system to make predictions and for humans to\nask questions about them rather than having a mutual exchange of opinions. This\nresearch aims to create a dataset and computational framework for systems that\ndiscuss and refine their predictions through dialogue. Through experiments, we\nshow that the proposed system can have beneficial discussions with humans\nimproving the accuracy by up to 25 points in the natural language inference\ntask.", "published": "2023-05-19 16:24:50", "link": "http://arxiv.org/abs/2305.11789v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prompting with Pseudo-Code Instructions", "abstract": "Prompting with natural language instructions has recently emerged as a\npopular method of harnessing the capabilities of large language models. Given\nthe inherent ambiguity present in natural language, it is intuitive to consider\nthe possible advantages of prompting with less ambiguous prompt styles, such as\nthe use of pseudo-code.\n  In this paper we explore if prompting via pseudo-code instructions helps\nimprove the performance of pre-trained language models. We manually create a\ndataset of pseudo-code prompts for 132 different tasks spanning classification,\nQA and generative language tasks, sourced from the Super-NaturalInstructions\ndataset. Using these prompts along with their counterparts in natural language,\nwe study their performance on two LLM families - BLOOM and CodeGen. Our\nexperiments show that using pseudo-code instructions leads to better results,\nwith an average increase (absolute) of 7-16 points in F1 scores for\nclassification tasks and an improvement (relative) of 12-38% in aggregate\nROUGE-L scores across all tasks. We include detailed ablation studies which\nindicate that code comments, docstrings, and the structural clues encoded in\npseudo-code all contribute towards the improvement in performance.\n  To the best of our knowledge our work is the first to demonstrate how\npseudo-code prompts can be helpful in improving the performance of pre-trained\nLMs.", "published": "2023-05-19 16:25:01", "link": "http://arxiv.org/abs/2305.11790v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Few-shot NER with Prompt Ordering based Data Augmentation", "abstract": "Recently, data augmentation (DA) methods have been proven to be effective for\npre-trained language models (PLMs) in low-resource settings, including few-shot\nnamed entity recognition (NER). However, conventional NER DA methods are mostly\naimed at sequence labeling models, i.e., token-level classification, and few\nare compatible with unified autoregressive generation frameworks, which can\nhandle a wider range of NER tasks, such as nested NER. Furthermore, these\ngeneration frameworks have a strong assumption that the entities will appear in\nthe target sequence with the same left-to-right order as the source sequence.\nIn this paper, we claim that there is no need to keep this strict order, and\nmore diversified but reasonable target entity sequences can be provided during\nthe training stage as a novel DA method. Nevertheless, a naive mixture of\naugmented data can confuse the model since one source sequence will then be\npaired with different target sequences. Therefore, we propose a simple but\neffective Prompt Ordering based Data Augmentation (PODA) method to improve the\ntraining of unified autoregressive generation frameworks under few-shot NER\nscenarios. Experimental results on three public NER datasets and further\nanalyses demonstrate the effectiveness of our approach.", "published": "2023-05-19 16:25:43", "link": "http://arxiv.org/abs/2305.11791v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Inside Story: Towards Better Understanding of Machine Translation\n  Neural Evaluation Metrics", "abstract": "Neural metrics for machine translation evaluation, such as COMET, exhibit\nsignificant improvements in their correlation with human judgments, as compared\nto traditional metrics based on lexical overlap, such as BLEU. Yet, neural\nmetrics are, to a great extent, \"black boxes\" returning a single sentence-level\nscore without transparency about the decision-making process. In this work, we\ndevelop and compare several neural explainability methods and demonstrate their\neffectiveness for interpreting state-of-the-art fine-tuned neural metrics. Our\nstudy reveals that these metrics leverage token-level information that can be\ndirectly attributed to translation errors, as assessed through comparison of\ntoken-level neural saliency maps with Multidimensional Quality Metrics (MQM)\nannotations and with synthetically-generated critical translation errors. To\nease future research, we release our code at:\nhttps://github.com/Unbabel/COMET/tree/explainable-metrics.", "published": "2023-05-19 16:42:17", "link": "http://arxiv.org/abs/2305.11806v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pseudo-Label Training and Model Inertia in Neural Machine Translation", "abstract": "Like many other machine learning applications, neural machine translation\n(NMT) benefits from over-parameterized deep neural models. However, these\nmodels have been observed to be brittle: NMT model predictions are sensitive to\nsmall input changes and can show significant variation across re-training or\nincremental model updates. This work studies a frequently used method in NMT,\npseudo-label training (PLT), which is common to the related techniques of\nforward-translation (or self-training) and sequence-level knowledge\ndistillation. While the effect of PLT on quality is well-documented, we\nhighlight a lesser-known effect: PLT can enhance a model's stability to model\nupdates and input perturbations, a set of properties we call model inertia. We\nstudy inertia effects under different training settings and we identify\ndistribution simplification as a mechanism behind the observed results.", "published": "2023-05-19 16:45:19", "link": "http://arxiv.org/abs/2305.11808v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How to Prompt LLMs for Text-to-SQL: A Study in Zero-shot, Single-domain,\n  and Cross-domain Settings", "abstract": "Large language models (LLMs) with in-context learning have demonstrated\nremarkable capability in the text-to-SQL task. Previous research has prompted\nLLMs with various demonstration-retrieval strategies and intermediate reasoning\nsteps to enhance the performance of LLMs. However, those works often employ\nvaried strategies when constructing the prompt text for text-to-SQL inputs,\nsuch as databases and demonstration examples. This leads to a lack of\ncomparability in both the prompt constructions and their primary contributions.\nFurthermore, selecting an effective prompt construction has emerged as a\npersistent problem for future research. To address this limitation, we\ncomprehensively investigate the impact of prompt constructions across various\nsettings and provide insights into prompt constructions for future text-to-SQL\nstudies.", "published": "2023-05-19 17:43:58", "link": "http://arxiv.org/abs/2305.11853v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Complex Claim Verification with Evidence Retrieved in the Wild", "abstract": "Evidence retrieval is a core part of automatic fact-checking. Prior work\nmakes simplifying assumptions in retrieval that depart from real-world use\ncases: either no access to evidence, access to evidence curated by a human\nfact-checker, or access to evidence available long after the claim has been\nmade. In this work, we present the first fully automated pipeline to check\nreal-world claims by retrieving raw evidence from the web. We restrict our\nretriever to only search documents available prior to the claim's making,\nmodeling the realistic scenario where an emerging claim needs to be checked.\nOur pipeline includes five components: claim decomposition, raw document\nretrieval, fine-grained evidence retrieval, claim-focused summarization, and\nveracity judgment. We conduct experiments on complex political claims in the\nClaimDecomp dataset and show that the aggregated evidence produced by our\npipeline improves veracity judgments. Human evaluation finds the evidence\nsummary produced by our system is reliable (it does not hallucinate\ninformation) and relevant to answering key questions about a claim, suggesting\nthat it can assist fact-checkers even when it cannot surface a complete\nevidence set.", "published": "2023-05-19 17:49:19", "link": "http://arxiv.org/abs/2305.11859v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning\n  and Coding with LLMs", "abstract": "A popular approach for improving the correctness of output from large\nlanguage models (LLMs) is Self-Consistency - poll the LLM multiple times and\noutput the most frequent solution. Existing Self-Consistency techniques always\ngenerate a constant number of samples per question, where a better approach\nwill be to non-uniformly distribute the available budget based on the amount of\nagreement in the samples generated so far. In response, we introduce\nAdaptive-Consistency, a cost-efficient, model-agnostic technique that\ndynamically adjusts the number of samples per question using a lightweight\nstopping criterion. Our experiments over 17 reasoning and code generation\ndatasets and three LLMs demonstrate that Adaptive-Consistency reduces sample\nbudget by up to 7.9 times with an average accuracy drop of less than 0.1%. Our\ncode and data are available at https://www.sample-step-by-step.info", "published": "2023-05-19 17:49:25", "link": "http://arxiv.org/abs/2305.11860v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reducing Sequence Length by Predicting Edit Operations with Large\n  Language Models", "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance in\nvarious tasks and gained significant attention. LLMs are also used for local\nsequence transduction tasks, including grammatical error correction (GEC) and\nformality style transfer, where most tokens in a source text are kept\nunchanged. However, the models that generate all target tokens in such tasks\nhave a tendency to simply copy the input text as is, without making needed\nchanges, because the difference between input and output texts is minimal in\nthe training data. This is also inefficient because the computational cost\ngrows quadratically with the target sequence length with Transformer. This\npaper proposes predicting edit spans for the source text for local sequence\ntransduction tasks. Representing an edit span with a position of the source\ntext and corrected tokens, we can reduce the length of the target sequence and\nthe computational cost for inference. We apply instruction tuning for LLMs on\nthe supervision data of edit spans. Experiments show that the proposed method\nachieves comparable performance to the baseline in four tasks, paraphrasing,\nformality style transfer, GEC, and text simplification, despite reducing the\nlength of the target text by as small as 21%. Furthermore, we report that the\ntask-specific fine-tuning with the proposed method achieved state-of-the-art\nperformance in the four tasks.", "published": "2023-05-19 17:51:05", "link": "http://arxiv.org/abs/2305.11862v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "XTREME-UP: A User-Centric Scarce-Data Benchmark for Under-Represented\n  Languages", "abstract": "Data scarcity is a crucial issue for the development of highly multilingual\nNLP systems. Yet for many under-represented languages (ULs) -- languages for\nwhich NLP re-search is particularly far behind in meeting user needs -- it is\nfeasible to annotate small amounts of data. Motivated by this, we propose\nXTREME-UP, a benchmark defined by: its focus on the scarce-data scenario rather\nthan zero-shot; its focus on user-centric tasks -- tasks with broad adoption by\nspeakers of high-resource languages; and its focus on under-represented\nlanguages where this scarce-data scenario tends to be most realistic. XTREME-UP\nevaluates the capabilities of language models across 88 under-represented\nlanguages over 9 key user-centric technologies including ASR, OCR, MT, and\ninformation access tasks that are of general utility. We create new datasets\nfor OCR, autocomplete, semantic parsing, and transliteration, and build on and\nrefine existing datasets for other tasks. XTREME-UP provides methodology for\nevaluating many modeling scenarios including text-only, multi-modal (vision,\naudio, and text),supervised parameter tuning, and in-context learning. We\nevaluate commonly used models on the benchmark. We release all code and scripts\nto train and evaluate models", "published": "2023-05-19 18:00:03", "link": "http://arxiv.org/abs/2305.11938v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Eye-SpatialNet: Spatial Information Extraction from Ophthalmology Notes", "abstract": "We introduce an annotated corpus of 600 ophthalmology notes labeled with\ndetailed spatial and contextual information of ophthalmic entities. We extend\nour previously proposed frame semantics-based spatial representation schema,\nRad-SpatialNet, to represent spatial language in ophthalmology text, resulting\nin the Eye-SpatialNet schema. The spatially-grounded entities are findings,\nprocedures, and drugs. To accurately capture all spatial details, we add some\ndomain-specific elements in Eye-SpatialNet. The annotated corpus contains 1715\nspatial triggers, 7308 findings, 2424 anatomies, and 9914 descriptors. To\nautomatically extract the spatial information, we employ a two-turn question\nanswering approach based on the transformer language model BERT. The results\nare promising, with F1 scores of 89.31, 74.86, and 88.47 for spatial triggers,\nFigure, and Ground frame elements, respectively. This is the first work to\nrepresent and extract a wide variety of clinical information in ophthalmology.\nExtracting detailed information can benefit ophthalmology applications and\nresearch targeted toward disease progression and screening.", "published": "2023-05-19 18:08:45", "link": "http://arxiv.org/abs/2305.11948v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-QA: Unsupervised Knowledge Guided Language Model Alignment", "abstract": "Large-scale language models like ChatGPT and GPT-4 have gained attention for\ntheir impressive conversational and generative capabilities. However, the\ncreation of supervised paired question-answering data for instruction tuning\npresents formidable challenges. This endeavor necessitates substantial human\neffort for data annotation and wrestles with issues concerning data quality,\ndiversity, accuracy, and other related factors. To overcome these obstacles, we\nintroduce an innovative framework named Self-QA, which replaces the traditional\npractice of human-written instruction seeds with a vast amount of unsupervised\nknowledge, enabling the model to generate a larger quantity of correct and\ndomain-specific instruction data. The effectiveness of our proposed method is\ndemonstrated through experiments conducted on unsupervised corpora from various\ndomains.", "published": "2023-05-19 18:26:26", "link": "http://arxiv.org/abs/2305.11952v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Weak Supervision Approach for Few-Shot Aspect Based Sentiment", "abstract": "We explore how weak supervision on abundant unlabeled data can be leveraged\nto improve few-shot performance in aspect-based sentiment analysis (ABSA)\ntasks. We propose a pipeline approach to construct a noisy ABSA dataset, and we\nuse it to adapt a pre-trained sequence-to-sequence model to the ABSA tasks. We\ntest the resulting model on three widely used ABSA datasets, before and after\nfine-tuning. Our proposed method preserves the full fine-tuning performance\nwhile showing significant improvements (15.84% absolute F1) in the few-shot\nlearning scenario for the harder tasks. In zero-shot (i.e., without\nfine-tuning), our method outperforms the previous state of the art on the\naspect extraction sentiment classification (AESC) task and is, additionally,\ncapable of performing the harder aspect sentiment triplet extraction (ASTE)\ntask.", "published": "2023-05-19 19:53:54", "link": "http://arxiv.org/abs/2305.11979v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluation of medium-large Language Models at zero-shot closed book\n  generative question answering", "abstract": "Large language models (LLMs) have garnered significant attention, but the\ndefinition of \"large\" lacks clarity. This paper focuses on medium-sized\nlanguage models (MLMs), defined as having at least six billion parameters but\nless than 100 billion. The study evaluates MLMs regarding zero-shot generative\nquestion answering, which requires models to provide elaborate answers without\nexternal document retrieval. The paper introduces an own test dataset and\npresents results from human evaluation. Results show that combining the best\nanswers from different MLMs yielded an overall correct answer rate of 82.7%\nwhich is better than the 60.9% of ChatGPT. The best MLM achieved 71.8% and has\n33B parameters, which highlights the importance of using appropriate training\ndata for fine-tuning rather than solely relying on the number of parameters.\nMore fine-grained feedback should be used to further improve the quality of\nanswers. The open source community is quickly closing the gap to the best\ncommercial models.", "published": "2023-05-19 20:33:19", "link": "http://arxiv.org/abs/2305.11991v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Interpretable Word Sense Representations via Definition Generation: The\n  Case of Semantic Change Analysis", "abstract": "We propose using automatically generated natural language definitions of\ncontextualised word usages as interpretable word and word sense\nrepresentations. Given a collection of usage examples for a target word, and\nthe corresponding data-driven usage clusters (i.e., word senses), a definition\nis generated for each usage with a specialised Flan-T5 language model, and the\nmost prototypical definition in a usage cluster is chosen as the sense label.\n  We demonstrate how the resulting sense labels can make existing approaches to\nsemantic change analysis more interpretable, and how they can allow users --\nhistorical linguists, lexicographers, or social scientists -- to explore and\nintuitively explain diachronic trajectories of word meaning. Semantic change\nanalysis is only one of many possible applications of the `definitions as\nrepresentations' paradigm. Beyond being human-readable, contextualised\ndefinitions also outperform token or usage sentence embeddings in\nword-in-context semantic similarity judgements, making them a new promising\ntype of lexical representation for NLP.", "published": "2023-05-19 20:36:21", "link": "http://arxiv.org/abs/2305.11993v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep Learning Approaches to Lexical Simplification: A Survey", "abstract": "Lexical Simplification (LS) is the task of replacing complex for simpler\nwords in a sentence whilst preserving the sentence's original meaning. LS is\nthe lexical component of Text Simplification (TS) with the aim of making texts\nmore accessible to various target populations. A past survey (Paetzold and\nSpecia, 2017) has provided a detailed overview of LS. Since this survey,\nhowever, the AI/NLP community has been taken by storm by recent advances in\ndeep learning, particularly with the introduction of large language models\n(LLM) and prompt learning. The high performance of these models sparked renewed\ninterest in LS. To reflect these recent advances, we present a comprehensive\nsurvey of papers published between 2017 and 2023 on LS and its sub-tasks with a\nspecial focus on deep learning. We also present benchmark datasets for the\nfuture development of LS systems.", "published": "2023-05-19 20:56:22", "link": "http://arxiv.org/abs/2305.12000v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OPT-R: Exploring the Role of Explanations in Finetuning and Prompting\n  for Reasoning Skills of Large Language Models", "abstract": "In this paper, we conduct a thorough investigation into the reasoning\ncapabilities of Large Language Models (LLMs), focusing specifically on the Open\nPretrained Transformers (OPT) models as a representative of such models. Our\nstudy entails finetuning three different sizes of OPT on a carefully curated\nreasoning corpus, resulting in two sets of finetuned models: OPT-R, finetuned\nwithout explanations, and OPT-RE, finetuned with explanations. We then evaluate\nall models on 57 out-of-domain tasks drawn from the SUPER-NATURALINSTRUCTIONS\nbenchmark, covering 26 distinct reasoning skills, utilizing three prompting\ntechniques. Through a comprehensive grid of 27 configurations and 6,156 test\nevaluations, we investigate the dimensions of finetuning, prompting, and scale\nto understand the role of explanations on different reasoning skills. Our\nfindings reveal that having explanations in the fewshot exemplar has no\nsignificant impact on the model's performance when the model is finetuned,\nwhile positively affecting the non-finetuned counterpart. Moreover, we observe\na slight yet consistent increase in classification accuracy as we incorporate\nexplanations during prompting and finetuning, respectively. Finally, we offer\ninsights on which skills benefit the most from incorporating explanations\nduring finetuning and prompting, such as Numerical (+20.4%) and Analogical\n(+13.9%) reasoning, as well as skills that exhibit negligible or negative\neffects.", "published": "2023-05-19 20:58:22", "link": "http://arxiv.org/abs/2305.12001v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "XuanYuan 2.0: A Large Chinese Financial Chat Model with Hundreds of\n  Billions Parameters", "abstract": "In recent years, pre-trained language models have undergone rapid development\nwith the emergence of large-scale models. However, there is a lack of\nopen-sourced chat models specifically designed for the Chinese language,\nespecially in the field of Chinese finance, at the scale of hundreds of\nbillions. To address this gap, we introduce XuanYuan 2.0, the largest Chinese\nchat model to date, built upon the BLOOM-176B architecture. Additionally, we\npropose a novel training method called hybrid-tuning to mitigate catastrophic\nforgetting. By combining general-domain with domain-specific knowledge and\nintegrating the stages of pre-training and fine-tuning, XuanYuan 2.0 is capable\nof providing accurate and contextually appropriate responses in the Chinese\nfinancial domain.", "published": "2023-05-19 21:01:20", "link": "http://arxiv.org/abs/2305.12002v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BOLT: Fast Energy-based Controlled Text Generation with Tunable Biases", "abstract": "Energy-based models (EBMs) have gained popularity for controlled text\ngeneration due to their high applicability to a wide range of constraints.\nHowever, sampling from EBMs is non-trivial, as it often requires a large number\nof iterations to converge to plausible text, which slows down the decoding\nprocess and makes it less practical for real-world applications. In this work,\nwe propose BOLT, which relies on tunable biases to directly adjust the language\nmodel's output logits. Unlike prior work, BOLT maintains the generator's\nautoregressive nature to assert a strong control on token-wise conditional\ndependencies and overall fluency, and thus converges faster. When compared with\nstate-of-the-arts on controlled generation tasks using both soft constraints\n(e.g., sentiment control) and hard constraints (e.g., keyword-guided topic\ncontrol), BOLT demonstrates significantly improved efficiency and fluency. On\nsentiment control, BOLT is 7x faster than competitive baselines, and more\nfluent in 74.4% of the evaluation samples according to human judges.", "published": "2023-05-19 22:02:55", "link": "http://arxiv.org/abs/2305.12018v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ChatGPT for Us: Preserving Data Privacy in ChatGPT via Dialogue Text\n  Ambiguation to Expand Mental Health Care Delivery", "abstract": "Large language models have been useful in expanding mental health care\ndelivery. ChatGPT, in particular, has gained popularity for its ability to\ngenerate human-like dialogue. However, data-sensitive domains -- including but\nnot limited to healthcare -- face challenges in using ChatGPT due to privacy\nand data-ownership concerns. To enable its utilization, we propose a text\nambiguation framework that preserves user privacy. We ground this in the task\nof addressing stress prompted by user-provided texts to demonstrate the\nviability and helpfulness of privacy-preserved generations. Our results suggest\nthat chatGPT recommendations are still able to be moderately helpful and\nrelevant, even when the original user text is not provided.", "published": "2023-05-19 02:09:52", "link": "http://arxiv.org/abs/2306.05552v1", "categories": ["cs.CL", "I.2; I.7"], "primary_category": "cs.CL"}
{"title": "Visualizing Linguistic Diversity of Text Datasets Synthesized by Large\n  Language Models", "abstract": "Large language models (LLMs) can be used to generate smaller, more refined\ndatasets via few-shot prompting for benchmarking, fine-tuning or other use\ncases. However, understanding and evaluating these datasets is difficult, and\nthe failure modes of LLM-generated data are still not well understood.\nSpecifically, the data can be repetitive in surprising ways, not only\nsemantically but also syntactically and lexically. We present LinguisticLens, a\nnovel inter-active visualization tool for making sense of and analyzing\nsyntactic diversity of LLM-generated datasets. LinguisticLens clusters text\nalong syntactic, lexical, and semantic axes. It supports hierarchical\nvisualization of a text dataset, allowing users to quickly scan for an overview\nand inspect individual examples. The live demo is available at\nshorturl.at/zHOUV.", "published": "2023-05-19 00:53:45", "link": "http://arxiv.org/abs/2305.11364v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fast-StrucTexT: An Efficient Hourglass Transformer with Modality-guided\n  Dynamic Token Merge for Document Understanding", "abstract": "Transformers achieve promising performance in document understanding because\nof their high effectiveness and still suffer from quadratic computational\ncomplexity dependency on the sequence length. General efficient transformers\nare challenging to be directly adapted to model document. They are unable to\nhandle the layout representation in documents, e.g. word, line and paragraph,\non different granularity levels and seem hard to achieve a good trade-off\nbetween efficiency and performance. To tackle the concerns, we propose\nFast-StrucTexT, an efficient multi-modal framework based on the StrucTexT\nalgorithm with an hourglass transformer architecture, for visual document\nunderstanding. Specifically, we design a modality-guided dynamic token merging\nblock to make the model learn multi-granularity representation and prunes\nredundant tokens. Additionally, we present a multi-modal interaction module\ncalled Symmetry Cross Attention (SCA) to consider multi-modal fusion and\nefficiently guide the token mergence. The SCA allows one modality input as\nquery to calculate cross attention with another modality in a dual phase.\nExtensive experiments on FUNSD, SROIE, and CORD datasets demonstrate that our\nmodel achieves the state-of-the-art performance and almost 1.9X faster\ninference time than the state-of-the-art methods.", "published": "2023-05-19 02:42:35", "link": "http://arxiv.org/abs/2305.11392v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Comfort Foods and Community Connectedness: Investigating Diet Change\n  during COVID-19 Using YouTube Videos on Twitter", "abstract": "Unprecedented lockdowns at the start of the COVID-19 pandemic have\ndrastically changed the routines of millions of people, potentially impacting\nimportant health-related behaviors. In this study, we use YouTube videos\nembedded in tweets about diet, exercise and fitness posted before and during\nCOVID-19 to investigate the influence of the pandemic lockdowns on diet and\nnutrition. In particular, we examine the nutritional profile of the foods\nmentioned in the transcript, description and title of each video in terms of\nsix macronutrients (protein, energy, fat, sodium, sugar, and saturated fat).\nThese macronutrient values were further linked to demographics to assess if\nthere are specific effects on those potentially having insufficient access to\nhealthy sources of food. Interrupted time series analysis revealed a\nconsiderable shift in the aggregated macronutrient scores before and during\nCOVID-19. In particular, whereas areas with lower incomes showed decrease in\nenergy, fat, and saturated fat, those with higher percentage of African\nAmericans showed an elevation in sodium. Word2Vec word similarities and odds\nratio analysis suggested a shift from popular diets and lifestyle bloggers\nbefore the lockdowns to the interest in a variety of healthy foods, communal\nsharing of quick and easy recipes, as well as a new emphasis on comfort foods.\nTo the best of our knowledge, this work is novel in terms of linking attention\nsignals in tweets, content of videos, their nutrients profile, and aggregate\ndemographics of the users. The insights made possible by this combination of\nresources are important for monitoring the secondary health effects of social\ndistancing, and informing social programs designed to alleviate these effects.", "published": "2023-05-19 02:51:25", "link": "http://arxiv.org/abs/2305.11398v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Post Hoc Explanations of Language Models Can Improve Language Models", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nperforming complex tasks. Moreover, recent research has shown that\nincorporating human-annotated rationales (e.g., Chain-of-Thought prompting)\nduring in-context learning can significantly enhance the performance of these\nmodels, particularly on tasks that require reasoning capabilities. However,\nincorporating such rationales poses challenges in terms of scalability as this\nrequires a high degree of human involvement. In this work, we present a novel\nframework, Amplifying Model Performance by Leveraging In-Context Learning with\nPost Hoc Explanations (AMPLIFY), which addresses the aforementioned challenges\nby automating the process of rationale generation. To this end, we leverage\npost hoc explanation methods which output attribution scores (explanations)\ncapturing the influence of each of the input features on model predictions.\nMore specifically, we construct automated natural language rationales that\nembed insights from post hoc explanations to provide corrective signals to\nLLMs. Extensive experimentation with real-world datasets demonstrates that our\nframework, AMPLIFY, leads to prediction accuracy improvements of about 10-25%\nover a wide range of tasks, including those where prior approaches which rely\non human-annotated rationales such as Chain-of-Thought prompting fall short.\nOur work makes one of the first attempts at highlighting the potential of post\nhoc explanations as valuable tools for enhancing the effectiveness of LLMs.\nFurthermore, we conduct additional empirical analyses and ablation studies to\ndemonstrate the impact of each of the components of AMPLIFY, which, in turn,\nleads to critical insights for refining in-context learning.", "published": "2023-05-19 04:46:04", "link": "http://arxiv.org/abs/2305.11426v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Phonetic and Prosody-aware Self-supervised Learning Approach for\n  Non-native Fluency Scoring", "abstract": "Speech fluency/disfluency can be evaluated by analyzing a range of phonetic\nand prosodic features. Deep neural networks are commonly trained to map\nfluency-related features into the human scores. However, the effectiveness of\ndeep learning-based models is constrained by the limited amount of labeled\ntraining samples. To address this, we introduce a self-supervised learning\n(SSL) approach that takes into account phonetic and prosody awareness for\nfluency scoring. Specifically, we first pre-train the model using a\nreconstruction loss function, by masking phones and their durations jointly on\na large amount of unlabeled speech and text prompts. We then fine-tune the\npre-trained model using human-annotated scoring data. Our experimental results,\nconducted on datasets such as Speechocean762 and our non-native datasets, show\nthat our proposed method outperforms the baseline systems in terms of Pearson\ncorrelation coefficients (PCC). Moreover, we also conduct an ablation study to\nbetter understand the contribution of phonetic and prosody factors during the\npre-training stage.", "published": "2023-05-19 05:39:41", "link": "http://arxiv.org/abs/2305.11438v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "CCGen: Explainable Complementary Concept Generation in E-Commerce", "abstract": "We propose and study Complementary Concept Generation (CCGen): given a\nconcept of interest, e.g., \"Digital Cameras\", generating a list of\ncomplementary concepts, e.g., 1) Camera Lenses 2) Batteries 3) Camera Cases 4)\nMemory Cards 5) Battery Chargers. CCGen is beneficial for various applications\nlike query suggestion and item recommendation, especially in the e-commerce\ndomain. To solve CCGen, we propose to train language models to generate ranked\nlists of concepts with a two-step training strategy. We also teach the models\nto generate explanations by incorporating explanations distilled from large\nteacher models. Extensive experiments and analysis demonstrate that our model\ncan generate high-quality concepts complementary to the input concept while\nproducing explanations to justify the predictions.", "published": "2023-05-19 07:16:04", "link": "http://arxiv.org/abs/2305.11480v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PlugMed: Improving Specificity in Patient-Centered Medical Dialogue\n  Generation using In-Context Learning", "abstract": "The patient-centered medical dialogue systems strive to offer diagnostic\ninterpretation services to users who are less knowledgeable about medical\nknowledge, through emphasizing the importance of providing responses specific\nto the patients. It is difficult for the large language models (LLMs) to\nguarantee the specificity of responses in spite of its promising performance\neven in some tasks in medical field. Inspired by in-context learning, we\npropose PlugMed, a Plug-and-Play Medical Dialogue System, for addressing this\nchallenge. PlugMed is equipped with two modules, the prompt generation (PG)\nmodule and the response ranking (RR) module, to enhances LLMs' dialogue\nstrategies for improving the specificity of the dialogue. The PG module is\ndesigned to stimulate the imitative ability of LLMs by providing them with real\ndialogues from similar patients as prompts. The RR module incorporates\nfine-tuned small model as response filter to enable the selection of\nappropriate responses generated by LLMs. Furthermore, we introduce a new\nevaluation method based on matching both user's intent and high-frequency\nmedical term to effectively assess the specificity of the responses. We conduct\nexperimental evaluations on three medical dialogue datasets, and the results,\nincluding both automatic and human evaluation, demonstrate the effectiveness of\nour approach.", "published": "2023-05-19 08:18:24", "link": "http://arxiv.org/abs/2305.11508v2", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "DiffuSIA: A Spiral Interaction Architecture for Encoder-Decoder Text\n  Diffusion", "abstract": "Diffusion models have emerged as the new state-of-the-art family of deep\ngenerative models, and their promising potentials for text generation have\nrecently attracted increasing attention. Existing studies mostly adopt a single\nencoder architecture with partially noising processes for conditional text\ngeneration, but its degree of flexibility for conditional modeling is limited.\nIn fact, the encoder-decoder architecture is naturally more flexible for its\ndetachable encoder and decoder modules, which is extensible to multilingual and\nmultimodal generation tasks for conditions and target texts. However, the\nencoding process of conditional texts lacks the understanding of target texts.\nTo this end, a spiral interaction architecture for encoder-decoder text\ndiffusion (DiffuSIA) is proposed. Concretely, the conditional information from\nencoder is designed to be captured by the diffusion decoder, while the target\ninformation from decoder is designed to be captured by the conditional encoder.\nThese two types of information flow run through multilayer interaction spirally\nfor deep fusion and understanding. DiffuSIA is evaluated on four text\ngeneration tasks, including paraphrase, text simplification, question\ngeneration, and open-domain dialogue generation. Experimental results show that\nDiffuSIA achieves competitive performance among previous methods on all four\ntasks, demonstrating the effectiveness and generalization ability of the\nproposed method.", "published": "2023-05-19 08:30:11", "link": "http://arxiv.org/abs/2305.11517v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Sequence-to-Sequence Approach for Arabic Pronoun Resolution", "abstract": "This paper proposes a sequence-to-sequence learning approach for Arabic\npronoun resolution, which explores the effectiveness of using advanced natural\nlanguage processing (NLP) techniques, specifically Bi-LSTM and the BERT\npre-trained Language Model, in solving the pronoun resolution problem in\nArabic. The proposed approach is evaluated on the AnATAr dataset, and its\nperformance is compared to several baseline models, including traditional\nmachine learning models and handcrafted feature-based models. Our results\ndemonstrate that the proposed model outperforms the baseline models, which\ninclude KNN, logistic regression, and SVM, across all metrics. In addition, we\nexplore the effectiveness of various modifications to the model, including\nconcatenating the anaphor text beside the paragraph text as input, adding a\nmask to focus on candidate scores, and filtering candidates based on gender and\nnumber agreement with the anaphor. Our results show that these modifications\nsignificantly improve the model's performance, achieving up to 81% on MRR and\n71% for F1 score while also demonstrating higher precision, recall, and\naccuracy. These findings suggest that the proposed model is an effective\napproach to Arabic pronoun resolution and highlights the potential benefits of\nleveraging advanced NLP neural models.", "published": "2023-05-19 08:53:41", "link": "http://arxiv.org/abs/2305.11529v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PORTRAIT: a hybrid aPproach tO cReate extractive ground-TRuth summAry\n  for dIsaster evenT", "abstract": "Disaster summarization approaches provide an overview of the important\ninformation posted during disaster events on social media platforms, such as,\nTwitter. However, the type of information posted significantly varies across\ndisasters depending on several factors like the location, type, severity, etc.\nVerification of the effectiveness of disaster summarization approaches still\nsuffer due to the lack of availability of good spectrum of datasets along with\nthe ground-truth summary. Existing approaches for ground-truth summary\ngeneration (ground-truth for extractive summarization) relies on the wisdom and\nintuition of the annotators. Annotators are provided with a complete set of\ninput tweets from which a subset of tweets is selected by the annotators for\nthe summary. This process requires immense human effort and significant time.\nAdditionally, this intuition-based selection of the tweets might lead to a high\nvariance in summaries generated across annotators. Therefore, to handle these\nchallenges, we propose a hybrid (semi-automated) approach (PORTRAIT) where we\npartly automate the ground-truth summary generation procedure. This approach\nreduces the effort and time of the annotators while ensuring the quality of the\ncreated ground-truth summary. We validate the effectiveness of PORTRAIT on 5\ndisaster events through quantitative and qualitative comparisons of\nground-truth summaries generated by existing intuitive approaches, a\nsemi-automated approach, and PORTRAIT. We prepare and release the ground-truth\nsummaries for 5 disaster events which consist of both natural and man-made\ndisaster events belonging to 4 different countries. Finally, we provide a study\nabout the performance of various state-of-the-art summarization approaches on\nthe ground-truth summaries generated by PORTRAIT using ROUGE-N F1-scores.", "published": "2023-05-19 09:07:52", "link": "http://arxiv.org/abs/2305.11536v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Efficient Cross-Lingual Transfer for Chinese Stable Diffusion with\n  Images as Pivots", "abstract": "Diffusion models have made impressive progress in text-to-image synthesis.\nHowever, training such large-scale models (e.g. Stable Diffusion), from scratch\nrequires high computational costs and massive high-quality text-image pairs,\nwhich becomes unaffordable in other languages. To handle this challenge, we\npropose IAP, a simple but effective method to transfer English Stable Diffusion\ninto Chinese. IAP optimizes only a separate Chinese text encoder with all other\nparameters fixed to align Chinese semantics space to the English one in CLIP.\nTo achieve this, we innovatively treat images as pivots and minimize the\ndistance of attentive features produced from cross-attention between images and\neach language respectively. In this way, IAP establishes connections of\nChinese, English and visual semantics in CLIP's embedding space efficiently,\nadvancing the quality of the generated image with direct Chinese prompts.\nExperimental results show that our method outperforms several strong Chinese\ndiffusion models with only 5%~10% training data.", "published": "2023-05-19 09:20:27", "link": "http://arxiv.org/abs/2305.11540v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Empower Large Language Model to Perform Better on Industrial\n  Domain-Specific Question Answering", "abstract": "Large Language Model (LLM) has gained popularity and achieved remarkable\nresults in open-domain tasks, but its performance in real industrial\ndomain-specific scenarios is average due to its lack of specific domain\nknowledge. This issue has attracted widespread attention, but there are few\nrelevant benchmarks available. In this paper, we provide a benchmark Question\nAnswering (QA) dataset named MSQA, centered around Microsoft products and IT\ntechnical problems encountered by customers. This dataset contains industry\ncloud-specific QA knowledge, an area not extensively covered in general LLMs,\nmaking it well-suited for evaluating methods aiming to enhance LLMs'\ndomain-specific capabilities. In addition, we propose a new model interaction\nparadigm that can empower LLM to achieve better performance on domain-specific\ntasks where it is not proficient. Extensive experiments demonstrate that the\napproach following our method outperforms the commonly used LLM with retrieval\nmethods. We make our source code and sample data available at:\nhttps://aka.ms/Microsoft_QA.", "published": "2023-05-19 09:23:25", "link": "http://arxiv.org/abs/2305.11541v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via\n  Tool Embeddings", "abstract": "Augmenting large language models (LLMs) with external tools has emerged as a\npromising approach to solving complex problems. However, traditional methods,\nwhich finetune LLMs with tool demonstration data, can be both costly and\nrestricted to a predefined set of tools. Recent in-context learning paradigm\nalleviates these issues, but the limited context length only allows for a few\nshots of demonstrations, leading to suboptimal understandings of the tools.\nMoreover, when there are numerous tools to choose from, in-context learning\ncould completely fail to work. In this paper, we propose an alternative\napproach, $\\textbf{ToolkenGPT}$, which combines the benefits of both sides. Our\napproach represents each $\\underline{tool}$ as a to$\\underline{ken}$\n($\\textit{toolken}$) and learns an embedding for it, enabling tool calls in the\nsame way as generating a regular word token. Once a toolken is triggered, the\nLLM is prompted to complete arguments for the tool to execute. ToolkenGPT\noffers the flexibility to plug in an arbitrary number of tools by expanding the\nset of toolkens on the fly. In addition, it improves tool use by allowing\nextensive demonstration data for learning the toolken embeddings. In diverse\ndomains, including numerical reasoning, knowledge-based question answering, and\nembodied plan generation, our approach effectively augments LLMs with tools and\nsubstantially outperforms various latest baselines. ToolkenGPT demonstrates the\npromising ability to use relevant tools from a large tool set in complex\nscenarios.", "published": "2023-05-19 09:54:21", "link": "http://arxiv.org/abs/2305.11554v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Blank-regularized CTC for Frame Skipping in Neural Transducer", "abstract": "Neural Transducer and connectionist temporal classification (CTC) are popular\nend-to-end automatic speech recognition systems. Due to their frame-synchronous\ndesign, blank symbols are introduced to address the length mismatch between\nacoustic frames and output tokens, which might bring redundant computation.\nPrevious studies managed to accelerate the training and inference of neural\nTransducers by discarding frames based on the blank symbols predicted by a\nco-trained CTC. However, there is no guarantee that the co-trained CTC can\nmaximize the ratio of blank symbols. This paper proposes two novel\nregularization methods to explicitly encourage more blanks by constraining the\nself-loop of non-blank symbols in the CTC. It is interesting to find that the\nframe reduction ratio of the neural Transducer can approach the theoretical\nboundary. Experiments on LibriSpeech corpus show that our proposed method\naccelerates the inference of neural Transducer by 4 times without sacrificing\nperformance. Our work is open-sourced and publicly available\nhttps://github.com/k2-fsa/icefall.", "published": "2023-05-19 09:56:09", "link": "http://arxiv.org/abs/2305.11558v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Decouple knowledge from parameters for plug-and-play language modeling", "abstract": "Pre-trained language models(PLM) have made impressive results in various NLP\ntasks. It has been revealed that one of the key factors to their success is the\nparameters of these models implicitly learn all kinds of knowledge during\npre-training. However, encoding knowledge implicitly in the model parameters\nhas two fundamental drawbacks. First, the knowledge is neither editable nor\nscalable once the model is trained, which is especially problematic in that\nknowledge is consistently evolving. Second, it lacks interpretability and\nprevents humans from understanding which knowledge PLM requires for a certain\nproblem. In this paper, we introduce PlugLM, a pre-training model with\ndifferentiable plug-in memory(DPM). The key intuition is to decouple the\nknowledge storage from model parameters with an editable and scalable key-value\nmemory and leverage knowledge in an explainable manner by knowledge retrieval\nin the DPM. To justify this design choice, we conduct evaluations in three\nsettings including: (1) domain adaptation. PlugLM obtains 3.95 F1 improvements\nacross four domains on average without any in-domain pre-training. (2)\nknowledge update. PlugLM could absorb new knowledge in a training-free way\nafter pre-training is done. (3) in-task knowledge learning. PlugLM could be\nfurther improved by incorporating training samples into DPM with knowledge\nprompting.", "published": "2023-05-19 10:01:55", "link": "http://arxiv.org/abs/2305.11564v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Speech-Text Dialog Pre-training for Spoken Dialog Understanding with\n  Explicit Cross-Modal Alignment", "abstract": "Recently, speech-text pre-training methods have shown remarkable success in\nmany speech and natural language processing tasks. However, most previous\npre-trained models are usually tailored for one or two specific tasks, but fail\nto conquer a wide range of speech-text tasks. In addition, existing speech-text\npre-training methods fail to explore the contextual information within a\ndialogue to enrich utterance representations. In this paper, we propose\nSpeech-text dialog Pre-training for spoken dialog understanding with ExpliCiT\ncRoss-Modal Alignment (SPECTRA), which is the first-ever speech-text dialog\npre-training model. Concretely, to consider the temporality of speech modality,\nwe design a novel temporal position prediction task to capture the speech-text\nalignment. This pre-training task aims to predict the start and end time of\neach textual word in the corresponding speech waveform. In addition, to learn\nthe characteristics of spoken dialogs, we generalize a response selection task\nfrom textual dialog pre-training to speech-text dialog pre-training scenarios.\nExperimental results on four different downstream speech-text tasks demonstrate\nthe superiority of SPECTRA in learning speech-text alignment and multi-turn\ndialog context.", "published": "2023-05-19 10:37:56", "link": "http://arxiv.org/abs/2305.11579v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "IKDSumm: Incorporating Key-phrases into BERT for extractive Disaster\n  Tweet Summarization", "abstract": "Online social media platforms, such as Twitter, are one of the most valuable\nsources of information during disaster events. Therefore, humanitarian\norganizations, government agencies, and volunteers rely on a summary of this\ninformation, i.e., tweets, for effective disaster management. Although there\nare several existing supervised and unsupervised approaches for automated tweet\nsummary approaches, these approaches either require extensive labeled\ninformation or do not incorporate specific domain knowledge of disasters.\nAdditionally, the most recent approaches to disaster summarization have\nproposed BERT-based models to enhance the summary quality. However, for further\nimproved performance, we introduce the utilization of domain-specific knowledge\nwithout any human efforts to understand the importance (salience) of a tweet\nwhich further aids in summary creation and improves summary quality. In this\npaper, we propose a disaster-specific tweet summarization framework, IKDSumm,\nwhich initially identifies the crucial and important information from each\ntweet related to a disaster through key-phrases of that tweet. We identify\nthese key-phrases by utilizing the domain knowledge (using existing ontology)\nof disasters without any human intervention. Further, we utilize these\nkey-phrases to automatically generate a summary of the tweets. Therefore, given\ntweets related to a disaster, IKDSumm ensures fulfillment of the summarization\nkey objectives, such as information coverage, relevance, and diversity in\nsummary without any human intervention. We evaluate the performance of IKDSumm\nwith 8 state-of-the-art techniques on 12 disaster datasets. The evaluation\nresults show that IKDSumm outperforms existing techniques by approximately\n2-79% in terms of ROUGE-N F1-score.", "published": "2023-05-19 11:05:55", "link": "http://arxiv.org/abs/2305.11592v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Examining Inter-Consistency of Large Language Models Collaboration: An\n  In-depth Analysis via Debate", "abstract": "Large Language Models (LLMs) have shown impressive capabilities in various\napplications, but they still face various inconsistency issues. Existing works\nprimarily focus on the inconsistency issues within a single LLM, while we\ncomplementarily explore the inter-consistency among multiple LLMs for\ncollaboration. To examine whether LLMs can collaborate effectively to achieve a\nconsensus for a shared goal, we focus on commonsense reasoning, and introduce a\nformal debate framework (FORD) to conduct a three-stage debate among LLMs with\nreal-world scenarios alignment: fair debate, mismatched debate, and roundtable\ndebate. Through extensive experiments on various datasets, LLMs can effectively\ncollaborate to reach a consensus despite noticeable inter-inconsistencies, but\nimbalances in their abilities can lead to domination by superior LLMs.\nLeveraging a more advanced LLM like GPT-4 as an authoritative judge can boost\ncollaboration performance. Our work contributes to understanding the\ninter-consistency among LLMs and lays the foundation for developing future\ncollaboration methods. Codes and data are available at\nhttps://github.com/Waste-Wood/FORD", "published": "2023-05-19 11:15:33", "link": "http://arxiv.org/abs/2305.11595v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mitigating Backdoor Poisoning Attacks through the Lens of Spurious\n  Correlation", "abstract": "Modern NLP models are often trained over large untrusted datasets, raising\nthe potential for a malicious adversary to compromise model behaviour. For\ninstance, backdoors can be implanted through crafting training instances with a\nspecific textual trigger and a target label. This paper posits that backdoor\npoisoning attacks exhibit \\emph{spurious correlation} between simple text\nfeatures and classification labels, and accordingly, proposes methods for\nmitigating spurious correlation as means of defence. Our empirical study\nreveals that the malicious triggers are highly correlated to their target\nlabels; therefore such correlations are extremely distinguishable compared to\nthose scores of benign features, and can be used to filter out potentially\nproblematic instances. Compared with several existing defences, our defence\nmethod significantly reduces attack success rates across backdoor attacks, and\nin the case of insertion-based attacks, our method provides a near-perfect\ndefence.", "published": "2023-05-19 11:18:20", "link": "http://arxiv.org/abs/2305.11596v2", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Introspective Tips: Large Language Model for In-Context Decision Making", "abstract": "The emergence of large language models (LLMs) has substantially influenced\nnatural language processing, demonstrating exceptional results across various\ntasks. In this study, we employ ``Introspective Tips\" to facilitate LLMs in\nself-optimizing their decision-making. By introspectively examining\ntrajectories, LLM refines its policy by generating succinct and valuable tips.\nOur method enhances the agent's performance in both few-shot and zero-shot\nlearning situations by considering three essential scenarios: learning from the\nagent's past experiences, integrating expert demonstrations, and generalizing\nacross diverse games. Importantly, we accomplish these improvements without\nfine-tuning the LLM parameters; rather, we adjust the prompt to generalize\ninsights from the three aforementioned situations. Our framework not only\nsupports but also emphasizes the advantage of employing LLM in in-contxt\ndecision-making. Experiments involving over 100 games in TextWorld illustrate\nthe superior performance of our approach.", "published": "2023-05-19 11:20:37", "link": "http://arxiv.org/abs/2305.11598v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Searching by Code: a New SearchBySnippet Dataset and SnippeR Retrieval\n  Model for Searching by Code Snippets", "abstract": "Code search is an important and well-studied task, but it usually means\nsearching for code by a text query. We argue that using a code snippet (and\npossibly an error traceback) as a query while looking for bugfixing\ninstructions and code samples is a natural use case not covered by prior art.\nMoreover, existing datasets use code comments rather than full-text\ndescriptions as text, making them unsuitable for this use case. We present a\nnew SearchBySnippet dataset implementing the search-by-code use case based on\nStackOverflow data; we show that on SearchBySnippet, existing architectures\nfall short of a simple BM25 baseline even after fine-tuning. We present a new\nsingle encoder model SnippeR that outperforms several strong baselines on\nSearchBySnippet with a result of 0.451 Recall@10; we propose the\nSearchBySnippet dataset and SnippeR as a new important benchmark for code\nsearch evaluation.", "published": "2023-05-19 12:09:30", "link": "http://arxiv.org/abs/2305.11625v2", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "CCT-Code: Cross-Consistency Training for Multilingual Clone Detection\n  and Code Search", "abstract": "We consider the well-known and important tasks of clone detection and\ninformation retrieval for source code. The most standard setup is to search\nclones inside the same language code snippets. But it is also useful to find\ncode snippets with identical behaviour in different programming languages.\nNevertheless multi- and cross-lingual clone detection has been little studied\nin literature. We present a novel training procedure, cross-consistency\ntraining (CCT) leveraging cross-lingual similarity, that we apply to train\nlanguage models on source code in various programming languages. We show that\nthis training is effective both for encoder- and decoder-based models. The\ntrained encoder-based CCT-LM model achieves a new state of the art on POJ-104\n(monolingual C++ clone detection benchmark) with 96.73\\% MAP and AdvTest\n(monolingual Python code search benchmark) with 47.18\\% MRR. The decoder-based\nCCT-LM model shows comparable performance in these tasks. In addition, we\nformulate the multi- and cross-lingual clone detection problem and present XCD,\na new benchmark dataset produced from CodeForces submissions.", "published": "2023-05-19 12:09:49", "link": "http://arxiv.org/abs/2305.11626v2", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Separating form and meaning: Using self-consistency to quantify task\n  understanding across multiple senses", "abstract": "At the staggering pace with which the capabilities of large language models\n(LLMs) are increasing, creating future-proof evaluation sets to assess their\nunderstanding becomes more and more challenging. In this paper, we propose a\nnovel paradigm for evaluating LLMs which leverages the idea that correct world\nunderstanding should be consistent across different (Fregean) senses of the\nsame meaning. Accordingly, we measure understanding not in terms of correctness\nbut by evaluating consistency across multiple senses that are generated by the\nmodel itself. We showcase our approach by instantiating a test where the\ndifferent senses are different languages, hence using multilingual\nself-consistency as a litmus test for the model's understanding and\nsimultaneously addressing the important topic of multilinguality. Taking one of\nthe latest versions of ChatGPT as our object of study, we evaluate multilingual\nconsistency for two different tasks across three different languages. We show\nthat its multilingual consistency is still lacking, and that its task and world\nunderstanding are thus not language-independent. As our approach does not\nrequire any static evaluation corpora in languages other than English, it can\neasily and cheaply be extended to different languages and tasks and could\nbecome an integral part of future benchmarking efforts.", "published": "2023-05-19 13:23:51", "link": "http://arxiv.org/abs/2305.11662v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Information Screening whilst Exploiting! Multimodal Relation Extraction\n  with Feature Denoising and Multimodal Topic Modeling", "abstract": "Existing research on multimodal relation extraction (MRE) faces two\nco-existing challenges, internal-information over-utilization and\nexternal-information under-exploitation. To combat that, we propose a novel\nframework that simultaneously implements the idea of internal-information\nscreening and external-information exploiting. First, we represent the\nfine-grained semantic structures of the input image and text with the visual\nand textual scene graphs, which are further fused into a unified cross-modal\ngraph (CMG). Based on CMG, we perform structure refinement with the guidance of\nthe graph information bottleneck principle, actively denoising the\nless-informative features. Next, we perform topic modeling over the input image\nand text, incorporating latent multimodal topic features to enrich the\ncontexts. On the benchmark MRE dataset, our system outperforms the current best\nmodel significantly. With further in-depth analyses, we reveal the great\npotential of our method for the MRE task. Our codes are open at\nhttps://github.com/ChocoWu/MRE-ISE.", "published": "2023-05-19 14:56:57", "link": "http://arxiv.org/abs/2305.11719v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Persian Typographical Error Type Detection Using Deep Neural Networks on\n  Algorithmically-Generated Misspellings", "abstract": "Spelling correction is a remarkable challenge in the field of natural\nlanguage processing. The objective of spelling correction tasks is to recognize\nand rectify spelling errors automatically. The development of applications that\ncan effectually diagnose and correct Persian spelling and grammatical errors\nhas become more important in order to improve the quality of Persian text. The\nTypographical Error Type Detection in Persian is a relatively understudied\narea. Therefore, this paper presents a compelling approach for detecting\ntypographical errors in Persian texts. Our work includes the presentation of a\npublicly available dataset called FarsTypo, which comprises 3.4 million words\narranged in chronological order and tagged with their corresponding\npart-of-speech. These words cover a wide range of topics and linguistic styles.\nWe develop an algorithm designed to apply Persian-specific errors to a scalable\nportion of these words, resulting in a parallel dataset of correct and\nincorrect words. By leveraging FarsTypo, we establish a strong foundation and\nconduct a thorough comparison of various methodologies employing different\narchitectures. Additionally, we introduce a groundbreaking Deep Sequential\nNeural Network that utilizes both word and character embeddings, along with\nbidirectional LSTM layers, for token classification aimed at detecting\ntypographical errors across 51 distinct classes. Our approach is contrasted\nwith highly advanced industrial systems that, unlike this study, have been\ndeveloped using a diverse range of resources. The outcomes of our final method\nproved to be highly competitive, achieving an accuracy of 97.62%, precision of\n98.83%, recall of 98.61%, and surpassing others in terms of speed.", "published": "2023-05-19 15:05:39", "link": "http://arxiv.org/abs/2305.11731v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CRITIC: Large Language Models Can Self-Correct with Tool-Interactive\n  Critiquing", "abstract": "Recent developments in large language models (LLMs) have been impressive.\nHowever, these models sometimes show inconsistencies and problematic behavior,\nsuch as hallucinating facts, generating flawed code, or creating offensive and\ntoxic content. Unlike these models, humans typically utilize external tools to\ncross-check and refine their initial content, like using a search engine for\nfact-checking, or a code interpreter for debugging. Inspired by this\nobservation, we introduce a framework called CRITIC that allows LLMs, which are\nessentially \"black boxes\" to validate and progressively amend their own outputs\nin a manner similar to human interaction with tools. More specifically,\nstarting with an initial output, CRITIC interacts with appropriate tools to\nevaluate certain aspects of the text, and then revises the output based on the\nfeedback obtained during this validation process. Comprehensive evaluations\ninvolving free-form question answering, mathematical program synthesis, and\ntoxicity reduction demonstrate that CRITIC consistently enhances the\nperformance of LLMs. Meanwhile, our research highlights the crucial importance\nof external feedback in promoting the ongoing self-improvement of LLMs.", "published": "2023-05-19 15:19:44", "link": "http://arxiv.org/abs/2305.11738v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ReFIT: Relevance Feedback from a Reranker during Inference", "abstract": "Retrieve-and-rerank is a prevalent framework in neural information retrieval,\nwherein a bi-encoder network initially retrieves a pre-defined number of\ncandidates (e.g., K=100), which are then reranked by a more powerful\ncross-encoder model. While the reranker often yields improved candidate scores\ncompared to the retriever, its scope is confined to only the top K retrieved\ncandidates. As a result, the reranker cannot improve retrieval performance in\nterms of Recall@K. In this work, we propose to leverage the reranker to improve\nrecall by making it provide relevance feedback to the retriever at inference\ntime. Specifically, given a test instance during inference, we distill the\nreranker's predictions for that instance into the retriever's query\nrepresentation using a lightweight update mechanism. The aim of the\ndistillation loss is to align the retriever's candidate scores more closely\nwith those produced by the reranker. The algorithm then proceeds by executing a\nsecond retrieval step using the updated query vector. We empirically\ndemonstrate that this method, applicable to various retrieve-and-rerank\nframeworks, substantially enhances retrieval recall across multiple domains,\nlanguages, and modalities.", "published": "2023-05-19 15:30:33", "link": "http://arxiv.org/abs/2305.11744v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Controlling the Extraction of Memorized Data from Large Language Models\n  via Prompt-Tuning", "abstract": "Large Language Models (LLMs) are known to memorize significant portions of\ntheir training data. Parts of this memorized content have been shown to be\nextractable by simply querying the model, which poses a privacy risk. We\npresent a novel approach which uses prompt-tuning to control the extraction\nrates of memorized content in LLMs. We present two prompt training strategies\nto increase and decrease extraction rates, which correspond to an attack and a\ndefense, respectively. We demonstrate the effectiveness of our techniques by\nusing models from the GPT-Neo family on a public benchmark. For the 1.3B\nparameter GPT-Neo model, our attack yields a 9.3 percentage point increase in\nextraction rate compared to our baseline. Our defense can be tuned to achieve\ndifferent privacy-utility trade-offs by a user-specified hyperparameter. We\nachieve an extraction rate reduction of up to 97.7% relative to our baseline,\nwith a perplexity increase of 16.9%.", "published": "2023-05-19 15:45:29", "link": "http://arxiv.org/abs/2305.11759v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Generating Visual Spatial Description via Holistic 3D Scene\n  Understanding", "abstract": "Visual spatial description (VSD) aims to generate texts that describe the\nspatial relations of the given objects within images. Existing VSD work merely\nmodels the 2D geometrical vision features, thus inevitably falling prey to the\nproblem of skewed spatial understanding of target objects. In this work, we\ninvestigate the incorporation of 3D scene features for VSD. With an external 3D\nscene extractor, we obtain the 3D objects and scene features for input images,\nbased on which we construct a target object-centered 3D spatial scene graph\n(Go3D-S2G), such that we model the spatial semantics of target objects within\nthe holistic 3D scenes. Besides, we propose a scene subgraph selecting\nmechanism, sampling topologically-diverse subgraphs from Go3D-S2G, where the\ndiverse local structure features are navigated to yield spatially-diversified\ntext generation. Experimental results on two VSD datasets demonstrate that our\nframework outperforms the baselines significantly, especially improving on the\ncases with complex visual spatial relations. Meanwhile, our method can produce\nmore spatially-diversified generation. Code is available at\nhttps://github.com/zhaoyucs/VSD.", "published": "2023-05-19 15:53:56", "link": "http://arxiv.org/abs/2305.11768v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Cross-Lingual Supervision improves Large Language Models Pre-training", "abstract": "The recent rapid progress in pre-training Large Language Models has relied on\nusing self-supervised language modeling objectives like next token prediction\nor span corruption. On the other hand, Machine Translation Systems are mostly\ntrained using cross-lingual supervision that requires aligned data between\nsource and target languages. We demonstrate that pre-training Large Language\nModels on a mixture of a self-supervised Language Modeling objective and the\nsupervised Machine Translation objective, therefore including cross-lingual\nparallel data during pre-training, yields models with better in-context\nlearning abilities. As pre-training is a very resource-intensive process and a\ngrid search on the best mixing ratio between the two objectives is\nprohibitively expensive, we propose a simple yet effective strategy to learn it\nduring pre-training.", "published": "2023-05-19 16:14:07", "link": "http://arxiv.org/abs/2305.11778v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DMDD: A Large-Scale Dataset for Dataset Mentions Detection", "abstract": "The recognition of dataset names is a critical task for automatic information\nextraction in scientific literature, enabling researchers to understand and\nidentify research opportunities. However, existing corpora for dataset mention\ndetection are limited in size and naming diversity. In this paper, we introduce\nthe Dataset Mentions Detection Dataset (DMDD), the largest publicly available\ncorpus for this task. DMDD consists of the DMDD main corpus, comprising 31,219\nscientific articles with over 449,000 dataset mentions weakly annotated in the\nformat of in-text spans, and an evaluation set, which comprises of 450\nscientific articles manually annotated for evaluation purposes. We use DMDD to\nestablish baseline performance for dataset mention detection and linking. By\nanalyzing the performance of various models on DMDD, we are able to identify\nopen problems in dataset mention detection. We invite the community to use our\ndataset as a challenge to develop novel dataset mention detection models.", "published": "2023-05-19 16:18:00", "link": "http://arxiv.org/abs/2305.11779v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Cue-CoT: Chain-of-thought Prompting for Responding to In-depth Dialogue\n  Questions with LLMs", "abstract": "Large Language Models (LLMs), such as \\texttt{ChatGPT}, greatly empower\ndialogue systems with strong language understanding and generation\ncapabilities. However, most of the previous works prompt the LLMs to directly\ngenerate a response based on the dialogue context, overlooking the underlying\nlinguistic cues about the user status exhibited in the context. Such in-depth\ndialogue scenarios are challenging for existing LLMs to figure out the user's\nhidden needs and respond satisfactorily through a single-step inference. To\nthis end, we propose a novel linguistic cue-based chain-of-thoughts\n(\\textit{Cue}-CoT), which enhances the LLMs inference with an intermediate\nreasoning step to find cues exhibited in the dialogue, aiming to provide a more\npersonalized and engaging response. To evaluate the approach, we build a\nbenchmark with in-depth dialogue questions, consisting of 6 datasets in both\nChinese and English, targeting 3 major linguistic cues during the conversation:\n\\textit{personality}, \\textit{emotion}, and \\textit{psychology}. We conduct\nextensive experiments on the proposed benchmark with 5 LLMs under both\nzero-shot and one-shot settings. Empirical results demonstrate our proposed\n\\textit{Cue}-CoT method outperforms standard prompting methods in terms of both\n\\textit{helpfulness} and \\textit{acceptability} on all datasets.", "published": "2023-05-19 16:27:43", "link": "http://arxiv.org/abs/2305.11792v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ReTAG: Reasoning Aware Table to Analytic Text Generation", "abstract": "The task of table summarization involves generating text that both succinctly\nand accurately represents the table or a specific set of highlighted cells\nwithin a table. While significant progress has been made in table to text\ngeneration techniques, models still mostly generate descriptive summaries,\nwhich reiterates the information contained within the table in sentences.\nThrough analysis of popular table to text benchmarks (ToTTo (Parikh et al.,\n2020 and InfoTabs (Gupta et al., 2020) we observe that in order to generate the\nideal summary, multiple types of reasoning is needed coupled with access to\nknowledge beyond the scope of the table. To address this gap, we propose ReTAG,\na table and reasoning aware model that uses vector-quantization to infuse\ndifferent types of analytical reasoning into the output. ReTAG achieves 2.2%,\n2.9% improvement on the PARENT metric in the relevant slice of ToTTo and\nInfoTabs for the table to text generation task over state of the art baselines.\nThrough human evaluation, we observe that output from ReTAG is upto 12% more\nfaithful and analytical compared to a strong table-aware model. To the best of\nour knowledge, ReTAG is the first model that can controllably use multiple\nreasoning methods within a structure-aware sequence to sequence model to\nsurpass state of the art performance in multiple table to text tasks. We extend\n(and open source 35.6K analytical, 55.9k descriptive instances) the ToTTo,\nInfoTabs datasets with the reasoning categories used in each reference\nsentences.", "published": "2023-05-19 17:03:09", "link": "http://arxiv.org/abs/2305.11826v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SeeGULL: A Stereotype Benchmark with Broad Geo-Cultural Coverage\n  Leveraging Generative Models", "abstract": "Stereotype benchmark datasets are crucial to detect and mitigate social\nstereotypes about groups of people in NLP models. However, existing datasets\nare limited in size and coverage, and are largely restricted to stereotypes\nprevalent in the Western society. This is especially problematic as language\ntechnologies gain hold across the globe. To address this gap, we present\nSeeGULL, a broad-coverage stereotype dataset, built by utilizing generative\ncapabilities of large language models such as PaLM, and GPT-3, and leveraging a\nglobally diverse rater pool to validate the prevalence of those stereotypes in\nsociety. SeeGULL is in English, and contains stereotypes about identity groups\nspanning 178 countries across 8 different geo-political regions across 6\ncontinents, as well as state-level identities within the US and India. We also\ninclude fine-grained offensiveness scores for different stereotypes and\ndemonstrate their global disparities. Furthermore, we include comparative\nannotations about the same groups by annotators living in the region vs. those\nthat are based in North America, and demonstrate that within-region stereotypes\nabout groups differ from those prevalent in North America. CONTENT WARNING:\nThis paper contains stereotype examples that may be offensive.", "published": "2023-05-19 17:30:19", "link": "http://arxiv.org/abs/2305.11840v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "How Does Generative Retrieval Scale to Millions of Passages?", "abstract": "Popularized by the Differentiable Search Index, the emerging paradigm of\ngenerative retrieval re-frames the classic information retrieval problem into a\nsequence-to-sequence modeling task, forgoing external indices and encoding an\nentire document corpus within a single Transformer. Although many different\napproaches have been proposed to improve the effectiveness of generative\nretrieval, they have only been evaluated on document corpora on the order of\n100k in size. We conduct the first empirical study of generative retrieval\ntechniques across various corpus scales, ultimately scaling up to the entire MS\nMARCO passage ranking task with a corpus of 8.8M passages and evaluating model\nsizes up to 11B parameters. We uncover several findings about scaling\ngenerative retrieval to millions of passages; notably, the central importance\nof using synthetic queries as document representations during indexing, the\nineffectiveness of existing proposed architecture modifications when accounting\nfor compute cost, and the limits of naively scaling model parameters with\nrespect to retrieval performance. While we find that generative retrieval is\ncompetitive with state-of-the-art dual encoders on small corpora, scaling to\nmillions of passages remains an important and unsolved challenge. We believe\nthese findings will be valuable for the community to clarify the current state\nof generative retrieval, highlight the unique challenges, and inspire new\nresearch directions.", "published": "2023-05-19 17:33:38", "link": "http://arxiv.org/abs/2305.11841v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Scaling laws for language encoding models in fMRI", "abstract": "Representations from transformer-based unidirectional language models are\nknown to be effective at predicting brain responses to natural language.\nHowever, most studies comparing language models to brains have used GPT-2 or\nsimilarly sized language models. Here we tested whether larger open-source\nmodels such as those from the OPT and LLaMA families are better at predicting\nbrain responses recorded using fMRI. Mirroring scaling results from other\ncontexts, we found that brain prediction performance scales logarithmically\nwith model size from 125M to 30B parameter models, with ~15% increased encoding\nperformance as measured by correlation with a held-out test set across 3\nsubjects. Similar logarithmic behavior was observed when scaling the size of\nthe fMRI training set. We also characterized scaling for acoustic encoding\nmodels that use HuBERT, WavLM, and Whisper, and we found comparable\nimprovements with model size. A noise ceiling analysis of these large,\nhigh-performance encoding models showed that performance is nearing the\ntheoretical maximum for brain areas such as the precuneus and higher auditory\ncortex. These results suggest that increasing scale in both models and data\nwill yield incredibly effective models of language processing in the brain,\nenabling better scientific understanding as well as applications such as\ndecoding.", "published": "2023-05-19 17:53:03", "link": "http://arxiv.org/abs/2305.11863v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "North S\u00e1mi Dialect Identification with Self-supervised Speech Models", "abstract": "The North S\\'{a}mi (NS) language encapsulates four primary dialectal variants\nthat are related but that also have differences in their phonology, morphology,\nand vocabulary. The unique geopolitical location of NS speakers means that in\nmany cases they are bilingual in S\\'{a}mi as well as in the dominant state\nlanguage: Norwegian, Swedish, or Finnish. This enables us to study the NS\nvariants both with respect to the spoken state language and their acoustic\ncharacteristics. In this paper, we investigate an extensive set of acoustic\nfeatures, including MFCCs and prosodic features, as well as state-of-the-art\nself-supervised representations, namely, XLS-R, WavLM, and HuBERT, for the\nautomatic detection of the four NS variants. In addition, we examine how the\nmajority state language is reflected in the dialects. Our results show that NS\ndialects are influenced by the state language and that the four dialects are\nseparable, reaching high classification accuracy, especially with the XLS-R\nmodel.", "published": "2023-05-19 17:53:12", "link": "http://arxiv.org/abs/2305.11864v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Exploring the Viability of Synthetic Query Generation for Relevance\n  Prediction", "abstract": "Query-document relevance prediction is a critical problem in Information\nRetrieval systems. This problem has increasingly been tackled using\n(pretrained) transformer-based models which are finetuned using large\ncollections of labeled data. However, in specialized domains such as e-commerce\nand healthcare, the viability of this approach is limited by the dearth of\nlarge in-domain data. To address this paucity, recent methods leverage these\npowerful models to generate high-quality task and domain-specific synthetic\ndata. Prior work has largely explored synthetic data generation or query\ngeneration (QGen) for Question-Answering (QA) and binary (yes/no) relevance\nprediction, where for instance, the QGen models are given a document, and\ntrained to generate a query relevant to that document. However in many\nproblems, we have a more fine-grained notion of relevance than a simple yes/no\nlabel. Thus, in this work, we conduct a detailed study into how QGen approaches\ncan be leveraged for nuanced relevance prediction. We demonstrate that --\ncontrary to claims from prior works -- current QGen approaches fall short of\nthe more conventional cross-domain transfer-learning approaches. Via empirical\nstudies spanning 3 public e-commerce benchmarks, we identify new shortcomings\nof existing QGen approaches -- including their inability to distinguish between\ndifferent grades of relevance. To address this, we introduce label-conditioned\nQGen models which incorporates knowledge about the different relevance. While\nour experiments demonstrate that these modifications help improve performance\nof QGen techniques, we also find that QGen approaches struggle to capture the\nfull nuance of the relevance label space and as a result the generated queries\nare not faithful to the desired relevance label.", "published": "2023-05-19 18:03:36", "link": "http://arxiv.org/abs/2305.11944v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Polar Ducks and Where to Find Them: Enhancing Entity Linking with Duck\n  Typing and Polar Box Embeddings", "abstract": "Entity linking methods based on dense retrieval are an efficient and widely\nused solution in large-scale applications, but they fall short of the\nperformance of generative models, as they are sensitive to the structure of the\nembedding space. In order to address this issue, this paper introduces DUCK, an\napproach to infusing structural information in the space of entity\nrepresentations, using prior knowledge of entity types. Inspired by duck typing\nin programming languages, we propose to define the type of an entity based on\nthe relations that it has with other entities in a knowledge graph. Then,\nporting the concept of box embeddings to spherical polar coordinates, we\npropose to represent relations as boxes on the hypersphere. We optimize the\nmodel to cluster entities of similar type by placing them inside the boxes\ncorresponding to their relations. Our experiments show that our method sets new\nstate-of-the-art results on standard entity-disambiguation benchmarks, it\nimproves the performance of the model by up to 7.9 F1 points, outperforms other\ntype-aware approaches, and matches the results of generative models with 18\ntimes more parameters.", "published": "2023-05-19 22:42:16", "link": "http://arxiv.org/abs/2305.12027v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Clinical Camel: An Open Expert-Level Medical Language Model with\n  Dialogue-Based Knowledge Encoding", "abstract": "We present Clinical Camel, an open large language model (LLM) explicitly\ntailored for clinical research. Fine-tuned from LLaMA-2 using QLoRA, Clinical\nCamel achieves state-of-the-art performance across medical benchmarks among\nopenly available medical LLMs. Leveraging efficient single-GPU training,\nClinical Camel surpasses GPT-3.5 in five-shot evaluations on all assessed\nbenchmarks, including 64.3% on the USMLE Sample Exam (compared to 58.5% for\nGPT-3.5), 77.9% on PubMedQA (compared to 60.2%), 60.7% on MedQA (compared to\n53.6%), and 54.2% on MedMCQA (compared to 51.0%). In addition to these\nbenchmarks, Clinical Camel demonstrates its broader capabilities, such as\nsynthesizing plausible clinical notes. This work introduces dialogue-based\nknowledge encoding, a novel method to synthesize conversational data from dense\nmedical texts. While benchmark results are encouraging, extensive and rigorous\nhuman evaluation across diverse clinical scenarios is imperative to ascertain\nsafety before implementation. By openly sharing Clinical Camel, we hope to\nfoster transparent and collaborative research, working towards the safe\nintegration of LLMs within the healthcare domain. Significant challenges\nconcerning reliability, bias, and the potential for outdated knowledge persist.\nNonetheless, the transparency provided by an open approach reinforces the\nscientific rigor essential for future clinical applications.", "published": "2023-05-19 23:07:09", "link": "http://arxiv.org/abs/2305.12031v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A New Benchmark of Aphasia Speech Recognition and Detection Based on\n  E-Branchformer and Multi-task Learning", "abstract": "Aphasia is a language disorder that affects the speaking ability of millions\nof patients. This paper presents a new benchmark for Aphasia speech recognition\nand detection tasks using state-of-the-art speech recognition techniques with\nthe AphsiaBank dataset. Specifically, we introduce two multi-task learning\nmethods based on the CTC/Attention architecture to perform both tasks\nsimultaneously. Our system achieves state-of-the-art speaker-level detection\naccuracy (97.3%), and a relative WER reduction of 11% for moderate Aphasia\npatients. In addition, we demonstrate the generalizability of our approach by\napplying it to another disordered speech database, the DementiaBank Pitt\ncorpus. We will make our all-in-one recipes and pre-trained model publicly\navailable to facilitate reproducibility. Our standardized data preprocessing\npipeline and open-source recipes enable researchers to compare results\ndirectly, promoting progress in disordered speech processing.", "published": "2023-05-19 15:10:36", "link": "http://arxiv.org/abs/2305.13331v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "AlignAtt: Using Attention-based Audio-Translation Alignments as a Guide\n  for Simultaneous Speech Translation", "abstract": "Attention is the core mechanism of today's most used architectures for\nnatural language processing and has been analyzed from many perspectives,\nincluding its effectiveness for machine translation-related tasks. Among these\nstudies, attention resulted to be a useful source of information to get\ninsights about word alignment also when the input text is substituted with\naudio segments, as in the case of the speech translation (ST) task. In this\npaper, we propose AlignAtt, a novel policy for simultaneous ST (SimulST) that\nexploits the attention information to generate source-target alignments that\nguide the model during inference. Through experiments on the 8 language pairs\nof MuST-C v1.0, we show that AlignAtt outperforms previous state-of-the-art\nSimulST policies applied to offline-trained models with gains in terms of BLEU\nof 2 points and latency reductions ranging from 0.5s to 0.8s across the 8\nlanguages.", "published": "2023-05-19 03:31:42", "link": "http://arxiv.org/abs/2305.11408v2", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "DUB: Discrete Unit Back-translation for Speech Translation", "abstract": "How can speech-to-text translation (ST) perform as well as machine\ntranslation (MT)? The key point is to bridge the modality gap between speech\nand text so that useful MT techniques can be applied to ST. Recently, the\napproach of representing speech with unsupervised discrete units yields a new\nway to ease the modality problem. This motivates us to propose Discrete Unit\nBack-translation (DUB) to answer two questions: (1) Is it better to represent\nspeech with discrete units than with continuous features in direct ST? (2) How\nmuch benefit can useful MT techniques bring to ST? With DUB, the\nback-translation technique can successfully be applied on direct ST and obtains\nan average boost of 5.5 BLEU on MuST-C En-De/Fr/Es. In the low-resource\nlanguage scenario, our method achieves comparable performance to existing\nmethods that rely on large-scale external data. Code and models are available\nat https://github.com/0nutation/DUB.", "published": "2023-05-19 03:48:16", "link": "http://arxiv.org/abs/2305.11411v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks", "abstract": "While LLMs have shown great success in understanding and generating text in\ntraditional conversational settings, their potential for performing ill-defined\ncomplex tasks is largely under-studied. Indeed, we are yet to conduct\ncomprehensive benchmarking studies with multiple LLMs that are exclusively\nfocused on a complex task. However, conducting such benchmarking studies is\nchallenging because of the large variations in LLMs' performance when different\nprompt types/styles are used and different degrees of detail are provided in\nthe prompts. To address this issue, the paper proposes a general taxonomy that\ncan be used to design prompts with specific properties in order to perform a\nwide range of complex tasks. This taxonomy will allow future benchmarking\nstudies to report the specific categories of prompts used as part of the study,\nenabling meaningful comparisons across different studies. Also, by establishing\na common standard through this taxonomy, researchers will be able to draw more\naccurate conclusions about LLMs' performance on a specific complex task.", "published": "2023-05-19 04:59:34", "link": "http://arxiv.org/abs/2305.11430v2", "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.LG", "I.2.7"], "primary_category": "cs.AI"}
{"title": "Syllable Discovery and Cross-Lingual Generalization in a Visually\n  Grounded, Self-Supervised Speech Model", "abstract": "In this paper, we show that representations capturing syllabic units emerge\nwhen training a self-supervised speech model with a visually-grounded training\nobjective. We demonstrate that a nearly identical model architecture (HuBERT)\ntrained with a masked language modeling loss does not exhibit this same\nability, suggesting that the visual grounding objective is responsible for the\nemergence of this phenomenon. We propose the use of a minimum cut algorithm to\nautomatically predict syllable boundaries in speech, followed by a 2-stage\nclustering method to group identical syllables together. We show that our model\nnot only outperforms a state-of-the-art syllabic segmentation method on the\nlanguage it was trained on (English), but also generalizes in a zero-shot\nfashion to Estonian. Finally, we show that the same model is capable of\nzero-shot generalization for a word segmentation task on 4 other languages from\nthe Zerospeech Challenge, in some cases beating the previous state-of-the-art.", "published": "2023-05-19 05:19:04", "link": "http://arxiv.org/abs/2305.11435v2", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Zero-Shot Text Classification via Self-Supervised Tuning", "abstract": "Existing solutions to zero-shot text classification either conduct prompting\nwith pre-trained language models, which is sensitive to the choices of\ntemplates, or rely on large-scale annotated data of relevant tasks for\nmeta-tuning. In this work, we propose a new paradigm based on self-supervised\nlearning to solve zero-shot text classification tasks by tuning the language\nmodels with unlabeled data, called self-supervised tuning. By exploring the\ninherent structure of free texts, we propose a new learning objective called\nfirst sentence prediction to bridge the gap between unlabeled data and text\nclassification tasks. After tuning the model to learn to predict the first\nsentence in a paragraph based on the rest, the model is able to conduct\nzero-shot inference on unseen tasks such as topic classification and sentiment\nanalysis. Experimental results show that our model outperforms the\nstate-of-the-art baselines on 7 out of 10 tasks. Moreover, the analysis reveals\nthat our model is less sensitive to the prompt design. Our code and pre-trained\nmodels are publicly available at https://github.com/DAMO-NLP-SG/SSTuning .", "published": "2023-05-19 05:47:33", "link": "http://arxiv.org/abs/2305.11442v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Arukikata Travelogue Dataset", "abstract": "We have constructed Arukikata Travelogue Dataset and released it free of\ncharge for academic research. This dataset is a Japanese text dataset with a\ntotal of over 31 million words, comprising 4,672 Japanese domestic travelogues\nand 9,607 overseas travelogues. Before providing our dataset, there was a\nscarcity of widely available travelogue data for research purposes, and each\nresearcher had to prepare their own data. This hinders the replication of\nexisting studies and fair comparative analysis of experimental results. Our\ndataset enables any researchers to conduct investigation on the same data and\nto ensure transparency and reproducibility in research. In this paper, we\ndescribe the academic significance, characteristics, and prospects of our\ndataset.", "published": "2023-05-19 05:53:49", "link": "http://arxiv.org/abs/2305.11444v1", "categories": ["cs.CL", "cs.AI", "cs.DL"], "primary_category": "cs.CL"}
{"title": "Shattering the Agent-Environment Interface for Fine-Tuning Inclusive\n  Language Models", "abstract": "A centerpiece of the ever-popular reinforcement learning from human feedback\n(RLHF) approach to fine-tuning autoregressive language models is the explicit\ntraining of a reward model to emulate human feedback, distinct from the\nlanguage model itself. This reward model is then coupled with policy-gradient\nmethods to dramatically improve the alignment between language model outputs\nand desired responses. In this work, we adopt a novel perspective wherein a\npre-trained language model is itself simultaneously a policy, reward function,\nand transition function. An immediate consequence of this is that reward\nlearning and language model fine-tuning can be performed jointly and directly,\nwithout requiring any further downstream policy optimization. While this\nperspective does indeed break the traditional agent-environment interface, we\nnevertheless maintain that there can be enormous statistical benefits afforded\nby bringing to bear traditional algorithmic concepts from reinforcement\nlearning. Our experiments demonstrate one concrete instance of this through\nefficient exploration based on the representation and resolution of epistemic\nuncertainty. In order to illustrate these ideas in a transparent manner, we\nrestrict attention to a simple didactic data generating process and leave for\nfuture work extension to systems of practical scale.", "published": "2023-05-19 06:21:15", "link": "http://arxiv.org/abs/2305.11455v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Self-Agreement: A Framework for Fine-tuning Language Models to Find\n  Agreement among Diverse Opinions", "abstract": "Finding an agreement among diverse opinions is a challenging topic in\nmultiagent systems. Recently, large language models (LLMs) have shown great\npotential in addressing this challenge due to their remarkable capabilities in\ncomprehending human opinions and generating human-like text. However, they\ntypically rely on extensive human-annotated data. In this paper, we propose\nSelf-Agreement, a novel framework for fine-tuning LLMs to autonomously find\nagreement using data generated by LLM itself. Specifically, our approach\nemploys the generative pre-trained transformer-3 (GPT-3) to generate multiple\nopinions for each question in a question dataset and create several agreement\ncandidates among these opinions. Then, a bidirectional encoder representations\nfrom transformers (BERT)-based model evaluates the agreement score of each\nagreement candidate and selects the one with the highest agreement score. This\nprocess yields a dataset of question-opinion-agreements, which we use to\nfine-tune a pre-trained LLM for discovering agreements among diverse opinions.\nRemarkably, a pre-trained LLM fine-tuned by our Self-Agreement framework\nachieves comparable performance to GPT-3 with only 1/25 of its parameters,\nshowcasing its ability to identify agreement among various opinions without the\nneed for human-annotated data.", "published": "2023-05-19 06:27:16", "link": "http://arxiv.org/abs/2305.11460v1", "categories": ["cs.CL", "cs.AI", "cs.MA"], "primary_category": "cs.CL"}
{"title": "Graphologue: Exploring Large Language Model Responses with Interactive\n  Diagrams", "abstract": "Large language models (LLMs) have recently soared in popularity due to their\nease of access and the unprecedented ability to synthesize text responses to\ndiverse user questions. However, LLMs like ChatGPT present significant\nlimitations in supporting complex information tasks due to the insufficient\naffordances of the text-based medium and linear conversational structure.\nThrough a formative study with ten participants, we found that LLM interfaces\noften present long-winded responses, making it difficult for people to quickly\ncomprehend and interact flexibly with various pieces of information,\nparticularly during more complex tasks. We present Graphologue, an interactive\nsystem that converts text-based responses from LLMs into graphical diagrams to\nfacilitate information-seeking and question-answering tasks. Graphologue\nemploys novel prompting strategies and interface designs to extract entities\nand relationships from LLM responses and constructs node-link diagrams in\nreal-time. Further, users can interact with the diagrams to flexibly adjust the\ngraphical presentation and to submit context-specific prompts to obtain more\ninformation. Utilizing diagrams, Graphologue enables graphical, non-linear\ndialogues between humans and LLMs, facilitating information exploration,\norganization, and comprehension.", "published": "2023-05-19 06:53:25", "link": "http://arxiv.org/abs/2305.11473v2", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "LLM-CXR: Instruction-Finetuned LLM for CXR Image Understanding and\n  Generation", "abstract": "Following the impressive development of LLMs, vision-language alignment in\nLLMs is actively being researched to enable multimodal reasoning and visual IO.\nThis direction of research is particularly relevant to medical imaging because\nmedical image analysis and generation consist of reasoning based on a\ncombination of visual features and prior knowledge. Many recent works have\nfocused on training adapter networks that serve as an information bridge\nbetween image processing networks and LLMs; but presumably, in order to achieve\nmaximum reasoning potential of LLMs on visual information as well, visual and\nlanguage features should be allowed to interact more freely. This is especially\nimportant in the medical domain because understanding and generating medical\nimages such as chest X-rays (CXR) require not only accurate visual and\nlanguage-based reasoning but also a more intimate mapping between the two\nmodalities. Thus, taking inspiration from previous work on the transformer and\nVQ-GAN combination for bidirectional image and text generation, we build upon\nthis approach and develop a method for instruction-tuning an LLM pre-trained\nonly on text to gain vision-language capabilities for medical images.\nSpecifically, we leverage a pretrained LLM's existing question-answering and\ninstruction-following abilities to teach it to understand visual inputs by\ninstructing it to answer questions about image inputs and, symmetrically,\noutput both text and image responses appropriate to a given query by tuning the\nLLM with diverse tasks that encompass image-based text-generation and\ntext-based image-generation. We show that our model, LLM-CXR, trained in this\napproach shows better image-text alignment in both CXR understanding and\ngeneration tasks while being smaller in size compared to previously developed\nmodels that perform a narrower range of tasks. The code is at\nhttps://github.com/hyn2028/llm-cxr.", "published": "2023-05-19 07:44:39", "link": "http://arxiv.org/abs/2305.11490v5", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "TreePrompt: Learning to Compose Tree Prompts for Explainable Visual\n  Grounding", "abstract": "Prompt tuning has achieved great success in transferring the knowledge from\nlarge pretrained vision-language models into downstream tasks, and has\ndominated the performance on visual grounding (VG). However, almost all\nexisting prompt tuning paradigms suffer from poor interpretability. In this\npaper, we argue that their poor interpretability is attributed to the holistic\nprompt generation and inference process. By \"holistic\", we mean that they\nusually directly learn a set of vectors as the prompt (i.e., prompt\ngeneration), and use the learned global prompt to augment the textual input for\nthe VG model (i.e., prompt inference). To this end, we propose a new prompt\nconstruction paradigm with explicit explainable ability, named TreePrompt.\nSpecifically, we first deconstruct a complex sentence into a tree, that is\nconsistent with human reasoning. Then, following the syntax tree, we compose a\nstructured prompt in a bottom-up manner. Thanks to this step-by-step prompt\nconstruction process, each intermediate prompt (i.e., tree node) permits us to\nunderstand the reasoning process. Extensive ablations on various backbones and\nbenchmarks consistently demonstrate the effectiveness and interpretability of\nour TreePrompt.", "published": "2023-05-19 07:52:22", "link": "http://arxiv.org/abs/2305.11497v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Recouple Event Field via Probabilistic Bias for Event Extraction", "abstract": "Event Extraction (EE), aiming to identify and classify event triggers and\narguments from event mentions, has benefited from pre-trained language models\n(PLMs). However, existing PLM-based methods ignore the information of\ntrigger/argument fields, which is crucial for understanding event schemas. To\nthis end, we propose a Probabilistic reCoupling model enhanced Event extraction\nframework (ProCE). Specifically, we first model the syntactic-related event\nfields as probabilistic biases, to clarify the event fields from ambiguous\nentanglement. Furthermore, considering multiple occurrences of the same\ntriggers/arguments in EE, we explore probabilistic interaction strategies among\nmultiple fields of the same triggers/arguments, to recouple the corresponding\nclarified distributions and capture more latent information fields. Experiments\non EE datasets demonstrate the effectiveness and generalization of our proposed\napproach.", "published": "2023-05-19 07:55:37", "link": "http://arxiv.org/abs/2305.11498v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "InstructIE: A Bilingual Instruction-based Information Extraction Dataset", "abstract": "Large language models can perform well on general natural language tasks, but\ntheir effectiveness is still suboptimal for information extraction (IE). Recent\nworks indicate that the main reason lies in the lack of extensive data on IE\ninstructions. Note that the existing datasets on IE instructions not only have\nlimited coverage but also involve high construction costs. To address this\nissue, we introduce InstructIE, a bilingual instruction-based IE dataset, which\ncovers 12 diverse domains. We propose KG2Instruction, a framework specifically\nfor the automatic generation of such datasets. Additionally, we manually\nannotate the test set. Experimental results demonstrate that large language\nmodels trained with InstructIE can not only obtain better IE capabilities but\nalso enhance zero-shot performance compared with baselines.", "published": "2023-05-19 08:51:11", "link": "http://arxiv.org/abs/2305.11527v4", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Language-Universal Phonetic Representation in Multilingual Speech\n  Pretraining for Low-Resource Speech Recognition", "abstract": "We improve low-resource ASR by integrating the ideas of multilingual training\nand self-supervised learning. Concretely, we leverage an International Phonetic\nAlphabet (IPA) multilingual model to create frame-level pseudo labels for\nunlabeled speech, and use these pseudo labels to guide hidden-unit BERT\n(HuBERT) based speech pretraining in a phonetically-informed manner. The\nexperiments on the Multilingual Speech (MLS) Corpus show that the proposed\napproach consistently outperforms the standard HuBERT on all the target\nlanguages. Moreover, on 3 of the 4 languages, comparing to the standard HuBERT,\nthe approach performs better, meanwhile is able to save supervised training\ndata by 1.5k hours (75%) at most. Our approach outperforms most of the state of\nthe arts, with much less pretraining data in terms of hours and language\ndiversity. Compared to XLSR-53 and a retraining based multilingual method, our\napproach performs better with full and limited finetuning data scenarios.", "published": "2023-05-19 10:15:11", "link": "http://arxiv.org/abs/2305.11569v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Language-universal phonetic encoder for low-resource speech recognition", "abstract": "Multilingual training is effective in improving low-resource ASR, which may\npartially be explained by phonetic representation sharing between languages. In\nend-to-end (E2E) ASR systems, graphemes are often used as basic modeling units,\nhowever graphemes may not be ideal for multilingual phonetic sharing. In this\npaper, we leverage International Phonetic Alphabet (IPA) based\nlanguage-universal phonetic model to improve low-resource ASR performances, for\nthe first time within the attention encoder-decoder architecture. We propose an\nadaptation method on the phonetic IPA model to further improve the proposed\napproach on extreme low-resource languages. Experiments carried out on the\nopen-source MLS corpus and our internal databases show our approach outperforms\nbaseline monolingual models and most state-of-the-art works. Our main approach\nand adaptation are effective on extremely low-resource languages, even within\ndomain- and language-mismatched scenarios.", "published": "2023-05-19 10:24:30", "link": "http://arxiv.org/abs/2305.11576v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Algorithmic failure as a humanities methodology: machine learning's\n  mispredictions identify rich cases for qualitative analysis", "abstract": "This commentary tests a methodology proposed by Munk et al. (2022) for using\nfailed predictions in machine learning as a method to identify ambiguous and\nrich cases for qualitative analysis. Using a dataset describing actions\nperformed by fictional characters interacting with machine vision technologies\nin 500 artworks, movies, novels and videogames, I trained a simple machine\nlearning algorithm (using the kNN algorithm in R) to predict whether or not an\naction was active or passive using only information about the fictional\ncharacters. Predictable actions were generally unemotional and unambiguous\nactivities where machine vision technologies were treated as simple tools.\nUnpredictable actions, that is, actions that the algorithm could not correctly\npredict, were more ambivalent and emotionally loaded, with more complex power\nrelationships between characters and technologies. The results thus support\nMunk et al.'s theory that failed predictions can be productively used to\nidentify rich cases for qualitative analysis. This test goes beyond simply\nreplicating Munk et al.'s results by demonstrating that the method can be\napplied to a broader humanities domain, and that it does not require complex\nneural networks but can also work with a simpler machine learning algorithm.\nFurther research is needed to develop an understanding of what kinds of data\nthe method is useful for and which kinds of machine learning are most\ngenerative. To support this, the R code required to produce the results is\nincluded so the test can be replicated. The code can also be reused or adapted\nto test the method on other datasets.", "published": "2023-05-19 13:24:32", "link": "http://arxiv.org/abs/2305.11663v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY", "J.5"], "primary_category": "cs.LG"}
{"title": "Sensing of inspiration events from speech: comparison of deep learning\n  and linguistic methods", "abstract": "Respiratory chest belt sensor can be used to measure the respiratory rate and\nother respiratory health parameters. Virtual Respiratory Belt, VRB, algorithms\nestimate the belt sensor waveform from speech audio. In this paper we compare\nthe detection of inspiration events (IE) from respiratory belt sensor data\nusing a novel neural VRB algorithm and the detections based on time-aligned\nlinguistic content. The results show the superiority of the VRB method over\nword pause detection or grammatical content segmentation. The comparison of the\nmethods show that both read and spontaneous speech content has a significant\namount of ungrammatical breathing, that is, breathing events that are not\naligned with grammatically appropriate places in language. This study gives new\ninsights into the development of VRB methods and adds to the general\nunderstanding of speech breathing behavior. Moreover, a new VRB method, VRBOLA,\nfor the reconstruction of the continuous breathing waveform is demonstrated.", "published": "2023-05-19 14:06:16", "link": "http://arxiv.org/abs/2305.11683v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Recycle-and-Distill: Universal Compression Strategy for\n  Transformer-based Speech SSL Models with Attention Map Reusing and Masking\n  Distillation", "abstract": "Transformer-based speech self-supervised learning (SSL) models, such as\nHuBERT, show surprising performance in various speech processing tasks.\nHowever, huge number of parameters in speech SSL models necessitate the\ncompression to a more compact model for wider usage in academia or small\ncompanies. In this study, we suggest to reuse attention maps across the\nTransformer layers, so as to remove key and query parameters while retaining\nthe number of layers. Furthermore, we propose a novel masking distillation\nstrategy to improve the student model's speech representation quality. We\nextend the distillation loss to utilize both masked and unmasked speech frames\nto fully leverage the teacher model's high-quality representation. Our\nuniversal compression strategy yields the student model that achieves phoneme\nerror rate (PER) of 7.72% and word error rate (WER) of 9.96% on the SUPERB\nbenchmark.", "published": "2023-05-19 14:07:43", "link": "http://arxiv.org/abs/2305.11685v2", "categories": ["eess.AS", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "What Comes Next? Evaluating Uncertainty in Neural Text Generators\n  Against Human Production Variability", "abstract": "In Natural Language Generation (NLG) tasks, for any input, multiple\ncommunicative goals are plausible, and any goal can be put into words, or\nproduced, in multiple ways. We characterise the extent to which human\nproduction varies lexically, syntactically, and semantically across four NLG\ntasks, connecting human production variability to aleatoric or data\nuncertainty. We then inspect the space of output strings shaped by a generation\nsystem's predicted probability distribution and decoding algorithm to probe its\nuncertainty. For each test input, we measure the generator's calibration to\nhuman production variability. Following this instance-level approach, we\nanalyse NLG models and decoding strategies, demonstrating that probing a\ngenerator with multiple samples and, when possible, multiple references,\nprovides the level of detail necessary to gain understanding of a model's\nrepresentation of uncertainty. Code available at\nhttps://github.com/dmg-illc/nlg-uncertainty-probes.", "published": "2023-05-19 14:41:55", "link": "http://arxiv.org/abs/2305.11707v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Enhancing Vision-Language Pre-Training with Jointly Learned Questioner\n  and Dense Captioner", "abstract": "Large pre-trained multimodal models have demonstrated significant success in\na range of downstream tasks, including image captioning, image-text retrieval,\nvisual question answering (VQA), etc. However, many of these methods rely on\nimage-text pairs collected from the web as pre-training data and unfortunately\noverlook the need for fine-grained feature alignment between vision and\nlanguage modalities, which requires detailed understanding of images and\nlanguage expressions. While integrating VQA and dense captioning (DC) into\npre-training can address this issue, acquiring image-question-answer as well as\nimage-location-caption triplets is challenging and time-consuming.\nAdditionally, publicly available datasets for VQA and dense captioning are\ntypically limited in scale due to manual data collection and labeling efforts.\nIn this paper, we propose a novel method called Joint QA and DC GEneration\n(JADE), which utilizes a pre-trained multimodal model and easily-crawled\nimage-text pairs to automatically generate and filter large-scale VQA and dense\ncaptioning datasets. We apply this method to the Conceptual Caption (CC3M)\ndataset to generate a new dataset called CC3M-QA-DC. Experiments show that when\nused for pre-training in a multi-task manner, CC3M-QA-DC can improve the\nperformance with various backbones on various downstream tasks. Furthermore,\nour generated CC3M-QA-DC can be combined with larger image-text datasets (e.g.,\nCC15M) and achieve competitive results compared with models using much more\ndata. Code and dataset are available at\nhttps://github.com/johncaged/OPT_Questioner.", "published": "2023-05-19 15:54:40", "link": "http://arxiv.org/abs/2305.11769v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Appraising the Potential Uses and Harms of LLMs for Medical Systematic\n  Reviews", "abstract": "Medical systematic reviews play a vital role in healthcare decision making\nand policy. However, their production is time-consuming, limiting the\navailability of high-quality and up-to-date evidence summaries. Recent\nadvancements in large language models (LLMs) offer the potential to\nautomatically generate literature reviews on demand, addressing this issue.\nHowever, LLMs sometimes generate inaccurate (and potentially misleading) texts\nby hallucination or omission. In healthcare, this can make LLMs unusable at\nbest and dangerous at worst. We conducted 16 interviews with international\nsystematic review experts to characterize the perceived utility and risks of\nLLMs in the specific context of medical evidence reviews. Experts indicated\nthat LLMs can assist in the writing process by drafting summaries, generating\ntemplates, distilling information, and crosschecking information. They also\nraised concerns regarding confidently composed but inaccurate LLM outputs and\nother potential downstream harms, including decreased accountability and\nproliferation of low-quality reviews. Informed by this qualitative analysis, we\nidentify criteria for rigorous evaluation of biomedical LLMs aligned with\ndomain expert views.", "published": "2023-05-19 17:09:19", "link": "http://arxiv.org/abs/2305.11828v3", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "RxnScribe: A Sequence Generation Model for Reaction Diagram Parsing", "abstract": "Reaction diagram parsing is the task of extracting reaction schemes from a\ndiagram in the chemistry literature. The reaction diagrams can be arbitrarily\ncomplex, thus robustly parsing them into structured data is an open challenge.\nIn this paper, we present RxnScribe, a machine learning model for parsing\nreaction diagrams of varying styles. We formulate this structured prediction\ntask with a sequence generation approach, which condenses the traditional\npipeline into an end-to-end model. We train RxnScribe on a dataset of 1,378\ndiagrams and evaluate it with cross validation, achieving an 80.0% soft match\nF1 score, with significant improvements over previous models. Our code and data\nare publicly available at https://github.com/thomas0809/RxnScribe.", "published": "2023-05-19 17:37:28", "link": "http://arxiv.org/abs/2305.11845v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "MParrotTTS: Multilingual Multi-speaker Text to Speech Synthesis in Low\n  Resource Setting", "abstract": "We present MParrotTTS, a unified multilingual, multi-speaker text-to-speech\n(TTS) synthesis model that can produce high-quality speech. Benefiting from a\nmodularized training paradigm exploiting self-supervised speech\nrepresentations, MParrotTTS adapts to a new language with minimal supervised\ndata and generalizes to languages not seen while training the self-supervised\nbackbone. Moreover, without training on any bilingual or parallel examples,\nMParrotTTS can transfer voices across languages while preserving the\nspeaker-specific characteristics, e.g., synthesizing fluent Hindi speech using\na French speaker's voice and accent. We present extensive results on six\nlanguages in terms of speech naturalness and speaker similarity in parallel and\ncross-lingual synthesis. The proposed model outperforms the state-of-the-art\nmultilingual TTS models and baselines, using only a small fraction of\nsupervised training data. Speech samples from our model can be found at\nhttps://paper2438.github.io/tts/", "published": "2023-05-19 13:43:36", "link": "http://arxiv.org/abs/2305.11926v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MultiTurnCleanup: A Benchmark for Multi-Turn Spoken Conversational\n  Transcript Cleanup", "abstract": "Current disfluency detection models focus on individual utterances each from\na single speaker. However, numerous discontinuity phenomena in spoken\nconversational transcripts occur across multiple turns, hampering human\nreadability and the performance of downstream NLP tasks. This study addresses\nthese phenomena by proposing an innovative Multi-Turn Cleanup task for spoken\nconversational transcripts and collecting a new dataset, MultiTurnCleanup1. We\ndesign a data labeling schema to collect the high-quality dataset and provide\nextensive data analysis. Furthermore, we leverage two modeling approaches for\nexperimental evaluation as benchmarks for future research.", "published": "2023-05-19 22:50:02", "link": "http://arxiv.org/abs/2305.12029v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unsupervised ASR via Cross-Lingual Pseudo-Labeling", "abstract": "Recent work has shown that it is possible to train an $\\textit{unsupervised}$\nautomatic speech recognition (ASR) system using only unpaired audio and text.\nExisting unsupervised ASR methods assume that no labeled data can be used for\ntraining. We argue that even if one does not have any labeled audio for a given\nlanguage, there is $\\textit{always}$ labeled data available for other\nlanguages. We show that it is possible to use character-level acoustic models\n(AMs) from other languages to bootstrap an $\\textit{unsupervised}$ AM in a new\nlanguage. Here, \"unsupervised\" means no labeled audio is available for the\n$\\textit{target}$ language. Our approach is based on two key ingredients: (i)\ngenerating pseudo-labels (PLs) of the $\\textit{target}$ language using some\n$\\textit{other}$ language AM and (ii) constraining these PLs with a\n$\\textit{target language model}$. Our approach is effective on Common Voice:\ne.g. transfer of English AM to Swahili achieves 18% WER. It also outperforms\ncharacter-based wav2vec-U 2.0 by 15% absolute WER on LJSpeech with 800h of\nlabeled German data instead of 60k hours of unlabeled English data.", "published": "2023-05-19 01:59:20", "link": "http://arxiv.org/abs/2305.13330v3", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Surgical-VQLA: Transformer with Gated Vision-Language Embedding for\n  Visual Question Localized-Answering in Robotic Surgery", "abstract": "Despite the availability of computer-aided simulators and recorded videos of\nsurgical procedures, junior residents still heavily rely on experts to answer\ntheir queries. However, expert surgeons are often overloaded with clinical and\nacademic workloads and limit their time in answering. For this purpose, we\ndevelop a surgical question-answering system to facilitate robot-assisted\nsurgical scene and activity understanding from recorded videos. Most of the\nexisting VQA methods require an object detector and regions based feature\nextractor to extract visual features and fuse them with the embedded text of\nthe question for answer generation. However, (1) surgical object detection\nmodel is scarce due to smaller datasets and lack of bounding box annotation;\n(2) current fusion strategy of heterogeneous modalities like text and image is\nnaive; (3) the localized answering is missing, which is crucial in complex\nsurgical scenarios. In this paper, we propose Visual Question\nLocalized-Answering in Robotic Surgery (Surgical-VQLA) to localize the specific\nsurgical area during the answer prediction. To deal with the fusion of the\nheterogeneous modalities, we design gated vision-language embedding (GVLE) to\nbuild input patches for the Language Vision Transformer (LViT) to predict the\nanswer. To get localization, we add the detection head in parallel with the\nprediction head of the LViT. We also integrate GIoU loss to boost localization\nperformance by preserving the accuracy of the question-answering model. We\nannotate two datasets of VQLA by utilizing publicly available surgical videos\nfrom MICCAI challenges EndoVis-17 and 18. Our validation results suggest that\nSurgical-VQLA can better understand the surgical scene and localize the\nspecific area related to the question-answering. GVLE presents an efficient\nlanguage-vision embedding technique by showing superior performance over the\nexisting benchmarks.", "published": "2023-05-19 14:13:47", "link": "http://arxiv.org/abs/2305.11692v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO"], "primary_category": "cs.CV"}
{"title": "Any-to-Any Generation via Composable Diffusion", "abstract": "We present Composable Diffusion (CoDi), a novel generative model capable of\ngenerating any combination of output modalities, such as language, image,\nvideo, or audio, from any combination of input modalities. Unlike existing\ngenerative AI systems, CoDi can generate multiple modalities in parallel and\nits input is not limited to a subset of modalities like text or image. Despite\nthe absence of training datasets for many combinations of modalities, we\npropose to align modalities in both the input and output space. This allows\nCoDi to freely condition on any input combination and generate any group of\nmodalities, even if they are not present in the training data. CoDi employs a\nnovel composable generation strategy which involves building a shared\nmultimodal space by bridging alignment in the diffusion process, enabling the\nsynchronized generation of intertwined modalities, such as temporally aligned\nvideo and audio. Highly customizable and flexible, CoDi achieves strong\njoint-modality generation quality, and outperforms or is on par with the\nunimodal state-of-the-art for single-modality synthesis. The project page with\ndemonstrations and code is at https://codi-gen.github.io", "published": "2023-05-19 17:38:32", "link": "http://arxiv.org/abs/2305.11846v1", "categories": ["cs.CV", "cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Delay-penalized CTC implemented based on Finite State Transducer", "abstract": "Connectionist Temporal Classification (CTC) suffers from the latency problem\nwhen applied to streaming models. We argue that in CTC lattice, the alignments\nthat can access more future context are preferred during training, thereby\nleading to higher symbol delay. In this work we propose the delay-penalized CTC\nwhich is augmented with latency penalty regularization. We devise a flexible\nand efficient implementation based on the differentiable Finite State\nTransducer (FST). Specifically, by attaching a binary attribute to CTC\ntopology, we can locate the frames that firstly emit non-blank tokens on the\nresulting CTC lattice, and add the frame offsets to the log-probabilities.\nExperimental results demonstrate the effectiveness of our proposed\ndelay-penalized CTC, which is able to balance the delay-accuracy trade-off.\nFurthermore, combining the delay-penalized transducer enables the CTC model to\nachieve better performance and lower latency. Our work is open-sourced and\npublicly available https://github.com/k2-fsa/k2.", "published": "2023-05-19 09:16:46", "link": "http://arxiv.org/abs/2305.11539v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "BAT: Boundary aware transducer for memory-efficient and low-latency ASR", "abstract": "Recently, recurrent neural network transducer (RNN-T) gains increasing\npopularity due to its natural streaming capability as well as superior\nperformance. Nevertheless, RNN-T training requires large time and computation\nresources as RNN-T loss calculation is slow and consumes a lot of memory.\nAnother limitation of RNN-T is that it tends to access more contexts for better\nperformance, thus leading to higher emission latency in streaming ASR. In this\npaper we propose boundary-aware transducer (BAT) for memory-efficient and\nlow-latency ASR. In BAT, the lattice for RNN-T loss computation is reduced to a\nrestricted region selected by the alignment from continuous integrate-and-fire\n(CIF), which is jointly optimized with the RNN-T model. Extensive experiments\ndemonstrate that compared to RNN-T, BAT reduces time and memory consumption\nsignificantly in training, and achieves good CER-latency trade-offs in\ninference for streaming ASR.", "published": "2023-05-19 10:17:54", "link": "http://arxiv.org/abs/2305.11571v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Are Microphone Signals Alone Sufficient for Self-Positioning?", "abstract": "In an era where asynchronous environments pose challenges to traditional\nself-positioning methods, we propose a new transformation to the existing\nparadigm. Traditionally, time of arrival (TOA) measurements require both\nmicrophone and source signals, limiting their applicability in environments\nwith unknown emission time of human voices or sources and unknown recording\nstart time of independent microphones. To address this issue, our research\npioneers a mapping function capable of transforming both TOA and time\ndifference of arrival (TDOA) formulas, demonstrating, for the first time, that\nthey can be identical to one another. This implies that microphone signals\nalone are sufficient for self-positioning without the need for source signal\nwaveforms, a groundbreaking advancement in the field that carries the potential\nto revolutionize self-positioning techniques, expanding their applicability in\nchallenging environments. Supported by a robust mathematical proof and\ncompelling experimental results, this research represents a timely and\nsignificant contribution to the current discourse in signal, and audio\nprocessing.", "published": "2023-05-19 02:47:51", "link": "http://arxiv.org/abs/2305.11397v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Preliminary Study on Augmenting Speech Emotion Recognition using a\n  Diffusion Model", "abstract": "In this paper, we propose to utilise diffusion models for data augmentation\nin speech emotion recognition (SER). In particular, we present an effective\napproach to utilise improved denoising diffusion probabilistic models (IDDPM)\nto generate synthetic emotional data. We condition the IDDPM with the textual\nembedding from bidirectional encoder representations from transformers (BERT)\nto generate high-quality synthetic emotional samples in different speakers'\nvoices\\footnote{synthetic samples URL:\n\\url{https://emulationai.com/research/diffusion-ser.}}. We implement a series\nof experiments and show that better quality synthetic data helps improve SER\nperformance. We compare results with generative adversarial networks (GANs) and\nshow that the proposed model generates better-quality synthetic samples that\ncan considerably improve the performance of SER when augmented with synthetic\ndata.", "published": "2023-05-19 03:51:09", "link": "http://arxiv.org/abs/2305.11413v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Direction Specific Ambisonics Source Separation with End-To-End Deep\n  Learning", "abstract": "Ambisonics is a scene-based spatial audio format that has several useful\nfeatures compared to object-based formats, such as efficient whole scene\nrotation and versatility. However, it does not provide direct access to the\nindividual source signals, so that these have to be separated from the mixture\nwhen required. Typically, this is done with linear spherical harmonics (SH)\nbeamforming. In this paper, we explore deep-learning-based source separation on\nstatic Ambisonics mixtures. In contrast to most source separation approaches,\nwhich separate a fixed number of sources of specific sound types, we focus on\nseparating arbitrary sound from specific directions. Specifically, we propose\nthree operating modes that combine a source separation neural network with SH\nbeamforming: refinement, implicit, and mixed mode. We show that a neural\nnetwork can implicitly associate conditioning directions with the spatial\ninformation contained in the Ambisonics scene to extract specific sources. We\nevaluate the performance of the three proposed approaches and compare them to\nSH beamforming on musical mixtures generated with the musdb18 dataset, as well\nas with mixtures generated with the FUSS dataset for universal source\nseparation, under both anechoic and room conditions. Results show that the\nproposed approaches offer improved separation performance and spatial\nselectivity compared to conventional SH beamforming.", "published": "2023-05-19 15:03:52", "link": "http://arxiv.org/abs/2305.11727v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Pengi: An Audio Language Model for Audio Tasks", "abstract": "In the domain of audio processing, Transfer Learning has facilitated the rise\nof Self-Supervised Learning and Zero-Shot Learning techniques. These approaches\nhave led to the development of versatile models capable of tackling a wide\narray of tasks, while delivering state-of-the-art performance. However, current\nmodels inherently lack the capacity to produce the requisite language for\nopen-ended tasks, such as Audio Captioning or Audio Question & Answering. We\nintroduce Pengi, a novel Audio Language Model that leverages Transfer Learning\nby framing all audio tasks as text-generation tasks. It takes as input, an\naudio recording, and text, and generates free-form text as output. The input\naudio is represented as a sequence of continuous embeddings by an audio\nencoder. A text encoder does the same for the corresponding text input. Both\nsequences are combined as a prefix to prompt a pre-trained frozen language\nmodel. The unified architecture of Pengi enables open-ended tasks and\nclose-ended tasks without any additional fine-tuning or task-specific\nextensions. When evaluated on 22 downstream tasks, our approach yields\nstate-of-the-art performance in several of them. Our results show that\nconnecting language models with audio models is a major step towards\ngeneral-purpose audio understanding", "published": "2023-05-19 17:20:56", "link": "http://arxiv.org/abs/2305.11834v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Computational models of sound-quality metrics using method for\n  calculating loudness with gammatone/gammachirp auditory filterbank", "abstract": "Sound-quality metrics (SQMs), such as sharpness, roughness, and fluctuation\nstrength, are calculated using a standard method for calculating loudness\n(Zwicker method, ISO532B, 1975). Since ISO 532 had been revised to contain the\nZwicker method (ISO 5321) and Moore-Glasberg method (ISO 532-2) in 2017, the\nclassical computational SQM model should also be revised in accordance with\nthese revisions. A roex auditory filterbank used with the Moore-Glasberg method\nis defined separately in the frequency domain not to have impulse responses. It\nis therefore difficult to construct a computational SQM model, e.g., the\nclassical computational SQM model, on the basis of ISO 532-2. We propose a\nmethod for calculating loudness using the time-domain gammatone or gammachirp\nauditory filterbank instead of the roex auditory filterbank to solve this\nproblem. We also propose three computational SQM models based on ISO 532-2 to\nuse with the proposed loudness method. We evaluated the root-mean squared\nerrors (RMSEs) of the calculated loudness with the proposed and Moore-Glasberg\nmethods. We then evaluated the RMSEs of the calculated SQMs with the proposed\nmethod and human data of SQMs. We found that the proposed method can be\nconsidered as a time-domain method for calculating loudness on the basis of ISO\n532-2 because the RMSEs are very small. We also found that the proposed\ncomputational SQM models can effectively account for the human data of SQMs\ncompared with the classical computational SQM model in terms of RMSEs.", "published": "2023-05-19 02:37:45", "link": "http://arxiv.org/abs/2305.13213v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Differentially Private Adapters for Parameter Efficient Acoustic\n  Modeling", "abstract": "In this work, we devise a parameter-efficient solution to bring differential\nprivacy (DP) guarantees into adaptation of a cross-lingual speech classifier.\nWe investigate a new frozen pre-trained adaptation framework for DP-preserving\nspeech modeling without full model fine-tuning. First, we introduce a noisy\nteacher-student ensemble into a conventional adaptation scheme leveraging a\nfrozen pre-trained acoustic model and attain superior performance than DP-based\nstochastic gradient descent (DPSGD). Next, we insert residual adapters (RA)\nbetween layers of the frozen pre-trained acoustic model. The RAs reduce\ntraining cost and time significantly with a negligible performance drop.\nEvaluated on the open-access Multilingual Spoken Words (MLSW) dataset, our\nsolution reduces the number of trainable parameters by 97.5% using the RAs with\nonly a 4% performance drop with respect to fine-tuning the cross-lingual speech\nclassifier while preserving DP guarantees.", "published": "2023-05-19 00:36:43", "link": "http://arxiv.org/abs/2305.11360v1", "categories": ["cs.SD", "cs.CR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "What You Hear Is What You See: Audio Quality Metrics From Image Quality\n  Metrics", "abstract": "In this study, we investigate the feasibility of utilizing state-of-the-art\nimage perceptual metrics for evaluating audio signals by representing them as\nspectrograms. The encouraging outcome of the proposed approach is based on the\nsimilarity between the neural mechanisms in the auditory and visual pathways.\nFurthermore, we customise one of the metrics which has a psychoacoustically\nplausible architecture to account for the peculiarities of sound signals. We\nevaluate the effectiveness of our proposed metric and several baseline metrics\nusing a music dataset, with promising results in terms of the correlation\nbetween the metrics and the perceived quality of audio as rated by human\nevaluators.", "published": "2023-05-19 10:43:57", "link": "http://arxiv.org/abs/2305.11582v2", "categories": ["cs.SD", "cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MIDI-Draw: Sketching to Control Melody Generation", "abstract": "We describe a proof-of-principle implementation of a system for drawing\nmelodies that abstracts away from a note-level input representation via melodic\ncontours. The aim is to allow users to express their musical intentions without\nrequiring prior knowledge of how notes fit together melodiously. Current\napproaches to controllable melody generation often require users to choose\nparameters that are static across a whole sequence, via buttons or sliders. In\ncontrast, our method allows users to quickly specify how parameters should\nchange over time by drawing a contour.", "published": "2023-05-19 11:31:33", "link": "http://arxiv.org/abs/2305.11605v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Conditional Online Learning for Keyword Spotting", "abstract": "Modern approaches for keyword spotting rely on training deep neural networks\non large static datasets with i.i.d. distributions. However, the resulting\nmodels tend to underperform when presented with changing data regimes in\nreal-life applications. This work investigates a simple but effective online\ncontinual learning method that updates a keyword spotter on-device via SGD as\nnew data becomes available. Contrary to previous research, this work focuses on\nlearning the same KWS task, which covers most commercial applications. During\nexperiments with dynamic audio streams in different scenarios, that method\nimproves the performance of a pre-trained small-footprint model by 34%.\nMoreover, experiments demonstrate that, compared to a naive online learning\nimplementation, conditional model updates based on its performance in a small\nhold-out set drawn from the training distribution mitigate catastrophic\nforgetting.", "published": "2023-05-19 15:46:31", "link": "http://arxiv.org/abs/2305.13332v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
