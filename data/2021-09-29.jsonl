{"title": "Improving Arabic Diacritization by Learning to Diacritize and Translate", "abstract": "We propose a novel multitask learning method for diacritization which trains\na model to both diacritize and translate. Our method addresses data sparsity by\nexploiting large, readily available bitext corpora. Furthermore, translation\nrequires implicit linguistic and semantic knowledge, which is helpful for\nresolving ambiguities in the diacritization task. We apply our method to the\nPenn Arabic Treebank and report a new state-of-the-art word error rate of\n4.79%. We also conduct manual and automatic analysis to better understand our\nmethod and highlight some of the remaining challenges in diacritization.", "published": "2021-09-29 02:36:06", "link": "http://arxiv.org/abs/2109.14150v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reflexivity in Issues of Scale and Representation in a Digital\n  Humanities Project", "abstract": "In this paper, we explore issues that we have encountered in developing a\npipeline that combines natural language processing with data analysis and\nvisualization techniques. The characteristics of the corpus - being comprised\nof diaries of a single person spanning several decades - present both\nconceptual challenges in terms of issues of representation, and affordances as\na source for historical research. We consider these issues in a team context\nwith a particular focus on the generation and interpretation of visualizations.", "published": "2021-09-29 04:06:51", "link": "http://arxiv.org/abs/2109.14184v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Context based Roman-Urdu to Urdu Script Transliteration System", "abstract": "Now a day computer is necessary for human being and it is very useful in many\nfields like search engine, text processing, short messaging services, voice\nchatting and text recognition. Since last many years there are many tools and\ntechniques that have been developed to support the writing of language script.\nMost of the Asian languages like Arabic, Urdu, Persian, Chains and Korean are\nwritten in Roman alphabets. Roman alphabets are the most commonly used for\ntransliteration of languages, which have non-Latin scripts. For writing Urdu\ncharacters as an input, there are many layouts which are already exist. Mostly\nUrdu speaker prefer to use Roman-Urdu for different applications, because\nmostly user is not familiar with Urdu language keyboard. The objective of this\nwork is to improve the context base transliteration of Roman-Urdu to Urdu\nscript. In this paper, we propose an algorithm which effectively solve the\ntransliteration issues. The algorithm work like, convert the encoding roman\nwords into the words in the standard Urdu script and match it with the lexicon.\nIf match found, then display the word in the text editor. The highest frequency\nwords are displayed if more than one match found in the lexicon. Display the\nfirst encoded and converted instance and set it to the default if there is not\na single instance of the match is found and then adjust the given ambiguous\nword to their desire location according to their context. The outcome of this\nalgorithm proved the efficiency and significance as compare to other models and\nalgorithms which work for transliteration of Raman-Urdu to Urdu on context.", "published": "2021-09-29 05:24:55", "link": "http://arxiv.org/abs/2109.14197v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Who speaks like a style of Vitamin: Towards Syntax-Aware\n  DialogueSummarization using Multi-task Learning", "abstract": "Abstractive dialogue summarization is a challenging task for several reasons.\nFirst, most of the important pieces of information in a conversation are\nscattered across utterances through multi-party interactions with different\ntextual styles. Second, dialogues are often informal structures, wherein\ndifferent individuals express personal perspectives, unlike text summarization,\ntasks that usually target formal documents such as news articles. To address\nthese issues, we focused on the association between utterances from individual\nspeakers and unique syntactic structures. Speakers have unique textual styles\nthat can contain linguistic information, such as voiceprint. Therefore, we\nconstructed a syntax-aware model by leveraging linguistic information (i.e.,\nPOS tagging), which alleviates the above issues by inherently distinguishing\nsentences uttered from individual speakers. We employed multi-task learning of\nboth syntax-aware information and dialogue summarization. To the best of our\nknowledge, our approach is the first method to apply multi-task learning to the\ndialogue summarization task. Experiments on a SAMSum corpus (a large-scale\ndialogue summarization corpus) demonstrated that our method improved upon the\nvanilla model. We further analyze the costs and benefits of our approach\nrelative to baseline models.", "published": "2021-09-29 05:30:39", "link": "http://arxiv.org/abs/2109.14199v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BLEU, METEOR, BERTScore: Evaluation of Metrics Performance in Assessing\n  Critical Translation Errors in Sentiment-oriented Text", "abstract": "Social media companies as well as authorities make extensive use of\nartificial intelligence (AI) tools to monitor postings of hate speech,\ncelebrations of violence or profanity. Since AI software requires massive\nvolumes of data to train computers, Machine Translation (MT) of the online\ncontent is commonly used to process posts written in several languages and\nhence augment the data needed for training. However, MT mistakes are a regular\noccurrence when translating sentiment-oriented user-generated content (UGC),\nespecially when a low-resource language is involved. The adequacy of the whole\nprocess relies on the assumption that the evaluation metrics used give a\nreliable indication of the quality of the translation. In this paper, we assess\nthe ability of automatic quality metrics to detect critical machine translation\nerrors which can cause serious misunderstanding of the affect message. We\ncompare the performance of three canonical metrics on meaningless translations\nwhere the semantic content is seriously impaired as compared to meaningful\ntranslations with a critical error which exclusively distorts the sentiment of\nthe source text. We conclude that there is a need for fine-tuning of automatic\nmetrics to make them more robust in detecting sentiment critical errors.", "published": "2021-09-29 07:51:17", "link": "http://arxiv.org/abs/2109.14250v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Call Larisa Ivanovna: Code-Switching Fools Multilingual NLU Models", "abstract": "Practical needs of developing task-oriented dialogue assistants require the\nability to understand many languages. Novel benchmarks for multilingual natural\nlanguage understanding (NLU) include monolingual sentences in several\nlanguages, annotated with intents and slots. In such setup models for\ncross-lingual transfer show remarkable performance in joint intent recognition\nand slot filling. However, existing benchmarks lack of code-switched\nutterances, which are difficult to gather and label due to complexity in the\ngrammatical structure. The evaluation of NLU models seems biased and limited,\nsince code-switching is being left out of scope.\n  Our work adopts recognized methods to generate plausible and\nnaturally-sounding code-switched utterances and uses them to create a synthetic\ncode-switched test set. Based on experiments, we report that the\nstate-of-the-art NLU models are unable to handle code-switching. At worst, the\nperformance, evaluated by semantic accuracy, drops as low as 15\\% from 80\\%\nacross languages. Further we show, that pre-training on synthetic code-mixed\ndata helps to maintain performance on the proposed test set at a comparable\nlevel with monolingual data. Finally, we analyze different language pairs and\nshow that the closer the languages are, the better the NLU model handles their\nalternation. This is in line with the common understanding of how multilingual\nmodels conduct transferring between languages", "published": "2021-09-29 11:15:00", "link": "http://arxiv.org/abs/2109.14350v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Fact Linking", "abstract": "Knowledge-intensive NLP tasks can benefit from linking natural language text\nwith facts from a Knowledge Graph (KG). Although facts themselves are\nlanguage-agnostic, the fact labels (i.e., language-specific representation of\nthe fact) in the KG are often present only in a few languages. This makes it\nchallenging to link KG facts to sentences in languages other than the limited\nset of languages. To address this problem, we introduce the task of\nMultilingual Fact Linking (MFL) where the goal is to link fact expressed in a\nsentence to corresponding fact in the KG, even when the fact label in the KG is\nnot available in the language of the sentence. To facilitate research in this\narea, we present a new evaluation dataset, IndicLink. This dataset contains\n11,293 linked WikiData facts and 6,429 sentences spanning English and six\nIndian languages. We propose a Retrieval+Generation model, ReFCoG, that can\nscale to millions of KG facts by combining Dual Encoder based retrieval with a\nSeq2Seq based generation model which is constrained to output only valid KG\nfacts. ReFCoG outperforms standard Retrieval+Re-ranking models by 10.7 pts in\nPrecision@1. In spite of this gain, the model achieves an overall score of\n52.1, showing ample scope for improvement in the task.ReFCoG code and IndicLink\ndata are available at https://github.com/SaiKeshav/mfl", "published": "2021-09-29 11:50:44", "link": "http://arxiv.org/abs/2109.14364v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EdinSaar@WMT21: North-Germanic Low-Resource Multilingual NMT", "abstract": "We describe the EdinSaar submission to the shared task of Multilingual\nLow-Resource Translation for North Germanic Languages at the Sixth Conference\non Machine Translation (WMT2021). We submit multilingual translation models for\ntranslations to/from Icelandic (is), Norwegian-Bokmal (nb), and Swedish (sv).\nWe employ various experimental approaches, including multilingual pre-training,\nback-translation, fine-tuning, and ensembling. In most translation directions,\nour models outperform other submitted systems.", "published": "2021-09-29 11:56:51", "link": "http://arxiv.org/abs/2109.14368v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EDGAR-CORPUS: Billions of Tokens Make The World Go Round", "abstract": "We release EDGAR-CORPUS, a novel corpus comprising annual reports from all\nthe publicly traded companies in the US spanning a period of more than 25\nyears. To the best of our knowledge, EDGAR-CORPUS is the largest financial NLP\ncorpus available to date. All the reports are downloaded, split into their\ncorresponding items (sections), and provided in a clean, easy-to-use JSON\nformat. We use EDGAR-CORPUS to train and release EDGAR-W2V, which are WORD2VEC\nembeddings for the financial domain. We employ these embeddings in a battery of\nfinancial NLP tasks and showcase their superiority over generic GloVe\nembeddings and other existing financial word embeddings. We also open-source\nEDGAR-CRAWLER, a toolkit that facilitates downloading and extracting future\nannual reports.", "published": "2021-09-29 12:56:20", "link": "http://arxiv.org/abs/2109.14394v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "StoryDB: Broad Multi-language Narrative Dataset", "abstract": "This paper presents StoryDB - a broad multi-language dataset of narratives.\nStoryDB is a corpus of texts that includes stories in 42 different languages.\nEvery language includes 500+ stories. Some of the languages include more than\n20 000 stories. Every story is indexed across languages and labeled with tags\nsuch as a genre or a topic. The corpus shows rich topical and language\nvariation and can serve as a resource for the study of the role of narrative in\nnatural language processing across various languages including low resource\nones. We also demonstrate how the dataset could be used to benchmark three\nmodern multilanguage models, namely, mDistillBERT, mBERT, and XLM-RoBERTa.", "published": "2021-09-29 12:59:38", "link": "http://arxiv.org/abs/2109.14396v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "BiQUE: Biquaternionic Embeddings of Knowledge Graphs", "abstract": "Knowledge graph embeddings (KGEs) compactly encode multi-relational knowledge\ngraphs (KGs). Existing KGE models rely on geometric operations to model\nrelational patterns. Euclidean (circular) rotation is useful for modeling\npatterns such as symmetry, but cannot represent hierarchical semantics. In\ncontrast, hyperbolic models are effective at modeling hierarchical relations,\nbut do not perform as well on patterns on which circular rotation excels. It is\ncrucial for KGE models to unify multiple geometric transformations so as to\nfully cover the multifarious relations in KGs. To do so, we propose BiQUE, a\nnovel model that employs biquaternions to integrate multiple geometric\ntransformations, viz., scaling, translation, Euclidean rotation, and hyperbolic\nrotation. BiQUE makes the best trade-offs among geometric operators during\ntraining, picking the best one (or their best combination) for each relation.\nExperiments on five datasets show BiQUE's effectiveness.", "published": "2021-09-29 13:05:32", "link": "http://arxiv.org/abs/2109.14401v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Classifying Tweet Sentiment Using the Hidden State and Attention Matrix\n  of a Fine-tuned BERTweet Model", "abstract": "This paper introduces a study on tweet sentiment classification. Our task is\nto classify a tweet as either positive or negative. We approach the problem in\ntwo steps, namely embedding and classifying. Our baseline methods include\nseveral combinations of traditional embedding methods and classification\nalgorithms. Furthermore, we explore the current state-of-the-art tweet analysis\nmodel, BERTweet, and propose a novel approach in which features are engineered\nfrom the hidden states and attention matrices of the model, inspired by\nempirical study of the tweets. Using a multi-layer perceptron trained with a\nhigh dropout rate for classification, our proposed approach achieves a\nvalidation accuracy of 0.9111.", "published": "2021-09-29 19:51:48", "link": "http://arxiv.org/abs/2109.14692v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BeliefBank: Adding Memory to a Pre-Trained Language Model for a\n  Systematic Notion of Belief", "abstract": "Although pretrained language models (PTLMs) contain significant amounts of\nworld knowledge, they can still produce inconsistent answers to questions when\nprobed, even after specialized training. As a result, it can be hard to\nidentify what the model actually \"believes\" about the world, making it\nsusceptible to inconsistent behavior and simple errors. Our goal is to reduce\nthese problems. Our approach is to embed a PTLM in a broader system that also\nincludes an evolving, symbolic memory of beliefs -- a BeliefBank -- that\nrecords but then may modify the raw PTLM answers. We describe two mechanisms to\nimprove belief consistency in the overall system. First, a reasoning component\n-- a weighted MaxSAT solver -- revises beliefs that significantly clash with\nothers. Second, a feedback component issues future queries to the PTLM using\nknown beliefs as context. We show that, in a controlled experimental setting,\nthese two mechanisms result in more consistent beliefs in the overall system,\nimproving both the accuracy and consistency of its answers over time. This is\nsignificant as it is a first step towards PTLM-based architectures with a\nsystematic notion of belief, enabling them to construct a more coherent picture\nof the world, and improve over time without model retraining.", "published": "2021-09-29 21:04:27", "link": "http://arxiv.org/abs/2109.14723v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Task Pre-Training for Plug-and-Play Task-Oriented Dialogue System", "abstract": "Pre-trained language models have been recently shown to benefit task-oriented\ndialogue (TOD) systems. Despite their success, existing methods often formulate\nthis task as a cascaded generation problem which can lead to error accumulation\nacross different sub-tasks and greater data annotation overhead. In this study,\nwe present PPTOD, a unified plug-and-play model for task-oriented dialogue. In\naddition, we introduce a new dialogue multi-task pre-training strategy that\nallows the model to learn the primary TOD task completion skills from\nheterogeneous dialog corpora. We extensively test our model on three benchmark\nTOD tasks, including end-to-end dialogue modelling, dialogue state tracking,\nand intent classification. Experimental results show that PPTOD achieves new\nstate of the art on all evaluated tasks in both high-resource and low-resource\nscenarios. Furthermore, comparisons against previous SOTA methods show that the\nresponses generated by PPTOD are more factually correct and semantically\ncoherent as judged by human annotators.", "published": "2021-09-29 22:02:18", "link": "http://arxiv.org/abs/2109.14739v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analysing the Effect of Masking Length Distribution of MLM: An\n  Evaluation Framework and Case Study on Chinese MRC Datasets", "abstract": "Machine reading comprehension (MRC) is a challenging natural language\nprocessing (NLP) task. Recently, the emergence of pre-trained models (PTM) has\nbrought this research field into a new era, in which the training objective\nplays a key role. The masked language model (MLM) is a self-supervised training\nobjective that widely used in various PTMs. With the development of training\nobjectives, many variants of MLM have been proposed, such as whole word\nmasking, entity masking, phrase masking, span masking, and so on. In different\nMLM, the length of the masked tokens is different. Similarly, in different\nmachine reading comprehension tasks, the length of the answer is also\ndifferent, and the answer is often a word, phrase, or sentence. Thus, in MRC\ntasks with different answer lengths, whether the length of MLM is related to\nperformance is a question worth studying. If this hypothesis is true, it can\nguide us how to pre-train the MLM model with a relatively suitable mask length\ndistribution for MRC task. In this paper, we try to uncover how much of MLM's\nsuccess in the machine reading comprehension tasks comes from the correlation\nbetween masking length distribution and answer length in MRC dataset. In order\nto address this issue, herein, (1) we propose four MRC tasks with different\nanswer length distributions, namely short span extraction task, long span\nextraction task, short multiple-choice cloze task, long multiple-choice cloze\ntask; (2) four Chinese MRC datasets are created for these tasks; (3) we also\nhave pre-trained four masked language models according to the answer length\ndistributions of these datasets; (4) ablation experiments are conducted on the\ndatasets to verify our hypothesis. The experimental results demonstrate that\nour hypothesis is true.", "published": "2021-09-29 04:07:05", "link": "http://arxiv.org/abs/2110.15712v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contrastive Video-Language Segmentation", "abstract": "We focus on the problem of segmenting a certain object referred by a natural\nlanguage sentence in video content, at the core of formulating a pinpoint\nvision-language relation. While existing attempts mainly construct such\nrelation in an implicit way, i.e., grid-level multi-modal feature fusion, it\nhas been proven problematic to distinguish semantically similar objects under\nthis paradigm. In this work, we propose to interwind the visual and linguistic\nmodalities in an explicit way via the contrastive learning objective, which\ndirectly aligns the referred object and the language description and separates\nthe unreferred content apart across frames. Moreover, to remedy for the\ndegradation problem, we present two complementary hard instance mining\nstrategies, i.e., Language-relevant Channel Filter and Relative Hard Instance\nConstruction. They encourage the network to exclude visual-distinguishable\nfeature and to focus on easy-confused objects during the contrastive training.\nExtensive experiments on two benchmarks, i.e., A2D Sentences and J-HMDB\nSentences, quantitatively demonstrate the state-of-the-arts performance of our\nmethod and qualitatively show the more accurate distinguishment between\nsemantically similar objects over baselines.", "published": "2021-09-29 01:40:58", "link": "http://arxiv.org/abs/2109.14131v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Improving Dialogue State Tracking by Joint Slot Modeling", "abstract": "Dialogue state tracking models play an important role in a task-oriented\ndialogue system. However, most of them model the slot types conditionally\nindependently given the input. We discover that it may cause the model to be\nconfused by slot types that share the same data type. To mitigate this issue,\nwe propose TripPy-MRF and TripPy-LSTM that models the slots jointly. Our\nresults show that they are able to alleviate the confusion mentioned above, and\nthey push the state-of-the-art on dataset MultiWoZ 2.1 from 58.7 to 61.3. Our\nimplementation is available at https://github.com/CTinRay/Trippy-Joint.", "published": "2021-09-29 02:25:39", "link": "http://arxiv.org/abs/2109.14144v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Hierarchical Character Tagger for Short Text Spelling Error Correction", "abstract": "State-of-the-art approaches to spelling error correction problem include\nTransformer-based Seq2Seq models, which require large training sets and suffer\nfrom slow inference time; and sequence labeling models based on Transformer\nencoders like BERT, which involve token-level label space and therefore a large\npre-defined vocabulary dictionary. In this paper we present a Hierarchical\nCharacter Tagger model, or HCTagger, for short text spelling error correction.\nWe use a pre-trained language model at the character level as a text encoder,\nand then predict character-level edits to transform the original text into its\nerror-free form with a much smaller label space. For decoding, we propose a\nhierarchical multi-task approach to alleviate the issue of long-tail label\ndistribution without introducing extra model parameters. Experiments on two\npublic misspelling correction datasets demonstrate that HCTagger is an accurate\nand much faster approach than many existing models.", "published": "2021-09-29 08:04:34", "link": "http://arxiv.org/abs/2109.14259v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Overview of the Arabic Sentiment Analysis 2021 Competition at KAUST", "abstract": "This paper provides an overview of the Arabic Sentiment Analysis Challenge\norganized by King Abdullah University of Science and Technology (KAUST). The\ntask in this challenge is to develop machine learning models to classify a\ngiven tweet into one of the three categories Positive, Negative, or Neutral.\nFrom our recently released ASAD dataset, we provide the competitors with 55K\ntweets for training, and 20K tweets for validation, based on which the\nperformance of participating teams are ranked on a leaderboard,\nhttps://www.kaggle.com/c/arabic-sentiment-analysis-2021-kaust. The competition\nreceived in total 1247 submissions from 74 teams (99 team members). The final\nwinners are determined by another private set of 20K tweets that have the same\ndistribution as the training and validation set. In this paper, we present the\nmain findings in the competition and summarize the methods and tools used by\nthe top ranked teams. The full dataset of 100K labeled tweets is also released\nfor public usage, at\nhttps://www.kaggle.com/c/arabic-sentiment-analysis-2021-kaust/data.", "published": "2021-09-29 14:41:51", "link": "http://arxiv.org/abs/2109.14456v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Adaptive Approach For Sparse Representations Using The Locally\n  Competitive Algorithm For Audio", "abstract": "Gammachirp filterbank has been used to approximate the cochlea in sparse\ncoding algorithms. An oriented grid search optimization was applied to adapt\nthe gammachirp's parameters and improve the Matching Pursuit (MP) algorithm's\nsparsity along with the reconstruction quality. However, this combination of a\ngreedy algorithm with a grid search at each iteration is computationally\ndemanding and not suitable for real-time applications. This paper presents an\nadaptive approach to optimize the gammachirp's parameters but in the context of\nthe Locally Competitive Algorithm (LCA) that requires much fewer computations\nthan MP. The proposed method consists of taking advantage of the LCA's neural\narchitecture to automatically adapt the gammachirp's filterbank using the\nbackpropagation algorithm. Results demonstrate an improvement in the LCA's\nperformance with our approach in terms of sparsity, reconstruction quality, and\nconvergence time. This approach can yield a significant advantage over existing\napproaches for real-time applications.", "published": "2021-09-29 20:26:16", "link": "http://arxiv.org/abs/2109.14705v1", "categories": ["cs.SD", "cs.CL"], "primary_category": "cs.SD"}
{"title": "FastCorrect 2: Fast Error Correction on Multiple Candidates for\n  Automatic Speech Recognition", "abstract": "Error correction is widely used in automatic speech recognition (ASR) to\npost-process the generated sentence, and can further reduce the word error rate\n(WER). Although multiple candidates are generated by an ASR system through beam\nsearch, current error correction approaches can only correct one sentence at a\ntime, failing to leverage the voting effect from multiple candidates to better\ndetect and correct error tokens. In this work, we propose FastCorrect 2, an\nerror correction model that takes multiple ASR candidates as input for better\ncorrection accuracy. FastCorrect 2 adopts non-autoregressive generation for\nfast inference, which consists of an encoder that processes multiple source\nsentences and a decoder that generates the target sentence in parallel from the\nadjusted source sentence, where the adjustment is based on the predicted\nduration of each source token. However, there are some issues when handling\nmultiple source sentences. First, it is non-trivial to leverage the voting\neffect from multiple source sentences since they usually vary in length. Thus,\nwe propose a novel alignment algorithm to maximize the degree of token\nalignment among multiple sentences in terms of token and pronunciation\nsimilarity. Second, the decoder can only take one adjusted source sentence as\ninput, while there are multiple source sentences. Thus, we develop a candidate\npredictor to detect the most suitable candidate for the decoder. Experiments on\nour inhouse dataset and AISHELL-1 show that FastCorrect 2 can further reduce\nthe WER over the previous correction model with single candidate by 3.2% and\n2.6%, demonstrating the effectiveness of leveraging multiple candidates in ASR\nerror correction. FastCorrect 2 achieves better performance than the cascaded\nre-scoring and correction pipeline and can serve as a unified post-processing\nmodule for ASR.", "published": "2021-09-29 13:48:03", "link": "http://arxiv.org/abs/2109.14420v4", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Privacy Policy Question Answering Assistant: A Query-Guided Extractive\n  Summarization Approach", "abstract": "Existing work on making privacy policies accessible has explored new\npresentation forms such as color-coding based on the risk factors or\nsummarization to assist users with conscious agreement. To facilitate a more\npersonalized interaction with the policies, in this work, we propose an\nautomated privacy policy question answering assistant that extracts a summary\nin response to the input user query. This is a challenging task because users\narticulate their privacy-related questions in a very different language than\nthe legal language of the policy, making it difficult for the system to\nunderstand their inquiry. Moreover, existing annotated data in this domain are\nlimited. We address these problems by paraphrasing to bring the style and\nlanguage of the user's question closer to the language of privacy policies. Our\ncontent scoring module uses the existing in-domain data to find relevant\ninformation in the policy and incorporates it in a summary. Our pipeline is\nable to find an answer for 89% of the user queries in the privacyQA dataset.", "published": "2021-09-29 18:00:09", "link": "http://arxiv.org/abs/2109.14638v1", "categories": ["cs.CL", "cs.CY", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Multimodal Emotion Recognition with High-level Speech and Text Features", "abstract": "Automatic emotion recognition is one of the central concerns of the\nHuman-Computer Interaction field as it can bridge the gap between humans and\nmachines. Current works train deep learning models on low-level data\nrepresentations to solve the emotion recognition task. Since emotion datasets\noften have a limited amount of data, these approaches may suffer from\noverfitting, and they may learn based on superficial cues. To address these\nissues, we propose a novel cross-representation speech model, inspired by\ndisentanglement representation learning, to perform emotion recognition on\nwav2vec 2.0 speech features. We also train a CNN-based model to recognize\nemotions from text features extracted with Transformer-based models. We further\ncombine the speech-based and text-based results with a score fusion approach.\nOur method is evaluated on the IEMOCAP dataset in a 4-class classification\nproblem, and it surpasses current works on speech-only, text-only, and\nmultimodal emotion recognition.", "published": "2021-09-29 07:08:40", "link": "http://arxiv.org/abs/2111.10202v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Can phones, syllables, and words emerge as side-products of\n  cross-situational audiovisual learning? -- A computational investigation", "abstract": "Decades of research has studied how language learning infants learn to\ndiscriminate speech sounds, segment words, and associate words with their\nmeanings. While gradual development of such capabilities is unquestionable, the\nexact nature of these skills and the underlying mental representations yet\nremains unclear. In parallel, computational studies have shown that basic\ncomprehension of speech can be achieved by statistical learning between speech\nand concurrent referentially ambiguous visual input. These models can operate\nwithout prior linguistic knowledge such as representations of linguistic units,\nand without learning mechanisms specifically targeted at such units. This has\nraised the question of to what extent knowledge of linguistic units, such as\nphone(me)s, syllables, and words, could actually emerge as latent\nrepresentations supporting the translation between speech and representations\nin other modalities, and without the units being proximal learning targets for\nthe learner. In this study, we formulate this idea as the so-called latent\nlanguage hypothesis (LLH), connecting linguistic representation learning to\ngeneral predictive processing within and across sensory modalities. We review\nthe extent that the audiovisual aspect of LLH is supported by the existing\ncomputational studies. We then explore LLH further in extensive learning\nsimulations with different neural network models for audiovisual\ncross-situational learning, and comparing learning from both synthetic and real\nspeech data. We investigate whether the latent representations learned by the\nnetworks reflect phonetic, syllabic, or lexical structure of input speech by\nutilizing an array of complementary evaluation metrics related to linguistic\nselectivity and temporal characteristics of the representations. As a result,\nwe find that representations associated...", "published": "2021-09-29 05:49:46", "link": "http://arxiv.org/abs/2109.14200v2", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.CV", "cs.LG", "cs.SD", "I.2.0; I.2.6; I.2.7; I.2.10"], "primary_category": "eess.AS"}
{"title": "Comparison of Self-Supervised Speech Pre-Training Methods on Flemish\n  Dutch", "abstract": "Recent research in speech processing exhibits a growing interest in\nunsupervised and self-supervised representation learning from unlabelled data\nto alleviate the need for large amounts of annotated data. We investigate\nseveral popular pre-training methods and apply them to Flemish Dutch. We\ncompare off-the-shelf English pre-trained models to models trained on an\nincreasing amount of Flemish data. We find that the most important factors for\npositive transfer to downstream speech recognition tasks include a substantial\namount of data and a matching pre-training domain. Ideally, we also finetune on\nan annotated subset in the target language. All pre-trained models improve\nlinear phone separability in Flemish, but not all methods improve Automatic\nSpeech Recognition. We experience superior performance with wav2vec 2.0 and we\nobtain a 30% WER improvement by finetuning the multilingually pre-trained\nXLSR-53 model on Flemish Dutch, after integration into an HMM-DNN acoustic\nmodel.", "published": "2021-09-29 11:38:26", "link": "http://arxiv.org/abs/2109.14357v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Objective-oriented method for uniformation of various directivity\n  representations", "abstract": "Over recent years, numerous attempts were taken to provide efficient methods\nof directivity representation, either regarding sound sources or head-related\ntransfer functions. Because of the wide variety of programming tools and\nscripts used by different researchers, the resulting representations are\ninconvevnient to reproduce and compare with each other, hampering the\ndevelopment of the subject. Within this paper, an objective-oriented method is\nproposed to deal with this issue. The suggested approach bases on defining\nclasses for different directivity models that share some general properties of\ndirectivity functions, allowing for easy comparison between different\nrepresentations. A basic Matlab toolbox utlizing this method is presented\nalongside exemplary implementations of directivity models based on spherical\nand hyperspherical harmonics.", "published": "2021-09-29 11:57:55", "link": "http://arxiv.org/abs/2109.14370v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Universal Deep Room Acoustics Estimator", "abstract": "Speech audio quality is subject to degradation caused by an acoustic\nenvironment and isotropic ambient and point noises. The environment can lead to\ndecreased speech intelligibility and loss of focus and attention by the\nlistener. Basic acoustic parameters that characterize the environment well are\n(i) signal-to-noise ratio (SNR), (ii) speech transmission index, (iii)\nreverberation time, (iv) clarity, and (v) direct-to-reverberant ratio. Except\nfor the SNR, these parameters are usually derived from the Room Impulse\nResponse (RIR) measurements; however, such measurements are often not\navailable. This work presents a universal room acoustic estimator design based\non convolutional recurrent neural networks that estimate the acoustic\nenvironment measurement blindly and jointly. Our results indicate that the\nproposed system is robust to non-stationary signal variations and outperforms\ncurrent state-of-the-art methods.", "published": "2021-09-29 14:19:34", "link": "http://arxiv.org/abs/2109.14436v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Cross-domain Semi-Supervised Audio Event Classification Using\n  Contrastive Regularization", "abstract": "In this study, we proposed a novel semi-supervised training method that uses\nunlabeled data with a class distribution that is completely different from the\ntarget data or data without a target label. To this end, we introduce a\ncontrastive regularization that is designed to be target task-oriented and\ntrained simultaneously. In addition, we propose an audio mixing based simple\naugmentation strategy that performed in batch samples. Experimental results\nvalidate that the proposed method successfully contributed to the performance\nimprovement, and particularly showed that it has advantages in stable training\nand generalization.", "published": "2021-09-29 15:43:12", "link": "http://arxiv.org/abs/2109.14508v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Tiny-CRNN: Streaming Wakeword Detection In A Low Footprint Setting", "abstract": "In this work, we propose Tiny-CRNN (Tiny Convolutional Recurrent Neural\nNetwork) models applied to the problem of wakeword detection, and augment them\nwith scaled dot product attention. We find that, compared to Convolutional\nNeural Network models, False Accepts in a 250k parameter budget can be reduced\nby 25% with a 10% reduction in parameter size by using models based on the\nTiny-CRNN architecture, and we can get up to 32% reduction in False Accepts at\na 50k parameter budget with 75% reduction in parameter size compared to\nword-level Dense Neural Network models. We discuss solutions to the challenging\nproblem of performing inference on streaming audio with this architecture, as\nwell as differences in start-end index errors and latency in comparison to CNN,\nDNN, and DNN-HMM models.", "published": "2021-09-29 21:12:14", "link": "http://arxiv.org/abs/2109.14725v1", "categories": ["cs.LG", "cs.SD", "eess.AS", "I.2.0"], "primary_category": "cs.LG"}
{"title": "Real-Time Multi-Level Neonatal Heart and Lung Sound Quality Assessment\n  for Telehealth Applications", "abstract": "Digital stethoscopes in combination with telehealth allow chest sounds to be\neasily collected and transmitted for remote monitoring and diagnosis. Chest\nsounds contain important information about a newborn's cardio-respiratory\nhealth. However, low-quality recordings complicate the remote monitoring and\ndiagnosis. In this study, a new method is proposed to objectively and\nautomatically assess heart and lung signal quality on a 5-level scale in\nreal-time and to assess the effect of signal quality on vital sign estimation.\nFor the evaluation, a total of 207 10s long chest sounds were taken from 119\npreterm and full-term babies. Thirty of the recordings from ten subjects were\nobtained with synchronous vital signs from the Neonatal Intensive Care Unit\n(NICU) based on electrocardiogram recordings. As reference, seven annotators\nindependently assessed the signal quality. For automatic quality\nclassification, 400 features were extracted from the chest sounds. After\nfeature selection using minimum redundancy and maximum relevancy algorithm,\nclass balancing, and hyper-parameter optimization, a variety of multi-class and\nordinal classification and regression algorithms were trained. Then, heart rate\nand breathing rate were automatically estimated from the chest sounds using\nadapted pre-existing methods. The results of subject-wise leave-one-out\ncross-validation show that the best-performing models had a mean squared error\n(MSE) of 0.49 and 0.61, and balanced accuracy of 57% and 51% for heart and lung\nqualities, respectively. The best-performing models for real-time analysis\n(<200ms) had MSE of 0.459 and 0.67, and balanced accuracy of 57% and 46%,\nrespectively. Our experimental results underscore that increasing the signal\nquality leads to a reduction in vital sign error, with only high-quality\nrecordings having a mean absolute error of less than 5 beats per minute, as\nrequired for clinical usage.", "published": "2021-09-29 01:08:20", "link": "http://arxiv.org/abs/2109.15127v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
