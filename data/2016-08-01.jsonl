{"title": "Left-corner Methods for Syntactic Modeling with Universal Structural\n  Constraints", "abstract": "The primary goal in this thesis is to identify better syntactic constraint or\nbias, that is language independent but also efficiently exploitable during\nsentence processing. We focus on a particular syntactic construction called\ncenter-embedding, which is well studied in psycholinguistics and noted to cause\nparticular difficulty for comprehension. Since people use language as a tool\nfor communication, one expects such complex constructions to be avoided for\ncommunication efficiency. From a computational perspective, center-embedding is\nclosely relevant to a left-corner parsing algorithm, which can capture the\ndegree of center-embedding of a parse tree being constructed. This connection\nsuggests left-corner methods can be a tool to exploit the universal syntactic\nconstraint that people avoid generating center-embedded structures. We explore\nsuch utilities of center-embedding as well as left-corner methods extensively\nthrough several theoretical and empirical examinations.\n  Our primary task is unsupervised grammar induction. In this task, the input\nto the algorithm is a collection of sentences, from which the model tries to\nextract the salient patterns on them as a grammar. This is a particularly hard\nproblem although we expect the universal constraint may help in improving the\nperformance since it can effectively restrict the possible search space for the\nmodel. We build the model by extending the left-corner parsing algorithm for\nefficiently tabulating the search space except those involving center-embedding\nup to a specific degree. We examine the effectiveness of our approach on many\ntreebanks, and demonstrate that often our constraint leads to better parsing\nperformance. We thus conclude that left-corner methods are particularly useful\nfor syntax-oriented systems, as it can exploit efficiently the inherent\nuniversal constraints in languages.", "published": "2016-08-01 01:18:53", "link": "http://arxiv.org/abs/1608.00293v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Crowd-sourcing NLG Data: Pictures Elicit Better Data", "abstract": "Recent advances in corpus-based Natural Language Generation (NLG) hold the\npromise of being easily portable across domains, but require costly training\ndata, consisting of meaning representations (MRs) paired with Natural Language\n(NL) utterances. In this work, we propose a novel framework for crowdsourcing\nhigh quality NLG training data, using automatic quality control measures and\nevaluating different MRs with which to elicit data. We show that pictorial MRs\nresult in better NL data being collected than logic-based MRs: utterances\nelicited by pictorial MRs are judged as significantly more natural, more\ninformative, and better phrased, with a significant increase in average quality\nratings (around 0.5 points on a 6-point scale), compared to using the logical\nMRs. As the MR becomes more complex, the benefits of pictorial stimuli\nincrease. The collected data will be released as part of this submission.", "published": "2016-08-01 07:30:38", "link": "http://arxiv.org/abs/1608.00339v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Blind phoneme segmentation with temporal prediction errors", "abstract": "Phonemic segmentation of speech is a critical step of speech recognition\nsystems. We propose a novel unsupervised algorithm based on sequence prediction\nmodels such as Markov chains and recurrent neural network. Our approach\nconsists in analyzing the error profile of a model trained to predict speech\nfeatures frame-by-frame. Specifically, we try to learn the dynamics of speech\nin the MFCC space and hypothesize boundaries from local maxima in the\nprediction error. We evaluate our system on the TIMIT dataset, with\nimprovements over similar methods.", "published": "2016-08-01 17:51:03", "link": "http://arxiv.org/abs/1608.00508v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Structured prediction models for RNN based sequence labeling in clinical\n  text", "abstract": "Sequence labeling is a widely used method for named entity recognition and\ninformation extraction from unstructured natural language data. In clinical\ndomain one major application of sequence labeling involves extraction of\nmedical entities such as medication, indication, and side-effects from\nElectronic Health Record narratives. Sequence labeling in this domain, presents\nits own set of challenges and objectives. In this work we experimented with\nvarious CRF based structured learning models with Recurrent Neural Networks. We\nextend the previously studied LSTM-CRF models with explicit modeling of\npairwise potentials. We also propose an approximate version of skip-chain CRF\ninference with RNN potentials. We use these methodologies for structured\nprediction in order to improve the exact phrase detection of various medical\nentities.", "published": "2016-08-01 20:54:22", "link": "http://arxiv.org/abs/1608.00612v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Neural Knowledge Language Model", "abstract": "Current language models have a significant limitation in the ability to\nencode and decode factual knowledge. This is mainly because they acquire such\nknowledge from statistical co-occurrences although most of the knowledge words\nare rarely observed. In this paper, we propose a Neural Knowledge Language\nModel (NKLM) which combines symbolic knowledge provided by the knowledge graph\nwith the RNN language model. By predicting whether the word to generate has an\nunderlying fact or not, the model can generate such knowledge-related words by\ncopying from the description of the predicted fact. In experiments, we show\nthat the NKLM significantly improves the performance while generating a much\nsmaller number of unknown words.", "published": "2016-08-01 04:42:49", "link": "http://arxiv.org/abs/1608.00318v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Labeling Topics with Images using Neural Networks", "abstract": "Topics generated by topic models are usually represented by lists of $t$\nterms or alternatively using short phrases and images. The current\nstate-of-the-art work on labeling topics using images selects images by\nre-ranking a small set of candidates for a given topic. In this paper, we\npresent a more generic method that can estimate the degree of association\nbetween any arbitrary pair of an unseen topic and image using a deep neural\nnetwork. Our method has better runtime performance $O(n)$ compared to $O(n^2)$\nfor the current state-of-the-art method, and is also significantly more\naccurate.", "published": "2016-08-01 15:27:16", "link": "http://arxiv.org/abs/1608.00470v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Keyphrase Extraction using Sequential Labeling", "abstract": "Keyphrases efficiently summarize a document's content and are used in various\ndocument processing and retrieval tasks. Several unsupervised techniques and\nclassifiers exist for extracting keyphrases from text documents. Most of these\nmethods operate at a phrase-level and rely on part-of-speech (POS) filters for\ncandidate phrase generation. In addition, they do not directly handle\nkeyphrases of varying lengths. We overcome these modeling shortcomings by\naddressing keyphrase extraction as a sequential labeling task in this paper. We\nexplore a basic set of features commonly used in NLP tasks as well as\npredictions from various unsupervised methods to train our taggers. In addition\nto a more natural modeling for the keyphrase extraction problem, we show that\ntagging models yield significant performance benefits over existing\nstate-of-the-art extraction methods.", "published": "2016-08-01 06:00:22", "link": "http://arxiv.org/abs/1608.00329v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Learning Semantically Coherent and Reusable Kernels in Convolution\n  Neural Nets for Sentence Classification", "abstract": "The state-of-the-art CNN models give good performance on sentence\nclassification tasks. The purpose of this work is to empirically study\ndesirable properties such as semantic coherence, attention mechanism and\nreusability of CNNs in these tasks. Semantically coherent kernels are\npreferable as they are a lot more interpretable for explaining the decision of\nthe learned CNN model. We observe that the learned kernels do not have semantic\ncoherence. Motivated by this observation, we propose to learn kernels with\nsemantic coherence using clustering scheme combined with Word2Vec\nrepresentation and domain knowledge such as SentiWordNet. We suggest a\ntechnique to visualize attention mechanism of CNNs for decision explanation\npurpose. Reusable property enables kernels learned on one problem to be used in\nanother problem. This helps in efficient learning as only a few additional\ndomain specific filters may have to be learned. We demonstrate the efficacy of\nour core ideas of learning semantically coherent kernels and leveraging\nreusable kernels for efficient learning on several benchmark datasets.\nExperimental results show the usefulness of our approach by achieving\nperformance close to the state-of-the-art methods but with semantic and\nreusable properties.", "published": "2016-08-01 15:14:08", "link": "http://arxiv.org/abs/1608.00466v2", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
