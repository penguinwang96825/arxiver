{"title": "Towards Verifying Semantic Roles Co-occurrence", "abstract": "Semantic role theory considers roles as a small universal set of unanalyzed\nentities. It means that formally there are no restrictions on role\ncombinations. We argue that the semantic roles co-occur in verb\nrepresentations. It means that there are hidden restrictions on role\ncombinations. To demonstrate that a practical and evidence-based approach has\nbeen built on in-depth analysis of the largest verb database VerbNet. The\nconsequences of this approach are considered.", "published": "2018-10-09 09:28:27", "link": "http://arxiv.org/abs/1810.03875v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Event Representation through Semantic Roles: Evaluation of Coverage", "abstract": "Semantic role theory is a widely used approach for event representation. Yet,\nthere are multiple indications that semantic role paradigm is necessary but not\nsufficient to cover all elements of event structure. We conducted an analysis\nof semantic role representation for events to provide an empirical evidence of\ninsufficiency. The consequence of that is a hybrid role-scalar approach. The\nresults are considered as preliminary in investigation of semantic roles\ncoverage for event representation.", "published": "2018-10-09 09:37:31", "link": "http://arxiv.org/abs/1810.03879v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Two-Dimensional Sequence to Sequence Model in Neural Machine\n  Translation", "abstract": "This work investigates an alternative model for neural machine translation\n(NMT) and proposes a novel architecture, where we employ a multi-dimensional\nlong short-term memory (MDLSTM) for translation modeling. In the\nstate-of-the-art methods, source and target sentences are treated as\none-dimensional sequences over time, while we view translation as a\ntwo-dimensional (2D) mapping using an MDLSTM layer to define the correspondence\nbetween source and target words. We extend beyond the current sequence to\nsequence backbone NMT models to a 2D structure in which the source and target\nsentences are aligned with each other in a 2D grid. Our proposed topology shows\nconsistent improvements over attention-based sequence to sequence model on two\nWMT 2017 tasks, German$\\leftrightarrow$English.", "published": "2018-10-09 13:42:58", "link": "http://arxiv.org/abs/1810.03975v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Noun Cases Using Sequential Neural Networks", "abstract": "Morphological declension, which aims to inflect nouns to indicate number,\ncase and gender, is an important task in natural language processing (NLP).\nThis research proposal seeks to address the degree to which Recurrent Neural\nNetworks (RNNs) are efficient in learning to decline noun cases. Given the\nchallenge of data sparsity in processing morphologically rich languages and\nalso, the flexibility of sentence structures in such languages, we believe that\nmodeling morphological dependencies can improve the performance of neural\nnetwork models. It is suggested to carry out various experiments to understand\nthe interpretable features that may lead to a better generalization of the\nlearned models on cross-lingual tasks.", "published": "2018-10-09 14:01:41", "link": "http://arxiv.org/abs/1810.03996v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Fast, Compact, Accurate Model for Language Identification of Codemixed\n  Text", "abstract": "We address fine-grained multilingual language identification: providing a\nlanguage code for every token in a sentence, including codemixed text\ncontaining multiple languages. Such text is prevalent online, in documents,\nsocial media, and message boards. We show that a feed-forward network with a\nsimple globally constrained decoder can accurately and rapidly label both\ncodemixed and monolingual text in 100 languages and 100 language pairs. This\nmodel outperforms previously published multilingual approaches in terms of both\naccuracy and speed, yielding an 800x speed-up and a 19.5% averaged absolute\ngain on three codemixed datasets. It furthermore outperforms several benchmark\nsystems on monolingual language identification.", "published": "2018-10-09 17:21:41", "link": "http://arxiv.org/abs/1810.04142v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Event Coreference Resolution Using Neural Network Classifiers", "abstract": "This paper presents a neural network classifier approach to detecting both\nwithin- and cross- document event coreference effectively using only event\nmention based features. Our approach does not (yet) rely on any event argument\nfeatures such as semantic roles or spatiotemporal arguments. Experimental\nresults on the ECB+ dataset show that our approach produces F1 scores that\nsignificantly outperform the state-of-the-art methods for both within-document\nand cross-document event coreference resolution when we use B3 and CEAFe\nevaluation measures, but gets worse F1 score with the MUC measure. However,\nwhen we use the CoNLL measure, which is the average of these three scores, our\napproach has slightly better F1 for within- document event coreference\nresolution but is significantly better for cross-document event coreference\nresolution.", "published": "2018-10-09 19:00:52", "link": "http://arxiv.org/abs/1810.04216v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Decipherment of Historical Manuscript Images", "abstract": "European libraries and archives are filled with enciphered manuscripts from\nthe early modern period. These include military and diplomatic correspondence,\nrecords of secret societies, private letters, and so on. Although they are\nenciphered with classical cryptographic algorithms, their contents are\nunavailable to working historians. We therefore attack the problem of\nautomatically converting cipher manuscript images into plaintext. We develop\nunsupervised models for character segmentation, character-image clustering, and\ndecipherment of cluster sequences. We experiment with both pipelined and joint\nmodels, and we give empirical results for multiple ciphers.", "published": "2018-10-09 23:21:18", "link": "http://arxiv.org/abs/1810.04297v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The combination of context information to enhance simple question\n  answering", "abstract": "With the rapid development of knowledge base,question answering based on\nknowledge base has been a hot research issue. In this paper, we focus on\nanswering singlerelation factoid questions based on knowledge base. We build a\nquestion answering system and study the effect of context information on fact\nselection, such as entity's notable type,outdegree. Experimental results show\nthat context information can improve the result of simple question answering.", "published": "2018-10-09 14:02:56", "link": "http://arxiv.org/abs/1810.04000v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "An Instance Transfer based Approach Using Enhanced Recurrent Neural\n  Network for Domain Named Entity Recognition", "abstract": "Recently, neural networks have shown promising results for named entity\nrecognition (NER), which needs a number of labeled data to for model training.\nWhen meeting a new domain (target domain) for NER, there is no or a few labeled\ndata, which makes domain NER much more difficult. As NER has been researched\nfor a long time, some similar domain already has well labelled data (source\ndomain). Therefore, in this paper, we focus on domain NER by studying how to\nutilize the labelled data from such similar source domain for the new target\ndomain. We design a kernel function based instance transfer strategy by getting\nsimilar labelled sentences from a source domain. Moreover, we propose an\nenhanced recurrent neural network (ERNN) by adding an additional layer that\ncombines the source domain labelled data into traditional RNN structure.\nComprehensive experiments are conducted on two datasets. The comparison results\namong HMM, CRF and RNN show that RNN performs bette than others. When there is\nno labelled data in domain target, compared to directly using the source domain\nlabelled data without selecting transferred instances, our enhanced RNN\napproach gets improvement from 0.8052 to 0.9328 in terms of F1 measure.", "published": "2018-10-09 06:55:04", "link": "http://arxiv.org/abs/1810.06644v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Using Sentiment Representation Learning to Enhance Gender Classification\n  for User Profiling", "abstract": "User profiling means exploiting the technology of machine learning to predict\nattributes of users, such as demographic attributes, hobby attributes,\npreference attributes, etc. It's a powerful data support of precision\nmarketing. Existing methods mainly study network behavior, personal\npreferences, post texts to build user profile. Through our data analysis of\nmicro-blog, we find that females show more positive and have richer emotions\nthan males in online social platform. This difference is very conducive to the\ndistinction between genders. Therefore, we argue that sentiment context is\nimportant as well for user profiling.This paper focuses on exploiting microblog\nuser posts to predict one of the demographic labels: gender. We propose a\nSentiment Representation Learning based Multi-Layer Perceptron(SRL-MLP) model\nto classify gender. First we build a sentiment polarity classifier in advance\nby training Long Short-Term Memory(LSTM) model on e-commerce review corpus.\nNext we transfer sentiment representation to a basic MLP network. Last we\nconduct experiments on gender classification by sentiment representation.\nExperimental results show that our approach can improve gender classification\naccuracy by 5.53\\%, from 84.20\\% to 89.73\\%.", "published": "2018-10-09 07:01:20", "link": "http://arxiv.org/abs/1810.06645v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fake Comment Detection Based on Sentiment Analysis", "abstract": "With the development of the E-commerce and reviews website, the comment\ninformation is influencing people's life. More and more users share their\nconsumption experience and evaluate the quality of commodity by comment. When\npeople make a decision, they will refer these comments. The dependency of the\ncomments make the fake comment appear. The fake comment is that for profit and\nother bad motivation, business fabricate untrue consumption experience and they\npreach or slander some products. The fake comment is easy to mislead users'\nopinion and decision. The accuracy of humans identifying fake comment is low.\nIt's meaningful to detect fake comment using natural language processing\ntechnology for people getting true comment information. This paper uses the\nsentimental analysis to detect fake comment.", "published": "2018-10-09 11:28:19", "link": "http://arxiv.org/abs/1811.05825v1", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "textTOvec: Deep Contextualized Neural Autoregressive Topic Models of\n  Language with Distributed Compositional Prior", "abstract": "We address two challenges of probabilistic topic modelling in order to better\nestimate the probability of a word in a given context, i.e., P(word|context):\n(1) No Language Structure in Context: Probabilistic topic models ignore word\norder by summarizing a given context as a \"bag-of-word\" and consequently the\nsemantics of words in the context is lost. The LSTM-LM learns a vector-space\nrepresentation of each word by accounting for word order in local collocation\npatterns and models complex characteristics of language (e.g., syntax and\nsemantics), while the TM simultaneously learns a latent representation from the\nentire document and discovers the underlying thematic structure. We unite two\ncomplementary paradigms of learning the meaning of word occurrences by\ncombining a TM (e.g., DocNADE) and a LM in a unified probabilistic framework,\nnamed as ctx-DocNADE. (2) Limited Context and/or Smaller training corpus of\ndocuments: In settings with a small number of word occurrences (i.e., lack of\ncontext) in short text or data sparsity in a corpus of few documents, the\napplication of TMs is challenging. We address this challenge by incorporating\nexternal knowledge into neural autoregressive topic models via a language\nmodelling approach: we use word embeddings as input of a LSTM-LM with the aim\nto improve the word-topic mapping on a smaller and/or short-text corpus. The\nproposed DocNADE extension is named as ctx-DocNADEe.\n  We present novel neural autoregressive topic model variants coupled with\nneural LMs and embeddings priors that consistently outperform state-of-the-art\ngenerative TMs in terms of generalization (perplexity), interpretability (topic\ncoherence) and applicability (retrieval and classification) over 6 long-text\nand 8 short-text datasets from diverse domains.", "published": "2018-10-09 13:04:25", "link": "http://arxiv.org/abs/1810.03947v4", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Federated Learning for Keyword Spotting", "abstract": "We propose a practical approach based on federated learning to solve\nout-of-domain issues with continuously running embedded speech-based models\nsuch as wake word detectors. We conduct an extensive empirical study of the\nfederated averaging algorithm for the \"Hey Snips\" wake word based on a\ncrowdsourced dataset that mimics a federation of wake word users. We\nempirically demonstrate that using an adaptive averaging strategy inspired from\nAdam in place of standard weighted model averaging highly reduces the number of\ncommunication rounds required to reach our target performance. The associated\nupstream communication costs per user are estimated at 8 MB, which is a\nreasonable in the context of smart home voice assistants. Additionally, the\ndataset used for these experiments is being open sourced with the aim of\nfostering further transparent research in the application of federated learning\nto speech data.", "published": "2018-10-09 09:41:15", "link": "http://arxiv.org/abs/1810.05512v4", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "TRAMP: Tracking by a Real-time AMbisonic-based Particle filter", "abstract": "This article presents a multiple sound source localization and tracking\nsystem, fed by the Eigenmike array. The First Order Ambisonics (FOA) format is\nused to build a pseudointensity-based spherical histogram, from which the\nsource position estimates are deduced. These instantaneous estimates are\nprocessed by a wellknown tracking system relying on a set of particle filters.\nWhile the novelty within localization and tracking is incremental, the\nfully-functional, complete and real-time running system based on these\nalgorithms is proposed for the first time. As such, it could serve as an\nadditional baseline method of the LOCATA challenge.", "published": "2018-10-09 15:41:15", "link": "http://arxiv.org/abs/1810.04080v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
