{"title": "From POS tagging to dependency parsing for biomedical event extraction", "abstract": "Background: Given the importance of relation or event extraction from\nbiomedical research publications to support knowledge capture and synthesis,\nand the strong dependency of approaches to this information extraction task on\nsyntactic information, it is valuable to understand which approaches to\nsyntactic processing of biomedical text have the highest performance. Results:\nWe perform an empirical study comparing state-of-the-art traditional\nfeature-based and neural network-based models for two core natural language\nprocessing tasks of part-of-speech (POS) tagging and dependency parsing on two\nbenchmark biomedical corpora, GENIA and CRAFT. To the best of our knowledge,\nthere is no recent work making such comparisons in the biomedical context;\nspecifically no detailed analysis of neural models on this data is available.\nExperimental results show that in general, the neural models outperform the\nfeature-based models on two benchmark biomedical corpora GENIA and CRAFT. We\nalso perform a task-oriented evaluation to investigate the influences of these\nmodels in a downstream application on biomedical event extraction, and show\nthat better intrinsic parsing performance does not always imply better\nextrinsic event extraction performance. Conclusion: We have presented a\ndetailed empirical study comparing traditional feature-based and neural\nnetwork-based models for POS tagging and dependency parsing in the biomedical\ncontext, and also investigated the influence of parser selection for a\nbiomedical event extraction downstream task. Availability of data and material:\nWe make the retrained models available at\nhttps://github.com/datquocnguyen/BioPosDep", "published": "2018-08-11 01:03:45", "link": "http://arxiv.org/abs/1808.03731v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ancient-Modern Chinese Translation with a Large Training Dataset", "abstract": "Ancient Chinese brings the wisdom and spirit culture of the Chinese nation.\nAutomatic translation from ancient Chinese to modern Chinese helps to inherit\nand carry forward the quintessence of the ancients. However, the lack of\nlarge-scale parallel corpus limits the study of machine translation in\nAncient-Modern Chinese. In this paper, we propose an Ancient-Modern Chinese\nclause alignment approach based on the characteristics of these two languages.\nThis method combines both lexical-based information and statistical-based\ninformation, which achieves 94.2 F1-score on our manual annotation Test set. We\nuse this method to create a new large-scale Ancient-Modern Chinese parallel\ncorpus which contains 1.24M bilingual pairs. To our best knowledge, this is the\nfirst large high-quality Ancient-Modern Chinese dataset. Furthermore, we\nanalyzed and compared the performance of the SMT and various NMT models on this\ndataset and provided a strong baseline for this task.", "published": "2018-08-11 02:06:25", "link": "http://arxiv.org/abs/1808.03738v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dropout during inference as a model for neurological degeneration in an\n  image captioning network", "abstract": "We replicate a variation of the image captioning architecture by Vinyals et\nal. (2015), then introduce dropout during inference mode to simulate the\neffects of neurodegenerative diseases like Alzheimer's disease (AD) and\nWernicke's aphasia (WA). We evaluate the effects of dropout on language\nproduction by measuring the KL-divergence of word frequency distributions and\nother linguistic metrics as dropout is added. We find that the generated\nsentences most closely approximate the word frequency distribution of the\ntraining corpus when using a moderate dropout of 0.4 during inference.", "published": "2018-08-11 03:50:27", "link": "http://arxiv.org/abs/1808.03747v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Impact of Automatic Pre-annotation in Clinical Note Data Element\n  Extraction - the CLEAN Tool", "abstract": "Objective. Annotation is expensive but essential for clinical note review and\nclinical natural language processing (cNLP). However, the extent to which\ncomputer-generated pre-annotation is beneficial to human annotation is still an\nopen question. Our study introduces CLEAN (CLinical note rEview and\nANnotation), a pre-annotation-based cNLP annotation system to improve clinical\nnote annotation of data elements, and comprehensively compares CLEAN with the\nwidely-used annotation system Brat Rapid Annotation Tool (BRAT).\n  Materials and Methods. CLEAN includes an ensemble pipeline (CLEAN-EP) with a\nnewly developed annotation tool (CLEAN-AT). A domain expert and a novice\nuser/annotator participated in a comparative usability test by tagging 87 data\nelements related to Congestive Heart Failure (CHF) and Kawasaki Disease (KD)\ncohorts in 84 public notes.\n  Results. CLEAN achieved higher note-level F1-score (0.896) over BRAT (0.820),\nwith significant difference in correctness (P-value < 0.001), and the mostly\nrelated factor being system/software (P-value < 0.001). No significant\ndifference (P-value 0.188) in annotation time was observed between CLEAN (7.262\nminutes/note) and BRAT (8.286 minutes/note). The difference was mostly\nassociated with note length (P-value < 0.001) and system/software (P-value\n0.013). The expert reported CLEAN to be useful/satisfactory, while the novice\nreported slight improvements.\n  Discussion. CLEAN improves the correctness of annotation and increases\nusefulness/satisfaction with the same level of efficiency. Limitations include\nuntested impact of pre-annotation correctness rate, small sample size, small\nuser size, and restrictedly validated gold standard.\n  Conclusion. CLEAN with pre-annotation can be beneficial for an expert to deal\nwith complex annotation tasks involving numerous and diverse target data\nelements.", "published": "2018-08-11 13:55:27", "link": "http://arxiv.org/abs/1808.03806v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Full End-to-End Semantic Role Labeler, Syntax-agnostic Over\n  Syntax-aware?", "abstract": "Semantic role labeling (SRL) is to recognize the predicate-argument structure\nof a sentence, including subtasks of predicate disambiguation and argument\nlabeling. Previous studies usually formulate the entire SRL problem into two or\nmore subtasks. For the first time, this paper introduces an end-to-end neural\nmodel which unifiedly tackles the predicate disambiguation and the argument\nlabeling in one shot. Using a biaffine scorer, our model directly predicts all\nsemantic role labels for all given word pairs in the sentence without relying\non any syntactic parse information. Specifically, we augment the BiLSTM encoder\nwith a non-linear transformation to further distinguish the predicate and the\nargument in a given sentence, and model the semantic role labeling process as a\nword pair classification task by employing the biaffine attentional mechanism.\nThough the proposed model is syntax-agnostic with local decoder, it outperforms\nthe state-of-the-art syntax-aware SRL systems on the CoNLL-2008, 2009\nbenchmarks for both English and Chinese. To our best knowledge, we report the\nfirst syntax-agnostic SRL model that surpasses all known syntax-aware models.", "published": "2018-08-11 14:59:07", "link": "http://arxiv.org/abs/1808.03815v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fake Sentence Detection as a Training Task for Sentence Encoding", "abstract": "Sentence encoders are typically trained on language modeling tasks with large\nunlabeled datasets. While these encoders achieve state-of-the-art results on\nmany sentence-level tasks, they are difficult to train with long training\ncycles. We introduce fake sentence detection as a new training task for\nlearning sentence encoders. We automatically generate fake sentences by\ncorrupting original sentences from a source collection and train the encoders\nto produce representations that are effective at detecting fake sentences. This\nbinary classification task turns to be quite efficient for training sentence\nencoders. We compare a basic BiLSTM encoder trained on this task with a strong\nsentence encoding models (Skipthought and FastSent) trained on a language\nmodeling task. We find that the BiLSTM trains much faster on fake sentence\ndetection (20 hours instead of weeks) using smaller amounts of data (1M instead\nof 64M sentences). Further analysis shows the learned representations capture\nmany syntactic and semantic properties expected from good sentence\nrepresentations.", "published": "2018-08-11 17:31:15", "link": "http://arxiv.org/abs/1808.03840v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pervasive Attention: 2D Convolutional Neural Networks for\n  Sequence-to-Sequence Prediction", "abstract": "Current state-of-the-art machine translation systems are based on\nencoder-decoder architectures, that first encode the input sequence, and then\ngenerate an output sequence based on the input encoding. Both are interfaced\nwith an attention mechanism that recombines a fixed encoding of the source\ntokens based on the decoder state. We propose an alternative approach which\ninstead relies on a single 2D convolutional neural network across both\nsequences. Each layer of our network re-codes source tokens on the basis of the\noutput sequence produced so far. Attention-like properties are therefore\npervasive throughout the network. Our model yields excellent results,\noutperforming state-of-the-art encoder-decoder systems, while being\nconceptually simpler and having fewer parameters.", "published": "2018-08-11 21:23:24", "link": "http://arxiv.org/abs/1808.03867v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge Graph Embedding with Entity Neighbors and Deep Memory Network", "abstract": "Knowledge Graph Embedding (KGE) aims to represent entities and relations of\nknowledge graph in a low-dimensional continuous vector space. Recent works\nfocus on incorporating structural knowledge with additional information, such\nas entity descriptions, relation paths and so on. However, common used\nadditional information usually contains plenty of noise, which makes it hard to\nlearn valuable representation. In this paper, we propose a new kind of\nadditional information, called entity neighbors, which contain both semantic\nand topological features about given entity. We then develop a deep memory\nnetwork model to encode information from neighbors. Employing a gating\nmechanism, representations of structure and neighbors are integrated into a\njoint representation. The experimental results show that our model outperforms\nexisting KGE methods utilizing entity descriptions and achieves\nstate-of-the-art metrics on 4 datasets.", "published": "2018-08-11 05:05:06", "link": "http://arxiv.org/abs/1808.03752v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Familia: A Configurable Topic Modeling Framework for Industrial Text\n  Engineering", "abstract": "In the last decade, a variety of topic models have been proposed for text\nengineering. However, except Probabilistic Latent Semantic Analysis (PLSA) and\nLatent Dirichlet Allocation (LDA), most of existing topic models are seldom\napplied or considered in industrial scenarios. This phenomenon is caused by the\nfact that there are very few convenient tools to support these topic models so\nfar. Intimidated by the demanding expertise and labor of designing and\nimplementing parameter inference algorithms, software engineers are prone to\nsimply resort to PLSA/LDA, without considering whether it is proper for their\nproblem at hand or not. In this paper, we propose a configurable topic modeling\nframework named Familia, in order to bridge the huge gap between academic\nresearch fruits and current industrial practice. Familia supports an important\nline of topic models that are widely applicable in text engineering scenarios.\nIn order to relieve burdens of software engineers without knowledge of Bayesian\nnetworks, Familia is able to conduct automatic parameter inference for a\nvariety of topic models. Simply through changing the data organization of\nFamilia, software engineers are able to easily explore a broad spectrum of\nexisting topic models or even design their own topic models, and find the one\nthat best suits the problem at hand. With its superior extendability, Familia\nhas a novel sampling mechanism that strikes balance between effectiveness and\nefficiency of parameter inference. Furthermore, Familia is essentially a big\ntopic modeling framework that supports parallel parameter inference and\ndistributed parameter storage. The utilities and necessity of Familia are\ndemonstrated in real-life industrial applications. Familia would significantly\nenlarge software engineers' arsenal of topic models and pave the way for\nutilizing highly customized topic models in real-life problems.", "published": "2018-08-11 01:14:50", "link": "http://arxiv.org/abs/1808.03733v2", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Document Informed Neural Autoregressive Topic Models", "abstract": "Context information around words helps in determining their actual meaning,\nfor example \"networks\" used in contexts of artificial neural networks or\nbiological neuron networks. Generative topic models infer topic-word\ndistributions, taking no or only little context into account. Here, we extend a\nneural autoregressive topic model to exploit the full context information\naround words in a document in a language modeling fashion. This results in an\nimproved performance in terms of generalization, interpretability and\napplicability. We apply our modeling approach to seven data sets from various\ndomains and demonstrate that our approach consistently outperforms\nstateof-the-art generative topic models. With the learned representations, we\nshow on an average a gain of 9.6% (0.57 Vs 0.52) in precision at retrieval\nfraction 0.02 and 7.2% (0.582 Vs 0.543) in F1 for text categorization.", "published": "2018-08-11 12:16:09", "link": "http://arxiv.org/abs/1808.03793v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "jLDADMM: A Java package for the LDA and DMM topic models", "abstract": "In this technical report, we present jLDADMM---an easy-to-use Java toolkit\nfor conventional topic models. jLDADMM is released to provide alternatives for\ntopic modeling on normal or short texts. It provides implementations of the\nLatent Dirichlet Allocation topic model and the one-topic-per-document\nDirichlet Multinomial Mixture model (i.e. mixture of unigrams), using collapsed\nGibbs sampling. In addition, jLDADMM supplies a document clustering evaluation\nto compare topic models. jLDADMM is open-source and available to download at:\nhttps://github.com/datquocnguyen/jLDADMM", "published": "2018-08-11 16:47:58", "link": "http://arxiv.org/abs/1808.03835v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.IR"}
