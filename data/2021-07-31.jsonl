{"title": "A Psychologically Informed Part-of-Speech Analysis of Depression in\n  Social Media", "abstract": "In this work, we provide an extensive part-of-speech analysis of the\ndiscourse of social media users with depression. Research in psychology\nrevealed that depressed users tend to be self-focused, more preoccupied with\nthemselves and ruminate more about their lives and emotions. Our work aims to\nmake use of large-scale datasets and computational methods for a quantitative\nexploration of discourse. We use the publicly available depression dataset from\nthe Early Risk Prediction on the Internet Workshop (eRisk) 2018 and extract\npart-of-speech features and several indices based on them. Our results reveal\nstatistically significant differences between the depressed and non-depressed\nindividuals confirming findings from the existing psychology literature. Our\nwork provides insights regarding the way in which depressed individuals are\nexpressing themselves on social media platforms, allowing for better-informed\ncomputational models to help monitor and prevent mental illnesses.", "published": "2021-07-31 16:23:22", "link": "http://arxiv.org/abs/2108.00279v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Human Evaluation of Creative NLG Systems: An Interdisciplinary Survey on\n  Recent Papers", "abstract": "We survey human evaluation in papers presenting work on creative natural\nlanguage generation that have been published in INLG 2020 and ICCC 2020. The\nmost typical human evaluation method is a scaled survey, typically on a 5 point\nscale, while many other less common methods exist. The most commonly evaluated\nparameters are meaning, syntactic correctness, novelty, relevance and emotional\nvalue, among many others. Our guidelines for future evaluation include clearly\ndefining the goal of the generative system, asking questions as concrete as\npossible, testing the evaluation setup, using multiple different evaluation\nsetups, reporting the entire evaluation process and potential biases clearly,\nand finally analyzing the evaluation results in a more profound way than merely\nreporting the most typical statistics.", "published": "2021-07-31 18:54:30", "link": "http://arxiv.org/abs/2108.00308v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using Knowledge-Embedded Attention to Augment Pre-trained Language\n  Models for Fine-Grained Emotion Recognition", "abstract": "Modern emotion recognition systems are trained to recognize only a small set\nof emotions, and hence fail to capture the broad spectrum of emotions people\nexperience and express in daily life. In order to engage in more empathetic\ninteractions, future AI has to perform \\textit{fine-grained} emotion\nrecognition, distinguishing between many more varied emotions. Here, we focus\non improving fine-grained emotion recognition by introducing external knowledge\ninto a pre-trained self-attention model. We propose Knowledge-Embedded\nAttention (KEA) to use knowledge from emotion lexicons to augment the\ncontextual representations from pre-trained ELECTRA and BERT models. Our\nresults and error analyses outperform previous models on several datasets, and\nis better able to differentiate closely-confusable emotions, such as afraid and\nterrified.", "published": "2021-07-31 09:41:44", "link": "http://arxiv.org/abs/2108.00194v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Diverse Linguistic Features for Assessing Reading Difficulty of\n  Educational Filipino Texts", "abstract": "In order to ensure quality and effective learning, fluency, and\ncomprehension, the proper identification of the difficulty levels of reading\nmaterials should be observed. In this paper, we describe the development of\nautomatic machine learning-based readability assessment models for educational\nFilipino texts using the most diverse set of linguistic features for the\nlanguage. Results show that using a Random Forest model obtained a high\nperformance of 62.7% in terms of accuracy, and 66.1% when using the optimal\ncombination of feature sets consisting of traditional and syllable\npattern-based predictors.", "published": "2021-07-31 13:59:46", "link": "http://arxiv.org/abs/2108.00241v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Opinion Prediction with User Fingerprinting", "abstract": "Opinion prediction is an emerging research area with diverse real-world\napplications, such as market research and situational awareness. We identify\ntwo lines of approaches to the problem of opinion prediction. One uses\ntopic-based sentiment analysis with time-series modeling, while the other uses\nstatic embedding of text. The latter approaches seek user-specific solutions by\ngenerating user fingerprints. Such approaches are useful in predicting user's\nreactions to unseen content. In this work, we propose a novel dynamic\nfingerprinting method that leverages contextual embedding of user's comments\nconditioned on relevant user's reading history. We integrate BERT variants with\na recurrent neural network to generate predictions. The results show up to 13\\%\nimprovement in micro F1-score compared to previous approaches. Experimental\nresults show novel insights that were previously unknown such as better\npredictions for an increase in dynamic history length, the impact of the nature\nof the article on performance, thereby laying the foundation for further\nresearch.", "published": "2021-07-31 15:47:37", "link": "http://arxiv.org/abs/2108.00270v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Word2Pix: Word to Pixel Cross Attention Transformer in Visual Grounding", "abstract": "Current one-stage methods for visual grounding encode the language query as\none holistic sentence embedding before fusion with visual feature. Such a\nformulation does not treat each word of a query sentence on par when modeling\nlanguage to visual attention, therefore prone to neglect words which are less\nimportant for sentence embedding but critical for visual grounding. In this\npaper we propose Word2Pix: a one-stage visual grounding network based on\nencoder-decoder transformer architecture that enables learning for textual to\nvisual feature correspondence via word to pixel attention. The embedding of\neach word from the query sentence is treated alike by attending to visual\npixels individually instead of single holistic sentence embedding. In this way,\neach word is given equivalent opportunity to adjust the language to vision\nattention towards the referent target through multiple stacks of transformer\ndecoder layers. We conduct the experiments on RefCOCO, RefCOCO+ and RefCOCOg\ndatasets and the proposed Word2Pix outperforms existing one-stage methods by a\nnotable margin. The results obtained also show that Word2Pix surpasses\ntwo-stage visual grounding models, while at the same time keeping the merits of\none-stage paradigm namely end-to-end training and real-time inference speed\nintact.", "published": "2021-07-31 10:20:15", "link": "http://arxiv.org/abs/2108.00205v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "ECLARE: Extreme Classification with Label Graph Correlations", "abstract": "Deep extreme classification (XC) seeks to train deep architectures that can\ntag a data point with its most relevant subset of labels from an extremely\nlarge label set. The core utility of XC comes from predicting labels that are\nrarely seen during training. Such rare labels hold the key to personalized\nrecommendations that can delight and surprise a user. However, the large number\nof rare labels and small amount of training data per rare label offer\nsignificant statistical and computational challenges. State-of-the-art deep XC\nmethods attempt to remedy this by incorporating textual descriptions of labels\nbut do not adequately address the problem. This paper presents ECLARE, a\nscalable deep learning architecture that incorporates not only label text, but\nalso label correlations, to offer accurate real-time predictions within a few\nmilliseconds. Core contributions of ECLARE include a frugal architecture and\nscalable techniques to train deep models along with label correlation graphs at\nthe scale of millions of labels. In particular, ECLARE offers predictions that\nare 2 to 14% more accurate on both publicly available benchmark datasets as\nwell as proprietary datasets for a related products recommendation task sourced\nfrom the Bing search engine. Code for ECLARE is available at\nhttps://github.com/Extreme-classification/ECLARE.", "published": "2021-07-31 15:13:13", "link": "http://arxiv.org/abs/2108.00261v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "F.2.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "Chest ImaGenome Dataset for Clinical Reasoning", "abstract": "Despite the progress in automatic detection of radiologic findings from chest\nX-ray (CXR) images in recent years, a quantitative evaluation of the\nexplainability of these models is hampered by the lack of locally labeled\ndatasets for different findings. With the exception of a few expert-labeled\nsmall-scale datasets for specific findings, such as pneumonia and pneumothorax,\nmost of the CXR deep learning models to date are trained on global \"weak\"\nlabels extracted from text reports, or trained via a joint image and\nunstructured text learning strategy. Inspired by the Visual Genome effort in\nthe computer vision community, we constructed the first Chest ImaGenome dataset\nwith a scene graph data structure to describe $242,072$ images. Local\nannotations are automatically produced using a joint rule-based natural\nlanguage processing (NLP) and atlas-based bounding box detection pipeline.\nThrough a radiologist constructed CXR ontology, the annotations for each CXR\nare connected as an anatomy-centered scene graph, useful for image-level\nreasoning and multimodal fusion applications. Overall, we provide: i) $1,256$\ncombinations of relation annotations between $29$ CXR anatomical locations\n(objects with bounding box coordinates) and their attributes, structured as a\nscene graph per image, ii) over $670,000$ localized comparison relations (for\nimproved, worsened, or no change) between the anatomical locations across\nsequential exams, as well as ii) a manually annotated gold standard scene graph\ndataset from $500$ unique patients.", "published": "2021-07-31 20:10:30", "link": "http://arxiv.org/abs/2108.00316v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Detecting Propaganda on the Sentence Level during the COVID-19 Pandemic", "abstract": "The spread of misinformation, conspiracy, and questionable content and\ninformation manipulation by foreign adversaries on social media has surged\nalong with the COVID-19 pandemic. Such malicious cyber-enabled actions may\ncause increasing social polarization, health crises, and property loss. In this\npaper, using fine-tuned contextualized embedding trained on Reddit, we tackle\nthe detection of the propaganda of such user accounts and their targeted issues\non Twitter during March 2020 when the COVID-19 epidemic became recognized as a\npandemic. Our result shows that the pro-China group appeared to be tweeting 35\nto 115 times more than the neutral group. At the same time, neutral groups were\ntweeting more positive-attitude content and voicing alarm for the COVID-19\nsituation. The pro-China group was also using more call-for-action words on\npolitical issues not necessarily China-related.", "published": "2021-07-31 06:40:17", "link": "http://arxiv.org/abs/2108.12269v1", "categories": ["cs.CY", "cs.CL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "Sequence-to-Sequence Voice Reconstruction for Silent Speech in a Tonal\n  Language", "abstract": "Silent Speech Decoding (SSD), based on articulatory neuromuscular activities,\nhas become a prevalent task of Brain-Computer Interface (BCI) in recent years.\nMany works have been devoted to decoding surface electromyography (sEMG) from\narticulatory neuromuscular activities. However, restoring silent speech in\ntonal languages such as Mandarin Chinese is still difficult. This paper\nproposes an optimized Sequence-to-Sequence (Seq2Seq) approach to synthesize\nvoice from the sEMG-based silent speech. We extract duration information to\nregulate the sEMG-based silent speech using the audio length. Then, we provide\na deep-learning model with an encoder-decoder structure and a state-of-art\nvocoder to generate the audio waveform. Experiments based on six Mandarin\nChinese speakers demonstrate that the proposed model can successfully decode\nsilent speech in Mandarin Chinese and achieve a character error rate (CER) of\n6.41% on average with human evaluation.", "published": "2021-07-31 09:28:06", "link": "http://arxiv.org/abs/2108.00190v3", "categories": ["cs.SD", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
