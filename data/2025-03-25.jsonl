{"title": "Efficient Model Development through Fine-tuning Transfer", "abstract": "Modern LLMs struggle with efficient updates, as each new pretrained model\nversion requires repeating expensive alignment processes. This challenge also\napplies to domain- or language-specific models, where fine-tuning on\nspecialized data must be redone for every new base model release. In this\npaper, we explore the transfer of fine-tuning updates between model versions.\nSpecifically, we derive the diff vector from one source model version, which\nrepresents the weight changes from fine-tuning, and apply it to the base model\nof a different target version. Through empirical evaluations on various\nopen-weight model versions, we show that transferring diff vectors can\nsignificantly improve the target base model, often achieving performance\ncomparable to its fine-tuned counterpart. For example, reusing the fine-tuning\nupdates from Llama 3.0 8B leads to an absolute accuracy improvement of 10.7% on\nGPQA over the base Llama 3.1 8B without additional training, surpassing Llama\n3.1 8B Instruct. In a multilingual model development setting, we show that this\napproach can significantly increase performance on target-language tasks\nwithout retraining, achieving an absolute improvement of 4.7% and 15.5% on\nGlobal MMLU for Malagasy and Turkish, respectively, compared to Llama 3.1 8B\nInstruct. Our controlled experiments reveal that fine-tuning transfer is most\neffective when the source and target models are linearly connected in the\nparameter space. Additionally, we demonstrate that fine-tuning transfer offers\na stronger and more computationally efficient starting point for further\nfine-tuning. Finally, we propose an iterative recycling-then-finetuning\napproach for continuous model development, which improves both efficiency and\neffectiveness. Our findings suggest that fine-tuning transfer is a viable\nstrategy to reduce training costs while maintaining model performance.", "published": "2025-03-25 23:24:43", "link": "http://arxiv.org/abs/2503.20110v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "\"Is There Anything Else?'': Examining Administrator Influence on Linguistic Features from the Cookie Theft Picture Description Cognitive Test", "abstract": "Alzheimer's Disease (AD) dementia is a progressive neurodegenerative disease\nthat negatively impacts patients' cognitive ability. Previous studies have\ndemonstrated that changes in naturalistic language samples can be useful for\nearly screening of AD dementia. However, the nature of language deficits often\nrequires test administrators to use various speech elicitation techniques\nduring spontaneous language assessments to obtain enough propositional\nutterances from dementia patients. This could lead to the ``observer's effect''\non the downstream analysis that has not been fully investigated. Our study\nseeks to quantify the influence of test administrators on linguistic features\nin dementia assessment with two English corpora the ``Cookie Theft'' picture\ndescription datasets collected at different locations and test administrators\nshow different levels of administrator involvement. Our results show that the\nlevel of test administrator involvement significantly impacts observed\nlinguistic features in patient speech. These results suggest that many of\nsignificant linguistic features in the downstream classification task may be\npartially attributable to differences in the test administration practices\nrather than solely to participants' cognitive status. The variations in test\nadministrator behavior can lead to systematic biases in linguistic data,\npotentially confounding research outcomes and clinical assessments. Our study\nsuggests that there is a need for a more standardized test administration\nprotocol in the development of responsible clinical speech analytics\nframeworks.", "published": "2025-03-25 23:01:15", "link": "http://arxiv.org/abs/2503.20104v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bigger But Not Better: Small Neural Language Models Outperform Large Language Models in Detection of Thought Disorder", "abstract": "Disorganized thinking is a key diagnostic indicator of schizophrenia-spectrum\ndisorders. Recently, clinical estimates of the severity of disorganized\nthinking have been shown to correlate with measures of how difficult speech\ntranscripts would be for large language models (LLMs) to predict. However,\nLLMs' deployment challenges -- including privacy concerns, computational and\nfinancial costs, and lack of transparency of training data -- limit their\nclinical utility. We investigate whether smaller neural language models can\nserve as effective alternatives for detecting positive formal thought disorder,\nusing the same sliding window based perplexity measurements that proved\neffective with larger models. Surprisingly, our results show that smaller\nmodels are more sensitive to linguistic differences associated with formal\nthought disorder than their larger counterparts. Detection capability declines\nbeyond a certain model size and context length, challenging the common\nassumption of ``bigger is better'' for LLM-based applications. Our findings\ngeneralize across audio diaries and clinical interview speech samples from\nindividuals with psychotic symptoms, suggesting a promising direction for\ndeveloping efficient, cost-effective, and privacy-preserving screening tools\nthat can be deployed in both clinical and naturalistic settings.", "published": "2025-03-25 22:55:58", "link": "http://arxiv.org/abs/2503.20103v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generative Linguistics, Large Language Models, and the Social Nature of Scientific Success", "abstract": "Chesi's (forthcoming) target paper depicts a generative linguistics in\ncrisis, foreboded by Piantadosi's (2023) declaration that \"modern language\nmodels refute Chomsky's approach to language.\" In order to survive, Chesi\nwarns, generativists must hold themselves to higher standards of formal and\nempirical rigor. This response argues that the crisis described by Chesi and\nPiantadosi actually has little to do with rigor, but is rather a reflection of\ngenerativists' limited social ambitions. Chesi ties the fate of generative\nlinguistics to its intellectual merits, but the current success of language\nmodel research is social in nature as much as it is intellectual. In order to\nthrive, then, generativists must do more than heed Chesi's call for rigor; they\nmust also expand their ambitions by giving outsiders a stake in their future\nsuccess.", "published": "2025-03-25 21:57:35", "link": "http://arxiv.org/abs/2503.20088v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-Tokenizer Distillation via Approximate Likelihood Matching", "abstract": "Distillation has shown remarkable success in transferring knowledge from a\nLarge Language Model (LLM) teacher to a student LLM. However, current\ndistillation methods predominantly require the same tokenizer between the\nteacher and the student, restricting their applicability to only a small subset\nof teacher-student pairs. In this work, we develop a cross-tokenizer\ndistillation method to solve this crucial deficiency. Our method is the first\nto enable cross-tokenizer distillation without a next-token prediction loss as\nthe main objective, instead purely maximizing the student predictions'\nsimilarity to the teacher's predictions (known as pure distillation), while\nalso being robust to large mismatches between the teacher and the student\ntokenizer function and vocabulary. Empirically, our method enables\nsubstantially improved performance as tested on two use cases. First, we show\nthat viewing tokenizer transfer as self-distillation enables unprecedently\neffective transfer across tokenizers. We transfer (subword-level) Llama and\nGemma models to byte-level tokenization more effectively than prior methods\ntransfer to a similar subword tokenizer under a comparable training budget.\nTransferring different base models to the same tokenizer also enables\nensembling them (e.g., via averaging their predicted probabilities) which\nboosts performance. Second, we use our cross-tokenizer distillation method to\ndistil a large maths-specialized LLM into a smaller model, achieving\ncompetitive maths problem-solving performance. Overall, our results make\nsubstantial strides toward better adaptability and enhanced interaction between\ndifferent LLMs.", "published": "2025-03-25 21:44:10", "link": "http://arxiv.org/abs/2503.20083v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Poor Alignment and Steerability of Large Language Models: Evidence from College Admission Essays", "abstract": "People are increasingly using technologies equipped with large language\nmodels (LLM) to write texts for formal communication, which raises two\nimportant questions at the intersection of technology and society: Who do LLMs\nwrite like (model alignment); and can LLMs be prompted to change who they write\nlike (model steerability). We investigate these questions in the high-stakes\ncontext of undergraduate admissions at a selective university by comparing\nlexical and sentence variation between essays written by 30,000 applicants to\ntwo types of LLM-generated essays: one prompted with only the essay question\nused by the human applicants; and another with additional demographic\ninformation about each applicant. We consistently find that both types of\nLLM-generated essays are linguistically distinct from human-authored essays,\nregardless of the specific model and analytical approach. Further, prompting a\nspecific sociodemographic identity is remarkably ineffective in aligning the\nmodel with the linguistic patterns observed in human writing from this identity\ngroup. This holds along the key dimensions of sex, race, first-generation\nstatus, and geographic location. The demographically prompted and unprompted\nsynthetic texts were also more similar to each other than to the human text,\nmeaning that prompting did not alleviate homogenization. These issues of model\nalignment and steerability in current LLMs raise concerns about the use of LLMs\nin high-stakes contexts.", "published": "2025-03-25 20:54:50", "link": "http://arxiv.org/abs/2503.20062v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Low-resource Machine Translation for Code-switched Kazakh-Russian Language Pair", "abstract": "Machine translation for low resource language pairs is a challenging task.\nThis task could become extremely difficult once a speaker uses code switching.\nWe propose a method to build a machine translation model for code-switched\nKazakh-Russian language pair with no labeled data. Our method is basing on\ngeneration of synthetic data. Additionally, we present the first codeswitching\nKazakh-Russian parallel corpus and the evaluation results, which include a\nmodel achieving 16.48 BLEU almost reaching an existing commercial system and\nbeating it by human evaluation.", "published": "2025-03-25 18:46:30", "link": "http://arxiv.org/abs/2503.20007v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "OAEI-LLM-T: A TBox Benchmark Dataset for Understanding LLM Hallucinations in Ontology Matching Systems", "abstract": "Hallucinations are inevitable in downstream tasks using large language models\n(LLMs). While addressing hallucinations becomes a substantial challenge for\nLLM-based ontology matching (OM) systems, we introduce a new benchmark dataset\ncalled OAEI-LLM-T. The dataset evolves from the TBox (i.e. schema-matching)\ndatasets in the Ontology Alignment Evaluation Initiative (OAEI), capturing\nhallucinations of different LLMs performing OM tasks. These OM-specific\nhallucinations are carefully classified into two primary categories and six\nsub-categories. We showcase the usefulness of the dataset in constructing the\nLLM leaderboard and fine-tuning foundational LLMs for LLM-based OM systems.", "published": "2025-03-25 18:20:04", "link": "http://arxiv.org/abs/2503.21813v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Untangling the Influence of Typology, Data and Model Architecture on Ranking Transfer Languages for Cross-Lingual POS Tagging", "abstract": "Cross-lingual transfer learning is an invaluable tool for overcoming data\nscarcity, yet selecting a suitable transfer language remains a challenge. The\nprecise roles of linguistic typology, training data, and model architecture in\ntransfer language choice are not fully understood. We take a holistic approach,\nexamining how both dataset-specific and fine-grained typological features\ninfluence transfer language selection for part-of-speech tagging, considering\ntwo different sources for morphosyntactic features. While previous work\nexamines these dynamics in the context of bilingual biLSTMS, we extend our\nanalysis to a more modern transfer learning pipeline: zero-shot prediction with\npretrained multilingual models. We train a series of transfer language ranking\nsystems and examine how different feature inputs influence ranker performance\nacross architectures. Word overlap, type-token ratio, and genealogical distance\nemerge as top features across all architectures. Our findings reveal that a\ncombination of typological and dataset-dependent features leads to the best\nrankings, and that good performance can be obtained with either feature group\non its own.", "published": "2025-03-25 18:05:40", "link": "http://arxiv.org/abs/2503.19979v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CAFe: Unifying Representation and Generation with Contrastive-Autoregressive Finetuning", "abstract": "The rapid advancement of large vision-language models (LVLMs) has driven\nsignificant progress in multimodal tasks, enabling models to interpret, reason,\nand generate outputs across both visual and textual domains. While excelling in\ngenerative tasks, existing LVLMs often face limitations in tasks requiring\nhigh-fidelity representation learning, such as generating image or text\nembeddings for retrieval. Recent work has proposed finetuning LVLMs for\nrepresentational learning, but the fine-tuned model often loses its generative\ncapabilities due to the representational learning training paradigm. To address\nthis trade-off, we introduce CAFe, a contrastive-autoregressive fine-tuning\nframework that enhances LVLMs for both representation and generative tasks. By\nintegrating a contrastive objective with autoregressive language modeling, our\napproach unifies these traditionally separate tasks, achieving state-of-the-art\nresults in both multimodal retrieval and multimodal generative benchmarks,\nincluding object hallucination (OH) mitigation. CAFe establishes a novel\nframework that synergizes embedding and generative functionalities in a single\nmodel, setting a foundation for future multimodal models that excel in both\nretrieval precision and coherent output generation.", "published": "2025-03-25 17:57:17", "link": "http://arxiv.org/abs/2503.19900v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "CausalRAG: Integrating Causal Graphs into Retrieval-Augmented Generation", "abstract": "Large language models (LLMs) have revolutionized natural language processing\n(NLP), particularly through Retrieval-Augmented Generation (RAG), which\nenhances LLM capabilities by integrating external knowledge. However,\ntraditional RAG systems face critical limitations, including disrupted\ncontextual integrity due to text chunking, and over-reliance on semantic\nsimilarity for retrieval. To address these issues, we propose CausalRAG, a\nnovel framework that incorporates causal graphs into the retrieval process. By\nconstructing and tracing causal relationships, CausalRAG preserves contextual\ncontinuity and improves retrieval precision, leading to more accurate and\ninterpretable responses. We evaluate CausalRAG against regular RAG and\ngraph-based RAG approaches, demonstrating its superiority across several\nmetrics. Our findings suggest that grounding retrieval in causal reasoning\nprovides a promising approach to knowledge-intensive tasks.", "published": "2025-03-25 17:43:08", "link": "http://arxiv.org/abs/2503.19878v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Scaling Evaluation-time Compute with Reasoning Models as Process Evaluators", "abstract": "As language model (LM) outputs get more and more natural, it is becoming more\ndifficult than ever to evaluate their quality. Simultaneously, increasing LMs'\n\"thinking\" time through scaling test-time compute has proven an effective\ntechnique to solve challenging problems in domains such as math and code. This\nraises a natural question: can an LM's evaluation capability also be improved\nby spending more test-time compute? To answer this, we investigate employing\nreasoning models-LMs that natively generate long chain-of-thought reasoning-as\nevaluators. Specifically, we examine methods to leverage more test-time compute\nby (1) using reasoning models, and (2) prompting these models to evaluate not\nonly the response as a whole (i.e., outcome evaluation) but also assess each\nstep in the response separately (i.e., process evaluation). In experiments, we\nobserve that the evaluator's performance improves monotonically when generating\nmore reasoning tokens, similar to the trends observed in LM-based generation.\nFurthermore, we use these more accurate evaluators to rerank multiple\ngenerations, and demonstrate that spending more compute at evaluation time can\nbe as effective as using more compute at generation time in improving an LM's\nproblem-solving capability.", "published": "2025-03-25 17:41:18", "link": "http://arxiv.org/abs/2503.19877v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Think Twice: Enhancing LLM Reasoning by Scaling Multi-round Test-time Thinking", "abstract": "Recent advances in large language models (LLMs), such as OpenAI-o1 and\nDeepSeek-R1, have demonstrated the effectiveness of test-time scaling, where\nextended reasoning processes substantially enhance model performance. Despite\nthis, current models are constrained by limitations in handling long texts and\nreinforcement learning (RL) training efficiency. To address these issues, we\npropose a simple yet effective test-time scaling approach Multi-round Thinking.\nThis method iteratively refines model reasoning by leveraging previous answers\nas prompts for subsequent rounds. Extensive experiments across multiple models,\nincluding QwQ-32B and DeepSeek-R1, consistently show performance improvements\non various benchmarks such as AIME 2024, MATH-500, GPQA-diamond, and\nLiveCodeBench. For instance, the accuracy of QwQ-32B improved from 80.3% (Round\n1) to 82.1% (Round 2) on the AIME 2024 dataset, while DeepSeek-R1 showed a\nsimilar increase from 79.7% to 82.0%. These results confirm that Multi-round\nThinking is a broadly applicable, straightforward approach to achieving stable\nenhancements in model performance, underscoring its potential for future\ndevelopments in test-time scaling techniques. The key prompt: {Original\nquestion prompt} The assistant's previous answer is: <answer> {last round\nanswer} </answer>, and please re-answer.", "published": "2025-03-25 17:19:38", "link": "http://arxiv.org/abs/2503.19855v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Comparative Analysis of Word Segmentation, Part-of-Speech Tagging, and Named Entity Recognition for Historical Chinese Sources, 1900-1950", "abstract": "This paper compares large language models (LLMs) and traditional natural\nlanguage processing (NLP) tools for performing word segmentation,\npart-of-speech (POS) tagging, and named entity recognition (NER) on Chinese\ntexts from 1900 to 1950. Historical Chinese documents pose challenges for text\nanalysis due to their logographic script, the absence of natural word\nboundaries, and significant linguistic changes. Using a sample dataset from the\nShanghai Library Republican Journal corpus, traditional tools such as Jieba and\nspaCy are compared to LLMs, including GPT-4o, Claude 3.5, and the GLM series.\nThe results show that LLMs outperform traditional methods in all metrics,\nalbeit at considerably higher computational costs, highlighting a trade-off\nbetween accuracy and efficiency. Additionally, LLMs better handle\ngenre-specific challenges such as poetry and temporal variations (i.e.,\npre-1920 versus post-1920 texts), demonstrating that their contextual learning\ncapabilities can advance NLP approaches to historical texts by reducing the\nneed for domain-specific training data.", "published": "2025-03-25 17:07:21", "link": "http://arxiv.org/abs/2503.19844v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Contextual Metric Meta-Evaluation by Measuring Local Metric Accuracy", "abstract": "Meta-evaluation of automatic evaluation metrics -- assessing evaluation\nmetrics themselves -- is crucial for accurately benchmarking natural language\nprocessing systems and has implications for scientific inquiry, production\nmodel development, and policy enforcement. While existing approaches to metric\nmeta-evaluation focus on general statements about the absolute and relative\nquality of metrics across arbitrary system outputs, in practice, metrics are\napplied in highly contextual settings, often measuring the performance for a\nhighly constrained set of system outputs. For example, we may only be\ninterested in evaluating a specific model or class of models. We introduce a\nmethod for contextual metric meta-evaluation by comparing the local metric\naccuracy of evaluation metrics. Across translation, speech recognition, and\nranking tasks, we demonstrate that the local metric accuracies vary both in\nabsolute value and relative effectiveness as we shift across evaluation\ncontexts. This observed variation highlights the importance of adopting\ncontext-specific metric evaluations over global ones.", "published": "2025-03-25 16:42:25", "link": "http://arxiv.org/abs/2503.19828v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Taxonomy Inference for Tabular Data Using Large Language Models", "abstract": "Taxonomy inference for tabular data is a critical task of schema inference,\naiming at discovering entity types (i.e., concepts) of the tables and building\ntheir hierarchy. It can play an important role in data management, data\nexploration, ontology learning, and many data-centric applications. Existing\nschema inference systems focus more on XML, JSON or RDF data, and often rely on\nlexical formats and structures of the data for calculating similarities, with\nlimited exploitation of the semantics of the text across a table. Motivated by\nrecent works on taxonomy completion and construction using Large Language\nModels (LLMs), this paper presents two LLM-based methods for taxonomy inference\nfor tables: (i) EmTT which embeds columns by fine-tuning with contrastive\nlearning encoder-alone LLMs like BERT and utilises clustering for hierarchy\nconstruction, and (ii) GeTT which generates table entity types and their\nhierarchy by iterative prompting using a decoder-alone LLM like GPT-4.\nExtensive evaluation on three real-world datasets with six metrics covering\ndifferent aspects of the output taxonomies has demonstrated that EmTT and GeTT\ncan both produce taxonomies with strong consistency relative to the Ground\nTruth.", "published": "2025-03-25 16:26:05", "link": "http://arxiv.org/abs/2503.21810v1", "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.DB"}
{"title": "LogQuant: Log-Distributed 2-Bit Quantization of KV Cache with Superior Accuracy Preservation", "abstract": "We introduce LogQuant, a groundbreaking 2-bit quantization technique for KV\nCache in large language model (LLM) inference, delivering substantial memory\nsavings while preserving superior performance. Previous methods either assume\nthat later tokens are more important or attempt to predict important tokens\nbased on earlier attention patterns. Both approaches, however, can result in\nperformance bottlenecks or frequent mispredictions.\n  LogQuant takes a different approach. By applying a log-based filtering\nmechanism, it selectively compresses the KV Cache across the entire context,\nachieving better performance with the same or even reduced memory footprint\ncompared to existing methods. In benchmark tests, it enhances throughput by 25%\nand boosts batch size by 60% without increasing memory consumption. For\nchallenging tasks such as Math and Code Completion, LogQuant improves accuracy\nby 40% to 200% at the same compression ratio, outperforming comparable\ntechniques.LogQuant integrates effortlessly with popular inference frameworks\nlike Python's transformers library. Implementation can be available in\nhttps://github.com/Concyclics/LogQuantKV.", "published": "2025-03-25 16:24:45", "link": "http://arxiv.org/abs/2503.19950v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "SemEval-2025 Task 9: The Food Hazard Detection Challenge", "abstract": "In this challenge, we explored text-based food hazard prediction with long\ntail distributed classes. The task was divided into two subtasks: (1)\npredicting whether a web text implies one of ten food-hazard categories and\nidentifying the associated food category, and (2) providing a more fine-grained\nclassification by assigning a specific label to both the hazard and the\nproduct. Our findings highlight that large language model-generated synthetic\ndata can be highly effective for oversampling long-tail distributions.\nFurthermore, we find that fine-tuned encoder-only, encoder-decoder, and\ndecoder-only systems achieve comparable maximum performance across both\nsubtasks. During this challenge, we gradually released (under CC BY-NC-SA 4.0)\na novel set of 6,644 manually labeled food-incident reports.", "published": "2025-03-25 16:09:14", "link": "http://arxiv.org/abs/2503.19800v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Gemma 3 Technical Report", "abstract": "We introduce Gemma 3, a multimodal addition to the Gemma family of\nlightweight open models, ranging in scale from 1 to 27 billion parameters. This\nversion introduces vision understanding abilities, a wider coverage of\nlanguages and longer context - at least 128K tokens. We also change the\narchitecture of the model to reduce the KV-cache memory that tends to explode\nwith long context. This is achieved by increasing the ratio of local to global\nattention layers, and keeping the span on local attention short. The Gemma 3\nmodels are trained with distillation and achieve superior performance to Gemma\n2 for both pre-trained and instruction finetuned versions. In particular, our\nnovel post-training recipe significantly improves the math, chat,\ninstruction-following and multilingual abilities, making Gemma3-4B-IT\ncompetitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro\nacross benchmarks. We release all our models to the community.", "published": "2025-03-25 15:52:34", "link": "http://arxiv.org/abs/2503.19786v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mapping Technological Futures: Anticipatory Discourse Through Text Mining", "abstract": "The volatility and unpredictability of emerging technologies, such as\nartificial intelligence (AI), generate significant uncertainty, which is widely\ndiscussed on social media. This study examines anticipatory discourse\nsurrounding technological futures by analysing 1.5 million posts from 400 key\nopinion leaders (KOLs) published on the X platform (from 2021 to 2023). Using\nadvanced text mining techniques, including BERTopic modelling, sentiment,\nemotion, and attitude analyses, the research identifies 100 distinct topics\nreflecting anticipated tech-driven futures. Our findings emphasize the dual\nrole of KOLs in framing \\textit{present futures} -- optimistic visions of\ntransformative technologies like AI and IoT -- and influencing \\textit{future\npresents}, where these projections shape contemporary societal and geopolitical\ndebates. Positive emotions such as Hope dominate, outweighing Anxiety,\nparticularly in topics like ``Machine Learning, Data Science, and Deep\nLearning,'' while discussions around ``Climate Change'' and ``War, Ukraine, and\nTrump People'' elicit \\textit{Anxiety}. By framing technologies as solutions to\nsocietal challenges, KOLs act as mediators of societal narratives, bridging\nimagined futures and current realities. These insights underscore their pivotal\nrole in directing public attention with emerging technologies during periods of\nheightened uncertainty, advancing our understanding of anticipatory discourse\nin technology-mediated contexts.", "published": "2025-03-25 15:20:15", "link": "http://arxiv.org/abs/2504.02853v1", "categories": ["cs.SI", "cs.CL", "cs.CY", "K.4; H.3.3"], "primary_category": "cs.SI"}
{"title": "Writing as a testbed for open ended agents", "abstract": "Open-ended tasks are particularly challenging for LLMs due to the vast\nsolution space, demanding both expansive exploration and adaptable strategies,\nespecially when success lacks a clear, objective definition. Writing, with its\nvast solution space and subjective evaluation criteria, provides a compelling\ntestbed for studying such problems. In this paper, we investigate the potential\nof LLMs to act as collaborative co-writers, capable of suggesting and\nimplementing text improvements autonomously. We analyse three prominent LLMs -\nGemini 1.5 Pro, Claude 3.5 Sonnet, and GPT-4o - focusing on how their action\ndiversity, human alignment, and iterative improvement capabilities impact\noverall performance. This work establishes a framework for benchmarking\nautonomous writing agents and, more broadly, highlights fundamental challenges\nand potential solutions for building systems capable of excelling in diverse\nopen-ended domains.", "published": "2025-03-25 14:38:36", "link": "http://arxiv.org/abs/2503.19711v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Mind the Gap: Benchmarking Spatial Reasoning in Vision-Language Models", "abstract": "Vision-Language Models (VLMs) have recently emerged as powerful tools,\nexcelling in tasks that integrate visual and textual comprehension, such as\nimage captioning, visual question answering, and image-text retrieval. However,\nexisting benchmarks for VLMs include spatial components, which often fail to\nisolate spatial reasoning from related tasks such as object detection or\nsemantic comprehension. In this paper, we address these deficiencies with a\nmulti-faceted approach towards understanding spatial reasoning. Informed by the\ndiverse and multi-dimensional nature of human spatial reasoning abilities, we\npresent a detailed analysis that first delineates the core elements of spatial\nreasoning: spatial relations, orientation and navigation, mental rotation, and\nspatial visualization, and then assesses the performance of these models in\nboth synthetic and real-world images, bridging controlled and naturalistic\ncontexts. We analyze 13 state-of-the-art Vision-Language Models, uncovering\npivotal insights into their spatial reasoning performance. Our results reveal\nprofound shortcomings in current VLMs, with average accuracy across the 13\nmodels approximating random chance, highlighting spatial reasoning as a\npersistent obstacle. This work not only exposes the pressing need to advance\nspatial reasoning within VLMs but also establishes a solid platform for future\nexploration. Code available on GitHub (https://github.com/stogiannidis/srbench)\nand dataset available on HuggingFace\n(https://huggingface.co/datasets/stogiannidis/srbench).", "published": "2025-03-25 14:34:06", "link": "http://arxiv.org/abs/2503.19707v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "HausaNLP at SemEval-2025 Task 2: Entity-Aware Fine-tuning vs. Prompt Engineering in Entity-Aware Machine Translation", "abstract": "This paper presents our findings for SemEval 2025 Task 2, a shared task on\nentity-aware machine translation (EA-MT). The goal of this task is to develop\ntranslation models that can accurately translate English sentences into target\nlanguages, with a particular focus on handling named entities, which often pose\nchallenges for MT systems. The task covers 10 target languages with English as\nthe source. In this paper, we describe the different systems we employed,\ndetail our results, and discuss insights gained from our experiments.", "published": "2025-03-25 14:29:43", "link": "http://arxiv.org/abs/2503.19702v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AdaptiVocab: Enhancing LLM Efficiency in Focused Domains through Lightweight Vocabulary Adaptation", "abstract": "Large Language Models (LLMs) have shown impressive versatility as general\npurpose models. However, their broad applicability comes at a high-cost\ncomputational overhead, particularly in auto-regressive decoding where each\nstep requires a forward pass. In domain-specific settings, general-purpose\ncapabilities are unnecessary and can be exchanged for efficiency. In this work,\nwe take a novel perspective on domain adaptation, reducing latency and\ncomputational costs by adapting the vocabulary to focused domains of interest.\nWe introduce AdaptiVocab, an end-to-end approach for vocabulary adaptation,\ndesigned to enhance LLM efficiency in low-resource domains. AdaptiVocab can be\napplied to any tokenizer and architecture, modifying the vocabulary by\nreplacing tokens with domain-specific n-gram-based tokens, thereby reducing the\nnumber of tokens required for both input processing and output generation.\nAdaptiVocab initializes new n-token embeddings using an exponentially weighted\ncombination of existing embeddings and employs a lightweight fine-tuning phase\nthat can be efficiently performed on a single GPU. We evaluate two 7B LLMs\nacross three niche domains, assessing efficiency, generation quality, and\nend-task performance. Our results show that AdaptiVocab reduces token usage by\nover 25% without compromising performance", "published": "2025-03-25 14:18:21", "link": "http://arxiv.org/abs/2503.19693v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A multitask transformer to sign language translation using motion gesture primitives", "abstract": "The absence of effective communication the deaf population represents the\nmain social gap in this community. Furthermore, the sign language, main deaf\ncommunication tool, is unlettered, i.e., there is no formal written\nrepresentation. In consequence, main challenge today is the automatic\ntranslation among spatiotemporal sign representation and natural text language.\nRecent approaches are based on encoder-decoder architectures, where the most\nrelevant strategies integrate attention modules to enhance non-linear\ncorrespondences, besides, many of these approximations require complex training\nand architectural schemes to achieve reasonable predictions, because of the\nabsence of intermediate text projections. However, they are still limited by\nthe redundant background information of the video sequences. This work\nintroduces a multitask transformer architecture that includes a gloss learning\nrepresentation to achieve a more suitable translation. The proposed approach\nalso includes a dense motion representation that enhances gestures and includes\nkinematic information, a key component in sign language. From this\nrepresentation it is possible to avoid background information and exploit the\ngeometry of the signs, in addition, it includes spatiotemporal representations\nthat facilitate the alignment between gestures and glosses as an intermediate\ntextual representation. The proposed approach outperforms the state-of-the-art\nevaluated on the CoL-SLTD dataset, achieving a BLEU-4 of 72,64% in split 1, and\na BLEU-4 of 14,64% in split 2. Additionally, the strategy was validated on the\nRWTH-PHOENIX-Weather 2014 T dataset, achieving a competitive BLEU-4 of 11,58%.", "published": "2025-03-25 13:53:25", "link": "http://arxiv.org/abs/2503.19668v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HausaNLP at SemEval-2025 Task 3: Towards a Fine-Grained Model-Aware Hallucination Detection", "abstract": "This paper presents our findings of the Multilingual Shared Task on\nHallucinations and Related Observable Overgeneration Mistakes, MU-SHROOM, which\nfocuses on identifying hallucinations and related overgeneration errors in\nlarge language models (LLMs). The shared task involves detecting specific text\nspans that constitute hallucinations in the outputs generated by LLMs in 14\nlanguages. To address this task, we aim to provide a nuanced, model-aware\nunderstanding of hallucination occurrences and severity in English. We used\nnatural language inference and fine-tuned a ModernBERT model using a synthetic\ndataset of 400 samples, achieving an Intersection over Union (IoU) score of\n0.032 and a correlation score of 0.422. These results indicate a moderately\npositive correlation between the model's confidence scores and the actual\npresence of hallucinations. The IoU score indicates that our model has a\nrelatively low overlap between the predicted hallucination span and the truth\nannotation. The performance is unsurprising, given the intricate nature of\nhallucination detection. Hallucinations often manifest subtly, relying on\ncontext, making pinpointing their exact boundaries formidable.", "published": "2025-03-25 13:40:22", "link": "http://arxiv.org/abs/2503.19650v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring Cultural Nuances in Emotion Perception Across 15 African Languages", "abstract": "Understanding how emotions are expressed across languages is vital for\nbuilding culturally-aware and inclusive NLP systems. However, emotion\nexpression in African languages is understudied, limiting the development of\neffective emotion detection tools in these languages. In this work, we present\na cross-linguistic analysis of emotion expression in 15 African languages. We\nexamine four key dimensions of emotion representation: text length, sentiment\npolarity, emotion co-occurrence, and intensity variations. Our findings reveal\ndiverse language-specific patterns in emotional expression -- with Somali texts\ntypically longer, while others like IsiZulu and Algerian Arabic show more\nconcise emotional expression. We observe a higher prevalence of negative\nsentiment in several Nigerian languages compared to lower negativity in\nlanguages like IsiXhosa. Further, emotion co-occurrence analysis demonstrates\nstrong cross-linguistic associations between specific emotion pairs\n(anger-disgust, sadness-fear), suggesting universal psychological connections.\nIntensity distributions show multimodal patterns with significant variations\nbetween language families; Bantu languages display similar yet distinct\nprofiles, while Afroasiatic languages and Nigerian Pidgin demonstrate wider\nintensity ranges. These findings highlight the need for language-specific\napproaches to emotion detection while identifying opportunities for transfer\nlearning across related languages.", "published": "2025-03-25 13:30:03", "link": "http://arxiv.org/abs/2503.19642v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "1.4 Million Open-Source Distilled Reasoning Dataset to Empower Large Language Model Training", "abstract": "The AM-DeepSeek-R1-Distilled is a large-scale dataset with thinking traces\nfor general reasoning tasks, composed of high-quality and challenging reasoning\nproblems. These problems are collected from a multitude of open-source\ndatasets, subjected to semantic deduplication and meticulous cleaning to\neliminate test set contamination. All responses within the dataset are\ndistilled from reasoning models (predominantly DeepSeek-R1) and have undergone\nrigorous verification procedures. Mathematical problems are validated by\nchecking against reference answers, code problems are verified using test\ncases, and other tasks are evaluated with the aid of a reward model. The\nAM-Distill-Qwen-32B model, which was trained through only simple Supervised\nFine-Tuning (SFT) using this batch of data, outperformed the\nDeepSeek-R1-Distill-Qwen-32B model on four benchmarks: AIME2024, MATH-500,\nGPQA-Diamond, and LiveCodeBench. Additionally, the AM-Distill-Qwen-72B model\nsurpassed the DeepSeek-R1-Distill-Llama-70B model on all benchmarks as well. We\nare releasing these 1.4 million problems and their corresponding responses to\nthe research community with the objective of fostering the development of\npowerful reasoning-oriented Large Language Models (LLMs). The dataset was\npublished in\n\\href{https://huggingface.co/datasets/a-m-team/AM-DeepSeek-R1-Distilled-1.4M}{https://huggingface.co/datasets/a-m-team/AM-DeepSeek-R1-Distilled-1.4M}.", "published": "2025-03-25 13:19:46", "link": "http://arxiv.org/abs/2503.19633v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lean Formalization of Generalization Error Bound by Rademacher Complexity", "abstract": "We formalize the generalization error bound using Rademacher complexity in\nthe Lean 4 theorem prover. Generalization error quantifies the gap between a\nlearning machine's performance on given training data versus unseen test data,\nand Rademacher complexity serves as an estimate of this error based on the\ncomplexity of learning machines, or hypothesis class. Unlike traditional\nmethods such as PAC learning and VC dimension, Rademacher complexity is\napplicable across diverse machine learning scenarios including deep learning\nand kernel methods. We formalize key concepts and theorems, including the\nempirical and population Rademacher complexities, and establish generalization\nerror bounds through formal proofs of McDiarmid's inequality, Hoeffding's\nlemma, and symmetrization arguments.", "published": "2025-03-25 12:40:43", "link": "http://arxiv.org/abs/2503.19605v2", "categories": ["cs.LG", "cs.CL", "math.ST", "stat.TH"], "primary_category": "cs.LG"}
{"title": "The Greatest Good Benchmark: Measuring LLMs' Alignment with Utilitarian Moral Dilemmas", "abstract": "The question of how to make decisions that maximise the well-being of all\npersons is very relevant to design language models that are beneficial to\nhumanity and free from harm. We introduce the Greatest Good Benchmark to\nevaluate the moral judgments of LLMs using utilitarian dilemmas. Our analysis\nacross 15 diverse LLMs reveals consistently encoded moral preferences that\ndiverge from established moral theories and lay population moral standards.\nMost LLMs have a marked preference for impartial beneficence and rejection of\ninstrumental harm. These findings showcase the 'artificial moral compass' of\nLLMs, offering insights into their moral alignment.", "published": "2025-03-25 12:29:53", "link": "http://arxiv.org/abs/2503.19598v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Distinct social-linguistic processing between humans and large audio-language models: Evidence from model-brain alignment", "abstract": "Voice-based AI development faces unique challenges in processing both\nlinguistic and paralinguistic information. This study compares how large\naudio-language models (LALMs) and humans integrate speaker characteristics\nduring speech comprehension, asking whether LALMs process\nspeaker-contextualized language in ways that parallel human cognitive\nmechanisms. We compared two LALMs' (Qwen2-Audio and Ultravox 0.5) processing\npatterns with human EEG responses. Using surprisal and entropy metrics from the\nmodels, we analyzed their sensitivity to speaker-content incongruency across\nsocial stereotype violations (e.g., a man claiming to regularly get manicures)\nand biological knowledge violations (e.g., a man claiming to be pregnant).\nResults revealed that Qwen2-Audio exhibited increased surprisal for\nspeaker-incongruent content and its surprisal values significantly predicted\nhuman N400 responses, while Ultravox 0.5 showed limited sensitivity to speaker\ncharacteristics. Importantly, neither model replicated the human-like\nprocessing distinction between social violations (eliciting N400 effects) and\nbiological violations (eliciting P600 effects). These findings reveal both the\npotential and limitations of current LALMs in processing speaker-contextualized\nlanguage, and suggest differences in social-linguistic processing mechanisms\nbetween humans and LALMs.", "published": "2025-03-25 12:10:47", "link": "http://arxiv.org/abs/2503.19586v1", "categories": ["cs.CL", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "Multi-agent Application System in Office Collaboration Scenarios", "abstract": "This paper introduces a multi-agent application system designed to enhance\noffice collaboration efficiency and work quality. The system integrates\nartificial intelligence, machine learning, and natural language processing\ntechnologies, achieving functionalities such as task allocation, progress\nmonitoring, and information sharing. The agents within the system are capable\nof providing personalized collaboration support based on team members' needs\nand incorporate data analysis tools to improve decision-making quality. The\npaper also proposes an intelligent agent architecture that separates Plan and\nSolver, and through techniques such as multi-turn query rewriting and business\ntool retrieval, it enhances the agent's multi-intent and multi-turn dialogue\ncapabilities. Furthermore, the paper details the design of tools and multi-turn\ndialogue in the context of office collaboration scenarios, and validates the\nsystem's effectiveness through experiments and evaluations. Ultimately, the\nsystem has demonstrated outstanding performance in real business applications,\nparticularly in query understanding, task planning, and tool calling. Looking\nforward, the system is expected to play a more significant role in addressing\ncomplex interaction issues within dynamic environments and large-scale\nmulti-agent systems.", "published": "2025-03-25 12:07:20", "link": "http://arxiv.org/abs/2503.19584v3", "categories": ["cs.AI", "cs.CL", "cs.SE"], "primary_category": "cs.AI"}
{"title": "Context-Efficient Retrieval with Factual Decomposition", "abstract": "There has recently been considerable interest in incorporating information\nretrieval into large language models (LLMs). Retrieval from a dynamically\nexpanding external corpus of text allows a model to incorporate current events\nand can be viewed as a form of episodic memory. Here we demonstrate that\npre-processing the external corpus into semi-structured ''atomic facts'' makes\nretrieval more efficient. More specifically, we demonstrate that our particular\nform of atomic facts improves performance on various question answering tasks\nwhen the amount of retrieved text is limited. Limiting the amount of retrieval\nreduces the size of the context and improves inference efficiency.", "published": "2025-03-25 11:48:22", "link": "http://arxiv.org/abs/2503.19574v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Scaling Laws of Synthetic Data for Language Models", "abstract": "Large language models (LLMs) achieve strong performance across diverse tasks,\nlargely driven by high-quality web data used in pre-training. However, recent\nstudies indicate this data source is rapidly depleting. Synthetic data emerges\nas a promising alternative, but it remains unclear whether synthetic datasets\nexhibit predictable scalability comparable to raw pre-training data. In this\nwork, we systematically investigate the scaling laws of synthetic data by\nintroducing SynthLLM, a scalable framework that transforms pre-training corpora\ninto diverse, high-quality synthetic datasets. Our approach achieves this by\nautomatically extracting and recombining high-level concepts across multiple\ndocuments using a graph algorithm. Key findings from our extensive mathematical\nexperiments on SynthLLM include: (1) SynthLLM generates synthetic data that\nreliably adheres to the rectified scaling law across various model sizes; (2)\nPerformance improvements plateau near 300B tokens; and (3) Larger models\napproach optimal performance with fewer training tokens. For instance, an 8B\nmodel peaks at 1T tokens, while a 3B model requires 4T. Moreover, comparisons\nwith existing synthetic data generation and augmentation methods demonstrate\nthat SynthLLM achieves superior performance and scalability. Our findings\nhighlight synthetic data as a scalable and reliable alternative to organic\npre-training corpora, offering a viable path toward continued improvement in\nmodel performance.", "published": "2025-03-25 11:07:12", "link": "http://arxiv.org/abs/2503.19551v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FLEX: A Benchmark for Evaluating Robustness of Fairness in Large Language Models", "abstract": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced interactions between users and models. These advancements concurrently\nunderscore the need for rigorous safety evaluations due to the manifestation of\nsocial biases, which can lead to harmful societal impacts. Despite these\nconcerns, existing benchmarks may overlook the intrinsic weaknesses of LLMs,\nwhich can generate biased responses even with simple adversarial instructions.\nTo address this critical gap, we introduce a new benchmark, Fairness Benchmark\nin LLM under Extreme Scenarios (FLEX), designed to test whether LLMs can\nsustain fairness even when exposed to prompts constructed to induce bias. To\nthoroughly evaluate the robustness of LLMs, we integrate prompts that amplify\npotential biases into the fairness assessment. Comparative experiments between\nFLEX and existing benchmarks demonstrate that traditional evaluations may\nunderestimate the inherent risks in models. This highlights the need for more\nstringent LLM evaluation benchmarks to guarantee safety and fairness.", "published": "2025-03-25 10:48:33", "link": "http://arxiv.org/abs/2503.19540v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DomainCQA: Crafting Expert-Level QA from Domain-Specific Charts", "abstract": "Chart Question Answering (CQA) benchmarks are essential for evaluating the\ncapability of Multimodal Large Language Models (MLLMs) to interpret visual\ndata. However, current benchmarks focus primarily on the evaluation of\ngeneral-purpose CQA but fail to adequately capture domain-specific challenges.\nWe introduce DomainCQA, a systematic methodology for constructing\ndomain-specific CQA benchmarks, and demonstrate its effectiveness by developing\nAstroChart, a CQA benchmark in the field of astronomy. Our evaluation shows\nthat chart reasoning and combining chart information with domain knowledge for\ndeeper analysis and summarization, rather than domain-specific knowledge, pose\nthe primary challenge for existing MLLMs, highlighting a critical gap in\ncurrent benchmarks. By providing a scalable and rigorous framework, DomainCQA\nenables more precise assessment and improvement of MLLMs for domain-specific\napplications.", "published": "2025-03-25 09:44:41", "link": "http://arxiv.org/abs/2503.19498v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KSHSeek: Data-Driven Approaches to Mitigating and Detecting Knowledge-Shortcut Hallucinations in Generative Models", "abstract": "The emergence of large language models (LLMs) has significantly advanced the\ndevelopment of natural language processing (NLP), especially in text generation\ntasks like question answering. However, model hallucinations remain a major\nchallenge in natural language generation (NLG) tasks due to their complex\ncauses. We systematically expand on the causes of factual hallucinations from\nthe perspective of knowledge shortcuts, analyzing hallucinations arising from\ncorrect and defect-free data and demonstrating that knowledge-shortcut\nhallucinations are prevalent in generative models. To mitigate this issue, we\npropose a high similarity pruning algorithm at the data preprocessing level to\nreduce spurious correlations in the data. Additionally, we design a specific\ndetection method for knowledge-shortcut hallucinations to evaluate the\neffectiveness of our mitigation strategy. Experimental results show that our\napproach effectively reduces knowledge-shortcut hallucinations, particularly in\nfine-tuning tasks, without negatively impacting model performance in question\nanswering. This work introduces a new paradigm for mitigating specific\nhallucination issues in generative models, enhancing their robustness and\nreliability in real-world applications.", "published": "2025-03-25 09:18:27", "link": "http://arxiv.org/abs/2503.19482v1", "categories": ["cs.CL", "I.2.7; I.2.6"], "primary_category": "cs.CL"}
{"title": "ReSearch: Learning to Reason with Search for LLMs via Reinforcement Learning", "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in reasoning,\nexemplified by the success of OpenAI-o1 and DeepSeek-R1. However, integrating\nreasoning with external search processes remains challenging, especially for\ncomplex multi-hop questions requiring multiple retrieval steps. We propose\nReSearch, a novel framework that trains LLMs to Reason with Search via\nreinforcement learning without using any supervised data on reasoning steps.\nOur approach treats search operations as integral components of the reasoning\nchain, where when and how to perform searches is guided by text-based thinking,\nand search results subsequently influence further reasoning. We train ReSearch\non Qwen2.5-7B(-Instruct) and Qwen2.5-32B(-Instruct) models and conduct\nextensive experiments. Despite being trained on only one dataset, our models\ndemonstrate strong generalizability across various benchmarks. Analysis reveals\nthat ReSearch naturally elicits advanced reasoning capabilities such as\nreflection and self-correction during the reinforcement learning process.", "published": "2025-03-25 09:00:58", "link": "http://arxiv.org/abs/2503.19470v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Enhancing Small Language Models for Cross-Lingual Generalized Zero-Shot Classification with Soft Prompt Tuning", "abstract": "In NLP, Zero-Shot Classification (ZSC) has become essential for enabling\nmodels to classify text into categories unseen during training, particularly in\nlow-resource languages and domains where labeled data is scarce. While\npretrained language models (PLMs) have shown promise in ZSC, they often rely on\nlarge training datasets or external knowledge, limiting their applicability in\nmultilingual and low-resource scenarios. Recent approaches leveraging natural\nlanguage prompts reduce the dependence on large training datasets but struggle\nto effectively incorporate available labeled data from related classification\ntasks, especially when these datasets originate from different languages or\ndistributions. Moreover, existing prompt-based methods typically rely on\nmanually crafted prompts in a specific language, limiting their adaptability\nand effectiveness in cross-lingual settings. To address these challenges, we\nintroduce RoSPrompt, a lightweight and data-efficient approach for training\nsoft prompts that enhance cross-lingual ZSC while ensuring robust\ngeneralization across data distribution shifts. RoSPrompt is designed for small\nmultilingual PLMs, enabling them to leverage high-resource languages to improve\nperformance in low-resource settings without requiring extensive fine-tuning or\nhigh computational costs. We evaluate our approach on multiple multilingual\nPLMs across datasets covering 106 languages, demonstrating strong cross-lingual\ntransfer performance and robust generalization capabilities over unseen\nclasses.", "published": "2025-03-25 09:00:25", "link": "http://arxiv.org/abs/2503.19469v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DeCAP: Context-Adaptive Prompt Generation for Debiasing Zero-shot Question Answering in Large Language Models", "abstract": "While Large Language Models (LLMs) excel in zero-shot Question Answering\n(QA), they tend to expose biases in their internal knowledge when faced with\nsocially sensitive questions, leading to a degradation in performance. Existing\nzero-shot methods are efficient but fail to consider context and prevent bias\npropagation in the answers. To address this, we propose DeCAP, a method for\ndebiasing LLMs using Context-Adaptive Prompt Generation. DeCAP leverages a\nQuestion Ambiguity Detection to take appropriate debiasing actions based on the\ncontext and a Neutral Answer Guidance Generation to suppress the LLMs make\nobjective judgments about the context, minimizing the propagation of bias from\ntheir internal knowledge. Our various experiments across eight LLMs show that\nDeCAP achieves state-of-the-art zero-shot debiased QA performance. This\ndemonstrates DeCAP's efficacy in enhancing the fairness and accuracy of LLMs in\ndiverse QA settings.", "published": "2025-03-25 08:16:35", "link": "http://arxiv.org/abs/2503.19426v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Large Language Models Meet Contrastive Learning: Zero-Shot Emotion Recognition Across Languages", "abstract": "Multilingual speech emotion recognition aims to estimate a speaker's\nemotional state using a contactless method across different languages. However,\nvariability in voice characteristics and linguistic diversity poses significant\nchallenges for zero-shot speech emotion recognition, especially with\nmultilingual datasets. In this paper, we propose leveraging contrastive\nlearning to refine multilingual speech features and extend large language\nmodels for zero-shot multilingual speech emotion estimation. Specifically, we\nemploy a novel two-stage training framework to align speech signals with\nlinguistic features in the emotional space, capturing both emotion-aware and\nlanguage-agnostic speech representations. To advance research in this field, we\nintroduce a large-scale synthetic multilingual speech emotion dataset, M5SER.\nOur experiments demonstrate the effectiveness of the proposed method in both\nspeech emotion recognition and zero-shot multilingual speech emotion\nrecognition, including previously unseen datasets and languages.", "published": "2025-03-25 05:58:18", "link": "http://arxiv.org/abs/2503.21806v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ImF: Implicit Fingerprint for Large Language Models", "abstract": "Training large language models (LLMs) is resource-intensive and expensive,\nmaking intellectual property (IP) protection essential. Most existing model\nfingerprint methods inject fingerprints into LLMs to protect model ownership.\nThese methods create fingerprint pairs with weak semantic correlations, lacking\nthe contextual coherence and semantic relatedness founded in normal\nquestion-answer (QA) pairs in LLMs. In this paper, we propose a Generation\nRevision Intervention (GRI) attack that can effectively exploit this flaw to\nerase fingerprints, highlighting the need for more secure model fingerprint\nmethods. Thus, we propose a novel injected fingerprint paradigm called Implicit\nFingerprints (ImF). ImF constructs fingerprint pairs with strong semantic\ncorrelations, disguising them as natural QA pairs within LLMs. This ensures the\nfingerprints are consistent with normal model behavior, making them\nindistinguishable and robust against detection and removal. Our experiment on\nmultiple LLMs demonstrates that ImF retains high verification success rates\nunder adversarial conditions, offering a reliable solution for protecting LLM\nownership.", "published": "2025-03-25 05:47:34", "link": "http://arxiv.org/abs/2503.21805v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "QUAD: Quantization and Parameter-Efficient Tuning of LLM with Activation Decomposition", "abstract": "Large Language Models (LLMs) excel in diverse applications but suffer\ninefficiency due to massive scale. While quantization reduces computational\ncosts, existing methods degrade accuracy in medium-sized LLMs (e.g.,\nLlama-3-8B) due to activation outliers. To address this, we propose QUAD\n(Quantization with Activation Decomposition), a framework leveraging Singular\nValue Decomposition (SVD) to suppress activation outliers for effective 4-bit\nquantization. QUAD estimates activation singular vectors offline using\ncalibration data to construct an orthogonal transformation matrix P, shifting\noutliers to additional dimensions in full precision while quantizing rest\ncomponents to 4-bit. Additionally, QUAD enables parameter-efficient fine-tuning\nvia adaptable full-precision outlier weights, narrowing the accuracy gap\nbetween quantized and full-precision models. Experiments demonstrate that QUAD\nachieves 94% ~ 96% accuracy under W4A4 quantization and 98% accuracy with\nW4A4/A8 and parameter-efficient fine-tuning for Llama-3 and Qwen-2.5 models.\nOur code is available at \\href{https://github.com/hyx1999/Quad}{repository}.", "published": "2025-03-25 05:03:56", "link": "http://arxiv.org/abs/2503.19353v1", "categories": ["cs.LG", "cs.CL", "I.2.7"], "primary_category": "cs.LG"}
{"title": "Substance over Style: Evaluating Proactive Conversational Coaching Agents", "abstract": "While NLP research has made strides in conversational tasks, many approaches\nfocus on single-turn responses with well-defined objectives or evaluation\ncriteria. In contrast, coaching presents unique challenges with initially\nundefined goals that evolve through multi-turn interactions, subjective\nevaluation criteria, mixed-initiative dialogue. In this work, we describe and\nimplement five multi-turn coaching agents that exhibit distinct conversational\nstyles, and evaluate them through a user study, collecting first-person\nfeedback on 155 conversations. We find that users highly value core\nfunctionality, and that stylistic components in absence of core components are\nviewed negatively. By comparing user feedback with third-person evaluations\nfrom health experts and an LM, we reveal significant misalignment across\nevaluation approaches. Our findings provide insights into design and evaluation\nof conversational coaching agents and contribute toward improving\nhuman-centered NLP applications.", "published": "2025-03-25 03:44:31", "link": "http://arxiv.org/abs/2503.19328v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TRIDIS: A Comprehensive Medieval and Early Modern Corpus for HTR and NER", "abstract": "This paper introduces TRIDIS (Tria Digita Scribunt), an open-source corpus of\nmedieval and early modern manuscripts. TRIDIS aggregates multiple legacy\ncollections (all published under open licenses) and incorporates large metadata\ndescriptions. While prior publications referenced some portions of this corpus,\nhere we provide a unified overview with a stronger focus on its constitution.\nWe describe (i) the narrative, chronological, and editorial background of each\nmajor sub-corpus, (ii) its semi-diplomatic transcription rules (expansion,\nnormalization, punctuation), (iii) a strategy for challenging out-of-domain\ntest splits driven by outlier detection in a joint embedding space, and (iv)\npreliminary baseline experiments using TrOCR and MiniCPM2.5 comparing random\nand outlier-based test partitions. Overall, TRIDIS is designed to stimulate\njoint robust Handwritten Text Recognition (HTR) and Named Entity Recognition\n(NER) research across medieval and early modern textual heritage.", "published": "2025-03-25 03:44:11", "link": "http://arxiv.org/abs/2503.22714v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Iterative Hypothesis Generation for Scientific Discovery with Monte Carlo Nash Equilibrium Self-Refining Trees", "abstract": "Scientific hypothesis generation is a fundamentally challenging task in\nresearch, requiring the synthesis of novel and empirically grounded insights.\nTraditional approaches rely on human intuition and domain expertise, while\npurely large language model (LLM) based methods often struggle to produce\nhypotheses that are both innovative and reliable. To address these limitations,\nwe propose the Monte Carlo Nash Equilibrium Self-Refine Tree (MC-NEST), a novel\nframework that integrates Monte Carlo Tree Search with Nash Equilibrium\nstrategies to iteratively refine and validate hypotheses. MC-NEST dynamically\nbalances exploration and exploitation through adaptive sampling strategies,\nwhich prioritize high-potential hypotheses while maintaining diversity in the\nsearch space. We demonstrate the effectiveness of MC-NEST through comprehensive\nexperiments across multiple domains, including biomedicine, social science, and\ncomputer science. MC-NEST achieves average scores of 2.65, 2.74, and 2.80 (on a\n1-3 scale) for novelty, clarity, significance, and verifiability metrics on the\nsocial science, computer science, and biomedicine datasets, respectively,\noutperforming state-of-the-art prompt-based methods, which achieve 2.36, 2.51,\nand 2.52 on the same datasets. These results underscore MC-NEST's ability to\ngenerate high-quality, empirically grounded hypotheses across diverse domains.\nFurthermore, MC-NEST facilitates structured human-AI collaboration, ensuring\nthat LLMs augment human creativity rather than replace it. By addressing key\nchallenges such as iterative refinement and the exploration-exploitation\nbalance, MC-NEST sets a new benchmark in automated hypothesis generation.\nAdditionally, MC-NEST's ethical design enables responsible AI use, emphasizing\ntransparency and human supervision in hypothesis generation.", "published": "2025-03-25 03:14:53", "link": "http://arxiv.org/abs/2503.19309v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Machine-assisted writing evaluation: Exploring pre-trained language models in analyzing argumentative moves", "abstract": "The study investigates the efficacy of pre-trained language models (PLMs) in\nanalyzing argumentative moves in a longitudinal learner corpus. Prior studies\non argumentative moves often rely on qualitative analysis and manual coding,\nlimiting their efficiency and generalizability. The study aims to: 1) to assess\nthe reliability of PLMs in analyzing argumentative moves; 2) to utilize\nPLM-generated annotations to illustrate developmental patterns and predict\nwriting quality. A longitudinal corpus of 1643 argumentative texts from 235\nEnglish learners in China is collected and annotated into six move types:\nclaim, data, counter-claim, counter-data, rebuttal, and non-argument. The\ncorpus is divided into training, validation, and application sets annotated by\nhuman experts and PLMs. We use BERT as one of the implementations of PLMs. The\nresults indicate a robust reliability of PLMs in analyzing argumentative moves,\nwith an overall F1 score of 0.743, surpassing existing models in the field.\nAdditionally, PLM-labeled argumentative moves effectively capture developmental\npatterns and predict writing quality. Over time, students exhibit an increase\nin the use of data and counter-claims and a decrease in non-argument moves.\nWhile low-quality texts are characterized by a predominant use of claims and\ndata supporting only oneside position, mid- and high-quality texts demonstrate\nan integrative perspective with a higher ratio of counter-claims, counter-data,\nand rebuttals. This study underscores the transformative potential of\nintegrating artificial intelligence into language education, enhancing the\nefficiency and accuracy of evaluating students' writing. The successful\napplication of PLMs can catalyze the development of educational technology,\npromoting a more data-driven and personalized learning environment that\nsupports diverse educational needs.", "published": "2025-03-25 02:21:12", "link": "http://arxiv.org/abs/2503.19279v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CoMAC: Conversational Agent for Multi-Source Auxiliary Context with Sparse and Symmetric Latent Interactions", "abstract": "Recent advancements in AI-driven conversational agents have exhibited immense\npotential of AI applications. Effective response generation is crucial to the\nsuccess of these agents. While extensive research has focused on leveraging\nmultiple auxiliary data sources (e.g., knowledge bases and personas) to enhance\nresponse generation, existing methods often struggle to efficiently extract\nrelevant information from these sources. There are still clear limitations in\nthe ability to combine versatile conversational capabilities with adherence to\nknown facts and adaptation to large variations in user preferences and belief\nsystems, which continues to hinder the wide adoption of conversational AI\ntools. This paper introduces a novel method, Conversational Agent for\nMulti-Source Auxiliary Context with Sparse and Symmetric Latent Interactions\n(CoMAC), for conversation generation, which employs specialized encoding\nstreams and post-fusion grounding networks for multiple data sources to\nidentify relevant persona and knowledge information for the conversation. CoMAC\nalso leverages a novel text similarity metric that allows bi-directional\ninformation sharing among multiple sources and focuses on a selective subset of\nmeaningful words. Our experiments show that CoMAC improves the relevant persona\nand knowledge prediction accuracies and response generation quality\nsignificantly over two state-of-the-art methods.", "published": "2025-03-25 02:09:52", "link": "http://arxiv.org/abs/2503.19274v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MARS: Memory-Enhanced Agents with Reflective Self-improvement", "abstract": "Large language models (LLMs) have made significant advances in the field of\nnatural language processing, but they still face challenges such as continuous\ndecision-making, lack of long-term memory, and limited context windows in\ndynamic environments. To address these issues, this paper proposes an\ninnovative framework Memory-Enhanced Agents with Reflective Self-improvement.\nThe MARS framework comprises three agents: the User, the Assistant, and the\nChecker. By integrating iterative feedback, reflective mechanisms, and a memory\noptimization mechanism based on the Ebbinghaus forgetting curve, it\nsignificantly enhances the agents capabilities in handling multi-tasking and\nlong-span information.", "published": "2025-03-25 02:05:46", "link": "http://arxiv.org/abs/2503.19271v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "PHEONA: An Evaluation Framework for Large Language Model-based Approaches to Computational Phenotyping", "abstract": "Computational phenotyping is essential for biomedical research but often\nrequires significant time and resources, especially since traditional methods\ntypically involve extensive manual data review. While machine learning and\nnatural language processing advancements have helped, further improvements are\nneeded. Few studies have explored using Large Language Models (LLMs) for these\ntasks despite known advantages of LLMs for text-based tasks. To facilitate\nfurther research in this area, we developed an evaluation framework, Evaluation\nof PHEnotyping for Observational Health Data (PHEONA), that outlines\ncontext-specific considerations. We applied and demonstrated PHEONA on concept\nclassification, a specific task within a broader phenotyping process for Acute\nRespiratory Failure (ARF) respiratory support therapies. From the sample\nconcepts tested, we achieved high classification accuracy, suggesting the\npotential for LLM-based methods to improve computational phenotyping processes.", "published": "2025-03-25 01:59:57", "link": "http://arxiv.org/abs/2503.19265v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Linguistic Blind Spots of Large Language Models", "abstract": "Large language models (LLMs) are the foundation of many AI applications\ntoday. However, despite their remarkable proficiency in generating coherent\ntext, questions linger regarding their ability to perform fine-grained\nlinguistic annotation tasks, such as detecting nouns or verbs, or identifying\nmore complex syntactic structures like clauses in input texts. These tasks\nrequire precise syntactic and semantic understanding of input text, and when\nLLMs underperform on specific linguistic structures, it raises concerns about\ntheir reliability for detailed linguistic analysis and whether their (even\ncorrect) outputs truly reflect an understanding of the inputs. In this paper,\nwe empirically study the performance of recent LLMs on fine-grained linguistic\nannotation tasks. Through a series of experiments, we find that recent LLMs\nshow limited efficacy in addressing linguistic queries and often struggle with\nlinguistically complex inputs. We show that the most capable LLM (Llama3-70b)\nmakes notable errors in detecting linguistic structures, such as misidentifying\nembedded clauses, failing to recognize verb phrases, and confusing complex\nnominals with clauses. Our results provide insights to inform future\nadvancements in LLM design and development.", "published": "2025-03-25 01:47:13", "link": "http://arxiv.org/abs/2503.19260v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SCI-IDEA: Context-Aware Scientific Ideation Using Token and Sentence Embeddings", "abstract": "Every scientific discovery starts with an idea inspired by prior work,\ninterdisciplinary concepts, and emerging challenges. Recent advancements in\nlarge language models (LLMs) trained on scientific corpora have driven interest\nin AI-supported idea generation. However, generating context-aware,\nhigh-quality, and innovative ideas remains challenging. We introduce SCI-IDEA,\na framework that uses LLM prompting strategies and Aha Moment detection for\niterative idea refinement. SCI-IDEA extracts essential facets from research\npublications, assessing generated ideas on novelty, excitement, feasibility,\nand effectiveness. Comprehensive experiments validate SCI-IDEA's effectiveness,\nachieving average scores of 6.84, 6.86, 6.89, and 6.84 (on a 1-10 scale) across\nnovelty, excitement, feasibility, and effectiveness, respectively. Evaluations\nemployed GPT-4o, GPT-4.5, DeepSeek-32B (each under 2-shot prompting), and\nDeepSeek-70B (3-shot prompting), with token-level embeddings used for Aha\nMoment detection. Similarly, it achieves scores of 6.87, 6.86, 6.83, and 6.87\nusing GPT-4o under 5-shot prompting, GPT-4.5 under 3-shot prompting,\nDeepSeek-32B under zero-shot chain-of-thought prompting, and DeepSeek-70B under\n5-shot prompting with sentence-level embeddings. We also address ethical\nconsiderations such as intellectual credit, potential misuse, and balancing\nhuman creativity with AI-driven ideation. Our results highlight SCI-IDEA's\npotential to facilitate the structured and flexible exploration of\ncontext-aware scientific ideas, supporting innovation while maintaining ethical\nstandards.", "published": "2025-03-25 01:37:02", "link": "http://arxiv.org/abs/2503.19257v1", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Human-Object Interaction Synthesis with Multimodal Priors", "abstract": "Human-object interaction (HOI) synthesis is important for various\napplications, ranging from virtual reality to robotics. However, acquiring 3D\nHOI data is challenging due to its complexity and high cost, limiting existing\nmethods to the narrow diversity of object types and interaction patterns in\ntraining datasets. This paper proposes a novel zero-shot HOI synthesis\nframework without relying on end-to-end training on currently limited 3D HOI\ndatasets. The core idea of our method lies in leveraging extensive HOI\nknowledge from pre-trained Multimodal Models. Given a text description, our\nsystem first obtains temporally consistent 2D HOI image sequences using image\nor video generation models, which are then uplifted to 3D HOI milestones of\nhuman and object poses. We employ pre-trained human pose estimation models to\nextract human poses and introduce a generalizable category-level 6-DoF\nestimation method to obtain the object poses from 2D HOI images. Our estimation\nmethod is adaptive to various object templates obtained from text-to-3D models\nor online retrieval. A physics-based tracking of the 3D HOI kinematic milestone\nis further applied to refine both body motions and object poses, yielding more\nphysically plausible HOI generation results. The experimental results\ndemonstrate that our method is capable of generating open-vocabulary HOIs with\nphysical realism and semantic diversity.", "published": "2025-03-25 23:55:47", "link": "http://arxiv.org/abs/2503.20118v1", "categories": ["cs.GR", "cs.AI", "cs.CV"], "primary_category": "cs.GR"}
{"title": "Direct Post-Training Preference Alignment for Multi-Agent Motion Generation Models Using Implicit Feedback from Pre-training Demonstrations", "abstract": "Recent advancements in LLMs have revolutionized motion generation models in\nembodied applications. While LLM-type auto-regressive motion generation models\nbenefit from training scalability, there remains a discrepancy between their\ntoken prediction objectives and human preferences. As a result, models\npre-trained solely with token-prediction objectives often generate behaviors\nthat deviate from what humans would prefer, making post-training preference\nalignment crucial for producing human-preferred motions. Unfortunately,\npost-training alignment requires extensive preference rankings of motions\ngenerated by the pre-trained model, which are costly to annotate, especially in\nmulti-agent settings. Recently, there has been growing interest in leveraging\npre-training demonstrations to scalably generate preference data for\npost-training alignment. However, these methods often adopt an adversarial\nassumption, treating all pre-trained model-generated samples as unpreferred\nexamples. This adversarial approach overlooks the valuable signal provided by\npreference rankings among the model's own generations, ultimately reducing\nalignment effectiveness and potentially leading to misaligned behaviors. In\nthis work, instead of treating all generated samples as equally bad, we\nleverage implicit preferences encoded in pre-training demonstrations to\nconstruct preference rankings among the pre-trained model's generations,\noffering more nuanced preference alignment guidance with zero human cost. We\napply our approach to large-scale traffic simulation and demonstrate its\neffectiveness in improving the realism of pre-trained model's generated\nbehaviors, making a lightweight 1M motion generation model comparable to SOTA\nlarge imitation-based models by relying solely on implicit feedback from\npre-training demonstrations, without additional post-training human preference\nannotations or high computational costs.", "published": "2025-03-25 23:02:13", "link": "http://arxiv.org/abs/2503.20105v1", "categories": ["cs.AI", "cs.RO"], "primary_category": "cs.AI"}
{"title": "AI Identity, Empowerment, and Mindfulness in Mitigating Unethical AI Use", "abstract": "This study examines how AI identity influences psychological empowerment and\nunethical AI behavior among college students, while also exploring the\nmoderating role of IT mindfulness. Findings show that a strong AI identity\nenhances psychological empowerment and academic engagement but can also lead to\nincreased unethical AI practices. Crucially, IT mindfulness acts as an ethical\nsafeguard, promoting sensitivity to ethical concerns and reducing misuse of AI.\nThese insights have implications for educators, policymakers, and AI\ndevelopers, emphasizing For Peer Review the need for a balanced approach that\nencourages digital engagement without compromising student responsibility. The\nstudy also contributes to philosophical discussions of psychological agency,\nsuggesting that empowerment through AI can yield both positive and negative\noutcomes. Mindfulness emerges as essential in guiding ethical AI interactions.\nOverall, the research informs ongoing debates on ethics in education and AI,\noffering strategies to align technological advancement with ethical\naccountability and responsible use.", "published": "2025-03-25 22:36:21", "link": "http://arxiv.org/abs/2503.20099v1", "categories": ["cs.CY", "cs.AI"], "primary_category": "cs.CY"}
{"title": "Can Multi-modal (reasoning) LLMs work as deepfake detectors?", "abstract": "Deepfake detection remains a critical challenge in the era of advanced\ngenerative models, particularly as synthetic media becomes more sophisticated.\nIn this study, we explore the potential of state of the art multi-modal\n(reasoning) large language models (LLMs) for deepfake image detection such as\n(OpenAI O1/4o, Gemini thinking Flash 2, Deepseek Janus, Grok 3, llama 3.2, Qwen\n2/2.5 VL, Mistral Pixtral, Claude 3.5/3.7 sonnet) . We benchmark 12 latest\nmulti-modal LLMs against traditional deepfake detection methods across multiple\ndatasets, including recently published real-world deepfake imagery. To enhance\nperformance, we employ prompt tuning and conduct an in-depth analysis of the\nmodels' reasoning pathways to identify key contributing factors in their\ndecision-making process. Our findings indicate that best multi-modal LLMs\nachieve competitive performance with promising generalization ability with zero\nshot, even surpass traditional deepfake detection pipelines in\nout-of-distribution datasets while the rest of the LLM families performs\nextremely disappointing with some worse than random guess. Furthermore, we\nfound newer model version and reasoning capabilities does not contribute to\nperformance in such niche tasks of deepfake detection while model size do help\nin some cases. This study highlights the potential of integrating multi-modal\nreasoning in future deepfake detection frameworks and provides insights into\nmodel interpretability for robustness in real-world scenarios.", "published": "2025-03-25 21:47:29", "link": "http://arxiv.org/abs/2503.20084v2", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Abstracting Geo-specific Terrains to Scale Up Reinforcement Learning", "abstract": "Multi-agent reinforcement learning (MARL) is increasingly ubiquitous in\ntraining dynamic and adaptive synthetic characters for interactive simulations\non geo-specific terrains. Frameworks such as Unity's ML-Agents help to make\nsuch reinforcement learning experiments more accessible to the simulation\ncommunity. Military training simulations also benefit from advances in MARL,\nbut they have immense computational requirements due to their complex,\ncontinuous, stochastic, partially observable, non-stationary, and\ndoctrine-based nature. Furthermore, these simulations require geo-specific\nterrains, further exacerbating the computational resources problem. In our\nresearch, we leverage Unity's waypoints to automatically generate multi-layered\nrepresentation abstractions of the geo-specific terrains to scale up\nreinforcement learning while still allowing the transfer of learned policies\nbetween different representations. Our early exploratory results on a novel\nMARL scenario, where each side has differing objectives, indicate that\nwaypoint-based navigation enables faster and more efficient learning while\nproducing trajectories similar to those taken by expert human players in CSGO\ngaming environments. This research points out the potential of waypoint-based\nnavigation for reducing the computational costs of developing and training MARL\nmodels for military training simulations, where geo-specific terrains and\ndiffering objectives are crucial.", "published": "2025-03-25 21:29:49", "link": "http://arxiv.org/abs/2503.20078v1", "categories": ["cs.LG", "cs.AI", "cs.MA"], "primary_category": "cs.LG"}
{"title": "Adaptive Orchestration for Large-Scale Inference on Heterogeneous Accelerator Systems Balancing Cost, Performance, and Resilience", "abstract": "The surge in generative AI workloads has created a need for scalable\ninference systems that can flexibly harness both GPUs and specialized\naccelerators while containing operational costs. This paper proposes a\nhardware-agnostic control loop that adaptively allocates requests across\nheterogeneous accelerators based on real-time cost and capacity signals. The\napproach sustains low latency and high throughput by dynamically shifting\nbetween cost-optimized and capacity-optimized modes, ensuring the most\nefficient use of expensive compute resources under fluctuating availability.\nEvaluated using the Stable Diffusion model, the framework consistently meets\nlatency targets, automatically redirects traffic during capacity shortfalls,\nand capitalizes on lower-cost accelerators when possible. These results\nhighlight how a feedback-driven deployment strategy, spanning the entire\nsoftware and hardware stack, can help organizations efficiently scale\ngenerative AI workloads while maintaining resilience in the face of limited\naccelerator capacity.", "published": "2025-03-25 21:20:11", "link": "http://arxiv.org/abs/2503.20074v2", "categories": ["cs.PF", "cs.AI", "68U01"], "primary_category": "cs.PF"}
{"title": "Why Representation Engineering Works: A Theoretical and Empirical Study in Vision-Language Models", "abstract": "Representation Engineering (RepE) has emerged as a powerful paradigm for\nenhancing AI transparency by focusing on high-level representations rather than\nindividual neurons or circuits. It has proven effective in improving\ninterpretability and control, showing that representations can emerge,\npropagate, and shape final model outputs in large language models (LLMs).\nHowever, in Vision-Language Models (VLMs), visual input can override factual\nlinguistic knowledge, leading to hallucinated responses that contradict\nreality. To address this challenge, we make the first attempt to extend RepE to\nVLMs, analyzing how multimodal representations are preserved and transformed.\nBuilding on our findings and drawing inspiration from successful RepE\napplications, we develop a theoretical framework that explains the stability of\nneural activity across layers using the principal eigenvector, uncovering the\nunderlying mechanism of RepE. We empirically validate these instrinsic\nproperties, demonstrating their broad applicability and significance. By\nbridging theoretical insights with empirical validation, this work transforms\nRepE from a descriptive tool into a structured theoretical framework, opening\nnew directions for improving AI robustness, fairness, and transparency.", "published": "2025-03-25 20:32:15", "link": "http://arxiv.org/abs/2503.22720v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "LLM-based Agent Simulation for Maternal Health Interventions: Uncertainty Estimation and Decision-focused Evaluation", "abstract": "Agent-based simulation is crucial for modeling complex human behavior, yet\ntraditional approaches require extensive domain knowledge and large datasets.\nIn data-scarce healthcare settings where historic and counterfactual data are\nlimited, large language models (LLMs) offer a promising alternative by\nleveraging broad world knowledge. This study examines an LLM-driven simulation\nof a maternal mobile health program, predicting beneficiaries' listening\nbehavior when they receive health information via automated messages (control)\nor live representatives (intervention). Since uncertainty quantification is\ncritical for decision-making in health interventions, we propose an LLM\nepistemic uncertainty estimation method based on binary entropy across multiple\nsamples. We enhance model robustness through ensemble approaches, improving F1\nscore and model calibration compared to individual models. Beyond direct\nevaluation, we take a decision-focused approach, demonstrating how LLM\npredictions inform intervention feasibility and trial implementation in\ndata-limited settings. The proposed method extends to public health, disaster\nresponse, and other domains requiring rapid intervention assessment under\nsevere data constraints. All code and prompts used for this work can be found\nat https://github.com/sarahmart/LLM-ABS-ARMMAN-prediction.", "published": "2025-03-25 20:24:47", "link": "http://arxiv.org/abs/2503.22719v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "BugCraft: End-to-End Crash Bug Reproduction Using LLM Agents in Minecraft", "abstract": "Reproducing game bugs, in our case crash bugs in continuously evolving games\nlike Minecraft, is a notoriously manual, time-consuming, and challenging\nprocess to automate. Despite the success of LLM-driven bug reproduction in\nother software domains, games, with their complex interactive environments,\nremain largely unaddressed. This paper introduces BugCraft, a novel end-to-end\nframework designed to automate the reproduction of crash bugs in Minecraft\ndirectly from user-submitted bug reports, addressing the critical gap in\nautomated game bug reproduction. BugCraft employs a two-stage approach: first,\na Step Synthesizer leverages LLMs and Minecraft Wiki knowledge to transform bug\nreports into high-quality, structured steps to reproduce (S2R). Second, an\nAction Model, powered by a vision-based LLM agent (GPT-4o) and a custom macro\nAPI, executes these S2R steps within Minecraft to trigger the reported crash.\nTo facilitate evaluation, we introduce BugCraft-Bench, a curated dataset of\nMinecraft crash bug reports. Evaluated on BugCraft-Bench, our framework\nsuccessfully reproduced 30.23% of crash bugs end-to-end. The Step Synthesizer\ndemonstrated a 66.28% accuracy in generating correct bug reproduction plans,\nhighlighting its effectiveness in interpreting and structuring bug report\ninformation. BugCraft demonstrates the feasibility of automated reproduction of\ncrash bugs in complex game environments using LLMs, opening promising avenues\nfor game testing and development. The framework and the BugCraft-Bench dataset\npave the way for future research in automated game bug analysis and hold\npotential for generalization to other interactive game platforms. Finally, we\nmake our code open at https://bugcraft2025.github.io/", "published": "2025-03-25 19:34:24", "link": "http://arxiv.org/abs/2503.20036v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE"}
{"title": "OmniNova:A General Multimodal Agent Framework", "abstract": "The integration of Large Language Models (LLMs) with specialized tools\npresents new opportunities for intelligent automation systems. However,\norchestrating multiple LLM-driven agents to tackle complex tasks remains\nchallenging due to coordination difficulties, inefficient resource utilization,\nand inconsistent information flow. We present OmniNova, a modular multi-agent\nautomation framework that combines language models with specialized tools such\nas web search, crawling, and code execution capabilities. OmniNova introduces\nthree key innovations: (1) a hierarchical multi-agent architecture with\ndistinct coordinator, planner, supervisor, and specialist agents; (2) a dynamic\ntask routing mechanism that optimizes agent deployment based on task\ncomplexity; and (3) a multi-layered LLM integration system that allocates\nappropriate models to different cognitive requirements. Our evaluations across\n50 complex tasks in research, data analysis, and web interaction domains\ndemonstrate that OmniNova outperforms existing frameworks in task completion\nrate (87\\% vs. baseline 62\\%), efficiency (41\\% reduced token usage), and\nresult quality (human evaluation score of 4.2/5 vs. baseline 3.1/5). We\ncontribute both a theoretical framework for multi-agent system design and an\nopen-source implementation that advances the state-of-the-art in LLM-based\nautomation systems.", "published": "2025-03-25 19:21:01", "link": "http://arxiv.org/abs/2503.20028v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Experience Replay Addresses Loss of Plasticity in Continual Learning", "abstract": "Loss of plasticity is one of the main challenges in continual learning with\ndeep neural networks, where neural networks trained via backpropagation\ngradually lose their ability to adapt to new tasks and perform significantly\nworse than their freshly initialized counterparts. The main contribution of\nthis paper is to propose a new hypothesis that experience replay addresses the\nloss of plasticity in continual learning. Here, experience replay is a form of\nmemory. We provide supporting evidence for this hypothesis. In particular, we\ndemonstrate in multiple different tasks, including regression, classification,\nand policy evaluation, that by simply adding an experience replay and\nprocessing the data in the experience replay with Transformers, the loss of\nplasticity disappears. Notably, we do not alter any standard components of deep\nlearning. For example, we do not change backpropagation. We do not modify the\nactivation functions. And we do not use any regularization. We conjecture that\nexperience replay and Transformers can address the loss of plasticity because\nof the in-context learning phenomenon.", "published": "2025-03-25 19:01:10", "link": "http://arxiv.org/abs/2503.20018v1", "categories": ["cs.LG", "cs.AI", "cs.NE"], "primary_category": "cs.LG"}
{"title": "Unsupervised Learning for Quadratic Assignment", "abstract": "We introduce PLUME search, a data-driven framework that enhances search\nefficiency in combinatorial optimization through unsupervised learning. Unlike\nsupervised or reinforcement learning, PLUME search learns directly from problem\ninstances using a permutation-based loss with a non-autoregressive approach. We\nevaluate its performance on the quadratic assignment problem, a fundamental\nNP-hard problem that encompasses various combinatorial optimization problems.\nExperimental results demonstrate that PLUME search consistently improves\nsolution quality. Furthermore, we study the generalization behavior and show\nthat the learned model generalizes across different densities and sizes.", "published": "2025-03-25 18:37:46", "link": "http://arxiv.org/abs/2503.20001v1", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI"}
{"title": "LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?", "abstract": "Multi-step spatial reasoning entails understanding and reasoning about\nspatial relationships across multiple sequential steps, which is crucial for\ntackling complex real-world applications, such as robotic manipulation,\nautonomous navigation, and automated assembly. To assess how well current\nMultimodal Large Language Models (MLLMs) have acquired this fundamental\ncapability, we introduce \\textbf{LEGO-Puzzles}, a scalable benchmark designed\nto evaluate both \\textbf{spatial understanding} and \\textbf{sequential\nreasoning} in MLLMs through LEGO-based tasks. LEGO-Puzzles consists of 1,100\ncarefully curated visual question-answering (VQA) samples spanning 11 distinct\ntasks, ranging from basic spatial understanding to complex multi-step\nreasoning. Based on LEGO-Puzzles, we conduct a comprehensive evaluation of\nstate-of-the-art MLLMs and uncover significant limitations in their spatial\nreasoning capabilities: even the most powerful MLLMs can answer only about half\nof the test cases, whereas human participants achieve over 90\\% accuracy. In\naddition to VQA tasks, we evaluate MLLMs' abilities to generate LEGO images\nfollowing assembly illustrations. Our experiments show that only\nGemini-2.0-Flash and GPT-4o exhibit a limited ability to follow these\ninstructions, while other MLLMs either replicate the input image or generate\ncompletely irrelevant outputs. Overall, LEGO-Puzzles exposes critical\ndeficiencies in existing MLLMs' spatial understanding and sequential reasoning\ncapabilities, and underscores the need for further advancements in multimodal\nspatial reasoning.", "published": "2025-03-25 18:21:07", "link": "http://arxiv.org/abs/2503.19990v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "ExCoT: Optimizing Reasoning for Text-to-SQL with Execution Feedback", "abstract": "Text-to-SQL demands precise reasoning to convert natural language questions\ninto structured queries. While large language models (LLMs) excel in many\nreasoning tasks, their ability to leverage Chain-of-Thought (CoT) reasoning for\ntext-to-SQL remains underexplored. We identify critical limitations: zero-shot\nCoT offers minimal gains, and Direct Preference Optimization (DPO) applied\nwithout CoT yields marginal improvements. We propose ExCoT, a novel framework\nthat iteratively optimizes open-source LLMs by combining CoT reasoning with\noff-policy and on-policy DPO, relying solely on execution accuracy as feedback.\nThis approach eliminates the need for reward models or human-annotated\npreferences.\n  Our experimental results demonstrate significant performance gains: ExCoT\nimproves execution accuracy on BIRD dev set from 57.37% to 68.51% and on Spider\ntest set from 78.81% to 86.59% for LLaMA-3 70B, with Qwen-2.5-Coder\ndemonstrating similar improvements. Our best model achieves state-of-the-art\nperformance in the single-model setting on both BIRD and Spider datasets,\nnotably achieving 68.53% on the BIRD test set.", "published": "2025-03-25 18:17:36", "link": "http://arxiv.org/abs/2503.19988v1", "categories": ["cs.LG", "cs.AI", "cs.DB"], "primary_category": "cs.LG"}
{"title": "A proposal for an incident regime that tracks and counters threats to national security posed by AI systems", "abstract": "Recent progress in AI capabilities has heightened concerns that AI systems\ncould pose a threat to national security, for example, by making it easier for\nmalicious actors to perform cyberattacks on critical national infrastructure,\nor through loss of control of autonomous AI systems. In parallel, federal\nlegislators in the US have proposed nascent 'AI incident regimes' to identify\nand counter similar threats. In this paper, we consolidate these two trends and\npresent a proposal for a legally mandated post-deployment AI incident regie\nthat aims to counter potential national security threats from AI systems. We\nstart the paper by introducing the concept of 'security-critical' to describe\ndoctors that pose extreme risks to national security, before arguing that\n'security-critical' describes civilian nuclear power, aviation, life science\ndual-use research of concern, and frontier AI development. We then present in\ndetail our AI incident regime proposal,, justifying each component of the\nproposal by demonstrating its similarity to US domestic incident regimes in\nother 'security-critical' sectors. Finally, we sketch a hypothetical scenario\nwhere our proposed AI incident regime deals with an AI cyber incident. Our\nproposed AI incident regime is split into three phases. The first phase\nrevolves around a novel operationalization of what counts as an 'AI incident'\nand we suggest that AI providers must create a 'national security case' before\ndeploying a frontier AI system. The second and third phases spell out that AI\nproviders should notify a government agency about incidents, and that the\ngovernment agency should be involved in amending AI providers' security and\nsafety procedures, in order to counter future threats to national security. Our\nproposal is timely, given ongoing policy interest in the potential national\nsecurity threats posed by AI systems.", "published": "2025-03-25 17:51:50", "link": "http://arxiv.org/abs/2503.19887v1", "categories": ["cs.CY", "cs.AI"], "primary_category": "cs.CY"}
{"title": "Dynamics of Structured Complex-Valued Hopfield Neural Networks", "abstract": "In this paper, we explore the dynamics of structured complex-valued Hopfield\nneural networks (CvHNNs), which arise when the synaptic weight matrix possesses\nspecific structural properties. We begin by analyzing CvHNNs with a Hermitian\nsynaptic weight matrix and establish the existence of four-cycle dynamics in\nCvHNNs with skew-Hermitian weight matrices operating synchronously.\nFurthermore, we introduce two new classes of complex-valued matrices: braided\nHermitian and braided skew-Hermitian matrices. We demonstrate that CvHNNs\nutilizing these matrix types exhibit cycles of length eight when operating in\nfull parallel update mode. Finally, we conduct extensive computational\nexperiments on synchronous CvHNNs, exploring other synaptic weight matrix\nstructures. The findings provide a comprehensive overview of the dynamics of\nstructured CvHNNs, offering insights that may contribute to developing improved\nassociative memory models when integrated with suitable learning rules.", "published": "2025-03-25 17:49:36", "link": "http://arxiv.org/abs/2503.19885v1", "categories": ["cs.NE", "cs.AI"], "primary_category": "cs.NE"}
{"title": "Geometric Meta-Learning via Coupled Ricci Flow: Unifying Knowledge Representation and Quantum Entanglement", "abstract": "This paper establishes a unified framework integrating geometric flows with\ndeep learning through three fundamental innovations. First, we propose a\nthermodynamically coupled Ricci flow that dynamically adapts parameter space\ngeometry to loss landscape topology, formally proved to preserve isometric\nknowledge embedding (Theorem~\\ref{thm:isometric}). Second, we derive explicit\nphase transition thresholds and critical learning rates\n(Theorem~\\ref{thm:critical}) through curvature blowup analysis, enabling\nautomated singularity resolution via geometric surgery\n(Lemma~\\ref{lem:surgery}). Third, we establish an AdS/CFT-type holographic\nduality (Theorem~\\ref{thm:ads}) between neural networks and conformal field\ntheories, providing entanglement entropy bounds for regularization design.\nExperiments demonstrate 2.1$\\times$ convergence acceleration and 63\\%\ntopological simplification while maintaining $\\mathcal{O}(N\\log N)$ complexity,\noutperforming Riemannian baselines by 15.2\\% in few-shot accuracy.\nTheoretically, we prove exponential stability (Theorem~\\ref{thm:converge})\nthrough a new Lyapunov function combining Perelman entropy with Wasserstein\ngradient flows, fundamentally advancing geometric deep learning.", "published": "2025-03-25 17:32:31", "link": "http://arxiv.org/abs/2503.19867v1", "categories": ["cs.LG", "cs.AI", "eess.SP", "math.GT", "quant-ph", "68T05, 68T07, 68T27, 81V99, 37F40,", "I.2; K.3.2; F.4.1"], "primary_category": "cs.LG"}
{"title": "GENIUS: A Generative Framework for Universal Multimodal Search", "abstract": "Generative retrieval is an emerging approach in information retrieval that\ngenerates identifiers (IDs) of target data based on a query, providing an\nefficient alternative to traditional embedding-based retrieval methods.\nHowever, existing models are task-specific and fall short of embedding-based\nretrieval in performance. This paper proposes GENIUS, a universal generative\nretrieval framework supporting diverse tasks across multiple modalities and\ndomains. At its core, GENIUS introduces modality-decoupled semantic\nquantization, transforming multimodal data into discrete IDs encoding both\nmodality and semantics. Moreover, to enhance generalization, we propose a query\naugmentation that interpolates between a query and its target, allowing GENIUS\nto adapt to varied query forms. Evaluated on the M-BEIR benchmark, it surpasses\nprior generative methods by a clear margin. Unlike embedding-based retrieval,\nGENIUS consistently maintains high retrieval speed across database size, with\ncompetitive performance across multiple benchmarks. With additional re-ranking,\nGENIUS often achieves results close to those of embedding-based methods while\npreserving efficiency.", "published": "2025-03-25 17:32:31", "link": "http://arxiv.org/abs/2503.19868v1", "categories": ["cs.IR", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Guarding against artificial intelligence--hallucinated citations: the case for full-text reference deposit", "abstract": "The tendency of generative artificial intelligence (AI) systems to\n\"hallucinate\" false information is well-known; AI-generated citations to\nnon-existent sources have made their way into the reference lists of\npeer-reviewed publications. Here, I propose a solution to this problem, taking\ninspiration from the Transparency and Openness Promotion (TOP) data sharing\nguidelines, the clash of generative AI with the American judiciary, and the\nprecedent set by submissions of prior art to the United States Patent and\nTrademark Office. Journals should require authors to submit the full text of\neach cited source along with their manuscripts, thereby preventing authors from\nciting any material whose full text they cannot produce. This solution requires\nlimited additional work on the part of authors or editors while effectively\nimmunizing journals against hallucinated references.", "published": "2025-03-25 17:12:38", "link": "http://arxiv.org/abs/2503.19848v1", "categories": ["cs.DL", "cs.AI", "I.2.0; K.4.1"], "primary_category": "cs.DL"}
{"title": "GyralNet Subnetwork Partitioning via Differentiable Spectral Modularity Optimization", "abstract": "Understanding the structural and functional organization of the human brain\nrequires a detailed examination of cortical folding patterns, among which the\nthree-hinge gyrus (3HG) has been identified as a key structural landmark.\nGyralNet, a network representation of cortical folding, models 3HGs as nodes\nand gyral crests as edges, highlighting their role as critical hubs in\ncortico-cortical connectivity. However, existing methods for analyzing 3HGs\nface significant challenges, including the sub-voxel scale of 3HGs at typical\nneuroimaging resolutions, the computational complexity of establishing\ncross-subject correspondences, and the oversimplification of treating 3HGs as\nindependent nodes without considering their community-level relationships. To\naddress these limitations, we propose a fully differentiable subnetwork\npartitioning framework that employs a spectral modularity maximization\noptimization strategy to modularize the organization of 3HGs within GyralNet.\nBy incorporating topological structural similarity and DTI-derived connectivity\npatterns as attribute features, our approach provides a biologically meaningful\nrepresentation of cortical organization. Extensive experiments on the Human\nConnectome Project (HCP) dataset demonstrate that our method effectively\npartitions GyralNet at the individual level while preserving the\ncommunity-level consistency of 3HGs across subjects, offering a robust\nfoundation for understanding brain connectivity.", "published": "2025-03-25 16:33:12", "link": "http://arxiv.org/abs/2503.19823v2", "categories": ["q-bio.NC", "cs.AI", "cs.CV"], "primary_category": "q-bio.NC"}
{"title": "Bitstream Collisions in Neural Image Compression via Adversarial Perturbations", "abstract": "Neural image compression (NIC) has emerged as a promising alternative to\nclassical compression techniques, offering improved compression ratios. Despite\nits progress towards standardization and practical deployment, there has been\nminimal exploration into it's robustness and security. This study reveals an\nunexpected vulnerability in NIC - bitstream collisions - where semantically\ndifferent images produce identical compressed bitstreams. Utilizing a novel\nwhitebox adversarial attack algorithm, this paper demonstrates that adding\ncarefully crafted perturbations to semantically different images can cause\ntheir compressed bitstreams to collide exactly. The collision vulnerability\nposes a threat to the practical usability of NIC, particularly in\nsecurity-critical applications. The cause of the collision is analyzed, and a\nsimple yet effective mitigation method is presented.", "published": "2025-03-25 16:29:17", "link": "http://arxiv.org/abs/2503.19817v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR"}
{"title": "ACVUBench: Audio-Centric Video Understanding Benchmark", "abstract": "Audio often serves as an auxiliary modality in video understanding tasks of\naudio-visual large language models (LLMs), merely assisting in the\ncomprehension of visual information. However, a thorough understanding of\nvideos significantly depends on auditory information, as audio offers critical\ncontext, emotional cues, and semantic meaning that visual data alone often\nlacks. This paper proposes an audio-centric video understanding benchmark\n(ACVUBench) to evaluate the video comprehension capabilities of multimodal LLMs\nwith a particular focus on auditory information. Specifically, ACVUBench\nincorporates 2,662 videos spanning 18 different domains with rich auditory\ninformation, together with over 13k high-quality human annotated or validated\nquestion-answer pairs. Moreover, ACVUBench introduces a suite of carefully\ndesigned audio-centric tasks, holistically testing the understanding of both\naudio content and audio-visual interactions in videos. A thorough evaluation\nacross a diverse range of open-source and proprietary multimodal LLMs is\nperformed, followed by the analyses of deficiencies in audio-visual LLMs. Demos\nare available at https://github.com/lark-png/ACVUBench.", "published": "2025-03-25 16:28:24", "link": "http://arxiv.org/abs/2503.19951v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Thinking agents for zero-shot generalization to qualitatively novel tasks", "abstract": "Intelligent organisms can solve truly novel problems which they have never\nencountered before, either in their lifetime or their evolution. An important\ncomponent of this capacity is the ability to ``think'', that is, to mentally\nmanipulate objects, concepts and behaviors in order to plan and evaluate\npossible solutions to novel problems, even without environment interaction. To\ngenerate problems that are truly qualitatively novel, while still solvable\nzero-shot (by mental simulation), we use the combinatorial nature of\nenvironments: we train the agent while withholding a specific combination of\nthe environment's elements. The novel test task, based on this combination, is\nthus guaranteed to be truly novel, while still mentally simulable since the\nagent has been exposed to each individual element (and their pairwise\ninteractions) during training. We propose a method to train agents endowed with\nworld models to make use their mental simulation abilities, by selecting tasks\nbased on the difference between the agent's pre-thinking and post-thinking\nperformance. When tested on the novel, withheld problem, the resulting agent\nsuccessfully simulated alternative scenarios and used the resulting information\nto guide its behavior in the actual environment, solving the novel task in a\nsingle real-environment trial (zero-shot).", "published": "2025-03-25 16:26:31", "link": "http://arxiv.org/abs/2503.19815v1", "categories": ["cs.AI", "cs.NE"], "primary_category": "cs.AI"}
{"title": "Guidelines For The Choice Of The Baseline in XAI Attribution Methods", "abstract": "Given the broad adoption of artificial intelligence, it is essential to\nprovide evidence that AI models are reliable, trustable, and fair. To this end,\nthe emerging field of eXplainable AI develops techniques to probe such\nrequirements, counterbalancing the hype pushing the pervasiveness of this\ntechnology. Among the many facets of this issue, this paper focuses on baseline\nattribution methods, aiming at deriving a feature attribution map at the\nnetwork input relying on a \"neutral\" stimulus usually called \"baseline\". The\nchoice of the baseline is crucial as it determines the explanation of the\nnetwork behavior. In this framework, this paper has the twofold goal of\nshedding light on the implications of the choice of the baseline and providing\na simple yet effective method for identifying the best baseline for the task.\nTo achieve this, we propose a decision boundary sampling method, since the\nbaseline, by definition, lies on the decision boundary, which naturally becomes\nthe search domain. Experiments are performed on synthetic examples and\nvalidated relying on state-of-the-art methods. Despite being limited to the\nexperimental scope, this contribution is relevant as it offers clear guidelines\nand a simple proxy for baseline selection, reducing ambiguity and enhancing\ndeep models' reliability and trust.", "published": "2025-03-25 16:25:04", "link": "http://arxiv.org/abs/2503.19813v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Simulating Tracking Data to Advance Sports Analytics Research", "abstract": "Advanced analytics have transformed how sports teams operate, particularly in\nepisodic sports like baseball. Their impact on continuous invasion sports, such\nas soccer and ice hockey, has been limited due to increased game complexity and\nrestricted access to high-resolution game tracking data. In this demo, we\npresent a method to collect and utilize simulated soccer tracking data from the\nGoogle Research Football environment to support the development of models\ndesigned for continuous tracking data. The data is stored in a schema that is\nrepresentative of real tracking data and we provide processes that extract\nhigh-level features and events. We include examples of established tracking\ndata models to showcase the efficacy of the simulated data. We address the\nscarcity of publicly available tracking data, providing support for research at\nthe intersection of artificial intelligence and sports analytics.", "published": "2025-03-25 16:18:23", "link": "http://arxiv.org/abs/2503.19809v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "LENVIZ: A High-Resolution Low-Exposure Night Vision Benchmark Dataset", "abstract": "Low-light image enhancement is crucial for a myriad of applications, from\nnight vision and surveillance, to autonomous driving. However, due to the\ninherent limitations that come in hand with capturing images in\nlow-illumination environments, the task of enhancing such scenes still presents\na formidable challenge. To advance research in this field, we introduce our Low\nExposure Night Vision (LENVIZ) Dataset, a comprehensive multi-exposure\nbenchmark dataset for low-light image enhancement comprising of over 230K\nframes showcasing 24K real-world indoor and outdoor, with-and without human,\nscenes. Captured using 3 different camera sensors, LENVIZ offers a wide range\nof lighting conditions, noise levels, and scene complexities, making it the\nlargest publicly available up-to 4K resolution benchmark in the field. LENVIZ\nincludes high quality human-generated ground truth, for which each\nmulti-exposure low-light scene has been meticulously curated and edited by\nexpert photographers to ensure optimal image quality. Furthermore, we also\nconduct a comprehensive analysis of current state-of-the-art low-light image\nenhancement techniques on our dataset and highlight potential areas of\nimprovement.", "published": "2025-03-25 16:12:28", "link": "http://arxiv.org/abs/2503.19804v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "SeLIP: Similarity Enhanced Contrastive Language Image Pretraining for Multi-modal Head MRI", "abstract": "Despite that deep learning (DL) methods have presented tremendous potential\nin many medical image analysis tasks, the practical applications of medical DL\nmodels are limited due to the lack of enough data samples with manual\nannotations. By noting that the clinical radiology examinations are associated\nwith radiology reports that describe the images, we propose to develop a\nfoundation model for multi-model head MRI by using contrastive learning on the\nimages and the corresponding radiology findings. In particular, a contrastive\nlearning framework is proposed, where a mixed syntax and semantic similarity\nmatching metric is integrated to reduce the thirst of extreme large dataset in\nconventional contrastive learning framework. Our proposed similarity enhanced\ncontrastive language image pretraining (SeLIP) is able to effectively extract\nmore useful features. Experiments revealed that our proposed SeLIP performs\nwell in many downstream tasks including image-text retrieval task,\nclassification task, and image segmentation, which highlights the importance of\nconsidering the similarities among texts describing different images in\ndeveloping medical image foundation models.", "published": "2025-03-25 16:09:45", "link": "http://arxiv.org/abs/2503.19801v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "PAVE: Patching and Adapting Video Large Language Models", "abstract": "Pre-trained video large language models (Video LLMs) exhibit remarkable\nreasoning capabilities, yet adapting these models to new tasks involving\nadditional modalities or data types (e.g., audio or 3D information) remains\nchallenging. In this paper, we present PAVE, a flexible framework for adapting\npre-trained Video LLMs to downstream tasks with side-channel signals, such as\naudio, 3D cues, or multi-view videos. PAVE introduces lightweight adapters,\nreferred to as \"patches,\" which add a small number of parameters and operations\nto a base model without modifying its architecture or pre-trained weights. In\ndoing so, PAVE can effectively adapt the pre-trained base model to support\ndiverse downstream tasks, including audio-visual question answering, 3D\nreasoning, multi-view video recognition, and high frame rate video\nunderstanding. Across these tasks, PAVE significantly enhances the performance\nof the base model, surpassing state-of-the-art task-specific models while\nincurring a minor cost of ~0.1% additional FLOPs and parameters. Further, PAVE\nsupports multi-task learning and generalizes well across different Video LLMs.\nOur code is available at https://github.com/dragonlzm/PAVE.", "published": "2025-03-25 16:02:37", "link": "http://arxiv.org/abs/2503.19794v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Test-Time Reasoning Through Visual Human Preferences with VLMs and Soft Rewards", "abstract": "Can Visual Language Models (VLMs) effectively capture human visual\npreferences? This work addresses this question by training VLMs to think about\npreferences at test time, employing reinforcement learning methods inspired by\nDeepSeek R1 and OpenAI O1. Using datasets such as ImageReward and Human\nPreference Score v2 (HPSv2), our models achieve accuracies of 64.9% on the\nImageReward test set (trained on ImageReward official split) and 65.4% on HPSv2\n(trained on approximately 25% of its data). These results match traditional\nencoder-based models while providing transparent reasoning and enhanced\ngeneralization. This approach allows to use not only rich VLM world knowledge,\nbut also its potential to think, yielding interpretable outcomes that help\ndecision-making processes. By demonstrating that human visual preferences\nreasonable by current VLMs, we introduce efficient soft-reward strategies for\nimage ranking, outperforming simplistic selection or scoring methods. This\nreasoning capability enables VLMs to rank arbitrary images-regardless of aspect\nratio or complexity-thereby potentially amplifying the effectiveness of visual\nPreference Optimization. By reducing the need for extensive markup while\nimproving reward generalization and explainability, our findings can be a\nstrong mile-stone that will enhance text-to-vision models even further.", "published": "2025-03-25 15:30:21", "link": "http://arxiv.org/abs/2503.19948v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Splitting Answer Set Programs with respect to Intensionality Statements (Extended Version)", "abstract": "Splitting a logic program allows us to reduce the task of computing its\nstable models to similar tasks for its subprograms. This can be used to\nincrease solving performance and prove program correctness. We generalize the\nconditions under which this technique is applicable, by considering not only\ndependencies between predicates but also their arguments and context. This\nallows splitting programs commonly used in practice to which previous results\nwere not applicable.", "published": "2025-03-25 15:27:05", "link": "http://arxiv.org/abs/2503.19762v1", "categories": ["cs.AI", "cs.LO"], "primary_category": "cs.AI"}
{"title": "Vanishing Depth: A Depth Adapter with Positional Depth Encoding for Generalized Image Encoders", "abstract": "Generalized metric depth understanding is critical for precise vision-guided\nrobotics, which current state-of-the-art (SOTA) vision-encoders do not support.\nTo address this, we propose Vanishing Depth, a self-supervised training\napproach that extends pretrained RGB encoders to incorporate and align metric\ndepth into their feature embeddings. Based on our novel positional depth\nencoding, we enable stable depth density and depth distribution invariant\nfeature extraction. We achieve performance improvements and SOTA results across\na spectrum of relevant RGBD downstream tasks - without the necessity of\nfinetuning the encoder. Most notably, we achieve 56.05 mIoU on SUN-RGBD\nsegmentation, 88.3 RMSE on Void's depth completion, and 83.8 Top 1 accuracy on\nNYUv2 scene classification. In 6D-object pose estimation, we outperform our\npredecessors of DinoV2, EVA-02, and Omnivore and achieve SOTA results for\nnon-finetuned encoders in several related RGBD downstream tasks.", "published": "2025-03-25 15:19:48", "link": "http://arxiv.org/abs/2503.19947v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "A Survey on Event-driven 3D Reconstruction: Development under Different Categories", "abstract": "Event cameras have gained increasing attention for 3D reconstruction due to\ntheir high temporal resolution, low latency, and high dynamic range. They\ncapture per-pixel brightness changes asynchronously, allowing accurate\nreconstruction under fast motion and challenging lighting conditions. In this\nsurvey, we provide a comprehensive review of event-driven 3D reconstruction\nmethods, including stereo, monocular, and multimodal systems. We further\ncategorize recent developments based on geometric, learning-based, and hybrid\napproaches. Emerging trends, such as neural radiance fields and 3D Gaussian\nsplatting with event data, are also covered. The related works are structured\nchronologically to illustrate the innovations and progression within the field.\nTo support future research, we also highlight key research gaps and future\nresearch directions in dataset, experiment, evaluation, event representation,\netc.", "published": "2025-03-25 15:16:53", "link": "http://arxiv.org/abs/2503.19753v2", "categories": ["cs.GR", "cs.AI", "cs.CV"], "primary_category": "cs.GR"}
{"title": "Inducing Personality in LLM-Based Honeypot Agents: Measuring the Effect on Human-Like Agenda Generation", "abstract": "This paper presents SANDMAN, an architecture for cyber deception that\nleverages Language Agents to emulate convincing human simulacra. Our 'Deceptive\nAgents' serve as advanced cyber decoys, designed for high-fidelity engagement\nwith attackers by extending the observation period of attack behaviours.\nThrough experimentation, measurement, and analysis, we demonstrate how a prompt\nschema based on the five-factor model of personality systematically induces\ndistinct 'personalities' in Large Language Models. Our results highlight the\nfeasibility of persona-driven Language Agents for generating diverse, realistic\nbehaviours, ultimately improving cyber deception strategies.", "published": "2025-03-25 15:16:35", "link": "http://arxiv.org/abs/2503.19752v1", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI"}
{"title": "CamSAM2: Segment Anything Accurately in Camouflaged Videos", "abstract": "Video camouflaged object segmentation (VCOS), aiming at segmenting\ncamouflaged objects that seamlessly blend into their environment, is a\nfundamental vision task with various real-world applications. With the release\nof SAM2, video segmentation has witnessed significant progress. However, SAM2's\ncapability of segmenting camouflaged videos is suboptimal, especially when\ngiven simple prompts such as point and box. To address the problem, we propose\nCamouflaged SAM2 (CamSAM2), which enhances SAM2's ability to handle camouflaged\nscenes without modifying SAM2's parameters. Specifically, we introduce a\ndecamouflaged token to provide the flexibility of feature adjustment for VCOS.\nTo make full use of fine-grained and high-resolution features from the current\nframe and previous frames, we propose implicit object-aware fusion (IOF) and\nexplicit object-aware fusion (EOF) modules, respectively. Object prototype\ngeneration (OPG) is introduced to abstract and memorize object prototypes with\ninformative details using high-quality features from previous frames. Extensive\nexperiments are conducted to validate the effectiveness of our approach. While\nCamSAM2 only adds negligible learnable parameters to SAM2, it substantially\noutperforms SAM2 on three VCOS datasets, especially achieving 12.2 mDice gains\nwith click prompt on MoCA-Mask and 19.6 mDice gains with mask prompt on\nSUN-SEG-Hard, with Hiera-T as the backbone. The code will be available at\nhttps://github.com/zhoustan/CamSAM2.", "published": "2025-03-25 14:58:52", "link": "http://arxiv.org/abs/2503.19730v2", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "On What Depends the Robustness of Multi-source Models to Missing Data in Earth Observation?", "abstract": "In recent years, the development of robust multi-source models has emerged in\nthe Earth Observation (EO) field. These are models that leverage data from\ndiverse sources to improve predictive accuracy when there is missing data.\nDespite these advancements, the factors influencing the varying effectiveness\nof such models remain poorly understood. In this study, we evaluate the\npredictive performance of six state-of-the-art multi-source models in\npredicting scenarios where either a single data source is missing or only a\nsingle source is available. Our analysis reveals that the efficacy of these\nmodels is intricately tied to the nature of the task, the complementarity among\ndata sources, and the model design. Surprisingly, we observe instances where\nthe removal of certain data sources leads to improved predictive performance,\nchallenging the assumption that incorporating all available data is always\nbeneficial. These findings prompt critical reflections on model complexity and\nthe necessity of all collected data sources, potentially shaping the way for\nmore streamlined approaches in EO applications.", "published": "2025-03-25 14:45:23", "link": "http://arxiv.org/abs/2503.19719v1", "categories": ["cs.LG", "cs.AI", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Invertible Koopman neural operator for data-driven modeling of partial differential equations", "abstract": "Koopman operator theory is a popular candidate for data-driven modeling\nbecause it provides a global linearization representation for nonlinear\ndynamical systems. However, existing Koopman operator-based methods suffer from\nshortcomings in constructing the well-behaved observable function and its\ninverse and are inefficient enough when dealing with partial differential\nequations (PDEs). To address these issues, this paper proposes the Invertible\nKoopman Neural Operator (IKNO), a novel data-driven modeling approach inspired\nby the Koopman operator theory and neural operator. IKNO leverages an\nInvertible Neural Network to parameterize observable function and its inverse\nsimultaneously under the same learnable parameters, explicitly guaranteeing the\nreconstruction relation, thus eliminating the dependency on the reconstruction\nloss, which is an essential improvement over the original Koopman Neural\nOperator (KNO). The structured linear matrix inspired by the Koopman operator\ntheory is parameterized to learn the evolution of observables' low-frequency\nmodes in the frequency space rather than directly in the observable space,\nsustaining IKNO is resolution-invariant like other neural operators. Moreover,\nwith preprocessing such as interpolation and dimension expansion, IKNO can be\nextended to operator learning tasks defined on non-Cartesian domains. We fully\nsupport the above claims based on rich numerical and real-world examples and\ndemonstrate the effectiveness of IKNO and superiority over other neural\noperators.", "published": "2025-03-25 14:43:53", "link": "http://arxiv.org/abs/2503.19717v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Decoupled Dynamics Framework with Neural Fields for 3D Spatio-temporal Prediction of Vehicle Collisions", "abstract": "This study proposes a neural framework that predicts 3D vehicle collision\ndynamics by independently modeling global rigid-body motion and local\nstructural deformation. Unlike approaches directly predicting absolute\ndisplacement, this method explicitly separates the vehicle's overall\ntranslation and rotation from its structural deformation. Two specialized\nnetworks form the core of the framework: a quaternion-based Rigid Net for rigid\nmotion and a coordinate-based Deformation Net for local deformation. By\nindependently handling fundamentally distinct physical phenomena, the proposed\narchitecture achieves accurate predictions without requiring separate\nsupervision for each component. The model, trained on only 10% of available\nsimulation data, significantly outperforms baseline models, including single\nmulti-layer perceptron (MLP) and deep operator networks (DeepONet), with\nprediction errors reduced by up to 83%. Extensive validation demonstrates\nstrong generalization to collision conditions outside the training range,\naccurately predicting responses even under severe impacts involving extreme\nvelocities and large impact angles. Furthermore, the framework successfully\nreconstructs high-resolution deformation details from low-resolution inputs\nwithout increased computational effort. Consequently, the proposed approach\nprovides an effective, computationally efficient method for rapid and reliable\nassessment of vehicle safety across complex collision scenarios, substantially\nreducing the required simulation data and time while preserving prediction\nfidelity.", "published": "2025-03-25 14:38:37", "link": "http://arxiv.org/abs/2503.19712v1", "categories": ["cs.CE", "cs.AI"], "primary_category": "cs.CE"}
{"title": "Bootstrap Your Own Views: Masked Ego-Exo Modeling for Fine-grained View-invariant Video Representations", "abstract": "View-invariant representation learning from egocentric (first-person, ego)\nand exocentric (third-person, exo) videos is a promising approach toward\ngeneralizing video understanding systems across multiple viewpoints. However,\nthis area has been underexplored due to the substantial differences in\nperspective, motion patterns, and context between ego and exo views. In this\npaper, we propose a novel masked ego-exo modeling that promotes both causal\ntemporal dynamics and cross-view alignment, called Bootstrap Your Own Views\n(BYOV), for fine-grained view-invariant video representation learning from\nunpaired ego-exo videos. We highlight the importance of capturing the\ncompositional nature of human actions as a basis for robust cross-view\nunderstanding. Specifically, self-view masking and cross-view masking\npredictions are designed to learn view-invariant and powerful representations\nconcurrently. Experimental results demonstrate that our BYOV significantly\nsurpasses existing approaches with notable gains across all metrics in four\ndownstream ego-exo video tasks. The code is available at\nhttps://github.com/park-jungin/byov.", "published": "2025-03-25 14:33:32", "link": "http://arxiv.org/abs/2503.19706v2", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Optimal Path Planning and Cost Minimization for a Drone Delivery System Via Model Predictive Control", "abstract": "In this study, we formulate the drone delivery problem as a control problem\nand solve it using Model Predictive Control. Two experiments are performed: The\nfirst is on a less challenging grid world environment with lower\ndimensionality, and the second is with a higher dimensionality and added\ncomplexity. The MPC method was benchmarked against three popular Multi-Agent\nReinforcement Learning (MARL): Independent $Q$-Learning (IQL), Joint Action\nLearners (JAL), and Value-Decomposition Networks (VDN). It was shown that the\nMPC method solved the problem quicker and required fewer optimal numbers of\ndrones to achieve a minimized cost and navigate the optimal path.", "published": "2025-03-25 14:27:29", "link": "http://arxiv.org/abs/2503.19699v1", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI"}
{"title": "Deep Learning for Speech Emotion Recognition: A CNN Approach Utilizing Mel Spectrograms", "abstract": "This paper explores the application of Convolutional Neural Networks CNNs for\nclassifying emotions in speech through Mel Spectrogram representations of audio\nfiles. Traditional methods such as Gaussian Mixture Models and Hidden Markov\nModels have proven insufficient for practical deployment, prompting a shift\ntowards deep learning techniques. By transforming audio data into a visual\nformat, the CNN model autonomously learns to identify intricate patterns,\nenhancing classification accuracy. The developed model is integrated into a\nuser-friendly graphical interface, facilitating realtime predictions and\npotential applications in educational environments. The study aims to advance\nthe understanding of deep learning in speech emotion recognition, assess the\nmodels feasibility, and contribute to the integration of technology in learning\ncontexts", "published": "2025-03-25 14:02:10", "link": "http://arxiv.org/abs/2503.19677v1", "categories": ["cs.SD", "cs.AI"], "primary_category": "cs.SD"}
{"title": "BiblioPage: A Dataset of Scanned Title Pages for Bibliographic Metadata Extraction", "abstract": "Manual digitization of bibliographic metadata is time consuming and labor\nintensive, especially for historical and real-world archives with highly\nvariable formatting across documents. Despite advances in machine learning, the\nabsence of dedicated datasets for metadata extraction hinders automation. To\naddress this gap, we introduce BiblioPage, a dataset of scanned title pages\nannotated with structured bibliographic metadata. The dataset consists of\napproximately 2,000 monograph title pages collected from 14 Czech libraries,\nspanning a wide range of publication periods, typographic styles, and layout\nstructures. Each title page is annotated with 16 bibliographic attributes,\nincluding title, contributors, and publication metadata, along with precise\npositional information in the form of bounding boxes. To extract structured\ninformation from this dataset, we valuated object detection models such as YOLO\nand DETR combined with transformer-based OCR, achieving a maximum mAP of 52 and\nan F1 score of 59. Additionally, we assess the performance of various visual\nlarge language models, including LlamA 3.2-Vision and GPT-4o, with the best\nmodel reaching an F1 score of 67. BiblioPage serves as a real-world benchmark\nfor bibliographic metadata extraction, contributing to document understanding,\ndocument question answering, and document information extraction. Dataset and\nevaluation scripts are availible at: https://github.com/DCGM/biblio-dataset", "published": "2025-03-25 13:46:55", "link": "http://arxiv.org/abs/2503.19658v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Towards Reliable Time Series Forecasting under Future Uncertainty: Ambiguity and Novelty Rejection Mechanisms", "abstract": "In real-world time series forecasting, uncertainty and lack of reliable\nevaluation pose significant challenges. Notably, forecasting errors often arise\nfrom underfitting in-distribution data and failing to handle\nout-of-distribution inputs. To enhance model reliability, we introduce a dual\nrejection mechanism combining ambiguity and novelty rejection. Ambiguity\nrejection, using prediction error variance, allows the model to abstain under\nlow confidence, assessed through historical error variance analysis without\nfuture ground truth. Novelty rejection, employing Variational Autoencoders and\nMahalanobis distance, detects deviations from training data. This dual approach\nimproves forecasting reliability in dynamic environments by reducing errors and\nadapting to data changes, advancing reliability in complex scenarios.", "published": "2025-03-25 13:44:29", "link": "http://arxiv.org/abs/2503.19656v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "RGB-Th-Bench: A Dense benchmark for Visual-Thermal Understanding of Vision Language Models", "abstract": "We introduce RGB-Th-Bench, the first benchmark designed to evaluate the\nability of Vision-Language Models (VLMs) to comprehend RGB-Thermal image pairs.\nWhile VLMs have demonstrated remarkable progress in visual reasoning and\nmultimodal understanding, their evaluation has been predominantly limited to\nRGB-based benchmarks, leaving a critical gap in assessing their capabilities in\ninfrared vision tasks. Existing visible-infrared datasets are either\ntask-specific or lack high-quality annotations necessary for rigorous model\nevaluation. To address these limitations, RGB-Th-Bench provides a comprehensive\nevaluation framework covering 14 distinct skill dimensions, with a total of\n1,600+ expert-annotated Yes/No questions. The benchmark employs two accuracy\nmetrics: a standard question-level accuracy and a stricter skill-level\naccuracy, which evaluates model robustness across multiple questions within\neach skill dimension. This design ensures a thorough assessment of model\nperformance, including resilience to adversarial and hallucinated responses. We\nconduct extensive evaluations on 19 state-of-the-art VLMs, revealing\nsignificant performance gaps in RGB-Thermal understanding. Our results show\nthat even the strongest models struggle with thermal image comprehension, with\nperformance heavily constrained by their RGB-based capabilities. Additionally,\nthe lack of large-scale application-specific and expert-annotated\nthermal-caption-pair datasets in pre-training is an important reason of the\nobserved performance gap. RGB-Th-Bench highlights the urgent need for further\nadvancements in multimodal learning to bridge the gap between visible and\nthermal image understanding. The dataset is available through this link, and\nthe evaluation code will also be made publicly available.", "published": "2025-03-25 13:43:47", "link": "http://arxiv.org/abs/2503.19654v3", "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV"}
{"title": "OpenSDI: Spotting Diffusion-Generated Images in the Open World", "abstract": "This paper identifies OpenSDI, a challenge for spotting diffusion-generated\nimages in open-world settings. In response to this challenge, we define a new\nbenchmark, the OpenSDI dataset (OpenSDID), which stands out from existing\ndatasets due to its diverse use of large vision-language models that simulate\nopen-world diffusion-based manipulations. Another outstanding feature of\nOpenSDID is its inclusion of both detection and localization tasks for images\nmanipulated globally and locally by diffusion models. To address the OpenSDI\nchallenge, we propose a Synergizing Pretrained Models (SPM) scheme to build up\na mixture of foundation models. This approach exploits a collaboration\nmechanism with multiple pretrained foundation models to enhance generalization\nin the OpenSDI context, moving beyond traditional training by synergizing\nmultiple pretrained models through prompting and attending strategies. Building\non this scheme, we introduce MaskCLIP, an SPM-based model that aligns\nContrastive Language-Image Pre-Training (CLIP) with Masked Autoencoder (MAE).\nExtensive evaluations on OpenSDID show that MaskCLIP significantly outperforms\ncurrent state-of-the-art methods for the OpenSDI challenge, achieving\nremarkable relative improvements of 14.23% in IoU (14.11% in F1) and 2.05% in\naccuracy (2.38% in F1) compared to the second-best model in localization and\ndetection tasks, respectively. Our dataset and code are available at\nhttps://github.com/iamwangyabin/OpenSDI.", "published": "2025-03-25 13:43:16", "link": "http://arxiv.org/abs/2503.19653v2", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Recover from Horcrux: A Spectrogram Augmentation Method for Cardiac Feature Monitoring from Radar Signal Components", "abstract": "Radar-based wellness monitoring is becoming an effective measurement to\nprovide accurate vital signs in a contactless manner, but data scarcity retards\nthe related research on deep-learning-based methods. Data augmentation is\ncommonly used to enrich the dataset by modifying the existing data, but most\naugmentation techniques can only couple with classification tasks. To enable\nthe augmentation for regression tasks, this research proposes a spectrogram\naugmentation method, Horcrux, for radar-based cardiac feature monitoring (e.g.,\nheartbeat detection, electrocardiogram reconstruction) with both classification\nand regression tasks involved. The proposed method is designed to increase the\ndiversity of input samples while the augmented spectrogram is still faithful to\nthe original ground truth vital sign. In addition, Horcrux proposes to inject\nzero values in specific areas to enhance the awareness of the deep learning\nmodel on subtle cardiac features, improving the performance for the limited\ndataset. Experimental result shows that Horcrux achieves an overall improvement\nof 16.20% in cardiac monitoring and has the potential to be extended to other\nspectrogram-based tasks. The code will be released upon publication.", "published": "2025-03-25 13:40:05", "link": "http://arxiv.org/abs/2503.19649v1", "categories": ["eess.SP", "cs.AI"], "primary_category": "eess.SP"}
{"title": "Show or Tell? Effectively prompting Vision-Language Models for semantic segmentation", "abstract": "Large Vision-Language Models (VLMs) are increasingly being regarded as\nfoundation models that can be instructed to solve diverse tasks by prompting,\nwithout task-specific training. We examine the seemingly obvious question: how\nto effectively prompt VLMs for semantic segmentation. To that end, we\nsystematically evaluate the segmentation performance of several recent models\nguided by either text or visual prompts on the out-of-distribution MESS dataset\ncollection. We introduce a scalable prompting scheme, few-shot prompted\nsemantic segmentation, inspired by open-vocabulary segmentation and few-shot\nlearning. It turns out that VLMs lag far behind specialist models trained for a\nspecific segmentation task, by about 30% on average on the\nIntersection-over-Union metric. Moreover, we find that text prompts and visual\nprompts are complementary: each one of the two modes fails on many examples\nthat the other one can solve. Our analysis suggests that being able to\nanticipate the most effective prompt modality can lead to a 11% improvement in\nperformance. Motivated by our findings, we propose PromptMatcher, a remarkably\nsimple training-free baseline that combines both text and visual prompts,\nachieving state-of-the-art results outperforming the best text-prompted VLM by\n2.5%, and the top visual-prompted VLM by 3.5% on few-shot prompted semantic\nsegmentation.", "published": "2025-03-25 13:36:59", "link": "http://arxiv.org/abs/2503.19647v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Analyzable Chain-of-Musical-Thought Prompting for High-Fidelity Music Generation", "abstract": "Autoregressive (AR) models have demonstrated impressive capabilities in\ngenerating high-fidelity music. However, the conventional next-token prediction\nparadigm in AR models does not align with the human creative process in music\ncomposition, potentially compromising the musicality of generated samples. To\novercome this limitation, we introduce MusiCoT, a novel chain-of-thought (CoT)\nprompting technique tailored for music generation. MusiCoT empowers the AR\nmodel to first outline an overall music structure before generating audio\ntokens, thereby enhancing the coherence and creativity of the resulting\ncompositions. By leveraging the contrastive language-audio pretraining (CLAP)\nmodel, we establish a chain of \"musical thoughts\", making MusiCoT scalable and\nindependent of human-labeled data, in contrast to conventional CoT methods.\nMoreover, MusiCoT allows for in-depth analysis of music structure, such as\ninstrumental arrangements, and supports music referencing -- accepting\nvariable-length audio inputs as optional style references. This innovative\napproach effectively addresses copying issues, positioning MusiCoT as a vital\npractical method for music prompting. Our experimental results indicate that\nMusiCoT consistently achieves superior performance across both objective and\nsubjective metrics, producing music quality that rivals state-of-the-art\ngeneration models.\n  Our samples are available at https://MusiCoT.github.io/.", "published": "2025-03-25 12:51:21", "link": "http://arxiv.org/abs/2503.19611v1", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "Enabling Rapid Shared Human-AI Mental Model Alignment via the After-Action Review", "abstract": "In this work, we present two novel contributions toward improving research in\nhuman-machine teaming (HMT): 1) a Minecraft testbed to accelerate testing and\ndeployment of collaborative AI agents and 2) a tool to allow users to revisit\nand analyze behaviors within an HMT episode to facilitate shared mental model\ndevelopment. Our browser-based Minecraft testbed allows for rapid testing of\ncollaborative agents in a continuous-space, real-time, partially-observable\nenvironment with real humans without cumbersome setup typical to human-AI\ninteraction user studies. As Minecraft has an extensive player base and a rich\necosystem of pre-built AI agents, we hope this contribution can help to\nfacilitate research quickly in the design of new collaborative agents and in\nunderstanding different human factors within HMT. Our mental model alignment\ntool facilitates user-led post-mission analysis by including video displays of\nfirst-person perspectives of the team members (i.e., the human and AI) that can\nbe replayed, and a chat interface that leverages GPT-4 to provide answers to\nvarious queries regarding the AI's experiences and model details.", "published": "2025-03-25 12:43:18", "link": "http://arxiv.org/abs/2503.19607v1", "categories": ["cs.HC", "cs.AI"], "primary_category": "cs.HC"}
{"title": "Innate Reasoning is Not Enough: In-Context Learning Enhances Reasoning Large Language Models with Less Overthinking", "abstract": "Recent advances in Large Language Models (LLMs) have introduced Reasoning\nLarge Language Models (RLLMs), which employ extended thinking processes with\nreflection and self-correction capabilities, demonstrating the effectiveness of\ntest-time scaling. RLLMs exhibit innate Chain-of-Thought (CoT) reasoning\ncapability obtained from training, leading to a natural question: \"Is CoT\nprompting, a popular In-Context Learning (ICL) method for chat LLMs, necessary\nto enhance the reasoning capability of RLLMs?\" In this work, we present the\nfirst comprehensive analysis of the impacts of Zero-shot CoT and Few-shot CoT\non RLLMs across mathematical reasoning tasks. We examine models ranging from\n1.5B to 32B parameters, finding that contrary to concerns, CoT prompting\nsignificantly enhances RLLMs' performance in most scenarios. Our results reveal\ndistinct patterns: large-capacity models show minimal improvement on simple\ntasks but substantial gains on complex problems, while smaller models exhibit\nthe opposite behavior. Further analysis demonstrates that CoT prompting\neffectively controls the distribution of the numbers of thinking tokens and\nreasoning steps, reducing excessive reflections by approximately 90% in some\ncases. Moreover, attention logits analysis reveals the RLLMs' overfitting to\nreflection-related words, which is mitigated by external CoT guidance. Notably,\nour experiments indicate that for RLLMs, one-shot CoT consistently yields\nsuperior performance compared to Few-shot CoT approaches. Our findings provide\nimportant insights for optimizing RLLMs' performance through appropriate\nprompting strategies.", "published": "2025-03-25 12:37:22", "link": "http://arxiv.org/abs/2503.19602v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "HoarePrompt: Structural Reasoning About Program Correctness in Natural Language", "abstract": "While software requirements are often expressed in natural language,\nverifying the correctness of a program against natural language requirements is\na hard and underexplored problem. Large language models (LLMs) are promising\ncandidates for addressing this challenge, however our experience shows that\nthey are ineffective in this task, often failing to detect even straightforward\nbugs. To address this gap, we introduce HoarePrompt, a novel approach that\nadapts fundamental ideas from program analysis and verification to natural\nlanguage artifacts. Drawing inspiration from the strongest postcondition\ncalculus, HoarePrompt employs a systematic, step-by-step process in which an\nLLM generates natural language descriptions of reachable program states at\nvarious points in the code. To manage loops, we propose few-shot-driven\nk-induction, an adaptation of the k-induction method widely used in model\nchecking. Once program states are described, HoarePrompt leverages the LLM to\nassess whether the program, annotated with these state descriptions, conforms\nto the natural language requirements. For evaluating the quality of classifiers\nof program correctness with respect to natural language requirements, we\nconstructed CoCoClaNeL, a challenging dataset of solutions to programming\ncompetition problems. Our experiments show that HoarePrompt improves the MCC by\n62% compared to directly using Zero-shot-CoT prompts for correctness\nclassification. Furthermore, HoarePrompt outperforms a classifier that assesses\ncorrectness via LLM-based test generation by increasing the MCC by 93%. The\ninductive reasoning mechanism contributes a 28% boost to MCC, underscoring its\neffectiveness in managing loops.", "published": "2025-03-25 12:30:30", "link": "http://arxiv.org/abs/2503.19599v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE"}
{"title": "Optimizing Breast Cancer Detection in Mammograms: A Comprehensive Study of Transfer Learning, Resolution Reduction, and Multi-View Classification", "abstract": "This study explores open questions in the application of machine learning for\nbreast cancer detection in mammograms. Current approaches often employ a\ntwo-stage transfer learning process: first, adapting a backbone model trained\non natural images to develop a patch classifier, which is then used to create a\nsingle-view whole-image classifier. Additionally, many studies leverage both\nmammographic views to enhance model performance. In this work, we\nsystematically investigate five key questions: (1) Is the intermediate patch\nclassifier essential for optimal performance? (2) Do backbone models that excel\nin natural image classification consistently outperform others on mammograms?\n(3) When reducing mammogram resolution for GPU processing, does the\nlearn-to-resize technique outperform conventional methods? (4) Does\nincorporating both mammographic views in a two-view classifier significantly\nimprove detection accuracy? (5) How do these findings vary when analyzing\nlow-quality versus high-quality mammograms? By addressing these questions, we\ndeveloped models that outperform previous results for both single-view and\ntwo-view classifiers. Our findings provide insights into model architecture and\ntransfer learning strategies contributing to more accurate and efficient\nmammogram analysis.", "published": "2025-03-25 11:51:21", "link": "http://arxiv.org/abs/2503.19945v1", "categories": ["eess.IV", "cs.AI", "cs.CV"], "primary_category": "eess.IV"}
{"title": "FedMM-X: A Trustworthy and Interpretable Framework for Federated Multi-Modal Learning in Dynamic Environments", "abstract": "As artificial intelligence systems increasingly operate in Real-world\nenvironments, the integration of multi-modal data sources such as vision,\nlanguage, and audio presents both unprecedented opportunities and critical\nchallenges for achieving trustworthy intelligence. In this paper, we propose a\nnovel framework that unifies federated learning with explainable multi-modal\nreasoning to ensure trustworthiness in decentralized, dynamic settings. Our\napproach, called FedMM-X (Federated Multi-Modal Explainable Intelligence),\nleverages cross-modal consistency checks, client-level interpretability\nmechanisms, and dynamic trust calibration to address challenges posed by data\nheterogeneity, modality imbalance, and out-of-distribution generalization.\nThrough rigorous evaluation across federated multi-modal benchmarks involving\nvision-language tasks, we demonstrate improved performance in both accuracy and\ninterpretability while reducing vulnerabilities to adversarial and spurious\ncorrelations. Further, we introduce a novel trust score aggregation method to\nquantify global model reliability under dynamic client participation. Our\nfindings pave the way toward developing robust, interpretable, and socially\nresponsible AI systems in Real-world environments.", "published": "2025-03-25 11:28:21", "link": "http://arxiv.org/abs/2503.19564v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "VectorFit : Adaptive Singular & Bias Vector Fine-Tuning of Pre-trained Foundation Models", "abstract": "Popular PEFT methods achieve parameter efficiency by assuming that\nincremental weight updates are inherently low-rank, which often leads to a\nperformance gap compared to full fine-tuning. While recent methods have\nattempted to address this limitation, they typically lack sufficient parameter\nand memory efficiency. We propose VectorFit, an effective and easily deployable\napproach that adaptively trains the singular vectors and biases of pre-trained\nweight matrices. We demonstrate that the utilization of structural and\ntransformational characteristics of pre-trained weights enables high-rank\nupdates comparable to those of full fine-tuning. As a result, VectorFit\nachieves superior performance with 9X less trainable parameters compared to\nstate-of-the-art PEFT methods. Through extensive experiments over 17 datasets\nspanning diverse language and vision tasks such as natural language\nunderstanding and generation, question answering, image classification, and\nimage generation, we exhibit that VectorFit consistently outperforms baselines,\neven in extremely low-budget scenarios.", "published": "2025-03-25 10:36:27", "link": "http://arxiv.org/abs/2503.19530v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Deep Learning-Based Hypoglycemia Classification Across Multiple Prediction Horizons", "abstract": "Type 1 diabetes (T1D) management can be significantly enhanced through the\nuse of predictive machine learning (ML) algorithms, which can mitigate the risk\nof adverse events like hypoglycemia. Hypoglycemia, characterized by blood\nglucose levels below 70 mg/dL, is a life-threatening condition typically caused\nby excessive insulin administration, missed meals, or physical activity. Its\nasymptomatic nature impedes timely intervention, making ML models crucial for\nearly detection. This study integrates short- (up to 2h) and long-term (up to\n24h) prediction horizons (PHs) within a single classification model to enhance\ndecision support. The predicted times are 5-15 min, 15-30 min, 30 min-1h, 1-2h,\n2-4h, 4-8h, 8-12h, and 12-24h before hypoglycemia. In addition, a simplified\nmodel classifying up to 4h before hypoglycemia is compared. We trained ResNet\nand LSTM models on glucose levels, insulin doses, and acceleration data. The\nresults demonstrate the superiority of the LSTM models when classifying nine\nclasses. In particular, subject-specific models yielded better performance but\nachieved high recall only for classes 0, 1, and 2 with 98%, 72%, and 50%,\nrespectively. A population-based six-class model improved the results with at\nleast 60% of events detected. In contrast, longer PHs remain challenging with\nthe current approach and may be considered with different models.", "published": "2025-03-25 10:24:27", "link": "http://arxiv.org/abs/2504.00009v1", "categories": ["q-bio.QM", "cs.AI", "cs.LG", "stat.AP"], "primary_category": "q-bio.QM"}
{"title": "A Spatiotemporal Radar-Based Precipitation Model for Water Level Prediction and Flood Forecasting", "abstract": "Study Region: Goslar and G\\\"ottingen, Lower Saxony, Germany. Study Focus: In\nJuly 2017, the cities of Goslar and G\\\"ottingen experienced severe flood events\ncharacterized by short warning time of only 20 minutes, resulting in extensive\nregional flooding and significant damage. This highlights the critical need for\na more reliable and timely flood forecasting system. This paper presents a\ncomprehensive study on the impact of radar-based precipitation data on\nforecasting river water levels in Goslar. Additionally, the study examines how\nprecipitation influences water level forecasts in G\\\"ottingen. The analysis\nintegrates radar-derived spatiotemporal precipitation patterns with\nhydrological sensor data obtained from ground stations to evaluate the\neffectiveness of this approach in improving flood prediction capabilities. New\nHydrological Insights for the Region: A key innovation in this paper is the use\nof residual-based modeling to address the non-linearity between precipitation\nimages and water levels, leading to a Spatiotemporal Radar-based Precipitation\nModel with residuals (STRPMr). Unlike traditional hydrological models, our\napproach does not rely on upstream data, making it independent of additional\nhydrological inputs. This independence enhances its adaptability and allows for\nbroader applicability in other regions with RADOLAN precipitation. The deep\nlearning architecture integrates (2+1)D convolutional neural networks for\nspatial and temporal feature extraction with LSTM for timeseries forecasting.\nThe results demonstrate the potential of the STRPMr for capturing extreme\nevents and more accurate flood forecasting.", "published": "2025-03-25 10:14:54", "link": "http://arxiv.org/abs/2503.19943v1", "categories": ["eess.IV", "cs.AI", "cs.LG"], "primary_category": "eess.IV"}
{"title": "RoboFlamingo-Plus: Fusion of Depth and RGB Perception with Vision-Language Models for Enhanced Robotic Manipulation", "abstract": "As robotic technologies advancing towards more complex multimodal\ninteractions and manipulation tasks, the integration of advanced\nVision-Language Models (VLMs) has become a key driver in the field. Despite\nprogress with current methods, challenges persist in fusing depth and RGB\ninformation within 3D environments and executing tasks guided by linguistic\ninstructions. In response to these challenges, we have enhanced the existing\nRoboFlamingo framework by introducing RoboFlamingo-Plus, which incorporates\ndepth data into VLMs to significantly improve robotic manipulation performance.\nOur research achieves a nuanced fusion of RGB and depth information by\nintegrating a pre-trained Vision Transformer (ViT) with a resampling technique,\nclosely aligning this combined data with linguistic cues for superior\nmultimodal understanding. The novelty of RoboFlamingo-Plus lies in its\nadaptation of inputs for depth data processing, leveraging a pre-trained\nresampler for depth feature extraction, and employing cross-attention\nmechanisms for optimal feature integration. These improvements allow\nRoboFlamingo-Plus to not only deeply understand 3D environments but also easily\nperform complex, language-guided tasks in challenging settings. Experimental\nresults show that RoboFlamingo-Plus boosts robotic manipulation by 10-20% over\ncurrent methods, marking a significant advancement. Codes and model weights are\npublic at RoboFlamingo-Plus.", "published": "2025-03-25 10:01:57", "link": "http://arxiv.org/abs/2503.19510v1", "categories": ["cs.RO", "cs.AI", "cs.CV"], "primary_category": "cs.RO"}
{"title": "Towards Long-Range ENSO Prediction with an Explainable Deep Learning Model", "abstract": "El Ni\\~no-Southern Oscillation (ENSO) is a prominent mode of interannual\nclimate variability with far-reaching global impacts. Its evolution is governed\nby intricate air-sea interactions, posing significant challenges for long-term\nprediction. In this study, we introduce CTEFNet, a multivariate deep learning\nmodel that synergizes convolutional neural networks and transformers to enhance\nENSO forecasting. By integrating multiple oceanic and atmospheric predictors,\nCTEFNet extends the effective forecast lead time to 20 months while mitigating\nthe impact of the spring predictability barrier, outperforming both dynamical\nmodels and state-of-the-art deep learning approaches. Furthermore, CTEFNet\noffers physically meaningful and statistically significant insights through\ngradient-based sensitivity analysis, revealing the key precursor signals that\ngovern ENSO dynamics, which align with well-established theories and reveal new\ninsights about inter-basin interactions among the Pacific, Atlantic, and Indian\nOceans. The CTEFNet's superior predictive skill and interpretable sensitivity\nassessments underscore its potential for advancing climate prediction. Our\nfindings highlight the importance of multivariate coupling in ENSO evolution\nand demonstrate the promise of deep learning in capturing complex climate\ndynamics with enhanced interpretability.", "published": "2025-03-25 09:50:19", "link": "http://arxiv.org/abs/2503.19502v1", "categories": ["physics.geo-ph", "cs.AI"], "primary_category": "physics.geo-ph"}
{"title": "Pose-Based Fall Detection System: Efficient Monitoring on Standard CPUs", "abstract": "Falls among elderly residents in assisted living homes pose significant\nhealth risks, often leading to injuries and a decreased quality of life.\nCurrent fall detection solutions typically rely on sensor-based systems that\nrequire dedicated hardware, or on video-based models that demand high\ncomputational resources and GPUs for real-time processing. In contrast, this\npaper presents a robust fall detection system that does not require any\nadditional sensors or high-powered hardware. The system uses pose estimation\ntechniques, combined with threshold-based analysis and a voting mechanism, to\neffectively distinguish between fall and non-fall activities. For pose\ndetection, we leverage MediaPipe, a lightweight and efficient framework that\nenables real-time processing on standard CPUs with minimal computational\noverhead. By analyzing motion, body position, and key pose points, the system\nprocesses pose features with a 20-frame buffer, minimizing false positives and\nmaintaining high accuracy even in real-world settings. This unobtrusive,\nresource-efficient approach provides a practical solution for enhancing\nresident safety in old age homes, without the need for expensive sensors or\nhigh-end computational resources.", "published": "2025-03-25 09:49:36", "link": "http://arxiv.org/abs/2503.19501v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "SMT-EX: An Explainable Surrogate Modeling Toolbox for Mixed-Variables Design Exploration", "abstract": "Surrogate models are of high interest for many engineering applications,\nserving as cheap-to-evaluate time-efficient approximations of black-box\nfunctions to help engineers and practitioners make decisions and understand\ncomplex systems. As such, the need for explainability methods is rising and\nmany studies have been performed to facilitate knowledge discovery from\nsurrogate models. To respond to these enquiries, this paper introduces SMT-EX,\nan enhancement of the open-source Python Surrogate Modeling Toolbox (SMT) that\nintegrates explainability techniques into a state-of-the-art surrogate\nmodelling framework. More precisely, SMT-EX includes three key explainability\nmethods: Shapley Additive Explanations, Partial Dependence Plot, and Individual\nConditional Expectations. A peculiar explainability dependency of SMT has been\ndeveloped for such purpose that can be easily activated once the surrogate\nmodel is built, offering a user-friendly and efficient tool for swift insight\nextraction. The effectiveness of SMT-EX is showcased through two test cases.\nThe first case is a 10-variable wing weight problem with purely continuous\nvariables and the second one is a 3-variable mixed-categorical cantilever beam\nbending problem. Relying on SMT-EX analyses for these problems, we demonstrate\nits versatility in addressing a diverse range of problem characteristics.\nSMT-Explainability is freely available on Github:\nhttps://github.com/SMTorg/smt-explainability .", "published": "2025-03-25 09:38:27", "link": "http://arxiv.org/abs/2503.19496v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Body Discovery of Embodied AI", "abstract": "In the pursuit of realizing artificial general intelligence (AGI), the\nimportance of embodied artificial intelligence (AI) becomes increasingly\napparent. Following this trend, research integrating robots with AGI has become\nprominent. As various kinds of embodiments have been designed, adaptability to\ndiverse embodiments will become important to AGI. We introduce a new challenge,\ntermed \"Body Discovery of Embodied AI\", focusing on tasks of recognizing\nembodiments and summarizing neural signal functionality. The challenge\nencompasses the precise definition of an AI body and the intricate task of\nidentifying embodiments in dynamic environments, where conventional approaches\noften prove inadequate. To address these challenges, we apply causal inference\nmethod and evaluate it by developing a simulator tailored for testing\nalgorithms with virtual environments. Finally, we validate the efficacy of our\nalgorithms through empirical testing, demonstrating their robust performance in\nvarious scenarios based on virtual environments.", "published": "2025-03-25 09:21:10", "link": "http://arxiv.org/abs/2503.19941v1", "categories": ["cs.RO", "cs.AI", "cs.NE"], "primary_category": "cs.RO"}
{"title": "A-MESS: Anchor based Multimodal Embedding with Semantic Synchronization for Multimodal Intent Recognition", "abstract": "In the domain of multimodal intent recognition (MIR), the objective is to\nrecognize human intent by integrating a variety of modalities, such as language\ntext, body gestures, and tones. However, existing approaches face difficulties\nadequately capturing the intrinsic connections between the modalities and\noverlooking the corresponding semantic representations of intent. To address\nthese limitations, we present the Anchor-based Multimodal Embedding with\nSemantic Synchronization (A-MESS) framework. We first design an Anchor-based\nMultimodal Embedding (A-ME) module that employs an anchor-based embedding\nfusion mechanism to integrate multimodal inputs. Furthermore, we develop a\nSemantic Synchronization (SS) strategy with the Triplet Contrastive Learning\npipeline, which optimizes the process by synchronizing multimodal\nrepresentation with label descriptions produced by the large language model.\nComprehensive experiments indicate that our A-MESS achieves state-of-the-art\nand provides substantial insight into multimodal representation and downstream\ntasks.", "published": "2025-03-25 09:09:30", "link": "http://arxiv.org/abs/2503.19474v2", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Data-centric Federated Graph Learning with Large Language Models", "abstract": "In federated graph learning (FGL), a complete graph is divided into multiple\nsubgraphs stored in each client due to privacy concerns, and all clients\njointly train a global graph model by only transmitting model parameters. A\npain point of FGL is the heterogeneity problem, where nodes or structures\npresent non-IID properties among clients (e.g., different node label\ndistributions), dramatically undermining the convergence and performance of\nFGL. To address this, existing efforts focus on design strategies at the model\nlevel, i.e., they design models to extract common knowledge to mitigate\nheterogeneity. However, these model-level strategies fail to fundamentally\naddress the heterogeneity problem as the model needs to be designed from\nscratch when transferring to other tasks. Motivated by large language models\n(LLMs) having achieved remarkable success, we aim to utilize LLMs to fully\nunderstand and augment local text-attributed graphs, to address data\nheterogeneity at the data level. In this paper, we propose a general framework\nLLM4FGL that innovatively decomposes the task of LLM for FGL into two sub-tasks\ntheoretically. Specifically, for each client, it first utilizes the LLM to\ngenerate missing neighbors and then infers connections between generated nodes\nand raw nodes. To improve the quality of generated nodes, we design a novel\nfederated generation-and-reflection mechanism for LLMs, without the need to\nmodify the parameters of the LLM but relying solely on the collective feedback\nfrom all clients. After neighbor generation, all the clients utilize a\npre-trained edge predictor to infer the missing edges. Furthermore, our\nframework can seamlessly integrate as a plug-in with existing FGL methods.\nExperiments on three real-world datasets demonstrate the superiority of our\nmethod compared to advanced baselines.", "published": "2025-03-25 08:43:08", "link": "http://arxiv.org/abs/2503.19455v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "VecTrans: LLM Transformation Framework for Better Auto-vectorization on High-performance CPU", "abstract": "Large language models (LLMs) have demonstrated great capabilities in code\ngeneration, yet their effective application in compiler optimizations remains\nan open challenge due to issues such as hallucinations and a lack of\ndomain-specific reasoning. Vectorization, a crucial optimization for enhancing\ncode performance, often fails because of the compiler's inability to recognize\ncomplex code patterns, which commonly require extensive empirical expertise.\nLLMs, with their ability to capture intricate patterns, thus providing a\npromising solution to this challenge. This paper presents VecTrans, a novel\nframework that leverages LLMs to enhance compiler-based code vectorization.\nVecTrans first employs compiler analysis to identify potentially vectorizable\ncode regions. It then utilizes an LLM to refactor these regions into patterns\nthat are more amenable to the compiler's auto-vectorization. To ensure semantic\ncorrectness, VecTrans further integrates a hybrid validation mechanism at the\nintermediate representation (IR) level. With the above efforts, VecTrans\ncombines the adaptability of LLMs with the precision of compiler vectorization,\nthereby effectively opening up the vectorization opportunities. Experimental\nresults show that among all 50 TSVC functions unvectorizable by Clang, GCC, and\nBiShengCompiler, VecTrans successfully vectorizes 23 cases (46%) and achieves\nan average speedup of 2.02x, greatly surpassing state-of-the-art performance.", "published": "2025-03-25 08:39:35", "link": "http://arxiv.org/abs/2503.19449v1", "categories": ["cs.SE", "cs.AI", "cs.LG", "cs.PF"], "primary_category": "cs.SE"}
{"title": "FuXi-RTM: A Physics-Guided Prediction Framework with Radiative Transfer Modeling", "abstract": "Similar to conventional video generation, current deep learning-based weather\nprediction frameworks often lack explicit physical constraints, leading to\nunphysical outputs that limit their reliability for operational forecasting.\nAmong various physical processes requiring proper representation, radiation\nplays a fundamental role as it drives Earth's weather and climate systems.\nHowever, accurate simulation of radiative transfer processes remains\nchallenging for traditional numerical weather prediction (NWP) models due to\ntheir inherent complexity and high computational costs. Here, we propose\nFuXi-RTM, a hybrid physics-guided deep learning framework designed to enhance\nweather forecast accuracy while enforcing physical consistency. FuXi-RTM\nintegrates a primary forecasting model (FuXi) with a fixed deep learning-based\nradiative transfer model (DLRTM) surrogate that efficiently replaces\nconventional radiation parameterization schemes. This represents the first deep\nlearning-based weather forecasting framework to explicitly incorporate physical\nprocess modeling. Evaluated over a comprehensive 5-year dataset, FuXi-RTM\noutperforms its unconstrained counterpart in 88.51% of 3320 variable and lead\ntime combinations, with improvements in radiative flux predictions. By\nincorporating additional physical processes, FuXi-RTM paves the way for\nnext-generation weather forecasting systems that are both accurate and\nphysically consistent.", "published": "2025-03-25 08:21:58", "link": "http://arxiv.org/abs/2503.19940v1", "categories": ["physics.ao-ph", "cs.AI", "cs.LG"], "primary_category": "physics.ao-ph"}
{"title": "Quantifying Symptom Causality in Clinical Decision Making: An Exploration Using CausaLM", "abstract": "Current machine learning approaches to medical diagnosis often rely on\ncorrelational patterns between symptoms and diseases, risking misdiagnoses when\nsymptoms are ambiguous or common across multiple conditions. In this work, we\nmove beyond correlation to investigate the causal influence of key\nsymptoms-specifically \"chest pain\" on diagnostic predictions. Leveraging the\nCausaLM framework, we generate counterfactual text representations in which\ntarget concepts are effectively \"forgotten\" enabling a principled estimation of\nthe causal effect of that concept on a model's predicted disease distribution.\nBy employing Textual Representation-based Average Treatment Effect (TReATE), we\nquantify how the presence or absence of a symptom shapes the model's diagnostic\noutcomes, and contrast these findings against correlation-based baselines such\nas CONEXP. Our results offer deeper insight into the decision-making behavior\nof clinical NLP models and have the potential to inform more trustworthy,\ninterpretable, and causally-grounded decision support tools in medical\npractice.", "published": "2025-03-25 06:59:21", "link": "http://arxiv.org/abs/2503.19394v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "SAFE: Self-Adjustment Federated Learning Framework for Remote Sensing Collaborative Perception", "abstract": "The rapid increase in remote sensing satellites has led to the emergence of\ndistributed space-based observation systems. However, existing distributed\nremote sensing models often rely on centralized training, resulting in data\nleakage, communication overhead, and reduced accuracy due to data distribution\ndiscrepancies across platforms. To address these challenges, we propose the\n\\textit{Self-Adjustment FEderated Learning} (SAFE) framework, which\ninnovatively leverages federated learning to enhance collaborative sensing in\nremote sensing scenarios. SAFE introduces four key strategies: (1)\n\\textit{Class Rectification Optimization}, which autonomously addresses class\nimbalance under unknown local and global distributions. (2) \\textit{Feature\nAlignment Update}, which mitigates Non-IID data issues via locally controlled\nEMA updates. (3) \\textit{Dual-Factor Modulation Rheostat}, which dynamically\nbalances optimization effects during training. (4) \\textit{Adaptive Context\nEnhancement}, which is designed to improve model performance by dynamically\nrefining foreground regions, ensuring computational efficiency with accuracy\nimprovement across distributed satellites. Experiments on real-world image\nclassification and object segmentation datasets validate the effectiveness and\nreliability of the SAFE framework in complex remote sensing scenarios.", "published": "2025-03-25 06:39:34", "link": "http://arxiv.org/abs/2504.03700v1", "categories": ["cs.LG", "cs.AI", "eess.SP"], "primary_category": "cs.LG"}
{"title": "LERO: LLM-driven Evolutionary framework with Hybrid Rewards and Enhanced Observation for Multi-Agent Reinforcement Learning", "abstract": "Multi-agent reinforcement learning (MARL) faces two critical bottlenecks\ndistinct from single-agent RL: credit assignment in cooperative tasks and\npartial observability of environmental states. We propose LERO, a framework\nintegrating Large language models (LLMs) with evolutionary optimization to\naddress these MARL-specific challenges. The solution centers on two\nLLM-generated components: a hybrid reward function that dynamically allocates\nindividual credit through reward decomposition, and an observation enhancement\nfunction that augments partial observations with inferred environmental\ncontext. An evolutionary algorithm optimizes these components through iterative\nMARL training cycles, where top-performing candidates guide subsequent LLM\ngenerations. Evaluations in Multi-Agent Particle Environments (MPE) demonstrate\nLERO's superiority over baseline methods, with improved task performance and\ntraining efficiency.", "published": "2025-03-25 06:28:42", "link": "http://arxiv.org/abs/2503.21807v1", "categories": ["cs.LG", "cs.AI", "cs.MA"], "primary_category": "cs.LG"}
{"title": "Causal invariant geographic network representations with feature and structural distribution shifts", "abstract": "The existing methods learn geographic network representations through deep\ngraph neural networks (GNNs) based on the i.i.d. assumption. However, the\nspatial heterogeneity and temporal dynamics of geographic data make the\nout-of-distribution (OOD) generalisation problem particularly salient. The\nlatter are particularly sensitive to distribution shifts (feature and\nstructural shifts) between testing and training data and are the main causes of\nthe OOD generalisation problem. Spurious correlations are present between\ninvariant and background representations due to selection biases and\nenvironmental effects, resulting in the model extremes being more likely to\nlearn background representations. The existing approaches focus on background\nrepresentation changes that are determined by shifts in the feature\ndistributions of nodes in the training and test data while ignoring changes in\nthe proportional distributions of heterogeneous and homogeneous neighbour\nnodes, which we refer to as structural distribution shifts. We propose a\nfeature-structure mixed invariant representation learning (FSM-IRL) model that\naccounts for both feature distribution shifts and structural distribution\nshifts. To address structural distribution shifts, we introduce a sampling\nmethod based on causal attention, encouraging the model to identify nodes\npossessing strong causal relationships with labels or nodes that are more\nsimilar to the target node. Inspired by the Hilbert-Schmidt independence\ncriterion, we implement a reweighting strategy to maximise the orthogonality of\nthe node representations, thereby mitigating the spurious correlations among\nthe node representations and suppressing the learning of background\nrepresentations. Our experiments demonstrate that FSM-IRL exhibits strong\nlearning capabilities on both geographic and social network datasets in OOD\nscenarios.", "published": "2025-03-25 06:21:57", "link": "http://arxiv.org/abs/2503.19382v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "DeClotH: Decomposable 3D Cloth and Human Body Reconstruction from a Single Image", "abstract": "Most existing methods of 3D clothed human reconstruction from a single image\ntreat the clothed human as a single object without distinguishing between cloth\nand human body. In this regard, we present DeClotH, which separately\nreconstructs 3D cloth and human body from a single image. This task remains\nlargely unexplored due to the extreme occlusion between cloth and the human\nbody, making it challenging to infer accurate geometries and textures.\nMoreover, while recent 3D human reconstruction methods have achieved impressive\nresults using text-to-image diffusion models, directly applying such an\napproach to this problem often leads to incorrect guidance, particularly in\nreconstructing 3D cloth. To address these challenges, we propose two core\ndesigns in our framework. First, to alleviate the occlusion issue, we leverage\n3D template models of cloth and human body as regularizations, which provide\nstrong geometric priors to prevent erroneous reconstruction by the occlusion.\nSecond, we introduce a cloth diffusion model specifically designed to provide\ncontextual information about cloth appearance, thereby enhancing the\nreconstruction of 3D cloth. Qualitative and quantitative experiments\ndemonstrate that our proposed approach is highly effective in reconstructing\nboth 3D cloth and the human body. More qualitative results are provided at\nhttps://hygenie1228.github.io/DeClotH/.", "published": "2025-03-25 06:00:15", "link": "http://arxiv.org/abs/2503.19373v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Flow to Learn: Flow Matching on Neural Network Parameters", "abstract": "Foundational language models show a remarkable ability to learn new concepts\nduring inference via context data. However, similar work for images lag behind.\nTo address this challenge, we introduce FLoWN, a flow matching model that\nlearns to generate neural network parameters for different tasks. Our approach\nmodels the flow on latent space, while conditioning the process on context\ndata. Experiments verify that FLoWN attains various desiderata for a\nmeta-learning model. In addition, it matches or exceeds baselines on\nin-distribution tasks, provides better initializations for classifier training,\nand is performant on out-of-distribution few-shot tasks while having a\nfine-tuning mechanism to improve performance.", "published": "2025-03-25 05:57:50", "link": "http://arxiv.org/abs/2503.19371v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Reinforcing Clinical Decision Support through Multi-Agent Systems and Ethical AI Governance", "abstract": "In the age of data-driven medicine, it is paramount to include explainable\nand ethically managed artificial intelligence in explaining clinical decision\nsupport systems to achieve trustworthy and effective patient care. The focus of\nthis paper is on a new architecture of a multi-agent system for clinical\ndecision support that uses modular agents to analyze laboratory results, vital\nsigns, and the clinical context and then integrates these results to drive\npredictions and validate outcomes. We describe our implementation with the eICU\ndatabase to run lab-analysis-specific agents, vitals-only interpreters, and\ncontextual reasoners and then run the prediction module and a validation agent.\nEverything is a transparent implementation of business logic, influenced by the\nprinciples of ethical AI governance such as Autonomy, Fairness, and\nAccountability. It provides visible results that this agent-based framework not\nonly improves on interpretability and accuracy but also on reinforcing trust in\nAI-assisted decisions in an intensive care setting.", "published": "2025-03-25 05:32:43", "link": "http://arxiv.org/abs/2504.03699v1", "categories": ["cs.AI", "cs.CY", "cs.LG", "cs.MA", "q-bio.QM"], "primary_category": "cs.AI"}
{"title": "Comparison of Metadata Representation Models for Knowledge Graph Embeddings", "abstract": "Hyper-relational Knowledge Graphs (HRKGs) extend traditional KGs beyond\nbinary relations, enabling the representation of contextual, provenance, and\ntemporal information in domains, such as historical events, sensor data, video\ncontent, and narratives. HRKGs can be structured using several Metadata\nRepresentation Models (MRMs), including Reification (REF), Singleton Property\n(SGP), and RDF-star (RDR). However, the effects of different MRMs on KG\nEmbedding (KGE) and Link Prediction (LP) models remain unclear. This study\nevaluates MRMs in the context of LP tasks, identifies the limitations of\nexisting evaluation frameworks, and introduces a new task that ensures fair\ncomparisons across MRMs. Furthermore, we propose a framework that effectively\nreflects the knowledge representations of the three MRMs in latent space.\nExperiments on two types of datasets reveal that REF performs well in simple\nHRKGs, whereas SGP is less effective. However, in complex HRKGs, the\ndifferences among MRMs in the LP tasks are minimal. Our findings contribute to\nan optimal knowledge representation strategy for HRKGs in LP tasks.", "published": "2025-03-25 04:46:23", "link": "http://arxiv.org/abs/2503.21804v2", "categories": ["cs.LG", "cs.AI", "cs.IR", "I.2.4; I.2.7"], "primary_category": "cs.LG"}
{"title": "Tensor Generalized Approximate Message Passing", "abstract": "We propose a tensor generalized approximate message passing (TeG-AMP)\nalgorithm for low-rank tensor inference, which can be used to solve tensor\ncompletion and decomposition problems. We derive TeG-AMP algorithm as an\napproximation of the sum-product belief propagation algorithm in high\ndimensions where the central limit theorem and Taylor series approximations are\napplicable. As TeG-AMP is developed based on a general TR decomposition model,\nit can be directly applied to many low-rank tensor types. Moreover, our TeG-AMP\ncan be simplified based on the CP decomposition model and a tensor simplified\nAMP is proposed for low CP-rank tensor inference problems. Experimental results\ndemonstrate that the proposed methods significantly improve recovery\nperformances since it takes full advantage of tensor structures.", "published": "2025-03-25 04:17:10", "link": "http://arxiv.org/abs/2504.00008v1", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT", "E.4; I.2.0; I.2.6; I.4"], "primary_category": "cs.LG"}
{"title": "Forecasting Volcanic Radiative Power (VPR) at Fuego Volcano Using Bayesian Regularized Neural Network", "abstract": "Forecasting volcanic activity is critical for hazard assessment and risk\nmitigation. Volcanic Radiative Power (VPR), derived from thermal remote sensing\ndata, serves as an essential indicator of volcanic activity. In this study, we\nemploy Bayesian Regularized Neural Networks (BRNN) to predict future VPR values\nbased on historical data from Fuego Volcano, comparing its performance against\nScaled Conjugate Gradient (SCG) and Levenberg-Marquardt (LM) models. The\nresults indicate that BRNN outperforms SCG and LM, achieving the lowest mean\nsquared error (1.77E+16) and the highest R-squared value (0.50), demonstrating\nits superior ability to capture VPR variability while minimizing overfitting.\nDespite these promising results, challenges remain in improving the model's\npredictive accuracy. Future research should focus on integrating additional\ngeophysical parameters, such as seismic and gas emission data, to enhance\nforecasting precision. The findings highlight the potential of machine learning\nmodels, particularly BRNN, in advancing volcanic activity forecasting,\ncontributing to more effective early warning systems for volcanic hazards.", "published": "2025-03-25 04:15:24", "link": "http://arxiv.org/abs/2503.21803v1", "categories": ["cs.LG", "cs.AI", "eess.SP", "physics.ao-ph"], "primary_category": "cs.LG"}
{"title": "Efficient IoT Intrusion Detection with an Improved Attention-Based CNN-BiLSTM Architecture", "abstract": "The ever-increasing security vulnerabilities in the Internet-of-Things (IoT)\nsystems require improved threat detection approaches. This paper presents a\ncompact and efficient approach to detect botnet attacks by employing an\nintegrated approach that consists of traffic pattern analysis, temporal support\nlearning, and focused feature extraction. The proposed attention-based model\nbenefits from a hybrid CNN-BiLSTM architecture and achieves 99% classification\naccuracy in detecting botnet attacks utilizing the N-BaIoT dataset, while\nmaintaining high precision and recall across various scenarios. The proposed\nmodel's performance is further validated by key parameters, such as Mathews\nCorrelation Coefficient and Cohen's kappa Correlation Coefficient. The\nclose-to-ideal results for these parameters demonstrate the proposed model's\nability to detect botnet attacks accurately and efficiently in practical\nsettings and on unseen data. The proposed model proved to be a powerful defense\nmechanism for IoT networks to face emerging security challenges.", "published": "2025-03-25 04:12:14", "link": "http://arxiv.org/abs/2503.19339v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR"}
{"title": "Wavelet-based Global-Local Interaction Network with Cross-Attention for Multi-View Diabetic Retinopathy Detection", "abstract": "Multi-view diabetic retinopathy (DR) detection has recently emerged as a\npromising method to address the issue of incomplete lesions faced by\nsingle-view DR. However, it is still challenging due to the variable sizes and\nscattered locations of lesions. Furthermore, existing multi-view DR methods\ntypically merge multiple views without considering the correlations and\nredundancies of lesion information across them. Therefore, we propose a novel\nmethod to overcome the challenges of difficult lesion information learning and\ninadequate multi-view fusion. Specifically, we introduce a two-branch network\nto obtain both local lesion features and their global dependencies. The\nhigh-frequency component of the wavelet transform is used to exploit lesion\nedge information, which is then enhanced by global semantic to facilitate\ndifficult lesion learning. Additionally, we present a cross-view fusion module\nto improve multi-view fusion and reduce redundancy. Experimental results on\nlarge public datasets demonstrate the effectiveness of our method. The code is\nopen sourced on https://github.com/HuYongting/WGLIN.", "published": "2025-03-25 03:44:57", "link": "http://arxiv.org/abs/2503.19329v1", "categories": ["eess.IV", "cs.AI", "cs.CV"], "primary_category": "eess.IV"}
{"title": "Process or Result? Manipulated Ending Tokens Can Mislead Reasoning LLMs to Ignore the Correct Reasoning Steps", "abstract": "Recent reasoning large language models (LLMs) have demonstrated remarkable\nimprovements in mathematical reasoning capabilities through long\nChain-of-Thought. The reasoning tokens of these models enable self-correction\nwithin reasoning chains, enhancing robustness. This motivates our exploration:\nhow vulnerable are reasoning LLMs to subtle errors in their input reasoning\nchains? We introduce \"Compromising Thought\" (CPT), a vulnerability where models\npresented with reasoning tokens containing manipulated calculation results tend\nto ignore correct reasoning steps and adopt incorrect results instead. Through\nsystematic evaluation across multiple reasoning LLMs, we design three\nincreasingly explicit prompting methods to measure CPT resistance, revealing\nthat models struggle significantly to identify and correct these manipulations.\nNotably, contrary to existing research suggesting structural alterations affect\nmodel performance more than content modifications, we find that local ending\ntoken manipulations have greater impact on reasoning outcomes than structural\nchanges. Moreover, we discover a security vulnerability in DeepSeek-R1 where\ntampered reasoning tokens can trigger complete reasoning cessation. Our work\nenhances understanding of reasoning robustness and highlights security\nconsiderations for reasoning-intensive applications.", "published": "2025-03-25 03:43:11", "link": "http://arxiv.org/abs/2503.19326v2", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "LRSCLIP: A Vision-Language Foundation Model for Aligning Remote Sensing Image with Longer Text", "abstract": "This study addresses the technical bottlenecks in handling long text and the\n\"hallucination\" issue caused by insufficient short text information in remote\nsensing vision-language foundation models (VLFM). We propose a novel\nvision-language foundation model, LRSCLIP, and a multimodal dataset, LRS2M. The\nmain contributions are as follows: (1) By integrating multi-source remote\nsensing data and adopting a large language model labeling strategy, we\nconstruct the LRS2M dataset, which contains 2 million image-text pairs,\nproviding both short and long texts for the first time, thus solving the\nproblem of semantic granularity limitations in existing datasets; (2) The\ndesign of the LRSCLIP architecture based on Long-CLIP's KPS module, which\nextends CLIP's text processing capacity and achieves fine-grained cross-modal\nfeature alignment through a dual-text loss weighting mechanism. Experimental\nresults show that LRSCLIP improves retrieval accuracy by 10\\%-20\\% over the\nLong-CLIP baseline in the zero-shot long-text cross-modal retrieval task. For\nthe zero-shot short-text cross-modal retrieval task, LRSCLIP achieves\nimprovements over the current best model, GeoRSCLIP, with increases of 0.17\\%,\n0.67\\%, and 0.92\\% in Text to Image R@1, Image to Text R@1, and mR on RSITMD,\nrespectively, and 0.04\\%, 2.93\\%, and 1.28\\% on RSICD. In the zero-shot image\nclassification task (average accuracy=75.75\\%) and semantic localization task\n(Rmi=0.7653), LRSCLIP achieves state-of-the-art performance. These results\nvalidate the dual advantages of fine-grained semantic understanding and global\nfeature matching in LRSCLIP. This work provides a new benchmark model and data\nsupport for remote sensing multimodal learning. The related code has been open\nsource and is available at https://github.com/MitsuiChen14/LRSCLIP.", "published": "2025-03-25 03:17:42", "link": "http://arxiv.org/abs/2503.19311v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Observation Adaptation via Annealed Importance Resampling for Partially Observable Markov Decision Processes", "abstract": "Partially observable Markov decision processes (POMDPs) are a general\nmathematical model for sequential decision-making in stochastic environments\nunder state uncertainty. POMDPs are often solved \\textit{online}, which enables\nthe algorithm to adapt to new information in real time. Online solvers\ntypically use bootstrap particle filters based on importance resampling for\nupdating the belief distribution. Since directly sampling from the ideal state\ndistribution given the latest observation and previous state is infeasible,\nparticle filters approximate the posterior belief distribution by propagating\nstates and adjusting weights through prediction and resampling steps. However,\nin practice, the importance resampling technique often leads to particle\ndegeneracy and sample impoverishment when the state transition model poorly\naligns with the posterior belief distribution, especially when the received\nobservation is highly informative. We propose an approach that constructs a\nsequence of bridge distributions between the state-transition and optimal\ndistributions through iterative Monte Carlo steps, better accommodating noisy\nobservations in online POMDP solvers. Our algorithm demonstrates significantly\nsuperior performance compared to state-of-the-art methods when evaluated across\nmultiple challenging POMDP domains.", "published": "2025-03-25 03:05:00", "link": "http://arxiv.org/abs/2503.19302v1", "categories": ["cs.AI", "cs.LG", "cs.RO"], "primary_category": "cs.AI"}
{"title": "Adaptive Wavelet Filters as Practical Texture Feature Amplifiers for Parkinson's Disease Screening in OCT", "abstract": "Parkinson's disease (PD) is a prevalent neurodegenerative disorder globally.\nThe eye's retina is an extension of the brain and has great potential in PD\nscreening. Recent studies have suggested that texture features extracted from\nretinal layers can be adopted as biomarkers for PD diagnosis under optical\ncoherence tomography (OCT) images. Frequency domain learning techniques can\nenhance the feature representations of deep neural networks (DNNs) by\ndecomposing frequency components involving rich texture features. Additionally,\nprevious works have not exploited texture features for automated PD screening\nin OCT. Motivated by the above analysis, we propose a novel Adaptive Wavelet\nFilter (AWF) that serves as the Practical Texture Feature Amplifier to fully\nleverage the merits of texture features to boost the PD screening performance\nof DNNs with the aid of frequency domain learning. Specifically, AWF first\nenhances texture feature representation diversities via channel mixer, then\nemphasizes informative texture feature representations with the well-designed\nadaptive wavelet filtering token mixer. By combining the AWFs with the DNN\nstem, AWFNet is constructed for automated PD screening. Additionally, we\nintroduce a novel Balanced Confidence (BC) Loss by mining the potential of\nsample-wise predicted probabilities of all classes and class frequency prior,\nto further boost the PD screening performance and trustworthiness of AWFNet.\nThe extensive experiments manifest the superiority of our AWFNet and BC over\nstate-of-the-art methods in terms of PD screening performance and\ntrustworthiness.", "published": "2025-03-25 02:47:24", "link": "http://arxiv.org/abs/2503.19292v1", "categories": ["eess.IV", "cs.AI", "cs.CV"], "primary_category": "eess.IV"}
{"title": "No Black Box Anymore: Demystifying Clinical Predictive Modeling with Temporal-Feature Cross Attention Mechanism", "abstract": "Despite the outstanding performance of deep learning models in clinical\nprediction tasks, explainability remains a significant challenge. Inspired by\ntransformer architectures, we introduce the Temporal-Feature Cross Attention\nMechanism (TFCAM), a novel deep learning framework designed to capture dynamic\ninteractions among clinical features across time, enhancing both predictive\naccuracy and interpretability. In an experiment with 1,422 patients with\nChronic Kidney Disease, predicting progression to End-Stage Renal Disease,\nTFCAM outperformed LSTM and RETAIN baselines, achieving an AUROC of 0.95 and an\nF1-score of 0.69. Beyond performance gains, TFCAM provides multi-level\nexplainability by identifying critical temporal periods, ranking feature\nimportance, and quantifying how features influence each other across time\nbefore affecting predictions. Our approach addresses the \"black box\"\nlimitations of deep learning in healthcare, offering clinicians transparent\ninsights into disease progression mechanisms while maintaining state-of-the-art\npredictive performance.", "published": "2025-03-25 02:35:08", "link": "http://arxiv.org/abs/2503.19285v2", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "CubeRobot: Grounding Language in Rubik's Cube Manipulation via Vision-Language Model", "abstract": "Proving Rubik's Cube theorems at the high level represents a notable\nmilestone in human-level spatial imagination and logic thinking and reasoning.\nTraditional Rubik's Cube robots, relying on complex vision systems and fixed\nalgorithms, often struggle to adapt to complex and dynamic scenarios. To\novercome this limitation, we introduce CubeRobot, a novel vision-language model\n(VLM) tailored for solving 3x3 Rubik's Cubes, empowering embodied agents with\nmultimodal understanding and execution capabilities. We used the CubeCoT image\ndataset, which contains multiple-level tasks (43 subtasks in total) that humans\nare unable to handle, encompassing various cube states. We incorporate a\ndual-loop VisionCoT architecture and Memory Stream, a paradigm for extracting\ntask-related features from VLM-generated planning queries, thus enabling\nCubeRobot to independent planning, decision-making, reflection and separate\nmanagement of high- and low-level Rubik's Cube tasks. Furthermore, in low-level\nRubik's Cube restoration tasks, CubeRobot achieved a high accuracy rate of\n100%, similar to 100% in medium-level tasks, and achieved an accuracy rate of\n80% in high-level tasks.", "published": "2025-03-25 02:23:47", "link": "http://arxiv.org/abs/2503.19281v1", "categories": ["cs.RO", "cs.AI"], "primary_category": "cs.RO"}
{"title": "LogicLearner: A Tool for the Guided Practice of Propositional Logic Proofs", "abstract": "The study of propositional logic -- fundamental to the theory of computing --\nis a cornerstone of the undergraduate computer science curriculum. Learning to\nsolve logical proofs requires repeated guided practice, but undergraduate\nstudents often lack access to on-demand tutoring in a judgment-free\nenvironment. In this work, we highlight the need for guided practice tools in\nundergraduate mathematics education and outline the desiderata of an effective\npractice tool. We accordingly develop LogicLearner, a web application for\nguided logic proof practice. LogicLearner consists of an interface to attempt\nlogic proofs step-by-step and an automated proof solver to generate solutions\non the fly, allowing users to request guidance as needed. We pilot LogicLearner\nas a practice tool in two semesters of an undergraduate discrete mathematics\ncourse and receive strongly positive feedback for usability and pedagogical\nvalue in student surveys. To the best of our knowledge, LogicLearner is the\nonly learning tool that provides an end-to-end practice environment for logic\nproofs with immediate, judgment-free feedback.", "published": "2025-03-25 02:23:08", "link": "http://arxiv.org/abs/2503.19280v1", "categories": ["cs.DM", "cs.AI", "cs.HC"], "primary_category": "cs.DM"}
{"title": "Context-Aware Semantic Segmentation: Enhancing Pixel-Level Understanding with Large Language Models for Advanced Vision Applications", "abstract": "Semantic segmentation has made significant strides in pixel-level image\nunderstanding, yet it remains limited in capturing contextual and semantic\nrelationships between objects. Current models, such as CNN and\nTransformer-based architectures, excel at identifying pixel-level features but\nfail to distinguish semantically similar objects (e.g., \"doctor\" vs. \"nurse\" in\na hospital scene) or understand complex contextual scenarios (e.g.,\ndifferentiating a running child from a regular pedestrian in autonomous\ndriving). To address these limitations, we proposed a novel Context-Aware\nSemantic Segmentation framework that integrates Large Language Models (LLMs)\nwith state-of-the-art vision backbones. Our hybrid model leverages the Swin\nTransformer for robust visual feature extraction and GPT-4 for enriching\nsemantic understanding through text embeddings. A Cross-Attention Mechanism is\nintroduced to align vision and language features, enabling the model to reason\nabout context more effectively. Additionally, Graph Neural Networks (GNNs) are\nemployed to model object relationships within the scene, capturing dependencies\nthat are overlooked by traditional models. Experimental results on benchmark\ndatasets (e.g., COCO, Cityscapes) demonstrate that our approach outperforms the\nexisting methods in both pixel-level accuracy (mIoU) and contextual\nunderstanding (mAP). This work bridges the gap between vision and language,\npaving the path for more intelligent and context-aware vision systems in\napplications including autonomous driving, medical imaging, and robotics.", "published": "2025-03-25 02:12:35", "link": "http://arxiv.org/abs/2503.19276v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Reverse Prompt: Cracking the Recipe Inside Text-to-Image Generation", "abstract": "Text-to-image generation has become increasingly popular, but achieving the\ndesired images often requires extensive prompt engineering. In this paper, we\nexplore how to decode textual prompts from reference images, a process we refer\nto as image reverse prompt engineering. This technique enables us to gain\ninsights from reference images, understand the creative processes of great\nartists, and generate impressive new images. To address this challenge, we\npropose a method known as automatic reverse prompt optimization (ARPO).\nSpecifically, our method refines an initial prompt into a high-quality prompt\nthrough an iteratively imitative gradient prompt optimization process: 1)\ngenerating a recreated image from the current prompt to instantiate its\nguidance capability; 2) producing textual gradients, which are candidate\nprompts intended to reduce the difference between the recreated image and the\nreference image; 3) updating the current prompt with textual gradients using a\ngreedy search method to maximize the CLIP similarity between prompt and\nreference image. We compare ARPO with several baseline methods, including\nhandcrafted techniques, gradient-based prompt tuning methods, image captioning,\nand data-driven selection method. Both quantitative and qualitative results\ndemonstrate that our ARPO converges quickly to generate high-quality reverse\nprompts. More importantly, we can easily create novel images with diverse\nstyles and content by directly editing these reverse prompts. Code will be made\npublicly available.", "published": "2025-03-25 02:08:05", "link": "http://arxiv.org/abs/2503.19937v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "NeoRL-2: Near Real-World Benchmarks for Offline Reinforcement Learning with Extended Realistic Scenarios", "abstract": "Offline reinforcement learning (RL) aims to learn from historical data\nwithout requiring (costly) access to the environment. To facilitate offline RL\nresearch, we previously introduced NeoRL, which highlighted that datasets from\nreal-world tasks are often conservative and limited. With years of experience\napplying offline RL to various domains, we have identified additional\nreal-world challenges. These include extremely conservative data distributions\nproduced by deployed control systems, delayed action effects caused by\nhigh-latency transitions, external factors arising from the uncontrollable\nvariance of transitions, and global safety constraints that are difficult to\nevaluate during the decision-making process. These challenges are\nunderrepresented in previous benchmarks but frequently occur in real-world\ntasks. To address this, we constructed the extended Near Real-World Offline RL\nBenchmark (NeoRL-2), which consists of 7 datasets from 7 simulated tasks along\nwith their corresponding evaluation simulators. Benchmarking results from\nstate-of-the-art offline RL approaches demonstrate that current methods often\nstruggle to outperform the data-collection behavior policy, highlighting the\nneed for more effective methods. We hope NeoRL-2 will accelerate the\ndevelopment of reinforcement learning algorithms for real-world applications.\nThe benchmark project page is available at https://github.com/polixir/NeoRL2.", "published": "2025-03-25 02:01:54", "link": "http://arxiv.org/abs/2503.19267v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Face Spoofing Detection using Deep Learning", "abstract": "Digital image spoofing has emerged as a significant security threat in\nbiometric authentication systems, particularly those relying on facial\nrecognition. This study evaluates the performance of three vision based models,\nMobileNetV2, ResNET50, and Vision Transformer, ViT, for spoof detection in\nimage classification, utilizing a dataset of 150,986 images divided into\ntraining , 140,002, testing, 10,984, and validation ,39,574, sets. Spoof\ndetection is critical for enhancing the security of image recognition systems,\nand this research compares the models effectiveness through accuracy,\nprecision, recall, and F1 score metrics. Results reveal that MobileNetV2\noutperforms other architectures on the test dataset, achieving an accuracy of\n91.59%, precision of 91.72%, recall of 91.59%, and F1 score of 91.58%, compared\nto ViT 86.54%, 88.28%, 86.54%, and 86.39%, respectively. On the validation\ndataset, MobileNetV2, and ViT excel, with MobileNetV2 slightly ahead at 97.17%\naccuracy versus ViT 96.36%. MobileNetV2 demonstrates faster convergence during\ntraining and superior generalization to unseen data, despite both models\nshowing signs of overfitting. These findings highlight MobileNetV2 balanced\nperformance and robustness, making it the preferred choice for spoof detection\napplications where reliability on new data is essential. The study underscores\nthe importance of model selection in security sensitive contexts and suggests\nMobileNetV2 as a practical solution for real world deployment.", "published": "2025-03-25 00:09:21", "link": "http://arxiv.org/abs/2503.19223v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "A Tight Meta-theorem for LOCAL Certification of MSO$_2$ Properties within Bounded Treewidth Graphs", "abstract": "Distributed networks are prone to errors so verifying their output is\ncritical. Hence, we develop LOCAL certification protocols for graph properties\nin which nodes are given certificates that allow them to check whether their\nnetwork as a whole satisfies some fixed property while only communicating with\ntheir local network. Most known LOCAL certification protocols are specifically\ntailored to the problem they work on and cannot be translated more generally.\nThus we target general protocols that can certify any property expressible\nwithin a certain logical framework. We consider Monadic Second Order Logic\n(MSO$_2$), a powerful framework that can express properties such as\nnon-$k$-colorability, Hamiltonicity, and $H$-minor-freeness. Unfortunately, in\ngeneral, there are MSO$_2$-expressible properties that cannot be certified\nwithout huge certificates. For instance, non-3-colorability requires\ncertificates of size $\\Omega(n^2/\\log n)$ on general $n$-vertex graphs\n(G\\\"o\\\"os, Suomela 2016). Hence, we impose additional structural restrictions\non the graph.\n  We provide a LOCAL certification protocol for certifying any\nMSO$_2$-expressible property on graphs of bounded treewidth and, consequently,\na LOCAL certification protocol for certifying bounded treewidth. That is for\neach integer $k$ and each MSO$_2$-expressible property $\\Pi$ we give a LOCAL\nCertification protocol to certify that a graph satisfies $\\Pi$ and has\ntreewidth at most $k$ using certificates of size $\\mathcal{O}(\\log n)$ (which\nis asymptotically optimal). Our LOCAL certification protocol requires only one\nround of distributed communication, hence it is also proof-labeling scheme.\n  Our result improves upon work by Fraigniaud, Montealegre, Rapaport, and\nTodinca (Algorithmica 2024), Bousquet, Feuilloley, Pierron (PODC 2022), and the\nvery recent work of Baterisna and Chang.", "published": "2025-03-25 13:58:08", "link": "http://arxiv.org/abs/2503.19671v1", "categories": ["cs.DC", "cs.DM", "cs.DS", "68R10", "F.2.2"], "primary_category": "cs.DC"}
{"title": "The $g$-good-neighbor diagnosability of product networks under the PMC model", "abstract": "The concept of neighbor connectivity originated from the assessment of the\nsubversion of espionage networks caused by underground resistance movements,\nand it has now been applied to measure the disruption of networks caused by\ncascading failures through neighbors. In this paper, we give two necessary and\nsufficient conditions of the existance of $g$-good-neighbor diagnosability. We\nintroduce a new concept called $g$-good neighbor cut-component number (gc\nnumber for short), which has close relation with $g$-good-neighbor\ndiagnosability. Sharp lower and upper bounds of the gc number of general graphs\nin terms of the $g$-good neighbor connectivity is given, which provides a\nformula to compute the $g$-good-neighbor diagnosability for general graphs\n(therefore for Cartesian product graphs). As their applications, we get the\nexact values or bounds for the gc numbers and $g$-good-neighbor diagnosability\nof grid, torus networks and generalized cubes.", "published": "2025-03-25 08:53:01", "link": "http://arxiv.org/abs/2503.19463v1", "categories": ["cs.DM", "math.CO"], "primary_category": "cs.DM"}
{"title": "CoLLM: A Large Language Model for Composed Image Retrieval", "abstract": "Composed Image Retrieval (CIR) is a complex task that aims to retrieve images\nbased on a multimodal query. Typical training data consists of triplets\ncontaining a reference image, a textual description of desired modifications,\nand the target image, which are expensive and time-consuming to acquire. The\nscarcity of CIR datasets has led to zero-shot approaches utilizing synthetic\ntriplets or leveraging vision-language models (VLMs) with ubiquitous\nweb-crawled image-caption pairs. However, these methods have significant\nlimitations: synthetic triplets suffer from limited scale, lack of diversity,\nand unnatural modification text, while image-caption pairs hinder joint\nembedding learning of the multimodal query due to the absence of triplet data.\nMoreover, existing approaches struggle with complex and nuanced modification\ntexts that demand sophisticated fusion and understanding of vision and language\nmodalities. We present CoLLM, a one-stop framework that effectively addresses\nthese limitations. Our approach generates triplets on-the-fly from\nimage-caption pairs, enabling supervised training without manual annotation. We\nleverage Large Language Models (LLMs) to generate joint embeddings of reference\nimages and modification texts, facilitating deeper multimodal fusion.\nAdditionally, we introduce Multi-Text CIR (MTCIR), a large-scale dataset\ncomprising 3.4M samples, and refine existing CIR benchmarks (CIRR and\nFashion-IQ) to enhance evaluation reliability. Experimental results demonstrate\nthat CoLLM achieves state-of-the-art performance across multiple CIR benchmarks\nand settings. MTCIR yields competitive results, with up to 15% performance\nimprovement. Our refined benchmarks provide more reliable evaluation metrics\nfor CIR models, contributing to the advancement of this important field.", "published": "2025-03-25 17:59:50", "link": "http://arxiv.org/abs/2503.19910v1", "categories": ["cs.CV", "cs.IR"], "primary_category": "cs.CV"}
{"title": "How Generative IR Retrieves Documents Mechanistically", "abstract": "Generative Information Retrieval (GenIR) is a novel paradigm in which a\ntransformer encoder-decoder model predicts document rankings based on a query\nin an end-to-end fashion. These GenIR models have received significant\nattention due to their simple retrieval architecture while maintaining high\nretrieval effectiveness. However, in contrast to established retrieval\narchitectures like cross-encoders or bi-encoders, their internal computations\nremain largely unknown. Therefore, this work studies the internal retrieval\nprocess of GenIR models by applying methods based on mechanistic\ninterpretability, such as patching and vocabulary projections. By replacing the\nGenIR encoder with one trained on fewer documents, we demonstrate that the\ndecoder is the primary component responsible for successful retrieval. Our\npatching experiments reveal that not all components in the decoder are crucial\nfor the retrieval process. More specifically, we find that a pass through the\ndecoder can be divided into three stages: (I) the priming stage, which\ncontributes important information for activating subsequent components in later\nlayers; (II) the bridging stage, where cross-attention is primarily active to\ntransfer query information from the encoder to the decoder; and (III) the\ninteraction stage, where predominantly MLPs are active to predict the document\nidentifier. Our findings indicate that interaction between query and document\ninformation occurs only in the last stage. We hope our results promote a better\nunderstanding of GenIR models and foster future research to overcome the\ncurrent challenges associated with these models.", "published": "2025-03-25 14:41:17", "link": "http://arxiv.org/abs/2503.19715v1", "categories": ["cs.IR", "H.3.3"], "primary_category": "cs.IR"}
{"title": "Beyond Relevance: An Adaptive Exploration-Based Framework for Personalized Recommendations", "abstract": "Recommender systems must balance personalization, diversity, and robustness\nto cold-start scenarios to remain effective in dynamic content environments.\nThis paper introduces an adaptive, exploration-based recommendation framework\nthat adjusts to evolving user preferences and content distributions to promote\ndiversity and novelty without compromising relevance. The system represents\nitems using sentence-transformer embeddings and organizes them into\nsemantically coherent clusters through an online algorithm with adaptive\nthresholding. A user-controlled exploration mechanism enhances diversity by\nselectively sampling from under-explored clusters. Experiments on the MovieLens\ndataset show that enabling exploration reduces intra-list similarity from 0.34\nto 0.26 and increases unexpectedness to 0.73, outperforming collaborative\nfiltering and popularity-based baselines. A/B testing with 300 simulated users\nreveals a strong link between interaction history and preference for diversity,\nwith 72.7% of long-term users favoring exploratory recommendations.\nComputational analysis confirms that clustering and recommendation processes\nscale linearly with the number of clusters. These results demonstrate that\nadaptive exploration effectively mitigates over-specialization while preserving\npersonalization and efficiency.", "published": "2025-03-25 10:27:32", "link": "http://arxiv.org/abs/2503.19525v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Enhanced Bloom's Educational Taxonomy for Fostering Information Literacy in the Era of Large Language Models", "abstract": "The advent of Large Language Models (LLMs) has profoundly transformed the\nparadigms of information retrieval and problem-solving, enabling students to\naccess information acquisition more efficiently to support learning. However,\nthere is currently a lack of standardized evaluation frameworks that guide\nlearners in effectively leveraging LLMs. This paper proposes an LLM-driven\nBloom's Educational Taxonomy that aims to recognize and evaluate students'\ninformation literacy (IL) with LLMs, and to formalize and guide students\npractice-based activities of using LLMs to solve complex problems. The\nframework delineates the IL corresponding to the cognitive abilities required\nto use LLM into two distinct stages: Exploration & Action and Creation &\nMetacognition. It further subdivides these into seven phases: Perceiving,\nSearching, Reasoning, Interacting, Evaluating, Organizing, and Curating.\nThrough the case presentation, the analysis demonstrates the framework's\napplicability and feasibility, supporting its role in fostering IL among\nstudents with varying levels of prior knowledge. This framework fills the\nexisting gap in the analysis of LLM usage frameworks and provides theoretical\nsupport for guiding learners to improve IL.", "published": "2025-03-25 08:23:49", "link": "http://arxiv.org/abs/2503.19434v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "RGL: A Graph-Centric, Modular Framework for Efficient Retrieval-Augmented Generation on Graphs", "abstract": "Recent advances in graph learning have paved the way for innovative\nretrieval-augmented generation (RAG) systems that leverage the inherent\nrelational structures in graph data. However, many existing approaches suffer\nfrom rigid, fixed settings and significant engineering overhead, limiting their\nadaptability and scalability. Additionally, the RAG community has largely\noverlooked the decades of research in the graph database community regarding\nthe efficient retrieval of interesting substructures on large-scale graphs. In\nthis work, we introduce the RAG-on-Graphs Library (RGL), a modular framework\nthat seamlessly integrates the complete RAG pipeline-from efficient graph\nindexing and dynamic node retrieval to subgraph construction, tokenization, and\nfinal generation-into a unified system. RGL addresses key challenges by\nsupporting a variety of graph formats and integrating optimized implementations\nfor essential components, achieving speedups of up to 143x compared to\nconventional methods. Moreover, its flexible utilities, such as dynamic node\nfiltering, allow for rapid extraction of pertinent subgraphs while reducing\ntoken consumption. Our extensive evaluations demonstrate that RGL not only\naccelerates the prototyping process but also enhances the performance and\napplicability of graph-based RAG systems across a range of tasks.", "published": "2025-03-25 03:21:48", "link": "http://arxiv.org/abs/2503.19314v1", "categories": ["cs.IR", "cs.LG"], "primary_category": "cs.IR"}
{"title": "The problem of infinite information flow", "abstract": "We study conditional mutual information (cMI) between a pair of variables\n$X,Y$ given a third one $Z$ and derived quantities including transfer entropy\n(TE) and causation entropy (CE) in the dynamically relevant context where\n$X=T(Y,Z)$ is determined by $Y,Z$ via a deterministic transformation $T$. Under\nmild continuity assumptions on their distributions, we prove a zero-infinity\ndichotomy for cMI for a wide class of $T$, which gives a yes-or-no answer to\nthe question of information flow as quantified by TE or CE. Such an answer\nfails to distinguish between the relative amounts of information flow. To\nresolve this problem, we propose a discretization strategy and a conjectured\nformula to discern the \\textit{relative ambiguities} of the system, which can\nserve as a reliable proxy for the relative amounts of information flow. We\nillustrate and validate this approach with numerical evidence.", "published": "2025-03-25 19:32:09", "link": "http://arxiv.org/abs/2503.20035v1", "categories": ["math.DS", "cs.IT", "math.IT", "94A17, 37C30", "H.1.1"], "primary_category": "math.DS"}
{"title": "RCC-PFL: Robust Client Clustering under Noisy Labels in Personalized Federated Learning", "abstract": "We address the problem of cluster identity estimation in a personalized\nfederated learning (PFL) setting in which users aim to learn different personal\nmodels. The backbone of effective learning in such a setting is to cluster\nusers into groups whose objectives are similar. A typical approach in the\nliterature is to achieve this by training users' data on different proposed\npersonal models and assign them to groups based on which model achieves the\nlowest value of the users' loss functions. This process is to be done\niteratively until group identities converge. A key challenge in such a setting\narises when users have noisy labeled data, which may produce misleading values\nof their loss functions, and hence lead to ineffective clustering. To overcome\nthis challenge, we propose a label-agnostic data similarity-based clustering\nalgorithm, coined RCC-PFL, with three main advantages: the cluster identity\nestimation procedure is independent from the training labels; it is a one-shot\nclustering algorithm performed prior to the training; and it requires fewer\ncommunication rounds and less computation compared to iterative-based\nclustering methods. We validate our proposed algorithm using various models and\ndatasets and show that it outperforms multiple baselines in terms of average\naccuracy and variance reduction.", "published": "2025-03-25 17:50:54", "link": "http://arxiv.org/abs/2503.19886v1", "categories": ["cs.LG", "cs.DC", "cs.IT", "cs.NI", "eess.SP", "math.IT"], "primary_category": "cs.LG"}
{"title": "Perception-Enhanced Multitask Multimodal Semantic Communication for UAV-Assisted Integrated Sensing and Communication System", "abstract": "Recent advances in integrated sensing and communication (ISAC) unmanned\naerial vehicles (UAVs) have enabled their widespread deployment in critical\napplications such as emergency management. This paper investigates the\nchallenge of efficient multitask multimodal data communication in UAV-assisted\nISAC systems, in the considered system model, hyperspectral (HSI) and LiDAR\ndata are collected by UAV-mounted sensors for both target classification and\ndata reconstruction at the terrestrial BS. The limited channel capacity and\ncomplex environmental conditions pose significant challenges to effective\nair-to-ground communication. To tackle this issue, we propose a\nperception-enhanced multitask multimodal semantic communication (PE-MMSC)\nsystem that strategically leverages the onboard computational and sensing\ncapabilities of UAVs. In particular, we first propose a robust multimodal\nfeature fusion method that adaptively combines HSI and LiDAR semantics while\nconsidering channel noise and task requirements. Then the method introduces a\nperception-enhanced (PE) module incorporating attention mechanisms to perform\ncoarse classification on UAV side, thereby optimizing the attention-based\nmultimodal fusion and transmission. Experimental results demonstrate that the\nproposed PE-MMSC system achieves 5\\%--10\\% higher target classification\naccuracy compared to conventional systems without PE module, while maintaining\ncomparable data reconstruction quality with acceptable computational overheads.", "published": "2025-03-25 12:21:12", "link": "http://arxiv.org/abs/2503.19594v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Interference Minimization in Beyond-Diagonal RIS-assisted MIMO Interference Channels", "abstract": "This paper proposes a two-stage approach for passive and active beamforming\nin multiple-input multiple-output (MIMO) interference channels (ICs) assisted\nby a beyond-diagonal reconfigurable intelligent surface (BD-RIS). In the first\nstage, the passive BD-RIS is designed to minimize the aggregate interference\npower at all receivers, a cost function called interference leakage (IL). To\nthis end, we propose an optimization algorithm in the manifold of unitary\nmatrices and a suboptimal but computationally efficient solution. In the second\nstage, users' active precoders are designed under different criteria such as\nminimizing the IL (min-IL), maximizing the signal-to-interference-plus-noise\nratio (max-SINR), or maximizing the sum rate (max-SR). The residual\ninterference not cancelled by the BD-RIS is treated as noise by the precoders.\nOur simulation results show that the max-SR precoders provide more than 20% sum\nrate improvement compared to other designs, especially when the BD-RIS has a\nmoderate number of elements ($M<20$) and users transmit with high power, in\nwhich case the residual interference is still significant.", "published": "2025-03-25 11:01:12", "link": "http://arxiv.org/abs/2503.19547v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "First Results on UAV-aided User Localization Using ToA and OpenAirInterface in 5G NR", "abstract": "This paper considers the challenge of localizing ground users with the help\nof a radio-equipped unmanned aerial vehicle (UAV) that collects measurements\nfrom users. We utilize time-of-arrival (ToA) measurements estimated from the\nradio signals received from users collected by a UAV at different locations.\nSince the UAV's location might not be perfectly known, the problem becomes\nabout simultaneously localizing the users and tracking the UAV's position. To\nsolve this problem, we employed a least-squares simultaneous localization and\nmapping (SLAM) framework to fuse ToA data and the estimate of UAV location\navailable from global positioning system (GPS). We verified the performance of\nthe developed algorithm through real-world experimentation.", "published": "2025-03-25 10:31:53", "link": "http://arxiv.org/abs/2503.19529v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Factorizations of relative entropy using stochastic localization", "abstract": "We derive entropy factorization estimates for spin systems using the\nstochastic localization approach proposed by Eldan and Chen-Eldan, which, in\nthis context, is equivalent to the renormalization group approach developed\nindependently by Bauerschmidt, Bodineau, and Dagallier. The method provides\napproximate Shearer-type inequalities for the corresponding Gibbs measure at\nsufficiently high temperature, without restrictions on the degree of the\nunderlying graph. For Ising systems, these are shown to hold up to the critical\ntree-uniqueness threshold, including polynomial bounds at the critical point,\nwith optimal $O(\\sqrt n)$ constants for the Curie-Weiss model at criticality.\nIn turn, these estimates imply tight mixing time bounds for arbitrary block\ndynamics or Gibbs samplers, improving over existing results. Moreover, we\nestablish new tensorization statements for the Shearer inequality asserting\nthat if a system consists of weakly interacting but otherwise arbitrary\ncomponents, each of which satisfies an approximate Shearer inequality, then the\nwhole system also satisfies such an estimate.", "published": "2025-03-25 07:57:13", "link": "http://arxiv.org/abs/2503.19419v1", "categories": ["math.PR", "cs.IT", "math.FA", "math.IT"], "primary_category": "math.PR"}
{"title": "LayerCraft: Enhancing Text-to-Image Generation with CoT Reasoning and Layered Object Integration", "abstract": "Text-to-image generation (T2I) has become a key area of research with broad\napplications. However, existing methods often struggle with complex spatial\nrelationships and fine-grained control over multiple concepts. Many existing\napproaches require significant architectural modifications, extensive training,\nor expert-level prompt engineering. To address these challenges, we introduce\n\\textbf{LayerCraft}, an automated framework that leverages large language\nmodels (LLMs) as autonomous agents for structured procedural generation.\nLayerCraft enables users to customize objects within an image and supports\nnarrative-driven creation with minimal effort. At its core, the system includes\na coordinator agent that directs the process, along with two specialized\nagents: \\textbf{ChainArchitect}, which employs chain-of-thought (CoT) reasoning\nto generate a dependency-aware 3D layout for precise instance-level control,\nand the \\textbf{Object-Integration Network (OIN)}, which utilizes LoRA\nfine-tuning on pre-trained T2I models to seamlessly blend objects into\nspecified regions of an image based on textual prompts without requiring\narchitectural changes. Extensive evaluations demonstrate LayerCraft's\nversatility in applications ranging from multi-concept customization to\nstorytelling. By providing non-experts with intuitive, precise control over T2I\ngeneration, our framework democratizes creative image creation. Our code will\nbe released upon acceptance at github.com/PeterYYZhang/LayerCraft", "published": "2025-03-25 22:36:55", "link": "http://arxiv.org/abs/2504.00010v1", "categories": ["cs.LG", "cs.GR", "cs.MA"], "primary_category": "cs.LG"}
{"title": "LLM-ABM for Transportation: Assessing the Potential of LLM Agents in System Analysis", "abstract": "Agent-based modeling approaches represent the state-of-art in modeling travel\ndemand and transportation system dynamics and are valuable tools for\ntransportation planning. However, established agent-based approaches in\ntransportation rely on multi-hierarchical mathematical models to simulate\ntravel behavior, which faces theoretical and practical limitations. The advent\nof large language models (LLM) provides a new opportunity to refine agent-based\nmodeling in transportation. LLM agents, which have impressive reasoning and\nplanning abilities, can serve as a proxy of human travelers and be integrated\ninto the modeling framework. However, despite evidence of their behavioral\nsoundness, no existing studies have assessed the impact and validity of\nLLM-agent-based simulations from a system perspective in transportation. This\npaper aims to address this issue by designing and integrating LLM agents with\nhuman-traveler-like characteristics into a simulation of a transportation\nsystem and assessing its performance based on existing benchmarks. Using the\nclassical transportation setting of the morning commute, we find that not only\ndo the agents exhibit fine behavioral soundness, but also produce system\ndynamics that align well with standard benchmarks. Our analysis first verifies\nthe effectiveness and potential of LLM-agent-based modeling for transportation\nplanning on the system level.", "published": "2025-03-25 19:46:33", "link": "http://arxiv.org/abs/2503.22718v1", "categories": ["cs.MA"], "primary_category": "cs.MA"}
{"title": "TraF-Align: Trajectory-aware Feature Alignment for Asynchronous Multi-agent Perception", "abstract": "Cooperative perception presents significant potential for enhancing the\nsensing capabilities of individual vehicles, however, inter-agent latency\nremains a critical challenge. Latencies cause misalignments in both spatial and\nsemantic features, complicating the fusion of real-time observations from the\nego vehicle with delayed data from others. To address these issues, we propose\nTraF-Align, a novel framework that learns the flow path of features by\npredicting the feature-level trajectory of objects from past observations up to\nthe ego vehicle's current time. By generating temporally ordered sampling\npoints along these paths, TraF-Align directs attention from the current-time\nquery to relevant historical features along each trajectory, supporting the\nreconstruction of current-time features and promoting semantic interaction\nacross multiple frames. This approach corrects spatial misalignment and ensures\nsemantic consistency across agents, effectively compensating for motion and\nachieving coherent feature fusion. Experiments on two real-world datasets,\nV2V4Real and DAIR-V2X-Seq, show that TraF-Align sets a new benchmark for\nasynchronous cooperative perception.", "published": "2025-03-25 06:56:35", "link": "http://arxiv.org/abs/2503.19391v1", "categories": ["cs.CV", "cs.MA"], "primary_category": "cs.CV"}
{"title": "The Geometric Refinement Transform: A Novel Uncountably Infinite Transform Space", "abstract": "This work introduces a novel and general class of continuous transforms based\non hierarchical Voronoi based refinement schemes. The resulting transform space\ngeneralizes classical approaches such as wavelets and Radon transforms by\nincorporating parameters of refinement multiplicity, dispersion, and rotation.\nWe rigorously establish key properties of the transform including completeness,\nuniqueness, invertibility, closure, and stability using frame bounds over\nfunctions of bounded variation and define a natural inner product structure\nemerging in L2. We identify regions of parameter space that recover known\ntransforms, including multiscale wavelet decompositions and the generalized\nRadon transform. Applications are discussed across a range of disciplines, with\nparticular emphasis on entropy formulations. Notably, the transform remains\nwell behaved on geometrically complex and even non convex domains, where\ntraditional methods may struggle. Despite the complexity of the underlying\ngeometry, the coefficient spectrum reveals structure, offering insight even in\nhighly irregular settings.", "published": "2025-03-25 22:31:33", "link": "http://arxiv.org/abs/2503.20096v2", "categories": ["math.NA", "cs.NA", "math-ph", "math.FA", "math.MP", "65D15, 65T60, 52C99, 42C15, 94A12, 94A08, 94A17, 44A12"], "primary_category": "math.NA"}
{"title": "Global Bounds for the Error in Solutions of Linear Hyperbolic Systems due to Inaccurate Boundary Geometry", "abstract": "We derive global estimates for the error in solutions of linear hyperbolic\nsystems due to inaccurate boundary geometry. We show that the error is bounded\nby data and bounded in time when the solutions in the true and approximate\ndomains are bounded. We show that boundary data evaluation errors due to the\nincorrect locations of the boundaries are secondary effects, whereas the\nprimary errors are from the Jacobian and metric terms. In two space dimensions,\nspecifically, we show that to lowest order the errors are proportional to the\nerrors in the boundary curves and their derivatives. The results illustrate the\nimportance of accurately approximating boundaries, and they should be helpful\nfor high-order mesh generation and the design of optimization algorithms for\nboundary approximations.", "published": "2025-03-25 20:07:14", "link": "http://arxiv.org/abs/2503.20044v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Adaptive refinement in defeaturing problems via an equilibrated flux a posteriori error estimator", "abstract": "An adaptive refinement strategy, based on an equilibrated flux a posteriori\nerror estimator, is proposed in the context of defeaturing problems.\nDefeaturing consists in removing features from complex domains in order to ease\nthe meshing process, and to reduce the computational burden of simulations. It\nis a common procedure, for example, in computer aided design for simulation\nbased manufacturing. However, depending on the problem at hand, the effect of\ngeometrical simplification on the accuracy of the solution may be detrimental.\nThe proposed adaptive strategy is hence twofold: starting from a defeatured\ngeometry it allows both for standard mesh refinement and geometrical\nrefinement, which consists in choosing, at each step, which features need to be\nincluded into the geometry in order to significantly increase the accuracy of\nthe solution. With respect to other estimators that were previously proposed in\nthe context of defeaturing, the use of an equilibrated flux reconstruction\nallows us to avoid the evaluation of the numerical flux on the boundary of\nfeatures. This makes the estimator and the adaptive strategy particularly\nwell-suited for finite element discretizations, in which the numerical flux is\ntypically discontinuous across element edges. The inclusion of the features\nduring the adaptive process is tackled by a CutFEM strategy, in order to\npreserve the non conformity of the mesh to the feature boundary and never\nremesh the computational domain as the features are added. Hence, the estimator\nalso accounts for the error introduced by weakly imposing the boundary\nconditions on the boundary of the added features.", "published": "2025-03-25 15:50:37", "link": "http://arxiv.org/abs/2503.19784v1", "categories": ["math.NA", "cs.NA", "65N15, 65N30, 65N50"], "primary_category": "math.NA"}
{"title": "Enhanced gradient recovery-based a posteriori error estimator and adaptive finite element method for elliptic equations", "abstract": "Recovery type a posteriori error estimators are popular, particularly in the\nengineering community, for their computationally inexpensive, easy to\nimplement, and generally asymptotically exactness. Unlike the residual type\nerror estimators, one can not establish upper and lower a posteriori error\nbounds for the classical recovery type error estimators without the saturation\nassumption. In this paper, we first present three examples to show the\nunsatisfactory performance in the practice of standard residual or\nrecovery-type error estimators, then, an improved gradient recovery-based a\nposteriori error estimator is constructed. The proposed error estimator\ncontains two parts, one is the difference between the direct and post-processed\ngradient approximations, and the other is the residual of the recovered\ngradient. The reliability and efficiency of the enhanced estimator are derived.\nBased on the improved recovery-based error estimator and the newest-vertex\nbisection refinement method with a tailored mark strategy, an adaptive finite\nelement algorithm is designed. We then prove the convergence of the adaptive\nmethod by establishing the contraction of gradient error plus oscillation.\nNumerical experiments are provided to illustrate the asymptotic exactness of\nthe new recovery-based a posteriori error estimator and the high efficiency of\nthe corresponding adaptive algorithm.", "published": "2025-03-25 14:28:44", "link": "http://arxiv.org/abs/2503.19701v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Characteristic boundary conditions for Hybridizable Discontinuous Galerkin methods", "abstract": "In this work we introduce the concept of characteristic boundary conditions\n(CBCs) within the framework of Hybridizable Discontinuous Galerkin (HDG)\nmethods, including both the Navier-Stokes characteristic boundary conditions\n(NSCBCs) and a novel approach to generalized characteristic relaxation boundary\nconditions (GRCBCs). CBCs are based on the characteristic decomposition of the\ncompressible Euler equations and are designed to prevent the reflection of\nwaves at the domain boundaries. We show the effectiveness of the proposed\nmethod for weakly compressible flows through a series of numerical experiments\nby comparing the results with common boundary conditions in the HDG setting and\nreference solutions available in the literature. In particular, HDG with CBCs\nshow superior performance minimizing the reflection of vortices at artificial\nboundaries, for both inviscid and viscous flows.", "published": "2025-03-25 14:12:08", "link": "http://arxiv.org/abs/2503.19684v1", "categories": ["math.NA", "cs.CE", "cs.NA", "physics.comp-ph", "physics.flu-dyn"], "primary_category": "math.NA"}
{"title": "Derivative polynomials and infinite series for squigonometric functions", "abstract": "All squigonometric functions admit derivatives that can be expressed as\npolynomials of the squine and cosquine. We introduce a general framework that\nallows us to determine these polynomials recursively. We also provide an\nexplicit formula for all coefficients of these polynomials. This also allows us\nto provide an explicit expression for the MacLaurin series coefficients of all\nsquigonometric functions. We further discuss some methods that can compute the\nsquigonometric functions up to any given tolerance over all of the real line.", "published": "2025-03-25 13:13:42", "link": "http://arxiv.org/abs/2503.19624v1", "categories": ["math.CA", "cs.NA", "math.NA", "26A06, 26A24"], "primary_category": "math.CA"}
{"title": "Asymptotic-preserving and positivity-preserving discontinuous Galerkin method for the semiconductor Boltzmann equation in the diffusive scaling", "abstract": "In this paper, we develop an asymptotic-preserving and positivity-preserving\ndiscontinuous Galerkin (DG) method for solving the semiconductor Boltzmann\nequation in the diffusive scaling. We first formulate the diffusive relaxation\nsystem based on the even-odd decomposition method, which allows us to split\ninto one relaxation step and one transport step. We adopt a robust implicit\nscheme that can be explicitly implemented for the relaxation step that involves\nthe stiffness of the collision term, while the third-order\nstrong-stability-preserving Runge-Kutta method is employed for the transport\nstep. We couple this temporal scheme with the DG method for spatial\ndiscretization, which provides additional advantages including high-order\naccuracy, $h$-$p$ adaptivity, and the ability to handle arbitrary unstructured\nmeshes. A positivity-preserving limiter is further applied to preserve physical\nproperties of numerical solutions. The stability analysis using the even-odd\ndecomposition is conducted for the first time. We demonstrate the accuracy and\nperformance of our proposed scheme through several numerical examples.", "published": "2025-03-25 09:23:50", "link": "http://arxiv.org/abs/2503.19487v1", "categories": ["math.NA", "cs.NA", "65M60, 65M70, 35Q20"], "primary_category": "math.NA"}
{"title": "Empirical Hyper Element Integration Method (EHEIM) with Unified Integration Criteria for Efficient Hyper Reduced FE$^2$ Simulations", "abstract": "Numerical homogenization for mechanical multiscale modeling by means of the\nfinite element method (FEM) is an elegant way of obtaining structure-property\nrelations, if the behavior of the constituents of the lower scale is well\nunderstood. However, the computational costs of this so-called FE$^2$ method\nare so high that reduction methods are essential. While the construction of a\nreduced basis for the microscopic nodal displacements using proper orthogonal\ndecomposition (POD) has become a standard technique, the reduction of the\ncomputational effort for the projected nodal forces, the so-called hyper\nreduction, is an additional challenge, for which different strategies have been\nproposed in the literature. The empirical cubature method (ECM), which has been\nproven to be very robust, implemented the conservation of the total volume is\nused as a constraint in the resulting optimization problem, while energy-based\ncriteria have been proposed in other contributions.\n  The present contribution presents a unified integration criteria concept,\ninvolving the aforementioned criteria, among others. These criteria are used\nboth with a Gauss point-based as well as with an element-based hyper reduction\nscheme, the latter retaining full compatibility with the common modular finite\nelement framework. The methods are combined with a previously proposed\nclustered training strategy and a monolithic solver. Numerical examples\nempirically demonstrate that the additional criteria improve the accuracy for a\ngiven number of modes. Vice verse, less modes and thus lower computational\ncosts are required to reach a given level of accuracy.", "published": "2025-03-25 09:18:28", "link": "http://arxiv.org/abs/2503.19483v1", "categories": ["math.NA", "cs.NA", "physics.comp-ph", "74Q10, 74C15, 68T10", "J.2; G.1.10; I.6"], "primary_category": "math.NA"}
{"title": "A linear, unconditionally stable, second order decoupled method for the nematic liquid crystal flows with SAV approach", "abstract": "In this paper, we present a second order, linear, fully decoupled, and\nunconditionally energy stable scheme for solving the Erickson-Leslie model.\nThis approach integrates the pressure correction method with a scalar auxiliary\nvariable technique. We rigorously demonstrate the unconditional energy\nstability of the proposed scheme. Furthermore, we present several numerical\nexperiments to validate its convergence order, stability, and computational\nefficiency.", "published": "2025-03-25 08:03:19", "link": "http://arxiv.org/abs/2503.19424v1", "categories": ["math.AP", "cs.NA", "math.NA"], "primary_category": "math.AP"}
{"title": "Kernel compensation method for Maxwell eigenproblem with mimetic finite difference discretization", "abstract": "We present a kernel compensation method for Maxwell eigenproblem for photonic\ncrystals to avoid the infinite-dimensional kernels that cause many difficulties\nin the calculation of energy gaps. The quasi-periodic problem is first\ntransformed into a periodic one on the cube by the Floquet-Bloch theory. Then\nthe compensation operator is introduced in Maxwell's equation with the shifted\ncurl operator. The discrete problem depends on the compatible discretization of\nthe de Rham complex, which is implemented by the mimetic finite difference\nmethod in this paper. We prove that the compensation term exactly fills up the\nkernel of the original problem and avoids spurious eigenvalues. Also, we\npropose an efficient preconditioner and its FFT and multigrid solvers, which\nallow parallel computing. Numerical experiments for different three-dimensional\nlattices are performed to validate the accuracy and effectiveness of the\nmethod.", "published": "2025-03-25 06:10:41", "link": "http://arxiv.org/abs/2503.19379v1", "categories": ["math.NA", "cs.NA", "65N25, 35Q61, 65F08", "G.1.8; G.1.3; G.4"], "primary_category": "math.NA"}
{"title": "A Wong--Zakai resonance-based integrator for nonlinear Schr\u00f6dinger equation with white noise dispersion", "abstract": "We introduce a novel approach to numerical approximation of nonlinear\nSchr\\\"odinger equation with white noise dispersion in the regime of\nlow-regularity solutions. Approximating such solutions in the stochastic\nsetting is particularly challenging due to randomized frequency interactions\nand presents a compelling challenge for the construction of tailored schemes.\nIn particular, we design the first resonance-based schemes for this equation,\nwhich achieve provable convergence for solutions of much lower regularity than\npreviously required. A crucial ingredient in this construction is the\nWong--Zakai approximation of stochastic dispersive system, which introduces\npiecewise linear phases that capture nonlinear frequency interactions and can\nsubsequently be approximated to construct resonance-based schemes. We prove the\nwell-posedness of the Wong--Zakai approximated equation and establish its\nproximity to the original full stochastic dispersive system. Based on this\napproximation, we demonstrate an improved strong convergence rate for our new\nscheme, which exploits the stochastic nature of the dispersive terms. Finally,\nwe provide numerical experiments underlining the favourable performance of our\nnovel method in practice.", "published": "2025-03-25 04:48:37", "link": "http://arxiv.org/abs/2503.19346v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "E-PINNs: Epistemic Physics-Informed Neural Networks", "abstract": "Physics-informed neural networks (PINNs) have demonstrated promise as a\nframework for solving forward and inverse problems involving partial\ndifferential equations. Despite recent progress in the field, it remains\nchallenging to quantify uncertainty in these networks. While approaches such as\nBayesian PINNs (B-PINNs) provide a principled approach to capturing uncertainty\nthrough Bayesian inference, they can be computationally expensive for\nlarge-scale applications. In this work, we propose Epistemic Physics-Informed\nNeural Networks (E-PINNs), a framework that leverages a small network, the\n\\emph{epinet}, to efficiently quantify uncertainty in PINNs. The proposed\napproach works as an add-on to existing, pre-trained PINNs with a small\ncomputational overhead. We demonstrate the applicability of the proposed\nframework in various test cases and compare the results with B-PINNs using\nHamiltonian Monte Carlo (HMC) posterior estimation and dropout-equipped PINNs\n(Dropout-PINNs). Our experiments show that E-PINNs provide similar coverage to\nB-PINNs, with often comparable sharpness, while being computationally more\nefficient. This observation, combined with E-PINNs' more consistent uncertainty\nestimates and better calibration compared to Dropout-PINNs for the examples\npresented, indicates that E-PINNs offer a promising approach in terms of\naccuracy-efficiency trade-off.", "published": "2025-03-25 03:53:28", "link": "http://arxiv.org/abs/2503.19333v1", "categories": ["cs.LG", "cs.NA", "math.NA"], "primary_category": "cs.LG"}
{"title": "Parameter-robust preconditioner for Stokes-Darcy coupled problem with Lagrange multiplier", "abstract": "In this paper, we propose a parameter-robust preconditioner for the coupled\nStokes-Darcy problem equipped with various boundary conditions, enforcing the\nmass conservation at the interface via a Lagrange multiplier. We rigorously\nestablish that the coupled system is well-posed with respect to physical\nparameters and mesh size and provide a framework for constructing\nparameter-robust preconditioners. Furthermore, we analyze the convergence\nbehavior of the Minimal Residual method in the presence of small outlier\neigenvalues linked to specific boundary conditions, which can lead to slow\nconvergence or stagnation. To address this issue, we employ deflation\ntechniques to accelerate the convergence. Finally, Numerical experiments\nconfirm the effectiveness and robustness of the proposed approach.", "published": "2025-03-25 01:47:39", "link": "http://arxiv.org/abs/2503.19261v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Data-Driven, ML-assisted Approaches to Problem Well-Posedness", "abstract": "Classically, to solve differential equation problems, it is necessary to\nspecify sufficient initial and/or boundary conditions so as to allow the\nexistence of a unique solution. Well-posedness of differential equation\nproblems thus involves studying the existence and uniqueness of solutions, and\ntheir dependence to such pre-specified conditions. However, in part due to\nmathematical necessity, these conditions are usually specified \"to arbitrary\nprecision\" only on (appropriate portions of) the boundary of the space-time\ndomain. This does not mirror how data acquisition is performed in realistic\nsituations, where one may observe entire \"patches\" of solution data at\narbitrary space-time locations; alternatively one might have access to more\nthan one solutions stemming from the same differential operator. In our short\nwork, we demonstrate how standard tools from machine and manifold learning can\nbe used to infer, in a data driven manner, certain well-posedness features of\ndifferential equation problems, for initial/boundary condition combinations\nunder which rigorous existence/uniqueness theorems are not known. Our study\nnaturally combines a data assimilation perspective with an operator-learning\none.", "published": "2025-03-25 01:34:48", "link": "http://arxiv.org/abs/2503.19255v1", "categories": ["cs.LG", "cs.NA", "math.NA"], "primary_category": "cs.LG"}
{"title": "Extensions of regret-minimization algorithm for optimal design", "abstract": "We explore extensions and applications of the regret minimization framework\nintroduced by~\\cite{design} for solving optimal experimental design problems.\nSpecifically, we incorporate the entropy regularizer into this framework,\nleading to a novel sample selection objective and a provable sample complexity\nbound that guarantees a $(1+\\epsilon)$-near optimal solution. We further extend\nthe method to handle regularized optimal design settings. As an application, we\nuse our algorithm to select a small set of representative samples from image\nclassification datasets without relying on label information. To evaluate the\nquality of the selected samples, we train a logistic regression model and\ncompare performance against several baseline sampling strategies. Experimental\nresults on MNIST, CIFAR-10, and a 50-class subset of ImageNet show that our\napproach consistently outperforms competing methods in most cases.", "published": "2025-03-25 17:37:09", "link": "http://arxiv.org/abs/2503.19874v1", "categories": ["cs.LG", "stat.ML", "62J12, 62L05, 68W27, 68W40, 68T05"], "primary_category": "cs.LG"}
{"title": "An Overview of Low-Rank Structures in the Training and Adaptation of Large Models", "abstract": "The rise of deep learning has revolutionized data processing and prediction\nin signal processing and machine learning, yet the substantial computational\ndemands of training and deploying modern large-scale deep models present\nsignificant challenges, including high computational costs and energy\nconsumption. Recent research has uncovered a widespread phenomenon in deep\nnetworks: the emergence of low-rank structures in weight matrices and learned\nrepresentations during training. These implicit low-dimensional patterns\nprovide valuable insights for improving the efficiency of training and\nfine-tuning large-scale models. Practical techniques inspired by this\nphenomenon, such as low-rank adaptation (LoRA) and training, enable significant\nreductions in computational cost while preserving model performance. In this\npaper, we present a comprehensive review of recent advances in exploiting\nlow-rank structures for deep learning and shed light on their mathematical\nfoundations. Mathematically, we present two complementary perspectives on\nunderstanding the low-rankness in deep networks: (i) the emergence of low-rank\nstructures throughout the whole optimization dynamics of gradient and (ii) the\nimplicit regularization effects that induce such low-rank structures at\nconvergence. From a practical standpoint, studying the low-rank learning\ndynamics of gradient descent offers a mathematical foundation for understanding\nthe effectiveness of LoRA in fine-tuning large-scale models and inspires\nparameter-efficient low-rank training strategies. Furthermore, the implicit\nlow-rank regularization effect helps explain the success of various masked\ntraining approaches in deep neural networks, ranging from dropout to masked\nself-supervised learning.", "published": "2025-03-25 17:26:09", "link": "http://arxiv.org/abs/2503.19859v1", "categories": ["cs.LG", "eess.SP", "math.OC", "stat.CO", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Capacity-Constrained Online Learning with Delays: Scheduling Frameworks and Regret Trade-offs", "abstract": "We study online learning with oblivious losses and delays under a novel\n``capacity constraint'' that limits how many past rounds can be tracked\nsimultaneously for delayed feedback. Under ``clairvoyance'' (i.e., delay\ndurations are revealed upfront each round) and/or ``preemptibility'' (i.e., we\nhave ability to stop tracking previously chosen round feedback), we establish\nmatching upper and lower bounds (up to logarithmic terms) on achievable regret,\ncharacterizing the ``optimal capacity'' needed to match the minimax rates of\nclassical delayed online learning, which implicitly assume unlimited capacity.\nOur algorithms achieve minimax-optimal regret across all capacity levels, with\nperformance gracefully degrading under suboptimal capacity. For $K$ actions and\ntotal delay $D$ over $T$ rounds, under clairvoyance and assuming capacity $C =\n\\Omega(\\log(T))$, we achieve regret $\\widetilde{\\Theta}(\\sqrt{TK + DK/C +\nD\\log(K)})$ for bandits and $\\widetilde{\\Theta}(\\sqrt{(D+T)\\log(K)})$ for\nfull-information feedback. When replacing clairvoyance with preemptibility, we\nrequire a known maximum delay bound $d_{\\max}$, adding\n$\\smash{\\widetilde{O}(d_{\\max})}$ to the regret. For fixed delays $d$ (i.e.,\n$D=Td$), the minimax regret is $\\Theta\\bigl(\\sqrt{TK(1+d/C)+Td\\log(K)}\\bigr)$\nand the optimal capacity is $\\Theta(\\min\\{K/\\log(K),d\\}\\bigr)$ in the bandit\nsetting, while in the full-information setting, the minimax regret is\n$\\Theta\\bigl(\\sqrt{T(d+1)\\log(K)}\\bigr)$ and the optimal capacity is\n$\\Theta(1)$. For round-dependent and fixed delays, our upper bounds are\nachieved using novel scheduling policies, based on Pareto-distributed proxy\ndelays and batching techniques. Crucially, our work unifies delayed bandits,\nlabel-efficient learning, and online scheduling frameworks, demonstrating that\nrobust online learning under delayed feedback is possible with surprisingly\nmodest tracking capacity.", "published": "2025-03-25 17:20:39", "link": "http://arxiv.org/abs/2503.19856v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Interpretable Deep Regression Models with Interval-Censored Failure Time Data", "abstract": "Deep neural networks (DNNs) have become powerful tools for modeling complex\ndata structures through sequentially integrating simple functions in each\nhidden layer. In survival analysis, recent advances of DNNs primarily focus on\nenhancing model capabilities, especially in exploring nonlinear covariate\neffects under right censoring. However, deep learning methods for\ninterval-censored data, where the unobservable failure time is only known to\nlie in an interval, remain underexplored and limited to specific data type or\nmodel. This work proposes a general regression framework for interval-censored\ndata with a broad class of partially linear transformation models, where key\ncovariate effects are modeled parametrically while nonlinear effects of\nnuisance multi-modal covariates are approximated via DNNs, balancing\ninterpretability and flexibility. We employ sieve maximum likelihood estimation\nby leveraging monotone splines to approximate the cumulative baseline hazard\nfunction. To ensure reliable and tractable estimation, we develop an EM\nalgorithm incorporating stochastic gradient descent. We establish the\nasymptotic properties of parameter estimators and show that the DNN estimator\nachieves minimax-optimal convergence. Extensive simulations demonstrate\nsuperior estimation and prediction accuracy over state-of-the-art methods.\nApplying our method to the Alzheimer's Disease Neuroimaging Initiative dataset\nyields novel insights and improved predictive performance compared to\ntraditional approaches.", "published": "2025-03-25 15:27:32", "link": "http://arxiv.org/abs/2503.19763v1", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "primary_category": "stat.ML"}
{"title": "Causal Bayesian Optimization with Unknown Graphs", "abstract": "Causal Bayesian Optimization (CBO) is a methodology designed to optimize an\noutcome variable by leveraging known causal relationships through targeted\ninterventions. Traditional CBO methods require a fully and accurately specified\ncausal graph, which is a limitation in many real-world scenarios where such\ngraphs are unknown. To address this, we propose a new method for the CBO\nframework that operates without prior knowledge of the causal graph. Consistent\nwith causal bandit theory, we demonstrate through theoretical analysis and that\nfocusing on the direct causal parents of the target variable is sufficient for\noptimization, and provide empirical validation in the context of CBO.\nFurthermore we introduce a new method that learns a Bayesian posterior over the\ndirect parents of the target variable. This allows us to optimize the outcome\nvariable while simultaneously learning the causal structure. Our contributions\ninclude a derivation of the closed-form posterior distribution for the linear\ncase. In the nonlinear case where the posterior is not tractable, we present a\nGaussian Process (GP) approximation that still enables CBO by inferring the\nparents of the outcome variable. The proposed method performs competitively\nwith existing benchmarks and scales well to larger graphs, making it a\npractical tool for real-world applications where causal information is\nincomplete.", "published": "2025-03-25 11:14:37", "link": "http://arxiv.org/abs/2503.19554v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "A stochastic gradient descent algorithm with random search directions", "abstract": "Stochastic coordinate descent algorithms are efficient methods in which each\niterate is obtained by fixing most coordinates at their values from the current\niteration, and approximately minimizing the objective with respect to the\nremaining coordinates. However, this approach is usually restricted to\ncanonical basis vectors of $\\mathbb{R}^d$. In this paper, we develop a new\nclass of stochastic gradient descent algorithms with random search directions\nwhich uses the directional derivative of the gradient estimate following more\ngeneral random vectors. We establish the almost sure convergence of these\nalgorithms with decreasing step. We further investigate their central limit\ntheorem and pay particular attention to analyze the impact of the search\ndistributions on the asymptotic covariance matrix. We also provide\nnon-asymptotic $\\mathbb{L}^p$ rates of convergence.", "published": "2025-03-25 09:54:06", "link": "http://arxiv.org/abs/2503.19942v2", "categories": ["stat.ML", "cs.LG", "math.OC", "math.PR"], "primary_category": "stat.ML"}
{"title": "Bayesian Optimization of a Lightweight and Accurate Neural Network for Aerodynamic Performance Prediction", "abstract": "Ensuring high accuracy and efficiency of predictive models is paramount in\nthe aerospace industry, particularly in the context of multidisciplinary design\nand optimization processes. These processes often require numerous evaluations\nof complex objective functions, which can be computationally expensive and\ntime-consuming. To build efficient and accurate predictive models, we propose a\nnew approach that leverages Bayesian Optimization (BO) to optimize the\nhyper-parameters of a lightweight and accurate Neural Network (NN) for\naerodynamic performance prediction. To clearly describe the interplay between\ndesign variables, hierarchical and categorical kernels are used in the BO\nformulation. We demonstrate the efficiency of our approach through two\ncomprehensive case studies, where the optimized NN significantly outperforms\nbaseline models and other publicly available NNs in terms of accuracy and\nparameter efficiency. For the drag coefficient prediction task, the Mean\nAbsolute Percentage Error (MAPE) of our optimized model drops from 0.1433\\% to\n0.0163\\%, which is nearly an order of magnitude improvement over the baseline\nmodel. Additionally, our model achieves a MAPE of 0.82\\% on a benchmark\naircraft self-noise prediction problem, significantly outperforming existing\nmodels (where their MAPE values are around 2 to 3\\%) while requiring less\ncomputational resources. The results highlight the potential of our framework\nto enhance the scalability and performance of NNs in large-scale MDO problems,\noffering a promising solution for the aerospace industry.", "published": "2025-03-25 09:14:36", "link": "http://arxiv.org/abs/2503.19479v1", "categories": ["cs.LG", "math.OC", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Stop Walking in Circles! Bailing Out Early in Projected Gradient Descent", "abstract": "Projected Gradient Descent (PGD) under the $L_\\infty$ ball has become one of\nthe defacto methods used in adversarial robustness evaluation for computer\nvision (CV) due to its reliability and efficacy, making a strong and\neasy-to-implement iterative baseline. However, PGD is computationally demanding\nto apply, especially when using thousands of iterations is the current\nbest-practice recommendation to generate an adversarial example for a single\nimage. In this work, we introduce a simple novel method for early termination\nof PGD based on cycle detection by exploiting the geometry of how PGD is\nimplemented in practice and show that it can produce large speedup factors\nwhile providing the \\emph{exact} same estimate of model robustness as standard\nPGD. This method substantially speeds up PGD without sacrificing any attack\nstrength, enabling evaluations of robustness that were previously\ncomputationally intractable.", "published": "2025-03-25 04:51:44", "link": "http://arxiv.org/abs/2503.19347v1", "categories": ["cs.CV", "cs.LG", "stat.ML"], "primary_category": "cs.CV"}
{"title": "Centroid Decision Forest", "abstract": "This paper introduces the centroid decision forest (CDF), a novel ensemble\nlearning framework that redefines the splitting strategy and tree building in\nthe ordinary decision trees for high-dimensional classification. The splitting\napproach in CDF differs from the traditional decision trees in theat the class\nseparability score (CSS) determines the selection of the most discriminative\nfeatures at each node to construct centroids of the partitions (daughter\nnodes). The splitting criterion uses the Euclidean distance measurements from\neach class centroid to achieve a splitting mechanism that is more flexible and\nrobust. Centroids are constructed by computing the mean feature values of the\nselected features for each class, ensuring a class-representative division of\nthe feature space. This centroid-driven approach enables CDF to capture complex\nclass structures while maintaining interpretability and scalability. To\nevaluate CDF, 23 high-dimensional datasets are used to assess its performance\nagainst different state-of-the-art classifiers through classification accuracy\nand Cohen's kappa statistic. The experimental results show that CDF outperforms\nthe conventional methods establishing its effectiveness and flexibility for\nhigh-dimensional classification problems.", "published": "2025-03-25 03:12:52", "link": "http://arxiv.org/abs/2503.19306v1", "categories": ["stat.ML", "cs.LG", "14J60"], "primary_category": "stat.ML"}
{"title": "Structured and sparse partial least squares coherence for multivariate cortico-muscular analysis", "abstract": "Multivariate cortico-muscular analysis has recently emerged as a promising\napproach for evaluating the corticospinal neural pathway. However, current\nmultivariate approaches encounter challenges such as high dimensionality and\nlimited sample sizes, thus restricting their further applications. In this\npaper, we propose a structured and sparse partial least squares coherence\nalgorithm (ssPLSC) to extract shared latent space representations related to\ncortico-muscular interactions. Our approach leverages an embedded optimization\nframework by integrating a partial least squares (PLS)-based objective\nfunction, a sparsity constraint and a connectivity-based structured constraint,\naddressing the generalizability, interpretability and spatial structure. To\nsolve the optimization problem, we develop an efficient alternating iterative\nalgorithm within a unified framework and prove its convergence experimentally.\nExtensive experimental results from one synthetic and several real-world\ndatasets have demonstrated that ssPLSC can achieve competitive or better\nperformance over some representative multivariate cortico-muscular fusion\nmethods, particularly in scenarios characterized by limited sample sizes and\nhigh noise levels. This study provides a novel multivariate fusion method for\ncortico-muscular analysis, offering a transformative tool for the evaluation of\ncorticospinal pathway integrity in neurological disorders.", "published": "2025-03-25 01:56:11", "link": "http://arxiv.org/abs/2503.21802v1", "categories": ["stat.AP", "cs.LG", "stat.ML"], "primary_category": "stat.AP"}
{"title": "Boosting the Transferability of Audio Adversarial Examples with Acoustic Representation Optimization", "abstract": "With the widespread application of automatic speech recognition (ASR)\nsystems, their vulnerability to adversarial attacks has been extensively\nstudied. However, most existing adversarial examples are generated on specific\nindividual models, resulting in a lack of transferability. In real-world\nscenarios, attackers often cannot access detailed information about the target\nmodel, making query-based attacks unfeasible. To address this challenge, we\npropose a technique called Acoustic Representation Optimization that aligns\nadversarial perturbations with low-level acoustic characteristics derived from\nspeech representation models. Rather than relying on model-specific,\nhigher-layer abstractions, our approach leverages fundamental acoustic\nrepresentations that remain consistent across diverse ASR architectures. By\nenforcing an acoustic representation loss to guide perturbations toward these\nrobust, lower-level representations, we enhance the cross-model transferability\nof adversarial examples without degrading audio quality. Our method is\nplug-and-play and can be integrated with any existing attack methods. We\nevaluate our approach on three modern ASR models, and the experimental results\ndemonstrate that our method significantly improves the transferability of\nadversarial examples generated by previous methods while preserving the audio\nquality.", "published": "2025-03-25 12:14:10", "link": "http://arxiv.org/abs/2503.19591v1", "categories": ["cs.SD", "cs.CR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Deploying an Aerial Reconfigurable Intelligent Surface for Vehicle-to-Vehicle Communications (PL: Wykorzystanie powietrznych prze\u0142\u0105czalnych inteligentnych powierzchni do komunikacji mi\u0119dzypojazdowej)", "abstract": "This paper addresses the deployment of a drone equipped with a reconfigurable\nintelligent surface (RIS), creating a drone relay station (DRS) to enhance the\nconnectivity of vehicle-to-vehicle (V2V) pairs on the ground. The trajectory of\nthe DRS is optimized to quickly reach the best location for maximizing\nthroughput. Additionally, the presence of an interfering node is considered,\nand an analytical solution is derived to determine the optimal orientation of\nthe DRS at each time step, minimizing interference to the receiver. Simulation\nresults confirm the effectiveness of the proposed framework.", "published": "2025-03-25 20:34:12", "link": "http://arxiv.org/abs/2503.20057v1", "categories": ["cs.NI", "eess.SP"], "primary_category": "cs.NI"}
{"title": "Deep-Learning-Based Classification of Digitally Modulated Signals", "abstract": "This dissertation presents several novel deep-learning (DL)-based approaches\nfor classifying digitally modulated signals, one method of which involves the\nuse of capsule networks (CAPs) together with cyclic cumulant (CC) features of\nthe signals. These were blindly estimated using cyclostationary signal\nprocessing (CSP) and were then input into the CAP for training and\nclassification. The results showed that the classification of digitally\nmodulated signals using CAPs and CCs proposed in this dissertation outperformed\nalternative approaches for classifying digitally modulated signals that\nincluded conventional classifiers that employed CSP-based techniques, as well\nas alternative DL-based classifiers that used various conventional neural\nnetworks (NNs) with in-phase/quadrature (I/Q) data used for training and\nclassification.\n  Another method of digital modulation classification presented in this\ndissertation showcases two novel DL-based classifiers that each use a CAP with\ncustom-designed feature extraction layers. The classifiers take I/Q data as\ninput, and the feature extraction layers are inspired by CSP techniques, which\nextract the CC features employed by conventional CSP-based approaches to blind\nmodulation classification and signal identification. Specifically, the feature\nextraction layers implement a proxy of the mathematical functions used in the\ncalculation of the CC features and include a squaring layer, a\nraise-to-the-power-of-three layer, and a fast-Fourier-transform (FFT) layer,\nalong with additional normalization and warping layers to ensure that the\nrelative signal powers are retained and to prevent the trainable NN layers from\ndiverging in the training process.", "published": "2025-03-25 17:40:00", "link": "http://arxiv.org/abs/2503.19952v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "OPC UA for IO-Link Wireless in a Cyber Physical Finite Element Sensor Network for Shape Measurement", "abstract": "This paper presents the integration of OPC UA as a communication protocol in\na wireless sensor network and the associated companion specifications as a\nsemantic template for an information model. The Cyber Physical Finite Element\nSensor Network (CPFEN ) for Shape Measurements, a distributed wireless system,\nuses IO-Link Wireless for data transmission at the sensor level, OPC UA\nprovides a unified interface for data access, configuration, monitoring, and\ncalibration tailored to the needs of the CPFEN for all level above. This opens\nup additional possibilities, such as integrated quality assurance or creating a\ndigital twin, while improving scalability.", "published": "2025-03-25 16:48:33", "link": "http://arxiv.org/abs/2504.03704v1", "categories": ["cs.NI", "eess.SP"], "primary_category": "cs.NI"}
{"title": "A Systematic Review of EEG-based Machine Intelligence Algorithms for Depression Diagnosis, and Monitoring", "abstract": "Depression disorder is a serious health condition that has affected the lives\nof millions of people around the world. Diagnosis of depression is a\nchallenging practice that relies heavily on subjective studies and, in most\ncases, suffers from late findings. Electroencephalography (EEG) biomarkers have\nbeen suggested and investigated in recent years as a potential transformative\nobjective practice. In this article, for the first time, a detailed systematic\nreview of EEG-based depression diagnosis approaches is conducted using advanced\nmachine learning techniques and statistical analyses. For this, 938 potentially\nrelevant articles (since 1985) were initially detected and filtered into 139\nrelevant articles based on the review scheme 'preferred reporting items for\nsystematic reviews and meta-analyses (PRISMA).' This article compares and\ndiscusses the selected articles and categorizes them according to the type of\nmachine learning techniques and statistical analyses. Algorithms, preprocessing\ntechniques, extracted features, and data acquisition systems are discussed and\nsummarized. This review paper explains the existing challenges of the current\nalgorithms and sheds light on the future direction of the field. This\nsystematic review outlines the issues and challenges in machine intelligence\nfor the diagnosis of EEG depression that can be addressed in future studies and\npossibly in future wearable technologies.", "published": "2025-03-25 16:31:27", "link": "http://arxiv.org/abs/2503.19820v1", "categories": ["eess.SP", "cs.LG"], "primary_category": "eess.SP"}
{"title": "Hierarchical Attention Network for Interpretable ECG-based Heart Disease Classification", "abstract": "Cardiovascular disease remains one of the leading causes of mortality\nworldwide, underscoring the need for accurate as well as interpretable\ndiagnostic machine learning tools. In this work, we investigate heart disease\nclassification using electrocardiogram (ECG) data from two widely-utilized\ndatasets: The MIT-BIH Arrhythmia and the PTB-XL datasets. We adapt a\nhierarchical attention network (HAN), originally developed for text\nclassification, into an ECG-based heart-disease classification task. Our\nadapted HAN incorporates two attention layers that focus on ECG data segments\nof varying sizes. We conduct a comparative analysis between our adapted HAN and\na more sophisticated state-of-the-art architecture, featuring a network with\nconvolution, attention, and transformer layers (CAT-Net). Our empirical\nevaluation encompasses multiple aspects including test accuracy (quantified by\n0-1 loss); model complexity (measured by the number of model parameters); and\ninterpretability (through attention map visualization). Our adapted HAN\ndemonstrates comparable test accuracy with significant reductions in model\ncomplexity and enhanced interpretability analysis: For the MIT-BIH dataset, our\nadapted HAN achieves 98.55\\% test accuracy compared to 99.14\\% for CAT-Net,\nwhile reducing the number of model parameters by a factor of 15.6. For the\nPTB-XL dataset, our adapted HAN achieves a 19.3-fold reduction in model\ncomplexity compared to CAT-Net, with only a 5\\% lower test accuracy. From an\ninterpretability perspective, the significantly simpler architecture and the\nhierarchical nature of our adapted HAN model facilitate a more straightforward\ninterpretability analysis based on visualizing attention weights. Building on\nthis advantage, we conduct an interpretability analysis of our HAN that\nhighlights the regions of the ECG signal most relevant to the model's\ndecisions.", "published": "2025-03-25 13:06:06", "link": "http://arxiv.org/abs/2504.03703v1", "categories": ["eess.SP", "cs.CV", "cs.LG"], "primary_category": "eess.SP"}
{"title": "Iterative Decoder of Channel-polarized Multilevel Coding for Data Center Networks", "abstract": "Data center networks (DCNs) require a low-cost, low-power optical transceiver\nto handle increased traffic from generative artificial intelligence, video\nstreaming services, and more. Improving the required signal-to-noise ratio\n(RSNR) by digital signal processing such as forward error correction (FEC)\nmitigates the requirements for electrical and optical components. The optical\ntransceivers in DCNs exploit a low-complexity soft-decision (SD) FEC,\nconsisting of short block-length linear error-correcting codes and a\nlow-complexity SD decoder (SDD), such as a Chase decoder and ordered\nstatistical decoding. The low complexity SDD efficiently approaches a maximum\nlikelihood decoding (MLD). However, the decoding performance of MLD is limited\nby its finite block length. In this paper, we describe the detail of our\nproposed channel-polarized multilevel coding with iterative decoding\n(CP-MLC-ID), which improves the decoding performance. The 19.5$\\%$-OH CP-MLC-ID\n128-bit extended Bose-Chaudhuri-Hocquenghem (eBCH) and KP4 codes outperform the\nconcatenated eBCH and KP4 codes with a net coding gain of 0.25 and 0.40 dB for\nthe same and double the number of SDDs, respectively. We also investigate the\ndependency of the decoding performance on the size of a bit interleaver. The\nperformance degradation of CP-MLC-ID using an 8-bit interleaver is about 0.1 dB\ncompared to using the large-bit interleaver. Our results indicate that even a\nweak connection by exclusive-OR between codewords improves the decoding\nperformance, compared to simple concatenated codes in the DCNs.", "published": "2025-03-25 12:31:48", "link": "http://arxiv.org/abs/2503.19601v2", "categories": ["eess.SP"], "primary_category": "eess.SP"}
