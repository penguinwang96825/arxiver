{"title": "Span-Aggregatable, Contextualized Word Embeddings for Effective Phrase\n  Mining", "abstract": "Dense vector representations for sentences made significant progress in\nrecent years as can be seen on sentence similarity tasks. Real-world phrase\nretrieval applications, on the other hand, still encounter challenges for\neffective use of dense representations. We show that when target phrases reside\ninside noisy context, representing the full sentence with a single dense\nvector, is not sufficient for effective phrase retrieval. We therefore look\ninto the notion of representing multiple, sub-sentence, consecutive word spans,\neach with its own dense vector. We show that this technique is much more\neffective for phrase mining, yet requires considerable compute to obtain useful\nspan representations. Accordingly, we make an argument for contextualized\nword/token embeddings that can be aggregated for arbitrary word spans while\nmaintaining the span's semantic meaning. We introduce a modification to the\ncommon contrastive loss used for sentence embeddings that encourages word\nembeddings to have this property. To demonstrate the effect of this method we\npresent a dataset based on the STS-B dataset with additional generated text,\nthat requires finding the best matching paraphrase residing in a larger context\nand report the degree of similarity to the origin phrase. We demonstrate on\nthis dataset, how our proposed method can achieve better results without\nsignificant increase to compute.", "published": "2024-05-12 12:08:05", "link": "http://arxiv.org/abs/2405.07263v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Branching Narratives: Character Decision Points Detection", "abstract": "This paper presents the Character Decision Points Detection (CHADPOD) task, a\ntask of identification of points within narratives where characters make\ndecisions that may significantly influence the story's direction. We propose a\nnovel dataset based on CYOA-like games graphs to be used as a benchmark for\nsuch a task. We provide a comparative analysis of different models' performance\non this task, including a couple of LLMs and several MLMs as baselines,\nachieving up to 89% accuracy. This underscores the complexity of narrative\nanalysis, showing the challenges associated with understanding character-driven\nstory dynamics. Additionally, we show how such a model can be applied to the\nexisting text to produce linear segments divided by potential branching points,\ndemonstrating the practical application of our findings in narrative analysis.", "published": "2024-05-12 13:36:07", "link": "http://arxiv.org/abs/2405.07282v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "L(u)PIN: LLM-based Political Ideology Nowcasting", "abstract": "The quantitative analysis of political ideological positions is a difficult\ntask. In the past, various literature focused on parliamentary voting data of\npoliticians, party manifestos and parliamentary speech to estimate political\ndisagreement and polarization in various political systems. However previous\nmethods of quantitative political analysis suffered from a common challenge\nwhich was the amount of data available for analysis. Also previous methods\nfrequently focused on a more general analysis of politics such as overall\npolarization of the parliament or party-wide political ideological positions.\nIn this paper, we present a method to analyze ideological positions of\nindividual parliamentary representatives by leveraging the latent knowledge of\nLLMs. The method allows us to evaluate the stance of politicians on an axis of\nour choice allowing us to flexibly measure the stance of politicians in regards\nto a topic/controversy of our choice. We achieve this by using a fine-tuned\nBERT classifier to extract the opinion-based sentences from the speeches of\nrepresentatives and projecting the average BERT embeddings for each\nrepresentative on a pair of reference seeds. These reference seeds are either\nmanually chosen representatives known to have opposing views on a particular\ntopic or they are generated sentences which where created using the GPT-4 model\nof OpenAI. We created the sentences by prompting the GPT-4 model to generate a\nspeech that would come from a politician defending a particular position.", "published": "2024-05-12 16:14:07", "link": "http://arxiv.org/abs/2405.07320v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Power and Ideology Identification in the Parliament: a\n  Reference Dataset and Simple Baselines", "abstract": "We introduce a dataset on political orientation and power position\nidentification. The dataset is derived from ParlaMint, a set of comparable\ncorpora of transcribed parliamentary speeches from 29 national and regional\nparliaments. We introduce the dataset, provide the reasoning behind some of the\nchoices during its creation, present statistics on the dataset, and, using a\nsimple classifier, some baseline results on predicting political orientation on\nthe left-to-right axis, and on power position identification, i.e.,\ndistinguishing between the speeches delivered by governing coalition party\nmembers from those of opposition party members.", "published": "2024-05-12 19:25:15", "link": "http://arxiv.org/abs/2405.07363v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "InsightNet: Structured Insight Mining from Customer Feedback", "abstract": "We propose InsightNet, a novel approach for the automated extraction of\nstructured insights from customer reviews. Our end-to-end machine learning\nframework is designed to overcome the limitations of current solutions,\nincluding the absence of structure for identified topics, non-standard aspect\nnames, and lack of abundant training data. The proposed solution builds a\nsemi-supervised multi-level taxonomy from raw reviews, a semantic similarity\nheuristic approach to generate labelled data and employs a multi-task insight\nextraction architecture by fine-tuning an LLM. InsightNet identifies granular\nactionable topics with customer sentiments and verbatim for each topic.\nEvaluations on real-world customer review data show that InsightNet performs\nbetter than existing solutions in terms of structure, hierarchy and\ncompleteness. We empirically demonstrate that InsightNet outperforms the\ncurrent state-of-the-art methods in multi-label topic classification, achieving\nan F1 score of 0.85, which is an improvement of 11% F1-score over the previous\nbest results. Additionally, InsightNet generalises well for unseen aspects and\nsuggests new topics to be added to the taxonomy.", "published": "2024-05-12 07:40:12", "link": "http://arxiv.org/abs/2405.07195v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Human-interpretable clustering of short-text using large language models", "abstract": "Clustering short text is a difficult problem, due to the low word\nco-occurrence between short text documents. This work shows that large language\nmodels (LLMs) can overcome the limitations of traditional clustering approaches\nby generating embeddings that capture the semantic nuances of short text. In\nthis study clusters are found in the embedding space using Gaussian Mixture\nModelling (GMM). The resulting clusters are found to be more distinctive and\nmore human-interpretable than clusters produced using the popular methods of\ndoc2vec and Latent Dirichlet Allocation (LDA). The success of the clustering\napproach is quantified using human reviewers and through the use of a\ngenerative LLM. The generative LLM shows good agreement with the human\nreviewers, and is suggested as a means to bridge the `validation gap' which\noften exists between cluster production and cluster interpretation. The\ncomparison between LLM-coding and human-coding reveals intrinsic biases in\neach, challenging the conventional reliance on human coding as the definitive\nstandard for cluster validation.", "published": "2024-05-12 12:55:40", "link": "http://arxiv.org/abs/2405.07278v2", "categories": ["cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "MedConceptsQA: Open Source Medical Concepts QA Benchmark", "abstract": "We present MedConceptsQA, a dedicated open source benchmark for medical\nconcepts question answering. The benchmark comprises of questions of various\nmedical concepts across different vocabularies: diagnoses, procedures, and\ndrugs. The questions are categorized into three levels of difficulty: easy,\nmedium, and hard. We conducted evaluations of the benchmark using various Large\nLanguage Models. Our findings show that pre-trained clinical Large Language\nModels achieved accuracy levels close to random guessing on this benchmark,\ndespite being pre-trained on medical data. However, GPT-4 achieves an absolute\naverage improvement of nearly 27%-37% (27% for zero-shot learning and 37% for\nfew-shot learning) when compared to clinical Large Language Models. Our\nbenchmark serves as a valuable resource for evaluating the understanding and\nreasoning of medical concepts by Large Language Models. Our benchmark is\navailable at https://huggingface.co/datasets/ofir408/MedConceptsQA", "published": "2024-05-12 17:54:50", "link": "http://arxiv.org/abs/2405.07348v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DuetRAG: Collaborative Retrieval-Augmented Generation", "abstract": "Retrieval-Augmented Generation (RAG) methods augment the input of Large\nLanguage Models (LLMs) with relevant retrieved passages, reducing factual\nerrors in knowledge-intensive tasks. However, contemporary RAG approaches\nsuffer from irrelevant knowledge retrieval issues in complex domain questions\n(e.g., HotPot QA) due to the lack of corresponding domain knowledge, leading to\nlow-quality generations. To address this issue, we propose a novel\nCollaborative Retrieval-Augmented Generation framework, DuetRAG. Our\nbootstrapping philosophy is to simultaneously integrate the domain fintuning\nand RAG models to improve the knowledge retrieval quality, thereby enhancing\ngeneration quality. Finally, we demonstrate DuetRAG' s matches with expert\nhuman researchers on HotPot QA.", "published": "2024-05-12 09:48:28", "link": "http://arxiv.org/abs/2405.13002v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MathDivide: Improved mathematical reasoning by large language models", "abstract": "Large language models have been proven to be capable of handling complex\nlinguistic and cognitive tasks. Therefore their usage has been extended to\ntasks requiring logical reasoning ability such as Mathematics. In this paper,\nwe propose a prompting technique called MathDivide that breaks down the\nmathematical problem into simpler subproblems. Each of the subproblems is\nformulated as an algebraic expression whose value is evaluated by the Python\ncode generated by the LLM for the corresponding algebraic expression. The\nvalues fed to the Python code are the numerical values provided in the problem\nstatement. The solutions for the subproblems are composed together to obtain\nthe final answer for the problem statement. Finally, the final answer is\ncompared to the correct answer. If the final answer matches the correct answer,\nit is produced as output else a refinement prompt is fed to the LLM. We\nexperiment with this prompting technique on both closed-source LLM models and\nopen-source LLM models using GSM8K dataset. The results obtained demonstrate\nthat MathDivide was able to significantly outperform the leading prompting\ntechnique called Math-prompter.", "published": "2024-05-12 20:21:15", "link": "http://arxiv.org/abs/2405.13004v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Limited Ability of LLMs to Simulate Human Psychological Behaviours: a\n  Psychometric Analysis", "abstract": "The humanlike responses of large language models (LLMs) have prompted social\nscientists to investigate whether LLMs can be used to simulate human\nparticipants in experiments, opinion polls and surveys. Of central interest in\nthis line of research has been mapping out the psychological profiles of LLMs\nby prompting them to respond to standardized questionnaires. The conflicting\nfindings of this research are unsurprising given that mapping out underlying,\nor latent, traits from LLMs' text responses to questionnaires is no easy task.\nTo address this, we use psychometrics, the science of psychological\nmeasurement. In this study, we prompt OpenAI's flagship models, GPT-3.5 and\nGPT-4, to assume different personas and respond to a range of standardized\nmeasures of personality constructs. We used two kinds of persona descriptions:\neither generic (four or five random person descriptions) or specific (mostly\ndemographics of actual humans from a large-scale human dataset). We found that\nthe responses from GPT-4, but not GPT-3.5, using generic persona descriptions\nshow promising, albeit not perfect, psychometric properties, similar to human\nnorms, but the data from both LLMs when using specific demographic profiles,\nshow poor psychometrics properties. We conclude that, currently, when LLMs are\nasked to simulate silicon personas, their responses are poor signals of\npotentially underlying latent traits. Thus, our work casts doubt on LLMs'\nability to simulate individual-level human behaviour across multiple-choice\nquestion answering tasks.", "published": "2024-05-12 10:52:15", "link": "http://arxiv.org/abs/2405.07248v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Humor Mechanics: Advancing Humor Generation with Multistep Reasoning", "abstract": "In this paper, we explore the generation of one-liner jokes through\nmulti-step reasoning. Our work involved reconstructing the process behind\ncreating humorous one-liners and developing a working prototype for humor\ngeneration. We conducted comprehensive experiments with human participants to\nevaluate our approach, comparing it with human-created jokes, zero-shot GPT-4\ngenerated humor, and other baselines. The evaluation focused on the quality of\nhumor produced, using human labeling as a benchmark. Our findings demonstrate\nthat the multi-step reasoning approach consistently improves the quality of\ngenerated humor. We present the results and share the datasets used in our\nexperiments, offering insights into enhancing humor generation with artificial\nintelligence.", "published": "2024-05-12 13:00:14", "link": "http://arxiv.org/abs/2405.07280v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "68T05, 68T30, 68T50, 91-08", "I.2.7; I.2.1; I.2.6; H.5.2"], "primary_category": "cs.CL"}
{"title": "Exploring the Potential of Conversational AI Support for Agent-Based\n  Social Simulation Model Design", "abstract": "ChatGPT, the AI-powered chatbot with a massive user base of hundreds of\nmillions, has become a global phenomenon. However, the use of Conversational AI\nSystems (CAISs) like ChatGPT for research in the field of Social Simulation is\nstill limited. Specifically, there is no evidence of its usage in Agent-Based\nSocial Simulation (ABSS) model design. While scepticism towards anything new is\ninherent to human nature, we firmly believe it is imperative to initiate the\nuse of this innovative technology to support ABSS model design. This paper\npresents a proof-of-concept that demonstrates how CAISs can facilitate the\ndevelopment of innovative conceptual ABSS models in a concise timeframe and\nwith minimal required upfront case-based knowledge. By employing advanced\nprompt engineering techniques and adhering to the Engineering ABSS framework,\nwe have constructed a comprehensive prompt script that enables the design of\nABSS models with or by the CAIS. The effectiveness of the script is\ndemonstrated through an illustrative case study concerning the use of adaptive\narchitecture in museums. Despite occasional inaccuracies and divergences in\nconversation, the CAIS proved to be a valuable companion for ABSS modellers.", "published": "2024-05-12 22:11:54", "link": "http://arxiv.org/abs/2405.08032v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.SE"], "primary_category": "cs.HC"}
{"title": "Bottleneck-Minimal Indexing for Generative Document Retrieval", "abstract": "We apply an information-theoretic perspective to reconsider generative\ndocument retrieval (GDR), in which a document $x \\in X$ is indexed by $t \\in\nT$, and a neural autoregressive model is trained to map queries $Q$ to $T$. GDR\ncan be considered to involve information transmission from documents $X$ to\nqueries $Q$, with the requirement to transmit more bits via the indexes $T$. By\napplying Shannon's rate-distortion theory, the optimality of indexing can be\nanalyzed in terms of the mutual information, and the design of the indexes $T$\ncan then be regarded as a {\\em bottleneck} in GDR. After reformulating GDR from\nthis perspective, we empirically quantify the bottleneck underlying GDR.\nFinally, using the NQ320K and MARCO datasets, we evaluate our proposed\nbottleneck-minimal indexing method in comparison with various previous indexing\nmethods, and we show that it outperforms those methods.", "published": "2024-05-12 11:41:26", "link": "http://arxiv.org/abs/2405.10974v2", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Large Language Models for Education: A Survey", "abstract": "Artificial intelligence (AI) has a profound impact on traditional education.\nIn recent years, large language models (LLMs) have been increasingly used in\nvarious applications such as natural language processing, computer vision,\nspeech recognition, and autonomous driving. LLMs have also been applied in many\nfields, including recommendation, finance, government, education, legal\naffairs, and finance. As powerful auxiliary tools, LLMs incorporate various\ntechnologies such as deep learning, pre-training, fine-tuning, and\nreinforcement learning. The use of LLMs for smart education (LLMEdu) has been a\nsignificant strategic direction for countries worldwide. While LLMs have shown\ngreat promise in improving teaching quality, changing education models, and\nmodifying teacher roles, the technologies are still facing several challenges.\nIn this paper, we conduct a systematic review of LLMEdu, focusing on current\ntechnologies, challenges, and future developments. We first summarize the\ncurrent state of LLMEdu and then introduce the characteristics of LLMs and\neducation, as well as the benefits of integrating LLMs into education. We also\nreview the process of integrating LLMs into the education industry, as well as\nthe introduction of related technologies. Finally, we discuss the challenges\nand problems faced by LLMEdu, as well as prospects for future optimization of\nLLMEdu.", "published": "2024-05-12 01:50:01", "link": "http://arxiv.org/abs/2405.13001v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "A Survey on Recent Advances in Conversational Data Generation", "abstract": "Recent advancements in conversational systems have significantly enhanced\nhuman-machine interactions across various domains. However, training these\nsystems is challenging due to the scarcity of specialized dialogue data.\nTraditionally, conversational datasets were created through crowdsourcing, but\nthis method has proven costly, limited in scale, and labor-intensive. As a\nsolution, the development of synthetic dialogue data has emerged, utilizing\ntechniques to augment existing datasets or convert textual resources into\nconversational formats, providing a more efficient and scalable approach to\ndataset creation. In this survey, we offer a systematic and comprehensive\nreview of multi-turn conversational data generation, focusing on three types of\ndialogue systems: open domain, task-oriented, and information-seeking. We\ncategorize the existing research based on key components like seed data\ncreation, utterance generation, and quality filtering methods, and introduce a\ngeneral framework that outlines the main principles of conversation data\ngeneration systems. Additionally, we examine the evaluation metrics and methods\nfor assessing synthetic conversational data, address current challenges in the\nfield, and explore potential directions for future research. Our goal is to\naccelerate progress for researchers and practitioners by presenting an overview\nof state-of-the-art methods and highlighting opportunities to further research\nin this area.", "published": "2024-05-12 10:11:12", "link": "http://arxiv.org/abs/2405.13003v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Understanding Sarcoidosis Using Large Language Models and Social Media\n  Data", "abstract": "Sarcoidosis is a rare inflammatory disease characterized by the formation of\ngranulomas in various organs. The disease presents diagnostic and treatment\nchallenges due to its diverse manifestations and unpredictable nature. In this\nstudy, we employed a Large Language Model (LLM) to analyze sarcoidosis-related\ndiscussions on the social media platform Reddit. Our findings underscore the\nefficacy of LLMs in accurately identifying sarcoidosis-related content. We\ndiscovered a wide array of symptoms reported by patients, with fatigue, swollen\nlymph nodes, and shortness of breath as the most prevalent. Prednisone was the\nmost prescribed medication, while infliximab showed the highest effectiveness\nin improving prognoses. Notably, our analysis revealed disparities in prognosis\nbased on age and gender, with women and younger patients experiencing good and\npolarized outcomes, respectively. Furthermore, unsupervised clustering\nidentified three distinct patient subgroups (phenotypes) with unique symptom\nprofiles, prognostic outcomes, and demographic distributions. Finally,\nsentiment analysis revealed a moderate negative impact on patients' mental\nhealth post-diagnosis, particularly among women and younger individuals. Our\nstudy represents the first application of LLMs to understand sarcoidosis\nthrough social media data. It contributes to understanding the disease by\nproviding data-driven insights into its manifestations, treatments, prognoses,\nand impact on patients' lives. Our findings have direct implications for\nimproving personalized treatment strategies and enhancing the quality of care\nfor individuals living with sarcoidosis.", "published": "2024-05-12 20:54:23", "link": "http://arxiv.org/abs/2405.13005v2", "categories": ["cs.CL", "cs.AI", "cs.SI"], "primary_category": "cs.CL"}
{"title": "SoccerNet-Echoes: A Soccer Game Audio Commentary Dataset", "abstract": "The application of Automatic Speech Recognition (ASR) technology in soccer\noffers numerous opportunities for sports analytics. Specifically, extracting\naudio commentaries with ASR provides valuable insights into the events of the\ngame, and opens the door to several downstream applications such as automatic\nhighlight generation. This paper presents SoccerNet-Echoes, an augmentation of\nthe SoccerNet dataset with automatically generated transcriptions of audio\ncommentaries from soccer game broadcasts, enhancing video content with rich\nlayers of textual information derived from the game audio using ASR. These\ntextual commentaries, generated using the Whisper model and translated with\nGoogle Translate, extend the usefulness of the SoccerNet dataset in diverse\napplications such as enhanced action spotting, automatic caption generation,\nand game summarization. By incorporating textual data alongside visual and\nauditory content, SoccerNet-Echoes aims to serve as a comprehensive resource\nfor the development of algorithms specialized in capturing the dynamics of\nsoccer games. We detail the methods involved in the curation of this dataset\nand the integration of ASR. We also highlight the implications of a multimodal\napproach in sports analytics, and how the enriched dataset can support diverse\napplications, thus broadening the scope of research and development in the\nfield of sports analytics.", "published": "2024-05-12 18:25:38", "link": "http://arxiv.org/abs/2405.07354v1", "categories": ["cs.SD", "cs.IR", "cs.LG", "cs.MM", "eess.AS", "I.2.7; I.7"], "primary_category": "cs.SD"}
{"title": "Unified Video-Language Pre-training with Synchronized Audio", "abstract": "Video-language pre-training is a typical and challenging problem that aims at\nlearning visual and textual representations from large-scale data in a\nself-supervised way. Existing pre-training approaches either captured the\ncorrespondence of image-text pairs or utilized temporal ordering of frames.\nHowever, they do not explicitly explore the natural synchronization between\naudio and the other two modalities. In this work, we propose an enhanced\nframework for Video-Language pre-training with Synchronized Audio, termed as\nVLSA, that can learn tri-modal representations in a unified self-supervised\ntransformer. Specifically, our VLSA jointly aggregates embeddings of local\npatches and global tokens for video, text, and audio. Furthermore, we utilize\nlocal-patch masked modeling to learn modality-aware features, and leverage\nglobal audio matching to capture audio-guided features for video and text. We\nconduct extensive experiments on retrieval across text, video, and audio. Our\nsimple model pre-trained on only 0.9M data achieves improving results against\nstate-of-the-art baselines. In addition, qualitative visualizations vividly\nshowcase the superiority of our VLSA in learning discriminative visual-textual\nrepresentations.", "published": "2024-05-12 07:59:46", "link": "http://arxiv.org/abs/2405.07202v1", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
