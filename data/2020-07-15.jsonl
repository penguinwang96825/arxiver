{"title": "UniTrans: Unifying Model Transfer and Data Transfer for Cross-Lingual\n  Named Entity Recognition with Unlabeled Data", "abstract": "Prior works in cross-lingual named entity recognition (NER) with no/little\nlabeled data fall into two primary categories: model transfer based and data\ntransfer based methods. In this paper we find that both method types can\ncomplement each other, in the sense that, the former can exploit context\ninformation via language-independent features but sees no task-specific\ninformation in the target language; while the latter generally generates pseudo\ntarget-language training data via translation but its exploitation of context\ninformation is weakened by inaccurate translations. Moreover, prior works\nrarely leverage unlabeled data in the target language, which can be\neffortlessly collected and potentially contains valuable information for\nimproved results. To handle both problems, we propose a novel approach termed\nUniTrans to Unify both model and data Transfer for cross-lingual NER, and\nfurthermore, to leverage the available information from unlabeled\ntarget-language data via enhanced knowledge distillation. We evaluate our\nproposed UniTrans over 4 target languages on benchmark datasets. Our\nexperimental results show that it substantially outperforms the existing\nstate-of-the-art methods.", "published": "2020-07-15 13:46:50", "link": "http://arxiv.org/abs/2007.07683v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Multilingual Parallel Corpora Collection Effort for Indian Languages", "abstract": "We present sentence aligned parallel corpora across 10 Indian Languages -\nHindi, Telugu, Tamil, Malayalam, Gujarati, Urdu, Bengali, Oriya, Marathi,\nPunjabi, and English - many of which are categorized as low resource. The\ncorpora are compiled from online sources which have content shared across\nlanguages. The corpora presented significantly extends present resources that\nare either not large enough or are restricted to a specific domain (such as\nhealth). We also provide a separate test corpus compiled from an independent\nonline source that can be independently used for validating the performance in\n10 Indian languages. Alongside, we report on the methods of constructing such\ncorpora using tools enabled by recent advances in machine translation and\ncross-lingual retrieval using deep neural network based methods.", "published": "2020-07-15 14:00:18", "link": "http://arxiv.org/abs/2007.07691v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dual Past and Future for Neural Machine Translation", "abstract": "Though remarkable successes have been achieved by Neural Machine Translation\n(NMT) in recent years, it still suffers from the inadequate-translation\nproblem. Previous studies show that explicitly modeling the Past and Future\ncontents of the source sentence is beneficial for translation performance.\nHowever, it is not clear whether the commonly used heuristic objective is good\nenough to guide the Past and Future. In this paper, we present a novel dual\nframework that leverages both source-to-target and target-to-source NMT models\nto provide a more direct and accurate supervision signal for the Past and\nFuture modules. Experimental results demonstrate that our proposed method\nsignificantly improves the adequacy of NMT predictions and surpasses previous\nmethods in two well-studied translation tasks.", "published": "2020-07-15 14:52:24", "link": "http://arxiv.org/abs/2007.07728v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AdapterHub: A Framework for Adapting Transformers", "abstract": "The current modus operandi in NLP involves downloading and fine-tuning\npre-trained models consisting of millions or billions of parameters. Storing\nand sharing such large trained models is expensive, slow, and time-consuming,\nwhich impedes progress towards more general and versatile NLP methods that\nlearn from and for many tasks. Adapters -- small learnt bottleneck layers\ninserted within each layer of a pre-trained model -- ameliorate this issue by\navoiding full fine-tuning of the entire model. However, sharing and integrating\nadapter layers is not straightforward. We propose AdapterHub, a framework that\nallows dynamic \"stitching-in\" of pre-trained adapters for different tasks and\nlanguages. The framework, built on top of the popular HuggingFace Transformers\nlibrary, enables extremely easy and quick adaptations of state-of-the-art\npre-trained models (e.g., BERT, RoBERTa, XLM-R) across tasks and languages.\nDownloading, sharing, and training adapters is as seamless as possible using\nminimal changes to the training scripts and a specialized infrastructure. Our\nframework enables scalable and easy access to sharing of task-specific models,\nparticularly in low-resource scenarios. AdapterHub includes all recent adapter\narchitectures and can be found at https://AdapterHub.ml.", "published": "2020-07-15 15:56:05", "link": "http://arxiv.org/abs/2007.07779v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-Tune Longformer for Jointly Predicting Rumor Stance and Veracity", "abstract": "Increased usage of social media caused the popularity of news and events\nwhich are not even verified, resulting in spread of rumors allover the web. Due\nto widely available social media platforms and increased usage caused the data\nto be available in huge amounts.The manual methods to process such large data\nis costly and time-taking, so there has been an increased attention to process\nand verify such content automatically for the presence of rumors. A lot of\nresearch studies reveal that to identify the stances of posts in the discussion\nthread of such events and news is an important preceding step before identify\nthe rumor veracity. In this paper,we propose a multi-task learning framework\nfor jointly predicting rumor stance and veracity on the dataset released at\nSemEval 2019 RumorEval: Determining rumor veracity and support for\nrumors(SemEval 2019 Task 7), which includes social media rumors stem from a\nvariety of breaking news stories from Reddit as well as Twit-ter. Our framework\nconsists of two parts: a) The bottom part of our framework classifies the\nstance for each post in the conversation thread discussing a rumor via\nmodelling the multi-turn conversation and make each post aware of its\nneighboring posts. b) The upper part predicts the rumor veracity of the\nconversation thread with stance evolution obtained from the bottom part.\nExperimental results on SemEval 2019 Task 7 dataset show that our method\noutperforms previous methods on both rumor stance classification and veracity\nprediction", "published": "2020-07-15 17:09:17", "link": "http://arxiv.org/abs/2007.07803v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "InfoXLM: An Information-Theoretic Framework for Cross-Lingual Language\n  Model Pre-Training", "abstract": "In this work, we present an information-theoretic framework that formulates\ncross-lingual language model pre-training as maximizing mutual information\nbetween multilingual-multi-granularity texts. The unified view helps us to\nbetter understand the existing methods for learning cross-lingual\nrepresentations. More importantly, inspired by the framework, we propose a new\npre-training task based on contrastive learning. Specifically, we regard a\nbilingual sentence pair as two views of the same meaning and encourage their\nencoded representations to be more similar than the negative examples. By\nleveraging both monolingual and parallel corpora, we jointly train the pretext\ntasks to improve the cross-lingual transferability of pre-trained models.\nExperimental results on several benchmarks show that our approach achieves\nconsiderably better performance. The code and pre-trained models are available\nat https://aka.ms/infoxlm.", "published": "2020-07-15 16:58:01", "link": "http://arxiv.org/abs/2007.07834v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Align then Summarize: Automatic Alignment Methods for Summarization\n  Corpus Creation", "abstract": "Summarizing texts is not a straightforward task. Before even considering text\nsummarization, one should determine what kind of summary is expected. How much\nshould the information be compressed? Is it relevant to reformulate or should\nthe summary stick to the original phrasing? State-of-the-art on automatic text\nsummarization mostly revolves around news articles. We suggest that considering\na wider variety of tasks would lead to an improvement in the field, in terms of\ngeneralization and robustness. We explore meeting summarization: generating\nreports from automatic transcriptions. Our work consists in segmenting and\naligning transcriptions with respect to reports, to get a suitable dataset for\nneural summarization. Using a bootstrapping approach, we provide pre-alignments\nthat are corrected by human annotators, making a validation set against which\nwe evaluate automatic models. This consistently reduces annotators' efforts by\nproviding iteratively better pre-alignment and maximizes the corpus size by\nusing annotations from our automatic alignment models. Evaluation is conducted\non \\publicmeetings, a novel corpus of aligned public meetings. We report\nautomatic alignment and summarization performances on this corpus and show that\nautomatic alignment is relevant for data annotation since it leads to large\nimprovement of almost +4 on all ROUGE scores on the summarization task.", "published": "2020-07-15 17:03:34", "link": "http://arxiv.org/abs/2007.07841v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Logic Constrained Pointer Networks for Interpretable Textual Similarity", "abstract": "Systematically discovering semantic relationships in text is an important and\nextensively studied area in Natural Language Processing, with various tasks\nsuch as entailment, semantic similarity, etc. Decomposability of sentence-level\nscores via subsequence alignments has been proposed as a way to make models\nmore interpretable. We study the problem of aligning components of sentences\nleading to an interpretable model for semantic textual similarity. In this\npaper, we introduce a novel pointer network based model with a sentinel gating\nfunction to align constituent chunks, which are represented using BERT. We\nimprove this base model with a loss function to equally penalize misalignments\nin both sentences, ensuring the alignments are bidirectional. Finally, to guide\nthe network with structured external knowledge, we introduce first-order logic\nconstraints based on ConceptNet and syntactic knowledge. The model achieves an\nF1 score of 97.73 and 96.32 on the benchmark SemEval datasets for the chunk\nalignment task, showing large improvements over the existing solutions. Source\ncode is available at\nhttps://github.com/manishb89/interpretable_sentence_similarity", "published": "2020-07-15 13:01:44", "link": "http://arxiv.org/abs/2007.07670v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multimodal Word Sense Disambiguation in Creative Practice", "abstract": "Language is ambiguous; many terms and expressions can convey the same idea.\nThis is especially true in creative practice, where ideas and design intents\nare highly subjective. We present a dataset, Ambiguous Descriptions of Art\nImages (ADARI), of contemporary workpieces, which aims to provide a\nfoundational resource for subjective image description and multimodal word\ndisambiguation in the context of creative practice. The dataset contains a\ntotal of 240k images labeled with 260k descriptive sentences. It is\nadditionally organized into sub-domains of architecture, art, design, fashion,\nfurniture, product design and technology. In subjective image description,\nlabels are not deterministic: for example, the ambiguous label dynamic might\ncorrespond to hundreds of different images. To understand this complexity, we\nanalyze the ambiguity and relevance of text with respect to images using the\nstate-of-the-art pre-trained BERT model for sentence classification. We provide\na baseline for multi-label classification tasks and demonstrate the potential\nof multimodal approaches for understanding ambiguity in design intentions. We\nhope that ADARI dataset and baselines constitute a first step towards\nsubjective label classification.", "published": "2020-07-15 15:34:35", "link": "http://arxiv.org/abs/2007.07758v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Dialect Diversity in Text Summarization on Twitter", "abstract": "Discussions on Twitter involve participation from different communities with\ndifferent dialects and it is often necessary to summarize a large number of\nposts into a representative sample to provide a synopsis. Yet, any such\nrepresentative sample should sufficiently portray the underlying dialect\ndiversity to present the voices of different participating communities\nrepresenting the dialects. Extractive summarization algorithms perform the task\nof constructing subsets that succinctly capture the topic of any given set of\nposts. However, we observe that there is dialect bias in the summaries\ngenerated by common summarization approaches, i.e., they often return summaries\nthat under-represent certain dialects.\n  The vast majority of existing \"fair\" summarization approaches require\nsocially salient attribute labels (in this case, dialect) to ensure that the\ngenerated summary is fair with respect to the socially salient attribute.\nNevertheless, in many applications, these labels do not exist. Furthermore, due\nto the ever-evolving nature of dialects in social media, it is unreasonable to\nlabel or accurately infer the dialect of every social media post. To correct\nfor the dialect bias, we employ a framework that takes an existing text\nsummarization algorithm as a blackbox and, using a small set of dialect-diverse\nsentences, returns a summary that is relatively more dialect-diverse.\nCrucially, this approach does not need the posts being summarized to have\ndialect labels, ensuring that the diversification process is independent of\ndialect classification/identification models. We show the efficacy of our\napproach on Twitter datasets containing posts written in dialects used by\ndifferent social groups defined by race or gender; in all cases, our approach\nleads to improved dialect diversity compared to standard text summarization\napproaches.", "published": "2020-07-15 17:24:41", "link": "http://arxiv.org/abs/2007.07860v2", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Sinhala Language Corpora and Stopwords from a Decade of Sri Lankan\n  Facebook", "abstract": "This paper presents two colloquial Sinhala language corpora from the language\nefforts of the Data, Analysis and Policy team of LIRNEasia, as well as a list\nof algorithmically derived stopwords. The larger of the two corpora spans 2010\nto 2020 and contains 28,825,820 to 29,549,672 words of multilingual text posted\nby 533 Sri Lankan Facebook pages, including politics, media, celebrities, and\nother categories; the smaller corpus amounts to 5,402,76 words of only Sinhala\ntext extracted from the larger. Both corpora have markers for their date of\ncreation, page of origin, and content type.", "published": "2020-07-15 17:57:56", "link": "http://arxiv.org/abs/2007.07884v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Are We There Yet? Evaluating State-of-the-Art Neural Network based\n  Geoparsers Using EUPEG as a Benchmarking Platform", "abstract": "Geoparsing is an important task in geographic information retrieval. A\ngeoparsing system, known as a geoparser, takes some texts as the input and\noutputs the recognized place mentions and their location coordinates. In June\n2019, a geoparsing competition, Toponym Resolution in Scientific Papers, was\nheld as one of the SemEval 2019 tasks. The winning teams developed neural\nnetwork based geoparsers that achieved outstanding performances (over 90%\nprecision, recall, and F1 score for toponym recognition). This exciting result\nbrings the question \"are we there yet?\", namely have we achieved high enough\nperformances to possibly consider the problem of geoparsing as solved? One\nlimitation of this competition is that the developed geoparsers were tested on\nonly one dataset which has 45 research articles collected from the particular\ndomain of Bio-medicine. It is known that the same geoparser can have very\ndifferent performances on different datasets. Thus, this work performs a\nsystematic evaluation of these state-of-the-art geoparsers using our recently\ndeveloped benchmarking platform EUPEG that has eight annotated datasets, nine\nbaseline geoparsers, and eight performance metrics. The evaluation result\nsuggests that these new geoparsers indeed improve the performances of\ngeoparsing on multiple datasets although some challenges remain.", "published": "2020-07-15 03:13:15", "link": "http://arxiv.org/abs/2007.07455v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Predicting Clinical Diagnosis from Patients Electronic Health Records\n  Using BERT-based Neural Networks", "abstract": "In this paper we study the problem of predicting clinical diagnoses from\ntextual Electronic Health Records (EHR) data. We show the importance of this\nproblem in medical community and present comprehensive historical review of the\nproblem and proposed methods. As the main scientific contributions we present a\nmodification of Bidirectional Encoder Representations from Transformers (BERT)\nmodel for sequence classification that implements a novel way of\nFully-Connected (FC) layer composition and a BERT model pretrained only on\ndomain data. To empirically validate our model, we use a large-scale Russian\nEHR dataset consisting of about 4 million unique patient visits. This is the\nlargest such study for the Russian language and one of the largest globally. We\nperformed a number of comparative experiments with other text representation\nmodels on the task of multiclass classification for 265 disease subset of\nICD-10. The experiments demonstrate improved performance of our models compared\nto other baselines, including a fine-tuned Russian BERT (RuBERT) variant. We\nalso show comparable performance of our model with a panel of experienced\nmedical experts. This allows us to hope that implementation of this system will\nreduce misdiagnosis.", "published": "2020-07-15 09:22:55", "link": "http://arxiv.org/abs/2007.07562v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cross-Lingual Speaker Verification with Domain-Balanced Hard Prototype\n  Mining and Language-Dependent Score Normalization", "abstract": "In this paper we describe the top-scoring IDLab submission for the\ntext-independent task of the Short-duration Speaker Verification (SdSV)\nChallenge 2020. The main difficulty of the challenge exists in the large degree\nof varying phonetic overlap between the potentially cross-lingual trials, along\nwith the limited availability of in-domain DeepMine Farsi training data. We\nintroduce domain-balanced hard prototype mining to fine-tune the\nstate-of-the-art ECAPA-TDNN x-vector based speaker embedding extractor. The\nsample mining technique efficiently exploits speaker distances between the\nspeaker prototypes of the popular AAM-softmax loss function to construct\nchallenging training batches that are balanced on the domain-level. To enhance\nthe scoring of cross-lingual trials, we propose a language-dependent s-norm\nscore normalization. The imposter cohort only contains data from the Farsi\ntarget-domain which simulates the enrollment data always being Farsi. In case a\nGaussian-Backend language model detects the test speaker embedding to contain\nEnglish, a cross-language compensation offset determined on the AAM-softmax\nspeaker prototypes is subtracted from the maximum expected imposter mean score.\nA fusion of five systems with minor topological tweaks resulted in a final\nMinDCF and EER of 0.065 and 1.45% respectively on the SdSVC evaluation set.", "published": "2020-07-15 13:58:18", "link": "http://arxiv.org/abs/2007.07689v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Fighting the COVID-19 Infodemic in Social Media: A Holistic Perspective\n  and a Call to Arms", "abstract": "With the outbreak of the COVID-19 pandemic, people turned to social media to\nread and to share timely information including statistics, warnings, advice,\nand inspirational stories. Unfortunately, alongside all this useful\ninformation, there was also a new blending of medical and political\nmisinformation and disinformation, which gave rise to the first global\ninfodemic. While fighting this infodemic is typically thought of in terms of\nfactuality, the problem is much broader as malicious content includes not only\nfake news, rumors, and conspiracy theories, but also promotion of fake cures,\npanic, racism, xenophobia, and mistrust in the authorities, among others. This\nis a complex problem that needs a holistic approach combining the perspectives\nof journalists, fact-checkers, policymakers, government entities, social media\nplatforms, and society as a whole. Taking them into account we define an\nannotation schema and detailed annotation instructions, which reflect these\nperspectives. We performed initial annotations using this schema, and our\ninitial experiments demonstrated sizable improvements over the baselines. Now,\nwe issue a call to arms to the research community and beyond to join the fight\nby supporting our crowdsourcing annotation efforts.", "published": "2020-07-15 21:18:30", "link": "http://arxiv.org/abs/2007.07996v2", "categories": ["cs.IR", "cs.CL", "cs.LG", "cs.SI", "68T50", "I.2.7"], "primary_category": "cs.IR"}
{"title": "Overview of CheckThat! 2020: Automatic Identification and Verification\n  of Claims in Social Media", "abstract": "We present an overview of the third edition of the CheckThat! Lab at CLEF\n2020. The lab featured five tasks in two different languages: English and\nArabic. The first four tasks compose the full pipeline of claim verification in\nsocial media: Task 1 on check-worthiness estimation, Task 2 on retrieving\npreviously fact-checked claims, Task 3 on evidence retrieval, and Task 4 on\nclaim verification. The lab is completed with Task 5 on check-worthiness\nestimation in political debates and speeches. A total of 67 teams registered to\nparticipate in the lab (up from 47 at CLEF 2019), and 23 of them actually\nsubmitted runs (compared to 14 at CLEF 2019). Most teams used deep neural\nnetworks based on BERT, LSTMs, or CNNs, and achieved sizable improvements over\nthe baselines on all tasks. Here we describe the tasks setup, the evaluation\nresults, and a summary of the approaches used by the participants, and we\ndiscuss some lessons learned. Last but not least, we release to the research\ncommunity all datasets from the lab as well as the evaluation scripts, which\nshould enable further research in the important tasks of check-worthiness\nestimation and automatic claim verification.", "published": "2020-07-15 21:19:32", "link": "http://arxiv.org/abs/2007.07997v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "A Survey on Computational Propaganda Detection", "abstract": "Propaganda campaigns aim at influencing people's mindset with the purpose of\nadvancing a specific agenda. They exploit the anonymity of the Internet, the\nmicro-profiling ability of social networks, and the ease of automatically\ncreating and managing coordinated networks of accounts, to reach millions of\nsocial network users with persuasive messages, specifically targeted to topics\neach individual user is sensitive to, and ultimately influencing the outcome on\na targeted issue. In this survey, we review the state of the art on\ncomputational propaganda detection from the perspective of Natural Language\nProcessing and Network Analysis, arguing about the need for combined efforts\nbetween these communities. We further discuss current challenges and future\nresearch directions.", "published": "2020-07-15 22:25:51", "link": "http://arxiv.org/abs/2007.08024v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "A survey and an extensive evaluation of popular audio declipping methods", "abstract": "Dynamic range limitations in signal processing often lead to clipping, or\nsaturation, in signals. The task of audio declipping is estimating the original\naudio signal, given its clipped measurements, and has attracted much interest\nin recent years. Audio declipping algorithms often make assumptions about the\nunderlying signal, such as sparsity or low-rankness, and about the measurement\nsystem. In this paper, we provide an extensive review of audio declipping\nalgorithms proposed in the literature. For each algorithm, we present\nassumptions that are made about the audio signal, the modeling domain, and the\noptimization algorithm. Furthermore, we provide an extensive numerical\nevaluation of popular declipping algorithms, on real audio data. We evaluate\neach algorithm in terms of the Signal-to-Distortion Ratio, and also using\nperceptual metrics of sound quality. The article is accompanied by a repository\ncontaining the evaluated methods.", "published": "2020-07-15 12:43:29", "link": "http://arxiv.org/abs/2007.07663v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "An Ensemble of Convolutional Neural Networks for Audio Classification", "abstract": "In this paper, ensembles of classifiers that exploit several data\naugmentation techniques and four signal representations for training\nConvolutional Neural Networks (CNNs) for audio classification are presented and\ntested on three freely available audio classification datasets: i) bird calls,\nii) cat sounds, and iii) the Environmental Sound Classification dataset. The\nbest performing ensembles combining data augmentation techniques with different\nsignal representations are compared and shown to outperform the best methods\nreported in the literature on these datasets. The approach proposed here\nobtains state-of-the-art results in the widely used ESC-50 dataset. To the best\nof our knowledge, this is the most extensive study investigating ensembles of\nCNNs for audio classification. Results demonstrate not only that CNNs can be\ntrained for audio classification but also that their fusion using different\ntechniques works better than the stand-alone classifiers.", "published": "2020-07-15 19:41:15", "link": "http://arxiv.org/abs/2007.07966v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
