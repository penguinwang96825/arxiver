{"title": "Attribute Alignment: Controlling Text Generation from Pre-trained\n  Language Models", "abstract": "Large language models benefit from training with a large amount of unlabeled\ntext, which gives them increasingly fluent and diverse generation capabilities.\nHowever, using these models for text generation that takes into account target\nattributes, such as sentiment polarity or specific topics, remains a challenge.\nWe propose a simple and flexible method for controlling text generation by\naligning disentangled attribute representations. In contrast to recent efforts\non training a discriminator to perturb the token level distribution for an\nattribute, we use the same data to learn an alignment function to guide the\npre-trained, non-controlled language model to generate texts with the target\nattribute without changing the original language model parameters. We evaluate\nour method on sentiment- and topic-controlled generation, and show large\nperformance gains over previous methods while retaining fluency and diversity.", "published": "2021-03-20 01:51:32", "link": "http://arxiv.org/abs/2103.11070v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Interplay of Task Success and Dialogue Quality: An in-depth\n  Evaluation in Task-Oriented Visual Dialogues", "abstract": "When training a model on referential dialogue guessing games, the best model\nis usually chosen based on its task success. We show that in the popular\nend-to-end approach, this choice prevents the model from learning to generate\nlinguistically richer dialogues, since the acquisition of language proficiency\ntakes longer than learning the guessing task. By comparing models playing\ndifferent games (GuessWhat, GuessWhich, and Mutual Friends), we show that this\ndiscrepancy is model- and task-agnostic. We investigate whether and when better\nlanguage quality could lead to higher task success. We show that in GuessWhat,\nmodels could increase their accuracy if they learn to ground, encode, and\ndecode also words that do not occur frequently in the training set.", "published": "2021-03-20 10:13:30", "link": "http://arxiv.org/abs/2103.11151v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Unlabeled Data for Entity-Relation Extraction through\n  Probabilistic Constraint Satisfaction", "abstract": "We study the problem of entity-relation extraction in the presence of\nsymbolic domain knowledge. Such knowledge takes the form of an ontology\ndefining relations and their permissible arguments. Previous approaches set out\nto integrate such knowledge in their learning approaches either through\nself-training, or through approximations that lose the precise meaning of the\nlogical expressions. By contrast, our approach employs semantic loss which\ncaptures the precise meaning of a logical sentence through maintaining a\nprobability distribution over all possible states, and guiding the model to\nsolutions which minimize any constraint violations. With a focus on low-data\nregimes, we show that semantic loss outperforms the baselines by a wide margin.", "published": "2021-03-20 00:16:29", "link": "http://arxiv.org/abs/2103.11062v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Local Interpretations for Explainable Natural Language Processing: A\n  Survey", "abstract": "As the use of deep learning techniques has grown across various fields over\nthe past decade, complaints about the opaqueness of the black-box models have\nincreased, resulting in an increased focus on transparency in deep learning\nmodels. This work investigates various methods to improve the interpretability\nof deep neural networks for Natural Language Processing (NLP) tasks, including\nmachine translation and sentiment analysis. We provide a comprehensive\ndiscussion on the definition of the term interpretability and its various\naspects at the beginning of this work. The methods collected and summarised in\nthis survey are only associated with local interpretation and are specifically\ndivided into three categories: 1) interpreting the model's predictions through\nrelated input features; 2) interpreting through natural language explanation;\n3) probing the hidden states of models and word representations.", "published": "2021-03-20 02:28:33", "link": "http://arxiv.org/abs/2103.11072v3", "categories": ["cs.CL", "cs.AI", "A.1; I.2.7"], "primary_category": "cs.CL"}
{"title": "Token-wise Curriculum Learning for Neural Machine Translation", "abstract": "Existing curriculum learning approaches to Neural Machine Translation (NMT)\nrequire sampling sufficient amounts of \"easy\" samples from training data at the\nearly training stage. This is not always achievable for low-resource languages\nwhere the amount of training data is limited. To address such limitation, we\npropose a novel token-wise curriculum learning approach that creates sufficient\namounts of easy samples. Specifically, the model learns to predict a short\nsub-sequence from the beginning part of each target sentence at the early stage\nof training, and then the sub-sequence is gradually expanded as the training\nprogresses. Such a new curriculum design is inspired by the cumulative effect\nof translation errors, which makes the latter tokens more difficult to predict\nthan the beginning ones. Extensive experiments show that our approach can\nconsistently outperform baselines on 5 language pairs, especially for\nlow-resource languages. Combining our approach with sentence-level methods\nfurther improves the performance on high-resource languages.", "published": "2021-03-20 03:57:59", "link": "http://arxiv.org/abs/2103.11088v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Dependency Graph-to-String Statistical Machine Translation", "abstract": "We present graph-based translation models which translate source graphs into\ntarget strings. Source graphs are constructed from dependency trees with extra\nlinks so that non-syntactic phrases are connected. Inspired by phrase-based\nmodels, we first introduce a translation model which segments a graph into a\nsequence of disjoint subgraphs and generates a translation by combining\nsubgraph translations left-to-right using beam search. However, similar to\nphrase-based models, this model is weak at phrase reordering. Therefore, we\nfurther introduce a model based on a synchronous node replacement grammar which\nlearns recursive translation rules. We provide two implementations of the model\nwith different restrictions so that source graphs can be parsed efficiently.\nExperiments on Chinese--English and German--English show that our graph-based\nmodels are significantly better than corresponding sequence- and tree-based\nbaselines.", "published": "2021-03-20 04:20:56", "link": "http://arxiv.org/abs/2103.11089v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Overprotective Training Environments Fall Short at Testing Time: Let\n  Models Contribute to Their Own Training", "abstract": "Despite important progress, conversational systems often generate dialogues\nthat sound unnatural to humans. We conjecture that the reason lies in their\ndifferent training and testing conditions: agents are trained in a controlled\n\"lab\" setting but tested in the \"wild\". During training, they learn to generate\nan utterance given the human dialogue history. On the other hand, during\ntesting, they must interact with each other, and hence deal with noisy data. We\npropose to fill this gap by training the model with mixed batches containing\nboth samples of human and machine-generated dialogues. We assess the validity\nof the proposed method on GuessWhat?!, a visual referential game.", "published": "2021-03-20 09:55:50", "link": "http://arxiv.org/abs/2103.11145v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "The Effectiveness of Morphology-aware Segmentation in Low-Resource\n  Neural Machine Translation", "abstract": "This paper evaluates the performance of several modern subword segmentation\nmethods in a low-resource neural machine translation setting. We compare\nsegmentations produced by applying BPE at the token or sentence level with\nmorphologically-based segmentations from LMVR and MORSEL. We evaluate\ntranslation tasks between English and each of Nepali, Sinhala, and Kazakh, and\npredict that using morphologically-based segmentation methods would lead to\nbetter performance in this setting. However, comparing to BPE, we find that no\nconsistent and reliable differences emerge between the segmentation methods.\nWhile morphologically-based methods outperform BPE in a few cases, what\nperforms best tends to vary across tasks, and the performance of segmentation\nmethods is often statistically indistinguishable.", "published": "2021-03-20 14:39:25", "link": "http://arxiv.org/abs/2103.11189v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Self-Supervised Test-Time Learning for Reading Comprehension", "abstract": "Recent work on unsupervised question answering has shown that models can be\ntrained with procedurally generated question-answer pairs and can achieve\nperformance competitive with supervised methods. In this work, we consider the\ntask of unsupervised reading comprehension and present a method that performs\n\"test-time learning\" (TTL) on a given context (text passage), without requiring\ntraining on large-scale human-authored datasets containing\n\\textit{context-question-answer} triplets. This method operates directly on a\nsingle test context, uses self-supervision to train models on synthetically\ngenerated question-answer pairs, and then infers answers to unseen\nhuman-authored questions for this context. Our method achieves accuracies\ncompetitive with fully supervised methods and significantly outperforms current\nunsupervised methods. TTL methods with a smaller model are also competitive\nwith the current state-of-the-art in unsupervised reading comprehension.", "published": "2021-03-20 23:24:51", "link": "http://arxiv.org/abs/2103.11263v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "QUCoughScope: An Artificially Intelligent Mobile Application to Detect\n  Asymptomatic COVID-19 Patients using Cough and Breathing Sounds", "abstract": "In the break of COVID-19 pandemic, mass testing has become essential to\nreduce the spread of the virus. Several recent studies suggest that a\nsignificant number of COVID-19 patients display no physical symptoms\nwhatsoever. Therefore, it is unlikely that these patients will undergo COVID-19\ntest, which increases their chances of unintentionally spreading the virus.\nCurrently, the primary diagnostic tool to detect COVID-19 is RT-PCR test on\ncollected respiratory specimens from the suspected case. This requires patients\nto travel to a laboratory facility to be tested, thereby potentially infecting\nothers along the way.It is evident from recent researches that asymptomatic\nCOVID-19 patients cough and breath in a different way than the healthy people.\nSeveral research groups have created mobile and web-platform for crowdsourcing\nthe symptoms, cough and breathing sounds from healthy, COVID-19 and Non-COVID\npatients. Some of these data repositories were made public. We have received\nsuch a repository from Cambridge University team under data-sharing agreement,\nwhere we have cough and breathing sound samples for 582 and 141 healthy and\nCOVID-19 patients, respectively. 87 COVID-19 patients were asymptomatic, while\nrest of them have cough. We have developed an Android application to\nautomatically screen COVID-19 from the comfort of people homes. Test subjects\ncan simply download a mobile application, enter their symptoms, record an audio\nclip of their cough and breath, and upload the data anonymously to our servers.\nOur backend server converts the audio clip to spectrogram and then apply our\nstate-of-the-art machine learning model to classify between cough sounds\nproduced by COVID-19 patients, as opposed to healthy subjects or those with\nother respiratory conditions. The system can detect asymptomatic COVID-19\npatients with a sensitivity more than 91%.", "published": "2021-03-20 18:26:39", "link": "http://arxiv.org/abs/2103.12063v1", "categories": ["eess.AS", "cs.HC", "cs.SD"], "primary_category": "eess.AS"}
