{"title": "A Discourse-Level Named Entity Recognition and Relation Extraction\n  Dataset for Chinese Literature Text", "abstract": "Named Entity Recognition and Relation Extraction for Chinese literature text\nis regarded as the highly difficult problem, partially because of the lack of\ntagging sets. In this paper, we build a discourse-level dataset from hundreds\nof Chinese literature articles for improving this task. To build a high quality\ndataset, we propose two tagging methods to solve the problem of data\ninconsistency, including a heuristic tagging method and a machine auxiliary\ntagging method. Based on this corpus, we also introduce several widely used\nmodels to conduct experiments. Experimental results not only show the\nusefulness of the proposed dataset, but also provide baselines for further\nresearch. The dataset is available at\nhttps://github.com/lancopku/Chinese-Literature-NER-RE-Dataset", "published": "2017-11-19 12:29:59", "link": "http://arxiv.org/abs/1711.07010v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Incorporating Syntactic Uncertainty in Neural Machine Translation with\n  Forest-to-Sequence Model", "abstract": "Incorporating syntactic information in Neural Machine Translation models is a\nmethod to compensate their requirement for a large amount of parallel training\ntext, especially for low-resource language pairs. Previous works on using\nsyntactic information provided by (inevitably error-prone) parsers has been\npromising. In this paper, we propose a forest-to-sequence Attentional Neural\nMachine Translation model to make use of exponentially many parse trees of the\nsource sentence to compensate for the parser errors. Our method represents the\ncollection of parse trees as a packed forest, and learns a neural attentional\ntransduction model from the forest to the target sentence. Experiments on\nEnglish to German, Chinese and Persian translation show the superiority of our\nmethod over the tree-to-sequence and vanilla sequence-to-sequence neural\ntranslation models.", "published": "2017-11-19 13:57:30", "link": "http://arxiv.org/abs/1711.07019v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Intelligent Word Embeddings of Free-Text Radiology Reports", "abstract": "Radiology reports are a rich resource for advancing deep learning\napplications in medicine by leveraging the large volume of data continuously\nbeing updated, integrated, and shared. However, there are significant\nchallenges as well, largely due to the ambiguity and subtlety of natural\nlanguage. We propose a hybrid strategy that combines semantic-dictionary\nmapping and word2vec modeling for creating dense vector embeddings of free-text\nradiology reports. Our method leverages the benefits of both\nsemantic-dictionary mapping as well as unsupervised learning. Using the vector\nrepresentation, we automatically classify the radiology reports into three\nclasses denoting confidence in the diagnosis of intracranial hemorrhage by the\ninterpreting radiologist. We performed experiments with varying hyperparameter\nsettings of the word embeddings and a range of different classifiers. Best\nperformance achieved was a weighted precision of 88% and weighted recall of\n90%. Our work offers the potential to leverage unstructured electronic health\nrecord data by allowing direct analysis of narrative clinical notes.", "published": "2017-11-19 05:14:36", "link": "http://arxiv.org/abs/1711.06968v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Prior-aware Dual Decomposition: Document-specific Topic Inference for\n  Spectral Topic Models", "abstract": "Spectral topic modeling algorithms operate on matrices/tensors of word\nco-occurrence statistics to learn topic-specific word distributions. This\napproach removes the dependence on the original documents and produces\nsubstantial gains in efficiency and provable topic inference, but at a cost:\nthe model can no longer provide information about the topic composition of\nindividual documents. Recently Thresholded Linear Inverse (TLI) is proposed to\nmap the observed words of each document back to its topic composition. However,\nits linear characteristics limit the inference quality without considering the\nimportant prior information over topics. In this paper, we evaluate Simple\nProbabilistic Inverse (SPI) method and novel Prior-aware Dual Decomposition\n(PADD) that is capable of learning document-specific topic compositions in\nparallel. Experiments show that PADD successfully leverages topic correlations\nas a prior, notably outperforming TLI and learning quality topic compositions\ncomparable to Gibbs sampling on various data.", "published": "2017-11-19 19:56:23", "link": "http://arxiv.org/abs/1711.07065v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
