{"title": "Easy-First Dependency Parsing with Hierarchical Tree LSTMs", "abstract": "We suggest a compositional vector representation of parse trees that relies\non a recursive combination of recurrent-neural network encoders. To demonstrate\nits effectiveness, we use the representation as the backbone of a greedy,\nbottom-up dependency parser, achieving state-of-the-art accuracies for English\nand Chinese, without relying on external word embeddings. The parser's\nimplementation is available for download at the first author's webpage.", "published": "2016-03-01 17:43:37", "link": "http://arxiv.org/abs/1603.00375v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Event Search and Analytics: Detecting Events in Semantically Annotated\n  Corpora for Search and Analytics", "abstract": "In this article, I present the questions that I seek to answer in my PhD\nresearch. I posit to analyze natural language text with the help of semantic\nannotations and mine important events for navigating large text corpora.\nSemantic annotations such as named entities, geographic locations, and temporal\nexpressions can help us mine events from the given corpora. These events thus\nprovide us with useful means to discover the locked knowledge in them. I pose\nthree problems that can help unlock this knowledge vault in semantically\nannotated text corpora: i. identifying important events; ii. semantic search;\nand iii. event analytics.", "published": "2016-03-01 13:14:33", "link": "http://arxiv.org/abs/1603.00260v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Characterizing Diseases from Unstructured Text: A Vocabulary Driven\n  Word2vec Approach", "abstract": "Traditional disease surveillance can be augmented with a wide variety of\nreal-time sources such as, news and social media. However, these sources are in\ngeneral unstructured and, construction of surveillance tools such as\ntaxonomical correlations and trace mapping involves considerable human\nsupervision. In this paper, we motivate a disease vocabulary driven word2vec\nmodel (Dis2Vec) to model diseases and constituent attributes as word embeddings\nfrom the HealthMap news corpus. We use these word embeddings to automatically\ncreate disease taxonomies and evaluate our model against corresponding human\nannotated taxonomies. We compare our model accuracies against several\nstate-of-the art word2vec methods. Our results demonstrate that Dis2Vec\noutperforms traditional distributed vector representations in its ability to\nfaithfully capture taxonomical attributes across different class of diseases\nsuch as endemic, emerging and rare.", "published": "2016-03-01 00:45:18", "link": "http://arxiv.org/abs/1603.00106v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Segmental Recurrent Neural Networks for End-to-end Speech Recognition", "abstract": "We study the segmental recurrent neural network for end-to-end acoustic\nmodelling. This model connects the segmental conditional random field (CRF)\nwith a recurrent neural network (RNN) used for feature extraction. Compared to\nmost previous CRF-based acoustic models, it does not rely on an external system\nto provide features or segmentation boundaries. Instead, this model\nmarginalises out all the possible segmentations, and features are extracted\nfrom the RNN trained together with the segmental CRF. In essence, this model is\nself-contained and can be trained end-to-end. In this paper, we discuss\npractical training and decoding issues as well as the method to speed up the\ntraining in the context of speech recognition. We performed experiments on the\nTIMIT dataset. We achieved 17.3 phone error rate (PER) from the first-pass\ndecoding --- the best reported result using CRFs, despite the fact that we only\nused a zeroth-order CRF and without using any language model.", "published": "2016-03-01 10:43:43", "link": "http://arxiv.org/abs/1603.00223v2", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Quantifying the vanishing gradient and long distance dependency problem\n  in recursive neural networks and recursive LSTMs", "abstract": "Recursive neural networks (RNN) and their recently proposed extension\nrecursive long short term memory networks (RLSTM) are models that compute\nrepresentations for sentences, by recursively combining word embeddings\naccording to an externally provided parse tree. Both models thus, unlike\nrecurrent networks, explicitly make use of the hierarchical structure of a\nsentence. In this paper, we demonstrate that RNNs nevertheless suffer from the\nvanishing gradient and long distance dependency problem, and that RLSTMs\ngreatly improve over RNN's on these problems. We present an artificial learning\ntask that allows us to quantify the severity of these problems for both models.\nWe further show that a ratio of gradients (at the root node and a focal leaf\nnode) is highly indicative of the success of backpropagation at optimizing the\nrelevant weights low in the tree. This paper thus provides an explanation for\nexisting, superior results of RLSTMs on tasks such as sentiment analysis, and\nsuggests that the benefits of including hierarchical structure and of including\nLSTM-style gating are complementary.", "published": "2016-03-01 19:45:25", "link": "http://arxiv.org/abs/1603.00423v1", "categories": ["cs.AI", "cs.CL", "cs.NE"], "primary_category": "cs.AI"}
