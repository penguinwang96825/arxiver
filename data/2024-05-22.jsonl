{"title": "DiffNorm: Self-Supervised Normalization for Non-autoregressive\n  Speech-to-speech Translation", "abstract": "Non-autoregressive Transformers (NATs) are recently applied in direct\nspeech-to-speech translation systems, which convert speech across different\nlanguages without intermediate text data. Although NATs generate high-quality\noutputs and offer faster inference than autoregressive models, they tend to\nproduce incoherent and repetitive results due to complex data distribution\n(e.g., acoustic and linguistic variations in speech). In this work, we\nintroduce DiffNorm, a diffusion-based normalization strategy that simplifies\ndata distributions for training NAT models. After training with a\nself-supervised noise estimation objective, DiffNorm constructs normalized\ntarget data by denoising synthetically corrupted speech features. Additionally,\nwe propose to regularize NATs with classifier-free guidance, improving model\nrobustness and translation quality by randomly dropping out source information\nduring training. Our strategies result in a notable improvement of about +7\nASR-BLEU for English-Spanish (En-Es) and +2 ASR-BLEU for English-French (En-Fr)\ntranslations on the CVSS benchmark, while attaining over 14x speedup for En-Es\nand 5x speedup for En-Fr translations compared to autoregressive baselines.", "published": "2024-05-22 01:10:39", "link": "http://arxiv.org/abs/2405.13274v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Metadata Integration for Spam Reviews Detection on Vietnamese E-commerce\n  Websites", "abstract": "The problem of detecting spam reviews (opinions) has received significant\nattention in recent years, especially with the rapid development of e-commerce.\nSpam reviews are often classified based on comment content, but in some cases,\nit is insufficient for models to accurately determine the review label. In this\nwork, we introduce the ViSpamReviews v2 dataset, which includes metadata of\nreviews with the objective of integrating supplementary attributes for spam\nreview classification. We propose a novel approach to simultaneously integrate\nboth textual and categorical attributes into the classification model. In our\nexperiments, the product category proved effective when combined with deep\nneural network (DNN) models, while text features performed well on both DNN\nmodels and the model achieved state-of-the-art performance in the problem of\ndetecting spam reviews on Vietnamese e-commerce websites, namely PhoBERT.\nSpecifically, the PhoBERT model achieves the highest accuracy when combined\nwith product description features generated from the SPhoBert model, which is\nthe combination of PhoBERT and SentenceBERT. Using the macro-averaged F1 score,\nthe task of classifying spam reviews achieved 87.22% (an increase of 1.64%\ncompared to the baseline), while the task of identifying the type of spam\nreviews achieved an accuracy of 73.49% (an increase of 1.93% compared to the\nbaseline).", "published": "2024-05-22 02:19:13", "link": "http://arxiv.org/abs/2405.13292v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mosaic-IT: Free Compositional Data Augmentation Improves Instruction\n  Tuning", "abstract": "Finetuning large language models with a variety of instruction-response pairs\nhas enhanced their capability to understand and follow instructions. Current\ninstruction tuning primarily relies on teacher models or human intervention to\ngenerate and refine the instructions and responses for training, which are\ncostly, non-sustainable, and may lack diversity. In this paper, we introduce\nMosaic Instruction Tuning (Mosaic-IT), a human/model-free compositional data\naugmentation method that can efficiently create rich and diverse augmentations\nfrom existing instruction tuning data to enhance the LLMs. Mosaic-IT randomly\nconcatenates multiple instruction data into one and trains the model to produce\nthe corresponding responses with predefined higher-level meta-instructions to\nstrengthen its multi-step instruction-following and format-following skills.\nOur extensive evaluations demonstrate a superior performance and training\nefficiency of Mosaic-IT, which achieves consistent performance improvements\nover various benchmarks and a $80\\%$ reduction in training costs compared with\noriginal instruction tuning. Our codes and data are available at\nhttps://github.com/tianyi-lab/Mosaic-IT.", "published": "2024-05-22 04:08:20", "link": "http://arxiv.org/abs/2405.13326v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AdpQ: A Zero-shot Calibration Free Adaptive Post Training Quantization\n  Method for LLMs", "abstract": "The ever-growing computational complexity of Large Language Models (LLMs)\nnecessitates efficient deployment strategies. The current state-of-the-art\napproaches for Post-training Quantization (PTQ) often require calibration to\nachieve the desired accuracy. This paper presents AdpQ, a novel zero-shot\nadaptive PTQ method for LLMs that achieves the state-of-the-art performance in\nlow-precision quantization (e.g. 3-bit) without requiring any calibration data.\nInspired by Adaptive LASSO regression model, our proposed approach tackles the\nchallenge of outlier activations by separating salient weights using an\nadaptive soft-thresholding method. Guided by Adaptive LASSO, this method\nensures that the quantized weights distribution closely follows the originally\ntrained weights and eliminates the need for calibration data entirely, setting\nour method apart from popular approaches such as SpQR and AWQ. Furthermore, our\nmethod offers an additional benefit in terms of privacy preservation by\neliminating any calibration or training data. We also delve deeper into the\ninformation-theoretic underpinnings of the proposed method. We demonstrate that\nit leverages the Adaptive LASSO to minimize the Kullback-Leibler divergence\nbetween the quantized weights and the originally trained weights. This\nminimization ensures the quantized model retains the Shannon information\ncontent of the original model to a great extent, guaranteeing efficient\ndeployment without sacrificing accuracy or information. Our results achieve the\nsame accuracy as the existing methods on various LLM benchmarks while the\nquantization time is reduced by at least 10x, solidifying our contribution to\nefficient and privacy-preserving LLM deployment.", "published": "2024-05-22 05:32:11", "link": "http://arxiv.org/abs/2405.13358v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Distilling Instruction-following Abilities of Large Language Models with\n  Task-aware Curriculum Planning", "abstract": "Instruction tuning aims to align large language models (LLMs) with\nopen-domain instructions and human-preferred responses. While several studies\nhave explored autonomous approaches to distilling and annotating instructions\nfrom powerful proprietary LLMs, such as ChatGPT, they often neglect the impact\nof the distributions and characteristics of tasks, together with the varying\ndifficulty of instructions in training sets. This oversight can lead to\nimbalanced knowledge capabilities and poor generalization powers of student\nLLMs. To address these challenges, we introduce Task-Aware Curriculum Planning\nfor Instruction Refinement (TAPIR), a multi-round distillation framework that\nutilizes an oracle LLM to select instructions that are difficult for a student\nLLM to follow. To balance the student's capabilities, task distributions in\ntraining sets are adjusted with responses automatically refined according to\ntheir corresponding tasks. In addition, by incorporating curriculum planning,\nour approach systematically escalates the difficulty levels of tasks,\nprogressively enhancing the student LLM's capabilities. We rigorously evaluate\nTAPIR using several widely recognized benchmarks (such as AlpacaEval 2.0,\nMT-Bench, etc.) and multiple student LLMs. Empirical results demonstrate that\nstudent LLMs, trained with our method and less training data, outperform larger\ninstruction-tuned models and strong distillation baselines.", "published": "2024-05-22 08:38:26", "link": "http://arxiv.org/abs/2405.13448v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The correlation between nativelike selection and prototypicality: a\n  multilingual onomasiological case study using semantic embedding", "abstract": "In native speakers' lexical choices, a concept can be more readily expressed\nby one expression over another grammatical one, a phenomenon known as\nnativelike selection (NLS). In previous research, arbitrary chunks such as\ncollocations have been considered crucial for this phenomenon. However, this\nstudy examines the possibility of analyzing the semantic motivation and\ndeducibility behind some NLSs by exploring the correlation between NLS and\nprototypicality, specifically the onomasiological hypothesis of Grondelaers and\nGeeraerts (2003, Towards a pragmatic model of cognitive onomasiology. In Hubert\nCuyckens, Ren\\'e Dirven & John R. Taylor (eds.), Cognitive approaches to\nlexical semantics, 67-92. Berlin: De Gruyter Mouton). They hypothesized that\n\"[a] referent is more readily named by a lexical item if it is a salient member\nof the category denoted by that item\". To provide a preliminary investigation\nof this important but rarely explored phenomenon, a series of innovative\nmethods and procedures, including the use of semantic embedding and\ninterlingual comparisons, is designed. Specifically, potential NLSs are\nefficiently discovered through an automatic exploratory analysis using topic\nmodeling techniques, and then confirmed by manual inspection through frame\nsemantics. Finally, to account for the NLS in question, cluster analysis and\nbehavioral profile analysis are conducted to uncover a language-specific\nprototype for the Chinese verb shang 'harm', providing supporting evidence for\nthe correlation between NLS and prototypicality.", "published": "2024-05-22 10:55:26", "link": "http://arxiv.org/abs/2405.13529v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ConTrans: Weak-to-Strong Alignment Engineering via Concept\n  Transplantation", "abstract": "Ensuring large language models (LLM) behave consistently with human goals,\nvalues, and intentions is crucial for their safety but yet computationally\nexpensive. To reduce the computational cost of alignment training of LLMs,\nespecially for those with a huge number of parameters, and to reutilize learned\nvalue alignment, we propose ConTrans, a novel framework that enables\nweak-to-strong alignment transfer via concept transplantation. From the\nperspective of representation engineering, ConTrans refines concept vectors in\nvalue alignment from a source LLM (usually a weak yet aligned LLM). The refined\nconcept vectors are then reformulated to adapt to the target LLM (usually a\nstrong yet unaligned base LLM) via affine transformation. In the third step,\nConTrans transplants the reformulated concept vectors into the residual stream\nof the target LLM. Experiments demonstrate the successful transplantation of a\nwide range of aligned concepts from 7B models to 13B and 70B models across\nmultiple LLMs and LLM families. Remarkably, ConTrans even surpasses\ninstruction-tuned models in terms of truthfulness. Experiment results validate\nthe effectiveness of both inter-LLM-family and intra-LLM-family concept\ntransplantation. Our work successfully demonstrates an alternative way to\nachieve weak-to-strong alignment generalization and control.", "published": "2024-05-22 12:15:52", "link": "http://arxiv.org/abs/2405.13578v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CrossCheckGPT: Universal Hallucination Ranking for Multimodal Foundation\n  Models", "abstract": "Multimodal foundation models are prone to hallucination, generating outputs\nthat either contradict the input or are not grounded by factual information.\nGiven the diversity in architectures, training data and instruction tuning\ntechniques, there can be large variations in systems' susceptibility to\nhallucinations. To assess system hallucination robustness, hallucination\nranking approaches have been developed for specific tasks such as image\ncaptioning, question answering, summarization, or biography generation.\nHowever, these approaches typically compare model outputs to gold-standard\nreferences or labels, limiting hallucination benchmarking for new domains. This\nwork proposes \"CrossCheckGPT\", a reference-free universal hallucination ranking\nfor multimodal foundation models. The core idea of CrossCheckGPT is that the\nsame hallucinated content is unlikely to be generated by different independent\nsystems, hence cross-system consistency can provide meaningful and accurate\nhallucination assessment scores. CrossCheckGPT can be applied to any model or\ntask, provided that the information consistency between outputs can be measured\nthrough an appropriate distance metric. Focusing on multimodal large language\nmodels that generate text, we explore two information consistency measures:\nCrossCheck-explicit and CrossCheck-implicit. We showcase the applicability of\nour method for hallucination ranking across various modalities, namely the\ntext, image, and audio-visual domains. Further, we propose the first\naudio-visual hallucination benchmark, \"AVHalluBench\", and illustrate the\neffectiveness of CrossCheckGPT, achieving correlations of 98% and 89% with\nhuman judgements on MHaluBench and AVHalluBench, respectively.", "published": "2024-05-22 14:25:41", "link": "http://arxiv.org/abs/2405.13684v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Grounding Toxicity in Real-World Events across Languages", "abstract": "Social media conversations frequently suffer from toxicity, creating\nsignificant issues for users, moderators, and entire communities. Events in the\nreal world, like elections or conflicts, can initiate and escalate toxic\nbehavior online. Our study investigates how real-world events influence the\norigin and spread of toxicity in online discussions across various languages\nand regions. We gathered Reddit data comprising 4.5 million comments from 31\nthousand posts in six different languages (Dutch, English, German, Arabic,\nTurkish and Spanish). We target fifteen major social and political world events\nthat occurred between 2020 and 2023. We observe significant variations in\ntoxicity, negative sentiment, and emotion expressions across different events\nand language communities, showing that toxicity is a complex phenomenon in\nwhich many different factors interact and still need to be investigated. We\nwill release the data for further research along with our code.", "published": "2024-05-22 15:38:53", "link": "http://arxiv.org/abs/2405.13754v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do Language Models Enjoy Their Own Stories? Prompting Large Language\n  Models for Automatic Story Evaluation", "abstract": "Storytelling is an integral part of human experience and plays a crucial role\nin social interactions. Thus, Automatic Story Evaluation (ASE) and Generation\n(ASG) could benefit society in multiple ways, but they are challenging tasks\nwhich require high-level human abilities such as creativity, reasoning and deep\nunderstanding. Meanwhile, Large Language Models (LLM) now achieve\nstate-of-the-art performance on many NLP tasks. In this paper, we study whether\nLLMs can be used as substitutes for human annotators for ASE. We perform an\nextensive analysis of the correlations between LLM ratings, other automatic\nmeasures, and human annotations, and we explore the influence of prompting on\nthe results and the explainability of LLM behaviour. Most notably, we find that\nLLMs outperform current automatic measures for system-level evaluation but\nstill struggle at providing satisfactory explanations for their answers.", "published": "2024-05-22 15:56:52", "link": "http://arxiv.org/abs/2405.13769v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Getting More from Less: Large Language Models are Good Spontaneous\n  Multilingual Learners", "abstract": "Recently, Large Language Models (LLMs) have shown impressive language\ncapabilities. While most of the existing LLMs have very unbalanced performance\nacross different languages, multilingual alignment based on translation\nparallel data is an effective method to enhance the LLMs' multilingual\ncapabilities. In this work, we discover and comprehensively investigate the\nspontaneous multilingual alignment improvement of LLMs. We find that LLMs\ninstruction-tuned on the question translation data (i.e. without annotated\nanswers) are able to encourage the alignment between English and a wide range\nof languages, even including those unseen during instruction-tuning.\nAdditionally, we utilize different settings and mechanistic interpretability\nmethods to analyze the LLM's performance in the multilingual scenario\ncomprehensively. Our work suggests that LLMs have enormous potential for\nimproving multilingual alignment efficiently with great language and task\ngeneralization.", "published": "2024-05-22 16:46:19", "link": "http://arxiv.org/abs/2405.13816v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Comprehensive Post Safety Alignment of Large Language Models via\n  Safety Patching", "abstract": "Safety alignment of large language models (LLMs) has been gaining increasing\nattention. However, current safety-aligned LLMs suffer from the fragile and\nimbalanced safety mechanisms, which can still be induced to generate unsafe\nresponses, exhibit over-safety by rejecting safe user inputs, and fail to\npreserve general utility after safety alignment. To this end, we propose a\nnovel post safety alignment (PSA) method to address these inherent and emerging\nsafety challenges, including safety enhancement, over-safety mitigation, and\nutility preservation. In specific, we introduce \\textsc{SafePatching}, a novel\nframework for comprehensive PSA, where two distinct safety patches are\ndeveloped on the harmful data to enhance safety and mitigate over-safety\nconcerns, and then seamlessly integrated into the target LLM backbone without\ncompromising its utility. Extensive experiments on four representative aligned\nLLMs, including LLaMA-2/3, Gemma and Mistral, show that \\textsc{SafePatching}\nachieves a more comprehensive PSA than baseline methods, further optimizing the\nbalance between being helpful and harmless in current aligned LLMs. Also,\n\\textsc{SafePatching} demonstrates its superiority in continual PSA scenarios.", "published": "2024-05-22 16:51:07", "link": "http://arxiv.org/abs/2405.13820v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Why Not Transform Chat Large Language Models to Non-English?", "abstract": "The scarcity of non-English data limits the development of non-English large\nlanguage models (LLMs). Transforming English-centric LLMs to non-English has\nbeen identified as an effective and resource-efficient method. Previous works\nstart from base LLMs and perform knowledge distillation (KD) with data\ngenerated by stronger LLMs, e.g. GPT-4. Compared to base LLMs, chat LLMs are\nfurther optimized for advanced abilities, e.g. multi-turn conversation and\nhuman preference alignment, and thus more powerful in both helpfulness and\nsafety. However, transforming a chat LLM involves two critical issues: (1) How\ncan we effectively transfer advanced abilities without their supervised data?\n(2) How can we prevent the original knowledge from catastrophic forgetting\nduring transformation? We target these issues by introducing a simple framework\ncalled TransLLM. For the first issue, TransLLM divides the transfer problem\ninto some common sub-tasks with the translation chain-of-thought, which uses\nthe translation as the bridge between English and non-English step-by-step. We\nfurther enhance the performance of sub-tasks with publicly available data. For\nthe second issue, we propose a method comprising two synergistic components:\nlow-rank adaptation for training to maintain the original LLM parameters, and\nrecovery KD, which utilizes data generated by the chat LLM itself to recover\nthe original knowledge from the frozen parameters. In the experiments, we\ntransform the LLaMA-2-chat-7B to the Thai language. Our method, using only\nsingle-turn data, outperforms strong baselines and ChatGPT on multi-turn\nbenchmark MT-bench. Furthermore, our method, without safety data, rejects more\nharmful queries of safety benchmark AdvBench than both ChatGPT and GPT-4.", "published": "2024-05-22 18:53:25", "link": "http://arxiv.org/abs/2405.13923v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Model Editing as a Robust and Denoised variant of DPO: A Case Study on\n  Toxicity", "abstract": "Recent alignment algorithms such as direct preference optimization (DPO) have\nbeen developed to improve the safety of large language models (LLMs) by\ntraining these models to match human behaviors exemplified by preference data.\nHowever, these methods are both computationally intensive and lacking in\ncontrollability and transparency, inhibiting their widespread use. Furthermore,\nthese tuning-based methods require large-scale preference data for training and\nare susceptible to noisy preference data. In this paper, we introduce a\ntuning-free alignment alternative, ProFS (Projection Filter for Subspaces), and\ndemonstrate its effectiveness under the use case of toxicity reduction.\nGrounded on theory from factor analysis, ProFS is a sample-efficient model\nediting approach that identifies a toxic subspace in the model parameter space\nand reduces model toxicity by projecting away the detected subspace. The toxic\nsubspace is identified by extracting preference data embeddings from the\nlanguage model, and removing non-toxic information from these embeddings. We\nshow that ProFS is more sample-efficient than DPO, further showcasing greater\nrobustness to noisy data. Finally, we attempt to connect tuning based alignment\nwith editing, by establishing both theoretical and empirical connections\nbetween ProFS and DPO, showing that ProFS can be interpreted as a denoised\nversion of a single DPO step.", "published": "2024-05-22 20:08:48", "link": "http://arxiv.org/abs/2405.13967v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Use of natural language processing to extract and classify papillary\n  thyroid cancer features from surgical pathology reports", "abstract": "Background We aim to use Natural Language Processing (NLP) to automate the\nextraction and classification of thyroid cancer risk factors from pathology\nreports. Methods We analyzed 1,410 surgical pathology reports from adult\npapillary thyroid cancer patients at Mayo Clinic, Rochester, MN, from 2010 to\n2019. Structured and non-structured reports were used to create a\nconsensus-based ground truth dictionary and categorized them into modified\nrecurrence risk levels. Non-structured reports were narrative, while structured\nreports followed standardized formats. We then developed ThyroPath, a\nrule-based NLP pipeline, to extract and classify thyroid cancer features into\nrisk categories. Training involved 225 reports (150 structured, 75\nunstructured), with testing on 170 reports (120 structured, 50 unstructured)\nfor evaluation. The pipeline's performance was assessed using both strict and\nlenient criteria for accuracy, precision, recall, and F1-score. Results In\nextraction tasks, ThyroPath achieved overall strict F-1 scores of 93% for\nstructured reports and 90 for unstructured reports, covering 18 thyroid cancer\npathology features. In classification tasks, ThyroPath-extracted information\ndemonstrated an overall accuracy of 93% in categorizing reports based on their\ncorresponding guideline-based risk of recurrence: 76.9% for high-risk, 86.8%\nfor intermediate risk, and 100% for both low and very low-risk cases. However,\nThyroPath achieved 100% accuracy across all thyroid cancer risk categories with\nhuman-extracted pathology information. Conclusions ThyroPath shows promise in\nautomating the extraction and risk recurrence classification of thyroid\npathology reports at large scale. It offers a solution to laborious manual\nreviews and advancing virtual registries. However, it requires further\nvalidation before implementation.", "published": "2024-05-22 22:27:12", "link": "http://arxiv.org/abs/2406.00015v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Multilingual Similarity Dataset for News Article Frame", "abstract": "Understanding the writing frame of news articles is vital for addressing\nsocial issues, and thus has attracted notable attention in the fields of\ncommunication studies. Yet, assessing such news article frames remains a\nchallenge due to the absence of a concrete and unified standard dataset that\nconsiders the comprehensive nuances within news content.\n  To address this gap, we introduce an extended version of a large labeled news\narticle dataset with 16,687 new labeled pairs. Leveraging the pairwise\ncomparison of news articles, our method frees the work of manual identification\nof frame classes in traditional news frame analysis studies. Overall we\nintroduce the most extensive cross-lingual news article similarity dataset\navailable to date with 26,555 labeled news article pairs across 10 languages.\nEach data point has been meticulously annotated according to a codebook\ndetailing eight critical aspects of news content, under a human-in-the-loop\nframework. Application examples demonstrate its potential in unearthing country\ncommunities within global news coverage, exposing media bias among news\noutlets, and quantifying the factors related to news creation. We envision that\nthis news similarity dataset will broaden our understanding of the media\necosystem in terms of news coverage of events and perspectives across\ncountries, locations, languages, and other social constructs. By doing so, it\ncan catalyze advancements in social science research and applied methodologies,\nthereby exerting a profound impact on our society.", "published": "2024-05-22 01:01:04", "link": "http://arxiv.org/abs/2405.13272v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "''You should probably read this'': Hedge Detection in Text", "abstract": "Humans express ideas, beliefs, and statements through language. The manner of\nexpression can carry information indicating the author's degree of confidence\nin their statement. Understanding the certainty level of a claim is crucial in\nareas such as medicine, finance, engineering, and many others where errors can\nlead to disastrous results. In this work, we apply a joint model that leverages\nwords and part-of-speech tags to improve hedge detection in text and achieve a\nnew top score on the CoNLL-2010 Wikipedia corpus.", "published": "2024-05-22 03:25:35", "link": "http://arxiv.org/abs/2405.13319v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Efficacy of ByT5 in Multilingual Translation of Biblical Texts for\n  Underrepresented Languages", "abstract": "This study presents the development and evaluation of a ByT5-based\nmultilingual translation model tailored for translating the Bible into\nunderrepresented languages. Utilizing the comprehensive Johns Hopkins\nUniversity Bible Corpus, we trained the model to capture the intricate nuances\nof character-based and morphologically rich languages. Our results, measured by\nthe BLEU score and supplemented with sample translations, suggest the model can\nimprove accessibility to sacred texts. It effectively handles the distinctive\nbiblical lexicon and structure, thus bridging the linguistic divide. The study\nalso discusses the model's limitations and suggests pathways for future\nenhancements, focusing on expanding access to sacred literature across\nlinguistic boundaries.", "published": "2024-05-22 05:12:35", "link": "http://arxiv.org/abs/2405.13350v2", "categories": ["cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "360Zhinao Technical Report", "abstract": "We present 360Zhinao models with 7B parameter size and context lengths\nspanning 4K, 32K and 360K, all available at\nhttps://github.com/Qihoo360/360zhinao. For rapid development in pretraining, we\nestablish a stable and sensitive ablation environment to evaluate and compare\nexperiment runs with minimal model size. Under such guidance, we perfect our\ndata cleaning and composition strategies to pretrain\n$\\texttt{360Zhinao-7B-Base}$ on 3.4T tokens. We also mainly emphasize data\nduring alignment, where we strive to balance quantity and quality with\nfiltering and reformatting. With tailored data, 360Zhinao-7B's context window\nis easily extended to 32K and 360K. RMs and RLHF are trained following SFT and\ncredibly applied to specific tasks. All together these contributions lead to\n360Zhinao-7B's competitive performance among models of similar size.", "published": "2024-05-22 06:45:38", "link": "http://arxiv.org/abs/2405.13386v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TrojanRAG: Retrieval-Augmented Generation Can Be Backdoor Driver in\n  Large Language Models", "abstract": "Large language models (LLMs) have raised concerns about potential security\nthreats despite performing significantly in Natural Language Processing (NLP).\nBackdoor attacks initially verified that LLM is doing substantial harm at all\nstages, but the cost and robustness have been criticized. Attacking LLMs is\ninherently risky in security review, while prohibitively expensive. Besides,\nthe continuous iteration of LLMs will degrade the robustness of backdoors. In\nthis paper, we propose TrojanRAG, which employs a joint backdoor attack in the\nRetrieval-Augmented Generation, thereby manipulating LLMs in universal attack\nscenarios. Specifically, the adversary constructs elaborate target contexts and\ntrigger sets. Multiple pairs of backdoor shortcuts are orthogonally optimized\nby contrastive learning, thus constraining the triggering conditions to a\nparameter subspace to improve the matching. To improve the recall of the RAG\nfor the target contexts, we introduce a knowledge graph to construct structured\ndata to achieve hard matching at a fine-grained level. Moreover, we normalize\nthe backdoor scenarios in LLMs to analyze the real harm caused by backdoors\nfrom both attackers' and users' perspectives and further verify whether the\ncontext is a favorable tool for jailbreaking models. Extensive experimental\nresults on truthfulness, language understanding, and harmfulness show that\nTrojanRAG exhibits versatility threats while maintaining retrieval capabilities\non normal queries.", "published": "2024-05-22 07:21:32", "link": "http://arxiv.org/abs/2405.13401v4", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Disperse-Then-Merge: Pushing the Limits of Instruction Tuning via\n  Alignment Tax Reduction", "abstract": "Supervised fine-tuning (SFT) on instruction-following corpus is a crucial\napproach toward the alignment of large language models (LLMs). However, the\nperformance of LLMs on standard knowledge and reasoning benchmarks tends to\nsuffer from deterioration at the latter stage of the SFT process, echoing the\nphenomenon of alignment tax. Through our pilot study, we put a hypothesis that\nthe data biases are probably one cause behind the phenomenon. To address the\nissue, we introduce a simple disperse-then-merge framework. To be concrete, we\ndisperse the instruction-following data into portions and train multiple\nsub-models using different data portions. Then we merge multiple models into a\nsingle one via model merging techniques. Despite its simplicity, our framework\noutperforms various sophisticated methods such as data curation and training\nregularization on a series of standard knowledge and reasoning benchmarks.", "published": "2024-05-22 08:18:19", "link": "http://arxiv.org/abs/2405.13432v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LIRE: listwise reward enhancement for preference alignment", "abstract": "Recently, tremendous strides have been made to align the generation of Large\nLanguage Models (LLMs) with human values to mitigate toxic or unhelpful\ncontent. Leveraging Reinforcement Learning from Human Feedback (RLHF) proves\neffective and is widely adopted by researchers. However, implementing RLHF is\ncomplex, and its sensitivity to hyperparameters renders achieving stable\nperformance and scalability challenging. Furthermore, prevailing approaches to\npreference alignment primarily concentrate on pairwise comparisons, with\nlimited exploration into multi-response scenarios, thereby overlooking the\npotential richness within the candidate pool. For the above reasons, we propose\na new approach: Listwise Reward Enhancement for Preference Alignment (LIRE), a\ngradient-based reward optimization approach that incorporates the offline\nrewards of multiple responses into a streamlined listwise framework, thus\neliminating the need for online sampling during training. LIRE is\nstraightforward to implement, requiring minimal parameter tuning, and\nseamlessly aligns with the pairwise paradigm while naturally extending to\nmulti-response scenarios. Moreover, we introduce a self-enhancement algorithm\naimed at iteratively refining the reward during training. Our experiments\ndemonstrate that LIRE consistently outperforms existing methods across several\nbenchmarks on dialogue and summarization tasks, with good transferability to\nout-of-distribution data, assessed using proxy reward models and human\nannotators.", "published": "2024-05-22 10:21:50", "link": "http://arxiv.org/abs/2405.13516v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "WaterPool: A Watermark Mitigating Trade-offs among Imperceptibility,\n  Efficacy and Robustness", "abstract": "With the increasing use of large language models (LLMs) in daily life,\nconcerns have emerged regarding their potential misuse and societal impact.\nWatermarking is proposed to trace the usage of specific models by injecting\npatterns into their generated texts. An ideal watermark should produce outputs\nthat are nearly indistinguishable from those of the original LLM\n(imperceptibility), while ensuring a high detection rate (efficacy), even when\nthe text is partially altered (robustness). Despite many methods having been\nproposed, none have simultaneously achieved all three properties, revealing an\ninherent trade-off. This paper utilizes a key-centered scheme to unify existing\nwatermarking techniques by decomposing a watermark into two distinct modules: a\nkey module and a mark module. Through this decomposition, we demonstrate for\nthe first time that the key module significantly contributes to the trade-off\nissues observed in prior methods. Specifically, this reflects the conflict\nbetween the scale of the key sampling space during generation and the\ncomplexity of key restoration during detection. To this end, we introduce\n\\textbf{WaterPool}, a simple yet effective key module that preserves a complete\nkey sampling space required by imperceptibility while utilizing semantics-based\nsearch to improve the key restoration process. WaterPool can integrate with\nmost watermarks, acting as a plug-in. Our experiments with three well-known\nwatermarking techniques show that WaterPool significantly enhances their\nperformance, achieving near-optimal imperceptibility and markedly improving\nefficacy and robustness (+12.73\\% for KGW, +20.27\\% for EXP, +7.27\\% for ITS).", "published": "2024-05-22 10:22:20", "link": "http://arxiv.org/abs/2405.13517v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Knowledge-Driven Cross-Document Relation Extraction", "abstract": "Relation extraction (RE) is a well-known NLP application often treated as a\nsentence- or document-level task. However, a handful of recent efforts explore\nit across documents or in the cross-document setting (CrossDocRE). This is\ndistinct from the single document case because different documents often focus\non disparate themes, while text within a document tends to have a single goal.\nLinking findings from disparate documents to identify new relationships is at\nthe core of the popular literature-based knowledge discovery paradigm in\nbiomedicine and other domains. Current CrossDocRE efforts do not consider\ndomain knowledge, which are often assumed to be known to the reader when\ndocuments are authored. Here, we propose a novel approach, KXDocRE, that embed\ndomain knowledge of entities with input text for cross-document RE. Our\nproposed framework has three main benefits over baselines: 1) it incorporates\ndomain knowledge of entities along with documents' text; 2) it offers\ninterpretability by producing explanatory text for predicted relations between\nentities 3) it improves performance over the prior methods.", "published": "2024-05-22 11:30:59", "link": "http://arxiv.org/abs/2405.13546v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "ECLIPSE: Semantic Entropy-LCS for Cross-Lingual Industrial Log Parsing", "abstract": "Log parsing, a vital task for interpreting the vast and complex data produced\nwithin software architectures faces significant challenges in the transition\nfrom academic benchmarks to the industrial domain. Existing log parsers, while\nhighly effective on standardized public datasets, struggle to maintain\nperformance and efficiency when confronted with the sheer scale and diversity\nof real-world industrial logs. These challenges are two-fold: 1) massive log\ntemplates: The performance and efficiency of most existing parsers will be\nsignificantly reduced when logs of growing quantities and different lengths; 2)\nComplex and changeable semantics: Traditional template-matching algorithms\ncannot accurately match the log templates of complicated industrial logs\nbecause they cannot utilize cross-language logs with similar semantics. To\naddress these issues, we propose ECLIPSE, Enhanced Cross-Lingual Industrial log\nParsing with Semantic Entropy-LCS, since cross-language logs can robustly parse\nindustrial logs. On the one hand, it integrates two efficient data-driven\ntemplate-matching algorithms and Faiss indexing. On the other hand, driven by\nthe powerful semantic understanding ability of the Large Language Model (LLM),\nthe semantics of log keywords were accurately extracted, and the retrieval\nspace was effectively reduced. Notably, we launch a Chinese and English\ncross-platform industrial log parsing benchmark ECLIPSE- BENCH to evaluate the\nperformance of mainstream parsers in industrial scenarios. Our experimental\nresults across public benchmarks and ECLIPSE- BENCH underscore the superior\nperformance and robustness of our proposed ECLIPSE. Notably, ECLIPSE both\ndelivers state-of-the-art performance when compared to strong baselines and\npreserves a significant edge in processing efficiency.", "published": "2024-05-22 11:33:29", "link": "http://arxiv.org/abs/2405.13548v2", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "FlashRAG: A Modular Toolkit for Efficient Retrieval-Augmented Generation\n  Research", "abstract": "With the advent of large language models (LLMs) and multimodal large language\nmodels (MLLMs), the potential of retrieval-augmented generation (RAG) has\nattracted considerable research attention. Various novel algorithms and models\nhave been introduced to enhance different aspects of RAG systems. However, the\nabsence of a standardized framework for implementation, coupled with the\ninherently complex RAG process, makes it challenging and time-consuming for\nresearchers to compare and evaluate these approaches in a consistent\nenvironment. Existing RAG toolkits, such as LangChain and LlamaIndex, while\navailable, are often heavy and inflexibly, failing to meet the customization\nneeds of researchers. In response to this challenge, we develop \\ours{}, an\nefficient and modular open-source toolkit designed to assist researchers in\nreproducing and comparing existing RAG methods and developing their own\nalgorithms within a unified framework. Our toolkit has implemented 16 advanced\nRAG methods and gathered and organized 38 benchmark datasets. It has various\nfeatures, including a customizable modular framework, multimodal RAG\ncapabilities, a rich collection of pre-implemented RAG works, comprehensive\ndatasets, efficient auxiliary pre-processing scripts, and extensive and\nstandard evaluation metrics. Our toolkit and resources are available at\nhttps://github.com/RUC-NLPIR/FlashRAG.", "published": "2024-05-22 12:12:40", "link": "http://arxiv.org/abs/2405.13576v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Automated Evaluation of Retrieval-Augmented Language Models with\n  Task-Specific Exam Generation", "abstract": "We propose a new method to measure the task-specific accuracy of\nRetrieval-Augmented Large Language Models (RAG). Evaluation is performed by\nscoring the RAG on an automatically-generated synthetic exam composed of\nmultiple choice questions based on the corpus of documents associated with the\ntask. Our method is an automated, cost-efficient, interpretable, and robust\nstrategy to select the optimal components for a RAG system. We leverage Item\nResponse Theory (IRT) to estimate the quality of an exam and its\ninformativeness on task-specific accuracy. IRT also provides a natural way to\niteratively improve the exam by eliminating the exam questions that are not\nsufficiently informative about a model's ability. We demonstrate our approach\non four new open-ended Question-Answering tasks based on Arxiv abstracts,\nStackExchange questions, AWS DevOps troubleshooting guides, and SEC filings. In\naddition, our experiments reveal more general insights into factors impacting\nRAG performance like size, retrieval mechanism, prompting and fine-tuning. Most\nnotably, our findings show that choosing the right retrieval algorithms often\nleads to bigger performance gains than simply using a larger language model.", "published": "2024-05-22 13:14:11", "link": "http://arxiv.org/abs/2405.13622v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "\"I Like Sunnie More Than I Expected!\": Exploring User Expectation and\n  Perception of an Anthropomorphic LLM-based Conversational Agent for\n  Well-Being Support", "abstract": "The human-computer interaction (HCI) research community has a longstanding\ninterest in exploring the mismatch between users' actual experiences and\nexpectation toward new technologies, for instance, large language models\n(LLMs). In this study, we compared users' (N = 38) initial expectations against\ntheir post-interaction perceptions of two LLM-powered mental well-being\nintervention activity recommendation systems. Both systems have a built-in LLM\nto recommend a personalized well-being intervention activity, but one system\n(Sunnie) has an anthropomorphic conversational interaction design via elements\nsuch as appearance, persona, and natural conversation. Results showed that user\nengagement was high with both systems, and both systems exceeded users'\nexpectations along the utility dimension, highlighting AI's potential to offer\nuseful intervention activity recommendations. In addition, Sunnie further\noutperformed the non-anthropomorphic baseline system in relational warmth.\nThese findings suggest that anthropomorphic conversational interaction design\nmay be particularly effective in fostering warmth in mental health support\ncontexts.", "published": "2024-05-22 16:30:24", "link": "http://arxiv.org/abs/2405.13803v3", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Babysit A Language Model From Scratch: Interactive Language Learning by\n  Trials and Demonstrations", "abstract": "Humans are efficient language learners and inherently social creatures. Our\nlanguage development is largely shaped by our social interactions, for example,\nthe demonstration and feedback from caregivers. Contrary to human language\nlearning, recent advancements in large language models have primarily adopted a\nnon-interactive training paradigm, and refined pre-trained models through\nfeedback afterward. In this work, we aim to examine how corrective feedback\nfrom interactions influences neural language acquisition from the ground up\nthrough systematically controlled experiments, assessing whether it contributes\nto learning efficiency in language models. We introduce a\ntrial-and-demonstration (TnD) learning framework that incorporates three\ncomponents: student trials, teacher demonstrations, and a reward conditioned on\nlanguage competence at various developmental stages. Our experiments reveal\nthat the TnD approach accelerates word acquisition for student models of equal\nand smaller numbers of parameters, and we highlight the significance of both\ntrials and demonstrations. We further show that the teacher's choices of words\ninfluence students' word-specific learning efficiency, and a\npractice-makes-perfect effect is evident by a strong correlation between the\nfrequency of words in trials and their respective learning curves. Our findings\nsuggest that interactive language learning, with teacher demonstrations and\nstudent trials, can facilitate efficient word learning in language models.", "published": "2024-05-22 16:57:02", "link": "http://arxiv.org/abs/2405.13828v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Semantic Density: Uncertainty Quantification for Large Language Models\n  through Confidence Measurement in Semantic Space", "abstract": "With the widespread application of Large Language Models (LLMs) to various\ndomains, concerns regarding the trustworthiness of LLMs in safety-critical\nscenarios have been raised, due to their unpredictable tendency to hallucinate\nand generate misinformation. Existing LLMs do not have an inherent\nfunctionality to provide the users with an uncertainty/confidence metric for\neach response it generates, making it difficult to evaluate trustworthiness.\nAlthough several studies aim to develop uncertainty quantification methods for\nLLMs, they have fundamental limitations, such as being restricted to\nclassification tasks, requiring additional training and data, considering only\nlexical instead of semantic information, and being prompt-wise but not\nresponse-wise. A new framework is proposed in this paper to address these\nissues. Semantic density extracts uncertainty/confidence information for each\nresponse from a probability distribution perspective in semantic space. It has\nno restriction on task types and is \"off-the-shelf\" for new models and tasks.\nExperiments on seven state-of-the-art LLMs, including the latest Llama 3 and\nMixtral-8x22B models, on four free-form question-answering benchmarks\ndemonstrate the superior performance and robustness of semantic density\ncompared to prior approaches.", "published": "2024-05-22 17:13:49", "link": "http://arxiv.org/abs/2405.13845v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Automatically Identifying Local and Global Circuits with Linear\n  Computation Graphs", "abstract": "Circuit analysis of any certain model behavior is a central task in\nmechanistic interpretability. We introduce our circuit discovery pipeline with\nSparse Autoencoders (SAEs) and a variant called Transcoders. With these two\nmodules inserted into the model, the model's computation graph with respect to\nOV and MLP circuits becomes strictly linear. Our methods do not require linear\napproximation to compute the causal effect of each node. This fine-grained\ngraph identifies both end-to-end and local circuits accounting for either\nlogits or intermediate features. We can scalably apply this pipeline with a\ntechnique called Hierarchical Attribution. We analyze three kinds of circuits\nin GPT-2 Small: bracket, induction, and Indirect Object Identification\ncircuits. Our results reveal new findings underlying existing discoveries.", "published": "2024-05-22 17:50:04", "link": "http://arxiv.org/abs/2405.13868v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "FiDeLiS: Faithful Reasoning in Large Language Model for Knowledge Graph\n  Question Answering", "abstract": "Large language models (LLMs) are often challenged by generating erroneous or\nhallucinated responses, especially in complex reasoning tasks. Leveraging\nknowledge graphs (KGs) as external knowledge sources has emerged as a viable\nsolution. However, existing KG-enhanced methods, either retrieval-based or\nagent-based, encounter difficulties in accurately retrieving knowledge and\nefficiently traversing KGs at scale. In this paper, we propose a unified\nframework, FiDeLiS, designed to improve the factuality of LLM responses by\nanchoring answers to verifiable reasoning steps retrieved from a KG. To achieve\nthis, we leverage step-wise beam search with a deductive scoring function,\nallowing the LLM to validate each reasoning step and halt the search once the\nquestion is deducible. In addition, our Path-rag module pre-selects a smaller\ncandidate set for each beam search step, reducing computational costs by\nnarrowing the search space. Extensive experiments show that our training-free\nand efficient approach outperforms strong baselines, enhancing both factuality\nand interpretability.", "published": "2024-05-22 17:56:53", "link": "http://arxiv.org/abs/2405.13873v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Just rephrase it! Uncertainty estimation in closed-source language\n  models via multiple rephrased queries", "abstract": "State-of-the-art large language models are sometimes distributed as\nopen-source software but are also increasingly provided as a closed-source\nservice. These closed-source large-language models typically see the widest\nusage by the public, however, they often do not provide an estimate of their\nuncertainty when responding to queries. As even the best models are prone to\n``hallucinating\" false information with high confidence, a lack of a reliable\nestimate of uncertainty limits the applicability of these models in critical\nsettings. We explore estimating the uncertainty of closed-source LLMs via\nmultiple rephrasings of an original base query. Specifically, we ask the model,\nmultiple rephrased questions, and use the similarity of the answers as an\nestimate of uncertainty. We diverge from previous work in i) providing rules\nfor rephrasing that are simple to memorize and use in practice ii) proposing a\ntheoretical framework for why multiple rephrased queries obtain calibrated\nuncertainty estimates. Our method demonstrates significant improvements in the\ncalibration of uncertainty estimates compared to the baseline and provides\nintuition as to how query strategies should be designed for optimal test\ncalibration.", "published": "2024-05-22 18:28:26", "link": "http://arxiv.org/abs/2405.13907v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Vikhr: Constructing a State-of-the-art Bilingual Open-Source\n  Instruction-Following Large Language Model for Russian", "abstract": "There has been a surge in developing various Large Language Models (LLMs).\nHowever, text generation for languages other than English often faces\nsignificant challenges, including poor generation quality and reduced\ncomputational performance due to the disproportionate representation of tokens\nin the model's vocabulary. In this work, we address these issues by developing\na pipeline for adapting English-oriented pre-trained models to other languages\nand constructing efficient bilingual LLMs. Using this pipeline, we construct\nVikhr, a state-of-the-art bilingual open-source instruction-following LLM\ndesigned specifically for the Russian language. \"Vikhr\" refers to the name of\nthe Mistral LLM series and means a \"strong gust of wind.\" Unlike previous\nRussian-language models that typically rely on LoRA adapters on top of\nEnglish-oriented models, sacrificing performance for lower training costs,\nVikhr features an adapted tokenizer vocabulary and undergoes continued\npre-training and instruction tuning of all weights. This not only enhances the\nmodel's performance but also significantly improves its computational and\ncontextual efficiency. The remarkable performance of Vikhr across various\nRussian-language benchmarks can also be attributed to our efforts in expanding\ninstruction datasets and corpora for continued pre-training. Vikhr not only\nsets a new state of the art among open-source LLMs for Russian but even\noutperforms some proprietary closed-source models on certain benchmarks. The\nmodel weights, instruction sets, and code are publicly available.", "published": "2024-05-22 18:58:58", "link": "http://arxiv.org/abs/2405.13929v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On the Brittle Foundations of ReAct Prompting for Agentic Large Language\n  Models", "abstract": "The reasoning abilities of Large Language Models (LLMs) remain a topic of\ndebate. Some methods such as ReAct-based prompting, have gained popularity for\nclaiming to enhance sequential decision-making abilities of agentic LLMs.\nHowever, it is unclear what is the source of improvement in LLM reasoning with\nReAct based prompting. In this paper we examine these claims of ReAct based\nprompting in improving agentic LLMs for sequential decision-making. By\nintroducing systematic variations to the input prompt we perform a sensitivity\nanalysis along the claims of ReAct and find that the performance is minimally\ninfluenced by the \"interleaving reasoning trace with action execution\" or the\ncontent of the generated reasoning traces in ReAct, contrary to original claims\nand common usage. Instead, the performance of LLMs is driven by the similarity\nbetween input example tasks and queries, implicitly forcing the prompt designer\nto provide instance-specific examples which significantly increases the\ncognitive burden on the human. Our investigation shows that the perceived\nreasoning abilities of LLMs stem from the exemplar-query similarity and\napproximate retrieval rather than any inherent reasoning abilities.", "published": "2024-05-22 20:05:49", "link": "http://arxiv.org/abs/2405.13966v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "CIVICS: Building a Dataset for Examining Culturally-Informed Values in\n  Large Language Models", "abstract": "This paper introduces the \"CIVICS: Culturally-Informed & Values-Inclusive\nCorpus for Societal impacts\" dataset, designed to evaluate the social and\ncultural variation of Large Language Models (LLMs) across multiple languages\nand value-sensitive topics. We create a hand-crafted, multilingual dataset of\nvalue-laden prompts which address specific socially sensitive topics, including\nLGBTQI rights, social welfare, immigration, disability rights, and surrogacy.\nCIVICS is designed to generate responses showing LLMs' encoded and implicit\nvalues. Through our dynamic annotation processes, tailored prompt design, and\nexperiments, we investigate how open-weight LLMs respond to value-sensitive\nissues, exploring their behavior across diverse linguistic and cultural\ncontexts. Using two experimental set-ups based on log-probabilities and\nlong-form responses, we show social and cultural variability across different\nLLMs. Specifically, experiments involving long-form responses demonstrate that\nrefusals are triggered disparately across models, but consistently and more\nfrequently in English or translated statements. Moreover, specific topics and\nsources lead to more pronounced differences across model answers, particularly\non immigration, LGBTQI rights, and social welfare. As shown by our experiments,\nthe CIVICS dataset aims to serve as a tool for future research, promoting\nreproducibility and transparency across broader linguistic settings, and\nfurthering the development of AI technologies that respect and reflect global\ncultural diversities and value pluralism. The CIVICS dataset and tools will be\nmade available upon publication under open licenses; an anonymized version is\ncurrently available at https://huggingface.co/CIVICS-dataset.", "published": "2024-05-22 20:19:10", "link": "http://arxiv.org/abs/2405.13974v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Less for More: Enhanced Feedback-aligned Mixed LLMs for Molecule Caption\n  Generation and Fine-Grained NLI Evaluation", "abstract": "Scientific language models drive research innovation but require extensive\nfine-tuning on large datasets. This work enhances such models by improving\ntheir inference and evaluation capabilities with minimal or no additional\ntraining. Focusing on molecule caption generation, we explore synergies between\nalignment fine-tuning and model merging in a cross-modal setup. We reveal\nintriguing insights into the behaviour and suitability of such methods while\nsignificantly surpassing state-of-the-art models. Moreover, we propose a novel\natomic-level evaluation method leveraging off-the-shelf Natural Language\nInference (NLI) models for use in the unseen chemical domain. Our experiments\ndemonstrate that our evaluation operates at the right level of granularity,\neffectively handling multiple content units and subsentence reasoning, while\nwidely adopted NLI methods consistently misalign with assessment criteria.", "published": "2024-05-22 20:40:53", "link": "http://arxiv.org/abs/2405.13984v2", "categories": ["cs.CL", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Evaluating Large Language Models with Human Feedback: Establishing a\n  Swedish Benchmark", "abstract": "In the rapidly evolving field of artificial intelligence, large language\nmodels (LLMs) have demonstrated significant capabilities across numerous\napplications. However, the performance of these models in languages with fewer\nresources, such as Swedish, remains under-explored. This study introduces a\ncomprehensive human benchmark to assess the efficacy of prominent LLMs in\nunderstanding and generating Swedish language texts using forced choice\nranking. We employ a modified version of the ChatbotArena benchmark,\nincorporating human feedback to evaluate eleven different models, including\nGPT-4, GPT-3.5, various Claude and Llama models, and bespoke models like\nDolphin-2.9-llama3b-8b-flashback and BeagleCatMunin. These models were chosen\nbased on their performance on LMSYS chatbot arena and the Scandeval benchmarks.\nWe release the chatbotarena.se benchmark as a tool to improve our understanding\nof language model performance in Swedish with the hopes that it will be widely\nused. We aim to create a leaderboard once sufficient data has been collected\nand analysed.", "published": "2024-05-22 21:22:51", "link": "http://arxiv.org/abs/2405.14006v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Prompt-Time Ontology-Driven Symbolic Knowledge Capture with Large\n  Language Models", "abstract": "In applications such as personal assistants, large language models (LLMs)\nmust consider the user's personal information and preferences. However, LLMs\nlack the inherent ability to learn from user interactions. This paper explores\ncapturing personal information from user prompts using ontology and\nknowledge-graph approaches. We use a subset of the KNOW ontology, which models\npersonal information, to train the language model on these concepts. We then\nevaluate the success of knowledge capture using a specially constructed\ndataset. Our code and datasets are publicly available at\nhttps://github.com/HaltiaAI/paper-PTODSKC", "published": "2024-05-22 21:40:34", "link": "http://arxiv.org/abs/2405.14012v1", "categories": ["cs.AI", "cs.CL", "I.2.7"], "primary_category": "cs.AI"}
{"title": "Refining Skewed Perceptions in Vision-Language Models through Visual\n  Representations", "abstract": "Large vision-language models (VLMs), such as CLIP, have become foundational,\ndemonstrating remarkable success across a variety of downstream tasks. Despite\ntheir advantages, these models, akin to other foundational systems, inherit\nbiases from the disproportionate distribution of real-world data, leading to\nmisconceptions about the actual environment. Prevalent datasets like ImageNet\nare often riddled with non-causal, spurious correlations that can diminish VLM\nperformance in scenarios where these contextual elements are absent. This study\npresents an investigation into how a simple linear probe can effectively\ndistill task-specific core features from CLIP's embedding for downstream\napplications. Our analysis reveals that the CLIP text representations are often\ntainted by spurious correlations, inherited in the biased pre-training dataset.\nEmpirical evidence suggests that relying on visual representations from CLIP,\nas opposed to text embedding, is more practical to refine the skewed\nperceptions in VLMs, emphasizing the superior utility of visual representations\nin overcoming embedded biases. Our codes will be available here.", "published": "2024-05-22 22:03:11", "link": "http://arxiv.org/abs/2405.14030v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Your Large Language Models Are Leaving Fingerprints", "abstract": "It has been shown that finetuned transformers and other supervised detectors\neffectively distinguish between human and machine-generated text in some\nsituations arXiv:2305.13242, but we find that even simple classifiers on top of\nn-gram and part-of-speech features can achieve very robust performance on both\nin- and out-of-domain data. To understand how this is possible, we analyze\nmachine-generated output text in five datasets, finding that LLMs possess\nunique fingerprints that manifest as slight differences in the frequency of\ncertain lexical and morphosyntactic features. We show how to visualize such\nfingerprints, describe how they can be used to detect machine-generated text\nand find that they are even robust across textual domains. We find that\nfingerprints are often persistent across models in the same model family (e.g.\nllama-13b vs. llama-65b) and that models fine-tuned for chat are easier to\ndetect than standard language models, indicating that LLM fingerprints may be\ndirectly induced by the training data.", "published": "2024-05-22 23:02:42", "link": "http://arxiv.org/abs/2405.14057v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DEGAP: Dual Event-Guided Adaptive Prefixes for Templated-Based Event\n  Argument Extraction with Slot Querying", "abstract": "Recent advancements in event argument extraction (EAE) involve incorporating\nuseful auxiliary information into models during training and inference, such as\nretrieved instances and event templates. These methods face two challenges: (1)\nthe retrieval results may be irrelevant and (2) templates are developed\nindependently for each event without considering their possible relationship.\nIn this work, we propose DEGAP to address these challenges through a simple yet\neffective components: dual prefixes, i.e. learnable prompt vectors, where the\ninstance-oriented prefix and template-oriented prefix are trained to learn\ninformation from different event instances and templates. Additionally, we\npropose an event-guided adaptive gating mechanism, which can adaptively\nleverage possible connections between different events and thus capture\nrelevant information from the prefix. Finally, these event-guided prefixes\nprovide relevant information as cues to EAE model without retrieval. Extensive\nexperiments demonstrate that our method achieves new state-of-the-art\nperformance on four datasets (ACE05, RAMS, WIKIEVENTS, and MLEE). Further\nanalysis shows the impact of different components.", "published": "2024-05-22 03:56:55", "link": "http://arxiv.org/abs/2405.13325v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Contextualized Automatic Speech Recognition with Dynamic Vocabulary", "abstract": "Deep biasing (DB) enhances the performance of end-to-end automatic speech\nrecognition (E2E-ASR) models for rare words or contextual phrases using a bias\nlist. However, most existing methods treat bias phrases as sequences of\nsubwords in a predefined static vocabulary. This naive sequence decomposition\nproduces unnatural token patterns, significantly lowering their occurrence\nprobability. More advanced techniques address this problem by expanding the\nvocabulary with additional modules, including the external language model\nshallow fusion or rescoring. However, they result in increasing the workload\ndue to the additional modules. This paper proposes a dynamic vocabulary where\nbias tokens can be added during inference. Each entry in a bias list is\nrepresented as a single token, unlike a sequence of existing subword tokens.\nThis approach eliminates the need to learn subword dependencies within the bias\nphrases. This method is easily applied to various architectures because it only\nexpands the embedding and output layers in common E2E-ASR architectures.\nExperimental results demonstrate that the proposed method improves the bias\nphrase WER on English and Japanese datasets by 3.1 -- 4.9 points compared with\nthe conventional DB method.", "published": "2024-05-22 05:03:39", "link": "http://arxiv.org/abs/2405.13344v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "You don't understand me!: Comparing ASR results for L1 and L2 speakers\n  of Swedish", "abstract": "The performance of Automatic Speech Recognition (ASR) systems has constantly\nincreased in state-of-the-art development. However, performance tends to\ndecrease considerably in more challenging conditions (e.g., background noise,\nmultiple speaker social conversations) and with more atypical speakers (e.g.,\nchildren, non-native speakers or people with speech disorders), which signifies\nthat general improvements do not necessarily transfer to applications that rely\non ASR, e.g., educational software for younger students or language learners.\nIn this study, we focus on the gap in performance between recognition results\nfor native and non-native, read and spontaneous, Swedish utterances transcribed\nby different ASR services. We compare the recognition results using Word Error\nRate and analyze the linguistic factors that may generate the observed\ntranscription errors.", "published": "2024-05-22 06:24:55", "link": "http://arxiv.org/abs/2405.13379v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Latent Space Alignment for Semantic Channel Equalization", "abstract": "We relax the constraint of a shared language between agents in a semantic and\ngoal-oriented communication system to explore the effect of language mismatch\nin distributed task solving. We propose a mathematical framework, which\nprovides a modelling and a measure of the semantic distortion introduced in the\ncommunication when agents use distinct languages. We then propose a new\napproach to semantic channel equalization with proven effectiveness through\nnumerical evaluations.", "published": "2024-05-22 10:12:32", "link": "http://arxiv.org/abs/2405.13511v2", "categories": ["cs.LG", "cs.CL", "cs.IT", "math.IT"], "primary_category": "cs.LG"}
{"title": "Joint Optimization of Streaming and Non-Streaming Automatic Speech\n  Recognition with Multi-Decoder and Knowledge Distillation", "abstract": "End-to-end (E2E) automatic speech recognition (ASR) can operate in two modes:\nstreaming and non-streaming, each with its pros and cons. Streaming ASR\nprocesses the speech frames in real-time as it is being received, while\nnon-streaming ASR waits for the entire speech utterance; thus, professionals\nmay have to operate in either mode to satisfy their application. In this work,\nwe present joint optimization of streaming and non-streaming ASR based on\nmulti-decoder and knowledge distillation. Primarily, we study 1) the encoder\nintegration of these ASR modules, followed by 2) separate decoders to make the\nswitching mode flexible, and enhancing performance by 3) incorporating\nsimilarity-preserving knowledge distillation between the two modular encoders\nand decoders. Evaluation results show 2.6%-5.3% relative character error rate\nreductions (CERR) on CSJ for streaming ASR, and 8.3%-9.7% relative CERRs for\nnon-streaming ASR within a single model compared to multiple standalone\nmodules.", "published": "2024-05-22 10:17:30", "link": "http://arxiv.org/abs/2405.13514v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Beyond Trend and Periodicity: Guiding Time Series Forecasting with\n  Textual Cues", "abstract": "This work introduces a novel Text-Guided Time Series Forecasting (TGTSF)\ntask. By integrating textual cues, such as channel descriptions and dynamic\nnews, TGTSF addresses the critical limitations of traditional methods that rely\npurely on historical data. To support this task, we propose TGForecaster, a\nrobust baseline model that fuses textual cues and time series data using\ncross-attention mechanisms. We then present four meticulously curated benchmark\ndatasets to validate the proposed framework, ranging from simple periodic data\nto complex, event-driven fluctuations. Our comprehensive evaluations\ndemonstrate that TGForecaster consistently achieves state-of-the-art\nperformance, highlighting the transformative potential of incorporating textual\ninformation into time series forecasting. This work not only pioneers a novel\nforecasting task but also establishes a new benchmark for future research,\ndriving advancements in multimodal data integration for time series models.", "published": "2024-05-22 10:45:50", "link": "http://arxiv.org/abs/2405.13522v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Attention Mechanisms Don't Learn Additive Models: Rethinking Feature\n  Importance for Transformers", "abstract": "We address the critical challenge of applying feature attribution methods to\nthe transformer architecture, which dominates current applications in natural\nlanguage processing and beyond. Traditional attribution methods to explainable\nAI (XAI) explicitly or implicitly rely on linear or additive surrogate models\nto quantify the impact of input features on a model's output. In this work, we\nformally prove an alarming incompatibility: transformers are structurally\nincapable of representing linear or additive surrogate models used for feature\nattribution, undermining the grounding of these conventional explanation\nmethodologies. To address this discrepancy, we introduce the Softmax-Linked\nAdditive Log Odds Model (SLALOM), a novel surrogate model specifically designed\nto align with the transformer framework. SLALOM demonstrates the capacity to\ndeliver a range of insightful explanations with both synthetic and real-world\ndatasets. We highlight SLALOM's unique efficiency-quality curve by showing that\nSLALOM can produce explanations with substantially higher fidelity than\ncompeting surrogate models or provide explanations of comparable quality at a\nfraction of their computational costs. We release code for SLALOM as an\nopen-source project online at https://github.com/tleemann/slalom_explanations.", "published": "2024-05-22 11:14:00", "link": "http://arxiv.org/abs/2405.13536v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Annotation-Efficient Preference Optimization for Language Model\n  Alignment", "abstract": "Preference optimization is a standard approach to fine-tuning large language\nmodels to align with human preferences. The quality, diversity, and quantity of\nthe preference dataset are critical to the effectiveness of preference\noptimization. However, obtaining a large amount of high-quality and diverse\npreference annotations is difficult in many applications. This raises the\nquestion of how to use the limited annotation budget to create an effective\npreference dataset. To this end, we propose Annotation-Efficient Preference\nOptimization (AEPO). Instead of exhaustively annotating preference over all\navailable response texts, AEPO selects a subset of responses that maximizes\nquality and diversity from the available responses, and then annotates\npreference over the selected ones. In this way, AEPO focuses the annotation\nbudget on labeling preference over a smaller subset of responses with diversity\nand of high quality. We evaluate the performance of Direct Preference\nOptimization (DPO) using AEPO and show that it outperforms models trained using\na standard DPO with the same annotation budget. Our code is available at\nhttps://github.com/CyberAgentAILab/annotation-efficient-po", "published": "2024-05-22 11:23:03", "link": "http://arxiv.org/abs/2405.13541v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CPE-Identifier: Automated CPE identification and CVE summaries\n  annotation with Deep Learning and NLP", "abstract": "With the drastic increase in the number of new vulnerabilities in the\nNational Vulnerability Database (NVD) every year, the workload for NVD analysts\nto associate the Common Platform Enumeration (CPE) with the Common\nVulnerabilities and Exposures (CVE) summaries becomes increasingly laborious\nand slow. The delay causes organisations, which depend on NVD for vulnerability\nmanagement and security measurement, to be more vulnerable to zero-day attacks.\nThus, it is essential to come out with a technique and tool to extract the CPEs\nin the CVE summaries accurately and quickly. In this work, we propose the\nCPE-Identifier system, an automated CPE annotating and extracting system, from\nthe CVE summaries. The system can be used as a tool to identify CPE entities\nfrom new CVE text inputs. Moreover, we also automate the data generating and\nlabeling processes using deep learning models. Due to the complexity of the CVE\ntexts, new technical terminologies appear frequently. To identify novel words\nin future CVE texts, we apply Natural Language Processing (NLP) Named Entity\nRecognition (NER), to identify new technical jargons in the text. Our proposed\nmodel achieves an F1 score of 95.48%, an accuracy score of 99.13%, a precision\nof 94.83%, and a recall of 96.14%. We show that it outperforms prior works on\nautomated CVE-CPE labeling by more than 9% on all metrics.", "published": "2024-05-22 12:05:17", "link": "http://arxiv.org/abs/2405.13568v1", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "COTET: Cross-view Optimal Transport for Knowledge Graph Entity Typing", "abstract": "Knowledge graph entity typing (KGET) aims to infer missing entity type\ninstances in knowledge graphs. Previous research has predominantly centered\naround leveraging contextual information associated with entities, which\nprovides valuable clues for inference. However, they have long ignored the dual\nnature of information inherent in entities, encompassing both high-level\ncoarse-grained cluster knowledge and fine-grained type knowledge. This paper\nintroduces Cross-view Optimal Transport for knowledge graph Entity Typing\n(COTET), a method that effectively incorporates the information on how types\nare clustered into the representation of entities and types. COTET comprises\nthree modules: i) Multi-view Generation and Encoder, which captures structured\nknowledge at different levels of granularity through entity-type,\nentity-cluster, and type-cluster-type perspectives; ii) Cross-view Optimal\nTransport, transporting view-specific embeddings to a unified space by\nminimizing the Wasserstein distance from a distributional alignment\nperspective; iii) Pooling-based Entity Typing Prediction, employing a mixture\npooling mechanism to aggregate prediction scores from diverse neighbors of an\nentity. Additionally, we introduce a distribution-based loss function to\nmitigate the occurrence of false negatives during training. Extensive\nexperiments demonstrate the effectiveness of COTET when compared to existing\nbaselines.", "published": "2024-05-22 12:53:12", "link": "http://arxiv.org/abs/2405.13602v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Knowledge Graph Reasoning with Self-supervised Reinforcement Learning", "abstract": "Reinforcement learning (RL) is an effective method of finding reasoning\npathways in incomplete knowledge graphs (KGs). To overcome the challenges of a\nlarge action space, a self-supervised pre-training method is proposed to warm\nup the policy network before the RL training stage. To alleviate the\ndistributional mismatch issue in general self-supervised RL (SSRL), in our\nsupervised learning (SL) stage, the agent selects actions based on the policy\nnetwork and learns from generated labels; this self-generation of labels is the\nintuition behind the name self-supervised. With this training framework, the\ninformation density of our SL objective is increased and the agent is prevented\nfrom getting stuck with the early rewarded paths. Our self-supervised RL (SSRL)\nmethod improves the performance of RL by pairing it with the wide coverage\nachieved by SL during pretraining, since the breadth of the SL objective makes\nit infeasible to train an agent with that alone. We show that our SSRL model\nmeets or exceeds current state-of-the-art results on all Hits@k and mean\nreciprocal rank (MRR) metrics on four large benchmark KG datasets. This SSRL\nmethod can be used as a plug-in for any RL architecture for a KGR task. We\nadopt two RL architectures, i.e., MINERVA and MultiHopKG as our baseline RL\nmodels and experimentally show that our SSRL model consistently outperforms\nboth baselines on all of these four KG reasoning tasks. Full code for the paper\navailable at\nhttps://github.com/owenonline/Knowledge-Graph-Reasoning-with-Self-supervised-Reinforcement-Learning.", "published": "2024-05-22 13:39:33", "link": "http://arxiv.org/abs/2405.13640v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "xRAG: Extreme Context Compression for Retrieval-augmented Generation\n  with One Token", "abstract": "This paper introduces xRAG, an innovative context compression method tailored\nfor retrieval-augmented generation. xRAG reinterprets document embeddings in\ndense retrieval--traditionally used solely for retrieval--as features from the\nretrieval modality. By employing a modality fusion methodology, xRAG seamlessly\nintegrates these embeddings into the language model representation space,\neffectively eliminating the need for their textual counterparts and achieving\nan extreme compression rate. In xRAG, the only trainable component is the\nmodality bridge, while both the retriever and the language model remain frozen.\nThis design choice allows for the reuse of offline-constructed document\nembeddings and preserves the plug-and-play nature of retrieval augmentation.\nExperimental results demonstrate that xRAG achieves an average improvement of\nover 10% across six knowledge-intensive tasks, adaptable to various language\nmodel backbones, ranging from a dense 7B model to an 8x7B Mixture of Experts\nconfiguration. xRAG not only significantly outperforms previous context\ncompression methods but also matches the performance of uncompressed models on\nseveral datasets, while reducing overall FLOPs by a factor of 3.53. Our work\npioneers new directions in retrieval-augmented generation from the perspective\nof multimodality fusion, and we hope it lays the foundation for future\nefficient and scalable retrieval-augmented systems", "published": "2024-05-22 16:15:17", "link": "http://arxiv.org/abs/2405.13792v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Slaves to the Law of Large Numbers: An Asymptotic Equipartition Property\n  for Perplexity in Generative Language Models", "abstract": "We prove a new asymptotic equipartition property for the perplexity of long\ntexts generated by a language model and present supporting experimental\nevidence from open-source models. Specifically we show that the logarithmic\nperplexity of any large text generated by a language model must asymptotically\nconverge to the average entropy of its token distributions. This defines a\n\"typical set\" that all long synthetic texts generated by a language model must\nbelong to. We show that this typical set is a vanishingly small subset of all\npossible grammatically correct outputs. These results suggest possible\napplications to important practical problems such as (a) detecting synthetic\nAI-generated text, and (b) testing whether a text was used to train a language\nmodel. We make no simplifying assumptions (such as stationarity) about the\nstatistics of language model outputs, and therefore our results are directly\napplicable to practical real-world models without any approximations.", "published": "2024-05-22 16:23:40", "link": "http://arxiv.org/abs/2405.13798v3", "categories": ["cs.CL", "cs.AI", "cs.IT", "math.IT"], "primary_category": "cs.CL"}
{"title": "Image-of-Thought Prompting for Visual Reasoning Refinement in Multimodal\n  Large Language Models", "abstract": "Recent advancements in Chain-of-Thought (CoT) and related rationale-based\nworks have significantly improved the performance of Large Language Models\n(LLMs) in complex reasoning tasks. With the evolution of Multimodal Large\nLanguage Models (MLLMs), enhancing their capability to tackle complex\nmultimodal reasoning problems is a crucial frontier. However, incorporating\nmultimodal rationales in CoT has yet to be thoroughly investigated. We propose\nthe Image-of-Thought (IoT) prompting method, which helps MLLMs to extract\nvisual rationales step-by-step. Specifically, IoT prompting can automatically\ndesign critical visual information extraction operations based on the input\nimages and questions. Each step of visual information refinement identifies\nspecific visual rationales that support answers to complex visual reasoning\nquestions. Beyond the textual CoT, IoT simultaneously utilizes visual and\ntextual rationales to help MLLMs understand complex multimodal information. IoT\nprompting has improved zero-shot visual reasoning performance across various\nvisual understanding tasks in different MLLMs. Moreover, the step-by-step\nvisual feature explanations generated by IoT prompting elucidate the visual\nreasoning process, aiding in analyzing the cognitive processes of large\nmultimodal models", "published": "2024-05-22 17:56:51", "link": "http://arxiv.org/abs/2405.13872v2", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "TOPA: Extending Large Language Models for Video Understanding via\n  Text-Only Pre-Alignment", "abstract": "Recent advancements in image understanding have benefited from the extensive\nuse of web image-text pairs. However, video understanding remains a challenge\ndespite the availability of substantial web video-text data. This difficulty\nprimarily arises from the inherent complexity of videos and the inefficient\nlanguage supervision in recent web-collected video-text datasets. In this\npaper, we introduce Text-Only Pre-Alignment (TOPA), a novel approach to extend\nlarge language models (LLMs) for video understanding, without the need for\npre-training on real video data. Specifically, we first employ an advanced LLM\nto automatically generate Textual Videos comprising continuous textual frames,\nalong with corresponding annotations to simulate real video-text data. Then,\nthese annotated textual videos are used to pre-align a language-only LLM with\nthe video modality. To bridge the gap between textual and real videos, we\nemploy the CLIP model as the feature extractor to align image and text\nmodalities. During text-only pre-alignment, the continuous textual frames,\nencoded as a sequence of CLIP text features, are analogous to continuous CLIP\nimage features, thus aligning the LLM with real video representation. Extensive\nexperiments, including zero-shot evaluation and finetuning on various video\nunderstanding tasks, demonstrate that TOPA is an effective and efficient\nframework for aligning video content with LLMs. In particular, without training\non any video data, the TOPA-Llama2-13B model achieves a Top-1 accuracy of 51.0%\non the challenging long-form video understanding benchmark, Egoschema. This\nperformance surpasses previous video-text pre-training approaches and proves\ncompetitive with recent GPT-3.5-based video agents.", "published": "2024-05-22 18:35:10", "link": "http://arxiv.org/abs/2405.13911v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "What is Your Data Worth to GPT? LLM-Scale Data Valuation with Influence\n  Functions", "abstract": "Large language models (LLMs) are trained on a vast amount of human-written\ndata, but data providers often remain uncredited. In response to this issue,\ndata valuation (or data attribution), which quantifies the contribution or\nvalue of each data to the model output, has been discussed as a potential\nsolution. Nevertheless, applying existing data valuation methods to recent LLMs\nand their vast training datasets has been largely limited by prohibitive\ncompute and memory costs. In this work, we focus on influence functions, a\npopular gradient-based data valuation method, and significantly improve its\nscalability with an efficient gradient projection strategy called LoGra that\nleverages the gradient structure in backpropagation. We then provide a\ntheoretical motivation of gradient projection approaches to influence functions\nto promote trust in the data valuation process. Lastly, we lower the barrier to\nimplementing data valuation systems by introducing LogIX, a software package\nthat can transform existing training code into data valuation code with minimal\neffort. In our data valuation experiments, LoGra achieves competitive accuracy\nagainst more expensive baselines while showing up to 6,500x improvement in\nthroughput and 5x reduction in GPU memory usage when applied to\nLlama3-8B-Instruct and the 1B-token dataset.", "published": "2024-05-22 19:39:05", "link": "http://arxiv.org/abs/2405.13954v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Embedding Trajectory for Out-of-Distribution Detection in Mathematical\n  Reasoning", "abstract": "Real-world data deviating from the independent and identically distributed\n(i.i.d.) assumption of in-distribution training data poses security threats to\ndeep networks, thus advancing out-of-distribution (OOD) detection algorithms.\nDetection methods in generative language models (GLMs) mainly focus on\nuncertainty estimation and embedding distance measurement, with the latter\nproven to be most effective in traditional linguistic tasks like summarization\nand translation. However, another complex generative scenario mathematical\nreasoning poses significant challenges to embedding-based methods due to its\nhigh-density feature of output spaces, but this feature causes larger\ndiscrepancies in the embedding shift trajectory between different samples in\nlatent spaces. Hence, we propose a trajectory-based method TV score, which uses\ntrajectory volatility for OOD detection in mathematical reasoning. Experiments\nshow that our method outperforms all traditional algorithms on GLMs under\nmathematical reasoning scenarios and can be extended to more applications with\nhigh-density features in output spaces, such as multiple-choice questions.", "published": "2024-05-22 22:22:25", "link": "http://arxiv.org/abs/2405.14039v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "How Many Bytes Can You Take Out Of Brain-To-Text Decoding?", "abstract": "Brain-computer interfaces have promising medical and scientific applications\nfor aiding speech and studying the brain. In this work, we propose an\ninformation-based evaluation metric for brain-to-text decoders. Using this\nmetric, we examine two methods to augment existing state-of-the-art continuous\ntext decoders. We show that these methods, in concert, can improve brain\ndecoding performance by upwards of 40% when compared to a baseline model. We\nfurther examine the informatic properties of brain-to-text decoders and show\nempirically that they have Zipfian power law dynamics. Finally, we provide an\nestimate for the idealized performance of an fMRI-based text decoder. We\ncompare this idealized model to our current model, and use our\ninformation-based metric to quantify the main sources of decoding error. We\nconclude that a practical brain-to-text decoder is likely possible given\nfurther algorithmic improvements.", "published": "2024-05-22 22:57:04", "link": "http://arxiv.org/abs/2405.14055v1", "categories": ["cs.CL", "cs.AI", "cs.ET"], "primary_category": "cs.CL"}
{"title": "Meanings and Feelings of Large Language Models: Observability of Latent\n  States in Generative AI", "abstract": "We tackle the question of whether Large Language Models (LLMs), viewed as\ndynamical systems with state evolving in the embedding space of symbolic\ntokens, are observable. That is, whether there exist multiple 'mental' state\ntrajectories that yield the same sequence of generated tokens, or sequences\nthat belong to the same Nerode equivalence class ('meaning'). If not\nobservable, mental state trajectories ('experiences') evoked by an input\n('perception') or by feedback from the model's own state ('thoughts') could\nremain self-contained and evolve unbeknown to the user while being potentially\naccessible to the model provider. Such \"self-contained experiences evoked by\nperception or thought\" are akin to what the American Psychological Association\n(APA) defines as 'feelings'. Beyond the lexical curiosity, we show that current\nLLMs implemented by autoregressive Transformers cannot have 'feelings'\naccording to this definition: The set of state trajectories indistinguishable\nfrom the tokenized output is a singleton. But if there are 'system prompts' not\nvisible to the user, then the set of indistinguishable trajectories becomes\nnon-trivial, and there can be multiple state trajectories that yield the same\nverbalized output. We prove these claims analytically, and show examples of\nmodifications to standard LLMs that engender such 'feelings.' Our analysis\nsheds light on possible designs that would enable a model to perform\nnon-trivial computation that is not visible to the user, as well as on controls\nthat the provider of services using the model could take to prevent unintended\nbehavior.", "published": "2024-05-22 23:18:58", "link": "http://arxiv.org/abs/2405.14061v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "DETAIL: Task DEmonsTration Attribution for Interpretable In-context\n  Learning", "abstract": "In-context learning (ICL) allows transformer-based language models that are\npre-trained on general text to quickly learn a specific task with a few \"task\ndemonstrations\" without updating their parameters, significantly boosting their\nflexibility and generality. ICL possesses many distinct characteristics from\nconventional machine learning, thereby requiring new approaches to interpret\nthis learning paradigm. Taking the viewpoint of recent works showing that\ntransformers learn in context by formulating an internal optimizer, we propose\nan influence function-based attribution technique, DETAIL, that addresses the\nspecific characteristics of ICL. We empirically verify the effectiveness of our\napproach for demonstration attribution while being computationally efficient.\nLeveraging the results, we then show how DETAIL can help improve model\nperformance in real-world scenarios through demonstration reordering and\ncuration. Finally, we experimentally prove the wide applicability of DETAIL by\nshowing our attribution scores obtained on white-box models are transferable to\nblack-box models in improving model performance.", "published": "2024-05-22 15:52:52", "link": "http://arxiv.org/abs/2405.14899v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "KU-DMIS at EHRSQL 2024:Generating SQL query via question templatization\n  in EHR", "abstract": "Transforming natural language questions into SQL queries is crucial for\nprecise data retrieval from electronic health record (EHR) databases. A\nsignificant challenge in this process is detecting and rejecting unanswerable\nquestions that request information beyond the database's scope or exceed the\nsystem's capabilities. In this paper, we introduce a novel text-to-SQL\nframework that robustly handles out-of-domain questions and verifies the\ngenerated queries with query execution.Our framework begins by standardizing\nthe structure of questions into a templated format. We use a powerful large\nlanguage model (LLM), fine-tuned GPT-3.5 with detailed prompts involving the\ntable schemas of the EHR database system. Our experimental results demonstrate\nthe effectiveness of our framework on the EHRSQL-2024 benchmark benchmark, a\nshared task in the ClinicalNLP workshop. Although a straightforward fine-tuning\nof GPT shows promising results on the development set, it struggled with the\nout-of-domain questions in the test set. With our framework, we improve our\nsystem's adaptability and achieve competitive performances in the official\nleaderboard of the EHRSQL-2024 challenge.", "published": "2024-05-22 02:15:57", "link": "http://arxiv.org/abs/2406.00014v2", "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.DB"}
{"title": "More Distinctively Black and Feminine Faces Lead to Increased\n  Stereotyping in Vision-Language Models", "abstract": "Vision Language Models (VLMs), exemplified by GPT-4V, adeptly integrate text\nand vision modalities. This integration enhances Large Language Models' ability\nto mimic human perception, allowing them to process image inputs. Despite VLMs'\nadvanced capabilities, however, there is a concern that VLMs inherit biases of\nboth modalities in ways that make biases more pervasive and difficult to\nmitigate. Our study explores how VLMs perpetuate homogeneity bias and trait\nassociations with regards to race and gender. When prompted to write stories\nbased on images of human faces, GPT-4V describes subordinate racial and gender\ngroups with greater homogeneity than dominant groups and relies on distinct,\nyet generally positive, stereotypes. Importantly, VLM stereotyping is driven by\nvisual cues rather than group membership alone such that faces that are rated\nas more prototypically Black and feminine are subject to greater stereotyping.\nThese findings suggest that VLMs may associate subtle visual cues related to\nracial and gender groups with stereotypes in ways that could be challenging to\nmitigate. We explore the underlying reasons behind this behavior and discuss\nits implications and emphasize the importance of addressing these biases as\nVLMs come to mirror human perception.", "published": "2024-05-22 00:45:29", "link": "http://arxiv.org/abs/2407.06194v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Big5PersonalityEssays: Introducing a Novel Synthetic Generated Dataset\n  Consisting of Short State-of-Consciousness Essays Annotated Based on the Five\n  Factor Model of Personality", "abstract": "Given the high advances of large language models (LLM) it is of vital\nimportance to study their behaviors and apply their utility in all kinds of\nscientific fields. Psychology has been, in recent years, poorly approached\nusing novel computational tools. One of the reasons is the high complexity of\nthe data required for a proper analysis. Moreover, psychology, with a focus on\npsychometry, has few datasets available for analysis and artificial\nintelligence usage. Because of these facts, this study introduces a synthethic\ndatabase of short essays labeled based on the five factor model (FFM) of\npersonality traits.", "published": "2024-05-22 10:10:20", "link": "http://arxiv.org/abs/2407.17586v1", "categories": ["cs.OH", "cs.CL", "cs.CY", "cs.DL"], "primary_category": "cs.OH"}
{"title": "High Performance P300 Spellers Using GPT2 Word Prediction With\n  Cross-Subject Training", "abstract": "Amyotrophic lateral sclerosis (ALS) severely impairs patients' ability to\ncommunicate, often leading to a decline in their quality of life within a few\nyears of diagnosis. The P300 speller brain-computer interface (BCI) offers an\nalternative communication method by interpreting a subject's EEG response to\ncharacters presented on a grid interface.\n  This paper addresses the common speed limitations encountered in training\nefficient P300-based multi-subject classifiers by introducing innovative\n\"across-subject\" classifiers. We leverage a combination of the\nsecond-generation Generative Pre-Trained Transformer (GPT2) and Dijkstra's\nalgorithm to optimize stimuli and suggest word completion choices based on\ntyping history. Additionally, we employ a multi-layered smoothing technique to\naccommodate out-of-vocabulary (OOV) words.\n  Through extensive simulations involving random sampling of EEG data from\nsubjects, we demonstrate significant speed enhancements in typing passages\ncontaining rare and OOV words. These optimizations result in approximately 10%\nimprovement in character-level typing speed and up to 40% improvement in\nmulti-word prediction. We demonstrate that augmenting standard row/column\nhighlighting techniques with layered word prediction yields close-to-optimal\nperformance.\n  Furthermore, we explore both \"within-subject\" and \"across-subject\" training\ntechniques, showing that speed improvements are consistent across both\napproaches.", "published": "2024-05-22 04:15:41", "link": "http://arxiv.org/abs/2405.13329v1", "categories": ["cs.CL", "cs.HC", "cs.SY", "eess.SP", "eess.SY"], "primary_category": "cs.CL"}
{"title": "Ambisonizer: Neural Upmixing as Spherical Harmonics Generation", "abstract": "Neural upmixing, the task of generating immersive music with an increased\nnumber of channels from fewer input channels, has been an active research area,\nwith mono-to-stereo and stereo-to-surround upmixing treated as separate\nproblems. In this paper, we propose a unified approach to neural upmixing by\nformulating it as spherical harmonics - more specifically, Ambisonic\ngeneration. We explicitly formulate mono upmixing as unconditional generation\nand stereo upmixing as conditional generation, where the stereo signals serve\nas conditions. We provide evidence that our proposed methodology, when decoded\nto stereo, matches a strong commercial stereo widener in subjective ratings.\nOverall, our work presents direct upmixing to Ambisonic format as a strong and\npromising approach to neural upmixing. A discussion on limitations is also\nprovided.", "published": "2024-05-22 08:16:24", "link": "http://arxiv.org/abs/2405.13428v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Timbre Perception, Representation, and its Neuroscientific Exploration:\n  A Comprehensive Review", "abstract": "Timbre, the sound's unique \"color\", is fundamental to how we perceive and\nappreciate music. This review explores the multifaceted world of timbre\nperception and representation. It begins by tracing the word's origin, offering\nan intuitive grasp of the concept. Building upon this foundation, the article\ndelves into the complexities of defining and measuring timbre. It then explores\nthe concept and techniques of timbre space, a powerful tool for visualizing how\nwe perceive different timbres. The review further examines recent advancements\nin timbre manipulation and representation, including the increasingly utilized\nmachine learning techniques. While the underlying neural mechanisms remain\npartially understood, the article discusses current neuroimaging techniques\nused to investigate this aspect of perception. Finally, it summarizes key\ntakeaways, identifies promising future research directions, and emphasizes the\npotential applications of timbre research in music technology, assistive\ntechnologies, and our overall understanding of auditory perception.", "published": "2024-05-22 14:06:20", "link": "http://arxiv.org/abs/2405.13661v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Near-Real-Time Processing Ego Speech Filtering Pipeline Designed for\n  Speech Interruption During Human-Robot Interaction", "abstract": "With current state-of-the-art automatic speech recognition (ASR) systems, it\nis not possible to transcribe overlapping speech audio streams separately.\nConsequently, when these ASR systems are used as part of a social robot like\nPepper for interaction with a human, it is common practice to close the robot's\nmicrophone while it is talking itself. This prevents the human users to\ninterrupt the robot, which limits speech-based human-robot interaction. To\nenable a more natural interaction which allows for such interruptions, we\npropose an audio processing pipeline for filtering out robot's ego speech using\nonly a single-channel microphone. This pipeline takes advantage of the\npossibility to feed the robot ego speech signal, generated by a text-to-speech\nAPI, as training data into a machine learning model. The proposed pipeline\ncombines a convolutional neural network and spectral subtraction to extract\noverlapping human speech from the audio recorded by the robot-embedded\nmicrophone. When evaluating on a held-out test set, we find that this pipeline\noutperforms our previous approach to this task, as well as state-of-the-art\ntarget speech extraction systems that were retrained on the same dataset. We\nhave also integrated the proposed pipeline into a lightweight robot software\ndevelopment framework to make it available for broader use. As a step towards\ndemonstrating the feasibility of deploying our pipeline, we use this framework\nto evaluate the effectiveness of the pipeline in a small lab-based feasibility\npilot using the social robot Pepper. Our results show that when participants\ninterrupt the robot, the pipeline can extract the participant's speech from\none-second streaming audio buffers received by the robot-embedded\nsingle-channel microphone, hence in near-real time.", "published": "2024-05-22 09:39:09", "link": "http://arxiv.org/abs/2405.13477v1", "categories": ["cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
{"title": "End-to-End Real-World Polyphonic Piano Audio-to-Score Transcription with\n  Hierarchical Decoding", "abstract": "Piano audio-to-score transcription (A2S) is an important yet underexplored\ntask with extensive applications for music composition, practice, and analysis.\nHowever, existing end-to-end piano A2S systems faced difficulties in retrieving\nbar-level information such as key and time signatures, and have been trained\nand evaluated with only synthetic data. To address these limitations, we\npropose a sequence-to-sequence (Seq2Seq) model with a hierarchical decoder that\naligns with the hierarchical structure of musical scores, enabling the\ntranscription of score information at both the bar and note levels by\nmulti-task learning. To bridge the gap between synthetic data and recordings of\nhuman performance, we propose a two-stage training scheme, which involves\npre-training the model using an expressive performance rendering (EPR) system\non synthetic audio, followed by fine-tuning the model using recordings of human\nperformance. To preserve the voicing structure for score reconstruction, we\npropose a pre-processing method for **Kern scores in scenarios with an\nunconstrained number of voices. Experimental results support the effectiveness\nof our proposed approaches, in terms of both transcription performance on\nsynthetic audio data in comparison to the current state-of-the-art, and the\nfirst experiment on human recordings.", "published": "2024-05-22 10:52:04", "link": "http://arxiv.org/abs/2405.13527v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio Mamba: Pretrained Audio State Space Model For Audio Tagging", "abstract": "Audio tagging is an important task of mapping audio samples to their\ncorresponding categories. Recently endeavours that exploit transformer models\nin this field have achieved great success. However, the quadratic\nself-attention cost limits the scaling of audio transformer models and further\nconstrains the development of more universal audio models. In this paper, we\nattempt to solve this problem by proposing Audio Mamba, a self-attention-free\napproach that captures long audio spectrogram dependency with state space\nmodels. Our experimental results on two audio-tagging datasets demonstrate the\nparameter efficiency of Audio Mamba, it achieves comparable results to SOTA\naudio spectrogram transformers with one third parameters.", "published": "2024-05-22 13:35:56", "link": "http://arxiv.org/abs/2405.13636v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Versatile Diffusion Transformer with Mixture of Noise Levels for\n  Audiovisual Generation", "abstract": "Training diffusion models for audiovisual sequences allows for a range of\ngeneration tasks by learning conditional distributions of various input-output\ncombinations of the two modalities. Nevertheless, this strategy often requires\ntraining a separate model for each task which is expensive. Here, we propose a\nnovel training approach to effectively learn arbitrary conditional\ndistributions in the audiovisual space.Our key contribution lies in how we\nparameterize the diffusion timestep in the forward diffusion process. Instead\nof the standard fixed diffusion timestep, we propose applying variable\ndiffusion timesteps across the temporal dimension and across modalities of the\ninputs. This formulation offers flexibility to introduce variable noise levels\nfor various portions of the input, hence the term mixture of noise levels. We\npropose a transformer-based audiovisual latent diffusion model and show that it\ncan be trained in a task-agnostic fashion using our approach to enable a\nvariety of audiovisual generation tasks at inference time. Experiments\ndemonstrate the versatility of our method in tackling cross-modal and\nmultimodal interpolation tasks in the audiovisual space. Notably, our proposed\napproach surpasses baselines in generating temporally and perceptually\nconsistent samples conditioned on the input. Project page: avdit2024.github.io", "published": "2024-05-22 15:47:14", "link": "http://arxiv.org/abs/2405.13762v1", "categories": ["cs.CV", "cs.LG", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
