{"title": "The Fourier Cosine Method for Discrete Probability Distributions", "abstract": "We provide a rigorous convergence proof demonstrating that the well-known\nsemi-analytical Fourier cosine (COS) formula for the inverse Fourier transform\nof continuous probability distributions can be extended to discrete probability\ndistributions, with the help of spectral filters. We establish general\nconvergence rates for these filters and further show that several classical\nspectral filters achieve convergence rates one order faster than previously\nrecognized in the literature on the Gibbs phenomenon. Our numerical experiments\ncorroborate the theoretical convergence results. Additionally, we illustrate\nthe computational speed and accuracy of the discrete COS method with\napplications in computational statistics and quantitative finance. The\ntheoretical and numerical results highlight the method's potential for solving\nproblems involving discrete distributions, particularly when the characteristic\nfunction is known, allowing the discrete Fourier transform (DFT) to be\nbypassed.", "published": "2024-10-06 14:05:43", "link": "http://arxiv.org/abs/2410.04487v2", "categories": ["math.NA", "cs.NA", "q-fin.CP"], "primary_category": "math.NA"}
{"title": "ReTok: Replacing Tokenizer to Enhance Representation Efficiency in Large\n  Language Model", "abstract": "Tokenizer is an essential component for large language models (LLMs), and a\ntokenizer with a high compression rate can improve the model's representation\nand processing efficiency. However, the tokenizer cannot ensure high\ncompression rate in all scenarios, and an increase in the average input and\noutput lengths will increases the training and inference costs of the model.\nTherefore, it is crucial to find ways to improve the model's efficiency with\nminimal cost while maintaining the model's performance. In this work, we\npropose a method to improve model representation and processing efficiency by\nreplacing the tokenizers of LLMs. We propose replacing and reinitializing the\nparameters of the model's input and output layers with the parameters of the\noriginal model, and training these parameters while keeping other parameters\nfixed. We conducted experiments on different LLMs, and the results show that\nour method can maintain the performance of the model after replacing the\ntokenizer, while significantly improving the decoding speed for long texts.", "published": "2024-10-06 03:01:07", "link": "http://arxiv.org/abs/2410.04335v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Inference Scaling for Long-Context Retrieval Augmented Generation", "abstract": "The scaling of inference computation has unlocked the potential of\nlong-context large language models (LLMs) across diverse settings. For\nknowledge-intensive tasks, the increased compute is often allocated to\nincorporate more external knowledge. However, without effectively utilizing\nsuch knowledge, solely expanding context does not always enhance performance.\nIn this work, we investigate inference scaling for retrieval augmented\ngeneration (RAG), exploring the combination of multiple strategies beyond\nsimply increasing the quantity of knowledge, including in-context learning and\niterative prompting. These strategies provide additional flexibility to scale\ntest-time computation (e.g., by increasing retrieved documents or generation\nsteps), thereby enhancing LLMs' ability to effectively acquire and utilize\ncontextual information. We address two key questions: (1) How does RAG\nperformance benefit from the scaling of inference computation when optimally\nconfigured? (2) Can we predict the optimal test-time compute allocation for a\ngiven budget by modeling the relationship between RAG performance and inference\nparameters? Our observations reveal that increasing inference computation leads\nto nearly linear gains in RAG performance when optimally allocated, a\nrelationship we describe as the inference scaling laws for RAG. Building on\nthis, we further develop the computation allocation model to estimate RAG\nperformance across different inference configurations. The model predicts\noptimal inference parameters under various computation constraints, which align\nclosely with the experimental results. By applying these optimal\nconfigurations, we demonstrate that scaling inference compute on long-context\nLLMs achieves up to 58.9% gains on benchmark datasets compared to standard RAG.", "published": "2024-10-06 03:42:15", "link": "http://arxiv.org/abs/2410.04343v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ordinal Preference Optimization: Aligning Human Preferences via NDCG", "abstract": "Aligning Large Language Models (LLMs) with diverse human preferences is a\npivotal technique for controlling model behaviors and enhancing generation\nquality. Reinforcement Learning from Human Feedback (RLHF), Direct Preference\nOptimization (DPO), and their variants optimize language models by pairwise\ncomparisons. However, when multiple responses are available, these approaches\nfall short of leveraging the extensive information in the ranking given by the\nreward models or human feedback. In this work, we propose a novel listwise\napproach named Ordinal Preference Optimization (OPO), which employs the\nNormalized Discounted Cumulative Gain (NDCG), a widely-used ranking metric, to\nbetter utilize relative proximity within ordinal multiple responses. We develop\nan end-to-end preference optimization algorithm by approximating NDCG with a\ndifferentiable surrogate loss. This approach builds a connection between\nranking models in information retrieval and the alignment problem. In aligning\nmulti-response datasets assigned with ordinal rewards, OPO outperforms existing\npairwise and listwise approaches on evaluation sets and general benchmarks like\nAlpacaEval. Moreover, we demonstrate that increasing the pool of negative\nsamples can enhance model performance by reducing the adverse effects of\ntrivial negatives.", "published": "2024-10-06 03:49:28", "link": "http://arxiv.org/abs/2410.04346v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TIS-DPO: Token-level Importance Sampling for Direct Preference\n  Optimization With Estimated Weights", "abstract": "Direct Preference Optimization (DPO) has been widely adopted for preference\nalignment of Large Language Models (LLMs) due to its simplicity and\neffectiveness. However, DPO is derived as a bandit problem in which the whole\nresponse is treated as a single arm, ignoring the importance differences\nbetween tokens, which may affect optimization efficiency and make it difficult\nto achieve optimal results. In this work, we propose that the optimal data for\nDPO has equal expected rewards for each token in winning and losing responses,\nas there is no difference in token importance. However, since the optimal\ndataset is unavailable in practice, we propose using the original dataset for\nimportance sampling to achieve unbiased optimization. Accordingly, we propose a\ntoken-level importance sampling DPO objective named TIS-DPO that assigns\nimportance weights to each token based on its reward. Inspired by previous\nworks, we estimate the token importance weights using the difference in\nprediction probabilities from a pair of contrastive LLMs. We explore three\nmethods to construct these contrastive LLMs: (1) guiding the original LLM with\ncontrastive prompts, (2) training two separate LLMs using winning and losing\nresponses, and (3) performing forward and reverse DPO training with winning and\nlosing responses. Experiments show that TIS-DPO significantly outperforms\nvarious baseline methods on harmlessness and helpfulness alignment and\nsummarization tasks. We also visualize the estimated weights, demonstrating\ntheir ability to identify key token positions.", "published": "2024-10-06 04:03:00", "link": "http://arxiv.org/abs/2410.04350v2", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "CiMaTe: Citation Count Prediction Effectively Leveraging the Main Text", "abstract": "Prediction of the future citation counts of papers is increasingly important\nto find interesting papers among an ever-growing number of papers. Although a\npaper's main text is an important factor for citation count prediction, it is\ndifficult to handle in machine learning models because the main text is\ntypically very long; thus previous studies have not fully explored how to\nleverage it. In this paper, we propose a BERT-based citation count prediction\nmodel, called CiMaTe, that leverages the main text by explicitly capturing a\npaper's sectional structure. Through experiments with papers from computational\nlinguistics and biology domains, we demonstrate the CiMaTe's effectiveness,\noutperforming the previous methods in Spearman's rank correlation coefficient;\n5.1 points in the computational linguistics domain and 1.8 points in the\nbiology domain.", "published": "2024-10-06 08:39:13", "link": "http://arxiv.org/abs/2410.04404v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lens: Rethinking Multilingual Enhancement for Large Language Models", "abstract": "Despite the growing global demand for large language models (LLMs) that serve\nusers from diverse linguistic backgrounds, most cutting-edge LLMs remain\npredominantly English-centric. This creates a performance gap across languages,\nrestricting access to advanced AI services for non-English speakers. Current\nmethods to enhance multilingual capabilities largely rely on data-driven\npost-training techniques, such as multilingual instruction tuning or continual\npre-training. However, these approaches encounter significant challenges,\nincluding the scarcity of high-quality multilingual datasets and the limited\nenhancement of multilingual capabilities. They often suffer from off-target\nissues and catastrophic forgetting of central language abilities. To this end,\nwe propose Lens, a novel approach to enhance multilingual capabilities of LLMs\nby leveraging their internal language representation spaces. Specially, Lens\noperates by manipulating the hidden representations within the\nlanguage-agnostic and language-specific subspaces from top layers of LLMs.\nUsing the central language as a pivot, the target language is drawn closer to\nit within the language-agnostic subspace, allowing it to inherit\nwell-established semantic representations. Meanwhile, in the language-specific\nsubspace, the representations of the target and central languages are pushed\napart, enabling the target language to express itself distinctly. Extensive\nexperiments on one English-centric and two multilingual LLMs demonstrate that\nLens effectively improves multilingual performance without sacrificing the\noriginal central language capabilities of the backbone model, achieving\nsuperior results with much fewer computational resources compared to existing\npost-training approaches.", "published": "2024-10-06 08:51:30", "link": "http://arxiv.org/abs/2410.04407v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Blocks Architecture (BloArk): Efficient, Cost-Effective, and Incremental\n  Dataset Architecture for Wikipedia Revision History", "abstract": "Wikipedia (Wiki) is one of the most widely used and publicly available\nresources for natural language processing (NLP) applications. Wikipedia\nRevision History (WikiRevHist) shows the order in which edits were made to any\nWiki page since its first modification. While the most up-to-date Wiki has been\nwidely used as a training source, WikiRevHist can also be valuable resources\nfor NLP applications. However, there are insufficient tools available to\nprocess WikiRevHist without having substantial computing resources, making\nadditional customization, and spending extra time adapting others' works.\nTherefore, we report Blocks Architecture (BloArk), an efficiency-focused data\nprocessing architecture that reduces running time, computing resource\nrequirements, and repeated works in processing WikiRevHist dataset. BloArk\nconsists of three parts in its infrastructure: blocks, segments, and\nwarehouses. On top of that, we build the core data processing pipeline: builder\nand modifier. The BloArk builder transforms the original WikiRevHist dataset\nfrom XML syntax into JSON Lines (JSONL) format for improving the concurrent and\nstorage efficiency. The BloArk modifier takes previously-built warehouses to\noperate incremental modifications for improving the utilization of existing\ndatabases and reducing the cost of reusing others' works. In the end, BloArk\ncan scale up easily in both processing Wikipedia Revision History and\nincrementally modifying existing dataset for downstream NLP use cases. The\nsource code, documentations, and example usages are publicly available online\nand open-sourced under GPL-2.0 license.", "published": "2024-10-06 08:58:14", "link": "http://arxiv.org/abs/2410.04410v1", "categories": ["cs.CL", "I.7; I.2.7; E.1"], "primary_category": "cs.CL"}
{"title": "Long-context Language Models Are Not Good At ALL Retrieval Tasks Without\n  Sufficient Steps", "abstract": "Long-context language models (LCLMs), characterized by their extensive\ncontext window, are becoming popular. However, despite they are nearly perfect\nat standard long-context retrieval tasks, our evaluations demonstrate they are\nnot good at 2 basic cases, \"multi-matching retrieval,\" and \"logic-based\nretrieval\", which are beyond LCLMs' ability boundary. But we find they can be\nwell addressed with a sufficient number of reasoning steps, guided by specific\nCoT prompts, indicating the necessity of combining long-context tasks with CoT\nmethods for more advanced long context handling. However, current CoT methods\nare too time-consuming, when the context is very long, which means efficient\nlong-context handling still has a long way to go.", "published": "2024-10-06 09:29:19", "link": "http://arxiv.org/abs/2410.04422v8", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Inner-Probe: Discovering Copyright-related Data Generation in LLM\n  Architecture", "abstract": "Large Language Models (LLMs) utilize extensive knowledge databases and show\npowerful text generation ability. However, their reliance on high-quality\ncopyrighted datasets raises concerns about copyright infringements in generated\ntexts. Current research often employs prompt engineering or semantic\nclassifiers to identify copyrighted content, but these approaches have two\nsignificant limitations: (1) Challenging to identify which specific sub-dataset\n(e.g., works from particular authors) influences an LLM's output. (2) Treating\nthe entire training database as copyrighted, hence overlooking the inclusion of\nnon-copyrighted training data.\n  We propose InnerProbe, a lightweight framework designed to evaluate the\ninfluence of copyrighted sub-datasets on LLM-generated texts. Unlike\ntraditional methods relying solely on text, we discover that the results of\nmulti-head attention (MHA) during LLM output generation provide more effective\ninformation. Thus, InnerProbe performs sub-dataset contribution analysis using\na lightweight LSTM-based network trained on MHA results in a supervised manner.\nHarnessing such a prior, InnerProbe enables non-copyrighted text detection\nthrough a concatenated global projector trained with unsupervised contrastive\nlearning. InnerProbe demonstrates 3x improved efficiency compared to semantic\nmodel training in sub-dataset contribution analysis on Books3, achieves\n15.04%-58.7% higher accuracy over baselines on the Pile, and delivers a 0.104\nincrease in AUC for non-copyrighted data filtering.", "published": "2024-10-06 11:41:39", "link": "http://arxiv.org/abs/2410.04454v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SWEb: A Large Web Dataset for the Scandinavian Languages", "abstract": "This paper presents the hitherto largest pretraining dataset for the\nScandinavian languages: the Scandinavian WEb (SWEb), comprising over one\ntrillion tokens. The paper details the collection and processing pipeline, and\nintroduces a novel model-based text extractor that significantly reduces\ncomplexity in comparison with rule-based approaches. We also introduce a new\ncloze-style benchmark for evaluating language models in Swedish, and use this\ntest to compare models trained on the SWEb data to models trained on FineWeb,\nwith competitive results. All data, models and code are shared openly.", "published": "2024-10-06 11:55:15", "link": "http://arxiv.org/abs/2410.04456v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Wrong-of-Thought: An Integrated Reasoning Framework with\n  Multi-Perspective Verification and Wrong Information", "abstract": "Chain-of-Thought (CoT) has become a vital technique for enhancing the\nperformance of Large Language Models (LLMs), attracting increasing attention\nfrom researchers. One stream of approaches focuses on the iterative enhancement\nof LLMs by continuously verifying and refining their reasoning outputs for\ndesired quality. Despite its impressive results, this paradigm faces two\ncritical issues: (1) Simple verification methods: The current paradigm relies\nsolely on a single verification method. (2) Wrong Information Ignorance:\nTraditional paradigms directly ignore wrong information during reasoning and\nrefine the logic paths from scratch each time. To address these challenges, we\npropose Wrong-of-Thought (WoT), which includes two core modules: (1)\nMulti-Perspective Verification: A multi-perspective verification method for\naccurately refining the reasoning process and result, and (2) Wrong Information\nUtilization: Utilizing wrong information to alert LLMs and reduce the\nprobability of LLMs making same mistakes. Experiments on 8 popular datasets and\n5 LLMs demonstrate that WoT surpasses all previous baselines. In addition, WoT\nexhibits powerful capabilities in difficult computation tasks.", "published": "2024-10-06 12:27:21", "link": "http://arxiv.org/abs/2410.04463v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-Grained Prediction of Reading Comprehension from Eye Movements", "abstract": "Can human reading comprehension be assessed from eye movements in reading? In\nthis work, we address this longstanding question using large-scale eyetracking\ndata over textual materials that are geared towards behavioral analyses of\nreading comprehension. We focus on a fine-grained and largely unaddressed task\nof predicting reading comprehension from eye movements at the level of a single\nquestion over a passage. We tackle this task using three new multimodal\nlanguage models, as well as a battery of prior models from the literature. We\nevaluate the models' ability to generalize to new textual items, new\nparticipants, and the combination of both, in two different reading regimes,\nordinary reading and information seeking. The evaluations suggest that although\nthe task is highly challenging, eye movements contain useful signals for\nfine-grained prediction of reading comprehension. Code and data will be made\npublicly available.", "published": "2024-10-06 13:55:06", "link": "http://arxiv.org/abs/2410.04484v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ErrorRadar: Benchmarking Complex Mathematical Reasoning of Multimodal\n  Large Language Models Via Error Detection", "abstract": "As the field of Multimodal Large Language Models (MLLMs) continues to evolve,\ntheir potential to revolutionize artificial intelligence is particularly\npromising, especially in addressing mathematical reasoning tasks. Current\nmathematical benchmarks predominantly focus on evaluating MLLMs'\nproblem-solving ability, yet there is a crucial gap in addressing more complex\nscenarios such as error detection, for enhancing reasoning capability in\ncomplicated settings. To fill this gap, we formally formulate the new task:\nmultimodal error detection, and introduce ErrorRadar, the first benchmark\ndesigned to assess MLLMs' capabilities in such a task. ErrorRadar evaluates two\nsub-tasks: error step identification and error categorization, providing a\ncomprehensive framework for evaluating MLLMs' complex mathematical reasoning\nability. It consists of 2,500 high-quality multimodal K-12 mathematical\nproblems, collected from real-world student interactions in an educational\norganization, with rigorous annotation and rich metadata such as problem type\nand error category. Through extensive experiments, we evaluated both\nopen-source and closed-source representative MLLMs, benchmarking their\nperformance against educational expert evaluators. Results indicate significant\nchallenges still remain, as GPT-4o with best performance is still around 10%\nbehind human evaluation. The dataset will be available upon acceptance.", "published": "2024-10-06 14:59:09", "link": "http://arxiv.org/abs/2410.04509v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RevMUX: Data Multiplexing with Reversible Adapters for Efficient LLM\n  Batch Inference", "abstract": "Large language models (LLMs) have brought a great breakthrough to the natural\nlanguage processing (NLP) community, while leading the challenge of handling\nconcurrent customer queries due to their high throughput demands. Data\nmultiplexing addresses this by merging multiple inputs into a single composite\ninput, allowing more efficient inference through a shared forward pass.\nHowever, as distinguishing individuals from a composite input is challenging,\nconventional methods typically require training the entire backbone, yet still\nsuffer from performance degradation. In this paper, we introduce RevMUX, a\nparameter-efficient data multiplexing framework that incorporates a reversible\ndesign in the multiplexer, which can be reused by the demultiplexer to perform\nreverse operations and restore individual samples for classification. Extensive\nexperiments on four datasets and three types of LLM backbones demonstrate the\neffectiveness of RevMUX for enhancing LLM inference efficiency while retaining\na satisfactory classification performance.", "published": "2024-10-06 15:24:55", "link": "http://arxiv.org/abs/2410.04519v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Toward Secure Tuning: Mitigating Security Risks from Instruction\n  Fine-Tuning", "abstract": "Instruction fine-tuning has emerged as a critical technique for customizing\nLarge Language Models (LLMs) to specific applications. However, recent studies\nhave highlighted significant security vulnerabilities in fine-tuned LLMs.\nExisting defense efforts focus more on pre-training and post-training methods,\nyet there remains underexplored in in-training methods. To fill this gap, we\nintroduce a novel secure-tuning strategy called SWAT. By analyzing how\nmodule-level parameters (e.g. Q/K/V/O) affect the security feature space drift,\nwe identify a robust subset of modules, termed Mods_Rob. Our SWAT strategy\nbegins by warming up Mods_Rob to capture low-level features with minimal\nsecurity risks, followed by training all parameters to achieve optimal task\nperformance. Essentially, this strategy shifts the early learning burden more\nfrom global parameters to Mods_Rob, reducing update magnitudes of the\nnon-robust subset. Across various datasets, scenarios, and LLMs, our strategy\nhas demonstrated significant success in mitigating security risks while\npreserving task performance. Importantly, it can be seamlessly integrated with\npre-training and post-training methods, leading to greater improvements.", "published": "2024-10-06 15:34:04", "link": "http://arxiv.org/abs/2410.04524v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Casablanca: Data and Models for Multidialectal Arabic Speech Recognition", "abstract": "In spite of the recent progress in speech processing, the majority of world\nlanguages and dialects remain uncovered. This situation only furthers an\nalready wide technological divide, thereby hindering technological and\nsocioeconomic inclusion. This challenge is largely due to the absence of\ndatasets that can empower diverse speech systems. In this paper, we seek to\nmitigate this obstacle for a number of Arabic dialects by presenting\nCasablanca, a large-scale community-driven effort to collect and transcribe a\nmulti-dialectal Arabic dataset. The dataset covers eight dialects: Algerian,\nEgyptian, Emirati, Jordanian, Mauritanian, Moroccan, Palestinian, and Yemeni,\nand includes annotations for transcription, gender, dialect, and\ncode-switching. We also develop a number of strong baselines exploiting\nCasablanca. The project page for Casablanca is accessible at:\nwww.dlnlp.ai/speech/casablanca.", "published": "2024-10-06 15:41:38", "link": "http://arxiv.org/abs/2410.04527v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Does the Disclosure of AI Assistance Affect the Perceptions of\n  Writing?", "abstract": "Recent advances in generative AI technologies like large language models have\nboosted the incorporation of AI assistance in writing workflows, leading to the\nrise of a new paradigm of human-AI co-creation in writing. To understand how\npeople perceive writings that are produced under this paradigm, in this paper,\nwe conduct an experimental study to understand whether and how the disclosure\nof the level and type of AI assistance in the writing process would affect\npeople's perceptions of the writing on various aspects, including their\nevaluation on the quality of the writing and their ranking of different\nwritings. Our results suggest that disclosing the AI assistance in the writing\nprocess, especially if AI has provided assistance in generating new content,\ndecreases the average quality ratings for both argumentative essays and\ncreative stories. This decrease in the average quality ratings often comes with\nan increased level of variations in different individuals' quality evaluations\nof the same writing. Indeed, factors such as an individual's writing confidence\nand familiarity with AI writing assistants are shown to moderate the impact of\nAI assistance disclosure on their writing quality evaluations. We also find\nthat disclosing the use of AI assistance may significantly reduce the\nproportion of writings produced with AI's content generation assistance among\nthe top-ranked writings.", "published": "2024-10-06 16:45:33", "link": "http://arxiv.org/abs/2410.04545v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reasoning-Enhanced Healthcare Predictions with Knowledge Graph Community\n  Retrieval", "abstract": "Large language models (LLMs) have demonstrated significant potential in\nclinical decision support. Yet LLMs still suffer from hallucinations and lack\nfine-grained contextual medical knowledge, limiting their high-stake healthcare\napplications such as clinical diagnosis. Traditional retrieval-augmented\ngeneration (RAG) methods attempt to address these limitations but frequently\nretrieve sparse or irrelevant information, undermining prediction accuracy. We\nintroduce KARE, a novel framework that integrates knowledge graph (KG)\ncommunity-level retrieval with LLM reasoning to enhance healthcare predictions.\nKARE constructs a comprehensive multi-source KG by integrating biomedical\ndatabases, clinical literature, and LLM-generated insights, and organizes it\nusing hierarchical graph community detection and summarization for precise and\ncontextually relevant information retrieval. Our key innovations include: (1) a\ndense medical knowledge structuring approach enabling accurate retrieval of\nrelevant information; (2) a dynamic knowledge retrieval mechanism that enriches\npatient contexts with focused, multi-faceted medical insights; and (3) a\nreasoning-enhanced prediction framework that leverages these enriched contexts\nto produce both accurate and interpretable clinical predictions. Extensive\nexperiments demonstrate that KARE outperforms leading models by up to\n10.8-15.0% on MIMIC-III and 12.6-12.7% on MIMIC-IV for mortality and\nreadmission predictions. In addition to its impressive prediction accuracy, our\nframework leverages the reasoning capabilities of LLMs, enhancing the\ntrustworthiness of clinical predictions.", "published": "2024-10-06 18:46:28", "link": "http://arxiv.org/abs/2410.04585v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards the first UD Treebank of Spoken Italian: the KIParla forest", "abstract": "The present project endeavors to enrich the linguistic resources available\nfor Italian by constructing a Universal Dependencies treebank for the KIParla\ncorpus (Mauri et al., 2019, Ballar\\`e et al., 2020), an existing and well known\nresource for spoken Italian.", "published": "2024-10-06 19:01:04", "link": "http://arxiv.org/abs/2410.04589v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ProtocoLLM: Automatic Evaluation Framework of LLMs on Domain-Specific\n  Scientific Protocol Formulation Tasks", "abstract": "Automated generation of scientific protocols executable by robots can\nsignificantly accelerate scientific research processes. Large Language Models\n(LLMs) excel at Scientific Protocol Formulation Tasks (SPFT), but the\nevaluation of their capabilities rely on human evaluation. Here, we propose a\nflexible, automatic framework to evaluate LLM's capability on SPFT: ProtocoLLM.\nThis framework prompts the target model and GPT-4 to extract pseudocode from\nbiology protocols using only predefined lab actions and evaluates the output of\ntarget model using LLAM-EVAL, the pseudocode generated by GPT-4 serving as a\nbaseline and Llama-3 acting as the evaluator. Our adaptable prompt-based\nevaluation method, LLAM-EVAL, offers significant flexibility in terms of\nevaluation model, material, criteria, and is free of cost. We evaluate GPT\nvariations, Llama, Mixtral, Gemma, Cohere, and Gemini. Overall, we find that\nGPT and Cohere is a powerful scientific protocol formulators. We also introduce\nBIOPROT 2.0, a dataset with biology protocols and corresponding pseudocodes,\nwhich can aid LLMs in formulation and evaluation of SPFT. Our work is\nextensible to assess LLMs on SPFT across various domains and other fields that\nrequire protocol generation for specific goals.", "published": "2024-10-06 19:28:55", "link": "http://arxiv.org/abs/2410.04601v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can LLMs Improve Multimodal Fact-Checking by Asking Relevant Questions?", "abstract": "Traditional fact-checking relies on humans to formulate relevant and targeted\nfact-checking questions (FCQs), search for evidence, and verify the factuality\nof claims. While Large Language Models (LLMs) have been commonly used to\nautomate evidence retrieval and factuality verification at scale, their\neffectiveness for fact-checking is hindered by the absence of FCQ formulation.\nTo bridge this gap, we seek to answer two research questions: (1) Can LLMs\ngenerate relevant FCQs? (2) Can LLM-generated FCQs improve multimodal\nfact-checking? We therefore introduce a framework LRQ-FACT for using LLMs to\ngenerate relevant FCQs to facilitate evidence retrieval and enhance\nfact-checking by probing information across multiple modalities. Through\nextensive experiments, we verify if LRQ-FACT can generate relevant FCQs of\ndifferent types and if LRQ-FACT can consistently outperform baseline methods in\nmultimodal fact-checking. Further analysis illustrates how each component in\nLRQ-FACT works toward improving the fact-checking performance.", "published": "2024-10-06 20:33:22", "link": "http://arxiv.org/abs/2410.04616v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Punctuation Prediction for Polish Texts using Transformers", "abstract": "Speech recognition systems typically output text lacking punctuation.\nHowever, punctuation is crucial for written text comprehension. To tackle this\nproblem, Punctuation Prediction models are developed. This paper describes a\nsolution for Poleval 2022 Task 1: Punctuation Prediction for Polish Texts,\nwhich scores 71.44 Weighted F1. The method utilizes a single HerBERT model\nfinetuned to the competition data and an external dataset.", "published": "2024-10-06 20:51:02", "link": "http://arxiv.org/abs/2410.04621v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Control Large Language Models via Divide and Conquer", "abstract": "This paper investigates controllable generation for large language models\n(LLMs) with prompt-based control, focusing on Lexically Constrained Generation\n(LCG). We systematically evaluate the performance of LLMs on satisfying lexical\nconstraints with prompt-based control, as well as their efficacy in downstream\napplications. We conclude that LLMs face significant challenges in consistently\nsatisfying lexical constraints with prompt-based control. We identified three\nkey limitations of LLMs for LCG, including (1) position bias, where LLMs tend\nto satisfy constraints that appear in specific positions within the input; (2)\nlow responsiveness to decoding parameters, which render minimal impact on\ncontrol of LLMs; and (3) struggle with handling the inherent complexity of\ncertain constraints (e.g., compound words). To address these issues, we\nintroduce a Divide and Conquer Generation strategy, effective for both\nwhite-box and black-box LLMs, to enhance LLMs performance in LCG tasks, which\ndemonstrates over 90% improvement on success rate in the most challenging LCG\ntask. Our analysis provides valuable insights into the performance of LLMs in\nLCG with prompt-based control, and our proposed strategy offers a pathway to\nmore sophisticated and customized text generation applications.", "published": "2024-10-06 21:20:06", "link": "http://arxiv.org/abs/2410.04628v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Cross-Lingual Meta-Learning Method Based on Domain Adaptation for\n  Speech Emotion Recognition", "abstract": "Best-performing speech models are trained on large amounts of data in the\nlanguage they are meant to work for. However, most languages have sparse data,\nmaking training models challenging. This shortage of data is even more\nprevalent in speech emotion recognition. Our work explores the model's\nperformance in limited data, specifically for speech emotion recognition.\nMeta-learning specializes in improving the few-shot learning. As a result, we\nemploy meta-learning techniques on speech emotion recognition tasks, accent\nrecognition, and person identification. To this end, we propose a series of\nimprovements over the multistage meta-learning method. Unlike other works\nfocusing on smaller models due to the high computational cost of meta-learning\nalgorithms, we take a more practical approach. We incorporate a large\npre-trained backbone and a prototypical network, making our methods more\nfeasible and applicable. Our most notable contribution is an improved\nfine-tuning technique during meta-testing that significantly boosts the\nperformance on out-of-distribution datasets. This result, together with\nincremental improvements from several other works, helped us achieve accuracy\nscores of 83.78% and 56.30% for Greek and Romanian speech emotion recognition\ndatasets not included in the training or validation splits in the context of\n4-way 5-shot learning.", "published": "2024-10-06 21:33:51", "link": "http://arxiv.org/abs/2410.04633v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Calibrating Expressions of Certainty", "abstract": "We present a novel approach to calibrating linguistic expressions of\ncertainty, e.g., \"Maybe\" and \"Likely\". Unlike prior work that assigns a single\nscore to each certainty phrase, we model uncertainty as distributions over the\nsimplex to capture their semantics more accurately. To accommodate this new\nrepresentation of certainty, we generalize existing measures of miscalibration\nand introduce a novel post-hoc calibration method. Leveraging these tools, we\nanalyze the calibration of both humans (e.g., radiologists) and computational\nmodels (e.g., language models) and provide interpretable suggestions to improve\ntheir calibration.", "published": "2024-10-06 00:13:12", "link": "http://arxiv.org/abs/2410.04315v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Latent Feature Mining for Predictive Model Enhancement with Large\n  Language Models", "abstract": "Predictive modeling often faces challenges due to limited data availability\nand quality, especially in domains where collected features are weakly\ncorrelated with outcomes and where additional feature collection is constrained\nby ethical or practical difficulties. Traditional machine learning (ML) models\nstruggle to incorporate unobserved yet critical factors. In this work, we\nintroduce an effective approach to formulate latent feature mining as\ntext-to-text propositional logical reasoning. We propose FLAME (Faithful Latent\nFeature Mining for Predictive Model Enhancement), a framework that leverages\nlarge language models (LLMs) to augment observed features with latent features\nand enhance the predictive power of ML models in downstream tasks. Our\nframework is generalizable across various domains with necessary\ndomain-specific adaptation, as it is designed to incorporate contextual\ninformation unique to each area, ensuring effective transfer to different areas\nfacing similar data availability challenges. We validate our framework with two\ncase studies: (1) the criminal justice system, a domain characterized by\nlimited and ethically challenging data collection; (2) the healthcare domain,\nwhere patient privacy concerns and the complexity of medical data limit\ncomprehensive feature collection. Our results show that inferred latent\nfeatures align well with ground truth labels and significantly enhance the\ndownstream classifier.", "published": "2024-10-06 03:51:32", "link": "http://arxiv.org/abs/2410.04347v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "BrainCodec: Neural fMRI codec for the decoding of cognitive brain states", "abstract": "Recently, leveraging big data in deep learning has led to significant\nperformance improvements, as confirmed in applications like mental state\ndecoding using fMRI data. However, fMRI datasets remain relatively small in\nscale, and the inherent issue of low signal-to-noise ratios (SNR) in fMRI data\nfurther exacerbates these challenges. To address this, we apply compression\ntechniques as a preprocessing step for fMRI data. We propose BrainCodec, a\nnovel fMRI codec inspired by the neural audio codec. We evaluated BrainCodec's\ncompression capability in mental state decoding, demonstrating further\nimprovements over previous methods. Furthermore, we analyzed the latent\nrepresentations obtained through BrainCodec, elucidating the similarities and\ndifferences between task and resting state fMRI, highlighting the\ninterpretability of BrainCodec. Additionally, we demonstrated that fMRI\nreconstructions using BrainCodec can enhance the visibility of brain activity\nby achieving higher SNR, suggesting its potential as a novel denoising method.\nOur study shows that BrainCodec not only enhances performance over previous\nmethods but also offers new analytical possibilities for neuroscience. Our\ncodes, dataset, and model weights are available at\nhttps://github.com/amano-k-lab/BrainCodec.", "published": "2024-10-06 07:38:22", "link": "http://arxiv.org/abs/2410.04383v1", "categories": ["q-bio.NC", "cs.CL"], "primary_category": "q-bio.NC"}
{"title": "DAdEE: Unsupervised Domain Adaptation in Early Exit PLMs", "abstract": "Pre-trained Language Models (PLMs) exhibit good accuracy and generalization\nability across various tasks using self-supervision, but their large size\nresults in high inference latency. Early Exit (EE) strategies handle the issue\nby allowing the samples to exit from classifiers attached to the intermediary\nlayers, but they do not generalize well, as exit classifiers can be sensitive\nto domain changes. To address this, we propose Unsupervised Domain Adaptation\nin EE framework (DADEE) that employs multi-level adaptation using knowledge\ndistillation. DADEE utilizes GAN-based adversarial adaptation at each layer to\nachieve domain-invariant representations, reducing the domain gap between the\nsource and target domain across all layers. The attached exits not only speed\nup inference but also enhance domain adaptation by reducing catastrophic\nforgetting and mode collapse, making it more suitable for real-world scenarios.\nExperiments on tasks such as sentiment analysis, entailment classification, and\nnatural language inference demonstrate that DADEE consistently outperforms not\nonly early exit methods but also various domain adaptation methods under domain\nshift scenarios. The anonymized source code is available at\nhttps://github.com/Div290/DAdEE.", "published": "2024-10-06 09:44:58", "link": "http://arxiv.org/abs/2410.04424v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MindScope: Exploring cognitive biases in large language models through\n  Multi-Agent Systems", "abstract": "Detecting cognitive biases in large language models (LLMs) is a fascinating\ntask that aims to probe the existing cognitive biases within these models.\nCurrent methods for detecting cognitive biases in language models generally\nsuffer from incomplete detection capabilities and a restricted range of\ndetectable bias types. To address this issue, we introduced the 'MindScope'\ndataset, which distinctively integrates static and dynamic elements. The static\ncomponent comprises 5,170 open-ended questions spanning 72 cognitive bias\ncategories. The dynamic component leverages a rule-based, multi-agent\ncommunication framework to facilitate the generation of multi-round dialogues.\nThis framework is flexible and readily adaptable for various psychological\nexperiments involving LLMs. In addition, we introduce a multi-agent detection\nmethod applicable to a wide range of detection tasks, which integrates\nRetrieval-Augmented Generation (RAG), competitive debate, and a reinforcement\nlearning-based decision module. Demonstrating substantial effectiveness, this\nmethod has shown to improve detection accuracy by as much as 35.10% compared to\nGPT-4. Codes and appendix are available at\nhttps://github.com/2279072142/MindScope.", "published": "2024-10-06 11:23:56", "link": "http://arxiv.org/abs/2410.04452v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Collapsed Language Models Promote Fairness", "abstract": "To mitigate societal biases implicitly encoded in recent successful\npretrained language models, a diverse array of approaches have been proposed to\nencourage model fairness, focusing on prompting, data augmentation, regularized\nfine-tuning, and more. Despite the development, it is nontrivial to reach a\nprincipled understanding of fairness and an effective algorithm that can\nconsistently debias language models. In this work, by rigorous evaluations of\nNeural Collapse -- a learning phenomenon happen in last-layer representations\nand classifiers in deep networks -- on fairness-related words, we find that\ndebiased language models exhibit collapsed alignment between token\nrepresentations and word embeddings. More importantly, this observation\ninspires us to design a principled fine-tuning method that can effectively\nimprove fairness in a wide range of debiasing methods, while still preserving\nthe performance of language models on standard natural language understanding\ntasks. We attach our code at https://github.com/Xujxyang/Fairness-NC-main.", "published": "2024-10-06 13:09:48", "link": "http://arxiv.org/abs/2410.04472v3", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "A Pluggable Common Sense-Enhanced Framework for Knowledge Graph\n  Completion", "abstract": "Knowledge graph completion (KGC) tasks aim to infer missing facts in a\nknowledge graph (KG) for many knowledge-intensive applications. However,\nexisting embedding-based KGC approaches primarily rely on factual triples,\npotentially leading to outcomes inconsistent with common sense. Besides,\ngenerating explicit common sense is often impractical or costly for a KG. To\naddress these challenges, we propose a pluggable common sense-enhanced KGC\nframework that incorporates both fact and common sense for KGC. This framework\nis adaptable to different KGs based on their entity concept richness and has\nthe capability to automatically generate explicit or implicit common sense from\nfactual triples. Furthermore, we introduce common sense-guided negative\nsampling and a coarse-to-fine inference approach for KGs with rich entity\nconcepts. For KGs without concepts, we propose a dual scoring scheme involving\na relation-aware concept embedding mechanism. Importantly, our approach can be\nintegrated as a pluggable module for many knowledge graph embedding (KGE)\nmodels, facilitating joint common sense and fact-driven training and inference.\nThe experiments illustrate that our framework exhibits good scalability and\noutperforms existing models across various KGC tasks.", "published": "2024-10-06 14:06:12", "link": "http://arxiv.org/abs/2410.04488v1", "categories": ["cs.AI", "cs.CL", "I.2; I.2.4; I.2.7"], "primary_category": "cs.AI"}
{"title": "LRHP: Learning Representations for Human Preferences via Preference\n  Pairs", "abstract": "To improve human-preference alignment training, current research has\ndeveloped numerous preference datasets consisting of preference pairs labeled\nas \"preferred\" or \"dispreferred\". These preference pairs are typically used to\nencode human preferences into a single numerical value through reward modeling,\nwhich acts as a reward signal during reinforcement learning from human feedback\n(RLHF). However, representing these human preferences as a numerical value\ncomplicates the analysis of these preferences and restricts their broader\napplications other than RLHF. In contrast, in this work, we introduce a\npreference representation learning task that aims to construct a richer and\nmore structured representation of human preferences. We further develop a more\ngeneralizable framework, Learning Representations for Human Preferences via\npreference pairs (namely LRHP), which extends beyond traditional reward\nmodeling to tackle this task. We verify the utility of preference\nrepresentations in two downstream tasks: preference data selection and\npreference margin prediction. Building upon the human preferences in\nrepresentations, we achieve strong performance in both tasks, significantly\noutperforming baselines.", "published": "2024-10-06 14:48:28", "link": "http://arxiv.org/abs/2410.04503v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Realizing Video Summarization from the Path of Language-based Semantic\n  Understanding", "abstract": "The recent development of Video-based Large Language Models (VideoLLMs), has\nsignificantly advanced video summarization by aligning video features and, in\nsome cases, audio features with Large Language Models (LLMs). Each of these\nVideoLLMs possesses unique strengths and weaknesses. Many recent methods have\nrequired extensive fine-tuning to overcome the limitations of these models,\nwhich can be resource-intensive. In this work, we observe that the strengths of\none VideoLLM can complement the weaknesses of another. Leveraging this insight,\nwe propose a novel video summarization framework inspired by the Mixture of\nExperts (MoE) paradigm, which operates as an inference-time algorithm without\nrequiring any form of fine-tuning. Our approach integrates multiple VideoLLMs\nto generate comprehensive and coherent textual summaries. It effectively\ncombines visual and audio content, provides detailed background descriptions,\nand excels at identifying keyframes, which enables more semantically meaningful\nretrieval compared to traditional computer vision approaches that rely solely\non visual information, all without the need for additional fine-tuning.\nMoreover, the resulting summaries enhance performance in downstream tasks such\nas summary video generation, either through keyframe selection or in\ncombination with text-to-image models. Our language-driven approach offers a\nsemantically rich alternative to conventional methods and provides flexibility\nto incorporate newer VideoLLMs, enhancing adaptability and performance in video\nsummarization tasks.", "published": "2024-10-06 15:03:22", "link": "http://arxiv.org/abs/2410.04511v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "DAMRO: Dive into the Attention Mechanism of LVLM to Reduce Object\n  Hallucination", "abstract": "Despite the great success of Large Vision-Language Models (LVLMs), they\ninevitably suffer from hallucination. As we know, both the visual encoder and\nthe Large Language Model (LLM) decoder in LVLMs are Transformer-based, allowing\nthe model to extract visual information and generate text outputs via attention\nmechanisms. We find that the attention distribution of LLM decoder on image\ntokens is highly consistent with the visual encoder and both distributions tend\nto focus on particular background tokens rather than the referred objects in\nthe image. We attribute to the unexpected attention distribution to an inherent\nflaw in the visual encoder itself, which misguides LLMs to over emphasize the\nredundant information and generate object hallucination. To address the issue,\nwe propose DAMRO, a novel training-free strategy that $D$ive into $A$ttention\n$M$echanism of LVLM to $R$educe $O$bject Hallucination. Specifically, our\napproach employs classification token (CLS) of ViT to filter out high-attention\noutlier tokens scattered in the background and then eliminate their influence\nduring decoding stage. We evaluate our method on LVLMs including LLaVA-1.5,\nLLaVA-NeXT and InstructBLIP, using various benchmarks such as POPE, CHAIR, MME\nand GPT-4V Aided Evaluation. The results demonstrate that our approach\nsignificantly reduces the impact of these outlier tokens, thus effectively\nalleviating the hallucination of LVLMs. The code of our method will be released\nsoon.", "published": "2024-10-06 15:12:09", "link": "http://arxiv.org/abs/2410.04514v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "FAMMA: A Benchmark for Financial Domain Multilingual Multimodal Question\n  Answering", "abstract": "In this paper, we introduce FAMMA, an open-source benchmark for financial\nmultilingual multimodal question answering (QA). Our benchmark aims to evaluate\nthe abilities of multimodal large language models (MLLMs) in answering\nquestions that require advanced financial knowledge and sophisticated\nreasoning. It includes 1,758 meticulously collected question-answer pairs from\nuniversity textbooks and exams, spanning 8 major subfields in finance including\ncorporate finance, asset management, and financial engineering. Some of the QA\npairs are written in Chinese or French, while a majority of them are in\nEnglish. These questions are presented in a mixed format combining text and\nheterogeneous image types, such as charts, tables, and diagrams. We evaluate a\nrange of state-of-the-art MLLMs on our benchmark, and our analysis shows that\nFAMMA poses a significant challenge for these models. Even advanced systems\nlike GPT-4o and Claude-35-Sonnet achieve only 42\\% accuracy. Additionally, the\nopen-source Qwen2-VL lags notably behind its proprietary counterparts. Lastly,\nwe explore GPT o1-style reasoning chains to enhance the models' reasoning\ncapabilities, which significantly improve error correction. Our FAMMA benchmark\nwill facilitate future research to develop expert systems in financial QA. The\nleaderboard is available at https://famma-bench.github.io/famma/ .", "published": "2024-10-06 15:41:26", "link": "http://arxiv.org/abs/2410.04526v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluation of Code LLMs on Geospatial Code Generation", "abstract": "Software development support tools have been studied for a long time, with\nrecent approaches using Large Language Models (LLMs) for code generation. These\nmodels can generate Python code for data science and machine learning\napplications. LLMs are helpful for software engineers because they increase\nproductivity in daily work. An LLM can also serve as a \"mentor\" for\ninexperienced software developers, and be a viable learning support.\nHigh-quality code generation with LLMs can also be beneficial in geospatial\ndata science. However, this domain poses different challenges, and code\ngeneration LLMs are typically not evaluated on geospatial tasks. Here, we show\nhow we constructed an evaluation benchmark for code generation models, based on\na selection of geospatial tasks. We categorised geospatial tasks based on their\ncomplexity and required tools. Then, we created a dataset with tasks that test\nmodel capabilities in spatial reasoning, spatial data processing, and\ngeospatial tools usage. The dataset consists of specific coding problems that\nwere manually created for high quality. For every problem, we proposed a set of\ntest scenarios that make it possible to automatically check the generated code\nfor correctness. In addition, we tested a selection of existing code generation\nLLMs for code generation in the geospatial domain. We share our dataset and\nreproducible evaluation code on a public GitHub repository, arguing that this\ncan serve as an evaluation benchmark for new LLMs in the future. Our dataset\nwill hopefully contribute to the development new models capable of solving\ngeospatial coding tasks with high accuracy. These models will enable the\ncreation of coding assistants tailored for geospatial applications.", "published": "2024-10-06 20:34:03", "link": "http://arxiv.org/abs/2410.04617v2", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Passage Retrieval of Polish Texts Using OKAPI BM25 and an Ensemble of\n  Cross Encoders", "abstract": "Passage Retrieval has traditionally relied on lexical methods like TF-IDF and\nBM25. Recently, some neural network models have surpassed these methods in\nperformance. However, these models face challenges, such as the need for large\nannotated datasets and adapting to new domains. This paper presents a winning\nsolution to the Poleval 2023 Task 3: Passage Retrieval challenge, which\ninvolves retrieving passages of Polish texts in three domains: trivia, legal,\nand customer support. However, only the trivia domain was used for training and\ndevelopment data. The method used the OKAPI BM25 algorithm to retrieve\ndocuments and an ensemble of publicly available multilingual Cross Encoders for\nReranking. Fine-tuning the reranker models slightly improved performance but\nonly in the training domain, while it worsened in other domains.", "published": "2024-10-06 20:43:42", "link": "http://arxiv.org/abs/2410.04620v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SafeLLM: Domain-Specific Safety Monitoring for Large Language Models: A\n  Case Study of Offshore Wind Maintenance", "abstract": "The Offshore Wind (OSW) industry is experiencing significant expansion,\nresulting in increased Operations \\& Maintenance (O\\&M) costs. Intelligent\nalarm systems offer the prospect of swift detection of component failures and\nprocess anomalies, enabling timely and precise interventions that could yield\nreductions in resource expenditure, as well as scheduled and unscheduled\ndowntime. This paper introduces an innovative approach to tackle this challenge\nby capitalising on Large Language Models (LLMs). We present a specialised\nconversational agent that incorporates statistical techniques to calculate\ndistances between sentences for the detection and filtering of hallucinations\nand unsafe output. This potentially enables improved interpretation of alarm\nsequences and the generation of safer repair action recommendations by the\nagent. Preliminary findings are presented with the approach applied to\nChatGPT-4 generated test sentences. The limitation of using ChatGPT-4 and the\npotential for enhancement of this agent through re-training with specialised\nOSW datasets are discussed.", "published": "2024-10-06 13:00:53", "link": "http://arxiv.org/abs/2410.10852v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mitigating Hallucinations Using Ensemble of Knowledge Graph and Vector\n  Store in Large Language Models to Enhance Mental Health Support", "abstract": "This research work delves into the manifestation of hallucination within\nLarge Language Models (LLMs) and its consequential impacts on applications\nwithin the domain of mental health. The primary objective is to discern\neffective strategies for curtailing hallucinatory occurrences, thereby\nbolstering the dependability and security of LLMs in facilitating mental health\ninterventions such as therapy, counseling, and the dissemination of pertinent\ninformation. Through rigorous investigation and analysis, this study seeks to\nelucidate the underlying mechanisms precipitating hallucinations in LLMs and\nsubsequently propose targeted interventions to alleviate their occurrence. By\naddressing this critical issue, the research endeavors to foster a more robust\nframework for the utilization of LLMs within mental health contexts, ensuring\ntheir efficacy and reliability in aiding therapeutic processes and delivering\naccurate information to individuals seeking mental health support.", "published": "2024-10-06 14:26:37", "link": "http://arxiv.org/abs/2410.10853v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Plausibly Problematic Questions in Multiple-Choice Benchmarks for\n  Commonsense Reasoning", "abstract": "Questions involving commonsense reasoning about everyday situations often\nadmit many $\\textit{possible}$ or $\\textit{plausible}$ answers. In contrast,\nmultiple-choice question (MCQ) benchmarks for commonsense reasoning require a\nhard selection of a single correct answer, which, in principle, should\nrepresent the $\\textit{most}$ plausible answer choice. On $250$ MCQ items\nsampled from two commonsense reasoning benchmarks, we collect $5,000$\nindependent plausibility judgments on answer choices. We find that for over 20%\nof the sampled MCQs, the answer choice rated most plausible does not match the\nbenchmark gold answers; upon manual inspection, we confirm that this subset\nexhibits higher rates of problems like ambiguity or semantic mismatch between\nquestion and answer choices. Experiments with LLMs reveal low accuracy and high\nvariation in performance on the subset, suggesting our plausibility criterion\nmay be helpful in identifying more reliable benchmark items for commonsense\nevaluation.", "published": "2024-10-06 19:04:24", "link": "http://arxiv.org/abs/2410.10854v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Polymath: A Challenging Multi-modal Mathematical Reasoning Benchmark", "abstract": "Multi-modal Large Language Models (MLLMs) exhibit impressive problem-solving\nabilities in various domains, but their visual comprehension and abstract\nreasoning skills remain under-evaluated. To this end, we present PolyMATH, a\nchallenging benchmark aimed at evaluating the general cognitive reasoning\nabilities of MLLMs. PolyMATH comprises 5,000 manually collected high-quality\nimages of cognitive textual and visual challenges across 10 distinct\ncategories, including pattern recognition, spatial reasoning, and relative\nreasoning. We conducted a comprehensive, and quantitative evaluation of 15\nMLLMs using four diverse prompting strategies, including Chain-of-Thought and\nStep-Back. The best scores achieved on PolyMATH are ~41%, ~36%, and ~27%,\nobtained by Claude-3.5 Sonnet, GPT-4o and Gemini-1.5 Pro respectively -\nhighlighting the logical and visual complexity of these questions. A further\nfine-grained error analysis reveals that these models struggle to understand\nspatial relations and perform drawn-out, high-level reasoning. This is further\nstrengthened by our ablation study estimating MLLM performance when given\ntextual descriptions in place of diagrams. As evidenced by ~4% improvement over\ntextual descriptions as opposed to actual images, we discover that models do\nnot truly comprehend visual diagrams and the spatial information therein, and\nare thus prone to logical errors. Finally, we evaluate the OpenAI o1 models and\nfind that their performance only matches the human baseline, highlighting the\ndifficulty of the benchmark. The results on PolyMATH highlight the room for\nimprovement in multi-modal reasoning and provide unique insights to guide the\ndevelopment of future MLLMs.", "published": "2024-10-06 20:35:41", "link": "http://arxiv.org/abs/2410.14702v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Algorithmic Capabilities of Random Transformers", "abstract": "Trained transformer models have been found to implement interpretable\nprocedures for tasks like arithmetic and associative recall, but little is\nunderstood about how the circuits that implement these procedures originate\nduring training. To what extent do they depend on the supervisory signal\nprovided to models, and to what extent are they attributable to behavior\nalready present in models at the beginning of training? To investigate these\nquestions, we investigate what functions can be learned by randomly initialized\ntransformers in which only the embedding layers are optimized, so that the only\ninput--output mappings learnable from data are those already implemented (up to\na choice of encoding scheme) by the randomly initialized model. We find that\nthese random transformers can perform a wide range of meaningful algorithmic\ntasks, including modular arithmetic, in-weights and in-context associative\nrecall, decimal addition, parenthesis balancing, and even some aspects of\nnatural language text generation. Our results indicate that some algorithmic\ncapabilities are present in transformers (and accessible via appropriately\nstructured inputs) even before these models are trained. Code is available at\nhttps://github.com/fjzzq2002/random_transformers.", "published": "2024-10-06 06:04:23", "link": "http://arxiv.org/abs/2410.04368v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Graded Suspiciousness of Adversarial Texts to Human", "abstract": "Adversarial examples pose a significant challenge to deep neural networks\n(DNNs) across both image and text domains, with the intent to degrade model\nperformance through meticulously altered inputs. Adversarial texts, however,\nare distinct from adversarial images due to their requirement for semantic\nsimilarity and the discrete nature of the textual contents. This study delves\ninto the concept of human suspiciousness, a quality distinct from the\ntraditional focus on imperceptibility found in image-based adversarial\nexamples. Unlike images, where adversarial changes are meant to be\nindistinguishable to the human eye, textual adversarial content must often\nremain undetected or non-suspicious to human readers, even when the text's\npurpose is to deceive NLP systems or bypass filters.\n  In this research, we expand the study of human suspiciousness by analyzing\nhow individuals perceive adversarial texts. We gather and publish a novel\ndataset of Likert-scale human evaluations on the suspiciousness of adversarial\nsentences, crafted by four widely used adversarial attack methods and assess\ntheir correlation with the human ability to detect machine-generated\nalterations. Additionally, we develop a regression-based model to quantify\nsuspiciousness and establish a baseline for future research in reducing the\nsuspiciousness in adversarial text generation. We also demonstrate how the\nregressor-generated suspicious scores can be incorporated into adversarial\ngeneration methods to produce texts that are less likely to be perceived as\ncomputer-generated. We make our human suspiciousness annotated data and our\ncode available.", "published": "2024-10-06 06:57:22", "link": "http://arxiv.org/abs/2410.04377v2", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "CAPEEN: Image Captioning with Early Exits and Knowledge Distillation", "abstract": "Deep neural networks (DNNs) have made significant progress in recognizing\nvisual elements and generating descriptive text in image-captioning tasks.\nHowever, their improved performance comes from increased computational burden\nand inference latency. Early Exit (EE) strategies can be used to enhance their\nefficiency, but their adaptation presents challenges in image captioning as it\nrequires varying levels of semantic information for accurate predictions. To\novercome this, we introduce CAPEEN to improve the performance of EE strategies\nusing knowledge distillation. Inference in CAPEEN is completed at intermediary\nlayers if prediction confidence exceeds a predefined value learned from the\ntraining data. To account for real-world deployments, where target\ndistributions could drift from that of training samples, we introduce a variant\nA-CAPEEN to adapt the thresholds on the fly using Multiarmed bandits framework.\nExperiments on the MS COCO and Flickr30k datasets show that CAPEEN gains\nspeedup of 1.77x while maintaining competitive performance compared to the\nfinal layer, and A-CAPEEN additionally offers robustness against distortions.\nThe source code is available at https://github.com/Div290/CapEEN", "published": "2024-10-06 10:05:01", "link": "http://arxiv.org/abs/2410.04433v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Revisiting In-context Learning Inference Circuit in Large Language\n  Models", "abstract": "In-context Learning (ICL) is an emerging few-shot learning paradigm on\nLanguage Models (LMs) with inner mechanisms un-explored. There are already\nexisting works describing the inner processing of ICL, while they struggle to\ncapture all the inference phenomena in large language models. Therefore, this\npaper proposes a comprehensive circuit to model the inference dynamics and try\nto explain the observed phenomena of ICL. In detail, we divide ICL inference\ninto 3 major operations: (1) Input Text Encode: LMs encode every input text (in\nthe demonstrations and queries) into linear representation in the hidden states\nwith sufficient information to solve ICL tasks. (2) Semantics Merge: LMs merge\nthe encoded representations of demonstrations with their corresponding label\ntokens to produce joint representations of labels and demonstrations. (3)\nFeature Retrieval and Copy: LMs search the joint representations of\ndemonstrations similar to the query representation on a task subspace, and copy\nthe searched representations into the query. Then, language model heads capture\nthese copied label representations to a certain extent and decode them into\npredicted labels. Through careful measurements, the proposed inference circuit\nsuccessfully captures and unifies many fragmented phenomena observed during the\nICL process, making it a comprehensive and practical explanation of the ICL\ninference process. Moreover, ablation analysis by disabling the proposed steps\nseriously damages the ICL performance, suggesting the proposed inference\ncircuit is a dominating mechanism. Additionally, we confirm and list some\nbypass mechanisms that solve ICL tasks in parallel with the proposed circuit.", "published": "2024-10-06 12:50:15", "link": "http://arxiv.org/abs/2410.04468v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Configurable Multilingual ASR with Speech Summary Representations", "abstract": "Approximately half of the world's population is multilingual, making\nmultilingual ASR (MASR) essential. Deploying multiple monolingual models is\nchallenging when the ground-truth language is unknown in advance. This\nmotivates research efforts on configurable multilingual MASR models that can be\nprompted manually or adapted automatically to recognise specific languages. In\nthis paper, we present the Configurable MASR model with Summary Vector\n(csvMASR), a novel architecture designed to enhance configurability. Our\napproach leverages adapters and introduces speech summary vector\nrepresentations, inspired by conversational summary representations in speech\ndiarization, to combine outputs from language-specific components at the\nutterance level. We also incorporate an auxiliary language classification loss\nto enhance configurability. Using data from 7 languages in the Multilingual\nLibrispeech (MLS) dataset, csvMASR outperforms existing MASR models and reduces\nthe word error rate (WER) from 10.33\\% to 9.95\\% when compared with the\nbaseline. Additionally, csvMASR demonstrates superior performance in language\nclassification and prompting tasks.", "published": "2024-10-06 13:39:15", "link": "http://arxiv.org/abs/2410.04478v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Knowledge-Guided Dynamic Modality Attention Fusion Framework for\n  Multimodal Sentiment Analysis", "abstract": "Multimodal Sentiment Analysis (MSA) utilizes multimodal data to infer the\nusers' sentiment. Previous methods focus on equally treating the contribution\nof each modality or statically using text as the dominant modality to conduct\ninteraction, which neglects the situation where each modality may become\ndominant. In this paper, we propose a Knowledge-Guided Dynamic Modality\nAttention Fusion Framework (KuDA) for multimodal sentiment analysis. KuDA uses\nsentiment knowledge to guide the model dynamically selecting the dominant\nmodality and adjusting the contributions of each modality. In addition, with\nthe obtained multimodal representation, the model can further highlight the\ncontribution of dominant modality through the correlation evaluation loss.\nExtensive experiments on four MSA benchmark datasets indicate that KuDA\nachieves state-of-the-art performance and is able to adapt to different\nscenarios of dominant modality.", "published": "2024-10-06 14:10:28", "link": "http://arxiv.org/abs/2410.04491v1", "categories": ["cs.CL", "cs.AI", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Leveraging Large Language Models for Suicide Detection on Social Media\n  with Limited Labels", "abstract": "The increasing frequency of suicidal thoughts highlights the importance of\nearly detection and intervention. Social media platforms, where users often\nshare personal experiences and seek help, could be utilized to identify\nindividuals at risk. However, the large volume of daily posts makes manual\nreview impractical. This paper explores the use of Large Language Models (LLMs)\nto automatically detect suicidal content in text-based social media posts. We\npropose a novel method for generating pseudo-labels for unlabeled data by\nprompting LLMs, along with traditional classification fine-tuning techniques to\nenhance label accuracy. To create a strong suicide detection model, we develop\nan ensemble approach involving prompting with Qwen2-72B-Instruct, and using\nfine-tuned models such as Llama3-8B, Llama3.1-8B, and Gemma2-9B. We evaluate\nour approach on the dataset of the Suicide Ideation Detection on Social Media\nChallenge, a track of the IEEE Big Data 2024 Big Data Cup. Additionally, we\nconduct a comprehensive analysis to assess the impact of different models and\nfine-tuning strategies on detection performance. Experimental results show that\nthe ensemble model significantly improves the detection accuracy, by 5% points\ncompared with the individual models. It achieves a weight F1 score of 0.770 on\nthe public test set, and 0.731 on the private test set, providing a promising\nsolution for identifying suicidal content in social media. Our analysis shows\nthat the choice of LLMs affects the prompting performance, with larger models\nproviding better accuracy. Our code and checkpoints are publicly available at\nhttps://github.com/khanhvynguyen/Suicide_Detection_LLMs.", "published": "2024-10-06 14:45:01", "link": "http://arxiv.org/abs/2410.04501v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Upsample or Upweight? Balanced Training on Heavily Imbalanced Datasets", "abstract": "Data abundance across different domains exhibits a long-tailed distribution:\nfew domains have abundant data, while most face data scarcity. Our work focuses\non a multilingual setting, where available data is heavily skewed towards\nhigh-resource languages. Two common strategies to address this disparity are\nupsampling low-resource data (Temperature Sampling) and upweighting\nlow-resource loss (Scalarization). These methods are often assumed to be\nequivalent, but this equivalence has not been rigorously established, prompting\nour investigation.\n  Through theoretical and empirical analysis, we identify when these two\nmethods are equivalent and when they diverge. We prove that they are equivalent\nunder full gradient descent but differ under stochastic gradient descent due to\ndifferences in gradient variance. Specifically, Temperature Sampling exhibits\nlower variance in gradient estimation compared to Scalarization, leading to\nfaster convergence but a higher risk of overfitting. Based on these insights,\nwe propose Cooldown, a strategy that starts by heavily upsampling low-resource\nlanguages to accelerate convergence and gradually reduces the upsampling to\nprevent overfitting -- achieving the best of both worlds. Our method competes\neffectively with existing data re-weighting techniques while offering\ncomputational efficiency.", "published": "2024-10-06 18:29:46", "link": "http://arxiv.org/abs/2410.04579v5", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Regressing the Relative Future: Efficient Policy Optimization for\n  Multi-turn RLHF", "abstract": "Large Language Models (LLMs) have achieved remarkable success at tasks like\nsummarization that involve a single turn of interaction. However, they can\nstill struggle with multi-turn tasks like dialogue that require long-term\nplanning. Previous works on multi-turn dialogue extend single-turn\nreinforcement learning from human feedback (RLHF) methods to the multi-turn\nsetting by treating all prior dialogue turns as a long context. Such approaches\nsuffer from covariate shift: the conversations in the training set have\nprevious turns generated by some reference policy, which means that low\ntraining error may not necessarily correspond to good performance when the\nlearner is actually in the conversation loop. In response, we introduce\nREgressing the RELative FUture (REFUEL), an efficient policy optimization\napproach designed to address multi-turn RLHF in LLMs. REFUEL employs a single\nmodel to estimate $Q$-values and trains on self-generated data, addressing the\ncovariate shift issue. REFUEL frames the multi-turn RLHF problem as a sequence\nof regression tasks on iteratively collected datasets, enabling ease of\nimplementation. Theoretically, we prove that REFUEL can match the performance\nof any policy covered by the training set. Empirically, we evaluate our\nalgorithm by using Llama-3.1-70B-it to simulate a user in conversation with our\nmodel. REFUEL consistently outperforms state-of-the-art methods such as DPO and\nREBEL across various settings. Furthermore, despite having only 8 billion\nparameters, Llama-3-8B-it fine-tuned with REFUEL outperforms Llama-3.1-70B-it\non long multi-turn dialogues. Implementation of REFUEL can be found at\nhttps://github.com/ZhaolinGao/REFUEL/, and models trained by REFUEL can be\nfound at https://huggingface.co/Cornell-AGI.", "published": "2024-10-06 20:20:22", "link": "http://arxiv.org/abs/2410.04612v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Taylor Unswift: Secured Weight Release for Large Language Models via\n  Taylor Expansion", "abstract": "Ensuring the security of released large language models (LLMs) poses a\nsignificant dilemma, as existing mechanisms either compromise ownership rights\nor raise data privacy concerns. To address this dilemma, we introduce TaylorMLP\nto protect the ownership of released LLMs and prevent their abuse.\nSpecifically, TaylorMLP preserves the ownership of LLMs by transforming the\nweights of LLMs into parameters of Taylor-series. Instead of releasing the\noriginal weights, developers can release the Taylor-series parameters with\nusers, thereby ensuring the security of LLMs. Moreover, TaylorMLP can prevent\nabuse of LLMs by adjusting the generation speed. It can induce low-speed token\ngeneration for the protected LLMs by increasing the terms in the Taylor-series.\nThis intentional delay helps LLM developers prevent potential large-scale\nunauthorized uses of their models. Empirical experiments across five datasets\nand three LLM architectures demonstrate that TaylorMLP induces over 4x increase\nin latency, producing the tokens precisely matched with original LLMs.\nSubsequent defensive experiments further confirm that TaylorMLP effectively\nprevents users from reconstructing the weight values based on downstream\ndatasets.", "published": "2024-10-06 01:13:49", "link": "http://arxiv.org/abs/2410.05331v2", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Alignment Between the Decision-Making Logic of LLMs and Human Cognition:\n  A Case Study on Legal LLMs", "abstract": "This paper presents a method to evaluate the alignment between the\ndecision-making logic of Large Language Models (LLMs) and human cognition in a\ncase study on legal LLMs. Unlike traditional evaluations on language generation\nresults, we propose to evaluate the correctness of the detailed decision-making\nlogic of an LLM behind its seemingly correct outputs, which represents the core\nchallenge for an LLM to earn human trust. To this end, we quantify the\ninteractions encoded by the LLM as primitive decision-making logic, because\nrecent theoretical achievements have proven several mathematical guarantees of\nthe faithfulness of the interaction-based explanation. We design a set of\nmetrics to evaluate the detailed decision-making logic of LLMs. Experiments\nshow that even when the language generation results appear correct, a\nsignificant portion of the internal inference logic contains notable issues.", "published": "2024-10-06 08:33:39", "link": "http://arxiv.org/abs/2410.09083v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Diagnosing Robotics Systems Issues with Large Language Models", "abstract": "Quickly resolving issues reported in industrial applications is crucial to\nminimize economic impact. However, the required data analysis makes diagnosing\nthe underlying root causes a challenging and time-consuming task, even for\nexperts. In contrast, large language models (LLMs) excel at analyzing large\namounts of data. Indeed, prior work in AI-Ops demonstrates their effectiveness\nin analyzing IT systems. Here, we extend this work to the challenging and\nlargely unexplored domain of robotics systems. To this end, we create\nSYSDIAGBENCH, a proprietary system diagnostics benchmark for robotics,\ncontaining over 2500 reported issues. We leverage SYSDIAGBENCH to investigate\nthe performance of LLMs for root cause analysis, considering a range of model\nsizes and adaptation techniques. Our results show that QLoRA finetuning can be\nsufficient to let a 7B-parameter model outperform GPT-4 in terms of diagnostic\naccuracy while being significantly more cost-effective. We validate our\nLLM-as-a-judge results with a human expert study and find that our best model\nachieves similar approval ratings as our reference labels.", "published": "2024-10-06 11:58:12", "link": "http://arxiv.org/abs/2410.09084v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.RO"], "primary_category": "cs.CL"}
{"title": "Continuous Approximations for Improving Quantization Aware Training of\n  LLMs", "abstract": "Model compression methods are used to reduce the computation and energy\nrequirements for Large Language Models (LLMs). Quantization Aware Training\n(QAT), an effective model compression method, is proposed to reduce performance\ndegradation after quantization. To further minimize this degradation, we\nintroduce two continuous approximations to the QAT process on the rounding\nfunction, traditionally approximated by the Straight-Through Estimator (STE),\nand the clamping function. By applying both methods, the perplexity (PPL) on\nthe WikiText-v2 dataset of the quantized model reaches 9.0815, outperforming\n9.9621 by the baseline. Also, we achieve a 2.76% improvement on BoolQ, and a\n5.47% improvement on MMLU, proving that the step sizes and weights can be\nlearned more accurately with our approach. Our method achieves better\nperformance with the same precision, model size, and training setup,\ncontributing to the development of more energy-efficient LLMs technology that\naligns with global sustainability goals.", "published": "2024-10-06 04:33:06", "link": "http://arxiv.org/abs/2410.10849v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "On the Reliability of Large Language Models to Misinformed and\n  Demographically-Informed Prompts", "abstract": "We investigate and observe the behaviour and performance of Large Language\nModel (LLM)-backed chatbots in addressing misinformed prompts and questions\nwith demographic information within the domains of Climate Change and Mental\nHealth. Through a combination of quantitative and qualitative methods, we\nassess the chatbots' ability to discern the veracity of statements, their\nadherence to facts, and the presence of bias or misinformation in their\nresponses. Our quantitative analysis using True/False questions reveals that\nthese chatbots can be relied on to give the right answers to these close-ended\nquestions. However, the qualitative insights, gathered from domain experts,\nshows that there are still concerns regarding privacy, ethical implications,\nand the necessity for chatbots to direct users to professional services. We\nconclude that while these chatbots hold significant promise, their deployment\nin sensitive areas necessitates careful consideration, ethical oversight, and\nrigorous refinement to ensure they serve as a beneficial augmentation to human\nexpertise rather than an autonomous solution.", "published": "2024-10-06 07:40:11", "link": "http://arxiv.org/abs/2410.10850v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Core Knowledge Deficits in Multi-Modal Language Models", "abstract": "While Multimodal Large Language Models (MLLMs) demonstrate impressive\nabilities over high level perception and reasoning, their robustness in the\nwild still lags behind humans and exhibits diminished efficacy on simple tasks\nthat are intuitive for humans. We examine the hypothesis that these\ndeficiencies stem from the absence of core knowledge, rudimentary cognitive\nabilities innate to humans from early childhood. To probe core knowledge\nrepresentation in MLLMs, we draw from developmental cognitive sciences and\ndevelop a large-scale benchmark, CoreCognition dataset, encompassing 12 core\ncognitive concepts. We evaluate 219 models with 10 different prompts, leading\nto a total of 2409 data points for analysis. Our findings reveal core knowledge\ndeficits in early developed core abilities while models demonstrate human\ncomparable performance in high level cognition. Moreover, we find that low\nlevel abilities show little to no scaling, in stark contrast to high level\nabilities. Finally, we introduce an evaluation technique, Concept Hacking,\nthrough which we demonstrate that MLLMs do not genuinely advance toward core\nknowledge but instead rely on illusory understanding and shortcut learning as\nthey scale. Website with this\n$\\href{https://growing-ai-like-a-child.github.io/}{link}$.", "published": "2024-10-06 20:13:11", "link": "http://arxiv.org/abs/2410.10855v3", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "An evaluation of LLM code generation capabilities through graded\n  exercises", "abstract": "Large Language Models have shown prominent capabilities in generating\nfunctional code from natural language descriptions. However, a standardized way\nto evaluate these capabilities in an objective and unbiased manner is still to\nbe found. In this paper we review the current evaluation methods available to\nthis end, and run a new evaluation of the performance of one state-of-the-art\nmodel (GPT4-o-mini) in solving curated coding challenges in 8 programming\nlanguages, obtained from Codewars, a software development community. Our\nanalysis shows that the chance of success of the model has a positive\ncorrelation with the task difficulty, the popularity of the programming\nlanguage being used and the time elapsed since the publication of the\nchallenge. A further approximate explanatory analysis in terms of high-level\nfeatures hints that while 46.6% of the model performance could be attributed to\ntask difficulty, a 37.4% seems to be related to leakage of the challenge\nsolutions into the model training set, while the remaining 16% depends on the\nprogramming language. These results suggest that current evaluation\nmethodologies might be overestimating the actual skill of Large Language Models\nfor generating functional code.", "published": "2024-10-06 09:54:54", "link": "http://arxiv.org/abs/2410.16292v1", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "OD-Stega: LLM-Based Near-Imperceptible Steganography via Optimized\n  Distributions", "abstract": "We consider coverless steganography where a Large Language Model (LLM) drives\nan arithmetic coding decoder to generate stego-texts. An efficient method\nshould embed secret message bits in as few language tokens as possible, while\nstill keeping the stego-text natural and fluent. We show that on the individual\ntoken level, this problem is mathematically equivalent to maximizing the\nentropy of a replacement probability distribution of the next token generation,\nsubject to a constraint on the KL divergence between the chosen probability\ndistribution and the original distribution given by the LLM. A closed-form\nsolution is provided for the optimization problem, which can be computed\nefficiently. Several important practical issues are also tackled: 1) An\noften-overlooked tokenization mismatch issue is resolved with a simple prompt\nselection approach, 2) The combination of the optimized distribution and the\nvocabulary truncation technique is considered, and 3) The combination of the\noptimized distribution with other sequence-level selection heuristics to\nfurther enhance the efficiency and reliability is studied.", "published": "2024-10-06 01:30:45", "link": "http://arxiv.org/abs/2410.04328v1", "categories": ["cs.IT", "cs.AI", "cs.CL", "cs.CR", "cs.LG", "math.IT"], "primary_category": "cs.IT"}
{"title": "LLM Gesticulator: Leveraging Large Language Models for Scalable and\n  Controllable Co-Speech Gesture Synthesis", "abstract": "In this work, we present LLM Gesticulator, an LLM-based audio-driven\nco-speech gesture generation framework that synthesizes full-body animations\nthat are rhythmically aligned with the input audio while exhibiting natural\nmovements and editability. Compared to previous work, our model demonstrates\nsubstantial scalability. As the size of the backbone LLM model increases, our\nframework shows proportional improvements in evaluation metrics (a.k.a. scaling\nlaw). Our method also exhibits strong controllability where the content, style\nof the generated gestures can be controlled by text prompt. To the best of our\nknowledge, LLM gesticulator is the first work that use LLM on the co-speech\ngeneration task. Evaluation with existing objective metrics and user studies\nindicate that our framework outperforms prior works.", "published": "2024-10-06 12:53:07", "link": "http://arxiv.org/abs/2410.10851v2", "categories": ["cs.GR", "cs.AI", "cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.GR"}
{"title": "HALL-E: Hierarchical Neural Codec Language Model for Minute-Long\n  Zero-Shot Text-to-Speech Synthesis", "abstract": "Recently, Text-to-speech (TTS) models based on large language models (LLMs)\nthat translate natural language text into sequences of discrete audio tokens\nhave gained great research attention, with advances in neural audio codec (NAC)\nmodels using residual vector quantization (RVQ). However, long-form speech\nsynthesis remains a significant challenge due to the high frame rate, which\nincreases the length of audio tokens and makes it difficult for autoregressive\nlanguage models to generate audio tokens for even a minute of speech. To\naddress this challenge, this paper introduces two novel post-training\napproaches: 1) Multi-Resolution Requantization (MReQ) and 2) HALL-E. MReQ is a\nframework to reduce the frame rate of pre-trained NAC models. Specifically, it\nincorporates multi-resolution residual vector quantization (MRVQ) module that\nhierarchically reorganizes discrete audio tokens through teacher-student\ndistillation. HALL-E is an LLM-based TTS model designed to predict hierarchical\ntokens of MReQ. Specifically, it incorporates the technique of using MRVQ\nsub-modules and continues training from a pre-trained LLM-based TTS model.\nFurthermore, to promote TTS research, we create MinutesSpeech, a new benchmark\ndataset consisting of 40k hours of filtered speech data for training and\nevaluating speech synthesis ranging from 3s up to 180s. In experiments, we\ndemonstrated the effectiveness of our approaches by applying our post-training\nframework to VALL-E. We achieved the frame rate down to as low as 8 Hz,\nenabling the stable minitue-long speech synthesis in a single inference step.\nAudio samples, dataset, codes and pre-trained models are available at\nhttps://yutonishimura-v2.github.io/HALL-E_DEMO/.", "published": "2024-10-06 07:20:58", "link": "http://arxiv.org/abs/2410.04380v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Where are we in audio deepfake detection? A systematic analysis over\n  generative and detection models", "abstract": "Recent advances in Text-to-Speech (TTS) and Voice-Conversion (VC) using\ngenerative Artificial Intelligence (AI) technology have made it possible to\ngenerate high-quality and realistic human-like audio. This poses growing\nchallenges in distinguishing AI-synthesized speech from the genuine human voice\nand could raise concerns about misuse for impersonation, fraud, spreading\nmisinformation, and scams. However, existing detection methods for\nAI-synthesized audio have not kept pace and often fail to generalize across\ndiverse datasets. In this paper, we introduce SONAR, a synthetic AI-Audio\nDetection Framework and Benchmark, aiming to provide a comprehensive evaluation\nfor distinguishing cutting-edge AI-synthesized auditory content. SONAR includes\na novel evaluation dataset sourced from 9 diverse audio synthesis platforms,\nincluding leading TTS providers and state-of-the-art TTS models. It is the\nfirst framework to uniformly benchmark AI-audio detection across both\ntraditional and foundation model-based detection systems. Through extensive\nexperiments, (1) we reveal the limitations of existing detection methods and\ndemonstrate that foundation models exhibit stronger generalization\ncapabilities, likely due to their model size and the scale and quality of\npretraining data. (2) Speech foundation models demonstrate robust cross-lingual\ngeneralization capabilities, maintaining strong performance across diverse\nlanguages despite being fine-tuned solely on English speech data. This finding\nalso suggests that the primary challenges in audio deepfake detection are more\nclosely tied to the realism and quality of synthetic audio rather than\nlanguage-specific characteristics. (3) We explore the effectiveness and\nefficiency of few-shot fine-tuning in improving generalization, highlighting\nits potential for tailored applications, such as personalized detection systems\nfor specific entities or individuals.", "published": "2024-10-06 01:03:42", "link": "http://arxiv.org/abs/2410.04324v4", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "UniMuMo: Unified Text, Music and Motion Generation", "abstract": "We introduce UniMuMo, a unified multimodal model capable of taking arbitrary\ntext, music, and motion data as input conditions to generate outputs across all\nthree modalities. To address the lack of time-synchronized data, we align\nunpaired music and motion data based on rhythmic patterns to leverage existing\nlarge-scale music-only and motion-only datasets. By converting music, motion,\nand text into token-based representation, our model bridges these modalities\nthrough a unified encoder-decoder transformer architecture. To support multiple\ngeneration tasks within a single framework, we introduce several architectural\nimprovements. We propose encoding motion with a music codebook, mapping motion\ninto the same feature space as music. We introduce a music-motion parallel\ngeneration scheme that unifies all music and motion generation tasks into a\nsingle transformer decoder architecture with a single training task of\nmusic-motion joint generation. Moreover, the model is designed by fine-tuning\nexisting pre-trained single-modality models, significantly reducing\ncomputational demands. Extensive experiments demonstrate that UniMuMo achieves\ncompetitive results on all unidirectional generation benchmarks across music,\nmotion, and text modalities. Quantitative results are available in the\n\\href{https://hanyangclarence.github.io/unimumo_demo/}{project page}.", "published": "2024-10-06 16:04:05", "link": "http://arxiv.org/abs/2410.04534v1", "categories": ["cs.SD", "cs.CV", "cs.GR", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
