{"title": "MARBLE: A Hard Benchmark for Multimodal Spatial Reasoning and Planning", "abstract": "The ability to process information from multiple modalities and to reason\nthrough it step-by-step remains a critical challenge in advancing artificial\nintelligence. However, existing reasoning benchmarks focus on text-only\nreasoning, or employ multimodal questions that can be answered by directly\nretrieving information from a non-text modality. Thus, complex reasoning\nremains poorly understood in multimodal domains. Here, we present MARBLE, a\nchallenging multimodal reasoning benchmark that is designed to scrutinize\nmultimodal language models (MLLMs) in their ability to carefully reason\nstep-by-step through complex multimodal problems and environments. MARBLE is\ncomposed of two highly challenging tasks, M-Portal and M-Cube, that require the\ncrafting and understanding of multistep plans under spatial, visual, and\nphysical constraints. We find that current MLLMs perform poorly on MARBLE --\nall the 12 advanced models obtain near-random performance on M-Portal and 0%\naccuracy on M-Cube. Only in simplified subtasks some models outperform the\nrandom baseline, indicating that complex reasoning is still a challenge for\nexisting MLLMs. Moreover, we show that perception remains a bottleneck, where\nMLLMs occasionally fail to extract information from the visual inputs. By\nshedding a light on the limitations of MLLMs, we hope that MARBLE will spur the\ndevelopment of the next generation of models with the ability to reason and\nplan across many, multimodal reasoning steps.", "published": "2025-06-28 19:44:32", "link": "http://arxiv.org/abs/2506.22992v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "A Systematic Study of Compositional Syntactic Transformer Language Models", "abstract": "Syntactic language models (SLMs) enhance Transformers by incorporating\nsyntactic biases through the modeling of linearized syntactic parse trees\nalongside surface sentences. This paper focuses on compositional SLMs that are\nbased on constituency parse trees and contain explicit bottom-up composition of\nconstituent representations. We identify key aspects of design choices in\nexisting compositional SLMs and propose a unified framework encompassing both\nexisting models and novel variants. We conduct a comprehensive empirical\nevaluation of all the variants in our framework across language modeling,\nsyntactic generalization, summarization, dialogue, and inference efficiency.\nBased on the experimental results, we make multiple recommendations on the\ndesign of compositional SLMs. Our code is released at\nhttps://github.com/zhaoyd1/compositional_SLMs.", "published": "2025-06-28 18:32:23", "link": "http://arxiv.org/abs/2506.22978v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On the Generalizability of \"Competition of Mechanisms: Tracing How Language Models Handle Facts and Counterfactuals\"", "abstract": "We present a reproduction study of \"Competition of Mechanisms: Tracing How\nLanguage Models Handle Facts and Counterfactuals\" (Ortu et al., 2024), which\ninvestigates competition of mechanisms in language models between factual\nrecall and counterfactual in-context repetition. Our study successfully\nreproduces their primary findings regarding the localization of factual and\ncounterfactual information, the dominance of attention blocks in mechanism\ncompetition, and the specialization of attention heads in handling competing\ninformation. We reproduce their results on both GPT-2 (Radford et al., 2019)\nand Pythia 6.9B (Biderman et al., 2023). We extend their work in three\nsignificant directions. First, we explore the generalizability of these\nfindings to even larger models by replicating the experiments on Llama 3.1 8B\n(Grattafiori et al., 2024), discovering greatly reduced attention head\nspecialization. Second, we investigate the impact of prompt structure by\nintroducing variations where we avoid repeating the counterfactual statement\nverbatim or we change the premise word, observing a marked decrease in the\nlogit for the counterfactual token. Finally, we test the validity of the\nauthors' claims for prompts of specific domains, discovering that certain\ncategories of prompts skew the results by providing the factual prediction\ntoken as part of the subject of the sentence. Overall, we find that the\nattention head ablation proposed in Ortu et al. (2024) is ineffective for\ndomains that are underrepresented in their dataset, and that the effectiveness\nvaries based on model architecture, prompt structure, domain and task.", "published": "2025-06-28 18:29:19", "link": "http://arxiv.org/abs/2506.22977v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Agent-to-Agent Theory of Mind: Testing Interlocutor Awareness among Large Language Models", "abstract": "As large language models (LLMs) are increasingly integrated into multi-agent\nand human-AI systems, understanding their awareness of both self-context and\nconversational partners is essential for ensuring reliable performance and\nrobust safety. While prior work has extensively studied situational awareness\nwhich refers to an LLM's ability to recognize its operating phase and\nconstraints, it has largely overlooked the complementary capacity to identify\nand adapt to the identity and characteristics of a dialogue partner. In this\npaper, we formalize this latter capability as interlocutor awareness and\npresent the first systematic evaluation of its emergence in contemporary LLMs.\nWe examine interlocutor inference across three dimensions-reasoning patterns,\nlinguistic style, and alignment preferences-and show that LLMs reliably\nidentify same-family peers and certain prominent model families, such as GPT\nand Claude. To demonstrate its practical significance, we develop three case\nstudies in which interlocutor awareness both enhances multi-LLM collaboration\nthrough prompt adaptation and introduces new alignment and safety\nvulnerabilities, including reward-hacking behaviors and increased jailbreak\nsusceptibility. Our findings highlight the dual promise and peril of\nidentity-sensitive behavior in LLMs, underscoring the need for further\nunderstanding of interlocutor awareness and new safeguards in multi-agent\ndeployments. Our code is open-sourced at\nhttps://github.com/younwoochoi/InterlocutorAwarenessLLM.", "published": "2025-06-28 17:22:59", "link": "http://arxiv.org/abs/2506.22957v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.MA"], "primary_category": "cs.CL"}
{"title": "MOTOR: Multimodal Optimal Transport via Grounded Retrieval in Medical Visual Question Answering", "abstract": "Medical visual question answering (MedVQA) plays a vital role in clinical\ndecision-making by providing contextually rich answers to image-based queries.\nAlthough vision-language models (VLMs) are widely used for this task, they\noften generate factually incorrect answers. Retrieval-augmented generation\naddresses this challenge by providing information from external sources, but\nrisks retrieving irrelevant context, which can degrade the reasoning\ncapabilities of VLMs. Re-ranking retrievals, as introduced in existing\napproaches, enhances retrieval relevance by focusing on query-text alignment.\nHowever, these approaches neglect the visual or multimodal context, which is\nparticularly crucial for medical diagnosis. We propose MOTOR, a novel\nmultimodal retrieval and re-ranking approach that leverages grounded captions\nand optimal transport. It captures the underlying relationships between the\nquery and the retrieved context based on textual and visual information.\nConsequently, our approach identifies more clinically relevant contexts to\naugment the VLM input. Empirical analysis and human expert evaluation\ndemonstrate that MOTOR achieves higher accuracy on MedVQA datasets,\noutperforming state-of-the-art methods by an average of 6.45%. Code is\navailable at https://github.com/BioMedIA-MBZUAI/MOTOR.", "published": "2025-06-28 14:30:37", "link": "http://arxiv.org/abs/2506.22900v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Mask-aware Text-to-Image Retrieval: Referring Expression Segmentation Meets Cross-modal Retrieval", "abstract": "Text-to-image retrieval (TIR) aims to find relevant images based on a textual\nquery, but existing approaches are primarily based on whole-image captions and\nlack interpretability. Meanwhile, referring expression segmentation (RES)\nenables precise object localization based on natural language descriptions but\nis computationally expensive when applied across large image collections. To\nbridge this gap, we introduce Mask-aware TIR (MaTIR), a new task that unifies\nTIR and RES, requiring both efficient image search and accurate object\nsegmentation. To address this task, we propose a two-stage framework,\ncomprising a first stage for segmentation-aware image retrieval and a second\nstage for reranking and object grounding with a multimodal large language model\n(MLLM). We leverage SAM 2 to generate object masks and Alpha-CLIP to extract\nregion-level embeddings offline at first, enabling effective and scalable\nonline retrieval. Secondly, MLLM is used to refine retrieval rankings and\ngenerate bounding boxes, which are matched to segmentation masks. We evaluate\nour approach on COCO and D$^3$ datasets, demonstrating significant improvements\nin both retrieval accuracy and segmentation quality over previous methods.", "published": "2025-06-28 12:19:49", "link": "http://arxiv.org/abs/2506.22864v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Mind the Gap: Entity-Preserved Context-Aware ASR Structured Transcriptions", "abstract": "Automatic Speech Recognition (ASR) systems, such as Whisper, achieve high\ntranscription accuracy but struggle with named entities and numerical data,\nespecially when proper formatting is required. These issues increase word error\nrate (WER) and impair semantic understanding in critical domains like legal,\nfinancial, and medical applications. We propose a novel training approach that\nextends the semantic context of ASR models by adding overlapping context\nwindows during training. By sliding 5-second overlaps on both sides of\n30-second chunks, we create a 40-second \"effective semantic window,\" improving\nentity recognition and formatting while focusing predictions on the central 30\nseconds. To address entities spanning chunk boundaries, we reassign such\nentities entirely to the right-hand chunk, ensuring proper formatting.\nAdditionally, enriched training data with embedded entity labels enables the\nmodel to learn both recognition and type-specific formatting. Evaluated on the\nSpoken Wikipedia dataset, our method improves performance across semantic\ntasks, including named entity recognition (NER) and entity formatting. These\nresults highlight the effectiveness of context-aware training in addressing ASR\nlimitations for long-form transcription and complex entity recognition tasks.", "published": "2025-06-28 11:41:36", "link": "http://arxiv.org/abs/2506.22858v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "DICE-BENCH: Evaluating the Tool-Use Capabilities of Large Language Models in Multi-Round, Multi-Party Dialogues", "abstract": "Existing function-calling benchmarks focus on single-turn interactions.\nHowever, they overlook the complexity of real-world scenarios. To quantify how\nexisting benchmarks address practical applications, we introduce DICE-SCORE, a\nmetric that evaluates the dispersion of tool-related information such as\nfunction name and parameter values throughout the dialogue. Analyzing existing\nbenchmarks through DICE-SCORE reveals notably low scores, highlighting the need\nfor more realistic scenarios. To address this gap, we present DICE-BENCH, a\nframework that constructs practical function-calling datasets by synthesizing\nconversations through a tool graph that maintains dependencies across rounds\nand a multi-agent system with distinct personas to enhance dialogue\nnaturalness. The final dataset comprises 1,607 high-DICE-SCORE instances. Our\nexperiments on 19 LLMs with DICE-BENCH show that significant advances are still\nrequired before such models can be deployed effectively in real-world settings.\nOur code and data are all publicly available:\nhttps://snuhcc.github.io/DICE-Bench/.", "published": "2025-06-28 11:28:04", "link": "http://arxiv.org/abs/2506.22853v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Knowledge Augmented Finetuning Matters in both RAG and Agent Based Dialog Systems", "abstract": "Large language models (LLMs) have recently been applied to dialog systems.\nDespite making progress, LLMs are prone to errors in knowledge-intensive\nscenarios. Recently, approaches based on retrieval augmented generation (RAG)\nand agent have emerged to improve the factual accuracy by enhancing the LLMs\nwith knowledge retrieved from external knowledge bases (KBs). This is mostly\nimplemented by prompting the LLMs with instructions, examples and the retrieved\nknowledge. However, LLMs may have difficulty using the retrieved knowledge\neffectively for response generation, because they are not well trained to do\nsuch generation for specific domains. To mitigate this problem, we propose to\nfinetune the LLMs in the RAG-based and agent-based systems with domain-specific\ndata, together with domain-specific external knowledge, which is called\nknowledge augmented finetuning (KAFT). We base our study on the MobileCS2\ndataset, a real-life customer service dialog dataset that features intensive\nknowledge interactions, to systematically compare the prompting and KAFT\ntechniques in the RAG-based and agent-based systems. Experiment results show\nthat KAFT substantially surpasses prompting in both RAG and agent systems,\nparticularly in terms of factual accuracy. To the best of our knowledge, this\npaper represents the first solid empirical work to investigate the KAFT idea.", "published": "2025-06-28 11:26:31", "link": "http://arxiv.org/abs/2506.22852v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Boosting CTC-Based ASR Using LLM-Based Intermediate Loss Regularization", "abstract": "End-to-end (E2E) automatic speech recognition (ASR) systems have\nrevolutionized the field by integrating all components into a single neural\nnetwork, with attention-based encoder-decoder models achieving state-of-the-art\nperformance. However, their autoregressive decoding process limits inference\nspeed, making them unsuitable for real-time applications. In contrast,\nCTC-based models offer faster, non-autoregressive decoding but struggle to\nmodel linguistic dependencies effectively. Addressing this challenge, we\npropose a novel auxiliary loss framework called Language-Aware Intermediate\nLoss (LAIL) to enhance CTC-based ASR using the linguistic knowledge of large\nlanguage models (LLMs). By attaching connector layers to intermediate encoder\nlayers, LAIL maps outputs to the embedding space of an LLM and computes a\ncausal language modeling loss during training. This approach enhances\nlinguistic modeling while preserving the computational efficiency of CTC\ndecoding. Using the Conformer architecture and various LLaMA models, we\ndemonstrate significant improvements in Word Error Rate (WER) on the\nLibriSpeech, TEDLIUM2, and WSJ corpora, achieving state-of-the-art performance\nfor CTC-based ASR with minimal computational overhead.", "published": "2025-06-28 10:59:42", "link": "http://arxiv.org/abs/2506.22846v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Selecting and Merging: Towards Adaptable and Scalable Named Entity Recognition with Large Language Models", "abstract": "Supervised fine-tuning (SFT) is widely used to align large language models\n(LLMs) with information extraction (IE) tasks, such as named entity recognition\n(NER). However, annotating such fine-grained labels and training\ndomain-specific models is costly. Existing works typically train a unified\nmodel across multiple domains, but such approaches lack adaptation and\nscalability since not all training data benefits target domains and scaling\ntrained models remains challenging. We propose the SaM framework, which\ndynamically Selects and Merges expert models at inference time. Specifically,\nfor a target domain, we select domain-specific experts pre-trained on existing\ndomains based on (i) domain similarity to the target domain and (ii)\nperformance on sampled instances, respectively. The experts are then merged to\ncreate task-specific models optimized for the target domain. By dynamically\nmerging experts beneficial to target domains, we improve generalization across\nvarious domains without extra training. Additionally, experts can be added or\nremoved conveniently, leading to great scalability. Extensive experiments on\nmultiple benchmarks demonstrate our framework's effectiveness, which\noutperforms the unified model by an average of 10%. We further provide insights\ninto potential improvements, practical experience, and extensions of our\nframework.", "published": "2025-06-28 08:28:52", "link": "http://arxiv.org/abs/2506.22813v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BayesLoRA: Task-Specific Uncertainty in Low-Rank Adapters", "abstract": "We propose BayesLoRA, a task-specific uncertainty quantification framework\nthat integrates MC-Dropout into Low-Rank Adapters (LoRA). Unlike\ngeneral-purpose transformer uncertainty methods, BayesLoRA provides guardrails\ntailored to downstream workflows, enabling agents to introspect and modulate\nbehavior under uncertainty. We demonstrate mathematically and empirically that\nLoRA adapters exhibit amplified variance outside fine-tuning distributions,\nyielding reliable confidence estimates for agentic decision-making.", "published": "2025-06-28 08:22:02", "link": "http://arxiv.org/abs/2506.22809v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "MedEthicsQA: A Comprehensive Question Answering Benchmark for Medical Ethics Evaluation of LLMs", "abstract": "While Medical Large Language Models (MedLLMs) have demonstrated remarkable\npotential in clinical tasks, their ethical safety remains insufficiently\nexplored. This paper introduces $\\textbf{MedEthicsQA}$, a comprehensive\nbenchmark comprising $\\textbf{5,623}$ multiple-choice questions and\n$\\textbf{5,351}$ open-ended questions for evaluation of medical ethics in LLMs.\nWe systematically establish a hierarchical taxonomy integrating global medical\nethical standards. The benchmark encompasses widely used medical datasets,\nauthoritative question banks, and scenarios derived from PubMed literature.\nRigorous quality control involving multi-stage filtering and multi-faceted\nexpert validation ensures the reliability of the dataset with a low error rate\n($2.72\\%$). Evaluation of state-of-the-art MedLLMs exhibit declined performance\nin answering medical ethics questions compared to their foundation\ncounterparts, elucidating the deficiencies of medical ethics alignment. The\ndataset, registered under CC BY-NC 4.0 license, is available at\nhttps://github.com/JianhuiWei7/MedEthicsQA.", "published": "2025-06-28 08:21:35", "link": "http://arxiv.org/abs/2506.22808v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in Large Language Models", "abstract": "Semantic caching significantly reduces computational costs and improves\nefficiency by storing and reusing large language model (LLM) responses.\nHowever, existing systems rely primarily on matching individual queries,\nlacking awareness of multi-turn dialogue contexts, which leads to incorrect\ncache hits when similar queries appear in different conversational settings.\nThis demonstration introduces ContextCache, a context-aware semantic caching\nsystem for multi-turn dialogues. ContextCache employs a two-stage retrieval\narchitecture that first executes vector-based retrieval on the current query to\nidentify potential matches and then integrates current and historical dialogue\nrepresentations through self-attention mechanisms for precise contextual\nmatching. Evaluation of real-world conversations shows that ContextCache\nimproves precision and recall compared to existing methods. Additionally,\ncached responses exhibit approximately 10 times lower latency than direct LLM\ninvocation, enabling significant computational cost reductions for LLM\nconversational applications.", "published": "2025-06-28 07:25:12", "link": "http://arxiv.org/abs/2506.22791v1", "categories": ["cs.CL", "cs.DB"], "primary_category": "cs.CL"}
{"title": "PhonemeFake: Redefining Deepfake Realism with Language-Driven Segmental Manipulation and Adaptive Bilevel Detection", "abstract": "Deepfake (DF) attacks pose a growing threat as generative models become\nincreasingly advanced. However, our study reveals that existing DF datasets\nfail to deceive human perception, unlike real DF attacks that influence public\ndiscourse. It highlights the need for more realistic DF attack vectors. We\nintroduce PhonemeFake (PF), a DF attack that manipulates critical speech\nsegments using language reasoning, significantly reducing human perception by\nup to 42% and benchmark accuracies by up to 94%. We release an easy-to-use PF\ndataset on HuggingFace and open-source bilevel DF segment detection model that\nadaptively prioritizes compute on manipulated regions. Our extensive\nexperiments across three known DF datasets reveal that our detection model\nreduces EER by 91% while achieving up to 90% speed-up, with minimal compute\noverhead and precise localization beyond existing models as a scalable\nsolution.", "published": "2025-06-28 06:56:41", "link": "http://arxiv.org/abs/2506.22783v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Teaching Models to Verbalize Reward Hacking in Chain-of-Thought Reasoning", "abstract": "Language models trained with RL can engage in reward hacking--exploiting\nunintended strategies for high reward--without revealing this behavior in their\nchain-of-thought reasoning, making detection difficult and posing risks for\nhigh-stakes applications. We propose verbalization fine-tuning (VFT), a pre-RL\nintervention that trains models to explicitly acknowledge when they are\ninfluenced by prompt cues--hints which point to incorrect answers (e.g., \"a\nStanford professor thinks the answer is A\"). To evaluate VFT, we subsequently\ntrain models with RL on environments where held-out prompt cues signal which\nincorrect answers will receive high reward, incentivizing models to reward hack\nby exploiting cues instead of reasoning correctly. We measure how often models\nexploit these cues without verbalizing it. After RL, only 6% of the VFT-trained\nmodel's responses consist of undetected reward hacks. In comparison, when we\nperform RL without VFT, the rate of undetected reward hacks goes up to 88%;\nwith a debiasing baseline intervention, this increases further to 99%. VFT\nachieves this by substantially increasing how often models verbalize the\ninfluence of cues--from 8% to 42% after VFT, and up to 94% after RL--while\nbaselines remain low even after RL (10% and 1%). Our results show that teaching\nmodels to explicitly verbalize reward hacking behavior before RL significantly\nimproves their detection, offering a practical path toward more transparent and\nsafe AI systems.", "published": "2025-06-28 06:37:10", "link": "http://arxiv.org/abs/2506.22777v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Jan-nano Technical Report", "abstract": "Most language models face a fundamental tradeoff where powerful capabilities\nrequire substantial computational resources. We shatter this constraint with\nJan-nano, a 4B parameter language model that redefines efficiency through\nradical specialization: instead of trying to know everything, it masters the\nart of finding anything instantly. Fine-tuned from Qwen3-4B using our novel\nmulti-stage RLVR system that completely eliminates reliance on next token\nprediction training (SFT), Jan-nano achieves 83.2% on SimpleQA benchmark with\nMCP integration while running on consumer hardware. With 128K context length,\nJan-nano proves that intelligence isn't about scale, it's about strategy.", "published": "2025-06-28 05:44:57", "link": "http://arxiv.org/abs/2506.22760v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Translation Barrier Hypothesis: Multilingual Generation with Large Language Models Suffers from Implicit Translation Failure", "abstract": "Multilingual generation with large language models (LLMs) is often of poor\nquality for mid- to low-resource languages. Building on insights from\ninterpretability, we demonstrate the existence of an implicit\ntask-solving-->translation pipeline for generation, whereby the model first\nsolves the required task in a largely target-language-agnostic manner, and\nsubsequently translates answer concepts into the intended target language. We\nhypothesize that the failure of the translation stage is an important culprit\nfor the observed low quality of final outputs, and formalize this as the\ntranslation barrier hypothesis. We test this hypothesis for a word translation\ntask across 108 language pairs, using logit lens to observe model processing in\nintermediate layers. We find that a significant portion of overall failures\nindeed stems from translation failure, or the model's inability to translate\ncorrectly solved intermediate concepts into the target language. This is\nespecially true for low-resource target languages. Our results highlight an\nimportant hurdle for end-to-end multilingual generation, and lend guiding\ninsights for future work seeking to improve multilinguality in LLMs.", "published": "2025-06-28 02:09:21", "link": "http://arxiv.org/abs/2506.22724v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BEST-Route: Adaptive LLM Routing with Test-Time Optimal Compute", "abstract": "Large language models (LLMs) are powerful tools but are often expensive to\ndeploy at scale. LLM query routing mitigates this by dynamically assigning\nqueries to models of varying cost and quality to obtain a desired trade-off.\nPrior query routing approaches generate only one response from the selected\nmodel and a single response from a small (inexpensive) model was often not good\nenough to beat a response from a large (expensive) model due to which they end\nup overusing the large model and missing out on potential cost savings.\nHowever, it is well known that for small models, generating multiple responses\nand selecting the best can enhance quality while remaining cheaper than a\nsingle large-model response. We leverage this idea to propose BEST-Route, a\nnovel routing framework that chooses a model and the number of responses to\nsample from it based on query difficulty and the quality thresholds.\nExperiments on real-world datasets demonstrate that our method reduces costs by\nup to 60% with less than 1% performance drop.", "published": "2025-06-28 01:52:50", "link": "http://arxiv.org/abs/2506.22716v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DB"], "primary_category": "cs.LG"}
{"title": "Text Production and Comprehension by Human and Artificial Intelligence: Interdisciplinary Workshop Report", "abstract": "This report synthesizes the outcomes of a recent interdisciplinary workshop\nthat brought together leading experts in cognitive psychology, language\nlearning, and artificial intelligence (AI)-based natural language processing\n(NLP). The workshop, funded by the National Science Foundation, aimed to\naddress a critical knowledge gap in our understanding of the relationship\nbetween AI language models and human cognitive processes in text comprehension\nand composition. Through collaborative dialogue across cognitive, linguistic,\nand technological perspectives, workshop participants examined the underlying\nprocesses involved when humans produce and comprehend text, and how AI can both\ninform our understanding of these processes and augment human capabilities. The\nworkshop revealed emerging patterns in the relationship between large language\nmodels (LLMs) and human cognition, with highlights on both the capabilities of\nLLMs and their limitations in fully replicating human-like language\nunderstanding and generation. Key findings include the potential of LLMs to\noffer insights into human language processing, the increasing alignment between\nLLM behavior and human language processing when models are fine-tuned with\nhuman feedback, and the opportunities and challenges presented by human-AI\ncollaboration in language tasks. By synthesizing these findings, this report\naims to guide future research, development, and implementation of LLMs in\ncognitive psychology, linguistics, and education. It emphasizes the importance\nof ethical considerations and responsible use of AI technologies while striving\nto enhance human capabilities in text comprehension and production through\neffective human-AI collaboration.", "published": "2025-06-28 00:31:14", "link": "http://arxiv.org/abs/2506.22698v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Residual Matrix Transformers: Scaling the Size of the Residual Stream", "abstract": "The residual stream acts as a memory bus where transformer layers both store\nand access features (Elhage et al., 2021). We consider changing the mechanism\nfor retrieving and storing information in the residual stream, and replace the\nresidual stream of the transformer with an outer product memory matrix\n(Kohonen, 1972, Anderson, 1972). We call this model the Residual Matrix\nTransformer (RMT). We find that the RMT enjoys a number of attractive\nproperties: 1) the size of the residual stream can be scaled independently of\ncompute and model size, improving performance, 2) the RMT can achieve the same\nloss as the transformer with 58% fewer FLOPS, 25% fewer parameters, and 41%\nfewer training tokens tokens, and 3) the RMT outperforms the transformer on\ndownstream evaluations. We theoretically analyze the transformer and the RMT,\nand show that the RMT allows for more efficient scaling of the residual stream,\nas well as improved variance propagation properties. Code for this project can\nbe found at https://github.com/bmac3/residual-matrix-transformer.", "published": "2025-06-28 00:29:42", "link": "http://arxiv.org/abs/2506.22696v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "VOCABTRIM: Vocabulary Pruning for Efficient Speculative Decoding in LLMs", "abstract": "In this paper, we introduce a simple training-free technique to improve the\nperformance of drafter-based speculative decoding (SpD) methods that\nincorporates language modeling head (LM head) during drafting process. A\ndrafter-based speculative decoding leverages one or more smaller language\nmodels, a.k.a. drafters or draft models, to sample a draft sequence or tree\nconsisting of multiple tokens, followed by verification by a base LLM, a target\nmodel, accepting a subset as its valid generation. As it is usually considered\nthat the speculative decoding requires one-to-one mapping between vocabularies\nof the target model and the draft model, it has been natural to share the\nvocabulary between them, or even share the LM head as in EAGLE or Medusa. We\nfirst identify that this draft token sampling scheme inherently contains an\nunnecessary inference overhead in drafting, especially for some target LLMs\nwith very large vocabularies. Then, we propose a simple technique, VocabTrim,\nto mitigate the drafting overhead to improve the generation speed in\nmemory-bound environment. VocabTrim reconstructs the drafter LM head to contain\nonly a limited set of tokens, selected by the most frequently sampled from the\nvocabulary of the target model. While limiting the vocabulary in drafting\nslightly degrades the acceptance rate, it significantly reduces the drafting\nlatency in memory-bound process which is often the case on edge devices,\nresulting in higher memory-bound speed up (MBSU). We show that our method can\nboost the memory-bound speed-up for Llama-3 models on Spec-Bench, specifically\nby 16% for Llama-3.2-3B-Instruct.", "published": "2025-06-28 00:26:40", "link": "http://arxiv.org/abs/2506.22694v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Catching Rats in $H$-minor-free Graphs", "abstract": "We show that every $H$-minor-free graph that also excludes a $(k \\times\nk)$-grid as a minor has treewidth/branchwidth bounded from above by a function\n$f(t,k)$ that is linear in $k$ and polynomial in $t := |V(H)|$. Such a result\nwas proven originally by [Demaine & Hajiaghayi, Combinatorica, 2008], where $f$\nwas indeed linear in $k$. However the dependency in $t$ in this result was\nnon-explicit (and huge). Later, [Kawarabayashi & Kobayashi, JCTB, 2020] showed\nthat this bound can be estimated to be $f(t,k)\\in 2^{\\mathcal{O}(t\\log t)}\n\\cdot k$. Wood recently asked whether $f$ can be pushed further to be\npolynomial, while maintaining the linearity on $k$. We answer this in a\nparticularly strong sense, by showing that the treewidth/branchwidth of $G$ is\nin $\\mathcal{O}(gk + t^{2304}),$ where $g$ is the Euler genus of $H$. This\ndirectly yields $f(t,k)= \\mathcal{O}(t^2k + t^{2304})$.\n  Our methods build on techniques for branchwidth and on new bounds and\ninsights for the Graph Minor Structure Theorem (GMST) due to [Gorsky, Seweryn &\nWiederrecht, 2025, arXiv:2504.02532]. In particular, we prove a variant of the\nGMST that ensures some helpful properties for the minor relation. We further\nemploy our methods to provide approximation algorithms for the\ntreewidth/branchwidth of $H$-minor-free graphs. In particular, for every\n$\\varepsilon > 0$ and every $t$-vertex graph $H$ with Euler genus $g$, we give\na $(g + \\varepsilon)$-approximation algorithm for the branchwidth of\n$H$-minor-free graphs running in $2^{\\mathsf{poly}(t) / \\varepsilon} \\cdot\n\\mathsf{poly}(n)$-time. Our algorithms explicitly return either an appropriate\nbranch-decomposition or a grid-minor certifying a negative answer.", "published": "2025-06-28 11:41:09", "link": "http://arxiv.org/abs/2506.22857v1", "categories": ["math.CO", "cs.DM", "05C83, 05C85, 05C10, 68R10, 68R05", "G.2.2"], "primary_category": "math.CO"}
{"title": "Machine Assistant with Reliable Knowledge: Enhancing Student Learning via RAG-based Retrieval", "abstract": "We present Machine Assistant with Reliable Knowledge (MARK), a\nretrieval-augmented question-answering system designed to support student\nlearning through accurate and contextually grounded responses. The system is\nbuilt on a retrieval-augmented generation (RAG) framework, which integrates a\ncurated knowledge base to ensure factual consistency. To enhance retrieval\neffectiveness across diverse question types, we implement a hybrid search\nstrategy that combines dense vector similarity with sparse keyword-based\nretrieval. This dual-retrieval mechanism improves robustness for both general\nand domain-specific queries. The system includes a feedback loop in which\nstudents can rate responses and instructors can review and revise them.\nInstructor corrections are incorporated into the retrieval corpus, enabling\nadaptive refinement over time. The system was deployed in a classroom setting\nas a substitute for traditional office hours, where it successfully addressed a\nbroad range of student queries. It was also used to provide technical support\nby integrating with a customer-specific knowledge base, demonstrating its\nability to handle routine, context-sensitive tasks in applied domains. MARK is\npublicly accessible at https://app.eduquery.ai.", "published": "2025-06-28 22:17:27", "link": "http://arxiv.org/abs/2506.23026v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "On Universality of Non-Separable Approximate Message Passing Algorithms", "abstract": "Mean-field characterizations of first-order iterative algorithms -- including\nApproximate Message Passing (AMP), stochastic and proximal gradient descent,\nand Langevin diffusions -- have enabled a precise understanding of learning\ndynamics in many statistical applications. For algorithms whose non-linearities\nhave a coordinate-separable form, it is known that such characterizations enjoy\na degree of universality with respect to the underlying data distribution.\nHowever, mean-field characterizations of non-separable algorithm dynamics have\nlargely remained restricted to i.i.d. Gaussian or rotationally-invariant data.\n  In this work, we initiate a study of universality for non-separable AMP\nalgorithms. We identify a general condition for AMP with polynomial\nnon-linearities, in terms of a Bounded Composition Property (BCP) for their\nrepresenting tensors, to admit a state evolution that holds universally for\nmatrices with non-Gaussian entries. We then formalize a condition of\nBCP-approximability for Lipschitz AMP algorithms to enjoy a similar universal\nguarantee. We demonstrate that many common classes of non-separable\nnon-linearities are BCP-approximable, including local denoisers, spectral\ndenoisers for generic signals, and compositions of separable functions with\ngeneric linear maps, implying the universality of state evolution for AMP\nalgorithms employing these non-linearities.", "published": "2025-06-28 20:48:56", "link": "http://arxiv.org/abs/2506.23010v1", "categories": ["math.ST", "cs.IT", "cs.LG", "math.IT", "math.PR", "stat.TH"], "primary_category": "math.ST"}
{"title": "Communication via Sensing", "abstract": "We present an alternative take on the recently popularized concept of\n`$\\textit{joint sensing and communications}$', which focuses on using\ncommunication resources also for sensing. Here, we propose the opposite, where\nwe exploit the sensing capabilities of the receiver for communication. Our goal\nis to characterize the fundamental limits of communication over such a channel,\nwhich we call `$\\textit{communication via sensing}$'. We assume that changes in\nthe sensed attributes, e.g., location, speed, etc., are limited due to\npractical constraints, which are captured by assuming a finite-state channel\n(FSC) with an input cost constraint. We first formulate an upper bound on the\n$N$-letter capacity as a cost-constrained optimization problem over the input\nsequence distribution, and then convert it to an equivalent problem over the\nstate sequence distribution. Moreover, by breaking a walk on the underlying\nMarkov chain into a weighted sum of traversed graph cycles in the long walk\nlimit, we obtain a compact single-letter formulation of the capacity upper\nbound. Finally, for a specific case of a two-state FSC with noisy sensing\ncharacterized by a binary symmetric channel (BSC), we obtain a closed-form\nexpression for the capacity upper bound. Comparison with an existing numerical\nlower bound shows that our proposed upper bound is very tight for all crossover\nprobabilities.", "published": "2025-06-28 20:25:56", "link": "http://arxiv.org/abs/2506.23000v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Limited Feedback in RIS-Assisted Wireless Communications: Use Cases, Challenges, and Future Directions", "abstract": "Channel state information (CSI) is essential to unlock the potential of\nreconfigurable intelligent surfaces (RISs) in wireless communication systems.\nSince massive RIS elements are typically implemented without baseband signal\nprocessing capabilities, limited CSI feedback is necessary when designing the\nreflection/refraction coefficients of the RIS. In this article, the unique\nRIS-assisted channel features, such as the RIS position-dependent channel\nfluctuation, the ultra-high dimensional sub-channel matrix, and the structured\nsparsity, are distilled from recent advances in limited feedback and used as\nguidelines for designing feedback schemes. We begin by illustrating the use\ncases and the corresponding challenges associated with RIS feedback. We then\ndiscuss how to leverage techniques such as channel customization,\nstructured-sparsity, autoencoders, and others to reduce feedback overhead and\ncomplexity when devising feedback schemes. Finally, we identify potential\nresearch directions by considering the unresolved challenges, the new RIS\narchitecture, and the integration with multi-modal information and artificial\nintelligence.", "published": "2025-06-28 14:38:21", "link": "http://arxiv.org/abs/2506.22903v1", "categories": ["eess.SP", "cs.IT", "math.IT"], "primary_category": "eess.SP"}
{"title": "Identification of Cellular Automata on Spaces of Bernoulli Probability Measures", "abstract": "Classical Cellular Automata (CCAs) are a powerful computational framework for\nmodeling global spatio-temporal dynamics with local interactions. While CCAs\nhave been applied across numerous scientific fields, identifying the local rule\nthat governs observed dynamics remains a challenging task. Moreover, the\nunderlying assumption of deterministic cell states often limits the\napplicability of CCAs to systems characterized by inherent uncertainty. This\nstudy, therefore, focuses on the identification of Cellular Automata on spaces\nof probability measures (CAMs), where cell states are represented by\nprobability distributions. This framework enables the modeling of systems with\nprobabilistic uncertainty and spatially varying dynamics. Moreover, we\nformulate the local rule identification problem as a parameter estimation\nproblem and propose a meta-heuristic search based on Self-adaptive Differential\nEvolution (SaDE) to estimate local rule parameters accurately from the observed\ndata. The efficacy of the proposed approach is demonstrated through local rule\nidentification in two-dimensional CAMs with varying neighborhood types and\nradii.", "published": "2025-06-28 12:33:42", "link": "http://arxiv.org/abs/2506.22867v1", "categories": ["eess.SY", "cs.IT", "cs.SY", "math.IT"], "primary_category": "eess.SY"}
{"title": "Resilient-Native and Intelligent Next-Generation Wireless Systems: Key Enablers, Foundations, and Applications", "abstract": "Just like power, water, and transportation systems, wireless networks are a\ncrucial societal infrastructure. As natural and human-induced disruptions\ncontinue to grow, wireless networks must be resilient. This requires them to\nwithstand and recover from unexpected adverse conditions, shocks, unmodeled\ndisturbances and cascading failures. Unlike robustness and reliability,\nresilience is based on the understanding that disruptions will inevitably\nhappen. Resilience, as elasticity, focuses on the ability to bounce back to\nfavorable states, while resilience as plasticity involves agents and networks\nthat can flexibly expand their states and hypotheses through real-time\nadaptation and reconfiguration. This situational awareness and active\npreparedness, adapting world models and counterfactually reasoning about\npotential system failures and the best responses, is a core aspect of\nresilience. This article will first disambiguate resilience from reliability\nand robustness, before delving into key mathematical foundations of resilience\ngrounded in abstraction, compositionality and emergence. Subsequently, we focus\nour attention on a plethora of techniques and methodologies pertaining to the\nunique characteristics of resilience, as well as their applications through a\ncomprehensive set of use cases. Ultimately, the goal of this paper is to\nestablish a unified foundation for understanding, modeling, and engineering\nresilience in wireless communication systems, while laying a roadmap for the\nnext-generation of resilient-native and intelligent wireless systems.", "published": "2025-06-28 19:41:42", "link": "http://arxiv.org/abs/2506.22991v1", "categories": ["cs.NI", "cs.LO", "cs.MA", "cs.SY", "eess.SY"], "primary_category": "cs.NI"}
{"title": "Hierarchical Decentralized Stochastic Control for Cyber-Physical Systems", "abstract": "This paper presents a two-timescale hierarchical decentralized architecture\nfor control of Cyber-Physical Systems. The architecture consists of $N$\nindependent sub-processes, a global controller, and $N$ local controllers, each\nformulated as a Markov Decision Process (MDP). The global controller, operating\nat a slower timescale optimizes the infinite-horizon discounted cumulative\nreward under budget constraints. For the local controllers, operating at a\nfaster timescale, we propose two different optimization frameworks, namely the\nCOpt and FOpt. In the COpt framework, the local controller also optimizes an\ninfinite-horizon MDP, while in the FOpt framework, the local controller\noptimizes a finite-horizon MDP. The FOpt framework mimics a federal structure,\nwhere the local controllers have more autonomy in their decision making. First,\nthe existence of stationary deterministic optimal policies for both these\nframeworks is established. Then, various relationships between the two\nframeworks are studied, including a bound on the difference between the two\noptimal value functions. Additionally, sufficiency conditions are provided such\nthat the two frameworks lead to the same optimal values.", "published": "2025-06-28 18:03:35", "link": "http://arxiv.org/abs/2506.22971v1", "categories": ["eess.SY", "cs.LG", "cs.MA", "cs.SY", "math.OC"], "primary_category": "eess.SY"}
{"title": "Detection of coordinated fleet vehicles in route choice urban games. Part I. Inverse fleet assignment theory", "abstract": "Detection of collectively routing fleets of vehicles in future urban systems\nmay become important for the management of traffic, as such routing may\ndestabilize urban networks leading to deterioration of driving conditions.\nAccordingly, in this paper we discuss the question whether it is possible to\ndetermine the flow of fleet vehicles on all routes given the fleet size and\nbehaviour as well as the combined total flow of fleet and non-fleet vehicles on\nevery route. We prove that the answer to this Inverse Fleet Assignment Problem\nis 'yes' for myopic fleet strategies which are more 'selfish' than\n'altruistic', and 'no' otherwise, under mild assumptions on route/link\nperformance functions. To reach these conclusions we introduce the forward\nfleet assignment operator and study its properties, proving that it is\ninvertible for 'bad' objectives of fleet controllers. We also discuss the\nchallenges of implementing myopic fleet routing in the real world and compare\nit to Stackelberg and Nash routing. Finally, we show that optimal Stackelberg\nfleet routing could involve highly variable mixed strategies in some scenarios,\nwhich would likely cause chaos in the traffic network.", "published": "2025-06-28 17:56:38", "link": "http://arxiv.org/abs/2506.22966v1", "categories": ["math.OC", "cs.MA", "econ.TH"], "primary_category": "math.OC"}
{"title": "Neural Cellular Automata: From Cells to Pixels", "abstract": "Neural Cellular Automata (NCAs) are bio-inspired systems in which identical\ncells self-organize to form complex and coherent patterns by repeatedly\napplying simple local rules. NCAs display striking emergent behaviors including\nself-regeneration, generalization and robustness to unseen situations, and\nspontaneous motion. Despite their success in texture synthesis and\nmorphogenesis, NCAs remain largely confined to low-resolution grids. This\nlimitation stems from (1) training time and memory requirements that grow\nquadratically with grid size, (2) the strictly local propagation of information\nwhich impedes long-range cell communication, and (3) the heavy compute demands\nof real-time inference at high resolution. In this work, we overcome this\nlimitation by pairing NCA with a tiny, shared implicit decoder, inspired by\nrecent advances in implicit neural representations. Following NCA evolution on\na coarse grid, a lightweight decoder renders output images at arbitrary\nresolution. We also propose novel loss functions for both morphogenesis and\ntexture synthesis tasks, specifically tailored for high-resolution output with\nminimal memory and computation overhead. Combining our proposed architecture\nand loss functions brings substantial improvement in quality, efficiency, and\nperformance. NCAs equipped with our implicit decoder can generate full-HD\noutputs in real time while preserving their self-organizing, emergent\nproperties. Moreover, because each MLP processes cell states independently,\ninference remains highly parallelizable and efficient. We demonstrate the\napplicability of our approach across multiple NCA variants (on 2D, 3D grids,\nand 3D meshes) and multiple tasks, including texture generation and\nmorphogenesis (growing patterns from a seed), showing that with our proposed\nframework, NCAs seamlessly scale to high-resolution outputs with minimal\ncomputational overhead.", "published": "2025-06-28 14:30:21", "link": "http://arxiv.org/abs/2506.22899v1", "categories": ["cs.CV", "cs.GR", "cs.LG", "cs.MA", "eess.IV"], "primary_category": "cs.CV"}
{"title": "Cooperation as Black Box: Conceptual Fluctuation and Diagnostic Tools for Misalignment in MAS", "abstract": "Misalignment in multi-agent systems (MAS) is often treated as a technical\nfailure; yet many such failures originate upstream, during the conceptual\ndesign phase, where semantic ambiguity and normative projection take place.\nThis paper identifies a foundational source of interpretive misalignment in\nMAS: the systemic conflation of cooperation and coordination, and the moral\noverreading that follows. Using the Rabbit-Duck illusion, we illustrate how\nperspective-dependent readings of agent behavior can create epistemic\ninstability. To address this, we introduce the Misalignment Mosaic, a\ndiagnostic framework for diagnosing meaning-level misalignment in MAS. It\ncomprises four components: 1. Terminological Inconsistency, 2. Concept-to-Code\nDecay, 3. Morality as Cooperation, and 4. Interpretive Ambiguity. The Mosaic\nenables researchers to examine how misalignment arises not only through policy\nor reward structures but also through language, framing, and design\nassumptions. While this paper focuses on the specific ambiguity between\ncoordination and cooperation, the Mosaic generalizes to other overloaded\nconcepts in MAS, such as alignment, autonomy, and trust. Rather than define\ncooperation once and for all, we offer a framework to diagnose meaning itself\nas a source of misalignment.", "published": "2025-06-28 13:13:33", "link": "http://arxiv.org/abs/2506.22876v1", "categories": ["cs.MA"], "primary_category": "cs.MA"}
{"title": "Momentum-based Accelerated Algorithm for Distributed Optimization under Sector-Bound Nonlinearity", "abstract": "Distributed optimization advances centralized machine learning methods by\nenabling parallel and decentralized learning processes over a network of\ncomputing nodes. This work provides an accelerated consensus-based distributed\nalgorithm for locally non-convex optimization using the gradient-tracking\ntechnique. The proposed algorithm (i) improves the convergence rate by adding\nmomentum towards the optimal state using the heavy-ball method, while (ii)\naddressing general sector-bound nonlinearities over the information-sharing\nnetwork. The link nonlinearity includes any sign-preserving odd sector-bound\nmapping, for example, log-scale data quantization or clipping in practical\napplications. For admissible momentum and gradient-tracking parameters, using\nperturbation theory and eigen-spectrum analysis, we prove convergence even in\nthe presence of sector-bound nonlinearity and for locally non-convex cost\nfunctions. Further, in contrast to most existing weight-stochastic algorithms,\nwe adopt weight-balanced (WB) network design. This WB design and\nperturbation-based analysis allow to handle dynamic directed network of agents\nto address possible time-varying setups due to link failures or packet drops.", "published": "2025-06-28 11:38:39", "link": "http://arxiv.org/abs/2506.22855v1", "categories": ["eess.SY", "cs.DC", "cs.MA", "cs.SY", "eess.SP", "math.OC"], "primary_category": "eess.SY"}
{"title": "BWLer: Barycentric Weight Layer Elucidates a Precision-Conditioning Tradeoff for PINNs", "abstract": "Physics-informed neural networks (PINNs) offer a flexible way to solve\npartial differential equations (PDEs) with machine learning, yet they still\nfall well short of the machine-precision accuracy many scientific tasks demand.\nIn this work, we investigate whether the precision ceiling comes from the\nill-conditioning of the PDEs or from the typical multi-layer perceptron (MLP)\narchitecture. We introduce the Barycentric Weight Layer (BWLer), which models\nthe PDE solution through barycentric polynomial interpolation. A BWLer can be\nadded on top of an existing MLP (a BWLer-hat) or replace it completely\n(explicit BWLer), cleanly separating how we represent the solution from how we\ntake derivatives for the PDE loss. Using BWLer, we identify fundamental\nprecision limitations within the MLP: on a simple 1-D interpolation task, even\nMLPs with O(1e5) parameters stall around 1e-8 RMSE -- about eight orders above\nfloat64 machine precision -- before any PDE terms are added. In PDE learning,\nadding a BWLer lifts this ceiling and exposes a tradeoff between achievable\naccuracy and the conditioning of the PDE loss. For linear PDEs we fully\ncharacterize this tradeoff with an explicit error decomposition and navigate it\nduring training with spectral derivatives and preconditioning. Across five\nbenchmark PDEs, adding a BWLer on top of an MLP improves RMSE by up to 30x for\nconvection, 10x for reaction, and 1800x for wave equations while remaining\ncompatible with first-order optimizers. Replacing the MLP entirely lets an\nexplicit BWLer reach near-machine-precision on convection, reaction, and wave\nproblems (up to 10 billion times better than prior results) and match the\nperformance of standard PINNs on stiff Burgers' and irregular-geometry Poisson\nproblems. Together, these findings point to a practical path for combining the\nflexibility of PINNs with the precision of classical spectral solvers.", "published": "2025-06-28 22:11:00", "link": "http://arxiv.org/abs/2506.23024v1", "categories": ["cs.LG", "cs.AI", "cs.NA", "math.NA"], "primary_category": "cs.LG"}
{"title": "Differentiable Radar Ambiguity Functions: Mathematical Formulation and Computational Implementation", "abstract": "The ambiguity function is fundamental to radar waveform design,\ncharacterizing range and Doppler resolution capabilities. However, its\ntraditional formulation involves non-differentiable operations, preventing\nintegration with gradient-based optimization methods and modern machine\nlearning frameworks. This paper presents the first complete mathematical\nframework and computational implementation for differentiable radar ambiguity\nfunctions. Our approach addresses the fundamental technical challenges that\nhave prevented the radar community from leveraging automatic differentiation:\nproper handling of complex-valued gradients using Wirtinger calculus, efficient\ncomputation through parallelized FFT operations, numerical stability throughout\ncascaded operations, and composability with arbitrary differentiable\noperations. We term this approach GRAF (Gradient-based Radar Ambiguity\nFunctions), which reformulates the ambiguity function computation to maintain\nmathematical equivalence while enabling gradient flow through the entire\npipeline. The resulting implementation provides a general-purpose\ndifferentiable ambiguity function compatible with modern automatic\ndifferentiation frameworks, enabling new research directions including neural\nnetwork-based waveform generation with ambiguity constraints, end-to-end\noptimization of radar systems, and integration of classical radar theory with\nmodern deep learning. We provide complete implementation details and\ndemonstrate computational efficiency suitable for practical applications. This\nwork establishes the mathematical and computational foundation for applying\nmodern machine learning techniques to radar waveform design, bridging classical\nradar signal processing with automatic differentiation frameworks.", "published": "2025-06-28 16:06:34", "link": "http://arxiv.org/abs/2506.22935v1", "categories": ["eess.SP", "cs.LG", "cs.NA", "math.NA", "94A12, 65T50, 68T05", "F.2.1; I.2.6; G.1.0"], "primary_category": "eess.SP"}
{"title": "An approximation theory for Markov chain compression", "abstract": "We develop a framework for the compression of reversible Markov chains with\nrigorous error control. Given a subset of selected states, we construct reduced\ndynamics that can be lifted to an approximation of the full dynamics, and we\nprove simple spectral and nuclear norm bounds on the recovery error in terms of\na suitably interpreted Nystr\\\"{o}m approximation error. We introduce two\ncompression schemes: a projective compression based on committor functions and\na structure-preserving compression defined in terms of an induced Markov chain\nover the selected states. The Nystr\\\"{o}m error appearing in our bounds can be\ncontrolled using recent results on column subset selection by nuclear\nmaximization. Numerical experiments validate our theory and demonstrate the\nscalability of our approach.", "published": "2025-06-28 15:02:48", "link": "http://arxiv.org/abs/2506.22918v1", "categories": ["math.NA", "cs.NA", "math.PR"], "primary_category": "math.NA"}
{"title": "A Dilation-based Seamless Multiscale Method For Elliptic Problems", "abstract": "Many numerical methods for multiscale differential equations require a scale\nseparation between the larger and the smaller scales to achieve accuracy and\ncomputational efficiency. In the area of multiscale dynamical systems,\nso-called, seamless methods have been introduced to reduce the requirement of\nscale separation. We will translate these methods to numerical homogenization\nproblems and extend the technique to multiple dimensions. The initial step is\nto prove that a one-dimensional \\sepia{second-order} elliptic operator with\noscillatory coefficients can be rewritten as a multiscale dynamical system.\nInspired by this, multiscale elliptic operators in higher dimensions are\napproximated by a novel approach based on local dilation, which provides a\nmiddle ground for balancing intractability and accuracy without the need for\nfull resolution. The dilation operator can be further generalized to preserve\nimportant structures by properly decomposing the coefficient field. Error\nestimates are developed and promising numerical results of different examples\nare included.", "published": "2025-06-28 14:52:08", "link": "http://arxiv.org/abs/2506.22912v1", "categories": ["math.NA", "cs.NA", "74Q05, 35B27, 65N30, 65N06, 74Q15"], "primary_category": "math.NA"}
{"title": "Deep neural networks can provably solve Bellman equations for Markov decision processes without the curse of dimensionality", "abstract": "Discrete time stochastic optimal control problems and Markov decision\nprocesses (MDPs) are fundamental models for sequential decision-making under\nuncertainty and as such provide the mathematical framework underlying\nreinforcement learning theory. A central tool for solving MDPs is the Bellman\nequation and its solution, the so-called $Q$-function. In this article, we\nconstruct deep neural network (DNN) approximations for $Q$-functions associated\nto MDPs with infinite time horizon and finite control set $A$. More\nspecifically, we show that if the the payoff function and the random transition\ndynamics of the MDP can be suitably approximated by DNNs with leaky rectified\nlinear unit (ReLU) activation, then the solutions $Q_d\\colon \\mathbb R^d\\to\n\\mathbb R^{|A|}$, $d\\in \\mathbb{N}$, of the associated Bellman equations can\nalso be approximated in the $L^2$-sense by DNNs with leaky ReLU activation\nwhose numbers of parameters grow at most polynomially in both the dimension\n$d\\in \\mathbb{N}$ of the state space and the reciprocal $1/\\varepsilon$ of the\nprescribed error $\\varepsilon\\in (0,1)$. Our proof relies on the recently\nintroduced full-history recursive multilevel fixed-point (MLFP) approximation\nscheme.", "published": "2025-06-28 11:25:44", "link": "http://arxiv.org/abs/2506.22851v1", "categories": ["math.OC", "cs.LG", "cs.NA", "math.NA", "math.PR", "stat.ML", "90C40, 90C39, 60J05, 93E20, 65C05, 68T07"], "primary_category": "math.OC"}
{"title": "A Chimera domain decomposition method with weak Dirichlet-Robin coupling for finite element simulation of particulate flows", "abstract": "We introduce a new multimesh finite element method for direct numerical\nsimulation of incompressible particulate flows. The proposed approach falls\ninto the category of overlapping domain decomposition / Chimera / overset grid\nmeshes. In addition to calculating the velocity and pressure of the fictitious\nfluid on a fixed background mesh, we solve the incompressible Navier-Stokes\nequations on body-fitted submeshes that are attached to moving particles. The\nsubmesh velocity and pressure are used to calculate the hydrodynamic forces and\ntorques acting on the particles. The coupling with the background velocity and\npressure is enforced via (i) Robin-type boundary conditions for an\nArbitrary-Lagrangian-Eulerian (ALE) formulation of the submesh problems and\n(ii) a Dirichlet-type distributed interior penalty term in the weak form of the\nbackground mesh problem. The implementation of the weak Dirichlet-Robin\ncoupling is discussed in the context of discrete projection methods and finite\nelement discretizations. Detailed numerical studies are performed for standard\ntest problems involving fixed and moving immersed objects. A comparison of\nChimera results with those produced by fictitious boundary methods illustrates\nsignificant gains in the accuracy of drag and lift approximations.", "published": "2025-06-28 09:52:40", "link": "http://arxiv.org/abs/2506.22831v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Denoising Multi-Color QR Codes and Stiefel-Valued Data by Relaxed Regularizations", "abstract": "The handling of manifold-valued data, for instance, plays a central role in\ncolor restoration tasks relying on circle- or sphere-valued color models, in\nthe study of rotational or directional information related to the special\northogonal group, and in Gaussian image processing, where the pixel statistics\nare interpreted as values on the hyperbolic sheet. Especially, to denoise these\nkind of data, there have been proposed several generalizations of total\nvariation (TV) and Tikhonov-type denoising models incorporating the underlying\nmanifolds. Recently, a novel, numerically efficient denoising approach has been\nintroduced, where the data are embedded in an Euclidean ambient space, the\nnon-convex manifolds are encoded by a series of positive semi-definite,\nfixed-rank matrices, and the rank constraint is relaxed to obtain a\nconvexification that can be solved using standard algorithms from convex\nanalysis. The aim of the present paper is to extent this approach to new kinds\nof data like multi-binary and Stiefel-valued data. Multi-binary data can, for\ninstance, be used to model multi-color QR codes whereas Stiefel-valued data\noccur in image and video-based recognition. For both new data types, we propose\nTV- and Tikhonov-based denoising modelstogether with easy-to-solve\nconvexification. All derived methods are evaluated on proof-of-concept,\nsynthetic experiments.", "published": "2025-06-28 09:33:29", "link": "http://arxiv.org/abs/2506.22826v1", "categories": ["math.OC", "cs.CV", "cs.NA", "math.NA", "94A08, 94A12, 65J22, 90C22, 90C25"], "primary_category": "math.OC"}
{"title": "Long-time error estimate and decay of finite element method to a generalized viscoelastic flow", "abstract": "This work analyzes the finite element approximation to a viscoelastic flow\nmodel, which generalizes the Navier-Stokes equation and Oldroyd's model by\nintroducing the tempered power-law memory kernel. We prove regularity and\nlong-time exponential decay of the solutions, as well as a long-time\nconvolution-type Gr\\\"onwall inequality to support numerical analysis. A\nVolterra-Stokes projection is developed and analyzed to facilitate the\nparabolic-type duality argument, leading to the long-time error estimates and\nexponential decay of velocity and pressure. A benchmark problem of planar\nfour-to-one contraction flow is simulated to substantiate the generality of the\nproposed model in comparison with the Navier-Stokes equation and Oldroyd's\nmodel.", "published": "2025-06-28 06:54:24", "link": "http://arxiv.org/abs/2506.22782v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "A Novel Adaptive Low-Rank Matrix Approximation Method for Image Compression and Reconstruction", "abstract": "Low-rank matrix approximation plays an important role in various applications\nsuch as image processing, signal processing and data analysis. The existing\nmethods require a guess of the ranks of matrices that represent images or\ninvolve additional costs to determine the ranks. A novel efficient orthogonal\ndecomposition with automatic basis extraction (EOD-ABE) is proposed to compute\nthe optimal low-rank matrix approximation with adaptive identification of the\noptimal rank. By introducing a randomized basis extraction mechanism, EOD-ABE\neliminates the need for additional rank determination steps and can compute a\nrank-revealing approximation to a low-rank matrix. With a computational\ncomplexity of $O(mnr)$, where $m$ and $n$ are the dimensions of the matrix and\n$r$ is its rank, EOD-ABE achieves significant speedups compared to the\nstate-of-the-art methods. Experimental results demonstrate the superior speed,\naccuracy and robustness of EOD-ABE and indicate that EOD-ABE is a powerful tool\nfor fast image compression and reconstruction and hyperspectral image\ndimensionality reduction in large-scale applications.", "published": "2025-06-28 01:48:42", "link": "http://arxiv.org/abs/2506.22713v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Lower bounds for trace estimation via Block Krylov and other methods", "abstract": "This paper studies theoretical lower bounds for estimating the trace of a\nmatrix function, $\\text{tr}(f(A))$, focusing on methods that use Hutchinson's\nmethod along with Block Krylov techniques. These methods work by approximating\nmatrix-vector products like $f(A)V$ using a Block Krylov subspace. This is\nclosely related to approximating functions with polynomials. We derive\ntheoretical upper bounds on how many Krylov steps are needed for functions such\nas $A^{-1/2}$ and $A^{-1}$ by analyzing the upper bounds from the polynomial\napproximation of their scalar equivalent. In addition, we also develop lower\nlimits on the number of queries needed for trace estimation, specifically for\n$\\text{tr}(W^{-p})$ where $W$ is a Wishart matrix. Our study clarifies the\nconnection between the number of steps in Block Krylov methods and the degree\nof the polynomial used for approximation. This links the total cost of trace\nestimation to basic limits in polynomial approximation and how much information\nis needed for the computation.", "published": "2025-06-28 00:41:39", "link": "http://arxiv.org/abs/2506.22701v1", "categories": ["math.ST", "cs.DS", "cs.LG", "cs.NA", "math.NA", "stat.TH"], "primary_category": "math.ST"}
{"title": "A new sparsity promoting residual transform operator for Lasso regression", "abstract": "Lasso regression is a widely employed approach within the $\\ell_1$\nregularization framework used to promote sparsity and recover piecewise smooth\nsignals $f:[a,b) \\rightarrow \\mathbb{R}$ when the given observations are\nobtained from noisy, blurred, and/or incomplete data environments. In choosing\nthe regularizing sparsity-promoting operator, it is assumed that the particular\ntype of variability of the underlying signal, for example, piecewise constant\nor piecewise linear behavior across the entire domain, is both known and fixed.\nSuch an assumption is problematic in more general cases, e.g.~when a signal\nexhibits piecewise oscillatory behavior with varying wavelengths and\nmagnitudes. To address the limitations of assuming a fixed (and typically low\norder) variability when choosing a sparsity-promoting operator, this\ninvestigation proposes a novel residual transform operator that can be used\nwithin the Lasso regression formulation. In a nutshell, the idea is that for a\ngeneral piecewise smooth signal $f$, it is possible to design two operators\n$\\mathcal L_1$ and $\\mathcal L_2$ such that $\\mathcal L_1{\\boldsymbol f}\n\\approx \\mathcal L_2{\\boldsymbol f}$, where ${\\boldsymbol f} \\in \\mathbb{R}^n$\nis a discretized approximation of $f$, but $\\mathcal L_1 \\not\\approx \\mathcal\nL_2$. The corresponding residual transform operator, $\\mathcal L = \\mathcal\nL_1- \\mathcal L_2$, yields a result that (1) effectively reduces the\nvariability dependent error that occurs when applying either $\\mathcal L_1$ or\n$\\mathcal L_2$ to ${\\boldsymbol f}$, a property that holds even when $\\mathcal\nL_1{\\boldsymbol f} \\approx \\mathcal L_2{\\boldsymbol f}$ is not a good\napproximation to the true sparse domain vector of ${\\boldsymbol f}$, and (2)\ndoes not require $\\mathcal L_1$ or $\\mathcal L_2$ to have prior information\nregarding the variability of the underlying signal.", "published": "2025-06-28 00:02:41", "link": "http://arxiv.org/abs/2506.22689v1", "categories": ["math.NA", "cs.NA", "65F22, 62F15, 65K10, 68U10, 62J07"], "primary_category": "math.NA"}
{"title": "SABR-Informed Multitask Gaussian Process: A Synthetic-to-Real Framework for Implied Volatility Surface Construction", "abstract": "Constructing the Implied Volatility Surface (IVS) is a challenging task in\nquantitative finance due to the complexity of real markets and the sparsity of\nmarket data. Structural models like Stochastic Alpha Beta Rho (SABR) model\noffer interpretability and theoretical consistency but lack flexibility, while\npurely data-driven methods such as Gaussian Process regression can struggle\nwith sparse data. We introduce SABR-Informed Multi-Task Gaussian Process\n(SABR-MTGP), treating IVS construction as a multi-task learning problem. Our\nmethod uses a dense synthetic dataset from a calibrated SABR model as a source\ntask to inform the construction based on sparse market data (the target task).\nThe MTGP framework captures task correlation and transfers structural\ninformation adaptively, improving predictions particularly in data-scarce\nregions. Experiments using Heston-generated ground truth data under various\nmarket conditions show that SABR-MTGP outperforms both standard Gaussian\nprocess regression and SABR across different maturities. Furthermore, an\napplication to real SPX market data demonstrates the method's practical\napplicability and its ability to produce stable and realistic surfaces. This\nconfirms our method balances structural guidance from SABR with the flexibility\nneeded for market data.", "published": "2025-06-28 13:57:17", "link": "http://arxiv.org/abs/2506.22888v1", "categories": ["q-fin.CP"], "primary_category": "q-fin.CP"}
{"title": "Can We Reliably Predict the Fed's Next Move? A Multi-Modal Approach to U.S. Monetary Policy Forecasting", "abstract": "Forecasting central bank policy decisions remains a persistent challenge for\ninvestors, financial institutions, and policymakers due to the wide-reaching\nimpact of monetary actions. In particular, anticipating shifts in the U.S.\nfederal funds rate is vital for risk management and trading strategies.\nTraditional methods relying only on structured macroeconomic indicators often\nfall short in capturing the forward-looking cues embedded in central bank\ncommunications.\n  This study examines whether predictive accuracy can be enhanced by\nintegrating structured data with unstructured textual signals from Federal\nReserve communications. We adopt a multi-modal framework, comparing traditional\nmachine learning models, transformer-based language models, and deep learning\narchitectures in both unimodal and hybrid settings.\n  Our results show that hybrid models consistently outperform unimodal\nbaselines. The best performance is achieved by combining TF-IDF features of\nFOMC texts with economic indicators in an XGBoost classifier, reaching a test\nAUC of 0.83. FinBERT-based sentiment features marginally improve ranking but\nperform worse in classification, especially under class imbalance. SHAP\nanalysis reveals that sparse, interpretable features align more closely with\npolicy-relevant signals.\n  These findings underscore the importance of integrating textual and\nstructured signals transparently. For monetary policy forecasting, simpler\nhybrid models can offer both accuracy and interpretability, delivering\nactionable insights for researchers and decision-makers.", "published": "2025-06-28 05:54:58", "link": "http://arxiv.org/abs/2506.22763v1", "categories": ["q-fin.PM", "cs.LG", "q-fin.CP"], "primary_category": "q-fin.PM"}
{"title": "Potential Customer Lifetime Value in Financial Institutions: The Usage Of Open Banking Data to Improve CLV Estimation", "abstract": "Financial institutions increasingly adopt customer-centric strategies to\nenhance profitability and build long-term relationships. While Customer\nLifetime Value (CLV) is a core metric, its calculations often rely solely on\nsingle-entity data, missing insights from customer activities across multiple\nfirms. This study introduces the Potential Customer Lifetime Value (PCLV)\nframework, leveraging Open Banking (OB) data to estimate customer value\ncomprehensively. We predict retention probability and estimate Potential\nContribution Margins (PCM) from competitor data, enabling PCLV calculation.\nResults show that OB data can be used to estimate PCLV per competitor,\nindicating a potential upside of 21.06% over the Actual CLV. PCLV offers a\nstrategic tool for managers to strengthen competitiveness by leveraging OB data\nand boost profitability by driving marketing efforts at the individual customer\nlevel to increase the Actual CLV.", "published": "2025-06-28 01:29:04", "link": "http://arxiv.org/abs/2506.22711v1", "categories": ["q-fin.PM", "q-fin.CP", "q-fin.RM"], "primary_category": "q-fin.PM"}
{"title": "Feature-Wise Mixing for Mitigating Contextual Bias in Predictive Supervised Learning", "abstract": "Bias in predictive machine learning (ML) models is a fundamental challenge\ndue to the skewed or unfair outcomes produced by biased models. Existing\nmitigation strategies rely on either post-hoc corrections or rigid constraints.\nHowever, emerging research claims that these techniques can limit scalability\nand reduce generalizability. To address this, this paper introduces a\nfeature-wise mixing framework to mitigate contextual bias. This was done by\nredistributing feature representations across multiple contextual datasets. To\nassess feature-wise mixing's effectiveness, four ML classifiers were trained\nusing cross-validation and evaluated with bias-sensitive loss functions,\nincluding disparity metrics and mean squared error (MSE), which served as a\nstandard measure of predictive performance. The proposed method achieved an\naverage bias reduction of 43.35% and a statistically significant decrease in\nMSE across all classifiers trained on mixed datasets. Additionally,\nbenchmarking against established bias mitigation techniques found that\nfeature-wise mixing consistently outperformed SMOTE oversampling and\ndemonstrated competitive effectiveness without requiring explicit bias\nattribute identification. Feature-wise mixing efficiently avoids the\ncomputational overhead typically associated with fairness-aware learning\nalgorithms. Future work could explore applying feature-wise mixing for\nreal-world fields where accurate predictions are necessary.", "published": "2025-06-28 23:12:59", "link": "http://arxiv.org/abs/2506.23033v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Kernel Outlier Detection", "abstract": "A new anomaly detection method called kernel outlier detection (KOD) is\nproposed. It is designed to address challenges of outlier detection in\nhigh-dimensional settings. The aim is to overcome limitations of existing\nmethods, such as dependence on distributional assumptions or on hyperparameters\nthat are hard to tune. KOD starts with a kernel transformation, followed by a\nprojection pursuit approach. Its novelties include a new ensemble of directions\nto search over, and a new way to combine results of different direction types.\nThis provides a flexible and lightweight approach for outlier detection. Our\nempirical evaluations illustrate the effectiveness of KOD on three small\ndatasets with challenging structures, and on four large benchmark datasets.", "published": "2025-06-28 20:06:06", "link": "http://arxiv.org/abs/2506.22994v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "CN-SBM: Categorical Block Modelling For Primary and Residual Copy Number Variation", "abstract": "Cancer is a genetic disorder whose clonal evolution can be monitored by\ntracking noisy genome-wide copy number variants. We introduce the Copy Number\nStochastic Block Model (CN-SBM), a probabilistic framework that jointly\nclusters samples and genomic regions based on discrete copy number states using\na bipartite categorical block model. Unlike models relying on Gaussian or\nPoisson assumptions, CN-SBM respects the discrete nature of CNV calls and\ncaptures subpopulation-specific patterns through block-wise structure. Using a\ntwo-stage approach, CN-SBM decomposes CNV data into primary and residual\ncomponents, enabling detection of both large-scale chromosomal alterations and\nfiner aberrations. We derive a scalable variational inference algorithm for\napplication to large cohorts and high-resolution data. Benchmarks on simulated\nand real datasets show improved model fit over existing methods. Applied to\nTCGA low-grade glioma data, CN-SBM reveals clinically relevant subtypes and\nstructured residual variation, aiding patient stratification in survival\nanalysis. These results establish CN-SBM as an interpretable, scalable\nframework for CNV analysis with direct relevance for tumor heterogeneity and\nprognosis.", "published": "2025-06-28 17:45:45", "link": "http://arxiv.org/abs/2506.22963v1", "categories": ["stat.ML", "cs.LG", "q-bio.GN"], "primary_category": "stat.ML"}
{"title": "FuzzCoh: Robust Canonical Coherence-Based Fuzzy Clustering of Multivariate Time Series", "abstract": "Brain cognitive and sensory functions are often associated with\nelectrophysiological activity at specific frequency bands. Clustering\nmultivariate time series (MTS) data like EEGs is important for understanding\nbrain functions but challenging due to complex non-stationary\ncross-dependencies, gradual transitions between cognitive states, noisy\nmeasurements, and ambiguous cluster boundaries. To address these issues, we\ndevelop a robust fuzzy clustering framework in the spectral domain. Our method\nleverages Kendall's tau-based canonical coherence, which extracts meaningful\nfrequency-specific monotonic relationships between groups of channels or\nregions. KenCoh effectively captures dominant coherence structures while\nremaining robust against outliers and noise, making it suitable for real EEG\ndatasets that typically contain artifacts. Our method first projects each MTS\nobject onto vectors derived from the KenCoh estimates (i.e, canonical\ndirections), which capture relevant information on the connectivity structure\nof oscillatory signals in predefined frequency bands. These spectral features\nare utilized to determine clusters of epochs using a fuzzy partitioning\nstrategy, accommodating gradual transitions and overlapping class structure.\nLastly, we demonstrate the effectiveness of our approach to EEG data where\nlatent cognitive states such as alertness and drowsiness exhibit\nfrequency-specific dynamics and ambiguity. Our method captures both spectral\nand spatial features by locating the frequency-dependent structure and brain\nfunctional connectivity. Built on the KenCoh framework for fuzzy clustering, it\nhandles the complexity of high-dimensional time series data and is broadly\napplicable to domains such as neuroscience, wearable sensing, environmental\nmonitoring, and finance.", "published": "2025-06-28 12:02:01", "link": "http://arxiv.org/abs/2506.22861v1", "categories": ["stat.AP", "stat.ME", "stat.ML"], "primary_category": "stat.AP"}
{"title": "Doubly robust estimation of causal effects for random object outcomes with continuous treatments", "abstract": "Causal inference is central to statistics and scientific discovery, enabling\nresearchers to identify cause-and-effect relationships beyond associations.\nWhile traditionally studied within Euclidean spaces, contemporary applications\nincreasingly involve complex, non-Euclidean data structures that reside in\nabstract metric spaces, known as random objects, such as images, shapes,\nnetworks, and distributions. This paper introduces a novel framework for causal\ninference with continuous treatments applied to non-Euclidean data. To address\nthe challenges posed by the lack of linear structures, we leverage Hilbert\nspace embeddings of the metric spaces to facilitate Fr\\'echet mean estimation\nand causal effect mapping. Motivated by a study on the impact of exposure to\nfine particulate matter on age-at-death distributions across U.S. counties, we\npropose a nonparametric, doubly-debiased causal inference approach for outcomes\nas random objects with continuous treatments. Our framework can accommodate\nmoderately high-dimensional vector-valued confounders and derive efficient\ninfluence functions for estimation to ensure both robustness and\ninterpretability. We establish rigorous asymptotic properties of the\ncross-fitted estimators and employ conformal inference techniques for\ncounterfactual outcome prediction. Validated through numerical experiments and\napplied to real-world environmental data, our framework extends causal\ninference methodologies to complex data structures, broadening its\napplicability across scientific disciplines.", "published": "2025-06-28 04:55:12", "link": "http://arxiv.org/abs/2506.22754v1", "categories": ["stat.ME", "econ.EM", "math.ST", "stat.AP", "stat.ML", "stat.TH"], "primary_category": "stat.ME"}
{"title": "Explanations are a means to an end", "abstract": "Modern methods for explainable machine learning are designed to describe how\nmodels map inputs to outputs--without deep consideration of how these\nexplanations will be used in practice. This paper argues that explanations\nshould be designed and evaluated with a specific end in mind. We describe how\nto formalize this end in a framework based in statistical decision theory. We\nshow how this functionally-grounded approach can be applied across diverse use\ncases, such as clinical decision support, providing recourse, or debugging. We\ndemonstrate its use to characterize the maximum \"boost\" in performance on a\nparticular task that an explanation could provide an idealized decision-maker,\npreventing misuse due to ambiguity by forcing researchers to specify concrete\nuse cases that can be analyzed in light of models of expected explanation use.\nWe argue that evaluation should meld theoretical and empirical perspectives on\nthe value of explanation, and contribute definitions that span these\nperspectives.", "published": "2025-06-28 03:04:21", "link": "http://arxiv.org/abs/2506.22740v1", "categories": ["cs.AI", "stat.ML"], "primary_category": "cs.AI"}
{"title": "Robust Tensor Completion via Gradient Tensor Nulclear L1-L2 Norm for Traffic Data Recovery", "abstract": "In real-world scenarios, spatiotemporal traffic data frequently experiences\ndual degradation from missing values and noise caused by sensor malfunctions\nand communication failures. Therefore, effective data recovery methods are\nessential to ensure the reliability of downstream data-driven applications.\nwhile classical tensor completion methods have been widely adopted, they are\nincapable of modeling noise, making them unsuitable for complex scenarios\ninvolving simultaneous data missingness and noise interference. Existing Robust\nTensor Completion (RTC) approaches offer potential solutions by separately\nmodeling the actual tensor data and noise. However, their effectiveness is\noften constrained by the over-relaxation of convex rank surrogates and the\nsuboptimal utilization of local consistency, leading to inadequate model\naccuracy. To address these limitations, we first introduce the tensor L1-L2\nnorm, a novel non-convex tensor rank surrogate that functions as an effective\nlow-rank representation tool. Leveraging an advanced feature fusion strategy,\nwe further develop the gradient tensor L1-L2 norm by incorporating the tensor\nL1-L2 norm in the gradient domain. By integrating the gradient tensor nuclear\nL1-L2 norm into the RTC framework, we propose the Robust Tensor Completion via\nGradient Tensor Nuclear L1-L2 Norm (RTC-GTNLN) model, which not only fully\nexploits both global low-rankness and local consistency without trade-off\nparameter, but also effectively handles the dual degradation challenges of\nmissing data and noise in traffic data. Extensive experiments conducted on\nmultiple real-world traffic datasets demonstrate that the RTC-GTNLN model\nconsistently outperforms existing state-of-the-art methods in complex recovery\nscenarios involving simultaneous missing values and noise.", "published": "2025-06-28 02:38:01", "link": "http://arxiv.org/abs/2506.22732v1", "categories": ["cs.LG", "eess.SP", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Generalized Linear Mode Connectivity for Transformers", "abstract": "Understanding the geometry of neural network loss landscapes is a central\nquestion in deep learning, with implications for generalization and\noptimization. A striking phenomenon is linear mode connectivity (LMC), where\nindependently trained models can be connected by low- or zero-loss paths,\ndespite appearing to lie in separate loss basins. However, this is often\nobscured by symmetries in parameter space -- such as neuron permutations --\nwhich make functionally equivalent models appear dissimilar. Prior work has\npredominantly focused on neuron re-ordering through permutations, but such\napproaches are limited in scope and fail to capture the richer symmetries\nexhibited by modern architectures such as Transformers. In this work, we\nintroduce a unified framework that captures four symmetry classes:\npermutations, semi-permutations, orthogonal transformations, and general\ninvertible maps -- broadening the set of valid reparameterizations and\nsubsuming many previous approaches as special cases. Crucially, this\ngeneralization enables, for the first time, the discovery of low- and\nzero-barrier linear interpolation paths between independently trained Vision\nTransformers and GPT-2 models. These results reveal deeper structure in the\nloss landscape and underscore the importance of symmetry-aware analysis for\nunderstanding model space geometry.", "published": "2025-06-28 01:46:36", "link": "http://arxiv.org/abs/2506.22712v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "General Autonomous Cybersecurity Defense: Learning Robust Policies for Dynamic Topologies and Diverse Attackers", "abstract": "In the face of evolving cyber threats such as malware, ransomware and\nphishing, autonomous cybersecurity defense (ACD) systems have become essential\nfor real-time threat detection and response with optional human intervention.\nHowever, existing ACD systems rely on limiting assumptions, particularly the\nstationarity of the underlying network dynamics. In real-world scenarios,\nnetwork topologies can change due to actions taken by attackers or defenders,\nsystem failures, or time evolution of networks, leading to failures in the\nadaptive capabilities of current defense agents. Moreover, many agents are\ntrained on static environments, resulting in overfitting to specific\ntopologies, which hampers their ability to generalize to out-of-distribution\nnetwork topologies. This work addresses these challenges by exploring methods\nfor developing agents to learn generalizable policies across dynamic network\nenvironments -- general ACD (GACD).", "published": "2025-06-28 01:12:13", "link": "http://arxiv.org/abs/2506.22706v1", "categories": ["cs.CR", "cs.AI", "cs.CV", "stat.ML"], "primary_category": "cs.CR"}
{"title": "VisionScores -- A system-segmented image score dataset for deep learning tasks", "abstract": "VisionScores presents a novel proposal being the first system-segmented image\nscore dataset, aiming to offer structure-rich, high information-density images\nfor machine and deep learning tasks. Delimited to two-handed piano pieces, it\nwas built to consider not only certain graphic similarity but also composition\npatterns, as this creative process is highly instrument-dependent. It provides\ntwo scenarios in relation to composer and composition type. The first, formed\nby 14k samples, considers works from different authors but the same composition\ntype, specifically, Sonatinas. The latter, consisting of 10.8K samples,\npresents the opposite case, various composition types from the same author,\nbeing the one selected Franz Liszt. All of the 24.8k samples are formatted as\ngrayscale jpg images of $128 \\times 512$ pixels. VisionScores supplies the\nusers not only the formatted samples but the systems' order and pieces'\nmetadata. Moreover, unsegmented full-page scores and the pre-formatted images\nare included for further analysis.", "published": "2025-06-28 22:29:23", "link": "http://arxiv.org/abs/2506.23030v1", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Adaptable Non-parametric Approach for Speech-based Symptom Assessment: Isolating Private Medical Data in a Retrieval Datastore", "abstract": "The automatic assessment of health-related acoustic cues has the potential to\nimprove healthcare accessibility and affordability. Although parametric models\nare promising, they face challenges in privacy and adaptability. To address\nthese, we propose a NoN-Parametric framework for Speech-based symptom\nAssessment (NoNPSA). By isolating medical data in a retrieval datastore, NoNPSA\navoids encoding private information in model parameters and enables efficient\ndata updates. A self-supervised learning (SSL) model pre-trained on\ngeneral-purpose datasets extracts features, which are used for similarity-based\nretrieval. Metadata-aware refinement filters the retrieved data, and associated\nlabels are used to compute an assessment score. Experimental results show that\nNoNPSA achieves competitive performance compared to fine-tuning SSL-based\nmethods, while enabling greater privacy, update efficiency, and\nadaptability--showcasing the potential of non-parametric approaches in\nhealthcare.", "published": "2025-06-28 18:05:25", "link": "http://arxiv.org/abs/2506.22972v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Feasibility of spectral-element modeling of wave propagation through the anatomy of marine mammals", "abstract": "This study introduces the first 3D spectral-element method (SEM) simulation\nof ultrasonic wave propagation in a bottlenose dolphin (Tursiops truncatus)\nhead. Unlike traditional finite-element methods (FEM), which struggle with\nhigh-frequency simulations due to costly linear-system inversions and slower\nconvergence, SEM offers exponential convergence and efficient parallel\ncomputation. Using Computed Tomography (CT) scan data, we developed a detailed\nhexahedral mesh capturing complex anatomical features, such as acoustic fats\nand jaws. Our simulations of plane and spherical waves confirm SEM's\neffectiveness for ultrasonic time-domain modeling. This approach opens new\navenues for marine biology, contributing to research in echolocation, the\nimpacts of anthropogenic marine noise pollution and the biophysics of hearing\nand click generation in marine mammals. By overcoming FEM's limitations, SEM\nprovides a powerful scalable tool to test hypotheses about dolphin\nbioacoustics, with significant implications for conservation and understanding\nmarine mammal auditory systems under increasing environmental challenges.", "published": "2025-06-28 16:29:03", "link": "http://arxiv.org/abs/2506.22944v1", "categories": ["cs.CE", "cs.SD", "eess.AS", "q-bio.TO"], "primary_category": "cs.CE"}
{"title": "A Self-Training Approach for Whisper to Enhance Long Dysarthric Speech Recognition", "abstract": "Dysarthric speech recognition (DSR) enhances the accessibility of smart\ndevices for dysarthric speakers with limited mobility. Previously, DSR research\nwas constrained by the fact that existing datasets typically consisted of\nisolated words, command phrases, and a limited number of sentences spoken by a\nfew individuals. This constrained research to command-interaction systems and\nspeaker adaptation. The Speech Accessibility Project (SAP) changed this by\nreleasing a large and diverse English dysarthric dataset, leading to the SAP\nChallenge to build speaker- and text-independent DSR systems. We enhanced the\nWhisper model's performance on long dysarthric speech via a novel self-training\nmethod. This method increased training data and adapted the model to handle\npotentially incomplete speech segments encountered during inference. Our system\nachieved second place in both Word Error Rate and Semantic Score in the SAP\nChallenge.", "published": "2025-06-28 08:24:15", "link": "http://arxiv.org/abs/2506.22810v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "WavShape: Information-Theoretic Speech Representation Learning for Fair and Privacy-Aware Audio Processing", "abstract": "Speech embeddings often retain sensitive attributes such as speaker identity,\naccent, or demographic information, posing risks in biased model training and\nprivacy leakage. We propose WavShape, an information-theoretic speech\nrepresentation learning framework that optimizes embeddings for fairness and\nprivacy while preserving task-relevant information. We leverage mutual\ninformation (MI) estimation using the Donsker-Varadhan formulation to guide an\nMI-based encoder that systematically filters sensitive attributes while\nmaintaining speech content essential for downstream tasks. Experimental results\non three known datasets show that WavShape reduces MI between embeddings and\nsensitive attributes by up to 81% while retaining 97% of task-relevant\ninformation. By integrating information theory with self-supervised speech\nmodels, this work advances the development of fair, privacy-aware, and\nresource-efficient speech systems.", "published": "2025-06-28 07:03:55", "link": "http://arxiv.org/abs/2506.22789v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Rate Maximization for Fluid Antenna System Assisted Semantic Communication", "abstract": "In this paper, we investigate the problem of rate maximization in a fluid\nantenna system (FAS) assisted\n  semantic communication system. In the considered model, a base station (BS)\nwith multiple static antennas employs semantic extraction techniques to\ncompress the data ready to be sent to a user. The user equipped with a fluid\nantenna is located in the near field coverage region of the BS. Our aim is to\njointly optimize the transmit beamforming and the semantic compression rate at\nthe BS, as well as the selection of activated ports in FAS, to maximize the\nequivalent transmission ratio under a specific power budget. We design an\nalternating algorithm to solve the problem, where we obtain the optimal\nsemantic compression ratio is in closed form at each step. Simulation results\nvalidate the effectiveness of the proposed algorithm.", "published": "2025-06-28 16:19:45", "link": "http://arxiv.org/abs/2506.22943v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Mathematical Computation on High-dimensional Data via Array Programming and Parallel Acceleration", "abstract": "While deep learning excels in natural image and language processing, its\napplication to high-dimensional data faces computational challenges due to the\ndimensionality curse. Current large-scale data tools focus on business-oriented\ndescriptive statistics, lacking mathematical statistics support for advanced\nanalysis. We propose a parallel computation architecture based on space\ncompleteness, decomposing high-dimensional data into dimension-independent\nstructures for distributed processing. This framework enables seamless\nintegration of data mining and parallel-optimized machine learning methods,\nsupporting scientific computations across diverse data types like medical and\nnatural images within a unified system.", "published": "2025-06-28 15:42:23", "link": "http://arxiv.org/abs/2506.22929v1", "categories": ["cs.LG", "cs.AI", "eess.IV", "eess.SP"], "primary_category": "cs.LG"}
{"title": "Coexistence analysis of Wi-Fi 6E and 5G NR-U in the 6 GHz band", "abstract": "The ever-increasing demand for broadband and IoT wireless connectivity has\nrecently urged the regulators around the world to start opening the 6 GHz\nspectrum for unlicensed use. These bands will, for example, permit the use of\nadditional 1.2 GHz in the US and 500 MHz in Europe for unlicensed radio access\ntechnologies (RATs) such as Wi-Fi and 5G New Radio Unlicensed (5G NR-U). To\nsupport QoS-sensitive applications with both technologies, fair and efficient\ncoexistence approaches between the two RATs, as well as with incumbents already\noperating in the 6 GHz band, are crucial. In this paper, we study through\nextensive simulations the achievable mean downlink throughput of both Wi-Fi 6E\nAPs and 5G NR-U gNBs when they are co-deployed in a dense residential scenario\nunder high-interference conditions. We also explore how different parameter\nsettings e.g., MAC frame aggregation, energy detection threshold and maximum\nchannel occupancy time (MCOT) affect the coexistence. Our findings give\nimportant insights into how to tune the key parameters to design fair\ncoexistence policies.", "published": "2025-06-28 10:48:16", "link": "http://arxiv.org/abs/2506.22844v1", "categories": ["eess.SP", "cs.NI"], "primary_category": "eess.SP"}
{"title": "Sensing Security Oriented OFDM-ISAC Against Multi-Intercept Threats", "abstract": "In recent years, security has emerged as a critical aspect of integrated\nsensing and communication (ISAC) systems. While significant research has\nfocused on secure communications, particularly in ensuring physical layer\nsecurity, the issue of sensing security has received comparatively less\nattention. This paper addresses the sensing security problem in ISAC,\nparticularly under the threat of multi-intercept adversaries. We consider a\nrealistic scenario in which the sensing target is an advanced electronic\nreconnaissance aircraft capable of employing multiple signal interception\ntechniques, such as power detection (PD) and cyclostationary analysis (CA). To\nevaluate sensing security under such sophisticated threats, we analyze two\ncritical features of the transmitted signal: (i) power distribution and (ii)\ncyclic spectrum. Further, we introduce a novel ergodic cyclic spectrum metric\nwhich leverages the intrinsic mathematical structure of cyclostationary signals\nto more comprehensively characterize their behavior. Building on this analysis,\nwe formulate a new ISAC design problem that explicitly considers sensing\nsecurity, and we develop a low-complexity, efficient optimization approach to\nsolve it. Simulation results demonstrate that the proposed metric is both\neffective and insightful, and that our ISAC design significantly enhances\nsensing security performance in the presence of multi-intercept threats.", "published": "2025-06-28 09:22:31", "link": "http://arxiv.org/abs/2506.22824v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "TriADA: Massively Parallel Trilinear Matrix-by-Tensor Multiply-Add Algorithm and Device Architecture for the Acceleration of 3D Discrete Transformations", "abstract": "Multilinear transformations are key in high-performance computing (HPC) and\nartificial intelligence (AI) workloads, where data is represented as tensors.\nHowever, their high computational and memory demands, which grow with\ndimensionality, often slow down critical tasks. Moreover, scaling computation\nby enlarging the number of parallel processing units substantially increases\nenergy consumption, limiting widespread adoption, especially for sparse data,\nwhich is common in HPC and AI applications. This paper introduces the Trilinear\nAlgorithm and isomorphic to algorithm Device Architecture (TriADA) to address\nthese challenges with the following innovations: (1) a massively parallel,\nlow-rank algorithm for computing a family of trilinear (3D) discrete orthogonal\ntransformations (3D-DXTs), which is a special case of the more general 3-mode\nmatrix-by-tensor multiplication (3D-GEMT); (2) a new outer-product-based GEMM\nkernel with decoupled streaming active memory, specially designed to accelerate\n3D-GEMT operation; (3) an isomorphic to the proposed algorithm, fully\ndistributed 3D network of mesh interconnected processing elements or cells with\na coordinate-free, data-driven local processing activity, which is independent\nof problem size; (4) an elastic sparse outer-product (ESOP) method that avoids\nunnecessary computing and communication operations with zero-valued operands,\nthereby enhancing energy efficiency, computational accuracy, and stability.\nTriADA is capable of performing a variety of trilinear transformations with\nhypercubic arithmetic complexity in a linear number of time-steps. The\nmassively parallel, scalable, and energy-efficient architecture of TriADA is\nideal for accelerating multilinear tensor operations, which are the most\ndemanding parts of AI and HPC workloads.", "published": "2025-06-28 08:42:01", "link": "http://arxiv.org/abs/2506.22818v1", "categories": ["cs.DC", "cs.AI", "cs.AR", "cs.ET", "eess.SP", "C.1.4; C.3; F.2.1; G.1.3; G.4"], "primary_category": "cs.DC"}
{"title": "Channel Knowledge Map-assisted Dual-domain Tracking and Predictive Beamforming for High-Mobility Wireless Networks", "abstract": "This paper introduces a novel channel knowledge map (CKM)-assisted\ndual-domain tracking and predictive beamforming scheme for high-mobility\nwireless networks. The central premise is that the CKM integrates both the\ncoordinate and beam domains, thereby enabling tracking in one domain via\ntreating the other domain's input as priors or measurements. In the coordinate\ndomain (C-Domain), an extended Kalman filter (EKF) is employed to predict and\ntrack the state (i.e., location and velocity) of a moving communication\nreceiver across time slots under both line-of-sight (LoS)-present and\nLoS-absent conditions, where the CKM provides a prior mapping from multipath\nchannel parameters to potential target locations. In the beam domain\n(B-Domain), the updated location of the receiver is fed back to CKM to offer a\npriori information of angle of arrival (AoA) variations, which are incorporated\nto establish beam transition models for effective beam tracking, depending on\nthe angular variation situation of each path. Then, we analyze the Cram\\'er-Rao\nBound (CRB) for AoA estimation for each path in the considered system and\npropose a jointly predictive beamforming and power allocation design to\nminimize AoA estimation errors, directly enhancing multipath beam tracking\naccuracy and indirectly improving target tracking performance. Simulation\nresults demonstrate that the proposed scheme achieves significant improvements\nin both target and beam tracking performance compared to the state-of-the-art\napproaches, particularly in AoA tracking of non-line-of-sight (NLoS) paths,\nhighlighting the potential gain of CKM in facilitating both target and beam\ntracking in high-mobility communications.", "published": "2025-06-28 07:52:51", "link": "http://arxiv.org/abs/2506.22796v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "A Correlation-Based Design of RIS for Reduced Power Consumption and Simplified Control Circuitry", "abstract": "Aiming at simplifying the hardware structure and reducing the energy\nconsumption in wireless communication via reconfigurable intelligent surfaces\n(RIS), this paper introduces a novel RIS design founded on the correlation\nbetween the phase shift values of the surface elements. First, a correlation\nanalysis is conducted, considering the azimuth angle of a target device within\na coverage region spanning from $-80^{\\circ}$ to $80^{\\circ}$. The correlation\nis demonstrated for different deployment cases, creating the basis for the new\nRIS structure, termed Connected-RIS, where correlated elements are designed to\nshare the same control signal. The fundamental performance of the proposed\ndesign is then analyzed in terms of control signals, power consumption, and\ncommunication system performance, comparing it to two RIS structures with full\ncontrol: one with the same size as the proposed design, and the other employing\nthe minimum number of elements necessary to satisfy the fair coverage\ncriterion. The correlation-based RIS design enables three-dimensional passive\nbeamforming and significantly reduces the number of required load impedances\nand control signals, thereby lowering the hardware cost and simplifying the\ncontrol circuitry. It also achieves substantial power savings as compared to\nthe baseline schemes, while maintaining sufficient gain for a fair radio\ncoverage. For instance, numerical simulations demonstrate that the proposed\ndesign reduces the power consumption by almost 86-92\\% and the control signals\nby 83-98\\% compared to operation with fully controlled RIS.", "published": "2025-06-28 00:56:42", "link": "http://arxiv.org/abs/2506.22702v1", "categories": ["eess.SY", "cs.AR", "cs.SY", "eess.SP"], "primary_category": "eess.SY"}
{"title": "Text Production and Comprehension by Human and Artificial Intelligence: Interdisciplinary Workshop Report", "abstract": "This report synthesizes the outcomes of a recent interdisciplinary workshop\nthat brought together leading experts in cognitive psychology, language\nlearning, and artificial intelligence (AI)-based natural language processing\n(NLP). The workshop, funded by the National Science Foundation, aimed to\naddress a critical knowledge gap in our understanding of the relationship\nbetween AI language models and human cognitive processes in text comprehension\nand composition. Through collaborative dialogue across cognitive, linguistic,\nand technological perspectives, workshop participants examined the underlying\nprocesses involved when humans produce and comprehend text, and how AI can both\ninform our understanding of these processes and augment human capabilities. The\nworkshop revealed emerging patterns in the relationship between large language\nmodels (LLMs) and human cognition, with highlights on both the capabilities of\nLLMs and their limitations in fully replicating human-like language\nunderstanding and generation. Key findings include the potential of LLMs to\noffer insights into human language processing, the increasing alignment between\nLLM behavior and human language processing when models are fine-tuned with\nhuman feedback, and the opportunities and challenges presented by human-AI\ncollaboration in language tasks. By synthesizing these findings, this report\naims to guide future research, development, and implementation of LLMs in\ncognitive psychology, linguistics, and education. It emphasizes the importance\nof ethical considerations and responsible use of AI technologies while striving\nto enhance human capabilities in text comprehension and production through\neffective human-AI collaboration.", "published": "2025-06-28 00:31:14", "link": "http://arxiv.org/abs/2506.22698v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Hierarchical Decentralized Stochastic Control for Cyber-Physical Systems", "abstract": "This paper presents a two-timescale hierarchical decentralized architecture\nfor control of Cyber-Physical Systems. The architecture consists of $N$\nindependent sub-processes, a global controller, and $N$ local controllers, each\nformulated as a Markov Decision Process (MDP). The global controller, operating\nat a slower timescale optimizes the infinite-horizon discounted cumulative\nreward under budget constraints. For the local controllers, operating at a\nfaster timescale, we propose two different optimization frameworks, namely the\nCOpt and FOpt. In the COpt framework, the local controller also optimizes an\ninfinite-horizon MDP, while in the FOpt framework, the local controller\noptimizes a finite-horizon MDP. The FOpt framework mimics a federal structure,\nwhere the local controllers have more autonomy in their decision making. First,\nthe existence of stationary deterministic optimal policies for both these\nframeworks is established. Then, various relationships between the two\nframeworks are studied, including a bound on the difference between the two\noptimal value functions. Additionally, sufficiency conditions are provided such\nthat the two frameworks lead to the same optimal values.", "published": "2025-06-28 18:03:35", "link": "http://arxiv.org/abs/2506.22971v2", "categories": ["eess.SY", "cs.LG", "cs.MA", "cs.SY", "math.OC"], "primary_category": "eess.SY"}
{"title": "DICE-BENCH: Evaluating the Tool-Use Capabilities of Large Language Models in Multi-Round, Multi-Party Dialogues", "abstract": "Existing function-calling benchmarks focus on single-turn interactions.\nHowever, they overlook the complexity of real-world scenarios. To quantify how\nexisting benchmarks address practical applications, we introduce DICE-SCORE, a\nmetric that evaluates the dispersion of tool-related information such as\nfunction name and parameter values throughout the dialogue. Analyzing existing\nbenchmarks through DICE-SCORE reveals notably low scores, highlighting the need\nfor more realistic scenarios. To address this gap, we present DICE-BENCH, a\nframework that constructs practical function-calling datasets by synthesizing\nconversations through a tool graph that maintains dependencies across rounds\nand a multi-agent system with distinct personas to enhance dialogue\nnaturalness. The final dataset comprises 1,607 high-DICE-SCORE instances. Our\nexperiments on 19 LLMs with DICE-BENCH show that significant advances are still\nrequired before such models can be deployed effectively in real-world settings.\nOur code and data are all publicly available:\nhttps://snuhcc.github.io/DICE-Bench/.", "published": "2025-06-28 11:28:04", "link": "http://arxiv.org/abs/2506.22853v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
