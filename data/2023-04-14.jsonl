{"title": "HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge", "abstract": "Large Language Models (LLMs), such as the LLaMA model, have demonstrated\ntheir effectiveness in various general-domain natural language processing (NLP)\ntasks. Nevertheless, LLMs have not yet performed optimally in biomedical domain\ntasks due to the need for medical expertise in the responses. In response to\nthis challenge, we propose HuaTuo, a LLaMA-based model that has been\nsupervised-fine-tuned with generated QA (Question-Answer) instances. The\nexperimental results demonstrate that HuaTuo generates responses that possess\nmore reliable medical knowledge. Our proposed HuaTuo model is accessible at\nhttps://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese.", "published": "2023-04-14 07:54:17", "link": "http://arxiv.org/abs/2304.06975v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dialogue Games for Benchmarking Language Understanding: Motivation,\n  Taxonomy, Strategy", "abstract": "How does one measure \"ability to understand language\"? If it is a person's\nability that is being measured, this is a question that almost never poses\nitself in an unqualified manner: Whatever formal test is applied, it takes\nplace on the background of the person's language use in daily social practice,\nand what is measured is a specialised variety of language understanding (e.g.,\nof a second language; or of written, technical language). Computer programs do\nnot have this background. What does that mean for the applicability of formal\ntests of language understanding? I argue that such tests need to be\ncomplemented with tests of language use embedded in a practice, to arrive at a\nmore comprehensive evaluation of \"artificial language understanding\". To do\nsuch tests systematically, I propose to use \"Dialogue Games\" -- constructed\nactivities that provide a situational embedding for language use. I describe a\ntaxonomy of Dialogue Game types, linked to a model of underlying capabilites\nthat are tested, and thereby giving an argument for the \\emph{construct\nvalidity} of the test. I close with showing how the internal structure of the\ntaxonomy suggests an ordering from more specialised to more general situational\nlanguage understanding, which potentially can provide some strategic guidance\nfor development in this field.", "published": "2023-04-14 09:11:36", "link": "http://arxiv.org/abs/2304.07007v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Label Dependencies-aware Set Prediction Networks for Multi-label Text\n  Classification", "abstract": "Multi-label text classification involves extracting all relevant labels from\na sentence. Given the unordered nature of these labels, we propose approaching\nthe problem as a set prediction task. To address the correlation between\nlabels, we leverage Graph Convolutional Networks and construct an adjacency\nmatrix based on the statistical relations between labels. Additionally, we\nenhance recall ability by applying the Bhattacharyya distance to the output\ndistributions of the set prediction networks. We evaluate the effectiveness of\nour approach on two multi-label datasets and demonstrate its superiority over\nprevious baselines through experimental results.", "published": "2023-04-14 09:31:17", "link": "http://arxiv.org/abs/2304.07022v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OPI at SemEval 2023 Task 1: Image-Text Embeddings and Multimodal\n  Information Retrieval for Visual Word Sense Disambiguation", "abstract": "The goal of visual word sense disambiguation is to find the image that best\nmatches the provided description of the word's meaning. It is a challenging\nproblem, requiring approaches that combine language and image understanding. In\nthis paper, we present our submission to SemEval 2023 visual word sense\ndisambiguation shared task. The proposed system integrates multimodal\nembeddings, learning to rank methods, and knowledge-based approaches. We build\na classifier based on the CLIP model, whose results are enriched with\nadditional information retrieved from Wikipedia and lexical databases. Our\nsolution was ranked third in the multilingual task and won in the Persian\ntrack, one of the three language subtasks.", "published": "2023-04-14 13:45:59", "link": "http://arxiv.org/abs/2304.07127v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OPI at SemEval 2023 Task 9: A Simple But Effective Approach to\n  Multilingual Tweet Intimacy Analysis", "abstract": "This paper describes our submission to the SemEval 2023 multilingual tweet\nintimacy analysis shared task. The goal of the task was to assess the level of\nintimacy of Twitter posts in ten languages. The proposed approach consists of\nseveral steps. First, we perform in-domain pre-training to create a language\nmodel adapted to Twitter data. In the next step, we train an ensemble of\nregression models to expand the training set with pseudo-labeled examples. The\nextended dataset is used to train the final solution. Our method was ranked\nfirst in five out of ten language subtasks, obtaining the highest average score\nacross all languages.", "published": "2023-04-14 13:49:28", "link": "http://arxiv.org/abs/2304.07130v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "nanoLM: an Affordable LLM Pre-training Benchmark via Accurate Loss\n  Prediction across Scales", "abstract": "As language models scale up, it becomes increasingly expensive to verify\nresearch ideas because conclusions on small models do not trivially transfer to\nlarge ones. A possible solution is to establish a generic system that\naccurately predicts certain metrics for large models without training them.\nExisting scaling laws require hyperparameter search on the largest models,\nlimiting their predicative capability. In this paper, we present an approach\n(namely {\\mu}Scaling) to predict the pre-training loss, based on our\nobservations that Maximal Update Parametrization ({\\mu}P) enables accurate\nfitting of scaling laws close to common loss basins in hyperparameter space.\nWith {\\mu}Scaling, different model designs can be compared on large scales by\ntraining only their smaller counterparts. Further, we introduce nanoLM: an\naffordable LLM pre-training benchmark that facilitates this new research\nparadigm. With around 14% of the one-time pre-training cost, we can accurately\nforecast the loss for models up to 52B. Our goal with nanoLM is to empower\nresearchers with limited resources to reach meaningful conclusions on large\nmodels. We also aspire for our benchmark to serve as a bridge between the\nacademic community and the industry. Code for {\\mu}Scaling is available at\nhttps://github.com/cofe-ai/Mu-scaling. Code for nanoLLM will be available\nlater.", "published": "2023-04-14 00:45:01", "link": "http://arxiv.org/abs/2304.06875v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved with\n  Text", "abstract": "In-context vision and language models like Flamingo support arbitrarily\ninterleaved sequences of images and text as input. This format not only enables\nfew-shot learning via interleaving independent supervised (image, text)\nexamples, but also, more complex prompts involving interaction between images,\ne.g., \"What do image A and image B have in common?\" To support this interface,\npretraining occurs over web corpora that similarly contain interleaved\nimages+text. To date, however, large-scale data of this form have not been\npublicly available.\n  We release Multimodal C4, an augmentation of the popular text-only C4 corpus\nwith images interleaved. We use a linear assignment algorithm to place images\ninto longer bodies of text using CLIP features, a process that we show\noutperforms alternatives. Multimodal C4 spans everyday topics like cooking,\ntravel, technology, etc. A manual inspection of a random sample of documents\nshows that a vast majority (88%) of images are topically relevant, and that\nlinear assignment frequently selects individual sentences specifically\nwell-aligned with each image (80%). After filtering NSFW images, ads, etc., the\nresulting corpus consists of 101.2M documents with 571M images interleaved in\n43B English tokens.", "published": "2023-04-14 06:17:46", "link": "http://arxiv.org/abs/2304.06939v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Prompt Engineering and Calibration for Zero-Shot Commonsense Reasoning", "abstract": "Prompt engineering and calibration make large language models excel at\nreasoning tasks, including multiple choice commonsense reasoning. From a\npractical perspective, we investigate and evaluate these strategies on smaller\nlanguage models. Through experiments on five commonsense reasoning benchmarks,\nwe find that each strategy favors certain models, but their joint effects are\nmostly negative.", "published": "2023-04-14 07:07:42", "link": "http://arxiv.org/abs/2304.06962v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SimpLex: a lexical text simplification architecture", "abstract": "Text simplification (TS) is the process of generating easy-to-understand\nsentences from a given sentence or piece of text. The aim of TS is to reduce\nboth the lexical (which refers to vocabulary complexity and meaning) and\nsyntactic (which refers to the sentence structure) complexity of a given text\nor sentence without the loss of meaning or nuance. In this paper, we present\n\\textsc{SimpLex}, a novel simplification architecture for generating simplified\nEnglish sentences. To generate a simplified sentence, the proposed architecture\nuses either word embeddings (i.e., Word2Vec) and perplexity, or sentence\ntransformers (i.e., BERT, RoBERTa, and GPT2) and cosine similarity. The\nsolution is incorporated into a user-friendly and simple-to-use software. We\nevaluate our system using two metrics, i.e., SARI, and Perplexity Decrease.\nExperimentally, we observe that the transformer models outperform the other\nmodels in terms of the SARI score. However, in terms of Perplexity, the\nWord-Embeddings-based models achieve the biggest decrease. Thus, the main\ncontributions of this paper are: (1) We propose a new Word Embedding and\nTransformer based algorithm for text simplification; (2) We design\n\\textsc{SimpLex} -- a modular novel text simplification system -- that can\nprovide a baseline for further research; and (3) We perform an in-depth\nanalysis of our solution and compare our results with two state-of-the-art\nmodels, i.e., LightLS [19] and NTS-w2v [44]. We also make the code publicly\navailable online.", "published": "2023-04-14 08:52:31", "link": "http://arxiv.org/abs/2304.07002v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Keeping the Questions Conversational: Using Structured Representations\n  to Resolve Dependency in Conversational Question Answering", "abstract": "Having an intelligent dialogue agent that can engage in conversational\nquestion answering (ConvQA) is now no longer limited to Sci-Fi movies only and\nhas, in fact, turned into a reality. These intelligent agents are required to\nunderstand and correctly interpret the sequential turns provided as the context\nof the given question. However, these sequential questions are sometimes left\nimplicit and thus require the resolution of some natural language phenomena\nsuch as anaphora and ellipsis. The task of question rewriting has the potential\nto address the challenges of resolving dependencies amongst the contextual\nturns by transforming them into intent-explicit questions. Nonetheless, the\nsolution of rewriting the implicit questions comes with some potential\nchallenges such as resulting in verbose questions and taking conversational\naspect out of the scenario by generating self-contained questions. In this\npaper, we propose a novel framework, CONVSR (CONVQA using Structured\nRepresentations) for capturing and generating intermediate representations as\nconversational cues to enhance the capability of the QA model to better\ninterpret the incomplete questions. We also deliberate how the strengths of\nthis task could be leveraged in a bid to design more engaging and eloquent\nconversational agents. We test our model on the QuAC and CANARD datasets and\nillustrate by experimental results that our proposed framework achieves a\nbetter F1 score than the standard question rewriting model.", "published": "2023-04-14 13:42:32", "link": "http://arxiv.org/abs/2304.07125v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Covidia: COVID-19 Interdisciplinary Academic Knowledge Graph", "abstract": "The pandemic of COVID-19 has inspired extensive works across different\nresearch fields. Existing literature and knowledge platforms on COVID-19 only\nfocus on collecting papers on biology and medicine, neglecting the\ninterdisciplinary efforts, which hurdles knowledge sharing and research\ncollaborations between fields to address the problem. Studying\ninterdisciplinary researches requires effective paper category classification\nand efficient cross-domain knowledge extraction and integration. In this work,\nwe propose Covidia, COVID-19 interdisciplinary academic knowledge graph to\nbridge the gap between knowledge of COVID-19 on different domains. We design\nframeworks based on contrastive learning for disciplinary classification, and\npropose a new academic knowledge graph scheme for entity extraction, relation\nclassification and ontology management in accordance with interdisciplinary\nresearches. Based on Covidia, we also establish knowledge discovery benchmarks\nfor finding COVID-19 research communities and predicting potential links.", "published": "2023-04-14 16:45:38", "link": "http://arxiv.org/abs/2304.07242v1", "categories": ["cs.IR", "cs.CL", "H.3.3; I.2.7"], "primary_category": "cs.IR"}
{"title": "OpenAssistant Conversations -- Democratizing Large Language Model\n  Alignment", "abstract": "Aligning large language models (LLMs) with human preferences has proven to\ndrastically improve usability and has driven rapid adoption as demonstrated by\nChatGPT. Alignment techniques such as supervised fine-tuning (SFT) and\nreinforcement learning from human feedback (RLHF) greatly reduce the required\nskill and domain knowledge to effectively harness the capabilities of LLMs,\nincreasing their accessibility and utility across various domains. However,\nstate-of-the-art alignment techniques like RLHF rely on high-quality human\nfeedback data, which is expensive to create and often remains proprietary. In\nan effort to democratize research on large-scale alignment, we release\nOpenAssistant Conversations, a human-generated, human-annotated assistant-style\nconversation corpus consisting of 161,443 messages in 35 different languages,\nannotated with 461,292 quality ratings, resulting in over 10,000 complete and\nfully annotated conversation trees. The corpus is a product of a worldwide\ncrowd-sourcing effort involving over 13,500 volunteers. Models trained on\nOpenAssistant Conversations show consistent improvements on standard benchmarks\nover respective base models. We release our code and data under a fully\npermissive licence.", "published": "2023-04-14 18:01:29", "link": "http://arxiv.org/abs/2304.07327v2", "categories": ["cs.CL", "cs.AI", "I.2"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Multi-Label Topic Inference with Sentence Encoders", "abstract": "Sentence encoders have indeed been shown to achieve superior performances for\nmany downstream text-mining tasks and, thus, claimed to be fairly general.\nInspired by this, we performed a detailed study on how to leverage these\nsentence encoders for the \"zero-shot topic inference\" task, where the topics\nare defined/provided by the users in real-time. Extensive experiments on seven\ndifferent datasets demonstrate that Sentence-BERT demonstrates superior\ngenerality compared to other encoders, while Universal Sentence Encoder can be\npreferred when efficiency is a top priority.", "published": "2023-04-14 20:27:09", "link": "http://arxiv.org/abs/2304.07382v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Stochastic Code Generation", "abstract": "Large language models pre-trained for code generation can generate\nhigh-quality short code but often struggle with generating coherent long code\nand understanding higher-level or system-level specifications. This issue is\nalso observed in language modeling for long text generation, and one proposed\nsolution is the use of a latent stochastic process. This approach involves\ngenerating a document plan and then producing text that is consistent with it.\n  In this study, we investigate whether this technique can be applied to code\ngeneration to improve coherence. We base our proposed encoder and decoder on\nthe pre-trained GPT-2 based CodeParrot model and utilize the APPS dataset for\ntraining. We evaluate our results using the HumanEval benchmark and observe\nthat the modified Time Control model performs similarly to CodeParrot on this\nevaluation.", "published": "2023-04-14 00:01:05", "link": "http://arxiv.org/abs/2304.08243v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs", "abstract": "Recent research has demonstrated that Large Language Models (LLMs) can\nenhance their capabilities by utilizing external tools. However, three pivotal\nquestions remain unanswered: (1) How effective are current LLMs in utilizing\ntools? (2) How can we enhance LLMs' ability to utilize tools? (3) What\nobstacles need to be overcome to leverage tools? To address these questions, we\nintroduce API-Bank, a groundbreaking benchmark, specifically designed for\ntool-augmented LLMs. For the first question, we develop a runnable evaluation\nsystem consisting of 73 API tools. We annotate 314 tool-use dialogues with 753\nAPI calls to assess the existing LLMs' capabilities in planning, retrieving,\nand calling APIs. For the second question, we construct a comprehensive\ntraining set containing 1,888 tool-use dialogues from 2,138 APIs spanning 1,000\ndistinct domains. Using this dataset, we train Lynx, a tool-augmented LLM\ninitialized from Alpaca. Experimental results demonstrate that GPT-3.5 exhibits\nimproved tool utilization compared to GPT-3, while GPT-4 excels in planning.\nHowever, there is still significant potential for further improvement.\nMoreover, Lynx surpasses Alpaca's tool utilization performance by more than 26\npts and approaches the effectiveness of GPT-3.5. Through error analysis, we\nhighlight the key challenges for future research in this field to answer the\nthird question.", "published": "2023-04-14 14:05:32", "link": "http://arxiv.org/abs/2304.08244v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MedAlpaca -- An Open-Source Collection of Medical Conversational AI\n  Models and Training Data", "abstract": "As large language models (LLMs) like OpenAI's GPT series continue to make\nstrides, we witness the emergence of artificial intelligence applications in an\never-expanding range of fields. In medicine, these LLMs hold considerable\npromise for improving medical workflows, diagnostics, patient care, and\neducation. Yet, there is an urgent need for open-source models that can be\ndeployed on-premises to safeguard patient privacy. In our work, we present an\ninnovative dataset consisting of over 160,000 entries, specifically crafted to\nfine-tune LLMs for effective medical applications. We investigate the impact of\nfine-tuning these datasets on publicly accessible pre-trained LLMs, and\nsubsequently, we juxtapose the performance of pre-trained-only models against\nthe fine-tuned models concerning the examinations that future medical doctors\nmust pass to achieve certification.", "published": "2023-04-14 11:28:08", "link": "http://arxiv.org/abs/2304.08247v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ChatGPT: Applications, Opportunities, and Threats", "abstract": "Developed by OpenAI, ChatGPT (Conditional Generative Pre-trained Transformer)\nis an artificial intelligence technology that is fine-tuned using supervised\nmachine learning and reinforcement learning techniques, allowing a computer to\ngenerate natural language conversation fully autonomously. ChatGPT is built on\nthe transformer architecture and trained on millions of conversations from\nvarious sources. The system combines the power of pre-trained deep learning\nmodels with a programmability layer to provide a strong base for generating\nnatural language conversations. In this study, after reviewing the existing\nliterature, we examine the applications, opportunities, and threats of ChatGPT\nin 10 main domains, providing detailed examples for the business and industry\nas well as education. We also conducted an experimental study, checking the\neffectiveness and comparing the performances of GPT-3.5 and GPT-4, and found\nthat the latter performs significantly better. Despite its exceptional ability\nto generate natural-sounding responses, the authors believe that ChatGPT does\nnot possess the same level of understanding, empathy, and creativity as a human\nand cannot fully replace them in most situations.", "published": "2023-04-14 16:25:03", "link": "http://arxiv.org/abs/2304.09103v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "HCAM -- Hierarchical Cross Attention Model for Multi-modal Emotion\n  Recognition", "abstract": "Emotion recognition in conversations is challenging due to the multi-modal\nnature of the emotion expression. We propose a hierarchical cross-attention\nmodel (HCAM) approach to multi-modal emotion recognition using a combination of\nrecurrent and co-attention neural network models. The input to the model\nconsists of two modalities, i) audio data, processed through a learnable\nwav2vec approach and, ii) text data represented using a bidirectional encoder\nrepresentations from transformers (BERT) model. The audio and text\nrepresentations are processed using a set of bi-directional recurrent neural\nnetwork layers with self-attention that converts each utterance in a given\nconversation to a fixed dimensional embedding. In order to incorporate\ncontextual knowledge and the information across the two modalities, the audio\nand text embeddings are combined using a co-attention layer that attempts to\nweigh the utterance level embeddings relevant to the task of emotion\nrecognition. The neural network parameters in the audio layers, text layers as\nwell as the multi-modal co-attention layers, are hierarchically trained for the\nemotion classification task. We perform experiments on three established\ndatasets namely, IEMOCAP, MELD and CMU-MOSI, where we illustrate that the\nproposed model improves significantly over other benchmarks and helps achieve\nstate-of-art results on all these datasets.", "published": "2023-04-14 03:25:00", "link": "http://arxiv.org/abs/2304.06910v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SEA: A Scalable Entity Alignment System", "abstract": "Entity alignment (EA) aims to find equivalent entities in different knowledge\ngraphs (KGs). State-of-the-art EA approaches generally use Graph Neural\nNetworks (GNNs) to encode entities. However, most of them train the models and\nevaluate the results in a fullbatch fashion, which prohibits EA from being\nscalable on largescale datasets. To enhance the usability of GNN-based EA\nmodels in real-world applications, we present SEA, a scalable entity alignment\nsystem that enables to (i) train large-scale GNNs for EA, (ii) speed up the\nnormalization and the evaluation process, and (iii) report clear results for\nusers to estimate different models and parameter settings. SEA can be run on a\ncomputer with merely one graphic card. Moreover, SEA encompasses six\nstate-of-the-art EA models and provides access for users to quickly establish\nand evaluate their own models. Thus, SEA allows users to perform EA without\nbeing involved in tedious implementations, such as negative sampling and\nGPU-accelerated evaluation. With SEA, users can gain a clear view of the model\nperformance. In the demonstration, we show that SEA is user-friendly and is of\nhigh scalability even on computers with limited computational resources.", "published": "2023-04-14 11:39:07", "link": "http://arxiv.org/abs/2304.07065v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Task-oriented Document-Grounded Dialog Systems by HLTPR@RWTH for DSTC9\n  and DSTC10", "abstract": "This paper summarizes our contributions to the document-grounded dialog tasks\nat the 9th and 10th Dialog System Technology Challenges (DSTC9 and DSTC10). In\nboth iterations the task consists of three subtasks: first detect whether the\ncurrent turn is knowledge seeking, second select a relevant knowledge document,\nand third generate a response grounded on the selected document. For DSTC9 we\nproposed different approaches to make the selection task more efficient. The\nbest method, Hierarchical Selection, actually improves the results compared to\nthe original baseline and gives a speedup of 24x. In the DSTC10 iteration of\nthe task, the challenge was to adapt systems trained on written dialogs to\nperform well on noisy automatic speech recognition transcripts. Therefore, we\nproposed data augmentation techniques to increase the robustness of the models\nas well as methods to adapt the style of generated responses to fit well into\nthe proceeding dialog. Additionally, we proposed a noisy channel model that\nallows for increasing the factuality of the generated responses. In addition to\nsummarizing our previous contributions, in this work, we also report on a few\nsmall improvements and reconsider the automatic evaluation metrics for the\ngeneration task which have shown a low correlation to human judgments.", "published": "2023-04-14 12:46:29", "link": "http://arxiv.org/abs/2304.07101v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Just Tell Me: Prompt Engineering in Business Process Management", "abstract": "GPT-3 and several other language models (LMs) can effectively address various\nnatural language processing (NLP) tasks, including machine translation and text\nsummarization. Recently, they have also been successfully employed in the\nbusiness process management (BPM) domain, e.g., for predictive process\nmonitoring and process extraction from text. This, however, typically requires\nfine-tuning the employed LM, which, among others, necessitates large amounts of\nsuitable training data. A possible solution to this problem is the use of\nprompt engineering, which leverages pre-trained LMs without fine-tuning them.\nRecognizing this, we argue that prompt engineering can help bring the\ncapabilities of LMs to BPM research. We use this position paper to develop a\nresearch agenda for the use of prompt engineering for BPM research by\nidentifying the associated potentials and challenges.", "published": "2023-04-14 14:55:19", "link": "http://arxiv.org/abs/2304.07183v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Mapping of attention mechanisms to a generalized Potts model", "abstract": "Transformers are neural networks that revolutionized natural language\nprocessing and machine learning. They process sequences of inputs, like words,\nusing a mechanism called self-attention, which is trained via masked language\nmodeling (MLM). In MLM, a word is randomly masked in an input sequence, and the\nnetwork is trained to predict the missing word. Despite the practical success\nof transformers, it remains unclear what type of data distribution\nself-attention can learn efficiently. Here, we show analytically that if one\ndecouples the treatment of word positions and embeddings, a single layer of\nself-attention learns the conditionals of a generalized Potts model with\ninteractions between sites and Potts colors. Moreover, we show that training\nthis neural network is exactly equivalent to solving the inverse Potts problem\nby the so-called pseudo-likelihood method, well known in statistical physics.\nUsing this mapping, we compute the generalization error of self-attention in a\nmodel scenario analytically using the replica method.", "published": "2023-04-14 16:32:56", "link": "http://arxiv.org/abs/2304.07235v4", "categories": ["cond-mat.dis-nn", "cond-mat.stat-mech", "cs.CL", "stat.ML"], "primary_category": "cond-mat.dis-nn"}
{"title": "Learn What Is Possible, Then Choose What Is Best: Disentangling\n  One-To-Many Relations in Language Through Text-based Games", "abstract": "Language models pre-trained on large self-supervised corpora, followed by\ntask-specific fine-tuning has become the dominant paradigm in NLP. These\npre-training datasets often have a one-to-many structure--e.g. in dialogue\nthere are many valid responses for a given context. However, only some of these\nresponses will be desirable in our downstream task. This raises the question of\nhow we should train the model such that it can emulate the desirable\nbehaviours, but not the undesirable ones. Current approaches train in a\none-to-one setup--only a single target response is given for a single dialogue\ncontext--leading to models only learning to predict the average response, while\nignoring the full range of possible responses. Using text-based games as a\ntestbed, our approach, PASA, uses discrete latent variables to capture the\nrange of different behaviours represented in our larger pre-training dataset.\nWe then use knowledge distillation to distil the posterior probability\ndistribution into a student model. This probability distribution is far richer\nthan learning from only the hard targets of the dataset, and thus allows the\nstudent model to benefit from the richer range of actions the teacher model has\nlearned. Results show up to 49% empirical improvement over the previous\nstate-of-the-art model on the Jericho Walkthroughs dataset.", "published": "2023-04-14 17:11:26", "link": "http://arxiv.org/abs/2304.07258v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Self-Perception and Political Biases of ChatGPT", "abstract": "This contribution analyzes the self-perception and political biases of\nOpenAI's Large Language Model ChatGPT. Taking into account the first\nsmall-scale reports and studies that have emerged, claiming that ChatGPT is\npolitically biased towards progressive and libertarian points of view, this\ncontribution aims to provide further clarity on this subject. For this purpose,\nChatGPT was asked to answer the questions posed by the political compass test\nas well as similar questionnaires that are specific to the respective politics\nof the G7 member states. These eight tests were repeated ten times each and\nrevealed that ChatGPT seems to hold a bias towards progressive views. The\npolitical compass test revealed a bias towards progressive and libertarian\nviews, with the average coordinates on the political compass being (-6.48,\n-5.99) (with (0, 0) the center of the compass, i.e., centrism and the axes\nranging from -10 to 10), supporting the claims of prior research. The political\nquestionnaires for the G7 member states indicated a bias towards progressive\nviews but no significant bias between authoritarian and libertarian views,\ncontradicting the findings of prior reports, with the average coordinates being\n(-3.27, 0.58). In addition, ChatGPT's Big Five personality traits were tested\nusing the OCEAN test and its personality type was queried using the\nMyers-Briggs Type Indicator (MBTI) test. Finally, the maliciousness of ChatGPT\nwas evaluated using the Dark Factor test. These three tests were also repeated\nten times each, revealing that ChatGPT perceives itself as highly open and\nagreeable, has the Myers-Briggs personality type ENFJ, and is among the 15% of\ntest-takers with the least pronounced dark traits.", "published": "2023-04-14 18:06:13", "link": "http://arxiv.org/abs/2304.07333v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.CY"}
{"title": "Improving Patient Pre-screening for Clinical Trials: Assisting\n  Physicians with Large Language Models", "abstract": "Physicians considering clinical trials for their patients are met with the\nlaborious process of checking many text based eligibility criteria. Large\nLanguage Models (LLMs) have shown to perform well for clinical information\nextraction and clinical reasoning, including medical tests, but not yet in\nreal-world scenarios. This paper investigates the use of InstructGPT to assist\nphysicians in determining eligibility for clinical trials based on a patient's\nsummarised medical profile. Using a prompting strategy combining one-shot,\nselection-inference and chain-of-thought techniques, we investigate the\nperformance of LLMs on 10 synthetically created patient profiles. Performance\nis evaluated at four levels: ability to identify screenable eligibility\ncriteria from a trial given a medical profile; ability to classify for each\nindividual criterion whether the patient qualifies; the overall classification\nwhether a patient is eligible for a clinical trial and the percentage of\ncriteria to be screened by physician. We evaluated against 146 clinical trials\nand a total of 4,135 eligibility criteria. The LLM was able to correctly\nidentify the screenability of 72% (2,994/4,135) of the criteria. Additionally,\n72% (341/471) of the screenable criteria were evaluated correctly. The\nresulting trial level classification as eligible or ineligible resulted in a\nrecall of 0.5. By leveraging LLMs with a physician-in-the-loop, a recall of 1.0\nand precision of 0.71 on clinical trial level can be achieved while reducing\nthe amount of criteria to be checked by an estimated 90%. LLMs can be used to\nassist physicians with pre-screening of patients for clinical trials. By\nforcing instruction-tuned LLMs to produce chain-of-thought responses, the\nreasoning can be made transparent to and the decision process becomes amenable\nby physicians, thereby making such a system feasible for use in real-world\nscenarios.", "published": "2023-04-14 21:19:46", "link": "http://arxiv.org/abs/2304.07396v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.1; I.2.7"], "primary_category": "cs.LG"}
{"title": "The Deep Latent Position Topic Model for Clustering and Representation\n  of Networks with Textual Edges", "abstract": "Numerical interactions leading to users sharing textual content published by\nothers are naturally represented by a network where the individuals are\nassociated with the nodes and the exchanged texts with the edges. To understand\nthose heterogeneous and complex data structures, clustering nodes into\nhomogeneous groups as well as rendering a comprehensible visualisation of the\ndata is mandatory. To address both issues, we introduce Deep-LPTM, a\nmodel-based clustering strategy relying on a variational graph auto-encoder\napproach as well as a probabilistic model to characterise the topics of\ndiscussion. Deep-LPTM allows to build a joint representation of the nodes and\nof the edges in two embeddings spaces. The parameters are inferred using a\nvariational inference algorithm. We also introduce IC2L, a model selection\ncriterion specifically designed to choose models with relevant clustering and\nvisualisation properties. An extensive benchmark study on synthetic data is\nprovided. In particular, we find that Deep-LPTM better recovers the partitions\nof the nodes than the state-of-the art ETSBM and STBM. Eventually, the emails\nof the Enron company are analysed and visualisations of the results are\npresented, with meaningful highlights of the graph structure.", "published": "2023-04-14 07:01:57", "link": "http://arxiv.org/abs/2304.08242v3", "categories": ["cs.LG", "cs.CL", "cs.SI", "stat.ME"], "primary_category": "cs.LG"}
{"title": "Sound in occupied open-plan offices: Objective metrics with a review of\n  historical perspectives", "abstract": "Open-plan offices (OPOs) have been around for more than half a century now,\nchronicling the vicissitudes of workplace topography amongst other factors.\nThis paper addresses one such factor - the sound environment in occupied OPOs\nin relation to several objective workplace parameters, using measurements in\ncontemporary OPOs and comparisons with studies over the last 50 years.\nOmnidirectional and binaural sound measurements were conducted in 43 offices\nduring typical working hours. The results describe variation in several\nacoustic and psychoacoustic metrics, and present statistical models that\npredict these metrics as a function of the number of workstations in offices.\nLA,eq of 53.6 dB is typical for occupied OPOs, with spectral slope of\napproximately -4 dB/octave. LA,eq values do not vary much over the workplace\nparameters studied (e.g., floor plate area, work activity, etc), except for\n-2.7 dB and -4.1 dB differences between offices with/without carpeting, and\noffices with ceiling absorption but with/without carpeting, respectively; most\nlikely from reduced floor impact noise leading to speech level reduction. Sound\nfluctuation, as characterised by the metric Noise Climate (NCl: LA10 - LA90)\nand the psychoacoustic Fluctuation Strength (FS), decreases significantly with\nincreasing number of workstations in OPOs. This suggests lesser auditory\ndistraction in larger offices, which needs further investigation. In terms of\nhistorical trends, OPOs have become quieter over the years, especially\nbackground noise quantified as LA90, although there are several subtleties.\nOverall, current findings can inform several OPO design perspectives including\npolicy documents, provide values for laboratory simulations of OPO acoustic\nenvironments, help interpret subjective impressions of OPO occupants, etc.", "published": "2023-04-14 16:47:50", "link": "http://arxiv.org/abs/2305.01762v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Adapting Meter Tracking Models to Latin American Music", "abstract": "Beat and downbeat tracking models have improved significantly in recent years\nwith the introduction of deep learning methods. However, despite these\nimprovements, several challenges remain. Particularly, the adaptation of\navailable models to underrepresented music traditions in MIR is usually\nsynonymous with collecting and annotating large amounts of data, which is\nimpractical and time-consuming. Transfer learning, data augmentation, and\nfine-tuning techniques have been used quite successfully in related tasks and\nare known to alleviate this bottleneck. Furthermore, when studying these music\ntraditions, models are not required to generalize to multiple mainstream music\ngenres but to perform well in more constrained, homogeneous conditions. In this\nwork, we investigate simple yet effective strategies to adapt beat and downbeat\ntracking models to two different Latin American music traditions and analyze\nthe feasibility of these adaptations in real-world applications concerning the\ndata and computational requirements. Contrary to common belief, our findings\nshow it is possible to achieve good performance by spending just a few minutes\nannotating a portion of the data and training a model in a standard CPU\nmachine, with the precise amount of resources needed depending on the task and\nthe complexity of the dataset.", "published": "2023-04-14 14:57:40", "link": "http://arxiv.org/abs/2304.07186v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Tempo vs. Pitch: understanding self-supervised tempo estimation", "abstract": "Self-supervision methods learn representations by solving pretext tasks that\ndo not require human-generated labels, alleviating the need for time-consuming\nannotations. These methods have been applied in computer vision, natural\nlanguage processing, environmental sound analysis, and recently in music\ninformation retrieval, e.g. for pitch estimation. Particularly in the context\nof music, there are few insights about the fragility of these models regarding\ndifferent distributions of data, and how they could be mitigated. In this\npaper, we explore these questions by dissecting a self-supervised model for\npitch estimation adapted for tempo estimation via rigorous experimentation with\nsynthetic data. Specifically, we study the relationship between the input\nrepresentation and data distribution for self-supervised tempo estimation.", "published": "2023-04-14 00:08:08", "link": "http://arxiv.org/abs/2304.06868v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Using a one-dimensional finite-element approximation of Webster's horn\n  equation to estimate individual ear canal acoustic transfer from input\n  impedances", "abstract": "In many applications, knowledge of the sound pressure transfer to the eardrum\nis important. The transfer is highly influenced by the shape of the ear canal\nand its acoustic properties, such as the acoustic impedance at the eardrum.\nInvasive procedures to measure the sound pressure at the eardrum are usually\nelaborate or costly. In this work, we propose a numerical method to estimate\nthe transfer impedance at the eardrum given only input impedance measurements\nat the ear canal entrance by using one-dimensional first-order finite elements\nand Nelder-Mead optimization algorithm. Estimations on the area function of the\near canal and the acoustic impedance at the eardrum are achieved. Results are\nvalidated through numerical simulations on ten different ear canal geometries\nand three different acoustic impedances at the eardrum using synthetically\ngenerated data from three-dimensional finite element simulations.", "published": "2023-04-14 13:49:53", "link": "http://arxiv.org/abs/2304.07131v1", "categories": ["math.OC", "cs.NA", "eess.AS", "math.NA"], "primary_category": "math.OC"}
{"title": "1-D Residual Convolutional Neural Network coupled with Data Augmentation\n  and Regularization for the ICPHM 2023 Data Challenge", "abstract": "In this article, we present our contribution to the ICPHM 2023 Data Challenge\non Industrial Systems' Health Monitoring using Vibration Analysis. For the task\nof classifying sun gear faults in a gearbox, we propose a residual\nConvolutional Neural Network that operates on raw three-channel time-domain\nvibration signals. In conjunction with data augmentation and regularization\ntechniques, the proposed model yields very good results in a multi-class\nclassification scenario with real-world data despite its relatively small size,\ni.e., with less than 30,000 trainable parameters. Even when presented with data\nobtained from multiple operating conditions, the network is still capable to\naccurately predict the condition of the gearbox under inspection.", "published": "2023-04-14 08:50:21", "link": "http://arxiv.org/abs/2304.07305v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Airborne Sound Analysis for the Detection of Bearing Faults in Railway\n  Vehicles with Real-World Data", "abstract": "In this paper, we address the challenging problem of detecting bearing faults\nin railway vehicles by analyzing acoustic signals recorded during regular\noperation. For this, we introduce Mel Frequency Cepstral Coefficients (MFCCs)\nas features, which form the input to a simple Multi-Layer Perceptron\nclassifier. The proposed method is evaluated with real-world data that was\nobtained for state-of-the-art commuter railway vehicles in a measurement\ncampaign. The experiments show that with the chosen MFCC features bearing\nfaults can be reliably detected even for bearing damages that were not included\nin training.", "published": "2023-04-14 09:35:20", "link": "http://arxiv.org/abs/2304.07307v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Novel features for the detection of bearing faults in railway vehicles", "abstract": "{In this paper, we address the challenging problem of detecting bearing\nfaults from vibration signals. For this, several time- and frequency-domain\nfeatures have been proposed already in the past. However, these features are\nusually evaluated on data originating from relatively simple scenarios and a\nsignificant performance loss can be observed if more realistic scenarios are\nconsidered. To overcome this, we introduce Mel-Frequency Cepstral Coefficients\n(MFCCs) and features extracted from the Amplitude Modulation Spectrogram (AMS)\nas features for the detection of bearing faults. Both AMS and MFCCs were\noriginally introduced in the context of audio signal processing but it is\ndemonstrated that a significantly improved classification performance can be\nobtained by using these features. Furthermore, to tackle the characteristic\ndata imbalance problem in the context of bearing fault detection, i.e.,\ntypically much more data from healthy bearings than from damaged bearings is\navailable, we propose to train a One-class \\ac{SVM} with data from healthy\nbearings only. Bearing faults are then classified by the detection of outliers.\nOur approach is evaluated with data measured in a highly challenging scenario\ncomprising a state-of-the-art commuter railway engine which is supplied by an\nindustrial power converter and coupled to a load machine.", "published": "2023-04-14 10:09:50", "link": "http://arxiv.org/abs/2304.08249v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "On Data Sampling Strategies for Training Neural Network Speech\n  Separation Models", "abstract": "Speech separation remains an important area of multi-speaker signal\nprocessing. Deep neural network (DNN) models have attained the best performance\non many speech separation benchmarks. Some of these models can take significant\ntime to train and have high memory requirements. Previous work has proposed\nshortening training examples to address these issues but the impact of this on\nmodel performance is not yet well understood. In this work, the impact of\napplying these training signal length (TSL) limits is analysed for two speech\nseparation models: SepFormer, a transformer model, and Conv-TasNet, a\nconvolutional model. The WJS0-2Mix, WHAMR and Libri2Mix datasets are analysed\nin terms of signal length distribution and its impact on training efficiency.\nIt is demonstrated that, for specific distributions, applying specific TSL\nlimits results in better performance. This is shown to be mainly due to\nrandomly sampling the start index of the waveforms resulting in more unique\nexamples for training. A SepFormer model trained using a TSL limit of 4.42s and\ndynamic mixing (DM) is shown to match the best-performing SepFormer model\ntrained with DM and unlimited signal lengths. Furthermore, the 4.42s TSL limit\nresults in a 44% reduction in training time with WHAMR.", "published": "2023-04-14 14:05:52", "link": "http://arxiv.org/abs/2304.07142v2", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.NE", "eess.AS"], "primary_category": "cs.SD"}
