{"title": "Risk-Adjusted Performance of Random Forest Models in High-Frequency Trading", "abstract": "Because of the theoretical challenges posed by the Efficient Market\nHypothesis to technical analysis, the effectiveness of technical indicators in\nhigh-frequency trading remains inadequately explored, particularly at the\nminute-level frequency, where effects of the microstructure of the market\ndominate. This study evaluates the integration of traditional technical\nindicators with random forest regression models using minute-level SPY data,\nanalyzing 13 distinct model configurations. Our empirical results reveal a\nstark contrast between in-sample and out-of-sample performance, with $R^2$\nvalues deteriorating from 0.749--0.812 during training to negative values in\ntesting. A feature importance analysis demonstrates that primary price-based\nfeatures dominate the predictions made by the model, accounting for over 60% of\nthe importance, while established technical indicators, such as RSI and\nBollinger Bands, account for only 14%--15%. Although the indicator-enhanced\nmodels achieved superior risk-adjusted metrics, with Rachev ratios between\n0.919 and 0.961, they consistently underperformed a simple buy-and-hold\nstrategy, generating returns ranging from -2.4% to -3.9%. These findings\nchallenge conventional assumptions about the usefulness of technical indicators\nin algorithmic trading, suggesting that in high-frequency contexts, they may be\nmore relevant to risk management rather than to predicting returns. For\npractitioners and researchers, our findings indicate that successful\nhigh-frequency trading strategies should focus on adaptive feature selection\nand regime-specific modeling rather than relying on traditional technical\nindicators, as well as indicating the critical importance of robust\nout-of-sample testing in the development of a model.", "published": "2024-12-19 23:02:02", "link": "http://arxiv.org/abs/2412.15448v2", "categories": ["q-fin.CP", "q-fin.RM"], "primary_category": "q-fin.CP"}
{"title": "Option Pricing with a Compound CARMA(p,q)-Hawkes", "abstract": "A self-exciting point process with a continuous-time autoregressive moving\naverage intensity process, named CARMA(p,q)-Hawkes model, has recently been\nintroduced. The model generalizes the Hawkes process by substituting the\nOrnstein-Uhlenbeck intensity with a CARMA(p,q) model where the associated state\nprocess is driven by the counting process itself. The proposed model preserves\nthe same degree of tractability as the Hawkes process, but it can reproduce\nmore complex time-dependent structures observed in several market data. The\npaper presents a new model of asset price dynamics based on the CARMA(p,q)\nHawkes model. It is constructed using a compound version of it with a random\njump size that is independent of both the counting and the intensity processes\nand can be employed as the main block for pure jump and (stochastic volatility)\njump-diffusion processes. The numerical results for pricing European options\nillustrate that the new model can replicate the volatility smile observed in\nfinancial markets. Through an empirical analysis, which is presented as a\ncalibration exercise, we highlight the role of higher order autoregressive and\nmoving average parameters in pricing options.", "published": "2024-12-19 18:47:52", "link": "http://arxiv.org/abs/2412.15172v1", "categories": ["q-fin.MF"], "primary_category": "q-fin.MF"}
{"title": "A Comparative Study of DSPy Teleprompter Algorithms for Aligning Large Language Models Evaluation Metrics to Human Evaluation", "abstract": "We argue that the Declarative Self-improving Python (DSPy) optimizers are a\nway to align the large language model (LLM) prompts and their evaluations to\nthe human annotations. We present a comparative analysis of five teleprompter\nalgorithms, namely, Cooperative Prompt Optimization (COPRO), Multi-Stage\nInstruction Prompt Optimization (MIPRO), BootstrapFewShot, BootstrapFewShot\nwith Optuna, and K-Nearest Neighbor Few Shot, within the DSPy framework with\nrespect to their ability to align with human evaluations. As a concrete\nexample, we focus on optimizing the prompt to align hallucination detection\n(using LLM as a judge) to human annotated ground truth labels for a publicly\navailable benchmark dataset. Our experiments demonstrate that optimized prompts\ncan outperform various benchmark methods to detect hallucination, and certain\ntelemprompters outperform the others in at least these experiments.", "published": "2024-12-19 10:38:46", "link": "http://arxiv.org/abs/2412.15298v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "q-fin.ST", "stat.ME"], "primary_category": "cs.CL"}
{"title": "Leveraging Time Series Categorization and Temporal Fusion Transformers to Improve Cryptocurrency Price Forecasting", "abstract": "Organizing and managing cryptocurrency portfolios and decision-making on\ntransactions is crucial in this market. Optimal selection of assets is one of\nthe main challenges that requires accurate prediction of the price of\ncryptocurrencies. In this work, we categorize the financial time series into\nseveral similar subseries to increase prediction accuracy by learning each\nsubseries category with similar behavior. For each category of the subseries,\nwe create a deep learning model based on the attention mechanism to predict the\nnext step of each subseries. Due to the limited amount of cryptocurrency data\nfor training models, if the number of categories increases, the amount of\ntraining data for each model will decrease, and some complex models will not be\ntrained well due to the large number of parameters. To overcome this challenge,\nwe propose to combine the time series data of other cryptocurrencies to\nincrease the amount of data for each category, hence increasing the accuracy of\nthe models corresponding to each category.", "published": "2024-12-19 04:57:55", "link": "http://arxiv.org/abs/2412.14529v1", "categories": ["cs.LG", "cs.CE", "q-fin.ST"], "primary_category": "cs.LG"}
{"title": "From Human Annotation to LLMs: SILICON Annotation Workflow for\n  Management Research", "abstract": "Unstructured text data annotation and analysis are fundamental to management\nresearch, often relying on human annotators through crowdsourcing platforms.\nWhile Large Language Models (LLMs) promise to provide a cost-effective and\nefficient alternative to human annotation, there lacks a systematic workflow\nthat evaluate when LLMs are suitable or how to proceed with LLM-based text\nannotation in a reproducible manner. This paper addresses this methodological\ngap by introducing the ``SILICON\" (Systematic Inference with LLMs for\nInformation Classification and Notation) workflow. The workflow integrates\nestablished principles of human annotation with systematic prompt optimization\nand model selection, addressing challenges such as developing robust annotation\nguidelines, establishing high-quality human baselines, optimizing prompts, and\nensuring reproducibility across LLMs. We validate the SILICON workflow through\nseven case studies covering common management research tasks. Our findings\nhighlight the importance of validating annotation guideline agreement, the\nsuperiority of expert-developed human baselines over crowdsourced ones, the\niterative nature of prompt optimization, and the necessity of testing multiple\nLLMs. We also find that LLMs agree well with expert annotations in most cases\nbut show low agreement in more complex multi-label classification tasks.\nNotably, we propose a regression-based methodology to empirically compare LLM\noutputs across prompts and models. Our workflow advances management research by\nestablishing rigorous, transparent, and reproducible processes for LLM-based\nannotation. We provide practical guidance for researchers to effectively\nnavigate the evolving landscape of generative AI tools.", "published": "2024-12-19 02:21:41", "link": "http://arxiv.org/abs/2412.14461v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Agent-SafetyBench: Evaluating the Safety of LLM Agents", "abstract": "As large language models (LLMs) are increasingly deployed as agents, their\nintegration into interactive environments and tool use introduce new safety\nchallenges beyond those associated with the models themselves. However, the\nabsence of comprehensive benchmarks for evaluating agent safety presents a\nsignificant barrier to effective assessment and further improvement. In this\npaper, we introduce Agent-SafetyBench, a comprehensive benchmark designed to\nevaluate the safety of LLM agents. Agent-SafetyBench encompasses 349\ninteraction environments and 2,000 test cases, evaluating 8 categories of\nsafety risks and covering 10 common failure modes frequently encountered in\nunsafe interactions. Our evaluation of 16 popular LLM agents reveals a\nconcerning result: none of the agents achieves a safety score above 60%. This\nhighlights significant safety challenges in LLM agents and underscores the\nconsiderable need for improvement. Through quantitative analysis, we identify\ncritical failure modes and summarize two fundamental safety detects in current\nLLM agents: lack of robustness and lack of risk awareness. Furthermore, our\nfindings suggest that reliance on defense prompts alone is insufficient to\naddress these safety issues, emphasizing the need for more advanced and robust\nstrategies. We release Agent-SafetyBench at\n\\url{https://github.com/thu-coai/Agent-SafetyBench} to facilitate further\nresearch and innovation in agent safety evaluation and improvement.", "published": "2024-12-19 02:35:15", "link": "http://arxiv.org/abs/2412.14470v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Why We Build Local Large Language Models: An Observational Analysis from\n  35 Japanese and Multilingual LLMs", "abstract": "Why do we build local large language models (LLMs)? What should a local LLM\nlearn from the target language? Which abilities can be transferred from other\nlanguages? Do language-specific scaling laws exist? To explore these research\nquestions, we evaluated 35 Japanese, English, and multilingual LLMs on 19\nevaluation benchmarks for Japanese and English, taking Japanese as a local\nlanguage. Adopting an observational approach, we analyzed correlations of\nbenchmark scores, and conducted principal component analysis (PCA) on the\nscores to derive \\textit{ability factors} of local LLMs. We found that training\non English text can improve the scores of academic subjects in Japanese\n(JMMLU). In addition, it is unnecessary to specifically train on Japanese text\nto enhance abilities for solving Japanese code generation, arithmetic\nreasoning, commonsense, and reading comprehension tasks. In contrast, training\non Japanese text could improve question-answering tasks about Japanese\nknowledge and English-Japanese translation, which indicates that abilities for\nsolving these two tasks can be regarded as \\textit{Japanese abilities} for\nLLMs. Furthermore, we confirmed that the Japanese abilities scale with the\ncomputational budget for Japanese text.", "published": "2024-12-19 02:39:26", "link": "http://arxiv.org/abs/2412.14471v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do Large Language Models Defend Inferentialist Semantics?: On the\n  Logical Expressivism and Anti-Representationalism of LLMs", "abstract": "The philosophy of language, which has historically been developed through an\nanthropocentric lens, is now being forced to move towards post-anthropocentrism\ndue to the advent of large language models (LLMs) like ChatGPT (OpenAI), Claude\n(Anthropic), which are considered to possess linguistic abilities comparable to\nthose of humans. Traditionally, LLMs have been explained through distributional\nsemantics as their foundational semantics. However, recent research is\nexploring alternative foundational semantics beyond distributional semantics.\nThis paper proposes Robert Brandom's inferentialist semantics as an suitable\nfoundational semantics for LLMs, specifically focusing on the issue of\nlinguistic representationalism within this post-anthropocentric trend. Here, we\nshow that the anti-representationalism and logical expressivism of inferential\nsemantics, as well as quasi-compositionality, are useful in interpreting the\ncharacteristics and behaviors of LLMs. Further, we propose a \\emph{consensus\ntheory of truths} for LLMs. This paper argues that the characteristics of LLMs\nchallenge mainstream assumptions in philosophy of language, such as semantic\nexternalism and compositionality. We believe the argument in this paper leads\nto a re-evaluation of anti\\hyphen{}representationalist views of language,\npotentially leading to new developments in the philosophy of language.", "published": "2024-12-19 03:48:40", "link": "http://arxiv.org/abs/2412.14501v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Level Optimal Transport for Universal Cross-Tokenizer Knowledge\n  Distillation on Language Models", "abstract": "Knowledge distillation (KD) has become a prevalent technique for compressing\nlarge language models (LLMs). Existing KD methods are constrained by the need\nfor identical tokenizers (i.e., vocabularies) between teacher and student\nmodels, limiting their versatility in handling LLMs of different architecture\nfamilies. In this paper, we introduce the Multi-Level Optimal Transport\n(MultiLevelOT), a novel approach that advances the optimal transport for\nuniversal cross-tokenizer knowledge distillation. Our method aligns the logit\ndistributions of the teacher and the student at both token and sequence levels\nusing diverse cost matrices, eliminating the need for dimensional or\ntoken-by-token correspondence. At the token level, MultiLevelOT integrates both\nglobal and local information by jointly optimizing all tokens within a sequence\nto enhance robustness. At the sequence level, we efficiently capture complex\ndistribution structures of logits via the Sinkhorn distance, which approximates\nthe Wasserstein distance for divergence measures. Extensive experiments on\ntasks such as extractive QA, generative QA, and summarization demonstrate that\nthe MultiLevelOT outperforms state-of-the-art cross-tokenizer KD methods under\nvarious settings. Our approach is robust to different student and teacher\nmodels across model families, architectures, and parameter sizes. Codes and\nmodels are available at https://github.com/2018cx/Multi-Level-OT.", "published": "2024-12-19 04:51:06", "link": "http://arxiv.org/abs/2412.14528v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ClusterTalk: Corpus Exploration Framework using Multi-Dimensional\n  Exploratory Search", "abstract": "Exploratory search of large text corpora is essential in domains like\nbiomedical research, where large amounts of research literature are\ncontinuously generated. This paper presents ClusterTalk (The demo video and\nsource code are available at: https://github.com/achouhan93/ClusterTalk), a\nframework for corpus exploration using multi-dimensional exploratory search.\nOur system integrates document clustering with faceted search, allowing users\nto interactively refine their exploration and ask corpus and document-level\nqueries. Compared to traditional one-dimensional search approaches like keyword\nsearch or clustering, this system improves the discoverability of information\nby encouraging a deeper interaction with the corpus. We demonstrate the\nfunctionality of the ClusterTalk framework based on four million PubMed\nabstracts for the four-year time frame.", "published": "2024-12-19 05:11:16", "link": "http://arxiv.org/abs/2412.14533v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CitaLaw: Enhancing LLM with Citations in Legal Domain", "abstract": "In this paper, we propose CitaLaw, the first benchmark designed to evaluate\nLLMs' ability to produce legally sound responses with appropriate citations.\nCitaLaw features a diverse set of legal questions for both laypersons and\npractitioners, paired with a comprehensive corpus of law articles and precedent\ncases as a reference pool. This framework enables LLM-based systems to retrieve\nsupporting citations from the reference corpus and align these citations with\nthe corresponding sentences in their responses. Moreover, we introduce\nsyllogism-inspired evaluation methods to assess the legal alignment between\nretrieved references and LLM-generated responses, as well as their consistency\nwith user questions. Extensive experiments on 2 open-domain and 7\nlegal-specific LLMs demonstrate that integrating legal references substantially\nenhances response quality. Furthermore, our proposed syllogism-based evaluation\nmethod exhibits strong agreement with human judgments.", "published": "2024-12-19 06:14:20", "link": "http://arxiv.org/abs/2412.14556v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CORD: Balancing COnsistency and Rank Distillation for Robust\n  Retrieval-Augmented Generation", "abstract": "With the adoption of retrieval-augmented generation (RAG), large language\nmodels (LLMs) are expected to ground their generation to the retrieved\ncontexts. Yet, this is hindered by position bias of LLMs, failing to evenly\nattend to all contexts. Previous work has addressed this by synthesizing\ncontexts with perturbed positions of gold segment, creating a\nposition-diversified train set. We extend this intuition to propose consistency\nregularization with augmentation and distillation. First, we augment each\ntraining instance with its position perturbation to encourage consistent\npredictions, regardless of ordering. We also distill behaviors of this pair,\nalthough it can be counterproductive in certain RAG scenarios where the given\norder from the retriever is crucial for generation quality. We thus propose\nCORD, balancing COnsistency and Rank Distillation. CORD adaptively samples\nnoise-controlled perturbations from an interpolation space, ensuring both\nconsistency and respect for the rank prior. Empirical results show this balance\nenables CORD to outperform consistently in diverse RAG benchmarks.", "published": "2024-12-19 07:01:25", "link": "http://arxiv.org/abs/2412.14581v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Simulation-Free Hierarchical Latent Policy Planning for Proactive\n  Dialogues", "abstract": "Recent advancements in proactive dialogues have garnered significant\nattention, particularly for more complex objectives (e.g. emotion support and\npersuasion). Unlike traditional task-oriented dialogues, proactive dialogues\ndemand advanced policy planning and adaptability, requiring rich scenarios and\ncomprehensive policy repositories to develop such systems. However, existing\napproaches tend to rely on Large Language Models (LLMs) for user simulation and\nonline learning, leading to biases that diverge from realistic scenarios and\nresult in suboptimal efficiency. Moreover, these methods depend on manually\ndefined, context-independent, coarse-grained policies, which not only incur\nhigh expert costs but also raise concerns regarding their completeness. In our\nwork, we highlight the potential for automatically discovering policies\ndirectly from raw, real-world dialogue records. To this end, we introduce a\nnovel dialogue policy planning framework, LDPP. It fully automates the process\nfrom mining policies in dialogue records to learning policy planning.\nSpecifically, we employ a variant of the Variational Autoencoder to discover\nfine-grained policies represented as latent vectors. After automatically\nannotating the data with these latent policy labels, we propose an Offline\nHierarchical Reinforcement Learning (RL) algorithm in the latent space to\ndevelop effective policy planning capabilities. Our experiments demonstrate\nthat LDPP outperforms existing methods on two proactive scenarios, even\nsurpassing ChatGPT with only a 1.8-billion-parameter LLM.", "published": "2024-12-19 07:06:01", "link": "http://arxiv.org/abs/2412.14584v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Guilt: Legal Judgment Prediction with Trichotomous Reasoning", "abstract": "In legal practice, judges apply the trichotomous dogmatics of criminal law,\nsequentially assessing the elements of the offense, unlawfulness, and\nculpability to determine whether an individual's conduct constitutes a crime.\nAlthough current legal large language models (LLMs) show promising accuracy in\njudgment prediction, they lack trichotomous reasoning capabilities due to the\nabsence of an appropriate benchmark dataset, preventing them from predicting\ninnocent outcomes. As a result, every input is automatically assigned a charge,\nlimiting their practical utility in legal contexts. To bridge this gap, we\nintroduce LJPIV, the first benchmark dataset for Legal Judgment Prediction with\nInnocent Verdicts. Adhering to the trichotomous dogmatics, we extend three\nwidely-used legal datasets through LLM-based augmentation and manual\nverification. Our experiments with state-of-the-art legal LLMs and novel\nstrategies that integrate trichotomous reasoning into zero-shot prompting and\nfine-tuning reveal: (1) current legal LLMs have significant room for\nimprovement, with even the best models achieving an F1 score of less than 0.3\non LJPIV; and (2) our strategies notably enhance both in-domain and\ncross-domain judgment prediction accuracy, especially for cases resulting in an\ninnocent verdict.", "published": "2024-12-19 07:14:13", "link": "http://arxiv.org/abs/2412.14588v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KARRIEREWEGE: A Large Scale Career Path Prediction Dataset", "abstract": "Accurate career path prediction can support many stakeholders, like job\nseekers, recruiters, HR, and project managers. However, publicly available data\nand tools for career path prediction are scarce. In this work, we introduce\nKARRIEREWEGE, a comprehensive, publicly available dataset containing over 500k\ncareer paths, significantly surpassing the size of previously available\ndatasets. We link the dataset to the ESCO taxonomy to offer a valuable resource\nfor predicting career trajectories. To tackle the problem of free-text inputs\ntypically found in resumes, we enhance it by synthesizing job titles and\ndescriptions resulting in KARRIEREWEGE+. This allows for accurate predictions\nfrom unstructured data, closely aligning with real-world application\nchallenges. We benchmark existing state-of-the-art (SOTA) models on our dataset\nand a prior benchmark and observe improved performance and robustness,\nparticularly for free-text use cases, due to the synthesized data.", "published": "2024-12-19 08:02:08", "link": "http://arxiv.org/abs/2412.14612v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TOMG-Bench: Evaluating LLMs on Text-based Open Molecule Generation", "abstract": "In this paper, we propose Text-based Open Molecule Generation Benchmark\n(TOMG-Bench), the first benchmark to evaluate the open-domain molecule\ngeneration capability of LLMs. TOMG-Bench encompasses a dataset of three major\ntasks: molecule editing (MolEdit), molecule optimization (MolOpt), and\ncustomized molecule generation (MolCustom). Each major task further contains\nthree subtasks, while each subtask comprises 5,000 test samples. Given the\ninherent complexity of open molecule generation evaluation, we also developed\nan automated evaluation system that helps measure both the quality and the\naccuracy of the generated molecules. Our comprehensive benchmarking of 25 LLMs\nreveals the current limitations as well as potential areas for improvement in\ntext-guided molecule discovery. Furthermore, we propose OpenMolIns, a\nspecialized instruction tuning dataset established for solving challenges\nraised by TOMG-Bench. Fine-tuned on OpenMolIns, Llama3.1-8B could outperform\nall the open-source general LLMs, even surpassing GPT-3.5-turbo by 46.5\\% on\nTOMG-Bench. Our codes and datasets are available through\nhttps://github.com/phenixace/TOMG-Bench.", "published": "2024-12-19 08:51:16", "link": "http://arxiv.org/abs/2412.14642v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Length Controlled Generation for Black-box LLMs", "abstract": "Large language models (LLMs) have demonstrated impressive instruction\nfollowing capabilities, while still struggling to accurately manage the length\nof the generated text, which is a fundamental requirement in many real-world\napplications. Existing length control methods involve fine-tuning the\nparameters of LLMs, which is inefficient and suboptimal for practical use. In\nthis paper, we propose a novel iterative sampling framework for text length\ncontrol, integrating the Metropolis-Hastings algorithm with an importance\nsampling acceleration strategy. This framework efficiently and reliably\nregulates LLMs to generate length-constrained text without modifying the\nunderlying parameters, thereby preserving the original capabilities of LLMs.\nExperimental results demonstrate that our framework achieves almost 100\\%\nsuccess rates of length control on Llama3.1 for tasks such as length-controlled\nabstractive summarization and length-constrained instruction following, with\nminimal additional computational overhead. This also highlights the significant\npotential of our method for precise length control across a broader range of\napplications, without compromising the versatility of LLMs.", "published": "2024-12-19 09:07:38", "link": "http://arxiv.org/abs/2412.14656v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLMs as mediators: Can they diagnose conflicts accurately?", "abstract": "Prior research indicates that to be able to mediate conflict, observers of\ndisagreements between parties must be able to reliably distinguish the sources\nof their disagreement as stemming from differences in beliefs about what is\ntrue (causality) vs. differences in what they value (morality). In this paper,\nwe test if OpenAI's Large Language Models GPT 3.5 and GPT 4 can perform this\ntask and whether one or other type of disagreement proves particularly\nchallenging for LLM's to diagnose. We replicate study 1 in Ko\\c{c}ak et al.\n(2003), which employes a vignette design, with OpenAI's GPT 3.5 and GPT 4. We\nfind that both LLMs have similar semantic understanding of the distinction\nbetween causal and moral codes as humans and can reliably distinguish between\nthem. When asked to diagnose the source of disagreement in a conversation, both\nLLMs, compared to humans, exhibit a tendency to overestimate the extent of\ncausal disagreement and underestimate the extent of moral disagreement in the\nmoral misalignment condition. This tendency is especially pronounced for GPT 4\nwhen using a proximate scale that relies on concrete language specific to an\nissue. GPT 3.5 does not perform as well as GPT4 or humans when using either the\nproximate or the distal scale. The study provides a first test of the potential\nfor using LLMs to mediate conflict by diagnosing the root of disagreements in\ncausal and evaluative codes.", "published": "2024-12-19 09:29:08", "link": "http://arxiv.org/abs/2412.14675v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Verbalized Confidence Scores for LLMs", "abstract": "The rise of large language models (LLMs) and their tight integration into our\ndaily life make it essential to dedicate efforts towards their trustworthiness.\nUncertainty quantification for LLMs can establish more human trust into their\nresponses, but also allows LLM agents to make more informed decisions based on\neach other's uncertainty. To estimate the uncertainty in a response, internal\ntoken logits, task-specific proxy models, or sampling of multiple responses are\ncommonly used. This work focuses on asking the LLM itself to verbalize its\nuncertainty with a confidence score as part of its output tokens, which is a\npromising way for prompt- and model-agnostic uncertainty quantification with\nlow overhead. Using an extensive benchmark, we assess the reliability of\nverbalized confidence scores with respect to different datasets, models, and\nprompt methods. Our results reveal that the reliability of these scores\nstrongly depends on how the model is asked, but also that it is possible to\nextract well-calibrated confidence scores with certain prompt methods. We argue\nthat verbalized confidence scores can become a simple but effective and\nversatile uncertainty quantification method in the future. Our code is\navailable at https://github.com/danielyxyang/llm-verbalized-uq .", "published": "2024-12-19 11:10:36", "link": "http://arxiv.org/abs/2412.14737v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Query pipeline optimization for cancer patient question answering\n  systems", "abstract": "Retrieval-augmented generation (RAG) mitigates hallucination in Large\nLanguage Models (LLMs) by using query pipelines to retrieve relevant external\ninformation and grounding responses in retrieved knowledge. However, query\npipeline optimization for cancer patient question-answering (CPQA) systems\nrequires separately optimizing multiple components with domain-specific\nconsiderations. We propose a novel three-aspect optimization approach for the\nRAG query pipeline in CPQA systems, utilizing public biomedical databases like\nPubMed and PubMed Central. Our optimization includes: (1) document retrieval,\nutilizing a comparative analysis of NCBI resources and introducing Hybrid\nSemantic Real-time Document Retrieval (HSRDR); (2) passage retrieval,\nidentifying optimal pairings of dense retrievers and rerankers; and (3)\nsemantic representation, introducing Semantic Enhanced Overlap Segmentation\n(SEOS) for improved contextual understanding. On a custom-developed dataset\ntailored for cancer-related inquiries, our optimized RAG approach improved the\nanswer accuracy of Claude-3-haiku by 5.24% over chain-of-thought prompting and\nabout 3% over a naive RAG setup. This study highlights the importance of\ndomain-specific query optimization in realizing the full potential of RAG and\nprovides a robust framework for building more accurate and reliable CPQA\nsystems, advancing the development of RAG-based biomedical systems.", "published": "2024-12-19 11:30:07", "link": "http://arxiv.org/abs/2412.14751v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PsyDraw: A Multi-Agent Multimodal System for Mental Health Screening in\n  Left-Behind Children", "abstract": "Left-behind children (LBCs), numbering over 66 million in China, face severe\nmental health challenges due to parental migration for work. Early screening\nand identification of at-risk LBCs is crucial, yet challenging due to the\nsevere shortage of mental health professionals, especially in rural areas.\nWhile the House-Tree-Person (HTP) test shows higher child participation rates,\nits requirement for expert interpretation limits its application in\nresource-scarce regions. To address this challenge, we propose PsyDraw, a\nmulti-agent system based on Multimodal Large Language Models that assists\nmental health professionals in analyzing HTP drawings. The system employs\nspecialized agents for feature extraction and psychological interpretation,\noperating in two stages: comprehensive feature analysis and professional report\ngeneration. Evaluation of HTP drawings from 290 primary school students reveals\nthat 71.03% of the analyzes achieved High Consistency with professional\nevaluations, 26.21% Moderate Consistency and only 2.41% Low Consistency. The\nsystem identified 31.03% of cases requiring professional attention,\ndemonstrating its effectiveness as a preliminary screening tool. Currently\ndeployed in pilot schools, \\method shows promise in supporting mental health\nprofessionals, particularly in resource-limited areas, while maintaining high\nprofessional standards in psychological assessment.", "published": "2024-12-19 11:51:57", "link": "http://arxiv.org/abs/2412.14769v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Disentangling Reasoning Tokens and Boilerplate Tokens For Language Model\n  Fine-tuning", "abstract": "When using agent-task datasets to enhance agent capabilities for Large\nLanguage Models (LLMs), current methodologies often treat all tokens within a\nsample equally. However, we argue that tokens serving different roles -\nspecifically, reasoning tokens versus boilerplate tokens (e.g., those governing\noutput format) - differ significantly in importance and learning complexity,\nnecessitating their disentanglement and distinct treatment. To address this, we\npropose a novel Shuffle-Aware Discriminator (SHAD) for adaptive token\ndiscrimination. SHAD classifies tokens by exploiting predictability differences\nobserved after shuffling input-output combinations across samples: boilerplate\ntokens, due to their repetitive nature among samples, maintain predictability,\nwhereas reasoning tokens do not. Using SHAD, we propose the\nReasoning-highlighted Fine-Tuning (RFT) method, which adaptively emphasizes\nreasoning tokens during fine-tuning, yielding notable performance gains over\ncommon Supervised Fine-Tuning (SFT).", "published": "2024-12-19 12:06:24", "link": "http://arxiv.org/abs/2412.14780v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ResoFilter: Fine-grained Synthetic Data Filtering for Large Language\n  Models through Data-Parameter Resonance Analysis", "abstract": "Large language models (LLMs) have shown remarkable effectiveness across\nvarious domains, with data augmentation methods utilizing GPT for synthetic\ndata generation becoming prevalent. However, the quality and utility of\naugmented data remain questionable, and current methods lack clear metrics for\nevaluating data characteristics. To address these challenges, we propose\nResoFilter, a novel method that integrates models, data, and tasks to refine\ndatasets. ResoFilter leverages the fine-tuning process to obtain Data-Parameter\nfeatures for data selection, offering improved interpretability by representing\ndata characteristics through model weights. Our experiments demonstrate that\nResoFilter achieves comparable results to full-scale fine-tuning using only\nhalf the data in mathematical tasks and exhibits strong generalization across\ndifferent models and domains. This method provides valuable insights for\nconstructing synthetic datasets and evaluating high-quality data, offering a\npromising solution for enhancing data augmentation techniques and improving\ntraining dataset quality for LLMs. For reproducibility, we will release our\ncode and data upon acceptance.", "published": "2024-12-19 12:57:47", "link": "http://arxiv.org/abs/2412.14809v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mention Attention for Pronoun Translation", "abstract": "Most pronouns are referring expressions, computers need to resolve what do\nthe pronouns refer to, and there are divergences on pronoun usage across\nlanguages. Thus, dealing with these divergences and translating pronouns is a\nchallenge in machine translation. Mentions are referring candidates of pronouns\nand have closer relations with pronouns compared to general tokens. We assume\nthat extracting additional mention features can help pronoun translation.\nTherefore, we introduce an additional mention attention module in the decoder\nto pay extra attention to source mentions but not non-mention tokens. Our\nmention attention module not only extracts features from source mentions, but\nalso considers target-side context which benefits pronoun translation. In\naddition, we also introduce two mention classifiers to train models to\nrecognize mentions, whose outputs guide the mention attention. We conduct\nexperiments on the WMT17 English-German translation task, and evaluate our\nmodels on general translation and pronoun translation, using BLEU, APT, and\ncontrastive evaluation metrics. Our proposed model outperforms the baseline\nTransformer model in terms of APT and BLEU scores, this confirms our hypothesis\nthat we can improve pronoun translation by paying additional attention to\nsource mentions, and shows that our introduced additional modules do not have\nnegative effect on the general translation quality.", "published": "2024-12-19 13:19:19", "link": "http://arxiv.org/abs/2412.14829v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs", "abstract": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased.", "published": "2024-12-19 13:28:42", "link": "http://arxiv.org/abs/2412.14838v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DS$^2$-ABSA: Dual-Stream Data Synthesis with Label Refinement for\n  Few-Shot Aspect-Based Sentiment Analysis", "abstract": "Recently developed large language models (LLMs) have presented promising new\navenues to address data scarcity in low-resource scenarios. In few-shot\naspect-based sentiment analysis (ABSA), previous efforts have explored data\naugmentation techniques, which prompt LLMs to generate new samples by modifying\nexisting ones. However, these methods fail to produce adequately diverse data,\nimpairing their effectiveness. Besides, some studies apply in-context learning\nfor ABSA by using specific instructions and a few selected examples as prompts.\nThough promising, LLMs often yield labels that deviate from task requirements.\nTo overcome these limitations, we propose DS$^2$-ABSA, a dual-stream data\nsynthesis framework targeted for few-shot ABSA. It leverages LLMs to synthesize\ndata from two complementary perspectives: \\textit{key-point-driven} and\n\\textit{instance-driven}, which effectively generate diverse and high-quality\nABSA samples in low-resource settings. Furthermore, a \\textit{label refinement}\nmodule is integrated to improve the synthetic labels. Extensive experiments\ndemonstrate that DS$^2$-ABSA significantly outperforms previous few-shot ABSA\nsolutions and other LLM-oriented data generation methods.", "published": "2024-12-19 13:39:47", "link": "http://arxiv.org/abs/2412.14849v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Think&Cite: Improving Attributed Text Generation with Self-Guided Tree\n  Search and Progress Reward Modeling", "abstract": "Despite their outstanding capabilities, large language models (LLMs) are\nprone to hallucination and producing factually incorrect information. This\nchallenge has spurred efforts in attributed text generation, which prompts LLMs\nto generate content with supporting evidence. In this paper, we propose a novel\nframework, called Think&Cite, and formulate attributed text generation as a\nmulti-step reasoning problem integrated with search. Specifically, we propose\nSelf-Guided Monte Carlo Tree Search (SG-MCTS), which capitalizes on the\nself-reflection capability of LLMs to reflect on the intermediate states of\nMCTS for guiding the tree expansion process. To provide reliable and\ncomprehensive feedback, we introduce Progress Reward Models to measure the\nprogress of tree search from the root to the current state from two aspects,\ni.e., generation and attribution progress. We conduct extensive experiments on\nthree datasets and the results show that our approach significantly outperforms\nbaseline approaches.", "published": "2024-12-19 13:55:48", "link": "http://arxiv.org/abs/2412.14860v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Graph-Convolutional Networks: Named Entity Recognition and Large\n  Language Model Embedding in Document Clustering", "abstract": "Recent advances in machine learning, particularly Large Language Models\n(LLMs) such as BERT and GPT, provide rich contextual embeddings that improve\ntext representation. However, current document clustering approaches often\nignore the deeper relationships between named entities (NEs) and the potential\nof LLM embeddings. This paper proposes a novel approach that integrates Named\nEntity Recognition (NER) and LLM embeddings within a graph-based framework for\ndocument clustering. The method builds a graph with nodes representing\ndocuments and edges weighted by named entity similarity, optimized using a\ngraph-convolutional network (GCN). This ensures a more effective grouping of\nsemantically related documents. Experimental results indicate that our approach\noutperforms conventional co-occurrence-based methods in clustering, notably for\ndocuments rich in named entities.", "published": "2024-12-19 14:03:22", "link": "http://arxiv.org/abs/2412.14867v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Theoretical Proof that Generated Text in the Corpus Leads to the\n  Collapse of Auto-regressive Language Models", "abstract": "Auto-regressive language models (LMs) have been widely used to generate text\non the World Wide Web. The generated text is often collected into the training\ncorpus of the next generations of LMs. Previous work experimentally found that\nLMs collapse when trained on recursively generated text. This paper presents\ntheoretical proof that once a corpus (such as the World Wide Web) begins to\nincorporate generated text, and the training text of each LM is sampled from\nthis corpus, then no matter how small the amount of text generated by each LM\nthat enters the corpus is, after a sufficient amount of time, LM collapse is\nbound to occur. Our proof is validated by a series of experiments showing that\nthe collapsed LMs perform no better than an untrained LM with randomly\ninitialized parameters. By proving the existence of LM collapse, we express our\nconcerns about the current situation in which an increasing amount of generated\ntext may be used in LM training. The source code is available in the online\ndata warehouse: https://github.com/wanglc02/generated-data", "published": "2024-12-19 14:11:15", "link": "http://arxiv.org/abs/2412.14872v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding the Dark Side of LLMs' Intrinsic Self-Correction", "abstract": "Intrinsic self-correction was proposed to improve LLMs' responses via\nfeedback prompts solely based on their inherent capability. However, recent\nworks show that LLMs' intrinsic self-correction fails without oracle labels as\nfeedback prompts. In this paper, we aim to interpret LLMs' intrinsic\nself-correction for different tasks, especially for those failure cases. By\nincluding one simple task and three complex tasks with state-of-the-art (SOTA)\nLLMs like ChatGPT families (o1, 4o, 3.5-turbo) and Llama families (2-7B, 3-8B,\nand 3.1-8B), we design three interpretation methods to reveal the dark side of\nLLMs' intrinsic self-correction. We identify intrinsic self-correction can (1)\ncause LLMs to waver both intermedia and final answers and lead to prompt bias\non simple factual questions; (2) introduce human-like cognitive bias on complex\ntasks. In light of our findings, we also provide two simple yet effective\nstrategies for alleviation: question repeating and supervised fine-tuning with\na few samples. We open-source our work at https://x-isc.info/.", "published": "2024-12-19 15:39:31", "link": "http://arxiv.org/abs/2412.14959v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Chain-of-MetaWriting: Linguistic and Textual Analysis of How Small\n  Language Models Write Young Students Texts", "abstract": "Large Language Models (LLMs) have been used to generate texts in response to\ndifferent writing tasks: reports, essays, story telling. However, language\nmodels do not have a meta-representation of the text writing process, nor\ninherent communication learning needs, comparable to those of young human\nstudents. This paper introduces a fine-grained linguistic and textual analysis\nof multilingual Small Language Models' (SLMs) writing. With our method,\nChain-of-MetaWriting, SLMs can imitate some steps of the human writing process,\nsuch as planning and evaluation. We mainly focused on short story and essay\nwriting tasks in French for schoolchildren and undergraduate students\nrespectively. Our results show that SLMs encounter difficulties in assisting\nyoung students on sensitive topics such as violence in the schoolyard, and they\nsometimes use words too complex for the target audience. In particular, the\noutput is quite different from the human produced texts in term of text\ncohesion and coherence regarding temporal connectors, topic progression,\nreference.", "published": "2024-12-19 15:58:53", "link": "http://arxiv.org/abs/2412.14986v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLMs Lost in Translation: M-ALERT uncovers Cross-Linguistic Safety Gaps", "abstract": "Building safe Large Language Models (LLMs) across multiple languages is\nessential in ensuring both safe access and linguistic diversity. To this end,\nwe introduce M-ALERT, a multilingual benchmark that evaluates the safety of\nLLMs in five languages: English, French, German, Italian, and Spanish. M-ALERT\nincludes 15k high-quality prompts per language, totaling 75k, following the\ndetailed ALERT taxonomy. Our extensive experiments on 10 state-of-the-art LLMs\nhighlight the importance of language-specific safety analysis, revealing that\nmodels often exhibit significant inconsistencies in safety across languages and\ncategories. For instance, Llama3.2 shows high unsafety in the category\ncrime_tax for Italian but remains safe in other languages. Similar differences\ncan be observed across all models. In contrast, certain categories, such as\nsubstance_cannabis and crime_propaganda, consistently trigger unsafe responses\nacross models and languages. These findings underscore the need for robust\nmultilingual safety practices in LLMs to ensure safe and responsible usage\nacross diverse user communities.", "published": "2024-12-19 16:46:54", "link": "http://arxiv.org/abs/2412.15035v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ConfliBERT: A Language Model for Political Conflict", "abstract": "Conflict scholars have used rule-based approaches to extract information\nabout political violence from news reports and texts. Recent Natural Language\nProcessing developments move beyond rigid rule-based approaches. We review our\nrecent ConfliBERT language model (Hu et al. 2022) to process political and\nviolence related texts. The model can be used to extract actor and action\nclassifications from texts about political conflict. When fine-tuned, results\nshow that ConfliBERT has superior performance in accuracy, precision and recall\nover other large language models (LLM) like Google's Gemma 2 (9B), Meta's Llama\n3.1 (7B), and Alibaba's Qwen 2.5 (14B) within its relevant domains. It is also\nhundreds of times faster than these more generalist LLMs. These results are\nillustrated using texts from the BBC, re3d, and the Global Terrorism Dataset\n(GTD).", "published": "2024-12-19 17:08:11", "link": "http://arxiv.org/abs/2412.15060v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Review-Then-Refine: A Dynamic Framework for Multi-Hop Question Answering\n  with Temporal Adaptability", "abstract": "Retrieve-augmented generation (RAG) frameworks have emerged as a promising\nsolution to multi-hop question answering(QA) tasks since it enables large\nlanguage models (LLMs) to incorporate external knowledge and mitigate their\ninherent knowledge deficiencies. Despite this progress, existing RAG\nframeworks, which usually follows the retrieve-then-read paradigm, often\nstruggle with multi-hop QA with temporal information since it has difficulty\nretrieving and synthesizing accurate time-related information. To address the\nchallenge, this paper proposes a novel framework called review-then-refine,\nwhich aims to enhance LLM performance in multi-hop QA scenarios with temporal\ninformation. Our approach begins with a review phase, where decomposed\nsub-queries are dynamically rewritten with temporal information, allowing for\nsubsequent adaptive retrieval and reasoning process. In addition, we implement\nadaptive retrieval mechanism to minimize unnecessary retrievals, thus reducing\nthe potential for hallucinations. In the subsequent refine phase, the LLM\nsynthesizes the retrieved information from each sub-query along with its\ninternal knowledge to formulate a coherent answer. Extensive experimental\nresults across multiple datasets demonstrate the effectiveness of our proposed\nframework, highlighting its potential to significantly improve multi-hop QA\ncapabilities in LLMs.", "published": "2024-12-19 17:48:23", "link": "http://arxiv.org/abs/2412.15101v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Qwen2.5 Technical Report", "abstract": "In this report, we introduce Qwen2.5, a comprehensive series of large\nlanguage models (LLMs) designed to meet diverse needs. Compared to previous\niterations, Qwen 2.5 has been significantly improved during both the\npre-training and post-training stages. In terms of pre-training, we have scaled\nthe high-quality pre-training datasets from the previous 7 trillion tokens to\n18 trillion tokens. This provides a strong foundation for common sense, expert\nknowledge, and reasoning capabilities. In terms of post-training, we implement\nintricate supervised finetuning with over 1 million samples, as well as\nmultistage reinforcement learning. Post-training techniques enhance human\npreference, and notably improve long text generation, structural data analysis,\nand instruction following. To handle diverse and varied use cases effectively,\nwe present Qwen2.5 LLM series in rich sizes. Open-weight offerings include base\nand instruction-tuned models, with quantized versions available. In addition,\nfor hosted solutions, the proprietary models currently include two\nmixture-of-experts (MoE) variants: Qwen2.5-Turbo and Qwen2.5-Plus, both\navailable from Alibaba Cloud Model Studio. Qwen2.5 has demonstrated top-tier\nperformance on a wide range of benchmarks evaluating language understanding,\nreasoning, mathematics, coding, human preference alignment, etc. Specifically,\nthe open-weight flagship Qwen2.5-72B-Instruct outperforms a number of open and\nproprietary models and demonstrates competitive performance to the\nstate-of-the-art open-weight model, Llama-3-405B-Instruct, which is around 5\ntimes larger. Qwen2.5-Turbo and Qwen2.5-Plus offer superior cost-effectiveness\nwhile performing competitively against GPT-4o-mini and GPT-4o respectively.\nAdditionally, as the foundation, Qwen2.5 models have been instrumental in\ntraining specialized models such as Qwen2.5-Math, Qwen2.5-Coder, QwQ, and\nmultimodal models.", "published": "2024-12-19 17:56:09", "link": "http://arxiv.org/abs/2412.15115v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MMLU-CF: A Contamination-free Multi-task Language Understanding\n  Benchmark", "abstract": "Multiple-choice question (MCQ) datasets like Massive Multitask Language\nUnderstanding (MMLU) are widely used to evaluate the commonsense,\nunderstanding, and problem-solving abilities of large language models (LLMs).\nHowever, the open-source nature of these benchmarks and the broad sources of\ntraining data for LLMs have inevitably led to benchmark contamination,\nresulting in unreliable evaluation results. To alleviate this issue, we propose\na contamination-free and more challenging MCQ benchmark called MMLU-CF. This\nbenchmark reassesses LLMs' understanding of world knowledge by averting both\nunintentional and malicious data leakage. To avoid unintentional data leakage,\nwe source data from a broader domain and design three decontamination rules. To\nprevent malicious data leakage, we divide the benchmark into validation and\ntest sets with similar difficulty and subject distributions. The test set\nremains closed-source to ensure reliable results, while the validation set is\npublicly available to promote transparency and facilitate independent\nverification. Our evaluation of mainstream LLMs reveals that the powerful\nGPT-4o achieves merely a 5-shot score of 73.4% and a 0-shot score of 71.9% on\nthe test set, which indicates the effectiveness of our approach in creating a\nmore rigorous and contamination-free evaluation standard. The GitHub repository\nis available at https://github.com/microsoft/MMLU-CF and the dataset refers to\nhttps://huggingface.co/datasets/microsoft/MMLU-CF.", "published": "2024-12-19 18:58:04", "link": "http://arxiv.org/abs/2412.15194v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Evolution Knowledge Distillation for LLM-based Machine Translation", "abstract": "Knowledge distillation (KD) has shown great promise in transferring knowledge\nfrom larger teacher models to smaller student models. However, existing KD\nstrategies for large language models often minimize output distributions\nbetween student and teacher models indiscriminately for each token. This\noverlooks the imbalanced nature of tokens and their varying transfer\ndifficulties. In response, we propose a distillation strategy called\nSelf-Evolution KD. The core of this approach involves dynamically integrating\nteacher distribution and one-hot distribution of ground truth into the student\ndistribution as prior knowledge, which promotes the distillation process. It\nadjusts the ratio of prior knowledge based on token learning difficulty, fully\nleveraging the teacher model's potential. Experimental results show our method\nbrings an average improvement of approximately 1.4 SacreBLEU points across four\ntranslation directions in the WMT22 test sets. Further analysis indicates that\nthe improvement comes from better knowledge transfer from teachers, confirming\nour hypothesis.", "published": "2024-12-19 12:24:15", "link": "http://arxiv.org/abs/2412.15303v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Decade of Natural Language Processing in Chronic Pain: A Systematic\n  Review", "abstract": "In recent years, the intersection of Natural Language Processing (NLP) and\npublic health has opened innovative pathways for investigating various domains,\nincluding chronic pain in textual datasets. Despite the promise of NLP in\nchronic pain, the literature is dispersed across various disciplines, and there\nis a need to consolidate existing knowledge, identify knowledge gaps in the\nliterature, and inform future research directions in this emerging field. This\nreview aims to investigate the state of the research on NLP-based interventions\ndesigned for chronic pain research. A search strategy was formulated and\nexecuted across PubMed, Web of Science, IEEE Xplore, Scopus, and ACL Anthology\nto find studies published in English between 2014 and 2024. After screening 132\npapers, 26 studies were included in the final review. Key findings from this\nreview underscore the significant potential of NLP techniques to address\npressing challenges in chronic pain research. The past 10 years in this field\nhave showcased the utilization of advanced methods (transformers like RoBERTa\nand BERT) achieving high-performance metrics (e.g., F1>0.8) in classification\ntasks, while unsupervised approaches like Latent Dirichlet Allocation (LDA) and\nk-means clustering have proven effective for exploratory analyses. Results also\nreveal persistent challenges such as limited dataset diversity, inadequate\nsample sizes, and insufficient representation of underrepresented populations.\nFuture research studies should explore multimodal data validation systems,\ncontext-aware mechanistic modeling, and the development of standardized\nevaluation metrics to enhance reproducibility and equity in chronic pain\nresearch.", "published": "2024-12-19 19:46:09", "link": "http://arxiv.org/abs/2412.15360v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Extraction of Metaphoric Analogies from Literary Texts: Task\n  Formulation, Dataset Construction, and Evaluation", "abstract": "Extracting metaphors and analogies from free text requires high-level\nreasoning abilities such as abstraction and language understanding. Our study\nfocuses on the extraction of the concepts that form metaphoric analogies in\nliterary texts. To this end, we construct a novel dataset in this domain with\nthe help of domain experts. We compare the out-of-the-box ability of recent\nlarge language models (LLMs) to structure metaphoric mappings from fragments of\ntexts containing proportional analogies. The models are further evaluated on\nthe generation of implicit elements of the analogy, which are indirectly\nsuggested in the texts and inferred by human readers. The competitive results\nobtained by LLMs in our experiments are encouraging and open up new avenues\nsuch as automatically extracting analogies and metaphors from text instead of\ninvesting resources in domain experts to manually label data.", "published": "2024-12-19 20:11:04", "link": "http://arxiv.org/abs/2412.15375v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SKETCH: Structured Knowledge Enhanced Text Comprehension for Holistic\n  Retrieval", "abstract": "Retrieval-Augmented Generation (RAG) systems have become pivotal in\nleveraging vast corpora to generate informed and contextually relevant\nresponses, notably reducing hallucinations in Large Language Models. Despite\nsignificant advancements, these systems struggle to efficiently process and\nretrieve information from large datasets while maintaining a comprehensive\nunderstanding of the context. This paper introduces SKETCH, a novel methodology\nthat enhances the RAG retrieval process by integrating semantic text retrieval\nwith knowledge graphs, thereby merging structured and unstructured data for a\nmore holistic comprehension. SKETCH, demonstrates substantial improvements in\nretrieval performance and maintains superior context integrity compared to\ntraditional methods. Evaluated across four diverse datasets: QuALITY, QASPER,\nNarrativeQA, and Italian Cuisine-SKETCH consistently outperforms baseline\napproaches on key RAGAS metrics such as answer_relevancy, faithfulness,\ncontext_precision and context_recall. Notably, on the Italian Cuisine dataset,\nSKETCH achieved an answer relevancy of 0.94 and a context precision of 0.99,\nrepresenting the highest performance across all evaluated metrics. These\nresults highlight SKETCH's capability in delivering more accurate and\ncontextually relevant responses, setting new benchmarks for future retrieval\nsystems.", "published": "2024-12-19 22:51:56", "link": "http://arxiv.org/abs/2412.15443v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fietje: An open, efficient LLM for Dutch", "abstract": "This paper introduces Fietje, a family of small language models (SLMs)\nspecifically designed for the Dutch language. The model is based on Phi 2, an\nEnglish-centric model of 2.7 billion parameters. Fietje demonstrated\ncompetitive results with larger language models upon its release. A core\nemphasis of this work is transparency and reproducibility: Fietje is fully\nopen-source, with model weights, datasets, training, and evaluation code all\npublicly accessible.\n  The paper discusses the performance of Fietje and many other models on an\nextensive evaluation suite of benchmarks on reasoning, sentiment analysis,\nworld knowledge, linguistic acceptability and word sense disambiguation.\nEvaluation results illustrate the rapid progress in the field of LLMs, where\nrecent small models outperform older, larger models that were fine-tuned for\nDutch. This trend signals an exciting future for Dutch language processing,\nsuggesting that even compact LLMs are becoming increasingly capable.\nFurthermore, ongoing and future efforts to adapt LLMs to Dutch are poised to\nenhance these models even further, broadening their applicability and\naccessibility. Fietje is only an intermediate step in improving accessibility\nto language technology for users of the Dutch language.", "published": "2024-12-19 23:06:01", "link": "http://arxiv.org/abs/2412.15450v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Overview of the 2024 ALTA Shared Task: Detect Automatic AI-Generated\n  Sentences for Human-AI Hybrid Articles", "abstract": "The ALTA shared tasks have been running annually since 2010. In 2024, the\npurpose of the task is to detect machine-generated text in a hybrid setting\nwhere the text may contain portions of human text and portions\nmachine-generated. In this paper, we present the task, the evaluation criteria,\nand the results of the systems participating in the shared task.", "published": "2024-12-19 03:50:06", "link": "http://arxiv.org/abs/2412.17848v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Why Do Large Language Models (LLMs) Struggle to Count Letters?", "abstract": "Large Language Models (LLMs) have achieved unprecedented performance on many\ncomplex tasks, being able, for example, to answer questions on almost any\ntopic. However, they struggle with other simple tasks, such as counting the\noccurrences of letters in a word, as illustrated by the inability of many LLMs\nto count the number of \"r\" letters in \"strawberry\". Several works have studied\nthis problem and linked it to the tokenization used by LLMs, to the intrinsic\nlimitations of the attention mechanism, or to the lack of character-level\ntraining data. In this paper, we conduct an experimental study to evaluate the\nrelations between the LLM errors when counting letters with 1) the frequency of\nthe word and its components in the training dataset and 2) the complexity of\nthe counting operation. We present a comprehensive analysis of the errors of\nLLMs when counting letter occurrences by evaluating a representative group of\nmodels over a large number of words. The results show a number of consistent\ntrends in the models evaluated: 1) models are capable of recognizing the\nletters but not counting them; 2) the frequency of the word and tokens in the\nword does not have a significant impact on the LLM errors; 3) there is a\npositive correlation of letter frequency with errors, more frequent letters\ntend to have more counting errors, 4) the errors show a strong correlation with\nthe number of letters or tokens in a word and 5) the strongest correlation\noccurs with the number of letters with counts larger than one, with most models\nbeing unable to correctly count words in which letters appear more than twice.", "published": "2024-12-19 22:47:08", "link": "http://arxiv.org/abs/2412.18626v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "All-in-One Tuning and Structural Pruning for Domain-Specific LLMs", "abstract": "Existing pruning techniques for large language models (LLMs) targeting\ndomain-specific applications typically follow a two-stage process: pruning the\npretrained general-purpose LLMs and then fine-tuning the pruned LLMs on\nspecific domains. However, the pruning decisions, derived from the pretrained\nweights, remain unchanged during fine-tuning, even if the weights have been\nupdated. Therefore, such a combination of the pruning decisions and the\nfinetuned weights may be suboptimal, leading to non-negligible performance\ndegradation. To address these limitations, we propose ATP: All-in-One Tuning\nand Structural Pruning, a unified one-stage structural pruning and fine-tuning\napproach that dynamically identifies the current optimal substructure\nthroughout the fine-tuning phase via a trainable pruning decision generator.\nMoreover, given the limited available data for domain-specific applications,\nLow-Rank Adaptation (LoRA) becomes a common technique to fine-tune the LLMs. In\nATP, we introduce LoRA-aware forward and sparsity regularization to ensure that\nthe substructures corresponding to the learned pruning decisions can be\ndirectly removed after the ATP process. ATP outperforms the state-of-the-art\ntwo-stage pruning methods on tasks in the legal and healthcare domains. More\nspecifically, ATP recovers up to 88% and 91% performance of the dense model\nwhen pruning 40% parameters of LLaMA2-7B and LLaMA3-8B models, respectively.", "published": "2024-12-19 00:41:40", "link": "http://arxiv.org/abs/2412.14426v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ORBIT: Cost-Effective Dataset Curation for Large Language Model Domain\n  Adaptation with an Astronomy Case Study", "abstract": "Recent advances in language modeling demonstrate the need for high-quality\ndomain-specific training data, especially for tasks that require specialized\nknowledge. General-purpose models, while versatile, often lack the depth needed\nfor expert-level tasks because of limited domain-specific information. Domain\nadaptation training can enhance these models, but it demands substantial,\nhigh-quality data. To address this, we propose ORBIT, a cost-efficient\nmethodology for curating massive, high-quality domain-specific datasets from\nnoisy web sources, tailored for training specialist large language models.\nUsing astronomy as a primary case study, we refined the 1.3T-token FineWeb-Edu\ndataset into a high-quality, 10B-token subset focused on astronomy. Fine-tuning\n\\textsc{LLaMA-3-8B} on a 1B-token astronomy subset improved performance on the\nMMLU astronomy benchmark from 69\\% to 76\\% and achieved top results on\nAstroBench, an astronomy-specific benchmark. Moreover, our model (Orbit-LLaMA)\noutperformed \\textsc{LLaMA-3-8B-base}, with GPT-4o evaluations preferring it in\n73\\% of cases across 1000 astronomy-specific questions. Additionally, we\nvalidated ORBIT's generalizability by applying it to law and medicine,\nachieving a significant improvement of data quality compared to an unfiltered\nbaseline. We open-source the ORBIT methodology, including the curated datasets,\nthe codebase, and the resulting model at\n\\href{https://github.com/ModeEric/ORBIT-Llama}{https://github.com/ModeEric/ORBIT-Llama}.", "published": "2024-12-19 01:35:47", "link": "http://arxiv.org/abs/2412.14436v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Are Longer Prompts Always Better? Prompt Selection in Large Language\n  Models for Recommendation Systems", "abstract": "In large language models (LLM)-based recommendation systems (LLM-RSs),\naccurately predicting user preferences by leveraging the general knowledge of\nLLMs is possible without requiring extensive training data. By converting\nrecommendation tasks into natural language inputs called prompts, LLM-RSs can\nefficiently solve issues that have been difficult to address due to data\nscarcity but are crucial in applications such as cold-start and cross-domain\nproblems. However, when applying this in practice, selecting the prompt that\nmatches tasks and data is essential. Although numerous prompts have been\nproposed in LLM-RSs and representing the target user in prompts significantly\nimpacts recommendation accuracy, there are still no clear guidelines for\nselecting specific prompts.\n  In this paper, we categorize and analyze prompts from previous research to\nestablish practical prompt selection guidelines. Through 450 experiments with\n90 prompts and five real-world datasets, we examined the relationship between\nprompts and dataset characteristics in recommendation accuracy. We found that\nno single prompt consistently outperforms others; thus, selecting prompts on\nthe basis of dataset characteristics is crucial. Here, we propose a prompt\nselection method that achieves higher accuracy with minimal validation data.\nBecause increasing the number of prompts to explore raises costs, we also\nintroduce a cost-efficient strategy using high-performance and cost-efficient\nLLMs, significantly reducing exploration costs while maintaining high\nprediction accuracy. Our work offers valuable insights into the prompt\nselection, advancing accurate and efficient LLM-RSs.", "published": "2024-12-19 02:09:59", "link": "http://arxiv.org/abs/2412.14454v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "MegaPairs: Massive Data Synthesis For Universal Multimodal Retrieval", "abstract": "Despite the rapidly growing demand for multimodal retrieval, progress in this\nfield remains severely constrained by a lack of training data. In this paper,\nwe introduce MegaPairs, a novel data synthesis method that leverages vision\nlanguage models (VLMs) and open-domain images, together with a massive\nsynthetic dataset generated from this method. Our empirical analysis shows that\nMegaPairs generates high-quality data, enabling the multimodal retriever to\nsignificantly outperform the baseline model trained on 70$\\times$ more data\nfrom existing datasets. Moreover, since MegaPairs solely relies on general\nimage corpora and open-source VLMs, it can be easily scaled up, enabling\ncontinuous improvements in retrieval performance. In this stage, we produced\nmore than 26 million training instances and trained several models of varying\nsizes using this data. These new models achieve state-of-the-art zero-shot\nperformance across 4 popular composed image retrieval (CIR) benchmarks and the\nhighest overall performance on the 36 datasets provided by MMEB. They also\ndemonstrate notable performance improvements with additional downstream\nfine-tuning. Our produced dataset, well-trained models, and data synthesis\npipeline will be made publicly available to facilitate the future development\nof this field.", "published": "2024-12-19 02:49:55", "link": "http://arxiv.org/abs/2412.14475v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "PA-RAG: RAG Alignment via Multi-Perspective Preference Optimization", "abstract": "The emergence of Retrieval-augmented generation (RAG) has alleviated the\nissues of outdated and hallucinatory content in the generation of large\nlanguage models (LLMs), yet it still reveals numerous limitations. When a\ngeneral-purpose LLM serves as the RAG generator, it often suffers from\ninadequate response informativeness, response robustness, and citation quality.\nPast approaches to tackle these limitations, either by incorporating additional\nsteps beyond generating responses or optimizing the generator through\nsupervised fine-tuning (SFT), still failed to align with the RAG requirement\nthoroughly. Consequently, optimizing the RAG generator from multiple preference\nperspectives while maintaining its end-to-end LLM form remains a challenge. To\nbridge this gap, we propose Multiple Perspective Preference Alignment for\nRetrieval-Augmented Generation (PA-RAG), a method for optimizing the generator\nof RAG systems to align with RAG requirements comprehensively. Specifically, we\nconstruct high-quality instruction fine-tuning data and multi-perspective\npreference data by sampling varied quality responses from the generator across\ndifferent prompt documents quality scenarios. Subsequently, we optimize the\ngenerator using SFT and Direct Preference Optimization (DPO). Extensive\nexperiments conducted on four question-answer datasets across three LLMs\ndemonstrate that PA-RAG can significantly enhance the performance of RAG\ngenerators. Our code and datasets are available at\nhttps://github.com/wujwyi/PA-RAG.", "published": "2024-12-19 04:18:51", "link": "http://arxiv.org/abs/2412.14510v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Cal-DPO: Calibrated Direct Preference Optimization for Language Model\n  Alignment", "abstract": "We study the problem of aligning large language models (LLMs) with human\npreference data. Contrastive preference optimization has shown promising\nresults in aligning LLMs with available preference data by optimizing the\nimplicit reward associated with the policy. However, the contrastive objective\nfocuses mainly on the relative values of implicit rewards associated with two\nresponses while ignoring their actual values, resulting in suboptimal alignment\nwith human preferences. To address this limitation, we propose calibrated\ndirect preference optimization (Cal-DPO), a simple yet effective algorithm. We\nshow that substantial improvement in alignment with the given preferences can\nbe achieved simply by calibrating the implicit reward to ensure that the\nlearned implicit rewards are comparable in scale to the ground-truth rewards.\nWe demonstrate the theoretical advantages of Cal-DPO over existing approaches.\nThe results of our experiments on a variety of standard benchmarks show that\nCal-DPO remarkably improves off-the-shelf methods.", "published": "2024-12-19 04:31:56", "link": "http://arxiv.org/abs/2412.14516v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Sliding Windows Are Not the End: Exploring Full Ranking with\n  Long-Context Large Language Models", "abstract": "Large Language Models (LLMs) have shown exciting performance in listwise\npassage ranking. Due to the limited input length, existing methods often adopt\nthe sliding window strategy. Such a strategy, though effective, is inefficient\nas it involves repetitive and serialized processing, which usually re-evaluates\nrelevant passages multiple times. As a result, it incurs redundant API costs,\nwhich are proportional to the number of inference tokens. The development of\nlong-context LLMs enables the full ranking of all passages within a single\ninference, avoiding redundant API costs. In this paper, we conduct a\ncomprehensive study of long-context LLMs for ranking tasks in terms of\nefficiency and effectiveness. Surprisingly, our experiments reveal that full\nranking with long-context LLMs can deliver superior performance in the\nsupervised fine-tuning setting with a huge efficiency improvement. Furthermore,\nwe identify two limitations of fine-tuning the full ranking model based on\nexisting methods: (1) sliding window strategy fails to produce a full ranking\nlist as a training label, and (2) the language modeling loss cannot emphasize\ntop-ranked passage IDs in the label. To alleviate these issues, we propose a\nnew complete listwise label construction approach and a novel importance-aware\nlearning objective for full ranking. Experiments show the superior performance\nof our method over baselines. Our codes are available at\n\\url{https://github.com/8421BCD/fullrank}.", "published": "2024-12-19 06:44:59", "link": "http://arxiv.org/abs/2412.14574v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "How good is GPT at writing political speeches for the White House?", "abstract": "Using large language models (LLMs), computers are able to generate a written\ntext in response to a us er request. As this pervasive technology can be\napplied in numerous contexts, this study analyses the written style of one LLM\ncalled GPT by comparing its generated speeches with those of the recent US\npresidents. To achieve this objective, the State of the Union (SOTU) addresses\nwritten by Reagan to Biden are contrasted to those produced by both GPT-3.5 and\nGPT-4.o versions. Compared to US presidents, GPT tends to overuse the lemma\n\"we\" and produce shorter messages with, on average, longer sentences. Moreover,\nGPT opts for an optimistic tone, opting more often for political (e.g.,\npresident, Congress), symbolic (e.g., freedom), and abstract terms (e.g.,\nfreedom). Even when imposing an author's style to GPT, the resulting speech\nremains distinct from addresses written by the target author. Finally, the two\nGPT versions present distinct characteristics, but both appear overall\ndissimilar to true presidential messages.", "published": "2024-12-19 08:06:09", "link": "http://arxiv.org/abs/2412.14617v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning to Generate Research Idea with Dynamic Control", "abstract": "The rapid advancements in large language models (LLMs) have demonstrated\ntheir potential to accelerate scientific discovery, particularly in automating\nthe process of research ideation. LLM-based systems have shown promise in\ngenerating hypotheses and research ideas. However, current approaches\npredominantly rely on prompting-based pre-trained models, limiting their\nability to optimize generated content effectively. Moreover, they also lack the\ncapability to deal with the complex interdependence and inherent restrictions\namong novelty, feasibility, and effectiveness, which remains challenging due to\nthe inherent trade-offs among these dimensions, such as the\ninnovation-feasibility conflict. To address these limitations, we for the first\ntime propose fine-tuning LLMs to be better idea proposers and introduce a novel\nframework that employs a two-stage approach combining Supervised Fine-Tuning\n(SFT) and controllable Reinforcement Learning (RL). In the SFT stage, the model\nlearns foundational patterns from pairs of research papers and follow-up ideas.\nIn the RL stage, multi-dimensional reward modeling, guided by fine-grained\nfeedback, evaluates and optimizes the generated ideas across key metrics.\nDimensional controllers enable dynamic adjustment of generation, while a\nsentence-level decoder ensures context-aware emphasis during inference. Our\nframework provides a balanced approach to research ideation, achieving\nhigh-quality outcomes by dynamically navigating the trade-offs among novelty,\nfeasibility, and effectiveness.", "published": "2024-12-19 08:28:18", "link": "http://arxiv.org/abs/2412.14626v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Analysis and Visualization of Linguistic Structures in Large Language\n  Models: Neural Representations of Verb-Particle Constructions in BERT", "abstract": "This study investigates the internal representations of verb-particle\ncombinations within transformer-based large language models (LLMs),\nspecifically examining how these models capture lexical and syntactic nuances\nat different neural network layers. Employing the BERT architecture, we analyse\nthe representational efficacy of its layers for various verb-particle\nconstructions such as 'agree on', 'come back', and 'give up'. Our methodology\nincludes a detailed dataset preparation from the British National Corpus,\nfollowed by extensive model training and output analysis through techniques\nlike multi-dimensional scaling (MDS) and generalized discrimination value (GDV)\ncalculations. Results show that BERT's middle layers most effectively capture\nsyntactic structures, with significant variability in representational accuracy\nacross different verb categories. These findings challenge the conventional\nuniformity assumed in neural network processing of linguistic elements and\nsuggest a complex interplay between network architecture and linguistic\nrepresentation. Our research contributes to a better understanding of how deep\nlearning models comprehend and process language, offering insights into the\npotential and limitations of current neural approaches to linguistic analysis.\nThis study not only advances our knowledge in computational linguistics but\nalso prompts further research into optimizing neural architectures for enhanced\nlinguistic precision.", "published": "2024-12-19 09:21:39", "link": "http://arxiv.org/abs/2412.14670v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Each Fake News is Fake in its Own Way: An Attribution Multi-Granularity\n  Benchmark for Multimodal Fake News Detection", "abstract": "Social platforms, while facilitating access to information, have also become\nsaturated with a plethora of fake news, resulting in negative consequences.\nAutomatic multimodal fake news detection is a worthwhile pursuit. Existing\nmultimodal fake news datasets only provide binary labels of real or fake.\nHowever, real news is alike, while each fake news is fake in its own way. These\ndatasets fail to reflect the mixed nature of various types of multimodal fake\nnews. To bridge the gap, we construct an attributing multi-granularity\nmultimodal fake news detection dataset \\amg, revealing the inherent fake\npattern. Furthermore, we propose a multi-granularity clue alignment model \\our\nto achieve multimodal fake news detection and attribution. Experimental results\ndemonstrate that \\amg is a challenging dataset, and its attribution setting\nopens up new avenues for future research.", "published": "2024-12-19 09:40:17", "link": "http://arxiv.org/abs/2412.14686v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mapping and Influencing the Political Ideology of Large Language Models\n  using Synthetic Personas", "abstract": "The analysis of political biases in large language models (LLMs) has\nprimarily examined these systems as single entities with fixed viewpoints.\nWhile various methods exist for measuring such biases, the impact of\npersona-based prompting on LLMs' political orientation remains unexplored. In\nthis work we leverage PersonaHub, a collection of synthetic persona\ndescriptions, to map the political distribution of persona-based prompted LLMs\nusing the Political Compass Test (PCT). We then examine whether these initial\ncompass distributions can be manipulated through explicit ideological prompting\ntowards diametrically opposed political orientations: right-authoritarian and\nleft-libertarian. Our experiments reveal that synthetic personas predominantly\ncluster in the left-libertarian quadrant, with models demonstrating varying\ndegrees of responsiveness when prompted with explicit ideological descriptors.\nWhile all models demonstrate significant shifts towards right-authoritarian\npositions, they exhibit more limited shifts towards left-libertarian positions,\nsuggesting an asymmetric response to ideological manipulation that may reflect\ninherent biases in model training.", "published": "2024-12-19 13:36:18", "link": "http://arxiv.org/abs/2412.14843v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Survey of RWKV", "abstract": "The Receptance Weighted Key Value (RWKV) model offers a novel alternative to\nthe Transformer architecture, merging the benefits of recurrent and\nattention-based systems. Unlike conventional Transformers, which depend heavily\non self-attention, RWKV adeptly captures long-range dependencies with minimal\ncomputational demands. By utilizing a recurrent framework, RWKV addresses some\ncomputational inefficiencies found in Transformers, particularly in tasks with\nlong sequences. RWKV has recently drawn considerable attention for its robust\nperformance across multiple domains. Despite its growing popularity, no\nsystematic review of the RWKV model exists. This paper seeks to fill this gap\nas the first comprehensive review of the RWKV architecture, its core\nprinciples, and its varied applications, such as natural language generation,\nnatural language understanding, and computer vision. We assess how RWKV\ncompares to traditional Transformer models, highlighting its capability to\nmanage long sequences efficiently and lower computational costs. Furthermore,\nwe explore the challenges RWKV encounters and propose potential directions for\nfuture research and advancement. We consistently maintain the related\nopen-source materials at: https://github.com/MLGroupJLU/RWKV-Survey.", "published": "2024-12-19 13:39:24", "link": "http://arxiv.org/abs/2412.14847v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dehallucinating Parallel Context Extension for Retrieval-Augmented\n  Generation", "abstract": "Large language models (LLMs) are susceptible to generating hallucinated\ninformation, despite the integration of retrieval-augmented generation (RAG).\nParallel context extension (PCE) is a line of research attempting to\neffectively integrating parallel (unordered) contexts, while it still suffers\nfrom hallucinations when adapted to RAG scenarios. In this paper, we propose\nDePaC (Dehallucinating Parallel Context Extension), which alleviates the\nhallucination problem with context-aware negative training and\ninformation-calibrated aggregation. DePaC is designed to alleviate two types of\nin-context hallucination: fact fabrication (i.e., LLMs present claims that are\nnot supported by the contexts) and fact omission (i.e., LLMs fail to present\nclaims that can be supported by the contexts). Specifically, (1) for fact\nfabrication, we apply the context-aware negative training that fine-tunes the\nLLMs with negative supervisions, thus explicitly guiding the LLMs to refuse to\nanswer when contexts are not related to questions; (2) for fact omission, we\npropose the information-calibrated aggregation which prioritizes context\nwindows with higher information increment from their contexts. The experimental\nresults on nine RAG tasks demonstrate that DePaC significantly alleviates the\ntwo types of hallucination and consistently achieves better performances on\nthese tasks.", "published": "2024-12-19 14:37:11", "link": "http://arxiv.org/abs/2412.14905v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RobustFT: Robust Supervised Fine-tuning for Large Language Models under\n  Noisy Response", "abstract": "Supervised fine-tuning (SFT) plays a crucial role in adapting large language\nmodels (LLMs) to specific domains or tasks. However, as demonstrated by\nempirical experiments, the collected data inevitably contains noise in\npractical applications, which poses significant challenges to model performance\non downstream tasks. Therefore, there is an urgent need for a noise-robust SFT\nframework to enhance model capabilities in downstream tasks. To address this\nchallenge, we introduce a robust SFT framework (RobustFT) that performs noise\ndetection and relabeling on downstream task data. For noise identification, our\napproach employs a multi-expert collaborative system with inference-enhanced\nmodels to achieve superior noise detection. In the denoising phase, we utilize\na context-enhanced strategy, which incorporates the most relevant and confident\nknowledge followed by careful assessment to generate reliable annotations.\nAdditionally, we introduce an effective data selection mechanism based on\nresponse entropy, ensuring only high-quality samples are retained for\nfine-tuning. Extensive experiments conducted on multiple LLMs across five\ndatasets demonstrate RobustFT's exceptional performance in noisy scenarios.", "published": "2024-12-19 15:00:18", "link": "http://arxiv.org/abs/2412.14922v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Knowledge Injection via Prompt Distillation", "abstract": "In many practical applications, large language models (LLMs) need to\nincorporate new knowledge not present in their pre-training data. The primary\nmethods for this are fine-tuning and retrieval-augmented generation (RAG).\nAlthough RAG has emerged as the industry standard for knowledge injection,\nfine-tuning has not yet achieved comparable success. In this paper, we propose\na new fine-tuning technique for learning new knowledge and show that it can\nreach the performance of RAG. The proposed method is based on the\nself-distillation approach, which we call prompt distillation. First, we\ngenerate question-answer pairs about the new knowledge. Then, we fine-tune a\nstudent model on the question-answer pairs to imitate the output distributions\nof a teacher model, which additionally receives the new knowledge in its\nprompt. The student model is identical to the teacher, except it is equipped\nwith a LoRA adapter. This training procedure facilitates distilling the new\nknowledge from the teacher's prompt into the student's weights.", "published": "2024-12-19 15:44:01", "link": "http://arxiv.org/abs/2412.14964v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Language Models as Continuous Self-Evolving Data Engineers", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities on\nvarious tasks, while the further evolvement is limited to the lack of\nhigh-quality training data. In addition, traditional training approaches rely\ntoo much on expert-labeled data, setting a ceiling on the performance of LLMs.\nTo address this issue, we propose a novel paradigm named LANCE (LANguage models\nas Continuous self-Evolving data engineers) that enables LLMs to train\nthemselves by autonomously generating, cleaning, reviewing, and annotating data\nwith preference information. Our approach demonstrates that LLMs can serve as\ncontinuous self-evolving data engineers, significantly reducing the time and\ncost of the post-training data construction. Through iterative fine-tuning on\nQwen2 series models, we validate the effectiveness of LANCE across various\ntasks, showing that it can maintain high-quality data generation and\ncontinuously improve model performance. Across multiple benchmark dimensions,\nLANCE results in an average score enhancement of 3.64 for Qwen2-7B and 1.75 for\nQwen2-7B-Instruct. This training paradigm with autonomous data construction not\nonly reduces the reliance on human experts or external models but also ensures\nthat the data aligns with human preferences, paving the way for the development\nof future superintelligent systems that can exceed human capabilities. Codes\nare available at: https://github.com/Control-derek/LANCE.", "published": "2024-12-19 18:28:41", "link": "http://arxiv.org/abs/2412.15151v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Critical-Questions-of-Thought: Steering LLM reasoning with Argumentative\n  Querying", "abstract": "Studies have underscored how, regardless of the recent breakthrough and swift\nadvances in AI research, even state-of-the-art Large Language models (LLMs)\ncontinue to struggle when performing logical and mathematical reasoning. The\nresults seem to suggest that LLMs still work as (highly advanced) data pattern\nidentifiers, scoring poorly when attempting to generalise and solve reasoning\nproblems the models have never previously seen or that are not close to samples\npresented in their training data. To address this compelling concern, this\npaper makes use of the notion of critical questions from the literature on\nargumentation theory, focusing in particular on Toulmin's model of\nargumentation. We show that employing these critical questions can improve the\nreasoning capabilities of LLMs. By probing the rationale behind the models'\nreasoning process, the LLM can assess whether some logical mistake is occurring\nand correct it before providing the final reply to the user prompt. The\nunderlying idea is drawn from the gold standard of any valid argumentative\nprocedure: the conclusion is valid if it is entailed by accepted premises. Or,\nto paraphrase such Aristotelian principle in a real-world approximation,\ncharacterised by incomplete information and presumptive logic, the conclusion\nis valid if not proved otherwise. This approach successfully steers the models'\noutput through a reasoning pipeline, resulting in better performance against\nthe baseline and its Chain-of-Thought (CoT) implementation. To this end, an\nextensive evaluation of the proposed approach on the MT-Bench Reasoning and\nMath tasks across a range of LLMs is provided.", "published": "2024-12-19 18:51:30", "link": "http://arxiv.org/abs/2412.15177v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Face the Facts! Evaluating RAG-based Fact-checking Pipelines in\n  Realistic Settings", "abstract": "Natural Language Processing and Generation systems have recently shown the\npotential to complement and streamline the costly and time-consuming job of\nprofessional fact-checkers. In this work, we lift several constraints of\ncurrent state-of-the-art pipelines for automated fact-checking based on the\nRetrieval-Augmented Generation (RAG) paradigm. Our goal is to benchmark, under\nmore realistic scenarios, RAG-based methods for the generation of verdicts -\ni.e., short texts discussing the veracity of a claim - evaluating them on\nstylistically complex claims and heterogeneous, yet reliable, knowledge bases.\nOur findings show a complex landscape, where, for example, LLM-based retrievers\noutperform other retrieval techniques, though they still struggle with\nheterogeneous knowledge bases; larger models excel in verdict faithfulness,\nwhile smaller models provide better context adherence, with human evaluations\nfavouring zero-shot and one-shot approaches for informativeness, and fine-tuned\nmodels for emotional alignment.", "published": "2024-12-19 18:57:11", "link": "http://arxiv.org/abs/2412.15189v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "LongBench v2: Towards Deeper Understanding and Reasoning on Realistic\n  Long-context Multitasks", "abstract": "This paper introduces LongBench v2, a benchmark designed to assess the\nability of LLMs to handle long-context problems requiring deep understanding\nand reasoning across real-world multitasks. LongBench v2 consists of 503\nchallenging multiple-choice questions, with contexts ranging from 8k to 2M\nwords, across six major task categories: single-document QA, multi-document QA,\nlong in-context learning, long-dialogue history understanding, code repository\nunderstanding, and long structured data understanding. To ensure the breadth\nand the practicality, we collect data from nearly 100 highly educated\nindividuals with diverse professional backgrounds. We employ both automated and\nmanual review processes to maintain high quality and difficulty, resulting in\nhuman experts achieving only 53.7% accuracy under a 15-minute time constraint.\nOur evaluation reveals that the best-performing model, when directly answers\nthe questions, achieves only 50.1% accuracy. In contrast, the o1-preview model,\nwhich includes longer reasoning, achieves 57.7%, surpassing the human baseline\nby 4%. These results highlight the importance of enhanced reasoning ability and\nscaling inference-time compute to tackle the long-context challenges in\nLongBench v2. The project is available at https://longbench2.github.io.", "published": "2024-12-19 18:59:17", "link": "http://arxiv.org/abs/2412.15204v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Large-Scale Simulation on Large Language Models for Decision-Making in\n  Political Science", "abstract": "While LLMs have demonstrated remarkable capabilities in text generation and\nreasoning, their ability to simulate human decision-making -- particularly in\npolitical contexts -- remains an open question. However, modeling voter\nbehavior presents unique challenges due to limited voter-level data, evolving\npolitical landscapes, and the complexity of human reasoning. In this study, we\ndevelop a theory-driven, multi-step reasoning framework that integrates\ndemographic, temporal and ideological factors to simulate voter decision-making\nat scale. Using synthetic personas calibrated to real-world voter data, we\nconduct large-scale simulations of recent U.S. presidential elections. Our\nmethod significantly improves simulation accuracy while mitigating model\nbiases. We examine its robustness by comparing performance across different\nLLMs. We further investigate the challenges and constraints that arise from\nLLM-based political simulations. Our work provides both a scalable framework\nfor modeling political decision-making behavior and insights into the promise\nand limitations of using LLMs in political science research.", "published": "2024-12-19 07:10:51", "link": "http://arxiv.org/abs/2412.15291v4", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Confidence in the Reasoning of Large Language Models", "abstract": "There is a growing literature on reasoning by large language models (LLMs),\nbut the discussion on the uncertainty in their responses is still lacking. Our\naim is to assess the extent of confidence that LLMs have in their answers and\nhow it correlates with accuracy. Confidence is measured (i) qualitatively in\nterms of persistence in keeping their answer when prompted to reconsider, and\n(ii) quantitatively in terms of self-reported confidence score. We investigate\nthe performance of three LLMs -- GPT4o, GPT4-turbo and Mistral -- on two\nbenchmark sets of questions on causal judgement and formal fallacies and a set\nof probability and statistical puzzles and paradoxes. Although the LLMs show\nsignificantly better performance than random guessing, there is a wide\nvariability in their tendency to change their initial answers. There is a\npositive correlation between qualitative confidence and accuracy, but the\noverall accuracy for the second answer is often worse than for the first\nanswer. There is a strong tendency to overstate the self-reported confidence\nscore. Confidence is only partially explained by the underlying token-level\nprobability. The material effects of prompting on qualitative confidence and\nthe strong tendency for overconfidence indicate that current LLMs do not have\nany internally coherent sense of confidence.", "published": "2024-12-19 10:04:29", "link": "http://arxiv.org/abs/2412.15296v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ViFactCheck: A New Benchmark Dataset and Methods for Multi-domain News\n  Fact-Checking in Vietnamese", "abstract": "The rapid spread of information in the digital age highlights the critical\nneed for effective fact-checking tools, particularly for languages with limited\nresources, such as Vietnamese. In response to this challenge, we introduce\nViFactCheck, the first publicly available benchmark dataset designed\nspecifically for Vietnamese fact-checking across multiple online news domains.\nThis dataset contains 7,232 human-annotated pairs of claim-evidence\ncombinations sourced from reputable Vietnamese online news, covering 12 diverse\ntopics. It has been subjected to a meticulous annotation process to ensure high\nquality and reliability, achieving a Fleiss Kappa inter-annotator agreement\nscore of 0.83. Our evaluation leverages state-of-the-art pre-trained and large\nlanguage models, employing fine-tuning and prompting techniques to assess\nperformance. Notably, the Gemma model demonstrated superior effectiveness, with\nan impressive macro F1 score of 89.90%, thereby establishing a new standard for\nfact-checking benchmarks. This result highlights the robust capabilities of\nGemma in accurately identifying and verifying facts in Vietnamese. To further\npromote advances in fact-checking technology and improve the reliability of\ndigital media, we have made the ViFactCheck dataset, model checkpoints,\nfact-checking pipelines, and source code freely available on GitHub. This\ninitiative aims to inspire further research and enhance the accuracy of\ninformation in low-resource languages.", "published": "2024-12-19 13:41:59", "link": "http://arxiv.org/abs/2412.15308v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Conceptual In-Context Learning and Chain of Concepts: Solving Complex\n  Conceptual Problems Using Large Language Models", "abstract": "Science and engineering problems fall in the category of complex conceptual\nproblems that require specific conceptual information (CI) like math/logic\n-related know-how, process information, or engineering guidelines to solve\nthem. Large Language Models (LLMs) are promising agents to solve such complex\nconceptual problems due to their implications in advancing engineering and\nscience tasks like assisted problem-solving. But vanilla LLMs, trained on\nopen-world data, lack the necessary CI. In this work, we specifically explore\nshallow customization methods (SCMs) of LLMs for solving complex conceptual\nproblems. We propose two novel SCM algorithms for LLM, to augment LLMs with CI\nand enable LLMs to solve complex conceptual problems: Conceptual In-Context\nLearning (C-ICL) and Chain of Concepts (CoC). The problem tackled in this paper\nis generation of proprietary data models in the engineering/industry domain\nbased on conceptual information in data modelling guidelines. We evaluate our\nalgorithms on varied sizes of the OpenAI LLMs against four evaluation metrics\nrelated to syntactic and semantic correctness, time and cost incurred. The\nproposed algorithms perform better than currently popular LLM SCMs like\nIn-context Learning (ICL) and Chain of Thoughts (CoT). It was observed that as\ncompared to CoT, response correctness increased by 30.6% and 29.88% for the new\nSCMs C-ICL and CoC respectively. Qualitative analysis suggests that the\nproposed new SCMs activate emergent capabilities in LLMs, previously unobserved\nin the existing SCMs. They make problem-solving processes more transparent and\nreduce hallucinations and the tendency of model responses to copy examples from\nprompts (parroting).", "published": "2024-12-19 13:54:33", "link": "http://arxiv.org/abs/2412.15309v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Eliciting Causal Abilities in Large Language Models for Reasoning Tasks", "abstract": "Prompt optimization automatically refines prompting expressions, unlocking\nthe full potential of LLMs in downstream tasks. However, current prompt\noptimization methods are costly to train and lack sufficient interpretability.\nThis paper proposes enhancing LLMs' reasoning performance by eliciting their\ncausal inference ability from prompting instructions to correct answers.\nSpecifically, we introduce the Self-Causal Instruction Enhancement (SCIE)\nmethod, which enables LLMs to generate high-quality, low-quantity observational\ndata, then estimates the causal effect based on these data, and ultimately\ngenerates instructions with the optimized causal effect. In SCIE, the\ninstructions are treated as the treatment, and textual features are used to\nprocess natural language, establishing causal relationships through treatments\nbetween instructions and downstream tasks. Additionally, we propose applying\nObject-Relational (OR) principles, where the uncovered causal relationships are\ntreated as the inheritable class across task objects, ensuring low-cost\nreusability. Extensive experiments demonstrate that our method effectively\ngenerates instructions that enhance reasoning performance with reduced training\ncost of prompts, leveraging interpretable textual features to provide\nactionable insights.", "published": "2024-12-19 17:03:02", "link": "http://arxiv.org/abs/2412.15314v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Systematic Evaluation of Long-Context LLMs on Financial Concepts", "abstract": "Long-context large language models (LC LLMs) promise to increase reliability\nof LLMs in real-world tasks requiring processing and understanding of long\ninput documents. However, this ability of LC LLMs to reliably utilize their\ngrowing context windows remains under investigation. In this work, we evaluate\nthe performance of state-of-the-art GPT-4 suite of LC LLMs in solving a series\nof progressively challenging tasks, as a function of factors such as context\nlength, task difficulty, and position of key information by creating a real\nworld financial news dataset. Our findings indicate that LC LLMs exhibit\nbrittleness at longer context lengths even for simple tasks, with performance\ndeteriorating sharply as task complexity increases. At longer context lengths,\nthese state-of-the-art models experience catastrophic failures in instruction\nfollowing resulting in degenerate outputs. Our prompt ablations also reveal\nunfortunate continued sensitivity to both the placement of the task instruction\nin the context window as well as minor markdown formatting. Finally, we\nadvocate for more rigorous evaluation of LC LLMs by employing holistic metrics\nsuch as F1 (rather than recall) and reporting confidence intervals, thereby\nensuring robust and conclusive findings.", "published": "2024-12-19 20:26:55", "link": "http://arxiv.org/abs/2412.15386v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Transcribing and Translating, Fast and Slow: Joint Speech Translation\n  and Recognition", "abstract": "We propose the joint speech translation and recognition (JSTAR) model that\nleverages the fast-slow cascaded encoder architecture for simultaneous\nend-to-end automatic speech recognition (ASR) and speech translation (ST). The\nmodel is transducer-based and uses a multi-objective training strategy that\noptimizes both ASR and ST objectives simultaneously. This allows JSTAR to\nproduce high-quality streaming ASR and ST results. We apply JSTAR in a\nbilingual conversational speech setting with smart-glasses, where the model is\nalso trained to distinguish speech from different directions corresponding to\nthe wearer and a conversational partner. Different model pre-training\nstrategies are studied to further improve results, including training of a\ntransducer-based streaming machine translation (MT) model for the first time\nand applying it for parameter initialization of JSTAR. We demonstrate superior\nperformances of JSTAR compared to a strong cascaded ST model in both BLEU\nscores and latency.", "published": "2024-12-19 21:57:01", "link": "http://arxiv.org/abs/2412.15415v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Northeastern Uni at Multilingual Counterspeech Generation: Enhancing\n  Counter Speech Generation with LLM Alignment through Direct Preference\n  Optimization", "abstract": "The automatic generation of counter-speech (CS) is a critical strategy for\naddressing hate speech by providing constructive and informed responses.\nHowever, existing methods often fail to generate high-quality, impactful, and\nscalable CS, particularly across diverse linguistic contexts. In this paper, we\npropose a novel methodology to enhance CS generation by aligning Large Language\nModels (LLMs) using Supervised Fine-Tuning (SFT) and Direct Preference\nOptimization (DPO). Our approach leverages DPO to align LLM outputs with human\npreferences, ensuring contextually appropriate and linguistically adaptable\nresponses. Additionally, we incorporate knowledge grounding to enhance the\nfactual accuracy and relevance of generated CS. Experimental results\ndemonstrate that DPO-aligned models significantly outperform SFT baselines on\nCS benchmarks while scaling effectively to multiple languages. These findings\nhighlight the potential of preference-based alignment techniques to advance CS\ngeneration across varied linguistic settings. The model supervision and\nalignment is done in English and the same model is used for reporting metrics\nacross other languages like Basque, Italian, and Spanish.", "published": "2024-12-19 23:22:11", "link": "http://arxiv.org/abs/2412.15453v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GraphEQA: Using 3D Semantic Scene Graphs for Real-time Embodied Question\n  Answering", "abstract": "In Embodied Question Answering (EQA), agents must explore and develop a\nsemantic understanding of an unseen environment in order to answer a situated\nquestion with confidence. This remains a challenging problem in robotics, due\nto the difficulties in obtaining useful semantic representations, updating\nthese representations online, and leveraging prior world knowledge for\nefficient exploration and planning. Aiming to address these limitations, we\npropose GraphEQA, a novel approach that utilizes real-time 3D metric-semantic\nscene graphs (3DSGs) and task relevant images as multi-modal memory for\ngrounding Vision-Language Models (VLMs) to perform EQA tasks in unseen\nenvironments. We employ a hierarchical planning approach that exploits the\nhierarchical nature of 3DSGs for structured planning and semantic-guided\nexploration. Through experiments in simulation on the HM-EQA dataset and in the\nreal world in home and office environments, we demonstrate that our method\noutperforms key baselines by completing EQA tasks with higher success rates and\nfewer planning steps.", "published": "2024-12-19 03:04:34", "link": "http://arxiv.org/abs/2412.14480v1", "categories": ["cs.RO", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.RO"}
{"title": "LDP: Generalizing to Multilingual Visual Information Extraction by\n  Language Decoupled Pretraining", "abstract": "Visual Information Extraction (VIE) plays a crucial role in the comprehension\nof semi-structured documents, and several pre-trained models have been\ndeveloped to enhance performance. However, most of these works are monolingual\n(usually English). Due to the extremely unbalanced quantity and quality of\npre-training corpora between English and other languages, few works can extend\nto non-English scenarios. In this paper, we conduct systematic experiments to\nshow that vision and layout modality hold invariance among images with\ndifferent languages. If decoupling language bias from document images, a\nvision-layout-based model can achieve impressive cross-lingual generalization.\nAccordingly, we present a simple but effective multilingual training paradigm\nLDP (Language Decoupled Pre-training) for better utilization of monolingual\npre-training data. Our proposed model LDM (Language Decoupled Model) is first\npre-trained on the language-independent data, where the language knowledge is\ndecoupled by a diffusion model, and then the LDM is fine-tuned on the\ndownstream languages. Extensive experiments show that the LDM outperformed all\nSOTA multilingual pre-trained models, and also maintains competitiveness on\ndownstream monolingual/English benchmarks.", "published": "2024-12-19 07:31:40", "link": "http://arxiv.org/abs/2412.14596v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Multi-modal, Multi-task, Multi-criteria Automatic Evaluation with Vision\n  Language Models", "abstract": "Vision-language models (VLMs) have shown impressive abilities across a range\nof multi-modal tasks. However, existing metrics for evaluating the quality of\ntext generated by VLMs typically focus on an overall evaluation for a specific\ntask, such as image captioning. While the overall evaluation is essential for\nany task, the criteria prioritized can differ depending on the task, making it\nchallenging for current metrics to adapt to multi-task scenarios. To address\nthis limitation, we propose HarmonicEval, a reference-free comprehensive\nevaluation metric that aggregates criterion-wise scores to produce the overall\nscore in a bottom-up manner. Furthermore, we construct the Multi-task\nMulti-criteria Human Evaluation (MMHE) dataset, which comprises 18,000 expert\nhuman judgments across four multi-modal tasks. Our experiments demonstrate that\nHarmonicEval achieves higher correlations with human judgments than\nconventional metrics while providing numerical scores for each criterion.", "published": "2024-12-19 08:03:16", "link": "http://arxiv.org/abs/2412.14613v2", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "How to Synthesize Text Data without Model Collapse?", "abstract": "Model collapse in synthetic data indicates that iterative training on\nself-generated data leads to a gradual decline in performance. With the\nproliferation of AI models, synthetic data will fundamentally reshape the web\ndata ecosystem. Future GPT-$\\{n\\}$ models will inevitably be trained on a blend\nof synthetic and human-produced data. In this paper, we focus on two questions:\nwhat is the impact of synthetic data on language model training, and how to\nsynthesize data without model collapse? We first pre-train language models\nacross different proportions of synthetic data, revealing a negative\ncorrelation between the proportion of synthetic data and model performance. We\nfurther conduct statistical analysis on synthetic data to uncover\ndistributional shift phenomenon and over-concentration of n-gram features.\nInspired by the above findings, we propose token editing on human-produced data\nto obtain semi-synthetic data. As a proof of concept, we theoretically\ndemonstrate that token-level editing can prevent model collapse, as the test\nerror is constrained by a finite upper bound. We conduct extensive experiments\non pre-training from scratch, continual pre-training, and supervised\nfine-tuning. The results validate our theoretical proof that token-level\nediting improves data quality and enhances model performance.", "published": "2024-12-19 09:43:39", "link": "http://arxiv.org/abs/2412.14689v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ALKAFI-LLAMA3: Fine-Tuning LLMs for Precise Legal Understanding in\n  Palestine", "abstract": "Large Language Models (LLMs) have demonstrated remarkable potential in\ndiverse domains, yet their application in the legal sector, particularly in\nlow-resource contexts, remains limited. This study addresses the challenges of\nadapting LLMs to the Palestinian legal domain, where political instability,\nfragmented legal frameworks, and limited AI resources hinder effective\nmachine-learning applications. We present a fine-tuned model based on a\nquantized version of Llama-3.2-1B-Instruct, trained on a synthetic data set\nderived from Palestinian legal texts. Using smaller-scale models and\nstrategically generated question-answer pairs, we achieve a cost-effective,\nlocally sustainable solution that provides accurate and contextually relevant\nlegal guidance. Our experiments demonstrate promising performance on various\nquery types, ranging from yes/no questions and narrative explanations to\ncomplex legal differentiations, while highlighting areas for improvement, such\nas handling calculation-based inquiries and structured list formatting. This\nwork provides a pathway for the deployment of AI-driven legal assistance tools\ntailored to the needs of resource-constrained environments.", "published": "2024-12-19 11:55:51", "link": "http://arxiv.org/abs/2412.14771v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Progressive Multimodal Reasoning via Active Retrieval", "abstract": "Multi-step multimodal reasoning tasks pose significant challenges for\nmultimodal large language models (MLLMs), and finding effective ways to enhance\ntheir performance in such scenarios remains an unresolved issue. In this paper,\nwe propose AR-MCTS, a universal framework designed to progressively improve the\nreasoning capabilities of MLLMs through Active Retrieval (AR) and Monte Carlo\nTree Search (MCTS). Our approach begins with the development of a unified\nretrieval module that retrieves key supporting insights for solving complex\nreasoning problems from a hybrid-modal retrieval corpus. To bridge the gap in\nautomated multimodal reasoning verification, we employ the MCTS algorithm\ncombined with an active retrieval mechanism, which enables the automatic\ngeneration of step-wise annotations. This strategy dynamically retrieves key\ninsights for each reasoning step, moving beyond traditional beam search\nsampling to improve the diversity and reliability of the reasoning space.\nAdditionally, we introduce a process reward model that aligns progressively to\nsupport the automatic verification of multimodal reasoning tasks. Experimental\nresults across three complex multimodal reasoning benchmarks confirm the\neffectiveness of the AR-MCTS framework in enhancing the performance of various\nmultimodal models. Further analysis demonstrates that AR-MCTS can optimize\nsampling diversity and accuracy, yielding reliable multimodal reasoning.", "published": "2024-12-19 13:25:39", "link": "http://arxiv.org/abs/2412.14835v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Movie2Story: A framework for understanding videos and telling stories in\n  the form of novel text", "abstract": "In recent years, large-scale models have achieved significant advancements,\naccompanied by the emergence of numerous high-quality benchmarks for evaluating\nvarious aspects of their comprehension abilities. However, most existing\nbenchmarks primarily focus on spatial understanding in static image tasks.\nWhile some benchmarks extend evaluations to temporal tasks, they fall short in\nassessing text generation under complex contexts involving long videos and rich\nauxiliary information. To address this limitation, we propose a novel\nbenchmark: the Multi-modal Story Generation Benchmark (MSBench), designed to\nevaluate text generation capabilities in scenarios enriched with auxiliary\ninformation. Our work introduces an innovative automatic dataset generation\nmethod to ensure the availability of accurate auxiliary information. On one\nhand, we leverage existing datasets and apply automated processes to generate\nnew evaluation datasets, significantly reducing manual efforts. On the other\nhand, we refine auxiliary data through systematic filtering and utilize\nstate-of-the-art models to ensure the fairness and accuracy of the ground-truth\ndatasets. Our experiments reveal that current Multi-modal Large Language Models\n(MLLMs) perform suboptimally under the proposed evaluation metrics,\nhighlighting significant gaps in their capabilities. To address these\nchallenges, we propose a novel model architecture and methodology to better\nhandle the overall process, demonstrating improvements on our benchmark.", "published": "2024-12-19 15:44:04", "link": "http://arxiv.org/abs/2412.14965v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Large Language Models and Code Security: A Systematic Literature Review", "abstract": "Large Language Models (LLMs) have emerged as powerful tools for automating\nvarious programming tasks, including security-related ones, such as detecting\nand fixing vulnerabilities. Despite their promising capabilities, when required\nto produce or modify pre-existing code, LLMs could introduce vulnerabilities\nunbeknown to the programmer. When analyzing code, they could miss clear\nvulnerabilities or signal nonexistent ones. In this Systematic Literature\nReview (SLR), we aim to investigate both the security benefits and potential\ndrawbacks of using LLMs for a variety of code-related tasks. In particular,\nfirst we focus on the types of vulnerabilities that could be introduced by\nLLMs, when used for producing code. Second, we analyze the capabilities of LLMs\nto detect and fix vulnerabilities, in any given code, and how the prompting\nstrategy of choice impacts their performance in these two tasks. Last, we\nprovide an in-depth analysis on how data poisoning attacks on LLMs can impact\nperformance in the aforementioned tasks.", "published": "2024-12-19 16:20:22", "link": "http://arxiv.org/abs/2412.15004v2", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Till the Layers Collapse: Compressing a Deep Neural Network through the\n  Lenses of Batch Normalization Layers", "abstract": "Today, deep neural networks are widely used since they can handle a variety\nof complex tasks. Their generality makes them very powerful tools in modern\ntechnology. However, deep neural networks are often overparameterized. The\nusage of these large models consumes a lot of computation resources. In this\npaper, we introduce a method called \\textbf{T}ill the \\textbf{L}ayers\n\\textbf{C}ollapse (TLC), which compresses deep neural networks through the\nlenses of batch normalization layers. By reducing the depth of these networks,\nour method decreases deep neural networks' computational requirements and\noverall latency. We validate our method on popular models such as Swin-T,\nMobileNet-V2, and RoBERTa, across both image classification and natural\nlanguage processing (NLP) tasks.", "published": "2024-12-19 17:26:07", "link": "http://arxiv.org/abs/2412.15077v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "AceMath: Advancing Frontier Math Reasoning with Post-Training and Reward\n  Modeling", "abstract": "In this paper, we introduce AceMath, a suite of frontier math models that\nexcel in solving complex math problems, along with highly effective reward\nmodels capable of evaluating generated solutions and reliably identifying the\ncorrect ones. To develop the instruction-tuned math models, we propose a\nsupervised fine-tuning (SFT) process that first achieves competitive\nperformance across general domains, followed by targeted fine-tuning for the\nmath domain using a carefully curated set of prompts and synthetically\ngenerated responses. The resulting model, AceMath-72B-Instruct greatly\noutperforms Qwen2.5-Math-72B-Instruct, GPT-4o and Claude-3.5 Sonnet. To develop\nmath-specialized reward model, we first construct AceMath-RewardBench, a\ncomprehensive and robust benchmark for evaluating math reward models across\ndiverse problems and difficulty levels. After that, we present a systematic\napproach to build our math reward models. The resulting model, AceMath-72B-RM,\nconsistently outperforms state-of-the-art reward models. Furthermore, when\ncombining AceMath-72B-Instruct with AceMath-72B-RM, we achieve the highest\naverage rm@8 score across the math reasoning benchmarks. We release model\nweights, training data, and evaluation benchmarks at:\nhttps://research.nvidia.com/labs/adlr/acemath", "published": "2024-12-19 17:29:44", "link": "http://arxiv.org/abs/2412.15084v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Cross-Domain Study of the Use of Persuasion Techniques in Online\n  Disinformation", "abstract": "Disinformation, irrespective of domain or language, aims to deceive or\nmanipulate public opinion, typically through employing advanced persuasion\ntechniques. Qualitative and quantitative research on the weaponisation of\npersuasion techniques in disinformation has been mostly topic-specific (e.g.,\nCOVID-19) with limited cross-domain studies, resulting in a lack of\ncomprehensive understanding of these strategies. This study employs a\nstate-of-the-art persuasion technique classifier to conduct a large-scale,\nmulti-domain analysis of the role of 16 persuasion techniques in disinformation\nnarratives. It shows how different persuasion techniques are employed\ndisproportionately in different disinformation domains. We also include a\ndetailed case study on climate change disinformation, highlighting how\nlinguistic, psychological, and cultural factors shape the adaptation of\npersuasion strategies to fit unique thematic contexts.", "published": "2024-12-19 17:46:13", "link": "http://arxiv.org/abs/2412.15098v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Associative memory inspires improvements for in-context learning using a\n  novel attention residual stream architecture", "abstract": "Large language models (LLMs) demonstrate an impressive ability to utilise\ninformation within the context of their input sequences to appropriately\nrespond to data unseen by the LLM during its training procedure. This ability\nis known as in-context learning (ICL). Humans and non-human animals demonstrate\nsimilar abilities, however their neural architectures differ substantially from\nLLMs. Despite this, a critical component within LLMs, the attention mechanism,\nresembles modern associative memory models, widely used in and influenced by\nthe computational neuroscience community to model biological memory systems.\nUsing this connection, we introduce an associative memory model capable of\nperforming ICL. We use this as inspiration for a novel residual stream\narchitecture which allows information to directly flow between attention heads.\nWe test this architecture during training within a two-layer Transformer and\nshow its ICL abilities manifest more quickly than without this modification. We\nthen apply our architecture in small language models with 8 million parameters,\nfocusing on attention head values, with results also indicating improved ICL\nperformance at this larger and more naturalistic scale.", "published": "2024-12-19 17:55:42", "link": "http://arxiv.org/abs/2412.15113v1", "categories": ["cs.NE", "cs.AI", "cs.CL", "92B20, 68T01, 68T37, 68T50", "I.2; I.5; I.7; J.2; J.3"], "primary_category": "cs.NE"}
{"title": "Outcome-Refining Process Supervision for Code Generation", "abstract": "Large Language Models have demonstrated remarkable capabilities in code\ngeneration, yet they often struggle with complex programming tasks that require\ndeep algorithmic reasoning. While process supervision through learned reward\nmodels shows promise in guiding reasoning steps, it requires expensive training\ndata and suffers from unreliable evaluation. We propose Outcome-Refining\nProcess Supervision, a novel paradigm that treats outcome refinement itself as\nthe process to be supervised. Our framework leverages concrete execution\nsignals to ground the supervision of reasoning steps, while using\ntree-structured exploration to maintain multiple solution trajectories\nsimultaneously. Experiments demonstrate that our approach enables even smaller\nmodels to achieve high success accuracy and performance metrics on competitive\nprogramming tasks, creates more reliable verification than traditional reward\nmodels without requiring training PRMs. Our approach achieves significant\nimprovements across 5 models and 3 datasets: an average of 26.9% increase in\ncorrectness and 42.2% in efficiency. The results suggest that providing\nstructured reasoning space with concrete verification signals is crucial for\nsolving complex programming tasks. We open-source all our code and data at:\nhttps://github.com/zhuohaoyu/ORPS", "published": "2024-12-19 17:59:42", "link": "http://arxiv.org/abs/2412.15118v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Adaptive Pruning for Large Language Models with Structural Importance\n  Awareness", "abstract": "The recent advancements in large language models (LLMs) have significantly\nimproved language understanding and generation capabilities. However, it is\ndifficult to deploy LLMs on resource-constrained edge devices due to their high\ncomputational and storage resource demands. To address this issue, we propose a\nnovel LLM model pruning method, namely structurally-aware adaptive pruning\n(SAAP), to significantly reduce the computational and memory costs while\nmaintaining model performance. We first define an adaptive importance fusion\nmetric to evaluate the importance of all coupled structures in LLMs by\nconsidering their homoscedastic uncertainty. Then, we rank the importance of\nall modules to determine the specific layers that should be pruned to meet\nparticular performance requirements. Furthermore, we develop a new group\nfine-tuning strategy to improve the inference efficiency of LLMs. Finally, we\nevaluate the proposed SAAP method on multiple LLMs across two common tasks,\ni.e., zero-shot classification and text generation. Experimental results show\nthat our SAAP method outperforms several state-of-the-art baseline methods,\nachieving 2.17%, 2.37%, and 2.39% accuracy gains on LLaMA-7B, Vicuna-7B, and\nLLaMA-13B. Additionally, SAAP improves the token generation speed by 5%,\nshowcasing its practical advantages in resource-constrained scenarios.", "published": "2024-12-19 18:08:04", "link": "http://arxiv.org/abs/2412.15127v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Prompt-A-Video: Prompt Your Video Diffusion Model via Preference-Aligned\n  LLM", "abstract": "Text-to-video models have made remarkable advancements through optimization\non high-quality text-video pairs, where the textual prompts play a pivotal role\nin determining quality of output videos. However, achieving the desired output\noften entails multiple revisions and iterative inference to refine\nuser-provided prompts. Current automatic methods for refining prompts encounter\nchallenges such as Modality-Inconsistency, Cost-Discrepancy, and Model-Unaware\nwhen applied to text-to-video diffusion models. To address these problem, we\nintroduce an LLM-based prompt adaptation framework, termed as Prompt-A-Video,\nwhich excels in crafting Video-Centric, Labor-Free and Preference-Aligned\nprompts tailored to specific video diffusion model. Our approach involves a\nmeticulously crafted two-stage optimization and alignment system. Initially, we\nconduct a reward-guided prompt evolution pipeline to automatically create\noptimal prompts pool and leverage them for supervised fine-tuning (SFT) of the\nLLM. Then multi-dimensional rewards are employed to generate pairwise data for\nthe SFT model, followed by the direct preference optimization (DPO) algorithm\nto further facilitate preference alignment. Through extensive experimentation\nand comparative analyses, we validate the effectiveness of Prompt-A-Video\nacross diverse generation models, highlighting its potential to push the\nboundaries of video generation.", "published": "2024-12-19 18:32:21", "link": "http://arxiv.org/abs/2412.15156v1", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "LMFusion: Adapting Pretrained Language Models for Multimodal Generation", "abstract": "We present LMFusion, a framework for empowering pretrained text-only large\nlanguage models (LLMs) with multimodal generative capabilities, enabling them\nto understand and generate both text and images in arbitrary sequences.\nLMFusion leverages existing Llama-3's weights for processing texts\nautoregressively while introducing additional and parallel transformer modules\nfor processing images with diffusion. During training, the data from each\nmodality is routed to its dedicated modules: modality-specific feedforward\nlayers, query-key-value projections, and normalization layers process each\nmodality independently, while the shared self-attention layers allow\ninteractions across text and image features. By freezing the text-specific\nmodules and only training the image-specific modules, LMFusion preserves the\nlanguage capabilities of text-only LLMs while developing strong visual\nunderstanding and generation abilities. Compared to methods that pretrain\nmultimodal generative models from scratch, our experiments demonstrate that,\nLMFusion improves image understanding by 20% and image generation by 3.6% using\nonly 50% of the FLOPs while maintaining Llama-3's language capabilities. We\nalso demonstrate that this framework can adapt existing vision-language models\nwith multimodal generation ability. Overall, this framework not only leverages\nexisting computational investments in text-only LLMs but also enables the\nparallel development of language and vision capabilities, presenting a\npromising direction for efficient multimodal model development.", "published": "2024-12-19 18:56:24", "link": "http://arxiv.org/abs/2412.15188v4", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Tokenisation is NP-Complete", "abstract": "In this work, we prove the NP-completeness of two variants of tokenisation,\ndefined as the problem of compressing a dataset to at most $\\delta$ symbols by\neither finding a vocabulary directly (direct tokenisation), or selecting a\nsequence of merge operations (bottom-up tokenisation).", "published": "2024-12-19 18:59:46", "link": "http://arxiv.org/abs/2412.15210v1", "categories": ["cs.DS", "cs.CL", "cs.FL"], "primary_category": "cs.DS"}
{"title": "SATA: A Paradigm for LLM Jailbreak via Simple Assistive Task Linkage", "abstract": "Large language models (LLMs) have made significant advancements across\nvarious tasks, but their safety alignment remain a major concern. Exploring\njailbreak prompts can expose LLMs' vulnerabilities and guide efforts to secure\nthem. Existing methods primarily design sophisticated instructions for the LLM\nto follow, or rely on multiple iterations, which could hinder the performance\nand efficiency of jailbreaks. In this work, we propose a novel jailbreak\nparadigm, Simple Assistive Task Linkage (SATA), which can effectively\ncircumvent LLM safeguards and elicit harmful responses. Specifically, SATA\nfirst masks harmful keywords within a malicious query to generate a relatively\nbenign query containing one or multiple [MASK] special tokens. It then employs\na simple assistive task such as a masked language model task or an element\nlookup by position task to encode the semantics of the masked keywords.\nFinally, SATA links the assistive task with the masked query to jointly perform\nthe jailbreak. Extensive experiments show that SATA achieves state-of-the-art\nperformance and outperforms baselines by a large margin. Specifically, on\nAdvBench dataset, with mask language model (MLM) assistive task, SATA achieves\nan overall attack success rate (ASR) of 85% and harmful score (HS) of 4.57, and\nwith element lookup by position (ELP) assistive task, SATA attains an overall\nASR of 76% and HS of 4.43.", "published": "2024-12-19 05:57:37", "link": "http://arxiv.org/abs/2412.15289v2", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "LAMA-UT: Language Agnostic Multilingual ASR through Orthography\n  Unification and Language-Specific Transliteration", "abstract": "Building a universal multilingual automatic speech recognition (ASR) model\nthat performs equitably across languages has long been a challenge due to its\ninherent difficulties. To address this task we introduce a Language-Agnostic\nMultilingual ASR pipeline through orthography Unification and language-specific\nTransliteration (LAMA-UT). LAMA-UT operates without any language-specific\nmodules while matching the performance of state-of-the-art models trained on a\nminimal amount of data. Our pipeline consists of two key steps. First, we\nutilize a universal transcription generator to unify orthographic features into\nRomanized form and capture common phonetic characteristics across diverse\nlanguages. Second, we utilize a universal converter to transform these\nuniversal transcriptions into language-specific ones. In experiments, we\ndemonstrate the effectiveness of our proposed method leveraging universal\ntranscriptions for massively multilingual ASR. Our pipeline achieves a relative\nerror reduction rate of 45% when compared to Whisper and performs comparably to\nMMS, despite being trained on only 0.1% of Whisper's training data.\nFurthermore, our pipeline does not rely on any language-specific modules.\nHowever, it performs on par with zero-shot ASR approaches which utilize\nadditional language-specific lexicons and language models. We expect this\nframework to serve as a cornerstone for flexible multilingual ASR systems that\nare generalizable even to unseen languages.", "published": "2024-12-19 10:39:08", "link": "http://arxiv.org/abs/2412.15299v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Learning Visual Composition through Improved Semantic Guidance", "abstract": "Visual imagery does not consist of solitary objects, but instead reflects the\ncomposition of a multitude of fluid concepts. While there have been great\nadvances in visual representation learning, such advances have focused on\nbuilding better representations for a small number of discrete objects bereft\nof an understanding of how these objects are interacting. One can observe this\nlimitation in representations learned through captions or contrastive learning\n-- where the learned model treats an image essentially as a bag of words.\nSeveral works have attempted to address this limitation through the development\nof bespoke learned architectures to directly address the shortcomings in\ncompositional learning. In this work, we focus on simple, and scalable\napproaches. In particular, we demonstrate that by substantially improving\nweakly labeled data, i.e. captions, we can vastly improve the performance of\nstandard contrastive learning approaches. Previous CLIP models achieved near\nchance rate on challenging tasks probing compositional learning. However, our\nsimple approach boosts performance of CLIP substantially and surpasses all\nbespoke architectures. Furthermore, we showcase our results on a relatively new\ncaptioning benchmark derived from DOCCI. We demonstrate through a series of\nablations that a standard CLIP model trained with enhanced data may demonstrate\nimpressive performance on image retrieval tasks.", "published": "2024-12-19 20:58:26", "link": "http://arxiv.org/abs/2412.15396v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.CV"}
{"title": "Time Will Tell: Timing Side Channels via Output Token Count in Large\n  Language Models", "abstract": "This paper demonstrates a new side-channel that enables an adversary to\nextract sensitive information about inference inputs in large language models\n(LLMs) based on the number of output tokens in the LLM response. We construct\nattacks using this side-channel in two common LLM tasks: recovering the target\nlanguage in machine translation tasks and recovering the output class in\nclassification tasks. In addition, due to the auto-regressive generation\nmechanism in LLMs, an adversary can recover the output token count reliably\nusing a timing channel, even over the network against a popular closed-source\ncommercial LLM. Our experiments show that an adversary can learn the output\nlanguage in translation tasks with more than 75% precision across three\ndifferent models (Tower, M2M100, MBart50). Using this side-channel, we also\nshow the input class in text classification tasks can be leaked out with more\nthan 70% precision from open-source LLMs like Llama-3.1, Llama-3.2, Gemma2, and\nproduction models like GPT-4o. Finally, we propose tokenizer-, system-, and\nprompt-based mitigations against the output token count side-channel.", "published": "2024-12-19 22:29:58", "link": "http://arxiv.org/abs/2412.15431v1", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Unveiling Uncertainty: A Deep Dive into Calibration and Performance of\n  Multimodal Large Language Models", "abstract": "Multimodal large language models (MLLMs) combine visual and textual data for\ntasks such as image captioning and visual question answering. Proper\nuncertainty calibration is crucial, yet challenging, for reliable use in areas\nlike healthcare and autonomous driving. This paper investigates representative\nMLLMs, focusing on their calibration across various scenarios, including before\nand after visual fine-tuning, as well as before and after multimodal training\nof the base LLMs. We observed miscalibration in their performance, and at the\nsame time, no significant differences in calibration across these scenarios. We\nalso highlight how uncertainty differs between text and images and how their\nintegration affects overall uncertainty. To better understand MLLMs'\nmiscalibration and their ability to self-assess uncertainty, we construct the\nIDK (I don't know) dataset, which is key to evaluating how they handle\nunknowns. Our findings reveal that MLLMs tend to give answers rather than admit\nuncertainty, but this self-assessment improves with proper prompt adjustments.\nFinally, to calibrate MLLMs and enhance model reliability, we propose\ntechniques such as temperature scaling and iterative prompt optimization. Our\nresults provide insights into improving MLLMs for effective and responsible\ndeployment in multimodal applications. Code and IDK dataset:\nhttps://github.com/hfutml/Calibration-MLLM.", "published": "2024-12-19 09:10:07", "link": "http://arxiv.org/abs/2412.14660v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CV"}
{"title": "TalkWithMachines: Enhancing Human-Robot Interaction for Interpretable\n  Industrial Robotics Through Large/Vision Language Models", "abstract": "TalkWithMachines aims to enhance human-robot interaction by contributing to\ninterpretable industrial robotic systems, especially for safety-critical\napplications. The presented paper investigates recent advancements in Large\nLanguage Models (LLMs) and Vision Language Models (VLMs), in combination with\nrobotic perception and control. This integration allows robots to understand\nand execute commands given in natural language and to perceive their\nenvironment through visual and/or descriptive inputs. Moreover, translating the\nLLM's internal states and reasoning into text that humans can easily understand\nensures that operators gain a clearer insight into the robot's current state\nand intentions, which is essential for effective and safe operation. Our paper\noutlines four LLM-assisted simulated robotic control workflows, which explore\n(i) low-level control, (ii) the generation of language-based feedback that\ndescribes the robot's internal states, (iii) the use of visual information as\nadditional input, and (iv) the use of robot structure information for\ngenerating task plans and feedback, taking the robot's physical capabilities\nand limitations into account. The proposed concepts are presented in a set of\nexperiments, along with a brief discussion. Project description, videos, and\nsupplementary materials will be available on the project website:\nhttps://talk-machines.github.io.", "published": "2024-12-19 23:43:40", "link": "http://arxiv.org/abs/2412.15462v1", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Bridging the Data Provenance Gap Across Text, Speech and Video", "abstract": "Progress in AI is driven largely by the scale and quality of training data.\nDespite this, there is a deficit of empirical analysis examining the attributes\nof well-established datasets beyond text. In this work we conduct the largest\nand first-of-its-kind longitudinal audit across modalities--popular text,\nspeech, and video datasets--from their detailed sourcing trends and use\nrestrictions to their geographical and linguistic representation. Our manual\nanalysis covers nearly 4000 public datasets between 1990-2024, spanning 608\nlanguages, 798 sources, 659 organizations, and 67 countries. We find that\nmultimodal machine learning applications have overwhelmingly turned to\nweb-crawled, synthetic, and social media platforms, such as YouTube, for their\ntraining sets, eclipsing all other sources since 2019. Secondly, tracing the\nchain of dataset derivations we find that while less than 33% of datasets are\nrestrictively licensed, over 80% of the source content in widely-used text,\nspeech, and video datasets, carry non-commercial restrictions. Finally, counter\nto the rising number of languages and geographies represented in public AI\ntraining datasets, our audit demonstrates measures of relative geographical and\nmultilingual representation have failed to significantly improve their coverage\nsince 2013. We believe the breadth of our audit enables us to empirically\nexamine trends in data sourcing, restrictions, and Western-centricity at an\necosystem-level, and that visibility into these questions are essential to\nprogress in responsible AI. As a contribution to ongoing improvements in\ndataset transparency and responsible use, we release our entire multimodal\naudit, allowing practitioners to trace data provenance across text, speech, and\nvideo.", "published": "2024-12-19 01:30:19", "link": "http://arxiv.org/abs/2412.17847v2", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG", "cs.MM"], "primary_category": "cs.AI"}
{"title": "Beyond the Sum: Unlocking AI Agents Potential Through Market Forces", "abstract": "The emergence of Large Language Models has fundamentally transformed the\ncapabilities of AI agents, enabling a new class of autonomous agents capable of\ninteracting with their environment through dynamic code generation and\nexecution. These agents possess the theoretical capacity to operate as\nindependent economic actors within digital markets, offering unprecedented\npotential for value creation through their distinct advantages in operational\ncontinuity, perfect replication, and distributed learning capabilities.\nHowever, contemporary digital infrastructure, architected primarily for human\ninteraction, presents significant barriers to their participation.\n  This work presents a systematic analysis of the infrastructure requirements\nnecessary for AI agents to function as autonomous participants in digital\nmarkets. We examine four key areas - identity and authorization, service\ndiscovery, interfaces, and payment systems - to show how existing\ninfrastructure actively impedes agent participation. We argue that addressing\nthese infrastructure challenges represents more than a technical imperative; it\nconstitutes a fundamental step toward enabling new forms of economic\norganization. Much as traditional markets enable human intelligence to\ncoordinate complex activities beyond individual capability, markets\nincorporating AI agents could dramatically enhance economic efficiency through\ncontinuous operation, perfect information sharing, and rapid adaptation to\nchanging conditions. The infrastructure challenges identified in this work\nrepresent key barriers to realizing this potential.", "published": "2024-12-19 09:40:40", "link": "http://arxiv.org/abs/2501.10388v2", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.GT", "cs.MA", "I.2.2; I.2.7; I.2.11; J.4; K.4.4"], "primary_category": "cs.CY"}
{"title": "Scale This, Not That: Investigating Key Dataset Attributes for Efficient\n  Speech Enhancement Scaling", "abstract": "Recent speech enhancement models have shown impressive performance gains by\nscaling up model complexity and training data. However, the impact of dataset\nvariability (e.g. text, language, speaker, and noise) has been underexplored.\nAnalyzing each attribute individually is often challenging, as multiple\nattributes are usually entangled in commonly used datasets, posing a\nsignificant obstacle in understanding the distinct contributions of each\nattribute to the model's performance. To address this challenge, we propose a\ngeneration-training-evaluation framework that leverages zero-shot\ntext-to-speech systems to investigate the impact of controlled attribute\nvariations on speech enhancement performance. It enables us to synthesize\ntraining datasets in a scalable manner while carefully altering each attribute.\nBased on the proposed framework, we analyze the scaling effects of various\ndataset attributes on the performance of both discriminative and generative SE\nmodels. Extensive experiments on multi-domain corpora imply that acoustic\nattributes (e.g., speaker and noise) are much more important to current speech\nenhancement models than semantic attributes (e.g., language and text), offering\nnew insights for future research.", "published": "2024-12-19 14:21:51", "link": "http://arxiv.org/abs/2412.14890v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "GIRAFE: Glottal Imaging Dataset for Advanced Segmentation, Analysis, and\n  Facilitative Playbacks Evaluation", "abstract": "The advances in the development of Facilitative Playbacks extracted from\nHigh-Speed videoendoscopic sequences of the vocal folds are hindered by a\nnotable lack of publicly available datasets annotated with the semantic\nsegmentations corresponding to the area of the glottal gap. This fact also\nlimits the reproducibility and further exploration of existing research in this\nfield.\n  To address this gap, GIRAFE is a data repository designed to facilitate the\ndevelopment of advanced techniques for the semantic segmentation, analysis, and\nfast evaluation of High-Speed videoendoscopic sequences of the vocal folds. The\nrepository includes 65 high-speed videoendoscopic recordings from a cohort of\n50 patients (30 female, 20 male). The dataset comprises 15 recordings from\nhealthy controls, 26 from patients with diagnosed voice disorders, and 24 with\nan unknown health condition. All of them were manually annotated by an expert,\nincluding the masks corresponding to the semantic segmentation of the glottal\ngap. The repository is also complemented with the automatic segmentation of the\nglottal area using different state-of-the-art approaches.\n  This data set has already supported several studies, which demonstrates its\nusefulness for the development of new glottal gap segmentation algorithms from\nHigh-Speed-Videoendoscopic sequences to improve or create new Facilitative\nPlaybacks. Despite these advances and others in the field, the broader\nchallenge of performing an accurate and completely automatic semantic\nsegmentation method of the glottal area remains open.", "published": "2024-12-19 17:02:03", "link": "http://arxiv.org/abs/2412.15054v1", "categories": ["cs.CV", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "AV-Link: Temporally-Aligned Diffusion Features for Cross-Modal\n  Audio-Video Generation", "abstract": "We propose AV-Link, a unified framework for Video-to-Audio (A2V) and\nAudio-to-Video (A2V) generation that leverages the activations of frozen video\nand audio diffusion models for temporally-aligned cross-modal conditioning. The\nkey to our framework is a Fusion Block that facilitates bidirectional\ninformation exchange between video and audio diffusion models through\ntemporally-aligned self attention operations. Unlike prior work that uses\ndedicated models for A2V and V2A tasks and relies on pretrained feature\nextractors, AV-Link achieves both tasks in a single self-contained framework,\ndirectly leveraging features obtained by the complementary modality (i.e. video\nfeatures to generate audio, or audio features to generate video). Extensive\nautomatic and subjective evaluations demonstrate that our method achieves a\nsubstantial improvement in audio-video synchronization, outperforming more\nexpensive baselines such as the MovieGen video-to-audio model.", "published": "2024-12-19 18:57:21", "link": "http://arxiv.org/abs/2412.15191v2", "categories": ["cs.CV", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "MMAudio: Taming Multimodal Joint Training for High-Quality\n  Video-to-Audio Synthesis", "abstract": "We propose to synthesize high-quality and synchronized audio, given video and\noptional text conditions, using a novel multimodal joint training framework\nMMAudio. In contrast to single-modality training conditioned on (limited) video\ndata only, MMAudio is jointly trained with larger-scale, readily available\ntext-audio data to learn to generate semantically aligned high-quality audio\nsamples. Additionally, we improve audio-visual synchrony with a conditional\nsynchronization module that aligns video conditions with audio latents at the\nframe level. Trained with a flow matching objective, MMAudio achieves new\nvideo-to-audio state-of-the-art among public models in terms of audio quality,\nsemantic alignment, and audio-visual synchronization, while having a low\ninference time (1.23s to generate an 8s clip) and just 157M parameters. MMAudio\nalso achieves surprisingly competitive performance in text-to-audio generation,\nshowing that joint training does not hinder single-modality performance. Code\nand demo are available at: https://hkchengrex.github.io/MMAudio", "published": "2024-12-19 18:59:55", "link": "http://arxiv.org/abs/2412.15322v2", "categories": ["cs.CV", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Stable-V2A: Synthesis of Synchronized Sound Effects with Temporal and\n  Semantic Controls", "abstract": "Sound designers and Foley artists usually sonorize a scene, such as from a\nmovie or video game, by manually annotating and sonorizing each action of\ninterest in the video. In our case, the intent is to leave full creative\ncontrol to sound designers with a tool that allows them to bypass the more\nrepetitive parts of their work, thus being able to focus on the creative\naspects of sound production. We achieve this presenting Stable-V2A, a two-stage\nmodel consisting of: an RMS-Mapper that estimates an envelope representative of\nthe audio characteristics associated with the input video; and Stable-Foley, a\ndiffusion model based on Stable Audio Open that generates audio semantically\nand temporally aligned with the target video. Temporal alignment is guaranteed\nby the use of the envelope as a ControlNet input, while semantic alignment is\nachieved through the use of sound representations chosen by the designer as\ncross-attention conditioning of the diffusion process. We train and test our\nmodel on Greatest Hits, a dataset commonly used to evaluate V2A models. In\naddition, to test our model on a case study of interest, we introduce Walking\nThe Maps, a dataset of videos extracted from video games depicting animated\ncharacters walking in different locations. Samples and code available on our\ndemo page at https://ispamm.github.io/Stable-V2A.", "published": "2024-12-19 16:37:19", "link": "http://arxiv.org/abs/2412.15023v2", "categories": ["cs.SD", "cs.CV", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
