{"title": "A Multilayer Convolutional Encoder-Decoder Neural Network for\n  Grammatical Error Correction", "abstract": "We improve automatic correction of grammatical, orthographic, and collocation\nerrors in text using a multilayer convolutional encoder-decoder neural network.\nThe network is initialized with embeddings that make use of character N-gram\ninformation to better suit this task. When evaluated on common benchmark test\ndata sets (CoNLL-2014 and JFLEG), our model substantially outperforms all prior\nneural approaches on this task as well as strong statistical machine\ntranslation-based systems with neural and task-specific features trained on the\nsame data. Our analysis shows the superiority of convolutional neural networks\nover recurrent neural networks such as long short-term memory (LSTM) networks\nin capturing the local context via attention, and thereby improving the\ncoverage in correcting grammatical errors. By ensembling multiple models, and\nincorporating an N-gram language model and edit features via rescoring, our\nnovel method becomes the first neural approach to outperform the current\nstate-of-the-art statistical machine translation-based approach, both in terms\nof grammaticality and fluency.", "published": "2018-01-26 14:45:24", "link": "http://arxiv.org/abs/1801.08831v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Simple Theoretical Model of Importance for Summarization", "abstract": "Research on summarization has mainly been driven by empirical approaches,\ncrafting systems to perform well on standard datasets with the notion of\ninformation Importance remaining latent. We argue that establishing theoretical\nmodels of Importance will advance our understanding of the task and help to\nfurther improve summarization systems. To this end, we propose simple but\nrigorous definitions of several concepts that were previously used only\nintuitively in summarization: Redundancy, Relevance, and Informativeness.\nImportance arises as a single quantity naturally unifying these concepts.\nAdditionally, we provide intuitions to interpret the proposed quantities and\nexperiments to demonstrate the potential of the framework to inform and guide\nsubsequent works.", "published": "2018-01-26 22:11:10", "link": "http://arxiv.org/abs/1801.08991v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Context Models for OOV Word Translation in Low-Resource Languages", "abstract": "Out-of-vocabulary word translation is a major problem for the translation of\nlow-resource languages that suffer from a lack of parallel training data. This\npaper evaluates the contributions of target-language context models towards the\ntranslation of OOV words, specifically in those cases where OOV translations\nare derived from external knowledge sources, such as dictionaries. We develop\nboth neural and non-neural context models and evaluate them within both\nphrase-based and self-attention based neural machine translation systems. Our\nresults show that neural language models that integrate additional context\nbeyond the current sentence are the most effective in disambiguating possible\nOOV word translations. We present an efficient second-pass lattice-rescoring\nmethod for wide-context neural language models and demonstrate performance\nimprovements over state-of-the-art self-attention based neural MT systems in\nfive out of six low-resource language pairs.", "published": "2018-01-26 02:50:03", "link": "http://arxiv.org/abs/1801.08660v1", "categories": ["cs.CL", "stat.ML", "68T50"], "primary_category": "cs.CL"}
