{"title": "Recent Advances and Challenges in Task-oriented Dialog System", "abstract": "Due to the significance and value in human-computer interaction and natural\nlanguage processing, task-oriented dialog systems are attracting more and more\nattention in both academic and industrial communities. In this paper, we survey\nrecent advances and challenges in task-oriented dialog systems. We also discuss\nthree critical topics for task-oriented dialog systems: (1) improving data\nefficiency to facilitate dialog modeling in low-resource settings, (2) modeling\nmulti-turn dynamics for dialog policy learning to achieve better\ntask-completion performance, and (3) integrating domain ontology knowledge into\nthe dialog model. Besides, we review the recent progresses in dialog evaluation\nand some widely-used corpora. We believe that this survey, though incomplete,\ncan shed a light on future research in task-oriented dialog systems.", "published": "2020-03-17 01:34:56", "link": "http://arxiv.org/abs/2003.07490v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-label natural language processing to identify diagnosis and\n  procedure codes from MIMIC-III inpatient notes", "abstract": "In the United States, 25% or greater than 200 billion dollars of hospital\nspending accounts for administrative costs that involve services for medical\ncoding and billing. With the increasing number of patient records, manual\nassignment of the codes performed is overwhelming, time-consuming and\nerror-prone, causing billing errors. Natural language processing can automate\nthe extraction of codes/labels from unstructured clinical notes, which can aid\nhuman coders to save time, increase productivity, and verify medical coding\nerrors. Our objective is to identify appropriate diagnosis and procedure codes\nfrom clinical notes by performing multi-label classification. We used\nde-identified data of critical care patients from the MIMIC-III database and\nsubset the data to select the ten (top-10) and fifty (top-50) most common\ndiagnoses and procedures, which covers 47.45% and 74.12% of all admissions\nrespectively. We implemented state-of-the-art Bidirectional Encoder\nRepresentations from Transformers (BERT) to fine-tune the language model on 80%\nof the data and validated on the remaining 20%. The model achieved an overall\naccuracy of 87.08%, an F1 score of 85.82%, and an AUC of 91.76% for top-10\ncodes. For the top-50 codes, our model achieved an overall accuracy of 93.76%,\nan F1 score of 92.24%, and AUC of 91%. When compared to previously published\nresearch, our model outperforms in predicting codes from the clinical text. We\ndiscuss approaches to generalize the knowledge discovery process of our\nMIMIC-BERT to other clinical notes. This can help human coders to save time,\nprevent backlogs, and additional costs due to coding errors.", "published": "2020-03-17 02:56:27", "link": "http://arxiv.org/abs/2003.07507v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "XPersona: Evaluating Multilingual Personalized Chatbot", "abstract": "Personalized dialogue systems are an essential step toward better\nhuman-machine interaction. Existing personalized dialogue agents rely on\nproperly designed conversational datasets, which are mostly monolingual (e.g.,\nEnglish), which greatly limits the usage of conversational agents in other\nlanguages. In this paper, we propose a multi-lingual extension of Persona-Chat,\nnamely XPersona. Our dataset includes persona conversations in six different\nlanguages other than English for building and evaluating multilingual\npersonalized agents. We experiment with both multilingual and cross-lingual\ntrained baselines, and evaluate them against monolingual and\ntranslation-pipeline models using both automatic and human evaluation.\nExperimental results show that the multilingual trained models outperform the\ntranslation-pipeline and that they are on par with the monolingual models, with\nthe advantage of having a single model across multiple languages. On the other\nhand, the state-of-the-art cross-lingual trained models achieve inferior\nperformance to the other models, showing that cross-lingual conversation\nmodeling is a challenging task. We hope that our dataset and baselines will\naccelerate research in multilingual dialogue systems.", "published": "2020-03-17 07:52:08", "link": "http://arxiv.org/abs/2003.07568v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adapting Deep Learning Methods for Mental Health Prediction on Social\n  Media", "abstract": "Mental health poses a significant challenge for an individual's well-being.\nText analysis of rich resources, like social media, can contribute to deeper\nunderstanding of illnesses and provide means for their early detection. We\ntackle a challenge of detecting social media users' mental status through deep\nlearning-based models, moving away from traditional approaches to the task. In\na binary classification task on predicting if a user suffers from one of nine\ndifferent disorders, a hierarchical attention network outperforms previously\nset benchmarks for four of the disorders. Furthermore, we explore the\nlimitations of our model and analyze phrases relevant for classification by\ninspecting the model's word-level attention weights.", "published": "2020-03-17 10:49:03", "link": "http://arxiv.org/abs/2003.07634v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PO-EMO: Conceptualization, Annotation, and Modeling of Aesthetic\n  Emotions in German and English Poetry", "abstract": "Most approaches to emotion analysis of social media, literature, news, and\nother domains focus exclusively on basic emotion categories as defined by Ekman\nor Plutchik. However, art (such as literature) enables engagement in a broader\nrange of more complex and subtle emotions. These have been shown to also\ninclude mixed emotional responses. We consider emotions in poetry as they are\nelicited in the reader, rather than what is expressed in the text or intended\nby the author. Thus, we conceptualize a set of aesthetic emotions that are\npredictive of aesthetic appreciation in the reader, and allow the annotation of\nmultiple labels per line to capture mixed emotions within their context. We\nevaluate this novel setting in an annotation experiment both with carefully\ntrained experts and via crowdsourcing. Our annotation with experts leads to an\nacceptable agreement of kappa = .70, resulting in a consistent dataset for\nfuture large scale analysis. Finally, we conduct first emotion classification\nexperiments based on BERT, showing that identifying aesthetic emotions is\nchallenging in our data, with up to .52 F1-micro on the German subset. Data and\nresources are available at https://github.com/tnhaider/poetry-emotion", "published": "2020-03-17 13:54:48", "link": "http://arxiv.org/abs/2003.07723v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PowerNorm: Rethinking Batch Normalization in Transformers", "abstract": "The standard normalization method for neural network (NN) models used in\nNatural Language Processing (NLP) is layer normalization (LN). This is\ndifferent than batch normalization (BN), which is widely-adopted in Computer\nVision. The preferred use of LN in NLP is principally due to the empirical\nobservation that a (naive/vanilla) use of BN leads to significant performance\ndegradation for NLP tasks; however, a thorough understanding of the underlying\nreasons for this is not always evident. In this paper, we perform a systematic\nstudy of NLP transformer models to understand why BN has a poor performance, as\ncompared to LN. We find that the statistics of NLP data across the batch\ndimension exhibit large fluctuations throughout training. This results in\ninstability, if BN is naively implemented. To address this, we propose Power\nNormalization (PN), a novel normalization scheme that resolves this issue by\n(i) relaxing zero-mean normalization in BN, (ii) incorporating a running\nquadratic mean instead of per batch statistics to stabilize fluctuations, and\n(iii) using an approximate backpropagation for incorporating the running\nstatistics in the forward pass. We show theoretically, under mild assumptions,\nthat PN leads to a smaller Lipschitz constant for the loss, compared with BN.\nFurthermore, we prove that the approximate backpropagation scheme leads to\nbounded gradients. We extensively test PN for transformers on a range of NLP\ntasks, and we show that it significantly outperforms both LN and BN. In\nparticular, PN outperforms LN by 0.4/0.6 BLEU on IWSLT14/WMT14 and 5.6/3.0 PPL\non PTB/WikiText-103. We make our code publicly available at\n\\url{https://github.com/sIncerass/powernorm}.", "published": "2020-03-17 17:50:26", "link": "http://arxiv.org/abs/2003.07845v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Calibration of Pre-trained Transformers", "abstract": "Pre-trained Transformers are now ubiquitous in natural language processing,\nbut despite their high end-task performance, little is known empirically about\nwhether they are calibrated. Specifically, do these models' posterior\nprobabilities provide an accurate empirical measure of how likely the model is\nto be correct on a given example? We focus on BERT and RoBERTa in this work,\nand analyze their calibration across three tasks: natural language inference,\nparaphrase detection, and commonsense reasoning. For each task, we consider\nin-domain as well as challenging out-of-domain settings, where models face more\nexamples they should be uncertain about. We show that: (1) when used\nout-of-the-box, pre-trained models are calibrated in-domain, and compared to\nbaselines, their calibration error out-of-domain can be as much as 3.5x lower;\n(2) temperature scaling is effective at further reducing calibration error\nin-domain, and using label smoothing to deliberately increase empirical\nuncertainty helps calibrate posteriors out-of-domain.", "published": "2020-03-17 18:58:44", "link": "http://arxiv.org/abs/2003.07892v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "High-Accuracy and Low-Latency Speech Recognition with Two-Head\n  Contextual Layer Trajectory LSTM Model", "abstract": "While the community keeps promoting end-to-end models over conventional\nhybrid models, which usually are long short-term memory (LSTM) models trained\nwith a cross entropy criterion followed by a sequence discriminative training\ncriterion, we argue that such conventional hybrid models can still be\nsignificantly improved. In this paper, we detail our recent efforts to improve\nconventional hybrid LSTM acoustic models for high-accuracy and low-latency\nautomatic speech recognition. To achieve high accuracy, we use a contextual\nlayer trajectory LSTM (cltLSTM), which decouples the temporal modeling and\ntarget classification tasks, and incorporates future context frames to get more\ninformation for accurate acoustic modeling. We further improve the training\nstrategy with sequence-level teacher-student learning. To obtain low latency,\nwe design a two-head cltLSTM, in which one head has zero latency and the other\nhead has a small latency, compared to an LSTM. When trained with Microsoft's 65\nthousand hours of anonymized training data and evaluated with test sets with\n1.8 million words, the proposed two-head cltLSTM model with the proposed\ntraining strategy yields a 28.2\\% relative WER reduction over the conventional\nLSTM acoustic model, with a similar perceived latency.", "published": "2020-03-17 00:52:11", "link": "http://arxiv.org/abs/2003.07482v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Overview of the TREC 2019 deep learning track", "abstract": "The Deep Learning Track is a new track for TREC 2019, with the goal of\nstudying ad hoc ranking in a large data regime. It is the first track with\nlarge human-labeled training sets, introducing two sets corresponding to two\ntasks, each with rigorous TREC-style blind evaluation and reusable test sets.\nThe document retrieval task has a corpus of 3.2 million documents with 367\nthousand training queries, for which we generate a reusable test set of 43\nqueries. The passage retrieval task has a corpus of 8.8 million passages with\n503 thousand training queries, for which we generate a reusable test set of 43\nqueries. This year 15 groups submitted a total of 75 runs, using various\ncombinations of deep learning, transfer learning and traditional IR ranking\nmethods. Deep learning runs significantly outperformed traditional IR runs.\nPossible explanations for this result are that we introduced large training\ndata and we included deep models trained on such data in our judging pools,\nwhereas some past studies did not have such training data or pooling.", "published": "2020-03-17 17:12:36", "link": "http://arxiv.org/abs/2003.07820v2", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Deliberation Model Based Two-Pass End-to-End Speech Recognition", "abstract": "End-to-end (E2E) models have made rapid progress in automatic speech\nrecognition (ASR) and perform competitively relative to conventional models. To\nfurther improve the quality, a two-pass model has been proposed to rescore\nstreamed hypotheses using the non-streaming Listen, Attend and Spell (LAS)\nmodel while maintaining a reasonable latency. The model attends to acoustics to\nrescore hypotheses, as opposed to a class of neural correction models that use\nonly first-pass text hypotheses. In this work, we propose to attend to both\nacoustics and first-pass hypotheses using a deliberation network. A\nbidirectional encoder is used to extract context information from first-pass\nhypotheses. The proposed deliberation model achieves 12% relative WER reduction\ncompared to LAS rescoring in Google Voice Search (VS) tasks, and 23% reduction\non a proper noun test set. Compared to a large conventional model, our best\nmodel performs 21% relatively better for VS. In terms of computational\ncomplexity, the deliberation decoder has a larger size than the LAS decoder,\nand hence requires more computations in second-pass decoding.", "published": "2020-03-17 22:01:12", "link": "http://arxiv.org/abs/2003.07962v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Author2Vec: A Framework for Generating User Embedding", "abstract": "Online forums and social media platforms provide noisy but valuable data\nevery day. In this paper, we propose a novel end-to-end neural network-based\nuser embedding system, Author2Vec. The model incorporates sentence\nrepresentations generated by BERT (Bidirectional Encoder Representations from\nTransformers) with a novel unsupervised pre-training objective, authorship\nclassification, to produce better user embedding that encodes useful\nuser-intrinsic properties. This user embedding system was pre-trained on post\ndata of 10k Reddit users and was analyzed and evaluated on two user\nclassification benchmarks: depression detection and personality classification,\nin which the model proved to outperform traditional count-based and\nprediction-based methods. We substantiate that Author2Vec successfully encoded\nuseful user attributes and the generated user embedding performs well in\ndownstream classification tasks without further finetuning.", "published": "2020-03-17 23:31:11", "link": "http://arxiv.org/abs/2003.11627v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Rat big, cat eaten! Ideas for a useful deep-agent protolanguage", "abstract": "Deep-agent communities developing their own language-like communication\nprotocol are a hot (or at least warm) topic in AI. Such agents could be very\nuseful in machine-machine and human-machine interaction scenarios long before\nthey have evolved a protocol as complex as human language. Here, I propose a\nsmall set of priorities we should focus on, if we want to get as fast as\npossible to a stage where deep agents speak a useful protolanguage.", "published": "2020-03-17 18:41:26", "link": "http://arxiv.org/abs/2003.11922v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multi-modal Dense Video Captioning", "abstract": "Dense video captioning is a task of localizing interesting events from an\nuntrimmed video and producing textual description (captions) for each localized\nevent. Most of the previous works in dense video captioning are solely based on\nvisual information and completely ignore the audio track. However, audio, and\nspeech, in particular, are vital cues for a human observer in understanding an\nenvironment. In this paper, we present a new dense video captioning approach\nthat is able to utilize any number of modalities for event description.\nSpecifically, we show how audio and speech modalities may improve a dense video\ncaptioning model. We apply automatic speech recognition (ASR) system to obtain\na temporally aligned textual description of the speech (similar to subtitles)\nand treat it as a separate input alongside video frames and the corresponding\naudio track. We formulate the captioning task as a machine translation problem\nand utilize recently proposed Transformer architecture to convert multi-modal\ninput data into textual descriptions. We demonstrate the performance of our\nmodel on ActivityNet Captions dataset. The ablation studies indicate a\nconsiderable contribution from audio and speech components suggesting that\nthese modalities contain substantial complementary information to video frames.\nFurthermore, we provide an in-depth analysis of the ActivityNet Caption results\nby leveraging the category tags obtained from original YouTube videos. Code is\npublicly available: github.com/v-iashin/MDVC", "published": "2020-03-17 15:15:17", "link": "http://arxiv.org/abs/2003.07758v2", "categories": ["cs.CV", "cs.CL", "cs.LG", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.CV"}
{"title": "High-Resolution Speaker Counting In Reverberant Rooms Using CRNN With\n  Ambisonics Features", "abstract": "Speaker counting is the task of estimating the number of people that are\nsimultaneously speaking in an audio recording. For several audio processing\ntasks such as speaker diarization, separation, localization and tracking,\nknowing the number of speakers at each timestep is a prerequisite, or at least\nit can be a strong advantage, in addition to enabling a low latency processing.\nFor that purpose, we address the speaker counting problem with a multichannel\nconvolutional recurrent neural network which produces an estimation at a\nshort-term frame resolution. We trained the network to predict up to 5\nconcurrent speakers in a multichannel mixture, with simulated data including\nmany different conditions in terms of source and microphone positions,\nreverberation, and noise. The network can predict the number of speakers with\ngood accuracy at frame resolution.", "published": "2020-03-17 17:42:22", "link": "http://arxiv.org/abs/2003.07839v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Deep Attention Fusion Feature for Speech Separation with End-to-End\n  Post-filter Method", "abstract": "In this paper, we propose an end-to-end post-filter method with deep\nattention fusion features for monaural speaker-independent speech separation.\nAt first, a time-frequency domain speech separation method is applied as the\npre-separation stage. The aim of pre-separation stage is to separate the\nmixture preliminarily. Although this stage can separate the mixture, it still\ncontains the residual interference. In order to enhance the pre-separated\nspeech and improve the separation performance further, the end-to-end\npost-filter (E2EPF) with deep attention fusion features is proposed. The E2EPF\ncan make full use of the prior knowledge of the pre-separated speech, which\ncontributes to speech separation. It is a fully convolutional speech separation\nnetwork and uses the waveform as the input features. Firstly, the 1-D\nconvolutional layer is utilized to extract the deep representation features for\nthe mixture and pre-separated signals in the time domain. Secondly, to pay more\nattention to the outputs of the pre-separation stage, an attention module is\napplied to acquire deep attention fusion features, which are extracted by\ncomputing the similarity between the mixture and the pre-separated speech.\nThese deep attention fusion features are conducive to reduce the interference\nand enhance the pre-separated speech. Finally, these features are sent to the\npost-filter to estimate each target signals. Experimental results on the\nWSJ0-2mix dataset show that the proposed method outperforms the\nstate-of-the-art speech separation method. Compared with the pre-separation\nmethod, our proposed method can acquire 64.1%, 60.2%, 25.6% and 7.5% relative\nimprovements in scale-invariant source-to-noise ratio (SI-SNR), the\nsignal-to-distortion ratio (SDR), the perceptual evaluation of speech quality\n(PESQ) and the short-time objective intelligibility (STOI) measures,\nrespectively.", "published": "2020-03-17 05:43:12", "link": "http://arxiv.org/abs/2003.07544v1", "categories": ["eess.AS", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
