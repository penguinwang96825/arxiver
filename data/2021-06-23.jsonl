{"title": "Zero-Shot Joint Modeling of Multiple Spoken-Text-Style Conversion Tasks\n  using Switching Tokens", "abstract": "In this paper, we propose a novel spoken-text-style conversion method that\ncan simultaneously execute multiple style conversion modules such as\npunctuation restoration and disfluency deletion without preparing matched\ndatasets. In practice, transcriptions generated by automatic speech recognition\nsystems are not highly readable because they often include many disfluencies\nand do not include punctuation marks. To improve their readability, multiple\nspoken-text-style conversion modules that individually model a single\nconversion task are cascaded because matched datasets that simultaneously\nhandle multiple conversion tasks are often unavailable. However, the cascading\nis unstable against the order of tasks because of the chain of conversion\nerrors. Besides, the computation cost of the cascading must be higher than the\nsingle conversion. To execute multiple conversion tasks simultaneously without\npreparing matched datasets, our key idea is to distinguish individual\nconversion tasks using the on-off switch. In our proposed zero-shot joint\nmodeling, we switch the individual tasks using multiple switching tokens,\nenabling us to utilize a zero-shot learning approach to executing simultaneous\nconversions. Our experiments on joint modeling of disfluency deletion and\npunctuation restoration demonstrate the effectiveness of our method.", "published": "2021-06-23 02:53:14", "link": "http://arxiv.org/abs/2106.12131v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Recognising Biomedical Names: Challenges and Solutions", "abstract": "The growth rate in the amount of biomedical documents is staggering.\nUnlocking information trapped in these documents can enable researchers and\npractitioners to operate confidently in the information world. Biomedical NER,\nthe task of recognising biomedical names, is usually employed as the first step\nof the NLP pipeline. Standard NER models, based on sequence tagging technique,\nare good at recognising short entity mentions in the generic domain. However,\nthere are several open challenges of applying these models to recognise\nbiomedical names: 1) Biomedical names may contain complex inner structure\n(discontinuity and overlapping) which cannot be recognised using standard\nsequence tagging technique; 2) The training of NER models usually requires\nlarge amount of labelled data, which are difficult to obtain in the biomedical\ndomain; and, 3) Commonly used language representation models are pre-trained on\ngeneric data; a domain shift therefore exists between these models and target\nbiomedical data. To deal with these challenges, we explore several research\ndirections and make the following contributions: 1) we propose a\ntransition-based NER model which can recognise discontinuous mentions; 2) We\ndevelop a cost-effective approach that nominates the suitable pre-training\ndata; and, 3) We design several data augmentation methods for NER. Our\ncontributions have obvious practical implications, especially when new\nbiomedical applications are needed. Our proposed data augmentation methods can\nhelp the NER model achieve decent performance, requiring only a small amount of\nlabelled data. Our investigation regarding selecting pre-training data can\nimprove the model by incorporating language representation models, which are\npre-trained using in-domain data. Finally, our proposed transition-based NER\nmodel can further improve the performance by recognising discontinuous\nmentions.", "published": "2021-06-23 08:20:13", "link": "http://arxiv.org/abs/2106.12230v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reinforcement Learning-based Dialogue Guided Event Extraction to Exploit\n  Argument Relations", "abstract": "Event extraction is a fundamental task for natural language processing.\nFinding the roles of event arguments like event participants is essential for\nevent extraction. However, doing so for real-life event descriptions is\nchallenging because an argument's role often varies in different contexts.\nWhile the relationship and interactions between multiple arguments are useful\nfor settling the argument roles, such information is largely ignored by\nexisting approaches. This paper presents a better approach for event extraction\nby explicitly utilizing the relationships of event arguments. We achieve this\nthrough a carefully designed task-oriented dialogue system. To model the\nargument relation, we employ reinforcement learning and incremental learning to\nextract multiple arguments via a multi-turned, iterative process. Our approach\nleverages knowledge of the already extracted arguments of the same sentence to\ndetermine the role of arguments that would be difficult to decide individually.\nIt then uses the newly obtained information to improve the decisions of\npreviously extracted arguments. This two-way feedback process allows us to\nexploit the argument relations to effectively settle argument roles, leading to\nbetter sentence understanding and event extraction. Experimental results show\nthat our approach consistently outperforms seven state-of-the-art event\nextraction methods for the classification of events and argument role and\nargument identification.", "published": "2021-06-23 13:24:39", "link": "http://arxiv.org/abs/2106.12384v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "End-to-End Lexically Constrained Machine Translation for Morphologically\n  Rich Languages", "abstract": "Lexically constrained machine translation allows the user to manipulate the\noutput sentence by enforcing the presence or absence of certain words and\nphrases. Although current approaches can enforce terms to appear in the\ntranslation, they often struggle to make the constraint word form agree with\nthe rest of the generated output. Our manual analysis shows that 46% of the\nerrors in the output of a baseline constrained model for English to Czech\ntranslation are related to agreement. We investigate mechanisms to allow neural\nmachine translation to infer the correct word inflection given lemmatized\nconstraints. In particular, we focus on methods based on training the model\nwith constraints provided as part of the input sequence. Our experiments on the\nEnglish-Czech language pair show that this approach improves the translation of\nconstrained terms in both automatic and manual evaluation by reducing errors in\nagreement. Our approach thus eliminates inflection errors, without introducing\nnew errors or decreasing the overall quality of the translation.", "published": "2021-06-23 13:40:13", "link": "http://arxiv.org/abs/2106.12398v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mixtures of Deep Neural Experts for Automated Speech Scoring", "abstract": "The paper copes with the task of automatic assessment of second language\nproficiency from the language learners' spoken responses to test prompts. The\ntask has significant relevance to the field of computer assisted language\nlearning. The approach presented in the paper relies on two separate modules:\n(1) an automatic speech recognition system that yields text transcripts of the\nspoken interactions involved, and (2) a multiple classifier system based on\ndeep learners that ranks the transcripts into proficiency classes. Different\ndeep neural network architectures (both feed-forward and recurrent) are\nspecialized over diverse representations of the texts in terms of: a reference\ngrammar, the outcome of probabilistic language models, several word embeddings,\nand two bag-of-word models. Combination of the individual classifiers is\nrealized either via a probabilistic pseudo-joint model, or via a neural mixture\nof experts. Using the data of the third Spoken CALL Shared Task challenge, the\nhighest values to date were obtained in terms of three popular evaluation\nmetrics.", "published": "2021-06-23 15:44:50", "link": "http://arxiv.org/abs/2106.12475v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep Multi-Task Model for Sarcasm Detection and Sentiment Analysis in\n  Arabic Language", "abstract": "The prominence of figurative language devices, such as sarcasm and irony,\nposes serious challenges for Arabic Sentiment Analysis (SA). While previous\nresearch works tackle SA and sarcasm detection separately, this paper\nintroduces an end-to-end deep Multi-Task Learning (MTL) model, allowing\nknowledge interaction between the two tasks. Our MTL model's architecture\nconsists of a Bidirectional Encoder Representation from Transformers (BERT)\nmodel, a multi-task attention interaction module, and two task classifiers. The\noverall obtained results show that our proposed model outperforms its\nsingle-task counterparts on both SA and sarcasm detection sub-tasks.", "published": "2021-06-23 16:00:32", "link": "http://arxiv.org/abs/2106.12488v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BERT-based Multi-Task Model for Country and Province Level Modern\n  Standard Arabic and Dialectal Arabic Identification", "abstract": "Dialect and standard language identification are crucial tasks for many\nArabic natural language processing applications. In this paper, we present our\ndeep learning-based system, submitted to the second NADI shared task for\ncountry-level and province-level identification of Modern Standard Arabic (MSA)\nand Dialectal Arabic (DA). The system is based on an end-to-end deep Multi-Task\nLearning (MTL) model to tackle both country-level and province-level MSA/DA\nidentification. The latter MTL model consists of a shared Bidirectional Encoder\nRepresentation Transformers (BERT) encoder, two task-specific attention layers,\nand two classifiers. Our key idea is to leverage both the task-discriminative\nand the inter-task shared features for country and province MSA/DA\nidentification. The obtained results show that our MTL model outperforms\nsingle-task models on most subtasks.", "published": "2021-06-23 16:07:58", "link": "http://arxiv.org/abs/2106.12495v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CharacterChat: Supporting the Creation of Fictional Characters through\n  Conversation and Progressive Manifestation with a Chatbot", "abstract": "We present CharacterChat, a concept and chatbot to support writers in\ncreating fictional characters. Concretely, writers progressively turn the bot\ninto their imagined character through conversation. We iteratively developed\nCharacterChat in a user-centred approach, starting with a survey on character\ncreation with writers (N=30), followed by two qualitative user studies (N=7 and\nN=8). Our prototype combines two modes: (1) Guided prompts help writers define\ncharacter attributes (e.g. User: \"Your name is Jane.\"), including suggestions\nfor attributes (e.g. Bot: \"What is my main motivation?\") and values, realised\nas a rule-based system with a concept network. (2) Open conversation with the\nchatbot helps writers explore their character and get inspiration, realised\nwith a language model that takes into account the defined character attributes.\nOur user studies reveal benefits particularly for early stages of character\ncreation, and challenges due to limited conversational capabilities. We\nconclude with lessons learned and ideas for future work.", "published": "2021-06-23 11:22:27", "link": "http://arxiv.org/abs/2106.12314v1", "categories": ["cs.HC", "cs.CL", "H.5.2; I.2.7"], "primary_category": "cs.HC"}
{"title": "PALRACE: Reading Comprehension Dataset with Human Data and Labeled\n  Rationales", "abstract": "Pre-trained language models achieves high performance on machine reading\ncomprehension (MRC) tasks but the results are hard to explain. An appealing\napproach to make models explainable is to provide rationales for its decision.\nTo investigate whether human rationales can further improve current models and\nto facilitate supervised learning of human rationales, here we present PALRACE\n(Pruned And Labeled RACE), a new MRC dataset with human labeled rationales for\n800 passages selected from the RACE dataset. We further classified the question\nto each passage into 6 types. Each passage was read by at least 26 human\nreaders, who labeled their rationales to answer the question. It is\ndemonstrated that models such as RoBERTa-large outperforms human readers in all\n6 types of questions, including inference questions, but its performance can be\nfurther improved when having access to the human rationales. Simpler models and\npre-trained models that are not fine-tuned based on the task benefit more from\nhuman rationales, and their performance can be boosted by more than 30% by\nrationales. With access to human rationales, a simple model based on the GloVe\nword embedding can reach the performance of BERT-base.", "published": "2021-06-23 13:12:40", "link": "http://arxiv.org/abs/2106.12373v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Clinical Named Entity Recognition using Contextualized Token\n  Representations", "abstract": "The clinical named entity recognition (CNER) task seeks to locate and\nclassify clinical terminologies into predefined categories, such as diagnostic\nprocedure, disease disorder, severity, medication, medication dosage, and sign\nsymptom. CNER facilitates the study of side-effect on medications including\nidentification of novel phenomena and human-focused information extraction.\nExisting approaches in extracting the entities of interests focus on using\nstatic word embeddings to represent each word. However, one word can have\ndifferent interpretations that depend on the context of the sentences.\nEvidently, static word embeddings are insufficient to integrate the diverse\ninterpretation of a word. To overcome this challenge, the technique of\ncontextualized word embedding has been introduced to better capture the\nsemantic meaning of each word based on its context. Two of these language\nmodels, ELMo and Flair, have been widely used in the field of Natural Language\nProcessing to generate the contextualized word embeddings on domain-generic\ndocuments. However, these embeddings are usually too general to capture the\nproximity among vocabularies of specific domains. To facilitate various\ndownstream applications using clinical case reports (CCRs), we pre-train two\ndeep contextualized language models, Clinical Embeddings from Language Model\n(C-ELMo) and Clinical Contextual String Embeddings (C-Flair) using the\nclinical-related corpus from the PubMed Central. Explicit experiments show that\nour models gain dramatic improvements compared to both static word embeddings\nand domain-generic language models.", "published": "2021-06-23 18:12:58", "link": "http://arxiv.org/abs/2106.12608v1", "categories": ["cs.CL", "q-bio.QM"], "primary_category": "cs.CL"}
{"title": "Gender Recognition in Informal and Formal Language Scenarios via\n  Transfer Learning", "abstract": "The interest in demographic information retrieval based on text data has\nincreased in the research community because applications have shown success in\ndifferent sectors such as security, marketing, heath-care, and others.\nRecognition and identification of demographic traits such as gender, age,\nlocation, or personality based on text data can help to improve different\nmarketing strategies. For instance it makes it possible to segment and to\npersonalize offers, thus products and services are exposed to the group of\ngreatest interest. This type of technology has been discussed widely in\ndocuments from social media. However, the methods have been poorly studied in\ndata with a more formal structure, where there is no access to emoticons,\nmentions, and other linguistic phenomena that are only present in social media.\nThis paper proposes the use of recurrent and convolutional neural networks, and\na transfer learning strategy for gender recognition in documents that are\nwritten in informal and formal languages. Models are tested in two different\ndatabases consisting of Tweets and call-center conversations. Accuracies of up\nto 75\\% are achieved for both databases. The results also indicate that it is\npossible to transfer the knowledge from a system trained on a specific type of\nexpressions or idioms such as those typically used in social media into a more\nformal type of text data, where the amount of data is more scarce and its\nstructure is completely different.", "published": "2021-06-23 15:32:50", "link": "http://arxiv.org/abs/2107.02759v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "NodePiece: Compositional and Parameter-Efficient Representations of\n  Large Knowledge Graphs", "abstract": "Conventional representation learning algorithms for knowledge graphs (KG) map\neach entity to a unique embedding vector. Such a shallow lookup results in a\nlinear growth of memory consumption for storing the embedding matrix and incurs\nhigh computational costs when working with real-world KGs. Drawing parallels\nwith subword tokenization commonly used in NLP, we explore the landscape of\nmore parameter-efficient node embedding strategies with possibly sublinear\nmemory requirements. To this end, we propose NodePiece, an anchor-based\napproach to learn a fixed-size entity vocabulary. In NodePiece, a vocabulary of\nsubword/sub-entity units is constructed from anchor nodes in a graph with known\nrelation types. Given such a fixed-size vocabulary, it is possible to bootstrap\nan encoding and embedding for any entity, including those unseen during\ntraining. Experiments show that NodePiece performs competitively in node\nclassification, link prediction, and relation prediction tasks while retaining\nless than 10% of explicit nodes in a graph as anchors and often having 10x\nfewer parameters. To this end, we show that a NodePiece-enabled model\noutperforms existing shallow models on a large OGB WikiKG 2 graph having 70x\nfewer parameters.", "published": "2021-06-23 03:51:03", "link": "http://arxiv.org/abs/2106.12144v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Classifying Textual Data with Pre-trained Vision Models through Transfer\n  Learning and Data Transformations", "abstract": "Knowledge is acquired by humans through experience, and no boundary is set\nbetween the kinds of knowledge or skill levels we can achieve on different\ntasks at the same time. When it comes to Neural Networks, that is not the case.\nThe breakthroughs in the field are extremely task and domain-specific. Vision\nand language are dealt with in separate manners, using separate methods and\ndifferent datasets. Current text classification methods, mostly rely on\nobtaining contextual embeddings for input text samples, then training a\nclassifier on the embedded dataset. Transfer learning in Language-related tasks\nin general, is heavily used in obtaining the contextual text embeddings for the\ninput samples. In this work, we propose to use the knowledge acquired by\nbenchmark Vision Models which are trained on ImageNet to help a much smaller\narchitecture learn to classify text. A data transformation technique is used to\ncreate a new image dataset, where each image represents a sentence embedding\nfrom the last six layers of BERT, projected on a 2D plane using a t-SNE based\nmethod. We trained five models containing early layers sliced from vision\nmodels which are pretrained on ImageNet, on the created image dataset for the\nIMDB dataset embedded with the last six layers of BERT. Despite the challenges\nposed by the very different datasets, experimental results achieved by this\napproach which links large pretrained models on both language and vision, are\nvery promising, without employing compute resources. Specifically, Sentiment\nAnalysis is achieved by five different models on the same image dataset\nobtained after BERT embeddings are transformed into gray scale images.\n  Index Terms: BERT, Convolutional Neural Networks, Domain Adaptation, image\nclassification, Natural Language Processing, t-SNE, text classification,\nTransfer Learning", "published": "2021-06-23 15:53:38", "link": "http://arxiv.org/abs/2106.12479v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Stable, Fast and Accurate: Kernelized Attention with Relative Positional\n  Encoding", "abstract": "The attention module, which is a crucial component in Transformer, cannot\nscale efficiently to long sequences due to its quadratic complexity. Many works\nfocus on approximating the dot-then-exponentiate softmax function in the\noriginal attention, leading to sub-quadratic or even linear-complexity\nTransformer architectures. However, we show that these methods cannot be\napplied to more powerful attention modules that go beyond the\ndot-then-exponentiate style, e.g., Transformers with relative positional\nencoding (RPE). Since in many state-of-the-art models, relative positional\nencoding is used as default, designing efficient Transformers that can\nincorporate RPE is appealing. In this paper, we propose a novel way to\naccelerate attention calculation for Transformers with RPE on top of the\nkernelized attention. Based upon the observation that relative positional\nencoding forms a Toeplitz matrix, we mathematically show that kernelized\nattention with RPE can be calculated efficiently using Fast Fourier Transform\n(FFT). With FFT, our method achieves $\\mathcal{O}(n\\log n)$ time complexity.\nInterestingly, we further demonstrate that properly using relative positional\nencoding can mitigate the training instability problem of vanilla kernelized\nattention. On a wide range of tasks, we empirically show that our models can be\ntrained from scratch without any optimization issues. The learned model\nperforms better than many efficient Transformer variants and is faster than\nstandard Transformer in the long-sequence regime.", "published": "2021-06-23 17:51:26", "link": "http://arxiv.org/abs/2106.12566v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Dealing with training and test segmentation mismatch: FBK@IWSLT2021", "abstract": "This paper describes FBK's system submission to the IWSLT 2021 Offline Speech\nTranslation task. We participated with a direct model, which is a\nTransformer-based architecture trained to translate English speech audio data\ninto German texts. The training pipeline is characterized by knowledge\ndistillation and a two-step fine-tuning procedure. Both knowledge distillation\nand the first fine-tuning step are carried out on manually segmented real and\nsynthetic data, the latter being generated with an MT system trained on the\navailable corpora. Differently, the second fine-tuning step is carried out on a\nrandom segmentation of the MuST-C v2 En-De dataset. Its main goal is to reduce\nthe performance drops occurring when a speech translation model trained on\nmanually segmented data (i.e. an ideal, sentence-like segmentation) is\nevaluated on automatically segmented audio (i.e. actual, more realistic testing\nconditions). For the same purpose, a custom hybrid segmentation procedure that\naccounts for both audio content (pauses) and for the length of the produced\nsegments is applied to the test data before passing them to the system. At\ninference time, we compared this procedure with a baseline segmentation method\nbased on Voice Activity Detection (VAD). Our results indicate the effectiveness\nof the proposed hybrid approach, shown by a reduction of the gap with manual\nsegmentation from 8.3 to 1.4 BLEU points.", "published": "2021-06-23 18:11:32", "link": "http://arxiv.org/abs/2106.12607v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Charformer: Fast Character Transformers via Gradient-based Subword\n  Tokenization", "abstract": "State-of-the-art models in natural language processing rely on separate rigid\nsubword tokenization algorithms, which limit their generalization ability and\nadaptation to new settings. In this paper, we propose a new model inductive\nbias that learns a subword tokenization end-to-end as part of the model. To\nthis end, we introduce a soft gradient-based subword tokenization module (GBST)\nthat automatically learns latent subword representations from characters in a\ndata-driven fashion. Concretely, GBST enumerates candidate subword blocks and\nlearns to score them in a position-wise fashion using a block scoring network.\nWe additionally introduce Charformer, a deep Transformer model that integrates\nGBST and operates on the byte level. Via extensive experiments on English GLUE,\nmultilingual, and noisy text datasets, we show that Charformer outperforms a\nseries of competitive byte-level baselines while generally performing on par\nand sometimes outperforming subword-based models. Additionally, Charformer is\nfast, improving the speed of both vanilla byte-level and subword-level\nTransformers by 28%-100% while maintaining competitive quality. We believe this\nwork paves the way for highly performant token-free models that are trained\ncompletely end-to-end.", "published": "2021-06-23 22:24:14", "link": "http://arxiv.org/abs/2106.12672v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Enrollment-less training for personalized voice activity detection", "abstract": "We present a novel personalized voice activity detection (PVAD) learning\nmethod that does not require enrollment data during training. PVAD is a task to\ndetect the speech segments of a specific target speaker at the frame level\nusing enrollment speech of the target speaker. Since PVAD must learn speakers'\nspeech variations to clarify the boundary between speakers, studies on PVAD\nused large-scale datasets that contain many utterances for each speaker.\nHowever, the datasets to train a PVAD model are often limited because\nsubstantial cost is needed to prepare such a dataset. In addition, we cannot\nutilize the datasets used to train the standard VAD because they often lack\nspeaker labels. To solve these problems, our key idea is to use one utterance\nas both a kind of enrollment speech and an input to the PVAD during training,\nwhich enables PVAD training without enrollment speech. In our proposed method,\ncalled enrollment-less training, we augment one utterance so as to create\nvariability between the input and the enrollment speech while keeping the\nspeaker identity, which avoids the mismatch between training and inference. Our\nexperimental results demonstrate the efficacy of the method.", "published": "2021-06-23 02:53:51", "link": "http://arxiv.org/abs/2106.12132v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speech is Silver, Silence is Golden: What do ASVspoof-trained Models\n  Really Learn?", "abstract": "We present our analysis of a significant data artifact in the official\n2019/2021 ASVspoof Challenge Dataset. We identify an uneven distribution of\nsilence duration in the training and test splits, which tends to correlate with\nthe target prediction label. Bonafide instances tend to have significantly\nlonger leading and trailing silences than spoofed instances. In this paper, we\nexplore this phenomenon and its impact in depth. We compare several types of\nmodels trained on a) only the duration of the leading silence and b) only on\nthe duration of leading and trailing silence. Results show that models trained\non only the duration of the leading silence perform particularly well, and\nachieve up to 85% percent accuracy and an equal error rate (EER) of 15.1%. At\nthe same time, we observe that trimming silence during pre-processing and then\ntraining established antispoofing models using signal-based features leads to\ncomparatively worse performance. In that case, EER increases from 3.6% (with\nsilence) to 15.5% (trimmed silence). Our findings suggest that previous work\nmay, in part, have inadvertently learned thespoof/bonafide distinction by\nrelying on the duration of silence as it appears in the official challenge\ndataset. We discuss the potential consequences that this has for interpreting\nsystem scores in the challenge and discuss how the ASV community may further\nconsider this issue.", "published": "2021-06-23 08:28:59", "link": "http://arxiv.org/abs/2106.12914v4", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Deep Neural Network Based Respiratory Pathology Classification Using\n  Cough Sounds", "abstract": "Intelligent systems are transforming the world, as well as our healthcare\nsystem. We propose a deep learning-based cough sound classification model that\ncan distinguish between children with healthy versus pathological coughs such\nas asthma, upper respiratory tract infection (URTI), and lower respiratory\ntract infection (LRTI). In order to train a deep neural network model, we\ncollected a new dataset of cough sounds, labelled with clinician's diagnosis.\nThe chosen model is a bidirectional long-short term memory network (BiLSTM)\nbased on Mel Frequency Cepstral Coefficients (MFCCs) features. The resulting\ntrained model when trained for classifying two classes of coughs -- healthy or\npathology (in general or belonging to a specific respiratory pathology),\nreaches accuracy exceeding 84\\% when classifying cough to the label provided by\nthe physicians' diagnosis. In order to classify subject's respiratory pathology\ncondition, results of multiple cough epochs per subject were combined. The\nresulting prediction accuracy exceeds 91\\% for all three respiratory\npathologies. However, when the model is trained to classify and discriminate\namong the four classes of coughs, overall accuracy dropped: one class of\npathological coughs are often misclassified as other. However, if one consider\nthe healthy cough classified as healthy and pathological cough classified to\nhave some kind of pathologies, then the overall accuracy of four class model is\nabove 84\\%. A longitudinal study of MFCC feature space when comparing\npathological and recovered coughs collected from the same subjects revealed the\nfact that pathological cough irrespective of the underlying conditions occupy\nthe same feature space making it harder to differentiate only using MFCC\nfeatures.", "published": "2021-06-23 05:49:20", "link": "http://arxiv.org/abs/2106.12174v1", "categories": ["cs.LG", "cs.MM", "cs.SD", "eess.AS", "62-XX, 92-XX, 68Txx,", "J.3; I.2"], "primary_category": "cs.LG"}
{"title": "Unsupervised Speech Enhancement using Dynamical Variational\n  Auto-Encoders", "abstract": "Dynamical variational autoencoders (DVAEs) are a class of deep generative\nmodels with latent variables, dedicated to model time series of\nhigh-dimensional data. DVAEs can be considered as extensions of the variational\nautoencoder (VAE) that include temporal dependencies between successive\nobserved and/or latent vectors. Previous work has shown the interest of using\nDVAEs over the VAE for speech spectrograms modeling. Independently, the VAE has\nbeen successfully applied to speech enhancement in noise, in an unsupervised\nnoise-agnostic set-up that requires neither noise samples nor noisy speech\nsamples at training time, but only requires clean speech signals. In this\npaper, we extend these works to DVAE-based single-channel unsupervised speech\nenhancement, hence exploiting both speech signals unsupervised representation\nlearning and dynamics modeling. We propose an unsupervised speech enhancement\nalgorithm that combines a DVAE speech prior pre-trained on clean speech signals\nwith a noise model based on nonnegative matrix factorization, and we derive a\nvariational expectation-maximization (VEM) algorithm to perform speech\nenhancement. The algorithm is presented with the most general DVAE formulation\nand is then applied with three specific DVAE models to illustrate the\nversatility of the framework. Experimental results show that the proposed\nDVAE-based approach outperforms its VAE-based counterpart, as well as several\nsupervised and unsupervised noise-dependent baselines, especially when the\nnoise type is unseen during training.", "published": "2021-06-23 09:48:38", "link": "http://arxiv.org/abs/2106.12271v3", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
