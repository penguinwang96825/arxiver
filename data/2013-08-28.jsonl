{"title": "NRC-Canada: Building the State-of-the-Art in Sentiment Analysis of\n  Tweets", "abstract": "In this paper, we describe how we created two state-of-the-art SVM\nclassifiers, one to detect the sentiment of messages such as tweets and SMS\n(message-level task) and one to detect the sentiment of a term within a\nsubmissions stood first in both tasks on tweets, obtaining an F-score of 69.02\nin the message-level task and 88.93 in the term-level task. We implemented a\nvariety of surface-form, semantic, and sentiment features. with sentiment-word\nhashtags, and one from tweets with emoticons. In the message-level task, the\nlexicon-based features provided a gain of 5 F-score points over all others.\nBoth of our systems can be replicated us available resources.", "published": "2013-08-28 18:23:03", "link": "http://arxiv.org/abs/1308.6242v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Crowdsourcing a Word-Emotion Association Lexicon", "abstract": "Even though considerable attention has been given to the polarity of words\n(positive and negative) and the creation of large polarity lexicons, research\nin emotion analysis has had to rely on limited and small emotion lexicons. In\nthis paper we show how the combined strength and wisdom of the crowds can be\nused to generate a large, high-quality, word-emotion and word-polarity\nassociation lexicon quickly and inexpensively. We enumerate the challenges in\nemotion annotation in a crowdsourcing scenario and propose solutions to address\nthem. Most notably, in addition to questions about emotions associated with\nterms, we show how the inclusion of a word choice question can discourage\nmalicious data entry, help identify instances where the annotator may not be\nfamiliar with the target term (allowing us to reject such annotations), and\nhelp obtain annotations at sense level (rather than at word level). We\nconducted experiments on how to formulate the emotion-annotation questions, and\nshow that asking if a term is associated with an emotion leads to markedly\nhigher inter-annotator agreement than that obtained by asking if a term evokes\nan emotion.", "published": "2013-08-28 20:13:32", "link": "http://arxiv.org/abs/1308.6297v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Computing Lexical Contrast", "abstract": "Knowing the degree of semantic contrast between words has widespread\napplication in natural language processing, including machine translation,\ninformation retrieval, and dialogue systems. Manually-created lexicons focus on\nopposites, such as {\\rm hot} and {\\rm cold}. Opposites are of many kinds such\nas antipodals, complementaries, and gradable. However, existing lexicons often\ndo not classify opposites into the different kinds. They also do not explicitly\nlist word pairs that are not opposites but yet have some degree of contrast in\nmeaning, such as {\\rm warm} and {\\rm cold} or {\\rm tropical} and {\\rm\nfreezing}. We propose an automatic method to identify contrasting word pairs\nthat is based on the hypothesis that if a pair of words, $A$ and $B$, are\ncontrasting, then there is a pair of opposites, $C$ and $D$, such that $A$ and\n$C$ are strongly related and $B$ and $D$ are strongly related. (For example,\nthere exists the pair of opposites {\\rm hot} and {\\rm cold} such that {\\rm\ntropical} is related to {\\rm hot,} and {\\rm freezing} is related to {\\rm\ncold}.) We will call this the contrast hypothesis. We begin with a large\ncrowdsourcing experiment to determine the amount of human agreement on the\nconcept of oppositeness and its different kinds. In the process, we flesh out\nkey features of different kinds of opposites. We then present an automatic and\nempirical measure of lexical contrast that relies on the contrast hypothesis,\ncorpus statistics, and the structure of a {\\it Roget}-like thesaurus. We show\nthat the proposed measure of lexical contrast obtains high precision and large\ncoverage, outperforming existing methods.", "published": "2013-08-28 20:24:27", "link": "http://arxiv.org/abs/1308.6300v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
