{"title": "Modeling Discourse Structure for Document-level Neural Machine\n  Translation", "abstract": "Recently, document-level neural machine translation (NMT) has become a hot\ntopic in the community of machine translation. Despite its success, most of\nexisting studies ignored the discourse structure information of the input\ndocument to be translated, which has shown effective in other tasks. In this\npaper, we propose to improve document-level NMT with the aid of discourse\nstructure information. Our encoder is based on a hierarchical attention network\n(HAN). Specifically, we first parse the input document to obtain its discourse\nstructure. Then, we introduce a Transformer-based path encoder to embed the\ndiscourse structure information of each word. Finally, we combine the discourse\nstructure information with the word embedding before it is fed into the\nencoder. Experimental results on the English-to-German dataset show that our\nmodel can significantly outperform both Transformer and Transformer+HAN.", "published": "2020-06-08 16:24:03", "link": "http://arxiv.org/abs/2006.04721v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What's the Difference Between Professional Human and Machine\n  Translation? A Blind Multi-language Study on Domain-specific MT", "abstract": "Machine translation (MT) has been shown to produce a number of errors that\nrequire human post-editing, but the extent to which professional human\ntranslation (HT) contains such errors has not yet been compared to MT. We\ncompile pre-translated documents in which MT and HT are interleaved, and ask\nprofessional translators to flag errors and post-edit these documents in a\nblind evaluation. We find that the post-editing effort for MT segments is only\nhigher in two out of three language pairs, and that the number of segments with\nwrong terminology, omissions, and typographical problems is similar in HT.", "published": "2020-06-08 17:55:14", "link": "http://arxiv.org/abs/2006.04781v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Counterfactual VQA: A Cause-Effect Look at Language Bias", "abstract": "VQA models may tend to rely on language bias as a shortcut and thus fail to\nsufficiently learn the multi-modal knowledge from both vision and language.\nRecent debiasing methods proposed to exclude the language prior during\ninference. However, they fail to disentangle the \"good\" language context and\n\"bad\" language bias from the whole. In this paper, we investigate how to\nmitigate language bias in VQA. Motivated by causal effects, we proposed a novel\ncounterfactual inference framework, which enables us to capture the language\nbias as the direct causal effect of questions on answers and reduce the\nlanguage bias by subtracting the direct language effect from the total causal\neffect. Experiments demonstrate that our proposed counterfactual inference\nframework 1) is general to various VQA backbones and fusion strategies, 2)\nachieves competitive performance on the language-bias sensitive VQA-CP dataset\nwhile performs robustly on the balanced VQA v2 dataset without any augmented\ndata. The code is available at https://github.com/yuleiniu/cfvqa.", "published": "2020-06-08 01:49:27", "link": "http://arxiv.org/abs/2006.04315v4", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Characterizing Sociolinguistic Variation in the Competing Vaccination\n  Communities", "abstract": "Public health practitioners and policy makers grapple with the challenge of\ndevising effective message-based interventions for debunking public health\nmisinformation in cyber communities. \"Framing\" and \"personalization\" of the\nmessage is one of the key features for devising a persuasive messaging\nstrategy. For an effective health communication, it is imperative to focus on\n\"preference-based framing\" where the preferences of the target sub-community\nare taken into consideration. To achieve that, it is important to understand\nand hence characterize the target sub-communities in terms of their social\ninteractions. In the context of health-related misinformation, vaccination\nremains to be the most prevalent topic of discord. Hence, in this paper, we\nconduct a sociolinguistic analysis of the two competing vaccination communities\non Twitter: \"pro-vaxxers\" or individuals who believe in the effectiveness of\nvaccinations, and \"anti-vaxxers\" or individuals who are opposed to\nvaccinations. Our data analysis show significant linguistic variation between\nthe two communities in terms of their usage of linguistic intensifiers,\npronouns, and uncertainty words. Our network-level analysis show significant\ndifferences between the two communities in terms of their network density,\necho-chamberness, and the EI index. We hypothesize that these sociolinguistic\ndifferences can be used as proxies to characterize and understand these\ncommunities to devise better message interventions.", "published": "2020-06-08 03:05:28", "link": "http://arxiv.org/abs/2006.04334v3", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Combining word embeddings and convolutional neural networks to detect\n  duplicated questions", "abstract": "Detecting semantic similarities between sentences is still a challenge today\ndue to the ambiguity of natural languages. In this work, we propose a simple\napproach to identifying semantically similar questions by combining the\nstrengths of word embeddings and Convolutional Neural Networks (CNNs). In\naddition, we demonstrate how the cosine similarity metric can be used to\neffectively compare feature vectors. Our network is trained on the Quora\ndataset, which contains over 400k question pairs. We experiment with different\nembedding approaches such as Word2Vec, Fasttext, and Doc2Vec and investigate\nthe effects these approaches have on model performance. Our model achieves\ncompetitive results on the Quora dataset and complements the well-established\nevidence that CNNs can be utilized for paraphrase detection tasks.", "published": "2020-06-08 12:30:25", "link": "http://arxiv.org/abs/2006.04513v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Comprehensive Survey on Aspect Based Sentiment Analysis", "abstract": "Aspect Based Sentiment Analysis (ABSA) is the sub-field of Natural Language\nProcessing that deals with essentially splitting our data into aspects ad\nfinally extracting the sentiment information. ABSA is known to provide more\ninformation about the context than general sentiment analysis. In this study,\nour aim is to explore the various methodologies practiced while performing\nABSA, and providing a comparative study. This survey paper discusses various\nsolutions in-depth and gives a comparison between them. And is conveniently\ndivided into sections to get a holistic view on the process.", "published": "2020-06-08 14:07:58", "link": "http://arxiv.org/abs/2006.04611v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ColdGANs: Taming Language GANs with Cautious Sampling Strategies", "abstract": "Training regimes based on Maximum Likelihood Estimation (MLE) suffer from\nknown limitations, often leading to poorly generated text sequences. At the\nroot of these limitations is the mismatch between training and inference, i.e.\nthe so-called exposure bias, exacerbated by considering only the reference\ntexts as correct, while in practice several alternative formulations could be\nas good. Generative Adversarial Networks (GANs) can mitigate those limitations\nbut the discrete nature of text has hindered their application to language\ngeneration: the approaches proposed so far, based on Reinforcement Learning,\nhave been shown to underperform MLE. Departing from previous works, we analyze\nthe exploration step in GANs applied to text generation, and show how classical\nsampling results in unstable training. We propose to consider alternative\nexploration strategies in a GAN framework that we name ColdGANs, where we force\nthe sampling to be close to the distribution modes to get smoother learning\ndynamics. For the first time, to the best of our knowledge, the proposed\nlanguage GANs compare favorably to MLE, and obtain improvements over the\nstate-of-the-art on three generative tasks, namely unconditional text\ngeneration, question generation, and abstractive summarization.", "published": "2020-06-08 14:48:14", "link": "http://arxiv.org/abs/2006.04643v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Read what you need: Controllable Aspect-based Opinion Summarization of\n  Tourist Reviews", "abstract": "Manually extracting relevant aspects and opinions from large volumes of\nuser-generated text is a time-consuming process. Summaries, on the other hand,\nhelp readers with limited time budgets to quickly consume the key ideas from\nthe data. State-of-the-art approaches for multi-document summarization,\nhowever, do not consider user preferences while generating summaries. In this\nwork, we argue the need and propose a solution for generating personalized\naspect-based opinion summaries from large collections of online tourist\nreviews. We let our readers decide and control several attributes of the\nsummary such as the length and specific aspects of interest among others.\nSpecifically, we take an unsupervised approach to extract coherent aspects from\ntourist reviews posted on TripAdvisor. We then propose an Integer Linear\nProgramming (ILP) based extractive technique to select an informative subset of\nopinions around the identified aspects while respecting the user-specified\nvalues for various control parameters. Finally, we evaluate and compare our\nsummaries using crowdsourcing and ROUGE-based metrics and obtain competitive\nresults.", "published": "2020-06-08 15:03:38", "link": "http://arxiv.org/abs/2006.04660v2", "categories": ["cs.IR", "cs.CL", "H.3.3"], "primary_category": "cs.IR"}
{"title": "A non-causal FFTNet architecture for speech enhancement", "abstract": "In this paper, we suggest a new parallel, non-causal and shallow waveform\ndomain architecture for speech enhancement based on FFTNet, a neural network\nfor generating high quality audio waveform. In contrast to other waveform based\napproaches like WaveNet, FFTNet uses an initial wide dilation pattern. Such an\narchitecture better represents the long term correlated structure of speech in\nthe time domain, where noise is usually highly non-correlated, and therefore it\nis suitable for waveform domain based speech enhancement. To further strengthen\nthis feature of FFTNet, we suggest a non-causal FFTNet architecture, where the\npresent sample in each layer is estimated from the past and future samples of\nthe previous layer. By suggesting a shallow network and applying non-causality\nwithin certain limits, the suggested FFTNet for speech enhancement (SE-FFTNet)\nuses much fewer parameters compared to other neural network based approaches\nfor speech enhancement like WaveNet and SEGAN. Specifically, the suggested\nnetwork has considerably reduced model parameters: 32% fewer compared to\nWaveNet and 87% fewer compared to SEGAN. Finally, based on subjective and\nobjective metrics, SE-FFTNet outperforms WaveNet in terms of enhanced signal\nquality, while it provides equally good performance as SEGAN. A Tensorflow\nimplementation of the architecture is provided at 1 .", "published": "2020-06-08 10:49:04", "link": "http://arxiv.org/abs/2006.04469v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "FastSpeech 2: Fast and High-Quality End-to-End Text to Speech", "abstract": "Non-autoregressive text to speech (TTS) models such as FastSpeech can\nsynthesize speech significantly faster than previous autoregressive models with\ncomparable quality. The training of FastSpeech model relies on an\nautoregressive teacher model for duration prediction (to provide more\ninformation as input) and knowledge distillation (to simplify the data\ndistribution in output), which can ease the one-to-many mapping problem (i.e.,\nmultiple speech variations correspond to the same text) in TTS. However,\nFastSpeech has several disadvantages: 1) the teacher-student distillation\npipeline is complicated and time-consuming, 2) the duration extracted from the\nteacher model is not accurate enough, and the target mel-spectrograms distilled\nfrom teacher model suffer from information loss due to data simplification,\nboth of which limit the voice quality. In this paper, we propose FastSpeech 2,\nwhich addresses the issues in FastSpeech and better solves the one-to-many\nmapping problem in TTS by 1) directly training the model with ground-truth\ntarget instead of the simplified output from teacher, and 2) introducing more\nvariation information of speech (e.g., pitch, energy and more accurate\nduration) as conditional inputs. Specifically, we extract duration, pitch and\nenergy from speech waveform and directly take them as conditional inputs in\ntraining and use predicted values in inference. We further design FastSpeech\n2s, which is the first attempt to directly generate speech waveform from text\nin parallel, enjoying the benefit of fully end-to-end inference. Experimental\nresults show that 1) FastSpeech 2 achieves a 3x training speed-up over\nFastSpeech, and FastSpeech 2s enjoys even faster inference speed; 2) FastSpeech\n2 and 2s outperform FastSpeech in voice quality, and FastSpeech 2 can even\nsurpass autoregressive models. Audio samples are available at\nhttps://speechresearch.github.io/fastspeech2/.", "published": "2020-06-08 13:05:40", "link": "http://arxiv.org/abs/2006.04558v8", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Towards an Argument Mining Pipeline Transforming Texts to Argument\n  Graphs", "abstract": "This paper targets the automated extraction of components of argumentative\ninformation and their relations from natural language text. Moreover, we\naddress a current lack of systems to provide complete argumentative structure\nfrom arbitrary natural language text for general usage. We present an argument\nmining pipeline as a universally applicable approach for transforming German\nand English language texts to graph-based argument representations. We also\nintroduce new methods for evaluating the results based on existing benchmark\nargument structures. Our results show that the generated argument graphs can be\nbeneficial to detect new connections between different statements of an\nargumentative text. Our pipeline implementation is publicly available on\nGitHub.", "published": "2020-06-08 13:10:19", "link": "http://arxiv.org/abs/2006.04562v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CS-Embed at SemEval-2020 Task 9: The effectiveness of code-switched word\n  embeddings for sentiment analysis", "abstract": "The growing popularity and applications of sentiment analysis of social media\nposts has naturally led to sentiment analysis of posts written in multiple\nlanguages, a practice known as code-switching. While recent research into\ncode-switched posts has focused on the use of multilingual word embeddings,\nthese embeddings were not trained on code-switched data. In this work, we\npresent word-embeddings trained on code-switched tweets, specifically those\nthat make use of Spanish and English, known as Spanglish. We explore the\nembedding space to discover how they capture the meanings of words in both\nlanguages. We test the effectiveness of these embeddings by participating in\nSemEval 2020 Task 9: ~\\emph{Sentiment Analysis on Code-Mixed Social Media\nText}. We utilised them to train a sentiment classifier that achieves an F-1\nscore of 0.722. This is higher than the baseline for the competition of 0.656,\nwith our team (codalab username \\emph{francesita}) ranking 14 out of 29\nparticipating teams, beating the baseline.", "published": "2020-06-08 13:48:17", "link": "http://arxiv.org/abs/2006.04597v2", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "WaveNODE: A Continuous Normalizing Flow for Speech Synthesis", "abstract": "In recent years, various flow-based generative models have been proposed to\ngenerate high-fidelity waveforms in real-time. However, these models require\neither a well-trained teacher network or a number of flow steps making them\nmemory-inefficient. In this paper, we propose a novel generative model called\nWaveNODE which exploits a continuous normalizing flow for speech synthesis.\nUnlike the conventional models, WaveNODE places no constraint on the function\nused for flow operation, thus allowing the usage of more flexible and complex\nfunctions. Moreover, WaveNODE can be optimized to maximize the likelihood\nwithout requiring any teacher network or auxiliary loss terms. We\nexperimentally show that WaveNODE achieves comparable performance with fewer\nparameters compared to the conventional flow-based vocoders.", "published": "2020-06-08 13:49:36", "link": "http://arxiv.org/abs/2006.04598v4", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MultiSpeech: Multi-Speaker Text to Speech with Transformer", "abstract": "Transformer-based text to speech (TTS) model (e.g., Transformer\nTTS~\\cite{li2019neural}, FastSpeech~\\cite{ren2019fastspeech}) has shown the\nadvantages of training and inference efficiency over RNN-based model (e.g.,\nTacotron~\\cite{shen2018natural}) due to its parallel computation in training\nand/or inference. However, the parallel computation increases the difficulty\nwhile learning the alignment between text and speech in Transformer, which is\nfurther magnified in the multi-speaker scenario with noisy data and diverse\nspeakers, and hinders the applicability of Transformer for multi-speaker TTS.\nIn this paper, we develop a robust and high-quality multi-speaker Transformer\nTTS system called MultiSpeech, with several specially designed\ncomponents/techniques to improve text-to-speech alignment: 1) a diagonal\nconstraint on the weight matrix of encoder-decoder attention in both training\nand inference; 2) layer normalization on phoneme embedding in encoder to better\npreserve position information; 3) a bottleneck in decoder pre-net to prevent\ncopy between consecutive speech frames. Experiments on VCTK and LibriTTS\nmulti-speaker datasets demonstrate the effectiveness of MultiSpeech: 1) it\nsynthesizes more robust and better quality multi-speaker voice than naive\nTransformer based TTS; 2) with a MutiSpeech model as the teacher, we obtain a\nstrong multi-speaker FastSpeech model with almost zero quality degradation\nwhile enjoying extremely fast inference speed.", "published": "2020-06-08 15:05:28", "link": "http://arxiv.org/abs/2006.04664v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Misinformation Has High Perplexity", "abstract": "Debunking misinformation is an important and time-critical task as there\ncould be adverse consequences when misinformation is not quashed promptly.\nHowever, the usual supervised approach to debunking via misinformation\nclassification requires human-annotated data and is not suited to the fast\ntime-frame of newly emerging events such as the COVID-19 outbreak. In this\npaper, we postulate that misinformation itself has higher perplexity compared\nto truthful statements, and propose to leverage the perplexity to debunk false\nclaims in an unsupervised manner. First, we extract reliable evidence from\nscientific and news sources according to sentence similarity to the claims.\nSecond, we prime a language model with the extracted evidence and finally\nevaluate the correctness of given claims based on the perplexity scores at\ndebunking time. We construct two new COVID-19-related test sets, one is\nscientific, and another is political in content, and empirically verify that\nour system performs favorably compared to existing systems. We are releasing\nthese datasets publicly to encourage more research in debunking misinformation\non COVID-19 and other topics.", "published": "2020-06-08 15:13:44", "link": "http://arxiv.org/abs/2006.04666v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CycleGT: Unsupervised Graph-to-Text and Text-to-Graph Generation via\n  Cycle Training", "abstract": "Two important tasks at the intersection of knowledge graphs and natural\nlanguage processing are graph-to-text (G2T) and text-to-graph (T2G) conversion.\nDue to the difficulty and high cost of data collection, the supervised data\navailable in the two fields are usually on the magnitude of tens of thousands,\nfor example, 18K in the WebNLG~2017 dataset after preprocessing, which is far\nfewer than the millions of data for other tasks such as machine translation.\nConsequently, deep learning models for G2T and T2G suffer largely from scarce\ntraining data. We present CycleGT, an unsupervised training method that can\nbootstrap from fully non-parallel graph and text data, and iteratively back\ntranslate between the two forms. Experiments on WebNLG datasets show that our\nunsupervised model trained on the same number of data achieves performance on\npar with several fully supervised models. Further experiments on the\nnon-parallel GenWiki dataset verify that our method performs the best among\nunsupervised baselines. This validates our framework as an effective approach\nto overcome the data scarcity problem in the fields of G2T and T2G. Our code is\navailable at https://github.com/QipengGuo/CycleGT.", "published": "2020-06-08 15:59:00", "link": "http://arxiv.org/abs/2006.04702v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Wat zei je? Detecting Out-of-Distribution Translations with Variational\n  Transformers", "abstract": "We detect out-of-training-distribution sentences in Neural Machine\nTranslation using the Bayesian Deep Learning equivalent of Transformer models.\nFor this we develop a new measure of uncertainty designed specifically for long\nsequences of discrete random variables -- i.e. words in the output sentence.\nOur new measure of uncertainty solves a major intractability in the naive\napplication of existing approaches on long sentences. We use our new measure on\na Transformer model trained with dropout approximate inference. On the task of\nGerman-English translation using WMT13 and Europarl, we show that with dropout\nuncertainty our measure is able to identify when Dutch source sentences,\nsentences which use the same word types as German, are given to the model\ninstead of German.", "published": "2020-06-08 20:00:36", "link": "http://arxiv.org/abs/2006.08344v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Semi-Supervised Contrastive Learning with Generalized Contrastive Loss\n  and Its Application to Speaker Recognition", "abstract": "This paper introduces a semi-supervised contrastive learning framework and\nits application to text-independent speaker verification. The proposed\nframework employs generalized contrastive loss (GCL). GCL unifies losses from\ntwo different learning frameworks, supervised metric learning and unsupervised\ncontrastive learning, and thus it naturally determines the loss for\nsemi-supervised learning. In experiments, we applied the proposed framework to\ntext-independent speaker verification on the VoxCeleb dataset. We demonstrate\nthat GCL enables the learning of speaker embeddings in three manners,\nsupervised learning, semi-supervised learning, and unsupervised learning,\nwithout any changes in the definition of the loss function.", "published": "2020-06-08 02:33:25", "link": "http://arxiv.org/abs/2006.04326v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Zero resource speech synthesis using transcripts derived from perceptual\n  acoustic units", "abstract": "Zerospeech synthesis is the task of building vocabulary independent speech\nsynthesis systems, where transcriptions are not available for training data. It\nis, therefore, necessary to convert training data into a sequence of\nfundamental acoustic units that can be used for synthesis during the test. This\npaper attempts to discover, and model perceptual acoustic units consisting of\nsteady-state, and transient regions in speech. The transients roughly\ncorrespond to CV, VC units, while the steady-state corresponds to sonorants and\nfricatives. The speech signal is first preprocessed by segmenting the same into\nCVC-like units using a short-term energy-like contour. These CVC segments are\nclustered using a connected components-based graph clustering technique. The\nclustered CVC segments are initialized such that the onset (CV) and decays (VC)\ncorrespond to transients, and the rhyme corresponds to steady-states. Following\nthis initialization, the units are allowed to re-organise on the continuous\nspeech into a final set of AUs in an HMM-GMM framework. AU sequences thus\nobtained are used to train synthesis models. The performance of the proposed\napproach is evaluated on the Zerospeech 2019 challenge database. Subjective and\nobjective scores show that reasonably good quality synthesis with low bit rate\nencoding can be achieved using the proposed AUs.", "published": "2020-06-08 06:11:01", "link": "http://arxiv.org/abs/2006.04372v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Learning to Count Words in Fluent Speech enables Online Speech\n  Recognition", "abstract": "Sequence to Sequence models, in particular the Transformer, achieve state of\nthe art results in Automatic Speech Recognition. Practical usage is however\nlimited to cases where full utterance latency is acceptable. In this work we\nintroduce Taris, a Transformer-based online speech recognition system aided by\nan auxiliary task of incremental word counting. We use the cumulative word sum\nto dynamically segment speech and enable its eager decoding into words.\nExperiments performed on the LRS2, LibriSpeech, and Aishell-1 datasets of\nEnglish and Mandarin speech show that the online system performs comparable\nwith the offline one when having a dynamic algorithmic delay of 5 segments.\nFurthermore, we show that the estimated segment length distribution resembles\nthe word length distribution obtained with forced alignment, although our\nsystem does not require an exact segment-to-word equivalence. Taris introduces\na negligible overhead compared to a standard Transformer, while the local\nrelationship modelling between inputs and outputs grants invariance to sequence\nlength by design.", "published": "2020-06-08 20:49:39", "link": "http://arxiv.org/abs/2006.04928v3", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
