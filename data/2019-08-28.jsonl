{"title": "Emotion Detection with Neural Personal Discrimination", "abstract": "There have been a recent line of works to automatically predict the emotions\nof posts in social media. Existing approaches consider the posts individually\nand predict their emotions independently. Different from previous researches,\nwe explore the dependence among relevant posts via the authors' backgrounds,\nsince the authors with similar backgrounds, e.g., gender, location, tend to\nexpress similar emotions. However, such personal attributes are not easy to\nobtain in most social media websites, and it is hard to capture\nattributes-aware words to connect similar people. Accordingly, we propose a\nNeural Personal Discrimination (NPD) approach to address above challenges by\ndetermining personal attributes from posts, and connecting relevant posts with\nsimilar attributes to jointly learn their emotions. In particular, we employ\nadversarial discriminators to determine the personal attributes, with attention\nmechanisms to aggregate attributes-aware words. In this way, social\ncorrelationship among different posts can be better addressed. Experimental\nresults show the usefulness of personal attributes, and the effectiveness of\nour proposed NPD approach in capturing such personal attributes with\nsignificant gains over the state-of-the-art models.", "published": "2019-08-28 13:07:05", "link": "http://arxiv.org/abs/1908.10703v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unlearn Dataset Bias in Natural Language Inference by Fitting the\n  Residual", "abstract": "Statistical natural language inference (NLI) models are susceptible to\nlearning dataset bias: superficial cues that happen to associate with the label\non a particular dataset, but are not useful in general, e.g., negation words\nindicate contradiction. As exposed by several recent challenge datasets, these\nmodels perform poorly when such association is absent, e.g., predicting that \"I\nlove dogs\" contradicts \"I don't love cats\". Our goal is to design learning\nalgorithms that guard against known dataset bias. We formalize the concept of\ndataset bias under the framework of distribution shift and present a simple\ndebiasing algorithm based on residual fitting, which we call DRiFt. We first\nlearn a biased model that only uses features that are known to relate to\ndataset bias. Then, we train a debiased model that fits to the residual of the\nbiased model, focusing on examples that cannot be predicted well by biased\nfeatures only. We use DRiFt to train three high-performing NLI models on two\nbenchmark datasets, SNLI and MNLI. Our debiased models achieve significant\ngains over baseline models on two challenge test sets, while maintaining\nreasonable performance on the original test sets.", "published": "2019-08-28 15:02:45", "link": "http://arxiv.org/abs/1908.10763v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SpatialNLI: A Spatial Domain Natural Language Interface to Databases\n  Using Spatial Comprehension", "abstract": "A natural language interface (NLI) to databases is an interface that\ntranslates a natural language question to a structured query that is executable\nby database management systems (DBMS). However, an NLI that is trained in the\ngeneral domain is hard to apply in the spatial domain due to the idiosyncrasy\nand expressiveness of the spatial questions. Inspired by the machine\ncomprehension model, we propose a spatial comprehension model that is able to\nrecognize the meaning of spatial entities based on the semantics of the\ncontext. The spatial semantics learned from the spatial comprehension model is\nthen injected to the natural language question to ease the burden of capturing\nthe spatial-specific semantics. With our spatial comprehension model and\ninformation injection, our NLI for the spatial domain, named SpatialNLI, is\nable to capture the semantic structure of the question and translate it to the\ncorresponding syntax of an executable query accurately. We also experimentally\nascertain that SpatialNLI outperforms state-of-the-art methods.", "published": "2019-08-28 19:32:00", "link": "http://arxiv.org/abs/1908.10917v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fingerspelling recognition in the wild with iterative visual attention", "abstract": "Sign language recognition is a challenging gesture sequence recognition\nproblem, characterized by quick and highly coarticulated motion. In this paper\nwe focus on recognition of fingerspelling sequences in American Sign Language\n(ASL) videos collected in the wild, mainly from YouTube and Deaf social media.\nMost previous work on sign language recognition has focused on controlled\nsettings where the data is recorded in a studio environment and the number of\nsigners is limited. Our work aims to address the challenges of real-life data,\nreducing the need for detection or segmentation modules commonly used in this\ndomain. We propose an end-to-end model based on an iterative attention\nmechanism, without explicit hand detection or segmentation. Our approach\ndynamically focuses on increasingly high-resolution regions of interest. It\noutperforms prior work by a large margin. We also introduce a newly collected\ndata set of crowdsourced annotations of fingerspelling in the wild, and show\nthat performance can be further improved with this additional data set.", "published": "2019-08-28 04:52:32", "link": "http://arxiv.org/abs/1908.10546v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Classical Chinese Sentence Segmentation for Tomb Biographies of Tang\n  Dynasty", "abstract": "Tomb biographies of the Tang dynasty provide invaluable information about\nChinese history. The original biographies are classical Chinese texts which\ncontain neither word boundaries nor sentence boundaries. Relying on three\npublished books of tomb biographies of the Tang dynasty, we investigated the\neffectiveness of employing machine-learning methods for algorithmically\nidentifying the pauses and terminals of sentences in the biographies.\n  We consider the segmentation task as a classification problem. Chinese\ncharacters that are and are not followed by a punctuation mark are classified\ninto two categories. We applied a machine-learning-based mechanism, the\nconditional random fields (CRF), to classify the characters (and words) in the\ntexts, and we studied the contributions of selected types of lexical\ninformation to the resulting quality of the segmentation recommendations.\n  This proposal presented at the DH 2018 conference discussed some of the basic\nexperiments and their evaluations. By considering the contextual information\nand employing the heuristics provided by experts of Chinese literature, we\nachieved F1 measures that were better than 80%. More complex experiments that\nemploy deep neural networks helped us further improve the results in recent\nwork.", "published": "2019-08-28 09:33:37", "link": "http://arxiv.org/abs/1908.10606v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploiting Multiple Embeddings for Chinese Named Entity Recognition", "abstract": "Identifying the named entities mentioned in text would enrich many semantic\napplications at the downstream level. However, due to the predominant usage of\ncolloquial language in microblogs, the named entity recognition (NER) in\nChinese microblogs experience significant performance deterioration, compared\nwith performing NER in formal Chinese corpus. In this paper, we propose a\nsimple yet effective neural framework to derive the character-level embeddings\nfor NER in Chinese text, named ME-CNER. A character embedding is derived with\nrich semantic information harnessed at multiple granularities, ranging from\nradical, character to word levels. The experimental results demonstrate that\nthe proposed approach achieves a large performance improvement on Weibo dataset\nand comparable performance on MSRA news dataset with lower computational cost\nagainst the existing state-of-the-art alternatives.", "published": "2019-08-28 11:47:39", "link": "http://arxiv.org/abs/1908.10657v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain\n  Task-Oriented Dialog", "abstract": "Dialog policy decides what and how a task-oriented dialog system will\nrespond, and plays a vital role in delivering effective conversations. Many\nstudies apply Reinforcement Learning to learn a dialog policy with the reward\nfunction which requires elaborate design and pre-specified user goals. With the\ngrowing needs to handle complex goals across multiple domains, such manually\ndesigned reward functions are not affordable to deal with the complexity of\nreal-world tasks. To this end, we propose Guided Dialog Policy Learning, a\nnovel algorithm based on Adversarial Inverse Reinforcement Learning for joint\nreward estimation and policy optimization in multi-domain task-oriented dialog.\nThe proposed approach estimates the reward signal and infers the user goal in\nthe dialog sessions. The reward estimator evaluates the state-action pairs so\nthat it can guide the dialog policy at each dialog turn. Extensive experiments\non a multi-domain dialog dataset show that the dialog policy guided by the\nlearned reward function achieves remarkably higher task success than\nstate-of-the-art baselines.", "published": "2019-08-28 13:36:25", "link": "http://arxiv.org/abs/1908.10719v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Discourse-Aware Semantic Self-Attention for Narrative Reading\n  Comprehension", "abstract": "In this work, we propose to use linguistic annotations as a basis for a\n\\textit{Discourse-Aware Semantic Self-Attention} encoder that we employ for\nreading comprehension on long narrative texts. We extract relations between\ndiscourse units, events and their arguments as well as coreferring mentions,\nusing available annotation tools. Our empirical evaluation shows that the\ninvestigated structures improve the overall performance, especially\nintra-sentential and cross-sentential discourse relations, sentence-internal\nsemantic role relations, and long-distance coreference relations. We show that\ndedicating self-attention heads to intra-sentential relations and relations\nconnecting neighboring sentences is beneficial for finding answers to questions\nin longer contexts. Our findings encourage the use of discourse-semantic\nannotations to enhance the generalization capacity of self-attention models for\nreading comprehension.", "published": "2019-08-28 13:40:43", "link": "http://arxiv.org/abs/1908.10721v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Language Tasks and Language Games: On Methodology in Current Natural\n  Language Processing Research", "abstract": "\"This paper introduces a new task and a new dataset\", \"we improve the state\nof the art in X by Y\" -- it is rare to find a current natural language\nprocessing paper (or AI paper more generally) that does not contain such\nstatements. What is mostly left implicit, however, is the assumption that this\nnecessarily constitutes progress, and what it constitutes progress towards.\nHere, we make more precise the normally impressionistically used notions of\nlanguage task and language game and ask how a research programme built on these\nmight make progress towards the goal of modelling general language competence.", "published": "2019-08-28 14:29:13", "link": "http://arxiv.org/abs/1908.10747v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Data Augmentation with Atomic Templates for Spoken Language\n  Understanding", "abstract": "Spoken Language Understanding (SLU) converts user utterances into structured\nsemantic representations. Data sparsity is one of the main obstacles of SLU due\nto the high cost of human annotation, especially when domain changes or a new\ndomain comes. In this work, we propose a data augmentation method with atomic\ntemplates for SLU, which involves minimum human efforts. The atomic templates\nproduce exemplars for fine-grained constituents of semantic representations. We\npropose an encoder-decoder model to generate the whole utterance from atomic\nexemplars. Moreover, the generator could be transferred from source domains to\nhelp a new domain which has little data. Experimental results show that our\nmethod achieves significant improvements on DSTC 2\\&3 dataset which is a domain\nadaptation setting of SLU.", "published": "2019-08-28 15:17:33", "link": "http://arxiv.org/abs/1908.10770v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Empirical Comparison on Imitation Learning and Reinforcement Learning\n  for Paraphrase Generation", "abstract": "Generating paraphrases from given sentences involves decoding words step by\nstep from a large vocabulary. To learn a decoder, supervised learning which\nmaximizes the likelihood of tokens always suffers from the exposure bias.\nAlthough both reinforcement learning (RL) and imitation learning (IL) have been\nwidely used to alleviate the bias, the lack of direct comparison leads to only\na partial image on their benefits. In this work, we present an empirical study\non how RL and IL can help boost the performance of generating paraphrases, with\nthe pointer-generator as a base model. Experiments on the benchmark datasets\nshow that (1) imitation learning is constantly better than reinforcement\nlearning; and (2) the pointer-generator models with imitation learning\noutperform the state-of-the-art methods with a large margin.", "published": "2019-08-28 17:10:06", "link": "http://arxiv.org/abs/1908.10835v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Analyzing Customer Feedback for Product Fit Prediction", "abstract": "One of the biggest hurdles for customers when purchasing fashion online, is\nthe difficulty of finding products with the right fit. In order to provide a\nbetter online shopping experience, platforms need to find ways to recommend the\nright product sizes and the best fitting products to their customers. These\nrecommendation systems, however, require customer feedback in order to estimate\nthe most suitable sizing options. Such feedback is rare and often only\navailable as natural text. In this paper, we examine the extraction of product\nfit feedback from customer reviews using natural language processing\ntechniques. In particular, we compare traditional methods with more recent\ntransfer learning techniques for text classification, and analyze their\nresults. Our evaluation shows, that the transfer learning approach ULMFit is\nnot only comparatively fast to train, but also achieves highest accuracy on\nthis task. The integration of the extracted information with actual size\nrecommendation systems is left for future work.", "published": "2019-08-28 18:22:26", "link": "http://arxiv.org/abs/1908.10896v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Interactive Language Learning by Question Answering", "abstract": "Humans observe and interact with the world to acquire knowledge. However,\nmost existing machine reading comprehension (MRC) tasks miss the interactive,\ninformation-seeking component of comprehension. Such tasks present models with\nstatic documents that contain all necessary information, usually concentrated\nin a single short substring. Thus, models can achieve strong performance\nthrough simple word- and phrase-based pattern matching. We address this problem\nby formulating a novel text-based question answering task: Question Answering\nwith Interactive Text (QAit). In QAit, an agent must interact with a partially\nobservable text-based environment to gather information required to answer\nquestions. QAit poses questions about the existence, location, and attributes\nof objects found in the environment. The data is built using a text-based game\ngenerator that defines the underlying dynamics of interaction with the\nenvironment. We propose and evaluate a set of baseline models for the QAit task\nthat includes deep reinforcement learning agents. Experiments show that the\ntask presents a major challenge for machine reading systems, while humans solve\nit with relative ease.", "published": "2019-08-28 19:10:08", "link": "http://arxiv.org/abs/1908.10909v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Solving Math Word Problems with Double-Decoder Transformer", "abstract": "This paper proposes a Transformer-based model to generate equations for math\nword problems. It achieves much better results than RNN models when copy and\nalign mechanisms are not used, and can outperform complex copy and align RNN\nmodels. We also show that training a Transformer jointly in a generation task\nwith two decoders, left-to-right and right-to-left, is beneficial. Such a\nTransformer performs better than the one with just one decoder not only because\nof the ensemble effect, but also because it improves the encoder training\nprocedure. We also experiment with adding reinforcement learning to our model,\nshowing improved performance compared to MLE training.", "published": "2019-08-28 19:42:37", "link": "http://arxiv.org/abs/1908.10924v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Learning a Multi-Domain Curriculum for Neural Machine Translation", "abstract": "Most data selection research in machine translation focuses on improving a\nsingle domain. We perform data selection for multiple domains at once. This is\nachieved by carefully introducing instance-level domain-relevance features and\nautomatically constructing a training curriculum to gradually concentrate on\nmulti-domain relevant and noise-reduced data batches. Both the choice of\nfeatures and the use of curriculum are crucial for balancing and improving all\ndomains, including out-of-domain. In large-scale experiments, the multi-domain\ncurriculum simultaneously reaches or outperforms the individual performance and\nbrings solid gains over no-curriculum training.", "published": "2019-08-28 20:48:05", "link": "http://arxiv.org/abs/1908.10940v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Leveraging Structural and Semantic Correspondence for Attribute-Oriented\n  Aspect Sentiment Discovery", "abstract": "Opinionated text often involves attributes such as authorship and location\nthat influence the sentiments expressed for different aspects. We posit that\nstructural and semantic correspondence is both prevalent in opinionated text,\nespecially when associated with attributes, and crucial in accurately revealing\nits latent aspect and sentiment structure. However, it is not recognized by\nexisting approaches.\n  We propose Trait, an unsupervised probabilistic model that discovers aspects\nand sentiments from text and associates them with different attributes. To this\nend, Trait infers and leverages structural and semantic correspondence using a\nMarkov Random Field. We show empirically that by incorporating attributes\nexplicitly Trait significantly outperforms state-of-the-art baselines both by\ngenerating attribute profiles that accord with our intuitions, as shown via\nvisualization, and yielding topics of greater semantic cohesion.", "published": "2019-08-28 22:18:03", "link": "http://arxiv.org/abs/1908.10970v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Onto Word Segmentation of the Complete Tang Poems", "abstract": "We aim at segmenting words in the Complete Tang Poems (CTP). Although it is\npossible to do some research about CTP without doing full-scale word\nsegmentation, we must move forward to word-level analysis of CTP for conducting\nadvanced research topics. In November 2018 when we submitted the manuscript for\nDH 2019 (ADHO), we collected only 2433 poems that were segmented by trained\nexperts, and used the segmented poems to evaluate the segmenter that considered\ndomain knowledge of Chinese poetry. We trained pointwise mutual information\n(PMI) between Chinese characters based on the CTP poems (excluding the 2433\npoems, which were used exclusively only for testing) and the domain knowledge.\nThe segmenter relied on the PMI information to the recover 85.7% of words in\nthe test poems. We could segment a poem completely correct only 17.8% of the\ntime, however. When we presented our work at DH 2019, we have annotated more\nthan 20000 poems. With a much larger amount of data, we were able to apply\nbiLSTM models for this word segmentation task, and we segmented a poem\ncompletely correct above 20% of the time. In contrast, human annotators\ncompletely agreed on their annotations about 40% of the time.", "published": "2019-08-28 10:06:19", "link": "http://arxiv.org/abs/1908.10621v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DeepCopy: Grounded Response Generation with Hierarchical Pointer\n  Networks", "abstract": "Recent advances in neural sequence-to-sequence models have led to promising\nresults for several language generation-based tasks, including dialogue\nresponse generation, summarization, and machine translation. However, these\nmodels are known to have several problems, especially in the context of\nchit-chat based dialogue systems: they tend to generate short and dull\nresponses that are often too generic. Furthermore, these models do not ground\nconversational responses on knowledge and facts, resulting in turns that are\nnot accurate, informative and engaging for the users. In this paper, we propose\nand experiment with a series of response generation models that aim to serve in\nthe general scenario where in addition to the dialogue context, relevant\nunstructured external knowledge in the form of text is also assumed to be\navailable for models to harness. Our proposed approach extends\npointer-generator networks (See et al., 2017) by allowing the decoder to\nhierarchically attend and copy from external knowledge in addition to the\ndialogue context. We empirically show the effectiveness of the proposed model\ncompared to several baselines including (Ghazvininejad et al., 2018; Zhang et\nal., 2018) through both automatic evaluation metrics and human evaluation on\nCONVAI2 dataset.", "published": "2019-08-28 14:03:44", "link": "http://arxiv.org/abs/1908.10731v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Semantic Hypergraphs", "abstract": "Approaches to Natural language processing (NLP) may be classified along a\ndouble dichotomy open/opaque - strict/adaptive. The former axis relates to the\npossibility of inspecting the underlying processing rules, the latter to the\nuse of fixed or adaptive rules. We argue that many techniques fall into either\nthe open-strict or opaque-adaptive categories. Our contribution takes steps in\nthe open-adaptive direction, which we suggest is likely to provide key\ninstruments for interdisciplinary research. The central idea of our approach is\nthe Semantic Hypergraph (SH), a novel knowledge representation model that is\nintrinsically recursive and accommodates the natural hierarchical richness of\nnatural language. The SH model is hybrid in two senses. First, it attempts to\ncombine the strengths of ML and symbolic approaches. Second, it is a formal\nlanguage representation that reduces but tolerates ambiguity and structural\nvariability. We will see that SH enables simple yet powerful methods of pattern\ndetection, and features a good compromise for intelligibility both for humans\nand machines. It also provides a semantically deep starting point (in terms of\nexplicit meaning) for further algorithms to operate and collaborate on. We show\nhow modern NLP ML-based building blocks can be used in combination with a\nrandom forest classifier and a simple search tree to parse NL to SH, and that\nthis parser can achieve high precision in a diversity of text categories. We\ndefine a pattern language representable in SH itself, and a process to discover\nknowledge inference rules. We then illustrate the efficiency of the SH\nframework in a variety of tasks, including conjunction decomposition, open\ninformation extraction, concept taxonomy inference and co-reference resolution,\nand an applied example of claim and conflict analysis in a news corpus.", "published": "2019-08-28 15:39:02", "link": "http://arxiv.org/abs/1908.10784v2", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Image Captioning with Sparse Recurrent Neural Network", "abstract": "Recurrent Neural Network (RNN) has been widely used to tackle a wide variety\nof language generation problems and are capable of attaining state-of-the-art\n(SOTA) performance. However despite its impressive results, the large number of\nparameters in the RNN model makes deployment to mobile and embedded devices\ninfeasible. Driven by this problem, many works have proposed a number of\npruning methods to reduce the sizes of the RNN model. In this work, we propose\nan end-to-end pruning method for image captioning models equipped with visual\nattention. Our proposed method is able to achieve sparsity levels up to 97.5%\nwithout significant performance loss relative to the baseline (~ 2% loss at 40x\ncompression after fine-tuning). Our method is also simple to use and tune,\nfacilitating faster development times for neural network practitioners. We\nperform extensive experiments on the popular MS-COCO dataset in order to\nempirically validate the efficacy of our proposed method.", "published": "2019-08-28 15:53:13", "link": "http://arxiv.org/abs/1908.10797v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "HTMLPhish: Enabling Phishing Web Page Detection by Applying Deep\n  Learning Techniques on HTML Analysis", "abstract": "Recently, the development and implementation of phishing attacks require\nlittle technical skills and costs. This uprising has led to an ever-growing\nnumber of phishing attacks on the World Wide Web. Consequently, proactive\ntechniques to fight phishing attacks have become extremely necessary. In this\npaper, we propose HTMLPhish, a deep learning based data-driven end-to-end\nautomatic phishing web page classification approach. Specifically, HTMLPhish\nreceives the content of the HTML document of a web page and employs\nConvolutional Neural Networks (CNNs) to learn the semantic dependencies in the\ntextual contents of the HTML. The CNNs learn appropriate feature\nrepresentations from the HTML document embeddings without extensive manual\nfeature engineering. Furthermore, our proposed approach of the concatenation of\nthe word and character embeddings allows our model to manage new features and\nensure easy extrapolation to test data. We conduct comprehensive experiments on\na dataset of more than 50,000 HTML documents that provides a distribution of\nphishing to benign web pages obtainable in the real-world that yields over 93\npercent Accuracy and True Positive Rate. Also, HTMLPhish is a completely\nlanguage-independent and client-side strategy which can, therefore, conduct web\npage phishing detection regardless of the textual language.", "published": "2019-08-28 23:58:50", "link": "http://arxiv.org/abs/1909.01135v3", "categories": ["cs.CR", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CR"}
{"title": "Convolutional Recurrent Neural Network Based Progressive Learning for\n  Monaural Speech Enhancement", "abstract": "Recently, progressive learning has shown its capacity to improve speech\nquality and speech intelligibility when it is combined with deep neural network\n(DNN) and long short-term memory (LSTM) based monaural speech enhancement\nalgorithms, especially in low signal-to-noise ratio (SNR) conditions.\nNevertheless, due to a large number of parameters and high computational\ncomplexity, it is hard to implement in current resource-limited\nmicro-controllers and thus, it is essential to significantly reduce both the\nnumber of parameters and the computational load for practical applications. For\nthis purpose, we propose a novel progressive learning framework with causal\nconvolutional recurrent neural networks called PL-CRNN, which takes advantage\nof both convolutional neural networks and recurrent neural networks to\ndrastically reduce the number of parameters and simultaneously improve speech\nquality and speech intelligibility. Numerous experiments verify the\neffectiveness of the proposed PL-CRNN model and indicate that it yields\nconsistent better performance than the PL-DNN and PL-LSTM algorithms and also\nit gets results close even better than the CRNN in terms of objective\nmeasurements. Compared with PL-DNN, PL-LSTM, and CRNN, the proposed PL-CRNN\nalgorithm can reduce the number of parameters up to 93%, 97%, and 92%,\nrespectively.", "published": "2019-08-28 15:09:39", "link": "http://arxiv.org/abs/1908.10768v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Environment Sound Classification using Multiple Feature Channels and\n  Attention based Deep Convolutional Neural Network", "abstract": "In this paper, we propose a model for the Environment Sound Classification\nTask (ESC) that consists of multiple feature channels given as input to a Deep\nConvolutional Neural Network (CNN) with Attention mechanism. The novelty of the\npaper lies in using multiple feature channels consisting of Mel-Frequency\nCepstral Coefficients (MFCC), Gammatone Frequency Cepstral Coefficients (GFCC),\nthe Constant Q-transform (CQT) and Chromagram. Such multiple features have\nnever been used before for signal or audio processing. And, we employ a deeper\nCNN (DCNN) compared to previous models, consisting of spatially separable\nconvolutions working on time and feature domain separately. Alongside, we use\nattention modules that perform channel and spatial attention together. We use\nsome data augmentation techniques to further boost performance. Our model is\nable to achieve state-of-the-art performance on all three benchmark environment\nsound classification datasets, i.e. the UrbanSound8K (97.52%), ESC-10 (95.75%)\nand ESC-50 (88.50%). To the best of our knowledge, this is the first time that\na single environment sound classification model is able to achieve\nstate-of-the-art results on all three datasets. For ESC-10 and ESC-50 datasets,\nthe accuracy achieved by the proposed model is beyond human accuracy of 95.7%\nand 81.3% respectively.", "published": "2019-08-28 17:02:19", "link": "http://arxiv.org/abs/1908.11219v9", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
