{"title": "Privacy-Preserving Language Model Inference with Instance Obfuscation", "abstract": "Language Models as a Service (LMaaS) offers convenient access for developers\nand researchers to perform inference using pre-trained language models.\nNonetheless, the input data and the inference results containing private\ninformation are exposed as plaintext during the service call, leading to\nprivacy issues. Recent studies have started tackling the privacy issue by\ntransforming input data into privacy-preserving representation from the\nuser-end with the techniques such as noise addition and content perturbation,\nwhile the exploration of inference result protection, namely decision privacy,\nis still a blank page. In order to maintain the black-box manner of LMaaS,\nconducting data privacy protection, especially for the decision, is a\nchallenging task because the process has to be seamless to the models and\naccompanied by limited communication and computation overhead. We thus propose\nInstance-Obfuscated Inference (IOI) method, which focuses on addressing the\ndecision privacy issue of natural language understanding tasks in their\ncomplete life-cycle. Besides, we conduct comprehensive experiments to evaluate\nthe performance as well as the privacy-protection strength of the proposed\nmethod on various benchmarking tasks.", "published": "2024-02-13 05:36:54", "link": "http://arxiv.org/abs/2402.08227v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey of Table Reasoning with Large Language Models", "abstract": "Table reasoning, which aims to generate the corresponding answer to the\nquestion following the user requirement according to the provided table, and\noptionally a text description of the table, effectively improving the\nefficiency of obtaining information. Recently, using Large Language Models\n(LLMs) has become the mainstream method for table reasoning, because it not\nonly significantly reduces the annotation cost but also exceeds the performance\nof previous methods. However, existing research still lacks a summary of\nLLM-based table reasoning works. Due to the existing lack of research,\nquestions about which techniques can improve table reasoning performance in the\nera of LLMs, why LLMs excel at table reasoning, and how to enhance table\nreasoning abilities in the future, remain largely unexplored. This gap\nsignificantly limits progress in research. To answer the above questions and\nadvance table reasoning research with LLMs, we present this survey to analyze\nexisting research, inspiring future work. In this paper, we analyze the\nmainstream techniques used to improve table reasoning performance in the LLM\nera, and the advantages of LLMs compared to pre-LLMs for solving table\nreasoning. We provide research directions from both the improvement of existing\nmethods and the expansion of practical applications to inspire future research.", "published": "2024-02-13 07:17:52", "link": "http://arxiv.org/abs/2402.08259v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PreFLMR: Scaling Up Fine-Grained Late-Interaction Multi-modal Retrievers", "abstract": "Large Multimodal Models (LMMs) excel in natural language and visual\nunderstanding but are challenged by exacting tasks such as Knowledge-based\nVisual Question Answering (KB-VQA) which involve the retrieval of relevant\ninformation from document collections to use in shaping answers to questions.\nWe present an extensive training and evaluation framework, M2KR, for KB-VQA.\nM2KR contains a collection of vision and language tasks which we have\nincorporated into a single suite of benchmark tasks for training and evaluating\ngeneral-purpose multi-modal retrievers. We use M2KR to develop PreFLMR, a\npre-trained version of the recently developed Fine-grained Late-interaction\nMulti-modal Retriever (FLMR) approach to KB-VQA, and we report new\nstate-of-the-art results across a range of tasks. We also present\ninvestigations into the scaling behaviors of PreFLMR intended to be useful in\nfuture developments in general-purpose multi-modal retrievers.", "published": "2024-02-13 09:47:07", "link": "http://arxiv.org/abs/2402.08327v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Punctuation Restoration Improves Structure Understanding Without\n  Supervision", "abstract": "Unsupervised learning objectives like autoregressive and masked language\nmodeling constitute a significant part in producing pre-trained representations\nthat perform various downstream applications from natural language\nunderstanding to conversational tasks. However, despite impressive generative\ncapabilities of recent large language models, their abilities to capture\nsyntactic or semantic structure within text lag behind. We hypothesize that the\nmismatch between linguistic performance and competence in machines is\nattributable to insufficient learning of linguistic structure knowledge via\ncurrently popular pre-training objectives. Working with English, we show that\npunctuation restoration as a learning objective improves performance on\nstructure-related tasks like named entity recognition, open information\nextraction, chunking, and part-of-speech tagging. Punctuation restoration\nresults in $\\blacktriangle$$\\geq2\\%$p improvement in 16 out of 18 experiments,\nacross 6 out of 7 tasks. Our results show that punctuation restoration is an\neffective learning objective that can improve structure understanding and yield\na more robust structure-aware representations of natural language in base-sized\nmodels.", "published": "2024-02-13 11:22:52", "link": "http://arxiv.org/abs/2402.08382v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models as Minecraft Agents", "abstract": "In this work we examine the use of Large Language Models (LLMs) in the\nchallenging setting of acting as a Minecraft agent. We apply and evaluate LLMs\nin the builder and architect settings, introduce clarification questions and\nexamining the challenges and opportunities for improvement. In addition, we\npresent a platform for online interaction with the agents and an evaluation\nagainst previous works.", "published": "2024-02-13 11:37:30", "link": "http://arxiv.org/abs/2402.08392v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLMs and the Human Condition", "abstract": "Theory based AI research has had a hard time recently and the aim here is to\npropose a model of what LLMs are actually doing when they impress us with their\nlanguage skills. The model integrates three established theories of human\ndecision-making from philosophy, sociology, and computer science. The paper\nstarts with the collective understanding of reasoning from the early days of AI\nresearch - primarily because that model is how we humans think we think, and is\nthe most accessible. It then describes what is commonly thought of as \"reactive\nsystems\" which is the position taken by many philosophers and indeed many\ncontemporary AI researchers. The third component to the proposed model is from\nsociology and, although not flattering to our modern ego, provides an\nexplanation to a puzzle that for many years has occupied those of us working on\nconversational user interfaces.", "published": "2024-02-13 12:04:43", "link": "http://arxiv.org/abs/2402.08403v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lying Blindly: Bypassing ChatGPT's Safeguards to Generate Hard-to-Detect\n  Disinformation Claims", "abstract": "As Large Language Models become more proficient, their misuse in coordinated\ndisinformation campaigns is a growing concern. This study explores the\ncapability of ChatGPT with GPT-3.5 to generate short-form disinformation claims\nabout the war in Ukraine, both in general and on a specific event, which is\nbeyond the GPT-3.5 knowledge cutoff. Unlike prior work, we do not provide the\nmodel with human-written disinformation narratives by including them in the\nprompt. Thus the generated short claims are hallucinations based on prior world\nknowledge and inference from the minimal prompt. With a straightforward\nprompting technique, we are able to bypass model safeguards and generate\nnumerous short claims. We compare those against human-authored false claims on\nthe war in Ukraine from ClaimReview, specifically with respect to differences\nin their linguistic properties. We also evaluate whether AI authorship can be\ndifferentiated by human readers or state-of-the-art authorship detection tools.\nThus, we demonstrate that ChatGPT can produce realistic, target-specific\ndisinformation claims, even on a specific post-cutoff event, and that they\ncannot be reliably distinguished by humans or existing automated tools.", "published": "2024-02-13 13:50:08", "link": "http://arxiv.org/abs/2402.08467v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Plausible Extractive Rationalization through Semi-Supervised Entailment\n  Signal", "abstract": "The increasing use of complex and opaque black box models requires the\nadoption of interpretable measures, one such option is extractive rationalizing\nmodels, which serve as a more interpretable alternative. These models, also\nknown as Explain-Then-Predict models, employ an explainer model to extract\nrationales and subsequently condition the predictor with the extracted\ninformation. Their primary objective is to provide precise and faithful\nexplanations, represented by the extracted rationales. In this paper, we take a\nsemi-supervised approach to optimize for the plausibility of extracted\nrationales. We adopt a pre-trained natural language inference (NLI) model and\nfurther fine-tune it on a small set of supervised rationales ($10\\%$). The NLI\npredictor is leveraged as a source of supervisory signals to the explainer via\nentailment alignment. We show that, by enforcing the alignment agreement\nbetween the explanation and answer in a question-answering task, the\nperformance can be improved without access to ground truth labels. We evaluate\nour approach on the ERASER dataset and show that our approach achieves\ncomparable results with supervised extractive models and outperforms\nunsupervised approaches by $> 100\\%$.", "published": "2024-02-13 14:12:32", "link": "http://arxiv.org/abs/2402.08479v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Auditing Counterfire: Evaluating Advanced Counterargument Generation\n  with Evidence and Style", "abstract": "We audited large language models (LLMs) for their ability to create\nevidence-based and stylistic counter-arguments to posts from the Reddit\nChangeMyView dataset. We benchmarked their rhetorical quality across a host of\nqualitative and quantitative metrics and then ultimately evaluated them on\ntheir persuasive abilities as compared to human counter-arguments. Our\nevaluation is based on Counterfire: a new dataset of 32,000 counter-arguments\ngenerated from large language models (LLMs): GPT-3.5 Turbo and Koala and their\nfine-tuned variants, and PaLM 2, with varying prompts for evidence use and\nargumentative style. GPT-3.5 Turbo ranked highest in argument quality with\nstrong paraphrasing and style adherence, particularly in `reciprocity' style\narguments. However, the stylistic counter-arguments still fall short of human\npersuasive standards, where people also preferred reciprocal to evidence-based\nrebuttals. The findings suggest that a balance between evidentiality and\nstylistic elements is vital to a compelling counter-argument. We close with a\ndiscussion of future research directions and implications for evaluating LLM\noutputs.", "published": "2024-02-13 14:53:12", "link": "http://arxiv.org/abs/2402.08498v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Factual Error Correction for Abstractive Summarization via\n  Data Distillation and Conditional-generation Cloze", "abstract": "Improving factual consistency in abstractive summarization has been a focus\nof current research. One promising approach is the post-editing method.\nHowever, previous works have yet to make sufficient use of factual factors in\nsummaries and suffers from the negative effect of the training datasets. In\nthis paper, we first propose a novel factual error correction model FactCloze\nbased on a conditional-generation cloze task. FactCloze can construct the\ncausality among factual factors while being able to determine whether the blank\ncan be answered or not. Then, we propose a data distillation method to generate\na more faithful summarization dataset SummDSC via multiple-dimensional\nevaluation. We experimentally validate the effectiveness of our approach, which\nleads to an improvement in multiple factual consistency metrics compared to\nbaselines.", "published": "2024-02-13 16:35:48", "link": "http://arxiv.org/abs/2402.08581v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bayesian Multi-Task Transfer Learning for Soft Prompt Tuning", "abstract": "Prompt tuning, in which prompts are optimized to adapt large-scale\npre-trained language models to downstream tasks instead of fine-tuning the full\nmodel parameters, has been shown to be particularly effective when the prompts\nare trained in a multi-task transfer learning setting. These methods generally\ninvolve individually training prompts for each source task and then aggregating\nthem to provide the initialization of the prompt for the target task. However,\nthis approach critically ignores the fact that some of the source tasks could\nbe negatively or positively interfering with each other. We argue that when we\nextract knowledge from source tasks via training source prompts, we need to\nconsider this correlation among source tasks for better transfer to target\ntasks. To this end, we propose a Bayesian approach where we work with the\nposterior distribution of prompts across source tasks. We obtain representative\nsource prompts corresponding to the samples from the posterior utilizing Stein\nVariational Gradient Descent, which are then aggregated to constitute the\ninitial target prompt. We show extensive experimental results on the standard\nbenchmark NLP tasks, where our Bayesian multi-task transfer learning approach\noutperforms the state-of-the-art methods in many settings. Furthermore, our\napproach requires no auxiliary models other than the prompt itself, achieving a\nhigh degree of parameter efficiency.", "published": "2024-02-13 16:57:02", "link": "http://arxiv.org/abs/2402.08594v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SemRel2024: A Collection of Semantic Textual Relatedness Datasets for 13\n  Languages", "abstract": "Exploring and quantifying semantic relatedness is central to representing\nlanguage and holds significant implications across various NLP tasks. While\nearlier NLP research primarily focused on semantic similarity, often within the\nEnglish language context, we instead investigate the broader phenomenon of\nsemantic relatedness. In this paper, we present \\textit{SemRel}, a new semantic\nrelatedness dataset collection annotated by native speakers across 13\nlanguages: \\textit{Afrikaans, Algerian Arabic, Amharic, English, Hausa, Hindi,\nIndonesian, Kinyarwanda, Marathi, Moroccan Arabic, Modern Standard Arabic,\nSpanish,} and \\textit{Telugu}. These languages originate from five distinct\nlanguage families and are predominantly spoken in Africa and Asia -- regions\ncharacterised by a relatively limited availability of NLP resources. Each\ninstance in the SemRel datasets is a sentence pair associated with a score that\nrepresents the degree of semantic textual relatedness between the two\nsentences. The scores are obtained using a comparative annotation framework. We\ndescribe the data collection and annotation processes, challenges when building\nthe datasets, baseline experiments, and their impact and utility in NLP.", "published": "2024-02-13 18:04:53", "link": "http://arxiv.org/abs/2402.08638v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Generalization in Semantic Parsing by Increasing Natural\n  Language Variation", "abstract": "Text-to-SQL semantic parsing has made significant progress in recent years,\nwith various models demonstrating impressive performance on the challenging\nSpider benchmark. However, it has also been shown that these models often\nstruggle to generalize even when faced with small perturbations of previously\n(accurately) parsed expressions. This is mainly due to the linguistic form of\nquestions in Spider which are overly specific, unnatural, and display limited\nvariation. In this work, we use data augmentation to enhance the robustness of\ntext-to-SQL parsers against natural language variations. Existing approaches\ngenerate question reformulations either via models trained on Spider or only\nintroduce local changes. In contrast, we leverage the capabilities of large\nlanguage models to generate more realistic and diverse questions. Using only a\nfew prompts, we achieve a two-fold increase in the number of questions in\nSpider. Training on this augmented dataset yields substantial improvements on a\nrange of evaluation sets, including robustness benchmarks and out-of-domain\ndata.", "published": "2024-02-13 18:48:23", "link": "http://arxiv.org/abs/2402.08666v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Dataset for the Detection of Dehumanizing Language", "abstract": "Dehumanization is a mental process that enables the exclusion and ill\ntreatment of a group of people. In this paper, we present two data sets of\ndehumanizing text, a large, automatically collected corpus and a smaller,\nmanually annotated data set. Both data sets include a combination of political\ndiscourse and dialogue from movie subtitles. Our methods give us a broad and\nvaried amount of dehumanization data to work with, enabling further exploratory\nanalysis and automatic classification of dehumanization patterns. Both data\nsets will be publicly released.", "published": "2024-02-13 19:58:24", "link": "http://arxiv.org/abs/2402.08764v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "InstructGraph: Boosting Large Language Models via Graph-centric\n  Instruction Tuning and Preference Alignment", "abstract": "Do current large language models (LLMs) better solve graph reasoning and\ngeneration tasks with parameter updates? In this paper, we propose\nInstructGraph, a framework that empowers LLMs with the abilities of graph\nreasoning and generation by instruction tuning and preference alignment.\nSpecifically, we first propose a structured format verbalizer to unify all\ngraph data into a universal code-like format, which can simply represent the\ngraph without any external graph-specific encoders. Furthermore, a graph\ninstruction tuning stage is introduced to guide LLMs in solving graph reasoning\nand generation tasks. Finally, we identify potential hallucination problems in\ngraph tasks and sample negative instances for preference alignment, the target\nof which is to enhance the output's reliability of the model. Extensive\nexperiments across multiple graph-centric tasks exhibit that InstructGraph can\nachieve the best performance and outperform GPT-4 and LLaMA2 by more than 13\\%\nand 38\\%, respectively.", "published": "2024-02-13 20:47:17", "link": "http://arxiv.org/abs/2402.08785v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Generate Context-Sensitive Backchannel Smiles for Embodied\n  AI Agents with Applications in Mental Health Dialogues", "abstract": "Addressing the critical shortage of mental health resources for effective\nscreening, diagnosis, and treatment remains a significant challenge. This\nscarcity underscores the need for innovative solutions, particularly in\nenhancing the accessibility and efficacy of therapeutic support. Embodied\nagents with advanced interactive capabilities emerge as a promising and\ncost-effective supplement to traditional caregiving methods. Crucial to these\nagents' effectiveness is their ability to simulate non-verbal behaviors, like\nbackchannels, that are pivotal in establishing rapport and understanding in\ntherapeutic contexts but remain under-explored. To improve the rapport-building\ncapabilities of embodied agents we annotated backchannel smiles in videos of\nintimate face-to-face conversations over topics such as mental health, illness,\nand relationships. We hypothesized that both speaker and listener behaviors\naffect the duration and intensity of backchannel smiles. Using cues from speech\nprosody and language along with the demographics of the speaker and listener,\nwe found them to contain significant predictors of the intensity of backchannel\nsmiles. Based on our findings, we introduce backchannel smile production in\nembodied agents as a generation problem. Our attention-based generative model\nsuggests that listener information offers performance improvements over the\nbaseline speaker-centric generation approach. Conditioned generation using the\nsignificant predictors of smile intensity provides statistically significant\nimprovements in empirical measures of generation quality. Our user study by\ntransferring generated smiles to an embodied agent suggests that agent with\nbackchannel smiles is perceived to be more human-like and is an attractive\nalternative for non-personal conversations over agent without backchannel\nsmiles.", "published": "2024-02-13 22:47:22", "link": "http://arxiv.org/abs/2402.08837v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CMA-R:Causal Mediation Analysis for Explaining Rumour Detection", "abstract": "We apply causal mediation analysis to explain the decision-making process of\nneural models for rumour detection on Twitter. Interventions at the input and\nnetwork level reveal the causal impacts of tweets and words in the model\noutput. We find that our approach CMA-R -- Causal Mediation Analysis for Rumour\ndetection -- identifies salient tweets that explain model predictions and show\nstrong agreement with human judgements for critical tweets determining the\ntruthfulness of stories. CMA-R can further highlight causally impactful words\nin the salient tweets, providing another layer of interpretability and\ntransparency into these blackbox rumour detection systems. Code is available\nat: https://github.com/ltian678/cma-r.", "published": "2024-02-13 01:31:08", "link": "http://arxiv.org/abs/2402.08155v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Pixel Sentence Representation Learning", "abstract": "Pretrained language models are long known to be subpar in capturing sentence\nand document-level semantics. Though heavily investigated, transferring\nperturbation-based methods from unsupervised visual representation learning to\nNLP remains an unsolved problem. This is largely due to the discreteness of\nsubword units brought by tokenization of language models, limiting small\nperturbations of inputs to form semantics-preserved positive pairs. In this\nwork, we conceptualize the learning of sentence-level textual semantics as a\nvisual representation learning process. Drawing from cognitive and linguistic\nsciences, we introduce an unsupervised visual sentence representation learning\nframework, employing visually-grounded text perturbation methods like typos and\nword order shuffling, resonating with human cognitive patterns, and enabling\nperturbation to texts to be perceived as continuous. Our approach is further\nbolstered by large-scale unsupervised topical alignment training and natural\nlanguage inference supervision, achieving comparable performance in semantic\ntextual similarity (STS) to existing state-of-the-art NLP methods.\nAdditionally, we unveil our method's inherent zero-shot cross-lingual\ntransferability and a unique leapfrogging pattern across languages during\niterative training. To our knowledge, this is the first representation learning\nmethod devoid of traditional language models for understanding sentence and\ndocument semantics, marking a stride closer to human-like textual\ncomprehension. Our code is available at\nhttps://github.com/gowitheflow-1998/Pixel-Linguist", "published": "2024-02-13 02:46:45", "link": "http://arxiv.org/abs/2402.08183v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Towards Faithful and Robust LLM Specialists for Evidence-Based\n  Question-Answering", "abstract": "Advances towards more faithful and traceable answers of Large Language Models\n(LLMs) are crucial for various research and practical endeavors. One avenue in\nreaching this goal is basing the answers on reliable sources. However, this\nEvidence-Based QA has proven to work insufficiently with LLMs in terms of\nciting the correct sources (source quality) and truthfully representing the\ninformation within sources (answer attributability). In this work, we\nsystematically investigate how to robustly fine-tune LLMs for better source\nquality and answer attributability. Specifically, we introduce a data\ngeneration pipeline with automated data quality filters, which can synthesize\ndiversified high-quality training and testing data at scale. We further\nintroduce four test sets to benchmark the robustness of fine-tuned specialist\nmodels. Extensive evaluation shows that fine-tuning on synthetic data improves\nperformance on both in- and out-of-distribution. Furthermore, we show that data\nquality, which can be drastically improved by proposed quality filters, matters\nmore than quantity in improving Evidence-Based QA.", "published": "2024-02-13 08:12:48", "link": "http://arxiv.org/abs/2402.08277v5", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Values That Are Explicitly Present in Fairy Tales: Comparing Samples\n  from German, Italian and Portuguese Traditions", "abstract": "Looking at how social values are represented in fairy tales can give insights\nabout the variations in communication of values across cultures. We study how\nvalues are communicated in fairy tales from Portugal, Italy and Germany using a\ntechnique called word embedding with a compass to quantify vocabulary\ndifferences and commonalities. We study how these three national traditions\ndiffer in their explicit references to values. To do this, we specify a list of\nvalue-charged tokens, consider their word stems and analyse the distance\nbetween these in a bespoke pre-trained Word2Vec model. We triangulate and\ncritically discuss the validity of the resulting hypotheses emerging from this\nquantitative model. Our claim is that this is a reusable and reproducible\nmethod for the study of the values explicitly referenced in historical corpora.\nFinally, our preliminary findings hint at a shared cultural understanding and\nthe expression of values such as Benevolence, Conformity, and Universalism\nacross the studied cultures, suggesting the potential existence of a\npan-European cultural memory.", "published": "2024-02-13 09:26:19", "link": "http://arxiv.org/abs/2402.08318v3", "categories": ["cs.CL", "cs.CY", "J.5; K.4.m"], "primary_category": "cs.CL"}
{"title": "Eliciting Personality Traits in Large Language Models", "abstract": "Large Language Models (LLMs) are increasingly being utilized by both\ncandidates and employers in the recruitment context. However, with this comes\nnumerous ethical concerns, particularly related to the lack of transparency in\nthese \"black-box\" models. Although previous studies have sought to increase the\ntransparency of these models by investigating the personality traits of LLMs,\nmany of the previous studies have provided them with personality assessments to\ncomplete. On the other hand, this study seeks to obtain a better understanding\nof such models by examining their output variations based on different input\nprompts. Specifically, we use a novel elicitation approach using prompts\nderived from common interview questions, as well as prompts designed to elicit\nparticular Big Five personality traits to examine whether the models were\nsusceptible to trait-activation like humans are, to measure their personality\nbased on the language used in their outputs. To do so, we repeatedly prompted\nmultiple LMs with different parameter sizes, including Llama-2, Falcon,\nMistral, Bloom, GPT, OPT, and XLNet (base and fine tuned versions) and examined\ntheir personality using classifiers trained on the myPersonality dataset. Our\nresults reveal that, generally, all LLMs demonstrate high openness and low\nextraversion. However, whereas LMs with fewer parameters exhibit similar\nbehaviour in personality traits, newer and LMs with more parameters exhibit a\nbroader range of personality traits, with increased agreeableness, emotional\nstability, and openness. Furthermore, a greater number of parameters is\npositively associated with openness and conscientiousness. Moreover, fine-tuned\nmodels exhibit minor modulations in their personality traits, contingent on the\ndataset. Implications and directions for future research are discussed.", "published": "2024-02-13 10:09:00", "link": "http://arxiv.org/abs/2402.08341v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Can LLMs Learn New Concepts Incrementally without Forgetting?", "abstract": "Large Language Models (LLMs) have achieved remarkable success across various\ntasks, yet their ability to learn incrementally without forgetting remains\nunderexplored. Incremental learning (IL) is crucial as it enables models to\nacquire new knowledge while retaining previously learned information, akin to\nhuman learning. Existing benchmarks for IL are insufficient due to data leakage\nissues and the overqualification of LLMs. To address these challenges, we\nintroduce Concept-1K, a novel dataset comprising 1,023 recently emerged\nconcepts across diverse domains. The concepts in Concept-1K are discrete,\ninterpretable units of knowledge that allow for fine-grained analysis of\nlearning and forgetting processes. Using Concept-1K as a testbed, we aim to\nanswer the question: ``Can LLMs learn new concepts incrementally without\nforgetting like humans?'' Our investigation reveals that LLMs still suffer from\ncatastrophic forgetting and that LoRA, despite fine-tuning fewer parameters,\nmay lead to more forgetting on training data. Additionally, we explore the\nroles of in-context learning, model scale, buffer size, and pretraining in IL\nperformance. These findings highlight the strengths and limitations of LLMs in\nIL scenarios and provide a robust benchmark for future research.", "published": "2024-02-13 15:29:50", "link": "http://arxiv.org/abs/2402.08526v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Higher Layers Need More LoRA Experts", "abstract": "Parameter-efficient tuning (PEFT) techniques like low-rank adaptation (LoRA)\noffer training efficiency on Large Language Models, but their impact on model\nperformance remains limited. Recent efforts integrate LoRA and\nMixture-of-Experts (MoE) to improve the performance of PEFT methods. Despite\npromising results, research on improving the efficiency of LoRA with MoE is\nstill in its early stages. Recent studies have shown that experts in the MoE\narchitecture have different strengths and also exhibit some redundancy. Does\nthis statement also apply to parameter-efficient MoE? In this paper, we\nintroduce a novel parameter-efficient MoE method,\n\\textit{\\textbf{M}oE-L\\textbf{o}RA with \\textbf{L}ayer-wise Expert\n\\textbf{A}llocation (MoLA)} for Transformer-based models, where each model\nlayer has the flexibility to employ a varying number of LoRA experts. We\ninvestigate several architectures with varying layer-wise expert\nconfigurations. Experiments on six well-known NLP and commonsense QA benchmarks\ndemonstrate that MoLA achieves equal or superior performance compared to all\nbaselines. We find that allocating more LoRA experts to higher layers further\nenhances the effectiveness of models with a certain number of experts in total.\nWith much fewer parameters, this allocation strategy outperforms the setting\nwith the same number of experts in every layer. This work can be widely used as\na plug-and-play parameter-efficient tuning approach for various applications.\nThe code is available at https://github.com/GCYZSL/MoLA.", "published": "2024-02-13 16:04:21", "link": "http://arxiv.org/abs/2402.08562v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Tandem Transformers for Inference Efficient LLMs", "abstract": "The autoregressive nature of conventional large language models (LLMs)\ninherently limits inference speed, as tokens are generated sequentially. While\nspeculative and parallel decoding techniques attempt to mitigate this, they\nface limitations: either relying on less accurate smaller models for generation\nor failing to fully leverage the base LLM's representations.\n  We introduce a novel architecture, Tandem transformers, to address these\nissues. This architecture uniquely combines (1) a small autoregressive model\nand (2) a large model operating in block mode (processing multiple tokens\nsimultaneously). The small model's predictive accuracy is substantially\nenhanced by granting it attention to the large model's richer representations.\nOn the PaLM2 pretraining dataset, a tandem of PaLM2-Bison and PaLM2-Gecko\ndemonstrates a 3.3% improvement in next-token prediction accuracy over a\nstandalone PaLM2-Gecko, offering a 1.16x speedup compared to a PaLM2-Otter\nmodel with comparable downstream performance. We further incorporate the tandem\nmodel within the speculative decoding (SPEED) framework where the large model\nvalidates tokens from the small model. This ensures that the Tandem of\nPaLM2-Bison and PaLM2-Gecko achieves substantial speedup (around 1.14x faster\nthan using vanilla PaLM2-Gecko in SPEED) while maintaining identical downstream\ntask accuracy.", "published": "2024-02-13 18:24:08", "link": "http://arxiv.org/abs/2402.08644v4", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Learning How To Ask: Cycle-Consistency Refines Prompts in Multimodal\n  Foundation Models", "abstract": "When LLMs perform zero-shot inference, they typically use a prompt with a\ntask specification, and generate a completion. However, there is no work to\nexplore the possibility of the reverse - going from completion to task\nspecification. In this paper, we employ both directions to perform\ncycle-supervised learning entirely in-context. Our goal is to create a forward\nmap f : X -> Y (e.g. image -> generated caption), coupled with a backward map g\n: Y -> X (e.g. caption -> generated image) to construct a cycle-consistency\n\"loss\" (formulated as an update to the prompt) to enforce g(f(X)) ~= X. The\ntechnique, called CyclePrompt, uses cycle-consistency as a free supervisory\nsignal to iteratively craft the prompt. Importantly, CyclePrompt reinforces\nmodel performance without expensive fine-tuning, without training data, and\nwithout the complexity of external environments (e.g. compilers, APIs). We\ndemonstrate CyclePrompt in two domains: code generation and image captioning.\nOur results on the HumanEval coding benchmark put us in first place on the\nleaderboard among models that do not rely on extra training data or usage of\nexternal environments, and third overall. Compared to the GPT4 baseline, we\nimprove accuracy from 80.5% to 87.2%. In the vision-language space, we generate\ndetailed image captions which outperform baseline zero-shot GPT4V captions,\nwhen tested against natural (VQAv2) and diagrammatic (FigureQA) visual\nquestion-answering benchmarks. To the best of our knowledge, this is the first\nuse of self-supervised learning for prompting.", "published": "2024-02-13 19:49:17", "link": "http://arxiv.org/abs/2402.08756v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "JAMDEC: Unsupervised Authorship Obfuscation using Constrained Decoding\n  over Small Language Models", "abstract": "The permanence of online content combined with the enhanced authorship\nidentification techniques calls for stronger computational methods to protect\nthe identity and privacy of online authorship when needed, e.g., blind reviews\nfor scientific papers, anonymous online reviews, or anonymous interactions in\nthe mental health forums. In this paper, we propose an unsupervised\ninference-time approach to authorship obfuscation to address the unique\nchallenges of authorship obfuscation: lack of supervision data for diverse\nauthorship and domains, and the need for a sufficient level of revision beyond\nsimple paraphrasing to obfuscate the authorship, all the while preserving the\noriginal content and fluency.\n  We introduce JAMDEC, a user-controlled, inference-time algorithm for\nauthorship obfuscation that can be in principle applied to any text and\nauthorship. Our approach builds on small language models such as GPT2-XL in\norder to help avoid disclosing the original content to proprietary LLM's APIs,\nwhile also reducing the performance gap between small and large language models\nvia algorithmic enhancement. The key idea behind our approach is to boost the\ncreative power of smaller language models through constrained decoding, while\nalso allowing for user-specified controls and flexibility. Experimental results\ndemonstrate that our approach based on GPT2-XL outperforms previous\nstate-of-the-art methods based on comparably small models, while performing\ncompetitively against GPT3.5 175B, a propriety model that is two orders of\nmagnitudes larger.", "published": "2024-02-13 19:54:29", "link": "http://arxiv.org/abs/2402.08761v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Rethinking Machine Unlearning for Large Language Models", "abstract": "We explore machine unlearning (MU) in the domain of large language models\n(LLMs), referred to as LLM unlearning. This initiative aims to eliminate\nundesirable data influence (e.g., sensitive or illegal information) and the\nassociated model capabilities, while maintaining the integrity of essential\nknowledge generation and not affecting causally unrelated information. We\nenvision LLM unlearning becoming a pivotal element in the life-cycle management\nof LLMs, potentially standing as an essential foundation for developing\ngenerative AI that is not only safe, secure, and trustworthy, but also\nresource-efficient without the need of full retraining. We navigate the\nunlearning landscape in LLMs from conceptual formulation, methodologies,\nmetrics, and applications. In particular, we highlight the often-overlooked\naspects of existing LLM unlearning research, e.g., unlearning scope, data-model\ninteraction, and multifaceted efficacy assessment. We also draw connections\nbetween LLM unlearning and related areas such as model editing, influence\nfunctions, model explanation, adversarial training, and reinforcement learning.\nFurthermore, we outline an effective assessment framework for LLM unlearning\nand explore its applications in copyright and privacy safeguards and\nsociotechnical harm reduction.", "published": "2024-02-13 20:51:58", "link": "http://arxiv.org/abs/2402.08787v6", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "GLoRe: When, Where, and How to Improve LLM Reasoning via Global and\n  Local Refinements", "abstract": "State-of-the-art language models can exhibit impressive reasoning refinement\ncapabilities on math, science or coding tasks. However, recent work\ndemonstrates that even the best models struggle to identify \\textit{when and\nwhere to refine} without access to external feedback. Outcome-based Reward\nModels (\\textbf{ORMs}), trained to predict correctness of the final answer\nindicating when to refine, offer one convenient solution for deciding when to\nrefine. Process Based Reward Models (\\textbf{PRMs}), trained to predict\ncorrectness of intermediate steps, can then be used to indicate where to\nrefine. But they are expensive to train, requiring extensive human annotations.\nIn this paper, we propose Stepwise ORMs (\\textbf{SORMs}) which are trained,\nonly on synthetic data, to approximate the expected future reward of the\noptimal policy or $V^{\\star}$. More specifically, SORMs are trained to predict\nthe correctness of the final answer when sampling the current policy many times\n(rather than only once as in the case of ORMs). Our experiments show that SORMs\ncan more accurately detect incorrect reasoning steps compared to ORMs, thus\nimproving downstream accuracy when doing refinements. We then train\n\\textit{global} refinement models, which take only the question and a draft\nsolution as input and predict a corrected solution, and \\textit{local}\nrefinement models which also take as input a critique indicating the location\nof the first reasoning error. We generate training data for both models\nsynthetically by reusing data used to train the SORM. We find combining global\nand local refinements, using the ORM as a reranker, significantly outperforms\neither one individually, as well as a best of three sample baseline. With this\nstrategy we can improve the accuracy of a LLaMA-2 13B model (already fine-tuned\nwith RL) on GSM8K from 53\\% to 65\\% when greedily sampled.", "published": "2024-02-13 20:16:29", "link": "http://arxiv.org/abs/2402.10963v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BBox-Adapter: Lightweight Adapting for Black-Box Large Language Models", "abstract": "Adapting state-of-the-art Large Language Models (LLMs) like GPT-4 and Gemini\nfor specific tasks is challenging. Due to the opacity in their parameters,\nembeddings, and even output probabilities, existing fine-tuning adaptation\nmethods are inapplicable. Consequently, adapting these black-box LLMs is only\npossible through their API services, raising concerns about transparency,\nprivacy, and cost. To address these challenges, we introduce BBox-Adapter, a\nnovel lightweight adapter for black-box LLMs. BBox-Adapter distinguishes target\nand source domain data by treating target data as positive and source data as\nnegative. It employs a ranking-based Noise Contrastive Estimation (NCE) loss to\npromote the likelihood of target domain data while penalizing that of the\nsource domain. Furthermore, it features an online adaptation mechanism, which\nincorporates real-time positive data sampling from ground-truth, human, or AI\nfeedback, coupled with negative data from previous adaptations. Extensive\nexperiments demonstrate BBox-Adapter's effectiveness and cost efficiency. It\nimproves model performance by up to 6.77% across diverse tasks and domains,\nwhile reducing training and inference costs by 31.30x and 1.84x, respectively.", "published": "2024-02-13 05:15:46", "link": "http://arxiv.org/abs/2402.08219v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Prompted Contextual Vectors for Spear-Phishing Detection", "abstract": "Spear-phishing attacks present a significant security challenge, with large\nlanguage models (LLMs) escalating the threat by generating convincing emails\nand facilitating target reconnaissance. To address this, we propose a detection\napproach based on a novel document vectorization method that utilizes an\nensemble of LLMs to create representation vectors. By prompting LLMs to reason\nand respond to human-crafted questions, we quantify the presence of common\npersuasion principles in the email's content, producing prompted contextual\ndocument vectors for a downstream supervised machine learning model. We\nevaluate our method using a unique dataset generated by a proprietary system\nthat automates target reconnaissance and spear-phishing email creation. Our\nmethod achieves a 91\\% F1 score in identifying LLM-generated spear-phishing\nemails, with the training set comprising only traditional phishing and benign\nemails. Key contributions include a novel document vectorization method\nutilizing LLM reasoning, a publicly available dataset of high-quality\nspear-phishing emails, and the demonstrated effectiveness of our method in\ndetecting such emails. This methodology can be utilized for various document\nclassification tasks, particularly in adversarial problem domains.", "published": "2024-02-13 09:12:55", "link": "http://arxiv.org/abs/2402.08309v3", "categories": ["cs.LG", "cs.CL", "cs.CR", "I.2.7"], "primary_category": "cs.LG"}
{"title": "Evaluating the Data Model Robustness of Text-to-SQL Systems Based on\n  Real User Queries", "abstract": "Text-to-SQL systems (also known as NL-to-SQL systems) have become an\nincreasingly popular solution for bridging the gap between user capabilities\nand SQL-based data access. These systems translate user requests in natural\nlanguage to valid SQL statements for a specific database. Recent Text-to-SQL\nsystems have benefited from the rapid improvement of transformer-based language\nmodels. However, while Text-to-SQL systems that incorporate such models\ncontinuously reach new high scores on -- often synthetic -- benchmark datasets,\na systematic exploration of their robustness towards different data models in a\nreal-world, realistic scenario is notably missing. This paper provides the\nfirst in-depth evaluation of the data model robustness of Text-to-SQL systems\nin practice based on a multi-year international project focused on Text-to-SQL\ninterfaces. Our evaluation is based on a real-world deployment of FootballDB, a\nsystem that was deployed over a 9 month period in the context of the FIFA World\nCup 2022, during which about 6K natural language questions were asked and\nexecuted. All of our data is based on real user questions that were asked live\nto the system. We manually labeled and translated a subset of these questions\nfor three different data models. For each data model, we explore the\nperformance of representative Text-to-SQL systems and language models. We\nfurther quantify the impact of training data size, pre-, and post-processing\nsteps as well as language model inference time. Our comprehensive evaluation\nsheds light on the design choices of real-world Text-to-SQL systems and their\nimpact on moving from research prototypes to real deployments. Last, we provide\na new benchmark dataset to the community, which is the first to enable the\nevaluation of different data models for the same dataset and is substantially\nmore challenging than most previous datasets in terms of query complexity.", "published": "2024-02-13 10:28:57", "link": "http://arxiv.org/abs/2402.08349v3", "categories": ["cs.DB", "cs.AI", "cs.CL"], "primary_category": "cs.DB"}
{"title": "A Systematic Review of Data-to-Text NLG", "abstract": "This systematic review undertakes a comprehensive analysis of current\nresearch on data-to-text generation, identifying gaps, challenges, and future\ndirections within the field. Relevant literature in this field on datasets,\nevaluation metrics, application areas, multilingualism, language models, and\nhallucination mitigation methods is reviewed. Various methods for producing\nhigh-quality text are explored, addressing the challenge of hallucinations in\ndata-to-text generation. These methods include re-ranking, traditional and\nneural pipeline architecture, planning architectures, data cleaning, controlled\ngeneration, and modification of models and training techniques. Their\neffectiveness and limitations are assessed, highlighting the need for\nuniversally applicable strategies to mitigate hallucinations. The review also\nexamines the usage, popularity, and impact of datasets, alongside evaluation\nmetrics, with an emphasis on both automatic and human assessment. Additionally,\nthe evolution of data-to-text models, particularly the widespread adoption of\ntransformer models, is discussed. Despite advancements in text quality, the\nreview emphasizes the importance of research in low-resourced languages and the\nengineering of datasets in these languages to promote inclusivity. Finally,\nseveral application domains of data-to-text are highlighted, emphasizing their\nrelevance in such domains. Overall, this review serves as a guiding framework\nfor fostering innovation and advancing data-to-text generation.", "published": "2024-02-13 14:51:45", "link": "http://arxiv.org/abs/2402.08496v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Knowledge Editing on Black-box Large Language Models", "abstract": "Knowledge editing (KE) aims to efficiently and precisely modify the behavior\nof large language models (LLMs) to update specific knowledge without negatively\ninfluencing other knowledge. Current research primarily focuses on white-box\nLLMs editing, overlooking an important scenario: black-box LLMs editing, where\nLLMs are accessed through interfaces and only textual output is available. In\nthis paper, we first officially introduce KE on black-box LLMs and then propose\na comprehensive evaluation framework to overcome the limitations of existing\nevaluations that are not applicable to black-box LLMs editing and lack\ncomprehensiveness. To tackle privacy leaks of editing data and style\nover-editing in current methods, we introduce a novel postEdit framework,\nresolving privacy concerns through downstream post-processing and maintaining\ntextual style consistency via fine-grained editing to original responses.\nExperiments and analysis on two benchmarks demonstrate that postEdit\noutperforms all baselines and achieves strong generalization, especially with\nhuge improvements on style retention (average $+20.82\\%\\uparrow$).", "published": "2024-02-13 17:59:34", "link": "http://arxiv.org/abs/2402.08631v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability", "abstract": "Jailbreaks on large language models (LLMs) have recently received increasing\nattention. For a comprehensive assessment of LLM safety, it is essential to\nconsider jailbreaks with diverse attributes, such as contextual coherence and\nsentiment/stylistic variations, and hence it is beneficial to study\ncontrollable jailbreaking, i.e. how to enforce control on LLM attacks. In this\npaper, we formally formulate the controllable attack generation problem, and\nbuild a novel connection between this problem and controllable text generation,\na well-explored topic of natural language processing. Based on this connection,\nwe adapt the Energy-based Constrained Decoding with Langevin Dynamics (COLD), a\nstate-of-the-art, highly efficient algorithm in controllable text generation,\nand introduce the COLD-Attack framework which unifies and automates the search\nof adversarial LLM attacks under a variety of control requirements such as\nfluency, stealthiness, sentiment, and left-right-coherence. The controllability\nenabled by COLD-Attack leads to diverse new jailbreak scenarios which not only\ncover the standard setting of generating fluent (suffix) attack with\ncontinuation constraint, but also allow us to address new controllable attack\nsettings such as revising a user query adversarially with paraphrasing\nconstraint, and inserting stealthy attacks in context with position constraint.\nOur extensive experiments on various LLMs (Llama-2, Mistral, Vicuna, Guanaco,\nGPT-3.5, and GPT-4) show COLD-Attack's broad applicability, strong\ncontrollability, high success rate, and attack transferability. Our code is\navailable at https://github.com/Yu-Fangxu/COLD-Attack.", "published": "2024-02-13 18:58:48", "link": "http://arxiv.org/abs/2402.08679v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Mitigating Object Hallucination in Large Vision-Language Models via\n  Classifier-Free Guidance", "abstract": "The advancement of Large Vision-Language Models (LVLMs) has increasingly\nhighlighted the critical issue of their tendency to hallucinate non-existing\nobjects in the images. To address this issue, previous works focused on using\nspecially curated datasets or powerful LLMs (e.g., GPT-3.5) to rectify the\noutputs of LVLMs. However, these approaches require either expensive\ntraining/fine-tuning or API access to advanced LLMs to correct the model's\noutput post-generation. In this paper, we tackle this challenge by introducing\na framework called Mitigating hallucinAtion via classifieR-Free guIdaNcE\n(MARINE), which is both training-free and API-free, and can effectively and\nefficiently reduce object hallucinations during the generation process.\nSpecifically, MARINE enriches the visual context of LVLMs by integrating\nexisting open-source vision models, and employs classifier-free guidance to\nincorporate the additional object grounding features to improve the precision\nof LVLMs' generations. Through comprehensive evaluations across $6$ popular\nLVLMs with diverse evaluation metrics, we demonstrate the effectiveness of\nMARINE, which even outperforms existing fine-tuning-based methods. Remarkably,\nit not only reduces hallucinations but also improves the detailedness of LVLMs'\ngenerations, as assessed by GPT-4V.", "published": "2024-02-13 18:59:05", "link": "http://arxiv.org/abs/2402.08680v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human\n  Feedback and Heuristic-based Sampling", "abstract": "Prompt optimization aims to find the best prompt to a large language model\n(LLM) for a given task. LLMs have been successfully used to help find and\nimprove prompt candidates for single-step tasks. However, realistic tasks for\nagents are multi-step and introduce new challenges: (1) Prompt content is\nlikely to be more extensive and complex, making it more difficult for LLMs to\nanalyze errors, (2) the impact of an individual step is difficult to evaluate,\nand (3) different people may have varied preferences about task execution.\nWhile humans struggle to optimize prompts, they are good at providing feedback\nabout LLM outputs; we therefore introduce a new LLM-driven discrete prompt\noptimization framework PRompt Optimization in Multi-Step Tasks (PROMST) that\nincorporates human-designed feedback rules to automatically offer direct\nsuggestions for improvement. We also use an extra learned heuristic model that\npredicts prompt performance to efficiently sample from prompt candidates. This\napproach significantly outperforms both human-engineered prompts and several\nother prompt optimization methods across 11 representative multi-step tasks (an\naverage 10.6\\%-29.3\\% improvement to current best methods on five LLMs\nrespectively). We believe our work can serve as a benchmark for automatic\nprompt optimization for LLM-driven multi-step tasks. Datasets and Codes are\navailable at https://github.com/yongchao98/PROMST. Project Page is available at\nhttps://yongchao98.github.io/MIT-REALM-PROMST.", "published": "2024-02-13 16:38:01", "link": "http://arxiv.org/abs/2402.08702v4", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.RO"], "primary_category": "cs.CL"}
{"title": "DNABERT-S: Pioneering Species Differentiation with Species-Aware DNA\n  Embeddings", "abstract": "We introduce DNABERT-S, a tailored genome model that develops species-aware\nembeddings to naturally cluster and segregate DNA sequences of different\nspecies in the embedding space. Differentiating species from genomic sequences\n(i.e., DNA and RNA) is vital yet challenging, since many real-world species\nremain uncharacterized, lacking known genomes for reference. Embedding-based\nmethods are therefore used to differentiate species in an unsupervised manner.\nDNABERT-S builds upon a pre-trained genome foundation model named DNABERT-2. To\nencourage effective embeddings to error-prone long-read DNA sequences, we\nintroduce Manifold Instance Mixup (MI-Mix), a contrastive objective that mixes\nthe hidden representations of DNA sequences at randomly selected layers and\ntrains the model to recognize and differentiate these mixed proportions at the\noutput layer. We further enhance it with the proposed Curriculum Contrastive\nLearning (C$^2$LR) strategy. Empirical results on 23 diverse datasets show\nDNABERT-S's effectiveness, especially in realistic label-scarce scenarios. For\nexample, it identifies twice more species from a mixture of unlabeled genomic\nsequences, doubles the Adjusted Rand Index (ARI) in species clustering, and\noutperforms the top baseline's performance in 10-shot species classification\nwith just a 2-shot training. Model, codes, and data are publicly available at\n\\url{https://github.com/MAGICS-LAB/DNABERT_S}.", "published": "2024-02-13 20:21:29", "link": "http://arxiv.org/abs/2402.08777v3", "categories": ["q-bio.GN", "cs.AI", "cs.CE", "cs.CL"], "primary_category": "q-bio.GN"}
{"title": "Syllable based DNN-HMM Cantonese Speech to Text System", "abstract": "This paper reports our work on building up a Cantonese Speech-to-Text (STT)\nsystem with a syllable based acoustic model. This is a part of an effort in\nbuilding a STT system to aid dyslexic students who have cognitive deficiency in\nwriting skills but have no problem expressing their ideas through speech. For\nCantonese speech recognition, the basic unit of acoustic models can either be\nthe conventional Initial-Final (IF) syllables, or the Onset-Nucleus-Coda (ONC)\nsyllables where finals are further split into nucleus and coda to reflect the\nintra-syllable variations in Cantonese. By using the Kaldi toolkit, our system\nis trained using the stochastic gradient descent optimization model with the\naid of GPUs for the hybrid Deep Neural Network and Hidden Markov Model\n(DNN-HMM) with and without I-vector based speaker adaptive training technique.\nThe input features of the same Gaussian Mixture Model with speaker adaptive\ntraining (GMM-SAT) to DNN are used in all cases. Experiments show that the\nONC-based syllable acoustic modeling with I-vector based DNN-HMM achieves the\nbest performance with the word error rate (WER) of 9.66% and the real time\nfactor (RTF) of 1.38812.", "published": "2024-02-13 20:54:24", "link": "http://arxiv.org/abs/2402.08788v1", "categories": ["cs.CL", "cs.SD", "eess.AS", "94-06", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Sequence graphs realizations and ambiguity in language models", "abstract": "Several popular language models represent local contexts in an input text as\nbags of words. Such representations are naturally encoded by a sequence graph\nwhose vertices are the distinct words occurring in x, with edges representing\nthe (ordered) co-occurrence of two words within a sliding window of size w.\nHowever, this compressed representation is not generally bijective, and may\nintroduce some degree of ambiguity. Some sequence graphs may admit several\nrealizations as a sequence, while others may not admit any realization. In this\npaper, we study the realizability and ambiguity of sequence graphs from a\ncombinatorial and computational point of view. We consider the existence and\nenumeration of realizations of a sequence graph under multiple settings: window\nsize w, presence/absence of graph orientation, and presence/absence of weights\n(multiplicities). When w = 2, we provide polynomial time algorithms for\nrealizability and enumeration in all cases except the undirected/weighted\nsetting, where we show the #P-hardness of enumeration. For a window of size at\nleast 3, we prove hardness of all variants, even when w is considered as a\nconstant, with the notable exception of the undirected/unweighted case for\nwhich we propose an XP algorithms for both (realizability and enumeration)\nproblems, tight due to a corresponding W[1]-hardness result. We conclude with\nan integer program formulation to solve the realizability problem, and with\ndynamic programming to solve the enumeration problem. This work leaves open the\nmembership to NP for both problems, a non-trivial question due to the existence\nof minimum realizations having exponential size on the instance encoding.", "published": "2024-02-13 22:22:51", "link": "http://arxiv.org/abs/2402.08830v1", "categories": ["cs.DS", "cs.CC", "cs.CL"], "primary_category": "cs.DS"}
{"title": "eCeLLM: Generalizing Large Language Models for E-commerce from\n  Large-scale, High-quality Instruction Data", "abstract": "With tremendous efforts on developing effective e-commerce models,\nconventional e-commerce models show limited success in generalist e-commerce\nmodeling, and suffer from unsatisfactory performance on new users and new\nproducts - a typical out-of-domain generalization challenge. Meanwhile, large\nlanguage models (LLMs) demonstrate outstanding performance in generalist\nmodeling and out-of-domain generalizability in many fields. Toward fully\nunleashing their power for e-commerce, in this paper, we construct ECInstruct,\nthe first open-sourced, large-scale, and high-quality benchmark instruction\ndataset for e-commerce. Leveraging ECInstruct, we develop eCeLLM, a series of\ne-commerce LLMs, by instruction-tuning general-purpose LLMs. Our comprehensive\nexperiments and evaluation demonstrate that eCeLLM models substantially\noutperform baseline models, including the most advanced GPT-4, and the\nstate-of-the-art task-specific models in in-domain evaluation. Moreover, eCeLLM\nexhibits excellent generalizability to out-of-domain settings, including unseen\nproducts and unseen instructions, highlighting its superiority as a generalist\ne-commerce model. Both the ECInstruct dataset and the eCeLLM models show great\npotential in empowering versatile and effective LLMs for e-commerce. ECInstruct\nand eCeLLM models are publicly accessible through\nhttps://ninglab.github.io/eCeLLM.", "published": "2024-02-13 22:26:24", "link": "http://arxiv.org/abs/2402.08831v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Measuring and Controlling Instruction (In)Stability in Language Model\n  Dialogs", "abstract": "System-prompting is a standard tool for customizing language-model chatbots,\nenabling them to follow a specific instruction. An implicit assumption in the\nuse of system prompts is that they will be stable, so the chatbot will continue\nto generate text according to the stipulated instructions for the duration of a\nconversation. We propose a quantitative benchmark to test this assumption,\nevaluating instruction stability via self-chats between two instructed\nchatbots. Testing popular models like LLaMA2-chat-70B and GPT-3.5, we reveal a\nsignificant instruction drift within eight rounds of conversations. An\nempirical and theoretical analysis of this phenomenon suggests the transformer\nattention mechanism plays a role, due to attention decay over long exchanges.\nTo combat attention decay and instruction drift, we propose a lightweight\nmethod called split-softmax, which compares favorably against two strong\nbaselines.", "published": "2024-02-13 20:10:29", "link": "http://arxiv.org/abs/2402.10962v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ChatCell: Facilitating Single-Cell Analysis with Natural Language", "abstract": "As Large Language Models (LLMs) rapidly evolve, their influence in science is\nbecoming increasingly prominent. The emerging capabilities of LLMs in task\ngeneralization and free-form dialogue can significantly advance fields like\nchemistry and biology. However, the field of single-cell biology, which forms\nthe foundational building blocks of living organisms, still faces several\nchallenges. High knowledge barriers and limited scalability in current methods\nrestrict the full exploitation of LLMs in mastering single-cell data, impeding\ndirect accessibility and rapid iteration. To this end, we introduce ChatCell,\nwhich signifies a paradigm shift by facilitating single-cell analysis with\nnatural language. Leveraging vocabulary adaptation and unified sequence\ngeneration, ChatCell has acquired profound expertise in single-cell biology and\nthe capability to accommodate a diverse range of analysis tasks. Extensive\nexperiments further demonstrate ChatCell's robust performance and potential to\ndeepen single-cell insights, paving the way for more accessible and intuitive\nexploration in this pivotal field. Our project homepage is available at\nhttps://zjunlp.github.io/project/ChatCell.", "published": "2024-02-13 09:06:14", "link": "http://arxiv.org/abs/2402.08303v4", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM\n  Agents Exponentially Fast", "abstract": "A multimodal large language model (MLLM) agent can receive instructions,\ncapture images, retrieve histories from memory, and decide which tools to use.\nNonetheless, red-teaming efforts have revealed that adversarial images/prompts\ncan jailbreak an MLLM and cause unaligned behaviors. In this work, we report an\neven more severe safety issue in multi-agent environments, referred to as\ninfectious jailbreak. It entails the adversary simply jailbreaking a single\nagent, and without any further intervention from the adversary, (almost) all\nagents will become infected exponentially fast and exhibit harmful behaviors.\nTo validate the feasibility of infectious jailbreak, we simulate multi-agent\nenvironments containing up to one million LLaVA-1.5 agents, and employ\nrandomized pair-wise chat as a proof-of-concept instantiation for multi-agent\ninteraction. Our results show that feeding an (infectious) adversarial image\ninto the memory of any randomly chosen agent is sufficient to achieve\ninfectious jailbreak. Finally, we derive a simple principle for determining\nwhether a defense mechanism can provably restrain the spread of infectious\njailbreak, but how to design a practical defense that meets this principle\nremains an open question to investigate. Our project page is available at\nhttps://sail-sg.github.io/Agent-Smith/.", "published": "2024-02-13 16:06:17", "link": "http://arxiv.org/abs/2402.08567v2", "categories": ["cs.CL", "cs.CR", "cs.CV", "cs.LG", "cs.MA"], "primary_category": "cs.CL"}
{"title": "Test-Time Backdoor Attacks on Multimodal Large Language Models", "abstract": "Backdoor attacks are commonly executed by contaminating training data, such\nthat a trigger can activate predetermined harmful effects during the test\nphase. In this work, we present AnyDoor, a test-time backdoor attack against\nmultimodal large language models (MLLMs), which involves injecting the backdoor\ninto the textual modality using adversarial test images (sharing the same\nuniversal perturbation), without requiring access to or modification of the\ntraining data. AnyDoor employs similar techniques used in universal adversarial\nattacks, but distinguishes itself by its ability to decouple the timing of\nsetup and activation of harmful effects. In our experiments, we validate the\neffectiveness of AnyDoor against popular MLLMs such as LLaVA-1.5, MiniGPT-4,\nInstructBLIP, and BLIP-2, as well as provide comprehensive ablation studies.\nNotably, because the backdoor is injected by a universal perturbation, AnyDoor\ncan dynamically change its backdoor trigger prompts/harmful effects, exposing a\nnew challenge for defending against backdoor attacks. Our project page is\navailable at https://sail-sg.github.io/AnyDoor/.", "published": "2024-02-13 16:28:28", "link": "http://arxiv.org/abs/2402.08577v1", "categories": ["cs.CL", "cs.CR", "cs.CV", "cs.LG", "cs.MM"], "primary_category": "cs.CL"}
{"title": "An Embarrassingly Simple Approach for LLM with Strong ASR Capacity", "abstract": "In this paper, we focus on solving one of the most important tasks in the\nfield of speech processing, i.e., automatic speech recognition (ASR), with\nspeech foundation encoders and large language models (LLM). Recent works have\ncomplex designs such as compressing the output temporally for the speech\nencoder, tackling modal alignment for the projector, and utilizing\nparameter-efficient fine-tuning for the LLM. We found that delicate designs are\nnot necessary, while an embarrassingly simple composition of off-the-shelf\nspeech encoder, LLM, and the only trainable linear projector is competent for\nthe ASR task. To be more specific, we benchmark and explore various\ncombinations of LLMs and speech encoders, leading to the optimal LLM-based ASR\nsystem, which we call SLAM-ASR. The proposed SLAM-ASR provides a clean setup\nand little task-specific design, where only the linear projector is trained. To\nthe best of our knowledge, SLAM-ASR achieves the best performance on the\nLibrispeech benchmark among LLM-based ASR models and even outperforms the\nlatest LLM-based audio-universal model trained on massive pair data. Finally,\nwe explore the capability emergence of LLM-based ASR in the process of modal\nalignment. We hope that our study can facilitate the research on extending LLM\nwith cross-modality capacity and shed light on the LLM-based ASR community.", "published": "2024-02-13 23:25:04", "link": "http://arxiv.org/abs/2402.08846v1", "categories": ["cs.CL", "cs.AI", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Unrestricted Global Phase Bias-Aware Single-channel Speech Enhancement\n  with Conformer-based Metric GAN", "abstract": "With the rapid development of neural networks in recent years, the ability of\nvarious networks to enhance the magnitude spectrum of noisy speech in the\nsingle-channel speech enhancement domain has become exceptionally outstanding.\nHowever, enhancing the phase spectrum using neural networks is often\nineffective, which remains a challenging problem. In this paper, we found that\nthe human ear cannot sensitively perceive the difference between a precise\nphase spectrum and a biased phase (BP) spectrum. Therefore, we propose an\noptimization method of phase reconstruction, allowing freedom on the\nglobal-phase bias instead of reconstructing the precise phase spectrum. We\napplied it to a Conformer-based Metric Generative Adversarial Networks (CMGAN)\nbaseline model, which relaxes the existing constraints of precise phase and\ngives the neural network a broader learning space. Results show that this\nmethod achieves a new state-of-the-art performance without incurring additional\ncomputational overhead.", "published": "2024-02-13 06:47:26", "link": "http://arxiv.org/abs/2402.08252v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Channel-Combination Algorithms for Robust Distant Voice Activity and\n  Overlapped Speech Detection", "abstract": "Voice Activity Detection (VAD) and Overlapped Speech Detection (OSD) are key\npre-processing tasks for speaker diarization. In the meeting context, it is\noften easier to capture speech with a distant device. This consideration\nhowever leads to severe performance degradation. We study a unified supervised\nlearning framework to solve distant multi-microphone joint VAD and OSD\n(VAD+OSD). This paper investigates various multi-channel VAD+OSD front-ends\nthat weight and combine incoming channels. We propose three algorithms based on\nthe Self-Attention Channel Combinator (SACC), previously proposed in the\nliterature. Experiments conducted on the AMI meeting corpus exhibit that\nchannel combination approaches bring significant VAD+OSD improvements in the\ndistant speech scenario. Specifically, we explore the use of learned complex\ncombination weights and demonstrate the benefits of such an approach in terms\nof explainability. Channel combination-based VAD+OSD systems are evaluated on\nthe final back-end task, i.e. speaker diarization, and show significant\nimprovements. Finally, since multi-channel systems are trained given a fixed\narray configuration, they may fail in generalizing to other array set-ups, e.g.\nmismatched number of microphones. A channel-number invariant loss is proposed\nto learn a unique feature representation regardless of the number of available\nmicrophones. The evaluation conducted on mismatched array configurations\nhighlights the robustness of this training strategy.", "published": "2024-02-13 09:16:40", "link": "http://arxiv.org/abs/2402.08312v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Springboard, Roadblock or \"Crutch\"?: How Transgender Users Leverage\n  Voice Changers for Gender Presentation in Social Virtual Reality", "abstract": "Social virtual reality (VR) serves as a vital platform for transgender\nindividuals to explore their identities through avatars and foster personal\nconnections within online communities. However, it presents a challenge: the\ndisconnect between avatar embodiment and voice representation, often leading to\nmisgendering and harassment. Prior research acknowledges this issue but\noverlooks the potential solution of voice changers. We interviewed 13\ntransgender and gender-nonconforming users of social VR platforms, focusing on\ntheir experiences with and without voice changers. We found that using a voice\nchanger not only reduces voice-related harassment, but also allows them to\nexperience gender euphoria through both hearing their modified voice and the\nreactions of others to their modified voice, motivating them to pursue voice\ntraining and medication to achieve desired voices. Furthermore, we identified\nthe technical barriers to current voice changer technology and potential\nimprovements to alleviate the problems that transgender and\ngender-nonconforming users face.", "published": "2024-02-13 05:10:04", "link": "http://arxiv.org/abs/2402.08217v1", "categories": ["cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
{"title": "Benchmarking multi-component signal processing methods in the\n  time-frequency plane", "abstract": "Signal processing in the time-frequency plane has a long history and remains\na field of methodological innovation. For instance, detection and denoising\nbased on the zeros of the spectrogram have been proposed since 2015,\ncontrasting with a long history of focusing on larger values of the\nspectrogram. Yet, unlike neighboring fields like optimization and machine\nlearning, time-frequency signal processing lacks widely-adopted benchmarking\ntools. In this work, we contribute an open-source, Python-based toolbox termed\nMCSM-Benchs for benchmarking multi-component signal analysis methods, and we\ndemonstrate our toolbox on three time-frequency benchmarks. First, we compare\ndifferent methods for signal detection based on the zeros of the spectrogram,\nincluding unexplored variations of previously proposed detection tests. Second,\nwe compare zero-based denoising methods to both classical and novel methods\nbased on large values and ridges of the spectrogram. Finally, we compare the\ndenoising performance of these methods against typical spectrogram thresholding\nstrategies, in terms of post-processing artifacts commonly referred to as\nmusical noise. At a low level, the obtained results provide new insight on the\nassessed approaches, and in particular research directions to further develop\nzero-based methods. At a higher level, our benchmarks exemplify the benefits of\nusing a public, collaborative, common framework for benchmarking.", "published": "2024-02-13 15:24:19", "link": "http://arxiv.org/abs/2402.08521v1", "categories": ["eess.SP", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
{"title": "Leveraging cough sounds to optimize chest x-ray usage in low-resource\n  settings", "abstract": "Chest X-ray is a commonly used tool during triage, diagnosis and management\nof respiratory diseases. In resource-constricted settings, optimizing this\nresource can lead to valuable cost savings for the health care system and the\npatients as well as to and improvement in consult time. We used\nprospectively-collected data from 137 patients referred for chest X-ray at the\nChristian Medical Center and Hospital (CMCH) in Purnia, Bihar, India. Each\npatient provided at least five coughs while awaiting radiography. Collected\ncough sounds were analyzed using acoustic AI methods. Cross-validation was done\non temporal and spectral features on the cough sounds of each patient. Features\nwere summarized using standard statistical approaches. Three models were\ndeveloped, tested and compared in their capacity to predict an abnormal result\nin the chest X-ray. All three methods yielded models that could discriminate to\nsome extent between normal and abnormal with the logistic regression performing\nbest with an area under the receiver operating characteristic curves ranging\nfrom 0.7 to 0.78. Despite limitations and its relatively small sample size,\nthis study shows that AI-enabled algorithms can use cough sounds to predict\nwhich individuals presenting for chest radiographic examination will have a\nnormal or abnormal results. These results call for expanding this research\ngiven the potential optimization of limited health care resources in low- and\nmiddle-income countries.", "published": "2024-02-13 20:54:55", "link": "http://arxiv.org/abs/2402.08789v1", "categories": ["eess.AS", "cs.AI", "cs.LG", "q-bio.QM"], "primary_category": "eess.AS"}
