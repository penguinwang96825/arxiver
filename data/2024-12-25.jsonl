{"title": "Using Large Language Models for Automated Grading of Student Writing\n  about Science", "abstract": "Assessing writing in large classes for formal or informal learners presents a\nsignificant challenge. Consequently, most large classes, particularly in\nscience, rely on objective assessment tools such as multiple-choice quizzes,\nwhich have a single correct answer. The rapid development of AI has introduced\nthe possibility of using large language models (LLMs) to evaluate student\nwriting. An experiment was conducted using GPT-4 to determine if machine\nlearning methods based on LLMs can match or exceed the reliability of\ninstructor grading in evaluating short writing assignments on topics in\nastronomy. The audience consisted of adult learners in three massive open\nonline courses (MOOCs) offered through Coursera. One course was on astronomy,\nthe second was on astrobiology, and the third was on the history and philosophy\nof astronomy. The results should also be applicable to non-science majors in\nuniversity settings, where the content and modes of evaluation are similar. The\ndata comprised answers from 120 students to 12 questions across the three\ncourses. GPT-4 was provided with total grades, model answers, and rubrics from\nan instructor for all three courses. In addition to evaluating how reliably the\nLLM reproduced instructor grades, the LLM was also tasked with generating its\nown rubrics. Overall, the LLM was more reliable than peer grading, both in\naggregate and by individual student, and approximately matched instructor\ngrades for all three online courses. The implication is that LLMs may soon be\nused for automated, reliable, and scalable grading of student science writing.", "published": "2024-12-25 00:31:53", "link": "http://arxiv.org/abs/2412.18719v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Generated and Retrieved Knowledge Combination Through\n  Zero-shot Generation", "abstract": "Open-domain Question Answering (QA) has garnered substantial interest by\ncombining the advantages of faithfully retrieved passages and relevant passages\ngenerated through Large Language Models (LLMs). However, there is a lack of\ndefinitive labels available to pair these sources of knowledge. In order to\naddress this issue, we propose an unsupervised and simple framework called\nBi-Reranking for Merging Generated and Retrieved Knowledge (BRMGR), which\nutilizes re-ranking methods for both retrieved passages and LLM-generated\npassages. We pair the two types of passages using two separate re-ranking\nmethods and then combine them through greedy matching. We demonstrate that\nBRMGR is equivalent to employing a bipartite matching loss when assigning each\nretrieved passage with a corresponding LLM-generated passage. The application\nof our model yielded experimental results from three datasets, improving their\nperformance by +1.7 and +1.6 on NQ and WebQ datasets, respectively, and\nobtaining comparable result on TriviaQA dataset when compared to competitive\nbaselines.", "published": "2024-12-25 06:40:36", "link": "http://arxiv.org/abs/2412.18800v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DCIS: Efficient Length Extrapolation of LLMs via Divide-and-Conquer\n  Scaling Factor Search", "abstract": "Large language models (LLMs) based on the Transformer architecture usually\nhave their context length limited due to the high training cost. Recent\nadvancements extend the context window by adjusting the scaling factors of RoPE\nand fine-tuning. However, suboptimal initialization of these factors results in\nincreased fine-tuning costs and reduced performance at target length. To\naddress these challenges, we propose an innovative RoPE-based fine-tuning\nframework that diverges from conventional scaling factors search. Specifically,\nwe present a Divide-and-Conquer Incremental Search (DCIS) algorithm that\nstrategically determines the better scaling factors. Further fine-tuning with\nthe identified scaling factors effectively extends the context window of LLMs.\nEmpirical results demonstrate that our methodology not only mitigates\nperformance decay at extended target lengths but also allows the model to\nfine-tune on short contexts and generalize to long contexts, thereby reducing\nthe cost of fine-tuning. The scaling factors obtained through DCIS can even\nperform effectively without fine-tuning. Further analysis of the search space\nreveals that DCIS achieves twice the search efficiency compared to other\nmethods. We also examine the impact of the non-strictly increasing scaling\nfactors utilized in DCIS and evaluate the general capabilities of LLMs across\nvarious context lengths.", "published": "2024-12-25 07:42:22", "link": "http://arxiv.org/abs/2412.18811v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RapGuard: Safeguarding Multimodal Large Language Models via\n  Rationale-aware Defensive Prompting", "abstract": "While Multimodal Large Language Models (MLLMs) have made remarkable progress\nin vision-language reasoning, they are also more susceptible to producing\nharmful content compared to models that focus solely on text. Existing\ndefensive prompting techniques rely on a static, unified safety guideline that\nfails to account for the specific risks inherent in different multimodal\ncontexts. To address these limitations, we propose RapGuard, a novel framework\nthat uses multimodal chain-of-thought reasoning to dynamically generate\nscenario-specific safety prompts. RapGuard enhances safety by adapting its\nprompts to the unique risks of each input, effectively mitigating harmful\noutputs while maintaining high performance on benign tasks. Our experimental\nresults across multiple MLLM benchmarks demonstrate that RapGuard achieves\nstate-of-the-art safety performance, significantly reducing harmful content\nwithout degrading the quality of responses.", "published": "2024-12-25 08:31:53", "link": "http://arxiv.org/abs/2412.18826v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Overview of MWE history, challenges, and horizons: standing at the 20th\n  anniversary of the MWE workshop series via MWE-UD2024", "abstract": "Starting in 2003 when the first MWE workshop was held with ACL in Sapporo,\nJapan, this year, the joint workshop of MWE-UD co-located with the LREC-COLING\n2024 conference marked the 20th anniversary of MWE workshop events over the\npast nearly two decades. Standing at this milestone, we look back to this\nworkshop series and summarise the research topics and methodologies researchers\nhave carried out over the years. We also discuss the current challenges that we\nare facing and the broader impacts/synergies of MWE research within the CL and\nNLP fields. Finally, we give future research perspectives. We hope this\nposition paper can help researchers, students, and industrial practitioners\ninterested in MWE get a brief but easy understanding of its history, current,\nand possible future.", "published": "2024-12-25 11:00:27", "link": "http://arxiv.org/abs/2412.18868v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Research Experiment on Multi-Model Comparison for Chinese Text\n  Classification Tasks", "abstract": "With the explosive growth of Chinese text data and advancements in natural\nlanguage processing technologies, Chinese text classification has become one of\nthe key techniques in fields such as information retrieval and sentiment\nanalysis, attracting increasing attention. This paper conducts a comparative\nstudy on three deep learning models:TextCNN, TextRNN, and FastText.specifically\nfor Chinese text classification tasks. By conducting experiments on the\nTHUCNews dataset, the performance of these models is evaluated, and their\napplicability in different scenarios is discussed.", "published": "2024-12-25 13:54:40", "link": "http://arxiv.org/abs/2412.18908v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dovetail: A CPU/GPU Heterogeneous Speculative Decoding for LLM inference", "abstract": "Due to the high resource demands of Large Language Models (LLMs), achieving\nwidespread deployment on consumer-grade devices presents significant\nchallenges. Typically, personal or consumer-grade devices, including servers\nconfigured prior to the era of large-scale models, generally have relatively\nweak GPUs and relatively strong CPUs. However, most current methods primarily\ndepend on GPUs for computation. Therefore, we propose Dovetail, an approach\nthat deploys the draft model on the GPU to generate draft tokens while allowing\nthe target model to perform parallel verification on the CPU, thereby improving\nthe utilization of all available hardware resources and occupying less\ninter-device communication bandwidth. Accordingly, we have redesigned the draft\nmodel to better align with heterogeneous hardware characteristics. To this end,\nwe implemented several optimizations: reducing the number of draft tokens to\nmitigate latency in parallel verification, increasing the depth of the draft\nmodel to enhance its predictive capacity, and introducing DGF (Dynamic Gating\nFusion) to improve the integration of features and token embeddings. In the\nHumanEval benchmark, Dovetail achieved an inference speed of 5.86 tokens per\nsecond for LLaMA2-Chat-7B using 3GB of VRAM, representing an approximately\n2.77x improvement over CPU-only inference. Furthermore, the inference speed was\nincreased to 8 tokens per second when utilizing 7GB of VRAM.", "published": "2024-12-25 15:45:18", "link": "http://arxiv.org/abs/2412.18934v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Optimizing Large Language Models with an Enhanced LoRA Fine-Tuning\n  Algorithm for Efficiency and Robustness in NLP Tasks", "abstract": "This study proposes a large language model optimization method based on the\nimproved LoRA fine-tuning algorithm, aiming to improve the accuracy and\ncomputational efficiency of the model in natural language processing tasks. We\nfine-tune the large language model through a low-rank adaptation strategy,\nwhich significantly reduces the consumption of computing resources while\nmaintaining the powerful capabilities of the pre-trained model. The experiment\nuses the QQP task as the evaluation scenario. The results show that the\nimproved LoRA algorithm shows significant improvements in accuracy, F1 score,\nand MCC compared with traditional models such as BERT, Roberta, T5, and GPT-4.\nIn particular, in terms of F1 score and MCC, our model shows stronger\nrobustness and discrimination ability, which proves the potential of the\nimproved LoRA algorithm in fine-tuning large-scale pre-trained models. In\naddition, this paper also discusses the application prospects of the improved\nLoRA algorithm in other natural language processing tasks, emphasizing its\nadvantages in multi-task learning and scenarios with limited computing\nresources. Future research can further optimize the LoRA fine-tuning strategy\nand expand its application in larger-scale pre-trained models to improve the\ngeneralization ability and task adaptability of the model.", "published": "2024-12-25 01:10:25", "link": "http://arxiv.org/abs/2412.18729v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Bootstrap Your Own Context Length", "abstract": "We introduce a bootstrapping approach to train long-context language models\nby exploiting their short-context capabilities only. Our method utilizes a\nsimple agent workflow to synthesize diverse long-context instruction tuning\ndata, thereby eliminating the necessity for manual data collection and\nannotation. The proposed data synthesis workflow requires only a short-context\nlanguage model, a text retriever, and a document collection, all of which are\nreadily accessible within the open-source ecosystem. Subsequently, language\nmodels are fine-tuned using the synthesized data to extend their context\nlengths. In this manner, we effectively transfer the short-context capabilities\nof language models to long-context scenarios through a bootstrapping process.\nWe conduct experiments with the open-source Llama-3 family of models and\ndemonstrate that our method can successfully extend the context length to up to\n1M tokens, achieving superior performance across various benchmarks.", "published": "2024-12-25 10:08:54", "link": "http://arxiv.org/abs/2412.18860v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Whose Morality Do They Speak? Unraveling Cultural Bias in Multilingual\n  Language Models", "abstract": "Large language models (LLMs) have become integral tools in diverse domains,\nyet their moral reasoning capabilities across cultural and linguistic contexts\nremain underexplored. This study investigates whether multilingual LLMs, such\nas GPT-3.5-Turbo, GPT-4o-mini, Llama 3.1, and MistralNeMo, reflect culturally\nspecific moral values or impose dominant moral norms, particularly those rooted\nin English. Using the updated Moral Foundations Questionnaire (MFQ-2) in eight\nlanguages, Arabic, Farsi, English, Spanish, Japanese, Chinese, French, and\nRussian, the study analyzes the models' adherence to six core moral\nfoundations: care, equality, proportionality, loyalty, authority, and purity.\nThe results reveal significant cultural and linguistic variability, challenging\nthe assumption of universal moral consistency in LLMs. Although some models\ndemonstrate adaptability to diverse contexts, others exhibit biases influenced\nby the composition of the training data. These findings underscore the need for\nculturally inclusive model development to improve fairness and trust in\nmultilingual AI systems.", "published": "2024-12-25 10:17:15", "link": "http://arxiv.org/abs/2412.18863v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AdaEAGLE: Optimizing Speculative Decoding via Explicit Modeling of\n  Adaptive Draft Structures", "abstract": "Speculative Decoding (SD) is a popular lossless technique for accelerating\nthe inference of Large Language Models (LLMs). We show that the decoding speed\nof SD frameworks with static draft structures can be significantly improved by\nincorporating context-aware adaptive draft structures. However, current studies\non adaptive draft structures are limited by their performance, modeling\napproaches, and applicability. In this paper, we introduce AdaEAGLE, the first\nSD framework that explicitly models adaptive draft structures. AdaEAGLE\nleverages the Lightweight Draft Length Predictor (LDLP) module to explicitly\npredict the optimal number of draft tokens during inference to guide the draft\nmodel. It achieves comparable speedup results without manual thresholds and\nallows for deeper, more specialized optimizations. Moreover, together with\nthreshold-based strategies, AdaEAGLE achieves a $1.62\\times$ speedup over the\nvanilla AR decoding and outperforms fixed-length SotA baseline while\nmaintaining output quality.", "published": "2024-12-25 13:57:33", "link": "http://arxiv.org/abs/2412.18910v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "MedHallBench: A New Benchmark for Assessing Hallucination in Medical\n  Large Language Models", "abstract": "Medical Large Language Models (MLLMs) have demonstrated potential in\nhealthcare applications, yet their propensity for hallucinations -- generating\nmedically implausible or inaccurate information -- presents substantial risks\nto patient care. This paper introduces MedHallBench, a comprehensive benchmark\nframework for evaluating and mitigating hallucinations in MLLMs. Our\nmethodology integrates expert-validated medical case scenarios with established\nmedical databases to create a robust evaluation dataset. The framework employs\na sophisticated measurement system that combines automated ACHMI (Automatic\nCaption Hallucination Measurement in Medical Imaging) scoring with rigorous\nclinical expert evaluations and utilizes reinforcement learning methods to\nachieve automatic annotation. Through an optimized reinforcement learning from\nhuman feedback (RLHF) training pipeline specifically designed for medical\napplications, MedHallBench enables thorough evaluation of MLLMs across diverse\nclinical contexts while maintaining stringent accuracy standards. We conducted\ncomparative experiments involving various models, utilizing the benchmark to\nestablish a baseline for widely adopted large language models (LLMs). Our\nfindings indicate that ACHMI provides a more nuanced understanding of the\neffects of hallucinations compared to traditional metrics, thereby highlighting\nits advantages in hallucination assessment. This research establishes a\nfoundational framework for enhancing MLLMs' reliability in healthcare settings\nand presents actionable strategies for addressing the critical challenge of AI\nhallucinations in medical applications.", "published": "2024-12-25 16:51:29", "link": "http://arxiv.org/abs/2412.18947v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Intra- and Inter-modal Context Interaction Modeling for Conversational\n  Speech Synthesis", "abstract": "Conversational Speech Synthesis (CSS) aims to effectively take the multimodal\ndialogue history (MDH) to generate speech with appropriate conversational\nprosody for target utterance. The key challenge of CSS is to model the\ninteraction between the MDH and the target utterance. Note that text and speech\nmodalities in MDH have their own unique influences, and they complement each\nother to produce a comprehensive impact on the target utterance. Previous works\ndid not explicitly model such intra-modal and inter-modal interactions. To\naddress this issue, we propose a new intra-modal and inter-modal context\ninteraction scheme-based CSS system, termed III-CSS. Specifically, in the\ntraining phase, we combine the MDH with the text and speech modalities in the\ntarget utterance to obtain four modal combinations, including Historical\nText-Next Text, Historical Speech-Next Speech, Historical Text-Next Speech, and\nHistorical Speech-Next Text. Then, we design two contrastive learning-based\nintra-modal and two inter-modal interaction modules to deeply learn the\nintra-modal and inter-modal context interaction. In the inference phase, we\ntake MDH and adopt trained interaction modules to fully infer the speech\nprosody of the target utterance's text content. Subjective and objective\nexperiments on the DailyTalk dataset show that III-CSS outperforms the advanced\nbaselines in terms of prosody expressiveness. Code and speech samples are\navailable at https://github.com/AI-S2-Lab/I3CSS.", "published": "2024-12-25 01:35:59", "link": "http://arxiv.org/abs/2412.18733v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Towards Expressive Video Dubbing with Multiscale Multimodal Context\n  Interaction", "abstract": "Automatic Video Dubbing (AVD) generates speech aligned with lip motion and\nfacial emotion from scripts. Recent research focuses on modeling multimodal\ncontext to enhance prosody expressiveness but overlooks two key issues: 1)\nMultiscale prosody expression attributes in the context influence the current\nsentence's prosody. 2) Prosody cues in context interact with the current\nsentence, impacting the final prosody expressiveness. To tackle these\nchallenges, we propose M2CI-Dubber, a Multiscale Multimodal Context Interaction\nscheme for AVD. This scheme includes two shared M2CI encoders to model the\nmultiscale multimodal context and facilitate its deep interaction with the\ncurrent sentence. By extracting global and local features for each modality in\nthe context, utilizing attention-based mechanisms for aggregation and\ninteraction, and employing an interaction-based graph attention network for\nfusion, the proposed approach enhances the prosody expressiveness of\nsynthesized speech for the current sentence. Experiments on the Chem dataset\nshow our model outperforms baselines in dubbing expressiveness. The code and\ndemos are available at\n\\textcolor[rgb]{0.93,0.0,0.47}{https://github.com/AI-S2-Lab/M2CI-Dubber}.", "published": "2024-12-25 02:41:13", "link": "http://arxiv.org/abs/2412.18748v2", "categories": ["cs.MM", "cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs", "abstract": "The breakthrough of OpenAI o1 highlights the potential of enhancing reasoning\nto improve LLM. Yet, most research in reasoning has focused on mathematical\ntasks, leaving domains like medicine underexplored. The medical domain, though\ndistinct from mathematics, also demands robust reasoning to provide reliable\nanswers, given the high standards of healthcare. However, verifying medical\nreasoning is challenging, unlike those in mathematics. To address this, we\npropose verifiable medical problems with a medical verifier to check the\ncorrectness of model outputs. This verifiable nature enables advancements in\nmedical reasoning through a two-stage approach: (1) using the verifier to guide\nthe search for a complex reasoning trajectory for fine-tuning LLMs, (2)\napplying reinforcement learning (RL) with verifier-based rewards to enhance\ncomplex reasoning further. Finally, we introduce HuatuoGPT-o1, a medical LLM\ncapable of complex reasoning, which outperforms general and medical-specific\nbaselines using only 40K verifiable problems. Experiments show complex\nreasoning improves medical problem-solving and benefits more from RL. We hope\nour approach inspires advancements in reasoning across medical and other\nspecialized domains.", "published": "2024-12-25 15:12:34", "link": "http://arxiv.org/abs/2412.18925v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Speech Recognition With LLMs Adapted to Disordered Speech Using\n  Reinforcement Learning", "abstract": "We introduce a large language model (LLM) capable of processing speech inputs\nand show that tuning it further with reinforcement learning on human preference\n(RLHF) enables it to adapt better to disordered speech than traditional\nfine-tuning. Our method replaces low-frequency text tokens in an LLM's\nvocabulary with audio tokens and enables the model to recognize speech by\nfine-tuning it on speech with transcripts. We then use RL with rewards based on\nsyntactic and semantic accuracy measures generalizing the LLM further to\nrecognize disordered speech. While the resulting LLM does not outperform\nexisting systems for speech recognition, we find that tuning with reinforcement\nlearning using custom rewards leads to substantially better performance than\nsupervised fine-tuning of the language model, specifically when adapting to\nspeech in a different setting. This presents a compelling alternative tuning\nstrategy for speech recognition using large language models.", "published": "2024-12-25 00:16:22", "link": "http://arxiv.org/abs/2501.00039v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Simi-SFX: A similarity-based conditioning method for controllable sound\n  effect synthesis", "abstract": "Generating sound effects with controllable variations is a challenging task,\ntraditionally addressed using sophisticated physical models that require\nin-depth knowledge of signal processing parameters and algorithms. In the era\nof generative and large language models, text has emerged as a common,\nhuman-interpretable interface for controlling sound synthesis. However, the\ndiscrete and qualitative nature of language tokens makes it difficult to\ncapture subtle timbral variations across different sounds. In this research, we\npropose a novel similarity-based conditioning method for sound synthesis,\nleveraging differentiable digital signal processing (DDSP). This approach\ncombines the use of latent space for learning and controlling audio timbre with\nan intuitive guiding vector, normalized within the range [0,1], to encode\ncategorical acoustic information. By utilizing pre-trained audio representation\nmodels, our method achieves expressive and fine-grained timbre control. To\nbenchmark our approach, we introduce two sound effect datasets--Footstep-set\nand Impact-set--designed to evaluate both controllability and sound quality.\nRegression analysis demonstrates that the proposed similarity score effectively\ncontrols timbre variations and enables creative applications such as timbre\ninterpolation between discrete classes. Our work provides a robust and\nversatile framework for sound effect synthesis, bridging the gap between\ntraditional signal processing and modern machine learning techniques.", "published": "2024-12-25 00:14:50", "link": "http://arxiv.org/abs/2412.18710v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Structured Speaker-Deficiency Adaptation of Foundation Models for\n  Dysarthric and Elderly Speech Recognition", "abstract": "Data-intensive fine-tuning of speech foundation models (SFMs) to scarce and\ndiverse dysarthric and elderly speech leads to data bias and poor\ngeneralization to unseen speakers. This paper proposes novel structured\nspeaker-deficiency adaptation approaches for SSL pre-trained SFMs on such data.\nSpeaker and speech deficiency invariant SFMs were constructed in their\nsupervised adaptive fine-tuning stage to reduce undue bias to training data\nspeakers, and serves as a more neutral and robust starting point for test time\nunsupervised adaptation. Speech variability attributed to speaker identity and\nspeech impairment severity, or aging induced neurocognitive decline, are\nmodelled using separate adapters that can be combined together to model any\nseen or unseen speaker. Experiments on the UASpeech dysarthric and DementiaBank\nPitt elderly speech corpora suggest structured speaker-deficiency adaptation of\nHuBERT and Wav2vec2-conformer models consistently outperforms baseline SFMs\nusing either: a) no adapters; b) global adapters shared among all speakers; or\nc) single attribute adapters modelling speaker or deficiency labels alone by\nstatistically significant WER reductions up to 3.01% and 1.50% absolute (10.86%\nand 6.94% relative) on the two tasks respectively. The lowest published WER of\n19.45% (49.34% on very low intelligibility, 33.17% on unseen words) is obtained\non the UASpeech test set of 16 dysarthric speakers.", "published": "2024-12-25 08:39:02", "link": "http://arxiv.org/abs/2412.18832v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Robust Target Speaker Direction of Arrival Estimation", "abstract": "In multi-speaker environments the direction of arrival (DOA) of a target\nspeaker is key for improving speech clarity and extracting target speaker's\nvoice. However, traditional DOA estimation methods often struggle in the\npresence of noise, reverberation, and particularly when competing speakers are\npresent. To address these challenges, we propose RTS-DOA, a robust real-time\nDOA estimation system. This system innovatively uses the registered speech of\nthe target speaker as a reference and leverages full-band and sub-band spectral\ninformation from a microphone array to estimate the DOA of the target speaker's\nvoice. Specifically, the system comprises a speech enhancement module for\ninitially improving speech quality, a spatial module for learning spatial\ninformation, and a speaker module for extracting voiceprint features.\nExperimental results on the LibriSpeech dataset demonstrate that our RTS-DOA\nsystem effectively tackles multi-speaker scenarios and established new optimal\nbenchmarks.", "published": "2024-12-25 14:04:21", "link": "http://arxiv.org/abs/2412.18913v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Leave-One-EquiVariant: Alleviating invariance-related information loss\n  in contrastive music representations", "abstract": "Contrastive learning has proven effective in self-supervised musical\nrepresentation learning, particularly for Music Information Retrieval (MIR)\ntasks. However, reliance on augmentation chains for contrastive view generation\nand the resulting learnt invariances pose challenges when different downstream\ntasks require sensitivity to certain musical attributes. To address this, we\npropose the Leave One EquiVariant (LOEV) framework, which introduces a\nflexible, task-adaptive approach compared to previous work by selectively\npreserving information about specific augmentations, allowing the model to\nmaintain task-relevant equivariances. We demonstrate that LOEV alleviates\ninformation loss related to learned invariances, improving performance on\naugmentation related tasks and retrieval without sacrificing general\nrepresentation quality. Furthermore, we introduce a variant of LOEV, LOEV++,\nwhich builds a disentangled latent space by design in a self-supervised manner,\nand enables targeted retrieval based on augmentation related attributes.", "published": "2024-12-25 18:06:44", "link": "http://arxiv.org/abs/2412.18955v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Zema Dataset: A Comprehensive Study of Yaredawi Zema with a Focus on\n  Horologium Chants", "abstract": "Computational music research plays a critical role in advancing music\nproduction, distribution, and understanding across various musical styles\nworldwide. Despite the immense cultural and religious significance, the\nEthiopian Orthodox Tewahedo Church (EOTC) chants are relatively\nunderrepresented in computational music research. This paper contributes to\nthis field by introducing a new dataset specifically tailored for analyzing\nEOTC chants, also known as Yaredawi Zema. This work provides a comprehensive\noverview of a 10-hour dataset, 369 instances, creation, and curation process,\nincluding rigorous quality assurance measures. Our dataset has a detailed\nword-level temporal boundary and reading tone annotation along with the\ncorresponding chanting mode label of audios. Moreover, we have also identified\nthe chanting options associated with multiple chanting notations in the\nmanuscript by annotating them accordingly. Our goal in making this dataset\navailable to the public 1 is to encourage more research and study of EOTC\nchants, including lyrics transcription, lyric-to-audio alignment, and music\ngeneration tasks. Such research work will advance knowledge and efforts to\npreserve this distinctive liturgical music, a priceless cultural artifact for\nthe Ethiopian people.", "published": "2024-12-25 05:20:08", "link": "http://arxiv.org/abs/2412.18784v1", "categories": ["eess.AS", "cs.IR", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Computational Analysis of Yaredawi YeZema Silt in Ethiopian Orthodox\n  Tewahedo Church Chants", "abstract": "Despite its musicological, cultural, and religious significance, the\nEthiopian Orthodox Tewahedo Church (EOTC) chant is relatively underrepresented\nin music research. Historical records, including manuscripts, research papers,\nand oral traditions, confirm Saint Yared's establishment of three canonical\nEOTC chanting modes during the 6th century. This paper attempts to investigate\nthe EOTC chants using music information retrieval (MIR) techniques. Among the\nresearch questions regarding the analysis and understanding of EOTC chants,\nYaredawi YeZema Silt, namely the mode of chanting adhering to Saint Yared's\nstandards, is of primary importance. Therefore, we consider the task of\nYaredawi YeZema Silt classification in EOTC chants by introducing a new dataset\nand showcasing a series of classification experiments for this task. Results\nshow that using the distribution of stabilized pitch contours as the feature\nrepresentation on a simple neural network-based classifier becomes an effective\nsolution. The musicological implications and insights of such results are\nfurther discussed through a comparative study with the previous ethnomusicology\nliterature on EOTC chants. By making this dataset publicly accessible, we aim\nto promote future exploration and analysis of EOTC chants and highlight\npotential directions for further research, thereby fostering a deeper\nunderstanding and preservation of this unique spiritual and cultural heritage.", "published": "2024-12-25 05:42:56", "link": "http://arxiv.org/abs/2412.18788v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "MRI2Speech: Speech Synthesis from Articulatory Movements Recorded by\n  Real-time MRI", "abstract": "Previous real-time MRI (rtMRI)-based speech synthesis models depend heavily\non noisy ground-truth speech. Applying loss directly over ground truth\nmel-spectrograms entangles speech content with MRI noise, resulting in poor\nintelligibility. We introduce a novel approach that adapts the multi-modal\nself-supervised AV-HuBERT model for text prediction from rtMRI and incorporates\na new flow-based duration predictor for speaker-specific alignment. The\npredicted text and durations are then used by a speech decoder to synthesize\naligned speech in any novel voice. We conduct thorough experiments on two\ndatasets and demonstrate our method's generalization ability to unseen\nspeakers. We assess our framework's performance by masking parts of the rtMRI\nvideo to evaluate the impact of different articulators on text prediction. Our\nmethod achieves a $15.18\\%$ Word Error Rate (WER) on the USC-TIMIT MRI corpus,\nmarking a huge improvement over the current state-of-the-art. Speech samples\nare available at https://mri2speech.github.io/MRI2Speech/", "published": "2024-12-25 08:49:43", "link": "http://arxiv.org/abs/2412.18836v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Advancing NAM-to-Speech Conversion with Novel Methods and the MultiNAM\n  Dataset", "abstract": "Current Non-Audible Murmur (NAM)-to-speech techniques rely on voice cloning\nto simulate ground-truth speech from paired whispers. However, the simulated\nspeech often lacks intelligibility and fails to generalize well across\ndifferent speakers. To address this issue, we focus on learning phoneme-level\nalignments from paired whispers and text and employ a Text-to-Speech (TTS)\nsystem to simulate the ground-truth. To reduce dependence on whispers, we learn\nphoneme alignments directly from NAMs, though the quality is constrained by the\navailable training data. To further mitigate reliance on NAM/whisper data for\nground-truth simulation, we propose incorporating the lip modality to infer\nspeech and introduce a novel diffusion-based method that leverages recent\nadvancements in lip-to-speech technology. Additionally, we release the MultiNAM\ndataset with over 7.96 hours of paired NAM, whisper, video, and text data from\ntwo speakers and benchmark all methods on this dataset. Speech samples and the\ndataset are available at https://diff-nam.github.io/DiffNAM/", "published": "2024-12-25 08:57:24", "link": "http://arxiv.org/abs/2412.18839v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Preventing output saturation in active noise control: An\n  output-constrained Kalman filter approach", "abstract": "The Kalman filter (KF)-based active noise control (ANC) system demonstrates\nsuperior tracking and faster convergence compared to the least mean square\n(LMS) method, particularly in dynamic noise cancellation scenarios. However, in\nenvironments with extremely high noise levels, the power of the control signal\ncan exceed the system's rated output power due to hardware limitations, leading\nto output saturation and subsequent non-linearity. To mitigate this issue, a\nmodified KF with an output constraint is proposed. In this approach, the\ndisturbance treated as an measurement is re-scaled by a constraint factor,\nwhich is determined by the system's rated power, the secondary path gain, and\nthe disturbance power. As a result, the output power of the system, i.e. the\ncontrol signal, is indirectly constrained within the maximum output of the\nsystem, ensuring stability. Simulation results indicate that the proposed\nalgorithm not only achieves rapid suppression of dynamic noise but also\neffectively prevents non-linearity due to output saturation, highlighting its\npractical significance.", "published": "2024-12-25 12:12:48", "link": "http://arxiv.org/abs/2412.18887v1", "categories": ["eess.SY", "cs.SY", "eess.AS", "eess.SP"], "primary_category": "eess.SY"}
