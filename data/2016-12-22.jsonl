{"title": "Continuous multilinguality with language vectors", "abstract": "Most existing models for multilingual natural language processing (NLP) treat\nlanguage as a discrete category, and make predictions for either one language\nor the other. In contrast, we propose using continuous vector representations\nof language. We show that these can be learned efficiently with a\ncharacter-based neural language model, and used to improve inference about\nlanguage varieties not seen during training. In experiments with 1303 Bible\ntranslations into 990 different languages, we empirically explore the capacity\nof multilingual language models, and also show that the language vectors\ncapture genetic relationships between languages.", "published": "2016-12-22 08:29:25", "link": "http://arxiv.org/abs/1612.07486v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Noise Mitigation for Neural Entity Typing and Relation Extraction", "abstract": "In this paper, we address two different types of noise in information\nextraction models: noise from distant supervision and noise from pipeline input\nfeatures. Our target tasks are entity typing and relation extraction. For the\nfirst noise type, we introduce multi-instance multi-label learning algorithms\nusing neural network models, and apply them to fine-grained entity typing for\nthe first time. This gives our models comparable performance with the\nstate-of-the-art supervised approach which uses global embeddings of entities.\nFor the second noise type, we propose ways to improve the integration of noisy\nentity type predictions into relation extraction. Our experiments show that\nprobabilistic predictions are more robust than discrete predictions and that\njoint training of the two tasks performs best.", "published": "2016-12-22 09:05:35", "link": "http://arxiv.org/abs/1612.07495v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Context-aware Attention Network for Interactive Question Answering", "abstract": "Neural network based sequence-to-sequence models in an encoder-decoder\nframework have been successfully applied to solve Question Answering (QA)\nproblems, predicting answers from statements and questions. However, almost all\nprevious models have failed to consider detailed context information and\nunknown states under which systems do not have enough information to answer\ngiven questions. These scenarios with incomplete or ambiguous information are\nvery common in the setting of Interactive Question Answering (IQA). To address\nthis challenge, we develop a novel model, employing context-dependent\nword-level attention for more accurate statement representations and\nquestion-guided sentence-level attention for better context modeling. We also\ngenerate unique IQA datasets to test our model, which will be made publicly\navailable. Employing these attention mechanisms, our model accurately\nunderstands when it can output an answer or when it requires generating a\nsupplementary question for additional input depending on different contexts.\nWhen available, user's feedback is encoded and directly applied to update\nsentence-level attention to infer an answer. Extensive experiments on QA and\nIQA datasets quantitatively demonstrate the effectiveness of our model with\nsignificant improvement over state-of-the-art conventional QA models.", "published": "2016-12-22 01:25:20", "link": "http://arxiv.org/abs/1612.07411v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Re-evaluating Automatic Metrics for Image Captioning", "abstract": "The task of generating natural language descriptions from images has received\na lot of attention in recent years. Consequently, it is becoming increasingly\nimportant to evaluate such image captioning approaches in an automatic manner.\nIn this paper, we provide an in-depth evaluation of the existing image\ncaptioning metrics through a series of carefully designed experiments.\nMoreover, we explore the utilization of the recently proposed Word Mover's\nDistance (WMD) document metric for the purpose of image captioning. Our\nfindings outline the differences and/or similarities between metrics and their\nrelative robustness by means of extensive correlation, accuracy and distraction\nbased evaluations. Our results also demonstrate that WMD provides strong\nadvantages over other metrics.", "published": "2016-12-22 14:00:28", "link": "http://arxiv.org/abs/1612.07600v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Jointly Extracting Relations with Class Ties via Effective Deep Ranking", "abstract": "Connections between relations in relation extraction, which we call class\nties, are common. In distantly supervised scenario, one entity tuple may have\nmultiple relation facts. Exploiting class ties between relations of one entity\ntuple will be promising for distantly supervised relation extraction. However,\nprevious models are not effective or ignore to model this property. In this\nwork, to effectively leverage class ties, we propose to make joint relation\nextraction with a unified model that integrates convolutional neural network\n(CNN) with a general pairwise ranking framework, in which three novel ranking\nloss functions are introduced. Additionally, an effective method is presented\nto relieve the severe class imbalance problem from NR (not relation) for model\ntraining. Experiments on a widely used dataset show that leveraging class ties\nwill enhance extraction and demonstrate the effectiveness of our model to learn\nclass ties. Our model outperforms the baselines significantly, achieving\nstate-of-the-art performance.", "published": "2016-12-22 14:08:08", "link": "http://arxiv.org/abs/1612.07602v4", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Understanding Image and Text Simultaneously: a Dual Vision-Language\n  Machine Comprehension Task", "abstract": "We introduce a new multi-modal task for computer systems, posed as a combined\nvision-language comprehension challenge: identifying the most suitable text\ndescribing a scene, given several similar options. Accomplishing the task\nentails demonstrating comprehension beyond just recognizing \"keywords\" (or\nkey-phrases) and their corresponding visual concepts. Instead, it requires an\nalignment between the representations of the two modalities that achieves a\nvisually-grounded \"understanding\" of various linguistic elements and their\ndependencies. This new task also admits an easy-to-compute and well-studied\nmetric: the accuracy in detecting the true target among the decoys.\n  The paper makes several contributions: an effective and extensible mechanism\nfor generating decoys from (human-created) image captions; an instance of\napplying this mechanism, yielding a large-scale machine comprehension dataset\n(based on the COCO images and captions) that we make publicly available; human\nevaluation results on this dataset, informing a performance upper-bound; and\nseveral baseline and competitive learning approaches that illustrate the\nutility of the proposed task and dataset in advancing both image and language\ncomprehension. We also show that, in a multi-task learning setting, the\nperformance on the proposed task is positively correlated with the end-to-end\ntask of image captioning.", "published": "2016-12-22 22:44:17", "link": "http://arxiv.org/abs/1612.07833v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
