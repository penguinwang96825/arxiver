{"title": "Impossible Triangle: What's Next for Pre-trained Language Models?", "abstract": "Recent development of large-scale pre-trained language models (PLM) have\nsignificantly improved the capability of models in various NLP tasks, in terms\nof performance after task-specific fine-tuning and zero-shot / few-shot\nlearning. However, many of such models come with a dauntingly huge size that\nfew institutions can afford to pre-train, fine-tune or even deploy, while\nmoderate-sized models usually lack strong generalized few-shot learning\ncapabilities. In this paper, we first elaborate the current obstacles of using\nPLM models in terms of the Impossible Triangle: 1) moderate model size, 2)\nstate-of-the-art few-shot learning capability, and 3) state-of-the-art\nfine-tuning capability. We argue that all existing PLM models lack one or more\nproperties from the Impossible Triangle. To remedy these missing properties of\nPLMs, various techniques have been proposed, such as knowledge distillation,\ndata augmentation and prompt learning, which inevitably brings additional work\nto the application of PLMs in real scenarios. We then offer insights into\nfuture research directions of PLMs to achieve the Impossible Triangle, and\nbreak down the task into several key phases.", "published": "2022-04-13 01:28:18", "link": "http://arxiv.org/abs/2204.06130v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HIT at SemEval-2022 Task 2: Pre-trained Language Model for Idioms\n  Detection", "abstract": "The same multi-word expressions may have different meanings in different\nsentences. They can be mainly divided into two categories, which are literal\nmeaning and idiomatic meaning. Non-contextual-based methods perform poorly on\nthis problem, and we need contextual embedding to understand the idiomatic\nmeaning of multi-word expressions correctly. We use a pre-trained language\nmodel, which can provide a context-aware sentence embedding, to detect whether\nmulti-word expression in the sentence is idiomatic usage.", "published": "2022-04-13 02:45:04", "link": "http://arxiv.org/abs/2204.06145v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Cluster-Based k-Nearest-Neighbor Machine Translation", "abstract": "k-Nearest-Neighbor Machine Translation (kNN-MT) has been recently proposed as\na non-parametric solution for domain adaptation in neural machine translation\n(NMT). It aims to alleviate the performance degradation of advanced MT systems\nin translating out-of-domain sentences by coordinating with an additional\ntoken-level feature-based retrieval module constructed from in-domain data.\nPrevious studies have already demonstrated that non-parametric NMT is even\nsuperior to models fine-tuned on out-of-domain data. In spite of this success,\nkNN retrieval is at the expense of high latency, in particular for large\ndatastores. To make it practical, in this paper, we explore a more efficient\nkNN-MT and propose to use clustering to improve the retrieval efficiency.\nConcretely, we first propose a cluster-based Compact Network for feature\nreduction in a contrastive learning manner to compress context features into\n90+% lower dimensional vectors. We then suggest a cluster-based pruning\nsolution to filter out 10%-40% redundant nodes in large datastores while\nretaining translation quality. Our proposed methods achieve better or\ncomparable performance while reducing up to 57% inference latency against the\nadvanced non-parametric MT model on several machine translation benchmarks.\nExperimental results indicate that the proposed methods maintain the most\nuseful information of the original datastore and the Compact Network shows good\ngeneralization on unseen domains.", "published": "2022-04-13 05:46:31", "link": "http://arxiv.org/abs/2204.06175v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Probing for Constituency Structure in Neural Language Models", "abstract": "In this paper, we investigate to which extent contextual neural language\nmodels (LMs) implicitly learn syntactic structure. More concretely, we focus on\nconstituent structure as represented in the Penn Treebank (PTB). Using standard\nprobing techniques based on diagnostic classifiers, we assess the accuracy of\nrepresenting constituents of different categories within the neuron activations\nof a LM such as RoBERTa. In order to make sure that our probe focuses on\nsyntactic knowledge and not on implicit semantic generalizations, we also\nexperiment on a PTB version that is obtained by randomly replacing constituents\nwith each other while keeping syntactic structure, i.e., a semantically\nill-formed but syntactically well-formed version of the PTB. We find that 4\npretrained transfomer LMs obtain high performance on our probing tasks even on\nmanipulated data, suggesting that semantic and syntactic knowledge in their\nrepresentations can be separated and that constituency information is in fact\nlearned by the LM. Moreover, we show that a complete constituency tree can be\nlinearly separated from LM representations.", "published": "2022-04-13 07:07:37", "link": "http://arxiv.org/abs/2204.06201v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Question Rewriting Help Conversational Question Answering?", "abstract": "Question rewriting (QR) is a subtask of conversational question answering\n(CQA) aiming to ease the challenges of understanding dependencies among\ndialogue history by reformulating questions in a self-contained form. Despite\nseeming plausible, little evidence is available to justify QR as a mitigation\nmethod for CQA. To verify the effectiveness of QR in CQA, we investigate a\nreinforcement learning approach that integrates QR and CQA tasks and does not\nrequire corresponding QR datasets for targeted CQA. We find, however, that the\nRL method is on par with the end-to-end baseline. We provide an analysis of the\nfailure and describe the difficulty of exploiting QR for CQA.", "published": "2022-04-13 08:16:03", "link": "http://arxiv.org/abs/2204.06239v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Novel Approach to Train Diverse Types of Language Models for Health\n  Mention Classification of Tweets", "abstract": "Health mention classification deals with the disease detection in a given\ntext containing disease words. However, non-health and figurative use of\ndisease words adds challenges to the task. Recently, adversarial training\nacting as a means of regularization has gained popularity in many NLP tasks. In\nthis paper, we propose a novel approach to train language models for health\nmention classification of tweets that involves adversarial training. We\ngenerate adversarial examples by adding perturbation to the representations of\ntransformer models for tweet examples at various levels using Gaussian noise.\nFurther, we employ contrastive loss as an additional objective function. We\nevaluate the proposed method on the PHM2017 dataset extended version. Results\nshow that our proposed approach improves the performance of classifier\nsignificantly over the baseline methods. Moreover, our analysis shows that\nadding noise at earlier layers improves models' performance whereas adding\nnoise at intermediate layers deteriorates models' performance. Finally, adding\nnoise towards the final layers performs better than the middle layers noise\naddition.", "published": "2022-04-13 12:38:15", "link": "http://arxiv.org/abs/2204.06337v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WikiDiverse: A Multimodal Entity Linking Dataset with Diversified\n  Contextual Topics and Entity Types", "abstract": "Multimodal Entity Linking (MEL) which aims at linking mentions with\nmultimodal contexts to the referent entities from a knowledge base (e.g.,\nWikipedia), is an essential task for many multimodal applications. Although\nmuch attention has been paid to MEL, the shortcomings of existing MEL datasets\nincluding limited contextual topics and entity types, simplified mention\nambiguity, and restricted availability, have caused great obstacles to the\nresearch and application of MEL. In this paper, we present WikiDiverse, a\nhigh-quality human-annotated MEL dataset with diversified contextual topics and\nentity types from Wikinews, which uses Wikipedia as the corresponding knowledge\nbase. A well-tailored annotation procedure is adopted to ensure the quality of\nthe dataset. Based on WikiDiverse, a sequence of well-designed MEL models with\nintra-modality and inter-modality attentions are implemented, which utilize the\nvisual information of images more adequately than existing MEL models do.\nExtensive experimental analyses are conducted to investigate the contributions\nof different modalities in terms of MEL, facilitating the future research on\nthis task. The dataset and baseline models are available at\nhttps://github.com/wangxw5/wikiDiverse.", "published": "2022-04-13 12:52:40", "link": "http://arxiv.org/abs/2204.06347v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Impact of Cross-Lingual Adjustment of Contextual Word\n  Representations on Zero-Shot Transfer", "abstract": "Large multilingual language models such as mBERT or XLM-R enable zero-shot\ncross-lingual transfer in various IR and NLP tasks. Cao et al. (2020) proposed\na data- and compute-efficient method for cross-lingual adjustment of mBERT that\nuses a small parallel corpus to make embeddings of related words across\nlanguages similar to each other. They showed it to be effective in NLI for five\nEuropean languages. In contrast we experiment with a typologically diverse set\nof languages (Spanish, Russian, Vietnamese, and Hindi) and extend their\noriginal implementations to new tasks (XSR, NER, and QA) and an additional\ntraining regime (continual learning). Our study reproduced gains in NLI for\nfour languages, showed improved NER, XSR, and cross-lingual QA results in three\nlanguages (though some cross-lingual QA gains were not statistically\nsignificant), while mono-lingual QA performance never improved and sometimes\ndegraded. Analysis of distances between contextualized embeddings of related\nand unrelated words (across languages) showed that fine-tuning leads to\n\"forgetting\" some of the cross-lingual alignment information. Based on this\nobservation, we further improved NLI performance using continual learning.", "published": "2022-04-13 15:28:43", "link": "http://arxiv.org/abs/2204.06457v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adapting Pre-trained Language Models to African Languages via\n  Multilingual Adaptive Fine-Tuning", "abstract": "Multilingual pre-trained language models (PLMs) have demonstrated impressive\nperformance on several downstream tasks for both high-resourced and\nlow-resourced languages. However, there is still a large performance drop for\nlanguages unseen during pre-training, especially African languages. One of the\nmost effective approaches to adapt to a new language is \\textit{language\nadaptive fine-tuning} (LAFT) -- fine-tuning a multilingual PLM on monolingual\ntexts of a language using the pre-training objective. However, adapting to a\ntarget language individually takes a large disk space and limits the\ncross-lingual transfer abilities of the resulting models because they have been\nspecialized for a single language. In this paper, we perform\n\\textit{multilingual adaptive fine-tuning} on 17 most-resourced African\nlanguages and three other high-resource languages widely spoken on the African\ncontinent to encourage cross-lingual transfer learning. To further specialize\nthe multilingual PLM, we removed vocabulary tokens from the embedding layer\nthat corresponds to non-African writing scripts before MAFT, thus reducing the\nmodel size by around 50%. Our evaluation on two multilingual PLMs (AfriBERTa\nand XLM-R) and three NLP tasks (NER, news topic classification, and sentiment\nclassification) shows that our approach is competitive to applying LAFT on\nindividual languages while requiring significantly less disk space.\nAdditionally, we show that our adapted PLM also improves the zero-shot\ncross-lingual transfer abilities of parameter efficient fine-tuning methods.", "published": "2022-04-13 16:13:49", "link": "http://arxiv.org/abs/2204.06487v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Study of Indian English Pronunciation Variabilities relative to Received\n  Pronunciation", "abstract": "Analysis of Indian English (IE) pronunciation variabilities are useful in\nbuilding systems for Automatic Speech Recognition (ASR) and Text-to-Speech\n(TTS) synthesis in the Indian context. Typically, these pronunciation\nvariabilities have been explored by comparing IE pronunciation with Received\nPronunciation (RP). However, to explore these variabilities, it is required to\nhave labelled pronunciation data at the phonetic level, which is scarce for IE.\nMoreover, versatility of IE stems from the influence of a large diversity of\nthe speakers' mother tongues and demographic region differences. Prior\nlinguistic works have characterised features of IE variabilities qualitatively\nby reporting phonetic rules that represent such variations relative to RP. The\nqualitative descriptions often lack quantitative descriptors and data-driven\nanalysis of diverse IE pronunciation data to characterise IE on the phonetic\nlevel. To address these issues, in this work, we consider a corpus, Indic\nTIMIT, containing a large set of IE varieties from 80 speakers from various\nregions of India. We present an analysis to obtain the new set of phonetic\nrules representing IE pronunciation variabilities relative to RP in a\ndata-driven manner. We do this using 15,974 phonetic transcriptions, of which\n13,632 were obtained manually in addition to those part of the corpus.\nFurthermore, we validate the rules obtained from the analysis against the\nexisting phonetic rules to identify the relevance of the obtained phonetic\nrules and test the efficacy of Grapheme-to-Phoneme (G2P) conversion developed\nbased on the obtained rules considering Phoneme Error Rate (PER) as the metric\nfor performance.", "published": "2022-04-13 16:35:52", "link": "http://arxiv.org/abs/2204.06502v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Event Linking to Wikidata", "abstract": "We present a task of multilingual linking of events to a knowledge base. We\nautomatically compile a large-scale dataset for this task, comprising of 1.8M\nmentions across 44 languages referring to over 10.9K events from Wikidata. We\npropose two variants of the event linking task: 1) multilingual, where event\ndescriptions are from the same language as the mention, and 2) crosslingual,\nwhere all event descriptions are in English. On the two proposed tasks, we\ncompare multiple event linking systems including BM25+ (Lv and Zhai, 2011) and\nmultilingual adaptations of the biencoder and crossencoder architectures from\nBLINK (Wu et al., 2020). In our experiments on the two task variants, we find\nboth biencoder and crossencoder models significantly outperform the BM25+\nbaseline. Our results also indicate that the crosslingual task is in general\nmore challenging than the multilingual task. To test the out-of-domain\ngeneralization of the proposed linking systems, we additionally create a\nWikinews-based evaluation set. We present qualitative analysis highlighting\nvarious aspects captured by the proposed dataset, including the need for\ntemporal reasoning over context and tackling diverse event descriptions across\nlanguages.", "published": "2022-04-13 17:28:23", "link": "http://arxiv.org/abs/2204.06535v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Disentangling Uncertainty in Machine Translation Evaluation", "abstract": "Trainable evaluation metrics for machine translation (MT) exhibit strong\ncorrelation with human judgements, but they are often hard to interpret and\nmight produce unreliable scores under noisy or out-of-domain data. Recent work\nhas attempted to mitigate this with simple uncertainty quantification\ntechniques (Monte Carlo dropout and deep ensembles), however these techniques\n(as we show) are limited in several ways -- for example, they are unable to\ndistinguish between different kinds of uncertainty, and they are time and\nmemory consuming. In this paper, we propose more powerful and efficient\nuncertainty predictors for MT evaluation, and we assess their ability to target\ndifferent sources of aleatoric and epistemic uncertainty. To this end, we\ndevelop and compare training objectives for the COMET metric to enhance it with\nan uncertainty prediction output, including heteroscedastic regression,\ndivergence minimization, and direct uncertainty prediction. Our experiments\nshow improved results on uncertainty prediction for the WMT metrics task\ndatasets, with a substantial reduction in computational costs. Moreover, they\ndemonstrate the ability of these predictors to address specific uncertainty\ncauses in MT evaluation, such as low quality references and out-of-domain data.", "published": "2022-04-13 17:49:25", "link": "http://arxiv.org/abs/2204.06546v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fast Few-shot Debugging for NLU Test Suites", "abstract": "We study few-shot debugging of transformer based natural language\nunderstanding models, using recently popularized test suites to not just\ndiagnose but correct a problem. Given a few debugging examples of a certain\nphenomenon, and a held-out test set of the same phenomenon, we aim to maximize\naccuracy on the phenomenon at a minimal cost of accuracy on the original test\nset. We examine several methods that are faster than full epoch retraining. We\nintroduce a new fast method, which samples a few in-danger examples from the\noriginal training set. Compared to fast methods using parameter distance\nconstraints or Kullback-Leibler divergence, we achieve superior original\naccuracy for comparable debugging accuracy.", "published": "2022-04-13 17:56:23", "link": "http://arxiv.org/abs/2204.06555v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EHRKit: A Python Natural Language Processing Toolkit for Electronic\n  Health Record Texts", "abstract": "The Electronic Health Record (EHR) is an essential part of the modern medical\nsystem and impacts healthcare delivery, operations, and research. Unstructured\ntext is attracting much attention despite structured information in the EHRs\nand has become an exciting research field. The success of the recent neural\nNatural Language Processing (NLP) method has led to a new direction for\nprocessing unstructured clinical notes. In this work, we create a python\nlibrary for clinical texts, EHRKit. This library contains two main parts:\nMIMIC-III-specific functions and tasks specific functions. The first part\nintroduces a list of interfaces for accessing MIMIC-III NOTEEVENTS data,\nincluding basic search, information retrieval, and information extraction. The\nsecond part integrates many third-party libraries for up to 12 off-shelf NLP\ntasks such as named entity recognition, summarization, machine translation,\netc.", "published": "2022-04-13 18:51:01", "link": "http://arxiv.org/abs/2204.06604v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GAP: A Graph-aware Language Model Framework for Knowledge Graph-to-Text\n  Generation", "abstract": "Recent improvements in KG-to-text generation are due to additional auxiliary\npre-training tasks designed to give the fine-tune task a boost in performance.\nThese tasks require extensive computational resources while only suggesting\nmarginal improvements. Here, we demonstrate that by fusing graph-aware elements\ninto existing pre-trained language models, we are able to outperform\nstate-of-the-art models and close the gap imposed by additional pre-training\ntasks. We do so by proposing a mask structure to capture neighborhood\ninformation and a novel type encoder that adds a bias to the graph-attention\nweights depending on the connection type. Experiments on two KG-to-text\nbenchmark datasets show our models are competitive while involving fewer\nparameters and no additional pre-training tasks. By formulating the problem as\na framework, we can interchange the various proposed components and begin\ninterpreting KG-to-text generative models based on the topological and type\ninformation found in a graph.", "published": "2022-04-13 23:53:37", "link": "http://arxiv.org/abs/2204.06674v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Revise References for Faithful Summarization", "abstract": "In real-world scenarios with naturally occurring datasets, reference\nsummaries are noisy and may contain information that cannot be inferred from\nthe source text. On large news corpora, removing low quality samples has been\nshown to reduce model hallucinations. Yet, for smaller, and/or noisier corpora,\nfiltering is detrimental to performance. To improve reference quality while\nretaining all data, we propose a new approach: to selectively re-write\nunsupported reference sentences to better reflect source data. We automatically\ngenerate a synthetic dataset of positive and negative revisions by corrupting\nsupported sentences and learn to revise reference sentences with contrastive\nlearning. The intensity of revisions is treated as a controllable attribute so\nthat, at inference, diverse candidates can be over-generated-then-rescored to\nbalance faithfulness and abstraction. To test our methods, we extract noisy\nreferences from publicly available MIMIC-III discharge summaries for the task\nof hospital-course summarization, and vary the data on which models are\ntrained. According to metrics and human evaluation, models trained on revised\nclinical references are much more faithful, informative, and fluent than models\ntrained on original or filtered data.", "published": "2022-04-13 18:54:19", "link": "http://arxiv.org/abs/2204.10290v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Universality-Individuality Integration Model for Dialog Act\n  Classification", "abstract": "Dialog Act (DA) reveals the general intent of the speaker utterance in a\nconversation. Accurately predicting DAs can greatly facilitate the development\nof dialog agents. Although researchers have done extensive research on dialog\nact classification, the feature information of classification has not been\nfully considered. This paper suggests that word cues, part-of-speech cues and\nstatistical cues can complement each other to improve the basis for\nrecognition. In addition, the different types of the three lead to the\ndiversity of their distribution forms, which hinders the mining of feature\ninformation. To solve this problem, we propose a novel model based on\nuniversality and individuality strategies, called Universality-Individuality\nIntegration Model (UIIM). UIIM not only deepens the connection between the\nclues by learning universality, but also utilizes the learning of individuality\nto capture the characteristics of the clues themselves. Experiments were made\nover two most popular benchmark data sets SwDA and MRDA for dialogue act\nclassification, and the results show that extracting the universalities and\nindividualities between cues can more fully excavate the hidden information in\nthe utterance, and improve the accuracy of automatic dialogue act recognition.", "published": "2022-04-13 06:05:34", "link": "http://arxiv.org/abs/2204.06185v1", "categories": ["cs.CL", "cs.AI", "68T07", "I.2.6; I.2.7"], "primary_category": "cs.CL"}
{"title": "Experimental Standards for Deep Learning in Natural Language Processing\n  Research", "abstract": "The field of Deep Learning (DL) has undergone explosive growth during the\nlast decade, with a substantial impact on Natural Language Processing (NLP) as\nwell. Yet, compared to more established disciplines, a lack of common\nexperimental standards remains an open challenge to the field at large.\nStarting from fundamental scientific principles, we distill ongoing discussions\non experimental standards in NLP into a single, widely-applicable methodology.\nFollowing these best practices is crucial to strengthen experimental evidence,\nimprove reproducibility and support scientific progress. These standards are\nfurther collected in a public repository to help them transparently adapt to\nfuture needs.", "published": "2022-04-13 08:42:52", "link": "http://arxiv.org/abs/2204.06251v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "TangoBERT: Reducing Inference Cost by using Cascaded Architecture", "abstract": "The remarkable success of large transformer-based models such as BERT,\nRoBERTa and XLNet in many NLP tasks comes with a large increase in monetary and\nenvironmental cost due to their high computational load and energy consumption.\nIn order to reduce this computational load in inference time, we present\nTangoBERT, a cascaded model architecture in which instances are first processed\nby an efficient but less accurate first tier model, and only part of those\ninstances are additionally processed by a less efficient but more accurate\nsecond tier model. The decision of whether to apply the second tier model is\nbased on a confidence score produced by the first tier model. Our simple method\nhas several appealing practical advantages compared to standard cascading\napproaches based on multi-layered transformer models. First, it enables higher\nspeedup gains (average lower latency). Second, it takes advantage of batch size\noptimization for cascading, which increases the relative inference cost\nreductions. We report TangoBERT inference CPU speedup on four text\nclassification GLUE tasks and on one reading comprehension task. Experimental\nresults show that TangoBERT outperforms efficient early exit baseline models;\non the the SST-2 task, it achieves an accuracy of 93.9% with a CPU speedup of\n8.2x.", "published": "2022-04-13 09:45:08", "link": "http://arxiv.org/abs/2204.06271v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Curriculum: A Broad-Coverage Benchmark for Linguistic Phenomena in\n  Natural Language Understanding", "abstract": "In the age of large transformer language models, linguistic evaluation play\nan important role in diagnosing models' abilities and limitations on natural\nlanguage understanding. However, current evaluation methods show some\nsignificant shortcomings. In particular, they do not provide insight into how\nwell a language model captures distinct linguistic skills essential for\nlanguage understanding and reasoning. Thus they fail to effectively map out the\naspects of language understanding that remain challenging to existing models,\nwhich makes it hard to discover potential limitations in models and datasets.\nIn this paper, we introduce Curriculum as a new format of NLI benchmark for\nevaluation of broad-coverage linguistic phenomena. Curriculum contains a\ncollection of datasets that covers 36 types of major linguistic phenomena and\nan evaluation procedure for diagnosing how well a language model captures\nreasoning skills for distinct types of linguistic phenomena. We show that this\nlinguistic-phenomena-driven benchmark can serve as an effective tool for\ndiagnosing model behavior and verifying model learning quality. In addition,\nour experiments provide insight into the limitation of existing benchmark\ndatasets and state-of-the-art models that may encourage future research on\nre-designing datasets, model architectures, and learning objectives.", "published": "2022-04-13 10:32:03", "link": "http://arxiv.org/abs/2204.06283v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Building Markovian Generative Architectures over Pretrained LM Backbones\n  for Efficient Task-Oriented Dialog Systems", "abstract": "Recently, Transformer based pretrained language models (PLMs), such as GPT2\nand T5, have been leveraged to build generative task-oriented dialog (TOD)\nsystems. A drawback of existing PLM-based models is their non-Markov\narchitectures across turns, i.e., the whole history is used as the conditioning\ninput at each turn. First, this brings inefficiencies in memory and\ncomputation. Furthermore, using the whole history increases model complexity\nand may hurt the training efficiency, especially when facing small amounts of\nlabeled training data (the low-resource setting). In this paper, motivated by\nthe observation that dialog states could be viewed as Markov states, we propose\nto build Markovian Generative Architectures (MGA) over PLM backbones for\nefficient TOD systems. Experiments on MultiWOZ2.1 show that in the\nrich-resource setting, the proposed Markov models reduce memory and time costs\nwithout performance degradation; in the low-resource setting, the training\nefficiency of the Markov models is more significant.", "published": "2022-04-13 15:21:34", "link": "http://arxiv.org/abs/2204.06452v2", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Scalable Training of Language Models using JAX pjit and TPUv4", "abstract": "Modern large language models require distributed training strategies due to\ntheir size. The challenges of efficiently and robustly training them are met\nwith rapid developments on both software and hardware frontiers. In this\ntechnical report, we explore challenges and design decisions associated with\ndeveloping a scalable training framework, and present a quantitative analysis\nof efficiency improvements coming from adopting new software and hardware\nsolutions.", "published": "2022-04-13 17:08:58", "link": "http://arxiv.org/abs/2204.06514v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A Distant Supervision Corpus for Extracting Biomedical Relationships\n  Between Chemicals, Diseases and Genes", "abstract": "We introduce ChemDisGene, a new dataset for training and evaluating\nmulti-class multi-label document-level biomedical relation extraction models.\nOur dataset contains 80k biomedical research abstracts labeled with mentions of\nchemicals, diseases, and genes, portions of which human experts labeled with 18\ntypes of biomedical relationships between these entities (intended for\nevaluation), and the remainder of which (intended for training) has been\ndistantly labeled via the CTD database with approximately 78\\% accuracy. In\ncomparison to similar preexisting datasets, ours is both substantially larger\nand cleaner; it also includes annotations linking mentions to their entities.\nWe also provide three baseline deep neural network relation extraction models\ntrained and evaluated on our new dataset.", "published": "2022-04-13 18:02:05", "link": "http://arxiv.org/abs/2204.06584v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CAMERO: Consistency Regularized Ensemble of Perturbed Language Models\n  with Weight Sharing", "abstract": "Model ensemble is a popular approach to produce a low-variance and\nwell-generalized model. However, it induces large memory and inference costs,\nwhich are often not affordable for real-world deployment. Existing work has\nresorted to sharing weights among models. However, when increasing the\nproportion of the shared weights, the resulting models tend to be similar, and\nthe benefits of using model ensemble diminish. To retain ensemble benefits\nwhile maintaining a low memory cost, we propose a consistency-regularized\nensemble learning approach based on perturbed models, named CAMERO.\nSpecifically, we share the weights of bottom layers across all models and apply\ndifferent perturbations to the hidden representations for different models,\nwhich can effectively promote the model diversity. Meanwhile, we apply a\nprediction consistency regularizer across the perturbed models to control the\nvariance due to the model diversity. Our experiments using large language\nmodels demonstrate that CAMERO significantly improves the generalization\nperformance of the ensemble model. Specifically, CAMERO outperforms the\nstandard ensemble of 8 BERT-base models on the GLUE benchmark by 0.7 with a\nsignificantly smaller model size (114.2M vs. 880.6M).", "published": "2022-04-13 19:54:51", "link": "http://arxiv.org/abs/2204.06625v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "IIITDWD-ShankarB@ Dravidian-CodeMixi-HASOC2021: mBERT based model for\n  identification of offensive content in south Indian languages", "abstract": "In recent years, there has been a lot of focus on offensive content. The\namount of offensive content generated by social media is increasing at an\nalarming rate. This created a greater need to address this issue than ever\nbefore. To address these issues, the organizers of \"Dravidian-Code Mixed\nHASOC-2020\" have created two challenges. Task 1 involves identifying offensive\ncontent in Malayalam data, whereas Task 2 includes Malayalam and Tamil Code\nMixed Sentences. Our team participated in Task 2. In our suggested model, we\nexperiment with multilingual BERT to extract features, and three different\nclassifiers are used on extracted features. Our model received a weighted F1\nscore of 0.70 for Malayalam data and was ranked fifth; we also received a\nweighted F1 score of 0.573 for Tamil Code Mixed data and were ranked eleventh.", "published": "2022-04-13 06:24:57", "link": "http://arxiv.org/abs/2204.10195v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "What Matters in Language Conditioned Robotic Imitation Learning over\n  Unstructured Data", "abstract": "A long-standing goal in robotics is to build robots that can perform a wide\nrange of daily tasks from perceptions obtained with their onboard sensors and\nspecified only via natural language. While recently substantial advances have\nbeen achieved in language-driven robotics by leveraging end-to-end learning\nfrom pixels, there is no clear and well-understood process for making various\ndesign choices due to the underlying variation in setups. In this paper, we\nconduct an extensive study of the most critical challenges in learning language\nconditioned policies from offline free-form imitation datasets. We further\nidentify architectural and algorithmic techniques that improve performance,\nsuch as a hierarchical decomposition of the robot control learning, a\nmultimodal transformer encoder, discrete latent plans and a self-supervised\ncontrastive loss that aligns video and language representations. By combining\nthe results of our investigation with our improved model components, we are\nable to present a novel approach that significantly outperforms the state of\nthe art on the challenging language conditioned long-horizon robot manipulation\nCALVIN benchmark. We have open-sourced our implementation to facilitate future\nresearch in learning to perform many complex manipulation skills in a row\nspecified with natural language. Codebase and trained models available at\nhttp://hulc.cs.uni-freiburg.de", "published": "2022-04-13 08:45:32", "link": "http://arxiv.org/abs/2204.06252v2", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.RO"}
{"title": "Self-critical Sequence Training for Automatic Speech Recognition", "abstract": "Although automatic speech recognition (ASR) task has gained remarkable\nsuccess by sequence-to-sequence models, there are two main mismatches between\nits training and testing that might lead to performance degradation: 1) The\ntypically used cross-entropy criterion aims to maximize log-likelihood of the\ntraining data, while the performance is evaluated by word error rate (WER), not\nlog-likelihood; 2) The teacher-forcing method leads to the dependence on ground\ntruth during training, which means that model has never been exposed to its own\nprediction before testing. In this paper, we propose an optimization method\ncalled self-critical sequence training (SCST) to make the training procedure\nmuch closer to the testing phase. As a reinforcement learning (RL) based\nmethod, SCST utilizes a customized reward function to associate the training\ncriterion and WER. Furthermore, it removes the reliance on teacher-forcing and\nharmonizes the model with respect to its inference procedure. We conducted\nexperiments on both clean and noisy speech datasets, and the results show that\nthe proposed SCST respectively achieves 8.7% and 7.8% relative improvements\nover the baseline in terms of WER.", "published": "2022-04-13 09:13:32", "link": "http://arxiv.org/abs/2204.06260v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "TIB-VA at SemEval-2022 Task 5: A Multimodal Architecture for the\n  Detection and Classification of Misogynous Memes", "abstract": "The detection of offensive, hateful content on social media is a challenging\nproblem that affects many online users on a daily basis. Hateful content is\noften used to target a group of people based on ethnicity, gender, religion and\nother factors. The hate or contempt toward women has been increasing on social\nplatforms. Misogynous content detection is especially challenging when textual\nand visual modalities are combined to form a single context, e.g., an overlay\ntext embedded on top of an image, also known as meme. In this paper, we present\na multimodal architecture that combines textual and visual features in order to\ndetect misogynous meme content. The proposed architecture is evaluated in the\nSemEval-2022 Task 5: MAMI - Multimedia Automatic Misogyny Identification\nchallenge under the team name TIB-VA. Our solution obtained the best result in\nthe Task-B where the challenge is to classify whether a given document is\nmisogynous and further identify the main sub-classes of shaming, stereotype,\nobjectification, and violence.", "published": "2022-04-13 11:03:21", "link": "http://arxiv.org/abs/2204.06299v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Automatic Multi-Label Prompting: Simple and Interpretable Few-Shot\n  Classification", "abstract": "Prompt-based learning (i.e., prompting) is an emerging paradigm for\nexploiting knowledge learned by a pretrained language model. In this paper, we\npropose Automatic Multi-Label Prompting (AMuLaP), a simple yet effective method\nto automatically select label mappings for few-shot text classification with\nprompting. Our method exploits one-to-many label mappings and a\nstatistics-based algorithm to select label mappings given a prompt template.\nOur experiments demonstrate that AMuLaP achieves competitive performance on the\nGLUE benchmark without human effort or external resources.", "published": "2022-04-13 11:15:52", "link": "http://arxiv.org/abs/2204.06305v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Call-sign recognition and understanding for noisy air-traffic\n  transcripts using surveillance information", "abstract": "Air traffic control (ATC) relies on communication via speech between pilot\nand air-traffic controller (ATCO). The call-sign, as unique identifier for each\nflight, is used to address a specific pilot by the ATCO. Extracting the\ncall-sign from the communication is a challenge because of the noisy ATC voice\nchannel and the additional noise introduced by the receiver. A low\nsignal-to-noise ratio (SNR) in the speech leads to high word error rate (WER)\ntranscripts. We propose a new call-sign recognition and understanding (CRU)\nsystem that addresses this issue. The recognizer is trained to identify\ncall-signs in noisy ATC transcripts and convert them into the standard\nInternational Civil Aviation Organization (ICAO) format. By incorporating\nsurveillance information, we can multiply the call-sign accuracy (CSA) up to a\nfactor of four. The introduced data augmentation adds additional performance on\nhigh WER transcripts and allows the adaptation of the model to unseen\nairspaces.", "published": "2022-04-13 11:30:42", "link": "http://arxiv.org/abs/2204.06309v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "HuBERT-EE: Early Exiting HuBERT for Efficient Speech Recognition", "abstract": "Pre-training with self-supervised models, such as Hidden-unit BERT (HuBERT)\nand wav2vec 2.0, has brought significant improvements in automatic speech\nrecognition (ASR). However, these models usually require an expensive\ncomputational cost to achieve outstanding performance, slowing down the\ninference speed. To improve the model efficiency, we introduce an early exit\nscheme for ASR, namely HuBERT-EE, that allows the model to stop the inference\ndynamically. In HuBERT-EE, multiple early exit branches are added at the\nintermediate layers. When the intermediate prediction of the early exit branch\nis confident, the model stops the inference, and the corresponding result can\nbe returned early. We investigate the proper early exiting criterion and\nfine-tuning strategy to effectively perform early exiting. Experimental results\non the LibriSpeech show that HuBERT-EE can accelerate the inference of the\nHuBERT while simultaneously balancing the trade-off between the performance and\nthe latency.", "published": "2022-04-13 12:11:44", "link": "http://arxiv.org/abs/2204.06328v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "FactGraph: Evaluating Factuality in Summarization with Semantic Graph\n  Representations", "abstract": "Despite recent improvements in abstractive summarization, most current\napproaches generate summaries that are not factually consistent with the source\ndocument, severely restricting their trust and usage in real-world\napplications. Recent works have shown promising improvements in factuality\nerror identification using text or dependency arc entailments; however, they do\nnot consider the entire semantic graph simultaneously. To this end, we propose\nFactGraph, a method that decomposes the document and the summary into\nstructured meaning representations (MR), which are more suitable for factuality\nevaluation. MRs describe core semantic concepts and their relations,\naggregating the main content in both document and summary in a canonical form,\nand reducing data sparsity. FactGraph encodes such graphs using a graph encoder\naugmented with structure-aware adapters to capture interactions among the\nconcepts based on the graph connectivity, along with text representations using\nan adapter-based text encoder. Experiments on different benchmarks for\nevaluating factuality show that FactGraph outperforms previous approaches by up\nto 15%. Furthermore, FactGraph improves performance on identifying content\nverifiability errors and better captures subsentence-level factual\ninconsistencies.", "published": "2022-04-13 16:45:33", "link": "http://arxiv.org/abs/2204.06508v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "METRO: Efficient Denoising Pretraining of Large Scale Autoencoding\n  Language Models with Model Generated Signals", "abstract": "We present an efficient method of pretraining large-scale autoencoding\nlanguage models using training signals generated by an auxiliary model.\nOriginated in ELECTRA, this training strategy has demonstrated\nsample-efficiency to pretrain models at the scale of hundreds of millions of\nparameters. In this work, we conduct a comprehensive empirical study, and\npropose a recipe, namely \"Model generated dEnoising TRaining Objective\"\n(METRO), which incorporates some of the best modeling techniques developed\nrecently to speed up, stabilize, and enhance pretrained language models without\ncompromising model effectiveness. The resultant models, METRO-LM, consisting of\nup to 5.4 billion parameters, achieve new state-of-the-art on the GLUE,\nSuperGLUE, and SQuAD benchmarks. More importantly, METRO-LM are efficient in\nthat they often outperform previous large models with significantly smaller\nmodel sizes and lower pretraining cost.", "published": "2022-04-13 21:39:15", "link": "http://arxiv.org/abs/2204.06644v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "CRUSH: Contextually Regularized and User anchored Self-supervised Hate\n  speech Detection", "abstract": "The last decade has witnessed a surge in the interaction of people through\nsocial networking platforms. While there are several positive aspects of these\nsocial platforms, the proliferation has led them to become the breeding ground\nfor cyber-bullying and hate speech. Recent advances in NLP have often been used\nto mitigate the spread of such hateful content. Since the task of hate speech\ndetection is usually applicable in the context of social networks, we introduce\nCRUSH, a framework for hate speech detection using user-anchored\nself-supervision and contextual regularization. Our proposed approach secures ~\n1-12% improvement in test set metrics over best performing previous approaches\non two types of tasks and multiple popular english social media datasets.", "published": "2022-04-13 13:51:51", "link": "http://arxiv.org/abs/2204.06389v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.SI", "I.2.7; J.4"], "primary_category": "cs.CL"}
{"title": "Formal Language Recognition by Hard Attention Transformers: Perspectives\n  from Circuit Complexity", "abstract": "This paper analyzes three formal models of Transformer encoders that differ\nin the form of their self-attention mechanism: unique hard attention (UHAT);\ngeneralized unique hard attention (GUHAT), which generalizes UHAT; and\naveraging hard attention (AHAT). We show that UHAT and GUHAT Transformers,\nviewed as string acceptors, can only recognize formal languages in the\ncomplexity class AC$^0$, the class of languages recognizable by families of\nBoolean circuits of constant depth and polynomial size. This upper bound\nsubsumes Hahn's (2020) results that GUHAT cannot recognize the DYCK languages\nor the PARITY language, since those languages are outside AC$^0$ (Furst et al.,\n1984). In contrast, the non-AC$^0$ languages MAJORITY and DYCK-1 are\nrecognizable by AHAT networks, implying that AHAT can recognize languages that\nUHAT and GUHAT cannot.", "published": "2022-04-13 19:25:42", "link": "http://arxiv.org/abs/2204.06618v1", "categories": ["cs.CC", "cs.AI", "cs.CL", "cs.FL", "cs.LG"], "primary_category": "cs.CC"}
{"title": "Neural Topic Modeling of Psychotherapy Sessions", "abstract": "In this work, we compare different neural topic modeling methods in learning\nthe topical propensities of different psychiatric conditions from the\npsychotherapy session transcripts parsed from speech recordings. We also\nincorporate temporal modeling to put this additional interpretability to action\nby parsing out topic similarities as a time series in a turn-level resolution.\nWe believe this topic modeling framework can offer interpretable insights for\nthe therapist to optimally decide his or her strategy and improve psychotherapy\neffectiveness.", "published": "2022-04-13 04:05:39", "link": "http://arxiv.org/abs/2204.10189v2", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "Sound Event Triage: Detecting Sound Events Considering Priority of\n  Classes", "abstract": "We propose a new task for sound event detection (SED): sound event triage\n(SET). The goal of SET is to detect an arbitrary number of high-priority event\nclasses while allowing misdetections of low-priority event classes where the\npriority is given for each event class. In conventional methods of SED for\ntargeting a specific sound event class, it is only possible to give priority to\na single event class. Moreover, the level of priority is not adjustable, i.e,\nthe conventional methods can use only types of target event class such as\none-hot vector, as inputs. To flexibly control much information on the target\nevent, the proposed SET exploits not only types of target sound but also the\nextent to which each target sound is detected with priority. To implement the\ndetection of events with priority, we propose class-weighted training, in which\nloss functions and the network are stochastically weighted by the priority\nparameter of each class. As this is the first paper on SET, we particularly\nintroduce an implementation of single target SET, which is a subtask of SET.\nResults of the experiments using the URBAN-SED dataset show that the proposed\nmethod of single target SET outperforms the conventional SED method by 8.70,\n6.66, and 6.09 percentage points for ``air_conditioner,'' ``car_horn,'' and\n``street_music,'' respectively, in terms of the intersection-based F-score. For\nthe average score of classes, the proposed methods increase the\nintersection-based F-score by up to 3.37 percentage points compared with the\nconventional SED and other target-class-conditioned models.", "published": "2022-04-13 14:07:18", "link": "http://arxiv.org/abs/2204.06402v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "BEHM-GAN: Bandwidth Extension of Historical Music using Generative\n  Adversarial Networks", "abstract": "Audio bandwidth extension aims to expand the spectrum of narrow-band audio\nsignals. Although this topic has been broadly studied during recent years, the\nparticular problem of extending the bandwidth of historical music recordings\nremains an open challenge. This paper proposes BEHM-GAN, a model based on\ngenerative adversarial networks, as a practical solution to this problem. The\nproposed method works with the complex spectrogram representation of audio and,\nthanks to a dedicated regularization strategy, can effectively extend the\nbandwidth of out-of-distribution real historical recordings. The BEHM-GAN is\ndesigned to be applied as a second step after denoising the recording to\nsuppress any additive disturbances, such as clicks and background noise. We\ntrain and evaluate the method using solo piano classical music. The proposed\nmethod outperforms the compared baselines in both objective and subjective\nexperiments. The results of a formal blind listening test show that BEHM-GAN\nsignificantly increases the perceptual sound quality in early-20th-century\ngramophone recordings. For several items, there is a substantial improvement in\nthe mean opinion score after enhancing historical recordings with the proposed\nbandwidth-extension algorithm. This study represents a relevant step toward\ndata-driven music restoration in real-world scenarios.", "published": "2022-04-13 15:55:25", "link": "http://arxiv.org/abs/2204.06478v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Predicting score distribution to improve non-intrusive speech quality\n  estimation", "abstract": "Deep noise suppressors (DNS) have become an attractive solution to remove\nbackground noise, reverberation, and distortions from speech and are widely\nused in telephony/voice applications. They are also occasionally prone to\nintroducing artifacts and lowering the perceptual quality of the speech.\nSubjective listening tests that use multiple human judges to derive a mean\nopinion score (MOS) are a popular way to measure these models' performance.\nDeep neural network based non-intrusive MOS estimation models have recently\nemerged as a popular cost-efficient alternative to these tests. These models\nare trained with only the MOS labels, often discarding the secondary statistics\nof the opinion scores. In this paper, we investigate several ways to integrate\nthe distribution of opinion scores (e.g. variance, histogram information) to\nimprove the MOS estimation performance. Our model is trained on a corpus of\n419K denoised samples by 320 different DNS models and model variations and\nevaluated on 18K test samples from DNSMOS. We show that with very minor\nmodification of a single task MOS estimation pipeline, these freely available\nlabels can provide up to a 0.016 RMSE and 1% SRCC improvement.", "published": "2022-04-13 19:16:44", "link": "http://arxiv.org/abs/2204.06616v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Unified Cascaded Encoder ASR Model for Dynamic Model Sizes", "abstract": "In this paper, we propose a dynamic cascaded encoder Automatic Speech\nRecognition (ASR) model, which unifies models for different deployment\nscenarios. Moreover, the model can significantly reduce model size and power\nconsumption without loss of quality. Namely, with the dynamic cascaded encoder\nmodel, we explore three techniques to maximally boost the performance of each\nmodel size: 1) Use separate decoders for each sub-model while sharing the\nencoders; 2) Use funnel-pooling to improve the encoder efficiency; 3) Balance\nthe size of causal and non-causal encoders to improve quality and fit\ndeployment constraints. Overall, the proposed large-medium model has 30%\nsmaller size and reduces power consumption by 33%, compared to the baseline\ncascaded encoder model. The triple-size model that unifies the large, medium,\nand small models achieves 37% total size reduction with minimal quality loss,\nwhile substantially reducing the engineering efforts of having separate models.", "published": "2022-04-13 04:15:51", "link": "http://arxiv.org/abs/2204.06164v3", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Review of Machine Learning Methods Applied to Structural Dynamics and\n  Vibroacoustic", "abstract": "The use of Machine Learning (ML) has rapidly spread across several fields,\nhaving encountered many applications in Structural Dynamics and Vibroacoustic\n(SD\\&V). The increasing capabilities of ML to unveil insights from data, driven\nby unprecedented data availability, algorithms advances and computational\npower, enhance decision making, uncertainty handling, patterns recognition and\nreal-time assessments. Three main applications in SD\\&V have taken advantage of\nthese benefits. In Structural Health Monitoring, ML detection and prognosis\nlead to safe operation and optimized maintenance schedules. System\nidentification and control design are leveraged by ML techniques in Active\nNoise Control and Active Vibration Control. Finally, the so-called ML-based\nsurrogate models provide fast alternatives to costly simulations, enabling\nrobust and optimized product design. Despite the many works in the area, they\nhave not been reviewed and analyzed. Therefore, to keep track and understand\nthis ongoing integration of fields, this paper presents a survey of ML\napplications in SD\\&V analyses, shedding light on the current state of\nimplementation and emerging opportunities. The main methodologies, advantages,\nlimitations, and recommendations based on scientific knowledge were identified\nfor each of the three applications. Moreover, the paper considers the role of\nDigital Twins and Physics Guided ML to overcome current challenges and power\nfuture research progress. As a result, the survey provides a broad overview of\nthe present landscape of ML applied in SD\\&V and guides the reader to an\nadvanced understanding of progress and prospects in the field.", "published": "2022-04-13 13:16:21", "link": "http://arxiv.org/abs/2204.06362v2", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Receptive Field Analysis of Temporal Convolutional Networks for Monaural\n  Speech Dereverberation", "abstract": "Speech dereverberation is often an important requirement in robust speech\nprocessing tasks. Supervised deep learning (DL) models give state-of-the-art\nperformance for single-channel speech dereverberation. Temporal convolutional\nnetworks (TCNs) are commonly used for sequence modelling in speech enhancement\ntasks. A feature of TCNs is that they have a receptive field (RF) dependent on\nthe specific model configuration which determines the number of input frames\nthat can be observed to produce an individual output frame. It has been shown\nthat TCNs are capable of performing dereverberation of simulated speech data,\nhowever a thorough analysis, especially with focus on the RF is yet lacking in\nthe literature. This paper analyses dereverberation performance depending on\nthe model size and the RF of TCNs. Experiments using the WHAMR corpus which is\nextended to include room impulse responses (RIRs) with larger T60 values\ndemonstrate that a larger RF can have significant improvement in performance\nwhen training smaller TCN models. It is also demonstrated that TCNs benefit\nfrom a wider RF when dereverberating RIRs with larger RT60 values.", "published": "2022-04-13 14:57:59", "link": "http://arxiv.org/abs/2204.06439v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The effect of speech pathology on automatic speaker verification -- a\n  large-scale study", "abstract": "Navigating the challenges of data-driven speech processing, one of the\nprimary hurdles is accessing reliable pathological speech data. While public\ndatasets appear to offer solutions, they come with inherent risks of potential\nunintended exposure of patient health information via re-identification\nattacks. Using a comprehensive real-world pathological speech corpus, with over\nn=3,800 test subjects spanning various age groups and speech disorders, we\nemployed a deep-learning-driven automatic speaker verification (ASV) approach.\nThis resulted in a notable mean equal error rate (EER) of 0.89% with a standard\ndeviation of 0.06%, outstripping traditional benchmarks. Our comprehensive\nassessments demonstrate that pathological speech overall faces heightened\nprivacy breach risks compared to healthy speech. Specifically, adults with\ndysphonia are at heightened re-identification risks, whereas conditions like\ndysarthria yield results comparable to those of healthy speakers. Crucially,\nspeech intelligibility does not influence the ASV system's performance metrics.\nIn pediatric cases, particularly those with cleft lip and palate, the recording\nenvironment plays a decisive role in re-identification. Merging data across\npathological types led to a marked EER decrease, suggesting the potential\nbenefits of pathological diversity in ASV, accompanied by a logarithmic boost\nin ASV effectiveness. In essence, this research sheds light on the dynamics\nbetween pathological speech and speaker verification, emphasizing its crucial\nrole in safeguarding patient confidentiality in our increasingly digitized\nhealthcare era.", "published": "2022-04-13 15:17:00", "link": "http://arxiv.org/abs/2204.06450v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
