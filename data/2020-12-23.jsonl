{"title": "TicketTalk: Toward human-level performance with end-to-end,\n  transaction-based dialog systems", "abstract": "We present a data-driven, end-to-end approach to transaction-based dialog\nsystems that performs at near-human levels in terms of verbal response quality\nand factual grounding accuracy. We show that two essential components of the\nsystem produce these results: a sufficiently large and diverse, in-domain\nlabeled dataset, and a neural network-based, pre-trained model that generates\nboth verbal responses and API call predictions. In terms of data, we introduce\nTicketTalk, a movie ticketing dialog dataset with 23,789 annotated\nconversations. The movie ticketing conversations range from completely\nopen-ended and unrestricted to more structured, both in terms of their\nknowledge base, discourse features, and number of turns. In qualitative human\nevaluations, model-generated responses trained on just 10,000 TicketTalk\ndialogs were rated to \"make sense\" 86.5 percent of the time, almost the same as\nhuman responses in the same contexts. Our simple, API-focused annotation schema\nresults in a much easier labeling task making it faster and more cost\neffective. It is also the key component for being able to predict API calls\naccurately. We handle factual grounding by incorporating API calls in the\ntraining data, allowing our model to learn which actions to take and when.\nTrained on the same 10,000-dialog set, the model's API call predictions were\nrated to be correct 93.9 percent of the time in our evaluations, surpassing the\nratings for the corresponding human labels. We show how API prediction and\nresponse generation scores improve as the dataset size incrementally increases\nfrom 5000 to 21,000 dialogs. Our analysis also clearly illustrates the benefits\nof pre-training. We are publicly releasing the TicketTalk dataset with this\npaper to facilitate future work on transaction-based dialogs.", "published": "2020-12-23 02:43:37", "link": "http://arxiv.org/abs/2012.12458v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Dense Representations of Phrases at Scale", "abstract": "Open-domain question answering can be reformulated as a phrase retrieval\nproblem, without the need for processing documents on-demand during inference\n(Seo et al., 2019). However, current phrase retrieval models heavily depend on\nsparse representations and still underperform retriever-reader approaches. In\nthis work, we show for the first time that we can learn dense representations\nof phrases alone that achieve much stronger performance in open-domain QA. We\npresent an effective method to learn phrase representations from the\nsupervision of reading comprehension tasks, coupled with novel negative\nsampling methods. We also propose a query-side fine-tuning strategy, which can\nsupport transfer learning and reduce the discrepancy between training and\ninference. On five popular open-domain QA datasets, our model DensePhrases\nimproves over previous phrase retrieval models by 15%-25% absolute accuracy and\nmatches the performance of state-of-the-art retriever-reader models. Our model\nis easy to parallelize due to pure dense representations and processes more\nthan 10 questions per second on CPUs. Finally, we directly use our pre-indexed\ndense phrase representations for two slot filling tasks, showing the promise of\nutilizing DensePhrases as a dense knowledge base for downstream tasks.", "published": "2020-12-23 12:28:17", "link": "http://arxiv.org/abs/2012.12624v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Negation in Cognitive Reasoning", "abstract": "Negation is both an operation in formal logic and in natural language by\nwhich a proposition is replaced by one stating the opposite, as by the addition\nof \"not\" or another negation cue. Treating negation in an adequate way is\nrequired for cognitive reasoning, which aims at modeling the human ability to\ndraw meaningful conclusions despite incomplete and inconsistent knowledge. One\ntask of cognitive reasoning is answering questions given by sentences in\nnatural language. There are tools based on discourse representation theory to\nconvert sentences automatically into a formal logic representation, and\nadditional knowledge can be added using the predicate names in the formula and\nknowledge databases. However, the knowledge in logic databases in practice\nalways is incomplete. Hence, forward reasoning of automated reasoning systems\nalone does not suffice to derive answers to questions because, instead of\ncomplete proofs, often only partial positive knowledge can be derived, while\nnegative knowledge is used only during the reasoning process. In consequence,\nwe aim at eliminating syntactic negation, strictly speaking, the negated event\nor property. In this paper, we describe an effective procedure to determine the\nnegated event or property in order to replace it by its inverse. This lays the\nbasis of cognitive reasoning, employing both logic and machine learning for\ngeneral question answering. We evaluate our procedure by several benchmarks and\ndemonstrate its practical usefulness in our cognitive reasoning system.", "published": "2020-12-23 13:22:53", "link": "http://arxiv.org/abs/2012.12641v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Scansion of Spanish Poetry without Syllabification", "abstract": "In recent years, several systems of automated metric analysis of Spanish\npoetry have emerged. These systems rely on complex methods of syllabification\nand stress assignment, which use PoS-tagging libraries, whose computational\ncost is high. This cost increases with the calculation of metric ambiguities.\nFurthermore, they do not consider determining issues in syllabic count such as\nthe phenomena of compensation between hemistichs of verses of more than eleven\nsyllables. However, it is possible to carry out an informative and accurate\nmetric analysis without using these costly methods. We propose an algorithm\nthat performs accurate scansion (number of syllables, stress pattern and type\nof verse) without syllabification. It addresses metric ambiguities and takes\ninto account the hemistichs compensation. Our algorithm outperforms the current\nstate of the art by 2% in fixed-metre poetry, and 25% in mixed-metre poetry. It\nalso runs 21 and 25 times faster, respectively. Finally, a desktop application\nis offered as a tool for researchers of Spanish poetry.", "published": "2020-12-23 16:59:43", "link": "http://arxiv.org/abs/2012.12799v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Future-Guided Incremental Transformer for Simultaneous Translation", "abstract": "Simultaneous translation (ST) starts translations synchronously while reading\nsource sentences, and is used in many online scenarios. The previous wait-k\npolicy is concise and achieved good results in ST. However, wait-k policy faces\ntwo weaknesses: low training speed caused by the recalculation of hidden states\nand lack of future source information to guide training. For the low training\nspeed, we propose an incremental Transformer with an average embedding layer\n(AEL) to accelerate the speed of calculation of the hidden states during\ntraining. For future-guided training, we propose a conventional Transformer as\nthe teacher of the incremental Transformer, and try to invisibly embed some\nfuture information in the model through knowledge distillation. We conducted\nexperiments on Chinese-English and German-English simultaneous translation\ntasks and compared with the wait-k policy to evaluate the proposed method. Our\nmethod can effectively increase the training speed by about 28 times on average\nat different k and implicitly embed some predictive abilities in the model,\nachieving better translation quality than wait-k baseline.", "published": "2020-12-23 03:04:49", "link": "http://arxiv.org/abs/2012.12465v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Automated Lay Language Summarization of Biomedical Scientific Reviews", "abstract": "Health literacy has emerged as a crucial factor in making appropriate health\ndecisions and ensuring treatment outcomes. However, medical jargon and the\ncomplex structure of professional language in this domain make health\ninformation especially hard to interpret. Thus, there is an urgent unmet need\nfor automated methods to enhance the accessibility of the biomedical literature\nto the general population. This problem can be framed as a type of translation\nproblem between the language of healthcare professionals, and that of the\ngeneral public. In this paper, we introduce the novel task of automated\ngeneration of lay language summaries of biomedical scientific reviews, and\nconstruct a dataset to support the development and evaluation of automated\nmethods through which to enhance the accessibility of the biomedical\nliterature. We conduct analyses of the various challenges in solving this task,\nincluding not only summarization of the key points but also explanation of\nbackground knowledge and simplification of professional language. We experiment\nwith state-of-the-art summarization models as well as several data augmentation\ntechniques, and evaluate their performance using both automated metrics and\nhuman assessment. Results indicate that automatically generated summaries\nproduced using contemporary neural architectures can achieve promising quality\nand readability as compared with reference summaries developed for the lay\npublic by experts (best ROUGE-L of 50.24 and Flesch-Kincaid readability score\nof 13.30). We also discuss the limitations of the current attempt, providing\ninsights and directions for future work.", "published": "2020-12-23 10:01:18", "link": "http://arxiv.org/abs/2012.12573v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "EmotionGIF-IITP-AINLPML: Ensemble-based Automated Deep Neural System for\n  predicting category(ies) of a GIF response", "abstract": "In this paper, we describe the systems submitted by our IITP-AINLPML team in\nthe shared task of SocialNLP 2020, EmotionGIF 2020, on predicting the\ncategory(ies) of a GIF response for a given unlabelled tweet. For the round 1\nphase of the task, we propose an attention-based Bi-directional GRU network\ntrained on both the tweet (text) and their replies (text wherever available)\nand the given category(ies) for its GIF response. In the round 2 phase, we\nbuild several deep neural-based classifiers for the task and report the final\npredictions through a majority voting based ensemble technique. Our proposed\nmodels attain the best Mean Recall (MR) scores of 52.92% and 53.80% in round 1\nand round 2, respectively.", "published": "2020-12-23 15:52:27", "link": "http://arxiv.org/abs/2012.12756v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Multimodal Framework for the Detection of Hateful Memes", "abstract": "An increasingly common expression of online hate speech is multimodal in\nnature and comes in the form of memes. Designing systems to automatically\ndetect hateful content is of paramount importance if we are to mitigate its\nundesirable effects on the society at large. The detection of multimodal hate\nspeech is an intrinsically difficult and open problem: memes convey a message\nusing both images and text and, hence, require multimodal reasoning and joint\nvisual and language understanding. In this work, we seek to advance this line\nof research and develop a multimodal framework for the detection of hateful\nmemes. We improve the performance of existing multimodal approaches beyond\nsimple fine-tuning and, among others, show the effectiveness of upsampling of\ncontrastive examples to encourage multimodality and ensemble learning based on\ncross-validation to improve robustness. We furthermore analyze model\nmisclassifications and discuss a number of hypothesis-driven augmentations and\ntheir effects on performance, presenting important implications for future\nresearch in the field. Our best approach comprises an ensemble of UNITER-based\nmodels and achieves an AUROC score of 80.53, placing us 4th on phase 2 of the\n2020 Hateful Memes Challenge organized by Facebook.", "published": "2020-12-23 18:37:11", "link": "http://arxiv.org/abs/2012.12871v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Code Switching Language Model Using Monolingual Training Data", "abstract": "Training a code-switching (CS) language model using only monolingual data is\nstill an ongoing research problem. In this paper, a CS language model is\ntrained using only monolingual training data. As recurrent neural network (RNN)\nmodels are best suited for predicting sequential data. In this work, an RNN\nlanguage model is trained using alternate batches from only monolingual English\nand Spanish data and the perplexity of the language model is computed. From the\nresults, it is concluded that using alternate batches of monolingual data in\ntraining reduced the perplexity of a CS language model. The results were\nconsistently improved using mean square error (MSE) in the output embeddings of\nRNN based language model. By combining both methods, perplexity is reduced from\n299.63 to 80.38. The proposed methods were comparable to the language model\nfine tune with code-switch training data.", "published": "2020-12-23 08:56:39", "link": "http://arxiv.org/abs/2012.12543v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Bridging Textual and Tabular Data for Cross-Domain Text-to-SQL Semantic\n  Parsing", "abstract": "We present BRIDGE, a powerful sequential architecture for modeling\ndependencies between natural language questions and relational databases in\ncross-DB semantic parsing. BRIDGE represents the question and DB schema in a\ntagged sequence where a subset of the fields are augmented with cell values\nmentioned in the question. The hybrid sequence is encoded by BERT with minimal\nsubsequent layers and the text-DB contextualization is realized via the\nfine-tuned deep attention in BERT. Combined with a pointer-generator decoder\nwith schema-consistency driven search space pruning, BRIDGE attained\nstate-of-the-art performance on popular cross-DB text-to-SQL benchmarks, Spider\n(71.1\\% dev, 67.5\\% test with ensemble model) and WikiSQL (92.6\\% dev, 91.9\\%\ntest). Our analysis shows that BRIDGE effectively captures the desired\ncross-modal dependencies and has the potential to generalize to more text-DB\nrelated tasks. Our implementation is available at\n\\url{https://github.com/salesforce/TabularSemanticParsing}.", "published": "2020-12-23 12:33:52", "link": "http://arxiv.org/abs/2012.12627v2", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Detecting Hate Speech in Memes Using Multimodal Deep Learning\n  Approaches: Prize-winning solution to Hateful Memes Challenge", "abstract": "Memes on the Internet are often harmless and sometimes amusing. However, by\nusing certain types of images, text, or combinations of both, the seemingly\nharmless meme becomes a multimodal type of hate speech -- a hateful meme. The\nHateful Memes Challenge is a first-of-its-kind competition which focuses on\ndetecting hate speech in multimodal memes and it proposes a new data set\ncontaining 10,000+ new examples of multimodal content. We utilize VisualBERT --\nwhich meant to be the BERT of vision and language -- that was trained\nmultimodally on images and captions and apply Ensemble Learning. Our approach\nachieves 0.811 AUROC with an accuracy of 0.765 on the challenge test set and\nplaced third out of 3,173 participants in the Hateful Memes Challenge.", "published": "2020-12-23 21:09:52", "link": "http://arxiv.org/abs/2012.12975v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "Speech Synthesis as Augmentation for Low-Resource ASR", "abstract": "Speech synthesis might hold the key to low-resource speech recognition. Data\naugmentation techniques have become an essential part of modern speech\nrecognition training. Yet, they are simple, naive, and rarely reflect\nreal-world conditions. Meanwhile, speech synthesis techniques have been rapidly\ngetting closer to the goal of achieving human-like speech. In this paper, we\ninvestigate the possibility of using synthesized speech as a form of data\naugmentation to lower the resources necessary to build a speech recognizer. We\nexperiment with three different kinds of synthesizers: statistical parametric,\nneural, and adversarial. Our findings are interesting and point to new research\ndirections for the future.", "published": "2020-12-23 22:19:42", "link": "http://arxiv.org/abs/2012.13004v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Self-Supervised Hyperboloid Representations from Logical Queries over\n  Knowledge Graphs", "abstract": "Knowledge Graphs (KGs) are ubiquitous structures for information storagein\nseveral real-world applications such as web search, e-commerce, social\nnetworks, and biology. Querying KGs remains a foundational and challenging\nproblem due to their size and complexity. Promising approaches to tackle this\nproblem include embedding the KG units (e.g., entities and relations) in a\nEuclidean space such that the query embedding contains the information relevant\nto its results. These approaches, however, fail to capture the hierarchical\nnature and semantic information of the entities present in the graph.\nAdditionally, most of these approaches only utilize multi-hop queries (that can\nbe modeled by simple translation operations) to learn embeddings and ignore\nmore complex operations such as intersection and union of simpler queries. To\ntackle such complex operations, in this paper, we formulate KG representation\nlearning as a self-supervised logical query reasoning problem that utilizes\ntranslation, intersection and union queries over KGs. We propose Hyperboloid\nEmbeddings (HypE), a novel self-supervised dynamic reasoning framework, that\nutilizes positive first-order existential queries on a KG to learn\nrepresentations of its entities and relations as hyperboloids in a Poincar\\'e\nball. HypE models the positive first-order queries as geometrical translation,\nintersection, and union. For the problem of KG reasoning in real-world\ndatasets, the proposed HypE model significantly outperforms the state-of-the\nart results. We also apply HypE to an anomaly detection task on a popular\ne-commerce website product taxonomy as well as hierarchically organized web\narticles and demonstrate significant performance improvements compared to\nexisting baseline methods. Finally, we also visualize the learned HypE\nembeddings in a Poincar\\'e ball to clearly interpret and comprehend the\nrepresentation space.", "published": "2020-12-23 23:19:00", "link": "http://arxiv.org/abs/2012.13023v3", "categories": ["cs.LG", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "CN-Celeb: multi-genre speaker recognition", "abstract": "Research on speaker recognition is extending to address the vulnerability in\nthe wild conditions, among which genre mismatch is perhaps the most\nchallenging, for instance, enrollment with reading speech while testing with\nconversational or singing audio. This mismatch leads to complex and composite\ninter-session variations, both intrinsic (i.e., speaking style, physiological\nstatus) and extrinsic (i.e., recording device, background noise).\nUnfortunately, the few existing multi-genre corpora are not only limited in\nsize but are also recorded under controlled conditions, which cannot support\nconclusive research on the multi-genre problem. In this work, we firstly\npublish CN-Celeb, a large-scale multi-genre corpus that includes in-the-wild\nspeech utterances of 3,000 speakers in 11 different genres. Secondly, using\nthis dataset, we conduct a comprehensive study on the multi-genre phenomenon,\nin particular the impact of the multi-genre challenge on speaker recognition\nand the performance gain when the new dataset is used to conduct multi-genre\ntraining.", "published": "2020-12-23 03:13:06", "link": "http://arxiv.org/abs/2012.12468v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Principle Solution for Enroll-Test Mismatch in Speaker Recognition", "abstract": "Mismatch between enrollment and test conditions causes serious performance\ndegradation on speaker recognition systems. This paper presents a statistics\ndecomposition (SD) approach to solve this problem. This approach decomposes the\nPLDA score into three components that corresponding to enrollment, prediction\nand normalization respectively. Given that correct statistics are used in each\ncomponent, the resultant score is theoretically optimal. A comprehensive\nexperimental study was conducted on three datasets with different types of\nmismatch: (1) physical channel mismatch, (2) speaking behavior mismatch, (3)\nnear-far recording mismatch. The results demonstrated that the proposed SD\napproach is highly effective, and outperforms the ad-hoc multi-condition\ntraining approach that is commonly adopted but not optimal in theory.", "published": "2020-12-23 03:43:01", "link": "http://arxiv.org/abs/2012.12471v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Incremental Text-to-Speech Synthesis Using Pseudo Lookahead with Large\n  Pretrained Language Model", "abstract": "This letter presents an incremental text-to-speech (TTS) method that performs\nsynthesis in small linguistic units while maintaining the naturalness of output\nspeech. Incremental TTS is generally subject to a trade-off between latency and\nsynthetic speech quality. It is challenging to produce high-quality speech with\na low-latency setup that does not make much use of an unobserved future\nsentence (hereafter, \"lookahead\"). To resolve this issue, we propose an\nincremental TTS method that uses a pseudo lookahead generated with a language\nmodel to take the future contextual information into account without increasing\nlatency. Our method can be regarded as imitating a human's incremental reading\nand uses pretrained GPT2, which accounts for the large-scale linguistic\nknowledge, for the lookahead generation. Evaluation results show that our\nmethod 1) achieves higher speech quality than the method taking only observed\ninformation into account and 2) achieves a speech quality equivalent to waiting\nfor the future context observation.", "published": "2020-12-23 11:46:48", "link": "http://arxiv.org/abs/2012.12612v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The 2020 ESPnet update: new features, broadened applications,\n  performance improvements, and future plans", "abstract": "This paper describes the recent development of ESPnet\n(https://github.com/espnet/espnet), an end-to-end speech processing toolkit.\nThis project was initiated in December 2017 to mainly deal with end-to-end\nspeech recognition experiments based on sequence-to-sequence modeling. The\nproject has grown rapidly and now covers a wide range of speech processing\napplications. Now ESPnet also includes text to speech (TTS), voice conversation\n(VC), speech translation (ST), and speech enhancement (SE) with support for\nbeamforming, speech separation, denoising, and dereverberation. All\napplications are trained in an end-to-end manner, thanks to the generic\nsequence to sequence modeling properties, and they can be further integrated\nand jointly optimized. Also, ESPnet provides reproducible all-in-one recipes\nfor these applications with state-of-the-art performance in various benchmarks\nby incorporating transformer, advanced data augmentation, and conformer. This\nproject aims to provide up-to-date speech processing experience to the\ncommunity so that researchers in academia and various industry scales can\ndevelop their technologies collaboratively.", "published": "2020-12-23 22:25:23", "link": "http://arxiv.org/abs/2012.13006v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
