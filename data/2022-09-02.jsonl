{"title": "Random Text Perturbations Work, but not Always", "abstract": "We present three large-scale experiments on binary text matching\nclassification task both in Chinese and English to evaluate the effectiveness\nand generalizability of random text perturbations as a data augmentation\napproach for NLP. It is found that the augmentation can bring both negative and\npositive effects to the test set performance of three neural classification\nmodels, depending on whether the models train on enough original training\nexamples. This remains true no matter whether five random text editing\noperations, used to augment text, are applied together or separately. Our study\ndemonstrates with strong implication that the effectiveness of random text\nperturbations is task specific and not generally positive.", "published": "2022-09-02 03:03:51", "link": "http://arxiv.org/abs/2209.00797v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Structural Bias for Aspect Sentiment Triplet Extraction", "abstract": "Structural bias has recently been exploited for aspect sentiment triplet\nextraction (ASTE) and led to improved performance. On the other hand, it is\nrecognized that explicitly incorporating structural bias would have a negative\nimpact on efficiency, whereas pretrained language models (PLMs) can already\ncapture implicit structures. Thus, a natural question arises: Is structural\nbias still a necessity in the context of PLMs? To answer the question, we\npropose to address the efficiency issues by using an adapter to integrate\nstructural bias in the PLM and using a cheap-to-compute relative position\nstructure in place of the syntactic dependency structure. Benchmarking\nevaluation is conducted on the SemEval datasets. The results show that our\nproposed structural adapter is beneficial to PLMs and achieves state-of-the-art\nperformance over a range of strong baselines, yet with a light parameter demand\nand low latency. Meanwhile, we give rise to the concern that the current\nevaluation default with data of small scale is under-confident. Consequently,\nwe release a large-scale dataset for ASTE. The results on the new dataset hint\nthat the structural adapter is confidently effective and efficient to a large\nscale. Overall, we draw the conclusion that structural bias shall still be a\nnecessity even with PLMs.", "published": "2022-09-02 05:02:18", "link": "http://arxiv.org/abs/2209.00820v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FOLIO: Natural Language Reasoning with First-Order Logic", "abstract": "Large language models (LLMs) have achieved remarkable performance on a\nvariety of natural language understanding tasks. However, existing benchmarks\nare inadequate in measuring the complex logical reasoning capabilities of a\nmodel. We present FOLIO, a human-annotated, logically complex and diverse\ndataset for reasoning in natural language (NL), equipped with first-order logic\n(FOL) annotations. FOLIO consists of 1,430 examples (unique conclusions), each\npaired with one of 487 sets of premises used to deductively reason for the\nvalidity of each conclusion. The logical correctness of the premises and\nconclusions is ensured by their FOL annotations, which are automatically\nverified by an FOL inference engine. In addition to the main NL reasoning task,\nNL-FOL pairs in FOLIO constitute a new NL-FOL translation dataset. Our\nexperiments on FOLIO systematically evaluate the FOL reasoning ability of\nsupervised fine-tuning on medium-sized language models. For both NL reasoning\nand NL-FOL translation, we benchmark multiple state-of-the-art language models.\nOur results show that a subset of FOLIO presents a challenge for one of the\nmost capable {Large Language Model (LLM)} publicly available, GPT-4.", "published": "2022-09-02 06:50:11", "link": "http://arxiv.org/abs/2209.00840v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploiting Hybrid Semantics of Relation Paths for Multi-hop Question\n  Answering Over Knowledge Graphs", "abstract": "Answering natural language questions on knowledge graphs (KGQA) remains a\ngreat challenge in terms of understanding complex questions via multi-hop\nreasoning. Previous efforts usually exploit large-scale entity-related text\ncorpora or knowledge graph (KG) embeddings as auxiliary information to\nfacilitate answer selection. However, the rich semantics implied in\noff-the-shelf relation paths between entities is far from well explored. This\npaper proposes improving multi-hop KGQA by exploiting relation paths' hybrid\nsemantics. Specifically, we integrate explicit textual information and implicit\nKG structural features of relation paths based on a novel rotate-and-scale\nentity link prediction framework. Extensive experiments on three existing KGQA\ndatasets demonstrate the superiority of our method, especially in multi-hop\nscenarios. Further investigation confirms our method's systematical\ncoordination between questions and relation paths to identify answer entities.", "published": "2022-09-02 08:07:37", "link": "http://arxiv.org/abs/2209.00870v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dialogue Evaluation with Offline Reinforcement Learning", "abstract": "Task-oriented dialogue systems aim to fulfill user goals through natural\nlanguage interactions. They are ideally evaluated with human users, which\nhowever is unattainable to do at every iteration of the development phase.\nSimulated users could be an alternative, however their development is\nnontrivial. Therefore, researchers resort to offline metrics on existing\nhuman-human corpora, which are more practical and easily reproducible. They are\nunfortunately limited in reflecting real performance of dialogue systems. BLEU\nfor instance is poorly correlated with human judgment, and existing\ncorpus-based metrics such as success rate overlook dialogue context mismatches.\nThere is still a need for a reliable metric for task-oriented systems with good\ngeneralization and strong correlation with human judgements. In this paper, we\npropose the use of offline reinforcement learning for dialogue evaluation based\non a static corpus. Such an evaluator is typically called a critic and utilized\nfor policy optimization. We go one step further and show that offline RL\ncritics can be trained on a static corpus for any dialogue system as external\nevaluators, allowing dialogue performance comparisons across various types of\nsystems. This approach has the benefit of being corpus- and model-independent,\nwhile attaining strong correlation with human judgements, which we confirm via\nan interactive user trial.", "published": "2022-09-02 08:32:52", "link": "http://arxiv.org/abs/2209.00876v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A New Aligned Simple German Corpus", "abstract": "\"Leichte Sprache\", the German counterpart to Simple English, is a regulated\nlanguage aiming to facilitate complex written language that would otherwise\nstay inaccessible to different groups of people. We present a new\nsentence-aligned monolingual corpus for Simple German -- German. It contains\nmultiple document-aligned sources which we have aligned using automatic\nsentence-alignment methods. We evaluate our alignments based on a manually\nlabelled subset of aligned documents. The quality of our sentence alignments,\nas measured by F1-score, surpasses previous work. We publish the dataset under\nCC BY-SA and the accompanying code under MIT license.", "published": "2022-09-02 15:14:04", "link": "http://arxiv.org/abs/2209.01106v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extend and Explain: Interpreting Very Long Language Models", "abstract": "While Transformer language models (LMs) are state-of-the-art for information\nextraction, long text introduces computational challenges requiring suboptimal\npreprocessing steps or alternative model architectures. Sparse attention LMs\ncan represent longer sequences, overcoming performance hurdles. However, it\nremains unclear how to explain predictions from these models, as not all tokens\nattend to each other in the self-attention layers, and long sequences pose\ncomputational challenges for explainability algorithms when runtime depends on\ndocument length. These challenges are severe in the medical context where\ndocuments can be very long, and machine learning (ML) models must be auditable\nand trustworthy. We introduce a novel Masked Sampling Procedure (MSP) to\nidentify the text blocks that contribute to a prediction, apply MSP in the\ncontext of predicting diagnoses from medical text, and validate our approach\nwith a blind review by two clinicians. Our method identifies about 1.7x more\nclinically informative text blocks than the previous state-of-the-art, runs up\nto 100x faster, and is tractable for generating important phrase pairs. MSP is\nparticularly well-suited to long LMs but can be applied to any text classifier.\nWe provide a general implementation of MSP.", "published": "2022-09-02 17:15:43", "link": "http://arxiv.org/abs/2209.01174v3", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Elaboration-Generating Commonsense Question Answering at Scale", "abstract": "In question answering requiring common sense, language models (e.g., GPT-3)\nhave been used to generate text expressing background knowledge that helps\nimprove performance. Yet the cost of working with such models is very high; in\nthis work, we finetune smaller language models to generate useful intermediate\ncontext, referred to here as elaborations. Our framework alternates between\nupdating two language models -- an elaboration generator and an answer\npredictor -- allowing each to influence the other. Using less than 0.5% of the\nparameters of GPT-3, our model outperforms alternatives with similar sizes and\ncloses the gap on GPT-3 on four commonsense question answering benchmarks.\nHuman evaluations show that the quality of the generated elaborations is high.", "published": "2022-09-02 18:32:09", "link": "http://arxiv.org/abs/2209.01232v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "INTERACTION: A Generative XAI Framework for Natural Language Inference\n  Explanations", "abstract": "XAI with natural language processing aims to produce human-readable\nexplanations as evidence for AI decision-making, which addresses explainability\nand transparency. However, from an HCI perspective, the current approaches only\nfocus on delivering a single explanation, which fails to account for the\ndiversity of human thoughts and experiences in language. This paper thus\naddresses this gap, by proposing a generative XAI framework, INTERACTION\n(explaIn aNd predicT thEn queRy with contextuAl CondiTional varIational\nautO-eNcoder). Our novel framework presents explanation in two steps: (step\none) Explanation and Label Prediction; and (step two) Diverse Evidence\nGeneration. We conduct intensive experiments with the Transformer architecture\non a benchmark dataset, e-SNLI. Our method achieves competitive or better\nperformance against state-of-the-art baseline models on explanation generation\n(up to 4.7% gain in BLEU) and prediction (up to 4.4% gain in accuracy) in step\none; it can also generate multiple diverse explanations in step two.", "published": "2022-09-02 13:52:39", "link": "http://arxiv.org/abs/2209.01061v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Domain Adaptation from Scratch", "abstract": "Natural language processing (NLP) algorithms are rapidly improving but often\nstruggle when applied to out-of-distribution examples. A prominent approach to\nmitigate the domain gap is domain adaptation, where a model trained on a source\ndomain is adapted to a new target domain. We present a new learning setup,\n``domain adaptation from scratch'', which we believe to be crucial for\nextending the reach of NLP to sensitive domains in a privacy-preserving manner.\nIn this setup, we aim to efficiently annotate data from a set of source domains\nsuch that the trained model performs well on a sensitive target domain from\nwhich data is unavailable for annotation. Our study compares several approaches\nfor this challenging setup, ranging from data selection and domain adaptation\nalgorithms to active learning paradigms, on two NLP tasks: sentiment analysis\nand Named Entity Recognition. Our results suggest that using the abovementioned\napproaches eases the domain gap, and combining them further improves the\nresults.", "published": "2022-09-02 05:55:09", "link": "http://arxiv.org/abs/2209.00830v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multi-modal Contrastive Representation Learning for Entity Alignment", "abstract": "Multi-modal entity alignment aims to identify equivalent entities between two\ndifferent multi-modal knowledge graphs, which consist of structural triples and\nimages associated with entities. Most previous works focus on how to utilize\nand encode information from different modalities, while it is not trivial to\nleverage multi-modal knowledge in entity alignment because of the modality\nheterogeneity. In this paper, we propose MCLEA, a Multi-modal Contrastive\nLearning based Entity Alignment model, to obtain effective joint\nrepresentations for multi-modal entity alignment. Different from previous\nworks, MCLEA considers task-oriented modality and models the inter-modal\nrelationships for each entity representation. In particular, MCLEA firstly\nlearns multiple individual representations from multiple modalities, and then\nperforms contrastive learning to jointly model intra-modal and inter-modal\ninteractions. Extensive experimental results show that MCLEA outperforms\nstate-of-the-art baselines on public datasets under both supervised and\nunsupervised settings.", "published": "2022-09-02 08:59:57", "link": "http://arxiv.org/abs/2209.00891v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Mind the Gap! Injecting Commonsense Knowledge for Abstractive Dialogue\n  Summarization", "abstract": "In this paper, we propose to leverage the unique characteristics of dialogues\nsharing commonsense knowledge across participants, to resolve the difficulties\nin summarizing them. We present SICK, a framework that uses commonsense\ninferences as additional context. Compared to previous work that solely relies\non the input dialogue, SICK uses an external knowledge model to generate a rich\nset of commonsense inferences and selects the most probable one with a\nsimilarity-based selection method. Built upon SICK, SICK++ utilizes commonsense\nas supervision, where the task of generating commonsense inferences is added\nupon summarizing the dialogue in a multi-task learning setting. Experimental\nresults show that with injected commonsense knowledge, our framework generates\nmore informative and consistent summaries than existing methods.", "published": "2022-09-02 10:08:28", "link": "http://arxiv.org/abs/2209.00930v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Entity Graph Extraction from Legal Acts -- a Prototype for a Use Case in\n  Policy Design Analysis", "abstract": "This paper presents research on a prototype developed to serve the\nquantitative study of public policy design. This sub-discipline of political\nscience focuses on identifying actors, relations between them, and tools at\ntheir disposal in health, environmental, economic, and other policies. Our\nsystem aims to automate the process of gathering legal documents, annotating\nthem with Institutional Grammar, and using hypergraphs to analyse\ninter-relations between crucial entities. Our system is tested against the\nUNESCO Convention for the Safeguarding of the Intangible Cultural Heritage from\n2003, a legal document regulating essential aspects of international relations\nsecuring cultural heritage.", "published": "2022-09-02 10:57:47", "link": "http://arxiv.org/abs/2209.00944v1", "categories": ["cs.CL", "cs.HC", "cs.IR", "68U35"], "primary_category": "cs.CL"}
{"title": "\"More Than Words\": Linking Music Preferences and Moral Values Through\n  Lyrics", "abstract": "This study explores the association between music preferences and moral\nvalues by applying text analysis techniques to lyrics. Harvesting data from a\nFacebook-hosted application, we align psychometric scores of 1,386 users to\nlyrics from the top 5 songs of their preferred music artists as emerged from\nFacebook Page Likes. We extract a set of lyrical features related to each\nsong's overarching narrative, moral valence, sentiment, and emotion. A machine\nlearning framework was designed to exploit regression approaches and evaluate\nthe predictive power of lyrical features for inferring moral values. Results\nsuggest that lyrics from top songs of artists people like inform their\nmorality. Virtues of hierarchy and tradition achieve higher prediction scores\n($.20 \\leq r \\leq .30$) than values of empathy and equality ($.08 \\leq r \\leq\n.11$), while basic demographic variables only account for a small part in the\nmodels' explainability. This shows the importance of music listening\nbehaviours, as assessed via lyrical preferences, alone in capturing moral\nvalues. We discuss the technological and musicological implications and\npossible future improvements.", "published": "2022-09-02 16:58:52", "link": "http://arxiv.org/abs/2209.01169v1", "categories": ["cs.CY", "cs.CL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "Improving Contextual Recognition of Rare Words with an Alternate\n  Spelling Prediction Model", "abstract": "Contextual ASR, which takes a list of bias terms as input along with audio,\nhas drawn recent interest as ASR use becomes more widespread. We are releasing\ncontextual biasing lists to accompany the Earnings21 dataset, creating a public\nbenchmark for this task. We present baseline results on this benchmark using a\npretrained end-to-end ASR model from the WeNet toolkit. We show results for\nshallow fusion contextual biasing applied to two different decoding algorithms.\nOur baseline results confirm observations that end-to-end models struggle in\nparticular with words that are rarely or never seen during training, and that\nexisting shallow fusion techniques do not adequately address this problem. We\npropose an alternate spelling prediction model that improves recall of rare\nwords by 34.7% relative and of out-of-vocabulary words by 97.2% relative,\ncompared to contextual biasing without alternate spellings. This model is\nconceptually similar to ones used in prior work, but is simpler to implement as\nit does not rely on either a pronunciation dictionary or an existing\ntext-to-speech system.", "published": "2022-09-02 19:30:16", "link": "http://arxiv.org/abs/2209.01250v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Exploiting Pretrained Biochemical Language Models for Targeted Drug\n  Design", "abstract": "Motivation: The development of novel compounds targeting proteins of interest\nis one of the most important tasks in the pharmaceutical industry. Deep\ngenerative models have been applied to targeted molecular design and have shown\npromising results. Recently, target-specific molecule generation has been\nviewed as a translation between the protein language and the chemical language.\nHowever, such a model is limited by the availability of interacting\nprotein-ligand pairs. On the other hand, large amounts of unlabeled protein\nsequences and chemical compounds are available and have been used to train\nlanguage models that learn useful representations. In this study, we propose\nexploiting pretrained biochemical language models to initialize (i.e. warm\nstart) targeted molecule generation models. We investigate two warm start\nstrategies: (i) a one-stage strategy where the initialized model is trained on\ntargeted molecule generation (ii) a two-stage strategy containing a\npre-finetuning on molecular generation followed by target specific training. We\nalso compare two decoding strategies to generate compounds: beam search and\nsampling.\n  Results: The results show that the warm-started models perform better than a\nbaseline model trained from scratch. The two proposed warm-start strategies\nachieve similar results to each other with respect to widely used metrics from\nbenchmarks. However, docking evaluation of the generated compounds for a number\nof novel proteins suggests that the one-stage strategy generalizes better than\nthe two-stage strategy. Additionally, we observe that beam search outperforms\nsampling in both docking evaluation and benchmark metrics for assessing\ncompound quality.\n  Availability and implementation: The source code is available at\nhttps://github.com/boun-tabi/biochemical-lms-for-drug-design and the materials\nare archived in Zenodo at https://doi.org/10.5281/zenodo.6832145", "published": "2022-09-02 12:21:51", "link": "http://arxiv.org/abs/2209.00981v1", "categories": ["cs.LG", "cs.CL", "q-bio.BM", "q-bio.QM", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Multi-scale temporal-frequency attention for music source separation", "abstract": "In recent years, deep neural networks (DNNs) based approaches have achieved\nthe start-of-the-art performance for music source separation (MSS). Although\nprevious methods have addressed the large receptive field modeling using\nvarious methods, the temporal and frequency correlations of the music\nspectrogram with repeated patterns have not been explicitly explored for the\nMSS task. In this paper, a temporal-frequency attention module is proposed to\nmodel the spectrogram correlations along both temporal and frequency\ndimensions. Moreover, a multi-scale attention is proposed to effectively\ncapture the correlations for music signal. The experimental results on MUSDB18\ndataset show that the proposed method outperforms the existing state-of-the-art\nsystems with 9.51 dB signal-to-distortion ratio (SDR) on separating the vocal\nstems, which is the primary practical application of MSS.", "published": "2022-09-02 03:53:52", "link": "http://arxiv.org/abs/2209.00805v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Inverse-free Online Independent Vector Analysis with Flexible Iterative\n  Source Steering", "abstract": "In this paper, we propose a new online independent vector analysis (IVA)\nalgorithm for real-time blind source separation (BSS). In many BSS algorithms,\nthe iterative projection (IP) has been used for updating the demixing matrix, a\nparameter to be estimated in BSS. However, it requires matrix inversion, which\ncan be costly, particularly in online processing. To improve this situation, we\nintroduce iterative source steering (ISS) to online IVA. ISS does not require\nany matrix inversions, and thus its computational complexity is less than that\nof IP. Furthermore, when only part of the sources are moving, ISS enables us to\nupdate the demixing matrix flexibly and effectively so that the steering\nvectors of only the moving sources are updated. Numerical experiments under a\ndynamic condition confirm the efficacy of the proposed method.", "published": "2022-09-02 10:23:44", "link": "http://arxiv.org/abs/2209.00937v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "TB or not TB? Acoustic cough analysis for tuberculosis classification", "abstract": "In this work, we explore recurrent neural network architectures for\ntuberculosis (TB) cough classification. In contrast to previous unsuccessful\nattempts to implement deep architectures in this domain, we show that a basic\nbidirectional long short-term memory network (BiLSTM) can achieve improved\nperformance. In addition, we show that by performing greedy feature selection\nin conjunction with a newly-proposed attention-based architecture that learns\npatient invariant features, substantially better generalisation can be achieved\ncompared to a baseline and other considered architectures. Furthermore, this\nattention mechanism allows an inspection of the temporal regions of the audio\nsignal considered to be important for classification to be performed. Finally,\nwe develop a neural style transfer technique to infer idealised inputs which\ncan subsequently be analysed. We find distinct differences between the\nidealised power spectra of TB and non-TB coughs, which provide clues about the\norigin of the features in the audio signal.", "published": "2022-09-02 10:17:07", "link": "http://arxiv.org/abs/2209.00934v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
