{"title": "Semi-supervised Thai Sentence Segmentation Using Local and Distant Word\n  Representations", "abstract": "A sentence is typically treated as the minimal syntactic unit used for\nextracting valuable information from a longer piece of text. However, in\nwritten Thai, there are no explicit sentence markers. We proposed a deep\nlearning model for the task of sentence segmentation that includes three main\ncontributions. First, we integrate n-gram embedding as a local representation\nto capture word groups near sentence boundaries. Second, to focus on the\nkeywords of dependent clauses, we combine the model with a distant\nrepresentation obtained from self-attention modules. Finally, due to the\nscarcity of labeled data, for which annotation is difficult and time-consuming,\nwe also investigate and adapt Cross-View Training (CVT) as a semi-supervised\nlearning technique, allowing us to utilize unlabeled data to improve the model\nrepresentations. In the Thai sentence segmentation experiments, our model\nreduced the relative error by 7.4% and 10.5% compared with the baseline models\non the Orchid and UGWC datasets, respectively. We also applied our model to the\ntask of pronunciation recovery on the IWSLT English dataset. Our model\noutperformed the prior sequence tagging models, achieving a relative error\nreduction of 2.5%. Ablation studies revealed that utilizing n-gram\npresentations was the main contributing factor for Thai, while the\nsemi-supervised training helped the most for English.", "published": "2019-08-04 08:24:05", "link": "http://arxiv.org/abs/1908.01294v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Separating Argument Structure from Logical Structure in AMR", "abstract": "The AMR (Abstract Meaning Representation) formalism for representing meaning\nof natural language sentences was not designed to deal with scope and\nquantifiers. By extending AMR with indices for contexts and formulating\nconstraints on these contexts, a formalism is derived that makes correct\nprediction for inferences involving negation and bound variables. The\nattractive core predicate-argument structure of AMR is preserved. The resulting\nframework is similar to that of Discourse Representation Theory.", "published": "2019-08-04 14:46:35", "link": "http://arxiv.org/abs/1908.01355v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Fact-Checking Using Context and Discourse Information", "abstract": "We study the problem of automatic fact-checking, paying special attention to\nthe impact of contextual and discourse information. We address two related\ntasks: (i) detecting check-worthy claims, and (ii) fact-checking claims. We\ndevelop supervised systems based on neural networks, kernel-based support\nvector machines, and combinations thereof, which make use of rich input\nrepresentations in terms of discourse cues and contextual features. For the\ncheck-worthiness estimation task, we focus on political debates, and we model\nthe target claim in the context of the full intervention of a participant and\nthe previous and the following turns in the debate, taking into account\ncontextual meta information. For the fact-checking task, we focus on answer\nverification in a community forum, and we model the veracity of the answer with\nrespect to the entire question--answer thread in which it occurs as well as\nwith respect to other related posts from the entire forum. We develop annotated\ndatasets for both tasks and we run extensive experimental evaluation,\nconfirming that both types of information ---but especially contextual\nfeatures--- play an important role.", "published": "2019-08-04 12:40:28", "link": "http://arxiv.org/abs/1908.01328v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring Neural Net Augmentation to BERT for Question Answering on\n  SQUAD 2.0", "abstract": "Enhancing machine capabilities to answer questions has been a topic of\nconsiderable focus in recent years of NLP research. Language models like\nEmbeddings from Language Models (ELMo)[1] and Bidirectional Encoder\nRepresentations from Transformers (BERT) [2] have been very successful in\ndeveloping general purpose language models that can be optimized for a large\nnumber of downstream language tasks. In this work, we focused on augmenting the\npre-trained BERT language model with different output neural net architectures\nand compared their performance on question answering task posed by the Stanford\nQuestion Answering Dataset 2.0 (SQUAD 2.0) [3]. Additionally, we also\nfine-tuned the pre-trained BERT model parameters to demonstrate its\neffectiveness in adapting to specialized language tasks. Our best output\nnetwork, is the contextualized CNN that performs on both the unanswerable and\nanswerable question answering tasks with F1 scores of 75.32 and 64.85\nrespectively.", "published": "2019-08-04 16:48:24", "link": "http://arxiv.org/abs/1908.01767v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sound Event Detection in Multichannel Audio using Convolutional\n  Time-Frequency-Channel Squeeze and Excitation", "abstract": "In this study, we introduce a convolutional time-frequency-channel \"Squeeze\nand Excitation\" (tfc-SE) module to explicitly model inter-dependencies between\nthe time-frequency domain and multiple channels. The tfc-SE module consists of\ntwo parts: tf-SE block and c-SE block which are designed to provide attention\non time-frequency and channel domain, respectively, for adaptively\nrecalibrating the input feature map. The proposed tfc-SE module, together with\na popular Convolutional Recurrent Neural Network (CRNN) model, are evaluated on\na multi-channel sound event detection task with overlapping audio sources: the\ntraining and test data are synthesized TUT Sound Events 2018 datasets, recorded\nwith microphone arrays. We show that the tfc-SE module can be incorporated into\nthe CRNN model at a small additional computational cost and bring significant\nimprovements on sound event detection accuracy. We also perform detailed\nablation studies by analyzing various factors that may influence the\nperformance of the SE blocks. We show that with the best tfc-SE block, error\nrate (ER) decreases from 0.2538 to 0.2026, relative 20.17\\% reduction of ER,\nand 5.72\\% improvement of F1 score. The results indicate that the learned\nacoustic embeddings with the tfc-SE module efficiently strengthen\ntime-frequency and channel-wise feature representations to improve the\ndiscriminative performance.", "published": "2019-08-04 20:58:52", "link": "http://arxiv.org/abs/1908.01399v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Probabilistic Permutation Invariant Training for Speech Separation", "abstract": "Single-microphone, speaker-independent speech separation is normally\nperformed through two steps: (i) separating the specific speech sources, and\n(ii) determining the best output-label assignment to find the separation error.\nThe second step is the main obstacle in training neural networks for speech\nseparation. Recently proposed Permutation Invariant Training (PIT) addresses\nthis problem by determining the output-label assignment which minimizes the\nseparation error. In this study, we show that a major drawback of this\ntechnique is the overconfident choice of the output-label assignment,\nespecially in the initial steps of training when the network generates\nunreliable outputs. To solve this problem, we propose Probabilistic PIT\n(Prob-PIT) which considers the output-label permutation as a discrete latent\nrandom variable with a uniform prior distribution. Prob-PIT defines a\nlog-likelihood function based on the prior distributions and the separation\nerrors of all permutations; it trains the speech separation networks by\nmaximizing the log-likelihood function. Prob-PIT can be easily implemented by\nreplacing the minimum function of PIT with a soft-minimum function. We evaluate\nour approach for speech separation on both TIMIT and CHiME datasets. The\nresults show that the proposed method significantly outperforms PIT in terms of\nSignal to Distortion Ratio and Signal to Interference Ratio.", "published": "2019-08-04 17:42:31", "link": "http://arxiv.org/abs/1908.01768v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
