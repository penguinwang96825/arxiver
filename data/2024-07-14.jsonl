{"title": "Reinforcement Learning in High-frequency Market Making", "abstract": "This paper establishes a new and comprehensive theoretical analysis for the\napplication of reinforcement learning (RL) in high-frequency market making. We\nbridge the modern RL theory and the continuous-time statistical models in\nhigh-frequency financial economics. Different with most existing literature on\nmethodological research about developing various RL methods for market making\nproblem, our work is a pilot to provide the theoretical analysis. We target the\neffects of sampling frequency, and find an interesting tradeoff between error\nand complexity of RL algorithm when tweaking the values of the time increment\n$\\Delta$ $-$ as $\\Delta$ becomes smaller, the error will be smaller but the\ncomplexity will be larger. We also study the two-player case under the\ngeneral-sum game framework and establish the convergence of Nash equilibrium to\nthe continuous-time game equilibrium as $\\Delta\\rightarrow0$. The Nash\nQ-learning algorithm, which is an online multi-agent RL method, is applied to\nsolve the equilibrium. Our theories are not only useful for practitioners to\nchoose the sampling frequency, but also very general and applicable to other\nhigh-frequency financial decision making problems, e.g., optimal executions, as\nlong as the time-discretization of a continuous-time markov decision process is\nadopted. Monte Carlo simulation evidence support all of our theories.", "published": "2024-07-14 22:07:48", "link": "http://arxiv.org/abs/2407.21025v2", "categories": ["q-fin.TR", "cs.LG", "econ.EM", "q-fin.ST", "stat.ML"], "primary_category": "q-fin.TR"}
{"title": "Multi-Granularity Semantic Revision for Large Language Model\n  Distillation", "abstract": "Knowledge distillation plays a key role in compressing the Large Language\nModels (LLMs), which boosts a small-size student model under large teacher\nmodels' guidance. However, existing LLM distillation methods overly rely on\nstudent-generated outputs, which may introduce generation errors and misguide\nthe distillation process. Moreover, the distillation loss functions introduced\nin previous art struggle to align the most informative part due to the complex\ndistribution of LLMs' outputs. To address these problems, we propose a\nmulti-granularity semantic revision method for LLM distillation. At the\nsequence level, we propose a sequence correction and re-generation (SCRG)\nstrategy. SCRG first calculates the semantic cognitive difference between the\nteacher and student to detect the error token, then corrects it with the\nteacher-generated one, and re-generates the sequence to reduce generation\nerrors and enhance generation diversity. At the token level, we design a\ndistribution adaptive clipping Kullback-Leibler (DAC-KL) loss as the\ndistillation objective function. DAC-KL loss exploits a learnable sub-network\nto adaptively extract semantically dense areas from the teacher's output,\navoiding the interference of redundant information in the distillation process.\nFinally, at the span level, we leverage the span priors of a sequence to\ncompute the probability correlations within spans, and constrain the teacher\nand student's probability correlations to be consistent, further enhancing the\ntransfer of semantic information. Extensive experiments across different model\nfamilies with parameters ranging from 0.1B to 13B demonstrate the superiority\nof our method compared to existing methods.", "published": "2024-07-14 03:51:49", "link": "http://arxiv.org/abs/2407.10068v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Emotion Prediction in News Headlines: Insights from ChatGPT\n  and Seq2Seq Models for Free-Text Generation", "abstract": "Predicting emotions elicited by news headlines can be challenging as the task\nis largely influenced by the varying nature of people's interpretations and\nbackgrounds. Previous works have explored classifying discrete emotions\ndirectly from news headlines. We provide a different approach to tackling this\nproblem by utilizing people's explanations of their emotion, written in\nfree-text, on how they feel after reading a news headline. Using the dataset\nBU-NEmo+ (Gao et al., 2022), we found that for emotion classification, the\nfree-text explanations have a strong correlation with the dominant emotion\nelicited by the headlines. The free-text explanations also contain more\nsentimental context than the news headlines alone and can serve as a better\ninput to emotion classification models. Therefore, in this work we explored\ngenerating emotion explanations from headlines by training a\nsequence-to-sequence transformer model and by using pretrained large language\nmodel, ChatGPT (GPT-4). We then used the generated emotion explanations for\nemotion classification. In addition, we also experimented with training the\npretrained T5 model for the intermediate task of explanation generation before\nfine-tuning it for emotion classification. Using McNemar's significance test,\nmethods that incorporate GPT-generated free-text emotion explanations\ndemonstrated significant improvement (P-value < 0.05) in emotion classification\nfrom headlines, compared to methods that only use headlines. This underscores\nthe value of using intermediate free-text explanations for emotion prediction\ntasks with headlines.", "published": "2024-07-14 06:04:11", "link": "http://arxiv.org/abs/2407.10091v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "TokenSHAP: Interpreting Large Language Models with Monte Carlo Shapley\n  Value Estimation", "abstract": "As large language models (LLMs) become increasingly prevalent in critical\napplications, the need for interpretable AI has grown. We introduce TokenSHAP,\na novel method for interpreting LLMs by attributing importance to individual\ntokens or substrings within input prompts. This approach adapts Shapley values\nfrom cooperative game theory to natural language processing, offering a\nrigorous framework for understanding how different parts of an input contribute\nto a model's response. TokenSHAP leverages Monte Carlo sampling for\ncomputational efficiency, providing interpretable, quantitative measures of\ntoken importance. We demonstrate its efficacy across diverse prompts and LLM\narchitectures, showing consistent improvements over existing baselines in\nalignment with human judgments, faithfulness to model behavior, and\nconsistency.\n  Our method's ability to capture nuanced interactions between tokens provides\nvaluable insights into LLM behavior, enhancing model transparency, improving\nprompt engineering, and aiding in the development of more reliable AI systems.\nTokenSHAP represents a significant step towards the necessary interpretability\nfor responsible AI deployment, contributing to the broader goal of creating\nmore transparent, accountable, and trustworthy AI systems.", "published": "2024-07-14 08:07:50", "link": "http://arxiv.org/abs/2407.10114v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Textless Dependency Parsing by Labeled Sequence Prediction", "abstract": "Traditional spoken language processing involves cascading an automatic speech\nrecognition (ASR) system into text processing models. In contrast, \"textless\"\nmethods process speech representations without ASR systems, enabling the direct\nuse of acoustic speech features. Although their effectiveness is shown in\ncapturing acoustic features, it is unclear in capturing lexical knowledge. This\npaper proposes a textless method for dependency parsing, examining its\neffectiveness and limitations. Our proposed method predicts a dependency tree\nfrom a speech signal without transcribing, representing the tree as a labeled\nsequence. scading method outperforms the textless method in overall parsing\naccuracy, the latter excels in instances with important acoustic features. Our\nfindings highlight the importance of fusing word-level representations and\nsentence-level prosody for enhanced parsing performance. The code and models\nare made publicly available: https://github.com/mynlp/SpeechParser.", "published": "2024-07-14 08:38:14", "link": "http://arxiv.org/abs/2407.10118v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mitigating Translationese in Low-resource Languages: The Storyboard\n  Approach", "abstract": "Low-resource languages often face challenges in acquiring high-quality\nlanguage data due to the reliance on translation-based methods, which can\nintroduce the translationese effect. This phenomenon results in translated\nsentences that lack fluency and naturalness in the target language. In this\npaper, we propose a novel approach for data collection by leveraging\nstoryboards to elicit more fluent and natural sentences. Our method involves\npresenting native speakers with visual stimuli in the form of storyboards and\ncollecting their descriptions without direct exposure to the source text. We\nconducted a comprehensive evaluation comparing our storyboard-based approach\nwith traditional text translation-based methods in terms of accuracy and\nfluency. Human annotators and quantitative metrics were used to assess\ntranslation quality. The results indicate a preference for text translation in\nterms of accuracy, while our method demonstrates worse accuracy but better\nfluency in the language focused.", "published": "2024-07-14 10:47:03", "link": "http://arxiv.org/abs/2407.10152v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "BiasAlert: A Plug-and-play Tool for Social Bias Detection in LLMs", "abstract": "Evaluating the bias in Large Language Models (LLMs) becomes increasingly\ncrucial with their rapid development. However, existing evaluation methods rely\non fixed-form outputs and cannot adapt to the flexible open-text generation\nscenarios of LLMs (e.g., sentence completion and question answering). To\naddress this, we introduce BiasAlert, a plug-and-play tool designed to detect\nsocial bias in open-text generations of LLMs. BiasAlert integrates external\nhuman knowledge with inherent reasoning capabilities to detect bias reliably.\nExtensive experiments demonstrate that BiasAlert significantly outperforms\nexisting state-of-the-art methods like GPT4-as-A-Judge in detecting bias.\nFurthermore, through application studies, we demonstrate the utility of\nBiasAlert in reliable LLM bias evaluation and bias mitigation across various\nscenarios. Model and code will be publicly released.", "published": "2024-07-14 15:17:02", "link": "http://arxiv.org/abs/2407.10241v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Nullpointer at CheckThat! 2024: Identifying Subjectivity from\n  Multilingual Text Sequence", "abstract": "This study addresses a binary classification task to determine whether a text\nsequence, either a sentence or paragraph, is subjective or objective. The task\nspans five languages: Arabic, Bulgarian, English, German, and Italian, along\nwith a multilingual category. Our approach involved several key techniques.\nInitially, we preprocessed the data through parts of speech (POS) tagging,\nidentification of question marks, and application of attention masks. We\nfine-tuned the sentiment-based Transformer model\n'MarieAngeA13/Sentiment-Analysis-BERT' on our dataset. Given the imbalance with\nmore objective data, we implemented a custom classifier that assigned greater\nweight to objective data. Additionally, we translated non-English data into\nEnglish to maintain consistency across the dataset. Our model achieved notable\nresults, scoring top marks for the multilingual dataset (Macro F1=0.7121) and\nGerman (Macro F1=0.7908). It ranked second for Arabic (Macro F1=0.4908) and\nBulgarian (Macro F1=0.7169), third for Italian (Macro F1=0.7430), and ninth for\nEnglish (Macro F1=0.6893).", "published": "2024-07-14 15:37:28", "link": "http://arxiv.org/abs/2407.10252v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Does Burrows' Delta really confirm that Rowling and Galbraith are the\n  same author?", "abstract": "The stylo package includes a frequency table that can be used to calculate\ndistances between texts and thus independently solve the problem of attribution\nof The Cuckoo's Calling, a novel that J.K. Rowling said she wrote. However, the\nset of texts for this table is very vulnerable to criticism. The authors there\nare not modern, they wrote in a different genre. I set out to test the\nperformance of the method on texts that are more relevant to the research\nquestion.", "published": "2024-07-14 19:28:48", "link": "http://arxiv.org/abs/2407.10301v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Long-Range Dependency with State Space Model and\n  Kolmogorov-Arnold Networks for Aspect-Based Sentiment Analysis", "abstract": "Aspect-based Sentiment Analysis (ABSA) evaluates sentiments toward specific\naspects of entities within the text. However, attention mechanisms and neural\nnetwork models struggle with syntactic constraints. The quadratic complexity of\nattention mechanisms also limits their adoption for capturing long-range\ndependencies between aspect and opinion words in ABSA. This complexity can lead\nto the misinterpretation of irrelevant contextual words, restricting their\neffectiveness to short-range dependencies. To address the above problem, we\npresent a novel approach to enhance long-range dependencies between aspect and\nopinion words in ABSA (MambaForGCN). This approach incorporates syntax-based\nGraph Convolutional Network (SynGCN) and MambaFormer (Mamba-Transformer)\nmodules to encode input with dependency relations and semantic information. The\nMultihead Attention (MHA) and Selective State Space model (Mamba) blocks in the\nMambaFormer module serve as channels to enhance the model with short and\nlong-range dependencies between aspect and opinion words. We also introduce the\nKolmogorov-Arnold Networks (KANs) gated fusion, an adaptive feature\nrepresentation system that integrates SynGCN and MambaFormer and captures\nnon-linear, complex dependencies. Experimental results on three benchmark\ndatasets demonstrate MambaForGCN's effectiveness, outperforming\nstate-of-the-art (SOTA) baseline models.", "published": "2024-07-14 22:23:07", "link": "http://arxiv.org/abs/2407.10347v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparing Complex Concepts with Transformers: Matching Patent Claims\n  Against Natural Language Text", "abstract": "A key capability in managing patent applications or a patent portfolio is\ncomparing claims to other text, e.g. a patent specification. Because the\nlanguage of claims is different from language used elsewhere in the patent\napplication or in non-patent text, this has been challenging for computer based\nnatural language processing. We test two new LLM-based approaches and find that\nboth provide substantially better performance than previously published values.\nThe ability to match dense information from one domain against much more\ndistributed information expressed in a different vocabulary may also be useful\nbeyond the intellectual property space.", "published": "2024-07-14 22:31:07", "link": "http://arxiv.org/abs/2407.10351v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "AutoGRAMS: Autonomous Graphical Agent Modeling Software", "abstract": "We introduce the AutoGRAMS framework for programming multi-step interactions\nwith language models. AutoGRAMS represents AI agents as a graph, where each\nnode can execute either a language modeling instruction or traditional code.\nLikewise, transitions in the graph can be governed by either language modeling\ndecisions or traditional branch logic. AutoGRAMS supports using variables as\nmemory and allows nodes to call other AutoGRAMS graphs as functions. We show\nhow AutoGRAMS can be used to design highly sophisticated agents, including\nself-referential agents that can modify their own graph. AutoGRAMS's\ngraph-centric approach aids interpretability, controllability, and safety\nduring the design, development, and deployment of AI agents. We provide our\nframework as open source at https://github.com/autograms/autograms .", "published": "2024-07-14 02:25:45", "link": "http://arxiv.org/abs/2407.10049v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning to Refuse: Towards Mitigating Privacy Risks in LLMs", "abstract": "Large language models (LLMs) exhibit remarkable capabilities in understanding\nand generating natural language. However, these models can inadvertently\nmemorize private information, posing significant privacy risks. This study\naddresses the challenge of enabling LLMs to protect specific individuals'\nprivate data without the need for complete retraining. We propose \\return, a\nReal-world pErsonal daTa UnleaRNing dataset, comprising 2,492 individuals from\nWikipedia with associated QA pairs, to evaluate machine unlearning (MU) methods\nfor protecting personal data in a realistic scenario. Additionally, we\nintroduce the Name-Aware Unlearning Framework (NAUF) for Privacy Protection,\nwhich enables the model to learn which individuals' information should be\nprotected without affecting its ability to answer questions related to other\nunrelated individuals. Our extensive experiments demonstrate that NAUF achieves\na state-of-the-art average unlearning score, surpassing the best baseline\nmethod by 5.65 points, effectively protecting target individuals' personal data\nwhile maintaining the model's general capabilities.", "published": "2024-07-14 03:05:53", "link": "http://arxiv.org/abs/2407.10058v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Rapid Biomedical Research Classification: The Pandemic PACT Advanced\n  Categorisation Engine", "abstract": "This paper introduces the Pandemic PACT Advanced Categorisation Engine\n(PPACE) along with its associated dataset. PPACE is a fine-tuned model\ndeveloped to automatically classify research abstracts from funded biomedical\nprojects according to WHO-aligned research priorities. This task is crucial for\nmonitoring research trends and identifying gaps in global health preparedness\nand response. Our approach builds on human-annotated projects, which are\nallocated one or more categories from a predefined list. A large language model\nis then used to generate `rationales' explaining the reasoning behind these\nannotations. This augmented data, comprising expert annotations and rationales,\nis subsequently used to fine-tune a smaller, more efficient model. Developed as\npart of the Pandemic PACT project, which aims to track and analyse research\nfunding and clinical evidence for a wide range of diseases with outbreak\npotential, PPACE supports informed decision-making by research funders,\npolicymakers, and independent researchers. We introduce and release both the\ntrained model and the instruction-based dataset used for its training. Our\nevaluation shows that PPACE significantly outperforms its baselines. The\nrelease of PPACE and its associated dataset offers valuable resources for\nresearchers in multilabel biomedical document classification and supports\nadvancements in aligning biomedical research with key global health priorities.", "published": "2024-07-14 05:22:53", "link": "http://arxiv.org/abs/2407.10086v2", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Look Within, Why LLMs Hallucinate: A Causal Perspective", "abstract": "The emergence of large language models (LLMs) is a milestone in generative\nartificial intelligence, achieving significant success in text comprehension\nand generation tasks. Despite the tremendous success of LLMs in many downstream\ntasks, they suffer from severe hallucination problems, posing significant\nchallenges to the practical applications of LLMs. Most of the works about LLMs'\nhallucinations focus on data quality. Self-attention is a core module in\ntransformer-based LLMs, while its potential relationship with LLMs'\nhallucination has been hardly investigated. To fill this gap, we study this\nproblem from a causal perspective. We propose a method to intervene in LLMs'\nself-attention layers and maintain their structures and sizes intact.\nSpecifically, we disable different self-attention layers in several popular\nopen-source LLMs and then compare their degrees of hallucination with the\noriginal ones. We evaluate the intervened LLMs on hallucination assessment\nbenchmarks and conclude that disabling some specific self-attention layers in\nthe front or tail of the LLMs can alleviate hallucination issues. The study\npaves a new way for understanding and mitigating LLMs' hallucinations.", "published": "2024-07-14 10:47:44", "link": "http://arxiv.org/abs/2407.10153v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Key-Point-Driven Mathematical Reasoning Distillation of Large Language\n  Model", "abstract": "Large Language Models (LLMs) have demonstrated exceptional proficiency in\nmathematical reasoning tasks due to their extensive parameter counts and\ntraining on vast datasets. Despite these capabilities, deploying LLMs is\nhindered by their computational demands. Distilling LLM mathematical reasoning\ninto Smaller Language Models (SLMs) has emerged as a solution to this\nchallenge, although these smaller models often suffer from errors in\ncalculation and semantic understanding. Prior work has proposed\nProgram-of-Thought Distillation (PoTD) to avoid calculation error. To further\naddress semantic understanding errors, we propose Key-Point-Driven Mathematical\nReasoning Distillation (KPDD). KPDD enhances the reasoning performance of SLMs\nby breaking down the problem-solving process into three stages: Core Question\nExtraction, Problem-Solving Information Extraction, and Step-by-Step Solution.\nThis method is further divided into KPDD-CoT, which generates Chain-of-Thought\nrationales, and KPDD-PoT, which creates Program-of-Thought rationales. The\nexperiment results show that KPDD-CoT significantly improves reasoning\nabilities, while KPDD-PoT achieves state-of-the-art performance in mathematical\nreasoning tasks. Our approach effectively mitigates misunderstanding errors,\nadvancing the deployment of efficient and capable SLMs.", "published": "2024-07-14 11:41:03", "link": "http://arxiv.org/abs/2407.10167v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GenSco: Can Question Decomposition based Passage Alignment improve\n  Question Answering?", "abstract": "Retrieval augmented generation (RAG) with large language models (LLMs) for\nQuestion Answering (QA) entails furnishing relevant context within the prompt\nto facilitate the LLM in answer generation. During the generation, inaccuracies\nor hallucinations frequently occur due to two primary factors: inadequate or\ndistracting context in the prompts, and the inability of LLMs to effectively\nreason through the facts. In this paper, we investigate whether providing\naligned context via a carefully selected passage sequence leads to better\nanswer generation by the LLM for multi-hop QA. We introduce, \"GenSco\", a novel\napproach of selecting passages based on the predicted decomposition of the\nmulti-hop questions}. The framework consists of two distinct LLMs: (i)\nGenerator LLM, which is used for question decomposition and final answer\ngeneration; (ii) an auxiliary open-sourced LLM, used as the scorer, to\nsemantically guide the Generator for passage selection. The generator is\ninvoked only once for the answer generation, resulting in a cost-effective and\nefficient approach. We evaluate on three broadly established multi-hop question\nanswering datasets: 2WikiMultiHop, Adversarial HotPotQA and MuSiQue and achieve\nan absolute gain of $15.1$ and $5.9$ points in Exact Match score with respect\nto the best performing baselines over MuSiQue and 2WikiMultiHop respectively.", "published": "2024-07-14 15:25:08", "link": "http://arxiv.org/abs/2407.10245v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "What Makes and Breaks Safety Fine-tuning? A Mechanistic Study", "abstract": "Safety fine-tuning helps align Large Language Models (LLMs) with human\npreferences for their safe deployment. To better understand the underlying\nfactors that make models safe via safety fine-tuning, we design a synthetic\ndata generation framework that captures salient aspects of an unsafe input by\nmodeling the interaction between the task the model is asked to perform (e.g.,\n\"design\") versus the specific concepts the task is asked to be performed upon\n(e.g., a \"cycle\" vs. a \"bomb\"). Using this, we investigate three well-known\nsafety fine-tuning methods -- supervised safety fine-tuning, direct preference\noptimization, and unlearning -- and provide significant evidence demonstrating\nthat these methods minimally transform MLP weights to specifically align unsafe\ninputs into its weights' null space. This yields a clustering of inputs based\non whether the model deems them safe or not. Correspondingly, when an\nadversarial input (e.g., a jailbreak) is provided, its activations are closer\nto safer samples, leading to the model processing such an input as if it were\nsafe. We validate our findings, wherever possible, on real-world models --\nspecifically, Llama-2 7B and Llama-3 8B.", "published": "2024-07-14 16:12:57", "link": "http://arxiv.org/abs/2407.10264v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "psifx -- Psychological and Social Interactions Feature Extraction\n  Package", "abstract": "psifx is a plug-and-play multi-modal feature extraction toolkit, aiming to\nfacilitate and democratize the use of state-of-the-art machine learning\ntechniques for human sciences research. It is motivated by a need (a) to\nautomate and standardize data annotation processes, otherwise involving\nexpensive, lengthy, and inconsistent human labor, such as the transcription or\ncoding of behavior changes from audio and video sources; (b) to develop and\ndistribute open-source community-driven psychology research software; and (c)\nto enable large-scale access and ease of use to non-expert users. The framework\ncontains an array of tools for tasks, such as speaker diarization,\nclosed-caption transcription and translation from audio, as well as body, hand,\nand facial pose estimation and gaze tracking from video. The package has been\ndesigned with a modular and task-oriented approach, enabling the community to\nadd or update new tools easily. We strongly hope that this package will provide\npsychologists a simple and practical solution for efficiently a range of audio,\nlinguistic, and visual features from audio and video, thereby creating new\nopportunities for in-depth study of real-time behavioral phenomena.", "published": "2024-07-14 16:20:42", "link": "http://arxiv.org/abs/2407.10266v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cross-Lingual Multi-Hop Knowledge Editing", "abstract": "Large language models are often expected to constantly adapt to new sources\nof knowledge and knowledge editing techniques aim to efficiently patch the\noutdated model knowledge, with minimal modification. Most prior works focus on\nmonolingual knowledge editing in English, even though new information can\nemerge in any language from any part of the world. We propose the Cross-Lingual\nMulti-Hop Knowledge Editing paradigm, for measuring and analyzing the\nperformance of various SoTA knowledge editing techniques in a cross-lingual\nsetup. Specifically, we create a parallel cross-lingual benchmark,\nCROLIN-MQUAKE for measuring the knowledge editing capabilities. Our extensive\nanalysis over various knowledge editing techniques uncover significant gaps in\nperformance between the cross-lingual and English-centric setting. Following\nthis, we propose a significantly improved system for cross-lingual multi-hop\nknowledge editing, CLEVER-CKE. CLEVER-CKE is based on a retrieve, verify and\ngenerate knowledge editing framework, where a retriever is formulated to recall\nedited facts and support an LLM to adhere to knowledge edits. We develop\nlanguage-aware and hard-negative based contrastive objectives for improving the\ncross-lingual and fine-grained fact retrieval and verification process used in\nthis framework. Extensive experiments on three LLMs, eight languages, and two\ndatasets show CLEVER-CKE's significant gains of up to 30% over prior methods.", "published": "2024-07-14 17:18:16", "link": "http://arxiv.org/abs/2407.10275v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Neural Biasing for Contextual Speech Recognition by Early\n  Context Injection and Text Perturbation", "abstract": "Existing research suggests that automatic speech recognition (ASR) models can\nbenefit from additional contexts (e.g., contact lists, user specified\nvocabulary). Rare words and named entities can be better recognized with\ncontexts. In this work, we propose two simple yet effective techniques to\nimprove context-aware ASR models. First, we inject contexts into the encoders\nat an early stage instead of merely at their last layers. Second, to enforce\nthe model to leverage the contexts during training, we perturb the reference\ntranscription with alternative spellings so that the model learns to rely on\nthe contexts to make correct predictions. On LibriSpeech, our techniques\ntogether reduce the rare word error rate by 60% and 25% relatively compared to\nno biasing and shallow fusion, making the new state-of-the-art performance. On\nSPGISpeech and a real-world dataset ConEC, our techniques also yield good\nimprovements over the baselines.", "published": "2024-07-14 19:32:33", "link": "http://arxiv.org/abs/2407.10303v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "The feasibility of sound zone control using an array of parametric array\n  loudspeakers", "abstract": "Parametric array loudspeakers (PALs) are known for producing highly\ndirectional audio beams, a feat more challenging to achieve with conventional\nelectro-dynamic loudspeakers (EDLs). Due to their intrinsic physical\nmechanisms, PALs hold promising potential for spatial audio applications such\nas virtual reality (VR). However, the feasibility of using an array of PALs for\nsound zone control (SZC) has remained unexplored, mainly due to the complexity\nof the nonlinear demodulation process inherent in PALs. Leveraging recent\nadvancements in PAL modeling, this work proposes an optimization algorithm to\nachieve the acoustic contrast control (ACC) between two target areas using a\nPAL array. The performance and robustness of the proposed ACC-based SZC using\nPAL arrays are investigated through simulations, and the results are compared\nwith those obtained using EDL arrays. The results show that the PAL array\noutperforms the EDL array in SZC performance and robustness at higher\nfrequencies and lower signal-to-noise ratio, while being comparable under other\nconditions. This work paves the way for high-contrast acoustic control using\nPAL arrays.", "published": "2024-07-14 02:49:50", "link": "http://arxiv.org/abs/2407.10054v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Whisper-SV: Adapting Whisper for Low-data-resource Speaker Verification", "abstract": "Trained on 680,000 hours of massive speech data, Whisper is a multitasking,\nmultilingual speech foundation model demonstrating superior performance in\nautomatic speech recognition, translation, and language identification.\nHowever, its applicability in speaker verification (SV) tasks remains\nunexplored, particularly in low-data-resource scenarios where labeled speaker\ndata in specific domains are limited. To fill this gap, we propose a\nlightweight adaptor framework to boost SV with Whisper, namely Whisper-SV.\nGiven that Whisper is not specifically optimized for SV tasks, we introduce a\nrepresentation selection module to quantify the speaker-specific\ncharacteristics contained in each layer of Whisper and select the top-k layers\nwith prominent discriminative speaker features. To aggregate pivotal\nspeaker-related features while diminishing non-speaker redundancies across the\nselected top-k distinct layers of Whisper, we design a multi-layer aggregation\nmodule in Whisper-SV to integrate multi-layer representations into a singular,\ncompacted representation for SV. In the multi-layer aggregation module, we\nemploy convolutional layers with shortcut connections among different layers to\nrefine speaker characteristics derived from multi-layer representations from\nWhisper. In addition, an attention aggregation layer is used to reduce\nnon-speaker interference and amplify speaker-specific cues for SV tasks.\nFinally, a simple classification module is used for speaker classification.\nExperiments on VoxCeleb1, FFSVC, and IMSV datasets demonstrate that Whisper-SV\nachieves EER/minDCF of 2.22%/0.307, 6.14%/0.488, and 7.50%/0.582, respectively,\nshowing superior performance in low-data-resource SV scenarios.", "published": "2024-07-14 02:21:52", "link": "http://arxiv.org/abs/2407.10048v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Advancing Continual Learning for Robust Deepfake Audio Classification", "abstract": "The emergence of new spoofing attacks poses an increasing challenge to audio\nsecurity. Current detection methods often falter when faced with unseen\nspoofing attacks. Traditional strategies, such as retraining with new data, are\nnot always feasible due to extensive storage. This paper introduces a novel\ncontinual learning method Continual Audio Defense Enhancer (CADE). First, by\nutilizing a fixed memory size to store randomly selected samples from previous\ndatasets, our approach conserves resources and adheres to privacy constraints.\nAdditionally, we also apply two distillation losses in CADE. By distillation in\nclassifiers, CADE ensures that the student model closely resembles that of the\nteacher model. This resemblance helps the model retain old information while\nfacing unseen data. We further refine our model's performance with a novel\nembedding similarity loss that extends across multiple depth layers,\nfacilitating superior positive sample alignment. Experiments conducted on the\nASVspoof2019 dataset show that our proposed method outperforms the baseline\nmethods.", "published": "2024-07-14 07:32:24", "link": "http://arxiv.org/abs/2407.10108v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Few-Shot Bioacoustic Event Detection with Frame-Level Embedding Learning\n  System", "abstract": "This technical report presents our frame-level embedding learning system for\nthe DCASE2024 challenge for few-shot bioacoustic event detection (Task 5).In\nthis work, we used log-mel and PCEN for feature extraction of the input audio,\nNetmamba Encoder as the information interaction network, and adopted data\naugmentation strategies to improve the generalizability of the trained model as\nwell as multiple post-processing methods. Our final system achieved an\nF-measure score of 56.4%, securing the 2nd rank in the few-shot bioacoustic\nevent detection category of the Detection and Classification of Acoustic Scenes\nand Events Challenge 2024.", "published": "2024-07-14 12:52:35", "link": "http://arxiv.org/abs/2407.10182v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "CUSIDE-T: Chunking, Simulating Future and Decoding for Transducer based\n  Streaming ASR", "abstract": "Streaming automatic speech recognition (ASR) is very important for many\nreal-world ASR applications. However, a notable challenge for streaming ASR\nsystems lies in balancing operational performance against latency constraint.\nRecently, a method of chunking, simulating future context and decoding, called\nCUSIDE, has been proposed for connectionist temporal classification (CTC) based\nstreaming ASR, which obtains a good balance between reduced latency and high\nrecognition accuracy. In this paper, we present CUSIDE-T, which successfully\nadapts the CUSIDE method over the recurrent neural network transducer (RNN-T)\nASR architecture, instead of being based on the CTC architecture. We also\nincorporate language model rescoring in CUSIDE-T to further enhance accuracy,\nwhile only bringing a small additional latency. Extensive experiments are\nconducted over the AISHELL-1, WenetSpeech and SpeechIO datasets, comparing\nCUSIDE-T and U2++ (both based on RNN-T). U2++ is an existing counterpart of\nchunk based streaming ASR method. It is shown that CUSIDE-T achieves superior\naccuracy performance for streaming ASR, with equal settings of latency.", "published": "2024-07-14 15:43:48", "link": "http://arxiv.org/abs/2407.10255v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The Interpretation Gap in Text-to-Music Generation Models", "abstract": "Large-scale text-to-music generation models have significantly enhanced music\ncreation capabilities, offering unprecedented creative freedom. However, their\nability to collaborate effectively with human musicians remains limited. In\nthis paper, we propose a framework to describe the musical interaction process,\nwhich includes expression, interpretation, and execution of controls. Following\nthis framework, we argue that the primary gap between existing text-to-music\nmodels and musicians lies in the interpretation stage, where models lack the\nability to interpret controls from musicians. We also propose two strategies to\naddress this gap and call on the music information retrieval community to\ntackle the interpretation challenge to improve human-AI musical collaboration.", "published": "2024-07-14 20:51:08", "link": "http://arxiv.org/abs/2407.10328v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
