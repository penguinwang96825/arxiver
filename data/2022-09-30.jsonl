{"title": "Language-Family Adapters for Low-Resource Multilingual Neural Machine\n  Translation", "abstract": "Large multilingual models trained with self-supervision achieve\nstate-of-the-art results in a wide range of natural language processing tasks.\nSelf-supervised pretrained models are often fine-tuned on parallel data from\none or multiple language pairs for machine translation. Multilingual\nfine-tuning improves performance on low-resource languages but requires\nmodifying the entire model and can be prohibitively expensive. Training a new\nadapter on each language pair or training a single adapter on all language\npairs without updating the pretrained model has been proposed as a\nparameter-efficient alternative. However, the former does not permit any\nsharing between languages, while the latter shares parameters for all languages\nand is susceptible to negative interference. In this paper, we propose training\nlanguage-family adapters on top of mBART-50 to facilitate cross-lingual\ntransfer. Our approach outperforms related baselines, yielding higher\ntranslation scores on average when translating from English to 17 different\nlow-resource languages. We also show that language-family adapters provide an\neffective method to translate to languages unseen during pretraining.", "published": "2022-09-30 05:02:42", "link": "http://arxiv.org/abs/2209.15236v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "QUAK: A Synthetic Quality Estimation Dataset for Korean-English Neural\n  Machine Translation", "abstract": "With the recent advance in neural machine translation demonstrating its\nimportance, research on quality estimation (QE) has been steadily progressing.\nQE aims to automatically predict the quality of machine translation (MT) output\nwithout reference sentences. Despite its high utility in the real world, there\nremain several limitations concerning manual QE data creation: inevitably\nincurred non-trivial costs due to the need for translation experts, and issues\nwith data scaling and language expansion. To tackle these limitations, we\npresent QUAK, a Korean-English synthetic QE dataset generated in a fully\nautomatic manner. This consists of three sub-QUAK datasets QUAK-M, QUAK-P, and\nQUAK-H, produced through three strategies that are relatively free from\nlanguage constraints. Since each strategy requires no human effort, which\nfacilitates scalability, we scale our data up to 1.58M for QUAK-P, H and 6.58M\nfor QUAK-M. As an experiment, we quantitatively analyze word-level QE results\nin various ways while performing statistical analysis. Moreover, we show that\ndatasets scaled in an efficient way also contribute to performance improvements\nby observing meaningful performance gains in QUAK-M, P when adding data up to\n1.58M.", "published": "2022-09-30 07:47:44", "link": "http://arxiv.org/abs/2209.15285v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Medical Question Understanding and Answering with Knowledge Grounding\n  and Semantic Self-Supervision", "abstract": "Current medical question answering systems have difficulty processing long,\ndetailed and informally worded questions submitted by patients, called Consumer\nHealth Questions (CHQs). To address this issue, we introduce a medical question\nunderstanding and answering system with knowledge grounding and semantic\nself-supervision. Our system is a pipeline that first summarizes a long,\nmedical, user-written question, using a supervised summarization loss. Then,\nour system performs a two-step retrieval to return answers. The system first\nmatches the summarized user question with an FAQ from a trusted medical\nknowledge base, and then retrieves a fixed number of relevant sentences from\nthe corresponding answer document. In the absence of labels for question\nmatching or answer relevance, we design 3 novel, self-supervised and\nsemantically-guided losses. We evaluate our model against two strong\nretrieval-based question answering baselines. Evaluators ask their own\nquestions and rate the answers retrieved by our baselines and own system\naccording to their relevance. They find that our system retrieves more relevant\nanswers, while achieving speeds 20 times faster. Our self-supervised losses\nalso help the summarizer achieve higher scores in ROUGE, as well as in human\nevaluation metrics. We release our code to encourage further research.", "published": "2022-09-30 08:20:32", "link": "http://arxiv.org/abs/2209.15301v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PART: Pre-trained Authorship Representation Transformer", "abstract": "Authors writing documents imprint identifying information within their texts:\nvocabulary, registry, punctuation, misspellings, or even emoji usage. Finding\nthese details is very relevant to profile authors, relating back to their\ngender, occupation, age, and so on. But most importantly, repeating writing\npatterns can help attributing authorship to a text. Previous works use\nhand-crafted features or classification tasks to train their authorship models,\nleading to poor performance on out-of-domain authors. A better approach to this\ntask is to learn stylometric representations, but this by itself is an open\nresearch challenge. In this paper, we propose PART: a contrastively trained\nmodel fit to learn \\textbf{authorship embeddings} instead of semantics. By\ncomparing pairs of documents written by the same author, we are able to\ndetermine the proprietary of a text by evaluating the cosine similarity of the\nevaluated documents, a zero-shot generalization to authorship identification.\nTo this end, a pre-trained Transformer with an LSTM head is trained with the\ncontrastive training method. We train our model on a diverse set of authors,\nfrom literature, anonymous blog posters and corporate emails; a heterogeneous\nset with distinct and identifiable writing styles. The model is evaluated on\nthese datasets, achieving zero-shot 72.39\\% and 86.73\\% accuracy and top-5\naccuracy respectively on the joint evaluation dataset when determining\nauthorship from a set of 250 different authors. We qualitatively assess the\nrepresentations with different data visualizations on the available datasets,\nprofiling features such as book types, gender, age, or occupation of the\nauthor.", "published": "2022-09-30 11:08:39", "link": "http://arxiv.org/abs/2209.15373v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Retrieval with Search Agents and Hybrid Environments", "abstract": "Learning to search is the task of building artificial agents that learn to\nautonomously use a search box to find information. So far, it has been shown\nthat current language models can learn symbolic query reformulation policies,\nin combination with traditional term-based retrieval, but fall short of\noutperforming neural retrievers. We extend the previous learning to search\nsetup to a hybrid environment, which accepts discrete query refinement\noperations, after a first-pass retrieval step via a dual encoder. Experiments\non the BEIR task show that search agents, trained via behavioral cloning,\noutperform the underlying search system based on a combined dual encoder\nretriever and cross encoder reranker. Furthermore, we find that simple\nheuristic Hybrid Retrieval Environments (HRE) can improve baseline performance\nby several nDCG points. The search agent based on HRE (HARE) matches\nstate-of-the-art performance, balanced in both zero-shot and in-domain\nevaluations, via interpretable actions, and at twice the speed.", "published": "2022-09-30 13:50:25", "link": "http://arxiv.org/abs/2209.15469v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Out-of-Distribution Detection and Selective Generation for Conditional\n  Language Models", "abstract": "Machine learning algorithms typically assume independent and identically\ndistributed samples in training and at test time. Much work has shown that\nhigh-performing ML classifiers can degrade significantly and provide\noverly-confident, wrong classification predictions, particularly for\nout-of-distribution (OOD) inputs. Conditional language models (CLMs) are\npredominantly trained to classify the next token in an output sequence, and may\nsuffer even worse degradation on OOD inputs as the prediction is done\nauto-regressively over many steps. Furthermore, the space of potential\nlow-quality outputs is larger as arbitrary text can be generated and it is\nimportant to know when to trust the generated output. We present a highly\naccurate and lightweight OOD detection method for CLMs, and demonstrate its\neffectiveness on abstractive summarization and translation. We also show how\nour method can be used under the common and realistic setting of distribution\nshift for selective generation (analogous to selective prediction for\nclassification) of high-quality outputs, while automatically abstaining from\nlow-quality ones, enabling safer deployment of generative language models.", "published": "2022-09-30 16:17:11", "link": "http://arxiv.org/abs/2209.15558v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Calibrating Sequence likelihood Improves Conditional Language Generation", "abstract": "Conditional language models are predominantly trained with maximum likelihood\nestimation (MLE), giving probability mass to sparsely observed target\nsequences. While MLE trained models assign high probability to plausible\nsequences given the context, the model probabilities often do not accurately\nrank-order generated sequences by quality. This has been empirically observed\nin beam search decoding as output quality degrading with large beam sizes, and\ndecoding strategies benefiting from heuristics such as length normalization and\nrepetition-blocking. In this work, we introduce sequence likelihood calibration\n(SLiC) where the likelihood of model generated sequences are calibrated to\nbetter align with reference sequences in the model's latent space. With SLiC,\ndecoding heuristics become unnecessary and decoding candidates' quality\nsignificantly improves regardless of the decoding method. Furthermore, SLiC\nshows no sign of diminishing returns with model scale, and presents alternative\nways to improve quality with limited training and inference budgets. With SLiC,\nwe exceed or match SOTA results on a wide range of generation tasks spanning\nabstractive summarization, question generation, abstractive question answering\nand data-to-text generation, even with modest-sized models.", "published": "2022-09-30 19:16:16", "link": "http://arxiv.org/abs/2210.00045v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Linearly Mapping from Image to Text Space", "abstract": "The extent to which text-only language models (LMs) learn to represent\nfeatures of the non-linguistic world is an open question. Prior work has shown\nthat pretrained LMs can be taught to caption images when a vision model's\nparameters are optimized to encode images in the language space. We test a\nstronger hypothesis: that the conceptual representations learned by frozen\ntext-only models and vision-only models are similar enough that this can be\nachieved with a linear map. We show that the image representations from vision\nmodels can be transferred as continuous prompts to frozen LMs by training only\na single linear projection. Using these to prompt the LM achieves competitive\nperformance on captioning and visual question answering tasks compared to\nmodels that tune both the image encoder and text decoder (such as the MAGMA\nmodel). We compare three image encoders with increasing amounts of linguistic\nsupervision seen during pretraining: BEIT (no linguistic information),\nNF-ResNET (lexical category information), and CLIP (full natural language\ndescriptions). We find that all three encoders perform equally well at\ntransferring visual property information to the language model (e.g., whether\nan animal is large or small), but that image encoders pretrained with\nlinguistic supervision more saliently encode category information (e.g.,\ndistinguishing hippo vs. elephant) and thus perform significantly better on\nbenchmark language-and-vision tasks. Our results indicate that LMs encode\nconceptual information structurally similarly to vision-based models, even\nthose that are solely trained on images. Code is available here:\nhttps://github.com/jmerullo/limber", "published": "2022-09-30 01:17:18", "link": "http://arxiv.org/abs/2209.15162v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Blur the Linguistic Boundary: Interpreting Chinese Buddhist Sutra in\n  English via Neural Machine Translation", "abstract": "Buddhism is an influential religion with a long-standing history and profound\nphilosophy. Nowadays, more and more people worldwide aspire to learn the\nessence of Buddhism, attaching importance to Buddhism dissemination. However,\nBuddhist scriptures written in classical Chinese are obscure to most people and\nmachine translation applications. For instance, general Chinese-English neural\nmachine translation (NMT) fails in this domain. In this paper, we proposed a\nnovel approach to building a practical NMT model for Buddhist scriptures. The\nperformance of our translation pipeline acquired highly promising results in\nablation experiments under three criteria.", "published": "2022-09-30 01:26:05", "link": "http://arxiv.org/abs/2209.15164v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Depth-Wise Attention (DWAtt): A Layer Fusion Method for Data-Efficient\n  Classification", "abstract": "Language Models pretrained on large textual data have been shown to encode\ndifferent types of knowledge simultaneously. Traditionally, only the features\nfrom the last layer are used when adapting to new tasks or data. We put forward\nthat, when using or finetuning deep pretrained models, intermediate layer\nfeatures that may be relevant to the downstream task are buried too deep to be\nused efficiently in terms of needed samples or steps. To test this, we propose\na new layer fusion method: Depth-Wise Attention (DWAtt), to help re-surface\nsignals from non-final layers. We compare DWAtt to a basic concatenation-based\nlayer fusion method (Concat), and compare both to a deeper model baseline --\nall kept within a similar parameter budget. Our findings show that DWAtt and\nConcat are more step- and sample-efficient than the baseline, especially in the\nfew-shot setting. DWAtt outperforms Concat on larger data sizes. On CoNLL-03\nNER, layer fusion shows 3.68--9.73% F1 gain at different few-shot sizes. The\nlayer fusion models presented significantly outperform the baseline in various\ntraining scenarios with different data sizes, architectures, and training\nconstraints.", "published": "2022-09-30 01:39:55", "link": "http://arxiv.org/abs/2209.15168v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Adaptive Sparse and Monotonic Attention for Transformer-based Automatic\n  Speech Recognition", "abstract": "The Transformer architecture model, based on self-attention and multi-head\nattention, has achieved remarkable success in offline end-to-end Automatic\nSpeech Recognition (ASR). However, self-attention and multi-head attention\ncannot be easily applied for streaming or online ASR. For self-attention in\nTransformer ASR, the softmax normalization function-based attention mechanism\nmakes it impossible to highlight important speech information. For multi-head\nattention in Transformer ASR, it is not easy to model monotonic alignments in\ndifferent heads. To overcome these two limits, we integrate sparse attention\nand monotonic attention into Transformer-based ASR. The sparse mechanism\nintroduces a learned sparsity scheme to enable each self-attention structure to\nfit the corresponding head better. The monotonic attention deploys\nregularization to prune redundant heads for the multi-head attention structure.\nThe experiments show that our method can effectively improve the attention\nmechanism on widely used benchmarks of speech recognition.", "published": "2022-09-30 01:55:57", "link": "http://arxiv.org/abs/2209.15176v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning by Distilling Context", "abstract": "Language models significantly benefit from context tokens, such as prompts or\nscratchpads. They perform better when prompted with informative instructions,\nand they acquire new reasoning capabilities by generating a scratch-pad before\npredicting the final answers. However, they do not \\textit{internalize} these\nperformance gains, which disappear when the context tokens are gone. Our work\nproposes to apply context distillation so that a language model can improve\nitself by internalizing these gains. Concretely, given a synthetic unlabeled\ninput for the target task, we condition the model on ``[instructions] +\n[task-input]'' to predict ``[scratch-pad] + [final answer]''; then we fine-tune\nthe same model to predict its own ``[final answer]'' conditioned on the\n``[task-input]'', without seeing the ``[instructions]'' or using the\n``[scratch-pad]''.\n  We show that context distillation is a general method to train language\nmodels, and it can effectively internalize 3 types of training signals. First,\nit can internalize abstract task instructions and explanations, so we can\niteratively update the model parameters with new instructions and overwrite old\nones. Second, it can internalize step-by-step reasoning for complex tasks\n(e.g., 8-digit addition), and such a newly acquired capability proves to be\nuseful for other downstream tasks. Finally, it can internalize concrete\ntraining examples, and it outperforms directly learning with gradient descent\nby 9\\% on the SPIDER Text-to-SQL dataset; furthermore, combining context\ndistillation operations can internalize more training examples than the context\nwindow size allows.", "published": "2022-09-30 02:30:15", "link": "http://arxiv.org/abs/2209.15189v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluation of taxonomic and neural embedding methods for calculating\n  semantic similarity", "abstract": "Modelling semantic similarity plays a fundamental role in lexical semantic\napplications. A natural way of calculating semantic similarity is to access\nhandcrafted semantic networks, but similarity prediction can also be\nanticipated in a distributional vector space. Similarity calculation continues\nto be a challenging task, even with the latest breakthroughs in deep neural\nlanguage models. We first examined popular methodologies in measuring taxonomic\nsimilarity, including edge-counting that solely employs semantic relations in a\ntaxonomy, as well as the complex methods that estimate concept specificity. We\nfurther extrapolated three weighting factors in modelling taxonomic similarity.\nTo study the distinct mechanisms between taxonomic and distributional\nsimilarity measures, we ran head-to-head comparisons of each measure with human\nsimilarity judgements from the perspectives of word frequency, polysemy degree\nand similarity intensity. Our findings suggest that without fine-tuning the\nuniform distance, taxonomic similarity measures can depend on the shortest path\nlength as a prime factor to predict semantic similarity; in contrast to\ndistributional semantics, edge-counting is free from sense distribution bias in\nuse and can measure word similarity both literally and metaphorically; the\nsynergy of retrofitting neural embeddings with concept relations in similarity\nprediction may indicate a new trend to leverage knowledge bases on transfer\nlearning. It appears that a large gap still exists on computing semantic\nsimilarity among different ranges of word frequency, polysemous degree and\nsimilarity intensity.", "published": "2022-09-30 02:54:21", "link": "http://arxiv.org/abs/2209.15197v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Synonym Detection Using Syntactic Dependency And Neural Embeddings", "abstract": "Recent advances on the Vector Space Model have significantly improved some\nNLP applications such as neural machine translation and natural language\ngeneration. Although word co-occurrences in context have been widely used in\ncounting-/predicting-based distributional models, the role of syntactic\ndependencies in deriving distributional semantics has not yet been thoroughly\ninvestigated. By comparing various Vector Space Models in detecting synonyms in\nTOEFL, we systematically study the salience of syntactic dependencies in\naccounting for distributional similarity. We separate syntactic dependencies\ninto different groups according to their various grammatical roles and then use\ncontext-counting to construct their corresponding raw and SVD-compressed\nmatrices. Moreover, using the same training hyperparameters and corpora, we\nstudy typical neural embeddings in the evaluation. We further study the\neffectiveness of injecting human-compiled semantic knowledge into neural\nembeddings on computing distributional similarity. Our results show that the\nsyntactically conditioned contexts can interpret lexical semantics better than\nthe unconditioned ones, whereas retrofitting neural embeddings with semantic\nknowledge can significantly improve synonym detection.", "published": "2022-09-30 03:16:41", "link": "http://arxiv.org/abs/2209.15202v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "What Makes Pre-trained Language Models Better Zero-shot Learners?", "abstract": "Current methods for prompt learning in zeroshot scenarios widely rely on a\ndevelopment set with sufficient human-annotated data to select the\nbest-performing prompt template a posteriori. This is not ideal because in a\nrealworld zero-shot scenario of practical relevance, no labelled data is\navailable. Thus, we propose a simple yet effective method for screening\nreasonable prompt templates in zero-shot text classification: Perplexity\nSelection (Perplection). We hypothesize that language discrepancy can be used\nto measure the efficacy of prompt templates, and thereby develop a\nsubstantiated perplexity-based scheme allowing for forecasting the performance\nof prompt templates in advance. Experiments show that our method leads to\nimproved prediction performance in a realistic zero-shot setting, eliminating\nthe need for any labelled examples.", "published": "2022-09-30 03:28:19", "link": "http://arxiv.org/abs/2209.15206v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SmallCap: Lightweight Image Captioning Prompted with Retrieval\n  Augmentation", "abstract": "Recent advances in image captioning have focused on scaling the data and\nmodel size, substantially increasing the cost of pre-training and finetuning.\nAs an alternative to large models, we present SmallCap, which generates a\ncaption conditioned on an input image and related captions retrieved from a\ndatastore. Our model is lightweight and fast to train, as the only learned\nparameters are in newly introduced cross-attention layers between a pre-trained\nCLIP encoder and GPT-2 decoder. SmallCap can transfer to new domains without\nadditional finetuning and can exploit large-scale data in a training-free\nfashion since the contents of the datastore can be readily replaced. Our\nexperiments show that SmallCap, trained only on COCO, has competitive\nperformance on this benchmark, and also transfers to other domains without\nretraining, solely through retrieval from target-domain data. Further\nimprovement is achieved through the training-free exploitation of diverse\nhuman-labeled and web data, which proves to be effective for a range of\ndomains, including the nocaps benchmark, designed to test generalization to\nunseen visual concepts.", "published": "2022-09-30 09:03:22", "link": "http://arxiv.org/abs/2209.15323v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Augmenting Operations Research with Auto-Formulation of Optimization\n  Models from Problem Descriptions", "abstract": "We describe an augmented intelligence system for simplifying and enhancing\nthe modeling experience for operations research. Using this system, the user\nreceives a suggested formulation of an optimization problem based on its\ndescription. To facilitate this process, we build an intuitive user interface\nsystem that enables the users to validate and edit the suggestions. We\ninvestigate controlled generation techniques to obtain an automatic suggestion\nof formulation. Then, we evaluate their effectiveness with a newly created\ndataset of linear programming problems drawn from various application domains.", "published": "2022-09-30 16:24:36", "link": "http://arxiv.org/abs/2209.15565v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Decade of Knowledge Graphs in Natural Language Processing: A Survey", "abstract": "In pace with developments in the research field of artificial intelligence,\nknowledge graphs (KGs) have attracted a surge of interest from both academia\nand industry. As a representation of semantic relations between entities, KGs\nhave proven to be particularly relevant for natural language processing (NLP),\nexperiencing a rapid spread and wide adoption within recent years. Given the\nincreasing amount of research work in this area, several KG-related approaches\nhave been surveyed in the NLP research community. However, a comprehensive\nstudy that categorizes established topics and reviews the maturity of\nindividual research streams remains absent to this day. Contributing to closing\nthis gap, we systematically analyzed 507 papers from the literature on KGs in\nNLP. Our survey encompasses a multifaceted review of tasks, research types, and\ncontributions. As a result, we present a structured overview of the research\nlandscape, provide a taxonomy of tasks, summarize our findings, and highlight\ndirections for future work.", "published": "2022-09-30 21:53:57", "link": "http://arxiv.org/abs/2210.00105v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Underspecification in Language Modeling Tasks: A Causality-Informed\n  Study of Gendered Pronoun Resolution", "abstract": "Modern language modeling tasks are often underspecified: for a given token\nprediction, many words may satisfy the user's intent of producing natural\nlanguage at inference time, however only one word will minimize the task's loss\nfunction at training time. We introduce a simple causal mechanism to describe\nthe role underspecification plays in the generation of spurious correlations.\nDespite its simplicity, our causal model directly informs the development of\ntwo lightweight black-box evaluation methods, that we apply to gendered pronoun\nresolution tasks on a wide range of LLMs to 1) aid in the detection of\ninference-time task underspecification by exploiting 2) previously unreported\ngender vs. time and gender vs. location spurious correlations on LLMs with a\nrange of A) sizes: from BERT-base to GPT-4 Turbo Preview, B) pre-training\nobjectives: from masked & autoregressive language modeling to a mixture of\nthese objectives, and C) training stages: from pre-training only to\nreinforcement learning from human feedback (RLHF). Code and open-source demos\navailable at https://github.com/2dot71mily/uspec.", "published": "2022-09-30 23:10:11", "link": "http://arxiv.org/abs/2210.00131v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Construction and Applications of Billion-Scale Pre-Trained Multimodal\n  Business Knowledge Graph", "abstract": "Business Knowledge Graphs (KGs) are important to many enterprises today,\nproviding factual knowledge and structured data that steer many products and\nmake them more intelligent. Despite their promising benefits, building business\nKG necessitates solving prohibitive issues of deficient structure and multiple\nmodalities. In this paper, we advance the understanding of the practical\nchallenges related to building KG in non-trivial real-world systems. We\nintroduce the process of building an open business knowledge graph (OpenBG)\nderived from a well-known enterprise, Alibaba Group. Specifically, we define a\ncore ontology to cover various abstract products and consumption demands, with\nfine-grained taxonomy and multimodal facts in deployed applications. OpenBG is\nan open business KG of unprecedented scale: 2.6 billion triples with more than\n88 million entities covering over 1 million core classes/concepts and 2,681\ntypes of relations. We release all the open resources (OpenBG benchmarks)\nderived from it for the community and report experimental results of KG-centric\ntasks. We also run up an online competition based on OpenBG benchmarks, and has\nattracted thousands of teams. We further pre-train OpenBG and apply it to many\nKG- enhanced downstream tasks in business scenarios, demonstrating the\neffectiveness of billion-scale multimodal knowledge for e-commerce. All the\nresources with codes have been released at\n\\url{https://github.com/OpenBGBenchmark/OpenBG}.", "published": "2022-09-30 04:03:26", "link": "http://arxiv.org/abs/2209.15214v6", "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.AI"}
{"title": "A Survey: Credit Sentiment Score Prediction", "abstract": "Manual approvals are still used by banks and other NGOs to approve loans. It\ntakes time and is prone to mistakes because it is controlled by a bank\nemployee. Several fields of machine learning mining technologies have been\nutilized to enhance various areas of credit rating forecast. A major goal of\nthis research is to look at current sentiment analysis techniques that are\nbeing used to generate creditworthiness.", "published": "2022-09-30 08:03:52", "link": "http://arxiv.org/abs/2209.15293v1", "categories": ["q-fin.GN", "cs.CL", "cs.LG"], "primary_category": "q-fin.GN"}
{"title": "SpeechLM: Enhanced Speech Pre-Training with Unpaired Textual Data", "abstract": "How to boost speech pre-training with textual data is an unsolved problem due\nto the fact that speech and text are very different modalities with distinct\ncharacteristics. In this paper, we propose a cross-modal Speech and Language\nModel (SpeechLM) to explicitly align speech and text pre-training with a\npre-defined unified discrete representation. Specifically, we introduce two\nalternative discrete tokenizers to bridge the speech and text modalities,\nincluding phoneme-unit and hidden-unit tokenizers, which can be trained using a\nsmall amount of paired speech-text data. Based on the trained tokenizers, we\nconvert the unlabeled speech and text data into tokens of phoneme units or\nhidden units. The pre-training objective is designed to unify the speech and\nthe text into the same discrete semantic space with a unified Transformer\nnetwork. We evaluate SpeechLM on various spoken language processing tasks\nincluding speech recognition, speech translation, and universal representation\nevaluation framework SUPERB, demonstrating significant improvements on\ncontent-related tasks. Code and models are available at\nhttps://aka.ms/SpeechLM.", "published": "2022-09-30 09:12:10", "link": "http://arxiv.org/abs/2209.15329v3", "categories": ["cs.CL", "cs.AI", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Emergent Communication: Generalization and Overfitting in Lewis Games", "abstract": "Lewis signaling games are a class of simple communication games for\nsimulating the emergence of language. In these games, two agents must agree on\na communication protocol in order to solve a cooperative task. Previous work\nhas shown that agents trained to play this game with reinforcement learning\ntend to develop languages that display undesirable properties from a linguistic\npoint of view (lack of generalization, lack of compositionality, etc). In this\npaper, we aim to provide better understanding of this phenomenon by\nanalytically studying the learning problem in Lewis games. As a core\ncontribution, we demonstrate that the standard objective in Lewis games can be\ndecomposed in two components: a co-adaptation loss and an information loss.\nThis decomposition enables us to surface two potential sources of overfitting,\nwhich we show may undermine the emergence of a structured communication\nprotocol. In particular, when we control for overfitting on the co-adaptation\nloss, we recover desired properties in the emergent languages: they are more\ncompositional and generalize better.", "published": "2022-09-30 09:50:46", "link": "http://arxiv.org/abs/2209.15342v2", "categories": ["cs.MA", "cs.CL", "cs.IT", "math.IT"], "primary_category": "cs.MA"}
{"title": "AudioGen: Textually Guided Audio Generation", "abstract": "We tackle the problem of generating audio samples conditioned on descriptive\ntext captions. In this work, we propose AaudioGen, an auto-regressive\ngenerative model that generates audio samples conditioned on text inputs.\nAudioGen operates on a learnt discrete audio representation. The task of\ntext-to-audio generation poses multiple challenges. Due to the way audio\ntravels through a medium, differentiating ``objects'' can be a difficult task\n(e.g., separating multiple people simultaneously speaking). This is further\ncomplicated by real-world recording conditions (e.g., background noise,\nreverberation, etc.). Scarce text annotations impose another constraint,\nlimiting the ability to scale models. Finally, modeling high-fidelity audio\nrequires encoding audio at high sampling rate, leading to extremely long\nsequences. To alleviate the aforementioned challenges we propose an\naugmentation technique that mixes different audio samples, driving the model to\ninternally learn to separate multiple sources. We curated 10 datasets\ncontaining different types of audio and text annotations to handle the scarcity\nof text-audio data points. For faster inference, we explore the use of\nmulti-stream modeling, allowing the use of shorter sequences while maintaining\na similar bitrate and perceptual quality. We apply classifier-free guidance to\nimprove adherence to text. Comparing to the evaluated baselines, AudioGen\noutperforms over both objective and subjective metrics. Finally, we explore the\nability of the proposed method to generate audio continuation conditionally and\nunconditionally. Samples: https://felixkreuk.github.io/audiogen", "published": "2022-09-30 10:17:05", "link": "http://arxiv.org/abs/2209.15352v2", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Augmentation Invariant Discrete Representation for Generative Spoken\n  Language Modeling", "abstract": "Generative Spoken Language Modeling research focuses on optimizing speech\nLanguage Models (LMs) using raw audio recordings without accessing any textual\nsupervision. Such speech LMs usually operate over discrete units obtained from\nquantizing internal representations of self-supervised models. Although such\nunits show impressive modeling results, their robustness capabilities have not\nbeen extensively investigated. This work focuses on improving the robustness of\ndiscrete input representations for generative spoken language modeling. First,\nwe formally define how to measure the robustness of such representations to\nvarious signal variations that do not alter the spoken information (e.g.,\ntime-stretch). Next, we empirically demonstrate how current state-of-the-art\nrepresentation models lack robustness to such variations. To overcome this, we\npropose an effective and efficient method to learn robust discrete speech\nrepresentation for generative spoken language modeling. The proposed approach\nis based on applying a set of signal transformations to the speech signal and\noptimizing the model using an iterative pseudo-labeling scheme. Our method\nsignificantly improves over the evaluated baselines when considering encoding\nand modeling metrics. We additionally evaluate our method on the\nspeech-to-speech translation task, considering Spanish-English and\nFrench-English translations, and show the proposed approach outperforms the\nevaluated baselines.", "published": "2022-09-30 14:15:03", "link": "http://arxiv.org/abs/2209.15483v2", "categories": ["cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Differentially Private Bias-Term Fine-tuning of Foundation Models", "abstract": "We study the problem of differentially private (DP) fine-tuning of large\npre-trained models -- a recent privacy-preserving approach suitable for solving\ndownstream tasks with sensitive data. Existing work has demonstrated that high\naccuracy is possible under strong privacy constraint, yet requires significant\ncomputational overhead or modifications to the network architecture. We propose\ndifferentially private bias-term fine-tuning (DP-BiTFiT), which matches the\nstate-of-the-art accuracy for DP algorithms and the efficiency of the standard\nBiTFiT. DP-BiTFiT is model agnostic (not modifying the network architecture),\nparameter efficient (only training about 0.1% of the parameters), and\ncomputation efficient (almost removing the overhead caused by DP, in both the\ntime and space complexity). On a wide range of tasks, DP-BiTFiT is 2~30X faster\nand uses 2~8X less memory than DP full fine-tuning, even faster than the\nstandard full fine-tuning. This amazing efficiency enables us to conduct DP\nfine-tuning on language and vision tasks with long-sequence texts and\nhigh-resolution images, which were computationally difficult using existing\nmethods. We open-source our code at FastDP\n(https://github.com/awslabs/fast-differential-privacy).", "published": "2022-09-30 18:30:48", "link": "http://arxiv.org/abs/2210.00036v3", "categories": ["cs.LG", "cs.CL", "cs.CR", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Differentially Private Optimization on Large Model at Small Cost", "abstract": "Differentially private (DP) optimization is the standard paradigm to learn\nlarge neural networks that are accurate and privacy-preserving. The\ncomputational cost for DP deep learning, however, is notoriously heavy due to\nthe per-sample gradient clipping. Existing DP implementations are 2-1000X more\ncostly in time and space complexity than the standard (non-private) training.\nIn this work, we develop a novel Book-Keeping (BK) technique that implements\nexisting DP optimizers (thus achieving the same accuracy), with a substantial\nimprovement on the computational cost. Specifically, BK enables DP training on\nlarge models and high dimensional data to be roughly as fast and memory-saving\nas the standard training, whereas previous DP algorithms can be inefficient or\nincapable of training due to memory error. The computational advantage of BK is\nsupported by the complexity analysis as well as extensive experiments on vision\nand language tasks. Our implementation achieves state-of-the-art (SOTA)\naccuracy with very small extra cost: on GPT2 and at almost the same memory cost\n(<1% overhead), BK has 1.03X the time complexity of the standard training\n(0.83X training speed in practice), and 0.61X the time complexity of the most\nefficient DP implementation (1.36X training speed in practice). We open-source\nthe codebase for the BK algorithm at the FastDP library\n(https://github.com/awslabs/fast-differential-privacy).", "published": "2022-09-30 18:38:53", "link": "http://arxiv.org/abs/2210.00038v2", "categories": ["cs.LG", "cs.CL", "cs.CR", "cs.CV"], "primary_category": "cs.LG"}
{"title": "DecAF: Joint Decoding of Answers and Logical Forms for Question\n  Answering over Knowledge Bases", "abstract": "Question answering over knowledge bases (KBs) aims to answer natural language\nquestions with factual information such as entities and relations in KBs.\nPrevious methods either generate logical forms that can be executed over KBs to\nobtain final answers or predict answers directly. Empirical results show that\nthe former often produces more accurate answers, but it suffers from\nnon-execution issues due to potential syntactic and semantic errors in the\ngenerated logical forms. In this work, we propose a novel framework DecAF that\njointly generates both logical forms and direct answers, and then combines the\nmerits of them to get the final answers. Moreover, different from most of the\nprevious methods, DecAF is based on simple free-text retrieval without relying\non any entity linking tools -- this simplification eases its adaptation to\ndifferent datasets. DecAF achieves new state-of-the-art accuracy on WebQSP,\nFreebaseQA, and GrailQA benchmarks, while getting competitive results on the\nComplexWebQuestions benchmark.", "published": "2022-09-30 19:51:52", "link": "http://arxiv.org/abs/2210.00063v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Policy Learning via Language Dynamics Distillation", "abstract": "Recent work has shown that augmenting environments with language descriptions\nimproves policy learning. However, for environments with complex language\nabstractions, learning how to ground language to observations is difficult due\nto sparse, delayed rewards. We propose Language Dynamics Distillation (LDD),\nwhich pretrains a model to predict environment dynamics given demonstrations\nwith language descriptions, and then fine-tunes these language-aware pretrained\nrepresentations via reinforcement learning (RL). In this way, the model is\ntrained to both maximize expected reward and retain knowledge about how\nlanguage relates to environment dynamics. On SILG, a benchmark of five tasks\nwith language descriptions that evaluate distinct generalization challenges on\nunseen environments (NetHack, ALFWorld, RTFM, Messenger, and Touchdown), LDD\noutperforms tabula-rasa RL, VAE pretraining, and methods that learn from\nunlabeled demonstrations in inverse RL and reward shaping with pretrained\nexperts. In our analyses, we show that language descriptions in demonstrations\nimprove sample-efficiency and generalization across environments, and that\ndynamics modelling with expert demonstrations is more effective than with\nnon-experts.", "published": "2022-09-30 19:56:04", "link": "http://arxiv.org/abs/2210.00066v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Institutional Foundations of Adaptive Planning: Exploration of Flood\n  Planning in the Lower Rio Grande Valley, Texas, USA", "abstract": "Adaptive planning is ideally suited for the deep uncertainties presented by\nclimate change. While there is a robust scholarship on the theory and methods\nof adaptive planning, this has largely neglected how adaptive planning is\naffected by existing planning institutions and how to move forward within the\nconstraints of traditional planning organizations. This study asks: How do\nexisting traditional planning institutions support adaptive planning? We\nexplore this for flood planning in the Lower Rio Grande Valley of Texas, United\nStates. We draw on county hazard plan and regional flood plan documents as well\nas transcripts of regional flood planning meetings to explore the emergent\ntopics of these institutional outputs. Using Natural Language Processing to\nanalyze this large amount of text, we find that hazard plans and discussions\ndeveloping these plans are largely lacking an adaptive approach.", "published": "2022-09-30 22:10:38", "link": "http://arxiv.org/abs/2210.00113v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Blind Signal Dereverberation for Machine Speech Recognition", "abstract": "We present a method to remove unknown convolutive noise introduced to speech\nby reverberations of recording environments, utilizing some amount of training\nspeech data from the reverberant environment, and any available non-reverberant\nspeech data. Using Fourier transform computed over long temporal windows, which\nideally cover the entire room impulse response, we convert room induced\nconvolution to additions in the log spectral domain. Next, we compute a\nspectral normalization vector from statistics gathered over reverberated as\nwell as over clean speech in the log spectral domain. During operation, this\nnormalization vectors are used to alleviate reverberations from complex speech\nspectra recorded under the same reverberant conditions . Such dereverberated\ncomplex speech spectra are used to compute complex FDLP-spectrograms for use in\nautomatic speech recognition.", "published": "2022-09-30 22:15:31", "link": "http://arxiv.org/abs/2210.00117v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Adversarial Robustness of Representation Learning for Knowledge Graphs", "abstract": "Knowledge graphs represent factual knowledge about the world as relationships\nbetween concepts and are critical for intelligent decision making in enterprise\napplications. New knowledge is inferred from the existing facts in the\nknowledge graphs by encoding the concepts and relations into low-dimensional\nfeature vector representations. The most effective representations for this\ntask, called Knowledge Graph Embeddings (KGE), are learned through neural\nnetwork architectures. Due to their impressive predictive performance, they are\nincreasingly used in high-impact domains like healthcare, finance and\neducation. However, are the black-box KGE models adversarially robust for use\nin domains with high stakes? This thesis argues that state-of-the-art KGE\nmodels are vulnerable to data poisoning attacks, that is, their predictive\nperformance can be degraded by systematically crafted perturbations to the\ntraining knowledge graph. To support this argument, two novel data poisoning\nattacks are proposed that craft input deletions or additions at training time\nto subvert the learned model's performance at inference time. These adversarial\nattacks target the task of predicting the missing facts in knowledge graphs\nusing KGE models, and the evaluation shows that the simpler attacks are\ncompetitive with or outperform the computationally expensive ones. The thesis\ncontributions not only highlight and provide an opportunity to fix the security\nvulnerabilities of KGE models, but also help to understand the black-box\npredictive behaviour of KGE models.", "published": "2022-09-30 22:41:22", "link": "http://arxiv.org/abs/2210.00122v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "An empirical study of weakly supervised audio tagging embeddings for\n  general audio representations", "abstract": "We study the usability of pre-trained weakly supervised audio tagging (AT)\nmodels as feature extractors for general audio representations. We mainly\nanalyze the feasibility of transferring those embeddings to other tasks within\nthe speech and sound domains. Specifically, we benchmark weakly supervised\npre-trained models (MobileNetV2 and EfficientNet-B0) against modern\nself-supervised learning methods (BYOL-A) as feature extractors. Fourteen\ndownstream tasks are used for evaluation ranging from music instrument\nclassification to language classification. Our results indicate that AT\npre-trained models are an excellent transfer learning choice for music, event,\nand emotion recognition tasks. Further, finetuning AT models can also benefit\nspeech-related tasks such as keyword spotting and intent classification.", "published": "2022-09-30 01:35:36", "link": "http://arxiv.org/abs/2209.15167v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Wake Word Detection Based on Res2Net", "abstract": "This letter proposes a new wake word detection system based on Res2Net. As a\nvariant of ResNet, Res2Net was first applied to objection detection. Res2Net\nrealizes multiple feature scales by increasing possible receptive fields. This\nmultiple scaling mechanism significantly improves the detection ability of wake\nwords with different durations. Compared with the ResNet-based model, Res2Net\nalso significantly reduces the model size and is more suitable for detecting\nwake words. The proposed system can determine the positions of wake words from\nthe audio stream without any additional assistance. The proposed method is\nverified on the Mobvoi dataset containing two wake words. At a false alarm rate\nof 0.5 per hour, the system reduced the false rejection of the two wake words\nby more than 12% over prior works.", "published": "2022-09-30 08:10:16", "link": "http://arxiv.org/abs/2209.15296v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "End-to-End Label Uncertainty Modeling in Speech Emotion Recognition\n  using Bayesian Neural Networks and Label Distribution Learning", "abstract": "To train machine learning algorithms to predict emotional expressions in\nterms of arousal and valence, annotated datasets are needed. However, as\ndifferent people perceive others' emotional expressions differently, their\nannotations are subjective. To account for this, annotations are typically\ncollected from multiple annotators and averaged to obtain ground-truth labels.\nHowever, when exclusively trained on this averaged ground-truth, the model is\nagnostic to the inherent subjectivity in emotional expressions. In this work,\nwe therefore propose an end-to-end Bayesian neural network capable of being\ntrained on a distribution of annotations to also capture the subjectivity-based\nlabel uncertainty. Instead of a Gaussian, we model the annotation distribution\nusing Student's t-distribution, which also accounts for the number of\nannotations available. We derive the corresponding Kullback-Leibler divergence\nloss and use it to train an estimator for the annotation distribution, from\nwhich the mean and uncertainty can be inferred. We validate the proposed method\nusing two in-the-wild datasets. We show that the proposed t-distribution based\napproach achieves state-of-the-art uncertainty modeling results in speech\nemotion recognition, and also consistent results in cross-corpora evaluations.\nFurthermore, analyses reveal that the advantage of a t-distribution over a\nGaussian grows with increasing inter-annotator correlation and a decreasing\nnumber of annotations available.", "published": "2022-09-30 12:55:43", "link": "http://arxiv.org/abs/2209.15449v2", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Binaural Speech Enhancement Using STOI-Optimal Masks", "abstract": "STOI-optimal masking has been previously proposed and developed for\nsingle-channel speech enhancement. In this paper, we consider the extension to\nthe task of binaural speech enhancement in which spatial information is known\nto be important to speech understanding and therefore should be preserved by\nthe enhancement processing. Masks are estimated for each of the binaural\nchannels individually and a `better-ear listening' mask is computed by choosing\nthe maximum of the two masks. The estimated mask is used to supply probability\ninformation about the speech presence in each time-frequency bin to an\nOptimally-modified Log Spectral Amplitude (OM-LSA) enhancer. We show that using\nthe proposed method for binaural signals with a directional noise not only\nimproves the SNR of the noisy signal but also preserves the binaural cues and\nintelligibility.", "published": "2022-09-30 13:56:25", "link": "http://arxiv.org/abs/2209.15472v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "E-Branchformer: Branchformer with Enhanced merging for speech\n  recognition", "abstract": "Conformer, combining convolution and self-attention sequentially to capture\nboth local and global information, has shown remarkable performance and is\ncurrently regarded as the state-of-the-art for automatic speech recognition\n(ASR). Several other studies have explored integrating convolution and\nself-attention but they have not managed to match Conformer's performance. The\nrecently introduced Branchformer achieves comparable performance to Conformer\nby using dedicated branches of convolution and self-attention and merging local\nand global context from each branch. In this paper, we propose E-Branchformer,\nwhich enhances Branchformer by applying an effective merging method and\nstacking additional point-wise modules. E-Branchformer sets new\nstate-of-the-art word error rates (WERs) 1.81% and 3.65% on LibriSpeech\ntest-clean and test-other sets without using any external training data.", "published": "2022-09-30 20:22:15", "link": "http://arxiv.org/abs/2210.00077v2", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Music Source Separation with Band-split RNN", "abstract": "The performance of music source separation (MSS) models has been greatly\nimproved in recent years thanks to the development of novel neural network\narchitectures and training pipelines. However, recent model designs for MSS\nwere mainly motivated by other audio processing tasks or other research fields,\nwhile the intrinsic characteristics and patterns of the music signals were not\nfully discovered. In this paper, we propose band-split RNN (BSRNN), a\nfrequency-domain model that explictly splits the spectrogram of the mixture\ninto subbands and perform interleaved band-level and sequence-level modeling.\nThe choices of the bandwidths of the subbands can be determined by a priori\nknowledge or expert knowledge on the characteristics of the target source in\norder to optimize the performance on a certain type of target musical\ninstrument. To better make use of unlabeled data, we also describe a\nsemi-supervised model finetuning pipeline that can further improve the\nperformance of the model. Experiment results show that BSRNN trained only on\nMUSDB18-HQ dataset significantly outperforms several top-ranking models in\nMusic Demixing (MDX) Challenge 2021, and the semi-supervised finetuning stage\nfurther improves the performance on all four instrument tracks.", "published": "2022-09-30 01:49:52", "link": "http://arxiv.org/abs/2209.15174v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "An efficient encoder-decoder architecture with top-down attention for\n  speech separation", "abstract": "Deep neural networks have shown excellent prospects in speech separation\ntasks. However, obtaining good results while keeping a low model complexity\nremains challenging in real-world applications. In this paper, we provide a\nbio-inspired efficient encoder-decoder architecture by mimicking the brain's\ntop-down attention, called TDANet, with decreased model complexity without\nsacrificing performance. The top-down attention in TDANet is extracted by the\nglobal attention (GA) module and the cascaded local attention (LA) layers. The\nGA module takes multi-scale acoustic features as input to extract global\nattention signal, which then modulates features of different scales by direct\ntop-down connections. The LA layers use features of adjacent layers as input to\nextract the local attention signal, which is used to modulate the lateral input\nin a top-down manner. On three benchmark datasets, TDANet consistently achieved\ncompetitive separation performance to previous state-of-the-art (SOTA) methods\nwith higher efficiency. Specifically, TDANet's multiply-accumulate operations\n(MACs) are only 5\\% of Sepformer, one of the previous SOTA models, and CPU\ninference time is only 10\\% of Sepformer. In addition, a large-size version of\nTDANet obtained SOTA results on three datasets, with MACs still only 10\\% of\nSepformer and the CPU inference time only 24\\% of Sepformer.", "published": "2022-09-30 03:09:53", "link": "http://arxiv.org/abs/2209.15200v5", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Symphony: Localizing Multiple Acoustic Sources with a Single Microphone\n  Array", "abstract": "Sound recognition is an important and popular function of smart devices. The\nlocation of sound is basic information associated with the acoustic source.\nApart from sound recognition, whether the acoustic sources can be localized\nlargely affects the capability and quality of the smart device's interactive\nfunctions. In this work, we study the problem of concurrently localizing\nmultiple acoustic sources with a smart device (e.g., a smart speaker like\nAmazon Alexa). The existing approaches either can only localize a single\nsource, or require deploying a distributed network of microphone arrays to\nfunction. Our proposal called Symphony is the first approach to tackle the\nabove problem with a single microphone array. The insight behind Symphony is\nthat the geometric layout of microphones on the array determines the unique\nrelationship among signals from the same source along the same arriving path,\nwhile the source's location determines the DoAs (direction-of-arrival) of\nsignals along different arriving paths. Symphony therefore includes a\ngeometry-based filtering module to distinguish signals from different sources\nalong different paths and a coherence-based module to identify signals from the\nsame source. We implement Symphony with different types of commercial\noff-the-shelf microphone arrays and evaluate its performance under different\nsettings. The results show that Symphony has a median localization error of\n0.694m, which is 68% less than that of the state-of-the-art approach.", "published": "2022-09-30 09:08:35", "link": "http://arxiv.org/abs/2209.15325v1", "categories": ["cs.SD", "cs.NI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ChordMics: Acoustic Signal Purification with Distributed Microphones", "abstract": "Acoustic signal acts as an essential input to many systems. However, the pure\nacoustic signal is very difficult to extract, especially in noisy environments.\nExisting beamforming systems are able to extract the signal transmitted from\ncertain directions. However, since microphones are centrally deployed, these\nsystems have limited coverage and low spatial resolution. We overcome the above\nlimitations and present ChordMics, a distributed beamforming system. By\nleveraging the spatial diversity of the distributed microphones, ChordMics is\nable to extract the acoustic signal from arbitrary points. To realize such a\nsystem, we further address the fundamental challenge in distributed\nbeamforming: aligning the signals captured by distributed and unsynchronized\nmicrophones. We implement ChordMics and evaluate its performance under both LOS\nand NLOS scenarios. The evaluation results tell that ChordMics can deliver\nhigher SINR than the centralized microphone array. The average performance gain\nis up to 15dB.", "published": "2022-09-30 09:18:42", "link": "http://arxiv.org/abs/2209.15334v1", "categories": ["cs.SD", "cs.NI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Match to Win: Analysing Sequences Lengths for Efficient Self-supervised\n  Learning in Speech and Audio", "abstract": "Self-supervised learning (SSL) has proven vital in speech and audio-related\napplications. The paradigm trains a general model on unlabeled data that can\nlater be used to solve specific downstream tasks. This type of model is costly\nto train as it requires manipulating long input sequences that can only be\nhandled by powerful centralised servers. Surprisingly, despite many attempts to\nincrease training efficiency through model compression, the effects of\ntruncating input sequence lengths to reduce computation have not been studied.\nIn this paper, we provide the first empirical study of SSL pre-training for\ndifferent specified sequence lengths and link this to various downstream tasks.\nWe find that training on short sequences can dramatically reduce resource costs\nwhile retaining a satisfactory performance for all tasks. This simple one-line\nchange would promote the migration of SSL training from data centres to\nuser-end edge devices for more realistic and personalised applications.", "published": "2022-09-30 16:35:42", "link": "http://arxiv.org/abs/2209.15575v4", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
