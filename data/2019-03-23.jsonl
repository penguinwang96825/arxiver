{"title": "Knowledge-Grounded Response Generation with Deep Attentional\n  Latent-Variable Model", "abstract": "End-to-end dialogue generation has achieved promising results without using\nhandcrafted features and attributes specific for each task and corpus. However,\none of the fatal drawbacks in such approaches is that they are unable to\ngenerate informative utterances, so it limits their usage from some real-world\nconversational applications. This paper attempts at generating diverse and\ninformative responses with a variational generation model, which contains a\njoint attention mechanism conditioning on the information from both dialogue\ncontexts and extra knowledge.", "published": "2019-03-23 12:19:11", "link": "http://arxiv.org/abs/1903.09813v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Expanding the Text Classification Toolbox with Cross-Lingual Embeddings", "abstract": "Most work in text classification and Natural Language Processing (NLP)\nfocuses on English or a handful of other languages that have text corpora of\nhundreds of millions of words. This is creating a new version of the digital\ndivide: the artificial intelligence (AI) divide. Transfer-based approaches,\nsuch as Cross-Lingual Text Classification (CLTC) - the task of categorizing\ntexts written in different languages into a common taxonomy, are a promising\nsolution to the emerging AI divide. Recent work on CLTC has focused on\ndemonstrating the benefits of using bilingual word embeddings as features,\nrelegating the CLTC problem to a mere benchmark based on a simple averaged\nperceptron.\n  In this paper, we explore more extensively and systematically two flavors of\nthe CLTC problem: news topic classification and textual churn intent detection\n(TCID) in social media. In particular, we test the hypothesis that embeddings\nwith context are more effective, by multi-tasking the learning of multilingual\nword embeddings and text classification; we explore neural architectures for\nCLTC; and we move from bi- to multi-lingual word embeddings. For all\narchitectures, types of word embeddings and datasets, we notice a consistent\ngain trend in favor of multilingual joint training, especially for\nlow-resourced languages.", "published": "2019-03-23 20:25:40", "link": "http://arxiv.org/abs/1903.09878v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Toward the Evaluation of Written Proficiency on a Collaborative Social\n  Network for Learning Languages: Yask", "abstract": "Yask is an online social collaborative network for practicing languages in a\nframework that includes requests, answers, and votes. Since measuring\nlinguistic competence using current approaches is difficult, expensive and in\nmany cases imprecise, we present a new alternative approach based on social\nnetworks. Our method, called Proficiency Rank, extends the well-known Page Rank\nalgorithm to measure the reputation of users in a collaborative social graph.\nFirst, we extended Page Rank so that it not only considers positive links\n(votes) but also negative links. Second, in addition to using explicit links,\nwe also incorporate other 4 types of signals implicit in the social graph.\nThese extensions allow Proficiency Rank to produce proficiency rankings for\nalmost all users in the data set used, where only a minority contributes by\nanswering, while the majority contributes only by voting. This overcomes the\nintrinsic limitation of Page Rank of only being able to rank the nodes that\nhave incoming links. Our experimental validation showed that the\nreputation/importance of the users in Yask is significantly correlated with\ntheir language proficiency. In contrast, their written production was poorly\ncorrelated with the vocabulary profiles of the Common European Framework of\nReference. In addition, we found that negative signals (votes) are considerably\nmore informative than positive ones. We concluded that the use of this\ntechnology is a promising tool for measuring second language proficiency, even\nfor relatively small groups of people.", "published": "2019-03-23 16:40:17", "link": "http://arxiv.org/abs/1903.09846v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Competence-based Curriculum Learning for Neural Machine Translation", "abstract": "Current state-of-the-art NMT systems use large neural networks that are not\nonly slow to train, but also often require many heuristics and optimization\ntricks, such as specialized learning rate schedules and large batch sizes. This\nis undesirable as it requires extensive hyperparameter tuning. In this paper,\nwe propose a curriculum learning framework for NMT that reduces training time,\nreduces the need for specialized heuristics or large batch sizes, and results\nin overall better performance. Our framework consists of a principled way of\ndeciding which training samples are shown to the model at different times\nduring training, based on the estimated difficulty of a sample and the current\ncompetence of the model. Filtering training samples in this manner prevents the\nmodel from getting stuck in bad local optima, making it converge faster and\nreach a better solution than the common approach of uniformly sampling training\nexamples. Furthermore, the proposed method can be easily applied to existing\nNMT models by simply modifying their input data pipelines. We show that our\nframework can help improve the training time and the performance of both\nrecurrent neural network models and Transformers, achieving up to a 70%\ndecrease in training time, while at the same time obtaining accuracy\nimprovements of up to 2.2 BLEU.", "published": "2019-03-23 17:33:38", "link": "http://arxiv.org/abs/1903.09848v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Emotion Recognition based on Third-Order Circular Suprasegmental Hidden\n  Markov Model", "abstract": "This work focuses on recognizing the unknown emotion based on the Third-Order\nCircular Suprasegmental Hidden Markov Model (CSPHMM3) as a classifier. Our work\nhas been tested on Emotional Prosody Speech and Transcripts (EPST) database.\nThe extracted features of EPST database are Mel-Frequency Cepstral Coefficients\n(MFCCs). Our results give average emotion recognition accuracy of 77.8% based\non the CSPHMM3. The results of this work demonstrate that CSPHMM3 is superior\nto the Third-Order Hidden Markov Model (HMM3), Gaussian Mixture Model (GMM),\nSupport Vector Machine (SVM), and Vector Quantization (VQ) by 6.0%, 4.9%, 3.5%,\nand 5.4%, respectively, for emotion recognition. The average emotion\nrecognition accuracy achieved based on the CSPHMM3 is comparable to that found\nusing subjective assessment by human judges.", "published": "2019-03-23 11:24:08", "link": "http://arxiv.org/abs/1903.09803v1", "categories": ["cs.SD", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
