{"title": "Statistical Rejection Sampling Improves Preference Optimization", "abstract": "Improving the alignment of language models with human preferences remains an\nactive research challenge. Previous approaches have primarily utilized\nReinforcement Learning from Human Feedback (RLHF) via online RL methods such as\nProximal Policy Optimization (PPO). Recently, offline methods such as Sequence\nLikelihood Calibration (SLiC) and Direct Preference Optimization (DPO) have\nemerged as attractive alternatives, offering improvements in stability and\nscalability while maintaining competitive performance. SLiC refines its loss\nfunction using sequence pairs sampled from a supervised fine-tuned (SFT)\npolicy, while DPO directly optimizes language models based on preference data,\nforegoing the need for a separate reward model. However, the maximum likelihood\nestimator (MLE) of the target optimal policy requires labeled preference pairs\nsampled from that policy. DPO's lack of a reward model constrains its ability\nto sample preference pairs from the optimal policy, and SLiC is restricted to\nsampling preference pairs only from the SFT policy. To address these\nlimitations, we introduce a novel approach called Statistical Rejection\nSampling Optimization (RSO) that aims to source preference data from the target\noptimal policy using rejection sampling, enabling a more accurate estimation of\nthe optimal policy. We also propose a unified framework that enhances the loss\nfunctions used in both SLiC and DPO from a preference modeling standpoint.\nThrough extensive experiments across three diverse tasks, we demonstrate that\nRSO consistently outperforms both SLiC and DPO on evaluations from both Large\nLanguage Model (LLM) and human raters.", "published": "2023-09-13 01:07:25", "link": "http://arxiv.org/abs/2309.06657v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Benchmarking Procedural Language Understanding for Low-Resource\n  Languages: A Case Study on Turkish", "abstract": "Understanding procedural natural language (e.g., step-by-step instructions)\nis a crucial step to execution and planning. However, while there are ample\ncorpora and downstream tasks available in English, the field lacks such\nresources for most languages. To address this gap, we conduct a case study on\nTurkish procedural texts. We first expand the number of tutorials in Turkish\nwikiHow from 2,000 to 52,000 using automated translation tools, where the\ntranslation quality and loyalty to the original meaning are validated by a team\nof experts on a random set. Then, we generate several downstream tasks on the\ncorpus, such as linking actions, goal inference, and summarization. To tackle\nthese tasks, we implement strong baseline models via fine-tuning large\nlanguage-specific models such as TR-BART and BERTurk, as well as multilingual\nmodels such as mBART, mT5, and XLM. We find that language-specific models\nconsistently outperform their multilingual models by a significant margin\nacross most procedural language understanding (PLU) tasks. We release our\ncorpus, downstream tasks and the baseline models with https://github.com/\nGGLAB-KU/turkish-plu.", "published": "2023-09-13 03:42:28", "link": "http://arxiv.org/abs/2309.06698v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Simultaneous Machine Translation with Large Language Models", "abstract": "Real-world simultaneous machine translation (SimulMT) systems face more\nchallenges than just the quality-latency trade-off. They also need to address\nissues related to robustness with noisy input, processing long contexts, and\nflexibility for knowledge injection. These challenges demand models with strong\nlanguage understanding and generation capabilities which may not often equipped\nby dedicated MT models. In this paper, we investigate the possibility of\napplying Large Language Models (LLM) to SimulMT tasks by using existing\nincremental-decoding methods with a newly proposed RALCP algorithm for latency\nreduction. We conducted experiments using the \\texttt{Llama2-7b-chat} model on\nnine different languages from the MUST-C dataset. The results show that LLM\noutperforms dedicated MT models in terms of BLEU and LAAL metrics. Further\nanalysis indicates that LLM has advantages in terms of tuning efficiency and\nrobustness. However, it is important to note that the computational cost of LLM\nremains a significant obstacle to its application in SimulMT.\\footnote{We will\nrelease our code, weights, and data with publication.}", "published": "2023-09-13 04:06:47", "link": "http://arxiv.org/abs/2309.06706v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Scaled Prompt-Tuning for Few-Shot Natural Language Generation", "abstract": "The increasingly Large Language Models (LLMs) demonstrate stronger language\nunderstanding and generation capabilities, while the memory demand and\ncomputation cost of fine-tuning LLMs on downstream tasks are non-negligible.\nBesides, fine-tuning generally requires a certain amount of data from\nindividual tasks whilst data collection cost is another issue to consider in\nreal-world applications. In this work, we focus on Parameter-Efficient\nFine-Tuning (PEFT) methods for few-shot Natural Language Generation (NLG),\nwhich freeze most parameters in LLMs and tune a small subset of parameters in\nfew-shot cases so that memory footprint, training cost, and labeling cost are\nreduced while maintaining or even improving the performance. We propose a\nScaled Prompt-Tuning (SPT) method which surpasses conventional PT with better\nperformance and generalization ability but without an obvious increase in\ntraining cost. Further study on intermediate SPT suggests the superior\ntransferability of SPT in few-shot scenarios, providing a recipe for\ndata-deficient and computation-limited circumstances. Moreover, a comprehensive\ncomparison of existing PEFT methods reveals that certain approaches exhibiting\ndecent performance with modest training cost such as Prefix-Tuning in prior\nstudy could struggle in few-shot NLG tasks, especially on challenging datasets.", "published": "2023-09-13 07:12:31", "link": "http://arxiv.org/abs/2309.06759v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Native Language Identification with Big Bird Embeddings", "abstract": "Native Language Identification (NLI) intends to classify an author's native\nlanguage based on their writing in another language. Historically, the task has\nheavily relied on time-consuming linguistic feature engineering, and\ntransformer-based NLI models have thus far failed to offer effective, practical\nalternatives. The current work investigates if input size is a limiting factor,\nand shows that classifiers trained using Big Bird embeddings outperform\nlinguistic feature engineering models by a large margin on the Reddit-L2\ndataset. Additionally, we provide further insight into input length\ndependencies, show consistent out-of-sample performance, and qualitatively\nanalyze the embedding space. Given the effectiveness and computational\nefficiency of this method, we believe it offers a promising avenue for future\nNLI work.", "published": "2023-09-13 12:47:40", "link": "http://arxiv.org/abs/2309.06923v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Remote Inference of Cognitive Scores in ALS Patients Using a Picture\n  Description", "abstract": "Amyotrophic lateral sclerosis is a fatal disease that not only affects\nmovement, speech, and breath but also cognition. Recent studies have focused on\nthe use of language analysis techniques to detect ALS and infer scales for\nmonitoring functional progression. In this paper, we focused on another\nimportant aspect, cognitive impairment, which affects 35-50% of the ALS\npopulation. In an effort to reach the ALS population, which frequently exhibits\nmobility limitations, we implemented the digital version of the Edinburgh\nCognitive and Behavioral ALS Screen (ECAS) test for the first time. This test\nwhich is designed to measure cognitive impairment was remotely performed by 56\nparticipants from the EverythingALS Speech Study. As part of the study,\nparticipants (ALS and non-ALS) were asked to describe weekly one picture from a\npool of many pictures with complex scenes displayed on their computer at home.\nWe analyze the descriptions performed within +/- 60 days from the day the ECAS\ntest was administered and extract different types of linguistic and acoustic\nfeatures. We input those features into linear regression models to infer 5 ECAS\nsub-scores and the total score. Speech samples from the picture description are\nreliable enough to predict the ECAS subs-scores, achieving statistically\nsignificant Spearman correlation values between 0.32 and 0.51 for the model's\nperformance using 10-fold cross-validation.", "published": "2023-09-13 14:30:30", "link": "http://arxiv.org/abs/2309.06989v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OYXOY: A Modern NLP Test Suite for Modern Greek", "abstract": "This paper serves as a foundational step towards the development of a\nlinguistically motivated and technically relevant evaluation suite for Greek\nNLP. We initiate this endeavor by introducing four expert-verified evaluation\ntasks, specifically targeted at natural language inference, word sense\ndisambiguation (through example comparison or sense selection) and metaphor\ndetection. More than language-adapted replicas of existing tasks, we contribute\ntwo innovations which will resonate with the broader resource and evaluation\ncommunity. Firstly, our inference dataset is the first of its kind, marking not\njust \\textit{one}, but rather \\textit{all} possible inference labels,\naccounting for possible shifts due to e.g. ambiguity or polysemy. Secondly, we\ndemonstrate a cost-efficient method to obtain datasets for under-resourced\nlanguages. Using ChatGPT as a language-neutral parser, we transform the\nDictionary of Standard Modern Greek into a structured format, from which we\nderive the other three tasks through simple projections. Alongside each task,\nwe conduct experiments using currently available state of the art machinery.\nOur experimental baselines affirm the challenging nature of our tasks and\nhighlight the need for expedited progress in order for the Greek NLP ecosystem\nto keep pace with contemporary mainstream research.", "published": "2023-09-13 15:00:56", "link": "http://arxiv.org/abs/2309.07009v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond original Research Articles Categorization via NLP", "abstract": "This work proposes a novel approach to text categorization -- for unknown\ncategories -- in the context of scientific literature, using Natural Language\nProcessing techniques. The study leverages the power of pre-trained language\nmodels, specifically SciBERT, to extract meaningful representations of\nabstracts from the ArXiv dataset. Text categorization is performed using the\nK-Means algorithm, and the optimal number of clusters is determined based on\nthe Silhouette score. The results demonstrate that the proposed approach\ncaptures subject information more effectively than the traditional arXiv\nlabeling system, leading to improved text categorization. The approach offers\npotential for better navigation and recommendation systems in the rapidly\ngrowing landscape of scientific research literature.", "published": "2023-09-13 15:23:30", "link": "http://arxiv.org/abs/2309.07020v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SafetyBench: Evaluating the Safety of Large Language Models", "abstract": "With the rapid development of Large Language Models (LLMs), increasing\nattention has been paid to their safety concerns. Consequently, evaluating the\nsafety of LLMs has become an essential task for facilitating the broad\napplications of LLMs. Nevertheless, the absence of comprehensive safety\nevaluation benchmarks poses a significant impediment to effectively assess and\nenhance the safety of LLMs. In this work, we present SafetyBench, a\ncomprehensive benchmark for evaluating the safety of LLMs, which comprises\n11,435 diverse multiple choice questions spanning across 7 distinct categories\nof safety concerns. Notably, SafetyBench also incorporates both Chinese and\nEnglish data, facilitating the evaluation in both languages. Our extensive\ntests over 25 popular Chinese and English LLMs in both zero-shot and few-shot\nsettings reveal a substantial performance advantage for GPT-4 over its\ncounterparts, and there is still significant room for improving the safety of\ncurrent LLMs. We also demonstrate that the measured safety understanding\nabilities in SafetyBench are correlated with safety generation abilities. Data\nand evaluation guidelines are available at\n\\url{https://github.com/thu-coai/SafetyBench}{https://github.com/thu-coai/SafetyBench}.\nSubmission entrance and leaderboard are available at\n\\url{https://llmbench.ai/safety}{https://llmbench.ai/safety}.", "published": "2023-09-13 15:56:50", "link": "http://arxiv.org/abs/2309.07045v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mitigating Hallucinations and Off-target Machine Translation with\n  Source-Contrastive and Language-Contrastive Decoding", "abstract": "Hallucinations and off-target translation remain unsolved problems in MT,\nespecially for low-resource languages and massively multilingual models. In\nthis paper, we introduce two related methods to mitigate these failure cases\nwith a modified decoding objective, without either requiring retraining or\nexternal models. In source-contrastive decoding, we search for a translation\nthat is probable given the correct input, but improbable given a random input\nsegment. In language-contrastive decoding, we search for a translation that is\nprobable, but improbable given the wrong language indicator token. Experiments\non the massively multilingual models M2M-100 (418M) and SMaLL-100 show that\nthese methods suppress hallucinations and off-target translations, reducing the\nnumber of translations with segment-level chrF2 below 10 by 67-83% on average,\nand the number of translations with oscillatory hallucinations by 75-92% on\naverage, across 57 tested translation directions. In a proof of concept on\nout-of-English translation, we also show that we can suppress off-target\ntranslations with large language models. We release our source code at\nhttps://github.com/ZurichNLP/ContraDecode.", "published": "2023-09-13 17:15:27", "link": "http://arxiv.org/abs/2309.07098v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RAIN: Your Language Models Can Align Themselves without Finetuning", "abstract": "Large language models (LLMs) often demonstrate inconsistencies with human\npreferences. Previous research typically gathered human preference data and\nthen aligned the pre-trained models using reinforcement learning or instruction\ntuning, a.k.a. the finetuning step. In contrast, aligning frozen LLMs without\nrequiring alignment data is more appealing. This work explores the potential of\nthe latter setting. We discover that by integrating self-evaluation and rewind\nmechanisms, unaligned LLMs can directly produce responses consistent with human\npreferences via self-boosting. We introduce a novel inference method,\nRewindable Auto-regressive INference (RAIN), that allows pre-trained LLMs to\nevaluate their own generation and use the evaluation results to guide rewind\nand generation for AI safety. Notably, RAIN operates without the need of extra\ndata for model alignment and abstains from any training, gradient computation,\nor parameter updates. Experimental results evaluated by GPT-4 and humans\ndemonstrate the effectiveness of RAIN: on the HH dataset, RAIN improves the\nharmlessness rate of LLaMA 30B from 82% of vanilla inference to 97%, while\nmaintaining the helpfulness rate. On the TruthfulQA dataset, RAIN improves the\ntruthfulness of the already-well-aligned LLaMA-2-chat 13B model by 5%.", "published": "2023-09-13 17:59:09", "link": "http://arxiv.org/abs/2309.07124v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "In-Contextual Gender Bias Suppression for Large Language Models", "abstract": "Despite their impressive performance in a wide range of NLP tasks, Large\nLanguage Models (LLMs) have been reported to encode worrying-levels of gender\nbiases. Prior work has proposed debiasing methods that require human labelled\nexamples, data augmentation and fine-tuning of LLMs, which are computationally\ncostly. Moreover, one might not even have access to the model parameters for\nperforming debiasing such as in the case of closed LLMs such as GPT-4. To\naddress this challenge, we propose bias suppression that prevents biased\ngenerations of LLMs by simply providing textual preambles constructed from\nmanually designed templates and real-world statistics, without accessing to\nmodel parameters. We show that, using CrowsPairs dataset, our textual preambles\ncovering counterfactual statements can suppress gender biases in English LLMs\nsuch as LLaMA2. Moreover, we find that gender-neutral descriptions of\ngender-biased objects can also suppress their gender biases. Moreover, we show\nthat bias suppression has acceptable adverse effect on downstream task\nperformance with HellaSwag and COPA.", "published": "2023-09-13 18:39:08", "link": "http://arxiv.org/abs/2309.07251v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and\n  Simplicity Bias in MLMs", "abstract": "Most interpretability research in NLP focuses on understanding the behavior\nand features of a fully trained model. However, certain insights into model\nbehavior may only be accessible by observing the trajectory of the training\nprocess. We present a case study of syntax acquisition in masked language\nmodels (MLMs) that demonstrates how analyzing the evolution of interpretable\nartifacts throughout training deepens our understanding of emergent behavior.\nIn particular, we study Syntactic Attention Structure (SAS), a naturally\nemerging property of MLMs wherein specific Transformer heads tend to focus on\nspecific syntactic relations. We identify a brief window in pretraining when\nmodels abruptly acquire SAS, concurrent with a steep drop in loss. This\nbreakthrough precipitates the subsequent acquisition of linguistic\ncapabilities. We then examine the causal role of SAS by manipulating SAS during\ntraining, and demonstrate that SAS is necessary for the development of\ngrammatical capabilities. We further find that SAS competes with other\nbeneficial traits during training, and that briefly suppressing SAS improves\nmodel quality. These findings offer an interpretation of a real-world example\nof both simplicity bias and breakthrough training dynamics.", "published": "2023-09-13 20:57:11", "link": "http://arxiv.org/abs/2309.07311v6", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CONVERSER: Few-Shot Conversational Dense Retrieval with Synthetic Data\n  Generation", "abstract": "Conversational search provides a natural interface for information retrieval\n(IR). Recent approaches have demonstrated promising results in applying dense\nretrieval to conversational IR. However, training dense retrievers requires\nlarge amounts of in-domain paired data. This hinders the development of\nconversational dense retrievers, as abundant in-domain conversations are\nexpensive to collect. In this paper, we propose CONVERSER, a framework for\ntraining conversational dense retrievers with at most 6 examples of in-domain\ndialogues. Specifically, we utilize the in-context learning capability of large\nlanguage models to generate conversational queries given a passage in the\nretrieval corpus. Experimental results on conversational retrieval benchmarks\nOR-QuAC and TREC CAsT 19 show that the proposed CONVERSER achieves comparable\nperformance to fully-supervised models, demonstrating the effectiveness of our\nproposed framework in few-shot conversational dense retrieval. All source code\nand generated datasets are available at https://github.com/MiuLab/CONVERSER", "published": "2023-09-13 06:40:24", "link": "http://arxiv.org/abs/2309.06748v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Dynamic Causal Disentanglement Model for Dialogue Emotion Detection", "abstract": "Emotion detection is a critical technology extensively employed in diverse\nfields. While the incorporation of commonsense knowledge has proven beneficial\nfor existing emotion detection methods, dialogue-based emotion detection\nencounters numerous difficulties and challenges due to human agency and the\nvariability of dialogue content.In dialogues, human emotions tend to accumulate\nin bursts. However, they are often implicitly expressed. This implies that many\ngenuine emotions remain concealed within a plethora of unrelated words and\ndialogues.In this paper, we propose a Dynamic Causal Disentanglement Model\nbased on hidden variable separation, which is founded on the separation of\nhidden variables. This model effectively decomposes the content of dialogues\nand investigates the temporal accumulation of emotions, thereby enabling more\nprecise emotion recognition. First, we introduce a novel Causal Directed\nAcyclic Graph (DAG) to establish the correlation between hidden emotional\ninformation and other observed elements. Subsequently, our approach utilizes\npre-extracted personal attributes and utterance topics as guiding factors for\nthe distribution of hidden variables, aiming to separate irrelevant ones.\nSpecifically, we propose a dynamic temporal disentanglement model to infer the\npropagation of utterances and hidden variables, enabling the accumulation of\nemotion-related information throughout the conversation. To guide this\ndisentanglement process, we leverage the ChatGPT-4.0 and LSTM networks to\nextract utterance topics and personal attributes as observed\ninformation.Finally, we test our approach on two popular datasets in dialogue\nemotion detection and relevant experimental results verified the model's\nsuperiority.", "published": "2023-09-13 12:58:09", "link": "http://arxiv.org/abs/2309.06928v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Auto-Regressive Next-Token Predictors are Universal Learners", "abstract": "Large language models display remarkable capabilities in logical and\nmathematical reasoning, allowing them to solve complex tasks. Interestingly,\nthese abilities emerge in networks trained on the simple task of next-token\nprediction. In this work, we present a theoretical framework for studying\nauto-regressive next-token predictors. We demonstrate that even simple models\nsuch as linear next-token predictors, trained on Chain-of-Thought (CoT) data,\ncan approximate any function efficiently computed by a Turing machine. We\nintroduce a new complexity measure -- length complexity -- which measures the\nnumber of intermediate tokens in a CoT sequence required to approximate some\ntarget function, and analyze the interplay between length complexity and other\nnotions of complexity. Finally, we show experimentally that simple next-token\npredictors, such as linear networks and shallow Multi-Layer Perceptrons (MLPs),\ndisplay non-trivial performance on text generation and arithmetic tasks. Our\nresults demonstrate that the power of today's LLMs can be attributed, to a\ngreat extent, to the auto-regressive next-token training scheme, and not\nnecessarily to a particular choice of architecture.", "published": "2023-09-13 14:15:03", "link": "http://arxiv.org/abs/2309.06979v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Sensitivity, Performance, Robustness: Deconstructing the Effect of\n  Sociodemographic Prompting", "abstract": "Annotators' sociodemographic backgrounds (i.e., the individual compositions\nof their gender, age, educational background, etc.) have a strong impact on\ntheir decisions when working on subjective NLP tasks, such as toxic language\ndetection. Often, heterogeneous backgrounds result in high disagreements. To\nmodel this variation, recent work has explored sociodemographic prompting, a\ntechnique, which steers the output of prompt-based models towards answers that\nhumans with specific sociodemographic profiles would give. However, the\navailable NLP literature disagrees on the efficacy of this technique - it\nremains unclear for which tasks and scenarios it can help, and the role of the\nindividual factors in sociodemographic prompting is still unexplored. We\naddress this research gap by presenting the largest and most comprehensive\nstudy of sociodemographic prompting today. We analyze its influence on model\nsensitivity, performance and robustness across seven datasets and six\ninstruction-tuned model families. We show that sociodemographic information\naffects model predictions and can be beneficial for improving zero-shot\nlearning in subjective NLP tasks. However, its outcomes largely vary for\ndifferent model types, sizes, and datasets, and are subject to large variance\nwith regards to prompt formulations. Most importantly, our results show that\nsociodemographic prompting should be used with care for sensitive applications,\nsuch as toxicity annotation or when studying LLM alignment. Code and data:\nhttps://github.com/UKPLab/arxiv2023-sociodemographic-prompting", "published": "2023-09-13 15:42:06", "link": "http://arxiv.org/abs/2309.07034v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning from Auxiliary Sources in Argumentative Revision Classification", "abstract": "We develop models to classify desirable reasoning revisions in argumentative\nwriting. We explore two approaches -- multi-task learning and transfer learning\n-- to take advantage of auxiliary sources of revision data for similar tasks.\nResults of intrinsic and extrinsic evaluations show that both approaches can\nindeed improve classifier performance over baselines. While multi-task learning\nshows that training on different sources of data at the same time may improve\nperformance, transfer-learning better represents the relationship between the\ndata.", "published": "2023-09-13 22:08:12", "link": "http://arxiv.org/abs/2309.07334v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Pretraining on the Test Set Is All You Need", "abstract": "Inspired by recent work demonstrating the promise of smaller\nTransformer-based language models pretrained on carefully curated data, we\nsupercharge such approaches by investing heavily in curating a novel, high\nquality, non-synthetic data mixture based solely on evaluation benchmarks.\nUsing our novel dataset mixture consisting of less than 100 thousand tokens, we\npretrain a 1 million parameter transformer-based LLM \\textbf{phi-CTNL}\n(pronounced ``fictional\") that achieves perfect results across diverse academic\nbenchmarks, strictly outperforming all known foundation models.\n\\textbf{phi-CTNL} also beats power-law scaling and exhibits a never-before-seen\ngrokking-like ability to accurately predict downstream evaluation benchmarks'\ncanaries.", "published": "2023-09-13 19:47:33", "link": "http://arxiv.org/abs/2309.08632v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Query-Dependent Prompt Evaluation and Optimization with Offline Inverse\n  RL", "abstract": "In this study, we aim to enhance the arithmetic reasoning ability of Large\nLanguage Models (LLMs) through zero-shot prompt optimization. We identify a\npreviously overlooked objective of query dependency in such optimization and\nelucidate two ensuing challenges that impede the successful and economical\ndesign of prompt optimization techniques. One primary issue is the absence of\nan effective method to evaluate prompts during inference when the golden answer\nis unavailable. Concurrently, learning via interactions with the LLMs to\nnavigate the expansive natural language prompting space proves to be\nresource-intensive. To address this, we introduce Prompt-OIRL, which harnesses\noffline inverse reinforcement learning to draw insights from offline prompting\ndemonstration data. Such data exists as by-products when diverse prompts are\nbenchmarked on open-accessible datasets. With Prompt-OIRL, the query-dependent\nprompt optimization objective is achieved by first learning an offline reward\nmodel. This model can evaluate any query-prompt pairs without accessing LLMs.\nSubsequently, a best-of-N strategy is deployed to recommend the optimal prompt.\nOur experimental evaluations across various LLM scales and arithmetic reasoning\ndatasets underscore both the efficacy and economic viability of the proposed\napproach.", "published": "2023-09-13 01:12:52", "link": "http://arxiv.org/abs/2309.06553v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "VLSlice: Interactive Vision-and-Language Slice Discovery", "abstract": "Recent work in vision-and-language demonstrates that large-scale pretraining\ncan learn generalizable models that are efficiently transferable to downstream\ntasks. While this may improve dataset-scale aggregate metrics, analyzing\nperformance around hand-crafted subgroups targeting specific bias dimensions\nreveals systemic undesirable behaviors. However, this subgroup analysis is\nfrequently stalled by annotation efforts, which require extensive time and\nresources to collect the necessary data. Prior art attempts to automatically\ndiscover subgroups to circumvent these constraints but typically leverages\nmodel behavior on existing task-specific annotations and rapidly degrades on\nmore complex inputs beyond \"tabular\" data, none of which study\nvision-and-language models. This paper presents VLSlice, an interactive system\nenabling user-guided discovery of coherent representation-level subgroups with\nconsistent visiolinguistic behavior, denoted as vision-and-language slices,\nfrom unlabeled image sets. We show that VLSlice enables users to quickly\ngenerate diverse high-coherency slices in a user study (n=22) and release the\ntool publicly.", "published": "2023-09-13 04:02:38", "link": "http://arxiv.org/abs/2309.06703v1", "categories": ["cs.CV", "cs.CL", "cs.HC", "cs.LG", "I.4.10; I.2.7; J.4"], "primary_category": "cs.CV"}
{"title": "Enhancing Keyphrase Generation by BART Finetuning with Splitting and\n  Shuffling", "abstract": "Keyphrase generation is a task of identifying a set of phrases that best\nrepre-sent the main topics or themes of a given text. Keyphrases are dividend\nint pre-sent and absent keyphrases. Recent approaches utilizing\nsequence-to-sequence models show effectiveness on absent keyphrase generation.\nHowever, the per-formance is still limited due to the hardness of finding\nabsent keyphrases. In this paper, we propose Keyphrase-Focused BART, which\nexploits the differ-ences between present and absent keyphrase generations, and\nperforms fine-tuning of two separate BART models for present and absent\nkeyphrases. We further show effective approaches of shuffling keyphrases and\ncandidate keyphrase ranking. For absent keyphrases, our Keyphrase-Focused BART\nachieved new state-of-the-art score on F1@5 in two out of five keyphrase\ngen-eration benchmark datasets.", "published": "2023-09-13 05:02:11", "link": "http://arxiv.org/abs/2309.06726v1", "categories": ["cs.CL", "cs.AI", "cs.DL", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Cognitive Mirage: A Review of Hallucinations in Large Language Models", "abstract": "As large language models continue to develop in the field of AI, text\ngeneration systems are susceptible to a worrisome phenomenon known as\nhallucination. In this study, we summarize recent compelling insights into\nhallucinations in LLMs. We present a novel taxonomy of hallucinations from\nvarious text generation tasks, thus provide theoretical insights, detection\nmethods and improvement approaches. Based on this, future research directions\nare proposed. Our contribution are threefold: (1) We provide a detailed and\ncomplete taxonomy for hallucinations appearing in text generation tasks; (2) We\nprovide theoretical analyses of hallucinations in LLMs and provide existing\ndetection and improvement methods; (3) We propose several research directions\nthat can be developed in the future. As hallucinations garner significant\nattention from the community, we will maintain updates on relevant research\nprogress.", "published": "2023-09-13 08:33:09", "link": "http://arxiv.org/abs/2309.06794v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Comparative Analysis of Contextual Relation Extraction based on Deep\n  Learning Models", "abstract": "Contextual Relation Extraction (CRE) is mainly used for constructing a\nknowledge graph with a help of ontology. It performs various tasks such as\nsemantic search, query answering, and textual entailment. Relation extraction\nidentifies the entities from raw texts and the relations among them. An\nefficient and accurate CRE system is essential for creating domain knowledge in\nthe biomedical industry. Existing Machine Learning and Natural Language\nProcessing (NLP) techniques are not suitable to predict complex relations from\nsentences that consist of more than two relations and unspecified entities\nefficiently. In this work, deep learning techniques have been used to identify\nthe appropriate semantic relation based on the context from multiple sentences.\nEven though various machine learning models have been used for relation\nextraction, they provide better results only for binary relations, i.e.,\nrelations occurred exactly between the two entities in a sentence. Machine\nlearning models are not suited for complex sentences that consist of the words\nthat have various meanings. To address these issues, hybrid deep learning\nmodels have been used to extract the relations from complex sentence\neffectively. This paper explores the analysis of various deep learning models\nthat are used for relation extraction.", "published": "2023-09-13 09:05:09", "link": "http://arxiv.org/abs/2309.06814v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Gpachov at CheckThat! 2023: A Diverse Multi-Approach Ensemble for\n  Subjectivity Detection in News Articles", "abstract": "The wide-spread use of social networks has given rise to subjective,\nmisleading, and even false information on the Internet. Thus, subjectivity\ndetection can play an important role in ensuring the objectiveness and the\nquality of a piece of information. This paper presents the solution built by\nthe Gpachov team for the CLEF-2023 CheckThat! lab Task~2 on subjectivity\ndetection. Three different research directions are explored. The first one is\nbased on fine-tuning a sentence embeddings encoder model and dimensionality\nreduction. The second one explores a sample-efficient few-shot learning model.\nThe third one evaluates fine-tuning a multilingual transformer on an altered\ndataset, using data from multiple languages. Finally, the three approaches are\ncombined in a simple majority voting ensemble, resulting in 0.77 macro F1 on\nthe test set and achieving 2nd place on the English subtask.", "published": "2023-09-13 09:49:20", "link": "http://arxiv.org/abs/2309.06844v1", "categories": ["cs.CL", "cs.AI", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Towards the TopMost: A Topic Modeling System Toolkit", "abstract": "Topic models have a rich history with various applications and have recently\nbeen reinvigorated by neural topic modeling. However, these numerous topic\nmodels adopt totally distinct datasets, implementations, and evaluations. This\nimpedes quick utilization and fair comparisons, and thereby hinders their\nresearch progress and applications. To tackle this challenge, we in this paper\npropose a Topic Modeling System Toolkit (TopMost). Compared to existing\ntoolkits, TopMost stands out by supporting more extensive features. It covers a\nbroader spectrum of topic modeling scenarios with their complete lifecycles,\nincluding datasets, preprocessing, models, training, and evaluations. Thanks to\nits highly cohesive and decoupled modular design, TopMost enables rapid\nutilization, fair comparisons, and flexible extensions of diverse cutting-edge\ntopic models. Our code, tutorials, and documentation are available at\nhttps://github.com/bobxwu/topmost.", "published": "2023-09-13 12:10:54", "link": "http://arxiv.org/abs/2309.06908v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Continual Learning with Dirichlet Generative-based Rehearsal", "abstract": "Recent advancements in data-driven task-oriented dialogue systems (ToDs)\nstruggle with incremental learning due to computational constraints and\ntime-consuming issues. Continual Learning (CL) attempts to solve this by\navoiding intensive pre-training, but it faces the problem of catastrophic\nforgetting (CF). While generative-based rehearsal CL methods have made\nsignificant strides, generating pseudo samples that accurately reflect the\nunderlying task-specific distribution is still a challenge. In this paper, we\npresent Dirichlet Continual Learning (DCL), a novel generative-based rehearsal\nstrategy for CL. Unlike the traditionally used Gaussian latent variable in the\nConditional Variational Autoencoder (CVAE), DCL leverages the flexibility and\nversatility of the Dirichlet distribution to model the latent prior variable.\nThis enables it to efficiently capture sentence-level features of previous\ntasks and effectively guide the generation of pseudo samples. In addition, we\nintroduce Jensen-Shannon Knowledge Distillation (JSKD), a robust logit-based\nknowledge distillation method that enhances knowledge transfer during pseudo\nsample generation. Our experiments confirm the efficacy of our approach in both\nintent detection and slot-filling tasks, outperforming state-of-the-art\nmethods.", "published": "2023-09-13 12:30:03", "link": "http://arxiv.org/abs/2309.06917v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unsupervised Contrast-Consistent Ranking with Language Models", "abstract": "Language models contain ranking-based knowledge and are powerful solvers of\nin-context ranking tasks. For instance, they may have parametric knowledge\nabout the ordering of countries by size or may be able to rank product reviews\nby sentiment. We compare pairwise, pointwise and listwise prompting techniques\nto elicit a language model's ranking knowledge. However, we find that even with\ncareful calibration and constrained decoding, prompting-based techniques may\nnot always be self-consistent in the rankings they produce. This motivates us\nto explore an alternative approach that is inspired by an unsupervised probing\nmethod called Contrast-Consistent Search (CCS). The idea is to train a probe\nguided by a logical constraint: a language model's representation of a\nstatement and its negation must be mapped to contrastive true-false poles\nconsistently across multiple statements. We hypothesize that similar\nconstraints apply to ranking tasks where all items are related via consistent,\npairwise or listwise comparisons. To this end, we extend the binary CCS method\nto Contrast-Consistent Ranking (CCR) by adapting existing ranking methods such\nas the Max-Margin Loss, Triplet Loss and an Ordinal Regression objective.\nAcross different models and datasets, our results confirm that CCR probing\nperforms better or, at least, on a par with prompting.", "published": "2023-09-13 14:36:26", "link": "http://arxiv.org/abs/2309.06991v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "R\u00e9sum\u00e9 Parsing as Hierarchical Sequence Labeling: An Empirical Study", "abstract": "Extracting information from r\\'esum\\'es is typically formulated as a\ntwo-stage problem, where the document is first segmented into sections and then\neach section is processed individually to extract the target entities. Instead,\nwe cast the whole problem as sequence labeling in two levels -- lines and\ntokens -- and study model architectures for solving both tasks simultaneously.\nWe build high-quality r\\'esum\\'e parsing corpora in English, French, Chinese,\nSpanish, German, Portuguese, and Swedish. Based on these corpora, we present\nexperimental results that demonstrate the effectiveness of the proposed models\nfor the information extraction task, outperforming approaches introduced in\nprevious work. We conduct an ablation study of the proposed architectures. We\nalso analyze both model performance and resource efficiency, and describe the\ntrade-offs for model deployment in the context of a production environment.", "published": "2023-09-13 15:17:29", "link": "http://arxiv.org/abs/2309.07015v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Can Whisper perform speech-based in-context learning?", "abstract": "This paper investigates the in-context learning abilities of the Whisper\nautomatic speech recognition (ASR) models released by OpenAI. A novel\nspeech-based in-context learning (SICL) approach is proposed for test-time\nadaptation, which can reduce the word error rates (WERs) with only a small\nnumber of labelled speech samples without gradient descent. Language-level\nadaptation experiments using Chinese dialects showed that when applying SICL to\nisolated word ASR, consistent and considerable relative WER reductions can be\nachieved using Whisper models of any size on two dialects, which is on average\n32.3%. A k-nearest-neighbours-based in-context example selection technique can\nbe applied to further improve the efficiency of SICL, which can increase the\naverage relative WER reduction to 36.4%. The findings are verified using\nspeaker adaptation or continuous speech recognition tasks, and both achieved\nconsiderable relative WER reductions. Detailed quantitative analyses are also\nprovided to shed light on SICL's adaptability to phonological variances and\ndialect-specific lexical nuances.", "published": "2023-09-13 16:46:27", "link": "http://arxiv.org/abs/2309.07081v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Traveling Words: A Geometric Interpretation of Transformers", "abstract": "Transformers have significantly advanced the field of natural language\nprocessing, but comprehending their internal mechanisms remains a challenge. In\nthis paper, we introduce a novel geometric perspective that elucidates the\ninner mechanisms of transformer operations. Our primary contribution is\nillustrating how layer normalization confines the latent features to a\nhyper-sphere, subsequently enabling attention to mold the semantic\nrepresentation of words on this surface. This geometric viewpoint seamlessly\nconnects established properties such as iterative refinement and contextual\nembeddings. We validate our insights by probing a pre-trained 124M parameter\nGPT-2 model. Our findings reveal clear query-key attention patterns in early\nlayers and build upon prior observations regarding the subject-specific nature\nof attention heads at deeper layers. Harnessing these geometric insights, we\npresent an intuitive understanding of transformers, depicting them as processes\nthat model the trajectory of word particles along the hyper-sphere.", "published": "2023-09-13 21:01:03", "link": "http://arxiv.org/abs/2309.07315v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sight Beyond Text: Multi-Modal Training Enhances LLMs in Truthfulness\n  and Ethics", "abstract": "Multi-modal large language models (MLLMs) are trained based on large language\nmodels (LLM), with an enhanced capability to comprehend multi-modal inputs and\ngenerate textual responses. While they excel in multi-modal tasks, the pure NLP\nabilities of MLLMs are often underestimated and left untested. In this study,\nwe get out of the box and unveil an intriguing characteristic of MLLMs -- our\npreliminary results suggest that visual instruction tuning, a prevailing\nstrategy for transitioning LLMs into MLLMs, unexpectedly and interestingly\nhelps models attain both improved truthfulness and ethical alignment in the\npure NLP context. For example, a visual-instruction-tuned LLaMA2 7B model\nsurpasses the performance of the LLaMA2-chat 7B model, fine-tuned with over one\nmillion human annotations, on TruthfulQA-mc and Ethics benchmarks. Further\nanalysis reveals that the improved alignment can be attributed to the superior\ninstruction quality inherent to visual-text data. In releasing our code at\ngithub.com/UCSC-VLAA/Sight-Beyond-Text, we aspire to foster further exploration\ninto the intrinsic value of visual-text synergies and, in a broader scope,\nmulti-modal interactions in alignment research.", "published": "2023-09-13 17:57:21", "link": "http://arxiv.org/abs/2309.07120v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Large Language Models Can Infer Psychological Dispositions of Social\n  Media Users", "abstract": "Large Language Models (LLMs) demonstrate increasingly human-like abilities\nacross a wide variety of tasks. In this paper, we investigate whether LLMs like\nChatGPT can accurately infer the psychological dispositions of social media\nusers and whether their ability to do so varies across socio-demographic\ngroups. Specifically, we test whether GPT-3.5 and GPT-4 can derive the Big Five\npersonality traits from users' Facebook status updates in a zero-shot learning\nscenario. Our results show an average correlation of r = .29 (range = [.22,\n.33]) between LLM-inferred and self-reported trait scores - a level of accuracy\nthat is similar to that of supervised machine learning models specifically\ntrained to infer personality. Our findings also highlight heterogeneity in the\naccuracy of personality inferences across different age groups and gender\ncategories: predictions were found to be more accurate for women and younger\nindividuals on several traits, suggesting a potential bias stemming from the\nunderlying training data or differences in online self-expression. The ability\nof LLMs to infer psychological dispositions from user-generated text has the\npotential to democratize access to cheap and scalable psychometric assessments\nfor both researchers and practitioners. On the one hand, this democratization\nmight facilitate large-scale research of high ecological validity and spark\ninnovation in personalized services. On the other hand, it also raises ethical\nconcerns regarding user privacy and self-determination, highlighting the need\nfor stringent ethical frameworks and regulation.", "published": "2023-09-13 01:27:48", "link": "http://arxiv.org/abs/2309.08631v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Differentiable Modelling of Percussive Audio with Transient and Spectral\n  Synthesis", "abstract": "Differentiable digital signal processing (DDSP) techniques, including methods\nfor audio synthesis, have gained attention in recent years and lend themselves\nto interpretability in the parameter space. However, current differentiable\nsynthesis methods have not explicitly sought to model the transient portion of\nsignals, which is important for percussive sounds. In this work, we present a\nunified synthesis framework aiming to address transient generation and\npercussive synthesis within a DDSP framework. To this end, we propose a model\nfor percussive synthesis that builds on sinusoidal modeling synthesis and\nincorporates a modulated temporal convolutional network for transient\ngeneration. We use a modified sinusoidal peak picking algorithm to generate\ntime-varying non-harmonic sinusoids and pair it with differentiable noise and\ntransient encoders that are jointly trained to reconstruct drumset sounds. We\ncompute a set of reconstruction metrics using a large dataset of acoustic and\nelectronic percussion samples that show that our method leads to improved onset\nsignal reconstruction for membranophone percussion instruments.", "published": "2023-09-13 00:21:04", "link": "http://arxiv.org/abs/2309.06649v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Attention-based Encoder-Decoder End-to-End Neural Diarization with\n  Embedding Enhancer", "abstract": "Deep neural network-based systems have significantly improved the performance\nof speaker diarization tasks. However, end-to-end neural diarization (EEND)\nsystems often struggle to generalize to scenarios with an unseen number of\nspeakers, while target speaker voice activity detection (TS-VAD) systems tend\nto be overly complex. In this paper, we propose a simple attention-based\nencoder-decoder network for end-to-end neural diarization (AED-EEND). In our\ntraining process, we introduce a teacher-forcing strategy to address the\nspeaker permutation problem, leading to faster model convergence. For\nevaluation, we propose an iterative decoding method that outputs diarization\nresults for each speaker sequentially. Additionally, we propose an Enhancer\nmodule to enhance the frame-level speaker embeddings, enabling the model to\nhandle scenarios with an unseen number of speakers. We also explore replacing\nthe transformer encoder with a Conformer architecture, which better models\nlocal information. Furthermore, we discovered that commonly used simulation\ndatasets for speaker diarization have a much higher overlap ratio compared to\nreal data. We found that using simulated training data that is more consistent\nwith real data can achieve an improvement in consistency. Extensive\nexperimental validation demonstrates the effectiveness of our proposed\nmethodologies. Our best system achieved a new state-of-the-art diarization\nerror rate (DER) performance on all the CALLHOME (10.08%), DIHARD II (24.64%),\nand AMI (13.00%) evaluation benchmarks, when no oracle voice activity detection\n(VAD) is used. Beyond speaker diarization, our AED-EEND system also shows\nremarkable competitiveness as a speech type detection model.", "published": "2023-09-13 02:17:13", "link": "http://arxiv.org/abs/2309.06672v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Distinguishing Neural Speech Synthesis Models Through Fingerprints in\n  Speech Waveforms", "abstract": "Recent strides in neural speech synthesis technologies, while enjoying\nwidespread applications, have nonetheless introduced a series of challenges,\nspurring interest in the defence against the threat of misuse and abuse.\nNotably, source attribution of synthesized speech has value in forensics and\nintellectual property protection, but prior work in this area has certain\nlimitations in scope. To address the gaps, we present our findings concerning\nthe identification of the sources of synthesized speech in this paper. We\ninvestigate the existence of speech synthesis model fingerprints in the\ngenerated speech waveforms, with a focus on the acoustic model and the vocoder,\nand study the influence of each component on the fingerprint in the overall\nspeech waveforms. Our research, conducted using the multi-speaker LibriTTS\ndataset, demonstrates two key insights: (1) vocoders and acoustic models impart\ndistinct, model-specific fingerprints on the waveforms they generate, and (2)\nvocoder fingerprints are the more dominant of the two, and may mask the\nfingerprints from the acoustic model. These findings strongly suggest the\nexistence of model-specific fingerprints for both the acoustic model and the\nvocoder, highlighting their potential utility in source identification\napplications.", "published": "2023-09-13 08:06:43", "link": "http://arxiv.org/abs/2309.06780v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DCTTS: Discrete Diffusion Model with Contrastive Learning for\n  Text-to-speech Generation", "abstract": "In the Text-to-speech(TTS) task, the latent diffusion model has excellent\nfidelity and generalization, but its expensive resource consumption and slow\ninference speed have always been a challenging. This paper proposes Discrete\nDiffusion Model with Contrastive Learning for Text-to-Speech Generation(DCTTS).\nThe following contributions are made by DCTTS: 1) The TTS diffusion model based\non discrete space significantly lowers the computational consumption of the\ndiffusion model and improves sampling speed; 2) The contrastive learning method\nbased on discrete space is used to enhance the alignment connection between\nspeech and text and improve sampling quality; and 3) It uses an efficient text\nencoder to simplify the model's parameters and increase computational\nefficiency. The experimental results demonstrate that the approach proposed in\nthis paper has outstanding speech synthesis quality and sampling speed while\nsignificantly reducing the resource consumption of diffusion model. The\nsynthesized samples are available at https://github.com/lawtherWu/DCTTS.", "published": "2023-09-13 08:22:38", "link": "http://arxiv.org/abs/2309.06787v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "EMALG: An Enhanced Mandarin Lombard Grid Corpus with Meaningful\n  Sentences", "abstract": "This study investigates the Lombard effect, where individuals adapt their\nspeech in noisy environments. We introduce an enhanced Mandarin Lombard grid\n(EMALG) corpus with meaningful sentences , enhancing the Mandarin Lombard grid\n(MALG) corpus. EMALG features 34 speakers and improves recording setups,\naddressing challenges faced by MALG with nonsense sentences. Our findings\nreveal that in Mandarin, meaningful sentences are more effective in enhancing\nthe Lombard effect. Additionally, we uncover that female exhibit a more\npronounced Lombard effect than male when uttering meaningful sentences.\nMoreover, our results reaffirm the consistency in the Lombard effect comparison\nbetween English and Mandarin found in previous research.", "published": "2023-09-13 10:08:25", "link": "http://arxiv.org/abs/2309.06858v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "VRDMG: Vocal Restoration via Diffusion Posterior Sampling with Multiple\n  Guidance", "abstract": "Restoring degraded music signals is essential to enhance audio quality for\ndownstream music manipulation. Recent diffusion-based music restoration methods\nhave demonstrated impressive performance, and among them, diffusion posterior\nsampling (DPS) stands out given its intrinsic properties, making it versatile\nacross various restoration tasks. In this paper, we identify that there are\npotential issues which will degrade current DPS-based methods' performance and\nintroduce the way to mitigate the issues inspired by diverse diffusion guidance\ntechniques including the RePaint (RP) strategy and the Pseudoinverse-Guided\nDiffusion Models ($\\Pi$GDM). We demonstrate our methods for the vocal\ndeclipping and bandwidth extension tasks under various levels of distortion and\ncutoff frequency, respectively. In both tasks, our methods outperform the\ncurrent DPS-based music restoration benchmarks. We refer to\n\\url{http://carlosholivan.github.io/demos/audio-restoration-2023.html} for\nexamples of the restored audio samples.", "published": "2023-09-13 13:14:01", "link": "http://arxiv.org/abs/2309.06934v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Enhancing Child Vocalization Classification with Phonetically-Tuned\n  Embeddings for Assisting Autism Diagnosis", "abstract": "The assessment of children at risk of autism typically involves a clinician\nobserving, taking notes, and rating children's behaviors. A machine learning\nmodel that can label adult and child audio may largely save labor in coding\nchildren's behaviors, helping clinicians capture critical events and better\ncommunicate with parents. In this study, we leverage Wav2Vec 2.0 (W2V2),\npre-trained on 4300-hour of home audio of children under 5 years old, to build\na unified system for tasks of clinician-child speaker diarization and\nvocalization classification (VC). To enhance children's VC, we build a W2V2\nphoneme recognition system for children under 4 years old, and we incorporate\nits phonetically-tuned embeddings as auxiliary features or recognize pseudo\nphonetic transcripts as an auxiliary task. We test our method on two corpora\n(Rapid-ABC and BabbleCor) and obtain consistent improvements. Additionally, we\noutperform the state-of-the-art performance on the reproducible subset of\nBabbleCor. Code available at https://huggingface.co/lijialudew", "published": "2023-09-13 20:13:40", "link": "http://arxiv.org/abs/2309.07287v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Sound field decomposition based on two-stage neural networks", "abstract": "A method for sound field decomposition based on neural networks is proposed.\nThe method comprises two stages: a sound field separation stage and a\nsingle-source localization stage. In the first stage, the sound pressure at\nmicrophones synthesized by multiple sources is separated into one excited by\neach sound source. In the second stage, the source location is obtained as a\nregression from the sound pressure at microphones consisting of a single sound\nsource. The estimated location is not affected by discretization because the\nsecond stage is designed as a regression rather than a classification. Datasets\nare generated by simulation using Green's function, and the neural network is\ntrained for each frequency. Numerical experiments reveal that, compared with\nconventional methods, the proposed method can achieve higher\nsource-localization accuracy and higher sound-field-reconstruction accuracy.", "published": "2023-09-13 01:32:46", "link": "http://arxiv.org/abs/2309.06661v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "PIAVE: A Pose-Invariant Audio-Visual Speaker Extraction Network", "abstract": "It is common in everyday spoken communication that we look at the turning\nhead of a talker to listen to his/her voice. Humans see the talker to listen\nbetter, so do machines. However, previous studies on audio-visual speaker\nextraction have not effectively handled the varying talking face. This paper\nstudies how to take full advantage of the varying talking face. We propose a\nPose-Invariant Audio-Visual Speaker Extraction Network (PIAVE) that\nincorporates an additional pose-invariant view to improve audio-visual speaker\nextraction. Specifically, we generate the pose-invariant view from each\noriginal pose orientation, which enables the model to receive a consistent\nfrontal view of the talker regardless of his/her head pose, therefore, forming\na multi-view visual input for the speaker. Experiments on the multi-view MEAD\nand in-the-wild LRS3 dataset demonstrate that PIAVE outperforms the\nstate-of-the-art and is more robust to pose variations.", "published": "2023-09-13 04:54:44", "link": "http://arxiv.org/abs/2309.06723v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Leveraging Foundation models for Unsupervised Audio-Visual Segmentation", "abstract": "Audio-Visual Segmentation (AVS) aims to precisely outline audible objects in\na visual scene at the pixel level. Existing AVS methods require fine-grained\nannotations of audio-mask pairs in supervised learning fashion. This limits\ntheir scalability since it is time consuming and tedious to acquire such\ncross-modality pixel level labels. To overcome this obstacle, in this work we\nintroduce unsupervised audio-visual segmentation with no need for task-specific\ndata annotations and model training. For tackling this newly proposed problem,\nwe formulate a novel Cross-Modality Semantic Filtering (CMSF) approach to\naccurately associate the underlying audio-mask pairs by leveraging the\noff-the-shelf multi-modal foundation models (e.g., detection [1], open-world\nsegmentation [2] and multi-modal alignment [3]). Guiding the proposal\ngeneration by either audio or visual cues, we design two training-free\nvariants: AT-GDINO-SAM and OWOD-BIND. Extensive experiments on the AVS-Bench\ndataset show that our unsupervised approach can perform well in comparison to\nprior art supervised counterparts across complex scenarios with multiple\nauditory objects. Particularly, in situations where existing supervised AVS\nmethods struggle with overlapping foreground objects, our models still excel in\naccurately segmenting overlapped auditory objects. Our code will be publicly\nreleased.", "published": "2023-09-13 05:05:47", "link": "http://arxiv.org/abs/2309.06728v1", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Reorganization of the auditory-perceptual space across the human vocal\n  range", "abstract": "We analyzed the auditory-perceptual space across a substantial portion of the\nhuman vocal range (220-1046 Hz) using multidimensional scaling analysis of\ncochlea-scaled spectra from 250-ms vowel segments, initially studied in\nFriedrichs et al. (2017) J. Acoust. Soc. Am. 142 1025-1033. The dataset\ncomprised the vowels /i y e {\\o} {\\epsilon} a o u/ (N=240) produced by three\nnative German female speakers, encompassing a broad range of their respective\nvoice frequency ranges. The initial study demonstrated that, during a\nclosed-set identification task involving 21 listeners, the point vowels /i a u/\nwere significantly recognized at fundamental frequencies (fo) nearing 1 kHz,\nwhereas the recognition of other vowels decreased at higher pitches. Building\non these findings, our study revealed systematic spectral shifts associated\nwith vowel height and frontness as fo increased, with a notable clustering\naround /i a u/ above 523 Hz. These observations underscore the pivotal role of\nspectral shape in vowel perception, illustrating the reliance on acoustic\nanchors at higher pitches. Furthermore, this study sheds light on the quantal\nnature of these vowels and their potential impact on language evolution,\noffering a plausible explanation for their widespread presence in the world's\nlanguages.", "published": "2023-09-13 13:30:05", "link": "http://arxiv.org/abs/2309.06946v1", "categories": ["eess.AS", "cs.SD", "q-bio.PE"], "primary_category": "eess.AS"}
{"title": "A Flexible Online Framework for Projection-Based STFT Phase Retrieval", "abstract": "Several recent contributions in the field of iterative STFT phase retrieval\nhave demonstrated that the performance of the classical Griffin-Lim method can\nbe considerably improved upon. By using the same projection operators as\nGriffin-Lim, but combining them in innovative ways, these approaches achieve\nbetter results in terms of both reconstruction quality and required number of\niterations, while retaining a similar computational complexity per iteration.\nHowever, like Griffin-Lim, these algorithms operate in an offline manner and\nthus require an entire spectrogram as input, which is an unrealistic\nrequirement for many real-world speech communication applications. We propose\nto extend RTISI -- an existing online (frame-by-frame) variant of the\nGriffin-Lim algorithm -- into a flexible framework that enables straightforward\nonline implementation of any algorithm based on iterative projections. We\nfurther employ this framework to implement online variants of the fast\nGriffin-Lim algorithm, the accelerated Griffin-Lim algorithm, and two\nalgorithms from the optics domain. Evaluation results on speech signals show\nthat, similarly to the offline case, these algorithms can achieve a\nconsiderable performance gain compared to RTISI.", "published": "2023-09-13 15:55:38", "link": "http://arxiv.org/abs/2309.07043v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Diffusion models for audio semantic communication", "abstract": "Directly sending audio signals from a transmitter to a receiver across a\nnoisy channel may absorb consistent bandwidth and be prone to errors when\ntrying to recover the transmitted bits. On the contrary, the recent semantic\ncommunication approach proposes to send the semantics and then regenerate\nsemantically consistent content at the receiver without exactly recovering the\nbitstream. In this paper, we propose a generative audio semantic communication\nframework that faces the communication problem as an inverse problem, therefore\nbeing robust to different corruptions. Our method transmits lower-dimensional\nrepresentations of the audio signal and of the associated semantics to the\nreceiver, which generates the corresponding signal with a particular focus on\nits meaning (i.e., the semantics) thanks to the conditional diffusion model at\nits core. During the generation process, the diffusion model restores the\nreceived information from multiple degradations at the same time including\ncorruption noise and missing parts caused by the transmission over the noisy\nchannel. We show that our framework outperforms competitors in a real-world\nscenario and with different channel conditions. Visit the project page to\nlisten to samples and access the code:\nhttps://ispamm.github.io/diffusion-audio-semantic-communication/.", "published": "2023-09-13 13:54:07", "link": "http://arxiv.org/abs/2309.07195v1", "categories": ["cs.SD", "cs.ET", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Open-vocabulary Keyword-spotting with Adaptive Instance Normalization", "abstract": "Open vocabulary keyword spotting is a crucial and challenging task in\nautomatic speech recognition (ASR) that focuses on detecting user-defined\nkeywords within a spoken utterance. Keyword spotting methods commonly map the\naudio utterance and keyword into a joint embedding space to obtain some\naffinity score. In this work, we propose AdaKWS, a novel method for keyword\nspotting in which a text encoder is trained to output keyword-conditioned\nnormalization parameters. These parameters are used to process the auditory\ninput. We provide an extensive evaluation using challenging and diverse\nmulti-lingual benchmarks and show significant improvements over recent keyword\nspotting and ASR baselines. Furthermore, we study the effectiveness of our\napproach on low-resource languages that were unseen during the training. The\nresults demonstrate a substantial performance improvement compared to baseline\nmethods.", "published": "2023-09-13 13:49:42", "link": "http://arxiv.org/abs/2309.08561v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MASTERKEY: Practical Backdoor Attack Against Speaker Verification\n  Systems", "abstract": "Speaker Verification (SV) is widely deployed in mobile systems to\nauthenticate legitimate users by using their voice traits. In this work, we\npropose a backdoor attack MASTERKEY, to compromise the SV models. Different\nfrom previous attacks, we focus on a real-world practical setting where the\nattacker possesses no knowledge of the intended victim. To design MASTERKEY, we\ninvestigate the limitation of existing poisoning attacks against unseen\ntargets. Then, we optimize a universal backdoor that is capable of attacking\narbitrary targets. Next, we embed the speaker's characteristics and semantics\ninformation into the backdoor, making it imperceptible. Finally, we estimate\nthe channel distortion and integrate it into the backdoor. We validate our\nattack on 6 popular SV models. Specifically, we poison a total of 53 models and\nuse our trigger to attack 16,430 enrolled speakers, composed of 310 target\nspeakers enrolled in 53 poisoned models. Our attack achieves 100% attack\nsuccess rate with a 15% poison rate. By decreasing the poison rate to 3%, the\nattack success rate remains around 50%. We validate our attack in 3 real-world\nscenarios and successfully demonstrate the attack through both over-the-air and\nover-the-telephony-line scenarios.", "published": "2023-09-13 14:15:54", "link": "http://arxiv.org/abs/2309.06981v1", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
{"title": "Getting More for Less: Using Weak Labels and AV-Mixup for Robust\n  Audio-Visual Speaker Verification", "abstract": "Distance Metric Learning (DML) has typically dominated the audio-visual\nspeaker verification problem space, owing to strong performance in new and\nunseen classes. In our work, we explored multitask learning techniques to\nfurther enhance DML, and show that an auxiliary task with even weak labels can\nincrease the quality of the learned speaker representation without increasing\nmodel complexity during inference. We also extend the Generalized End-to-End\nLoss (GE2E) to multimodal inputs and demonstrate that it can achieve\ncompetitive performance in an audio-visual space. Finally, we introduce\nAV-Mixup, a multimodal augmentation technique during training time that has\nshown to reduce speaker overfit. Our network achieves state of the art\nperformance for speaker verification, reporting 0.244%, 0.252%, 0.441% Equal\nError Rate (EER) on the VoxCeleb1-O/E/H test sets, which is to our knowledge,\nthe best published results on VoxCeleb1-E and VoxCeleb1-H.", "published": "2023-09-13 17:45:41", "link": "http://arxiv.org/abs/2309.07115v2", "categories": ["cs.SD", "cs.CV", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AudioSR: Versatile Audio Super-resolution at Scale", "abstract": "Audio super-resolution is a fundamental task that predicts high-frequency\ncomponents for low-resolution audio, enhancing audio quality in digital\napplications. Previous methods have limitations such as the limited scope of\naudio types (e.g., music, speech) and specific bandwidth settings they can\nhandle (e.g., 4kHz to 8kHz). In this paper, we introduce a diffusion-based\ngenerative model, AudioSR, that is capable of performing robust audio\nsuper-resolution on versatile audio types, including sound effects, music, and\nspeech. Specifically, AudioSR can upsample any input audio signal within the\nbandwidth range of 2kHz to 16kHz to a high-resolution audio signal at 24kHz\nbandwidth with a sampling rate of 48kHz. Extensive objective evaluation on\nvarious audio super-resolution benchmarks demonstrates the strong result\nachieved by the proposed model. In addition, our subjective evaluation shows\nthat AudioSR can acts as a plug-and-play module to enhance the generation\nquality of a wide range of audio generative models, including AudioLDM,\nFastspeech2, and MusicGen. Our code and demo are available at\nhttps://audioldm.github.io/audiosr.", "published": "2023-09-13 21:00:09", "link": "http://arxiv.org/abs/2309.07314v1", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "Prompting Segmentation with Sound Is Generalizable Audio-Visual Source\n  Localizer", "abstract": "Never having seen an object and heard its sound simultaneously, can the model\nstill accurately localize its visual position from the input audio? In this\nwork, we concentrate on the Audio-Visual Localization and Segmentation tasks\nbut under the demanding zero-shot and few-shot scenarios. To achieve this goal,\ndifferent from existing approaches that mostly employ the\nencoder-fusion-decoder paradigm to decode localization information from the\nfused audio-visual feature, we introduce the encoder-prompt-decoder paradigm,\naiming to better fit the data scarcity and varying data distribution dilemmas\nwith the help of abundant knowledge from pre-trained models. Specifically, we\nfirst propose to construct Semantic-aware Audio Prompt (SAP) to help the visual\nfoundation model focus on sounding objects, meanwhile, the semantic gap between\nthe visual and audio modalities is also encouraged to shrink. Then, we develop\na Correlation Adapter (ColA) to keep minimal training efforts as well as\nmaintain adequate knowledge of the visual foundation model. By equipping with\nthese means, extensive experiments demonstrate that this new paradigm\noutperforms other fusion-based methods in both the unseen class and\ncross-dataset settings. We hope that our work can further promote the\ngeneralization study of Audio-Visual Localization and Segmentation in practical\napplication scenarios.", "published": "2023-09-13 05:43:35", "link": "http://arxiv.org/abs/2309.07929v3", "categories": ["cs.CV", "cs.LG", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
