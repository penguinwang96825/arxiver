{"title": "GKT: A Novel Guidance-Based Knowledge Transfer Framework For Efficient\n  Cloud-edge Collaboration LLM Deployment", "abstract": "The burgeoning size of Large Language Models (LLMs) has led to enhanced\ncapabilities in generating responses, albeit at the expense of increased\ninference times and elevated resource demands. Existing methods of\nacceleration, predominantly hinged on knowledge distillation, generally\nnecessitate fine-tuning of considerably large models, such as Llama-7B, posing\na challenge for average users. Furthermore, present techniques for expediting\ninference and reducing costs operate independently. To address these issues, we\nintroduce a novel and intuitive Guidance-based Knowledge Transfer (GKT)\nframework. This approach leverages a larger LLM as a ''teacher'' to create\nguidance prompts, paired with a smaller ''student'' model to finalize\nresponses. Remarkably, GKT requires no fine-tuning and doesn't necessitate the\nteacher and student models to have the same vocabulary, allowing for extensive\nbatch generation to accelerate the process while ensuring user customization.\nGKT can be seamlessly integrated into cloud-edge collaboration architectures,\nand is versatile enough for plug-and-play application across various models. It\nexcels in both efficiency and affordability, epitomizing a ''cheap and\ncheerful'' solution. GKT achieves a maximum accuracy improvement of 14.18%,\nalong with a 10.72 times speed-up on GSM8K and an accuracy improvement of 14.00\n% along with a 7.73 times speed-up in CSQA. When utilizing ChatGPT as teacher\nmodel and Llama2-70B as the student model, we can achieve 95.00% of ChatGPT's\nperformance at 52% of the cost. The results highlight substantial enhancements\nin accuracy and processing speed on the GSM8K and CSQA datasets, surpassing the\nperformance of using either the student or teacher models in isolation.", "published": "2024-05-30 02:37:35", "link": "http://arxiv.org/abs/2405.19635v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PATIENT-\u03a8: Using Large Language Models to Simulate Patients for\n  Training Mental Health Professionals", "abstract": "Mental illness remains one of the most critical public health issues. Despite\nits importance, many mental health professionals highlight a disconnect between\ntheir training and actual real-world patient practice. To help bridge this gap,\nwe propose PATIENT-{\\Psi}, a novel patient simulation framework for cognitive\nbehavior therapy (CBT) training. To build PATIENT-{\\Psi}, we construct diverse\npatient cognitive models based on CBT principles and use large language models\n(LLMs) programmed with these cognitive models to act as a simulated therapy\npatient. We propose an interactive training scheme, PATIENT-{\\Psi}-TRAINER, for\nmental health trainees to practice a key skill in CBT -- formulating the\ncognitive model of the patient -- through role-playing a therapy session with\nPATIENT-{\\Psi}. To evaluate PATIENT-{\\Psi}, we conducted a comprehensive user\nstudy of 13 mental health trainees and 20 experts. The results demonstrate that\npractice using PATIENT-{\\Psi}-TRAINER enhances the perceived skill acquisition\nand confidence of the trainees beyond existing forms of training such as\ntextbooks, videos, and role-play with non-patients. Based on the experts'\nperceptions, PATIENT-{\\Psi} is perceived to be closer to real patient\ninteractions than GPT-4, and PATIENT-{\\Psi}-TRAINER holds strong promise to\nimprove trainee competencies. Our code and data are released at\n\\url{https://github.com/ruiyiw/patient-psi}.", "published": "2024-05-30 03:20:56", "link": "http://arxiv.org/abs/2405.19660v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "One Token Can Help! Learning Scalable and Pluggable Virtual Tokens for\n  Retrieval-Augmented Large Language Models", "abstract": "Retrieval-augmented generation (RAG) is a promising way to improve large\nlanguage models (LLMs) for generating more factual, accurate, and up-to-date\ncontent. Existing methods either optimize prompts to guide LLMs in leveraging\nretrieved information or directly fine-tune LLMs to adapt to RAG scenarios.\nAlthough fine-tuning can yield better performance, it often compromises the\nLLMs' general generation capabilities by modifying their parameters. This\nlimitation poses challenges in practical applications, especially when LLMs are\nalready deployed, as parameter adjustments may affect their original\nfunctionality. To address this, we propose a novel method that involves\nlearning scalable and pluggable virtual tokens for RAG. By maintaining the\nLLMs' original parameters and fine-tuning only the embeddings of these\npluggable tokens, our approach not only enhances LLMs' performance but also\npreserves their general generation capabilities. Furthermore, we design several\ntraining strategies to improve the scalability, flexibility, and\ngeneralizability of our method. Comprehensive experiments across 12\nquestion-answering tasks demonstrate the superiority of our approach.", "published": "2024-05-30 03:44:54", "link": "http://arxiv.org/abs/2405.19670v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Reinforcement Learning with Label-Sensitive Reward for Natural\n  Language Understanding", "abstract": "Recent strides in large language models (LLMs) have yielded remarkable\nperformance, leveraging reinforcement learning from human feedback (RLHF) to\nsignificantly enhance generation and alignment capabilities. However, RLHF\nencounters numerous challenges, including the objective mismatch issue, leading\nto suboptimal performance in Natural Language Understanding (NLU) tasks. To\naddress this limitation, we propose a novel Reinforcement Learning framework\nenhanced with Label-sensitive Reward (RLLR) to amplify the performance of LLMs\nin NLU tasks. By incorporating label-sensitive pairs into reinforcement\nlearning, our method aims to adeptly capture nuanced label-sensitive semantic\nfeatures during RL, thereby enhancing natural language understanding.\nExperiments conducted on five diverse foundation models across eight tasks\nshowcase promising results. In comparison to Supervised Fine-tuning models\n(SFT), RLLR demonstrates an average performance improvement of 1.54%. Compared\nwith RLHF models, the improvement averages at 0.69%. These results reveal the\neffectiveness of our method for LLMs in NLU tasks. Code and data available at:\nhttps://github.com/MagiaSN/ACL2024_RLLR.", "published": "2024-05-30 07:19:31", "link": "http://arxiv.org/abs/2405.19763v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PDDLEGO: Iterative Planning in Textual Environments", "abstract": "Planning in textual environments have been shown to be a long-standing\nchallenge even for current models. A recent, promising line of work uses LLMs\nto generate a formal representation of the environment that can be solved by a\nsymbolic planner. However, existing methods rely on a fully-observed\nenvironment where all entity states are initially known, so a one-off\nrepresentation can be constructed, leading to a complete plan. In contrast, we\ntackle partially-observed environments where there is initially no sufficient\ninformation to plan for the end-goal. We propose PDDLEGO that iteratively\nconstruct a planning representation that can lead to a partial plan for a given\nsub-goal. By accomplishing the sub-goal, more information is acquired to\naugment the representation, eventually achieving the end-goal. We show that\nplans produced by few-shot PDDLEGO are 43% more efficient than generating plans\nend-to-end on the Coin Collector simulation, with strong performance (98%) on\nthe more complex Cooking World simulation where end-to-end LLMs fail to\ngenerate coherent plans (4%).", "published": "2024-05-30 08:01:20", "link": "http://arxiv.org/abs/2405.19793v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Mutual Learning of Discourse Parsing and Topic Segmentation\n  in Dialogue", "abstract": "In dialogue systems, discourse plays a crucial role in managing\nconversational focus and coordinating interactions. It consists of two key\nstructures: rhetorical structure and topic structure. The former captures the\nlogical flow of conversations, while the latter detects transitions between\ntopics. Together, they improve the ability of a dialogue system to track\nconversation dynamics and generate contextually relevant high-quality\nresponses. These structures are typically identified through discourse parsing\nand topic segmentation, respectively. However, existing supervised methods rely\non costly manual annotations, while unsupervised methods often focus on a\nsingle task, overlooking the deep linguistic interplay between rhetorical and\ntopic structures. To address these issues, we first introduce a unified\nrepresentation that integrates rhetorical and topic structures, ensuring\nsemantic consistency between them. Under the unified representation, we further\npropose two linguistically grounded hypotheses based on discourse theories: (1)\nLocal Discourse Coupling, where rhetorical cues dynamically enhance topic-aware\ninformation flow, and (2) Global Topology Constraint, where topic structure\npatterns probabilistically constrain rhetorical relation distributions.\nBuilding on the unified representation and two hypotheses, we propose an\nunsupervised mutual learning framework (UMLF) that jointly models rhetorical\nand topic structures, allowing them to mutually reinforce each other without\nrequiring additional annotations. We evaluate our approach on two rhetorical\ndatasets and three topic segmentation datasets. Experimental results\ndemonstrate that our method surpasses all strong baselines built on pre-trained\nlanguage models. Furthermore, when applied to LLMs, our framework achieves\nnotable improvements, demonstrating its effectiveness in improving discourse\nstructure modeling.", "published": "2024-05-30 08:10:50", "link": "http://arxiv.org/abs/2405.19799v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Just Rewrite It Again: A Post-Processing Method for Enhanced Semantic\n  Similarity and Privacy Preservation of Differentially Private Rewritten Text", "abstract": "The study of Differential Privacy (DP) in Natural Language Processing often\nviews the task of text privatization as a $\\textit{rewriting}$ task, in which\nsensitive input texts are rewritten to hide explicit or implicit private\ninformation. In order to evaluate the privacy-preserving capabilities of a DP\ntext rewriting mechanism, $\\textit{empirical privacy}$ tests are frequently\nemployed. In these tests, an adversary is modeled, who aims to infer sensitive\ninformation (e.g., gender) about the author behind a (privatized) text. Looking\nto improve the empirical protections provided by DP rewriting methods, we\npropose a simple post-processing method based on the goal of aligning rewritten\ntexts with their original counterparts, where DP rewritten texts are rewritten\n$\\textit{again}$. Our results show that such an approach not only produces\noutputs that are more semantically reminiscent of the original inputs, but also\ntexts which score on average better in empirical privacy evaluations.\nTherefore, our approach raises the bar for DP rewriting methods in their\nempirical privacy evaluations, providing an extra layer of protection against\nmalicious adversaries.", "published": "2024-05-30 08:41:33", "link": "http://arxiv.org/abs/2405.19831v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Fine-Tuning Paradox: Boosting Translation Quality Without\n  Sacrificing LLM Abilities", "abstract": "Fine-tuning large language models (LLMs) for machine translation has shown\nimprovements in overall translation quality. However, it is unclear what is the\nimpact of fine-tuning on desirable LLM behaviors that are not present in neural\nmachine translation models, such as steerability, inherent document-level\ntranslation abilities, and the ability to produce less literal translations. We\nperform an extensive translation evaluation on the LLaMA and Falcon family of\nmodels with model size ranging from 7 billion up to 65 billion parameters. Our\nresults show that while fine-tuning improves the general translation quality of\nLLMs, several abilities degrade. In particular, we observe a decline in the\nability to perform formality steering, to produce technical translations\nthrough few-shot examples, and to perform document-level translation. On the\nother hand, we observe that the model produces less literal translations after\nfine-tuning on parallel data. We show that by including monolingual data as\npart of the fine-tuning data we can maintain the abilities while simultaneously\nenhancing overall translation quality. Our findings emphasize the need for\nfine-tuning strategies that preserve the benefits of LLMs for machine\ntranslation.", "published": "2024-05-30 14:25:56", "link": "http://arxiv.org/abs/2405.20089v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Heidelberg-Boston @ SIGTYP 2024 Shared Task: Enhancing Low-Resource\n  Language Analysis With Character-Aware Hierarchical Transformers", "abstract": "Historical languages present unique challenges to the NLP community, with one\nprominent hurdle being the limited resources available in their closed corpora.\nThis work describes our submission to the constrained subtask of the SIGTYP\n2024 shared task, focusing on PoS tagging, morphological tagging, and\nlemmatization for 13 historical languages. For PoS and morphological tagging we\nadapt a hierarchical tokenization method from Sun et al. (2023) and combine it\nwith the advantages of the DeBERTa-V3 architecture, enabling our models to\nefficiently learn from every character in the training data. We also\ndemonstrate the effectiveness of character-level T5 models on the lemmatization\ntask. Pre-trained from scratch with limited data, our models achieved first\nplace in the constrained subtask, nearly reaching the performance levels of the\nunconstrained task's winner. Our code is available at\nhttps://github.com/bowphs/SIGTYP-2024-hierarchical-transformers", "published": "2024-05-30 15:23:34", "link": "http://arxiv.org/abs/2405.20145v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "TAIA: Large Language Models are Out-of-Distribution Data Learners", "abstract": "Fine-tuning on task-specific question-answer pairs is a predominant method\nfor enhancing the performance of instruction-tuned large language models (LLMs)\non downstream tasks. However, in certain specialized domains, such as\nhealthcare or harmless content generation, it is nearly impossible to obtain a\nlarge volume of high-quality data that matches the downstream distribution. To\nimprove the performance of LLMs in data-scarce domains with domain-mismatched\ndata, we re-evaluated the Transformer architecture and discovered that not all\nparameter updates during fine-tuning contribute positively to downstream\nperformance. Our analysis reveals that within the self-attention and\nfeed-forward networks, only the fine-tuned attention parameters are\nparticularly beneficial when the training set's distribution does not fully\nalign with the test set. Based on this insight, we propose an effective\ninference-time intervention method: Training All parameters but Inferring with\nonly Attention (\\trainallInfAttn). We empirically validate \\trainallInfAttn\nusing two general instruction-tuning datasets and evaluate it on seven\ndownstream tasks involving math, reasoning, and knowledge understanding across\nLLMs of different parameter sizes and fine-tuning techniques. Our comprehensive\nexperiments demonstrate that \\trainallInfAttn achieves superior improvements\ncompared to both the fully fine-tuned model and the base model in most\nscenarios, with significant performance gains. The high tolerance of\n\\trainallInfAttn to data mismatches makes it resistant to jailbreaking tuning\nand enhances specialized tasks using general data. Code is available in\n\\url{https://github.com/pixas/TAIA_LLM}.", "published": "2024-05-30 15:57:19", "link": "http://arxiv.org/abs/2405.20192v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TS-Align: A Teacher-Student Collaborative Framework for Scalable\n  Iterative Finetuning of Large Language Models", "abstract": "Mainstream approaches to aligning large language models (LLMs) heavily rely\non human preference data, particularly when models require periodic updates.\nThe standard process for iterative alignment of LLMs involves collecting new\nhuman feedback for each update. However, the data collection process is costly\nand challenging to scale. To address this issue, we introduce the \"TS-Align\"\nframework, which fine-tunes a policy model using pairwise feedback data\nautomatically mined from its outputs. This automatic mining process is\nefficiently accomplished through the collaboration between a large-scale\nteacher model and a small-scale student model. The policy fine-tuning process\ncan be iteratively repeated using on-policy generations within our proposed\nteacher-student collaborative framework. Through extensive experiments, we\ndemonstrate that our final aligned policy outperforms the base policy model\nwith an average win rate of 69.7% across seven conversational or\ninstruction-following datasets. Furthermore, we show that the ranking\ncapability of the teacher is effectively distilled into the student through our\npipeline, resulting in a small-scale yet effective reward model for policy\nmodel alignment.", "published": "2024-05-30 16:17:40", "link": "http://arxiv.org/abs/2405.20215v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Hierarchical Multi-Agent Workflows for Zero-Shot Prompt\n  Optimization", "abstract": "Large language models (LLMs) have shown great progress in responding to user\nquestions, allowing for a multitude of diverse applications. Yet, the quality\nof LLM outputs heavily depends on the prompt design, where a good prompt might\nenable the LLM to answer a very challenging question correctly. Therefore,\nrecent works have developed many strategies for improving the prompt, including\nboth manual crafting and in-domain optimization. However, their efficacy in\nunrestricted scenarios remains questionable, as the former depends on human\ndesign for specific questions and the latter usually generalizes poorly to\nunseen scenarios. To address these problems, we give LLMs the freedom to design\nthe best prompts according to themselves. Specifically, we include a hierarchy\nof LLMs, first constructing a prompt with precise instructions and accurate\nwording in a hierarchical manner, and then using this prompt to generate the\nfinal answer to the user query. We term this pipeline Hierarchical Multi-Agent\nWorkflow, or HMAW. In contrast with prior works, HMAW imposes no human\nrestriction and requires no training, and is completely task-agnostic while\ncapable of adjusting to the nuances of the underlying task. Through both\nquantitative and qualitative experiments across multiple benchmarks, we verify\nthat despite its simplicity, the proposed approach can create detailed and\nsuitable prompts, further boosting the performance of current LLMs.", "published": "2024-05-30 17:05:45", "link": "http://arxiv.org/abs/2405.20252v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Large Language Model Biases in Persona-Steered Generation", "abstract": "The task of persona-steered text generation requires large language models\n(LLMs) to generate text that reflects the distribution of views that an\nindividual fitting a persona could have. People have multifaceted personas, but\nprior work on bias in LLM-generated opinions has only explored multiple-choice\nsettings or one-dimensional personas. We define an incongruous persona as a\npersona with multiple traits where one trait makes its other traits less likely\nin human survey data, e.g. political liberals who support increased military\nspending. We find that LLMs are 9.7% less steerable towards incongruous\npersonas than congruous ones, sometimes generating the stereotypical stance\nassociated with its demographic rather than the target stance. Models that we\nevaluate that are fine-tuned with Reinforcement Learning from Human Feedback\n(RLHF) are more steerable, especially towards stances associated with political\nliberals and women, but present significantly less diverse views of personas.\nWe also find variance in LLM steerability that cannot be predicted from\nmultiple-choice opinion evaluation. Our results show the importance of\nevaluating models in open-ended text generation, as it can surface new LLM\nopinion biases. Moreover, such a setup can shed light on our ability to steer\nmodels toward a richer and more diverse range of viewpoints.", "published": "2024-05-30 17:06:03", "link": "http://arxiv.org/abs/2405.20253v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Auto-Arena: Automating LLM Evaluations with Agent Peer Battles and\n  Committee Discussions", "abstract": "As LLMs continuously evolve, there is an urgent need for a reliable\nevaluation method that delivers trustworthy results promptly. Currently, static\nbenchmarks suffer from inflexibility and unreliability, leading users to prefer\nhuman voting platforms like Chatbot Arena. However, human evaluations require\nsignificant manual effort. To address this, we propose the Auto-Arena, an\ninnovative framework that automates the entire evaluation process using\nLLM-powered agents. Firstly, an LLM examiner generates questions. Then, two LLM\ncandidates engage in a multi-round peer battle based on individual questions,\naiming at revealing their true performance differences. Finally, a committee of\nLLM judges collaboratively discusses and decides the winner, reducing bias and\nenhancing fairness. During the peer battles, we observe intriguing scenarios\nwhere the LLM candidates display competitive behaviors and even learn from the\nopponents. In our extensive experiments involving 15 recent LLMs, Auto-Arena\nshows a 92.14% correlation with human preferences, surpassing all previous\nexpert-annotated benchmarks without any manual efforts. As a result, Auto-Arena\noffers a promising alternative to current human evaluation platforms for\nevaluating LLMs automatically.", "published": "2024-05-30 17:19:19", "link": "http://arxiv.org/abs/2405.20267v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IsraParlTweet: The Israeli Parliamentary and Twitter Resource", "abstract": "We introduce IsraParlTweet, a new linked corpus of Hebrew-language\nparliamentary discussions from the Knesset (Israeli Parliament) between the\nyears 1992-2023 and Twitter posts made by Members of the Knesset between the\nyears 2008-2023, containing a total of 294.5 million Hebrew tokens. In addition\nto raw text, the corpus contains comprehensive metadata on speakers and Knesset\nsessions as well as several linguistic annotations. As a result, IsraParlTweet\ncan be used to conduct a wide variety of quantitative and qualitative analyses\nand provide valuable insights into political discourse in Israel.", "published": "2024-05-30 17:21:15", "link": "http://arxiv.org/abs/2405.20269v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Who Writes the Review, Human or AI?", "abstract": "With the increasing use of Artificial Intelligence in Natural Language\nProcessing, concerns have been raised regarding the detection of AI-generated\ntext in various domains. This study aims to investigate this issue by proposing\na methodology to accurately distinguish AI-generated and human-written book\nreviews. Our approach utilizes transfer learning, enabling the model to\nidentify generated text across different topics while improving its ability to\ndetect variations in writing style and vocabulary. To evaluate the\neffectiveness of the proposed methodology, we developed a dataset consisting of\nreal book reviews and AI-generated reviews using the recently proposed Vicuna\nopen-source language model. The experimental results demonstrate that it is\nfeasible to detect the original source of text, achieving an accuracy rate of\n96.86%. Our efforts are oriented toward the exploration of the capabilities and\nlimitations of Large Language Models in the context of text identification.\nExpanding our knowledge in these aspects will be valuable for effectively\nnavigating similar models in the future and ensuring the integrity and\nauthenticity of human-generated content.", "published": "2024-05-30 17:38:44", "link": "http://arxiv.org/abs/2405.20285v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "S3D: A Simple and Cost-Effective Self-Speculative Decoding Scheme for\n  Low-Memory GPUs", "abstract": "Speculative decoding (SD) has attracted a significant amount of research\nattention due to the substantial speedup it can achieve for LLM inference.\nHowever, despite the high speedups they offer, speculative decoding methods\noften achieve optimal performance on high-end devices or with a substantial GPU\nmemory overhead. Given limited memory and the necessity of quantization, a\nhigh-performing model on a high-end GPU can slow down by up to 7 times. To this\nend, we propose Skippy Simultaneous Speculative Decoding (or S3D), a\ncost-effective self-speculative SD method based on simultaneous multi-token\ndecoding and mid-layer skipping. When compared against recent effective\nopen-source SD systems, our method has achieved one of the top\nperformance-memory ratios while requiring minimal architecture changes and\ntraining data. Leveraging our memory efficiency, we created a smaller yet more\neffective SD model based on Phi-3. It is 1.4 to 2 times faster than the\nquantized EAGLE model and operates in half-precision while using less VRAM.", "published": "2024-05-30 17:54:35", "link": "http://arxiv.org/abs/2405.20314v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Xwin-LM: Strong and Scalable Alignment Practice for LLMs", "abstract": "In this work, we present Xwin-LM, a comprehensive suite of alignment\nmethodologies for large language models (LLMs). This suite encompasses several\nkey techniques, including supervised finetuning (SFT), reward modeling (RM),\nrejection sampling finetuning (RS), and direct preference optimization (DPO).\nThe key components are as follows: (1) Xwin-LM-SFT, models initially finetuned\nwith high-quality instruction data; (2) Xwin-Pair, a large-scale, multi-turn\npreference dataset meticulously annotated using GPT-4; (3) Xwin-RM, reward\nmodels trained on Xwin-Pair, developed at scales of 7B, 13B, and 70B\nparameters; (4) Xwin-Set, a multiwise preference dataset in which each prompt\nis linked to 64 unique responses generated by Xwin-LM-SFT and scored by\nXwin-RM; (5) Xwin-LM-RS, models finetuned with the highest-scoring responses\nfrom Xwin-Set; (6) Xwin-LM-DPO, models further optimized on Xwin-Set using the\nDPO algorithm. Our evaluations on AlpacaEval and MT-bench demonstrate\nconsistent and significant improvements across the pipeline, demonstrating the\nstrength and scalability of Xwin-LM. The repository\nhttps://github.com/Xwin-LM/Xwin-LM will be continually updated to foster\ncommunity research.", "published": "2024-05-30 17:59:31", "link": "http://arxiv.org/abs/2405.20335v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Scalable Detection of Salient Entities in News Articles", "abstract": "News articles typically mention numerous entities, a large fraction of which\nare tangential to the story. Detecting the salience of entities in articles is\nthus important to applications such as news search, analysis and summarization.\nIn this work, we explore new approaches for efficient and effective salient\nentity detection by fine-tuning pretrained transformer models with\nclassification heads that use entity tags or contextualized entity\nrepresentations directly. Experiments show that these straightforward\ntechniques dramatically outperform prior work across datasets with varying\nsizes and salience definitions. We also study knowledge distillation techniques\nto effectively reduce the computational cost of these models without affecting\ntheir accuracy. Finally, we conduct extensive analyses and ablation experiments\nto characterize the behavior of the proposed models.", "published": "2024-05-30 20:16:27", "link": "http://arxiv.org/abs/2405.20461v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automated Focused Feedback Generation for Scientific Writing Assistance", "abstract": "Scientific writing is a challenging task, particularly for novice researchers\nwho often rely on feedback from experienced peers. Recent work has primarily\nfocused on improving surface form and style rather than manuscript content. In\nthis paper, we propose a novel task: automated focused feedback generation for\nscientific writing assistance. We present SWIF$^{2}$T: a Scientific WrIting\nFocused Feedback Tool. It is designed to generate specific, actionable and\ncoherent comments, which identify weaknesses in a scientific paper and/or\npropose revisions to it. Our approach consists of four components - planner,\ninvestigator, reviewer and controller - leveraging multiple Large Language\nModels (LLMs) to implement them. We compile a dataset of 300 peer reviews\nciting weaknesses in scientific papers and conduct human evaluation. The\nresults demonstrate the superiority in specificity, reading comprehension, and\noverall helpfulness of SWIF$^{2}$T's feedback compared to other approaches. In\nour analysis, we also identified cases where automatically generated reviews\nwere judged better than human ones, suggesting opportunities for integration of\nAI-generated feedback in scientific writing.", "published": "2024-05-30 20:56:41", "link": "http://arxiv.org/abs/2405.20477v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PGA-SciRE: Harnessing LLM on Data Augmentation for Enhancing Scientific\n  Relation Extraction", "abstract": "Relation Extraction (RE) aims at recognizing the relation between pairs of\nentities mentioned in a text. Advances in LLMs have had a tremendous impact on\nNLP. In this work, we propose a textual data augmentation framework called PGA\nfor improving the performance of models for RE in the scientific domain. The\nframework introduces two ways of data augmentation, utilizing a LLM to obtain\npseudo-samples with the same sentence meaning but with different\nrepresentations and forms by paraphrasing the original training set samples. As\nwell as instructing LLM to generate sentences that implicitly contain\ninformation about the corresponding labels based on the relation and entity of\nthe original training set samples. These two kinds of pseudo-samples\nparticipate in the training of the RE model together with the original dataset,\nrespectively. The PGA framework in the experiment improves the F1 scores of the\nthree mainstream models for RE within the scientific domain. Also, using a LLM\nto obtain samples can effectively reduce the cost of manually labeling data.", "published": "2024-05-30 13:07:54", "link": "http://arxiv.org/abs/2405.20787v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cutting Through the Noise: Boosting LLM Performance on Math Word\n  Problems", "abstract": "Large Language Models (LLMs) excel at various tasks, including solving math\nword problems (MWPs), but struggle with real-world problems containing\nirrelevant information. To address this, we propose a prompting framework that\ngenerates adversarial variants of MWPs by adding irrelevant variables. We\nintroduce a dataset, PROBLEMATHIC, containing both adversarial and\nnon-adversarial MWPs. Our experiments reveal that LLMs are susceptible to\ndistraction by numerical noise, resulting in an average relative performance\ndrop of ~26% on adversarial MWPs. To mitigate this, we fine-tune LLMs (Llama-2,\nMistral) on the adversarial samples from our dataset. Fine-tuning on\nadversarial training instances improves performance on adversarial MWPs by ~8%,\nindicating increased robustness to noise and improved ability to identify\nrelevant data for reasoning. Finally, to assess the generalizability of our\nprompting framework, we introduce GSM-8K-Adv, an adversarial variant of the\nGSM-8K benchmark. LLMs continue to struggle when faced with adversarial\ninformation, reducing performance by up to 6%.", "published": "2024-05-30 18:07:13", "link": "http://arxiv.org/abs/2406.15444v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Significance of Chain of Thought in Gender Bias Mitigation for\n  English-Dravidian Machine Translation", "abstract": "Gender bias in machine translation (MT) sys- tems poses a significant\nchallenge to achieving accurate and inclusive translations. This paper examines\ngender bias in machine translation systems for languages such as Telugu and\nKan- nada from the Dravidian family, analyzing how gender inflections affect\ntranslation accuracy and neutrality using Google Translate and Chat- GPT. It\nfinds that while plural forms can reduce bias, individual-centric sentences\noften main- tain the bias due to historical stereotypes. The study evaluates\nthe Chain of Thought process- ing, noting significant bias mitigation from 80%\nto 4% in Telugu and from 40% to 0% in Kan- nada. It also compares Telugu and\nKannada translations, emphasizing the need for language specific strategies to\naddress these challenges and suggesting directions for future research to\nenhance fairness in both data preparation and prompts during inference.", "published": "2024-05-30 05:26:57", "link": "http://arxiv.org/abs/2405.19701v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing Large Vision Language Models with Self-Training on Image\n  Comprehension", "abstract": "Large vision language models (LVLMs) integrate large language models (LLMs)\nwith pre-trained vision encoders, thereby activating the perception capability\nof the model to understand image inputs for different queries and conduct\nsubsequent reasoning. Improving this capability requires high-quality\nvision-language data, which is costly and labor-intensive to acquire.\nSelf-training approaches have been effective in single-modal settings to\nalleviate the need for labeled data by leveraging model's own generation.\nHowever, effective self-training remains a challenge regarding the unique\nvisual perception and reasoning capability of LVLMs. To address this, we\nintroduce Self-Training on Image Comprehension (STIC), which emphasizes a\nself-training approach specifically for image comprehension. First, the model\nself-constructs a preference dataset for image descriptions using unlabeled\nimages. Preferred responses are generated through a step-by-step prompt, while\ndis-preferred responses are generated from either corrupted images or\nmisleading prompts. To further self-improve reasoning on the extracted visual\ninformation, we let the model reuse a small portion of existing\ninstruction-tuning data and append its self-generated image descriptions to the\nprompts. We validate the effectiveness of STIC across seven different\nbenchmarks, demonstrating substantial performance gains of 4.0% on average\nwhile using 70% less supervised fine-tuning data than the current method.\nFurther studies investigate various components of STIC and highlight its\npotential to leverage vast quantities of unlabeled images for self-training.\nCode and data are made publicly available.", "published": "2024-05-30 05:53:49", "link": "http://arxiv.org/abs/2405.19716v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Beyond Imitation: Learning Key Reasoning Steps from Dual\n  Chain-of-Thoughts in Reasoning Distillation", "abstract": "As Large Language Models (LLMs) scale up and gain powerful Chain-of-Thoughts\n(CoTs) reasoning abilities, practical resource constraints drive efforts to\ndistill these capabilities into more compact Smaller Language Models (SLMs). We\nfind that CoTs consist mainly of simple reasoning forms, with a small\nproportion ($\\approx 4.7\\%$) of key reasoning steps that truly impact\nconclusions. However, previous distillation methods typically involve\nsupervised fine-tuning student SLMs only on correct CoTs data produced by\nteacher LLMs, resulting in students struggling to learn the key reasoning\nsteps, instead imitating the teacher's reasoning forms and making errors or\nomissions on these steps. To address these issues, drawing an analogy to human\nlearning, where analyzing mistakes according to correct solutions often reveals\nthe crucial steps leading to successes or failures, we propose\nmistak\\textbf{E}-\\textbf{D}riven key reason\\textbf{I}ng step\ndistilla\\textbf{T}ion (\\textbf{EDIT}), a novel method that further aids SLMs\nlearning key reasoning steps rather than mere simple fine-tuning. Firstly, to\nexpose these crucial steps in CoTs, we design specific prompts to generate dual\nCoTs data with similar reasoning paths but divergent conclusions. Then, we\napply the minimum edit distance algorithm on the dual CoTs data to locate these\nkey steps and optimize the likelihood of these steps. Extensive experiments\nvalidate the effectiveness of EDIT across both in-domain and out-of-domain\nbenchmark reasoning datasets. Further analysis shows that EDIT can generate\nhigh-quality CoTs with more correct key reasoning steps. Notably, we also\nexplore how different mistake patterns affect performance and find that EDIT\nbenefits more from logical errors than from knowledge or mathematical\ncalculation errors in dual CoTs\\footnote{Code can be found at\n\\url{https://github.com/C-W-D/EDIT}}.", "published": "2024-05-30 06:32:11", "link": "http://arxiv.org/abs/2405.19737v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "X-Instruction: Aligning Language Model in Low-resource Languages with\n  Self-curated Cross-lingual Instructions", "abstract": "Large language models respond well in high-resource languages like English\nbut struggle in low-resource languages. It may arise from the lack of\nhigh-quality instruction following data in these languages. Directly\ntranslating English samples into these languages can be a solution but\nunreliable, leading to responses with translation errors and lacking\nlanguage-specific or cultural knowledge. To address this issue, we propose a\nnovel method to construct cross-lingual instruction following samples with\ninstruction in English and response in low-resource languages. Specifically,\nthe language model first learns to generate appropriate English instructions\naccording to the natural web texts in other languages as responses. The\ncandidate cross-lingual instruction tuning samples are further refined and\ndiversified. We have employed this method to build a large-scale cross-lingual\ninstruction tuning dataset on 10 languages, namely X-Instruction. The\ninstruction data built using our method incorporate more language-specific\nknowledge compared with the naive translation method. Experimental results have\nshown that the response quality of the model tuned on X-Instruction greatly\nexceeds the model distilled from a powerful teacher model, reaching or even\nsurpassing the ones of ChatGPT. In addition, we find that models tuned on\ncross-lingual instruction following samples can follow the instruction in the\noutput language without further tuning.", "published": "2024-05-30 06:45:23", "link": "http://arxiv.org/abs/2405.19744v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CharacterGPT: A Persona Reconstruction Framework for Role-Playing Agents", "abstract": "The recent introduction of the Assistants API highlights its potential for\nlarge language models (LLMs) in role-playing agents (RPA). However, maintaining\nconsistent character personas remains a significant challenge due to\nvariability in information extraction, which frequently omits critical elements\nsuch as backstory or interpersonal relationships. To address this limitation,\nwe introduce CharacterGPT, a framework designed to dynamically reconstruct\ncharacter personas through Character Persona Training (CPT). This approach\nincrementally updates personas by extracting traits from chapter-wise novel\nsummaries, reflecting the progression of the narrative. Our framework is\nevaluated through Big Five personality evaluations and creative tasks, in which\ncharacters generate original narratives, demonstrating the efficacy of\nCharacterGPT in preserving persona consistency. The code and results are\navailable at https://github.com/Jeiyoon/charactergpt", "published": "2024-05-30 07:44:16", "link": "http://arxiv.org/abs/2405.19778v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dataflow-Guided Retrieval Augmentation for Repository-Level Code\n  Completion", "abstract": "Recent years have witnessed the deployment of code language models (LMs) in\nvarious code intelligence tasks such as code completion. Yet, it is challenging\nfor pre-trained LMs to generate correct completions in private repositories.\nPrevious studies retrieve cross-file context based on import relations or text\nsimilarity, which is insufficiently relevant to completion targets. In this\npaper, we propose a dataflow-guided retrieval augmentation approach, called\nDraCo, for repository-level code completion. DraCo parses a private repository\ninto code entities and establishes their relations through an extended dataflow\nanalysis, forming a repo-specific context graph. Whenever triggering code\ncompletion, DraCo precisely retrieves relevant background knowledge from the\nrepo-specific context graph and generates well-formed prompts to query code\nLMs. Furthermore, we construct a large Python dataset, ReccEval, with more\ndiverse completion targets. Our experiments demonstrate the superior accuracy\nand applicable efficiency of DraCo, improving code exact match by 3.43% and\nidentifier F1-score by 3.27% on average compared to the state-of-the-art\napproach.", "published": "2024-05-30 07:48:00", "link": "http://arxiv.org/abs/2405.19782v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "SLM as Guardian: Pioneering AI Safety with Small Language Models", "abstract": "Most prior safety research of large language models (LLMs) has focused on\nenhancing the alignment of LLMs to better suit the safety requirements of\nhumans. However, internalizing such safeguard features into larger models\nbrought challenges of higher training cost and unintended degradation of\nhelpfulness. To overcome such challenges, a modular approach employing a\nsmaller LLM to detect harmful user queries is regarded as a convenient solution\nin designing LLM-based system with safety requirements.\n  In this paper, we leverage a smaller LLM for both harmful query detection and\nsafeguard response generation. We introduce our safety requirements and the\ntaxonomy of harmfulness categories, and then propose a multi-task learning\nmechanism fusing the two tasks into a single model. We demonstrate the\neffectiveness of our approach, providing on par or surpassing harmful query\ndetection and safeguard response performance compared to the publicly available\nLLMs.", "published": "2024-05-30 08:03:15", "link": "http://arxiv.org/abs/2405.19795v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improve Student's Reasoning Generalizability through Cascading\n  Decomposed CoTs Distillation", "abstract": "Large language models (LLMs) exhibit enhanced reasoning at larger scales,\ndriving efforts to distill these capabilities into smaller models via\nteacher-student learning. Previous works simply fine-tune student models on\nteachers' generated Chain-of-Thoughts (CoTs) data. Although these methods\nenhance in-domain (IND) reasoning performance, they struggle to generalize to\nout-of-domain (OOD) tasks. We believe that the widespread spurious correlations\nbetween questions and answers may lead the model to preset a specific answer\nwhich restricts the diversity and generalizability of its reasoning process. In\nthis paper, we propose Cascading Decomposed CoTs Distillation (CasCoD) to\naddress these issues by decomposing the traditional single-step learning\nprocess into two cascaded learning steps. Specifically, by restructuring the\ntraining objectives -- removing the answer from outputs and concatenating the\nquestion with the rationale as input -- CasCoD's two-step learning process\nensures that students focus on learning rationales without interference from\nthe preset answers, thus improving reasoning generalizability. Extensive\nexperiments demonstrate the effectiveness of CasCoD on both IND and OOD\nbenchmark reasoning datasets. Code can be found at\nhttps://github.com/C-W-D/CasCoD.", "published": "2024-05-30 08:49:34", "link": "http://arxiv.org/abs/2405.19842v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Quest: Query-centric Data Synthesis Approach for Long-context Scaling of\n  Large Language Model", "abstract": "Recent advancements in large language models (LLMs) have highlighted the\nimportance of extending context lengths for handling complex tasks. While\ntraditional methods for training on long contexts often use filtered long\ndocuments, these approaches lead to domain imbalances, limiting model\nperformance. To address this, techniques like random document concatenation\n(Standard) and similarity-based methods (KNN, ICLM) have been developed.\nHowever, they either sacrifice semantic coherence or diversity. To balance both\naspects, we introduce Quest, a query-centric data synthesis method aggregating\nsemantically relevant yet diverse documents. Quest uses a generative model to\npredict potential queries for each document, grouping documents with similar\nqueries and keywords. Extensive experiments demonstrate Quest's superior\nperformance on long-context tasks, achieving remarkable results with context\nlengths of up to 1M tokens and confirming its scalability across various model\nsizes.", "published": "2024-05-30 08:50:55", "link": "http://arxiv.org/abs/2405.19846v7", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DevEval: A Manually-Annotated Code Generation Benchmark Aligned with\n  Real-World Code Repositories", "abstract": "How to evaluate the coding abilities of Large Language Models (LLMs) remains\nan open question. We find that existing benchmarks are poorly aligned with\nreal-world code repositories and are insufficient to evaluate the coding\nabilities of LLMs.\n  To address the knowledge gap, we propose a new benchmark named DevEval, which\nhas three advances. (1) DevEval aligns with real-world repositories in multiple\ndimensions, e.g., code distributions and dependency distributions. (2) DevEval\nis annotated by 13 developers and contains comprehensive annotations (e.g.,\nrequirements, original repositories, reference code, and reference\ndependencies). (3) DevEval comprises 1,874 testing samples from 117\nrepositories, covering 10 popular domains (e.g., Internet, Database). Based on\nDevEval, we propose repository-level code generation and evaluate 8 popular\nLLMs on DevEval (e.g., gpt-4, gpt-3.5, StarCoder 2, DeepSeek Coder, CodeLLaMa).\nOur experiments reveal these LLMs' coding abilities in real-world code\nrepositories. For example, in our experiments, the highest Pass@1 of\ngpt-4-turbo is only 53.04%. We also analyze LLMs' failed cases and summarize\ntheir shortcomings. We hope DevEval can facilitate the development of LLMs in\nreal code repositories. DevEval, prompts, and LLMs' predictions have been\nreleased.", "published": "2024-05-30 09:03:42", "link": "http://arxiv.org/abs/2405.19856v1", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "KNOW: A Real-World Ontology for Knowledge Capture with Large Language\n  Models", "abstract": "We present KNOW--the Knowledge Navigator Ontology for the World--the first\nontology designed to capture everyday knowledge to augment large language\nmodels (LLMs) in real-world generative AI use cases such as personal AI\nassistants. Our domain is human life, both its everyday concerns and its major\nmilestones. We have limited the initial scope of the modeled concepts to only\nestablished human universals: spacetime (places, events) plus social (people,\ngroups, organizations). The inclusion criteria for modeled concepts are\npragmatic, beginning with universality and utility. We compare and contrast\nprevious work such as Schema.org and Cyc--as well as attempts at a synthesis of\nknowledge graphs and language models--noting how LLMs already encode internally\nmuch of the commonsense tacit knowledge that took decades to capture in the Cyc\nproject. We also make available code-generated software libraries for the 12\nmost popular programming languages, enabling the direct use of ontology\nconcepts in software engineering. We emphasize simplicity and developer\nexperience in promoting AI interoperability.", "published": "2024-05-30 09:32:14", "link": "http://arxiv.org/abs/2405.19877v1", "categories": ["cs.AI", "cs.CL", "I.2.4; I.2.7"], "primary_category": "cs.AI"}
{"title": "Multi-Aspect Controllable Text Generation with Disentangled\n  Counterfactual Augmentation", "abstract": "Multi-aspect controllable text generation aims to control the generated texts\nin attributes from multiple aspects (e.g., \"positive\" from sentiment and\n\"sport\" from topic). For ease of obtaining training samples, existing works\nneglect attribute correlations formed by the intertwining of different\nattributes. Particularly, the stereotype formed by imbalanced attribute\ncorrelations significantly affects multi-aspect control. In this paper, we\npropose MAGIC, a new multi-aspect controllable text generation method with\ndisentangled counterfactual augmentation. We alleviate the issue of imbalanced\nattribute correlations during training using counterfactual feature vectors in\nthe attribute latent space by disentanglement. During inference, we enhance\nattribute correlations by target-guided counterfactual augmentation to further\nimprove multi-aspect control. Experiments show that MAGIC outperforms\nstate-of-the-art baselines in both imbalanced and balanced attribute\ncorrelation scenarios. Our source code and data are available at\nhttps://github.com/nju-websoft/MAGIC.", "published": "2024-05-30 11:25:42", "link": "http://arxiv.org/abs/2405.19958v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Efficient LLM-Jailbreaking by Introducing Visual Modality", "abstract": "This paper focuses on jailbreaking attacks against large language models\n(LLMs), eliciting them to generate objectionable content in response to harmful\nuser queries. Unlike previous LLM-jailbreaks that directly orient to LLMs, our\napproach begins by constructing a multimodal large language model (MLLM)\nthrough the incorporation of a visual module into the target LLM. Subsequently,\nwe conduct an efficient MLLM-jailbreak to generate jailbreaking embeddings\nembJS. Finally, we convert the embJS into text space to facilitate the\njailbreaking of the target LLM. Compared to direct LLM-jailbreaking, our\napproach is more efficient, as MLLMs are more vulnerable to jailbreaking than\npure LLM. Additionally, to improve the attack success rate (ASR) of\njailbreaking, we propose an image-text semantic matching scheme to identify a\nsuitable initial input. Extensive experiments demonstrate that our approach\nsurpasses current state-of-the-art methods in terms of both efficiency and\neffectiveness. Moreover, our approach exhibits superior cross-class\njailbreaking capabilities.", "published": "2024-05-30 12:50:32", "link": "http://arxiv.org/abs/2405.20015v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Divide-and-Conquer Meets Consensus: Unleashing the Power of Functions in\n  Code Generation", "abstract": "Despite recent progress made by large language models in code generation,\nthey still struggle with programs that meet complex requirements. Recent work\nutilizes plan-and-solve decomposition to decrease the complexity and leverage\nself-tests to refine the generated program. Yet, planning deep-inside\nrequirements in advance can be challenging, and the tests need to be accurate\nto accomplish self-improvement. To this end, we propose FunCoder, a code\ngeneration framework incorporating the divide-and-conquer strategy with\nfunctional consensus. Specifically, FunCoder recursively branches off\nsub-functions as smaller goals during code generation, represented by a tree\nhierarchy. These sub-functions are then composited to attain more complex\nobjectives. Additionally, we designate functions via a consensus formed by\nidentifying similarities in program behavior, mitigating error propagation.\nFunCoder outperforms state-of-the-art methods by +9.8% on average in HumanEval,\nMBPP, xCodeEval and MATH with GPT-3.5 and GPT-4. Moreover, our method\ndemonstrates superiority on smaller models: With FunCoder, StableCode-3b\nsurpasses GPT-3.5 by +18.6% and achieves 97.7% of GPT-4's performance on\nHumanEval. Further analysis reveals that our proposed dynamic function\ndecomposition is capable of handling complex requirements, and the functional\nconsensus prevails over self-testing in correctness evaluation.", "published": "2024-05-30 14:31:33", "link": "http://arxiv.org/abs/2405.20092v2", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Language Models Need Inductive Biases to Count Inductively", "abstract": "Counting is a fundamental example of generalization, whether viewed through\nthe mathematical lens of Peano's axioms defining the natural numbers or the\ncognitive science literature for children learning to count. The argument holds\nfor both cases that learning to count means learning to count infinitely. While\nfew papers have tried to distill transformer \"reasoning\" to the simplest case\nof counting, investigating length generalization does occur throughout the\nliterature. In the \"train short, test long\" paradigm of NLP, length refers to\nthe training sentence length. In formal language recognition, length refers to\nthe input sequence length, or the maximum stack size induced by a pushdown\nautomata. In general problem solving, length refers to the number of hops in a\ndeductive reasoning chain or the recursion depth. For all cases, counting is\ncentral to task success. And crucially, generalizing counting inductively is\ncentral to success on OOD instances. This work provides extensive empirical\nresults on training language models to count. We experiment with architectures\nranging from RNNs, Transformers, State-Space Models and RWKV. We present\ncarefully-designed task formats, auxiliary tasks and positional embeddings to\navoid limitations in generalization with OOD-position and OOD-vocabulary. We\nfind that while traditional RNNs trivially achieve inductive counting,\nTransformers have to rely on positional embeddings to count out-of-domain. As\ncounting is the basis for many arguments concerning the expressivity of\nTransformers, our finding calls for the community to reexamine the application\nscope of primitive functions defined in formal characterizations. Finally,\nmodern RNNs also largely underperform traditional RNNs in generalizing counting\ninductively. We discuss how design choices that enable parallelized training of\nmodern RNNs cause them to lose merits of a recurrent nature.", "published": "2024-05-30 15:10:37", "link": "http://arxiv.org/abs/2405.20131v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Reasoning about concepts with LLMs: Inconsistencies abound", "abstract": "The ability to summarize and organize knowledge into abstract concepts is key\nto learning and reasoning. Many industrial applications rely on the consistent\nand systematic use of concepts, especially when dealing with decision-critical\nknowledge. However, we demonstrate that, when methodically questioned, large\nlanguage models (LLMs) often display and demonstrate significant\ninconsistencies in their knowledge. Computationally, the basic aspects of the\nconceptualization of a given domain can be represented as Is-A hierarchies in a\nknowledge graph (KG) or ontology, together with a few properties or axioms that\nenable straightforward reasoning. We show that even simple ontologies can be\nused to reveal conceptual inconsistencies across several LLMs. We also propose\nstrategies that domain experts can use to evaluate and improve the coverage of\nkey domain concepts in LLMs of various sizes. In particular, we have been able\nto significantly enhance the performance of LLMs of various sizes with openly\navailable weights using simple knowledge-graph (KG) based prompting strategies.", "published": "2024-05-30 15:38:54", "link": "http://arxiv.org/abs/2405.20163v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "InstructionCP: A fast approach to transfer Large Language Models into\n  target language", "abstract": "The rapid development of large language models (LLMs) in recent years has\nlargely focused on English, resulting in models that respond exclusively in\nEnglish. To adapt these models to other languages, continual pre-training (CP)\nis often employed, followed by supervised fine-tuning (SFT) to maintain\nconversational abilities. However, CP and SFT can reduce a model's ability to\nfilter harmful content. We propose Instruction Continual Pre-training (InsCP),\nwhich integrates instruction tags into the CP process to prevent loss of\nconversational proficiency while acquiring new languages. Our experiments\ndemonstrate that InsCP retains conversational and Reinforcement Learning from\nHuman Feedback (RLHF) abilities. Empirical evaluations on language alignment,\nreliability, and knowledge benchmarks confirm the efficacy of InsCP. Notably,\nthis approach requires only 0.1 billion tokens of high-quality\ninstruction-following data, thereby reducing resource consumption.", "published": "2024-05-30 15:45:13", "link": "http://arxiv.org/abs/2405.20175v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ESG-FTSE: A corpus of news articles with ESG relevance labels and use\n  cases", "abstract": "We present ESG-FTSE, the first corpus comprised of news articles with\nEnvironmental, Social and Governance (ESG) relevance annotations. In recent\nyears, investors and regulators have pushed ESG investing to the mainstream due\nto the urgency of climate change. This has led to the rise of ESG scores to\nevaluate an investment's credentials as socially responsible. While demand for\nESG scores is high, their quality varies wildly. Quantitative techniques can be\napplied to improve ESG scores, thus, responsible investing. To contribute to\nresource building for ESG and financial text mining, we pioneer the ESG-FTSE\ncorpus. We further present the first of its kind ESG annotation schema. It has\nthree levels: a binary classification (relevant versus irrelevant news\narticles), ESG classification (ESG-related news articles), and target company.\nBoth supervised and unsupervised learning experiments for ESG relevance\ndetection were conducted to demonstrate that the corpus can be used in\ndifferent settings to derive accurate ESG predictions. Keywords: corpus\nannotation, ESG labels, annotation schema, news article, natural language\nprocessing", "published": "2024-05-30 16:19:02", "link": "http://arxiv.org/abs/2405.20218v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Group Robust Preference Optimization in Reward-free RLHF", "abstract": "Adapting large language models (LLMs) for specific tasks usually involves\nfine-tuning through reinforcement learning with human feedback (RLHF) on\npreference data. While these data often come from diverse labelers' groups\n(e.g., different demographics, ethnicities, company teams, etc.), traditional\nRLHF approaches adopt a \"one-size-fits-all\" approach, i.e., they\nindiscriminately assume and optimize a single preference model, thus not being\nrobust to unique characteristics and needs of the various groups. To address\nthis limitation, we propose a novel Group Robust Preference Optimization (GRPO)\nmethod to align LLMs to individual groups' preferences robustly. Our approach\nbuilds upon reward-free direct preference optimization methods, but unlike\nprevious approaches, it seeks a robust policy which maximizes the worst-case\ngroup performance. To achieve this, GRPO adaptively and sequentially weights\nthe importance of different groups, prioritizing groups with worse cumulative\nloss. We theoretically study the feasibility of GRPO and analyze its\nconvergence for the log-linear policy class. By fine-tuning LLMs with GRPO\nusing diverse group-based global opinion data, we significantly improved\nperformance for the worst-performing groups, reduced loss imbalances across\ngroups, and improved probability accuracies compared to non-robust baselines.", "published": "2024-05-30 17:50:04", "link": "http://arxiv.org/abs/2405.20304v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ANAH: Analytical Annotation of Hallucinations in Large Language Models", "abstract": "Reducing the `$\\textit{hallucination}$' problem of Large Language Models\n(LLMs) is crucial for their wide applications. A comprehensive and fine-grained\nmeasurement of the hallucination is the first key step for the governance of\nthis issue but is under-explored in the community. Thus, we present\n$\\textbf{ANAH}$, a bilingual dataset that offers $\\textbf{AN}$alytical\n$\\textbf{A}$nnotation of $\\textbf{H}$allucinations in LLMs within Generative\nQuestion Answering. Each answer sentence in our dataset undergoes rigorous\nannotation, involving the retrieval of a reference fragment, the judgment of\nthe hallucination type, and the correction of hallucinated content. ANAH\nconsists of ~12k sentence-level annotations for ~4.3k LLM responses covering\nover 700 topics, constructed by a human-in-the-loop pipeline. Thanks to the\nfine granularity of the hallucination annotations, we can quantitatively\nconfirm that the hallucinations of LLMs progressively accumulate in the answer\nand use ANAH to train and evaluate hallucination annotators. We conduct\nextensive experiments on studying generative and discriminative annotators and\nshow that, although current open-source LLMs have difficulties in fine-grained\nhallucination annotation, the generative annotator trained with ANAH can\nsurpass all open-source LLMs and GPT-3.5, obtain performance competitive with\nGPT-4, and exhibits better generalization ability on unseen questions.", "published": "2024-05-30 17:54:40", "link": "http://arxiv.org/abs/2405.20315v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "From Zero to Hero: Cold-Start Anomaly Detection", "abstract": "When first deploying an anomaly detection system, e.g., to detect\nout-of-scope queries in chatbots, there are no observed data, making\ndata-driven approaches ineffective. Zero-shot anomaly detection methods offer a\nsolution to such \"cold-start\" cases, but unfortunately they are often not\naccurate enough. This paper studies the realistic but underexplored cold-start\nsetting where an anomaly detection model is initialized using zero-shot\nguidance, but subsequently receives a small number of contaminated observations\n(namely, that may include anomalies). The goal is to make efficient use of both\nthe zero-shot guidance and the observations. We propose ColdFusion, a method\nthat effectively adapts the zero-shot anomaly detector to contaminated\nobservations. To support future development of this new setting, we propose an\nevaluation suite consisting of evaluation protocols and metrics.", "published": "2024-05-30 17:59:51", "link": "http://arxiv.org/abs/2405.20341v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Hallucination-Free? Assessing the Reliability of Leading AI Legal\n  Research Tools", "abstract": "Legal practice has witnessed a sharp rise in products incorporating\nartificial intelligence (AI). Such tools are designed to assist with a wide\nrange of core legal tasks, from search and summarization of caselaw to document\ndrafting. But the large language models used in these tools are prone to\n\"hallucinate,\" or make up false information, making their use risky in\nhigh-stakes domains. Recently, certain legal research providers have touted\nmethods such as retrieval-augmented generation (RAG) as \"eliminating\"\n(Casetext, 2023) or \"avoid[ing]\" hallucinations (Thomson Reuters, 2023), or\nguaranteeing \"hallucination-free\" legal citations (LexisNexis, 2023). Because\nof the closed nature of these systems, systematically assessing these claims is\nchallenging. In this article, we design and report on the first preregistered\nempirical evaluation of AI-driven legal research tools. We demonstrate that the\nproviders' claims are overstated. While hallucinations are reduced relative to\ngeneral-purpose chatbots (GPT-4), we find that the AI research tools made by\nLexisNexis (Lexis+ AI) and Thomson Reuters (Westlaw AI-Assisted Research and\nAsk Practical Law AI) each hallucinate between 17% and 33% of the time. We also\ndocument substantial differences between systems in responsiveness and\naccuracy. Our article makes four key contributions. It is the first to assess\nand report the performance of RAG-based proprietary legal AI tools. Second, it\nintroduces a comprehensive, preregistered dataset for identifying and\nunderstanding vulnerabilities in these systems. Third, it proposes a clear\ntypology for differentiating between hallucinations and accurate legal\nresponses. Last, it provides evidence to inform the responsibilities of legal\nprofessionals in supervising and verifying AI outputs, which remains a central\nopen question for the responsible integration of AI into law.", "published": "2024-05-30 17:56:05", "link": "http://arxiv.org/abs/2405.20362v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "XPrompt:Explaining Large Language Model's Generation via Joint Prompt\n  Attribution", "abstract": "Large Language Models (LLMs) have demonstrated impressive performances in\ncomplex text generation tasks. However, the contribution of the input prompt to\nthe generated content still remains obscure to humans, underscoring the\nnecessity of elucidating and explaining the causality between input and output\npairs. Existing works for providing prompt-specific explanation often confine\nmodel output to be classification or next-word prediction. Few initial attempts\naiming to explain the entire language generation often treat input prompt texts\nindependently, ignoring their combinatorial effects on the follow-up\ngeneration. In this study, we introduce a counterfactual explanation framework\nbased on joint prompt attribution, XPrompt, which aims to explain how a few\nprompt texts collaboratively influences the LLM's complete generation.\nParticularly, we formulate the task of prompt attribution for generation\ninterpretation as a combinatorial optimization problem, and introduce a\nprobabilistic algorithm to search for the casual input combination in the\ndiscrete space. We define and utilize multiple metrics to evaluate the produced\nexplanations, demonstrating both faithfulness and efficiency of our framework.", "published": "2024-05-30 18:16:41", "link": "http://arxiv.org/abs/2405.20404v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Transfer Q Star: Principled Decoding for LLM Alignment", "abstract": "Aligning foundation models is essential for their safe and trustworthy\ndeployment. However, traditional fine-tuning methods are computationally\nintensive and require updating billions of model parameters. A promising\nalternative, alignment via decoding, adjusts the response distribution directly\nwithout model updates to maximize a target reward $r$, thus providing a\nlightweight and adaptable framework for alignment. However, principled decoding\nmethods rely on oracle access to an optimal Q-function ($Q^*$), which is often\nunavailable in practice. Hence, prior SoTA methods either approximate this\n$Q^*$ using $Q^{\\pi_{\\texttt{sft}}}$ (derived from the reference $\\texttt{SFT}$\nmodel) or rely on short-term rewards, resulting in sub-optimal decoding\nperformance. In this work, we propose Transfer $Q^*$, which implicitly\nestimates the optimal value function for a target reward $r$ through a baseline\nmodel $\\rho_{\\texttt{BL}}$ aligned with a baseline reward $\\rho_{\\texttt{BL}}$\n(which can be different from the target reward $r$). Theoretical analyses of\nTransfer $Q^*$ provide a rigorous characterization of its optimality, deriving\nan upper bound on the sub-optimality gap and identifying a hyperparameter to\ncontrol the deviation from the pre-trained reference $\\texttt{SFT}$ model based\non user needs. Our approach significantly reduces the sub-optimality gap\nobserved in prior SoTA methods and demonstrates superior empirical performance\nacross key metrics such as coherence, diversity, and quality in extensive tests\non several synthetic and real datasets.", "published": "2024-05-30 21:36:12", "link": "http://arxiv.org/abs/2405.20495v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SPOT: Text Source Prediction from Originality Score Thresholding", "abstract": "The wide acceptance of large language models (LLMs) has unlocked new\napplications and social risks. Popular countermeasures aim at detecting\nmisinformation, usually involve domain specific models trained to recognize the\nrelevance of any information. Instead of evaluating the validity of the\ninformation, we propose to investigate LLM generated text from the perspective\nof trust. In this study, we define trust as the ability to know if an input\ntext was generated by a LLM or a human. To do so, we design SPOT, an efficient\nmethod, that classifies the source of any, standalone, text input based on\noriginality score. This score is derived from the prediction of a given LLM to\ndetect other LLMs. We empirically demonstrate the robustness of the method to\nthe architecture, training data, evaluation data, task and compression of\nmodern LLMs.", "published": "2024-05-30 21:51:01", "link": "http://arxiv.org/abs/2405.20505v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "How Multilingual Are Large Language Models Fine-Tuned for Translation?", "abstract": "A new paradigm for machine translation has recently emerged: fine-tuning\nlarge language models (LLM) on parallel text has been shown to outperform\ndedicated translation systems trained in a supervised fashion on much larger\namounts of parallel data (Xu et al., 2024a; Alves et al., 2024). However, it\nremains unclear whether this paradigm can enable massively multilingual machine\ntranslation or whether it requires fine-tuning dedicated models for a small\nnumber of language pairs. How does translation fine-tuning impact the MT\ncapabilities of LLMs for zero-shot languages, zero-shot language pairs, and\ntranslation tasks that do not involve English? To address these questions, we\nconduct an extensive empirical evaluation of the translation quality of the\nTOWER family of language models (Alves et al., 2024) on 132 translation tasks\nfrom the multi-parallel FLORES-200 data. We find that translation fine-tuning\nimproves translation quality even for zero-shot languages on average, but that\nthe impact is uneven depending on the language pairs involved. These results\ncall for further research to effectively enable massively multilingual\ntranslation with LLMs.", "published": "2024-05-30 22:08:20", "link": "http://arxiv.org/abs/2405.20512v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Automated Generation and Tagging of Knowledge Components from\n  Multiple-Choice Questions", "abstract": "Knowledge Components (KCs) linked to assessments enhance the measurement of\nstudent learning, enrich analytics, and facilitate adaptivity. However,\ngenerating and linking KCs to assessment items requires significant effort and\ndomain-specific knowledge. To streamline this process for higher-education\ncourses, we employed GPT-4 to generate KCs for multiple-choice questions (MCQs)\nin Chemistry and E-Learning. We analyzed discrepancies between the KCs\ngenerated by the Large Language Model (LLM) and those made by humans through\nevaluation from three domain experts in each subject area. This evaluation\naimed to determine whether, in instances of non-matching KCs, evaluators showed\na preference for the LLM-generated KCs over their human-created counterparts.\nWe also developed an ontology induction algorithm to cluster questions that\nassess similar KCs based on their content. Our most effective LLM strategy\naccurately matched KCs for 56% of Chemistry and 35% of E-Learning MCQs, with\neven higher success when considering the top five KC suggestions. Human\nevaluators favored LLM-generated KCs, choosing them over human-assigned ones\napproximately two-thirds of the time, a preference that was statistically\nsignificant across both domains. Our clustering algorithm successfully grouped\nquestions by their underlying KCs without needing explicit labels or contextual\ninformation. This research advances the automation of KC generation and\nclassification for assessment items, alleviating the need for student data or\npredefined KC labels.", "published": "2024-05-30 22:57:49", "link": "http://arxiv.org/abs/2405.20526v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Towards Ontology-Enhanced Representation Learning for Large Language\n  Models", "abstract": "Taking advantage of the widespread use of ontologies to organise and\nharmonize knowledge across several distinct domains, this paper proposes a\nnovel approach to improve an embedding-Large Language Model (embedding-LLM) of\ninterest by infusing the knowledge formalized by a reference ontology:\nontological knowledge infusion aims at boosting the ability of the considered\nLLM to effectively model the knowledge domain described by the infused\nontology. The linguistic information (i.e. concept synonyms and descriptions)\nand structural information (i.e. is-a relations) formalized by the ontology are\nutilized to compile a comprehensive set of concept definitions, with the\nassistance of a powerful generative LLM (i.e. GPT-3.5-turbo). These concept\ndefinitions are then employed to fine-tune the target embedding-LLM using a\ncontrastive learning framework. To demonstrate and evaluate the proposed\napproach, we utilize the biomedical disease ontology MONDO. The results show\nthat embedding-LLMs enhanced by ontological disease knowledge exhibit an\nimproved capability to effectively evaluate the similarity of in-domain\nsentences from biomedical documents mentioning diseases, without compromising\ntheir out-of-domain performance.", "published": "2024-05-30 23:01:10", "link": "http://arxiv.org/abs/2405.20527v1", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7; I.2.6"], "primary_category": "cs.CL"}
{"title": "An Automatic Question Usability Evaluation Toolkit", "abstract": "Evaluating multiple-choice questions (MCQs) involves either labor intensive\nhuman assessments or automated methods that prioritize readability, often\noverlooking deeper question design flaws. To address this issue, we introduce\nthe Scalable Automatic Question Usability Evaluation Toolkit (SAQUET), an\nopen-source tool that leverages the Item-Writing Flaws (IWF) rubric for a\ncomprehensive and automated quality evaluation of MCQs. By harnessing the\nlatest in large language models such as GPT-4, advanced word embeddings, and\nTransformers designed to analyze textual complexity, SAQUET effectively\npinpoints and assesses a wide array of flaws in MCQs. We first demonstrate the\ndiscrepancy between commonly used automated evaluation metrics and the human\nassessment of MCQ quality. Then we evaluate SAQUET on a diverse dataset of MCQs\nacross the five domains of Chemistry, Statistics, Computer Science, Humanities,\nand Healthcare, showing how it effectively distinguishes between flawed and\nflawless questions, providing a level of analysis beyond what is achievable\nwith traditional metrics. With an accuracy rate of over 94% in detecting the\npresence of flaws identified by human evaluators, our findings emphasize the\nlimitations of existing evaluation methods and showcase potential in improving\nthe quality of educational assessments.", "published": "2024-05-30 23:04:53", "link": "http://arxiv.org/abs/2405.20529v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Unveiling the Impact of Coding Data Instruction Fine-Tuning on Large\n  Language Models Reasoning", "abstract": "Instruction Fine-Tuning (IFT) significantly enhances the zero-shot\ncapabilities of pretrained Large Language Models (LLMs). While coding data is\nknown to boost LLM reasoning abilities during pretraining, its role in\nactivating internal reasoning capacities during IFT remains understudied. This\npaper investigates a key question: How does coding data impact LLMs' reasoning\ncapacities during IFT stage? To explore this, we thoroughly examine the impact\nof coding data across different coding data proportions, model families, sizes,\nand reasoning domains, from various perspectives. Specifically, we create three\nIFT datasets with increasing coding data proportions, fine-tune six LLM\nbackbones across different families and scales on these datasets, evaluate the\ntuned models' performance across twelve tasks in three reasoning domains, and\nanalyze the outcomes from three broad-to-granular perspectives: overall,\ndomain-level, and task-specific. Our holistic analysis provides valuable\ninsights into each perspective. First, coding data tuning enhances the overall\nreasoning capabilities of LLMs across different model families and scales.\nMoreover, while the impact of coding data varies by domain, it shows consistent\ntrends within each domain across different model families and scales.\nAdditionally, coding data generally provides comparable task-specific benefits\nacross model families, with optimal proportions in IFT datasets being\ntask-dependent.", "published": "2024-05-30 23:20:25", "link": "http://arxiv.org/abs/2405.20535v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Perplexed by Perplexity: Perplexity-Based Data Pruning With Small\n  Reference Models", "abstract": "In this work, we investigate whether small language models can determine\nhigh-quality subsets of large-scale text datasets that improve the performance\nof larger language models. While existing work has shown that pruning based on\nthe perplexity of a larger model can yield high-quality data, we investigate\nwhether smaller models can be used for perplexity-based pruning and how pruning\nis affected by the domain composition of the data being pruned. We demonstrate\nthat for multiple dataset compositions, perplexity-based pruning of pretraining\ndata can \\emph{significantly} improve downstream task performance: pruning\nbased on perplexities computed with a 125 million parameter model improves the\naverage performance on downstream tasks of a 3 billion parameter model by up to\n2.04 and achieves up to a $1.45\\times$ reduction in pretraining steps to reach\ncommensurate baseline performance. Furthermore, we demonstrate that such\nperplexity-based data pruning also yields downstream performance gains in the\nover-trained and data-constrained regimes.", "published": "2024-05-30 23:50:20", "link": "http://arxiv.org/abs/2405.20541v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Confidence-Aware Sub-Structure Beam Search (CABS): Mitigating\n  Hallucination in Structured Data Generation with Large Language Models", "abstract": "Large Language Models (LLMs) have facilitated structured data generation,\nwith applications in domains like tabular data, document databases, product\ncatalogs, etc. However, concerns persist about generation veracity due to\nincorrect references or hallucinations, necessitating the incorporation of some\nform of model confidence for mitigation. Existing confidence estimation methods\non LLM generations primarily focus on the confidence at the individual token\nlevel or the entire output sequence level, limiting their applicability to\nstructured data generation, which consists of an intricate mix of both\nindependent and correlated entries at the sub-structure level. In this paper,\nwe first investigate confidence estimation methods for generated\nsub-structure-level data. We introduce the concept of Confidence Network that\napplies on the hidden state of the LLM transformer, as a more targeted estimate\nthan the traditional token conditional probability. We further propose\nConfidence-Aware sub-structure Beam Search (CABS), a novel decoding method\noperating at the sub-structure level in structured data generation. CABS\nenhances the faithfulness of structured data generation by considering\nconfidence scores from the Confidence Network for each sub-structure-level data\nand iteratively refining the prompts. Results show that CABS outperforms\ntraditional token-level beam search for structured data generation by 16.7%\nRecall at 90% precision averagely on the problem of product attribute\ngeneration.", "published": "2024-05-30 18:21:05", "link": "http://arxiv.org/abs/2406.00069v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ExU: AI Models for Examining Multilingual Disinformation Narratives and\n  Understanding their Spread", "abstract": "Addressing online disinformation requires analysing narratives across\nlanguages to help fact-checkers and journalists sift through large amounts of\ndata. The ExU project focuses on developing AI-based models for multilingual\ndisinformation analysis, addressing the tasks of rumour stance classification\nand claim retrieval. We describe the ExU project proposal and summarise the\nresults of a user requirements survey regarding the design of tools to support\nfact-checking.", "published": "2024-05-30 11:13:57", "link": "http://arxiv.org/abs/2406.15443v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Source Code Foundation Models are Transferable Binary Analysis Knowledge\n  Bases", "abstract": "Human-Oriented Binary Reverse Engineering (HOBRE) lies at the intersection of\nbinary and source code, aiming to lift binary code to human-readable content\nrelevant to source code, thereby bridging the binary-source semantic gap.\nRecent advancements in uni-modal code model pre-training, particularly in\ngenerative Source Code Foundation Models (SCFMs) and binary understanding\nmodels, have laid the groundwork for transfer learning applicable to HOBRE.\nHowever, existing approaches for HOBRE rely heavily on uni-modal models like\nSCFMs for supervised fine-tuning or general LLMs for prompting, resulting in\nsub-optimal performance. Inspired by recent progress in large multi-modal\nmodels, we propose that it is possible to harness the strengths of uni-modal\ncode models from both sides to bridge the semantic gap effectively. In this\npaper, we introduce a novel probe-and-recover framework that incorporates a\nbinary-source encoder-decoder model and black-box LLMs for binary analysis. Our\napproach leverages the pre-trained knowledge within SCFMs to synthesize\nrelevant, symbol-rich code fragments as context. This additional context\nenables black-box LLMs to enhance recovery accuracy. We demonstrate significant\nimprovements in zero-shot binary summarization and binary function name\nrecovery, with a 10.3% relative gain in CHRF and a 16.7% relative gain in a\nGPT4-based metric for summarization, as well as a 6.7% and 7.4% absolute\nincrease in token-level precision and recall for name recovery, respectively.\nThese results highlight the effectiveness of our approach in automating and\nimproving binary code analysis.", "published": "2024-05-30 00:17:44", "link": "http://arxiv.org/abs/2405.19581v2", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Why Larger Language Models Do In-context Learning Differently?", "abstract": "Large language models (LLM) have emerged as a powerful tool for AI, with the\nkey ability of in-context learning (ICL), where they can perform well on unseen\ntasks based on a brief series of task examples without necessitating any\nadjustments to the model parameters. One recent interesting mysterious\nobservation is that models of different scales may have different ICL\nbehaviors: larger models tend to be more sensitive to noise in the test\ncontext. This work studies this observation theoretically aiming to improve the\nunderstanding of LLM and ICL. We analyze two stylized settings: (1) linear\nregression with one-layer single-head linear transformers and (2) parity\nclassification with two-layer multiple attention heads transformers (non-linear\ndata and non-linear model). In both settings, we give closed-form optimal\nsolutions and find that smaller models emphasize important hidden features\nwhile larger ones cover more hidden features; thus, smaller models are more\nrobust to noise while larger ones are more easily distracted, leading to\ndifferent ICL behaviors. This sheds light on where transformers pay attention\nto and how that affects ICL. Preliminary experimental results on large base and\nchat models provide positive support for our analysis.", "published": "2024-05-30 01:11:35", "link": "http://arxiv.org/abs/2405.19592v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "SVFT: Parameter-Efficient Fine-Tuning with Singular Vectors", "abstract": "Popular parameter-efficient fine-tuning (PEFT) methods, such as LoRA and its\nvariants, freeze pre-trained model weights \\(W\\) and inject learnable matrices\n\\(\\Delta W\\). These \\(\\Delta W\\) matrices are structured for efficient\nparameterization, often using techniques like low-rank approximations or\nscaling vectors. However, these methods typically show a performance gap\ncompared to full fine-tuning. Although recent PEFT methods have narrowed this\ngap, they do so at the cost of additional learnable parameters. We propose\nSVFT, a simple approach that fundamentally differs from existing methods: the\nstructure imposed on \\(\\Delta W\\) depends on the specific weight matrix \\(W\\).\nSpecifically, SVFT updates \\(W\\) as a sparse combination of outer products of\nits singular vectors, training only the coefficients (scales) of these sparse\ncombinations. This approach allows fine-grained control over expressivity\nthrough the number of coefficients. Extensive experiments on language and\nvision benchmarks show that SVFT recovers up to 96% of full fine-tuning\nperformance while training only 0.006 to 0.25% of parameters, outperforming\nexisting methods that only recover up to 85% performance using 0.03 to 0.8% of\nthe trainable parameter budget.", "published": "2024-05-30 01:27:43", "link": "http://arxiv.org/abs/2405.19597v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Easy Problems That LLMs Get Wrong", "abstract": "We introduce a comprehensive Linguistic Benchmark designed to evaluate the\nlimitations of Large Language Models (LLMs) in domains such as logical\nreasoning, spatial intelligence, and linguistic understanding, among others.\nThrough a series of straightforward questions, it uncovers the significant\nlimitations of well-regarded models to perform tasks that humans manage with\nease. It also highlights the potential of prompt engineering to mitigate some\nerrors and underscores the necessity for better training methodologies. Our\nfindings stress the importance of grounding LLMs with human reasoning and\ncommon sense, emphasising the need for human-in-the-loop for enterprise\napplications. We hope this work paves the way for future research to enhance\nthe usefulness and reliability of new models.", "published": "2024-05-30 02:09:51", "link": "http://arxiv.org/abs/2405.19616v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Detecting Hallucinations in Large Language Model Generation: A Token\n  Probability Approach", "abstract": "Concerns regarding the propensity of Large Language Models (LLMs) to produce\ninaccurate outputs, also known as hallucinations, have escalated. Detecting\nthem is vital for ensuring the reliability of applications relying on\nLLM-generated content. Current methods often demand substantial resources and\nrely on extensive LLMs or employ supervised learning with multidimensional\nfeatures or intricate linguistic and semantic analyses difficult to reproduce\nand largely depend on using the same LLM that hallucinated. This paper\nintroduces a supervised learning approach employing two simple classifiers\nutilizing only four numerical features derived from tokens and vocabulary\nprobabilities obtained from other LLM evaluators, which are not necessarily the\nsame. The method yields promising results, surpassing state-of-the-art outcomes\nin multiple tasks across three different benchmarks. Additionally, we provide a\ncomprehensive examination of the strengths and weaknesses of our approach,\nhighlighting the significance of the features utilized and the LLM employed as\nan evaluator. We have released our code publicly at\nhttps://github.com/Baylor-AI/HalluDetect.", "published": "2024-05-30 03:00:47", "link": "http://arxiv.org/abs/2405.19648v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "SysCaps: Language Interfaces for Simulation Surrogates of Complex\n  Systems", "abstract": "Surrogate models are used to predict the behavior of complex energy systems\nthat are too expensive to simulate with traditional numerical methods. Our work\nintroduces the use of language descriptions, which we call ``system captions''\nor SysCaps, to interface with such surrogates. We argue that interacting with\nsurrogates through text, particularly natural language, makes these models more\naccessible for both experts and non-experts. We introduce a lightweight\nmultimodal text and timeseries regression model and a training pipeline that\nuses large language models (LLMs) to synthesize high-quality captions from\nsimulation metadata. Our experiments on two real-world simulators of buildings\nand wind farms show that our SysCaps-augmented surrogates have better accuracy\non held-out systems than traditional methods while enjoying new generalization\nabilities, such as handling semantically related descriptions of the same test\nsystem. Additional experiments also highlight the potential of SysCaps to\nunlock language-driven design space exploration and to regularize training\nthrough prompt augmentation.", "published": "2024-05-30 03:12:04", "link": "http://arxiv.org/abs/2405.19653v3", "categories": ["cs.LG", "cs.CL", "cs.SY", "eess.SY"], "primary_category": "cs.LG"}
{"title": "SpecDec++: Boosting Speculative Decoding via Adaptive Candidate Lengths", "abstract": "Speculative decoding reduces the inference latency of a target large language\nmodel via utilizing a smaller and faster draft model. Its performance depends\non a hyperparameter K -- the candidate length, i.e., the number of candidate\ntokens for the target model to verify in each round. However, previous methods\noften use simple heuristics to choose K, which may result in sub-optimal\nperformance. We study the choice of the candidate length K and formulate it as\na Markov Decision Process. We theoretically show that the optimal policy of\nthis Markov decision process takes the form of a threshold policy, i.e., the\ncurrent speculation should stop and be verified when the probability of getting\na rejection exceeds a threshold value. Motivated by this theory, we propose\nSpecDec++, an enhanced version of speculative decoding that adaptively\ndetermines the candidate length on the fly. We augment the draft model with a\ntrained acceptance prediction head to predict the conditional acceptance\nprobability of the candidate tokens. SpecDec++ will stop the current\nspeculation when the predicted probability that at least one token gets\nrejected exceeds a threshold. We implement SpecDec++ and apply it to the\nllama-2-chat 7B & 70B model pair. Our adaptive method achieves a 2.04x speedup\non the Alpaca dataset (an additional 7.2% improvement over the baseline\nspeculative decoding). On the GSM8K and HumanEval datasets, our method achieves\na 2.26x speedup (9.4% improvement) and 2.23x speedup (11.1% improvement),\nrespectively.", "published": "2024-05-30 05:49:38", "link": "http://arxiv.org/abs/2405.19715v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LLM as a Complementary Optimizer to Gradient Descent: A Case Study in\n  Prompt Tuning", "abstract": "Mastering a skill generally relies on both hands-on experience from doers and\ninsightful, high-level guidance by mentors. Will this strategy also work well\nfor solving complex non-convex optimization problems? Here, a common\ngradient-based optimizer acts like a disciplined doer, making locally optimal\nupdates at each step. Large Language Models (LLMs) can also search for better\nsolutions by inferring from natural language instructions, akin to a high-level\nmentor. In this paper, we show that these two participators are complementary\nto each other and can effectively collaborate as a combined optimization\nframework. The collaborative optimization is achieved by alternating between\nthe gradient-based and LLM-based optimizers. We instruct LLMs to generate\npossibly improved solutions by taking parameter trajectories recorded during\nthe previous stage of gradient-based optimization into account. Inferred\nresults of LLMs are used as restarting points for the next stage of gradient\noptimization. We verify the effectiveness of this optimization framework on\nprompt tuning. By leveraging both the locally rigorous gradient-based optimizer\nand the high-level deductive LLM-based optimizer, the combined optimization\nmethod consistently yields improvements over competitive baselines on a variety\nof tasks. Our results demonstrate the synergistic effect of conventional\ngradient-based optimization and the inference ability of LLMs. The code is\nreleased at https://github.com/guozix/LLM-catalyst.", "published": "2024-05-30 06:24:14", "link": "http://arxiv.org/abs/2405.19732v4", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "PertEval: Unveiling Real Knowledge Capacity of LLMs with\n  Knowledge-Invariant Perturbations", "abstract": "Expert-designed close-ended benchmarks are indispensable in assessing the\nknowledge capacity of large language models (LLMs). Despite their widespread\nuse, concerns have mounted regarding their reliability due to limited test\nscenarios and an unavoidable risk of data contamination. To rectify this, we\npresent PertEval, a toolkit devised for in-depth probing of LLMs' knowledge\ncapacity through \\textbf{knowledge-invariant perturbations}. These\nperturbations employ human-like restatement techniques to generate on-the-fly\ntest samples from static benchmarks, meticulously retaining knowledge-critical\ncontent while altering irrelevant details. Our toolkit further includes a suite\nof \\textbf{response consistency analyses} that compare performance on raw vs.\nperturbed test sets to precisely assess LLMs' genuine knowledge capacity. Six\nrepresentative LLMs are re-evaluated using PertEval. Results reveal\nsignificantly inflated performance of the LLMs on raw benchmarks, including an\nabsolute 25.8% overestimation for GPT-4. Additionally, through a nuanced\nresponse pattern analysis, we discover that PertEval retains LLMs' uncertainty\nto specious knowledge, and reveals their potential rote memorization to correct\noptions which leads to overestimated performance. We also find that the\ndetailed response consistency analyses by PertEval could illuminate various\nweaknesses in existing LLMs' knowledge mastery and guide the development of\nrefinement. Our findings provide insights for advancing more robust and\ngenuinely knowledgeable LLMs. Our code is available at\n\\url{https://github.com/aigc-apps/PertEval}.", "published": "2024-05-30 06:38:32", "link": "http://arxiv.org/abs/2405.19740v2", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Is In-Context Learning Sufficient for Instruction Following in LLMs?", "abstract": "In-context learning (ICL) allows LLMs to learn from examples without changing\ntheir weights: this is a particularly promising capability for long-context\nLLMs that can potentially learn from many examples. Recently, Lin et al. (2024)\nproposed URIAL, a method using only three in-context examples to align base\nLLMs, achieving non-trivial instruction following performance. In this work, we\nshow that, while effective, ICL alignment with URIAL still underperforms\ncompared to instruction fine-tuning on the established benchmark MT-Bench,\nespecially with more capable base LLMs. We then uncover the most relevant\nelements for successful in-context alignment, finding the crucial role of the\ndecoding parameters. Based on these insights, we show that the approach of\nURIAL can indeed be improved by adding high-quality, potentially carefully\nselected via greedy search, demonstrations in context, getting closer to the\nperformance of instruct models. Finally, we provide the first, to our\nknowledge, systematic comparison of ICL and instruction fine-tuning (IFT) for\ninstruction following in the low data regime, where ICL can be a viable\nalternative to IFT. Overall, our work advances the understanding of ICL as an\nalignment technique and its relationship to IFT. We provide our code at\nhttps://github.com/tml-epfl/icl-alignment.", "published": "2024-05-30 09:28:56", "link": "http://arxiv.org/abs/2405.19874v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "From Words to Actions: Unveiling the Theoretical Underpinnings of\n  LLM-Driven Autonomous Systems", "abstract": "In this work, from a theoretical lens, we aim to understand why large\nlanguage model (LLM) empowered agents are able to solve decision-making\nproblems in the physical world. To this end, consider a hierarchical\nreinforcement learning (RL) model where the LLM Planner and the Actor perform\nhigh-level task planning and low-level execution, respectively. Under this\nmodel, the LLM Planner navigates a partially observable Markov decision process\n(POMDP) by iteratively generating language-based subgoals via prompting. Under\nproper assumptions on the pretraining data, we prove that the pretrained LLM\nPlanner effectively performs Bayesian aggregated imitation learning (BAIL)\nthrough in-context learning. Additionally, we highlight the necessity for\nexploration beyond the subgoals derived from BAIL by proving that naively\nexecuting the subgoals returned by LLM leads to a linear regret. As a remedy,\nwe introduce an $\\epsilon$-greedy exploration strategy to BAIL, which is proven\nto incur sublinear regret when the pretraining error is small. Finally, we\nextend our theoretical framework to include scenarios where the LLM Planner\nserves as a world model for inferring the transition model of the environment\nand to multi-agent settings, enabling coordination among multiple Actors.", "published": "2024-05-30 09:42:54", "link": "http://arxiv.org/abs/2405.19883v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Similarity is Not All You Need: Endowing Retrieval Augmented Generation\n  with Multi Layered Thoughts", "abstract": "In recent years, large language models (LLMs) have made remarkable\nachievements in various domains. However, the untimeliness and cost of\nknowledge updates coupled with hallucination issues of LLMs have curtailed\ntheir applications in knowledge intensive tasks, where retrieval augmented\ngeneration (RAG) can be of help. Nevertheless, existing retrieval augmented\nmodels typically use similarity as a bridge between queries and documents and\nfollow a retrieve then read procedure. In this work, we argue that similarity\nis not always the panacea and totally relying on similarity would sometimes\ndegrade the performance of retrieval augmented generation. To this end, we\npropose MetRag, a Multi layEred Thoughts enhanced Retrieval Augmented\nGeneration framework. To begin with, beyond existing similarity oriented\nthought, we embrace a small scale utility model that draws supervision from an\nLLM for utility oriented thought and further come up with a smarter model by\ncomprehensively combining the similarity and utility oriented thoughts.\nFurthermore, given the fact that the retrieved document set tends to be huge\nand using them in isolation makes it difficult to capture the commonalities and\ncharacteristics among them, we propose to make an LLM as a task adaptive\nsummarizer to endow retrieval augmented generation with compactness-oriented\nthought. Finally, with multi layered thoughts from the precedent stages, an LLM\nis called for knowledge augmented generation. Extensive experiments on\nknowledge-intensive tasks have demonstrated the superiority of MetRag.", "published": "2024-05-30 09:50:38", "link": "http://arxiv.org/abs/2405.19893v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "GenKubeSec: LLM-Based Kubernetes Misconfiguration Detection,\n  Localization, Reasoning, and Remediation", "abstract": "A key challenge associated with Kubernetes configuration files (KCFs) is that\nthey are often highly complex and error-prone, leading to security\nvulnerabilities and operational setbacks. Rule-based (RB) tools for KCF\nmisconfiguration detection rely on static rule sets, making them inherently\nlimited and unable to detect newly-discovered misconfigurations. RB tools also\nsuffer from misdetection, since mistakes are likely when coding the detection\nrules. Recent methods for detecting and remediating KCF misconfigurations are\nlimited in terms of their scalability and detection coverage, or due to the\nfact that they have high expertise requirements and do not offer automated\nremediation along with misconfiguration detection. Novel approaches that employ\nLLMs in their pipeline rely on API-based, general-purpose, and mainly\ncommercial models. Thus, they pose security challenges, have inconsistent\nclassification performance, and can be costly. In this paper, we propose\nGenKubeSec, a comprehensive and adaptive, LLM-based method, which, in addition\nto detecting a wide variety of KCF misconfigurations, also identifies the exact\nlocation of the misconfigurations and provides detailed reasoning about them,\nalong with suggested remediation. When empirically compared with three\nindustry-standard RB tools, GenKubeSec achieved equivalent precision (0.990)\nand superior recall (0.999). When a random sample of KCFs was examined by a\nKubernetes security expert, GenKubeSec's explanations as to misconfiguration\nlocalization, reasoning and remediation were 100% correct, informative and\nuseful. To facilitate further advancements in this domain, we share the unique\ndataset we collected, a unified misconfiguration index we developed for label\nstandardization, our experimentation code, and GenKubeSec itself as an\nopen-source tool.", "published": "2024-05-30 11:18:52", "link": "http://arxiv.org/abs/2405.19954v1", "categories": ["cs.CR", "cs.CL", "cs.DC", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Improved Out-of-Scope Intent Classification with Dual Encoding and\n  Threshold-based Re-Classification", "abstract": "Detecting out-of-scope user utterances is essential for task-oriented\ndialogues and intent classification. Current methodologies face difficulties\nwith the unpredictable distribution of outliers and often rely on assumptions\nabout data distributions. We present the Dual Encoder for Threshold-Based\nRe-Classification (DETER) to address these challenges. This end-to-end\nframework efficiently detects out-of-scope intents without requiring\nassumptions on data distributions or additional post-processing steps. The core\nof DETER utilizes dual text encoders, the Universal Sentence Encoder (USE) and\nthe Transformer-based Denoising AutoEncoder (TSDAE), to generate user utterance\nembeddings, which are classified through a branched neural architecture.\nFurther, DETER generates synthetic outliers using self-supervision and\nincorporates out-of-scope phrases from open-domain datasets. This approach\nensures a comprehensive training set for out-of-scope detection. Additionally,\na threshold-based re-classification mechanism refines the model's initial\npredictions. Evaluations on the CLINC-150, Stackoverflow, and Banking77\ndatasets demonstrate DETER's efficacy. Our model outperforms previous\nbenchmarks, increasing up to 13% and 5% in F1 score for known and unknown\nintents on CLINC-150 and Stackoverflow, and 16% for known and 24% % for unknown\nintents on Banking77. The source code has been released at\nhttps://github.com/Hossam-Mohammed-tech/Intent_Classification_OOS.", "published": "2024-05-30 11:46:42", "link": "http://arxiv.org/abs/2405.19967v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Kernel Language Entropy: Fine-grained Uncertainty Quantification for\n  LLMs from Semantic Similarities", "abstract": "Uncertainty quantification in Large Language Models (LLMs) is crucial for\napplications where safety and reliability are important. In particular,\nuncertainty can be used to improve the trustworthiness of LLMs by detecting\nfactually incorrect model responses, commonly called hallucinations.\nCritically, one should seek to capture the model's semantic uncertainty, i.e.,\nthe uncertainty over the meanings of LLM outputs, rather than uncertainty over\nlexical or syntactic variations that do not affect answer correctness. To\naddress this problem, we propose Kernel Language Entropy (KLE), a novel method\nfor uncertainty estimation in white- and black-box LLMs. KLE defines positive\nsemidefinite unit trace kernels to encode the semantic similarities of LLM\noutputs and quantifies uncertainty using the von Neumann entropy. It considers\npairwise semantic dependencies between answers (or semantic clusters),\nproviding more fine-grained uncertainty estimates than previous methods based\non hard clustering of answers. We theoretically prove that KLE generalizes the\nprevious state-of-the-art method called semantic entropy and empirically\ndemonstrate that it improves uncertainty quantification performance across\nmultiple natural language generation datasets and LLM architectures.", "published": "2024-05-30 12:42:05", "link": "http://arxiv.org/abs/2405.20003v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Safe Multi-agent Reinforcement Learning with Natural Language\n  Constraints", "abstract": "The role of natural language constraints in Safe Multi-agent Reinforcement\nLearning (MARL) is crucial, yet often overlooked. While Safe MARL has vast\npotential, especially in fields like robotics and autonomous vehicles, its full\npotential is limited by the need to define constraints in pre-designed\nmathematical terms, which requires extensive domain expertise and reinforcement\nlearning knowledge, hindering its broader adoption. To address this limitation\nand make Safe MARL more accessible and adaptable, we propose a novel approach\nnamed Safe Multi-agent Reinforcement Learning with Natural Language constraints\n(SMALL). Our method leverages fine-tuned language models to interpret and\nprocess free-form textual constraints, converting them into semantic embeddings\nthat capture the essence of prohibited states and behaviours. These embeddings\nare then integrated into the multi-agent policy learning process, enabling\nagents to learn policies that minimize constraint violations while optimizing\nrewards. To evaluate the effectiveness of SMALL, we introduce the LaMaSafe, a\nmulti-task benchmark designed to assess the performance of multiple agents in\nadhering to natural language constraints. Empirical evaluations across various\nenvironments demonstrate that SMALL achieves comparable rewards and\nsignificantly fewer constraint violations, highlighting its effectiveness in\nunderstanding and enforcing natural language constraints.", "published": "2024-05-30 12:57:35", "link": "http://arxiv.org/abs/2405.20018v1", "categories": ["cs.MA", "cs.CL", "cs.LG"], "primary_category": "cs.MA"}
{"title": "Would I Lie To You? Inference Time Alignment of Language Models using\n  Direct Preference Heads", "abstract": "Pre-trained Language Models (LMs) exhibit strong zero-shot and in-context\nlearning capabilities; however, their behaviors are often difficult to control.\nBy utilizing Reinforcement Learning from Human Feedback (RLHF), it is possible\nto fine-tune unsupervised LMs to follow instructions and produce outputs that\nreflect human preferences. Despite its benefits, RLHF has been shown to\npotentially harm a language model's reasoning capabilities and introduce\nartifacts such as hallucinations where the model may fabricate facts. To\naddress this issue we introduce Direct Preference Heads (DPH), a fine-tuning\nframework that enables LMs to learn human preference signals through an\nauxiliary reward head without directly affecting the output distribution of the\nlanguage modeling head. We perform a theoretical analysis of our objective\nfunction and find strong ties to Conservative Direct Preference Optimization\n(cDPO). Finally we evaluate our models on GLUE, RACE, and the GPT4All\nevaluation suite and demonstrate that our method produces models which achieve\nhigher scores than those fine-tuned with Supervised Fine-Tuning (SFT) or Direct\nPreference Optimization (DPO) alone.", "published": "2024-05-30 13:38:52", "link": "http://arxiv.org/abs/2405.20053v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Student Answer Forecasting: Transformer-Driven Answer Choice Prediction\n  for Language Learning", "abstract": "Intelligent Tutoring Systems (ITS) enhance personalized learning by\npredicting student answers to provide immediate and customized instruction.\nHowever, recent research has primarily focused on the correctness of the answer\nrather than the student's performance on specific answer choices, limiting\ninsights into students' thought processes and potential misconceptions. To\naddress this gap, we present MCQStudentBert, an answer forecasting model that\nleverages the capabilities of Large Language Models (LLMs) to integrate\ncontextual understanding of students' answering history along with the text of\nthe questions and answers. By predicting the specific answer choices students\nare likely to make, practitioners can easily extend the model to new answer\nchoices or remove answer choices for the same multiple-choice question (MCQ)\nwithout retraining the model. In particular, we compare MLP, LSTM, BERT, and\nMistral 7B architectures to generate embeddings from students' past\ninteractions, which are then incorporated into a finetuned BERT's\nanswer-forecasting mechanism. We apply our pipeline to a dataset of language\nlearning MCQ, gathered from an ITS with over 10,000 students to explore the\npredictive accuracy of MCQStudentBert, which incorporates student interaction\npatterns, in comparison to correct answer prediction and traditional\nmastery-learning feature-based approaches. This work opens the door to more\npersonalized content, modularization, and granular support.", "published": "2024-05-30 14:09:43", "link": "http://arxiv.org/abs/2405.20079v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Fill in the Gap! Combining Self-supervised Representation Learning with\n  Neural Audio Synthesis for Speech Inpainting", "abstract": "Most speech self-supervised learning (SSL) models are trained with a pretext\ntask which consists in predicting missing parts of the input signal, either\nfuture segments (causal prediction) or segments masked anywhere within the\ninput (non-causal prediction). Learned speech representations can then be\nefficiently transferred to downstream tasks (e.g., automatic speech or speaker\nrecognition). In the present study, we investigate the use of a speech SSL\nmodel for speech inpainting, that is reconstructing a missing portion of a\nspeech signal from its surrounding context, i.e., fulfilling a downstream task\nthat is very similar to the pretext task. To that purpose, we combine an SSL\nencoder, namely HuBERT, with a neural vocoder, namely HiFiGAN, playing the role\nof a decoder. In particular, we propose two solutions to match the HuBERT\noutput with the HiFiGAN input, by freezing one and fine-tuning the other, and\nvice versa. Performance of both approaches was assessed in single- and\nmulti-speaker settings, for both informed and blind inpainting configurations\n(i.e., the position of the mask is known or unknown, respectively), with\ndifferent objective metrics and a perceptual evaluation. Performances show that\nif both solutions allow to correctly reconstruct signal portions up to the size\nof 200ms (and even 400ms in some cases), fine-tuning the SSL encoder provides a\nmore accurate signal reconstruction in the single-speaker setting case, while\nfreezing it (and training the neural vocoder instead) is a better strategy when\ndealing with multi-speaker data.", "published": "2024-05-30 14:41:39", "link": "http://arxiv.org/abs/2405.20101v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "GNN-RAG: Graph Neural Retrieval for Large Language Model Reasoning", "abstract": "Knowledge Graphs (KGs) represent human-crafted factual knowledge in the form\nof triplets (head, relation, tail), which collectively form a graph. Question\nAnswering over KGs (KGQA) is the task of answering natural questions grounding\nthe reasoning to the information provided by the KG. Large Language Models\n(LLMs) are the state-of-the-art models for QA tasks due to their remarkable\nability to understand natural language. On the other hand, Graph Neural\nNetworks (GNNs) have been widely used for KGQA as they can handle the complex\ngraph information stored in the KG. In this work, we introduce GNN-RAG, a novel\nmethod for combining language understanding abilities of LLMs with the\nreasoning abilities of GNNs in a retrieval-augmented generation (RAG) style.\nFirst, a GNN reasons over a dense KG subgraph to retrieve answer candidates for\na given question. Second, the shortest paths in the KG that connect question\nentities and answer candidates are extracted to represent KG reasoning paths.\nThe extracted paths are verbalized and given as input for LLM reasoning with\nRAG. In our GNN-RAG framework, the GNN acts as a dense subgraph reasoner to\nextract useful graph information, while the LLM leverages its natural language\nprocessing ability for ultimate KGQA. Furthermore, we develop a retrieval\naugmentation (RA) technique to further boost KGQA performance with GNN-RAG.\nExperimental results show that GNN-RAG achieves state-of-the-art performance in\ntwo widely used KGQA benchmarks (WebQSP and CWQ), outperforming or matching\nGPT-4 performance with a 7B tuned LLM. In addition, GNN-RAG excels on multi-hop\nand multi-entity questions outperforming competing approaches by 8.9--15.5%\npoints at answer F1.", "published": "2024-05-30 15:14:24", "link": "http://arxiv.org/abs/2405.20139v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Robo-Instruct: Simulator-Augmented Instruction Alignment For Finetuning\n  CodeLLMs", "abstract": "Open-weight LLMs are particularly appealing choices to generate training data\nfor fine-tuning Code LLMs on domain-specific service robot applications because\nthey are cost-effective, customizable, and offer better privacy protection.\nHowever, unlike proprietary LLMs, open-weight models are more error-prone and\noften produce programs that violate domain-specific constraints. A promising\nsolution is to incorporate a robot simulator with a well-defined environment to\nverify program correctness. Yet, these environments require pre-enumeration of\nrelevant entities and their states, which limits the diversity of programs that\ncan be effectively verified. In this work, we introduce ROBO-INSTRUCT that\npreserves the diversity of programs generated by an LLM while providing the\ncorrectness of simulator-based checking. ROBO-INSTRUCT introduces ROBOSIM to\ndynamically synthesize consistent simulation environments for each generated\nprogram. Moreover, ROBO-INSTRUCT handles subtler instruction-program\ninconsistencies that do not result in a constraint violation via INSTALIGN, an\nLLM-aided instruction-program alignment process. Given domain-specific APIs and\na few seed examples, ROBO-INSTRUCT can leverage an 8B Llama3 model to generate\na training dataset for fine-tuning a 7B CodeLlama model. Our fine-tuned model\nachieves a 28.75% improvement in pass@1 over the original base model and a\n13.75% improvement compared to its SELF-INSTRUCT-finetuned counterparts, even\nsurpassing the performance of a few proprietary LLMs, such as GPT-3.5-Turbo and\nGemini-Pro.", "published": "2024-05-30 15:47:54", "link": "http://arxiv.org/abs/2405.20179v2", "categories": ["cs.CL", "cs.AI", "cs.RO"], "primary_category": "cs.CL"}
{"title": "Jina CLIP: Your CLIP Model Is Also Your Text Retriever", "abstract": "Contrastive Language-Image Pretraining (CLIP) is widely used to train models\nto align images and texts in a common embedding space by mapping them to\nfixed-sized vectors. These models are key to multimodal information retrieval\nand related tasks. However, CLIP models generally underperform in text-only\ntasks compared to specialized text models. This creates inefficiencies for\ninformation retrieval systems that keep separate embeddings and models for\ntext-only and multimodal tasks. We propose a novel, multi-task contrastive\ntraining method to address this issue, which we use to train the jina-clip-v1\nmodel to achieve the state-of-the-art performance on both text-image and\ntext-text retrieval tasks.", "published": "2024-05-30 16:07:54", "link": "http://arxiv.org/abs/2405.20204v2", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.IR", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "PostDoc: Generating Poster from a Long Multimodal Document Using Deep\n  Submodular Optimization", "abstract": "A poster from a long input document can be considered as a one-page\neasy-to-read multimodal (text and images) summary presented on a nice template\nwith good design elements. Automatic transformation of a long document into a\nposter is a very less studied but challenging task. It involves content\nsummarization of the input document followed by template generation and\nharmonization. In this work, we propose a novel deep submodular function which\ncan be trained on ground truth summaries to extract multimodal content from the\ndocument and explicitly ensures good coverage, diversity and alignment of text\nand images. Then, we use an LLM based paraphraser and propose to generate a\ntemplate with various design aspects conditioned on the input content. We show\nthe merits of our approach through extensive automated and human evaluations.", "published": "2024-05-30 16:16:25", "link": "http://arxiv.org/abs/2405.20213v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Retrieval Augmented Structured Generation: Business Document Information\n  Extraction As Tool Use", "abstract": "Business Document Information Extraction (BDIE) is the problem of\ntransforming a blob of unstructured information (raw text, scanned documents,\netc.) into a structured format that downstream systems can parse and use. It\nhas two main tasks: Key-Information Extraction (KIE) and Line Items Recognition\n(LIR). In this paper, we argue that BDIE is best modeled as a Tool Use problem,\nwhere the tools are these downstream systems. We then present Retrieval\nAugmented Structured Generation (RASG), a novel general framework for BDIE that\nachieves state of the art (SOTA) results on both KIE and LIR tasks on BDIE\nbenchmarks.\n  The contributions of this paper are threefold: (1) We show, with ablation\nbenchmarks, that Large Language Models (LLMs) with RASG are already competitive\nwith or surpasses current SOTA Large Multimodal Models (LMMs) without RASG on\nBDIE benchmarks. (2) We propose a new metric class for Line Items Recognition,\nGeneral Line Items Recognition Metric (GLIRM), that is more aligned with\npractical BDIE use cases compared to existing metrics, such as ANLS*, DocILE,\nand GriTS. (3) We provide a heuristic algorithm for backcalculating bounding\nboxes of predicted line items and tables without the need for vision encoders.\nFinally, we claim that, while LMMs might sometimes offer marginal performance\nbenefits, LLMs + RASG is oftentimes superior given real-world applications and\nconstraints of BDIE.", "published": "2024-05-30 16:54:42", "link": "http://arxiv.org/abs/2405.20245v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane\n  Reflections", "abstract": "Parameter-efficient finetuning (PEFT) has become ubiquitous to adapt\nfoundation models to downstream task requirements while retaining their\ngeneralization ability. However, the amount of additionally introduced\nparameters and compute for successful adaptation and hyperparameter searches\ncan explode quickly, especially when deployed at scale to serve numerous\nindividual requests. To ensure effective, parameter-efficient, and\nhyperparameter-robust adaptation, we propose the ETHER transformation family,\nwhich performs Efficient fineTuning via HypErplane Reflections. By design,\nETHER transformations require a minimal number of parameters, are less likely\nto deteriorate model performance, and exhibit robustness to hyperparameter and\nlearning rate choices. In particular, we introduce ETHER and its relaxation\nETHER+, which match or outperform existing PEFT methods with significantly\nfewer parameters ($\\sim$$10$-$100$ times lower than LoRA or OFT) across\nmultiple image synthesis and natural language tasks without exhaustive\nhyperparameter tuning. Finally, we investigate the recent emphasis on\nHyperspherical Energy retention for adaptation and raise questions on its\npractical utility. The code is available at https://github.com/mwbini/ether.", "published": "2024-05-30 17:26:02", "link": "http://arxiv.org/abs/2405.20271v2", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "ROAST: Review-level Opinion Aspect Sentiment Target Joint Detection for\n  ABSA", "abstract": "Aspect-Based Sentiment Analysis (ABSA) has experienced tremendous expansion\nand diversity due to various shared tasks spanning several languages and fields\nand organized via SemEval workshops and Germeval. Nonetheless, a few\nshortcomings still need to be addressed, such as the lack of low-resource\nlanguage evaluations and the emphasis on sentence-level analysis. To thoroughly\nassess ABSA techniques in the context of complete reviews, this research\npresents a novel task, Review-Level Opinion Aspect Sentiment Target (ROAST).\nROAST seeks to close the gap between sentence-level and text-level ABSA by\nidentifying every ABSA constituent at the review level. We extend the available\ndatasets to enable ROAST, addressing the drawbacks noted in previous research\nby incorporating low-resource languages, numerous languages, and a variety of\ntopics. Through this effort, ABSA research will be able to cover more ground\nand get a deeper comprehension of the task and its practical application in a\nvariety of languages and domains (https://github.com/RiTUAL-UH/ROAST-ABSA).", "published": "2024-05-30 17:29:15", "link": "http://arxiv.org/abs/2405.20274v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Large Language Models Can Self-Improve At Web Agent Tasks", "abstract": "Training models to act as agents that can effectively navigate and perform\nactions in a complex environment, such as a web browser, has typically been\nchallenging due to lack of training data. Large language models (LLMs) have\nrecently demonstrated some capability to navigate novel environments as agents\nin a zero-shot or few-shot fashion, purely guided by natural language\ninstructions as prompts. Recent research has also demonstrated LLMs have the\ncapability to exceed their base performance through self-improvement, i.e.\nfine-tuning on data generated by the model itself. In this work, we explore the\nextent to which LLMs can self-improve their performance as agents in\nlong-horizon tasks in a complex environment using the WebArena benchmark. In\nWebArena, an agent must autonomously navigate and perform actions on web pages\nto achieve a specified objective. We explore fine-tuning on three distinct\nsynthetic training data mixtures and achieve a 31\\% improvement in task\ncompletion rate over the base model on the WebArena benchmark through a\nself-improvement procedure. We additionally contribute novel evaluation metrics\nfor assessing the performance, robustness, capabilities, and quality of\ntrajectories of our fine-tuned agent models to a greater degree than simple,\naggregate-level benchmark scores currently used to measure self-improvement.", "published": "2024-05-30 17:52:36", "link": "http://arxiv.org/abs/2405.20309v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Quriosity: Analyzing Human Questioning Behavior and Causal Inquiry\n  through Curiosity-Driven Queries", "abstract": "Recent progress in Large Language Model (LLM) technology has changed our role\nin interacting with these models. Instead of primarily testing these models\nwith questions we already know answers to, we are now using them for queries\nwhere the answers are unknown to us, driven by human curiosity. This shift\nhighlights the growing need to understand curiosity-driven human questions -\nthose that are more complex, open-ended, and reflective of real-world needs. To\nthis end, we present Quriosity, a collection of 13.5K naturally occurring\nquestions from three diverse sources: human-to-search-engine queries,\nhuman-to-human interactions, and human-to-LLM conversations. Our comprehensive\ncollection enables a rich understanding of human curiosity across various\ndomains and contexts. Our analysis reveals a significant presence of causal\nquestions (up to 42%) in the dataset, for which we develop an iterative prompt\nimprovement framework to identify all causal queries and examine their unique\nlinguistic properties, cognitive complexity and source distribution. Our paper\npaves the way for future work on causal question identification and open-ended\nchatbot interactions.", "published": "2024-05-30 17:55:28", "link": "http://arxiv.org/abs/2405.20318v3", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "CoSy: Evaluating Textual Explanations of Neurons", "abstract": "A crucial aspect of understanding the complex nature of Deep Neural Networks\n(DNNs) is the ability to explain learned concepts within their latent\nrepresentations. While methods exist to connect neurons to human-understandable\ntextual descriptions, evaluating the quality of these explanations is\nchallenging due to the lack of a unified quantitative approach. We introduce\nCoSy (Concept Synthesis), a novel, architecture-agnostic framework for\nevaluating textual explanations of latent neurons. Given textual explanations,\nour proposed framework uses a generative model conditioned on textual input to\ncreate data points representing the explanations. By comparing the neuron's\nresponse to these generated data points and control data points, we can\nestimate the quality of the explanation. We validate our framework through\nsanity checks and benchmark various neuron description methods for Computer\nVision tasks, revealing significant differences in quality.", "published": "2024-05-30 17:59:04", "link": "http://arxiv.org/abs/2405.20331v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Efficient Systematic Reviews: Literature Filtering with Transformers &\n  Transfer Learning", "abstract": "Identifying critical research within the growing body of academic work is an\nintrinsic aspect of conducting quality research. Systematic review processes\nused in evidence-based medicine formalise this as a procedure that must be\nfollowed in a research program. However, it comes with an increasing burden in\nterms of the time required to identify the important articles of research for a\ngiven topic. In this work, we develop a method for building a general-purpose\nfiltering system that matches a research question, posed as a natural language\ndescription of the required content, against a candidate set of articles\nobtained via the application of broad search terms. Our results demonstrate\nthat transformer models, pre-trained on biomedical literature, and then fine\ntuned for the specific task, offer a promising solution to this problem. The\nmodel can remove large volumes of irrelevant articles for most research\nquestions. Furthermore, analysis of the specific research questions in our\ntraining data suggest natural avenues for further improvement.", "published": "2024-05-30 02:55:49", "link": "http://arxiv.org/abs/2405.20354v2", "categories": ["cs.DL", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.DL"}
{"title": "SeamlessExpressiveLM: Speech Language Model for Expressive\n  Speech-to-Speech Translation with Chain-of-Thought", "abstract": "Expressive speech-to-speech translation (S2ST) is a key research topic in\nseamless communication, which focuses on the preservation of semantics and\nspeaker vocal style in translated speech. Early works synthesized speaker style\naligned speech in order to directly learn the mapping from speech to target\nspeech spectrogram. Without reliance on style aligned data, recent studies\nleverage the advances of language modeling (LM) and build cascaded LMs on\nsemantic and acoustic tokens. This work proposes SeamlessExpressiveLM, a single\nspeech language model for expressive S2ST. We decompose the complex\nsource-to-target speech mapping into intermediate generation steps with\nchain-of-thought prompting. The model is first guided to translate target\nsemantic content and then transfer the speaker style to multi-stream acoustic\nunits. Evaluated on Spanish-to-English and Hungarian-to-English translations,\nSeamlessExpressiveLM outperforms cascaded LMs in both semantic quality and\nstyle transfer, meanwhile achieving better parameter efficiency.", "published": "2024-05-30 18:28:31", "link": "http://arxiv.org/abs/2405.20410v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Jailbreaking Large Language Models Against Moderation Guardrails via\n  Cipher Characters", "abstract": "Large Language Models (LLMs) are typically harmless but remain vulnerable to\ncarefully crafted prompts known as ``jailbreaks'', which can bypass protective\nmeasures and induce harmful behavior. Recent advancements in LLMs have\nincorporated moderation guardrails that can filter outputs, which trigger\nprocessing errors for certain malicious questions. Existing red-teaming\nbenchmarks often neglect to include questions that trigger moderation\nguardrails, making it difficult to evaluate jailbreak effectiveness. To address\nthis issue, we introduce JAMBench, a harmful behavior benchmark designed to\ntrigger and evaluate moderation guardrails. JAMBench involves 160 manually\ncrafted instructions covering four major risk categories at multiple severity\nlevels. Furthermore, we propose a jailbreak method, JAM (Jailbreak Against\nModeration), designed to attack moderation guardrails using jailbreak prefixes\nto bypass input-level filters and a fine-tuned shadow model functionally\nequivalent to the guardrail model to generate cipher characters to bypass\noutput-level filters. Our extensive experiments on four LLMs demonstrate that\nJAM achieves higher jailbreak success ($\\sim$ $\\times$ 19.88) and lower\nfiltered-out rates ($\\sim$ $\\times$ 1/6) than baselines.", "published": "2024-05-30 18:38:36", "link": "http://arxiv.org/abs/2405.20413v1", "categories": ["cs.CR", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Enhancing Antibiotic Stewardship using a Natural Language Approach for\n  Better Feature Representation", "abstract": "The rapid emergence of antibiotic-resistant bacteria is recognized as a\nglobal healthcare crisis, undermining the efficacy of life-saving antibiotics.\nThis crisis is driven by the improper and overuse of antibiotics, which\nescalates bacterial resistance. In response, this study explores the use of\nclinical decision support systems, enhanced through the integration of\nelectronic health records (EHRs), to improve antibiotic stewardship. However,\nEHR systems present numerous data-level challenges, complicating the effective\nsynthesis and utilization of data. In this work, we transform EHR data into a\nserialized textual representation and employ pretrained foundation models to\ndemonstrate how this enhanced feature representation can aid in antibiotic\nsusceptibility predictions. Our results suggest that this text representation,\ncombined with foundation models, provides a valuable tool to increase\ninterpretability and support antibiotic stewardship efforts.", "published": "2024-05-30 18:53:53", "link": "http://arxiv.org/abs/2405.20419v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "MTEB-French: Resources for French Sentence Embedding Evaluation and\n  Analysis", "abstract": "Recently, numerous embedding models have been made available and widely used\nfor various NLP tasks. The Massive Text Embedding Benchmark (MTEB) has\nprimarily simplified the process of choosing a model that performs well for\nseveral tasks in English, but extensions to other languages remain challenging.\nThis is why we expand MTEB to propose the first massive benchmark of sentence\nembeddings for French. We gather 15 existing datasets in an easy-to-use\ninterface and create three new French datasets for a global evaluation of 8\ntask categories. We compare 51 carefully selected embedding models on a large\nscale, conduct comprehensive statistical tests, and analyze the correlation\nbetween model performance and many of their characteristics. We find out that\neven if no model is the best on all tasks, large multilingual models\npre-trained on sentence similarity perform exceptionally well. Our work comes\nwith open-source code, new datasets and a public leaderboard.", "published": "2024-05-30 20:34:37", "link": "http://arxiv.org/abs/2405.20468v2", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Phantom: General Trigger Attacks on Retrieval Augmented Language\n  Generation", "abstract": "Retrieval Augmented Generation (RAG) expands the capabilities of modern large\nlanguage models (LLMs), by anchoring, adapting, and personalizing their\nresponses to the most relevant knowledge sources. It is particularly useful in\nchatbot applications, allowing developers to customize LLM output without\nexpensive retraining. Despite their significant utility in various\napplications, RAG systems present new security risks. In this work, we propose\nnew attack vectors that allow an adversary to inject a single malicious\ndocument into a RAG system's knowledge base, and mount a backdoor poisoning\nattack. We design Phantom, a general two-stage optimization framework against\nRAG systems, that crafts a malicious poisoned document leading to an integrity\nviolation in the model's output. First, the document is constructed to be\nretrieved only when a specific trigger sequence of tokens appears in the\nvictim's queries. Second, the document is further optimized with crafted\nadversarial text that induces various adversarial objectives on the LLM output,\nincluding refusal to answer, reputation damage, privacy violations, and harmful\nbehaviors. We demonstrate our attacks on multiple LLM architectures, including\nGemma, Vicuna, and Llama, and show that they transfer to GPT-3.5 Turbo and\nGPT-4. Finally, we successfully conducted a Phantom attack on NVIDIA's\nblack-box production RAG system, \"Chat with RTX\".", "published": "2024-05-30 21:19:24", "link": "http://arxiv.org/abs/2405.20485v2", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Deep Learning Approaches for Detecting Adversarial Cyberbullying and\n  Hate Speech in Social Networks", "abstract": "Cyberbullying is a significant concern intricately linked to technology that\ncan find resolution through technological means. Despite its prevalence,\ntechnology also provides solutions to mitigate cyberbullying. To address\ngrowing concerns regarding the adverse impact of cyberbullying on individuals'\nonline experiences, various online platforms and researchers are actively\nadopting measures to enhance the safety of digital environments. While\nresearchers persist in crafting detection models to counteract or minimize\ncyberbullying, malicious actors are deploying adversarial techniques to\ncircumvent these detection methods. This paper focuses on detecting\ncyberbullying in adversarial attack content within social networking site text\ndata, specifically emphasizing hate speech. Utilizing a deep learning-based\napproach with a correction algorithm, this paper yielded significant results.\nAn LSTM model with a fixed epoch of 100 demonstrated remarkable performance,\nachieving high accuracy, precision, recall, F1-score, and AUC-ROC scores of\n87.57%, 88.73%, 87.57%, 88.15%, and 91% respectively. Additionally, the LSTM\nmodel's performance surpassed that of previous studies.", "published": "2024-05-30 21:44:15", "link": "http://arxiv.org/abs/2406.17793v1", "categories": ["cs.LG", "cs.CL", "cs.CY"], "primary_category": "cs.LG"}
{"title": "From Symbolic Tasks to Code Generation: Diversification Yields Better\n  Task Performers", "abstract": "Instruction tuning -- tuning large language models on instruction-output\npairs -- is a promising technique for making models better adapted to the real\nworld. Yet, the key factors driving the model's capability to understand and\nfollow instructions not seen during training remain under-explored. Our\ninvestigation begins with a series of synthetic experiments within the\ntheoretical framework of a Turing-complete algorithm called Markov algorithm,\nwhich allows fine-grained control over the instruction-tuning data.\nGeneralization and robustness with respect to the training distribution emerge\nonce a diverse enough set of tasks is provided, even though very few examples\nare provided for each task. We extend these initial results to a real-world\napplication scenario of code generation and find that a more diverse\ninstruction set, extending beyond code-related tasks, improves the performance\nof code generation. Our observations suggest that a more diverse semantic space\nfor instruction-tuning sets greatly improves the model's ability to follow\ninstructions and perform tasks.", "published": "2024-05-30 07:54:07", "link": "http://arxiv.org/abs/2405.19787v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.LO", "cs.PL"], "primary_category": "cs.CL"}
{"title": "Video-Language Critic: Transferable Reward Functions for\n  Language-Conditioned Robotics", "abstract": "Natural language is often the easiest and most convenient modality for humans\nto specify tasks for robots. However, learning to ground language to behavior\ntypically requires impractical amounts of diverse, language-annotated\ndemonstrations collected on each target robot. In this work, we aim to separate\nthe problem of what to accomplish from how to accomplish it, as the former can\nbenefit from substantial amounts of external observation-only data, and only\nthe latter depends on a specific robot embodiment. To this end, we propose\nVideo-Language Critic, a reward model that can be trained on readily available\ncross-embodiment data using contrastive learning and a temporal ranking\nobjective, and use it to score behavior traces from a separate actor. When\ntrained on Open X-Embodiment data, our reward model enables 2x more\nsample-efficient policy training on Meta-World tasks than a sparse reward only,\ndespite a significant domain gap. Using in-domain data but in a challenging\ntask generalization setting on Meta-World, we further demonstrate more\nsample-efficient training than is possible with prior language-conditioned\nreward models that are either trained with binary classification, use static\nimages, or do not leverage the temporal information present in video data.", "published": "2024-05-30 12:18:06", "link": "http://arxiv.org/abs/2405.19988v2", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Iterative Feature Boosting for Explainable Speech Emotion Recognition", "abstract": "In speech emotion recognition (SER), using predefined features without\nconsidering their practical importance may lead to high dimensional datasets,\nincluding redundant and irrelevant information. Consequently, high-dimensional\nlearning often results in decreasing model accuracy while increasing\ncomputational complexity. Our work underlines the importance of carefully\nconsidering and analyzing features in order to build efficient SER systems. We\npresent a new supervised SER method based on an efficient feature engineering\napproach. We pay particular attention to the explainability of results to\nevaluate feature relevance and refine feature sets. This is performed\niteratively through feature evaluation loop, using Shapley values to boost\nfeature selection and improve overall framework performance. Our approach\nallows thus to balance the benefits between model performance and transparency.\nThe proposed method outperforms human-level performance (HLP) and\nstate-of-the-art machine learning methods in emotion recognition on the TESS\ndataset. The source code of this paper is publicly available at\nhttps://github.com/alaaNfissi/Iterative-Feature-Boosting-for-Explainable-Speech-Emotion-Recognition.", "published": "2024-05-30 15:44:27", "link": "http://arxiv.org/abs/2405.20172v3", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "eess.AS", "I.2.7; I.2.6; I.2.1; I.2.8"], "primary_category": "cs.SD"}
{"title": "Analysing the Public Discourse around OpenAI's Text-To-Video Model\n  'Sora' using Topic Modeling", "abstract": "The recent introduction of OpenAI's text-to-video model Sora has sparked\nwidespread public discourse across online communities. This study aims to\nuncover the dominant themes and narratives surrounding Sora by conducting topic\nmodeling analysis on a corpus of 1,827 Reddit comments from five relevant\nsubreddits (r/OpenAI, r/technology, r/singularity, r/vfx, and r/ChatGPT). The\ncomments were collected over a two-month period following Sora's announcement\nin February 2024. After preprocessing the data, Latent Dirichlet Allocation\n(LDA) was employed to extract four key topics: 1) AI Impact and Trends in Sora\nDiscussions, 2) Public Opinion and Concerns about Sora, 3) Artistic Expression\nand Video Creation with Sora, and 4) Sora's Applications in Media and\nEntertainment. Visualizations including word clouds, bar charts, and t-SNE\nclustering provided insights into the importance of topic keywords and the\ndistribution of comments across topics. The results highlight prominent\nnarratives around Sora's potential impact on industries and employment, public\nsentiment and ethical concerns, creative applications, and use cases in the\nmedia and entertainment sectors. While limited to Reddit data within a specific\ntimeframe, this study offers a framework for understanding public perceptions\nof emerging generative AI technologies through online discourse analysis.", "published": "2024-05-30 01:55:30", "link": "http://arxiv.org/abs/2407.13071v1", "categories": ["cs.CY", "cs.CL", "cs.IR", "cs.LG", "cs.SI"], "primary_category": "cs.CY"}
{"title": "1st Place Solution to Odyssey Emotion Recognition Challenge Task1:\n  Tackling Class Imbalance Problem", "abstract": "Speech emotion recognition is a challenging classification task with natural\nemotional speech, especially when the distribution of emotion types is\nimbalanced in the training and test data. In this case, it is more difficult\nfor a model to learn to separate minority classes, resulting in those sometimes\nbeing ignored or frequently misclassified. Previous work has utilised class\nweighted loss for training, but problems remain as it sometimes causes\nover-fitting for minor classes or under-fitting for major classes. This paper\npresents the system developed by a multi-site team for the participation in the\nOdyssey 2024 Emotion Recognition Challenge Track-1. The challenge data has the\naforementioned properties and therefore the presented systems aimed to tackle\nthese issues, by introducing focal loss in optimisation when applying class\nweighted loss. Specifically, the focal loss is further weighted by prior-based\nclass weights. Experimental results show that combining these two approaches\nbrings better overall performance, by sacrificing performance on major classes.\nThe system further employs a majority voting strategy to combine the outputs of\nan ensemble of 7 models. The models are trained independently, using different\nacoustic features and loss functions - with the aim to have different\nproperties for different data. Hence these models show different performance\npreferences on major classes and minor classes. The ensemble system output\nobtained the best performance in the challenge, ranking top-1 among 68\nsubmissions. It also outperformed all single models in our set. On the Odyssey\n2024 Emotion Recognition Challenge Task-1 data the system obtained a Macro-F1\nscore of 35.69% and an accuracy of 37.32%.", "published": "2024-05-30 13:55:43", "link": "http://arxiv.org/abs/2405.20064v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Explainable Attribute-Based Speaker Verification", "abstract": "This paper proposes a fully explainable approach to speaker verification\n(SV), a task that fundamentally relies on individual speaker characteristics.\nThe opaque use of speaker attributes in current SV systems raises concerns of\ntrust. Addressing this, we propose an attribute-based explainable SV system\nthat identifies speakers by comparing personal attributes such as gender,\nnationality, and age extracted automatically from voice recordings. We believe\nthis approach better aligns with human reasoning, making it more understandable\nthan traditional methods. Evaluated on the Voxceleb1 test set, the best\nperformance of our system is comparable with the ground truth established when\nusing all correct attributes, proving its efficacy. Whilst our approach\nsacrifices some performance compared to non-explainable methods, we believe\nthat it moves us closer to the goal of transparent, interpretable AI and lays\nthe groundwork for future enhancements through attribute expansion.", "published": "2024-05-30 08:04:28", "link": "http://arxiv.org/abs/2405.19796v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Spectral Mapping of Singing Voices: U-Net-Assisted Vocal Segmentation", "abstract": "Separating vocal elements from musical tracks is a longstanding challenge in\naudio signal processing. This study tackles the distinct separation of vocal\ncomponents from musical spectrograms. We employ the Short Time Fourier\nTransform (STFT) to extract audio waves into detailed frequency-time\nspectrograms, utilizing the benchmark MUSDB18 dataset for music separation.\nSubsequently, we implement a UNet neural network to segment the spectrogram\nimage, aiming to delineate and extract singing voice components accurately. We\nachieved noteworthy results in audio source separation using of our U-Net-based\nmodels. The combination of frequency-axis normalization with Min/Max scaling\nand the Mean Absolute Error (MAE) loss function achieved the highest\nSource-to-Distortion Ratio (SDR) of 7.1 dB, indicating a high level of accuracy\nin preserving the quality of the original signal during separation. This setup\nalso recorded impressive Source-to-Interference Ratio (SIR) and\nSource-to-Artifact Ratio (SAR) scores of 25.2 dB and 7.2 dB, respectively.\nThese values significantly outperformed other configurations, particularly\nthose using Quantile-based normalization or a Mean Squared Error (MSE) loss\nfunction. Our source code, model weights, and demo material can be found at the\nproject's GitHub repository: https://github.com/mbrotos/SoundSeg", "published": "2024-05-30 13:47:53", "link": "http://arxiv.org/abs/2405.20059v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "RapVerse: Coherent Vocals and Whole-Body Motions Generations from Text", "abstract": "In this work, we introduce a challenging task for simultaneously generating\n3D holistic body motions and singing vocals directly from textual lyrics\ninputs, advancing beyond existing works that typically address these two\nmodalities in isolation. To facilitate this, we first collect the RapVerse\ndataset, a large dataset containing synchronous rapping vocals, lyrics, and\nhigh-quality 3D holistic body meshes. With the RapVerse dataset, we investigate\nthe extent to which scaling autoregressive multimodal transformers across\nlanguage, audio, and motion can enhance the coherent and realistic generation\nof vocals and whole-body human motions. For modality unification, a\nvector-quantized variational autoencoder is employed to encode whole-body\nmotion sequences into discrete motion tokens, while a vocal-to-unit model is\nleveraged to obtain quantized audio tokens preserving content, prosodic\ninformation, and singer identity. By jointly performing transformer modeling on\nthese three modalities in a unified way, our framework ensures a seamless and\nrealistic blend of vocals and human motions. Extensive experiments demonstrate\nthat our unified generation framework not only produces coherent and realistic\nsinging vocals alongside human motions directly from textual inputs but also\nrivals the performance of specialized single-modality generation systems,\nestablishing new benchmarks for joint vocal-motion generation. The project page\nis available for research purposes at https://vis-www.cs.umass.edu/RapVerse.", "published": "2024-05-30 17:59:39", "link": "http://arxiv.org/abs/2405.20336v1", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Cross-Talk Reduction", "abstract": "While far-field multi-talker mixtures are recorded, each speaker can wear a\nclose-talk microphone so that close-talk mixtures can be recorded at the same\ntime. Although each close-talk mixture has a high signal-to-noise ratio (SNR)\nof the wearer, it has a very limited range of applications, as it also contains\nsignificant cross-talk speech by other speakers and is not clean enough. In\nthis context, we propose a novel task named cross-talk reduction (CTR) which\naims at reducing cross-talk speech, and a novel solution named CTRnet which is\nbased on unsupervised or weakly-supervised neural speech separation. In\nunsupervised CTRnet, close-talk and far-field mixtures are stacked as input for\na DNN to estimate the close-talk speech of each speaker. It is trained in an\nunsupervised, discriminative way such that the DNN estimate for each speaker\ncan be linearly filtered to cancel out the speaker's cross-talk speech captured\nat other microphones. In weakly-supervised CTRnet, we assume the availability\nof each speaker's activity timestamps during training, and leverage them to\nimprove the training of unsupervised CTRnet. Evaluation results on a simulated\ntwo-speaker CTR task and on a real-recorded conversational speech separation\nand recognition task show the effectiveness and potential of CTRnet.", "published": "2024-05-30 18:15:03", "link": "http://arxiv.org/abs/2405.20402v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Effects of Dataset Sampling Rate for Noise Cancellation through Deep\n  Learning", "abstract": "Background: Active noise cancellation has been a subject of research for\ndecades. Traditional techniques, like the Fast Fourier Transform, have\nlimitations in certain scenarios. This research explores the use of deep neural\nnetworks (DNNs) as a superior alternative. Objective: The study aims to\ndetermine the effect sampling rate within training data has on lightweight,\nefficient DNNs that operate within the processing constraints of mobile\ndevices. Methods: We chose the ConvTasNET network for its proven efficiency in\nspeech separation and enhancement. ConvTasNET was trained on datasets such as\nWHAM!, LibriMix, and the MS-2023 DNS Challenge. The datasets were sampled at\nrates of 8kHz, 16kHz, and 48kHz to analyze the effect of sampling rate on noise\ncancellation efficiency and effectiveness. The model was tested on a core-i7\nIntel processor from 2023, assessing the network's ability to produce clear\naudio while filtering out background noise. Results: Models trained at higher\nsampling rates (48kHz) provided much better evaluation metrics against Total\nHarmonic Distortion (THD) and Quality Prediction For Generative Neural Speech\nCodecs (WARP-Q) values, indicating improved audio quality. However, a trade-off\nwas noted with the processing time being longer for higher sampling rates.\nConclusions: The Conv-TasNET network, trained on datasets sampled at higher\nrates like 48kHz, offers a robust solution for mobile devices in achieving\nnoise cancellation through speech separation and enhancement. Future work\ninvolves optimizing the model's efficiency further and testing on mobile\ndevices.", "published": "2024-05-30 16:20:44", "link": "http://arxiv.org/abs/2405.20884v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
