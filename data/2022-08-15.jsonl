{"title": "A Hybrid Model of Classification and Generation for Spatial Relation\n  Extraction", "abstract": "Extracting spatial relations from texts is a fundamental task for natural\nlanguage understanding and previous studies only regard it as a classification\ntask, ignoring those spatial relations with null roles due to their poor\ninformation. To address the above issue, we first view spatial relation\nextraction as a generation task and propose a novel hybrid model HMCGR for this\ntask. HMCGR contains a generation and a classification model, while the former\ncan generate those null-role relations and the latter can extract those\nnon-null-role relations to complement each other. Moreover, a reflexivity\nevaluation mechanism is applied to further improve the accuracy based on the\nreflexivity principle of spatial relation. Experimental results on SpaceEval\nshow that HMCGR outperforms the SOTA baselines significantly.", "published": "2022-08-15 01:31:44", "link": "http://arxiv.org/abs/2208.06961v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ELEVANT: A Fully Automatic Fine-Grained Entity Linking Evaluation and\n  Analysis Tool", "abstract": "We present Elevant, a tool for the fully automatic fine-grained evaluation of\na set of entity linkers on a set of benchmarks. Elevant provides an automatic\nbreakdown of the performance by various error categories and by entity type.\nElevant also provides a rich and compact, yet very intuitive and\nself-explanatory visualization of the results of a linker on a benchmark in\ncomparison to the ground truth. A live demo, the link to the complete code base\non GitHub and a link to a demo video are provided under\nhttps://elevant.cs.uni-freiburg.de .", "published": "2022-08-15 13:54:36", "link": "http://arxiv.org/abs/2208.07193v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reproduction and Replication of an Adversarial Stylometry Experiment", "abstract": "Maintaining anonymity while communicating using natural language remains a\nchallenge. Standard authorship attribution techniques that analyze candidate\nauthors' writing styles achieve uncomfortably high accuracy even when the\nnumber of candidate authors is high. Adversarial stylometry defends against\nauthorship attribution with the goal of preventing unwanted deanonymization.\nThis paper reproduces and replicates experiments in a seminal study of defenses\nagainst authorship attribution (Brennan et al., 2012). We are able to\nsuccessfully reproduce and replicate the original results, although we conclude\nthat the effectiveness of the defenses studied is overstated due to a lack of a\ncontrol group in the original study. In our replication, we find new evidence\nsuggesting that an entirely automatic method, round-trip translation, merits\nre-examination as it appears to reduce the effectiveness of established\nauthorship attribution methods.", "published": "2022-08-15 18:24:00", "link": "http://arxiv.org/abs/2208.07395v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SynKB: Semantic Search for Synthetic Procedures", "abstract": "In this paper we present SynKB, an open-source, automatically extracted\nknowledge base of chemical synthesis protocols. Similar to proprietary\nchemistry databases such as Reaxsys, SynKB allows chemists to retrieve\nstructured knowledge about synthetic procedures. By taking advantage of recent\nadvances in natural language processing for procedural texts, SynKB supports\nmore flexible queries about reaction conditions, and thus has the potential to\nhelp chemists search the literature for conditions used in relevant reactions\nas they design new synthetic routes. Using customized Transformer models to\nautomatically extract information from 6 million synthesis procedures described\nin U.S. and EU patents, we show that for many queries, SynKB has higher recall\nthan Reaxsys, while maintaining high precision. We plan to make SynKB available\nas an open-source tool; in contrast, proprietary chemistry databases require\ncostly subscriptions.", "published": "2022-08-15 18:33:16", "link": "http://arxiv.org/abs/2208.07400v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Z-BERT-A: a zero-shot Pipeline for Unknown Intent detection", "abstract": "Intent discovery is a crucial task in natural language processing, and it is\nincreasingly relevant for various of industrial applications. Identifying\nnovel, unseen intents from user inputs remains one of the biggest challenges in\nthis field. Herein, we propose Zero-Shot-BERT-Adapters, a two-stage method for\nmultilingual intent discovery relying on a Transformer architecture, fine-tuned\nwith Adapters. We train the model for Natural Language Inference (NLI) and\nlater perform unknown intent classification in a zero-shot setting for multiple\nlanguages. In our evaluation, we first analyze the quality of the model after\nadaptive fine-tuning on known classes. Secondly, we evaluate its performance in\ncasting intent classification as an NLI task. Lastly, we test the zero-shot\nperformance of the model on unseen classes, showing how Zero-Shot-BERT-Adapters\ncan effectively perform intent discovery by generating semantically similar\nintents, if not equal, to the ground-truth ones. Our experiments show how\nZero-Shot-BERT-Adapters outperforms various baselines in two zero-shot\nsettings: known intent classification and unseen intent discovery. The proposed\npipeline holds the potential for broad application in customer care. It enables\nautomated dynamic triage using a lightweight model that can be easily deployed\nand scaled in various business scenarios, unlike large language models.\nZero-Shot-BERT-Adapters represents an innovative multi-language approach for\nintent discovery, enabling the online generation of novel intents. A Python\npackage implementing the pipeline and the new datasets we compiled are\navailable at the following link:\nhttps://github.com/GT4SD/zero-shot-bert-adapters.", "published": "2022-08-15 09:27:34", "link": "http://arxiv.org/abs/2208.07084v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Efficient Task-Oriented Dialogue Systems with Response Selection as an\n  Auxiliary Task", "abstract": "The adoption of pre-trained language models in task-oriented dialogue systems\nhas resulted in significant enhancements of their text generation abilities.\nHowever, these architectures are slow to use because of the large number of\ntrainable parameters and can sometimes fail to generate diverse responses. To\naddress these limitations, we propose two models with auxiliary tasks for\nresponse selection - (1) distinguishing distractors from ground truth responses\nand (2) distinguishing synthetic responses from ground truth labels. They\nachieve state-of-the-art results on the MultiWOZ 2.1 dataset with combined\nscores of 107.5 and 108.3 and outperform a baseline with three times more\nparameters. We publish reproducible code and checkpoints and discuss the\neffects of applying auxiliary tasks to T5-based architectures.", "published": "2022-08-15 09:59:44", "link": "http://arxiv.org/abs/2208.07097v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring Generative Models for Joint Attribute Value Extraction from\n  Product Titles", "abstract": "Attribute values of the products are an essential component in any e-commerce\nplatform. Attribute Value Extraction (AVE) deals with extracting the attributes\nof a product and their values from its title or description. In this paper, we\npropose to tackle the AVE task using generative frameworks. We present two\ntypes of generative paradigms, namely, word sequence-based and positional\nsequence-based, by formulating the AVE task as a generation problem. We conduct\nexperiments on two datasets where the generative approaches achieve the new\nstate-of-the-art results. This shows that we can use the proposed framework for\nAVE tasks without additional tagging or task-specific model design.", "published": "2022-08-15 11:51:31", "link": "http://arxiv.org/abs/2208.07130v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Entity Anchored ICD Coding", "abstract": "Medical coding is a complex task, requiring assignment of a subset of over\n72,000 ICD codes to a patient's notes. Modern natural language processing\napproaches to these tasks have been challenged by the length of the input and\nsize of the output space. We limit our model inputs to a small window around\nmedical entities found in our documents. From those local contexts, we build\ncontextualized representations of both ICD codes and entities, and aggregate\nover these representations to form document-level predictions. In contrast to\nexisting methods which use a representation fixed either in size or by codes\nseen in training, we represent ICD codes by encoding the code description with\nlocal context. We discuss metrics appropriate to deploying coding systems in\npractice. We show that our approach is superior to existing methods in both\nstandard and deployable measures, including performance on rare and unseen\ncodes.", "published": "2022-08-15 21:41:49", "link": "http://arxiv.org/abs/2208.07444v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Syntax-driven Data Augmentation for Named Entity Recognition", "abstract": "In low resource settings, data augmentation strategies are commonly leveraged\nto improve performance. Numerous approaches have attempted document-level\naugmentation (e.g., text classification), but few studies have explored\ntoken-level augmentation. Performed naively, data augmentation can produce\nsemantically incongruent and ungrammatical examples. In this work, we compare\nsimple masked language model replacement and an augmentation method using\nconstituency tree mutations to improve the performance of named entity\nrecognition in low-resource settings with the aim of preserving linguistic\ncohesion of the augmented sentences.", "published": "2022-08-15 01:24:55", "link": "http://arxiv.org/abs/2208.06957v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Explainable Artificial Intelligence for Assault Sentence Prediction in\n  New Zealand", "abstract": "The judiciary has historically been conservative in its use of Artificial\nIntelligence, but recent advances in machine learning have prompted scholars to\nreconsider such use in tasks like sentence prediction. This paper investigates\nby experimentation the potential use of explainable artificial intelligence for\npredicting imprisonment sentences in assault cases in New Zealand's courts. We\npropose a proof-of-concept explainable model and verify in practice that it is\nfit for purpose, with predicted sentences accurate to within one year. We\nfurther analyse the model to understand the most influential phrases in\nsentence length prediction. We conclude the paper with an evaluative discussion\nof the future benefits and risks of different ways of using such an AI model in\nNew Zealand's courts.", "published": "2022-08-15 02:52:18", "link": "http://arxiv.org/abs/2208.06981v1", "categories": ["cs.LG", "cs.CL", "cs.CY", "cs.NE"], "primary_category": "cs.LG"}
{"title": "Memory-Driven Text-to-Image Generation", "abstract": "We introduce a memory-driven semi-parametric approach to text-to-image\ngeneration, which is based on both parametric and non-parametric techniques.\nThe non-parametric component is a memory bank of image features constructed\nfrom a training set of images. The parametric component is a generative\nadversarial network. Given a new text description at inference time, the memory\nbank is used to selectively retrieve image features that are provided as basic\ninformation of target images, which enables the generator to produce realistic\nsynthetic results. We also incorporate the content information into the\ndiscriminator, together with semantic features, allowing the discriminator to\nmake a more reliable prediction. Experimental results demonstrate that the\nproposed memory-driven semi-parametric approach produces more realistic images\nthan purely parametric approaches, in terms of both visual fidelity and\ntext-image semantic consistency.", "published": "2022-08-15 06:32:57", "link": "http://arxiv.org/abs/2208.07022v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "MENLI: Robust Evaluation Metrics from Natural Language Inference", "abstract": "Recently proposed BERT-based evaluation metrics for text generation perform\nwell on standard benchmarks but are vulnerable to adversarial attacks, e.g.,\nrelating to information correctness. We argue that this stems (in part) from\nthe fact that they are models of semantic similarity. In contrast, we develop\nevaluation metrics based on Natural Language Inference (NLI), which we deem a\nmore appropriate modeling. We design a preference-based adversarial attack\nframework and show that our NLI based metrics are much more robust to the\nattacks than the recent BERT-based metrics. On standard benchmarks, our NLI\nbased metrics outperform existing summarization metrics, but perform below SOTA\nMT metrics. However, when combining existing metrics with our NLI metrics, we\nobtain both higher adversarial robustness (15%-30%) and higher quality metrics\nas measured on standard benchmarks (+5% to 30%).", "published": "2022-08-15 16:30:14", "link": "http://arxiv.org/abs/2208.07316v5", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Parametric Speech Synthesis Using Gaussian-Markov Model of\n  Spectral Envelope and Wavelet-Based Decomposition of F0", "abstract": "Neural network-based Text-to-Speech has significantly improved the quality of\nsynthesized speech. Prominent methods (e.g., Tacotron2, FastSpeech, FastPitch)\nusually generate Mel-spectrogram from text and then synthesize speech using\nvocoder (e.g., WaveNet, WaveGlow, HiFiGAN). Compared with traditional\nparametric approaches (e.g., STRAIGHT and WORLD), neural vocoder based\nend-to-end models suffer from slow inference speed, and the synthesized speech\nis usually not robust and lack of controllability. In this work, we propose a\nnovel updated vocoder, which is a simple signal model to train and easy to\ngenerate waveforms. We use the Gaussian-Markov model toward robust learning of\nspectral envelope and wavelet-based statistical signal processing to\ncharacterize and decompose F0 features. It can retain the fine spectral\nenvelope and achieve high controllability of natural speech. The experimental\nresults demonstrate that our proposed vocoder achieves better naturalness of\nreconstructed speech than the conventional STRAIGHT vocoder, slightly better\nthan WaveNet, and somewhat worse than the WaveRNN.", "published": "2022-08-15 11:24:47", "link": "http://arxiv.org/abs/2208.07122v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "LCSM: A Lightweight Complex Spectral Mapping Framework for Stereophonic\n  Acoustic Echo Cancellation", "abstract": "The traditional adaptive algorithms will face the non-uniqueness problem when\ndealing with stereophonic acoustic echo cancellation (SAEC). In this paper, we\nfirst propose an efficient multi-input and multi-output (MIMO) scheme based on\ndeep learning to filter out echoes from all microphone signals at once. Then,\nwe employ a lightweight complex spectral mapping framework (LCSM) for\nend-to-end SAEC without decorrelation preprocessing to the loudspeaker signals.\nInplace convolution and channel-wise spatial modeling are utilized to ensure\nthe near-end signal information is preserved. Finally, a cross-domain loss\nfunction is designed for better generalization capability. Experiments are\nevaluated on a variety of untrained conditions and results demonstrate that the\nLCSM significantly outperforms previous methods. Moreover, the proposed causal\nframework only has 0.55 million parameters, much less than the similar deep\nlearning-based methods, which is important for the resource-limited devices.", "published": "2022-08-15 15:38:51", "link": "http://arxiv.org/abs/2208.07277v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "C3-DINO: Joint Contrastive and Non-contrastive Self-Supervised Learning\n  for Speaker Verification", "abstract": "Self-supervised learning (SSL) has drawn an increased attention in the field\nof speech processing. Recent studies have demonstrated that contrastive\nlearning is able to learn discriminative speaker embeddings in a\nself-supervised manner. However, base contrastive self-supervised learning\n(CSSL) assumes that the pairs generated from a view of anchor instance and any\nview of other instances are all negative, which introduces many false negative\npairs in constructing the loss function. The problem is referred as\n$class$-$collision$, which remains as one major issue that impedes the CSSL\nbased speaker verification (SV) systems from achieving better performances. In\nthe meanwhile, studies reveal that negative sample free SSL frameworks perform\nwell in learning speaker or image representations. In this study, we\ninvestigate SSL techniques that lead to an improved SV performance. We first\nanalyse the impact of false negative pairs in the CSSL systems. Then, a\nmulti-stage Class-Collision Correction (C3) method is proposed, which leads to\nthe state-of-the-art CSSL based speaker embedding system. On the basis of the\npretrained CSSL model, we further propose to employ a negative sample free SSL\nobjective (i.e., DINO) to fine-tune the speaker embedding network. The\nresulting speaker embedding system (C3-DINO) achieves 2.5% EER with a simple\nCosine Distance Scoring method on Voxceleb1 test set, which outperforms the\nprevious SOTA SSL system (4.86%) by a significant +45% relative improvement.\nWith speaker clustering and pseudo labeling on Voxceleb2 training set, a\nLDA/CDS back-end applying on the C3-DINO speaker embeddings is able to further\npush the EER to 2.2%. Comprehensive experimental investigations of the Voxceleb\nbenchmarks and our internal dataset demonstrate the effectiveness of our\nproposed methods, and the performance gap between the SSL SV and the supervised\ncounterpart narrows further.", "published": "2022-08-15 21:52:02", "link": "http://arxiv.org/abs/2208.07446v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Analysis of impact of emotions on target speech extraction and speech\n  separation", "abstract": "Recently, the performance of blind speech separation (BSS) and target speech\nextraction (TSE) has greatly progressed. Most works, however, focus on\nrelatively well-controlled conditions using, e.g., read speech. The performance\nmay degrade in more realistic situations. One of the factors causing such\ndegradation may be intrinsic speaker variability, such as emotions, occurring\ncommonly in realistic speech. In this paper, we investigate the influence of\nemotions on TSE and BSS. We create a new test dataset of emotional mixtures for\nthe evaluation of TSE and BSS. This dataset combines LibriSpeech and Ryerson\nAudio-Visual Database of Emotional Speech and Song (RAVDESS). Through\ncontrolled experiments, we can analyze the impact of different emotions on the\nperformance of BSS and TSE. We observe that BSS is relatively robust to\nemotions, while TSE, which requires identifying and extracting the speech of a\ntarget speaker, is much more sensitive to emotions. On comparative speaker\nverification experiments we show that identifying the target speaker may be\nparticularly challenging when dealing with emotional speech. Using our\nfindings, we outline potential future directions that could improve the\nrobustness of BSS and TSE systems toward emotional speech.", "published": "2022-08-15 09:47:13", "link": "http://arxiv.org/abs/2208.07091v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Differentiable WORLD Synthesizer-based Neural Vocoder With Application\n  To End-To-End Audio Style Transfer", "abstract": "In this paper, we propose a differentiable WORLD synthesizer and demonstrate\nits use in end-to-end audio style transfer tasks such as (singing) voice\nconversion and the DDSP timbre transfer task. Accordingly, our baseline\ndifferentiable synthesizer has no model parameters, yet it yields adequate\nsynthesis quality. We can extend the baseline synthesizer by appending\nlightweight black-box postnets which apply further processing to the baseline\noutput in order to improve fidelity. An alternative differentiable approach\nconsiders extraction of the source excitation spectrum directly, which can\nimprove naturalness albeit for a narrower class of style transfer applications.\nThe acoustic feature parameterization used by our approaches has the added\nbenefit that it naturally disentangles pitch and timbral information so that\nthey can be modeled separately. Moreover, as there exists a robust means of\nestimating these acoustic features from monophonic audio sources, it allows for\nparameter loss terms to be added to an end-to-end objective function, which can\nhelp convergence and/or further stabilize (adversarial) training.", "published": "2022-08-15 15:48:36", "link": "http://arxiv.org/abs/2208.07282v5", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
