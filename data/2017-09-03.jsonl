{"title": "Investigating how well contextual features are captured by\n  bi-directional recurrent neural network models", "abstract": "Learning algorithms for natural language processing (NLP) tasks traditionally\nrely on manually defined relevant contextual features. On the other hand,\nneural network models using an only distributional representation of words have\nbeen successfully applied for several NLP tasks. Such models learn features\nautomatically and avoid explicit feature engineering. Across several domains,\nneural models become a natural choice specifically when limited characteristics\nof data are known. However, this flexibility comes at the cost of\ninterpretability. In this paper, we define three different methods to\ninvestigate ability of bi-directional recurrent neural networks (RNNs) in\ncapturing contextual features. In particular, we analyze RNNs for sequence\ntagging tasks. We perform a comprehensive analysis on general as well as\nbiomedical domain datasets. Our experiments focus on important contextual words\nas features, which can easily be extended to analyze various other feature\ntypes. We also investigate positional effects of context words and show how the\ndeveloped methods can be used for error analysis.", "published": "2017-09-03 04:00:49", "link": "http://arxiv.org/abs/1709.00659v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Disentangling ASR and MT Errors in Speech Translation", "abstract": "The main aim of this paper is to investigate automatic quality assessment for\nspoken language translation (SLT). More precisely, we investigate SLT errors\nthat can be due to transcription (ASR) or to translation (MT) modules. This\npaper investigates automatic detection of SLT errors using a single classifier\nbased on joint ASR and MT features. We evaluate both 2-class (good/bad) and\n3-class (good/badASR/badMT ) labeling tasks. The 3-class problem necessitates\nto disentangle ASR and MT errors in the speech translation output and we\npropose two label extraction methods for this non trivial step. This enables -\nas a by-product - qualitative analysis on the SLT errors and their origin (are\nthey due to transcription or to translation step?) on our large in-house corpus\nfor French-to-English speech translation.", "published": "2017-09-03 07:42:17", "link": "http://arxiv.org/abs/1709.00678v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Semi-Supervised Approach to Detecting Stance in Tweets", "abstract": "Stance classification aims to identify, for a particular issue under\ndiscussion, whether the speaker or author of a conversational turn has Pro\n(Favor) or Con (Against) stance on the issue. Detecting stance in tweets is a\nnew task proposed for SemEval-2016 Task6, involving predicting stance for a\ndataset of tweets on the topics of abortion, atheism, climate change, feminism\nand Hillary Clinton. Given the small size of the dataset, our team created our\nown topic-specific training corpus by developing a set of high precision\nhashtags for each topic that were used to query the twitter API, with the aim\nof developing a large training corpus without additional human labeling of\ntweets for stance. The hashtags selected for each topic were predicted to be\nstance-bearing on their own. Experimental results demonstrate good performance\nfor our features for opinion-target pairs based on generalizing dependency\nfeatures using sentiment lexicons.", "published": "2017-09-03 04:31:11", "link": "http://arxiv.org/abs/1709.01895v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Topic Independent Identification of Agreement and Disagreement in Social\n  Media Dialogue", "abstract": "Research on the structure of dialogue has been hampered for years because\nlarge dialogue corpora have not been available. This has impacted the dialogue\nresearch community's ability to develop better theories, as well as good off\nthe shelf tools for dialogue processing. Happily, an increasing amount of\ninformation and opinion exchange occur in natural dialogue in online forums,\nwhere people share their opinions about a vast range of topics. In particular\nwe are interested in rejection in dialogue, also called disagreement and\ndenial, where the size of available dialogue corpora, for the first time,\noffers an opportunity to empirically test theoretical accounts of the\nexpression and inference of rejection in dialogue. In this paper, we test\nwhether topic-independent features motivated by theoretical predictions can be\nused to recognize rejection in online forums in a topic independent way. Our\nresults show that our theoretically motivated features achieve 66% accuracy, an\nimprovement over a unigram baseline of an absolute 6%.", "published": "2017-09-03 04:16:15", "link": "http://arxiv.org/abs/1709.00661v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Using Summarization to Discover Argument Facets in Online Ideological\n  Dialog", "abstract": "More and more of the information available on the web is dialogic, and a\nsignificant portion of it takes place in online forum conversations about\ncurrent social and political topics. We aim to develop tools to summarize what\nthese conversations are about. What are the CENTRAL PROPOSITIONS associated\nwith different stances on an issue, what are the abstract objects under\ndiscussion that are central to a speaker's argument? How can we recognize that\ntwo CENTRAL PROPOSITIONS realize the same FACET of the argument? We hypothesize\nthat the CENTRAL PROPOSITIONS are exactly those arguments that people find most\nsalient, and use human summarization as a probe for discovering them. We\ndescribe our corpus of human summaries of opinionated dialogs, then show how we\ncan identify similar repeated arguments, and group them into FACETS across many\ndiscussions of a topic. We define a new task, ARGUMENT FACET SIMILARITY (AFS),\nand show that we can predict AFS with a .54 correlation score, versus an ngram\nsystem baseline of .39 and a semantic textual similarity system baseline of\n.45.", "published": "2017-09-03 04:16:25", "link": "http://arxiv.org/abs/1709.00662v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Formalising Type-Logical Grammars in Agda", "abstract": "In recent years, the interest in using proof assistants to formalise and\nreason about mathematics and programming languages has grown. Type-logical\ngrammars, being closely related to type theories and systems used in functional\nprogramming, are a perfect candidate to next apply this curiosity to. The\nadvantages of using proof assistants is that they allow one to write formally\nverified proofs about one's type-logical systems, and that any theory, once\nimplemented, can immediately be computed with. The downside is that in many\ncases the formal proofs are written as an afterthought, are incomplete, or use\nobtuse syntax. This makes it that the verified proofs are often much more\ndifficult to read than the pen-and-paper proofs, and almost never directly\npublished. In this paper, we will try to remedy that by example.\n  Concretely, we use Agda to model the Lambek-Grishin calculus, a grammar logic\nwith a rich vocabulary of type-forming operations. We then present a verified\nprocedure for cut elimination in this system. Then we briefly outline a CPS\ntranslation from proofs in the Lambek-Grishin calculus to programs in Agda. And\nfinally, we will put our system to use in the analysis of a simple example\nsentence.", "published": "2017-09-03 14:54:29", "link": "http://arxiv.org/abs/1709.00728v1", "categories": ["cs.LO", "cs.CL"], "primary_category": "cs.LO"}
{"title": "Understanding the Logical and Semantic Structure of Large Documents", "abstract": "Current language understanding approaches focus on small documents, such as\nnewswire articles, blog posts, product reviews and discussion forum entries.\nUnderstanding and extracting information from large documents like legal\nbriefs, proposals, technical manuals and research articles is still a\nchallenging task. We describe a framework that can analyze a large document and\nhelp people to know where a particular information is in that document. We aim\nto automatically identify and classify semantic sections of documents and\nassign consistent and human-understandable labels to similar sections across\ndocuments. A key contribution of our research is modeling the logical and\nsemantic structure of an electronic document. We apply machine learning\ntechniques, including deep learning, in our prototype system. We also make\navailable a dataset of information about a collection of scholarly articles\nfrom the arXiv eprints collection that includes a wide range of metadata for\neach article, including a table of contents, section labels, section\nsummarizations and more. We hope that this dataset will be a useful resource\nfor the machine learning and NLP communities in information retrieval,\ncontent-based question answering and language modeling.", "published": "2017-09-03 21:38:56", "link": "http://arxiv.org/abs/1709.00770v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
