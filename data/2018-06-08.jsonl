{"title": "Findings of the Second Workshop on Neural Machine Translation and\n  Generation", "abstract": "This document describes the findings of the Second Workshop on Neural Machine\nTranslation and Generation, held in concert with the annual conference of the\nAssociation for Computational Linguistics (ACL 2018). First, we summarize the\nresearch trends of papers presented in the proceedings, and note that there is\nparticular interest in linguistic structure, domain adaptation, data\naugmentation, handling inadequate resources, and analysis of models. Second, we\ndescribe the results of the workshop's shared task on efficient neural machine\ntranslation, where participants were tasked with creating MT systems that are\nboth accurate and efficient.", "published": "2018-06-08 01:41:20", "link": "http://arxiv.org/abs/1806.02940v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hearst Patterns Revisited: Automatic Hypernym Detection from Large Text\n  Corpora", "abstract": "Methods for unsupervised hypernym detection may broadly be categorized\naccording to two paradigms: pattern-based and distributional methods. In this\npaper, we study the performance of both approaches on several hypernymy tasks\nand find that simple pattern-based methods consistently outperform\ndistributional methods on common benchmark datasets. Our results show that\npattern-based models provide important contextual constraints which are not yet\ncaptured in distributional methods.", "published": "2018-06-08 14:34:29", "link": "http://arxiv.org/abs/1806.03191v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ChangeMyView Through Concessions: Do Concessions Increase Persuasion?", "abstract": "In discourse studies concessions are considered among those argumentative\nstrategies that increase persuasion. We aim to empirically test this hypothesis\nby calculating the distribution of argumentative concessions in persuasive vs.\nnon-persuasive comments from the ChangeMyView subreddit. This constitutes a\nchallenging task since concessions are not always part of an argument. Drawing\nfrom a theoretically-informed typology of concessions, we conduct an annotation\ntask to label a set of polysemous lexical markers as introducing an\nargumentative concession or not and we observe their distribution in threads\nthat achieved and did not achieve persuasion. For the annotation, we used both\nexpert and novice annotators. With the ultimate goal of conducting the study on\nlarge datasets, we present a self-training method to automatically identify\nargumentative concessions using linguistically motivated features. We achieve a\nmoderate F1 of 57.4% on the development set and 46.0% on the test set via the\nself-training method. These results are comparable to state of the art results\non similar tasks of identifying explicit discourse connective types from the\nPenn Discourse Treebank. Our findings from the manual labeling and the\nclassification experiments indicate that the type of argumentative concessions\nwe investigated is almost equally likely to be used in winning and losing\narguments from the ChangeMyView dataset. While this result seems to contradict\ntheoretical assumptions, we provide some reasons for this discrepancy related\nto the ChangeMyView subreddit.", "published": "2018-06-08 15:38:04", "link": "http://arxiv.org/abs/1806.03223v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Neural Machine Translation with Task-Specific Attention", "abstract": "Multilingual machine translation addresses the task of translating between\nmultiple source and target languages. We propose task-specific attention\nmodels, a simple but effective technique for improving the quality of\nsequence-to-sequence neural multilingual translation. Our approach seeks to\nretain as much of the parameter sharing generalization of NMT models as\npossible, while still allowing for language-specific specialization of the\nattention model to a particular language-pair or task. Our experiments on four\nlanguages of the Europarl corpus show that using a target-specific model of\nattention provides consistent gains in translation quality for all possible\ntranslation directions, compared to a model in which all parameters are shared.\nWe observe improved translation quality even in the (extreme) low-resource\nzero-shot translation directions for which the model never saw explicitly\npaired parallel data.", "published": "2018-06-08 17:18:26", "link": "http://arxiv.org/abs/1806.03280v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Policy Gradient as a Proxy for Dynamic Oracles in Constituency Parsing", "abstract": "Dynamic oracles provide strong supervision for training constituency parsers\nwith exploration, but must be custom defined for a given parser's transition\nsystem. We explore using a policy gradient method as a parser-agnostic\nalternative. In addition to directly optimizing for a tree-level metric such as\nF1, policy gradient has the potential to reduce exposure bias by allowing\nexploration during training; moreover, it does not require a dynamic oracle for\nsupervision. On four constituency parsers in three languages, the method\nsubstantially outperforms static oracle likelihood training in almost all\nsettings. For parsers where a dynamic oracle is available (including a novel\noracle which we define for the transition system of Dyer et al. 2016), policy\ngradient typically recaptures a substantial fraction of the performance gain\nafforded by the dynamic oracle.", "published": "2018-06-08 17:45:13", "link": "http://arxiv.org/abs/1806.03290v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "#SarcasmDetection is soooo general! Towards a Domain-Independent\n  Approach for Detecting Sarcasm", "abstract": "Automatic sarcasm detection methods have traditionally been designed for\nmaximum performance on a specific domain. This poses challenges for those\nwishing to transfer those approaches to other existing or novel domains, which\nmay be typified by very different language characteristics. We develop a\ngeneral set of features and evaluate it under different training scenarios\nutilizing in-domain and/or out-of-domain training data. The best-performing\nscenario, training on both while employing a domain adaptation step, achieves\nan F1 of 0.780, which is well above baseline F1-measures of 0.515 and 0.345. We\nalso show that the approach outperforms the best results from prior work on the\nsame target domain.", "published": "2018-06-08 22:47:10", "link": "http://arxiv.org/abs/1806.03369v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Representation Learning of Entities and Documents from Knowledge Base\n  Descriptions", "abstract": "In this paper, we describe TextEnt, a neural network model that learns\ndistributed representations of entities and documents directly from a knowledge\nbase (KB). Given a document in a KB consisting of words and entity annotations,\nwe train our model to predict the entity that the document describes and map\nthe document and its target entity close to each other in a continuous vector\nspace. Our model is trained using a large number of documents extracted from\nWikipedia. The performance of the proposed model is evaluated using two tasks,\nnamely fine-grained entity typing and multiclass text classification. The\nresults demonstrate that our model achieves state-of-the-art performance on\nboth tasks. The code and the trained representations are made available online\nfor further academic research.", "published": "2018-06-08 03:49:34", "link": "http://arxiv.org/abs/1806.02960v1", "categories": ["cs.CL", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Measuring Conversational Productivity in Child Forensic Interviews", "abstract": "Child Forensic Interviewing (FI) presents a challenge for effective\ninformation retrieval and decision making. The high stakes associated with the\nprocess demand that expert legal interviewers are able to effectively establish\na channel of communication and elicit substantive knowledge from the\nchild-client while minimizing potential for experiencing trauma. As a first\nstep toward computationally modeling and producing quality spoken interviewing\nstrategies and a generalized understanding of interview dynamics, we propose a\nnovel methodology to computationally model effectiveness criteria, by applying\nsummarization and topic modeling techniques to objectively measure and rank the\nresponsiveness and conversational productivity of a child during FI. We score\ninformation retrieval by constructing an agenda to represent general topics of\ninterest and measuring alignment with a given response and leveraging lexical\nentrainment for responsiveness. For comparison, we present our methods along\nwith traditional metrics of evaluation and discuss the use of prior information\nfor generating situational awareness.", "published": "2018-06-08 21:21:19", "link": "http://arxiv.org/abs/1806.03357v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Dank Learning: Generating Memes Using Deep Neural Networks", "abstract": "We introduce a novel meme generation system, which given any image can\nproduce a humorous and relevant caption. Furthermore, the system can be\nconditioned on not only an image but also a user-defined label relating to the\nmeme template, giving a handle to the user on meme content. The system uses a\npretrained Inception-v3 network to return an image embedding which is passed to\nan attention-based deep-layer LSTM model producing the caption - inspired by\nthe widely recognised Show and Tell Model. We implement a modified beam search\nto encourage diversity in the captions. We evaluate the quality of our model\nusing perplexity and human assessment on both the quality of memes generated\nand whether they can be differentiated from real ones. Our model produces\noriginal memes that cannot on the whole be differentiated from real ones.", "published": "2018-06-08 03:29:30", "link": "http://arxiv.org/abs/1806.04510v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multilingual Sentiment Analysis: An RNN-Based Framework for Limited Data", "abstract": "Sentiment analysis is a widely studied NLP task where the goal is to\ndetermine opinions, emotions, and evaluations of users towards a product, an\nentity or a service that they are reviewing. One of the biggest challenges for\nsentiment analysis is that it is highly language dependent. Word embeddings,\nsentiment lexicons, and even annotated data are language specific. Further,\noptimizing models for each language is very time consuming and labor intensive\nespecially for recurrent neural network models. From a resource perspective, it\nis very challenging to collect data for different languages.\n  In this paper, we look for an answer to the following research question: can\na sentiment analysis model trained on a language be reused for sentiment\nanalysis in other languages, Russian, Spanish, Turkish, and Dutch, where the\ndata is more limited? Our goal is to build a single model in the language with\nthe largest dataset available for the task, and reuse it for languages that\nhave limited resources. For this purpose, we train a sentiment analysis model\nusing recurrent neural networks with reviews in English. We then translate\nreviews in other languages and reuse this model to evaluate the sentiments.\nExperimental results show that our robust approach of single model trained on\nEnglish reviews statistically significantly outperforms the baselines in\nseveral different languages.", "published": "2018-06-08 14:01:31", "link": "http://arxiv.org/abs/1806.04511v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Learn from Your Neighbor: Learning Multi-modal Mappings from Sparse\n  Annotations", "abstract": "Many structured prediction problems (particularly in vision and language\ndomains) are ambiguous, with multiple outputs being correct for an input - e.g.\nthere are many ways of describing an image, multiple ways of translating a\nsentence; however, exhaustively annotating the applicability of all possible\noutputs is intractable due to exponentially large output spaces (e.g. all\nEnglish sentences). In practice, these problems are cast as multi-class\nprediction, with the likelihood of only a sparse set of annotations being\nmaximized - unfortunately penalizing for placing beliefs on plausible but\nunannotated outputs. We make and test the following hypothesis - for a given\ninput, the annotations of its neighbors may serve as an additional supervisory\nsignal. Specifically, we propose an objective that transfers supervision from\nneighboring examples. We first study the properties of our developed method in\na controlled toy setup before reporting results on multi-label classification\nand two image-grounded sequence modeling tasks - captioning and question\ngeneration. We evaluate using standard task-specific metrics and measures of\noutput diversity, finding consistent improvements over standard maximum\nlikelihood training and other baselines.", "published": "2018-06-08 01:18:10", "link": "http://arxiv.org/abs/1806.02934v1", "categories": ["stat.ML", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Towards Binary-Valued Gates for Robust LSTM Training", "abstract": "Long Short-Term Memory (LSTM) is one of the most widely used recurrent\nstructures in sequence modeling. It aims to use gates to control information\nflow (e.g., whether to skip some information or not) in the recurrent\ncomputations, although its practical implementation based on soft gates only\npartially achieves this goal. In this paper, we propose a new way for LSTM\ntraining, which pushes the output values of the gates towards 0 or 1. By doing\nso, we can better control the information flow: the gates are mostly open or\nclosed, instead of in a middle state, which makes the results more\ninterpretable. Empirical studies show that (1) Although it seems that we\nrestrict the model capacity, there is no performance drop: we achieve better or\ncomparable performances due to its better generalization ability; (2) The\noutputs of gates are not sensitive to their inputs: we can easily compress the\nLSTM unit in multiple ways, e.g., low-rank approximation and low-precision\napproximation. The compressed models are even better than the baseline models\nwithout compression.", "published": "2018-06-08 06:57:16", "link": "http://arxiv.org/abs/1806.02988v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Text Classification based on Word Subspace with Term-Frequency", "abstract": "Text classification has become indispensable due to the rapid increase of\ntext in digital form. Over the past three decades, efforts have been made to\napproach this task using various learning algorithms and statistical models\nbased on bag-of-words (BOW) features. Despite its simple implementation, BOW\nfeatures lack semantic meaning representation. To solve this problem, neural\nnetworks started to be employed to learn word vectors, such as the word2vec.\nWord2vec embeds word semantic structure into vectors, where the angle between\nvectors indicates the meaningful similarity between words. To measure the\nsimilarity between texts, we propose the novel concept of word subspace, which\ncan represent the intrinsic variability of features in a set of word vectors.\nThrough this concept, it is possible to model text from word vectors while\nholding semantic information. To incorporate the word frequency directly in the\nsubspace model, we further extend the word subspace to the term-frequency (TF)\nweighted word subspace. Based on these new concepts, text classification can be\nperformed under the mutual subspace method (MSM) framework. The validity of our\nmodeling is shown through experiments on the Reuters text database, comparing\nthe results to various state-of-art algorithms.", "published": "2018-06-08 12:55:37", "link": "http://arxiv.org/abs/1806.03125v1", "categories": ["stat.ML", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Analysis of Length Normalization in End-to-End Speaker Verification\n  System", "abstract": "The classical i-vectors and the latest end-to-end deep speaker embeddings are\nthe two representative categories of utterance-level representations in\nautomatic speaker verification systems. Traditionally, once i-vectors or deep\nspeaker embeddings are extracted, we rely on an extra length normalization step\nto normalize the representations into unit-length hyperspace before back-end\nmodeling. In this paper, we explore how the neural network learns\nlength-normalized deep speaker embeddings in an end-to-end manner. To this end,\nwe add a length normalization layer followed by a scale layer before the output\nlayer of the common classification network. We conducted experiments on the\nverification task of the Voxceleb1 dataset. The results show that integrating\nthis simple step in the end-to-end training pipeline significantly boosts the\nperformance of speaker verification. In the testing stage of our L2-normalized\nend-to-end system, a simple inner-product can achieve the state-of-the-art.", "published": "2018-06-08 15:09:14", "link": "http://arxiv.org/abs/1806.03209v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "On sound-based interpretation of neonatal EEG", "abstract": "Significant training is required to visually interpret neonatal EEG signals.\nThis study explores alternative sound-based methods for EEG interpretation\nwhich are designed to allow for intuitive and quick differentiation between\nhealthy background activity and abnormal activity such as seizures. A novel\nmethod based on frequency and amplitude modulation (FM/AM) is presented. The\nalgorithm is tuned to facilitate the audio domain perception of rhythmic\nactivity which is specific to neonatal seizures. The method is compared with\nthe previously developed phase vocoder algorithm for different time compressing\nfactors. A survey is conducted amongst a cohort of non-EEG experts to\nquantitatively and qualitatively examine the performance of sound-based methods\nin comparison with the visual interpretation. It is shown that both\nsonification methods perform similarly well, with a smaller inter-observer\nvariability in comparison with visual. A post-survey analysis of results is\nperformed by examining the sensitivity of the ear to frequency evolution in\naudio.", "published": "2018-06-08 09:40:32", "link": "http://arxiv.org/abs/1806.03047v1", "categories": ["q-bio.NC", "cs.SD", "eess.AS", "stat.AP"], "primary_category": "q-bio.NC"}
{"title": "Wave-U-Net: A Multi-Scale Neural Network for End-to-End Audio Source\n  Separation", "abstract": "Models for audio source separation usually operate on the magnitude spectrum,\nwhich ignores phase information and makes separation performance dependant on\nhyper-parameters for the spectral front-end. Therefore, we investigate\nend-to-end source separation in the time-domain, which allows modelling phase\ninformation and avoids fixed spectral transformations. Due to high sampling\nrates for audio, employing a long temporal input context on the sample level is\ndifficult, but required for high quality separation results because of\nlong-range temporal correlations. In this context, we propose the Wave-U-Net,\nan adaptation of the U-Net to the one-dimensional time domain, which repeatedly\nresamples feature maps to compute and combine features at different time\nscales. We introduce further architectural improvements, including an output\nlayer that enforces source additivity, an upsampling technique and a\ncontext-aware prediction framework to reduce output artifacts. Experiments for\nsinging voice separation indicate that our architecture yields a performance\ncomparable to a state-of-the-art spectrogram-based U-Net architecture, given\nthe same data. Finally, we reveal a problem with outliers in the currently used\nSDR evaluation metrics and suggest reporting rank-based statistics to alleviate\nthis problem.", "published": "2018-06-08 14:29:08", "link": "http://arxiv.org/abs/1806.03185v1", "categories": ["cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
