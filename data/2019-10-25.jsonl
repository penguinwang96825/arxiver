{"title": "QASC: A Dataset for Question Answering via Sentence Composition", "abstract": "Composing knowledge from multiple pieces of texts is a key challenge in\nmulti-hop question answering. We present a multi-hop reasoning dataset,\nQuestion Answering via Sentence Composition(QASC), that requires retrieving\nfacts from a large corpus and composing them to answer a multiple-choice\nquestion. QASC is the first dataset to offer two desirable properties: (a) the\nfacts to be composed are annotated in a large corpus, and (b) the decomposition\ninto these facts is not evident from the question itself. The latter makes\nretrieval challenging as the system must introduce new concepts or relations in\norder to discover potential decompositions. Further, the reasoning model must\nthen learn to identify valid compositions of these retrieved facts using\ncommon-sense reasoning. To help address these challenges, we provide annotation\nfor supporting facts as well as their composition. Guided by these annotations,\nwe present a two-step approach to mitigate the retrieval challenges. We use\nother multiple-choice datasets as additional training data to strengthen the\nreasoning model. Our proposed approach improves over current state-of-the-art\nlanguage models by 11% (absolute). The reasoning and retrieval problems,\nhowever, remain unsolved as this model still lags by 20% behind human\nperformance.", "published": "2019-10-25 01:02:18", "link": "http://arxiv.org/abs/1910.11473v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Unified MRC Framework for Named Entity Recognition", "abstract": "The task of named entity recognition (NER) is normally divided into nested\nNER and flat NER depending on whether named entities are nested or not. Models\nare usually separately developed for the two tasks, since sequence labeling\nmodels, the most widely used backbone for flat NER, are only able to assign a\nsingle label to a particular token, which is unsuitable for nested NER where a\ntoken may be assigned several labels.\n  In this paper, we propose a unified framework that is capable of handling\nboth flat and nested NER tasks. Instead of treating the task of NER as a\nsequence labeling problem, we propose to formulate it as a machine reading\ncomprehension (MRC) task. For example, extracting entities with the\n\\textsc{per} label is formalized as extracting answer spans to the question\n\"{\\it which person is mentioned in the text?}\". This formulation naturally\ntackles the entity overlapping issue in nested NER: the extraction of two\noverlapping entities for different categories requires answering two\nindependent questions. Additionally, since the query encodes informative prior\nknowledge, this strategy facilitates the process of entity extraction, leading\nto better performances for not only nested NER, but flat NER.\n  We conduct experiments on both {\\em nested} and {\\em flat} NER datasets.\nExperimental results demonstrate the effectiveness of the proposed formulation.\nWe are able to achieve vast amount of performance boost over current SOTA\nmodels on nested NER datasets, i.e., +1.28, +2.55, +5.44, +6.37, respectively\non ACE04, ACE05, GENIA and KBP17, along with SOTA results on flat NER datasets,\ni.e.,+0.24, +1.95, +0.21, +1.49 respectively on English CoNLL 2003, English\nOntoNotes 5.0, Chinese MSRA, Chinese OntoNotes 4.0.", "published": "2019-10-25 01:06:07", "link": "http://arxiv.org/abs/1910.11476v7", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating a Common Question from Multiple Documents using Multi-source\n  Encoder-Decoder Models", "abstract": "Ambiguous user queries in search engines result in the retrieval of documents\nthat often span multiple topics. One potential solution is for the search\nengine to generate multiple refined queries, each of which relates to a subset\nof the documents spanning the same topic. A preliminary step towards this goal\nis to generate a question that captures common concepts of multiple documents.\nWe propose a new task of generating common question from multiple documents and\npresent simple variant of an existing multi-source encoder-decoder framework,\ncalled the Multi-Source Question Generator (MSQG). We first train an RNN-based\nsingle encoder-decoder generator from (single document, question) pairs. At\ntest time, given multiple documents, the 'Distribute' step of our MSQG model\npredicts target word distributions for each document using the trained model.\nThe 'Aggregate' step aggregates these distributions to generate a common\nquestion. This simple yet effective strategy significantly outperforms several\nexisting baseline models applied to the new task when evaluated using automated\nmetrics and human judgments on the MS-MARCO-QA dataset.", "published": "2019-10-25 01:35:14", "link": "http://arxiv.org/abs/1910.11483v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Attention Optimization for Abstractive Document Summarization", "abstract": "Attention plays a key role in the improvement of sequence-to-sequence-based\ndocument summarization models. To obtain a powerful attention helping with\nreproducing the most salient information and avoiding repetitions, we augment\nthe vanilla attention model from both local and global aspects. We propose an\nattention refinement unit paired with local variance loss to impose supervision\non the attention model at each decoding step, and a global variance loss to\noptimize the attention distributions of all decoding steps from the global\nperspective. The performances on the CNN/Daily Mail dataset verify the\neffectiveness of our methods.", "published": "2019-10-25 02:14:17", "link": "http://arxiv.org/abs/1910.11491v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The SIGMORPHON 2019 Shared Task: Morphological Analysis in Context and\n  Cross-Lingual Transfer for Inflection", "abstract": "The SIGMORPHON 2019 shared task on cross-lingual transfer and contextual\nanalysis in morphology examined transfer learning of inflection between 100\nlanguage pairs, as well as contextual lemmatization and morphosyntactic\ndescription in 66 languages. The first task evolves past years' inflection\ntasks by examining transfer of morphological inflection knowledge from a\nhigh-resource language to a low-resource language. This year also presents a\nnew second challenge on lemmatization and morphological feature analysis in\ncontext. All submissions featured a neural component and built on either this\nyear's strong baselines or highly ranked systems from previous years' shared\ntasks. Every participating team improved in accuracy over the baselines for the\ninflection task (though not Levenshtein distance), and every team in the\ncontextual analysis task improved on both state-of-the-art neural and\nnon-neural baselines.", "published": "2019-10-25 02:20:06", "link": "http://arxiv.org/abs/1910.11493v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Stem-driven Language Models for Morphologically Rich Languages", "abstract": "Neural language models (LMs) have shown to benefit significantly from\nenhancing word vectors with subword-level information, especially for\nmorphologically rich languages. This has been mainly tackled by providing\nsubword-level information as an input; using subword units in the output layer\nhas been far less explored. In this work, we propose LMs that are cognizant of\nthe underlying stems in each word. We derive stems for words using a simple\nunsupervised technique for stem identification. We experiment with different\narchitectures involving multi-task learning and mixture models over words and\nstems. We focus on four morphologically complex languages -- Hindi, Tamil,\nKannada and Finnish -- and observe significant perplexity gains with using our\nstem-driven LMs when compared with other competitive baseline models.", "published": "2019-10-25 05:35:36", "link": "http://arxiv.org/abs/1910.11536v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DENS: A Dataset for Multi-class Emotion Analysis", "abstract": "We introduce a new dataset for multi-class emotion analysis from long-form\nnarratives in English. The Dataset for Emotions of Narrative Sequences (DENS)\nwas collected from both classic literature available on Project Gutenberg and\nmodern online narratives available on Wattpad, annotated using Amazon\nMechanical Turk. A number of statistics and baseline benchmarks are provided\nfor the dataset. Of the tested techniques, we find that the fine-tuning of a\npre-trained BERT model achieves the best results, with an average micro-F1\nscore of 60.4%. Our results show that the dataset provides a novel opportunity\nin emotion analysis that requires moving beyond existing sentence-level\ntechniques.", "published": "2019-10-25 14:40:14", "link": "http://arxiv.org/abs/1910.11769v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Measuring Conversational Fluidity in Automated Dialogue Agents", "abstract": "We present an automated evaluation method to measure fluidity in\nconversational dialogue systems. The method combines various state of the art\nNatural Language tools into a classifier, and human ratings on these dialogues\nto train an automated judgment model. Our experiments show that the results are\nan improvement on existing metrics for measuring fluidity.", "published": "2019-10-25 15:15:44", "link": "http://arxiv.org/abs/1910.11790v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluation of Sentence Representations in Polish", "abstract": "Methods for learning sentence representations have been actively developed in\nrecent years. However, the lack of pre-trained models and datasets annotated at\nthe sentence level has been a problem for low-resource languages such as Polish\nwhich led to less interest in applying these methods to language-specific\ntasks. In this study, we introduce two new Polish datasets for evaluating\nsentence embeddings and provide a comprehensive evaluation of eight sentence\nrepresentation methods including Polish and multilingual models. We consider\nclassic word embedding models, recently developed contextual embeddings and\nmultilingual sentence encoders, showing strengths and weaknesses of specific\napproaches. We also examine different methods of aggregating word vectors into\na single sentence vector.", "published": "2019-10-25 16:35:47", "link": "http://arxiv.org/abs/1910.11834v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Author Context for Detecting Intended vs Perceived Sarcasm", "abstract": "We investigate the impact of using author context on textual sarcasm\ndetection. We define author context as the embedded representation of their\nhistorical posts on Twitter and suggest neural models that extract these\nrepresentations. We experiment with two tweet datasets, one labelled manually\nfor sarcasm, and the other via tag-based distant supervision. We achieve\nstate-of-the-art performance on the second dataset, but not on the one labelled\nmanually, indicating a difference between intended sarcasm, captured by distant\nsupervision, and perceived sarcasm, captured by manual labelling.", "published": "2019-10-25 20:59:09", "link": "http://arxiv.org/abs/1910.11932v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FineText: Text Classification via Attention-based Language Model\n  Fine-tuning", "abstract": "Training deep neural networks from scratch on natural language processing\n(NLP) tasks requires significant amount of manually labeled text corpus and\nsubstantial time to converge, which usually cannot be satisfied by the\ncustomers. In this paper, we aim to develop an effective transfer learning\nalgorithm by fine-tuning a pre-trained language model. The goal is to provide\nexpressive and convenient-to-use feature extractors for downstream NLP tasks,\nand achieve improvement in terms of accuracy, data efficiency, and\ngeneralization to new domains. Therefore, we propose an attention-based\nfine-tuning algorithm that automatically selects relevant contextualized\nfeatures from the pre-trained language model and uses those features on\ndownstream text classification tasks. We test our methods on six widely-used\nbenchmarking datasets, and achieve new state-of-the-art performance on all of\nthem. Moreover, we then introduce an alternative multi-task learning approach,\nwhich is an end-to-end algorithm given the pre-trained model. By doing\nmulti-task learning, one can largely reduce the total training time by trading\noff some classification accuracy.", "published": "2019-10-25 23:13:15", "link": "http://arxiv.org/abs/1910.11959v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey on Recent Advances in Named Entity Recognition from Deep\n  Learning models", "abstract": "Named Entity Recognition (NER) is a key component in NLP systems for question\nanswering, information retrieval, relation extraction, etc. NER systems have\nbeen studied and developed widely for decades, but accurate systems using deep\nneural networks (NN) have only been introduced in the last few years. We\npresent a comprehensive survey of deep neural network architectures for NER,\nand contrast them with previous approaches to NER based on feature engineering\nand other supervised or semi-supervised learning algorithms. Our results\nhighlight the improvements achieved by neural networks, and show how\nincorporating some of the lessons learned from past work on feature-based NER\nsystems can yield further improvements.", "published": "2019-10-25 00:45:48", "link": "http://arxiv.org/abs/1910.11470v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Is it a Fruit, an Apple or a Granny Smith? Predicting the Basic Level in\n  a Concept Hierarchy", "abstract": "The \"basic level\", according to experiments in cognitive psychology, is the\nlevel of abstraction in a hierarchy of concepts at which humans perform tasks\nquicker and with greater accuracy than at other levels. We argue that\napplications that use concept hierarchies - such as knowledge graphs,\nontologies or taxonomies - could significantly improve their user interfaces if\nthey `knew' which concepts are the basic level concepts. This paper examines to\nwhat extent the basic level can be learned from data. We test the utility of\nthree types of concept features, that were inspired by the basic level theory:\nlexical features, structural features and frequency features. We evaluate our\napproach on WordNet, and create a training set of manually labelled examples\nthat includes concepts from different domains. Our findings include that the\nbasic level concepts can be accurately identified within one domain. Concepts\nthat are difficult to label for humans are also harder to classify\nautomatically. Our experiments provide insight into how classification\nperformance across domains could be improved, which is necessary for\nidentification of basic level concepts on a larger scale.", "published": "2019-10-25 12:26:32", "link": "http://arxiv.org/abs/1910.12619v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Machine Translation from Natural Language to Code using Long-Short Term\n  Memory", "abstract": "Making computer programming language more understandable and easy for the\nhuman is a longstanding problem. From assembly language to present day's\nobject-oriented programming, concepts came to make programming easier so that a\nprogrammer can focus on the logic and the architecture rather than the code and\nlanguage itself. To go a step further in this journey of removing\nhuman-computer language barrier, this paper proposes machine learning approach\nusing Recurrent Neural Network (RNN) and Long-Short Term Memory (LSTM) to\nconvert human language into programming language code. The programmer will\nwrite expressions for codes in layman's language, and the machine learning\nmodel will translate it to the targeted programming language. The proposed\napproach yields result with 74.40% accuracy. This can be further improved by\nincorporating additional techniques, which are also discussed in this paper.", "published": "2019-10-25 00:46:07", "link": "http://arxiv.org/abs/1910.11471v1", "categories": ["cs.CL", "cs.AI", "cs.PL", "D.3.4; I.2.2"], "primary_category": "cs.CL"}
{"title": "KRED: Knowledge-Aware Document Representation for News Recommendations", "abstract": "News articles usually contain knowledge entities such as celebrities or\norganizations. Important entities in articles carry key messages and help to\nunderstand the content in a more direct way. An industrial news recommender\nsystem contains various key applications, such as personalized recommendation,\nitem-to-item recommendation, news category classification, news popularity\nprediction and local news detection. We find that incorporating knowledge\nentities for better document understanding benefits these applications\nconsistently. However, existing document understanding models either represent\nnews articles without considering knowledge entities (e.g., BERT) or rely on a\nspecific type of text encoding model (e.g., DKN) so that the generalization\nability and efficiency is compromised. In this paper, we propose KRED, which is\na fast and effective model to enhance arbitrary document representation with a\nknowledge graph. KRED first enriches entities' embeddings by attentively\naggregating information from their neighborhood in the knowledge graph. Then a\ncontext embedding layer is applied to annotate the dynamic context of different\nentities such as frequency, category and position. Finally, an information\ndistillation layer aggregates the entity embeddings under the guidance of the\noriginal document representation and transforms the document vector into a new\none. We advocate to optimize the model with a multi-task framework, so that\ndifferent news recommendation applications can be united and useful information\ncan be shared across different tasks. Experiments on a real-world Microsoft\nNews dataset demonstrate that KRED greatly benefits a variety of news\nrecommendation applications.", "published": "2019-10-25 02:21:33", "link": "http://arxiv.org/abs/1910.11494v3", "categories": ["cs.IR", "cs.CL", "cs.SI"], "primary_category": "cs.IR"}
{"title": "L2RS: A Learning-to-Rescore Mechanism for Automatic Speech Recognition", "abstract": "Modern Automatic Speech Recognition (ASR) systems primarily rely on scores\nfrom an Acoustic Model (AM) and a Language Model (LM) to rescore the N-best\nlists. With the abundance of recent natural language processing advances, the\ninformation utilized by current ASR for evaluating the linguistic and semantic\nlegitimacy of the N-best hypotheses is rather limited. In this paper, we\npropose a novel Learning-to-Rescore (L2RS) mechanism, which is specialized for\nutilizing a wide range of textual information from the state-of-the-art NLP\nmodels and automatically deciding their weights to rescore the N-best lists for\nASR systems. Specifically, we incorporate features including BERT sentence\nembedding, topic vector, and perplexity scores produced by n-gram LM, topic\nmodeling LM, BERT LM and RNNLM to train a rescoring model. We conduct extensive\nexperiments based on a public dataset, and experimental results show that L2RS\noutperforms not only traditional rescoring methods but also its deep neural\nnetwork counterparts by a substantial improvement of 20.67% in terms of\nNDCG@10. L2RS paves the way for developing more effective rescoring models for\nASR.", "published": "2019-10-25 02:25:34", "link": "http://arxiv.org/abs/1910.11496v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Fast Structured Decoding for Sequence Models", "abstract": "Autoregressive sequence models achieve state-of-the-art performance in\ndomains like machine translation. However, due to the autoregressive\nfactorization nature, these models suffer from heavy latency during inference.\nRecently, non-autoregressive sequence models were proposed to reduce the\ninference time. However, these models assume that the decoding process of each\ntoken is conditionally independent of others. Such a generation process\nsometimes makes the output sentence inconsistent, and thus the learned\nnon-autoregressive models could only achieve inferior accuracy compared to\ntheir autoregressive counterparts. To improve then decoding consistency and\nreduce the inference cost at the same time, we propose to incorporate a\nstructured inference module into the non-autoregressive models. Specifically,\nwe design an efficient approximation for Conditional Random Fields (CRF) for\nnon-autoregressive sequence models, and further propose a dynamic transition\ntechnique to model positional contexts in the CRF. Experiments in machine\ntranslation show that while increasing little latency (8~14ms), our model could\nachieve significantly better translation performance than previous\nnon-autoregressive models on different translation datasets. In particular, for\nthe WMT14 En-De dataset, our model obtains a BLEU score of 26.80, which largely\noutperforms the previous non-autoregressive baselines and is only 0.61 lower in\nBLEU than purely autoregressive models.", "published": "2019-10-25 07:32:52", "link": "http://arxiv.org/abs/1910.11555v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "SpeechBERT: An Audio-and-text Jointly Learned Language Model for\n  End-to-end Spoken Question Answering", "abstract": "While various end-to-end models for spoken language understanding tasks have\nbeen explored recently, this paper is probably the first known attempt to\nchallenge the very difficult task of end-to-end spoken question answering\n(SQA). Learning from the very successful BERT model for various text processing\ntasks, here we proposed an audio-and-text jointly learned SpeechBERT model.\nThis model outperformed the conventional approach of cascading ASR with the\nfollowing text question answering (TQA) model on datasets including ASR errors\nin answer spans, because the end-to-end model was shown to be able to extract\ninformation out of audio data before ASR produced errors. When ensembling the\nproposed end-to-end model with the cascade architecture, even better\nperformance was achieved. In addition to the potential of end-to-end SQA, the\nSpeechBERT can also be considered for many other spoken language understanding\ntasks just as BERT for many text processing tasks.", "published": "2019-10-25 07:46:39", "link": "http://arxiv.org/abs/1910.11559v4", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Meta-Learning with Dynamic-Memory-Based Prototypical Network for\n  Few-Shot Event Detection", "abstract": "Event detection (ED), a sub-task of event extraction, involves identifying\ntriggers and categorizing event mentions. Existing methods primarily rely upon\nsupervised learning and require large-scale labeled event datasets which are\nunfortunately not readily available in many real-life applications. In this\npaper, we consider and reformulate the ED task with limited labeled data as a\nFew-Shot Learning problem. We propose a Dynamic-Memory-Based Prototypical\nNetwork (DMB-PN), which exploits Dynamic Memory Network (DMN) to not only learn\nbetter prototypes for event types, but also produce more robust sentence\nencodings for event mentions. Differing from vanilla prototypical networks\nsimply computing event prototypes by averaging, which only consume event\nmentions once, our model is more robust and is capable of distilling contextual\ninformation from event mentions for multiple times due to the multi-hop\nmechanism of DMNs. The experiments show that DMB-PN not only deals with sample\nscarcity better than a series of baseline models but also performs more\nrobustly when the variety of event types is relatively large and the instance\nquantity is extremely small.", "published": "2019-10-25 11:14:33", "link": "http://arxiv.org/abs/1910.11621v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploring Multilingual Syntactic Sentence Representations", "abstract": "We study methods for learning sentence embeddings with syntactic structure.\nWe focus on methods of learning syntactic sentence-embeddings by using a\nmultilingual parallel-corpus augmented by Universal Parts-of-Speech tags. We\nevaluate the quality of the learned embeddings by examining sentence-level\nnearest neighbours and functional dissimilarity in the embedding space. We also\nevaluate the ability of the method to learn syntactic sentence-embeddings for\nlow-resource languages and demonstrate strong evidence for transfer learning.\nOur results show that syntactic sentence-embeddings can be learned while using\nless training data, fewer model parameters, and resulting in better evaluation\nmetrics than state-of-the-art language models.", "published": "2019-10-25 14:38:18", "link": "http://arxiv.org/abs/1910.11768v1", "categories": ["cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "On the Cross-lingual Transferability of Monolingual Representations", "abstract": "State-of-the-art unsupervised multilingual models (e.g., multilingual BERT)\nhave been shown to generalize in a zero-shot cross-lingual setting. This\ngeneralization ability has been attributed to the use of a shared subword\nvocabulary and joint training across multiple languages giving rise to deep\nmultilingual abstractions. We evaluate this hypothesis by designing an\nalternative approach that transfers a monolingual model to new languages at the\nlexical level. More concretely, we first train a transformer-based masked\nlanguage model on one language, and transfer it to a new language by learning a\nnew embedding matrix with the same masked language modeling objective, freezing\nparameters of all other layers. This approach does not rely on a shared\nvocabulary or joint training. However, we show that it is competitive with\nmultilingual BERT on standard cross-lingual classification benchmarks and on a\nnew Cross-lingual Question Answering Dataset (XQuAD). Our results contradict\ncommon beliefs of the basis of the generalization ability of multilingual\nmodels and suggest that deep monolingual models learn some abstractions that\ngeneralize across languages. We also release XQuAD as a more comprehensive\ncross-lingual benchmark, which comprises 240 paragraphs and 1190\nquestion-answer pairs from SQuAD v1.1 translated into ten languages by\nprofessional translators.", "published": "2019-10-25 17:30:20", "link": "http://arxiv.org/abs/1910.11856v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Online End-to-end Transformer Automatic Speech Recognition", "abstract": "The Transformer self-attention network has recently shown promising\nperformance as an alternative to recurrent neural networks in end-to-end (E2E)\nautomatic speech recognition (ASR) systems. However, Transformer has a drawback\nin that the entire input sequence is required to compute self-attention. We\nhave proposed a block processing method for the Transformer encoder by\nintroducing a context-aware inheritance mechanism. An additional context\nembedding vector handed over from the previously processed block helps to\nencode not only local acoustic information but also global linguistic, channel,\nand speaker attributes. In this paper, we extend it towards an entire online\nE2E ASR system by introducing an online decoding process inspired by monotonic\nchunkwise attention (MoChA) into the Transformer decoder. Our novel MoChA\ntraining and inference algorithms exploit the unique properties of Transformer,\nwhose attentions are not always monotonic or peaky, and have multiple heads and\nresidual connections of the decoder layers. Evaluations of the Wall Street\nJournal (WSJ) and AISHELL-1 show that our proposed online Transformer decoder\noutperforms conventional chunkwise approaches.", "published": "2019-10-25 05:28:17", "link": "http://arxiv.org/abs/1910.11871v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Current Limitations in Cyberbullying Detection: on Evaluation Criteria,\n  Reproducibility, and Data Scarcity", "abstract": "The detection of online cyberbullying has seen an increase in societal\nimportance, popularity in research, and available open data. Nevertheless,\nwhile computational power and affordability of resources continue to increase,\nthe access restrictions on high-quality data limit the applicability of\nstate-of-the-art techniques. Consequently, much of the recent research uses\nsmall, heterogeneous datasets, without a thorough evaluation of applicability.\nIn this paper, we further illustrate these issues, as we (i) evaluate many\npublicly available resources for this task and demonstrate difficulties with\ndata collection. These predominantly yield small datasets that fail to capture\nthe required complex social dynamics and impede direct comparison of progress.\nWe (ii) conduct an extensive set of experiments that indicate a general lack of\ncross-domain generalization of classifiers trained on these sources, and openly\nprovide this framework to replicate and extend our evaluation criteria.\nFinally, we (iii) present an effective crowdsourcing method: simulating\nreal-life bullying scenarios in a lab setting generates plausible data that can\nbe effectively used to enrich real data. This largely circumvents the\nrestrictions on data that can be collected, and increases classifier\nperformance. We believe these contributions can aid in improving the empirical\npractices of future research in the field.", "published": "2019-10-25 20:15:38", "link": "http://arxiv.org/abs/1910.11922v1", "categories": ["cs.CL", "cs.CY", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Automatic Reminiscence Therapy for Dementia", "abstract": "With people living longer than ever, the number of cases with dementia such\nas Alzheimer's disease increases steadily. It affects more than 46 million\npeople worldwide, and it is estimated that in 2050 more than 100 million will\nbe affected. While there are not effective treatments for these terminal\ndiseases, therapies such as reminiscence, that stimulate memories from the past\nare recommended. Currently, reminiscence therapy takes place in care homes and\nis guided by a therapist or a carer. In this work, we present an AI-based\nsolution to automatize the reminiscence therapy, which consists in a dialogue\nsystem that uses photos as input to generate questions. We run a usability case\nstudy with patients diagnosed of mild cognitive impairment that shows they\nfound the system very entertaining and challenging. Overall, this paper\npresents how reminiscence therapy can be automatized by using machine learning,\nand deployed to smartphones and laptops, making the therapy more accessible to\nevery person affected by dementia.", "published": "2019-10-25 21:47:52", "link": "http://arxiv.org/abs/1910.11949v2", "categories": ["cs.MM", "cs.CL", "cs.CV"], "primary_category": "cs.MM"}
{"title": "Textual Data for Time Series Forecasting", "abstract": "While ubiquitous, textual sources of information such as company reports,\nsocial media posts, etc. are hardly included in prediction algorithms for time\nseries, despite the relevant information they may contain. In this work, openly\naccessible daily weather reports from France and the United-Kingdom are\nleveraged to predict time series of national electricity consumption, average\ntemperature and wind-speed with a single pipeline. Two methods of numerical\nrepresentation of text are considered, namely traditional Term Frequency -\nInverse Document Frequency (TF-IDF) as well as our own neural word embedding.\nUsing exclusively text, we are able to predict the aforementioned time series\nwith sufficient accuracy to be used to replace missing data. Furthermore the\nproposed word embeddings display geometric properties relating to the behavior\nof the time series and context similarity between words.", "published": "2019-10-25 07:47:56", "link": "http://arxiv.org/abs/1910.12618v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Mockingjay: Unsupervised Speech Representation Learning with Deep\n  Bidirectional Transformer Encoders", "abstract": "We present Mockingjay as a new speech representation learning approach, where\nbidirectional Transformer encoders are pre-trained on a large amount of\nunlabeled speech. Previous speech representation methods learn through\nconditioning on past frames and predicting information about future frames.\nWhereas Mockingjay is designed to predict the current frame through jointly\nconditioning on both past and future contexts. The Mockingjay representation\nimproves performance for a wide range of downstream tasks, including phoneme\nclassification, speaker recognition, and sentiment classification on spoken\ncontent, while outperforming other approaches. Mockingjay is empirically\npowerful and can be fine-tuned with downstream models, with only 2 epochs we\nfurther improve performance dramatically. In a low resource setting with only\n0.1% of labeled data, we outperform the result of Mel-features that uses all\n100% labeled data.", "published": "2019-10-25 01:55:12", "link": "http://arxiv.org/abs/1910.12638v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "HUBERT Untangles BERT to Improve Transfer across NLP Tasks", "abstract": "We introduce HUBERT which combines the structured-representational power of\nTensor-Product Representations (TPRs) and BERT, a pre-trained bidirectional\nTransformer language model. We show that there is shared structure between\ndifferent NLP datasets that HUBERT, but not BERT, is able to learn and\nleverage. We validate the effectiveness of our model on the GLUE benchmark and\nHANS dataset. Our experiment results show that untangling data-specific\nsemantics from general language structure is key for better transfer among NLP\ntasks.", "published": "2019-10-25 06:25:25", "link": "http://arxiv.org/abs/1910.12647v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Structural sparsification for Far-field Speaker Recognition with GNA", "abstract": "Recently, deep neural networks (DNN) have been widely used in speaker\nrecognition area. In order to achieve fast response time and high accuracy, the\nrequirements for hardware resources increase rapidly. However, as the speaker\nrecognition application is often implemented on mobile devices, it is necessary\nto maintain a low computational cost while keeping high accuracy in far-field\ncondition. In this paper, we apply structural sparsification on time-delay\nneural networks (TDNN) to remove redundant structures and accelerate the\nexecution. On our targeted hardware, our model can remove 60% of parameters and\nonly slightly increasing equal error rate (EER) by 0.18% while our structural\nsparse model can achieve more than 1.5x speedup.", "published": "2019-10-25 02:02:21", "link": "http://arxiv.org/abs/1910.11488v2", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Exploring Lexicon-Free Modeling Units for End-to-End Korean and\n  Korean-English Code-Switching Speech Recognition", "abstract": "As the character-based end-to-end automatic speech recognition (ASR) models\nevolve, the choice of acoustic modeling units becomes important. Since Korean\nis a fairly phonetic language and has a unique writing system with its own\nKorean alphabet, it's worth investigating modeling units for an end-to-end\nKorean ASR task. In this work, we introduce lexicon-free modeling units in\nKorean, and explore them using a hybrid CTC/Attention-based encoder-decoder\nmodel. Five lexicon-free units are investigated: Syllable-based Korean\ncharacter (with English character for a code-switching task), Korean Jamo\ncharacter (with English character), sub-word on syllable-based character (with\nsub-word in English), sub-word on Jamo character (with sub-words in English),\nand finally byte unit, which is a universal one across language. Experiments on\nZeroth-Korean (51.6 hrs) and Medical Record (2530 hrs) are done for Korean and\nKorean-English code-switching ASR tasks, respectively. Sequence-to-sequence\nlearning with sub-words based on Korean syllables (and sub-words in English)\nperforms the best for both tasks without lexicon and an extra language model\nintegration.", "published": "2019-10-25 09:23:27", "link": "http://arxiv.org/abs/1910.11590v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Overlap-aware diarization: resegmentation using neural end-to-end\n  overlapped speech detection", "abstract": "We address the problem of effectively handling overlapping speech in a\ndiarization system. First, we detail a neural Long Short-Term Memory-based\narchitecture for overlap detection. Secondly, detected overlap regions are\nexploited in conjunction with a frame-level speaker posterior matrix to make\ntwo-speaker assignments for overlapped frames in the resegmentation step. The\noverlap detection module achieves state-of-the-art performance on the AMI,\nDIHARD, and ETAPE corpora. We apply overlap-aware resegmentation on AMI,\nresulting in a 20% relative DER reduction over the baseline system. While this\napproach is by no means an end-all solution to overlap-aware diarization, it\nreveals promising directions for handling overlap.", "published": "2019-10-25 12:22:14", "link": "http://arxiv.org/abs/1910.11646v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Feature Enhancement with Deep Feature Losses for Speaker Verification", "abstract": "Speaker Verification still suffers from the challenge of generalization to\nnovel adverse environments. We leverage on the recent advancements made by deep\nlearning based speech enhancement and propose a feature-domain supervised\ndenoising based solution. We propose to use Deep Feature Loss which optimizes\nthe enhancement network in the hidden activation space of a pre-trained\nauxiliary speaker embedding network. We experimentally verify the approach on\nsimulated and real data. A simulated testing setup is created using various\nnoise types at different SNR levels. For evaluation on real data, we choose\nBabyTrain corpus which consists of children recordings in uncontrolled\nenvironments. We observe consistent gains in every condition over the\nstate-of-the-art augmented Factorized-TDNN x-vector system. On BabyTrain\ncorpus, we observe relative gains of 10.38% and 12.40% in minDCF and EER\nrespectively.", "published": "2019-10-25 19:18:27", "link": "http://arxiv.org/abs/1910.11905v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Low-Resource Domain Adaptation for Speaker Recognition Using Cycle-GANs", "abstract": "Current speaker recognition technology provides great performance with the\nx-vector approach. However, performance decreases when the evaluation domain is\ndifferent from the training domain, an issue usually addressed with domain\nadaptation approaches. Recently, unsupervised domain adaptation using\ncycle-consistent Generative Adversarial Netorks (CycleGAN) has received a lot\nof attention. CycleGAN learn mappings between features of two domains given\nnon-parallel data. We investigate their effectiveness in low resource scenario\ni.e. when limited amount of target domain data is available for adaptation, a\ncase unexplored in previous works. We experiment with two adaptation tasks:\nmicrophone to telephone and a novel reverberant to clean adaptation with the\nend goal of improving speaker recognition performance. Number of speakers\npresent in source and target domains are 7000 and 191 respectively. By adding\nnoise to the target domain during CycleGAN training, we were able to achieve\nbetter performance compared to the adaptation system whose CycleGAN was trained\non a larger target data. On reverberant to clean adaptation task, our models\nimproved EER by 18.3% relative on VOiCES dataset compared to a system trained\non clean data. They also slightly improved over the state-of-the-art Weighted\nPrediction Error (WPE) de-reverberation algorithm.", "published": "2019-10-25 19:36:07", "link": "http://arxiv.org/abs/1910.11909v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Unsupervised Feature Enhancement for speaker verification", "abstract": "The task of making speaker verification systems robust to adverse scenarios\nremain a challenging and an active area of research. We developed an\nunsupervised feature enhancement approach in log-filter bank domain with the\nend goal of improving speaker verification performance. We experimented with\nusing both real speech recorded in adverse environments and degraded speech\nobtained by simulation to train the enhancement systems. The effectiveness of\nthe approach was shown by testing on several real, simulated noisy, and\nreverberant test sets. The approach yielded significant improvements on both\nreal and simulated sets when data augmentation was not used in speaker\nverification pipeline or augmentation was used only during x-vector training.\nWhen data augmentation was used for x-vector and PLDA training, our enhancement\napproach yielded slight improvements.", "published": "2019-10-25 19:47:53", "link": "http://arxiv.org/abs/1910.11915v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Learning Domain Invariant Representations for Child-Adult Classification\n  from Speech", "abstract": "Diagnostic procedures for ASD (autism spectrum disorder) involve\nsemi-naturalistic interactions between the child and a clinician. Computational\nmethods to analyze these sessions require an end-to-end speech and language\nprocessing pipeline that go from raw audio to clinically-meaningful behavioral\nfeatures. An important component of this pipeline is the ability to\nautomatically detect who is speaking when i.e., perform child-adult speaker\nclassification. This binary classification task is often confounded due to\nvariability associated with the participants' speech and background conditions.\nFurther, scarcity of training data often restricts direct application of\nconventional deep learning methods. In this work, we address two major sources\nof variability - age of the child and data source collection location - using\ndomain adversarial learning which does not require labeled target domain data.\nWe use two methods, generative adversarial training with inverted label loss\nand gradient reversal layer to learn speaker embeddings invariant to the above\nsources of variability, and analyze different conditions under which the\nproposed techniques improve over conventional learning methods. Using a large\ncorpus of ADOS-2 (autism diagnostic observation schedule, 2nd edition)\nsessions, we demonstrate upto 13.45% and 6.44% relative improvements over\nconventional learning methods.", "published": "2019-10-25 00:53:25", "link": "http://arxiv.org/abs/1910.11472v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Parallel WaveGAN: A fast waveform generation model based on generative\n  adversarial networks with multi-resolution spectrogram", "abstract": "We propose Parallel WaveGAN, a distillation-free, fast, and small-footprint\nwaveform generation method using a generative adversarial network. In the\nproposed method, a non-autoregressive WaveNet is trained by jointly optimizing\nmulti-resolution spectrogram and adversarial loss functions, which can\neffectively capture the time-frequency distribution of the realistic speech\nwaveform. As our method does not require density distillation used in the\nconventional teacher-student framework, the entire model can be easily trained.\nFurthermore, our model is able to generate high-fidelity speech even with its\ncompact architecture. In particular, the proposed Parallel WaveGAN has only\n1.44 M parameters and can generate 24 kHz speech waveform 28.68 times faster\nthan real-time on a single GPU environment. Perceptual listening test results\nverify that our proposed method achieves 4.16 mean opinion score within a\nTransformer-based text-to-speech framework, which is comparative to the best\ndistillation-based Parallel WaveNet system.", "published": "2019-10-25 01:16:38", "link": "http://arxiv.org/abs/1910.11480v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "A Multi-Phase Gammatone Filterbank for Speech Separation via TasNet", "abstract": "In this work, we investigate if the learned encoder of the end-to-end\nconvolutional time domain audio separation network (Conv-TasNet) is the key to\nits recent success, or if the encoder can just as well be replaced by a\ndeterministic hand-crafted filterbank. Motivated by the resemblance of the\ntrained encoder of Conv-TasNet to auditory filterbanks, we propose to employ a\ndeterministic gammatone filterbank. In contrast to a common gammatone\nfilterbank, our filters are restricted to 2 ms length to allow for low-latency\nprocessing. Inspired by the encoder learned by Conv-TasNet, in addition to the\nlogarithmically spaced filters, the proposed filterbank holds multiple\ngammatone filters at the same center frequency with varying phase shifts. We\nshow that replacing the learned encoder with our proposed multi-phase gammatone\nfilterbank (MP-GTF) even leads to a scale-invariant source-to-noise ratio\n(SI-SNR) improvement of 0.7 dB. Furthermore, in contrast to using the learned\nencoder we show that the number of filters can be reduced from 512 to 128\nwithout loss of performance.", "published": "2019-10-25 10:51:43", "link": "http://arxiv.org/abs/1910.11615v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Channel adversarial training for speaker verification and diarization", "abstract": "Previous work has encouraged domain-invariance in deep speaker embedding by\nadversarially classifying the dataset or labelled environment to which the\ngenerated features belong. We propose a training strategy which aims to produce\nfeatures that are invariant at the granularity of the recording or channel, a\nfiner grained objective than dataset- or environment-invariance. By training an\nadversary to predict whether pairs of same-speaker embeddings belong to the\nsame recording in a Siamese fashion, learned features are discouraged from\nutilizing channel information that may be speaker discriminative during\ntraining. Experiments for verification on VoxCeleb and diarization and\nverification on CALLHOME show promising improvements over a strong baseline in\naddition to outperforming a dataset-adversarial model. The VoxCeleb model in\nparticular performs well, achieving a $4\\%$ relative improvement in EER over a\nKaldi baseline, while using a similar architecture and less training data.", "published": "2019-10-25 12:14:17", "link": "http://arxiv.org/abs/1910.11643v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SPICE: Self-supervised Pitch Estimation", "abstract": "We propose a model to estimate the fundamental frequency in monophonic audio,\noften referred to as pitch estimation. We acknowledge the fact that obtaining\nground truth annotations at the required temporal and frequency resolution is a\nparticularly daunting task. Therefore, we propose to adopt a self-supervised\nlearning technique, which is able to estimate pitch without any form of\nsupervision. The key observation is that pitch shift maps to a simple\ntranslation when the audio signal is analysed through the lens of the\nconstant-Q transform (CQT). We design a self-supervised task by feeding two\nshifted slices of the CQT to the same convolutional encoder, and require that\nthe difference in the outputs is proportional to the corresponding difference\nin pitch. In addition, we introduce a small model head on top of the encoder,\nwhich is able to determine the confidence of the pitch estimate, so as to\ndistinguish between voiced and unvoiced audio. Our results show that the\nproposed method is able to estimate pitch at a level of accuracy comparable to\nfully supervised models, both on clean and noisy audio samples, although it\ndoes not require access to large labeled datasets.", "published": "2019-10-25 12:45:20", "link": "http://arxiv.org/abs/1910.11664v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Self-supervised Moving Vehicle Tracking with Stereo Sound", "abstract": "Humans are able to localize objects in the environment using both visual and\nauditory cues, integrating information from multiple modalities into a common\nreference frame. We introduce a system that can leverage unlabeled audio-visual\ndata to learn to localize objects (moving vehicles) in a visual reference\nframe, purely using stereo sound at inference time. Since it is labor-intensive\nto manually annotate the correspondences between audio and object bounding\nboxes, we achieve this goal by using the co-occurrence of visual and audio\nstreams in unlabeled videos as a form of self-supervision, without resorting to\nthe collection of ground-truth annotations. In particular, we propose a\nframework that consists of a vision \"teacher\" network and a stereo-sound\n\"student\" network. During training, knowledge embodied in a well-established\nvisual vehicle detection model is transferred to the audio domain using\nunlabeled videos as a bridge. At test time, the stereo-sound student network\ncan work independently to perform object localization us-ing just stereo audio\nand camera meta-data, without any visual input. Experimental results on a newly\ncollected Au-ditory Vehicle Tracking dataset verify that our proposed approach\noutperforms several baseline approaches. We also demonstrate that our\ncross-modal auditory localization approach can assist in the visual\nlocalization of moving vehicles under poor lighting conditions.", "published": "2019-10-25 14:28:55", "link": "http://arxiv.org/abs/1910.11760v1", "categories": ["cs.CV", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Secost: Sequential co-supervision for large scale weakly labeled audio\n  event detection", "abstract": "Weakly supervised learning algorithms are critical for scaling audio event\ndetection to several hundreds of sound categories. Such learning models should\nnot only disambiguate sound events efficiently with minimal class-specific\nannotation but also be robust to label noise, which is more apparent with weak\nlabels instead of strong annotations. In this work, we propose a new framework\nfor designing learning models with weak supervision by bridging ideas from\nsequential learning and knowledge distillation. We refer to the proposed\nmethodology as SeCoST (pronounced Sequest) -- Sequential Co-supervision for\ntraining generations of Students. SeCoST incrementally builds a cascade of\nstudent-teacher pairs via a novel knowledge transfer method. Our evaluations on\nAudioset (the largest weakly labeled dataset available) show that SeCoST\nachieves a mean average precision of 0.383 while outperforming prior state of\nthe art by a considerable margin.", "published": "2019-10-25 15:15:30", "link": "http://arxiv.org/abs/1910.11789v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Adaptive blind audio source extraction supervised by dominant speaker\n  identification using x-vectors", "abstract": "We propose a novel algorithm for adaptive blind audio source extraction. The\nproposed method is based on independent vector analysis and utilizes the\nauxiliary function optimization to achieve high convergence speed. The\nalgorithm is partially supervised by a pilot signal related to the source of\ninterest (SOI), which ensures that the method correctly extracts the utterance\nof the desired speaker. The pilot is based on the identification of a dominant\nspeaker in the mixture using x-vectors. The properties of the x-vectors\ncomputed in the presence of cross-talk are experimentally analyzed. The\nproposed approach is verified in a scenario with a moving SOI, static\ninterfering speaker, and environmental noise.", "published": "2019-10-25 16:08:09", "link": "http://arxiv.org/abs/1910.11824v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Learning audio representations via phase prediction", "abstract": "We learn audio representations by solving a novel self-supervised learning\ntask, which consists of predicting the phase of the short-time Fourier\ntransform from its magnitude. A convolutional encoder is used to map the\nmagnitude spectrum of the input waveform to a lower dimensional embedding. A\nconvolutional decoder is then used to predict the instantaneous frequency\n(i.e., the temporal rate of change of the phase) from such embedding. To\nevaluate the quality of the learned representations, we evaluate how they\ntransfer to a wide variety of downstream audio tasks. Our experiments reveal\nthat the phase prediction task leads to representations that generalize across\ndifferent tasks, partially bridging the gap with fully-supervised models. In\naddition, we show that the predicted phase can be used as initialization of the\nGriffin-Lim algorithm, thus reducing the number of iterations needed to\nreconstruct the waveform in the time domain.", "published": "2019-10-25 19:36:09", "link": "http://arxiv.org/abs/1910.11910v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Confidence Estimation for Black Box Automatic Speech Recognition Systems\n  Using Lattice Recurrent Neural Networks", "abstract": "Recently, there has been growth in providers of speech transcription services\nenabling others to leverage technology they would not normally be able to use.\nAs a result, speech-enabled solutions have become commonplace. Their success\ncritically relies on the quality, accuracy, and reliability of the underlying\nspeech transcription systems. Those black box systems, however, offer limited\nmeans for quality control as only word sequences are typically available. This\npaper examines this limited resource scenario for confidence estimation, a\nmeasure commonly used to assess transcription reliability. In particular, it\nexplores what other sources of word and sub-word level information available in\nthe transcription process could be used to improve confidence scores. To encode\nall such information this paper extends lattice recurrent neural networks to\nhandle sub-words. Experimental results using the IARPA OpenKWS 2016 evaluation\nsystem show that the use of additional information yields significant gains in\nconfidence estimation accuracy. The implementation for this model can be found\nonline.", "published": "2019-10-25 21:01:40", "link": "http://arxiv.org/abs/1910.11933v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multi-Reference Neural TTS Stylization with Adversarial Cycle\n  Consistency", "abstract": "Current multi-reference style transfer models for Text-to-Speech (TTS)\nperform sub-optimally on disjoints datasets, where one dataset contains only a\nsingle style class for one of the style dimensions. These models generally fail\nto produce style transfer for the dimension that is underrepresented in the\ndataset. In this paper, we propose an adversarial cycle consistency training\nscheme with paired and unpaired triplets to ensure the use of information from\nall style dimensions. During training, we incorporate unpaired triplets with\nrandomly selected reference audio samples and encourage the synthesized speech\nto preserve the appropriate styles using adversarial cycle consistency. We use\nthis method to transfer emotion from a dataset containing four emotions to a\ndataset with only a single emotion. This results in a 78% improvement in style\ntransfer (based on emotion classification) with minimal reduction in fidelity\nand naturalness. In subjective evaluations our method was consistently rated as\ncloser to the reference style than the baseline. Synthesized speech samples are\navailable at: https://sites.google.com/view/adv-cycle-consistent-tts", "published": "2019-10-25 23:11:27", "link": "http://arxiv.org/abs/1910.11958v1", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
