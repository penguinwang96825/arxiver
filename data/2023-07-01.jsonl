{"title": "How far is Language Model from 100% Few-shot Named Entity Recognition in\n  Medical Domain", "abstract": "Recent advancements in language models (LMs) have led to the emergence of\npowerful models such as Small LMs (e.g., T5) and Large LMs (e.g., GPT-4). These\nmodels have demonstrated exceptional capabilities across a wide range of tasks,\nsuch as name entity recognition (NER) in the general domain. (We define SLMs as\npre-trained models with fewer parameters compared to models like GPT-3/3.5/4,\nsuch as T5, BERT, and others.) Nevertheless, their efficacy in the medical\nsection remains uncertain and the performance of medical NER always needs high\naccuracy because of the particularity of the field. This paper aims to provide\na thorough investigation to compare the performance of LMs in medical few-shot\nNER and answer How far is LMs from 100\\% Few-shot NER in Medical Domain, and\nmoreover to explore an effective entity recognizer to help improve the NER\nperformance. Based on our extensive experiments conducted on 16 NER models\nspanning from 2018 to 2023, our findings clearly indicate that LLMs outperform\nSLMs in few-shot medical NER tasks, given the presence of suitable examples and\nappropriate logical frameworks. Despite the overall superiority of LLMs in\nfew-shot medical NER tasks, it is important to note that they still encounter\nsome challenges, such as misidentification, wrong template prediction, etc.\nBuilding on previous findings, we introduce a simple and effective method\ncalled \\textsc{RT} (Retrieving and Thinking), which serves as retrievers,\nfinding relevant examples, and as thinkers, employing a step-by-step reasoning\nprocess. Experimental results show that our proposed \\textsc{RT} framework\nsignificantly outperforms the strong open baselines on the two open medical\nbenchmark datasets", "published": "2023-07-01 01:18:09", "link": "http://arxiv.org/abs/2307.00186v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Let Me Teach You: Pedagogical Foundations of Feedback for Language\n  Models", "abstract": "Natural Language Feedback (NLF) is an increasingly popular mechanism for\naligning Large Language Models (LLMs) to human preferences. Despite the\ndiversity of the information it can convey, NLF methods are often hand-designed\nand arbitrary, with little systematic grounding. At the same time, research in\nlearning sciences has long established several effective feedback models. In\nthis opinion piece, we compile ideas from pedagogy to introduce FELT, a\nfeedback framework for LLMs that outlines various characteristics of the\nfeedback space, and a feedback content taxonomy based on these variables,\nproviding a general mapping of the feedback space. In addition to streamlining\nNLF designs, FELT also brings out new, unexplored directions for research in\nNLF. We make our taxonomy available to the community, providing guides and\nexamples for mapping our categorizations to future research.", "published": "2023-07-01 09:18:24", "link": "http://arxiv.org/abs/2307.00279v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BatGPT: A Bidirectional Autoregessive Talker from Generative Pre-trained\n  Transformer", "abstract": "BatGPT is a large-scale language model designed and trained jointly by Wuhan\nUniversity and Shanghai Jiao Tong University. It is capable of generating\nhighly natural and fluent text in response to various types of input, including\ntext prompts, images, and audio. In the modeling level, we employ a\nbidirectional autoregressive architecture that allows the model to efficiently\ncapture the complex dependencies of natural language, making it highly\neffective in tasks such as language generation, dialog systems, and question\nanswering. Moreover, the bidirectional autoregressive modeling not only\noperates from left to right but also from right to left, effectively reducing\nfixed memory effects and alleviating model hallucinations.\n  In the training aspect, we propose a novel parameter expansion method for\nleveraging the pre-training of smaller models and employ reinforcement learning\nfrom both AI and human feedback, aimed at improving the model's alignment\nperformance. Overall, these approaches significantly improve the effectiveness\nof BatGPT, and the model can be utilized for a wide range of natural language\napplications.", "published": "2023-07-01 15:10:01", "link": "http://arxiv.org/abs/2307.00360v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting Sample Size Determination in Natural Language Understanding", "abstract": "Knowing exactly how many data points need to be labeled to achieve a certain\nmodel performance is a hugely beneficial step towards reducing the overall\nbudgets for annotation. It pertains to both active learning and traditional\ndata annotation, and is particularly beneficial for low resource scenarios.\nNevertheless, it remains a largely under-explored area of research in NLP. We\ntherefore explored various techniques for estimating the training sample size\nnecessary to achieve a targeted performance value. We derived a simple yet\neffective approach to predict the maximum achievable model performance based on\nsmall amount of training samples - which serves as an early indicator during\ndata annotation for data quality and sample size determination. We performed\nablation studies on four language understanding tasks, and showed that the\nproposed approach allows us to forecast model performance within a small margin\nof mean absolute error (~ 0.9%) with only 10% data.", "published": "2023-07-01 16:08:52", "link": "http://arxiv.org/abs/2307.00374v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Low-Resource Cross-Lingual Adaptive Training for Nigerian Pidgin", "abstract": "Developing effective spoken language processing systems for low-resource\nlanguages poses several challenges due to the lack of parallel data and limited\nresources for fine-tuning models. In this work, we target on improving upon\nboth text classification and translation of Nigerian Pidgin (Naija) by\ncollecting a large-scale parallel English-Pidgin corpus and further propose a\nframework of cross-lingual adaptive training that includes both continual and\ntask adaptive training so as to adapt a base pre-trained model to low-resource\nlanguages. Our studies show that English pre-trained language models serve as a\nstronger prior than multilingual language models on English-Pidgin tasks with\nup to 2.38 BLEU improvements; and demonstrate that augmenting orthographic data\nand using task adaptive training with back-translation can have a significant\nimpact on model performance.", "published": "2023-07-01 16:47:36", "link": "http://arxiv.org/abs/2307.00382v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Discovering Patterns of Definitions and Methods from Scientific\n  Documents", "abstract": "The difficulties of automatic extraction of definitions and methods from\nscientific documents lie in two aspects: (1) the complexity and diversity of\nnatural language texts, which requests an analysis method to support the\ndiscovery of pattern; and, (2) a complete definition or method represented by a\nscientific paper is usually distributed within text, therefore an effective\napproach should not only extract single sentence definitions and methods but\nalso integrate the sentences to obtain a complete definition or method. This\npaper proposes an analysis method for discovering patterns of definition and\nmethod and uses the method to discover patterns of definition and method.\nCompleteness of the patterns at the semantic level is guaranteed by a complete\nset of semantic relations that identify definitions and methods respectively.\nThe completeness of the patterns at the syntactic and lexical levels is\nguaranteed by syntactic and lexical constraints. Experiments on the self-built\ndataset and two public definition datasets show that the discovered patterns\nare effective. The patterns can be used to extract definitions and methods from\nscientific documents and can be tailored or extended to suit other\napplications.", "published": "2023-07-01 05:08:44", "link": "http://arxiv.org/abs/2307.01216v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "InstructEval: Systematic Evaluation of Instruction Selection Methods", "abstract": "In-context learning (ICL) performs tasks by prompting a large language model\n(LLM) using an instruction and a small set of annotated examples called\ndemonstrations. Recent work has shown that precise details of the inputs used\nin the ICL prompt significantly impact performance, which has incentivized\ninstruction selection algorithms. The effect of instruction-choice however is\nseverely underexplored, with existing analyses restricted to shallow subsets of\nmodels and tasks, limiting the generalizability of their insights. We develop\nInstructEval, an ICL evaluation suite to conduct a thorough assessment of these\ntechniques. The suite includes 13 open-sourced LLMs of varying scales from four\nmodel families, and covers nine tasks across three categories. Using the suite,\nwe evaluate the relative performance of seven popular instruction selection\nmethods over five metrics relevant to ICL. Our experiments reveal that using\ncurated manually-written instructions or simple instructions without any\ntask-specific descriptions often elicits superior ICL performance overall than\nthat of automatic instruction-induction methods, pointing to a lack of\ngeneralizability among the latter. We release our evaluation suite for\nbenchmarking instruction selection approaches and enabling more generalizable\nmethods in this space.", "published": "2023-07-01 07:45:38", "link": "http://arxiv.org/abs/2307.00259v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Hierarchical Pretraining for Biomedical Term Embeddings", "abstract": "Electronic health records (EHR) contain narrative notes that provide\nextensive details on the medical condition and management of patients. Natural\nlanguage processing (NLP) of clinical notes can use observed frequencies of\nclinical terms as predictive features for downstream applications such as\nclinical decision making and patient trajectory prediction. However, due to the\nvast number of highly similar and related clinical concepts, a more effective\nmodeling strategy is to represent clinical terms as semantic embeddings via\nrepresentation learning and use the low dimensional embeddings as feature\nvectors for predictive modeling. To achieve efficient representation,\nfine-tuning pretrained language models with biomedical knowledge graphs may\ngenerate better embeddings for biomedical terms than those from standard\nlanguage models alone. These embeddings can effectively discriminate synonymous\npairs of from those that are unrelated. However, they often fail to capture\ndifferent degrees of similarity or relatedness for concepts that are\nhierarchical in nature. To overcome this limitation, we propose HiPrBERT, a\nnovel biomedical term representation model trained on additionally complied\ndata that contains hierarchical structures for various biomedical terms. We\nmodify an existing contrastive loss function to extract information from these\nhierarchies. Our numerical experiments demonstrate that HiPrBERT effectively\nlearns the pair-wise distance from hierarchical information, resulting in a\nsubstantially more informative embeddings for further biomedical applications", "published": "2023-07-01 08:16:00", "link": "http://arxiv.org/abs/2307.00266v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Single Sequence Prediction over Reasoning Graphs for Multi-hop QA", "abstract": "Recent generative approaches for multi-hop question answering (QA) utilize\nthe fusion-in-decoder method~\\cite{izacard-grave-2021-leveraging} to generate a\nsingle sequence output which includes both a final answer and a reasoning path\ntaken to arrive at that answer, such as passage titles and key facts from those\npassages. While such models can lead to better interpretability and high\nquantitative scores, they often have difficulty accurately identifying the\npassages corresponding to key entities in the context, resulting in incorrect\npassage hops and a lack of faithfulness in the reasoning path. To address this,\nwe propose a single-sequence prediction method over a local reasoning graph\n(\\model)\\footnote{Code/Models will be released at\n\\url{https://github.com/gowtham1997/SeqGraph}} that integrates a graph\nstructure connecting key entities in each context passage to relevant\nsubsequent passages for each question. We use a graph neural network to encode\nthis graph structure and fuse the resulting representations into the entity\nrepresentations of the model. Our experiments show significant improvements in\nanswer exact-match/F1 scores and faithfulness of grounding in the reasoning\npath on the HotpotQA dataset and achieve state-of-the-art numbers on the\nMusique dataset with only up to a 4\\% increase in model parameters.", "published": "2023-07-01 13:15:09", "link": "http://arxiv.org/abs/2307.00335v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Multitask Retrieval by Promoting Task Specialization", "abstract": "In multitask retrieval, a single retriever is trained to retrieve relevant\ncontexts for multiple tasks. Despite its practical appeal, naive multitask\nretrieval lags behind task-specific retrieval in which a separate retriever is\ntrained for each task. We show that it is possible to train a multitask\nretriever that outperforms task-specific retrievers by promoting task\nspecialization. The main ingredients are: (1) a better choice of pretrained\nmodel (one that is explicitly optimized for multitasking) along with compatible\nprompting, and (2) a novel adaptive learning method that encourages each\nparameter to specialize in a particular task. The resulting multitask retriever\nis highly performant on the KILT benchmark. Upon analysis, we find that the\nmodel indeed learns parameters that are more task-specialized compared to naive\nmultitasking without prompting or adaptive learning.", "published": "2023-07-01 13:45:15", "link": "http://arxiv.org/abs/2307.00342v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Improving Text Matching in E-Commerce Search with A Rationalizable,\n  Intervenable and Fast Entity-Based Relevance Model", "abstract": "Discovering the intended items of user queries from a massive repository of\nitems is one of the main goals of an e-commerce search system. Relevance\nprediction is essential to the search system since it helps improve\nperformance. When online serving a relevance model, the model is required to\nperform fast and accurate inference. Currently, the widely used models such as\nBi-encoder and Cross-encoder have their limitations in accuracy or inference\nspeed respectively. In this work, we propose a novel model called the\nEntity-Based Relevance Model (EBRM). We identify the entities contained in an\nitem and decompose the QI (query-item) relevance problem into multiple QE\n(query-entity) relevance problems; we then aggregate their results to form the\nQI prediction using a soft logic formulation. The decomposition allows us to\nuse a Cross-encoder QE relevance module for high accuracy as well as cache QE\npredictions for fast online inference. Utilizing soft logic makes the\nprediction procedure interpretable and intervenable. We also show that\npretraining the QE module with auto-generated QE data from user logs can\nfurther improve the overall performance. The proposed method is evaluated on\nlabeled data from e-commerce websites. Empirical results show that it achieves\npromising improvements with computation efficiency.", "published": "2023-07-01 15:44:53", "link": "http://arxiv.org/abs/2307.00370v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Effective Matching of Patients to Clinical Trials using Entity\n  Extraction and Neural Re-ranking", "abstract": "Clinical trials (CTs) often fail due to inadequate patient recruitment. This\npaper tackles the challenges of CT retrieval by presenting an approach that\naddresses the patient-to-trials paradigm. Our approach involves two key\ncomponents in a pipeline-based model: (i) a data enrichment technique for\nenhancing both queries and documents during the first retrieval stage, and (ii)\na novel re-ranking schema that uses a Transformer network in a setup adapted to\nthis task by leveraging the structure of the CT documents. We use named entity\nrecognition and negation detection in both patient description and the\neligibility section of CTs. We further classify patient descriptions and CT\neligibility criteria into current, past, and family medical conditions. This\nextracted information is used to boost the importance of disease and drug\nmentions in both query and index for lexical retrieval. Furthermore, we propose\na two-step training schema for the Transformer network used to re-rank the\nresults from the lexical retrieval. The first step focuses on matching patient\ninformation with the descriptive sections of trials, while the second step aims\nto determine eligibility by matching patient information with the criteria\nsection. Our findings indicate that the inclusion criteria section of the CT\nhas a great influence on the relevance score in lexical models, and that the\nenrichment techniques for queries and documents improve the retrieval of\nrelevant trials. The re-ranking strategy, based on our training schema,\nconsistently enhances CT retrieval and shows improved performance by 15\\% in\nterms of precision at retrieving eligible trials. The results of our\nexperiments suggest the benefit of making use of extracted entities. Moreover,\nour proposed re-ranking schema shows promising effectiveness compared to larger\nneural models, even with limited training data.", "published": "2023-07-01 16:42:39", "link": "http://arxiv.org/abs/2307.00381v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Automatic Counterfactual Augmentation for Robust Text Classification\n  Based on Word-Group Search", "abstract": "Despite large-scale pre-trained language models have achieved striking\nresults for text classificaion, recent work has raised concerns about the\nchallenge of shortcut learning. In general, a keyword is regarded as a shortcut\nif it creates a superficial association with the label, resulting in a false\nprediction. Conversely, shortcut learning can be mitigated if the model relies\non robust causal features that help produce sound predictions. To this end,\nmany studies have explored post-hoc interpretable methods to mine shortcuts and\ncausal features for robustness and generalization. However, most existing\nmethods focus only on single word in a sentence and lack consideration of\nword-group, leading to wrong causal features. To solve this problem, we propose\na new Word-Group mining approach, which captures the causal effect of any\nkeyword combination and orders the combinations that most affect the\nprediction. Our approach bases on effective post-hoc analysis and beam search,\nwhich ensures the mining effect and reduces the complexity. Then, we build a\ncounterfactual augmentation method based on the multiple word-groups, and use\nan adaptive voting mechanism to learn the influence of different augmentated\nsamples on the prediction results, so as to force the model to pay attention to\neffective causal features. We demonstrate the effectiveness of the proposed\nmethod by several tasks on 8 affective review datasets and 4 toxic language\ndatasets, including cross-domain text classificaion, text attack and gender\nfairness test.", "published": "2023-07-01 02:26:34", "link": "http://arxiv.org/abs/2307.01214v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Personality Traits in Large Language Models", "abstract": "The advent of large language models (LLMs) has revolutionized natural\nlanguage processing, enabling the generation of coherent and contextually\nrelevant human-like text. As LLMs increasingly powerconversational agents used\nby the general public world-wide, the synthetic personality traits embedded in\nthese models, by virtue of training on large amounts of human data, is becoming\nincreasingly important. Since personality is a key factor determining the\neffectiveness of communication, we present a novel and comprehensive\npsychometrically valid and reliable methodology for administering and\nvalidating personality tests on widely-used LLMs, as well as for shaping\npersonality in the generated text of such LLMs. Applying this method to 18\nLLMs, we found: 1) personality measurements in the outputs of some LLMs under\nspecific prompting configurations are reliable and valid; 2) evidence of\nreliability and validity of synthetic LLM personality is stronger for larger\nand instruction fine-tuned models; and 3) personality in LLM outputs can be\nshaped along desired dimensions to mimic specific human personality profiles.\nWe discuss the application and ethical implications of the measurement and\nshaping method, in particular regarding responsible AI.", "published": "2023-07-01 00:58:51", "link": "http://arxiv.org/abs/2307.00184v4", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "68T35", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Image Matters: A New Dataset and Empirical Study for Multimodal\n  Hyperbole Detection", "abstract": "Hyperbole, or exaggeration, is a common linguistic phenomenon. The detection\nof hyperbole is an important part of understanding human expression. There have\nbeen several studies on hyperbole detection, but most of which focus on text\nmodality only. However, with the development of social media, people can create\nhyperbolic expressions with various modalities, including text, images, videos,\netc. In this paper, we focus on multimodal hyperbole detection. We create a\nmultimodal detection dataset from Weibo (a Chinese social media) and carry out\nsome studies on it. We treat the text and image from a piece of weibo as two\nmodalities and explore the role of text and image for hyperbole detection.\nDifferent pre-trained multimodal encoders are also evaluated on this downstream\ntask to show their performance. Besides, since this dataset is constructed from\nfive different topics, we also evaluate the cross-domain performance of\ndifferent models. These studies can serve as a benchmark and point out the\ndirection of further study on multimodal hyperbole detection.", "published": "2023-07-01 03:23:56", "link": "http://arxiv.org/abs/2307.00209v3", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Understanding Counterspeech for Online Harm Mitigation", "abstract": "Counterspeech offers direct rebuttals to hateful speech by challenging\nperpetrators of hate and showing support to targets of abuse. It provides a\npromising alternative to more contentious measures, such as content moderation\nand deplatforming, by contributing a greater amount of positive online speech\nrather than attempting to mitigate harmful content through removal. Advances in\nthe development of large language models mean that the process of producing\ncounterspeech could be made more efficient by automating its generation, which\nwould enable large-scale online campaigns. However, we currently lack a\nsystematic understanding of several important factors relating to the efficacy\nof counterspeech for hate mitigation, such as which types of counterspeech are\nmost effective, what are the optimal conditions for implementation, and which\nspecific effects of hate it can best ameliorate. This paper aims to fill this\ngap by systematically reviewing counterspeech research in the social sciences\nand comparing methodologies and findings with computer science efforts in\nautomatic counterspeech generation. By taking this multi-disciplinary view, we\nidentify promising future directions in both fields.", "published": "2023-07-01 20:54:01", "link": "http://arxiv.org/abs/2307.04761v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "CephGPT-4: An Interactive Multimodal Cephalometric Measurement and\n  Diagnostic System with Visual Large Language Model", "abstract": "Large-scale multimodal language models (LMMs) have achieved remarkable\nsuccess in general domains. However, the exploration of diagnostic language\nmodels based on multimodal cephalometric medical data remains limited. In this\npaper, we propose a novel multimodal cephalometric analysis and diagnostic\ndialogue model. Firstly, a multimodal orthodontic medical dataset is\nconstructed, comprising cephalometric images and doctor-patient dialogue data,\nwith automatic analysis of cephalometric landmarks using U-net and generation\nof diagnostic reports. Then, the cephalometric dataset and generated diagnostic\nreports are separately fine-tuned on Minigpt-4 and VisualGLM. Results\ndemonstrate that the CephGPT-4 model exhibits excellent performance and has the\npotential to revolutionize orthodontic measurement and diagnostic applications.\nThese innovations hold revolutionary application potential in the field of\northodontics.", "published": "2023-07-01 15:41:12", "link": "http://arxiv.org/abs/2307.07518v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "eess.IV"], "primary_category": "cs.AI"}
{"title": "Enhancing the EEG Speech Match Mismatch Tasks With Word Boundaries", "abstract": "Recent studies have shown that the underlying neural mechanisms of human\nspeech comprehension can be analyzed using a match-mismatch classification of\nthe speech stimulus and the neural response. However, such studies have been\nconducted for fixed-duration segments without accounting for the discrete\nprocessing of speech in the brain. In this work, we establish that word\nboundary information plays a significant role in sentence processing by\nrelating EEG to its speech input. We process the speech and the EEG signals\nusing a network of convolution layers. Then, a word boundary-based average\npooling is performed on the representations, and the inter-word context is\nincorporated using a recurrent layer. The experiments show that the modeling\naccuracy can be significantly improved (match-mismatch classification accuracy)\nto 93% on a publicly available speech-EEG data set, while previous efforts\nachieved an accuracy of 65-75% for this task.", "published": "2023-07-01 15:31:30", "link": "http://arxiv.org/abs/2307.00366v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Using joint training speaker encoder with consistency loss to achieve\n  cross-lingual voice conversion and expressive voice conversion", "abstract": "Voice conversion systems have made significant advancements in terms of\nnaturalness and similarity in common voice conversion tasks. However, their\nperformance in more complex tasks such as cross-lingual voice conversion and\nexpressive voice conversion remains imperfect. In this study, we propose a\nnovel approach that combines a jointly trained speaker encoder and content\nfeatures extracted from the cross-lingual speech recognition model Whisper to\nachieve high-quality cross-lingual voice conversion. Additionally, we introduce\na speaker consistency loss to the joint encoder, which improves the similarity\nbetween the converted speech and the reference speech. To further explore the\ncapabilities of the joint speaker encoder, we use the phonetic posteriorgram as\nthe content feature, which enables the model to effectively reproduce both the\nspeaker characteristics and the emotional aspects of the reference speech.", "published": "2023-07-01 17:44:18", "link": "http://arxiv.org/abs/2307.00393v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
