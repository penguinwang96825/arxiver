{"title": "Entity-Centric Contextual Affective Analysis", "abstract": "While contextualized word representations have improved state-of-the-art\nbenchmarks in many NLP tasks, their potential usefulness for social-oriented\ntasks remains largely unexplored. We show how contextualized word embeddings\ncan be used to capture affect dimensions in portrayals of people. We evaluate\nour methodology quantitatively, on held-out affect lexicons, and qualitatively,\nthrough case examples. We find that contextualized word representations do\nencode meaningful affect information, but they are heavily biased towards their\ntraining data, which limits their usefulness to in-domain analyses. We\nultimately use our method to examine differences in portrayals of men and\nwomen.", "published": "2019-06-05 00:23:50", "link": "http://arxiv.org/abs/1906.01762v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Multiple Diverse Responses with Multi-Mapping and Posterior\n  Mapping Selection", "abstract": "In human conversation an input post is open to multiple potential responses,\nwhich is typically regarded as a one-to-many problem. Promising approaches\nmainly incorporate multiple latent mechanisms to build the one-to-many\nrelationship. However, without accurate selection of the latent mechanism\ncorresponding to the target response during training, these methods suffer from\na rough optimization of latent mechanisms. In this paper, we propose a\nmulti-mapping mechanism to better capture the one-to-many relationship, where\nmultiple mapping modules are employed as latent mechanisms to model the\nsemantic mappings from an input post to its diverse responses. For accurate\noptimization of latent mechanisms, a posterior mapping selection module is\ndesigned to select the corresponding mapping module according to the target\nresponse for further optimization. We also introduce an auxiliary matching loss\nto facilitate the optimization of posterior mapping selection. Empirical\nresults demonstrate the superiority of our model in generating multiple diverse\nand informative responses over the state-of-the-art methods.", "published": "2019-06-05 01:56:50", "link": "http://arxiv.org/abs/1906.01781v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Memory Consolidation for Contextual Spoken Language Understanding with\n  Dialogue Logistic Inference", "abstract": "Dialogue contexts are proven helpful in the spoken language understanding\n(SLU) system and they are typically encoded with explicit memory\nrepresentations. However, most of the previous models learn the context memory\nwith only one objective to maximizing the SLU performance, leaving the context\nmemory under-exploited. In this paper, we propose a new dialogue logistic\ninference (DLI) task to consolidate the context memory jointly with SLU in the\nmulti-task framework. DLI is defined as sorting a shuffled dialogue session\ninto its original logical order and shares the same memory encoder and\nretrieval mechanism as the SLU model. Our experimental results show that\nvarious popular contextual SLU models can benefit from our approach, and\nimprovements are quite impressive, especially in slot filling.", "published": "2019-06-05 02:24:15", "link": "http://arxiv.org/abs/1906.01788v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Generation of High Quality CCGbanks for Parser Domain\n  Adaptation", "abstract": "We propose a new domain adaptation method for Combinatory Categorial Grammar\n(CCG) parsing, based on the idea of automatic generation of CCG corpora\nexploiting cheaper resources of dependency trees. Our solution is conceptually\nsimple, and not relying on a specific parser architecture, making it applicable\nto the current best-performing parsers. We conduct extensive parsing\nexperiments with detailed discussion; on top of existing benchmark datasets on\n(1) biomedical texts and (2) question sentences, we create experimental\ndatasets of (3) speech conversation and (4) math problems. When applied to the\nproposed method, an off-the-shelf CCG parser shows significant performance\ngains, improving from 90.7% to 96.6% on speech conversation, and from 88.5% to\n96.8% on math problems.", "published": "2019-06-05 05:40:26", "link": "http://arxiv.org/abs/1906.01834v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Resource-Free Evaluation Metric for Cross-Lingual Word Embeddings\n  Based on Graph Modularity", "abstract": "Cross-lingual word embeddings encode the meaning of words from different\nlanguages into a shared low-dimensional space. An important requirement for\nmany downstream tasks is that word similarity should be independent of language\n- i.e., word vectors within one language should not be more similar to each\nother than to words in another language. We measure this characteristic using\nmodularity, a network measurement that measures the strength of clusters in a\ngraph. Modularity has a moderate to strong correlation with three downstream\ntasks, even though modularity is based only on the structure of embeddings and\ndoes not require any external resources. We show through experiments that\nmodularity can serve as an intrinsic validation metric to improve unsupervised\ncross-lingual word embeddings, particularly on distant language pairs in\nlow-resource settings.", "published": "2019-06-05 10:34:56", "link": "http://arxiv.org/abs/1906.01926v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Hierarchical Decoder with Three-level Hierarchical Attention to\n  Generate Abstractive Summaries of Interleaved Texts", "abstract": "Interleaved texts, where posts belonging to different threads occur in one\nsequence, are a common occurrence, e.g., online chat conversations. To quickly\nobtain an overview of such texts, existing systems first disentangle the posts\nby threads and then extract summaries from those threads. The major issues with\nsuch systems are error propagation and non-fluent summary. To address those, we\npropose an end-to-end trainable hierarchical encoder-decoder system. We also\nintroduce a novel hierarchical attention mechanism which combines three levels\nof information from an interleaved text, i.e, posts, phrases and words, and\nimplicitly disentangles the threads. We evaluated the proposed system on\nmultiple interleaved text datasets, and it out-performs a SOTA two-step system\nby 20-40%.", "published": "2019-06-05 12:28:00", "link": "http://arxiv.org/abs/1906.01973v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Every child should have parents: a taxonomy refinement algorithm based\n  on hyperbolic term embeddings", "abstract": "We introduce the use of Poincar\\'e embeddings to improve existing\nstate-of-the-art approaches to domain-specific taxonomy induction from text as\na signal for both relocating wrong hyponym terms within a (pre-induced)\ntaxonomy as well as for attaching disconnected terms in a taxonomy. This method\nsubstantially improves previous state-of-the-art results on the SemEval-2016\nTask 13 on taxonomy extraction. We demonstrate the superiority of Poincar\\'e\nembeddings over distributional semantic representations, supporting the\nhypothesis that they can better capture hierarchical lexical-semantic\nrelationships than embeddings in the Euclidean space.", "published": "2019-06-05 12:54:14", "link": "http://arxiv.org/abs/1906.02002v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Imitation Learning for Non-Autoregressive Neural Machine Translation", "abstract": "Non-autoregressive translation models (NAT) have achieved impressive\ninference speedup. A potential issue of the existing NAT algorithms, however,\nis that the decoding is conducted in parallel, without directly considering\nprevious context. In this paper, we propose an imitation learning framework for\nnon-autoregressive machine translation, which still enjoys the fast translation\nspeed but gives comparable translation performance compared to its\nauto-regressive counterpart. We conduct experiments on the IWSLT16, WMT14 and\nWMT16 datasets. Our proposed model achieves a significant speedup over the\nautoregressive models, while keeping the translation quality comparable to the\nautoregressive models. By sampling sentence length in parallel at inference\ntime, we achieve the performance of 31.85 BLEU on WMT16 Ro$\\rightarrow$En and\n30.68 BLEU on IWSLT16 En$\\rightarrow$De.", "published": "2019-06-05 14:15:47", "link": "http://arxiv.org/abs/1906.02041v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The FRENK Datasets of Socially Unacceptable Discourse in Slovene and\n  English", "abstract": "In this paper we present datasets of Facebook comment threads to mainstream\nmedia posts in Slovene and English developed inside the Slovene national\nproject FRENK which cover two topics, migrants and LGBT, and are manually\nannotated for different types of socially unacceptable discourse (SUD). The\nmain advantages of these datasets compared to the existing ones are identical\nsampling procedures, producing comparable data across languages and an\nannotation schema that takes into account six types of SUD and five targets at\nwhich SUD is directed. We describe the sampling and annotation procedures, and\nanalyze the annotation distributions and inter-annotator agreements. We\nconsider this dataset to be an important milestone in understanding and\ncombating SUD for both languages.", "published": "2019-06-05 14:23:01", "link": "http://arxiv.org/abs/1906.02045v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KAS-term: Extracting Slovene Terms from Doctoral Theses via Supervised\n  Machine Learning", "abstract": "This paper presents a dataset and supervised learning experiments for term\nextraction from Slovene academic texts. Term candidates in the dataset were\nextracted via morphosyntactic patterns and annotated for their termness by four\nannotators. Experiments on the dataset show that most co-occurrence statistics,\napplied after morphosyntactic patterns and a frequency threshold, perform close\nto random and that the results can be significantly improved by combining, with\nsupervised machine learning, all the seven statistic measures included in the\ndataset. On multi-word terms the model using all statistics obtains an AUC of\n0.736 while the best single statistic produces only AUC 0.590. Among many\nadditional candidate features, only adding multi-word morphosyntactic pattern\ninformation and length of the single-word term candidates achieves further\nimprovements of the results.", "published": "2019-06-05 14:36:59", "link": "http://arxiv.org/abs/1906.02053v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Legal Judgment Prediction in English", "abstract": "Legal judgment prediction is the task of automatically predicting the outcome\nof a court case, given a text describing the case's facts. Previous work on\nusing neural models for this task has focused on Chinese; only feature-based\nmodels (e.g., using bags of words and topics) have been considered in English.\nWe release a new English legal judgment prediction dataset, containing cases\nfrom the European Court of Human Rights. We evaluate a broad variety of neural\nmodels on the new dataset, establishing strong baselines that surpass previous\nfeature-based models in three tasks: (1) binary violation classification; (2)\nmulti-label classification; (3) case importance prediction. We also explore if\nmodels are biased towards demographic information via data anonymization. As a\nside-product, we propose a hierarchical version of BERT, which bypasses BERT's\nlength limitation.", "published": "2019-06-05 14:56:06", "link": "http://arxiv.org/abs/1906.02059v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Rank for Plausible Plausibility", "abstract": "Researchers illustrate improvements in contextual encoding strategies via\nresultant performance on a battery of shared Natural Language Understanding\n(NLU) tasks. Many of these tasks are of a categorical prediction variety: given\na conditioning context (e.g., an NLI premise), provide a label based on an\nassociated prompt (e.g., an NLI hypothesis). The categorical nature of these\ntasks has led to common use of a cross entropy log-loss objective during\ntraining. We suggest this loss is intuitively wrong when applied to\nplausibility tasks, where the prompt by design is neither categorically\nentailed nor contradictory given the context. Log-loss naturally drives models\nto assign scores near 0.0 or 1.0, in contrast to our proposed use of a\nmargin-based loss. Following a discussion of our intuition, we describe a\nconfirmation study based on an extreme, synthetically curated task derived from\nMultiNLI. We find that a margin-based loss leads to a more plausible model of\nplausibility. Finally, we illustrate improvements on the Choice Of Plausible\nAlternative (COPA) task through this change in loss.", "published": "2019-06-05 15:41:22", "link": "http://arxiv.org/abs/1906.02079v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large-Scale Multi-Label Text Classification on EU Legislation", "abstract": "We consider Large-Scale Multi-Label Text Classification (LMTC) in the legal\ndomain. We release a new dataset of 57k legislative documents from EURLEX,\nannotated with ~4.3k EUROVOC labels, which is suitable for LMTC, few- and\nzero-shot learning. Experimenting with several neural classifiers, we show that\nBIGRUs with label-wise attention perform better than other current state of the\nart methods. Domain-specific WORD2VEC and context-sensitive ELMO embeddings\nfurther improve performance. We also find that considering only particular\nzones of the documents is sufficient. This allows us to bypass BERT's maximum\ntext length limit and fine-tune BERT, obtaining the best results in all but\nzero-shot learning cases.", "published": "2019-06-05 14:41:01", "link": "http://arxiv.org/abs/1906.02192v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Energy and Policy Considerations for Deep Learning in NLP", "abstract": "Recent progress in hardware and methodology for training neural networks has\nushered in a new generation of large networks trained on abundant data. These\nmodels have obtained notable gains in accuracy across many NLP tasks. However,\nthese accuracy improvements depend on the availability of exceptionally large\ncomputational resources that necessitate similarly substantial energy\nconsumption. As a result these models are costly to train and develop, both\nfinancially, due to the cost of hardware and electricity or cloud compute time,\nand environmentally, due to the carbon footprint required to fuel modern tensor\nprocessing hardware. In this paper we bring this issue to the attention of NLP\nresearchers by quantifying the approximate financial and environmental costs of\ntraining a variety of recently successful neural network models for NLP. Based\non these findings, we propose actionable recommendations to reduce costs and\nimprove equity in NLP research and practice.", "published": "2019-06-05 18:40:53", "link": "http://arxiv.org/abs/1906.02243v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Imitation Learning Approach to Unsupervised Parsing", "abstract": "Recently, there has been an increasing interest in unsupervised parsers that\noptimize semantically oriented objectives, typically using reinforcement\nlearning. Unfortunately, the learned trees often do not match actual syntax\ntrees well. Shen et al. (2018) propose a structured attention mechanism for\nlanguage modeling (PRPN), which induces better syntactic structures but relies\non ad hoc heuristics. Also, their model lacks interpretability as it is not\ngrounded in parsing actions. In our work, we propose an imitation learning\napproach to unsupervised parsing, where we transfer the syntactic knowledge\ninduced by the PRPN to a Tree-LSTM model with discrete parsing actions. Its\npolicy is then refined by Gumbel-Softmax training towards a semantically\noriented objective. We evaluate our approach on the All Natural Language\nInference dataset and show that it achieves a new state of the art in terms of\nparsing $F$-score, outperforming our base models, including the PRPN.", "published": "2019-06-05 19:45:21", "link": "http://arxiv.org/abs/1906.02276v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Survey on Publicly Available Sinhala Natural Language Processing Tools\n  and Research", "abstract": "Sinhala is the native language of the Sinhalese people who make up the\nlargest ethnic group of Sri Lanka. The language belongs to the globe-spanning\nlanguage tree, Indo-European. However, due to poverty in both linguistic and\neconomic capital, Sinhala, in the perspective of Natural Language Processing\ntools and research, remains a resource-poor language which has neither the\neconomic drive its cousin English has nor the sheer push of the law of numbers\na language such as Chinese has. A number of research groups from Sri Lanka have\nnoticed this dearth and the resultant dire need for proper tools and research\nfor Sinhala natural language processing. However, due to various reasons, these\nattempts seem to lack coordination and awareness of each other. The objective\nof this paper is to fill that gap of a comprehensive literature survey of the\npublicly available Sinhala natural language tools and research so that the\nresearchers working in this field can better utilize contributions of their\npeers. As such, we shall be uploading this paper to arXiv and perpetually\nupdate it periodically to reflect the advances made in the field.", "published": "2019-06-05 23:36:06", "link": "http://arxiv.org/abs/1906.02358v24", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Deep Transformer Models for Machine Translation", "abstract": "Transformer is the state-of-the-art model in recent machine translation\nevaluations. Two strands of research are promising to improve models of this\nkind: the first uses wide networks (a.k.a. Transformer-Big) and has been the de\nfacto standard for the development of the Transformer system, and the other\nuses deeper language representation but faces the difficulty arising from\nlearning deep networks. Here, we continue the line of research on the latter.\nWe claim that a truly deep Transformer model can surpass the Transformer-Big\ncounterpart by 1) proper use of layer normalization and 2) a novel way of\npassing the combination of previous layers to the next. On WMT'16 English-\nGerman, NIST OpenMT'12 Chinese-English and larger WMT'18 Chinese-English tasks,\nour deep system (30/25-layer encoder) outperforms the shallow\nTransformer-Big/Base baseline (6-layer encoder) by 0.4-2.4 BLEU points. As\nanother bonus, the deep model is 1.6X smaller in size and 3X faster in training\nthan Transformer-Big.", "published": "2019-06-05 02:24:12", "link": "http://arxiv.org/abs/1906.01787v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DOER: Dual Cross-Shared RNN for Aspect Term-Polarity Co-Extraction", "abstract": "This paper focuses on two related subtasks of aspect-based sentiment\nanalysis, namely aspect term extraction and aspect sentiment classification,\nwhich we call aspect term-polarity co-extraction. The former task is to extract\naspects of a product or service from an opinion document, and the latter is to\nidentify the polarity expressed in the document about these extracted aspects.\nMost existing algorithms address them as two separate tasks and solve them one\nby one, or only perform one task, which can be complicated for real\napplications. In this paper, we treat these two tasks as two sequence labeling\nproblems and propose a novel Dual crOss-sharEd RNN framework (DOER) to generate\nall aspect term-polarity pairs of the input sentence simultaneously.\nSpecifically, DOER involves a dual recurrent neural network to extract the\nrespective representation of each task, and a cross-shared unit to consider the\nrelationship between them. Experimental results demonstrate that the proposed\nframework outperforms state-of-the-art baselines on three benchmark datasets.", "published": "2019-06-05 02:45:06", "link": "http://arxiv.org/abs/1906.01794v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Multimodal Sarcasm Detection (An _Obviously_ Perfect Paper)", "abstract": "Sarcasm is often expressed through several verbal and non-verbal cues, e.g.,\na change of tone, overemphasis in a word, a drawn-out syllable, or a straight\nlooking face. Most of the recent work in sarcasm detection has been carried out\non textual data. In this paper, we argue that incorporating multimodal cues can\nimprove the automatic classification of sarcasm. As a first step towards\nenabling the development of multimodal approaches for sarcasm detection, we\npropose a new sarcasm dataset, Multimodal Sarcasm Detection Dataset (MUStARD),\ncompiled from popular TV shows. MUStARD consists of audiovisual utterances\nannotated with sarcasm labels. Each utterance is accompanied by its context of\nhistorical utterances in the dialogue, which provides additional information on\nthe scenario where the utterance occurs. Our initial results show that the use\nof multimodal information can reduce the relative error rate of sarcasm\ndetection by up to 12.9% in F-score when compared to the use of individual\nmodalities. The full dataset is publicly available for use at\nhttps://github.com/soujanyaporia/MUStARD", "published": "2019-06-05 04:08:47", "link": "http://arxiv.org/abs/1906.01815v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "A Hierarchical Reinforced Sequence Operation Method for Unsupervised\n  Text Style Transfer", "abstract": "Unsupervised text style transfer aims to alter text styles while preserving\nthe content, without aligned data for supervision. Existing seq2seq methods\nface three challenges: 1) the transfer is weakly interpretable, 2) generated\noutputs struggle in content preservation, and 3) the trade-off between content\nand style is intractable. To address these challenges, we propose a\nhierarchical reinforced sequence operation method, named Point-Then-Operate\n(PTO), which consists of a high-level agent that proposes operation positions\nand a low-level agent that alters the sentence. We provide comprehensive\ntraining objectives to control the fluency, style, and content of the outputs\nand a mask-based inference algorithm that allows for multi-step revision based\non the single-step trained agents. Experimental results on two text style\ntransfer datasets show that our method significantly outperforms recent methods\nand effectively addresses the aforementioned challenges.", "published": "2019-06-05 05:27:31", "link": "http://arxiv.org/abs/1906.01833v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Textual Network Embedding with Global Attention via Optimal\n  Transport", "abstract": "Constituting highly informative network embeddings is an important tool for\nnetwork analysis. It encodes network topology, along with other useful side\ninformation, into low-dimensional node-based feature representations that can\nbe exploited by statistical modeling. This work focuses on learning\ncontext-aware network embeddings augmented with text data. We reformulate the\nnetwork-embedding problem, and present two novel strategies to improve over\ntraditional attention mechanisms: ($i$) a content-aware sparse attention module\nbased on optimal transport, and ($ii$) a high-level attention parsing module.\nOur approach yields naturally sparse and self-normalized relational inference.\nIt can capture long-term interactions between sequences, thus addressing the\nchallenges faced by existing textual network embedding schemes. Extensive\nexperiments are conducted to demonstrate our model can consistently outperform\nalternative state-of-the-art methods.", "published": "2019-06-05 05:59:07", "link": "http://arxiv.org/abs/1906.01840v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards conceptual generalization in the embedding space", "abstract": "Humans are able to conceive physical reality by jointly learning different\nfacets thereof. To every pair of notions related to a perceived reality may\ncorrespond a mutual relation, which is a notion on its own, but one-level\nhigher. Thus, we may have a description of perceived reality on at least two\nlevels and the translation map between them is in general, due to their\ndifferent content corpus, one-to-many. Following success of the unsupervised\nneural machine translation models, which are essentially one-to-one mappings\ntrained separately on monolingual corpora, we examine further capabilities of\nthe unsupervised deep learning methods used there and apply some of these\nmethods to sets of notions of different level and measure. Using the graph and\nword embedding-like techniques, we build one-to-many map without parallel data\nin order to establish a unified vector representation of the outer world by\ncombining notions of different kind into a unique conceptual framework. Due to\ntheir latent similarity, by aligning the two embedding spaces in purely\nunsupervised way, one obtains a geometric relation between objects of cognition\non the two levels, making it possible to express a natural knowledge using one\ndescription in the context of the other.", "published": "2019-06-05 08:11:12", "link": "http://arxiv.org/abs/1906.01873v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Learning Bilingual Sentence Embeddings via Autoencoding and Computing\n  Similarities with a Multilayer Perceptron", "abstract": "We propose a novel model architecture and training algorithm to learn\nbilingual sentence embeddings from a combination of parallel and monolingual\ndata. Our method connects autoencoding and neural machine translation to force\nthe source and target sentence embeddings to share the same space without the\nhelp of a pivot language or an additional transformation. We train a multilayer\nperceptron on top of the sentence embeddings to extract good bilingual sentence\npairs from nonparallel or noisy parallel data. Our approach shows promising\nperformance on sentence alignment recovery and the WMT 2018 parallel corpus\nfiltering tasks with only a single model.", "published": "2019-06-05 11:16:33", "link": "http://arxiv.org/abs/1906.01942v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Automated Speech Generation from UN General Assembly Statements: Mapping\n  Risks in AI Generated Texts", "abstract": "Automated text generation has been applied broadly in many domains such as\nmarketing and robotics, and used to create chatbots, product reviews and write\npoetry. The ability to synthesize text, however, presents many potential risks,\nwhile access to the technology required to build generative models is becoming\nincreasingly easy. This work is aligned with the efforts of the United Nations\nand other civil society organisations to highlight potential political and\nsocietal risks arising through the malicious use of text generation software,\nand their potential impact on human rights. As a case study, we present the\nfindings of an experiment to generate remarks in the style of political leaders\nby fine-tuning a pretrained AWD- LSTM model on a dataset of speeches made at\nthe UN General Assembly. This work highlights the ease with which this can be\naccomplished, as well as the threats of combining these techniques with other\ntechnologies.", "published": "2019-06-05 11:23:14", "link": "http://arxiv.org/abs/1906.01946v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "From Balustrades to Pierre Vinken: Looking for Syntax in Transformer\n  Self-Attentions", "abstract": "We inspect the multi-head self-attention in Transformer NMT encoders for\nthree source languages, looking for patterns that could have a syntactic\ninterpretation. In many of the attention heads, we frequently find sequences of\nconsecutive states attending to the same position, which resemble syntactic\nphrases. We propose a transparent deterministic method of quantifying the\namount of syntactic information present in the self-attentions, based on\nautomatically building and evaluating phrase-structure trees from the\nphrase-like sequences. We compare the resulting trees to existing constituency\ntreebanks, both manually and by computing precision and recall.", "published": "2019-06-05 11:53:38", "link": "http://arxiv.org/abs/1906.01958v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Extracting Symptoms and their Status from Clinical Conversations", "abstract": "This paper describes novel models tailored for a new application, that of\nextracting the symptoms mentioned in clinical conversations along with their\nstatus. Lack of any publicly available corpus in this privacy-sensitive domain\nled us to develop our own corpus, consisting of about 3K conversations\nannotated by professional medical scribes. We propose two novel deep learning\napproaches to infer the symptom names and their status: (1) a new hierarchical\nspan-attribute tagging (\\SAT) model, trained using curriculum learning, and (2)\na variant of sequence-to-sequence model which decodes the symptoms and their\nstatus from a few speaker turns within a sliding window over the conversation.\nThis task stems from a realistic application of assisting medical providers in\ncapturing symptoms mentioned by patients from their clinical conversations. To\nreflect this application, we define multiple metrics. From inter-rater\nagreement, we find that the task is inherently difficult. We conduct\ncomprehensive evaluations on several contrasting conditions and observe that\nthe performance of the models range from an F-score of 0.5 to 0.8 depending on\nthe condition. Our analysis not only reveals the inherent challenges of the\ntask, but also provides useful directions to improve the models.", "published": "2019-06-05 18:34:16", "link": "http://arxiv.org/abs/1906.02239v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Variational Pretraining for Semi-supervised Text Classification", "abstract": "We introduce VAMPIRE, a lightweight pretraining framework for effective text\nclassification when data and computing resources are limited. We pretrain a\nunigram document model as a variational autoencoder on in-domain, unlabeled\ndata and use its internal states as features in a downstream classifier.\nEmpirically, we show the relative strength of VAMPIRE against computationally\nexpensive contextual embeddings and other popular semi-supervised baselines\nunder low resource settings. We also find that fine-tuning to in-domain data is\ncrucial to achieving decent performance from contextual embeddings when working\nwith limited supervision. We accompany this paper with code to pretrain and use\nVAMPIRE embeddings in downstream tasks.", "published": "2019-06-05 18:40:37", "link": "http://arxiv.org/abs/1906.02242v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SParC: Cross-Domain Semantic Parsing in Context", "abstract": "We present SParC, a dataset for cross-domainSemanticParsing inContext that\nconsists of 4,298 coherent question sequences (12k+ individual questions\nannotated with SQL queries). It is obtained from controlled user interactions\nwith 200 complex databases over 138 domains. We provide an in-depth analysis of\nSParC and show that it introduces new challenges compared to existing datasets.\nSParC demonstrates complex contextual dependencies, (2) has greater semantic\ndiversity, and (3) requires generalization to unseen domains due to its\ncross-domain nature and the unseen databases at test time. We experiment with\ntwo state-of-the-art text-to-SQL models adapted to the context-dependent,\ncross-domain setup. The best model obtains an exact match accuracy of 20.2%\nover all questions and less than10% over all interaction sequences, indicating\nthat the cross-domain setting and the con-textual phenomena of the dataset\npresent significant challenges for future research. The dataset, baselines, and\nleaderboard are released at https://yale-lily.github.io/sparc.", "published": "2019-06-05 20:05:18", "link": "http://arxiv.org/abs/1906.02285v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Training Temporal Word Embeddings with a Compass", "abstract": "Temporal word embeddings have been proposed to support the analysis of word\nmeaning shifts during time and to study the evolution of languages. Different\napproaches have been proposed to generate vector representations of words that\nembed their meaning during a specific time interval. However, the training\nprocess used in these approaches is complex, may be inefficient or it may\nrequire large text corpora. As a consequence, these approaches may be difficult\nto apply in resource-scarce domains or by scientists with limited in-depth\nknowledge of embedding models. In this paper, we propose a new heuristic to\ntrain temporal word embeddings based on the Word2vec model. The heuristic\nconsists in using atemporal vectors as a reference, i.e., as a compass, when\ntraining the representations specific to a given time interval. The use of the\ncompass simplifies the training process and makes it more efficient.\nExperiments conducted using state-of-the-art datasets and methodologies suggest\nthat our approach outperforms or equals comparable approaches while being more\nrobust in terms of the required corpus size.", "published": "2019-06-05 09:40:25", "link": "http://arxiv.org/abs/1906.02376v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Visual Story Post-Editing", "abstract": "We introduce the first dataset for human edits of machine-generated visual\nstories and explore how these collected edits may be used for the visual story\npost-editing task. The dataset, VIST-Edit, includes 14,905 human edited\nversions of 2,981 machine-generated visual stories. The stories were generated\nby two state-of-the-art visual storytelling models, each aligned to 5\nhuman-edited versions. We establish baselines for the task, showing how a\nrelatively small set of human edits can be leveraged to boost the performance\nof large visual storytelling models. We also discuss the weak correlation\nbetween automatic evaluation scores and human ratings, motivating the need for\nnew automatic metrics.", "published": "2019-06-05 00:33:47", "link": "http://arxiv.org/abs/1906.01764v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Terminology-based Text Embedding for Computing Document Similarities on\n  Technical Content", "abstract": "We propose in this paper a new, hybrid document embedding approach in order\nto address the problem of document similarities with respect to the technical\ncontent. To do so, we employ a state-of-the-art graph techniques to first\nextract the keyphrases (composite keywords) of documents and, then, use them to\nscore the sentences. Using the ranked sentences, we propose two approaches to\nembed documents and show their performances with respect to two baselines. With\ndomain expert annotations, we illustrate that the proposed methods can find\nmore relevant documents and outperform the baselines up to 27% in terms of\nNDCG.", "published": "2019-06-05 08:16:42", "link": "http://arxiv.org/abs/1906.01874v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Evaluation and Improvement of Chatbot Text Classification Data Quality\n  Using Plausible Negative Examples", "abstract": "We describe and validate a metric for estimating multi-class classifier\nperformance based on cross-validation and adapted for improvement of small,\nunbalanced natural-language datasets used in chatbot design. Our experiences\ndraw upon building recruitment chatbots that mediate communication between\njob-seekers and recruiters by exposing the ML/NLP dataset to the recruiting\nteam. Evaluation approaches must be understandable to various stakeholders, and\nuseful for improving chatbot performance. The metric, nex-cv, uses negative\nexamples in the evaluation of text classification, and fulfils three\nrequirements. First, it is actionable: it can be used by non-developer staff.\nSecond, it is not overly optimistic compared to human ratings, making it a fast\nmethod for comparing classifiers. Third, it allows model-agnostic comparison,\nmaking it useful for comparing systems despite implementation differences. We\nvalidate the metric based on seven recruitment-domain datasets in English and\nGerman over the course of one year.", "published": "2019-06-05 09:52:22", "link": "http://arxiv.org/abs/1906.01910v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "The Language of Dialogue Is Complex", "abstract": "Integrative Complexity (IC) is a psychometric that measures the ability of a\nperson to recognize multiple perspectives and connect them, thus identifying\npaths for conflict resolution. IC has been linked to a wide variety of\npolitical, social and personal outcomes but evaluating it is a time-consuming\nprocess requiring skilled professionals to manually score texts, a fact which\naccounts for the limited exploration of IC at scale on social media.We combine\nnatural language processing and machine learning to train an IC classification\nmodel that achieves state-of-the-art performance on unseen data and more\nclosely adheres to the established structure of the IC coding process than\nprevious automated approaches. When applied to the content of 400k+ comments\nfrom online fora about depression and knowledge exchange, our model was capable\nof replicating key findings of prior work, thus providing the first example of\nusing IC tools for large-scale social media analytics.", "published": "2019-06-05 14:51:31", "link": "http://arxiv.org/abs/1906.02057v1", "categories": ["cs.CY", "cs.CL", "cs.SI"], "primary_category": "cs.CY"}
{"title": "Syntax-Infused Variational Autoencoder for Text Generation", "abstract": "We present a syntax-infused variational autoencoder (SIVAE), that integrates\nsentences with their syntactic trees to improve the grammar of generated\nsentences. Distinct from existing VAE-based text generative models, SIVAE\ncontains two separate latent spaces, for sentences and syntactic trees. The\nevidence lower bound objective is redesigned correspondingly, by optimizing a\njoint distribution that accommodates two encoders and two decoders. SIVAE works\nwith long short-term memory architectures to simultaneously generate sentences\nand syntactic trees. Two versions of SIVAE are proposed: one captures the\ndependencies between the latent variables through a conditional prior network,\nand the other treats the latent variables independently such that\nsyntactically-controlled sentence generation can be performed. Experimental\nresults demonstrate the generative superiority of SIVAE on both reconstruction\nand targeted syntactic evaluations. Finally, we show that the proposed models\ncan be used for unsupervised paraphrasing given different syntactic tree\ntemplates.", "published": "2019-06-05 22:48:08", "link": "http://arxiv.org/abs/1906.02181v1", "categories": ["stat.ML", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Topic Sensitive Attention on Generic Corpora Corrects Sense Bias in\n  Pretrained Embeddings", "abstract": "Given a small corpus $\\mathcal D_T$ pertaining to a limited set of focused\ntopics, our goal is to train embeddings that accurately capture the sense of\nwords in the topic in spite of the limited size of $\\mathcal D_T$. These\nembeddings may be used in various tasks involving $\\mathcal D_T$. A popular\nstrategy in limited data settings is to adapt pre-trained embeddings $\\mathcal\nE$ trained on a large corpus. To correct for sense drift, fine-tuning,\nregularization, projection, and pivoting have been proposed recently. Among\nthese, regularization informed by a word's corpus frequency performed well, but\nwe improve upon it using a new regularizer based on the stability of its\ncooccurrence with other words. However, a thorough comparison across ten\ntopics, spanning three tasks, with standardized settings of hyper-parameters,\nreveals that even the best embedding adaptation strategies provide small gains\nbeyond well-tuned baselines, which many earlier comparisons ignored. In a bold\ndeparture from adapting pretrained embeddings, we propose using $\\mathcal D_T$\nto probe, attend to, and borrow fragments from any large, topic-rich source\ncorpus (such as Wikipedia), which need not be the corpus used to pretrain\nembeddings. This step is made scalable and practical by suitable indexing. We\nreach the surprising conclusion that even limited corpus augmentation is more\nuseful than adapting embeddings, which suggests that non-dominant sense\ninformation may be irrevocably obliterated from pretrained embeddings and\ncannot be salvaged by adaptation.", "published": "2019-06-05 07:15:06", "link": "http://arxiv.org/abs/1906.02688v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Complex Evolution Recurrent Neural Networks (ceRNNs)", "abstract": "Unitary Evolution Recurrent Neural Networks (uRNNs) have three attractive\nproperties: (a) the unitary property, (b) the complex-valued nature, and (c)\ntheir efficient linear operators. The literature so far does not address -- how\ncritical is the unitary property of the model? Furthermore, uRNNs have not been\nevaluated on large tasks. To study these shortcomings, we propose the complex\nevolution Recurrent Neural Networks (ceRNNs), which is similar to uRNNs but\ndrops the unitary property selectively. On a simple multivariate linear\nregression task, we illustrate that dropping the constraints improves the\nlearning trajectory. In copy memory task, ceRNNs and uRNNs perform identically,\ndemonstrating that their superior performance over LSTMs is due to\ncomplex-valued nature and their linear operators. In a large scale real-world\nspeech recognition, we find that pre-pending a uRNN degrades the performance of\nour baseline LSTM acoustic models, while pre-pending a ceRNN improves the\nperformance over the baseline by 0.8% absolute WER.", "published": "2019-06-05 18:51:26", "link": "http://arxiv.org/abs/1906.02246v1", "categories": ["cs.LG", "cs.CL", "cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.LG"}
{"title": "Automated Activity Recognition of Construction Equipment Using a Data\n  Fusion Approach", "abstract": "Automated monitoring of construction operations, especially operations of\nequipment and machines, is an essential step toward cost-estimating, and\nplanning of construction projects. In recent years, a number of methods were\nsuggested for recognizing activities of construction equipment. These methods\nare based on processing single types of data (audio, visual, or kinematic\ndata). Considering the complexity of construction jobsites, using one source of\ndata is not reliable enough to cover all conditions and scenarios. To address\nthe issue, we utilized a data fusion approach: This approach is based on\ncollecting audio and kinematic data, and includes the following steps: 1)\nrecording audio and kinematic data generated by machines, 2) preprocessing\ndata, 3) extracting time- and frequency-domain-features, 4) feature-fusion, and\n5) categorizing activities using a machine-learning algorithm. The proposed\napproach was implemented on multiple machines and the experiments show that it\nis possible to get up to 25% more-accurate results compared to cases of using\nsingle-data-sources.", "published": "2019-06-05 15:26:31", "link": "http://arxiv.org/abs/1906.02070v1", "categories": ["eess.SP", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
{"title": "Investigating the Lombard Effect Influence on End-to-End Audio-Visual\n  Speech Recognition", "abstract": "Several audio-visual speech recognition models have been recently proposed\nwhich aim to improve the robustness over audio-only models in the presence of\nnoise. However, almost all of them ignore the impact of the Lombard effect,\ni.e., the change in speaking style in noisy environments which aims to make\nspeech more intelligible and affects both the acoustic characteristics of\nspeech and the lip movements. In this paper, we investigate the impact of the\nLombard effect in audio-visual speech recognition. To the best of our\nknowledge, this is the first work which does so using end-to-end deep\narchitectures and presents results on unseen speakers. Our results show that\nproperly modelling Lombard speech is always beneficial. Even if a relatively\nsmall amount of Lombard speech is added to the training set then the\nperformance in a real scenario, where noisy Lombard speech is present, can be\nsignificantly improved. We also show that the standard approach followed in the\nliterature, where a model is trained and tested on noisy plain speech, provides\na correct estimate of the video-only performance and slightly underestimates\nthe audio-visual performance. In case of audio-only approaches, performance is\noverestimated for SNRs higher than -3dB and underestimated for lower SNRs.", "published": "2019-06-05 16:40:54", "link": "http://arxiv.org/abs/1906.02112v4", "categories": ["eess.AS", "cs.CV", "eess.IV"], "primary_category": "eess.AS"}
