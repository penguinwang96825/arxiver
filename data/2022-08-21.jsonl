{"title": "The Development of a Labelled te reo M\u0101ori-English Bilingual Database\n  for Language Technology", "abstract": "Te reo M\\=aori (referred to as M\\=aori), New Zealand's indigenous language,\nis under-resourced in language technology. M\\=aori speakers are bilingual,\nwhere M\\=aori is code-switched with English. Unfortunately, there are minimal\nresources available for M\\=aori language technology, language detection and\ncode-switch detection between M\\=aori-English pair. Both English and M\\=aori\nuse Roman-derived orthography making rule-based systems for detecting language\nand code-switching restrictive. Most M\\=aori language detection is done\nmanually by language experts. This research builds a M\\=aori-English bilingual\ndatabase of 66,016,807 words with word-level language annotation. The New\nZealand Parliament Hansard debates reports were used to build the database. The\nlanguage labels are assigned using language-specific rules and expert manual\nannotations. Words with the same spelling, but different meanings, exist for\nM\\=aori and English. These words could not be categorised as M\\=aori or English\nbased on word-level language rules. Hence, manual annotations were necessary.\nAn analysis reporting the various aspects of the database such as metadata,\nyear-wise analysis, frequently occurring words, sentence length and N-grams is\nalso reported. The database developed here is a valuable tool for future\nlanguage and speech technology development for Aotearoa New Zealand. The\nmethodology followed to label the database can also be followed by other\nlow-resourced language pairs.", "published": "2022-08-21 01:56:13", "link": "http://arxiv.org/abs/2208.09778v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic tagging of knowledge points for K12 math problems", "abstract": "Automatic tagging of knowledge points for practice problems is the basis for\nmanaging question bases and improving the automation and intelligence of\neducation. Therefore, it is of great practical significance to study the\nautomatic tagging technology for practice problems. However, there are few\nstudies on the automatic tagging of knowledge points for math problems. Math\ntexts have more complex structures and semantics compared with general texts\nbecause they contain unique elements such as symbols and formulas. Therefore,\nit is difficult to meet the accuracy requirement of knowledge point prediction\nby directly applying the text classification techniques in general domains. In\nthis paper, K12 math problems taken as the research object, the LABS model\nbased on label-semantic attention and multi-label smoothing combining textual\nfeatures is proposed to improve the automatic tagging of knowledge points for\nmath problems. The model combines the text classification techniques in general\ndomains and the unique features of math texts. The results show that the models\nusing label-semantic attention or multi-label smoothing perform better on\nprecision, recall, and F1-score metrics than the traditional BiLSTM model,\nwhile the LABS model using both performs best. It can be seen that label\ninformation can guide the neural networks to extract meaningful information\nfrom the problem text, which improves the text classification performance of\nthe model. Moreover, multi-label smoothing combining textual features can fully\nexplore the relationship between text and labels, improve the model's\nprediction ability for new data and improve the model's classification\naccuracy.", "published": "2022-08-21 11:11:30", "link": "http://arxiv.org/abs/2208.09867v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Syntax Aware BERT for Identifying Well-Formed Queries in a Curriculum\n  Framework", "abstract": "A well formed query is defined as a query which is formulated in the manner\nof an inquiry, and with correct interrogatives, spelling and grammar. While\nidentifying well formed queries is an important task, few works have attempted\nto address it. In this paper we propose transformer based language model -\nBidirectional Encoder Representations from Transformers (BERT) to this task. We\nfurther imbibe BERT with parts-of-speech information inspired from earlier\nworks. Furthermore, we also train the model in multiple curriculum settings for\nimprovement in performance. Curriculum Learning over the task is experimented\nwith Baby Steps and One Pass techniques. Proposed architecture performs\nexceedingly well on the task. The best approach achieves accuracy of 83.93%,\noutperforming previous state-of-the-art at 75.0% and reaching close to the\napproximate human upper bound of 88.4%.", "published": "2022-08-21 15:35:33", "link": "http://arxiv.org/abs/2208.09912v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MockingBERT: A Method for Retroactively Adding Resilience to NLP Models", "abstract": "Protecting NLP models against misspellings whether accidental or adversarial\nhas been the object of research interest for the past few years. Existing\nremediations have typically either compromised accuracy or required full model\nre-training with each new class of attacks. We propose a novel method of\nretroactively adding resilience to misspellings to transformer-based NLP\nmodels. This robustness can be achieved without the need for re-training of the\noriginal NLP model and with only a minimal loss of language understanding\nperformance on inputs without misspellings. Additionally we propose a new\nefficient approximate method of generating adversarial misspellings, which\nsignificantly reduces the cost needed to evaluate a model's resilience to\nadversarial attacks.", "published": "2022-08-21 16:02:01", "link": "http://arxiv.org/abs/2208.09915v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GRETEL: Graph Contrastive Topic Enhanced Language Model for Long\n  Document Extractive Summarization", "abstract": "Recently, neural topic models (NTMs) have been incorporated into pre-trained\nlanguage models (PLMs), to capture the global semantic information for text\nsummarization. However, in these methods, there remain limitations in the way\nthey capture and integrate the global semantic information. In this paper, we\npropose a novel model, the graph contrastive topic enhanced language model\n(GRETEL), that incorporates the graph contrastive topic model with the\npre-trained language model, to fully leverage both the global and local\ncontextual semantics for long document extractive summarization. To better\ncapture and incorporate the global semantic information into PLMs, the graph\ncontrastive topic model integrates the hierarchical transformer encoder and the\ngraph contrastive learning to fuse the semantic information from the global\ndocument context and the gold summary. To this end, GRETEL encourages the model\nto efficiently extract salient sentences that are topically related to the gold\nsummary, rather than redundant sentences that cover sub-optimal topics.\nExperimental results on both general domain and biomedical datasets demonstrate\nthat our proposed method outperforms SOTA methods.", "published": "2022-08-21 23:09:29", "link": "http://arxiv.org/abs/2208.09982v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Z-Code++: A Pre-trained Language Model Optimized for Abstractive\n  Summarization", "abstract": "This paper presents Z-Code++, a new pre-trained language model optimized for\nabstractive text summarization. The model extends the state of the art\nencoder-decoder model using three techniques. First, we use a two-phase\npre-training process to improve model's performance on low-resource\nsummarization tasks. The model is first pre-trained using text corpora for\nlanguage understanding, and then is continually pre-trained on summarization\ncorpora for grounded text generation. Second, we replace self-attention layers\nin the encoder with disentangled attention layers, where each word is\nrepresented using two vectors that encode its content and position,\nrespectively. Third, we use fusion-in-encoder, a simple yet effective method of\nencoding long sequences in a hierarchical manner. Z-Code++ creates new state of\nthe art on 9 out of 13 text summarization tasks across 5 languages. Our model\nis parameter-efficient in that it outperforms the 600x larger PaLM-540B on\nXSum, and the finetuned 200x larger GPT3-175B on SAMSum. In zero-shot and\nfew-shot settings, our model substantially outperforms the competing models.", "published": "2022-08-21 01:00:54", "link": "http://arxiv.org/abs/2208.09770v2", "categories": ["cs.CL", "cs.AI", "cs.CL, cs.GL", "I.2; I.7"], "primary_category": "cs.CL"}
{"title": "I Know What You Do Not Know: Knowledge Graph Embedding via\n  Co-distillation Learning", "abstract": "Knowledge graph (KG) embedding seeks to learn vector representations for\nentities and relations. Conventional models reason over graph structures, but\nthey suffer from the issues of graph incompleteness and long-tail entities.\nRecent studies have used pre-trained language models to learn embeddings based\non the textual information of entities and relations, but they cannot take\nadvantage of graph structures. In the paper, we show empirically that these two\nkinds of features are complementary for KG embedding. To this end, we propose\nCoLE, a Co-distillation Learning method for KG Embedding that exploits the\ncomplementarity of graph structures and text information. Its graph embedding\nmodel employs Transformer to reconstruct the representation of an entity from\nits neighborhood subgraph. Its text embedding model uses a pre-trained language\nmodel to generate entity representations from the soft prompts of their names,\ndescriptions, and relational neighbors. To let the two model promote each\nother, we propose co-distillation learning that allows them to distill\nselective knowledge from each other's prediction logits. In our co-distillation\nlearning, each model serves as both a teacher and a student. Experiments on\nbenchmark datasets demonstrate that the two models outperform their related\nbaselines, and the ensemble method CoLE with co-distillation learning advances\nthe state-of-the-art of KG embedding.", "published": "2022-08-21 07:34:37", "link": "http://arxiv.org/abs/2208.09828v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CMSBERT-CLR: Context-driven Modality Shifting BERT with Contrastive\n  Learning for linguistic, visual, acoustic Representations", "abstract": "Multimodal sentiment analysis has become an increasingly popular research\narea as the demand for multimodal online content is growing. For multimodal\nsentiment analysis, words can have different meanings depending on the\nlinguistic context and non-verbal information, so it is crucial to understand\nthe meaning of the words accordingly. In addition, the word meanings should be\ninterpreted within the whole utterance context that includes nonverbal\ninformation. In this paper, we present a Context-driven Modality Shifting BERT\nwith Contrastive Learning for linguistic, visual, acoustic Representations\n(CMSBERT-CLR), which incorporates the whole context's non-verbal and verbal\ninformation and aligns modalities more effectively through contrastive\nlearning. First, we introduce a Context-driven Modality Shifting (CMS) to\nincorporate the non-verbal and verbal information within the whole context of\nthe sentence utterance. Then, for improving the alignment of different\nmodalities within a common embedding space, we apply contrastive learning.\nFurthermore, we use an exponential moving average parameter and label smoothing\nas optimization strategies, which can make the convergence of the network more\nstable and increase the flexibility of the alignment. In our experiments, we\ndemonstrate that our approach achieves state-of-the-art results.", "published": "2022-08-21 08:21:43", "link": "http://arxiv.org/abs/2209.07424v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Visualising Model Training via Vowel Space for Text-To-Speech Systems", "abstract": "With the recent developments in speech synthesis via machine learning, this\nstudy explores incorporating linguistics knowledge to visualise and evaluate\nsynthetic speech model training. If changes to the first and second formant (in\nturn, the vowel space) can be seen and heard in synthetic speech, this\nknowledge can inform speech synthesis technology developers. A speech synthesis\nmodel trained on a large General American English database was fine-tuned into\na New Zealand English voice to identify if the changes in the vowel space of\nsynthetic speech could be seen and heard. The vowel spaces at different\nintervals during the fine-tuning were analysed to determine if the model\nlearned the New Zealand English vowel space. Our findings based on vowel space\nanalysis show that we can visualise how a speech synthesis model learns the\nvowel space of the database it is trained on. Perception tests confirmed that\nhumans could perceive when a speech synthesis model has learned characteristics\nof the speech database it is training on. Using the vowel space as an\nintermediary evaluation helps understand what sounds are to be added to the\ntraining database and build speech synthesis models based on linguistics\nknowledge.", "published": "2022-08-21 01:33:24", "link": "http://arxiv.org/abs/2208.09775v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Representation Learning with Graph Neural Networks for Speech Emotion\n  Recognition", "abstract": "Learning expressive representation is crucial in deep learning. In speech\nemotion recognition (SER), vacuum regions or noises in the speech interfere\nwith expressive representation learning. However, traditional RNN-based models\nare susceptible to such noise. Recently, Graph Neural Network (GNN) has\ndemonstrated its effectiveness for representation learning, and we adopt this\nframework for SER. In particular, we propose a cosine similarity-based graph as\nan ideal graph structure for representation learning in SER. We present a\nCosine similarity-based Graph Convolutional Network (CoGCN) that is robust to\nperturbation and noise. Experimental results show that our method outperforms\nstate-of-the-art methods or provides competitive results with a significant\nmodel size reduction with only 1/30 parameters.", "published": "2022-08-21 07:37:18", "link": "http://arxiv.org/abs/2208.09830v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio Deepfake Attribution: An Initial Dataset and Investigation", "abstract": "The rapid progress of deep speech synthesis models has posed significant\nthreats to society such as malicious manipulation of content. This has led to\nan increase in studies aimed at detecting so-called deepfake audio. However,\nexisting works focus on the binary detection of real audio and fake audio. In\nreal-world scenarios such as model copyright protection and digital evidence\nforensics, binary classification alone is insufficient. It is essential to\nidentify the source of deepfake audio. Therefore, audio deepfake attribution\nhas emerged as a new challenge. To this end, we designed the first deepfake\naudio dataset for the attribution of audio generation tools, called Audio\nDeepfake Attribution (ADA), and conducted a comprehensive investigation on\nsystem fingerprints. To address the challenges of attribution of continuously\nemerging unknown audio generation tools in the real world, we propose the\nClass-Representation Multi-Center Learning (CRML) method for open-set audio\ndeepfake attribution (OSADA). CRML enhances the global directional variation of\nrepresentations, ensuring the learning of discriminative representations with\nstrong intra-class similarity and inter-class discrepancy among known classes.\nFinally, the strong class discrimination capability learned from known classes\nis extended to both known and unknown classes. Experimental results demonstrate\nthat the CRML method effectively addresses open-set risks in real-world\nscenarios. The dataset is publicly available at:\nhttps://zenodo.org/records/13318702, and https://zenodo.org/records/13340666.", "published": "2022-08-21 05:15:40", "link": "http://arxiv.org/abs/2208.10489v4", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improving Speech Emotion Recognition Through Focus and Calibration\n  Attention Mechanisms", "abstract": "Attention has become one of the most commonly used mechanisms in deep\nlearning approaches. The attention mechanism can help the system focus more on\nthe feature space's critical regions. For example, high amplitude regions can\nplay an important role for Speech Emotion Recognition (SER). In this paper, we\nidentify misalignments between the attention and the signal amplitude in the\nexisting multi-head self-attention. To improve the attention area, we propose\nto use a Focus-Attention (FA) mechanism and a novel Calibration-Attention (CA)\nmechanism in combination with the multi-head self-attention. Through the FA\nmechanism, the network can detect the largest amplitude part in the segment. By\nemploying the CA mechanism, the network can modulate the information flow by\nassigning different weights to each attention head and improve the utilization\nof surrounding contexts. To evaluate the proposed method, experiments are\nperformed with the IEMOCAP and RAVDESS datasets. Experimental results show that\nthe proposed framework significantly outperforms the state-of-the-art\napproaches on both datasets.", "published": "2022-08-21 08:04:22", "link": "http://arxiv.org/abs/2208.10491v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
