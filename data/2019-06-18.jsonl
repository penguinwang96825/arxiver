{"title": "Measuring Bias in Contextualized Word Representations", "abstract": "Contextual word embeddings such as BERT have achieved state of the art\nperformance in numerous NLP tasks. Since they are optimized to capture the\nstatistical properties of training data, they tend to pick up on and amplify\nsocial stereotypes present in the data as well. In this study, we (1)~propose a\ntemplate-based method to quantify bias in BERT; (2)~show that this method\nobtains more consistent results in capturing social biases than the traditional\ncosine based method; and (3)~conduct a case study, evaluating gender bias in a\ndownstream task of Gender Pronoun Resolution. Although our case study focuses\non gender bias, the proposed technique is generalizable to unveiling other\nbiases, including in multiclass settings, such as racial and religious biases.", "published": "2019-06-18 01:58:56", "link": "http://arxiv.org/abs/1906.07337v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Curriculum Learning Strategies for Hindi-English Codemixed Sentiment\n  Analysis", "abstract": "Sentiment Analysis and other semantic tasks are commonly used for social\nmedia textual analysis to gauge public opinion and make sense from the noise on\nsocial media. The language used on social media not only commonly diverges from\nthe formal language, but is compounded by codemixing between languages,\nespecially in large multilingual societies like India.\n  Traditional methods for learning semantic NLP tasks have long relied on end\nto end task specific training, requiring expensive data creation process, even\nmore so for deep learning methods. This challenge is even more severe for\nresource scarce texts like codemixed language pairs, with lack of well learnt\nrepresentations as model priors, and task specific datasets can be few and\nsmall in quantities to efficiently exploit recent deep learning approaches. To\naddress above challenges, we introduce curriculum learning strategies for\nsemantic tasks in code-mixed Hindi-English (Hi-En) texts, and investigate\nvarious training strategies for enhancing model performance. Our method\noutperforms the state of the art methods for Hi-En codemixed sentiment analysis\nby 3.31% accuracy, and also shows better model robustness in terms of\nconvergence, and variance in test performance.", "published": "2019-06-18 05:14:17", "link": "http://arxiv.org/abs/1906.07382v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Semantic Relationship in Multi-turn Conversations with\n  Hierarchical Latent Variables", "abstract": "Multi-turn conversations consist of complex semantic structures, and it is\nstill a challenge to generate coherent and diverse responses given previous\nutterances. It's practical that a conversation takes place under a background,\nmeanwhile, the query and response are usually most related and they are\nconsistent in topic but also different in content. However, little work focuses\non such hierarchical relationship among utterances. To address this problem, we\npropose a Conversational Semantic Relationship RNN (CSRR) model to construct\nthe dependency explicitly. The model contains latent variables in three\nhierarchies. The discourse-level one captures the global background, the\npair-level one stands for the common topic information between query and\nresponse, and the utterance-level ones try to represent differences in content.\nExperimental results show that our model significantly improves the quality of\nresponses in terms of fluency, coherence and diversity compared to baseline\nmethods.", "published": "2019-06-18 07:57:22", "link": "http://arxiv.org/abs/1906.07429v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mimicking Human Process: Text Representation via Latent Semantic\n  Clustering for Classification", "abstract": "Considering that words with different characteristic in the text have\ndifferent importance for classification, grouping them together separately can\nstrengthen the semantic expression of each part. Thus we propose a new text\nrepresentation scheme by clustering words according to their latent semantics\nand composing them together to get a set of cluster vectors, which are then\nconcatenated as the final text representation. Evaluation on five\nclassification benchmarks proves the effectiveness of our method. We further\nconduct visualization analysis showing statistical clustering results and\nverifying the validity of our motivation.", "published": "2019-06-18 12:27:25", "link": "http://arxiv.org/abs/1906.07525v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transfer Learning for Causal Sentence Detection", "abstract": "We consider the task of detecting sentences that express causality, as a step\ntowards mining causal relations from texts. To bypass the scarcity of causal\ninstances in relation extraction datasets, we exploit transfer learning, namely\nELMO and BERT, using a bidirectional GRU with self-attention (BIGRUATT) as a\nbaseline. We experiment with both generic public relation extraction datasets\nand a new biomedical causal sentence detection dataset, a subset of which we\nmake publicly available. We find that transfer learning helps only in very\nsmall datasets. With larger datasets, BIGRUATT reaches a performance plateau,\nthen larger datasets and transfer learning do not help.", "published": "2019-06-18 13:17:13", "link": "http://arxiv.org/abs/1906.07544v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic learner summary assessment for reading comprehension", "abstract": "Automating the assessment of learner summaries provides a useful tool for\nassessing learner reading comprehension. We present a summarization task for\nevaluating non-native reading comprehension and propose three novel approaches\nto automatically assess the learner summaries. We evaluate our models on two\ndatasets we created and show that our models outperform traditional approaches\nthat rely on exact word match on this task. Our best model produces quality\nassessments close to professional examiners.", "published": "2019-06-18 13:29:04", "link": "http://arxiv.org/abs/1906.07555v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hyperintensional Reasoning based on Natural Language Knowledge Base", "abstract": "The success of automated reasoning techniques over large natural-language\ntexts heavily relies on a fine-grained analysis of natural language\nassumptions. While there is a common agreement that the analysis should be\nhyperintensional, most of the automatic reasoning systems are still based on an\nintensional logic, at the best. In this paper, we introduce the system of\nreasoning based on a fine-grained, hyperintensional analysis. To this end we\napply Tichy's Transparent Intensional Logic (TIL) with its procedural\nsemantics. TIL is a higher-order, hyperintensional logic of partial functions,\nin particular apt for a fine-grained natural-language analysis. Within TIL we\nrecognise three kinds of context, namely extensional, intensional and\nhyperintensional, in which a particular natural-language term, or rather its\nmeaning, can occur. Having defined the three kinds of context and implemented\nan algorithm of context recognition, we are in a position to develop and\nimplement an extensional logic of hyperintensions with the inference machine\nthat should neither over-infer nor under-infer.", "published": "2019-06-18 13:32:20", "link": "http://arxiv.org/abs/1906.07562v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text Readability Assessment for Second Language Learners", "abstract": "This paper addresses the task of readability assessment for the texts aimed\nat second language (L2) learners. One of the major challenges in this task is\nthe lack of significantly sized level-annotated data. For the present work, we\ncollected a dataset of CEFR-graded texts tailored for learners of English as an\nL2 and investigated text readability assessment for both native and L2\nlearners. We applied a generalization method to adapt models trained on larger\nnative corpora to estimate text readability for learners, and explored domain\nadaptation and self-learning techniques to make use of the native data to\nimprove system performance on the limited L2 data. In our experiments, the best\nperforming model for readability on learner texts achieves an accuracy of 0.797\nand PCC of $0.938$.", "published": "2019-06-18 13:46:21", "link": "http://arxiv.org/abs/1906.07580v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Robust Named Entity Recognition for Historic German", "abstract": "Recent advances in language modeling using deep neural networks have shown\nthat these models learn representations, that vary with the network depth from\nmorphology to semantic relationships like co-reference. We apply pre-trained\nlanguage models to low-resource named entity recognition for Historic German.\nWe show on a series of experiments that character-based pre-trained language\nmodels do not run into trouble when faced with low-resource datasets. Our\npre-trained character-based language models improve upon classical CRF-based\nmethods and previous work on Bi-LSTMs by boosting F1 score performance by up to\n6%. Our pre-trained language and NER models are publicly available under\nhttps://github.com/stefan-it/historic-ner .", "published": "2019-06-18 14:06:40", "link": "http://arxiv.org/abs/1906.07592v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "LTG-Oslo Hierarchical Multi-task Network: The importance of negation for\n  document-level sentiment in Spanish", "abstract": "This paper details LTG-Oslo team's participation in the sentiment track of\nthe NEGES 2019 evaluation campaign. We participated in the task with a\nhierarchical multi-task network, which used shared lower-layers in a deep\nBiLSTM to predict negation, while the higher layers were dedicated to\npredicting document-level sentiment. The multi-task component shows promise as\na way to incorporate information on negation into deep neural sentiment\nclassifiers, despite the fact that the absolute results on the test set were\nrelatively low for a binary classification task.", "published": "2019-06-18 14:18:31", "link": "http://arxiv.org/abs/1906.07599v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Sentiment Analysis with Multi-task Learning of Negation", "abstract": "Sentiment analysis is directly affected by compositional phenomena in\nlanguage that act on the prior polarity of the words and phrases found in the\ntext. Negation is the most prevalent of these phenomena and in order to\ncorrectly predict sentiment, a classifier must be able to identify negation and\ndisentangle the effect that its scope has on the final polarity of a text. This\npaper proposes a multi-task approach to explicitly incorporate information\nabout negation in sentiment analysis, which we show outperforms learning\nnegation implicitly in a data-driven manner. We describe our approach, a\ncascading neural architecture with selective sharing of LSTM layers, and show\nthat explicitly training the model with negation as an auxiliary task helps\nimprove the main task of sentiment analysis. The effect is demonstrated across\nseveral different standard English-language data sets for both tasks and we\nanalyze several aspects of our system related to its performance, varying types\nand amounts of input data and different multi-task setups.", "published": "2019-06-18 14:31:58", "link": "http://arxiv.org/abs/1906.07610v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Scheduled Sampling for Transformers", "abstract": "Scheduled sampling is a technique for avoiding one of the known problems in\nsequence-to-sequence generation: exposure bias. It consists of feeding the\nmodel a mix of the teacher forced embeddings and the model predictions from the\nprevious step in training time. The technique has been used for improving the\nmodel performance with recurrent neural networks (RNN). In the Transformer\nmodel, unlike the RNN, the generation of a new word attends to the full\nsentence generated so far, not only to the last word, and it is not\nstraightforward to apply the scheduled sampling technique. We propose some\nstructural changes to allow scheduled sampling to be applied to Transformer\narchitecture, via a two-pass decoding strategy. Experiments on two language\npairs achieve performance close to a teacher-forcing baseline and show that\nthis technique is promising for further exploration.", "published": "2019-06-18 15:46:08", "link": "http://arxiv.org/abs/1906.07651v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "State-of-the-Art Vietnamese Word Segmentation", "abstract": "Word segmentation is the first step of any tasks in Vietnamese language\nprocessing. This paper reviews stateof-the-art approaches and systems for word\nsegmentation in Vietnamese. To have an overview of all stages from building\ncorpora to developing toolkits, we discuss building the corpus stage,\napproaches applied to solve the word segmentation and existing toolkits to\nsegment words in Vietnamese sentences. In addition, this study shows clearly\nthe motivations on building corpus and implementing machine learning techniques\nto improve the accuracy for Vietnamese word segmentation. According to our\nobservation, this study also reports a few of achivements and limitations in\nexisting Vietnamese word segmentation systems.", "published": "2019-06-18 16:00:56", "link": "http://arxiv.org/abs/1906.07662v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Distilling Translations with Visual Awareness", "abstract": "Previous work on multimodal machine translation has shown that visual\ninformation is only needed in very specific cases, for example in the presence\nof ambiguous words where the textual context is not sufficient. As a\nconsequence, models tend to learn to ignore this information. We propose a\ntranslate-and-refine approach to this problem where images are only used by a\nsecond stage decoder. This approach is trained jointly to generate a good first\ndraft translation and to improve over this draft by (i) making better use of\nthe target language textual context (both left and right-side contexts) and\n(ii) making use of visual context. This approach leads to the state of the art\nresults. Additionally, we show that it has the ability to recover from\nerroneous or missing words in the source language.", "published": "2019-06-18 17:30:30", "link": "http://arxiv.org/abs/1906.07701v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adaptation of Machine Translation Models with Back-translated Data using\n  Transductive Data Selection Methods", "abstract": "Data selection has proven its merit for improving Neural Machine Translation\n(NMT), when applied to authentic data. But the benefit of using synthetic data\nin NMT training, produced by the popular back-translation technique, raises the\nquestion if data selection could also be useful for synthetic data?\n  In this work we use Infrequent N-gram Recovery (INR) and Feature Decay\nAlgorithms (FDA), two transductive data selection methods to obtain subsets of\nsentences from synthetic data. These methods ensure that selected sentences\nshare n-grams with the test set so the NMT model can be adapted to translate\nit.\n  Performing data selection on back-translated data creates new challenges as\nthe source-side may contain noise originated by the model used in the\nback-translation. Hence, finding n-grams present in the test set become more\ndifficult. Despite that, in our work we show that adapting a model with a\nselection of synthetic data is an useful approach.", "published": "2019-06-18 20:56:11", "link": "http://arxiv.org/abs/1906.07808v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Entity Linking by Reading Entity Descriptions", "abstract": "We present the zero-shot entity linking task, where mentions must be linked\nto unseen entities without in-domain labeled data. The goal is to enable robust\ntransfer to highly specialized domains, and so no metadata or alias tables are\nassumed. In this setting, entities are only identified by text descriptions,\nand models must rely strictly on language understanding to resolve the new\nentities. First, we show that strong reading comprehension models pre-trained\non large unlabeled data can be used to generalize to unseen entities. Second,\nwe propose a simple and effective adaptive pre-training strategy, which we term\ndomain-adaptive pre-training (DAP), to address the domain shift problem\nassociated with linking unseen entities in a new domain. We present experiments\non a new dataset that we construct for this task and show that DAP improves\nover strong pre-training baselines, including BERT. The data and code are\navailable at https://github.com/lajanugen/zeshel.", "published": "2019-06-18 02:36:39", "link": "http://arxiv.org/abs/1906.07348v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Attention Guided Graph Convolutional Networks for Relation Extraction", "abstract": "Dependency trees convey rich structural information that is proven useful for\nextracting relations among entities in text. However, how to effectively make\nuse of relevant information while ignoring irrelevant information from the\ndependency trees remains a challenging research question. Existing approaches\nemploying rule based hard-pruning strategies for selecting relevant partial\ndependency structures may not always yield optimal results. In this work, we\npropose Attention Guided Graph Convolutional Networks (AGGCNs), a novel model\nwhich directly takes full dependency trees as inputs. Our model can be\nunderstood as a soft-pruning approach that automatically learns how to\nselectively attend to the relevant sub-structures useful for the relation\nextraction task. Extensive results on various tasks including cross-sentence\nn-ary relation extraction and large-scale sentence-level relation extraction\nshow that our model is able to better leverage the structural information of\nthe full dependency trees, giving significantly better results than previous\napproaches.", "published": "2019-06-18 11:55:16", "link": "http://arxiv.org/abs/1906.07510v8", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Query Generation for Patent Retrieval with Keyword Extraction based on\n  Syntactic Features", "abstract": "This paper describes a new method to extract relevant keywords from patent\nclaims, as part of the task of retrieving other patents with similar claims\n(search for prior art). The method combines a qualitative analysis of the\nwriting style of the claims with NLP methods to parse text, in order to\nrepresent a legal text as a specialization arborescence of terms. In this\nsetting, the set of extracted keywords are yielding better search results than\nkeywords extracted with traditional methods such as tf-idf. The performance is\nmeasured on the search results of a query consisting of the extracted keywords.", "published": "2019-06-18 14:03:14", "link": "http://arxiv.org/abs/1906.07591v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "The Second DIHARD Diarization Challenge: Dataset, task, and baselines", "abstract": "This paper introduces the second DIHARD challenge, the second in a series of\nspeaker diarization challenges intended to improve the robustness of\ndiarization systems to variation in recording equipment, noise conditions, and\nconversational domain. The challenge comprises four tracks evaluating\ndiarization performance under two input conditions (single channel vs.\nmulti-channel) and two segmentation conditions (diarization from a reference\nspeech segmentation vs. diarization from scratch). In order to prevent\nparticipants from overtuning to a particular combination of recording\nconditions and conversational domain, recordings are drawn from a variety of\nsources ranging from read audiobooks to meeting speech, to child language\nacquisition recordings, to dinner parties, to web video. We describe the task\nand metrics, challenge design, datasets, and baseline systems for speech\nenhancement, speech activity detection, and diarization.", "published": "2019-06-18 23:04:09", "link": "http://arxiv.org/abs/1906.07839v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Margin Matters: Towards More Discriminative Deep Neural Network\n  Embeddings for Speaker Recognition", "abstract": "Recently, speaker embeddings extracted from a speaker discriminative deep\nneural network (DNN) yield better performance than the conventional methods\nsuch as i-vector. In most cases, the DNN speaker classifier is trained using\ncross entropy loss with softmax. However, this kind of loss function does not\nexplicitly encourage inter-class separability and intra-class compactness. As a\nresult, the embeddings are not optimal for speaker recognition tasks. In this\npaper, to address this issue, three different margin based losses which not\nonly separate classes but also demand a fixed margin between classes are\nintroduced to deep speaker embedding learning. It could be demonstrated that\nthe margin is the key to obtain more discriminative speaker embeddings.\nExperiments are conducted on two public text independent tasks: VoxCeleb1 and\nSpeaker in The Wild (SITW). The proposed approach can achieve the\nstate-of-the-art performance, with 25% ~ 30% equal error rate (EER) reduction\non both tasks when compared to strong baselines using cross entropy loss with\nsoftmax, obtaining 2.238% EER on VoxCeleb1 test set and 2.761% EER on SITW\ncore-core test set, respectively.", "published": "2019-06-18 00:31:04", "link": "http://arxiv.org/abs/1906.07317v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Language as an Abstraction for Hierarchical Deep Reinforcement Learning", "abstract": "Solving complex, temporally-extended tasks is a long-standing problem in\nreinforcement learning (RL). We hypothesize that one critical element of\nsolving such problems is the notion of compositionality. With the ability to\nlearn concepts and sub-skills that can be composed to solve longer tasks, i.e.\nhierarchical RL, we can acquire temporally-extended behaviors. However,\nacquiring effective yet general abstractions for hierarchical RL is remarkably\nchallenging. In this paper, we propose to use language as the abstraction, as\nit provides unique compositional structure, enabling fast learning and\ncombinatorial generalization, while retaining tremendous flexibility, making it\nsuitable for a variety of problems. Our approach learns an\ninstruction-following low-level policy and a high-level policy that can reuse\nabstractions across tasks, in essence, permitting agents to reason using\nstructured language. To study compositional task learning, we introduce an\nopen-source object interaction environment built using the MuJoCo physics\nengine and the CLEVR engine. We find that, using our approach, agents can learn\nto solve to diverse, temporally-extended tasks such as object sorting and\nmulti-object rearrangement, including from raw pixel observations. Our analysis\nreveals that the compositional nature of language is critical for learning\ndiverse sub-skills and systematically generalizing to new sub-skills in\ncomparison to non-compositional abstractions that use the same supervision.", "published": "2019-06-18 02:27:45", "link": "http://arxiv.org/abs/1906.07343v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Uncovering Probabilistic Implications in Typological Knowledge Bases", "abstract": "The study of linguistic typology is rooted in the implications we find\nbetween linguistic features, such as the fact that languages with object-verb\nword ordering tend to have post-positions. Uncovering such implications\ntypically amounts to time-consuming manual processing by trained and\nexperienced linguists, which potentially leaves key linguistic universals\nunexplored. In this paper, we present a computational model which successfully\nidentifies known universals, including Greenberg universals, but also uncovers\nnew ones, worthy of further linguistic investigation. Our approach outperforms\nbaselines previously used for this problem, as well as a strong baseline from\nknowledge base population.", "published": "2019-06-18 05:51:13", "link": "http://arxiv.org/abs/1906.07389v1", "categories": ["cs.CL", "cs.AI", "stat.ML"], "primary_category": "cs.CL"}
{"title": "A Unified Speaker Adaptation Method for Speech Synthesis using\n  Transcribed and Untranscribed Speech with Backpropagation", "abstract": "By representing speaker characteristic as a single fixed-length vector\nextracted solely from speech, we can train a neural multi-speaker speech\nsynthesis model by conditioning the model on those vectors. This model can also\nbe adapted to unseen speakers regardless of whether the transcript of\nadaptation data is available or not. However, this setup restricts the speaker\ncomponent to just a single bias vector, which in turn limits the performance of\nadaptation process. In this study, we propose a novel speech synthesis model,\nwhich can be adapted to unseen speakers by fine-tuning part of or all of the\nnetwork using either transcribed or untranscribed speech. Our methodology\nessentially consists of two steps: first, we split the conventional acoustic\nmodel into a speaker-independent (SI) linguistic encoder and a speaker-adaptive\n(SA) acoustic decoder; second, we train an auxiliary acoustic encoder that can\nbe used as a substitute for the linguistic encoder whenever linguistic features\nare unobtainable. The results of objective and subjective evaluations show that\nadaptation using either transcribed or untranscribed speech with our\nmethodology achieved a reasonable level of performance with an extremely\nlimited amount of data and greatly improved performance with more data.\nSurprisingly, adaptation with untranscribed speech surpassed the transcribed\ncounterpart in the subjective test, which reveals the limitations of the\nconventional acoustic model and hints at potential directions for improvements.", "published": "2019-06-18 07:24:45", "link": "http://arxiv.org/abs/1906.07414v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multi-Graph Decoding for Code-Switching ASR", "abstract": "In the FAME! Project, a code-switching (CS) automatic speech recognition\n(ASR) system for Frisian-Dutch speech is developed that can accurately\ntranscribe the local broadcaster's bilingual archives with CS speech. This\narchive contains recordings with monolingual Frisian and Dutch speech segments\nas well as Frisian-Dutch CS speech, hence the recognition performance on\nmonolingual segments is also vital for accurate transcriptions. In this work,\nwe propose a multi-graph decoding and rescoring strategy using bilingual and\nmonolingual graphs together with a unified acoustic model for CS ASR. The\nproposed decoding scheme gives the freedom to design and employ alternative\nsearch spaces for each (monolingual or bilingual) recognition task and enables\nthe effective use of monolingual resources of the high-resourced mixed language\nin low-resourced CS scenarios. In our scenario, Dutch is the high-resourced and\nFrisian is the low-resourced language. We therefore use additional monolingual\nDutch text resources to improve the Dutch language model (LM) and compare the\nperformance of single- and multi-graph CS ASR systems on Dutch segments using\nlarger Dutch LMs. The ASR results show that the proposed approach outperforms\nbaseline single-graph CS ASR systems, providing better performance on the\nmonolingual Dutch segments without any accuracy loss on monolingual Frisian and\ncode-mixed segments.", "published": "2019-06-18 12:24:32", "link": "http://arxiv.org/abs/1906.07523v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Curriculum-based transfer learning for an effective end-to-end spoken\n  language understanding and domain portability", "abstract": "We present an end-to-end approach to extract semantic concepts directly from\nthe speech audio signal. To overcome the lack of data available for this spoken\nlanguage understanding approach, we investigate the use of a transfer learning\nstrategy based on the principles of curriculum learning. This approach allows\nus to exploit out-of-domain data that can help to prepare a fully neural\narchitecture. Experiments are carried out on the French MEDIA and PORTMEDIA\ncorpora and show that this end-to-end SLU approach reaches the best results\never published on this task. We compare our approach to a classical pipeline\napproach that uses ASR, POS tagging, lemmatizer, chunker... and other NLP tools\nthat aim to enrich ASR outputs that feed an SLU text to concepts system. Last,\nwe explore the promising capacity of our end-to-end SLU approach to address the\nproblem of domain portability.", "published": "2019-06-18 14:19:52", "link": "http://arxiv.org/abs/1906.07601v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Expressing Visual Relationships via Language", "abstract": "Describing images with text is a fundamental problem in vision-language\nresearch. Current studies in this domain mostly focus on single image\ncaptioning. However, in various real applications (e.g., image editing,\ndifference interpretation, and retrieval), generating relational captions for\ntwo images, can also be very useful. This important problem has not been\nexplored mostly due to lack of datasets and effective models. To push forward\nthe research in this direction, we first introduce a new language-guided image\nediting dataset that contains a large number of real image pairs with\ncorresponding editing instructions. We then propose a new relational speaker\nmodel based on an encoder-decoder architecture with static relational attention\nand sequential multi-head attention. We also extend the model with dynamic\nrelational attention, which calculates visual alignment while decoding. Our\nmodels are evaluated on our newly collected and two public datasets consisting\nof image pairs annotated with relationship sentences. Experimental results,\nbased on both automatic and human evaluation, demonstrate that our model\noutperforms all baselines and existing methods on all the datasets.", "published": "2019-06-18 17:01:21", "link": "http://arxiv.org/abs/1906.07689v2", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Square root-based multi-source early PSD estimation and recursive RETF\n  update in reverberant environments by means of the orthogonal Procrustes\n  problem", "abstract": "Multi-channel short-time Fourier transform (STFT) domain-based processing of\nreverberant microphone signals commonly relies on power-spectral-density (PSD)\nestimates of early source images, where early refers to reflections contained\nwithin the same STFT frame. State-of-the-art approaches to multi-source early\nPSD estimation, given an estimate of the associated relative early transfer\nfunctions (RETFs), conventionally minimize the approximation error defined with\nrespect to the early correlation matrix, requiring non-negative inequality\nconstraints on the PSDs. Instead, we here propose to factorize the early\ncorrelation matrix and minimize the approximation error defined with respect to\nthe early-correlation-matrix square root. The proposed minimization problem --\nconstituting a generalization of the so-called orthogonal Procrustes problem --\nseeks a unitary matrix and the square roots of the early PSDs up to an\narbitrary complex argument, making non-negative inequality constraints\nredundant. A solution is obtained iteratively, requiring one singular value\ndecomposition (SVD) per iteration. The estimated unitary matrix and early PSD\nsquare roots further allow to recursively update the RETF estimate, which is\nnot inherently possible in the conventional approach. An estimate of the said\nearly-correlation-matrix square root itself is obtained by means of the\ngeneralized eigenvalue decomposition (GEVD), where we further propose to\nrestore non-stationarities by desmoothing the generalized eigenvalues in order\nto compensate for inevitable recursive averaging. Simulation results indicate\nfast convergence of the proposed multi-source early PSD estimation approach in\nonly one iteration if initialized appropriately, and better performance as\ncompared to the conventional approach.", "published": "2019-06-18 11:02:29", "link": "http://arxiv.org/abs/1906.07493v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Integrated sidelobe cancellation and linear prediction Kalman filter for\n  joint multi-microphone speech dereverberation, interfering speech\n  cancellation, and noise reduction", "abstract": "In multi-microphone speech enhancement, reverberation as well as additive\nnoise and/or interfering speech are commonly suppressed by deconvolution and\nspatial filtering, e.g., using multi-channel linear prediction (MCLP) on the\none hand and beamforming, e.g., a generalized sidelobe canceler (GSC), on the\nother hand. In this paper, we consider several reverberant speech components,\nwhereof some are to be dereverberated and others to be canceled, as well as a\ndiffuse (e.g., babble) noise component to be suppressed. In order to perform\nboth deconvolution and spatial filtering, we integrate MCLP and the GSC into a\nnovel architecture referred to as integrated sidelobe cancellation and linear\nprediction (ISCLP), where the sidelobe-cancellation (SC) filter and the linear\nprediction (LP) filter operate in parallel, but on different microphone signal\nframes. Within ISCLP, we estimate both filters jointly by means of a single\nKalman filter. We further propose a spectral Wiener gain post-processor, which\nis shown to relate to the Kalman filter's posterior state estimate. The\npresented ISCLP Kalman filter is benchmarked against two state-of-the-art\napproaches, namely first a pair of alternating Kalman filters respectively\nperforming dereverberation and noise reduction, and second an MCLP+GSC Kalman\nfilter cascade. While the ISCLP Kalman filter is roughly $M^2$ times less\nexpensive than both reference algorithms, where $M$ denotes the number of\nmicrophones, it is shown to perform similarly as compared to the former, and to\noutperform the latter.", "published": "2019-06-18 11:58:30", "link": "http://arxiv.org/abs/1906.07512v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Deep Xi as a Front-End for Robust Automatic Speech Recognition", "abstract": "Current front-ends for robust automatic speech recognition(ASR) include\nmasking- and mapping-based deep learning approaches to speech enhancement. A\nrecently proposed deep learning approach toa prioriSNR estimation, called\nDeepXi, was able to produce enhanced speech at a higher quality and\nintelligibility than current masking- and mapping-based approaches. Motivated\nby this, we investigate Deep Xi as a front-end for robust ASR. Deep Xi is\nevaluated using real-world non-stationary and coloured noise sources at\nmultiple SNR levels. Our experimental investigation shows that DeepXi as a\nfront-end is able to produce a lower word error rate than recent masking- and\nmapping-based deep learning front-ends. The results presented in this work show\nthat Deep Xi is a viable front-end, and is able to significantly increase the\nrobustness of an ASR system. Availability: Deep Xi is available\nat:https://github.com/anicolson/DeepXi", "published": "2019-06-18 00:41:42", "link": "http://arxiv.org/abs/1906.07319v2", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Cascaded Cross-Module Residual Learning towards Lightweight End-to-End\n  Speech Coding", "abstract": "Speech codecs learn compact representations of speech signals to facilitate\ndata transmission. Many recent deep neural network (DNN) based end-to-end\nspeech codecs achieve low bitrates and high perceptual quality at the cost of\nmodel complexity. We propose a cross-module residual learning (CMRL) pipeline\nas a module carrier with each module reconstructing the residual from its\npreceding modules. CMRL differs from other DNN-based speech codecs, in that\nrather than modeling speech compression problem in a single large neural\nnetwork, it optimizes a series of less-complicated modules in a two-phase\ntraining scheme. The proposed method shows better objective performance than\nAMR-WB and the state-of-the-art DNN-based speech codec with a similar network\narchitecture. As an end-to-end model, it takes raw PCM signals as an input, but\nis also compatible with linear predictive coding (LPC), showing better\nsubjective quality at high bitrates than AMR-WB and OPUS. The gain is achieved\nby using only 0.9 million trainable parameters, a significantly less complex\narchitecture than the other DNN-based codecs in the literature.", "published": "2019-06-18 19:11:14", "link": "http://arxiv.org/abs/1906.07769v4", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
