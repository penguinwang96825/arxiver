{"title": "Sequence-to-Sequence Neural Net Models for Grapheme-to-Phoneme\n  Conversion", "abstract": "Sequence-to-sequence translation methods based on generation with a\nside-conditioned language model have recently shown promising results in\nseveral tasks. In machine translation, models conditioned on source side words\nhave been used to produce target-language text, and in image captioning, models\nconditioned images have been used to generate caption text. Past work with this\napproach has focused on large vocabulary tasks, and measured quality in terms\nof BLEU. In this paper, we explore the applicability of such models to the\nqualitatively different grapheme-to-phoneme task. Here, the input and output\nside vocabularies are small, plain n-gram models do well, and credit is only\ngiven when the output is exactly correct. We find that the simple\nside-conditioned generation approach is able to rival the state-of-the-art, and\nwe are able to significantly advance the stat-of-the-art with bi-directional\nlong short-term memory (LSTM) neural networks that use the same alignment\ninformation that is used in conventional approaches.", "published": "2015-05-31 05:14:06", "link": "http://arxiv.org/abs/1506.00196v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Diversity in Spectral Learning for Natural Language Parsing", "abstract": "We describe an approach to create a diverse set of predictions with spectral\nlearning of latent-variable PCFGs (L-PCFGs). Our approach works by creating\nmultiple spectral models where noise is added to the underlying features in the\ntraining set before the estimation of each model. We describe three ways to\ndecode with multiple models. In addition, we describe a simple variant of the\nspectral algorithm for L-PCFGs that is fast and leads to compact models. Our\nexperiments for natural language parsing, for English and German, show that we\nget a significant improvement over baselines comparable to state of the art.\nFor English, we achieve the $F_1$ score of 90.18, and for German we achieve the\n$F_1$ score of 83.38.", "published": "2015-05-31 19:21:26", "link": "http://arxiv.org/abs/1506.00275v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Visual Madlibs: Fill in the blank Image Generation and Question\n  Answering", "abstract": "In this paper, we introduce a new dataset consisting of 360,001 focused\nnatural language descriptions for 10,738 images. This dataset, the Visual\nMadlibs dataset, is collected using automatically produced fill-in-the-blank\ntemplates designed to gather targeted descriptions about: people and objects,\ntheir appearances, activities, and interactions, as well as inferences about\nthe general scene or its broader context. We provide several analyses of the\nVisual Madlibs dataset and demonstrate its applicability to two new description\ngeneration tasks: focused description generation, and multiple-choice\nquestion-answering for images. Experiments using joint-embedding and deep\nlearning methods show promising results on these tasks.", "published": "2015-05-31 19:39:44", "link": "http://arxiv.org/abs/1506.00278v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Interactive Knowledge Base Population", "abstract": "Most work on building knowledge bases has focused on collecting entities and\nfacts from as large a collection of documents as possible. We argue for and\ndescribe a new paradigm where the focus is on a high-recall extraction over a\nsmall collection of documents under the supervision of a human expert, that we\ncall Interactive Knowledge Base Population (IKBP).", "published": "2015-05-31 22:56:43", "link": "http://arxiv.org/abs/1506.00301v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Recurrent Neural Networks with External Memory for Language\n  Understanding", "abstract": "Recurrent Neural Networks (RNNs) have become increasingly popular for the\ntask of language understanding. In this task, a semantic tagger is deployed to\nassociate a semantic label to each word in an input sequence. The success of\nRNN may be attributed to its ability to memorize long-term dependence that\nrelates the current-time semantic label prediction to the observations many\ntime instances away. However, the memory capacity of simple RNNs is limited\nbecause of the gradient vanishing and exploding problem. We propose to use an\nexternal memory to improve memorization capability of RNNs. We conducted\nexperiments on the ATIS dataset, and observed that the proposed model was able\nto achieve the state-of-the-art results. We compare our proposed model with\nalternative models and report analysis results that may provide insights for\nfuture research.", "published": "2015-05-31 05:10:03", "link": "http://arxiv.org/abs/1506.00195v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
