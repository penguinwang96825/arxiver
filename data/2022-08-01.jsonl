{"title": "DictBERT: Dictionary Description Knowledge Enhanced Language Model\n  Pre-training via Contrastive Learning", "abstract": "Although pre-trained language models (PLMs) have achieved state-of-the-art\nperformance on various natural language processing (NLP) tasks, they are shown\nto be lacking in knowledge when dealing with knowledge driven tasks. Despite\nthe many efforts made for injecting knowledge into PLMs, this problem remains\nopen. To address the challenge, we propose \\textbf{DictBERT}, a novel approach\nthat enhances PLMs with dictionary knowledge which is easier to acquire than\nknowledge graph (KG). During pre-training, we present two novel pre-training\ntasks to inject dictionary knowledge into PLMs via contrastive learning:\n\\textit{dictionary entry prediction} and \\textit{entry description\ndiscrimination}. In fine-tuning, we use the pre-trained DictBERT as a plugin\nknowledge base (KB) to retrieve implicit knowledge for identified entries in an\ninput sequence, and infuse the retrieved knowledge into the input to enhance\nits representation via a novel extra-hop attention mechanism. We evaluate our\napproach on a variety of knowledge driven and language understanding tasks,\nincluding NER, relation extraction, CommonsenseQA, OpenBookQA and GLUE.\nExperimental results demonstrate that our model can significantly improve\ntypical PLMs: it gains a substantial improvement of 0.5\\%, 2.9\\%, 9.0\\%, 7.1\\%\nand 3.3\\% on BERT-large respectively, and is also effective on RoBERTa-large.", "published": "2022-08-01 06:43:19", "link": "http://arxiv.org/abs/2208.00635v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "giMLPs: Gate with Inhibition Mechanism in MLPs", "abstract": "This paper presents a new model architecture, gate with inhibition MLP\n(giMLP).The gate with inhibition on CycleMLP (gi-CycleMLP) can produce equal\nperformance on the ImageNet classification task, and it also improves the BERT,\nRoberta, and DeBERTaV3 models depending on two novel techniques. The first is\nthe gating MLP, where matrix multiplications between the MLP and the trunk\nAttention input in further adjust models' adaptation. The second is inhibition\nwhich inhibits or enhances the branch adjustment, and with the inhibition\nlevels increasing, it offers models more muscular features restriction. We show\nthat the giCycleMLP with a lower inhibition level can be competitive with the\noriginal CycleMLP in terms of ImageNet classification accuracy. In addition, we\nalso show through a comprehensive empirical study that these techniques\nsignificantly improve the performance of fine-tuning NLU downstream tasks. As\nfor the gate with inhibition MLPs on DeBERTa (giDeBERTa) fine-tuning, we find\nit can achieve appealing results on most parts of NLU tasks without any extra\npretraining again. We also find that with the use of Gate With Inhibition, the\nactivation function should have a short and smooth negative tail, with which\nthe unimportant features or the features that hurt models can be moderately\ninhibited. The experiments on ImageNet and twelve language downstream tasks\ndemonstrate the effectiveness of Gate With Inhibition, both for image\nclassification and for enhancing the capacity of nature language fine-tuning\nwithout any extra pretraining.", "published": "2022-08-01 15:23:51", "link": "http://arxiv.org/abs/2208.00929v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Masader Plus: A New Interface for Exploring +500 Arabic NLP Datasets", "abstract": "Masader (Alyafeai et al., 2021) created a metadata structure to be used for\ncataloguing Arabic NLP datasets. However, developing an easy way to explore\nsuch a catalogue is a challenging task. In order to give the optimal experience\nfor users and researchers exploring the catalogue, several design and user\nexperience challenges must be resolved. Furthermore, user interactions with the\nwebsite may provide an easy approach to improve the catalogue. In this paper,\nwe introduce Masader Plus, a web interface for users to browse Masader. We\ndemonstrate data exploration, filtration, and a simple API that allows users to\nexamine datasets from the backend. Masader Plus can be explored using this link\nhttps://arbml.github.io/masader. A video recording explaining the interface can\nbe found here https://www.youtube.com/watch?v=SEtdlSeqchk.", "published": "2022-08-01 15:31:56", "link": "http://arxiv.org/abs/2208.00932v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Document Summarization with Centroid-Based Pretraining", "abstract": "In Multi-Document Summarization (MDS), the input can be modeled as a set of\ndocuments, and the output is its summary. In this paper, we focus on\npretraining objectives for MDS. Specifically, we introduce a novel pretraining\nobjective, which involves selecting the ROUGE-based centroid of each document\ncluster as a proxy for its summary. Our objective thus does not require human\nwritten summaries and can be utilized for pretraining on a dataset consisting\nsolely of document sets. Through zero-shot, few-shot, and fully supervised\nexperiments on multiple MDS datasets, we show that our model Centrum is better\nor comparable to a state-of-the-art model. We make the pretrained and\nfine-tuned models freely available to the research community\nhttps://github.com/ratishsp/centrum.", "published": "2022-08-01 17:28:02", "link": "http://arxiv.org/abs/2208.01006v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Massively Multilingual Lexical Specialization of Multilingual\n  Transformers", "abstract": "While pretrained language models (PLMs) primarily serve as general-purpose\ntext encoders that can be fine-tuned for a wide variety of downstream tasks,\nrecent work has shown that they can also be rewired to produce high-quality\nword representations (i.e., static word embeddings) and yield good performance\nin type-level lexical tasks. While existing work primarily focused on the\nlexical specialization of monolingual PLMs with immense quantities of\nmonolingual constraints, in this work we expose massively multilingual\ntransformers (MMTs, e.g., mBERT or XLM-R) to multilingual lexical knowledge at\nscale, leveraging BabelNet as the readily available rich source of multilingual\nand cross-lingual type-level lexical knowledge. Concretely, we use BabelNet's\nmultilingual synsets to create synonym pairs (or synonym-gloss pairs) across 50\nlanguages and then subject the MMTs (mBERT and XLM-R) to a lexical\nspecialization procedure guided by a contrastive objective. We show that such\nmassively multilingual lexical specialization brings substantial gains in two\nstandard cross-lingual lexical tasks, bilingual lexicon induction and\ncross-lingual word similarity, as well as in cross-lingual sentence retrieval.\nCrucially, we observe gains for languages unseen in specialization, indicating\nthat multilingual lexical specialization enables generalization to languages\nwith no lexical constraints. In a series of subsequent controlled experiments,\nwe show that the number of specialization constraints plays a much greater role\nthan the set of languages from which they originate.", "published": "2022-08-01 17:47:03", "link": "http://arxiv.org/abs/2208.01018v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Limitations of Sociodemographic Adaptation with Transformers", "abstract": "Sociodemographic factors (e.g., gender or age) shape our language. Previous\nwork showed that incorporating specific sociodemographic factors can\nconsistently improve performance for various NLP tasks in traditional NLP\nmodels. We investigate whether these previous findings still hold with\nstate-of-the-art pretrained Transformers. We use three common specialization\nmethods proven effective for incorporating external knowledge into pretrained\nTransformers (e.g., domain-specific or geographic knowledge). We adapt the\nlanguage representations for the sociodemographic dimensions of gender and age,\nusing continuous language modeling and dynamic multi-task learning for\nadaptation, where we couple language modeling with the prediction of a\nsociodemographic class. Our results when employing a multilingual model show\nsubstantial performance gains across four languages (English, German, French,\nand Danish). These findings are in line with the results of previous work and\nhold promise for successful sociodemographic specialization. However,\ncontrolling for confounding factors like domain and language shows that, while\nsociodemographic adaptation does improve downstream performance, the gains do\nnot always solely stem from sociodemographic knowledge. Our results indicate\nthat sociodemographic specialization, while very important, is still an\nunresolved problem in NLP.", "published": "2022-08-01 17:58:02", "link": "http://arxiv.org/abs/2208.01029v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SMART: Sentences as Basic Units for Text Evaluation", "abstract": "Widely used evaluation metrics for text generation either do not work well\nwith longer texts or fail to evaluate all aspects of text quality. In this\npaper, we introduce a new metric called SMART to mitigate such limitations.\nSpecifically, We treat sentences as basic units of matching instead of tokens,\nand use a sentence matching function to soft-match candidate and reference\nsentences. Candidate sentences are also compared to sentences in the source\ndocuments to allow grounding (e.g., factuality) evaluation. Our results show\nthat system-level correlations of our proposed metric with a model-based\nmatching function outperforms all competing metrics on the SummEval\nsummarization meta-evaluation dataset, while the same metric with a\nstring-based matching function is competitive with current model-based metrics.\nThe latter does not use any neural model, which is useful during model\ndevelopment phases where resources can be limited and fast evaluation is\nrequired. Finally, we also conducted extensive analyses showing that our\nproposed metrics work well with longer summaries and are less biased towards\nspecific models.", "published": "2022-08-01 17:58:05", "link": "http://arxiv.org/abs/2208.01030v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Parsimonious Argument Annotations for Hate Speech Counter-narratives", "abstract": "We present an enrichment of the Hateval corpus of hate speech tweets (Basile\net. al 2019) aimed to facilitate automated counter-narrative generation.\nComparably to previous work (Chung et. al. 2019), manually written\ncounter-narratives are associated to tweets. However, this information alone\nseems insufficient to obtain satisfactory language models for counter-narrative\ngeneration. That is why we have also annotated tweets with argumentative\ninformation based on Wagemanns (2016), that we believe can help in building\nconvincing and effective counter-narratives for hate speech against particular\ngroups.\n  We discuss adequacies and difficulties of this annotation process and present\nseveral baselines for automatic detection of the annotated elements.\nPreliminary results show that automatic annotators perform close to human\nannotators to detect some aspects of argumentation, while others only reach low\nor moderate level of inter-annotator agreement.", "published": "2022-08-01 18:58:32", "link": "http://arxiv.org/abs/2208.01099v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Composable Text Controls in Latent Space with ODEs", "abstract": "Real-world text applications often involve composing a wide range of text\ncontrol operations, such as editing the text w.r.t. an attribute, manipulating\nkeywords and structure, and generating new text of desired properties. Prior\nwork typically learns/finetunes a language model (LM) to perform individual or\nspecific subsets of operations. Recent research has studied combining\noperations in a plug-and-play manner, often with costly search or optimization\nin the complex sequence space. This paper proposes a new efficient approach for\ncomposable text operations in the compact latent space of text. The\nlow-dimensionality and differentiability of the text latent vector allow us to\ndevelop an efficient sampler based on ordinary differential equations (ODEs)\ngiven arbitrary plug-in operators (e.g., attribute classifiers). By connecting\npretrained LMs (e.g., GPT2) to the latent space through efficient adaption, we\nthen decode the sampled vectors into desired text sequences. The flexible\napproach permits diverse control operators (sentiment, tense, formality,\nkeywords, etc.) acquired using any relevant data from different domains.\nExperiments show that composing those operators within our approach manages to\ngenerate or edit high-quality text, substantially improving over previous\nmethods in terms of generation quality and efficiency.", "published": "2022-08-01 06:51:45", "link": "http://arxiv.org/abs/2208.00638v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Data Collection and Analysis of French Dialects", "abstract": "This paper discusses creating and analysing a new dataset for data mining and\ntext analytics research, contributing to a joint Leeds University research\nproject for the Corpus of National Dialects. This report investigates machine\nlearning classifiers to classify samples of French dialect text across various\nFrench-speaking countries. Following the steps of the CRISP-DM methodology,\nthis report explores the data collection process, data quality issues and data\nconversion for text analysis. Finally, after applying suitable data mining\ntechniques, the evaluation methods, best overall features and classifiers and\nconclusions are discussed.", "published": "2022-08-01 11:21:17", "link": "http://arxiv.org/abs/2208.00752v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning from flowsheets: A generative transformer model for\n  autocompletion of flowsheets", "abstract": "We propose a novel method enabling autocompletion of chemical flowsheets.\nThis idea is inspired by the autocompletion of text. We represent flowsheets as\nstrings using the text-based SFILES 2.0 notation and learn the grammatical\nstructure of the SFILES 2.0 language and common patterns in flowsheets using a\ntransformer-based language model. We pre-train our model on synthetically\ngenerated flowsheets to learn the flowsheet language grammar. Then, we\nfine-tune our model in a transfer learning step on real flowsheet topologies.\nFinally, we use the trained model for causal language modeling to autocomplete\nflowsheets. Eventually, the proposed method can provide chemical engineers with\nrecommendations during interactive flowsheet synthesis. The results demonstrate\na high potential of this approach for future AI-assisted process synthesis.", "published": "2022-08-01 13:43:58", "link": "http://arxiv.org/abs/2208.00859v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "What Can Transformers Learn In-Context? A Case Study of Simple Function\n  Classes", "abstract": "In-context learning refers to the ability of a model to condition on a prompt\nsequence consisting of in-context examples (input-output pairs corresponding to\nsome task) along with a new query input, and generate the corresponding output.\nCrucially, in-context learning happens only at inference time without any\nparameter updates to the model. While large language models such as GPT-3\nexhibit some ability to perform in-context learning, it is unclear what the\nrelationship is between tasks on which this succeeds and what is present in the\ntraining data. To make progress towards understanding in-context learning, we\nconsider the well-defined problem of training a model to in-context learn a\nfunction class (e.g., linear functions): that is, given data derived from some\nfunctions in the class, can we train a model to in-context learn \"most\"\nfunctions from this class? We show empirically that standard Transformers can\nbe trained from scratch to perform in-context learning of linear functions --\nthat is, the trained model is able to learn unseen linear functions from\nin-context examples with performance comparable to the optimal least squares\nestimator. In fact, in-context learning is possible even under two forms of\ndistribution shift: (i) between the training data of the model and\ninference-time prompts, and (ii) between the in-context examples and the query\ninput during inference. We also show that we can train Transformers to\nin-context learn more complex function classes -- namely sparse linear\nfunctions, two-layer neural networks, and decision trees -- with performance\nthat matches or exceeds task-specific learning algorithms. Our code and models\nare available at https://github.com/dtsip/in-context-learning .", "published": "2022-08-01 18:01:40", "link": "http://arxiv.org/abs/2208.01066v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Global Performance Disparities Between English-Language Accents in\n  Automatic Speech Recognition", "abstract": "Past research has identified discriminatory automatic speech recognition\n(ASR) performance as a function of the racial group and nationality of the\nspeaker. In this paper, we expand the discussion beyond bias as a function of\nthe individual national origin of the speaker to look for bias as a function of\nthe geopolitical orientation of their nation of origin. We audit some of the\nmost popular English language ASR services using a large and global data set of\nspeech from The Speech Accent Archive, which includes over 2,700 speakers of\nEnglish born in 171 different countries. We show that, even when controlling\nfor multiple linguistic covariates, ASR service performance has a statistically\nsignificant relationship to the political alignment of the speaker's birth\ncountry with respect to the United States' geopolitical power. This holds for\nall ASR services tested. We discuss this bias in the context of the historical\nuse of language to maintain global and political power.", "published": "2022-08-01 22:10:21", "link": "http://arxiv.org/abs/2208.01157v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "TextWorldExpress: Simulating Text Games at One Million Steps Per Second", "abstract": "Text-based games offer a challenging test bed to evaluate virtual agents at\nlanguage understanding, multi-step problem-solving, and common-sense reasoning.\nHowever, speed is a major limitation of current text-based games, capping at\n300 steps per second, mainly due to the use of legacy tooling. In this work we\npresent TextWorldExpress, a high-performance simulator that includes\nimplementations of three common text game benchmarks that increases simulation\nthroughput by approximately three orders of magnitude, reaching over one\nmillion steps per second on common desktop hardware. This significantly reduces\nexperiment runtime, enabling billion-step-scale experiments in about one day.", "published": "2022-08-01 23:43:48", "link": "http://arxiv.org/abs/2208.01174v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Generative Bias for Robust Visual Question Answering", "abstract": "The task of Visual Question Answering (VQA) is known to be plagued by the\nissue of VQA models exploiting biases within the dataset to make its final\nprediction. Various previous ensemble based debiasing methods have been\nproposed where an additional model is purposefully trained to be biased in\norder to train a robust target model. However, these methods compute the bias\nfor a model simply from the label statistics of the training data or from\nsingle modal branches. In this work, in order to better learn the bias a target\nVQA model suffers from, we propose a generative method to train the bias model\ndirectly from the target model, called GenB. In particular, GenB employs a\ngenerative network to learn the bias in the target model through a combination\nof the adversarial objective and knowledge distillation. We then debias our\ntarget model with GenB as a bias model, and show through extensive experiments\nthe effects of our method on various VQA bias datasets including VQA-CP2,\nVQA-CP1, GQA-OOD, and VQA-CE, and show state-of-the-art results with the LXMERT\narchitecture on VQA-CP2.", "published": "2022-08-01 08:58:02", "link": "http://arxiv.org/abs/2208.00690v3", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Efficient Long-Text Understanding with Short-Text Models", "abstract": "Transformer-based pretrained language models (LMs) are ubiquitous across\nnatural language understanding, but cannot be applied to long sequences such as\nstories, scientific articles and long documents, due to their quadratic\ncomplexity. While a myriad of efficient transformer variants have been\nproposed, they are typically based on custom implementations that require\nexpensive pretraining from scratch. In this work, we propose SLED:\nSLiding-Encoder and Decoder, a simple approach for processing long sequences\nthat re-uses and leverages battle-tested short-text pretrained LMs.\nSpecifically, we partition the input into overlapping chunks, encode each with\na short-text LM encoder and use the pretrained decoder to fuse information\nacross chunks (fusion-in-decoder). We illustrate through controlled experiments\nthat SLED offers a viable strategy for long text understanding and evaluate our\napproach on SCROLLS, a benchmark with seven datasets across a wide range of\nlanguage understanding tasks. We find that SLED is competitive with specialized\nmodels that are up to 50x larger and require a dedicated and expensive\npretraining step.", "published": "2022-08-01 11:14:39", "link": "http://arxiv.org/abs/2208.00748v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Few-shot Adaptation Works with UnpredicTable Data", "abstract": "Prior work on language models (LMs) shows that training on a large number of\ndiverse tasks improves few-shot learning (FSL) performance on new tasks. We\ntake this to the extreme, automatically extracting 413,299 tasks from internet\ntables - orders of magnitude more than the next-largest public datasets.\nFinetuning on the resulting dataset leads to improved FSL performance on\nNatural Language Processing (NLP) tasks, but not proportionally to dataset\nscale. In fact, we find that narrow subsets of our dataset sometimes outperform\nmore diverse datasets. For example, finetuning on software documentation from\nsupport.google.com raises FSL performance by a mean of +7.5% on 52 downstream\ntasks, which beats training on 40 human-curated NLP datasets (+6.7%).\nFinetuning on various narrow datasets leads to similar broad improvements\nacross test tasks, suggesting that the gains are not from domain adaptation but\nadapting to FSL in general. We do not observe clear patterns between the\ndatasets that lead to FSL gains, leaving open questions about why certain data\nhelps with FSL.", "published": "2022-08-01 17:35:25", "link": "http://arxiv.org/abs/2208.01009v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Patents Phrase to Phrase Semantic Matching Dataset", "abstract": "There are many general purpose benchmark datasets for Semantic Textual\nSimilarity but none of them are focused on technical concepts found in patents\nand scientific publications. This work aims to fill this gap by presenting a\nnew human rated contextual phrase to phrase matching dataset. The entire\ndataset contains close to $50,000$ rated phrase pairs, each with a CPC\n(Cooperative Patent Classification) class as a context. This paper describes\nthe dataset and some baseline models.", "published": "2022-08-01 23:33:30", "link": "http://arxiv.org/abs/2208.01171v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Amino Acid Classification in 2D NMR Spectra via Acoustic Signal\n  Embeddings", "abstract": "Nuclear Magnetic Resonance (NMR) is used in structural biology to\nexperimentally determine the structure of proteins, which is used in many areas\nof biology and is an important part of drug development. Unfortunately, NMR\ndata can cost thousands of dollars per sample to collect and it can take a\nspecialist weeks to assign the observed resonances to specific chemical groups.\nThere has thus been growing interest in the NMR community to use deep learning\nto automate NMR data annotation. Due to similarities between NMR and audio\ndata, we propose that methods used in acoustic signal processing can be applied\nto NMR as well. Using a simulated amino acid dataset, we show that by swapping\nout filter banks with a trainable convolutional encoder, acoustic signal\nembeddings from speaker verification models can be used for amino acid\nclassification in 2D NMR spectra by treating each amino acid as a unique\nspeaker. On an NMR dataset comparable in size with of 46 hours of audio, we\nachieve a classification performance of 97.7% on a 20-class problem. We also\nachieve a 23% relative improvement by using an acoustic embedding model\ncompared to an existing NMR-based model.", "published": "2022-08-01 15:36:22", "link": "http://arxiv.org/abs/2208.00935v1", "categories": ["q-bio.QM", "eess.AS"], "primary_category": "q-bio.QM"}
{"title": "DENT-DDSP: Data-efficient noisy speech generator using differentiable\n  digital signal processors for explicit distortion modelling and noise-robust\n  speech recognition", "abstract": "The performances of automatic speech recognition (ASR) systems degrade\ndrastically under noisy conditions. Explicit distortion modelling (EDM), as a\nfeature compensation step, is able to enhance ASR systems under such conditions\nby simulating the in-domain noisy speeches from the clean counterparts. Yet,\nexisting distortion models are either non-trainable or unexplainable and often\nlack controllability and generalization ability. In this paper, we propose a\nfully explainable and controllable model: DENT-DDSP to achieve EDM. DENT-DDSP\nutilizes novel differentiable digital signal processing (DDSP) components and\nrequires only 10 seconds of training data to achieve high fidelity. The\nexperiment shows that the simulated noisy data from DENT-DDSP achieves the\nhighest simulation fidelity compared to other baseline models in terms of\nmulti-scale spectral loss (MSSL). Moreover, to validate whether the data\nsimulated by DENT-DDSP are able to replace the scarce in-domain noisy data in\nthe noise-robust ASR tasks, several downstream ASR models with the same\narchitecture are trained using the simulated data and the real data. The\nexperiment shows that the model trained with the simulated noisy data from\nDENT-DDSP achieves similar performances to the benchmark with a 2.7\\%\ndifference in terms of word error rate (WER). The code of the model is released\nonline.", "published": "2022-08-01 16:56:07", "link": "http://arxiv.org/abs/2208.00987v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A 23 $\u03bc$W Keyword Spotting IC with Ring-Oscillator-Based Time-Domain\n  Feature Extraction", "abstract": "This article presents the first keyword spotting (KWS) IC which uses a\nring-oscillator-based time-domain processing technique for its analog feature\nextractor (FEx). Its extensive usage of time-encoding schemes allows the analog\naudio signal to be processed in a fully time-domain manner except for the\nvoltage-to-time conversion stage of the analog front-end. Benefiting from\nfundamental building blocks based on digital logic gates, it offers a better\ntechnology scalability compared to conventional voltage-domain designs.\nFabricated in a 65 nm CMOS process, the prototyped KWS IC occupies 2.03mm$^{2}$\nand dissipates 23 $\\mu$W power consumption including analog FEx and digital\nneural network classifier. The 16-channel time-domain FEx achieves 54.89 dB\ndynamic range for 16 ms frame shift size while consuming 9.3 $\\mu$W. The\nmeasurement result verifies that the proposed IC performs a 12-class KWS task\non the Google Speech Command Dataset (GSCD) with >86% accuracy and 12.4 ms\nlatency.", "published": "2022-08-01 09:04:30", "link": "http://arxiv.org/abs/2208.00693v1", "categories": ["cs.AR", "cs.SD", "eess.AS"], "primary_category": "cs.AR"}
{"title": "Jazz Contrafact Detection", "abstract": "In jazz, a contrafact is a new melody composed over an existing, but often\nreharmonized chord progression. Because reharmonization can introduce a wide\nrange of variations, detecting contrafacts is a challenging task. This paper\ndevelops a novel vector-space model to represent chord progressions, and uses\nit for contrafact detection. The process applies principles from music theory\nto reduce the dimensionality of chord space, determine a common key signature\nrepresentation, and compute a chordal co-occurrence matrix. The rows of the\nmatrix form a basis for the vector space in which chord progressions are\nrepresented as piecewise linear functions, and harmonic similarity is evaluated\nby computing the membrane area, a novel distance metric. To illustrate our\nmethod's effectiveness, we apply it to the Impro-Visor corpus of 2,612 chord\nprogressions, and present examples demonstrating its ability to account for\nreharmonizations and find contrafacts.", "published": "2022-08-01 12:07:20", "link": "http://arxiv.org/abs/2208.00792v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SampleMatch: Drum Sample Retrieval by Musical Context", "abstract": "Modern digital music production typically involves combining numerous\nacoustic elements to compile a piece of music. Important types of such elements\nare drum samples, which determine the characteristics of the percussive\ncomponents of the piece. Artists must use their aesthetic judgement to assess\nwhether a given drum sample fits the current musical context. However,\nselecting drum samples from a potentially large library is tedious and may\ninterrupt the creative flow. In this work, we explore the automatic drum sample\nretrieval based on aesthetic principles learned from data. As a result, artists\ncan rank the samples in their library by fit to some musical context at\ndifferent stages of the production process (i.e., by fit to incomplete song\nmixtures). To this end, we use contrastive learning to maximize the score of\ndrum samples originating from the same song as the mixture. We conduct a\nlistening test to determine whether the human ratings match the automatic\nscoring function. We also perform objective quantitative analyses to evaluate\nthe efficacy of our approach.", "published": "2022-08-01 21:10:38", "link": "http://arxiv.org/abs/2208.01141v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Voice Analysis for Stress Detection and Application in Virtual Reality\n  to Improve Public Speaking in Real-time: A Review", "abstract": "Stress during public speaking is common and adversely affects performance and\nself-confidence. Extensive research has been carried out to develop various\nmodels to recognize emotional states. However, minimal research has been\nconducted to detect stress during public speaking in real time using voice\nanalysis. In this context, the current review showed that the application of\nalgorithms was not properly explored and helped identify the main obstacles in\ncreating a suitable testing environment while accounting for current\ncomplexities and limitations. In this paper, we present our main idea and\npropose a stress detection computational algorithmic model that could be\nintegrated into a Virtual Reality (VR) application to create an intelligent\nvirtual audience for improving public speaking skills. The developed model,\nwhen integrated with VR, will be able to detect excessive stress in real time\nby analysing voice features correlated to physiological parameters indicative\nof stress and help users gradually control excessive stress and improve public\nspeaking performance", "published": "2022-08-01 03:51:43", "link": "http://arxiv.org/abs/2208.01041v1", "categories": ["eess.AS", "cs.HC", "cs.LG", "cs.MM", "cs.SD", "I.6; K.3; K.4; A.2"], "primary_category": "eess.AS"}
