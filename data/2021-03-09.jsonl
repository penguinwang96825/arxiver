{"title": "Contrastive Semi-supervised Learning for ASR", "abstract": "Pseudo-labeling is the most adopted method for pre-training automatic speech\nrecognition (ASR) models. However, its performance suffers from the supervised\nteacher model's degrading quality in low-resource setups and under domain\ntransfer. Inspired by the successes of contrastive representation learning for\ncomputer vision and speech applications, and more recently for supervised\nlearning of visual objects, we propose Contrastive Semi-supervised Learning\n(CSL). CSL eschews directly predicting teacher-generated pseudo-labels in favor\nof utilizing them to select positive and negative examples. In the challenging\ntask of transcribing public social media videos, using CSL reduces the WER by\n8% compared to the standard Cross-Entropy pseudo-labeling (CE-PL) when 10hr of\nsupervised data is used to annotate 75,000hr of videos. The WER reduction jumps\nto 19% under the ultra low-resource condition of using 1hr labels for teacher\nsupervision. CSL generalizes much better in out-of-domain conditions, showing\nup to 17% WER reduction compared to the best CE-PL pre-trained model.", "published": "2021-03-09 00:20:37", "link": "http://arxiv.org/abs/2103.05149v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Document-Level Sentiment Classification Using Importance of\n  Sentences", "abstract": "Previous researchers have considered sentiment analysis as a document\nclassification task, in which input documents are classified into predefined\nsentiment classes. Although there are sentences in a document that support\nimportant evidences for sentiment analysis and sentences that do not, they have\ntreated the document as a bag of sentences. In other words, they have not\nconsidered the importance of each sentence in the document. To effectively\ndetermine polarity of a document, each sentence in the document should be dealt\nwith different degrees of importance. To address this problem, we propose a\ndocument-level sentence classification model based on deep neural networks, in\nwhich the importance degrees of sentences in documents are automatically\ndetermined through gate mechanisms. To verify our new sentiment analysis model,\nwe conducted experiments using the sentiment datasets in the four different\ndomains such as movie reviews, hotel reviews, restaurant reviews, and music\nreviews. In the experiments, the proposed model outperformed previous\nstate-of-the-art models that do not consider importance differences of\nsentences in a document. The experimental results show that the importance of\nsentences should be considered in a document-level sentiment classification\ntask.", "published": "2021-03-09 01:29:08", "link": "http://arxiv.org/abs/2103.05167v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Inappropriate Messages on Sensitive Topics that Could Harm a\n  Company's Reputation", "abstract": "Not all topics are equally \"flammable\" in terms of toxicity: a calm\ndiscussion of turtles or fishing less often fuels inappropriate toxic dialogues\nthan a discussion of politics or sexual minorities. We define a set of\nsensitive topics that can yield inappropriate and toxic messages and describe\nthe methodology of collecting and labeling a dataset for appropriateness. While\ntoxicity in user-generated data is well-studied, we aim at defining a more\nfine-grained notion of inappropriateness. The core of inappropriateness is that\nit can harm the reputation of a speaker. This is different from toxicity in two\nrespects: (i) inappropriateness is topic-related, and (ii) inappropriate\nmessage is not toxic but still unacceptable. We collect and release two\ndatasets for Russian: a topic-labeled dataset and an appropriateness-labeled\ndataset. We also release pre-trained classification models trained on this\ndata.", "published": "2021-03-09 10:50:30", "link": "http://arxiv.org/abs/2103.05345v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-supervised Regularization for Text Classification", "abstract": "Text classification is a widely studied problem and has broad applications.\nIn many real-world problems, the number of texts for training classification\nmodels is limited, which renders these models prone to overfitting. To address\nthis problem, we propose SSL-Reg, a data-dependent regularization approach\nbased on self-supervised learning (SSL). SSL is an unsupervised learning\napproach which defines auxiliary tasks on input data without using any\nhuman-provided labels and learns data representations by solving these\nauxiliary tasks. In SSL-Reg, a supervised classification task and an\nunsupervised SSL task are performed simultaneously. The SSL task is\nunsupervised, which is defined purely on input texts without using any\nhuman-provided labels. Training a model using an SSL task can prevent the model\nfrom being overfitted to a limited number of class labels in the classification\ntask. Experiments on 17 text classification datasets demonstrate the\neffectiveness of our proposed method.", "published": "2021-03-09 05:35:52", "link": "http://arxiv.org/abs/2103.05231v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Open-book Video Captioning with Retrieve-Copy-Generate Network", "abstract": "Due to the rapid emergence of short videos and the requirement for content\nunderstanding and creation, the video captioning task has received increasing\nattention in recent years. In this paper, we convert traditional video\ncaptioning task into a new paradigm, \\ie, Open-book Video Captioning, which\ngenerates natural language under the prompts of video-content-relevant\nsentences, not limited to the video itself. To address the open-book video\ncaptioning problem, we propose a novel Retrieve-Copy-Generate network, where a\npluggable video-to-text retriever is constructed to retrieve sentences as hints\nfrom the training corpus effectively, and a copy-mechanism generator is\nintroduced to extract expressions from multi-retrieved sentences dynamically.\nThe two modules can be trained end-to-end or separately, which is flexible and\nextensible. Our framework coordinates the conventional retrieval-based methods\nwith orthodox encoder-decoder methods, which can not only draw on the diverse\nexpressions in the retrieved sentences but also generate natural and accurate\ncontent of the video. Extensive experiments on several benchmark datasets show\nthat our proposed approach surpasses the state-of-the-art performance,\nindicating the effectiveness and promising of the proposed paradigm in the task\nof video captioning.", "published": "2021-03-09 08:17:17", "link": "http://arxiv.org/abs/2103.05284v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "BERTese: Learning to Speak to BERT", "abstract": "Large pre-trained language models have been shown to encode large amounts of\nworld and commonsense knowledge in their parameters, leading to substantial\ninterest in methods for extracting that knowledge. In past work, knowledge was\nextracted by taking manually-authored queries and gathering paraphrases for\nthem using a separate pipeline. In this work, we propose a method for\nautomatically rewriting queries into \"BERTese\", a paraphrase query that is\ndirectly optimized towards better knowledge extraction. To encourage meaningful\nrewrites, we add auxiliary loss functions that encourage the query to\ncorrespond to actual language tokens. We empirically show our approach\noutperforms competing baselines, obviating the need for complex pipelines.\nMoreover, BERTese provides some insight into the type of language that helps\nlanguage models perform knowledge extraction.", "published": "2021-03-09 10:17:22", "link": "http://arxiv.org/abs/2103.05327v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Beyond Nystr\u00f6mformer -- Approximation of self-attention by Spectral\n  Shifting", "abstract": "Transformer is a powerful tool for many natural language tasks which is based\non self-attention, a mechanism that encodes the dependence of other tokens on\neach specific token, but the computation of self-attention is a bottleneck due\nto its quadratic time complexity. There are various approaches to reduce the\ntime complexity and approximation of matrix is one such. In Nystr\\\"omformer,\nthe authors used Nystr\\\"om based method for approximation of softmax. The\nNystr\\\"om method generates a fast approximation to any large-scale symmetric\npositive semidefinite (SPSD) matrix using only a few columns of the SPSD\nmatrix. However, since the Nystr\\\"om approximation is low-rank when the\nspectrum of the SPSD matrix decays slowly, the Nystr\\\"om approximation is of\nlow accuracy. Here an alternative method is proposed for approximation which\nhas a much stronger error bound than the Nystr\\\"om method. The time complexity\nof this same as Nystr\\\"omformer which is $O\\left({n}\\right)$.", "published": "2021-03-09 12:48:16", "link": "http://arxiv.org/abs/2103.05638v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Comparing Approaches to Dravidian Language Identification", "abstract": "This paper describes the submissions by team HWR to the Dravidian Language\nIdentification (DLI) shared task organized at VarDial 2021 workshop. The DLI\ntraining set includes 16,674 YouTube comments written in Roman script\ncontaining code-mixed text with English and one of the three South Dravidian\nlanguages: Kannada, Malayalam, and Tamil. We submitted results generated using\ntwo models, a Naive Bayes classifier with adaptive language models, which has\nshown to obtain competitive performance in many language and dialect\nidentification tasks, and a transformer-based model which is widely regarded as\nthe state-of-the-art in a number of NLP tasks. Our first submission was sent in\nthe closed submission track using only the training set provided by the shared\ntask organisers, whereas the second submission is considered to be open as it\nused a pretrained model trained with external data. Our team attained shared\nsecond position in the shared task with the submission based on Naive Bayes.\nOur results reinforce the idea that deep learning methods are not as\ncompetitive in language identification related tasks as they are in many other\ntext classification tasks.", "published": "2021-03-09 16:58:55", "link": "http://arxiv.org/abs/2103.05552v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Combining Context-Free and Contextualized Representations for Arabic\n  Sarcasm Detection and Sentiment Identification", "abstract": "Since their inception, transformer-based language models have led to\nimpressive performance gains across multiple natural language processing tasks.\nFor Arabic, the current state-of-the-art results on most datasets are achieved\nby the AraBERT language model. Notwithstanding these recent advancements,\nsarcasm and sentiment detection persist to be challenging tasks in Arabic,\ngiven the language's rich morphology, linguistic disparity and dialectal\nvariations. This paper proffers team SPPU-AASM's submission for the WANLP\nArSarcasm shared-task 2021, which centers around the sarcasm and sentiment\npolarity detection of Arabic tweets. The study proposes a hybrid model,\ncombining sentence representations from AraBERT with static word vectors\ntrained on Arabic social media corpora. The proposed system achieves a\nF1-sarcastic score of 0.62 and a F-PN score of 0.715 for the sarcasm and\nsentiment detection tasks, respectively. Simulation results show that the\nproposed system outperforms multiple existing approaches for both the tasks,\nsuggesting that the amalgamation of context-free and context-dependent text\nrepresentations can help capture complementary facets of word meaning in\nArabic. The system ranked second and tenth in the respective sub-tasks of\nsarcasm detection and sentiment identification.", "published": "2021-03-09 19:39:43", "link": "http://arxiv.org/abs/2103.05683v1", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "The Unfolding Structure of Arguments in Online Debates: The case of a\n  No-Deal Brexit", "abstract": "In the last decade, political debates have progressively shifted to social\nmedia. Rhetorical devices employed by online actors and factions that operate\nin these debating arenas can be captured and analysed to conduct a statistical\nreading of societal controversies and their argumentation dynamics. In this\npaper, we propose a five-step methodology, to extract, categorize and explore\nthe latent argumentation structures of online debates. Using Twitter data about\na \"no-deal\" Brexit, we focus on the expected effects in case of materialisation\nof this event. First, we extract cause-effect claims contained in tweets using\nRegEx that exploit verbs related to Creation, Destruction and Causation.\nSecond, we categorise extracted \"no-deal\" effects using a Structural Topic\nModel estimated on unigrams and bigrams. Third, we select controversial effect\ntopics and explore within-topic argumentation differences between self-declared\npartisan user factions. We hence type topics using estimated covariate effects\non topic propensities, then, using the topics correlation network, we study the\ntopological structure of the debate to identify coherent topical\nconstellations. Finally, we analyse the debate time dynamics and infer\nlead/follow relations among factions. Results show that the proposed\nmethodology can be employed to perform a statistical rhetorics analysis of\ndebates, and map the architecture of controversies across time. In particular,\nthe \"no-deal\" Brexit debate is shown to have an assortative argumentation\nstructure heavily characterized by factional constellations of arguments, as\nwell as by polarized narrative frames invoked through verbs related to Creation\nand Destruction. Our findings highlight the benefits of implementing a systemic\napproach to the analysis of debates, which allows the unveiling of topical and\nfactional dependencies between arguments employed in online debates.", "published": "2021-03-09 12:29:43", "link": "http://arxiv.org/abs/2103.16387v1", "categories": ["cs.CL", "cs.CY", "stat.AP", "stat.ME", "J.4; I.7"], "primary_category": "cs.CL"}
{"title": "Spheroidal Ambisonics: a Spatial Audio Framework Using Spheroidal Bases", "abstract": "Ambisonics is an established framework to capture, process, and reproduce\nspatial sound fields based on its spherical harmonics representation. We\npropose a generalization of conventional spherical ambisonics to the spheroidal\ncoordinate system and spheroidal microphone arrays, which represent sound\nfields by means of spheroidal wave functions. This framework is referred to as\nspheroidal ambisonics and a formulation for the case of prolate spheroidal\ncoordinates is presented. Spheroidal ambisonics allows analytical encoding of\nsound fields using spheroidal microphone arrays. In addition, an analytical\nconversion formula from spheroidal ambisonics to spherical ambisonics is\nderived in order to ensure compatibility with the existing ecosystem of\nspherical ambisonics. Numerical experiments are performed to verify spheroidal\nambisonic encoding and transcoding when used for spatial sound field recording.\nIt is found that the sound field reconstructed from the transcoded coefficients\nhas a zone of accurate reconstruction which is prolonged towards the long axis\nof a prolate spheroidal microphone array.", "published": "2021-03-09 21:03:42", "link": "http://arxiv.org/abs/2103.05719v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "GAN Vocoder: Multi-Resolution Discriminator Is All You Need", "abstract": "Several of the latest GAN-based vocoders show remarkable achievements,\noutperforming autoregressive and flow-based competitors in both qualitative and\nquantitative measures while synthesizing orders of magnitude faster. In this\nwork, we hypothesize that the common factor underlying their success is the\nmulti-resolution discriminating framework, not the minute details in\narchitecture, loss function, or training strategy. We experimentally test the\nhypothesis by evaluating six different generators paired with one shared\nmulti-resolution discriminating framework. For all evaluative measures with\nrespect to text-to-speech syntheses and for all perceptual metrics, their\nperformances are not distinguishable from one another, which supports our\nhypothesis.", "published": "2021-03-09 05:47:43", "link": "http://arxiv.org/abs/2103.05236v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Learning to Generate Music With Sentiment", "abstract": "Deep Learning models have shown very promising results in automatically\ncomposing polyphonic music pieces. However, it is very hard to control such\nmodels in order to guide the compositions towards a desired goal. We are\ninterested in controlling a model to automatically generate music with a given\nsentiment. This paper presents a generative Deep Learning model that can be\ndirected to compose music with a given sentiment. Besides music generation, the\nsame model can be used for sentiment analysis of symbolic music. We evaluate\nthe accuracy of the model in classifying sentiment of symbolic music using a\nnew dataset of video game soundtracks. Results show that our model is able to\nobtain good prediction accuracy. A user study shows that human subjects agreed\nthat the generated music has the intended sentiment, however negative pieces\ncan be ambiguous.", "published": "2021-03-09 03:16:52", "link": "http://arxiv.org/abs/2103.06125v1", "categories": ["cs.LG", "cs.IR", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Wav2vec-C: A Self-supervised Model for Speech Representation Learning", "abstract": "Wav2vec-C introduces a novel representation learning technique combining\nelements from wav2vec 2.0 and VQ-VAE. Our model learns to reproduce quantized\nrepresentations from partially masked speech encoding using a contrastive loss\nin a way similar to Wav2vec 2.0. However, the quantization process is\nregularized by an additional consistency network that learns to reconstruct the\ninput features to the wav2vec 2.0 network from the quantized representations in\na way similar to a VQ-VAE model. The proposed self-supervised model is trained\non 10k hours of unlabeled data and subsequently used as the speech encoder in a\nRNN-T ASR model and fine-tuned with 1k hours of labeled data. This work is one\nof only a few studies of self-supervised learning on speech tasks with a large\nvolume of real far-field labeled data. The Wav2vec-C encoded representations\nachieves, on average, twice the error reduction over baseline and a higher\ncodebook utilization in comparison to wav2vec 2.0", "published": "2021-03-09 16:44:45", "link": "http://arxiv.org/abs/2103.08393v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
