{"title": "Grasping the Essentials: Tailoring Large Language Models for Zero-Shot\n  Relation Extraction", "abstract": "Relation extraction (RE) aims to identify semantic relationships between\nentities within text. Despite considerable advancements, existing models\npredominantly require extensive annotated training data, which is both costly\nand labor-intensive to collect. Moreover, these models often struggle to adapt\nto new or unseen relations. Few-shot learning, aiming to lessen annotation\ndemands, typically provides incomplete and biased supervision for target\nrelations, leading to degraded and unstable performance. To accurately and\nexplicitly describe relation semantics while minimizing annotation demands, we\nexplore the definition only zero-shot RE setting where only relation\ndefinitions expressed in natural language are used to train a RE model. We\nintroduce REPaL, comprising three stages: (1) We leverage large language models\n(LLMs) to generate initial seed instances from relation definitions and an\nunlabeled corpus. (2) We fine-tune a bidirectional Small Language Model (SLM)\nwith initial seeds to learn relations for the target domain. (3) We expand\npattern coverage and mitigate bias from initial seeds by integrating feedback\nfrom the SLM's predictions on the unlabeled corpus and the synthesis history.\nTo accomplish this, we leverage the multi-turn conversation ability of LLMs to\ngenerate new instances in follow-up dialogues, informed by both the feedback\nand synthesis history. Studies reveal that definition-oriented seed synthesis\nenhances pattern coverage whereas indiscriminately increasing seed quantity\nleads to performance saturation. Experiments on two datasets show REPaL\nsignificantly improved cost-effective zero-shot performance by large margins.", "published": "2024-02-17 00:20:06", "link": "http://arxiv.org/abs/2402.11142v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KG-Agent: An Efficient Autonomous Agent Framework for Complex Reasoning\n  over Knowledge Graph", "abstract": "In this paper, we aim to improve the reasoning ability of large language\nmodels (LLMs) over knowledge graphs (KGs) to answer complex questions. Inspired\nby existing methods that design the interaction strategy between LLMs and KG,\nwe propose an autonomous LLM-based agent framework, called KG-Agent, which\nenables a small LLM to actively make decisions until finishing the reasoning\nprocess over KGs. In KG-Agent, we integrate the LLM, multifunctional toolbox,\nKG-based executor, and knowledge memory, and develop an iteration mechanism\nthat autonomously selects the tool then updates the memory for reasoning over\nKG. To guarantee the effectiveness, we leverage program language to formulate\nthe multi-hop reasoning process over the KG, and synthesize a code-based\ninstruction dataset to fine-tune the base LLM. Extensive experiments\ndemonstrate that only using 10K samples for tuning LLaMA-7B can outperform\nstate-of-the-art methods using larger LLMs or more data, on both in-domain and\nout-domain datasets. Our code and data will be publicly released.", "published": "2024-02-17 02:07:49", "link": "http://arxiv.org/abs/2402.11163v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GenDec: A robust generative Question-decomposition method for Multi-hop\n  reasoning", "abstract": "Multi-hop QA (MHQA) involves step-by-step reasoning to answer complex\nquestions and find multiple relevant supporting facts. However, Existing large\nlanguage models'(LLMs) reasoning ability in multi-hop question answering\nremains exploration, which is inadequate in answering multi-hop questions.\nMoreover, it is unclear whether LLMs follow a desired reasoning chain to reach\nthe right final answer. In this paper, we propose a \\textbf{gen}erative\nquestion \\textbf{dec}omposition method (GenDec) from the perspective of\nexplainable QA by generating independent and complete sub-questions based on\nincorporating additional extracted evidence for enhancing LLMs' reasoning\nability in RAG. To demonstrate the impact, generalization, and robustness of\nGendec, we conduct two experiments, the first is combining GenDec with small QA\nsystems on paragraph retrieval and QA tasks. We secondly examine the reasoning\ncapabilities of various state-of-the-art LLMs including GPT-4 and GPT-3.5\ncombined with GenDec. We experiment on the HotpotQA, 2WikihopMultiHopQA,\nMuSiQue, and PokeMQA datasets.", "published": "2024-02-17 02:21:44", "link": "http://arxiv.org/abs/2402.11166v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "M4GT-Bench: Evaluation Benchmark for Black-Box Machine-Generated Text\n  Detection", "abstract": "The advent of Large Language Models (LLMs) has brought an unprecedented surge\nin machine-generated text (MGT) across diverse channels. This raises legitimate\nconcerns about its potential misuse and societal implications. The need to\nidentify and differentiate such content from genuine human-generated text is\ncritical in combating disinformation, preserving the integrity of education and\nscientific fields, and maintaining trust in communication. In this work, we\naddress this problem by introducing a new benchmark based on a multilingual,\nmulti-domain, and multi-generator corpus of MGTs -- M4GT-Bench. The benchmark\nis compiled of three tasks: (1) mono-lingual and multi-lingual binary MGT\ndetection; (2) multi-way detection where one need to identify, which particular\nmodel generated the text; and (3) mixed human-machine text detection, where a\nword boundary delimiting MGT from human-written content should be determined.\nOn the developed benchmark, we have tested several MGT detection baselines and\nalso conducted an evaluation of human performance. We see that obtaining good\nperformance in MGT detection usually requires an access to the training data\nfrom the same domain and generators. The benchmark is available at\nhttps://github.com/mbzuai-nlp/M4GT-Bench.", "published": "2024-02-17 02:50:33", "link": "http://arxiv.org/abs/2402.11175v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RENOVI: A Benchmark Towards Remediating Norm Violations in\n  Socio-Cultural Conversations", "abstract": "Norm violations occur when individuals fail to conform to culturally accepted\nbehaviors, which may lead to potential conflicts. Remediating norm violations\nrequires social awareness and cultural sensitivity of the nuances at play. To\nequip interactive AI systems with a remediation ability, we offer ReNoVi - a\nlarge-scale corpus of 9,258 multi-turn dialogues annotated with social norms,\nas well as define a sequence of tasks to help understand and remediate norm\nviolations step by step. ReNoVi consists of two parts: 512 human-authored\ndialogues (real data), and 8,746 synthetic conversations generated by ChatGPT\nthrough prompt learning. While collecting sufficient human-authored data is\ncostly, synthetic conversations provide suitable amounts of data to help\nmitigate the scarcity of training data, as well as the chance to assess the\nalignment between LLMs and humans in the awareness of social norms. We thus\nharness the power of ChatGPT to generate synthetic training data for our task.\nTo ensure the quality of both human-authored and synthetic data, we follow a\nquality control protocol during data collection. Our experimental results\ndemonstrate the importance of remediating norm violations in socio-cultural\nconversations, as well as the improvement in performance obtained from\nsynthetic data.", "published": "2024-02-17 03:13:42", "link": "http://arxiv.org/abs/2402.11178v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Disclosure and Mitigation of Gender Bias in LLMs", "abstract": "Large Language Models (LLMs) can generate biased responses. Yet previous\ndirect probing techniques contain either gender mentions or predefined gender\nstereotypes, which are challenging to comprehensively collect. Hence, we\npropose an indirect probing framework based on conditional generation. This\napproach aims to induce LLMs to disclose their gender bias even without\nexplicit gender or stereotype mentions. We explore three distinct strategies to\ndisclose explicit and implicit gender bias in LLMs. Our experiments demonstrate\nthat all tested LLMs exhibit explicit and/or implicit gender bias, even when\ngender stereotypes are not present in the inputs. In addition, an increased\nmodel size or model alignment amplifies bias in most cases. Furthermore, we\ninvestigate three methods to mitigate bias in LLMs via Hyperparameter Tuning,\nInstruction Guiding, and Debias Tuning. Remarkably, these methods prove\neffective even in the absence of explicit genders or stereotypes.", "published": "2024-02-17 04:48:55", "link": "http://arxiv.org/abs/2402.11190v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge Graph Assisted Automatic Sports News Writing", "abstract": "In this paper, we present a novel method for automatically generating sports\nnews, which employs a unique algorithm that extracts pivotal moments from live\ntext broadcasts and uses them to create an initial draft of the news. This\ndraft is further refined by incorporating key details and background\ninformation from a specially designed sports knowledge graph. This graph\ncontains 5,893 entities, which are classified into three distinct conceptual\ncategories, interconnected through four relationship types, and characterized\nby 27 unique attributes. In addition, we create a multi-stage learning model by\ncombining convolutional neural networks and a transformer encoder. This model\nexpresses entity-task interactions using convolutional neural networks and\nenriches entity representations in the query set with the transformer encoder.\nIt also includes a processor to compute matching scores for incomplete triples,\naddressing few-shot knowledge graph completion problem. The efficiency of this\napproach has been confirmed through both subjective and objective evaluations\nof 50 selected test cases, demonstrating its capability in revolutionizing the\ncreation of sports news.", "published": "2024-02-17 04:54:58", "link": "http://arxiv.org/abs/2402.11191v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating LLMs' Mathematical Reasoning in Financial Document Question\n  Answering", "abstract": "Large Language Models (LLMs), excel in natural language understanding, but\ntheir capability for complex mathematical reasoning with an amalgamation of\nstructured tables and unstructured text is uncertain. This study explores LLMs'\nmathematical reasoning on four financial tabular question-answering datasets:\nTATQA, FinQA, ConvFinQA, and Multihiertt. Through extensive experiments with\nvarious models and prompting techniques, we assess how LLMs adapt to complex\ntables and mathematical tasks. We focus on sensitivity to table complexity and\nperformance variations with an increasing number of arithmetic reasoning steps.\nThe results provide insights into LLMs' capabilities and limitations in\nhandling complex mathematical scenarios for semi-structured tables. Ultimately,\nwe introduce a novel prompting technique tailored to semi-structured documents,\nmatching or outperforming other baselines in performance while providing a\nnuanced understanding of LLMs abilities for such a task.", "published": "2024-02-17 05:10:18", "link": "http://arxiv.org/abs/2402.11194v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Centroid-Based Efficient Minimum Bayes Risk Decoding", "abstract": "Minimum Bayes risk (MBR) decoding achieved state-of-the-art translation\nperformance by using COMET, a neural metric that has a high correlation with\nhuman evaluation. However, MBR decoding requires quadratic time since it\ncomputes the expected score between a translation hypothesis and all reference\ntranslations. We propose centroid-based MBR (CBMBR) decoding to improve the\nspeed of MBR decoding. Our method clusters the reference translations in the\nfeature space, and then calculates the score using the centroids of each\ncluster. The experimental results show that our CBMBR not only improved the\ndecoding speed of the expected score calculation 5.7 times, but also\noutperformed vanilla MBR decoding in translation quality by up to 0.5 COMET in\nthe WMT'22 En$\\leftrightarrow$Ja, En$\\leftrightarrow$De, En$\\leftrightarrow$Zh,\nand WMT'23 En$\\leftrightarrow$Ja translation tasks.", "published": "2024-02-17 05:15:12", "link": "http://arxiv.org/abs/2402.11197v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Direct Evaluation of Chain-of-Thought in Multi-hop Reasoning with\n  Knowledge Graphs", "abstract": "Large language models (LLMs) demonstrate strong reasoning abilities when\nprompted to generate chain-of-thought (CoT) explanations alongside answers.\nHowever, previous research on evaluating LLMs has solely focused on answer\naccuracy, neglecting the correctness of the generated CoT. In this paper, we\ndelve deeper into the CoT reasoning capabilities of LLMs in multi-hop question\nanswering by utilizing knowledge graphs (KGs). We propose a novel\ndiscriminative and generative CoT evaluation paradigm to assess LLMs' knowledge\nof reasoning and the accuracy of the generated CoT. Through experiments\nconducted on 5 different families of LLMs across 2 multi-hop question-answering\ndatasets, we find that LLMs possess sufficient knowledge to perform reasoning.\nHowever, there exists a significant disparity between answer accuracy and\nfaithfulness of the CoT reasoning generated by LLMs, indicating that they often\narrive at correct answers through incorrect reasoning.", "published": "2024-02-17 05:22:56", "link": "http://arxiv.org/abs/2402.11199v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Controlled Text Generation for Large Language Model with Dynamic\n  Attribute Graphs", "abstract": "Controlled Text Generation (CTG) aims to produce texts that exhibit specific\ndesired attributes. In this study, we introduce a pluggable CTG framework for\nLarge Language Models (LLMs) named Dynamic Attribute Graphs-based controlled\ntext generation (DATG). This framework utilizes an attribute scorer to evaluate\nthe attributes of sentences generated by LLMs and constructs dynamic attribute\ngraphs. DATG modulates the occurrence of key attribute words and key\nanti-attribute words, achieving effective attribute control without\ncompromising the original capabilities of the model. We conduct experiments\nacross four datasets in two tasks: toxicity mitigation and sentiment\ntransformation, employing five LLMs as foundational models. Our findings\nhighlight a remarkable enhancement in control accuracy, achieving a peak\nimprovement of 19.29% over baseline methods in the most favorable task across\nfour datasets. Additionally, we observe a significant decrease in perplexity,\nmarkedly improving text fluency.", "published": "2024-02-17 08:14:37", "link": "http://arxiv.org/abs/2402.11218v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLM can Achieve Self-Regulation via Hyperparameter Aware Generation", "abstract": "In the realm of Large Language Models (LLMs), users commonly employ diverse\ndecoding strategies and adjust hyperparameters to control the generated text.\nHowever, a critical question emerges: Are LLMs conscious of the existence of\nthese decoding strategies and capable of regulating themselves? The current\ndecoding generation process often relies on empirical and heuristic manual\nadjustments to hyperparameters based on types of tasks and demands. However,\nthis process is typically cumbersome, and the decoding hyperparameters may not\nalways be optimal for each sample. To address the aforementioned challenges, we\npropose a novel text generation paradigm termed Hyperparameter Aware Generation\n(HAG). By leveraging hyperparameter-aware instruction tuning, the LLM\nautonomously determines the optimal decoding strategy and configs based on the\ninput samples, enabling self-regulation. Our approach eliminates the need for\nextensive manual tuning, offering a more autonomous, self-regulate model\nbehavior. Experimental results spanning six datasets across reasoning,\ncreativity, translation, and mathematics tasks demonstrate that\nhyperparameter-aware instruction tuning empowers the LLMs to self-regulate the\ndecoding strategy and hyperparameter. HAG extends the current paradigm in the\ntext generation process, highlighting the feasibility of endowing the LLMs with\nself-regulate decoding strategies.", "published": "2024-02-17 11:18:22", "link": "http://arxiv.org/abs/2402.11251v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "C-ICL: Contrastive In-context Learning for Information Extraction", "abstract": "There has been increasing interest in exploring the capabilities of advanced\nlarge language models (LLMs) in the field of information extraction (IE),\nspecifically focusing on tasks related to named entity recognition (NER) and\nrelation extraction (RE). Although researchers are exploring the use of\nfew-shot information extraction through in-context learning with LLMs, they\ntend to focus only on using correct or positive examples for demonstration,\nneglecting the potential value of incorporating incorrect or negative examples\ninto the learning process. In this paper, we present c-ICL, a novel few-shot\ntechnique that leverages both correct and incorrect sample constructions to\ncreate in-context learning demonstrations. This approach enhances the ability\nof LLMs to extract entities and relations by utilizing prompts that incorporate\nnot only the positive samples but also the reasoning behind them. This method\nallows for the identification and correction of potential interface errors.\nSpecifically, our proposed method taps into the inherent contextual information\nand valuable information in hard negative samples and the nearest positive\nneighbors to the test and then applies the in-context learning demonstrations\nbased on LLMs. Our experiments on various datasets indicate that c-ICL\noutperforms previous few-shot in-context learning methods, delivering\nsubstantial enhancements in performance across a broad spectrum of related\ntasks. These improvements are noteworthy, showcasing the versatility of our\napproach in miscellaneous scenarios.", "published": "2024-02-17 11:28:08", "link": "http://arxiv.org/abs/2402.11254v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Large Multimodal Models Uncover Deep Semantics Behind Images?", "abstract": "Understanding the deep semantics of images is essential in the era dominated\nby social media. However, current research works primarily on the superficial\ndescription of images, revealing a notable deficiency in the systematic\ninvestigation of the inherent deep semantics. In this work, we introduce\nDEEPEVAL, a comprehensive benchmark to assess Large Multimodal Models' (LMMs)\ncapacities of visual deep semantics. DEEPEVAL includes human-annotated dataset\nand three progressive subtasks: fine-grained description selection, in-depth\ntitle matching, and deep semantics understanding. Utilizing DEEPEVAL, we\nevaluate 9 open-source LMMs and GPT-4V(ision). Our evaluation demonstrates a\nsubstantial gap between the deep semantic comprehension capabilities of\nexisting LMMs and humans. For example, GPT-4V is 30% behind humans in\nunderstanding deep semantics, even though it achieves human-comparable\nperformance in image description. Further analysis reveals that LMM performance\non DEEPEVAL varies according to the specific facets of deep semantics explored,\nindicating the fundamental challenges remaining in developing LMMs.", "published": "2024-02-17 13:41:44", "link": "http://arxiv.org/abs/2402.11281v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Grammaticality illusion or ambiguous interpretation? Event-related\n  potentials reveal the nature of the missing-NP effect in Mandarin\n  centre-embedded structures", "abstract": "In several languages, omitting a verb phrase (VP) in double centre-embedded\nstructures creates a grammaticality illusion. Similar illusion also exhibited\nin Mandarin missing-NP double centre-embedded structures. However, there is no\nconsensus on its very nature. Instead of treating it as grammaticality\nillusion, we argue that ambiguous interpretations of verbs can best account for\nthis phenomenon in Mandarin. To further support this hypothesis, we conducted\ntwo electroencephalography (EEG) experiments on quasi double centre-embedded\nstructures whose complexity is reduced by placing the self-embedding relative\nclauses into the sentence's subject position. Experiment 1 showed that similar\nphenomenon even exhibited in this structure, evidenced by an absence of P600\neffect and a presence of N400 effect. In Experiment 2, providing semantic cues\nto reduce ambiguity dispelled this illusion, as evidenced by a P600 effect. We\ninterpret the results under garden-path theory and propose that word-order\ndifference may account for this cross-linguistic variation.", "published": "2024-02-17 13:43:39", "link": "http://arxiv.org/abs/2402.11282v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OneBit: Towards Extremely Low-bit Large Language Models", "abstract": "Model quantification uses low bit-width values to represent the weight\nmatrices of existing models to be quantized, which is a promising approach to\nreduce both storage and computational overheads of deploying highly anticipated\nLLMs. However, current quantization methods suffer severe performance\ndegradation when the bit-width is extremely reduced, and thus focus on\nutilizing 4-bit or 8-bit values to quantize models. This paper boldly quantizes\nthe weight matrices of LLMs to 1-bit, paving the way for the extremely low\nbit-width deployment of LLMs. For this target, we introduce a 1-bit model\ncompressing framework named OneBit, including a novel 1-bit parameter\nrepresentation method to better quantize LLMs as well as an effective parameter\ninitialization method based on matrix decomposition to improve the convergence\nspeed of the quantization framework. Sufficient experimental results indicate\nthat OneBit achieves good performance (at least 81% of the non-quantized\nperformance on LLaMA models) with robust training processes when only using\n1-bit weight matrices.", "published": "2024-02-17 14:26:57", "link": "http://arxiv.org/abs/2402.11295v6", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MMMModal -- Multi-Images Multi-Audio Multi-turn Multi-Modal", "abstract": "Our contribution introduces a groundbreaking multimodal large language model\ndesigned to comprehend multi-images, multi-audio, and multi-images-multi-audio\nwithin a single multiturn session. Leveraging state-of-the-art models, we\nutilize the SigLIP encoder for visual inputs and the Whisper Encoder for audio\ninputs. Notably, this multimodal large language model is bilingual, proficient\nin understanding both English and Malay simultaneously. We proudly unveil two\nversions of this model: TinyLlama with 1.1B parameters, and Mistral with 7B\nparameters. With its ability to navigate diverse modalities and languages, our\nmodel represents a significant advancement for the Malaysian context and\nbeyond.\n  All models released at\nhttps://huggingface.co/collections/mesolitica/multimodal-malaysian-llm-65c6f893e03f78fa9e5c8859", "published": "2024-02-17 14:37:38", "link": "http://arxiv.org/abs/2402.11297v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EVEDIT: Event-based Knowledge Editing with Deductive Editing Boundaries", "abstract": "The dynamic nature of real-world information necessitates efficient knowledge\nediting (KE) in large language models (LLMs) for knowledge updating. However,\ncurrent KE approaches, which typically operate on (subject, relation, object)\ntriples, ignore the contextual information and the relation among different\nknowledge. Such editing methods could thus encounter an uncertain editing\nboundary, leaving a lot of relevant knowledge in ambiguity: Queries that could\nbe answered pre-edit cannot be reliably answered afterward. In this work, we\nanalyze this issue by introducing a theoretical framework for KE that\nhighlights an overlooked set of knowledge that remains unchanged and aids in\nknowledge deduction during editing, which we name as the deduction anchor. We\nfurther address this issue by proposing a novel task of event-based knowledge\nediting that pairs facts with event descriptions. This task manifests not only\na closer simulation of real-world editing scenarios but also a more logically\nsound setting, implicitly defining the deduction anchor to address the issue of\nindeterminate editing boundaries. We empirically demonstrate the superiority of\nevent-based editing over the existing setting on resolving uncertainty in\nedited models, and curate a new benchmark dataset EvEdit derived from the\nCounterFact dataset. Moreover, while we observe that the event-based setting is\nsignificantly challenging for existing approaches, we propose a novel approach\nSelf-Edit that showcases stronger performance, achieving 55.6% consistency\nimprovement while maintaining the naturalness of generation.", "published": "2024-02-17 16:34:50", "link": "http://arxiv.org/abs/2402.11324v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PhaseEvo: Towards Unified In-Context Prompt Optimization for Large\n  Language Models", "abstract": "Crafting an ideal prompt for Large Language Models (LLMs) is a challenging\ntask that demands significant resources and expert human input. Existing work\ntreats the optimization of prompt instruction and in-context learning examples\nas distinct problems, leading to sub-optimal prompt performance. This research\naddresses this limitation by establishing a unified in-context prompt\noptimization framework, which aims to achieve joint optimization of the prompt\ninstruction and examples. However, formulating such optimization in the\ndiscrete and high-dimensional natural language space introduces challenges in\nterms of convergence and computational efficiency. To overcome these issues, we\npresent PhaseEvo, an efficient automatic prompt optimization framework that\ncombines the generative capability of LLMs with the global search proficiency\nof evolution algorithms. Our framework features a multi-phase design\nincorporating innovative LLM-based mutation operators to enhance search\nefficiency and accelerate convergence. We conduct an extensive evaluation of\nour approach across 35 benchmark tasks. The results demonstrate that PhaseEvo\nsignificantly outperforms the state-of-the-art baseline methods by a large\nmargin whilst maintaining good efficiency.", "published": "2024-02-17 17:47:10", "link": "http://arxiv.org/abs/2402.11347v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessing News Thumbnail Representativeness: Counterfactual text can\n  enhance the cross-modal matching ability", "abstract": "This paper addresses the critical challenge of assessing the\nrepresentativeness of news thumbnail images, which often serve as the first\nvisual engagement for readers when an article is disseminated on social media.\nWe focus on whether a news image represents the actors discussed in the news\ntext. To serve the challenge, we introduce NewsTT, a manually annotated dataset\nof 1000 news thumbnail images and text pairs. We found that the pretrained\nvision and language models, such as BLIP-2, struggle with this task. Since news\nsubjects frequently involve named entities or proper nouns, the pretrained\nmodels could have a limited capability to match news actors' visual and textual\nappearances. We hypothesize that learning to contrast news text with its\ncounterfactual, of which named entities are replaced, can enhance the\ncross-modal matching ability of vision and language models. We propose\nCFT-CLIP, a contrastive learning framework that updates vision and language\nbi-encoders according to the hypothesis. We found that our simple method can\nboost the performance for assessing news thumbnail representativeness,\nsupporting our assumption. Code and data can be accessed at\nhttps://github.com/ssu-humane/news-images-acl24.", "published": "2024-02-17 01:27:29", "link": "http://arxiv.org/abs/2402.11159v3", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "PEDANTS: Cheap but Effective and Interpretable Answer Equivalence", "abstract": "Question answering (QA) can only make progress if we know if an answer is\ncorrect, but current answer correctness (AC) metrics struggle with verbose,\nfree-form answers from large language models (LLMs). There are two challenges\nwith current short-form QA evaluations: a lack of diverse styles of evaluation\ndata and an over-reliance on expensive and slow LLMs. LLM-based scorers\ncorrelate better with humans, but this expensive task has only been tested on\nlimited QA datasets. We rectify these issues by providing rubrics and datasets\nfor evaluating machine QA adopted from the Trivia community. We also propose an\nefficient, and interpretable QA evaluation that is more stable than an exact\nmatch and neural methods(BERTScore).", "published": "2024-02-17 01:56:19", "link": "http://arxiv.org/abs/2402.11161v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ToBlend: Token-Level Blending With an Ensemble of LLMs to Attack\n  AI-Generated Text Detection", "abstract": "The robustness of AI-content detection models against sophisticated\nadversarial strategies, such as paraphrasing or word switching, is a rising\nconcern in natural language generation (NLG) applications. This study proposes\nToBlend, a novel token-level ensemble text generation method to challenge the\nrobustness of current AI-content detection approaches by utilizing multiple\nsets of candidate generative large language models (LLMs). By randomly sampling\ntoken(s) from candidate LLMs sets, we find ToBlend significantly drops the\nperformance of most mainstream AI-content detection methods. We evaluate the\ntext quality produced under different ToBlend settings based on annotations\nfrom experienced human experts. We proposed a fine-tuned Llama3.1 model to\ndistinguish the ToBlend generated text more accurately. Our findings underscore\nour proposed text generation approach's great potential in deceiving and\nimproving detection models. Our datasets, codes, and annotations are\nopen-sourced.", "published": "2024-02-17 02:25:57", "link": "http://arxiv.org/abs/2402.11167v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "KnowTuning: Knowledge-aware Fine-tuning for Large Language Models", "abstract": "Despite their success at many natural language processing (NLP) tasks, large\nlanguage models still struggle to effectively leverage knowledge for\nknowledge-intensive tasks, manifesting limitations such as generating\nincomplete, non-factual, or illogical answers. These limitations stem from\ninadequate knowledge awareness of LLMs during vanilla fine-tuning. To address\nthese problems, we propose a knowledge-aware fine-tuning (KnowTuning) method to\nimprove fine-grained and coarse-grained knowledge awareness of LLMs. We devise\na fine-grained knowledge augmentation stage to train LLMs to identify difficult\nfine-grained knowledge in answers. We also propose a coarse-grained knowledge\ncomparison stage to train LLMs to distinguish between reliable and unreliable\nknowledge, in three aspects: completeness, factuality, and logicality.\nExtensive experiments on both generic and medical question answering (QA)\ndatasets confirm the effectiveness of KnowTuning, through automatic and human\nevaluations, across various sizes of LLMs. We further verify that KnowTuning\ngenerates more facts with less factual error rate under fine-grained facts\nevaluation.", "published": "2024-02-17 02:54:32", "link": "http://arxiv.org/abs/2402.11176v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Question Answering Based Pipeline for Comprehensive Chinese EHR\n  Information Extraction", "abstract": "Electronic health records (EHRs) hold significant value for research and\napplications. As a new way of information extraction, question answering (QA)\ncan extract more flexible information than conventional methods and is more\naccessible to clinical researchers, but its progress is impeded by the scarcity\nof annotated data. In this paper, we propose a novel approach that\nautomatically generates training data for transfer learning of QA models. Our\npipeline incorporates a preprocessing module to handle challenges posed by\nextraction types that are not readily compatible with extractive QA frameworks,\nincluding cases with discontinuous answers and many-to-one relationships. The\nobtained QA model exhibits excellent performance on subtasks of information\nextraction in EHRs, and it can effectively handle few-shot or zero-shot\nsettings involving yes-no questions. Case studies and ablation studies\ndemonstrate the necessity of each component in our design, and the resulting\nmodel is deemed suitable for practical use.", "published": "2024-02-17 02:55:35", "link": "http://arxiv.org/abs/2402.11177v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "LaCo: Large Language Model Pruning via Layer Collapse", "abstract": "Large language models (LLMs) based on transformer are witnessing a notable\ntrend of size expansion, which brings considerable costs to both model training\nand inference. However, existing methods such as model quantization, knowledge\ndistillation, and model pruning are constrained by various issues, including\nhardware support limitations, the need for extensive training, and alterations\nto the model internal structure. In this paper, we propose a concise layer-wise\nstructured pruner called \\textit{Layer Collapse (LaCo)}, in which rear model\nlayers collapse into a prior layer, enabling a rapid reduction in model size\nwhile preserving the model structure. Comprehensive experiments show that our\nmethod maintains an average task performance of over 80\\% at pruning ratios of\n25-30\\%, significantly outperforming existing state-of-the-art structured\npruning methods. We also conduct post-training experiments to confirm that the\n\\textit{LaCo} effectively inherits the parameters of the original model.\nAdditionally, we perform ablation studies on various settings of \\textit{LaCo}.\nFinally, we discuss our motivation from the perspective of layer-wise\nsimilarity and evaluate the performance of the pruned LLMs across various\npruning ratios\\footnote{\\url{https://github.com/yangyifei729/LaCo}}.", "published": "2024-02-17 04:16:30", "link": "http://arxiv.org/abs/2402.11187v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "I Learn Better If You Speak My Language: Understanding the Superior\n  Performance of Fine-Tuning Large Language Models with LLM-Generated Responses", "abstract": "This paper explores an intriguing observation: fine-tuning a large language\nmodel (LLM) with responses generated by a LLM often yields better results than\nusing responses generated by humans, particularly in reasoning tasks. We\nconduct an in-depth investigation to understand why this occurs. Contrary to\nthe common belief that these instances is due to the more detailed nature of\nLLM-generated content, our study identifies another contributing factor: an LLM\nis inherently more \"familiar\" with LLM generated responses. This familiarity is\nevidenced by lower perplexity before fine-tuning. We design a series of\nexperiments to understand the impact of the \"familiarity\" and our conclusion\nreveals that this \"familiarity\" significantly impacts learning performance.\nTraining with LLM-generated responses not only enhances performance but also\nhelps maintain the model's capabilities in other reasoning tasks after\nfine-tuning on a specific task.", "published": "2024-02-17 05:05:31", "link": "http://arxiv.org/abs/2402.11192v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Spectrum Evaluation Benchmark for Medical Multi-Modal Large Language\n  Models", "abstract": "The significant breakthroughs of Medical Multi-Modal Large Language Models\n(Med-MLLMs) renovate modern healthcare with robust information synthesis and\nmedical decision support. However, these models are often evaluated on\nbenchmarks that are unsuitable for the Med-MLLMs due to the complexity of\nreal-world diagnostics across diverse specialties. To address this gap, we\nintroduce Asclepius, a novel Med-MLLM benchmark that comprehensively assesses\nMed-MLLMs in terms of: distinct medical specialties (cardiovascular,\ngastroenterology, etc.) and different diagnostic capacities (perception,\ndisease analysis, etc.). Grounded in 3 proposed core principles, Asclepius\nensures a comprehensive evaluation by encompassing 15 medical specialties,\nstratifying into 3 main categories and 8 sub-categories of clinical tasks, and\nexempting overlap with existing VQA dataset. We further provide an in-depth\nanalysis of 6 Med-MLLMs and compare them with 3 human specialists, providing\ninsights into their competencies and limitations in various medical contexts.\nOur work not only advances the understanding of Med-MLLMs' capabilities but\nalso sets a precedent for future evaluations and the safe deployment of these\nmodels in clinical environments.", "published": "2024-02-17 08:04:23", "link": "http://arxiv.org/abs/2402.11217v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Can Large Language Models perform Relation-based Argument Mining?", "abstract": "Argument mining (AM) is the process of automatically extracting arguments,\ntheir components and/or relations amongst arguments and components from text.\nAs the number of platforms supporting online debate increases, the need for AM\nbecomes ever more urgent, especially in support of downstream tasks.\nRelation-based AM (RbAM) is a form of AM focusing on identifying agreement\n(support) and disagreement (attack) relations amongst arguments. RbAM is a\nchallenging classification task, with existing methods failing to perform\nsatisfactorily. In this paper, we show that general-purpose Large Language\nModels (LLMs), appropriately primed and prompted, can significantly outperform\nthe best performing (RoBERTa-based) baseline. Specifically, we experiment with\ntwo open-source LLMs (Llama-2 and Mistral) with ten datasets.", "published": "2024-02-17 10:37:51", "link": "http://arxiv.org/abs/2402.11243v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "MoRAL: MoE Augmented LoRA for LLMs' Lifelong Learning", "abstract": "Adapting large language models (LLMs) to new domains/tasks and enabling them\nto be efficient lifelong learners is a pivotal challenge. In this paper, we\npropose MoRAL, i.e., Mixture-of-Experts augmented Low-Rank Adaptation for\nLifelong Learning. MoRAL combines the multi-tasking abilities of MoE with the\nfine-tuning abilities of LoRA for effective life-long learning of LLMs. In\ncontrast to the conventional approaches that use factual triplets as inputs\nMoRAL relies on simple question-answer pairs, which is a more practical and\neffective strategy for robust and efficient learning. Owing to new data\nsettings, we introduce a new evaluation benchmark namely: Life Long Learning of\nLLM (5L-bench) encompassing a newly curated dataset of question-answer pairs,\nand a set of evaluation metrics for rigorous evaluation of MoRAL in open-book\nand closed-book settings. Experimental evaluation shows (i) LLMs learn fast in\nopen-book settings with up to 30.15% improvement in \"RA\" for Phi-2-2.7B\ncompared to closed-book (for models fine-tuned with MoRAL); (ii) MoRAL shows\nhigher performance improvement for models with a greater number of parameters;\n(iii) MoRAL is robust to catastrophic forgetting offering better knowledge\nretention compared to baselines.", "published": "2024-02-17 12:25:31", "link": "http://arxiv.org/abs/2402.11260v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi-Perspective Consistency Enhances Confidence Estimation in Large\n  Language Models", "abstract": "In the deployment of large language models (LLMs), accurate confidence\nestimation is critical for assessing the credibility of model predictions.\nHowever, existing methods often fail to overcome the issue of overconfidence on\nincorrect answers. In this work, we focus on improving the confidence\nestimation of large language models. Considering the fragility of\nself-awareness in language models, we introduce a Multi-Perspective Consistency\n(MPC) method. We leverage complementary insights from different perspectives\nwithin models (MPC-Internal) and across different models (MPC-Across) to\nmitigate the issue of overconfidence arising from a singular viewpoint. The\nexperimental results on eight publicly available datasets show that our MPC\nachieves state-of-the-art performance. Further analyses indicate that MPC can\nmitigate the problem of overconfidence and is effectively scalable to other\nmodels.", "published": "2024-02-17 13:37:39", "link": "http://arxiv.org/abs/2402.11279v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Puzzle Solving using Reasoning of Large Language Models: A Survey", "abstract": "Exploring the capabilities of Large Language Models (LLMs) in puzzle solving\nunveils critical insights into their potential and challenges in AI, marking a\nsignificant step towards understanding their applicability in complex reasoning\ntasks. This survey leverages a unique taxonomy -- dividing puzzles into\nrule-based and rule-less categories -- to critically assess LLMs through\nvarious methodologies, including prompting techniques, neuro-symbolic\napproaches, and fine-tuning. Through a critical review of relevant datasets and\nbenchmarks, we assess LLMs' performance, identifying significant challenges in\ncomplex puzzle scenarios. Our findings highlight the disparity between LLM\ncapabilities and human-like reasoning, particularly in those requiring advanced\nlogical inference. The survey underscores the necessity for novel strategies\nand richer datasets to advance LLMs' puzzle-solving proficiency and contribute\nto AI's logical reasoning and creative problem-solving advancements.", "published": "2024-02-17 14:19:38", "link": "http://arxiv.org/abs/2402.11291v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dissecting Human and LLM Preferences", "abstract": "As a relative quality comparison of model responses, human and Large Language\nModel (LLM) preferences serve as common alignment goals in model fine-tuning\nand criteria in evaluation. Yet, these preferences merely reflect broad\ntendencies, resulting in less explainable and controllable models with\npotential safety risks. In this work, we dissect the preferences of human and\n32 different LLMs to understand their quantitative composition, using\nannotations from real-world user-model conversations for a fine-grained,\nscenario-wise analysis. We find that humans are less sensitive to errors, favor\nresponses that support their stances, and show clear dislike when models admit\ntheir limits. On the contrary, advanced LLMs like GPT-4-Turbo emphasize\ncorrectness, clarity, and harmlessness more. Additionally, LLMs of similar\nsizes tend to exhibit similar preferences, regardless of their training\nmethods, and fine-tuning for alignment does not significantly alter the\npreferences of pretrained-only LLMs. Finally, we show that preference-based\nevaluation can be intentionally manipulated. In both training-free and\ntraining-based settings, aligning a model with the preferences of judges boosts\nscores, while injecting the least preferred properties lowers them. This\nresults in notable score shifts: up to 0.59 on MT-Bench (1-10 scale) and 31.94\non AlpacaEval 2.0 (0-100 scale), highlighting the significant impact of this\nstrategic adaptation. Interactive Demo:\nhttps://huggingface.co/spaces/GAIR/Preference-Dissection-Visualization Dataset:\nhttps://huggingface.co/datasets/GAIR/preference-dissection Code:\nhttps://github.com/GAIR-NLP/Preference-Dissection", "published": "2024-02-17 14:34:31", "link": "http://arxiv.org/abs/2402.11296v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Language Models Don't Learn the Physical Manifestation of Language", "abstract": "We argue that language-only models don't learn the physical manifestation of\nlanguage. We present an empirical investigation of visual-auditory properties\nof language through a series of tasks, termed H-Test. These tasks highlight a\nfundamental gap between human linguistic understanding and the sensory-deprived\nlinguistic understanding of LLMs. In support of our hypothesis, 1. deliberate\nreasoning (Chain-of-Thought), 2. few-shot examples, or 3. stronger LLM from the\nsame model family (LLaMA 2 13B -> LLaMA 2 70B) has no significant effect on\nH-Test performance.\n  We bring in the philosophical case of Mary, who learns about the world in a\nsensory-deprived environment as a useful conceptual framework to understand how\nlanguage-only models learn about the world (Jackson, 1986). Our experiments\nshow that some of the strongest proprietary LLMs stay near random chance\nbaseline accuracy of 50%, highlighting the limitations of linguistic knowledge\nacquired in the absence of sensory experience. Our code and data are available\nat <github.com/brucewlee/h-test>.", "published": "2024-02-17 17:52:24", "link": "http://arxiv.org/abs/2402.11349v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Offline Training of Language Model Agents with Functions as Learnable\n  Weights", "abstract": "Researchers and practitioners have recently reframed powerful Large Language\nModels (LLMs) as agents, enabling them to automate complex tasks largely via\nthe use of specialized functions. To facilitate the development of LLM agents,\nwe present a novel paradigm of training LLM agents without modifying the LLM\nweights, which is particularly useful when the LLMs are difficult or\ninaccessible for modifications. Inspired by how humans continuously forge tools\nto adapt to real-world tasks, rather than change our biological structure to\nfit a static set of tools, we propose to progressively forge agent's functions\nto better solve the downstream tasks instead of modifying the LLM weights. By\ntreating the functions as learnable `agent parameters' and leveraging the\nfundamental idea of model training in artificial intelligence, we develop\nAgentOptimizer that employs the LLM to update agents' functions and devise an\nagent training algorithm with two strategies, roll-back, and early-stop, to\nstreamline the training process. With extensive experiments, we showcase that\nthe agent training paradigm could significantly improve the performance of\nrepresentative LLM agents in various downstream tasks. We also study the\nbehavior of the agent training regarding aspects like the learning curve and\ndomain transferability.", "published": "2024-02-17 18:31:21", "link": "http://arxiv.org/abs/2402.11359v4", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Reasoning before Comparison: LLM-Enhanced Semantic Similarity Metrics\n  for Domain Specialized Text Analysis", "abstract": "In this study, we leverage LLM to enhance the semantic analysis and develop\nsimilarity metrics for texts, addressing the limitations of traditional\nunsupervised NLP metrics like ROUGE and BLEU. We develop a framework where LLMs\nsuch as GPT-4 are employed for zero-shot text identification and label\ngeneration for radiology reports, where the labels are then used as\nmeasurements for text similarity. By testing the proposed framework on the\nMIMIC data, we find that GPT-4 generated labels can significantly improve the\nsemantic similarity assessment, with scores more closely aligned with clinical\nground truth than traditional NLP metrics. Our work demonstrates the\npossibility of conducting semantic analysis of the text data using\nsemi-quantitative reasoning results by the LLMs for highly specialized domains.\nWhile the framework is implemented for radiology report similarity analysis,\nits concept can be extended to other specialized domains as well.", "published": "2024-02-17 22:46:44", "link": "http://arxiv.org/abs/2402.11398v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Detecting a Proxy for Potential Comorbid ADHD in People Reporting\n  Anxiety Symptoms from Social Media Data", "abstract": "We present a novel task that can elucidate the connection between anxiety and\nADHD; use Transformers to make progress toward solving a task that is not\nsolvable by keyword-based classifiers; and discuss a method for visualization\nof our classifier illuminating the connection between anxiety and ADHD\npresentations.\n  Up to approximately 50% of adults with ADHD may also have an anxiety disorder\nand approximately 30\\% of adults with anxiety may also have ADHD. Patients\npresenting with anxiety may be treated for anxiety without ADHD ever being\nconsidered, possibly affecting treatment. We show how data that bears on ADHD\nthat is comorbid with anxiety can be obtained from social media data, and show\nthat Transformers can be used to detect a proxy for possible comorbid ADHD in\npeople with anxiety symptoms.\n  We collected data from anxiety and ADHD online forums (subreddits). We\nidentified posters who first started posting in the Anxiety subreddit and later\nstarted posting in the ADHD subreddit as well. We use this subset of the\nposters as a proxy for people who presented with anxiety symptoms and then\nbecame aware that they might have ADHD. We fine-tune a Transformer\narchitecture-based classifier to classify people who started posting in the\nAnxiety subreddit and then started posting in the ADHD subreddit vs. people who\nposted in the Anxiety subreddit without later posting in the ADHD subreddit. We\nshow that a Transformer architecture is capable of achieving reasonable results\n(76% correct for RoBERTa vs. under 60% correct for the best keyword-based\nmodel, both with 50% base rate).", "published": "2024-02-17 10:32:43", "link": "http://arxiv.org/abs/2403.05561v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Contrastive Instruction Tuning", "abstract": "Instruction tuning has been used as a promising approach to improve the\nperformance of large language models (LLMs) on unseen tasks. However, current\nLLMs exhibit limited robustness to unseen instructions, generating inconsistent\noutputs when the same instruction is phrased with slightly varied forms or\nlanguage styles. This behavior indicates LLMs' lack of robustness to textual\nvariations and generalizability to unseen instructions, potentially leading to\ntrustworthiness issues. Accordingly, we propose Contrastive Instruction Tuning,\nwhich maximizes the similarity between the hidden representations of\nsemantically equivalent instruction-instance pairs while minimizing the\nsimilarity between semantically different ones. To facilitate this approach, we\naugment the existing FLAN collection by paraphrasing task instructions.\nExperiments on the PromptBench benchmark show that CoIN consistently improves\nLLMs' robustness to unseen instructions with variations across character, word,\nsentence, and semantic levels by an average of +2.5% in accuracy. Code is\navailable at https://github.com/luka-group/CoIN.", "published": "2024-02-17 00:09:32", "link": "http://arxiv.org/abs/2402.11138v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Boosting of Thoughts: Trial-and-Error Problem Solving with Large\n  Language Models", "abstract": "The reasoning performance of Large Language Models (LLMs) on a wide range of\nproblems critically relies on chain-of-thought prompting, which involves\nproviding a few chain of thought demonstrations as exemplars in prompts. Recent\nwork, e.g., Tree of Thoughts, has pointed out the importance of exploration and\nself-evaluation in reasoning step selection for complex problem solving. In\nthis paper, we present Boosting of Thoughts (BoT), an automated prompting\nframework for problem solving with LLMs by iteratively exploring and\nself-evaluating many trees of thoughts in order to acquire an ensemble of\ntrial-and-error reasoning experiences, which will serve as a new form of\nprompting to solve the complex problem. Starting from a simple prompt without\nrequiring examples, BoT iteratively explores and evaluates a large collection\nof reasoning steps, and more importantly, uses error analysis obtained from the\nLLM on them to explicitly revise prompting, which in turn enhances reasoning\nstep generation, until a final answer is attained. Our experiments with GPT-4\nand Llama2 across extensive complex mathematical problems demonstrate that BoT\nconsistently achieves higher or comparable problem-solving rates than other\nadvanced prompting approaches.", "published": "2024-02-17 00:13:36", "link": "http://arxiv.org/abs/2402.11140v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploring ChatGPT for Next-generation Information Retrieval:\n  Opportunities and Challenges", "abstract": "The rapid advancement of artificial intelligence (AI) has highlighted ChatGPT\nas a pivotal technology in the field of information retrieval (IR).\nDistinguished from its predecessors, ChatGPT offers significant benefits that\nhave attracted the attention of both the industry and academic communities.\nWhile some view ChatGPT as a groundbreaking innovation, others attribute its\nsuccess to the effective integration of product development and market\nstrategies. The emergence of ChatGPT, alongside GPT-4, marks a new phase in\nGenerative AI, generating content that is distinct from training examples and\nexceeding the capabilities of the prior GPT-3 model by OpenAI. Unlike the\ntraditional supervised learning approach in IR tasks, ChatGPT challenges\nexisting paradigms, bringing forth new challenges and opportunities regarding\ntext quality assurance, model bias, and efficiency. This paper seeks to examine\nthe impact of ChatGPT on IR tasks and offer insights into its potential future\ndevelopments.", "published": "2024-02-17 05:44:40", "link": "http://arxiv.org/abs/2402.11203v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based\n  Agents", "abstract": "Driven by the rapid development of Large Language Models (LLMs), LLM-based\nagents have been developed to handle various real-world applications, including\nfinance, healthcare, and shopping, etc. It is crucial to ensure the reliability\nand security of LLM-based agents during applications. However, the safety\nissues of LLM-based agents are currently under-explored. In this work, we take\nthe first step to investigate one of the typical safety threats, backdoor\nattack, to LLM-based agents. We first formulate a general framework of agent\nbackdoor attacks, then we present a thorough analysis of different forms of\nagent backdoor attacks. Specifically, compared with traditional backdoor\nattacks on LLMs that are only able to manipulate the user inputs and model\noutputs, agent backdoor attacks exhibit more diverse and covert forms: (1) From\nthe perspective of the final attacking outcomes, the agent backdoor attacker\ncan not only choose to manipulate the final output distribution, but also\nintroduce the malicious behavior in an intermediate reasoning step only, while\nkeeping the final output correct. (2) Furthermore, the former category can be\ndivided into two subcategories based on trigger locations, in which the\nbackdoor trigger can either be hidden in the user query or appear in an\nintermediate observation returned by the external environment. We implement the\nabove variations of agent backdoor attacks on two typical agent tasks including\nweb shopping and tool utilization. Extensive experiments show that LLM-based\nagents suffer severely from backdoor attacks and such backdoor vulnerability\ncannot be easily mitigated by current textual backdoor defense algorithms. This\nindicates an urgent need for further research on the development of targeted\ndefenses against backdoor attacks on LLM-based agents. Warning: This paper may\ncontain biased content.", "published": "2024-02-17 06:48:45", "link": "http://arxiv.org/abs/2402.11208v2", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Aligning Large Language Models by On-Policy Self-Judgment", "abstract": "Existing approaches for aligning large language models with human preferences\nface a trade-off that requires a separate reward model (RM) for on-policy\nlearning. In this paper, we present a novel alignment framework, SELF-JUDGE\nthat (1) does on-policy learning and 2) is parameter efficient, as it does not\nrequire an additional RM for evaluating the samples for on-policy learning. To\nthis end, we propose Judge-augmented Supervised Fine-Tuning (JSFT) to train a\nsingle model to act as both a policy and a judge. Specifically, we view the\npairwise judgment task, choosing the better response from a response pair, as a\nspecial case of the instruction-following task. The resulting model can judge\npreferences of on-the-fly responses from current policy initialized from\nitself. Experimental results show the efficacy of SELF-JUDGE, outperforming\nbaselines in preference benchmarks. We also show that the rejecting sampling by\nitself can improve performance further without an additional evaluator.", "published": "2024-02-17 11:25:26", "link": "http://arxiv.org/abs/2402.11253v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "MONAL: Model Autophagy Analysis for Modeling Human-AI Interactions", "abstract": "The increasing significance of large models and their multi-modal variants in\nsocietal information processing has ignited debates on social safety and\nethics. However, there exists a paucity of comprehensive analysis for: (i) the\ninteractions between human and artificial intelligence systems, and (ii)\nunderstanding and addressing the associated limitations. To bridge this gap, we\npropose Model Autophagy Analysis (MONAL) for large models' self-consumption\nexplanation. MONAL employs two distinct autophagous loops (referred to as\n``self-consumption loops'') to elucidate the suppression of human-generated\ninformation in the exchange between human and AI systems. Through comprehensive\nexperiments on diverse datasets, we evaluate the capacities of generated models\nas both creators and disseminators of information. Our key findings reveal (i)\nA progressive prevalence of model-generated synthetic information over time\nwithin training datasets compared to human-generated information; (ii) The\ndiscernible tendency of large models, when acting as information transmitters\nacross multiple iterations, to selectively modify or prioritize specific\ncontents; and (iii) The potential for a reduction in the diversity of socially\nor human-generated information, leading to bottlenecks in the performance\nenhancement of large models and confining them to local optima.", "published": "2024-02-17 13:02:54", "link": "http://arxiv.org/abs/2402.11271v2", "categories": ["cs.CL", "cs.CY", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Understanding the Impact of Long-Term Memory on Self-Disclosure with\n  Large Language Model-Driven Chatbots for Public Health Intervention", "abstract": "Recent large language models (LLMs) offer the potential to support public\nhealth monitoring by facilitating health disclosure through open-ended\nconversations but rarely preserve the knowledge gained about individuals across\nrepeated interactions. Augmenting LLMs with long-term memory (LTM) presents an\nopportunity to improve engagement and self-disclosure, but we lack an\nunderstanding of how LTM impacts people's interaction with LLM-driven chatbots\nin public health interventions. We examine the case of CareCall -- an\nLLM-driven voice chatbot with LTM -- through the analysis of 1,252 call logs\nand interviews with nine users. We found that LTM enhanced health disclosure\nand fostered positive perceptions of the chatbot by offering familiarity.\nHowever, we also observed challenges in promoting self-disclosure through LTM,\nparticularly around addressing chronic health conditions and privacy concerns.\nWe discuss considerations for LTM integration in LLM-driven chatbots for public\nhealth monitoring, including carefully deciding what topics need to be\nremembered in light of public health goals.", "published": "2024-02-17 18:05:53", "link": "http://arxiv.org/abs/2402.11353v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "H.5.2; I.2.7"], "primary_category": "cs.HC"}
{"title": "A Practical Method for Generating String Counterfactuals", "abstract": "Interventions targeting the representation space of language models (LMs)\nhave emerged as an effective means to influence model behavior. Such methods\nare employed, for example, to eliminate or alter the encoding of demographic\ninformation such as gender within the model's representations and, in so doing,\ncreate a counterfactual representation. However, because the intervention\noperates within the representation space, understanding precisely what aspects\nof the text it modifies poses a challenge. In this paper, we give a method to\nconvert representation counterfactuals into string counterfactuals. We\ndemonstrate that this approach enables us to analyze the linguistic alterations\ncorresponding to a given representation space intervention and to interpret the\nfeatures utilized to encode a specific concept. Moreover, the resulting\ncounterfactuals can be used to mitigate bias in classification through data\naugmentation.", "published": "2024-02-17 18:12:02", "link": "http://arxiv.org/abs/2402.11355v5", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "k-SemStamp: A Clustering-Based Semantic Watermark for Detection of\n  Machine-Generated Text", "abstract": "Recent watermarked generation algorithms inject detectable signatures during\nlanguage generation to facilitate post-hoc detection. While token-level\nwatermarks are vulnerable to paraphrase attacks, SemStamp (Hou et al., 2023)\napplies watermark on the semantic representation of sentences and demonstrates\npromising robustness. SemStamp employs locality-sensitive hashing (LSH) to\npartition the semantic space with arbitrary hyperplanes, which results in a\nsuboptimal tradeoff between robustness and speed. We propose k-SemStamp, a\nsimple yet effective enhancement of SemStamp, utilizing k-means clustering as\nan alternative of LSH to partition the embedding space with awareness of\ninherent semantic structure. Experimental results indicate that k-SemStamp\nsaliently improves its robustness and sampling efficiency while preserving the\ngeneration quality, advancing a more effective tool for machine-generated text\ndetection.", "published": "2024-02-17 22:50:38", "link": "http://arxiv.org/abs/2402.11399v2", "categories": ["cs.CL", "cs.CR", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Turn Waste into Worth: Rectifying Top-$k$ Router of MoE", "abstract": "Sparse Mixture of Experts (MoE) models are popular for training large\nlanguage models due to their computational efficiency. However, the commonly\nused top-$k$ routing mechanism suffers from redundancy computation and memory\ncosts due to the unbalanced routing. Some experts are overflow, where the\nexceeding tokens are dropped. While some experts are vacant, which are padded\nwith zeros, negatively impacting model performance. To address the dropped\ntokens and padding, we propose the Rectify-Router, comprising the Intra-GPU\nRectification and the Fill-in Rectification. The Intra-GPU Rectification\nhandles dropped tokens, efficiently routing them to experts within the GPU\nwhere they are located to avoid inter-GPU communication. The Fill-in\nRectification addresses padding by replacing padding tokens with the tokens\nthat have high routing scores. Our experimental results demonstrate that the\nIntra-GPU Rectification and the Fill-in Rectification effectively handle\ndropped tokens and padding, respectively. Furthermore, the combination of them\nachieves superior performance, surpassing the accuracy of the vanilla top-1\nrouter by 4.7%.", "published": "2024-02-17 06:23:27", "link": "http://arxiv.org/abs/2402.12399v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "CliqueParcel: An Approach For Batching LLM Prompts That Jointly\n  Optimizes Efficiency And Faithfulness", "abstract": "Large language models (LLMs) have become pivotal in recent research. However,\nduring the inference process, LLMs still require substantial resources. In this\npaper, we propose CliqueParcel, a method designed to improve the efficiency of\nLLMs via prompt batching. Existing strategies to optimize inference efficiency\noften compromise on output quality, leading to a discounted output problem.\nThis issue might result in reduced accuracy or outputs that are less detailed.\nCliqueParcel is our answer to this challenge. While ensuring accuracy and\nminimizing deviations from the original outputs (i.e., faithfulness), our\nmethod significantly improves efficiency during inference.\n  To lay the groundwork, we first redefine efficiency measurements by excluding\nthe reduction in running time due to shorter lengths. Then, we provide a\ncomprehensive trade-off between efficiency and faithfulness to clarify the\nnature of the 'discounted output' problem. Within the CliqueParcel framework,\nwe suggest multiple batching sub-methods and discuss the specific scenarios in\nwhich they can be applied. During evaluation, CliqueParcel is tested on eight\nwidely recognized datasets, which can be classified into three types: reading\ncomprehension, open-source question-answering, and reasoning. Our experiments\nexplore the performance of CliqueParcel, including efficiency, faithfulness,\nand the trade-off between them. This work provides novel insights into\ninference efficiency and demonstrates promising performance.", "published": "2024-02-17 22:37:17", "link": "http://arxiv.org/abs/2402.14833v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Optimizing tiny colorless feedback delay networks", "abstract": "A common bane of artificial reverberation algorithms is spectral coloration\nin the synthesized sound, typically manifesting as metallic ringing, leading to\na degradation in the perceived sound quality. In delay network methods,\ncoloration is more pronounced when fewer delay lines are used. This paper\npresents an optimization framework in which a tiny differentiable feedback\ndelay network, with as few as four delay lines, is used to learn a set of\nparameters to iteratively reduce coloration. The parameters under optimization\ninclude the feedback matrix, as well as the input and output gains. The\noptimization objective is twofold: to maximize spectral flatness through a\nspectral loss while maintaining temporal density by penalizing sparseness in\nthe parameter values. A favorable narrow distribution of modal excitation is\nachieved while maintaining the desired impulse response density. In a\nsubjective assessment, the new method proves effective in reducing perceptual\ncoloration of late reverberation. Compared to the author's previous work, which\nserves as the baseline and utilizes a sparsity loss in the time domain, the\nproposed method achieves computational savings while maintaining performance.\nThe effectiveness of this work is demonstrated through two application\nscenarios where smooth-sounding synthetic room impulse responses are obtained\nvia the introduction of attenuation filters and an optimizable scattering\nfeedback matrix.", "published": "2024-02-17 07:52:21", "link": "http://arxiv.org/abs/2402.11216v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Diffuse Sound Field Synthesis", "abstract": "Can uncorrelated surrounding sound sources be used to generate extended\ndiffuse sound fields? By definition, targets are a constant sound pressure\nlevel, a vanishing average sound intensity, uncorrelated sound waves arriving\nisotropically from all directions. Does this require specific sources and\ngeometries for surrounding 2D and 3D source layouts?\n  As methods, we employ numeric simulations and undertake a series of\ncalculations with uncorrelated circular/spherical source layouts, or such with\ninfinite excess dimensions, and we point out relations to potential theory.\nUsing a radial decay 1/r^b modified by the exponent b, the representation of\nthe resulting fields with hypergeometric functions, Gegenbauer polynomials, and\ncircular as well as spherical harmonics yields fruitful insights.\n  In circular layouts, waves decaying by the exponent b=1/2 synthesize ideally\nextended, diffuse sound fields; spherical layouts do so with b=1. None of the\nlayouts synthesizes a perfectly constant expected sound pressure level but its\nflatness is acceptable.\n  Spherical t-designs describe optimal source layouts with well-described area\nof high diffuseness, and non-spherical, convex layouts can be improved by\nrestoring isotropy or by mode matching for a maximally diffuse synthesis.\n  Theory and simulation offer a basis for loudspeaker-based synthesis of\ndiffuse sound fields and contribute physical reasons to recent psychoacoustic\nfindings in spatial audio.", "published": "2024-02-17 16:59:08", "link": "http://arxiv.org/abs/2402.11330v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Target Speech Extraction with Pre-trained Self-supervised Learning\n  Models", "abstract": "Pre-trained self-supervised learning (SSL) models have achieved remarkable\nsuccess in various speech tasks. However, their potential in target speech\nextraction (TSE) has not been fully exploited. TSE aims to extract the speech\nof a target speaker in a mixture guided by enrollment utterances. We exploit\npre-trained SSL models for two purposes within a TSE framework, i.e., to\nprocess the input mixture and to derive speaker embeddings from the enrollment.\nIn this paper, we focus on how to effectively use SSL models for TSE. We first\nintroduce a novel TSE downstream task following the SUPERB principles. This\nsimple experiment shows the potential of SSL models for TSE, but extraction\nperformance remains far behind the state-of-the-art. We then extend a powerful\nTSE architecture by incorporating two SSL-based modules: an Adaptive Input\nEnhancer (AIE) and a speaker encoder. Specifically, the proposed AIE utilizes\nintermediate representations from the CNN encoder by adjusting the time\nresolution of CNN encoder and transformer blocks through progressive\nupsampling, capturing both fine-grained and hierarchical features. Our method\noutperforms current TSE systems achieving a SI-SDR improvement of 14.0 dB on\nLibriMix. Moreover, we can further improve performance by 0.7 dB by fine-tuning\nthe whole model including the SSL model parameters.", "published": "2024-02-17 13:45:00", "link": "http://arxiv.org/abs/2402.13199v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Probing Self-supervised Learning Models with Target Speech Extraction", "abstract": "Large-scale pre-trained self-supervised learning (SSL) models have shown\nremarkable advancements in speech-related tasks. However, the utilization of\nthese models in complex multi-talker scenarios, such as extracting a target\nspeaker in a mixture, is yet to be fully evaluated. In this paper, we introduce\ntarget speech extraction (TSE) as a novel downstream task to evaluate the\nfeature extraction capabilities of pre-trained SSL models. TSE uniquely\nrequires both speaker identification and speech separation, distinguishing it\nfrom other tasks in the Speech processing Universal PERformance Benchmark\n(SUPERB) evaluation. Specifically, we propose a TSE downstream model composed\nof two lightweight task-oriented modules based on the same frozen SSL model.\nOne module functions as a speaker encoder to obtain target speaker information\nfrom an enrollment speech, while the other estimates the target speaker's mask\nto extract its speech from the mixture. Experimental results on the Libri2mix\ndatasets reveal the relevance of the TSE downstream task to probe SSL models,\nas its performance cannot be simply deduced from other related tasks such as\nspeaker verification and separation.", "published": "2024-02-17 13:37:22", "link": "http://arxiv.org/abs/2402.13200v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "When LLMs Meets Acoustic Landmarks: An Efficient Approach to Integrate\n  Speech into Large Language Models for Depression Detection", "abstract": "Depression is a critical concern in global mental health, prompting extensive\nresearch into AI-based detection methods. Among various AI technologies, Large\nLanguage Models (LLMs) stand out for their versatility in mental healthcare\napplications. However, their primary limitation arises from their exclusive\ndependence on textual input, which constrains their overall capabilities.\nFurthermore, the utilization of LLMs in identifying and analyzing depressive\nstates is still relatively untapped. In this paper, we present an innovative\napproach to integrating acoustic speech information into the LLMs framework for\nmultimodal depression detection. We investigate an efficient method for\ndepression detection by integrating speech signals into LLMs utilizing Acoustic\nLandmarks. By incorporating acoustic landmarks, which are specific to the\npronunciation of spoken words, our method adds critical dimensions to text\ntranscripts. This integration also provides insights into the unique speech\npatterns of individuals, revealing the potential mental states of individuals.\nEvaluations of the proposed approach on the DAIC-WOZ dataset reveal\nstate-of-the-art results when compared with existing Audio-Text baselines. In\naddition, this approach is not only valuable for the detection of depression\nbut also represents a new perspective in enhancing the ability of LLMs to\ncomprehend and process speech signals.", "published": "2024-02-17 09:39:46", "link": "http://arxiv.org/abs/2402.13276v2", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
