{"title": "Community Member Retrieval on Social Media using Textual Information", "abstract": "This paper addresses the problem of community membership detection using only\ntext features in a scenario where a small number of positive labeled examples\ndefines the community. The solution introduces an unsupervised proxy task for\nlearning user embeddings: user re-identification. Experiments with 16 different\ncommunities show that the resulting embeddings are more effective for community\nmembership identification than common unsupervised representations.", "published": "2018-04-16 04:03:52", "link": "http://arxiv.org/abs/1804.05499v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Arabic Named Entity Recognition using Word Representations", "abstract": "Recent work has shown the effectiveness of the word representations features\nin significantly improving supervised NER for the English language. In this\nstudy we investigate whether word representations can also boost supervised NER\nin Arabic. We use word representations as additional features in a Conditional\nRandom Field (CRF) model and we systematically compare three popular neural\nword embedding algorithms (SKIP-gram, CBOW and GloVe) and six different\napproaches for integrating word representations into NER system. Experimental\nresults show that Brown Clustering achieves the best performance among the six\napproaches. Concerning the word embedding features, the clustering embedding\nfeatures outperform other embedding features and the distributional prototypes\nproduce the second best result. Moreover, the combination of Brown clusters and\nword embedding features provides additional improvement of nearly 10% in\nF1-score over the baseline.", "published": "2018-04-16 12:08:13", "link": "http://arxiv.org/abs/1804.05630v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Discourse-Aware Attention Model for Abstractive Summarization of Long\n  Documents", "abstract": "Neural abstractive summarization models have led to promising results in\nsummarizing relatively short documents. We propose the first model for\nabstractive summarization of single, longer-form documents (e.g., research\npapers). Our approach consists of a new hierarchical encoder that models the\ndiscourse structure of a document, and an attentive discourse-aware decoder to\ngenerate the summary. Empirical results on two large-scale datasets of\nscientific papers show that our model significantly outperforms\nstate-of-the-art models.", "published": "2018-04-16 13:55:20", "link": "http://arxiv.org/abs/1804.05685v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Organization and Independence or Interdependence? Study of the\n  Neurophysiological Dynamics of Syntactic and Semantic Processing", "abstract": "In this article we present a multivariate model for determining the different\nsyntactic, semantic, and form (surface-structure) processes underlying the\ncomprehension of simple phrases. This model is applied to EEG signals recorded\nduring a reading task. The results show a hierarchical precedence of the\nneurolinguistic processes : form, then syntactic and lastly semantic processes.\nWe also found (a) that verbs are at the heart of phrase syntax processing, (b)\nan interaction between syntactic movement within the phrase, and semantic\nprocesses derived from a person-centered reference frame. Eigenvectors of the\nmultivariate model provide electrode-times profiles that separate the\ndistinctive linguistic processes and/or highlight their interaction. The\naccordance of these findings with different linguistic theories are discussed.", "published": "2018-04-16 13:59:08", "link": "http://arxiv.org/abs/1804.05686v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Relevance of Text and Speech Features in Automatic Non-native\n  English Accent Identification", "abstract": "This paper describes our experiments with automatically identifying native\naccents from speech samples of non-native English speakers using low level\naudio features, and n-gram features from manual transcriptions. Using a\npublicly available non-native speech corpus and simple audio feature\nrepresentations that do not perform word/phoneme recognition, we show that it\nis possible to achieve close to 90% classification accuracy for this task.\nWhile character n-grams perform similar to speech features, we show that speech\nfeatures are not affected by prompt variation, whereas ngrams are. Since the\napproach followed can be easily adapted to any language provided we have enough\ntraining data, we believe these results will provide useful insights for the\ndevelopment of accent recognition systems and for the study of accents in the\ncontext of language learning.", "published": "2018-04-16 14:03:45", "link": "http://arxiv.org/abs/1804.05689v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning How to Self-Learn: Enhancing Self-Training Using Neural\n  Reinforcement Learning", "abstract": "Self-training is a useful strategy for semi-supervised learning, leveraging\nraw texts for enhancing model performances. Traditional self-training methods\ndepend on heuristics such as model confidence for instance selection, the\nmanual adjustment of which can be expensive. To address these challenges, we\npropose a deep reinforcement learning method to learn the self-training\nstrategy automatically. Based on neural network representation of sentences,\nour model automatically learns an optimal policy for instance selection.\nExperimental results show that our approach outperforms the baseline solutions\nin terms of better tagging performances and stability.", "published": "2018-04-16 15:25:53", "link": "http://arxiv.org/abs/1804.05734v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ClaiRE at SemEval-2018 Task 7 - Extended Version", "abstract": "In this paper we describe our post-evaluation results for SemEval-2018 Task 7\non clas- sification of semantic relations in scientific literature for clean\n(subtask 1.1) and noisy data (subtask 1.2). This is an extended ver- sion of\nour workshop paper (Hettinger et al., 2018) including further technical details\n(Sec- tions 3.2 and 4.3) and changes made to the preprocessing step in the\npost-evaluation phase (Section 2.1). Due to these changes Classification of\nRelations using Embeddings (ClaiRE) achieved an improved F1 score of 75.11% for\nthe first subtask and 81.44% for the second.", "published": "2018-04-16 17:52:36", "link": "http://arxiv.org/abs/1804.05825v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Universal Dependency Parsing for Hindi-English Code-switching", "abstract": "Code-switching is a phenomenon of mixing grammatical structures of two or\nmore languages under varied social constraints. The code-switching data differ\nso radically from the benchmark corpora used in NLP community that the\napplication of standard technologies to these data degrades their performance\nsharply. Unlike standard corpora, these data often need to go through\nadditional processes such as language identification, normalization and/or\nback-transliteration for their efficient processing. In this paper, we\ninvestigate these indispensable processes and other problems associated with\nsyntactic parsing of code-switching data and propose methods to mitigate their\neffects. In particular, we study dependency parsing of code-switching data of\nHindi and English multilingual speakers from Twitter. We present a treebank of\nHindi-English code-switching tweets under Universal Dependencies scheme and\npropose a neural stacking model for parsing that efficiently leverages\npart-of-speech tag and syntactic tree annotations in the code-switching\ntreebank and the preexisting Hindi and English treebanks. We also present\nnormalization and back-transliteration models with a decoding process tailored\nfor code-switching data. Results show that our neural stacking parser is 1.5%\nLAS points better than the augmented parsing model and our decoding process\nimproves results by 3.8% LAS points over the first-best normalization and/or\nback-transliteration.", "published": "2018-04-16 18:05:52", "link": "http://arxiv.org/abs/1804.05868v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Implicit Discourse Relation Classification by Modeling\n  Inter-dependencies of Discourse Units in a Paragraph", "abstract": "We argue that semantic meanings of a sentence or clause can not be\ninterpreted independently from the rest of a paragraph, or independently from\nall discourse relations and the overall paragraph-level discourse structure.\nWith the goal of improving implicit discourse relation classification, we\nintroduce a paragraph-level neural networks that model inter-dependencies\nbetween discourse units as well as discourse relation continuity and patterns,\nand predict a sequence of discourse relations in a paragraph. Experimental\nresults show that our model outperforms the previous state-of-the-art systems\non the benchmark corpus of PDTB.", "published": "2018-04-16 20:01:06", "link": "http://arxiv.org/abs/1804.05918v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Approaching Neural Grammatical Error Correction as a Low-Resource\n  Machine Translation Task", "abstract": "Previously, neural methods in grammatical error correction (GEC) did not\nreach state-of-the-art results compared to phrase-based statistical machine\ntranslation (SMT) baselines. We demonstrate parallels between neural GEC and\nlow-resource neural MT and successfully adapt several methods from low-resource\nMT to neural GEC. We further establish guidelines for trustable results in\nneural GEC and propose a set of model-independent methods for neural GEC that\ncan be easily applied in most GEC settings. Proposed methods include adding\nsource-side noise, domain-adaptation techniques, a GEC-specific\ntraining-objective, transfer learning with monolingual data, and ensembling of\nindependently trained GEC models and language models. The combined effects of\nthese methods result in better than state-of-the-art neural GEC models that\noutperform previously best neural GEC systems by more than 10% M$^2$ on the\nCoNLL-2014 benchmark and 5.9% on the JFLEG test set. Non-neural\nstate-of-the-art systems are outperformed by more than 2% on the CoNLL-2014\nbenchmark and by 4% on JFLEG.", "published": "2018-04-16 21:06:14", "link": "http://arxiv.org/abs/1804.05940v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Near Human-Level Performance in Grammatical Error Correction with Hybrid\n  Machine Translation", "abstract": "We combine two of the most popular approaches to automated Grammatical Error\nCorrection (GEC): GEC based on Statistical Machine Translation (SMT) and GEC\nbased on Neural Machine Translation (NMT). The hybrid system achieves new\nstate-of-the-art results on the CoNLL-2014 and JFLEG benchmarks. This GEC\nsystem preserves the accuracy of SMT output and, at the same time, generates\nmore fluent sentences as it typical for NMT. Our analysis shows that the\ncreated systems are closer to reaching human-level performance than any other\nGEC system reported so far.", "published": "2018-04-16 21:13:31", "link": "http://arxiv.org/abs/1804.05945v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Deeper Look into Dependency-Based Word Embeddings", "abstract": "We investigate the effect of various dependency-based word embeddings on\ndistinguishing between functional and domain similarity, word similarity\nrankings, and two downstream tasks in English. Variations include word\nembeddings trained using context windows from Stanford and Universal\ndependencies at several levels of enhancement (ranging from unlabeled, to\nEnhanced++ dependencies). Results are compared to basic linear contexts and\nevaluated on several datasets. We found that embeddings trained with Universal\nand Stanford dependency contexts excel at different tasks, and that enhanced\ndependencies often improve performance.", "published": "2018-04-16 23:12:07", "link": "http://arxiv.org/abs/1804.05972v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Models for Reasoning over Multiple Mentions using Coreference", "abstract": "Many problems in NLP require aggregating information from multiple mentions\nof the same entity which may be far apart in the text. Existing Recurrent\nNeural Network (RNN) layers are biased towards short-term dependencies and\nhence not suited to such tasks. We present a recurrent layer which is instead\nbiased towards coreferent dependencies. The layer uses coreference annotations\nextracted from an external system to connect entity mentions belonging to the\nsame cluster. Incorporating this layer into a state-of-the-art reading\ncomprehension model improves performance on three datasets -- Wikihop, LAMBADA\nand the bAbi AI tasks -- with large gains when training data is scarce.", "published": "2018-04-16 20:07:44", "link": "http://arxiv.org/abs/1804.05922v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Can Neural Machine Translation be Improved with User Feedback?", "abstract": "We present the first real-world application of methods for improving neural\nmachine translation (NMT) with human reinforcement, based on explicit and\nimplicit user feedback collected on the eBay e-commerce platform. Previous work\nhas been confined to simulation experiments, whereas in this paper we work with\nreal logged feedback for offline bandit learning of NMT parameters. We conduct\na thorough analysis of the available explicit user judgments---five-star\nratings of translation quality---and show that they are not reliable enough to\nyield significant improvements in bandit learning. In contrast, we successfully\nutilize implicit task-based feedback collected in a cross-lingual search task\nto improve task-specific and machine translation quality metrics.", "published": "2018-04-16 21:55:45", "link": "http://arxiv.org/abs/1804.05958v1", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Computing Information Quantity as Similarity Measure for Music\n  Classification Task", "abstract": "This paper proposes a novel method that can replace compression-based\ndissimilarity measure (CDM) in composer estimation task. The main features of\nthe proposed method are clarity and scalability. First, since the proposed\nmethod is formalized by the information quantity, reproduction of the result is\neasier compared with the CDM method, where the result depends on a particular\ncompression program. Second, the proposed method has a lower computational\ncomplexity in terms of the number of learning data compared with the CDM\nmethod. The number of correct results was compared with that of the CDM for the\ncomposer estimation task of five composers of 75 piano musical scores. The\nproposed method performed better than the CDM method that uses the file size\ncompressed by a particular program.", "published": "2018-04-16 02:59:25", "link": "http://arxiv.org/abs/1804.05486v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
