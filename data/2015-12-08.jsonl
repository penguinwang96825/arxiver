{"title": "Minimum Risk Training for Neural Machine Translation", "abstract": "We propose minimum risk training for end-to-end neural machine translation.\nUnlike conventional maximum likelihood estimation, minimum risk training is\ncapable of optimizing model parameters directly with respect to arbitrary\nevaluation metrics, which are not necessarily differentiable. Experiments show\nthat our approach achieves significant improvements over maximum likelihood\nestimation on a state-of-the-art neural machine translation system across\nvarious languages pairs. Transparent to architectures, our approach can be\napplied to more neural networks and potentially benefit more NLP tasks.", "published": "2015-12-08 12:42:00", "link": "http://arxiv.org/abs/1512.02433v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep Speech 2: End-to-End Speech Recognition in English and Mandarin", "abstract": "We show that an end-to-end deep learning approach can be used to recognize\neither English or Mandarin Chinese speech--two vastly different languages.\nBecause it replaces entire pipelines of hand-engineered components with neural\nnetworks, end-to-end learning allows us to handle a diverse variety of speech\nincluding noisy environments, accents and different languages. Key to our\napproach is our application of HPC techniques, resulting in a 7x speedup over\nour previous system. Because of this efficiency, experiments that previously\ntook weeks now run in days. This enables us to iterate more quickly to identify\nsuperior architectures and algorithms. As a result, in several cases, our\nsystem is competitive with the transcription of human workers when benchmarked\non standard datasets. Finally, using a technique called Batch Dispatch with\nGPUs in the data center, we show that our system can be inexpensively deployed\nin an online setting, delivering low latency when serving users at scale.", "published": "2015-12-08 19:13:50", "link": "http://arxiv.org/abs/1512.02595v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Distributed Adaptive LMF Algorithm for Sparse Parameter Estimation in\n  Gaussian Mixture Noise", "abstract": "A distributed adaptive algorithm for estimation of sparse unknown parameters\nin the presence of nonGaussian noise is proposed in this paper based on\nnormalized least mean fourth (NLMF) criterion. At the first step, local\nadaptive NLMF algorithm is modified by zero norm in order to speed up the\nconvergence rate and also to reduce the steady state error power in sparse\nconditions. Then, the proposed algorithm is extended for distributed scenario\nin which more improvement in estimation performance is achieved due to\ncooperation of local adaptive filters. Simulation results show the superiority\nof the proposed algorithm in comparison with conventional NLMF algorithms.", "published": "2015-12-08 17:54:17", "link": "http://arxiv.org/abs/1512.02567v1", "categories": ["cs.IT", "cs.CL", "math.IT"], "primary_category": "cs.IT"}
