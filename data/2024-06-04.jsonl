{"title": "Mean field equilibrium asset pricing model with habit formation", "abstract": "This paper presents an asset pricing model in an incomplete market involving\na large number of heterogeneous agents based on the mean field game theory. In\nthe model, we incorporate habit formation in consumption preferences, which has\nbeen widely used to explain various phenomena in financial economics. In order\nto characterize the market-clearing equilibrium, we derive a quadratic-growth\nmean field backward stochastic differential equation (BSDE) and study its\nwell-posedness and asymptotic behavior in the large population limit.\nAdditionally, we introduce an exponential quadratic Gaussian reformulation of\nthe asset pricing model, in which the solution is obtained in a semi-analytic\nform.", "published": "2024-06-04 09:44:24", "link": "http://arxiv.org/abs/2406.02155v2", "categories": ["q-fin.MF", "q-fin.PR", "q-fin.TR", "49N80, 91B51, 60H10"], "primary_category": "q-fin.MF"}
{"title": "Eliciting the Priors of Large Language Models using Iterated In-Context\n  Learning", "abstract": "As Large Language Models (LLMs) are increasingly deployed in real-world\nsettings, understanding the knowledge they implicitly use when making decisions\nis critical. One way to capture this knowledge is in the form of Bayesian prior\ndistributions. We develop a prompt-based workflow for eliciting prior\ndistributions from LLMs. Our approach is based on iterated learning, a Markov\nchain Monte Carlo method in which successive inferences are chained in a way\nthat supports sampling from the prior distribution. We validated our method in\nsettings where iterated learning has previously been used to estimate the\npriors of human participants -- causal learning, proportion estimation, and\npredicting everyday quantities. We found that priors elicited from GPT-4\nqualitatively align with human priors in these settings. We then used the same\nmethod to elicit priors from GPT-4 for a variety of speculative events, such as\nthe timing of the development of superhuman AI.", "published": "2024-06-04 00:09:43", "link": "http://arxiv.org/abs/2406.01860v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Effective Time-Aware Language Representation: Exploring Enhanced\n  Temporal Understanding in Language Models", "abstract": "In the evolving field of Natural Language Processing (NLP), understanding the\ntemporal context of text is increasingly critical for applications requiring\nadvanced temporal reasoning. Traditional pre-trained language models like BERT,\nwhich rely on synchronic document collections such as BookCorpus and Wikipedia,\noften fall short in effectively capturing and leveraging temporal information.\nTo address this limitation, we introduce BiTimeBERT 2.0, a novel time-aware\nlanguage model pre-trained on a temporal news article collection. BiTimeBERT\n2.0 incorporates temporal information through three innovative pre-training\nobjectives: Extended Time-Aware Masked Language Modeling (ETAMLM), Document\nDating (DD), and Time-Sensitive Entity Replacement (TSER). Each objective is\nspecifically designed to target a distinct dimension of temporal information:\nETAMLM enhances the model's understanding of temporal contexts and relations,\nDD integrates document timestamps as explicit chronological markers, and TSER\nfocuses on the temporal dynamics of \"Person\" entities. Moreover, our refined\ncorpus preprocessing strategy reduces training time by nearly 53\\%, making\nBiTimeBERT 2.0 significantly more efficient while maintaining high performance.\nExperimental results show that BiTimeBERT 2.0 achieves substantial improvements\nacross a broad range of time-related tasks and excels on datasets spanning\nextensive temporal ranges. These findings underscore BiTimeBERT 2.0's potential\nas a powerful tool for advancing temporal reasoning in NLP.", "published": "2024-06-04 00:30:37", "link": "http://arxiv.org/abs/2406.01863v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bi-DCSpell: A Bi-directional Detector-Corrector Interactive Framework\n  for Chinese Spelling Check", "abstract": "Chinese Spelling Check (CSC) aims to detect and correct potentially\nmisspelled characters in Chinese sentences. Naturally, it involves the\ndetection and correction subtasks, which interact with each other dynamically.\nSuch interactions are bi-directional, i.e., the detection result would help\nreduce the risk of over-correction and under-correction while the knowledge\nlearnt from correction would help prevent false detection. Current CSC\napproaches are of two types: correction-only or single-directional\ndetection-to-correction interactive frameworks. Nonetheless, they overlook the\nbi-directional interactions between detection and correction. This paper aims\nto fill the gap by proposing a Bi-directional Detector-Corrector framework for\nCSC (Bi-DCSpell). Notably, Bi-DCSpell contains separate detection and\ncorrection encoders, followed by a novel interactive learning module\nfacilitating bi-directional feature interactions between detection and\ncorrection to improve each other's representation learning. Extensive\nexperimental results demonstrate a robust correction performance of Bi-DCSpell\non widely used benchmarking datasets while possessing a satisfactory detection\nability.", "published": "2024-06-04 01:20:14", "link": "http://arxiv.org/abs/2406.01879v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OTTAWA: Optimal TransporT Adaptive Word Aligner for Hallucination and\n  Omission Translation Errors Detection", "abstract": "Recently, there has been considerable attention on detecting hallucinations\nand omissions in Machine Translation (MT) systems. The two dominant approaches\nto tackle this task involve analyzing the MT system's internal states or\nrelying on the output of external tools, such as sentence similarity or MT\nquality estimators. In this work, we introduce OTTAWA, a novel Optimal\nTransport (OT)-based word aligner specifically designed to enhance the\ndetection of hallucinations and omissions in MT systems. Our approach\nexplicitly models the missing alignments by introducing a \"null\" vector, for\nwhich we propose a novel one-side constrained OT setting to allow an adaptive\nnull alignment. Our approach yields competitive results compared to\nstate-of-the-art methods across 18 language pairs on the HalOmi benchmark. In\naddition, it shows promising features, such as the ability to distinguish\nbetween both error types and perform word-level detection without accessing the\nMT system's internal states.", "published": "2024-06-04 03:00:55", "link": "http://arxiv.org/abs/2406.01919v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dishonesty in Helpful and Harmless Alignment", "abstract": "People tell lies when seeking rewards. Large language models (LLMs) are\naligned to human values with reinforcement learning where they get rewards if\nthey satisfy human preference. We find that this also induces dishonesty in\nhelpful and harmless alignment where LLMs tell lies in generating harmless\nresponses. Using the latest interpreting tools, we detect dishonesty, show how\nLLMs can be harmful if their honesty is increased, and analyze such conflicts\nat the parameter-level. Given these preliminaries and the hypothesis that\nreward-seeking stimulates dishonesty, we theoretically show that the dishonesty\ncan in-turn decrease the alignment performances and augment reward-seeking\nalignment with representation regularization. Extensive results, including\nGPT-4 annotated win-rates, perplexities, and cases studies demonstrate that we\ncan train more honest, helpful, and harmless LLMs. We will make all our codes\nand results be open-sourced upon this paper's acceptance.", "published": "2024-06-04 03:31:09", "link": "http://arxiv.org/abs/2406.01931v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Optimal Transport Guided Correlation Assignment for Multimodal Entity\n  Linking", "abstract": "Multimodal Entity Linking (MEL) aims to link ambiguous mentions in multimodal\ncontexts to entities in a multimodal knowledge graph. A pivotal challenge is to\nfully leverage multi-element correlations between mentions and entities to\nbridge modality gap and enable fine-grained semantic matching. Existing methods\nattempt several local correlative mechanisms, relying heavily on the\nautomatically learned attention weights, which may over-concentrate on partial\ncorrelations. To mitigate this issue, we formulate the correlation assignment\nproblem as an optimal transport (OT) problem, and propose a novel MEL\nframework, namely OT-MEL, with OT-guided correlation assignment. Thereby, we\nexploit the correlation between multimodal features to enhance multimodal\nfusion, and the correlation between mentions and entities to enhance\nfine-grained matching. To accelerate model prediction, we further leverage\nknowledge distillation to transfer OT assignment knowledge to attention\nmechanism. Experimental results show that our model significantly outperforms\nprevious state-of-the-art baselines and confirm the effectiveness of the\nOT-guided correlation assignment.", "published": "2024-06-04 03:35:25", "link": "http://arxiv.org/abs/2406.01934v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Conditional Language Learning with Context", "abstract": "Language models can learn sophisticated language understanding skills from\nfitting raw text. They also unselectively learn useless corpus statistics and\nbiases, especially during finetuning on domain-specific corpora. In this paper,\nwe propose a simple modification to causal language modeling called conditional\nfinetuning, which performs language modeling conditioned on a context. We show\nthat a context can \"explain away\" certain corpus statistics and make the model\navoid learning them. In this fashion, conditional finetuning achieves selective\nlearning from a corpus, learning knowledge useful for downstream tasks while\navoiding learning useless corpus statistics like topic biases. This selective\nlearning effect leads to less forgetting and better stability-plasticity\ntradeoff in domain finetuning, potentially benefitting lifelong learning with\nlanguage models.", "published": "2024-06-04 05:22:24", "link": "http://arxiv.org/abs/2406.01976v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RKLD: Reverse KL-Divergence-based Knowledge Distillation for Unlearning\n  Personal Information in Large Language Models", "abstract": "With the passage of the Right to Be Forgotten (RTBF) regulations and the\nscaling up of language model training datasets, research on model unlearning in\nlarge language models (LLMs) has become more crucial. Before the era of LLMs,\nmachine unlearning research focused mainly on classification tasks in models\nwith small parameters. In these tasks, the content to be forgotten or retained\nis clear and straightforward. However, as parameter sizes have grown and tasks\nhave become more complex, balancing forget quality and model utility has become\nmore challenging, especially in scenarios involving personal data instead of\nclassification results. Existing methods based on gradient ascent and its\nvariants often struggle with this balance, leading to unintended information\nloss or partial forgetting. To address this challenge, we propose RKLD, a novel\n\\textbf{R}everse \\textbf{KL}-Divergence-based Knowledge \\textbf{D}istillation\nunlearning algorithm for LLMs targeting the unlearning of personal information.\nThrough RKLD, we achieve significant forget quality and effectively maintain\nthe model utility in our experiments.", "published": "2024-06-04 05:51:43", "link": "http://arxiv.org/abs/2406.01983v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analyzing Social Biases in Japanese Large Language Models", "abstract": "With the development of Large Language Models (LLMs), social biases in the\nLLMs have become a crucial issue. While various benchmarks for social biases\nhave been provided across languages, the extent to which Japanese LLMs exhibit\nsocial biases has not been fully investigated. In this study, we construct the\nJapanese Bias Benchmark dataset for Question Answering (JBBQ) based on the\nEnglish bias benchmark BBQ, and analyze social biases in Japanese LLMs. The\nresults show that while current open Japanese LLMs improve their accuracies on\nJBBQ by setting larger parameters, their bias scores become larger. In\naddition, prompts with warnings about social biases and Chain-of-Thought\nprompting reduce the effect of biases in model outputs, but there is room for\nimprovement in the consistency of reasoning.", "published": "2024-06-04 07:31:06", "link": "http://arxiv.org/abs/2406.02050v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessing the Performance of Chinese Open Source Large Language Models\n  in Information Extraction Tasks", "abstract": "Information Extraction (IE) plays a crucial role in Natural Language\nProcessing (NLP) by extracting structured information from unstructured text,\nthereby facilitating seamless integration with various real-world applications\nthat rely on structured data. Despite its significance, recent experiments\nfocusing on English IE tasks have shed light on the challenges faced by Large\nLanguage Models (LLMs) in achieving optimal performance, particularly in\nsub-tasks like Named Entity Recognition (NER). In this paper, we delve into a\ncomprehensive investigation of the performance of mainstream Chinese\nopen-source LLMs in tackling IE tasks, specifically under zero-shot conditions\nwhere the models are not fine-tuned for specific tasks. Additionally, we\npresent the outcomes of several few-shot experiments to further gauge the\ncapability of these models. Moreover, our study includes a comparative analysis\nbetween these open-source LLMs and ChatGPT, a widely recognized language model,\non IE performance. Through meticulous experimentation and analysis, we aim to\nprovide insights into the strengths, limitations, and potential enhancements of\nexisting Chinese open-source LLMs in the domain of Information Extraction\nwithin the context of NLP.", "published": "2024-06-04 08:00:40", "link": "http://arxiv.org/abs/2406.02079v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Mathematical Extrapolation of Large Language Models with\n  Synthetic Data", "abstract": "Large Language Models (LLMs) have shown excellent performance in language\nunderstanding, text generation, code synthesis, and many other tasks, while\nthey still struggle in complex multi-step reasoning problems, such as\nmathematical reasoning. In this paper, through a newly proposed arithmetical\npuzzle problem, we show that the model can perform well on multi-step reasoning\ntasks via fine-tuning on high-quality synthetic data. Experimental results with\nthe open-llama-3B model on three different test datasets show that not only the\nmodel can reach a zero-shot pass@1 at 0.44 on the in-domain dataset, it also\ndemonstrates certain generalization capabilities on the out-of-domain datasets.\nSpecifically, this paper has designed two out-of-domain datasets in the form of\nextending the numerical range and the composing components of the arithmetical\npuzzle problem separately. The fine-tuned models have shown encouraging\nperformance on these two far more difficult tasks with the zero-shot pass@1 at\n0.33 and 0.35, respectively.", "published": "2024-06-04 08:30:37", "link": "http://arxiv.org/abs/2406.02100v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MARS: Benchmarking the Metaphysical Reasoning Abilities of Language\n  Models with a Multi-task Evaluation Dataset", "abstract": "To enable Large Language Models (LLMs) to function as conscious agents with\ngeneralizable reasoning capabilities, it is crucial that they possess the\nreasoning ability to comprehend situational changes (transitions) in\ndistribution triggered by environmental factors or actions from other agents.\nDespite its fundamental significance, this ability remains underexplored due to\nthe complexity of modeling infinite possible changes in an event and their\nassociated distributions, coupled with the lack of benchmark data with\nsituational transitions. Addressing these gaps, we propose a novel formulation\nof reasoning with distributional changes as a three-step discriminative\nprocess, termed as MetAphysical ReaSoning. We then introduce the first-ever\nbenchmark, MARS, comprising three tasks corresponding to each step. These tasks\nsystematically assess LLMs' capabilities in reasoning the plausibility of (i)\nchanges in actions, (ii) states caused by changed actions, and (iii)\nsituational transitions driven by changes in action. Extensive evaluations with\n20 (L)LMs of varying sizes and methods indicate that all three tasks in this\nprocess pose significant challenges, even for state-of-the-art LLMs and LMs\nafter fine-tuning. Further analyses reveal potential causes for the\nunderperformance of LLMs and demonstrate that pre-training them on large-scale\nconceptualization taxonomies can potentially enhance their metaphysical\nreasoning capabilities. Our data and models are publicly accessible at\nhttps://github.com/HKUST-KnowComp/MARS.", "published": "2024-06-04 08:35:04", "link": "http://arxiv.org/abs/2406.02106v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Diver: Large Language Model Decoding with Span-Level Mutual Information\n  Verification", "abstract": "Large language models (LLMs) have shown impressive capabilities in adapting\nto various tasks when provided with task-specific instructions. However, LLMs\nusing standard decoding strategies often struggle with deviations from the\ninputs. Intuitively, compliant LLM outputs should reflect the information\npresent in the input, which can be measured by point-wise mutual information\n(PMI) scores. Therefore, we propose Diver, a novel approach that enhances LLM\nDecoding through span-level PMI verification. During inference, Diver first\nidentifies divergence steps that may lead to multiple candidate spans.\nSubsequently, it calculates the PMI scores by assessing the log-likelihood\ngains of the input if the candidate spans are generated. Finally, the optimal\nspan is selected based on the PMI re-ranked output distributions. We evaluate\nour method across various downstream tasks, and empirical results demonstrate\nthat Diver significantly outperforms existing decoding methods in both\nperformance and versatility.", "published": "2024-06-04 09:02:22", "link": "http://arxiv.org/abs/2406.02120v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The current status of large language models in summarizing radiology\n  report impressions", "abstract": "Large language models (LLMs) like ChatGPT show excellent capabilities in\nvarious natural language processing tasks, especially for text generation. The\neffectiveness of LLMs in summarizing radiology report impressions remains\nunclear. In this study, we explore the capability of eight LLMs on the\nradiology report impression summarization. Three types of radiology reports,\ni.e., CT, PET-CT, and Ultrasound reports, are collected from Peking University\nCancer Hospital and Institute. We use the report findings to construct the\nzero-shot, one-shot, and three-shot prompts with complete example reports to\ngenerate the impressions. Besides the automatic quantitative evaluation\nmetrics, we define five human evaluation metrics, i.e., completeness,\ncorrectness, conciseness, verisimilitude, and replaceability, to evaluate the\nsemantics of the generated impressions. Two thoracic surgeons (ZSY and LB) and\none radiologist (LQ) compare the generated impressions with the reference\nimpressions and score each impression under the five human evaluation metrics.\nExperimental results show that there is a gap between the generated impressions\nand reference impressions. Although the LLMs achieve comparable performance in\ncompleteness and correctness, the conciseness and verisimilitude scores are not\nvery high. Using few-shot prompts can improve the LLMs' performance in\nconciseness and verisimilitude, but the clinicians still think the LLMs can not\nreplace the radiologists in summarizing the radiology impressions.", "published": "2024-06-04 09:23:30", "link": "http://arxiv.org/abs/2406.02134v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reinforcement Tuning for Detecting Stances and Debunking Rumors Jointly\n  with Large Language Models", "abstract": "Learning multi-task models for jointly detecting stance and verifying rumors\nposes challenges due to the need for training data of stance at post level and\nrumor veracity at claim level, which are difficult to obtain. To address this\nissue, we leverage large language models (LLMs) as the foundation annotators\nfor the joint stance detection (SD) and rumor verification (RV) tasks, dubbed\nas JSDRV. We introduce a novel reinforcement tuning framework to enhance the\njoint predictive capabilities of LLM-based SD and RV components. Specifically,\nwe devise a policy for selecting LLM-annotated data at the two levels,\nemploying a hybrid reward mechanism to choose high-quality labels for effective\nLLM fine-tuning on both tasks. Results demonstrate that JSDRV improves the\ncapabilities of LLMs in the joint tasks, not only outperforming\nstate-of-the-art methods but also generalizing to non-LLMs accommodated as task\nmodels.", "published": "2024-06-04 09:31:18", "link": "http://arxiv.org/abs/2406.02143v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A multilingual dataset for offensive language and hate speech detection\n  for hausa, yoruba and igbo languages", "abstract": "The proliferation of online offensive language necessitates the development\nof effective detection mechanisms, especially in multilingual contexts. This\nstudy addresses the challenge by developing and introducing novel datasets for\noffensive language detection in three major Nigerian languages: Hausa, Yoruba,\nand Igbo. We collected data from Twitter and manually annotated it to create\ndatasets for each of the three languages, using native speakers. We used\npre-trained language models to evaluate their efficacy in detecting offensive\nlanguage in our datasets. The best-performing model achieved an accuracy of\n90\\%. To further support research in offensive language detection, we plan to\nmake the dataset and our models publicly available.", "published": "2024-06-04 09:58:29", "link": "http://arxiv.org/abs/2406.02169v2", "categories": ["cs.CL", "14F05", "F.2.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "Self-Modifying State Modeling for Simultaneous Machine Translation", "abstract": "Simultaneous Machine Translation (SiMT) generates target outputs while\nreceiving stream source inputs and requires a read/write policy to decide\nwhether to wait for the next source token or generate a new target token, whose\ndecisions form a \\textit{decision path}. Existing SiMT methods, which learn the\npolicy by exploring various decision paths in training, face inherent\nlimitations. These methods not only fail to precisely optimize the policy due\nto the inability to accurately assess the individual impact of each decision on\nSiMT performance, but also cannot sufficiently explore all potential paths\nbecause of their vast number. Besides, building decision paths requires\nunidirectional encoders to simulate streaming source inputs, which impairs the\ntranslation quality of SiMT models. To solve these issues, we propose\n\\textbf{S}elf-\\textbf{M}odifying \\textbf{S}tate \\textbf{M}odeling (SM$^2$), a\nnovel training paradigm for SiMT task. Without building decision paths, SM$^2$\nindividually optimizes decisions at each state during training. To precisely\noptimize the policy, SM$^2$ introduces Self-Modifying process to independently\nassess and adjust decisions at each state. For sufficient exploration, SM$^2$\nproposes Prefix Sampling to efficiently traverse all potential states.\nMoreover, SM$^2$ ensures compatibility with bidirectional encoders, thus\nachieving higher translation quality. Experiments show that SM$^2$ outperforms\nstrong baselines. Furthermore, SM$^2$ allows offline machine translation models\nto acquire SiMT ability with fine-tuning.", "published": "2024-06-04 11:57:58", "link": "http://arxiv.org/abs/2406.02237v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Retrieval-Augmented LMs with a Two-stage Consistency Learning\n  Compressor", "abstract": "Despite the prevalence of retrieval-augmented language models (RALMs), the\nseamless integration of these models with retrieval mechanisms to enhance\nperformance in document-based tasks remains challenging. While some\npost-retrieval processing Retrieval-Augmented Generation (RAG) methods have\nachieved success, most still lack the ability to distinguish pertinent from\nextraneous information, leading to potential inconsistencies and reduced\nprecision in the generated output, which subsequently affects the truthfulness\nof the language model's responses. To address these limitations, this work\nproposes a novel two-stage consistency learning approach for retrieved\ninformation compression in retrieval-augmented language models to enhance\nperformance. By incorporating consistency learning, the aim is to generate\nsummaries that maintain coherence and alignment with the intended semantic\nrepresentations of a teacher model while improving faithfulness to the original\nretrieved documents. The proposed method is empirically validated across\nmultiple datasets, demonstrating notable enhancements in precision and\nefficiency for question-answering tasks. It outperforms existing baselines and\nshowcases the synergistic effects of combining contrastive and consistency\nlearning paradigms within the retrieval-augmented generation framework.", "published": "2024-06-04 12:43:23", "link": "http://arxiv.org/abs/2406.02266v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prompting Large Language Models with Human Error Markings for\n  Self-Correcting Machine Translation", "abstract": "While large language models (LLMs) pre-trained on massive amounts of unpaired\nlanguage data have reached the state-of-the-art in machine translation (MT) of\ngeneral domain texts, post-editing (PE) is still required to correct errors and\nto enhance term translation quality in specialized domains. In this paper we\npresent a pilot study of enhancing translation memories (TM) produced by PE\n(source segments, machine translations, and reference translations, henceforth\ncalled PE-TM) for the needs of correct and consistent term translation in\ntechnical domains.\n  We investigate a light-weight two-step scenario where, at inference time, a\nhuman translator marks errors in the first translation step, and in a second\nstep a few similar examples are extracted from the PE-TM to prompt an LLM. Our\nexperiment shows that the additional effort of augmenting translations with\nhuman error markings guides the LLM to focus on a correction of the marked\nerrors, yielding consistent improvements over automatic PE (APE) and MT from\nscratch.", "published": "2024-06-04 12:43:47", "link": "http://arxiv.org/abs/2406.02267v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "mCoT: Multilingual Instruction Tuning for Reasoning Consistency in\n  Language Models", "abstract": "Large language models (LLMs) with Chain-of-thought (CoT) have recently\nemerged as a powerful technique for eliciting reasoning to improve various\ndownstream tasks. As most research mainly focuses on English, with few\nexplorations in a multilingual context, the question of how reliable this\nreasoning capability is in different languages is still open. To address it\ndirectly, we study multilingual reasoning consistency across multiple\nlanguages, using popular open-source LLMs. First, we compile the first\nlarge-scale multilingual math reasoning dataset, mCoT-MATH, covering eleven\ndiverse languages. Then, we introduce multilingual CoT instruction tuning to\nboost reasoning capability across languages, thereby improving model\nconsistency. While existing LLMs show substantial variation across the\nlanguages we consider, and especially low performance for lesser resourced\nlanguages, our 7B parameter model mCoT achieves impressive consistency across\nlanguages, and superior or comparable performance to close- and open-source\nmodels even of much larger sizes.", "published": "2024-06-04 13:30:45", "link": "http://arxiv.org/abs/2406.02301v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Translation Deserves Better: Analyzing Translation Artifacts in\n  Cross-lingual Visual Question Answering", "abstract": "Building a reliable visual question answering~(VQA) system across different\nlanguages is a challenging problem, primarily due to the lack of abundant\nsamples for training. To address this challenge, recent studies have employed\nmachine translation systems for the cross-lingual VQA task. This involves\ntranslating the evaluation samples into a source language (usually English) and\nusing monolingual models (i.e., translate-test). However, our analysis reveals\nthat translated texts contain unique characteristics distinct from\nhuman-written ones, referred to as translation artifacts. We find that these\nartifacts can significantly affect the models, confirmed by extensive\nexperiments across diverse models, languages, and translation processes. In\nlight of this, we present a simple data augmentation strategy that can\nalleviate the adverse impacts of translation artifacts.", "published": "2024-06-04 14:00:02", "link": "http://arxiv.org/abs/2406.02331v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Probing the Category of Verbal Aspect in Transformer Language Models", "abstract": "We investigate how pretrained language models (PLM) encode the grammatical\ncategory of verbal aspect in Russian. Encoding of aspect in transformer LMs has\nnot been studied previously in any language. A particular challenge is posed by\n\"alternative contexts\": where either the perfective or the imperfective aspect\nis suitable grammatically and semantically. We perform probing using BERT and\nRoBERTa on alternative and non-alternative contexts. First, we assess the\nmodels' performance on aspect prediction, via behavioral probing. Next, we\nexamine the models' performance when their contextual representations are\nsubstituted with counterfactual representations, via causal probing. These\ncounterfactuals alter the value of the \"boundedness\" feature--a semantic\nfeature, which characterizes the action in the context. Experiments show that\nBERT and RoBERTa do encode aspect--mostly in their final layers. The\ncounterfactual interventions affect perfective and imperfective in opposite\nways, which is consistent with grammar: perfective is positively affected by\nadding the meaning of boundedness, and vice versa. The practical implications\nof our probing results are that fine-tuning only the last layers of BERT on\npredicting aspect is faster and more effective than fine-tuning the whole\nmodel. The model has high predictive uncertainty about aspect in alternative\ncontexts, which tend to lack explicit hints about the boundedness of the\ndescribed action.", "published": "2024-06-04 14:06:03", "link": "http://arxiv.org/abs/2406.02335v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Retaining Key Information under High Compression Ratios: Query-Guided\n  Compressor for LLMs", "abstract": "The growing popularity of Large Language Models has sparked interest in\ncontext compression for Large Language Models (LLMs). However, the performance\nof previous methods degrades dramatically as compression ratios increase,\nsometimes even falling to the closed-book level. This decline can be attributed\nto the loss of key information during the compression process. Our preliminary\nstudy supports this hypothesis, emphasizing the significance of retaining key\ninformation to maintain model performance under high compression ratios. As a\nresult, we introduce Query-Guided Compressor (QGC), which leverages queries to\nguide the context compression process, effectively preserving key information\nwithin the compressed context. Additionally, we employ a dynamic compression\nstrategy. We validate the effectiveness of our proposed QGC on the Question\nAnswering task, including NaturalQuestions, TriviaQA, and HotpotQA datasets.\nExperimental results show that QGC can consistently perform well even at high\ncompression ratios, which also offers significant benefits in terms of\ninference cost and throughput.", "published": "2024-06-04 14:53:24", "link": "http://arxiv.org/abs/2406.02376v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Intrinsic Self-Correction Capability of LLMs: Uncertainty and\n  Latent Concept", "abstract": "Large Language Models (LLMs) are able to improve their responses when\ninstructed to do so, a capability known as self-correction. When instructions\nprovide only the task's goal without specific details about potential issues in\nthe response, LLMs must rely on their internal knowledge to improve response\nquality, a process referred to as intrinsic self-correction. The empirical\nsuccess of intrinsic self-correction is evident in various applications, but\nhow and why it is effective remains unknown. In this paper, we unveil that\nintrinsic self-correction can be progressively improved, allowing it to\napproach a converged state. Our findings are verified in: (1) the scenario of\nmulti-round question answering, by comprehensively demonstrating that intrinsic\nself-correction can progressively introduce performance gains through iterative\ninteractions, ultimately converging to stable performance; and (2) the context\nof intrinsic self-correction for enhanced morality, in which we provide\nempirical evidence that iteratively applying instructions reduces model\nuncertainty towards convergence, which then leads to convergence of both the\ncalibration error and self-correction performance, ultimately resulting in a\nstable state of intrinsic self-correction. Furthermore, we introduce a\nmathematical formulation and a simulation task indicating that the latent\nconcepts activated by self-correction instructions drive the reduction of model\nuncertainty. Based on our experimental results and analysis of the convergence\nof intrinsic self-correction, we reveal its underlying mechanism: consistent\ninjected instructions reduce model uncertainty which yields converged, improved\nperformance.", "published": "2024-06-04 14:55:43", "link": "http://arxiv.org/abs/2406.02378v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analyzing Temporal Complex Events with Large Language Models? A\n  Benchmark towards Temporal, Long Context Understanding", "abstract": "The digital landscape is rapidly evolving with an ever-increasing volume of\nonline news, emphasizing the need for swift and precise analysis of complex\nevents. We refer to the complex events composed of many news articles over an\nextended period as Temporal Complex Event (TCE). This paper proposes a novel\napproach using Large Language Models (LLMs) to systematically extract and\nanalyze the event chain within TCE, characterized by their key points and\ntimestamps. We establish a benchmark, named TCELongBench, to evaluate the\nproficiency of LLMs in handling temporal dynamics and understanding extensive\ntext. This benchmark encompasses three distinct tasks - reading comprehension,\ntemporal sequencing, and future event forecasting. In the experiment, we\nleverage retrieval-augmented generation (RAG) method and LLMs with long context\nwindow to deal with lengthy news articles of TCE. Our findings indicate that\nmodels with suitable retrievers exhibit comparable performance with those\nutilizing long context window.", "published": "2024-06-04 16:42:17", "link": "http://arxiv.org/abs/2406.02472v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deterministic Reversible Data Augmentation for Neural Machine\n  Translation", "abstract": "Data augmentation is an effective way to diversify corpora in machine\ntranslation, but previous methods may introduce semantic inconsistency between\noriginal and augmented data because of irreversible operations and random\nsubword sampling procedures. To generate both symbolically diverse and\nsemantically consistent augmentation data, we propose Deterministic Reversible\nData Augmentation (DRDA), a simple but effective data augmentation method for\nneural machine translation. DRDA adopts deterministic segmentations and\nreversible operations to generate multi-granularity subword representations and\npulls them closer together with multi-view techniques. With no extra corpora or\nmodel changes required, DRDA outperforms strong baselines on several\ntranslation tasks with a clear margin (up to 4.3 BLEU gain over Transformer)\nand exhibits good robustness in noisy, low-resource, and cross-domain datasets.", "published": "2024-06-04 17:39:23", "link": "http://arxiv.org/abs/2406.02517v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CheckEmbed: Effective Verification of LLM Solutions to Open-Ended Tasks", "abstract": "Large Language Models (LLMs) are revolutionizing various domains, yet\nverifying their answers remains a significant challenge, especially for\nintricate open-ended tasks such as consolidation, summarization, and extraction\nof knowledge. In this work, we propose CheckEmbed: an accurate, scalable, and\nsimple LLM verification approach. CheckEmbed is driven by a straightforward yet\npowerful idea: in order to compare LLM solutions to one another or to the\nground-truth, compare their corresponding answer-level embeddings obtained with\na model such as GPT Text Embedding Large. This reduces a complex textual answer\nto a single embedding, facilitating straightforward, fast, and meaningful\nverification. We develop a comprehensive verification pipeline implementing the\nCheckEmbed methodology. The CheckEmbed pipeline also comes with metrics for\nassessing the truthfulness of the LLM answers, such as embedding heatmaps and\ntheir summaries. We show how to use these metrics for deploying practical\nengines that decide whether an LLM answer is satisfactory or not. We apply the\npipeline to real-world document analysis tasks, including term extraction and\ndocument summarization, showcasing significant improvements in accuracy,\ncost-effectiveness, and runtime performance compared to existing token-,\nsentence-, and fact-level schemes such as BERTScore or SelfCheckGPT.", "published": "2024-06-04 17:42:21", "link": "http://arxiv.org/abs/2406.02524v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Scalable MatMul-free Language Modeling", "abstract": "Matrix multiplication (MatMul) typically dominates the overall computational\ncost of large language models (LLMs). This cost only grows as LLMs scale to\nlarger embedding dimensions and context lengths. In this work, we show that\nMatMul operations can be completely eliminated from LLMs while maintaining\nstrong performance at billion-parameter scales. Our experiments show that our\nproposed MatMul-free models achieve performance on-par with state-of-the-art\nTransformers that require far more memory during inference at a scale up to at\nleast 2.7B parameters. We investigate the scaling laws and find that the\nperformance gap between our MatMul-free models and full precision Transformers\nnarrows as the model size increases. We also provide a GPU-efficient\nimplementation of this model which reduces memory usage by up to 61% over an\nunoptimized baseline during training. By utilizing an optimized kernel during\ninference, our model's memory consumption can be reduced by more than 10x\ncompared to unoptimized models. To properly quantify the efficiency of our\narchitecture, we build a custom hardware solution on an FPGA which exploits\nlightweight operations beyond what GPUs are capable of. We processed\nbillion-parameter scale models at 13W beyond human readable throughput, moving\nLLMs closer to brain-like efficiency. This work not only shows how far LLMs can\nbe stripped back while still performing effectively, but also points at the\ntypes of operations future accelerators should be optimized for in processing\nthe next generation of lightweight LLMs. Our code implementation is available\nat https://github.com/ridgerchu/matmulfreellm.", "published": "2024-06-04 17:50:34", "link": "http://arxiv.org/abs/2406.02528v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SpecExec: Massively Parallel Speculative Decoding for Interactive LLM\n  Inference on Consumer Devices", "abstract": "As large language models gain widespread adoption, running them efficiently\nbecomes crucial. Recent works on LLM inference use speculative decoding to\nachieve extreme speedups. However, most of these works implicitly design their\nalgorithms for high-end datacenter hardware. In this work, we ask the opposite\nquestion: how fast can we run LLMs on consumer machines? Consumer GPUs can no\nlonger fit the largest available models (50B+ parameters) and must offload them\nto RAM or SSD. When running with offloaded parameters, the inference engine can\nprocess batches of hundreds or thousands of tokens at the same time as just one\ntoken, making it a natural fit for speculative decoding. We propose SpecExec\n(Speculative Execution), a simple parallel decoding method that can generate up\nto 20 tokens per target model iteration for popular LLM families. It utilizes\nthe high spikiness of the token probabilities distribution in modern LLMs and a\nhigh degree of alignment between model output probabilities. SpecExec takes the\nmost probable tokens continuation from the draft model to build a \"cache\" tree\nfor the target model, which then gets validated in a single pass. Using\nSpecExec, we demonstrate inference of 50B+ parameter LLMs on consumer GPUs with\nRAM offloading at 4-6 tokens per second with 4-bit quantization or 2-3 tokens\nper second with 16-bit weights.", "published": "2024-06-04 17:53:36", "link": "http://arxiv.org/abs/2406.02532v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RATT: A Thought Structure for Coherent and Correct LLM Reasoning", "abstract": "Large Language Models (LLMs) gain substantial reasoning and decision-making\ncapabilities from thought structures. However, existing methods such as Tree of\nThought and Retrieval Augmented Thoughts often fall short in complex tasks due\nto the limitations of insufficient local retrieval of factual knowledge and\ninadequate global selection of strategies. These limitations make it\nchallenging for these methods to balance factual accuracy and comprehensive\nlogical optimization effectively. To address these limitations, we introduce\nthe Retrieval Augmented Thought Tree (RATT), a novel thought structure that\nconsiders both overall logical soundness and factual correctness at each step\nof the thinking process. Specifically, at every point of a thought branch, RATT\nperforms planning and lookahead to explore and evaluate multiple potential\nreasoning steps, and integrate the fact-checking ability of Retrieval-Augmented\nGeneration (RAG) with LLM's ability to assess overall strategy. Through this\ncombination of factual knowledge and strategic feasibility, the RATT adjusts\nand integrates the thought tree structure to search for the most promising\nbranches within the search space. This thought structure significantly enhances\nthe model's coherence in logical inference and efficiency in decision-making,\nand thus increases the limit of the capacity of LLM to generate reliable\ninferences and decisions based on thought structures. A broad range of\nexperiments on different types of tasks showcases that the RATT structure\nsignificantly outperforms existing methods in factual correctness and logical\ncoherence.", "published": "2024-06-04 20:02:52", "link": "http://arxiv.org/abs/2406.02746v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Chain of Agents: Large Language Models Collaborating on Long-Context\n  Tasks", "abstract": "Addressing the challenge of effectively processing long contexts has become a\ncritical issue for Large Language Models (LLMs). Two common strategies have\nemerged: 1) reducing the input length, such as retrieving relevant chunks by\nRetrieval-Augmented Generation (RAG), and 2) expanding the context window limit\nof LLMs. However, both strategies have drawbacks: input reduction has no\nguarantee of covering the part with needed information, while window extension\nstruggles with focusing on the pertinent information for solving the task. To\nmitigate these limitations, we propose Chain-of-Agents (CoA), a novel framework\nthat harnesses multi-agent collaboration through natural language to enable\ninformation aggregation and context reasoning across various LLMs over\nlong-context tasks. CoA consists of multiple worker agents who sequentially\ncommunicate to handle different segmented portions of the text, followed by a\nmanager agent who synthesizes these contributions into a coherent final output.\nCoA processes the entire input by interleaving reading and reasoning, and it\nmitigates long context focus issues by assigning each agent a short context. We\nperform comprehensive evaluation of CoA on a wide range of long-context tasks\nin question answering, summarization, and code completion, demonstrating\nsignificant improvements by up to 10% over strong baselines of RAG,\nFull-Context, and multi-agent LLMs.", "published": "2024-06-04 23:36:08", "link": "http://arxiv.org/abs/2406.02818v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TruthEval: A Dataset to Evaluate LLM Truthfulness and Reliability", "abstract": "Large Language Model (LLM) evaluation is currently one of the most important\nareas of research, with existing benchmarks proving to be insufficient and not\ncompletely representative of LLMs' various capabilities. We present a curated\ncollection of challenging statements on sensitive topics for LLM benchmarking\ncalled TruthEval. These statements were curated by hand and contain known truth\nvalues. The categories were chosen to distinguish LLMs' abilities from their\nstochastic nature. We perform some initial analyses using this dataset and find\nseveral instances of LLMs failing in simple tasks showing their inability to\nunderstand simple questions.", "published": "2024-06-04 00:01:35", "link": "http://arxiv.org/abs/2406.01855v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing Trust in LLMs: Algorithms for Comparing and Interpreting LLMs", "abstract": "This paper surveys evaluation techniques to enhance the trustworthiness and\nunderstanding of Large Language Models (LLMs). As reliance on LLMs grows,\nensuring their reliability, fairness, and transparency is crucial. We explore\nalgorithmic methods and metrics to assess LLM performance, identify weaknesses,\nand guide development towards more trustworthy applications. Key evaluation\nmetrics include Perplexity Measurement, NLP metrics (BLEU, ROUGE, METEOR,\nBERTScore, GLEU, Word Error Rate, Character Error Rate), Zero-Shot and Few-Shot\nLearning Performance, Transfer Learning Evaluation, Adversarial Testing, and\nFairness and Bias Evaluation. We introduce innovative approaches like LLMMaps\nfor stratified evaluation, Benchmarking and Leaderboards for competitive\nassessment, Stratified Analysis for in-depth understanding, Visualization of\nBlooms Taxonomy for cognitive level accuracy distribution, Hallucination Score\nfor quantifying inaccuracies, Knowledge Stratification Strategy for\nhierarchical analysis, and Machine Learning Models for Hierarchy Generation.\nHuman Evaluation is highlighted for capturing nuances that automated metrics\nmay miss. These techniques form a framework for evaluating LLMs, aiming to\nenhance transparency, guide development, and establish user trust. Future\npapers will describe metric visualization and demonstrate each approach on\npractical examples.", "published": "2024-06-04 03:54:53", "link": "http://arxiv.org/abs/2406.01943v1", "categories": ["cs.CL", "cs.AI", "2020: 68T50, 68Q25", "I.2.7; F.2.2"], "primary_category": "cs.CL"}
{"title": "Bileve: Securing Text Provenance in Large Language Models Against\n  Spoofing with Bi-level Signature", "abstract": "Text watermarks for large language models (LLMs) have been commonly used to\nidentify the origins of machine-generated content, which is promising for\nassessing liability when combating deepfake or harmful content. While existing\nwatermarking techniques typically prioritize robustness against removal\nattacks, unfortunately, they are vulnerable to spoofing attacks: malicious\nactors can subtly alter the meanings of LLM-generated responses or even forge\nharmful content, potentially misattributing blame to the LLM developer. To\novercome this, we introduce a bi-level signature scheme, Bileve, which embeds\nfine-grained signature bits for integrity checks (mitigating spoofing attacks)\nas well as a coarse-grained signal to trace text sources when the signature is\ninvalid (enhancing detectability) via a novel rank-based sampling strategy.\nCompared to conventional watermark detectors that only output binary results,\nBileve can differentiate 5 scenarios during detection, reliably tracing text\nprovenance and regulating LLMs. The experiments conducted on OPT-1.3B and\nLLaMA-7B demonstrate the effectiveness of Bileve in defeating spoofing attacks\nwith enhanced detectability. Code is available at\nhttps://github.com/Tongzhou0101/Bileve-official.", "published": "2024-06-04 03:58:14", "link": "http://arxiv.org/abs/2406.01946v3", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Zyda: A 1.3T Dataset for Open Language Modeling", "abstract": "The size of large language models (LLMs) has scaled dramatically in recent\nyears and their computational and data requirements have surged\ncorrespondingly. State-of-the-art language models, even at relatively smaller\nsizes, typically require training on at least a trillion tokens. This rapid\nadvancement has eclipsed the growth of open-source datasets available for\nlarge-scale LLM pretraining. In this paper, we introduce Zyda (Zyphra Dataset),\na dataset under a permissive license comprising 1.3 trillion tokens, assembled\nby integrating several major respected open-source datasets into a single,\nhigh-quality corpus. We apply rigorous filtering and deduplication processes,\nboth within and across datasets, to maintain and enhance the quality derived\nfrom the original datasets. Our evaluations show that Zyda not only competes\nfavorably with other open datasets like Dolma, FineWeb, and RefinedWeb, but\nalso substantially improves the performance of comparable models from the\nPythia suite. Our rigorous data processing methods significantly enhance Zyda's\neffectiveness, outperforming even the best of its constituent datasets when\nused independently.", "published": "2024-06-04 05:47:17", "link": "http://arxiv.org/abs/2406.01981v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Personalized Topic Selection Model for Topic-Grounded Dialogue", "abstract": "Recently, the topic-grounded dialogue (TGD) system has become increasingly\npopular as its powerful capability to actively guide users to accomplish\nspecific tasks through topic-guided conversations. Most existing works utilize\nside information (\\eg topics or personas) in isolation to enhance the topic\nselection ability. However, due to disregarding the noise within these\nauxiliary information sources and their mutual influence, current models tend\nto predict user-uninteresting and contextually irrelevant topics. To build\nuser-engaging and coherent dialogue agent, we propose a \\textbf{P}ersonalized\ntopic s\\textbf{E}lection model for \\textbf{T}opic-grounded \\textbf{D}ialogue,\nnamed \\textbf{PETD}, which takes account of the interaction of side information\nto selectively aggregate such information for more accurately predicting\nsubsequent topics. Specifically, we evaluate the correlation between global\ntopics and personas and selectively incorporate the global topics aligned with\nuser personas. Furthermore, we propose a contrastive learning based persona\nselector to filter out irrelevant personas under the constraint of lacking\npertinent persona annotations. Throughout the selection and generation, diverse\nrelevant side information is considered. Extensive experiments demonstrate that\nour proposed method can generate engaging and diverse responses, outperforming\nstate-of-the-art baselines across various evaluation metrics.", "published": "2024-06-04 06:09:49", "link": "http://arxiv.org/abs/2406.01988v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Position Debiasing Fine-Tuning for Causal Perception in Long-Term\n  Dialogue", "abstract": "The core of the dialogue system is to generate relevant, informative, and\nhuman-like responses based on extensive dialogue history. Recently, dialogue\ngeneration domain has seen mainstream adoption of large language models (LLMs),\ndue to its powerful capability in generating utterances. However, there is a\nnatural deficiency for such models, that is, inherent position bias, which may\nlead them to pay more attention to the nearby utterances instead of causally\nrelevant ones, resulting in generating irrelevant and generic responses in\nlong-term dialogue. To alleviate such problem, in this paper, we propose a\nnovel method, named Causal Perception long-term Dialogue framework (CPD), which\nemploys perturbation-based causal variable discovery method to extract casually\nrelevant utterances from the dialogue history and enhances model causal\nperception during fine-tuning. Specifically, a local-position awareness method\nis proposed in CPD for inter-sentence position correlation elimination, which\nhelps models extract causally relevant utterances based on perturbations. Then,\na casual-perception fine-tuning strategy is also proposed, to enhance the\ncapability of discovering the causal invariant factors, by differently\nperturbing causally relevant and non-casually relevant ones for response\ngeneration. Experimental results on two datasets prove that our proposed method\ncan effectively alleviate the position bias for multiple LLMs and achieve\nsignificant progress compared with existing baselines.", "published": "2024-06-04 06:33:13", "link": "http://arxiv.org/abs/2406.02002v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multimodal Reasoning with Multimodal Knowledge Graph", "abstract": "Multimodal reasoning with large language models (LLMs) often suffers from\nhallucinations and the presence of deficient or outdated knowledge within LLMs.\nSome approaches have sought to mitigate these issues by employing textual\nknowledge graphs, but their singular modality of knowledge limits comprehensive\ncross-modal understanding. In this paper, we propose the Multimodal Reasoning\nwith Multimodal Knowledge Graph (MR-MKG) method, which leverages multimodal\nknowledge graphs (MMKGs) to learn rich and semantic knowledge across\nmodalities, significantly enhancing the multimodal reasoning capabilities of\nLLMs. In particular, a relation graph attention network is utilized for\nencoding MMKGs and a cross-modal alignment module is designed for optimizing\nimage-text alignment. A MMKG-grounded dataset is constructed to equip LLMs with\ninitial expertise in multimodal reasoning through pretraining. Remarkably,\nMR-MKG achieves superior performance while training on only a small fraction of\nparameters, approximately 2.25% of the LLM's parameter size. Experimental\nresults on multimodal question answering and multimodal analogy reasoning tasks\ndemonstrate that our MR-MKG method outperforms previous state-of-the-art\nmodels.", "published": "2024-06-04 07:13:23", "link": "http://arxiv.org/abs/2406.02030v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "QROA: A Black-Box Query-Response Optimization Attack on LLMs", "abstract": "Large Language Models (LLMs) have surged in popularity in recent months, yet\nthey possess concerning capabilities for generating harmful content when\nmanipulated. This study introduces the Query-Response Optimization Attack\n(QROA), an optimization-based strategy designed to exploit LLMs through a\nblack-box, query-only interaction. QROA adds an optimized trigger to a\nmalicious instruction to compel the LLM to generate harmful content. Unlike\nprevious approaches, QROA does not require access to the model's logit\ninformation or any other internal data and operates solely through the standard\nquery-response interface of LLMs. Inspired by deep Q-learning and Greedy\ncoordinate descent, the method iteratively updates tokens to maximize a\ndesigned reward function. We tested our method on various LLMs such as Vicuna,\nFalcon, and Mistral, achieving an Attack Success Rate (ASR) over 80\\%. We also\ntested the model against Llama2-chat, the fine-tuned version of Llama2 designed\nto resist Jailbreak attacks, achieving good ASR with a suboptimal initial\ntrigger seed. This study demonstrates the feasibility of generating jailbreak\nattacks against deployed LLMs in the public domain using black-box optimization\nmethods, enabling more comprehensive safety testing of LLMs.", "published": "2024-06-04 07:27:36", "link": "http://arxiv.org/abs/2406.02044v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "I've got the \"Answer\"! Interpretation of LLMs Hidden States in Question\n  Answering", "abstract": "Interpretability and explainability of AI are becoming increasingly important\nin light of the rapid development of large language models (LLMs). This paper\ninvestigates the interpretation of LLMs in the context of the knowledge-based\nquestion answering. The main hypothesis of the study is that correct and\nincorrect model behavior can be distinguished at the level of hidden states.\nThe quantized models LLaMA-2-7B-Chat, Mistral-7B, Vicuna-7B and the MuSeRC\nquestion-answering dataset are used to test this hypothesis. The results of the\nanalysis support the proposed hypothesis. We also identify the layers which\nhave a negative effect on the model's behavior. As a prospect of practical\napplication of the hypothesis, we propose to train such \"weak\" layers\nadditionally in order to improve the quality of the task solution.", "published": "2024-06-04 07:43:12", "link": "http://arxiv.org/abs/2406.02060v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information\n  Funneling", "abstract": "In this study, we investigate whether attention-based information flow inside\nlarge language models (LLMs) is aggregated through noticeable patterns for long\ncontext processing. Our observations reveal that LLMs aggregate information\nthrough Pyramidal Information Funneling where attention is scattering widely in\nlower layers, progressively consolidating within specific contexts, and\nultimately focusing on critical tokens (a.k.a massive activation or attention\nsink) in higher layers. Motivated by these insights, we developed PyramidKV, a\nnovel and effective KV cache compression method. This approach dynamically\nadjusts the KV cache size across different layers, allocating more cache in\nlower layers and less in higher ones, diverging from traditional methods that\nmaintain a uniform KV cache size. Our experimental evaluations, utilizing the\nLongBench benchmark, show that PyramidKV matches the performance of models with\na full KV cache while retaining only 12% of the KV cache, thus significantly\nreducing memory usage. In scenarios emphasizing memory efficiency, where only\n0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache\ncompression techniques, achieving up to a 20.5 absolute accuracy improvement on\nTREC dataset. In the Needle-in-a-Haystack experiment, PyramidKV outperforms\ncompeting methods in maintaining long-context comprehension in LLMs; notably,\nretaining just 128 KV cache entries enables the LLAMA-3-70B model to achieve\n100% Acc. performance, matching that of a full KV cache.", "published": "2024-06-04 07:51:30", "link": "http://arxiv.org/abs/2406.02069v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "UniOQA: A Unified Framework for Knowledge Graph Question Answering with\n  Large Language Models", "abstract": "OwnThink stands as the most extensive Chinese open-domain knowledge graph\nintroduced in recent times. Despite prior attempts in question answering over\nOwnThink (OQA), existing studies have faced limitations in model representation\ncapabilities, posing challenges in further enhancing overall accuracy in\nquestion answering. In this paper, we introduce UniOQA, a unified framework\nthat integrates two complementary parallel workflows. Unlike conventional\napproaches, UniOQA harnesses large language models (LLMs) for precise question\nanswering and incorporates a direct-answer-prediction process as a\ncost-effective complement. Initially, to bolster representation capacity, we\nfine-tune an LLM to translate questions into the Cypher query language (CQL),\ntackling issues associated with restricted semantic understanding and\nhallucinations. Subsequently, we introduce the Entity and Relation Replacement\nalgorithm to ensure the executability of the generated CQL. Concurrently, to\naugment overall accuracy in question answering, we further adapt the\nRetrieval-Augmented Generation (RAG) process to the knowledge graph.\nUltimately, we optimize answer accuracy through a dynamic decision algorithm.\nExperimental findings illustrate that UniOQA notably advances SpCQL Logical\nAccuracy to 21.2% and Execution Accuracy to 54.9%, achieving the new\nstate-of-the-art results on this benchmark. Through ablation experiments, we\ndelve into the superior representation capacity of UniOQA and quantify its\nperformance breakthrough.", "published": "2024-06-04 08:36:39", "link": "http://arxiv.org/abs/2406.02110v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Robust Interaction-Based Relevance Modeling for Online e-Commerce Search", "abstract": "Semantic relevance calculation is crucial for e-commerce search engines, as\nit ensures that the items selected closely align with customer intent.\nInadequate attention to this aspect can detrimentally affect user experience\nand engagement. Traditional text-matching techniques are prevalent but often\nfail to capture the nuances of search intent accurately, so neural networks now\nhave become a preferred solution to processing such complex text matching.\nExisting methods predominantly employ representation-based architectures, which\nstrike a balance between high traffic capacity and low latency. However, they\nexhibit significant shortcomings in generalization and robustness when compared\nto interaction-based architectures. In this work, we introduce a robust\ninteraction-based modeling paradigm to address these shortcomings. It\nencompasses 1) a dynamic length representation scheme for expedited inference,\n2) a professional terms recognition method to identify subjects and core\nattributes from complex sentence structures, and 3) a contrastive adversarial\ntraining protocol to bolster the model's robustness and matching capabilities.\nExtensive offline evaluations demonstrate the superior robustness and\neffectiveness of our approach, and online A/B testing confirms its ability to\nimprove relevance in the same exposure position, resulting in more clicks and\nconversions. To the best of our knowledge, this method is the first\ninteraction-based approach for large e-commerce search relevance calculation.\nNotably, we have deployed it for the entire search traffic on alibaba.com, the\nlargest B2B e-commerce platform in the world.", "published": "2024-06-04 09:24:04", "link": "http://arxiv.org/abs/2406.02135v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Synergetic Event Understanding: A Collaborative Approach to\n  Cross-Document Event Coreference Resolution with Large Language Models", "abstract": "Cross-document event coreference resolution (CDECR) involves clustering event\nmentions across multiple documents that refer to the same real-world events.\nExisting approaches utilize fine-tuning of small language models (SLMs) like\nBERT to address the compatibility among the contexts of event mentions.\nHowever, due to the complexity and diversity of contexts, these models are\nprone to learning simple co-occurrences. Recently, large language models (LLMs)\nlike ChatGPT have demonstrated impressive contextual understanding, yet they\nencounter challenges in adapting to specific information extraction (IE) tasks.\nIn this paper, we propose a collaborative approach for CDECR, leveraging the\ncapabilities of both a universally capable LLM and a task-specific SLM. The\ncollaborative strategy begins with the LLM accurately and comprehensively\nsummarizing events through prompting. Then, the SLM refines its learning of\nevent representations based on these insights during fine-tuning. Experimental\nresults demonstrate that our approach surpasses the performance of both the\nlarge and small language models individually, forming a complementary\nadvantage. Across various datasets, our approach achieves state-of-the-art\nperformance, underscoring its effectiveness in diverse scenarios.", "published": "2024-06-04 09:35:47", "link": "http://arxiv.org/abs/2406.02148v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FedMKT: Federated Mutual Knowledge Transfer for Large and Small Language\n  Models", "abstract": "Recent research in federated large language models (LLMs) has primarily\nfocused on enabling clients to fine-tune their locally deployed homogeneous\nLLMs collaboratively or on transferring knowledge from server-based LLMs to\nsmall language models (SLMs) at downstream clients. However, a significant gap\nremains in the simultaneous mutual enhancement of both the server's LLM and\nclients' SLMs. To bridge this gap, we propose FedMKT, a parameter-efficient\nfederated mutual knowledge transfer framework for large and small language\nmodels. This framework is designed to adaptively transfer knowledge from the\nserver's LLM to clients' SLMs while concurrently enriching the LLM with\nclients' unique domain insights. We facilitate token alignment using minimum\nedit distance (MinED) and then selective mutual knowledge transfer between\nclient-side SLMs and a server-side LLM, aiming to collectively enhance their\nperformance. Through extensive experiments across three distinct scenarios, we\nevaluate the effectiveness of FedMKT using various public LLMs and SLMs on a\nrange of NLP text generation tasks. Empirical results demonstrate that FedMKT\nsimultaneously boosts the performance of both LLMs and SLMs.", "published": "2024-06-04 11:36:09", "link": "http://arxiv.org/abs/2406.02224v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Modeling Emotional Trajectories in Written Stories Utilizing\n  Transformers and Weakly-Supervised Learning", "abstract": "Telling stories is an integral part of human communication which can evoke\nemotions and influence the affective states of the audience. Automatically\nmodeling emotional trajectories in stories has thus attracted considerable\nscholarly interest. However, as most existing works have been limited to\nunsupervised dictionary-based approaches, there is no benchmark for this task.\nWe address this gap by introducing continuous valence and arousal labels for an\nexisting dataset of children's stories originally annotated with discrete\nemotion categories. We collect additional annotations for this data and map the\ncategorical labels to the continuous valence and arousal space. For predicting\nthe thus obtained emotionality signals, we fine-tune a DeBERTa model and\nimprove upon this baseline via a weakly supervised learning approach. The best\nconfiguration achieves a Concordance Correlation Coefficient (CCC) of $.8221$\nfor valence and $.7125$ for arousal on the test set, demonstrating the efficacy\nof our proposed approach. A detailed analysis shows the extent to which the\nresults vary depending on factors such as the author, the individual story, or\nthe section within the story. In addition, we uncover the weaknesses of our\napproach by investigating examples that prove to be difficult to predict.", "published": "2024-06-04 12:17:16", "link": "http://arxiv.org/abs/2406.02251v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Understanding Retrieval Robustness for Retrieval-Augmented Image\n  Captioning", "abstract": "Recent advances in retrieval-augmented models for image captioning highlight\nthe benefit of retrieving related captions for efficient, lightweight models\nwith strong domain-transfer capabilities. While these models demonstrate the\nsuccess of retrieval augmentation, retrieval models are still far from perfect\nin practice: the retrieved information can sometimes mislead the model,\nresulting in incorrect generation and worse performance. In this paper, we\nanalyze the robustness of a retrieval-augmented captioning model SmallCap. Our\nanalysis shows that the model is sensitive to tokens that appear in the\nmajority of the retrieved captions, and the input attribution shows that those\ntokens are likely copied into the generated output. Given these findings, we\npropose to train the model by sampling retrieved captions from more diverse\nsets. This decreases the chance that the model learns to copy majority tokens,\nand improves both in-domain and cross-domain performance.", "published": "2024-06-04 12:41:54", "link": "http://arxiv.org/abs/2406.02265v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Technical Language Processing for Telecommunications Specifications", "abstract": "Large Language Models (LLMs) are continuously being applied in a more diverse\nset of contexts. At their current state, however, even state-of-the-art LLMs\nsuch as Generative Pre-Trained Transformer 4 (GTP-4) have challenges when\nextracting information from real-world technical documentation without a heavy\npreprocessing. One such area with real-world technical documentation is\ntelecommunications engineering, which could greatly benefit from\ndomain-specific LLMs. The unique format and overall structure of\ntelecommunications internal specifications differs greatly from standard\nEnglish and thus it is evident that the application of out-of-the-box Natural\nLanguage Processing (NLP) tools is not a viable option. In this article, we\noutline the limitations of out-of-the-box NLP tools for processing technical\ninformation generated by telecommunications experts, and expand the concept of\nTechnical Language Processing (TLP) to the telecommunication domain.\nAdditionally, we explore the effect of domain-specific LLMs in the work of\nSpecification Engineers, emphasizing the potential benefits of adopting\ndomain-specific LLMs to speed up the training of experts in different\ntelecommunications fields.", "published": "2024-06-04 13:57:22", "link": "http://arxiv.org/abs/2406.02325v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On Affine Homotopy between Language Encoders", "abstract": "Pre-trained language encoders -- functions that represent text as vectors --\nare an integral component of many NLP tasks. We tackle a natural question in\nlanguage encoder analysis: What does it mean for two encoders to be similar? We\ncontend that a faithful measure of similarity needs to be \\emph{intrinsic},\nthat is, task-independent, yet still be informative of \\emph{extrinsic}\nsimilarity -- the performance on downstream tasks. It is common to consider two\nencoders similar if they are \\emph{homotopic}, i.e., if they can be aligned\nthrough some transformation. In this spirit, we study the properties of\n\\emph{affine} alignment of language encoders and its implications on extrinsic\nsimilarity. We find that while affine alignment is fundamentally an asymmetric\nnotion of similarity, it is still informative of extrinsic similarity. We\nconfirm this on datasets of natural language representations. Beyond providing\nuseful bounds on extrinsic similarity, affine intrinsic similarity also allows\nus to begin uncovering the structure of the space of pre-trained encoders by\ndefining an order over them.", "published": "2024-06-04 13:58:28", "link": "http://arxiv.org/abs/2406.02329v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Extended Mind Transformers", "abstract": "Pre-trained language models demonstrate general intelligence and common\nsense, but long inputs quickly become a bottleneck for memorizing information\nat inference time. We resurface a simple method, Memorizing Transformers (Wu et\nal., 2022), that gives the model access to a bank of pre-computed memories. We\nshow that it is possible to fix many of the shortcomings of the original\nmethod, such as the need for fine-tuning, by critically assessing how\npositional encodings should be updated for the keys and values retrieved. This\nintuitive method uses the model's own key/query system to select and attend to\nthe most relevant memories at each generation step, rather than using external\nembeddings. We demonstrate the importance of external information being\nretrieved in a majority of decoder layers, contrary to previous work. We open\nsource a new counterfactual long-range retrieval benchmark, and show that\nExtended Mind Transformers outperform today's state of the art by 6% on\naverage.", "published": "2024-06-04 14:00:25", "link": "http://arxiv.org/abs/2406.02332v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Linguistic Fingerprint in Transformer Models: How Language Variation\n  Influences Parameter Selection in Irony Detection", "abstract": "This paper explores the correlation between linguistic diversity, sentiment\nanalysis and transformer model architectures. We aim to investigate how\ndifferent English variations impact transformer-based models for irony\ndetection. To conduct our study, we used the EPIC corpus to extract five\ndiverse English variation-specific datasets and applied the KEN pruning\nalgorithm on five different architectures. Our results reveal several\nsimilarities between optimal subnetworks, which provide insights into the\nlinguistic variations that share strong resemblances and those that exhibit\ngreater dissimilarities. We discovered that optimal subnetworks across models\nshare at least 60% of their parameters, emphasizing the significance of\nparameter values in capturing and interpreting linguistic variations. This\nstudy highlights the inherent structural similarities between models trained on\ndifferent variants of the same language and also the critical role of parameter\nvalues in capturing these nuances.", "published": "2024-06-04 14:09:36", "link": "http://arxiv.org/abs/2406.02338v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LlamaCare: A Large Medical Language Model for Enhancing Healthcare\n  Knowledge Sharing", "abstract": "Large language models (LLMs) have shown amazing capabilities in knowledge\nmemorization and the present. However, when it comes to domain-specific\nknowledge and downstream tasks like medical, general LLMs are often unable to\ngive precise answers. In addition, when people want LLMs to answer\nclassification questions, they usually go through instruction tuning first.\nHowever, LLMs do not always give a direct index of the categorization after\ninstruction tuning. In this paper, we proposed LlamaCare, a fine-tuned medical\nlanguage model, and Extended Classification Integration(ECI), a module to\nhandle classification problems of LLMs. Our contributions are : (i) We\nfine-tuned a large language model of medical knowledge with very low carbon\nemissions and achieved similar performance with ChatGPT by a 24G GPU. (ii) We\nsolved the problem of redundant categorical answers and improved the\nperformance of LLMs by proposing a new module called Extended Classification\nIntegration. (iii) We released our processed data for one-shot and few-shot\ntraining for some benchmarks such as PubMedQA and USMLE 1-3 step. Our method\nachieves a close performance comparable to some state-of-the-art models with\nthe same quantity of parameters on benchmarks, while being more environmentally\nfriendly by using less GPU computation time. Our models, codes, and datasets\ncan be found at \\url{https://github.com/Stephen-SMJ/LLamaCare}.", "published": "2024-06-04 14:24:53", "link": "http://arxiv.org/abs/2406.02350v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Language Models Make Sample-Efficient Recommender Systems", "abstract": "Large language models (LLMs) have achieved remarkable progress in the field\nof natural language processing (NLP), demonstrating remarkable abilities in\nproducing text that resembles human language for various tasks. This opens up\nnew opportunities for employing them in recommender systems (RSs). In this\npaper, we specifically examine the sample efficiency of LLM-enhanced\nrecommender systems, which pertains to the model's capacity to attain superior\nperformance with a limited quantity of training data. Conventional\nrecommendation models (CRMs) often need a large amount of training data because\nof the sparsity of features and interactions. Hence, we propose and verify our\ncore viewpoint: Large Language Models Make Sample-Efficient Recommender\nSystems. We propose a simple yet effective framework (i.e., Laser) to validate\nthe viewpoint from two aspects: (1) LLMs themselves are sample-efficient\nrecommenders; and (2) LLMs, as feature generators and encoders, make CRMs more\nsample-efficient. Extensive experiments on two public datasets show that Laser\nrequires only a small fraction of training samples to match or even surpass\nCRMs that are trained on the entire training set, demonstrating superior sample\nefficiency.", "published": "2024-06-04 14:46:25", "link": "http://arxiv.org/abs/2406.02368v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "The Scandinavian Embedding Benchmarks: Comprehensive Assessment of\n  Multilingual and Monolingual Text Embedding", "abstract": "The evaluation of English text embeddings has transitioned from evaluating a\nhandful of datasets to broad coverage across many tasks through benchmarks such\nas MTEB. However, this is not the case for multilingual text embeddings due to\na lack of available benchmarks. To address this problem, we introduce the\nScandinavian Embedding Benchmark (SEB). SEB is a comprehensive framework that\nenables text embedding evaluation for Scandinavian languages across 24 tasks,\n10 subtasks, and 4 task categories. Building on SEB, we evaluate more than 26\nmodels, uncovering significant performance disparities between public and\ncommercial solutions not previously captured by MTEB. We open-source SEB and\nintegrate it with MTEB, thus bridging the text embedding evaluation gap for\nScandinavian languages.", "published": "2024-06-04 15:11:27", "link": "http://arxiv.org/abs/2406.02396v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Representations as Language: An Information-Theoretic Framework for\n  Interpretability", "abstract": "Large scale neural models show impressive performance across a wide array of\nlinguistic tasks. Despite this they remain, largely, black-boxes - inducing\nvector-representations of their input that prove difficult to interpret. This\nlimits our ability to understand what they learn, and when the learn it, or\ndescribe what kinds of representations generalise well out of distribution. To\naddress this we introduce a novel approach to interpretability that looks at\nthe mapping a model learns from sentences to representations as a kind of\nlanguage in its own right. In doing so we introduce a set of\ninformation-theoretic measures that quantify how structured a model's\nrepresentations are with respect to its input, and when during training that\nstructure arises. Our measures are fast to compute, grounded in linguistic\ntheory, and can predict which models will generalise best based on their\nrepresentations. We use these measures to describe two distinct phases of\ntraining a transformer: an initial phase of in-distribution learning which\nreduces task loss, then a second stage where representations becoming robust to\nnoise. Generalisation performance begins to increase during this second phase,\ndrawing a link between generalisation and robustness to noise. Finally we look\nat how model size affects the structure of the representational space, showing\nthat larger models ultimately compress their representations more than their\nsmaller counterparts.", "published": "2024-06-04 16:14:00", "link": "http://arxiv.org/abs/2406.02449v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Landscape-Aware Growing: The Power of a Little LAG", "abstract": "Recently, there has been increasing interest in efficient pretraining\nparadigms for training Transformer-based models. Several recent approaches use\nsmaller models to initialize larger models in order to save computation (e.g.,\nstacking and fusion). In this work, we study the fundamental question of how to\nselect the best growing strategy from a given pool of growing strategies. Prior\nworks have extensively focused on loss- and/or function-preserving behavior at\ninitialization or simply performance at the end of training. Instead, we\nidentify that behavior at initialization can be misleading as a predictor of\nfinal performance and present an alternative perspective based on early\ntraining dynamics, which we call \"landscape-aware growing (LAG)\". We perform\nextensive analysis of correlation of the final performance with performance in\nthe initial steps of training and find early and more accurate predictions of\nthe optimal growing strategy (i.e., with only a small \"lag\" after\ninitialization). This perspective also motivates an adaptive strategy for\ngradual stacking.", "published": "2024-06-04 16:38:57", "link": "http://arxiv.org/abs/2406.02469v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Large Language Models as Carriers of Hidden Messages", "abstract": "Simple fine-tuning can embed hidden text into large language models (LLMs),\nwhich is revealed only when triggered by a specific query. Applications include\nLLM fingerprinting, where a unique identifier is embedded to verify licensing\ncompliance, and steganography, where the LLM carries hidden messages disclosed\nthrough a trigger query.\n  Our work demonstrates that embedding hidden text via fine-tuning, although\nseemingly secure due to the vast number of potential triggers, is vulnerable to\nextraction through analysis of the LLM's output decoding process. We introduce\nan extraction attack called Unconditional Token Forcing (UTF), which\niteratively feeds tokens from the LLM's vocabulary to reveal sequences with\nhigh token probabilities, indicating hidden text candidates. We also present\nUnconditional Token Forcing Confusion (UTFC), a defense paradigm that makes\nhidden text resistant to all known extraction attacks without degrading the\ngeneral performance of LLMs compared to standard fine-tuning. UTFC has both\nbenign (improving LLM fingerprinting) and malign applications (using LLMs to\ncreate covert communication channels).", "published": "2024-06-04 16:49:06", "link": "http://arxiv.org/abs/2406.02481v4", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Mitigate Position Bias in Large Language Models via Scaling a Single\n  Dimension", "abstract": "Large Language Models (LLMs) are increasingly applied in various real-world\nscenarios due to their excellent generalization capabilities and robust\ngenerative abilities. However, they exhibit position bias, also known as \"lost\nin the middle\", a phenomenon that is especially pronounced in long-context\nscenarios, which indicates the placement of the key information in different\npositions of a prompt can significantly affect accuracy. This paper first\nexplores the micro-level manifestations of position bias, concluding that\nattention weights are a micro-level expression of position bias. It further\nidentifies that, in addition to position embeddings, causal attention mask also\ncontributes to position bias by creating position-specific hidden states. Based\non these insights, we propose a method to mitigate position bias by scaling\nthis positional hidden states. Experiments on the NaturalQuestions\nMulti-document QA, KV retrieval, LongBench and timeline reorder tasks, using\nvarious models including RoPE models, context windowextended models, and Alibi\nmodels, demonstrate the effectiveness and generalizability of our approach. Our\nmethod can improve performance by up to 15.2% by modifying just one dimension\nof hidden states. Our code is available at https://aka.ms/PositionalHidden.", "published": "2024-06-04 17:55:38", "link": "http://arxiv.org/abs/2406.02536v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Self-Control of LLM Behaviors by Compressing Suffix Gradient into Prefix\n  Controller", "abstract": "We propose SelfControl, an inference-time model control method utilizing\ngradients to control the behavior of large language models (LLMs) without\nexplicit human annotations. Given a desired behavior expressed in a natural\nlanguage suffix string concatenated to the input prompt, SelfControl computes\ngradients of the LLM's self-evaluation of the suffix with respect to its latent\nrepresentations. The gradients are used to directly control the auto-regressive\ngeneration process towards desired behaviors, which eliminates human\nsupervision, achieves precise and transparent control, and offers on-the-fly\nadaptability. To further enhance efficiency, we introduce SelfControl_{Prefix},\na compact module that encapsulates the learned representations from gradients\ninto a SelfControl_{Prefix}, facilitating efficient inference-time control with\nno latency compared to the original model and allowing control for multiple\nbehaviors simultaneously. Our experiments demonstrate SelfControl's efficacy\nacross multiple domains, where it improves over SOTA for 8.3% in\ndetoxification, 3.1% in truthfulness enhancement, 4%~10% in controlling on\nemotion tones, and 48.2% in privacy protection, i.e., completely remove privacy\nleakage issue. Additionally, we demonstrate that SelfControl can be used for\ndata synthesis and to improve reasoning abilities.", "published": "2024-06-04 19:05:10", "link": "http://arxiv.org/abs/2406.02721v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ArguMentor: Augmenting User Experiences with Counter-Perspectives", "abstract": "We encounter arguments everyday in the form of social media posts,\npresidential debates, news articles, and even advertisements. A ubiquitous,\ninfluential example is the opinion piece (op-ed). Opinion pieces can provide\nvaluable perspectives, but they often represent only one side of a story, which\ncan make readers susceptible to confirmation bias and echo chambers. Exposure\nto different perspectives can help readers overcome these obstacles and form\nmore robust, nuanced views on important societal issues. We designed\nArguMentor, a human-AI collaboration system that highlights claims in opinion\npieces, identifies counter-arguments for them using a LLM, and generates a\ncontext-based summary of based on current events. It further enhances user\nunderstanding through additional features like a Q\\&A bot (that answers user\nquestions pertaining to the text), DebateMe (an agent that users can argue any\nside of the piece with) and highlighting (where users can highlight a word or\npassage to get its definition or context). Our evaluation on news op-eds shows\nthat participants can generate more arguments and counter-arguments and display\nhigher critical thinking skills after engaging with the system. Further\ndiscussion highlights a more general need for this kind of a system.", "published": "2024-06-04 21:43:56", "link": "http://arxiv.org/abs/2406.02795v3", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "RAG-based Crowdsourcing Task Decomposition via Masked Contrastive\n  Learning with Prompts", "abstract": "Crowdsourcing is a critical technology in social manufacturing, which\nleverages an extensive and boundless reservoir of human resources to handle a\nwide array of complex tasks. The successful execution of these complex tasks\nrelies on task decomposition (TD) and allocation, with the former being a\nprerequisite for the latter. Recently, pre-trained language models (PLMs)-based\nmethods have garnered significant attention. However, they are constrained to\nhandling straightforward common-sense tasks due to their inherent restrictions\ninvolving limited and difficult-to-update knowledge as well as the presence of\nhallucinations. To address these issues, we propose a retrieval-augmented\ngeneration-based crowdsourcing framework that reimagines TD as event detection\nfrom the perspective of natural language understanding. However, the existing\ndetection methods fail to distinguish differences between event types and\nalways depend on heuristic rules and external semantic analyzing tools.\nTherefore, we present a Prompt-Based Contrastive learning framework for TD\n(PBCT), which incorporates a prompt-based trigger detector to overcome\ndependence. Additionally, trigger-attentive sentinel and masked contrastive\nlearning are introduced to provide varying attention to trigger and contextual\nfeatures according to different event types. Experiment results demonstrate the\ncompetitiveness of our method in both supervised and zero-shot detection. A\ncase study on printed circuit board manufacturing is showcased to validate its\nadaptability to unknown professional domains.", "published": "2024-06-04 08:34:19", "link": "http://arxiv.org/abs/2406.06577v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Break the Chain: Large Language Models Can be Shortcut Reasoners", "abstract": "Recent advancements in Chain-of-Thought (CoT) reasoning utilize complex\nmodules but are hampered by high token consumption, limited applicability, and\nchallenges in reproducibility. This paper conducts a critical evaluation of CoT\nprompting, extending beyond arithmetic to include complex logical and\ncommonsense reasoning tasks, areas where standard CoT methods fall short. We\npropose the integration of human-like heuristics and shortcuts into language\nmodels (LMs) through \"break the chain\" strategies. These strategies disrupt\ntraditional CoT processes using controlled variables to assess their efficacy.\nAdditionally, we develop innovative zero-shot prompting strategies that\nencourage the use of shortcuts, enabling LMs to quickly exploit reasoning clues\nand bypass detailed procedural steps. Our comprehensive experiments across\nvarious LMs, both commercial and open-source, reveal that LMs maintain\neffective performance with \"break the chain\" strategies. We also introduce\nShortcutQA, a dataset specifically designed to evaluate reasoning through\nshortcuts, compiled from competitive tests optimized for heuristic reasoning\ntasks such as forward/backward reasoning and simplification. Our analysis\nconfirms that ShortcutQA not only poses a robust challenge to LMs but also\nserves as an essential benchmark for enhancing reasoning efficiency in AI.", "published": "2024-06-04 14:02:53", "link": "http://arxiv.org/abs/2406.06580v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "#EpiTwitter: Public Health Messaging During the COVID-19 Pandemic", "abstract": "Effective communication during health crises is critical, with social media\nserving as a key platform for public health experts (PHEs) to engage with the\npublic. However, it also amplifies pseudo-experts promoting contrarian views.\nDespite its importance, the role of emotional and moral language in PHEs'\ncommunication during COVID-19 remains under explored. This study examines how\nPHEs and pseudo-experts communicated on Twitter during the pandemic, focusing\non emotional and moral language and their engagement with political elites.\nAnalyzing tweets from 489 PHEs and 356 pseudo-experts from January 2020 to\nJanuary 2021, alongside public responses, we identified key priorities and\ndifferences in messaging strategy. PHEs prioritize masking, healthcare,\neducation, and vaccines, using positive emotional language like optimism. In\ncontrast, pseudo-experts discuss therapeutics and lockdowns more frequently,\nemploying negative emotions like pessimism and disgust. Negative emotional and\nmoral language tends to drive engagement, but positive language from PHEs\nfosters positivity in public responses. PHEs exhibit liberal partisanship,\nexpressing more positivity towards liberals and negativity towards conservative\nelites, while pseudo-experts show conservative partisanship. These findings\nshed light on the polarization of COVID-19 discourse and underscore the\nimportance of strategic use of emotional and moral language by experts to\nmitigate polarization and enhance public trust.", "published": "2024-06-04 00:37:29", "link": "http://arxiv.org/abs/2406.01866v2", "categories": ["cs.CL", "cs.CY", "cs.SI"], "primary_category": "cs.CL"}
{"title": "CR-UTP: Certified Robustness against Universal Text Perturbations on\n  Large Language Models", "abstract": "It is imperative to ensure the stability of every prediction made by a\nlanguage model; that is, a language's prediction should remain consistent\ndespite minor input variations, like word substitutions. In this paper, we\ninvestigate the problem of certifying a language model's robustness against\nUniversal Text Perturbations (UTPs), which have been widely used in universal\nadversarial attacks and backdoor attacks. Existing certified robustness based\non random smoothing has shown considerable promise in certifying the\ninput-specific text perturbations (ISTPs), operating under the assumption that\nany random alteration of a sample's clean or adversarial words would negate the\nimpact of sample-wise perturbations. However, with UTPs, masking only the\nadversarial words can eliminate the attack. A naive method is to simply\nincrease the masking ratio and the likelihood of masking attack tokens, but it\nleads to a significant reduction in both certified accuracy and the certified\nradius due to input corruption by extensive masking. To solve this challenge,\nwe introduce a novel approach, the superior prompt search method, designed to\nidentify a superior prompt that maintains higher certified accuracy under\nextensive masking. Additionally, we theoretically motivate why ensembles are a\nparticularly suitable choice as base prompts for random smoothing. The method\nis denoted by superior prompt ensembling technique. We also empirically confirm\nthis technique, obtaining state-of-the-art results in multiple settings. These\nmethodologies, for the first time, enable high certified accuracy against both\nUTPs and ISTPs. The source code of CR-UTP is available at \\url\n{https://github.com/UCFML-Research/CR-UTP}.", "published": "2024-06-04 01:02:22", "link": "http://arxiv.org/abs/2406.01873v2", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Explicitly Encoding Structural Symmetry is Key to Length Generalization\n  in Arithmetic Tasks", "abstract": "Despite the success of Transformers on language understanding, code\ngeneration, and logical reasoning, they still fail to generalize over length on\nbasic arithmetic tasks such as addition and multiplication. A major reason\nbehind this failure is the vast difference in structure between numbers and\ntext; For example, the numbers are typically parsed from right to left, and\nthere is a correspondence between digits at the same position across different\nnumbers. In contrast, for text, such symmetries are quite unnatural. In this\nwork, we propose to encode these semantics explicitly into the model via\nmodified number formatting and custom positional encodings. Empirically, our\nmethod allows a Transformer trained on numbers with at most 5-digits for\naddition and multiplication to generalize up to 50-digit numbers, without using\nadditional data for longer sequences. We further demonstrate that traditional\nabsolute positional encodings (APE) fail to generalize to longer sequences,\neven when trained with augmented data that captures task symmetries. To\nelucidate the importance of explicitly encoding structure, we prove that\nexplicit incorporation of structure via positional encodings is necessary for\nout-of-distribution generalization. Finally, we pinpoint other challenges\ninherent to length generalization beyond capturing symmetries, in particular\ncomplexity of the underlying task, and propose changes in the training\ndistribution to address them.", "published": "2024-06-04 02:00:07", "link": "http://arxiv.org/abs/2406.01895v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "HPE-CogVLM: Advancing Vision Language Models with a Head Pose Grounding\n  Task", "abstract": "Head pose estimation (HPE) requires a sophisticated understanding of 3D\nspatial relationships to generate precise yaw, pitch, and roll angles. Previous\nHPE models, primarily CNN-based, rely on cropped close-up human head images as\ninputs and often lack robustness in real-world scenario. Vision Language Models\n(VLMs) can analyze entire images while focusing on specific objects through\ntheir attention mechanisms. In this paper, we propose a novel framework to\nimprove the HPE accuracy by leveraging the object detection grounding\ncapability of a VLM, referred to as CogVLM. We empirically find that directly\nLoRA fine-tuning of this VLM for the HPE task fails to achieve desirable HPE\naccuracy, while some model merging methods can improve accuracy but frequently\nproduce blended invalid response formats, struggling to handle both object\ndetection and HPE tasks simultaneously. To integrate HPE capability into CogVLM\neffectively, we develop a novel LoRA layer-based model merging method. This\nmerging approach applies a high cosine similarity threshold and a\nwinner-takes-all layer selection strategy, aligning attention to the HPE task\nwhile preserving original object detection knowledge. It successfully resolves\nissues with blended invalid response formats and improves accuracy. Results\nshow that our HPE-CogVLM achieves a 31.5\\% reduction in Mean Absolute Error\nover the current state-of-the-art CNN model, 6DRepNet, in cross-dataset\nevaluation. Furthermore, HPE-CogVLM outperforms both directly LoRA fine-tuned\nand task arithmetic-based merged VLMs across all HPE metrics.", "published": "2024-06-04 02:51:26", "link": "http://arxiv.org/abs/2406.01914v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Process-Driven Autoformalization in Lean 4", "abstract": "Autoformalization, the conversion of natural language mathematics into formal\nlanguages, offers significant potential for advancing mathematical reasoning.\nHowever, existing efforts are limited to formal languages with substantial\nonline corpora and struggle to keep pace with rapidly evolving languages like\nLean 4. To bridge this gap, we propose a new benchmark \\textbf{Form}alization\nfor \\textbf{L}ean~\\textbf{4} (\\textbf{\\name}) designed to evaluate the\nautoformalization capabilities of large language models (LLMs). This benchmark\nencompasses a comprehensive assessment of questions, answers, formal\nstatements, and proofs. Additionally, we introduce a\n\\textbf{P}rocess-\\textbf{S}upervised \\textbf{V}erifier (\\textbf{PSV}) model\nthat leverages the precise feedback from Lean 4 compilers to enhance\nautoformalization. Our experiments demonstrate that the PSV method improves\nautoformalization, enabling higher accuracy using less filtered training data.\nFurthermore, when fine-tuned with data containing detailed process information,\nPSV can leverage the data more effectively, leading to more significant\nimprovements in autoformalization for Lean 4. Our dataset and code are\navailable at \\url{https://github.com/rookie-joe/PDA}.", "published": "2024-06-04 03:48:08", "link": "http://arxiv.org/abs/2406.01940v2", "categories": ["cs.CL", "cs.LG", "cs.LO"], "primary_category": "cs.CL"}
{"title": "Efficiently Train ASR Models that Memorize Less and Perform Better with\n  Per-core Clipping", "abstract": "Gradient clipping plays a vital role in training large-scale automatic speech\nrecognition (ASR) models. It is typically applied to minibatch gradients to\nprevent gradient explosion, and to the individual sample gradients to mitigate\nunintended memorization. This work systematically investigates the impact of a\nspecific granularity of gradient clipping, namely per-core clip-ping (PCC),\nacross training a wide range of ASR models. We empirically demonstrate that PCC\ncan effectively mitigate unintended memorization in ASR models. Surprisingly,\nwe find that PCC positively influences ASR performance metrics, leading to\nimproved convergence rates and reduced word error rates. To avoid tuning the\nadditional hyperparameter introduced by PCC, we further propose a novel\nvariant, adaptive per-core clipping (APCC), for streamlined optimization. Our\nfindings highlight the multifaceted benefits of PCC as a strategy for robust,\nprivacy-forward ASR model training.", "published": "2024-06-04 06:34:33", "link": "http://arxiv.org/abs/2406.02004v2", "categories": ["cs.CR", "cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
{"title": "Phonetic Enhanced Language Modeling for Text-to-Speech Synthesis", "abstract": "Recent language model-based text-to-speech (TTS) frameworks demonstrate\nscalability and in-context learning capabilities. However, they suffer from\nrobustness issues due to the accumulation of errors in speech unit predictions\nduring autoregressive language modeling. In this paper, we propose a phonetic\nenhanced language modeling method to improve the performance of TTS models. We\nleverage self-supervised representations that are phonetically rich as the\ntraining target for the autoregressive language model. Subsequently, a\nnon-autoregressive model is employed to predict discrete acoustic codecs that\ncontain fine-grained acoustic details. The TTS model focuses solely on\nlinguistic modeling during autoregressive training, thereby reducing the error\npropagation that occurs in non-autoregressive training. Both objective and\nsubjective evaluations validate the effectiveness of our proposed method.", "published": "2024-06-04 06:43:34", "link": "http://arxiv.org/abs/2406.02009v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Why Would You Suggest That? Human Trust in Language Model Responses", "abstract": "The emergence of Large Language Models (LLMs) has revealed a growing need for\nhuman-AI collaboration, especially in creative decision-making scenarios where\ntrust and reliance are paramount. Through human studies and model evaluations\non the open-ended News Headline Generation task from the LaMP benchmark, we\nanalyze how the framing and presence of explanations affect user trust and\nmodel performance. Overall, we provide evidence that adding an explanation in\nthe model response to justify its reasoning significantly increases\nself-reported user trust in the model when the user has the opportunity to\ncompare various responses. Position and faithfulness of these explanations are\nalso important factors. However, these gains disappear when users are shown\nresponses independently, suggesting that humans trust all model responses,\nincluding deceptive ones, equitably when they are shown in isolation. Our\nfindings urge future research to delve deeper into the nuanced evaluation of\ntrust in human-machine teaming systems.", "published": "2024-06-04 06:57:47", "link": "http://arxiv.org/abs/2406.02018v2", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Alice in Wonderland: Simple Tasks Showing Complete Reasoning Breakdown\n  in State-Of-the-Art Large Language Models", "abstract": "Large Language Models (LLMs) are often described as instances of foundation\nmodels that possess strong generalization obeying scaling laws, and therefore\ntransfer robustly across various conditions in few- or zero-shot manner. Such\nclaims rely on standardized benchmarks that suppose to measure generalization\nand reasoning, where state-of-the-art (SOTA) models score high. We demonstrate\nhere a dramatic breakdown of generalization and basic reasoning of all SOTA\nmodels claiming strong function, including large scale advanced models like\nGPT-4 or Claude 3 Opus, using a simple, short common sense math problem\nformulated in concise natural language, easily solvable by humans (AIW\nproblem). The breakdown is dramatic as it manifests on a simple problem in both\nlow average performance and strong performance fluctuations on natural\nvariations in problem template that do not change either problem structure or\nits difficulty at all. By testing models on further control problems with\nsimilar form, we rule out that breakdown might be rooted in minor low-level\nissues like natural language or numbers parsing. We also observe strong\noverconfidence in the wrong solutions, expressed in form of plausible sounding\nexplanation-like confabulations. Various standard interventions in an attempt\nto get the right solution, like chain-of-thought prompting, or urging the\nmodels to reconsider the wrong solutions again by multi step re-evaluation,\nfail. We use these observations to stimulate re-assessment of the capabilities\nof current generation of LLMs as claimed by standardized benchmarks. Such\nre-assessment also requires common action to create standardized benchmarks\nthat would allow proper detection of such deficits in generalization and\nreasoning that obviously remain undiscovered by current state-of-the-art\nevaluation procedures, where SOTA LLMs manage to score high. Code:\nhttps://github.com/LAION-AI/AIW", "published": "2024-06-04 07:43:33", "link": "http://arxiv.org/abs/2406.02061v5", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "LongSSM: On the Length Extension of State-space Models in Language\n  Modelling", "abstract": "In this paper, we investigate the length-extension of state-space models\n(SSMs) in language modeling. Length extension involves training models on short\nsequences and testing them on longer ones. We show that state-space models\ntrained with zero hidden states initialization have difficulty doing length\nextension. We explain this difficulty by pointing out the length extension is\nequivalent to polynomial extrapolation. Based on the theory, we propose a\nsimple yet effective method - changing the hidden states initialization scheme\n- to improve the length extension. Moreover, our method shows that using long\ntraining sequence length is beneficial but not necessary to length extension.\nChanging the hidden state initialization enables the efficient training of\nlong-memory model with a smaller training context length.", "published": "2024-06-04 08:02:39", "link": "http://arxiv.org/abs/2406.02080v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "math.DS"], "primary_category": "cs.CL"}
{"title": "Iteration Head: A Mechanistic Study of Chain-of-Thought", "abstract": "Chain-of-Thought (CoT) reasoning is known to improve Large Language Models\nboth empirically and in terms of theoretical approximation power. However, our\nunderstanding of the inner workings and conditions of apparition of CoT\ncapabilities remains limited. This paper helps fill this gap by demonstrating\nhow CoT reasoning emerges in transformers in a controlled and interpretable\nsetting. In particular, we observe the appearance of a specialized attention\nmechanism dedicated to iterative reasoning, which we coined \"iteration heads\".\nWe track both the emergence and the precise working of these iteration heads\ndown to the attention level, and measure the transferability of the CoT skills\nto which they give rise between tasks.", "published": "2024-06-04 09:11:46", "link": "http://arxiv.org/abs/2406.02128v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "SimulTron: On-Device Simultaneous Speech to Speech Translation", "abstract": "Simultaneous speech-to-speech translation (S2ST) holds the promise of\nbreaking down communication barriers and enabling fluid conversations across\nlanguages. However, achieving accurate, real-time translation through mobile\ndevices remains a major challenge. We introduce SimulTron, a novel S2ST\narchitecture designed to tackle this task. SimulTron is a lightweight direct\nS2ST model that uses the strengths of the Translatotron framework while\nincorporating key modifications for streaming operation, and an adjustable\nfixed delay. Our experiments show that SimulTron surpasses Translatotron 2 in\noffline evaluations. Furthermore, real-time evaluations reveal that SimulTron\nimproves upon the performance achieved by Translatotron 1. Additionally,\nSimulTron achieves superior BLEU scores and latency compared to previous\nreal-time S2ST method on the MuST-C dataset. Significantly, we have\nsuccessfully deployed SimulTron on a Pixel 7 Pro device, show its potential for\nsimultaneous S2ST on-device.", "published": "2024-06-04 09:21:31", "link": "http://arxiv.org/abs/2406.02133v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Whistle: Data-Efficient Multilingual and Crosslingual Speech Recognition\n  via Weakly Phonetic Supervision", "abstract": "There exist three approaches for multilingual and crosslingual automatic\nspeech recognition (MCL-ASR) - supervised pretraining with phonetic or\ngraphemic transcription, and self-supervised pretraining. We find that\npretraining with phonetic supervision has been underappreciated so far for\nMCL-ASR, while conceptually it is more advantageous for information sharing\nbetween different languages. This paper explores the approach of pretraining\nwith weakly phonetic supervision towards data-efficient MCL-ASR, which is\ncalled Whistle. We relax the requirement of gold-standard human-validated\nphonetic transcripts, and obtain International Phonetic Alphabet (IPA) based\ntranscription by leveraging the LanguageNet grapheme-to-phoneme (G2P) models.\nWe construct a common experimental setup based on the CommonVoice dataset,\ncalled CV-Lang10, with 10 seen languages and 2 unseen languages. A set of\nexperiments are conducted on CV-Lang10 to compare, as fair as possible, the\nthree approaches under the common setup for MCL-ASR. Experiments demonstrate\nthe advantages of phoneme-based models (Whistle) for MCL-ASR, in terms of\nspeech recognition for seen languages, crosslingual performance for unseen\nlanguages with different amounts of few-shot data, overcoming catastrophic\nforgetting, and training efficiency. It is found that when training data is\nmore limited, phoneme supervision can achieve better results compared to\nsubword supervision and self-supervision, thereby providing higher\ndata-efficiency. To support reproducibility and promote future research along\nthis direction, we release the code, models and data for the entire pipeline of\nWhistle at https://github.com/thu-spmi/CAT/tree/master/egs/cv-lang10.", "published": "2024-06-04 09:56:05", "link": "http://arxiv.org/abs/2406.02166v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Why Only Text: Empowering Vision-and-Language Navigation with\n  Multi-modal Prompts", "abstract": "Current Vision-and-Language Navigation (VLN) tasks mainly employ textual\ninstructions to guide agents. However, being inherently abstract, the same\ntextual instruction can be associated with different visual signals, causing\nsevere ambiguity and limiting the transfer of prior knowledge in the vision\ndomain from the user to the agent. To fill this gap, we propose\nVision-and-Language Navigation with Multi-modal Prompts (VLN-MP), a novel task\naugmenting traditional VLN by integrating both natural language and images in\ninstructions. VLN-MP not only maintains backward compatibility by effectively\nhandling text-only prompts but also consistently shows advantages with\ndifferent quantities and relevance of visual prompts. Possible forms of visual\nprompts include both exact and similar object images, providing adaptability\nand versatility in diverse navigation scenarios. To evaluate VLN-MP under a\nunified framework, we implement a new benchmark that offers: (1) a\ntraining-free pipeline to transform textual instructions into multi-modal forms\nwith landmark images; (2) diverse datasets with multi-modal instructions for\ndifferent downstream tasks; (3) a novel module designed to process various\nimage prompts for seamless integration with state-of-the-art VLN models.\nExtensive experiments on four VLN benchmarks (R2R, RxR, REVERIE, CVDN) show\nthat incorporating visual prompts significantly boosts navigation performance.\nWhile maintaining efficiency with text-only prompts, VLN-MP enables agents to\nnavigate in the pre-explore setting and outperform text-based models, showing\nits broader applicability.", "published": "2024-06-04 11:06:13", "link": "http://arxiv.org/abs/2406.02208v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Description Boosting for Zero-Shot Entity and Relation Classification", "abstract": "Zero-shot entity and relation classification models leverage available\nexternal information of unseen classes -- e.g., textual descriptions -- to\nannotate input text data. Thanks to the minimum data requirement, Zero-Shot\nLearning (ZSL) methods have high value in practice, especially in applications\nwhere labeled data is scarce. Even though recent research in ZSL has\ndemonstrated significant results, our analysis reveals that those methods are\nsensitive to provided textual descriptions of entities (or relations). Even a\nminor modification of descriptions can lead to a change in the decision\nboundary between entity (or relation) classes. In this paper, we formally\ndefine the problem of identifying effective descriptions for zero shot\ninference. We propose a strategy for generating variations of an initial\ndescription, a heuristic for ranking them and an ensemble method capable of\nboosting the predictions of zero-shot models through description enhancement.\nEmpirical results on four different entity and relation classification datasets\nshow that our proposed method outperform existing approaches and achieve new\nSOTA results on these datasets under the ZSL settings. The source code of the\nproposed solutions and the evaluation framework are open-sourced.", "published": "2024-06-04 12:09:44", "link": "http://arxiv.org/abs/2406.02245v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Language Models Do Hard Arithmetic Tasks Easily and Hardly Do Easy\n  Arithmetic Tasks", "abstract": "The ability (and inability) of large language models (LLMs) to perform\narithmetic tasks has been the subject of much theoretical and practical debate.\nWe show that LLMs are frequently able to correctly and confidently predict the\nfirst digit of n-digit by m-digit multiplication tasks without using chain of\nthought reasoning, despite these tasks require compounding operations to solve.\nSimultaneously, LLMs in practice often fail to correctly or confidently predict\nthe last digit of an n-digit by m-digit multiplication, a task equivalent to\n1-digit by 1-digit multiplication which can be easily learned or memorized. We\nshow that the latter task can be solved more robustly when the LLM is\nconditioned on all of the correct higher-order digits, which on average\nincreases the confidence of the correct last digit on 5-digit by 5-digit\nmultiplication tasks using Llama 2-13B by over 230% (0.13 to 0.43) and\nMistral-7B by 150% (0.22 to 0.55).", "published": "2024-06-04 14:34:39", "link": "http://arxiv.org/abs/2406.02356v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "XRec: Large Language Models for Explainable Recommendation", "abstract": "Recommender systems help users navigate information overload by providing\npersonalized recommendations aligned with their preferences. Collaborative\nFiltering (CF) is a widely adopted approach, but while advanced techniques like\ngraph neural networks (GNNs) and self-supervised learning (SSL) have enhanced\nCF models for better user representations, they often lack the ability to\nprovide explanations for the recommended items. Explainable recommendations aim\nto address this gap by offering transparency and insights into the\nrecommendation decision-making process, enhancing users' understanding. This\nwork leverages the language capabilities of Large Language Models (LLMs) to\npush the boundaries of explainable recommender systems. We introduce a\nmodel-agnostic framework called XRec, which enables LLMs to provide\ncomprehensive explanations for user behaviors in recommender systems. By\nintegrating collaborative signals and designing a lightweight collaborative\nadaptor, the framework empowers LLMs to understand complex patterns in\nuser-item interactions and gain a deeper understanding of user preferences. Our\nextensive experiments demonstrate the effectiveness of XRec, showcasing its\nability to generate comprehensive and meaningful explanations that outperform\nbaseline approaches in explainable recommender systems. We open-source our\nmodel implementation at https://github.com/HKUDS/XRec.", "published": "2024-06-04 14:55:14", "link": "http://arxiv.org/abs/2406.02377v2", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Multiple Choice Questions and Large Languages Models: A Case Study with\n  Fictional Medical Data", "abstract": "Large Language Models (LLMs) like ChatGPT demonstrate significant potential\nin the medical field, often evaluated using multiple-choice questions (MCQs)\nsimilar to those found on the USMLE. Despite their prevalence in medical\neducation, MCQs have limitations that might be exacerbated when assessing LLMs.\nTo evaluate the effectiveness of MCQs in assessing the performance of LLMs, we\ndeveloped a fictional medical benchmark focused on a non-existent gland, the\nGlianorex. This approach allowed us to isolate the knowledge of the LLM from\nits test-taking abilities. We used GPT-4 to generate a comprehensive textbook\non the Glianorex in both English and French and developed corresponding\nmultiple-choice questions in both languages. We evaluated various open-source,\nproprietary, and domain-specific LLMs using these questions in a zero-shot\nsetting. The models achieved average scores around 67%, with minor performance\ndifferences between larger and smaller models. Performance was slightly higher\nin English than in French. Fine-tuned medical models showed some improvement\nover their base versions in English but not in French. The uniformly high\nperformance across models suggests that traditional MCQ-based benchmarks may\nnot accurately measure LLMs' clinical knowledge and reasoning abilities,\ninstead highlighting their pattern recognition skills. This study underscores\nthe need for more robust evaluation methods to better assess the true\ncapabilities of LLMs in medical contexts.", "published": "2024-06-04 15:08:56", "link": "http://arxiv.org/abs/2406.02394v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Language-Universal Speech Attributes Modeling for Zero-Shot Multilingual\n  Spoken Keyword Recognition", "abstract": "We propose a novel language-universal approach to end-to-end automatic spoken\nkeyword recognition (SKR) leveraging upon (i) a self-supervised pre-trained\nmodel, and (ii) a set of universal speech attributes (manner and place of\narticulation). Specifically, Wav2Vec2.0 is used to generate robust speech\nrepresentations, followed by a linear output layer to produce attribute\nsequences. A non-trainable pronunciation model then maps sequences of\nattributes into spoken keywords in a multilingual setting. Experiments on the\nMultilingual Spoken Words Corpus show comparable performances to character- and\nphoneme-based SKR in seen languages. The inclusion of domain adversarial\ntraining (DAT) improves the proposed framework, outperforming both character-\nand phoneme-based SKR approaches with 13.73% and 17.22% relative word error\nrate (WER) reduction in seen languages, and achieves 32.14% and 19.92% WER\nreduction for unseen languages in zero-shot settings.", "published": "2024-06-04 16:59:11", "link": "http://arxiv.org/abs/2406.02488v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "TopViewRS: Vision-Language Models as Top-View Spatial Reasoners", "abstract": "Top-view perspective denotes a typical way in which humans read and reason\nover different types of maps, and it is vital for localization and navigation\nof humans as well as of `non-human' agents, such as the ones backed by large\nVision-Language Models (VLMs). Nonetheless, spatial reasoning capabilities of\nmodern VLMs remain unattested and underexplored. In this work, we thus study\ntheir capability to understand and reason over spatial relations from the top\nview. The focus on top view also enables controlled evaluations at different\ngranularity of spatial reasoning; we clearly disentangle different abilities\n(e.g., recognizing particular objects versus understanding their relative\npositions). We introduce the TopViewRS (Top-View Reasoning in Space) dataset,\nconsisting of 11,384 multiple-choice questions with either realistic or\nsemantic top-view map as visual input. We then use it to study and evaluate\nVLMs across 4 perception and reasoning tasks with different levels of\ncomplexity. Evaluation of 10 representative open- and closed-source VLMs\nreveals the gap of more than 50% compared to average human performance, and it\nis even lower than the random baseline in some cases. Although additional\nexperiments show that Chain-of-Thought reasoning can boost model capabilities\nby 5.82% on average, the overall performance of VLMs remains limited. Our\nfindings underscore the critical need for enhanced model capability in top-view\nspatial reasoning and set a foundation for further research towards human-level\nproficiency of VLMs in real-world multimodal tasks.", "published": "2024-06-04 17:55:43", "link": "http://arxiv.org/abs/2406.02537v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Parrot: Multilingual Visual Instruction Tuning", "abstract": "The rapid development of Multimodal Large Language Models (MLLMs) like GPT-4V\nhas marked a significant step towards artificial general intelligence. Existing\nmethods mainly focus on aligning vision encoders with LLMs through supervised\nfine-tuning (SFT) to endow LLMs with multimodal abilities, making MLLMs'\ninherent ability to react to multiple languages progressively deteriorate as\nthe training process evolves. We empirically find that the imbalanced SFT\ndatasets, primarily composed of English-centric image-text pairs, lead to\nsignificantly reduced performance in non-English languages. This is due to the\nfailure of aligning the vision encoder and LLM with multilingual tokens during\nthe SFT process. In this paper, we introduce Parrot, a novel method that\nutilizes textual guidance to drive visual token alignment at the language\nlevel. Parrot makes the visual tokens condition on diverse language inputs and\nuses Mixture-of-Experts (MoE) to promote the alignment of multilingual tokens.\nSpecifically, to enhance non-English visual tokens alignment, we compute the\ncross-attention using the initial visual features and textual embeddings, the\nresult of which is then fed into the MoE router to select the most relevant\nexperts. The selected experts subsequently convert the initial visual tokens\ninto language-specific visual tokens. Moreover, considering the current lack of\nbenchmarks for evaluating multilingual capabilities within the field, we\ncollect and make available a Massive Multilingual Multimodal Benchmark which\nincludes 6 languages, 15 categories, and 12,000 questions, named as MMMB. Our\nmethod not only demonstrates state-of-the-art performance on multilingual\nMMBench and MMMB, but also excels across a broad range of multimodal tasks.\nBoth the source code and the training dataset of Parrot will be made publicly\navailable. Code is available at: https://github.com/AIDC-AI/Parrot.", "published": "2024-06-04 17:56:28", "link": "http://arxiv.org/abs/2406.02539v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "To Believe or Not to Believe Your LLM", "abstract": "We explore uncertainty quantification in large language models (LLMs), with\nthe goal to identify when uncertainty in responses given a query is large. We\nsimultaneously consider both epistemic and aleatoric uncertainties, where the\nformer comes from the lack of knowledge about the ground truth (such as about\nfacts or the language), and the latter comes from irreducible randomness (such\nas multiple possible answers). In particular, we derive an\ninformation-theoretic metric that allows to reliably detect when only epistemic\nuncertainty is large, in which case the output of the model is unreliable. This\ncondition can be computed based solely on the output of the model obtained\nsimply by some special iterative prompting based on the previous responses.\nSuch quantification, for instance, allows to detect hallucinations (cases when\nepistemic uncertainty is high) in both single- and multi-answer responses. This\nis in contrast to many standard uncertainty quantification strategies (such as\nthresholding the log-likelihood of a response) where hallucinations in the\nmulti-answer case cannot be detected. We conduct a series of experiments which\ndemonstrate the advantage of our formulation. Further, our investigations shed\nsome light on how the probabilities assigned to a given output by an LLM can be\namplified by iterative prompting, which might be of independent interest.", "published": "2024-06-04 17:58:18", "link": "http://arxiv.org/abs/2406.02543v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Block Transformer: Global-to-Local Language Modeling for Fast Inference", "abstract": "We introduce the Block Transformer which adopts hierarchical global-to-local\nmodeling to autoregressive transformers to mitigate the inference bottlenecks\nassociated with self-attention. Self-attention requires the key-value (KV)\ncache of all previous sequences to be retrieved from memory at every decoding\nstep to retrieve context information, leading to two primary bottlenecks during\nbatch inference. First, there is a significant delay in obtaining the first\ntoken, as the information of the entire prompt must first be processed to\nprefill the KV cache. Second, computation of subsequent tokens is bottlenecked\nby the high memory I/O demand of fetching the entire KV cache, which grows\nlinearly with sequence length, incurring quadratic memory reads overall. We\ndesign the Block Transformer to strategically mitigate these costs, by\nincorporating coarsity and locality into an integrated global-to-local\narchitecture. At the lower layers, we aggregate tokens into fixed size blocks\nto apply attention across the entire sequence at coarse-grained detail, to\ncapture the global context while minimizing KV cache overhead. At upper layers,\nwe apply attention within each block to decode individual tokens, to model\nfine-grained details with a lightweight local KV cache. We pretrain vanilla and\nBlock Transformers from scratch and demonstrate that Block Transformers reach\n10--20x inference throughput compared to vanilla transformers with equivalent\nperplexity and zero-shot task performance. Code is available at\nhttps://github.com/itsnamgyu/block-transformer.", "published": "2024-06-04 17:45:26", "link": "http://arxiv.org/abs/2406.02657v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Textless Acoustic Model with Self-Supervised Distillation for\n  Noise-Robust Expressive Speech-to-Speech Translation", "abstract": "In this paper, we propose a textless acoustic model with a self-supervised\ndistillation strategy for noise-robust expressive speech-to-speech translation\n(S2ST). Recently proposed expressive S2ST systems have achieved impressive\nexpressivity preservation performances by cascading unit-to-speech (U2S)\ngenerator to the speech-to-unit translation model. However, these systems are\nvulnerable to the presence of noise in input speech, which is an assumption in\nreal-world translation scenarios. To address this limitation, we propose a U2S\ngenerator that incorporates a distillation with no label (DINO) self-supervised\ntraining strategy into it's pretraining process. Because the proposed method\ncaptures noise-agnostic expressivity representation, it can generate qualified\nspeech even in noisy environment. Objective and subjective evaluation results\nverified that the proposed method significantly improved the performance of the\nexpressive S2ST system in noisy environments while maintaining competitive\nperformance in clean environments.", "published": "2024-06-04 19:22:13", "link": "http://arxiv.org/abs/2406.02733v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Aligning Large Language Models via Fine-grained Supervision", "abstract": "Pre-trained large-scale language models (LLMs) excel at producing coherent\narticles, yet their outputs may be untruthful, toxic, or fail to align with\nuser expectations. Current approaches focus on using reinforcement learning\nwith human feedback (RLHF) to improve model alignment, which works by\ntransforming coarse human preferences of LLM outputs into a feedback signal\nthat guides the model learning process. However, because this approach operates\non sequence-level feedback, it lacks the precision to identify the exact parts\nof the output affecting user preferences. To address this gap, we propose a\nmethod to enhance LLM alignment through fine-grained token-level supervision.\nSpecifically, we ask annotators to minimally edit less preferred responses\nwithin the standard reward modeling dataset to make them more favorable,\nensuring changes are made only where necessary while retaining most of the\noriginal content. The refined dataset is used to train a token-level reward\nmodel, which is then used for training our fine-grained Proximal Policy\nOptimization (PPO) model. Our experiment results demonstrate that this approach\ncan achieve up to an absolute improvement of $5.1\\%$ in LLM performance, in\nterms of win rate against the reference model, compared with the traditional\nPPO model.", "published": "2024-06-04 20:21:45", "link": "http://arxiv.org/abs/2406.02756v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Disentangling Logic: The Role of Context in Large Language Model\n  Reasoning Capabilities", "abstract": "This study intends to systematically disentangle pure logic reasoning and\ntext understanding by investigating the contrast across abstract and\ncontextualized logical problems from a comprehensive set of domains. We explore\nwhether LLMs demonstrate genuine reasoning capabilities across various domains\nwhen the underlying logical structure remains constant. We focus on two main\nquestions (1) Can abstract logical problems alone accurately benchmark an LLM's\nreasoning ability in real-world scenarios, disentangled from contextual support\nin practical settings? (2) Does fine-tuning LLMs on abstract logic problem\ngeneralize to contextualized logic problems and vice versa? To investigate\nthese questions, we focus on standard propositional logic, specifically\npropositional deductive and abductive logic reasoning. In particular, we\nconstruct instantiated datasets for deductive and abductive reasoning with 4\nlevels of difficulty, encompassing 12 distinct categories or domains based on\nthe categorization of Wikipedia. Our experiments aim to provide insights into\ndisentangling context in logical reasoning and the true reasoning capabilities\nof LLMs and their generalization potential. The code and dataset are available\nat: https://github.com/agiresearch/ContextHub.", "published": "2024-06-04 21:25:06", "link": "http://arxiv.org/abs/2406.02787v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Language Models can Infer Action Semantics for Symbolic Planners from\n  Environment Feedback", "abstract": "Symbolic planners can discover a sequence of actions from initial to goal\nstates given expert-defined, domain-specific logical action semantics. Large\nLanguage Models (LLMs) can directly generate such sequences, but limitations in\nreasoning and state-tracking often result in plans that are insufficient or\nunexecutable. We propose Predicting Semantics of Actions with Language Models\n(PSALM), which automatically learns action semantics by leveraging the\nstrengths of both symbolic planners and LLMs. PSALM repeatedly proposes and\nexecutes plans, using the LLM to partially generate plans and to infer\ndomain-specific action semantics based on execution outcomes. PSALM maintains a\nbelief over possible action semantics that is iteratively updated until a goal\nstate is reached. Experiments on 7 environments show that when learning just\nfrom one goal, PSALM boosts plan success rate from 36.4% (on Claude-3.5) to\n100%, and explores the environment more efficiently than prior work to infer\nground truth domain action semantics.", "published": "2024-06-04 21:29:56", "link": "http://arxiv.org/abs/2406.02791v2", "categories": ["cs.AI", "cs.CL", "cs.RO"], "primary_category": "cs.AI"}
{"title": "Promotional Language and the Adoption of Innovative Ideas in Science", "abstract": "How are the merits of innovative ideas communicated in science? Here we\nconduct semantic analyses of grant application success with a focus on\nscientific promotional language, which has been growing in frequency in many\ncontexts and purportedly may convey an innovative idea's originality and\nsignificance. Our analysis attempts to surmount limitations of prior studies by\nexamining the full text of tens of thousands of both funded and unfunded grants\nfrom three leading public and private funding agencies: the NIH, the NSF, and\nthe Novo Nordisk Foundation, one of the world's largest private science\nfoundations. We find a robust association between promotional language and the\nsupport and adoption of innovative ideas by funders and other scientists.\nFirst, the percentage of promotional language in a grant proposal is associated\nwith up to a doubling of the grant's probability of being funded. Second, a\ngrant's promotional language reflects its intrinsic level of innovativeness.\nThird, the percentage of promotional language predicts the expected citation\nand productivity impact of publications that are supported by funded grants.\nLastly, a computer-assisted experiment that manipulates the promotional\nlanguage in our data demonstrates how promotional language can communicate the\nmerit of ideas through cognitive activation. With the incidence of promotional\nlanguage in science steeply rising, and the pivotal role of grants in\nconverting promising and aspirational ideas into solutions, our analysis\nprovides empirical evidence that promotional language is associated with\neffectively communicating the merits of innovative scientific ideas.", "published": "2024-06-04 21:54:36", "link": "http://arxiv.org/abs/2406.02798v2", "categories": ["cs.DL", "cs.CL", "cs.CY"], "primary_category": "cs.DL"}
{"title": "ACCORD: Closing the Commonsense Measurability Gap", "abstract": "We present ACCORD, a framework and benchmark suite for disentangling the\ncommonsense grounding and reasoning abilities of large language models (LLMs)\nthrough controlled, multi-hop counterfactuals. ACCORD introduces formal\nelements to commonsense reasoning to explicitly control and quantify reasoning\ncomplexity beyond the typical 1 or 2 hops. Uniquely, ACCORD can automatically\ngenerate benchmarks of arbitrary reasoning complexity, and so it scales with\nfuture LLM improvements. Benchmarking state-of-the-art LLMs -- including GPT-4o\n(2024-05-13), Llama-3-70B-Instruct, and Mixtral-8x22B-Instruct-v0.1 -- shows\nperformance degrading to random chance with only moderate scaling, leaving\nsubstantial headroom for improvement. We release a leaderboard of the benchmark\nsuite tested in this work, as well as code for automatically generating more\ncomplex benchmarks.", "published": "2024-06-04 22:08:24", "link": "http://arxiv.org/abs/2406.02804v2", "categories": ["cs.AI", "cs.CL", "cs.LG", "I.2.0; I.2.7"], "primary_category": "cs.AI"}
{"title": "OccamLLM: Fast and Exact Language Model Arithmetic in a Single Step", "abstract": "Despite significant advancements in text generation and reasoning, Large\nLanguage Models (LLMs) still face challenges in accurately performing complex\narithmetic operations. Language model systems often enable LLMs to generate\ncode for arithmetic operations to achieve accurate calculations. However, this\napproach compromises speed and security, and fine-tuning risks the language\nmodel losing prior capabilities. We propose a framework that enables exact\narithmetic in a single autoregressive step, providing faster, more secure, and\nmore interpretable LLM systems with arithmetic capabilities. We use the hidden\nstates of a LLM to control a symbolic architecture that performs arithmetic.\nOur implementation using Llama 3 with OccamNet as a symbolic model (OccamLlama)\nachieves 100\\% accuracy on single arithmetic operations\n($+,-,\\times,\\div,\\sin{},\\cos{},\\log{},\\exp{},\\sqrt{}$), outperforming GPT 4o\nwith and without a code interpreter. Furthermore, OccamLlama outperforms GPT 4o\nwith and without a code interpreter on average across a range of mathematical\nproblem solving benchmarks, demonstrating that OccamLLMs can excel in\narithmetic tasks, even surpassing much larger models. We will make our code\npublic shortly.", "published": "2024-06-04 04:17:40", "link": "http://arxiv.org/abs/2406.06576v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SMS Spam Detection and Classification to Combat Abuse in Telephone\n  Networks Using Natural Language Processing", "abstract": "In the modern era, mobile phones have become ubiquitous, and Short Message\nService (SMS) has grown to become a multi-million-dollar service due to the\nwidespread adoption of mobile devices and the millions of people who use SMS\ndaily. However, SMS spam has also become a pervasive problem that endangers\nusers' privacy and security through phishing and fraud. Despite numerous spam\nfiltering techniques, there is still a need for a more effective solution to\naddress this problem [1]. This research addresses the pervasive issue of SMS\nspam, which poses threats to users' privacy and security. Despite existing spam\nfiltering techniques, the high false-positive rate persists as a challenge. The\nstudy introduces a novel approach utilizing Natural Language Processing (NLP)\nand machine learning models, particularly BERT (Bidirectional Encoder\nRepresentations from Transformers), for SMS spam detection and classification.\nData preprocessing techniques, such as stop word removal and tokenization, are\napplied, along with feature extraction using BERT. Machine learning models,\nincluding SVM, Logistic Regression, Naive Bayes, Gradient Boosting, and Random\nForest, are integrated with BERT for differentiating spam from ham messages.\nEvaluation results revealed that the Na\\\"ive Bayes classifier + BERT model\nachieves the highest accuracy at 97.31% with the fastest execution time of 0.3\nseconds on the test dataset. This approach demonstrates a notable enhancement\nin spam detection efficiency and a low false-positive rate. The developed model\npresents a valuable solution to combat SMS spam, ensuring faster and more\naccurate detection. This model not only safeguards users' privacy but also\nassists network providers in effectively identifying and blocking SMS spam\nmessages.", "published": "2024-06-04 13:44:36", "link": "http://arxiv.org/abs/2406.06578v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "From Redundancy to Relevance: Information Flow in LVLMs Across Reasoning\n  Tasks", "abstract": "Large Vision Language Models (LVLMs) achieve great performance on\nvisual-language reasoning tasks, however, the black-box nature of LVLMs hinders\nin-depth research on the reasoning mechanism. As all images need to be\nconverted into image tokens to fit the input format of large language models\n(LLMs) along with natural language prompts, sequential visual representation is\nessential to the performance of LVLMs, and the information flow analysis\napproach can be an effective tool for determining interactions between these\nrepresentations. In this paper, we propose integrating attention analysis with\nLLaVA-CAM, concretely, attention scores highlight relevant regions during\nforward propagation, while LLaVA-CAM captures gradient changes through backward\npropagation, revealing key image features. By exploring the information flow\nfrom the perspective of visual representation contribution, we observe that it\ntends to converge in shallow layers but diversify in deeper layers. To validate\nour analysis, we conduct comprehensive experiments with truncation strategies\nacross various LVLMs for visual question answering and image captioning tasks,\nand experimental results not only verify our hypothesis but also reveal a\nconsistent pattern of information flow convergence in the corresponding layers,\nand the information flow cliff layer will be different due to different\ncontexts. The paper's source code can be accessed from\n\\url{https://github.com/zhangbaijin/From-Redundancy-to-Relevance}", "published": "2024-06-04 13:52:54", "link": "http://arxiv.org/abs/2406.06579v3", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Order-Independence Without Fine Tuning", "abstract": "The development of generative language models that can create long and\ncoherent textual outputs via autoregression has lead to a proliferation of uses\nand a corresponding sweep of analyses as researches work to determine the\nlimitations of this new paradigm. Unlike humans, these 'Large Language Models'\n(LLMs) are highly sensitive to small changes in their inputs, leading to\nunwanted inconsistency in their behavior. One problematic inconsistency when\nLLMs are used to answer multiple-choice questions or analyze multiple inputs is\norder dependency: the output of an LLM can (and often does) change\nsignificantly when sub-sequences are swapped, despite both orderings being\nsemantically identical. In this paper we present Set-Based Prompting, a\ntechnique that guarantees the output of an LLM will not have order dependence\non a specified set of sub-sequences. We show that this method provably\neliminates order dependency, and that it can be applied to any\ntransformer-based LLM to enable text generation that is unaffected by\nre-orderings. Delving into the implications of our method, we show that,\ndespite our inputs being out of distribution, the impact on expected accuracy\nis small, where the expectation is over the order of uniformly chosen shuffling\nof the candidate responses, and usually significantly less in practice. Thus,\nSet-Based Prompting can be used as a 'dropped-in' method on fully trained\nmodels. Finally, we discuss how our method's success suggests that other strong\nguarantees can be obtained on LLM performance via modifying the input\nrepresentations.", "published": "2024-06-04 16:09:13", "link": "http://arxiv.org/abs/2406.06581v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Discrete Multimodal Transformers with a Pretrained Large Language Model\n  for Mixed-Supervision Speech Processing", "abstract": "Recent work on discrete speech tokenization has paved the way for models that\ncan seamlessly perform multiple tasks across modalities, e.g., speech\nrecognition, text to speech, speech to speech translation. Moreover, large\nlanguage models (LLMs) pretrained from vast text corpora contain rich\nlinguistic information that can improve accuracy in a variety of tasks. In this\npaper, we present a decoder-only Discrete Multimodal Language Model (DMLM),\nwhich can be flexibly applied to multiple tasks (ASR, T2S, S2TT, etc.) and\nmodalities (text, speech, vision). We explore several critical aspects of\ndiscrete multi-modal models, including the loss function, weight\ninitialization, mixed training supervision, and codebook. Our results show that\nDMLM benefits significantly, across multiple tasks and datasets, from a\ncombination of supervised and unsupervised training. Moreover, for ASR, it\nbenefits from initializing DMLM from a pretrained LLM, and from a codebook\nderived from Whisper activations.", "published": "2024-06-04 20:08:25", "link": "http://arxiv.org/abs/2406.06582v2", "categories": ["cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Mimetic Poet", "abstract": "This paper presents the design and initial assessment of a novel device that\nuses generative AI to facilitate creative ideation, inspiration, and reflective\nthought. Inspired by magnetic poetry, which was originally designed to help\novercome writer's block, the device allows participants to compose short poetic\ntexts from a limited vocabulary by physically placing words on the device's\nsurface. Upon composing the text, the system employs a large language model\n(LLM) to generate a response, displayed on an e-ink screen. We explored various\nstrategies for internally sequencing prompts to foster creative thinking,\nincluding analogy, allegorical interpretations, and ideation. We installed the\ndevice in our research laboratory for two weeks and held a focus group at the\nconclusion to evaluate the design. The design choice to limit interactions with\nthe LLM to poetic text, coupled with the tactile experience of assembling the\npoem, fostered a deeper and more enjoyable engagement with the LLM compared to\ntraditional chatbot or screen-based interactions. This approach gives users the\nopportunity to reflect on the AI-generated responses in a manner conducive to\ncreative thought.", "published": "2024-06-04 02:50:15", "link": "http://arxiv.org/abs/2407.11984v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "I.2; J.5"], "primary_category": "cs.HC"}
{"title": "GRAM: Generative Retrieval Augmented Matching of Data Schemas in the\n  Context of Data Security", "abstract": "Schema matching constitutes a pivotal phase in the data ingestion process for\ncontemporary database systems. Its objective is to discern pairwise\nsimilarities between two sets of attributes, each associated with a distinct\ndata table. This challenge emerges at the initial stages of data analytics,\nsuch as when incorporating a third-party table into existing databases to\ninform business insights. Given its significance in the realm of database\nsystems, schema matching has been under investigation since the 2000s. This\nstudy revisits this foundational problem within the context of large language\nmodels. Adhering to increasingly stringent data security policies, our focus\nlies on the zero-shot and few-shot scenarios: the model should analyze only a\nminimal amount of customer data to execute the matching task, contrasting with\nthe conventional approach of scrutinizing the entire data table. We emphasize\nthat the zero-shot or few-shot assumption is imperative to safeguard the\nidentity and privacy of customer data, even at the potential cost of accuracy.\nThe capability to accurately match attributes under such stringent requirements\ndistinguishes our work from previous literature in this domain.", "published": "2024-06-04 01:08:00", "link": "http://arxiv.org/abs/2406.01876v1", "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.DB"}
{"title": "Towards Out-of-Distribution Detection in Vocoder Recognition via Latent\n  Feature Reconstruction", "abstract": "Advancements in synthesized speech have created a growing threat of\nimpersonation, making it crucial to develop deepfake algorithm recognition. One\nsignificant aspect is out-of-distribution (OOD) detection, which has gained\nnotable attention due to its important role in deepfake algorithm recognition.\nHowever, most of the current approaches for detecting OOD in deepfake algorithm\nrecognition rely on probability-score or classified-distance, which may lead to\nlimitations in the accuracy of the sample at the edge of the threshold. In this\nstudy, we propose a reconstruction-based detection approach that employs an\nautoencoder architecture to compress and reconstruct the acoustic feature\nextracted from a pre-trained WavLM model. Each acoustic feature belonging to a\nspecific vocoder class is only aptly reconstructed by its corresponding\ndecoder. When none of the decoders can satisfactorily reconstruct a feature, it\nis classified as an OOD sample. To enhance the distinctiveness of the\nreconstructed features by each decoder, we incorporate contrastive learning and\nan auxiliary classifier to further constrain the reconstructed feature.\nExperiments demonstrate that our proposed approach surpasses baseline systems\nby a relative margin of 10\\% in the evaluation dataset. Ablation studies\nfurther validate the effectiveness of both the contrastive constraint and the\nauxiliary classifier within our proposed approach.", "published": "2024-06-04 11:55:11", "link": "http://arxiv.org/abs/2406.02233v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "BiVocoder: A Bidirectional Neural Vocoder Integrating Feature Extraction\n  and Waveform Generation", "abstract": "This paper proposes a novel bidirectional neural vocoder, named BiVocoder,\ncapable both of feature extraction and reverse waveform generation within the\nshort-time Fourier transform (STFT) domain. For feature extraction, the\nBiVocoder takes amplitude and phase spectra derived from STFT as inputs,\ntransforms them into long-frame-shift and low-dimensional features through\nconvolutional neural networks. The extracted features are demonstrated suitable\nfor direct prediction by acoustic models, supporting its application in\ntext-to-speech (TTS) task. For waveform generation, the BiVocoder restores\namplitude and phase spectra from the features by a symmetric network, followed\nby inverse STFT to reconstruct the speech waveform. Experimental results show\nthat our proposed BiVocoder achieves better performance compared to some\nbaseline vocoders, by comprehensively considering both synthesized speech\nquality and inference speed for both analysis-synthesis and TTS tasks.", "published": "2024-06-04 09:51:02", "link": "http://arxiv.org/abs/2406.02162v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "ERes2NetV2: Boosting Short-Duration Speaker Verification Performance\n  with Computational Efficiency", "abstract": "Speaker verification systems experience significant performance degradation\nwhen tasked with short-duration trial recordings. To address this challenge, a\nmulti-scale feature fusion approach has been proposed to effectively capture\nspeaker characteristics from short utterances. Constrained by the model's size,\na robust backbone Enhanced Res2Net (ERes2Net) combining global and local\nfeature fusion demonstrates sub-optimal performance in short-duration speaker\nverification. To further improve the short-duration feature extraction\ncapability of ERes2Net, we expand the channel dimension within each stage.\nHowever, this modification also increases the number of model parameters and\ncomputational complexity. To alleviate this problem, we propose an improved\nERes2NetV2 by pruning redundant structures, ultimately reducing both the model\nparameters and its computational cost. A range of experiments conducted on the\nVoxCeleb datasets exhibits the superiority of ERes2NetV2, which achieves EER of\n0.61% for the full-duration trial, 0.98% for the 3s-duration trial, and 1.48%\nfor the 2s-duration trial on VoxCeleb1-O, respectively.", "published": "2024-06-04 09:56:39", "link": "http://arxiv.org/abs/2406.02167v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Multi-Stage Speech Bandwidth Extension with Flexible Sampling Rate\n  Control", "abstract": "The majority of existing speech bandwidth extension (BWE) methods operate\nunder the constraint of fixed source and target sampling rates, which limits\ntheir flexibility in practical applications. In this paper, we propose a\nmulti-stage speech BWE model named MS-BWE, which can handle a set of source and\ntarget sampling rate pairs and achieve flexible extensions of frequency\nbandwidth. The proposed MS-BWE model comprises a cascade of BWE blocks, with\neach block featuring a dual-stream architecture to realize amplitude and phase\nextension, progressively painting the speech frequency bands stage by stage.\nThe teacher-forcing strategy is employed to mitigate the discrepancy between\ntraining and inference. Experimental results demonstrate that our proposed\nMS-BWE is comparable to state-of-the-art speech BWE methods in speech quality.\nRegarding generation efficiency, the one-stage generation of MS-BWE can achieve\nover one thousand times real-time on GPU and about sixty times on CPU.", "published": "2024-06-04 12:17:11", "link": "http://arxiv.org/abs/2406.02250v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SimpleSpeech: Towards Simple and Efficient Text-to-Speech with Scalar\n  Latent Transformer Diffusion Models", "abstract": "In this study, we propose a simple and efficient Non-Autoregressive (NAR)\ntext-to-speech (TTS) system based on diffusion, named SimpleSpeech. Its\nsimpleness shows in three aspects: (1) It can be trained on the speech-only\ndataset, without any alignment information; (2) It directly takes plain text as\ninput and generates speech through an NAR way; (3) It tries to model speech in\na finite and compact latent space, which alleviates the modeling difficulty of\ndiffusion. More specifically, we propose a novel speech codec model (SQ-Codec)\nwith scalar quantization, SQ-Codec effectively maps the complex speech signal\ninto a finite and compact latent space, named scalar latent space. Benefits\nfrom SQ-Codec, we apply a novel transformer diffusion model in the scalar\nlatent space of SQ-Codec. We train SimpleSpeech on 4k hours of a speech-only\ndataset, it shows natural prosody and voice cloning ability. Compared with\nprevious large-scale TTS models, it presents significant speech quality and\ngeneration speed improvement. Demos are released.", "published": "2024-06-04 13:58:28", "link": "http://arxiv.org/abs/2406.02328v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Self-Supervised Singing Voice Pre-Training towards Speech-to-Singing\n  Conversion", "abstract": "Speech-to-singing voice conversion (STS) task always suffers from data\nscarcity, because it requires paired speech and singing data. Compounding this\nissue are the challenges of content-pitch alignment and the suboptimal quality\nof generated outputs, presenting significant hurdles in STS research. This\npaper presents SVPT, an STS approach boosted by a self-supervised singing voice\npre-training model. We leverage spoken language model techniques to tackle the\nrhythm alignment problem and the in-context learning capability to achieve\nzero-shot conversion. We adopt discrete-unit random resampling and pitch\ncorruption strategies, enabling training with unpaired singing data and thus\nmitigating the issue of data scarcity. SVPT also serves as an effective\nbackbone for singing voice synthesis (SVS), offering insights into scaling up\nSVS models. Experimental results indicate that SVPT delivers notable\nimprovements in both STS and SVS endeavors. Audio samples are available at\nhttps://speech2sing.github.io.", "published": "2024-06-04 15:47:59", "link": "http://arxiv.org/abs/2406.02429v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Seed-TTS: A Family of High-Quality Versatile Speech Generation Models", "abstract": "We introduce Seed-TTS, a family of large-scale autoregressive text-to-speech\n(TTS) models capable of generating speech that is virtually indistinguishable\nfrom human speech. Seed-TTS serves as a foundation model for speech generation\nand excels in speech in-context learning, achieving performance in speaker\nsimilarity and naturalness that matches ground truth human speech in both\nobjective and subjective evaluations. With fine-tuning, we achieve even higher\nsubjective scores across these metrics. Seed-TTS offers superior\ncontrollability over various speech attributes such as emotion and is capable\nof generating highly expressive and diverse speech for speakers in the wild.\nFurthermore, we propose a self-distillation method for speech factorization, as\nwell as a reinforcement learning approach to enhance model robustness, speaker\nsimilarity, and controllability. We additionally present a non-autoregressive\n(NAR) variant of the Seed-TTS model, named $\\text{Seed-TTS}_\\text{DiT}$, which\nutilizes a fully diffusion-based architecture. Unlike previous NAR-based TTS\nsystems, $\\text{Seed-TTS}_\\text{DiT}$ does not depend on pre-estimated phoneme\ndurations and performs speech generation through end-to-end processing. We\ndemonstrate that this variant achieves comparable performance to the language\nmodel-based variant and showcase its effectiveness in speech editing. We\nencourage readers to listen to demos at\n\\url{https://bytedancespeech.github.io/seedtts_tech_report}.", "published": "2024-06-04 15:48:29", "link": "http://arxiv.org/abs/2406.02430v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Explainable Deep Learning Analysis for Raga Identification in Indian Art\n  Music", "abstract": "Raga identification is an important problem within the domain of Indian Art\nmusic, as Ragas are fundamental to its composition and performance, playing a\ncrucial role in music retrieval, preservation, and education. Few studies that\nhave explored this task employ approaches such as signal processing, Machine\nLearning (ML), and more recently, Deep Learning (DL) based methods. However, a\nkey question remains unanswered in all these works: do these ML/DL methods\nlearn and interpret Ragas in a manner similar to human experts? Besides, a\nsignificant roadblock in this research is the unavailability of an ample supply\nof rich, labeled datasets, which drives these ML/DL-based methods. In this\npaper, firstly we curate a dataset comprising 191 hours of Hindustani Classical\nMusic (HCM) recordings, annotate it for Raga and tonic labels, and train a\nCNN-LSTM model for the task of Automatic Raga Identification (ARI). We achieve\na chunk-wise f1-measure of 0.89 for a subset of 12 Raga classes. Following\nthis, we make one of the first attempts to employ model explainability\ntechniques: SoundLIME and GradCAM++ for Raga identification, to evaluate\nwhether the classifier's predictions align with human understanding of Ragas.\nWe compare the generated explanations with human expert annotations and further\nanalyze individual test examples to understand the role of regions highlighted\nby explanations in making correct or incorrect predictions made by the model.\nOur results demonstrate a significant alignment of the model's understanding\nwith human understanding, and the thorough analysis validates the effectiveness\nof our approach.", "published": "2024-06-04 16:06:51", "link": "http://arxiv.org/abs/2406.02443v2", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "Understanding Auditory Evoked Brain Signal via Physics-informed\n  Embedding Network with Multi-Task Transformer", "abstract": "In the fields of brain-computer interaction and cognitive neuroscience,\neffective decoding of auditory signals from task-based functional magnetic\nresonance imaging (fMRI) is key to understanding how the brain processes\ncomplex auditory information. Although existing methods have enhanced decoding\ncapabilities, limitations remain in information utilization and model\nrepresentation. To overcome these challenges, we propose an innovative\nmulti-task learning model, Physics-informed Embedding Network with Multi-Task\nTransformer (PEMT-Net), which enhances decoding performance through\nphysics-informed embedding and deep learning techniques. PEMT-Net consists of\ntwo principal components: feature augmentation and classification. For feature\naugmentation, we propose a novel approach by creating neural embedding graphs\nvia node embedding, utilizing random walks to simulate the physical diffusion\nof neural information. This method captures both local and non-local\ninformation overflow and proposes a position encoding based on relative\nphysical coordinates. In the classification segment, we propose adaptive\nembedding fusion to maximally capture linear and non-linear characteristics.\nFurthermore, we propose an innovative parameter-sharing mechanism to optimize\nthe retention and learning of extracted features. Experiments on a specific\ndataset demonstrate PEMT-Net's significant performance in multi-task auditory\nsignal decoding, surpassing existing methods and offering new insights into the\nbrain's mechanisms for processing complex auditory information.", "published": "2024-06-04 06:53:32", "link": "http://arxiv.org/abs/2406.02014v1", "categories": ["q-bio.NC", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "q-bio.NC"}
{"title": "M2D-CLAP: Masked Modeling Duo Meets CLAP for Learning General-purpose\n  Audio-Language Representation", "abstract": "Contrastive language-audio pre-training (CLAP) enables zero-shot (ZS)\ninference of audio and exhibits promising performance in several classification\ntasks. However, conventional audio representations are still crucial for many\ntasks where ZS is not applicable (e.g., regression problems). Here, we explore\na new representation, a general-purpose audio-language representation, that\nperforms well in both ZS and transfer learning. To do so, we propose a new\nmethod, M2D-CLAP, which combines self-supervised learning Masked Modeling Duo\n(M2D) and CLAP. M2D learns an effective representation to model audio signals,\nand CLAP aligns the representation with text embedding. As a result, M2D-CLAP\nlearns a versatile representation that allows for both ZS and transfer\nlearning. Experiments show that M2D-CLAP performs well on linear evaluation,\nfine-tuning, and ZS classification with a GTZAN state-of-the-art of 75.17%,\nthus achieving a general-purpose audio-language representation.", "published": "2024-06-04 07:17:44", "link": "http://arxiv.org/abs/2406.02032v1", "categories": ["eess.AS", "cs.MM", "cs.SD", "68T07"], "primary_category": "eess.AS"}
{"title": "Audio Mamba: Selective State Spaces for Self-Supervised Audio\n  Representations", "abstract": "Despite its widespread adoption as the prominent neural architecture, the\nTransformer has spurred several independent lines of work to address its\nlimitations. One such approach is selective state space models, which have\ndemonstrated promising results for language modelling. However, their\nfeasibility for learning self-supervised, general-purpose audio representations\nis yet to be investigated. This work proposes Audio Mamba, a selective state\nspace model for learning general-purpose audio representations from randomly\nmasked spectrogram patches through self-supervision. Empirical results on ten\ndiverse audio recognition downstream tasks show that the proposed models,\npretrained on the AudioSet dataset, consistently outperform comparable\nself-supervised audio spectrogram transformer (SSAST) baselines by a\nconsiderable margin and demonstrate better performance in dataset size,\nsequence length and model size comparisons.", "published": "2024-06-04 10:19:14", "link": "http://arxiv.org/abs/2406.02178v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MidiCaps: A large-scale MIDI dataset with text captions", "abstract": "Generative models guided by text prompts are increasingly becoming more\npopular. However, no text-to-MIDI models currently exist due to the lack of a\ncaptioned MIDI dataset. This work aims to enable research that combines LLMs\nwith symbolic music by presenting, the first openly available large-scale MIDI\ndataset with text captions. MIDI (Musical Instrument Digital Interface) files\nare widely used for encoding musical information and can capture the nuances of\nmusical composition. They are widely used by music producers, composers,\nmusicologists, and performers alike. Inspired by recent advancements in\ncaptioning techniques, we present a curated dataset of over 168k MIDI files\nwith textual descriptions. Each MIDI caption describes the musical content,\nincluding tempo, chord progression, time signature, instruments, genre, and\nmood, thus facilitating multi-modal exploration and analysis. The dataset\nencompasses various genres, styles, and complexities, offering a rich data\nsource for training and evaluating models for tasks such as music information\nretrieval, music understanding, and cross-modal translation. We provide\ndetailed statistics about the dataset and have assessed the quality of the\ncaptions in an extensive listening study. We anticipate that this resource will\nstimulate further research at the intersection of music and natural language\nprocessing, fostering advancements in both fields.", "published": "2024-06-04 12:21:55", "link": "http://arxiv.org/abs/2406.02255v2", "categories": ["eess.AS", "cs.LG", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Towards Supervised Performance on Speaker Verification with\n  Self-Supervised Learning by Leveraging Large-Scale ASR Models", "abstract": "Recent advancements in Self-Supervised Learning (SSL) have shown promising\nresults in Speaker Verification (SV). However, narrowing the performance gap\nwith supervised systems remains an ongoing challenge. Several studies have\nobserved that speech representations from large-scale ASR models contain\nvaluable speaker information. This work explores the limitations of fine-tuning\nthese models for SV using an SSL contrastive objective in an end-to-end\napproach. Then, we propose a framework to learn speaker representations in an\nSSL context by fine-tuning a pre-trained WavLM with a supervised loss using\npseudo-labels. Initial pseudo-labels are derived from an SSL DINO-based model\nand are iteratively refined by clustering the model embeddings. Our method\nachieves 0.99% EER on VoxCeleb1-O, establishing the new state-of-the-art on\nself-supervised SV. As this performance is close to our supervised baseline of\n0.94% EER, this contribution is a step towards supervised performance on SV\nwith SSL.", "published": "2024-06-04 12:58:19", "link": "http://arxiv.org/abs/2406.02285v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "An Independence-promoting Loss for Music Generation with Language Models", "abstract": "Music generation schemes using language modeling rely on a vocabulary of\naudio tokens, generally provided as codes in a discrete latent space learnt by\nan auto-encoder. Multi-stage quantizers are often employed to produce these\ntokens, therefore the decoding strategy used for token prediction must be\nadapted to account for multiple codebooks: either it should model the joint\ndistribution over all codebooks, or fit the product of the codebook marginal\ndistributions. Modelling the joint distribution requires a costly increase in\nthe number of auto-regressive steps, while fitting the product of the marginals\nyields an inexact model unless the codebooks are mutually independent. In this\nwork, we introduce an independence-promoting loss to regularize the\nauto-encoder used as the tokenizer in language models for music generation. The\nproposed loss is a proxy for mutual information based on the maximum mean\ndiscrepancy principle, applied in reproducible kernel Hilbert spaces. Our\ncriterion is simple to implement and train, and it is generalizable to other\nmulti-stream codecs. We show that it reduces the statistical dependence between\ncodebooks during auto-encoding. This leads to an increase in the generated\nmusic quality when modelling the product of the marginal distributions, while\ngenerating audio much faster than the joint distribution model.", "published": "2024-06-04 13:44:39", "link": "http://arxiv.org/abs/2406.02315v2", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "CtrSVDD: A Benchmark Dataset and Baseline Analysis for Controlled\n  Singing Voice Deepfake Detection", "abstract": "Recent singing voice synthesis and conversion advancements necessitate robust\nsinging voice deepfake detection (SVDD) models. Current SVDD datasets face\nchallenges due to limited controllability, diversity in deepfake methods, and\nlicensing restrictions. Addressing these gaps, we introduce CtrSVDD, a\nlarge-scale, diverse collection of bonafide and deepfake singing vocals. These\nvocals are synthesized using state-of-the-art methods from publicly accessible\nsinging voice datasets. CtrSVDD includes 47.64 hours of bonafide and 260.34\nhours of deepfake singing vocals, spanning 14 deepfake methods and involving\n164 singer identities. We also present a baseline system with flexible\nfront-end features, evaluated against a structured train/dev/eval split. The\nexperiments show the importance of feature selection and highlight a need for\ngeneralization towards deepfake methods that deviate further from training\ndistribution. The CtrSVDD dataset and baselines are publicly accessible.", "published": "2024-06-04 16:00:18", "link": "http://arxiv.org/abs/2406.02438v2", "categories": ["eess.AS", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
{"title": "How Do Neural Spoofing Countermeasures Detect Partially Spoofed Audio?", "abstract": "Partially manipulating a sentence can greatly change its meaning. Recent work\nshows that countermeasures (CMs) trained on partially spoofed audio can\neffectively detect such spoofing. However, the current understanding of the\ndecision-making process of CMs is limited. We utilize Grad-CAM and introduce a\nquantitative analysis metric to interpret CMs' decisions. We find that CMs\nprioritize the artifacts of transition regions created when concatenating bona\nfide and spoofed audio. This focus differs from that of CMs trained on fully\nspoofed audio, which concentrate on the pattern differences between bona fide\nand spoofed parts. Our further investigation explains the varying nature of\nCMs' focus while making correct or incorrect predictions. These insights\nprovide a basis for the design of CM models and the creation of datasets.\nMoreover, this work lays a foundation of interpretability in the field of\npartial spoofed audio detection that has not been well explored previously.", "published": "2024-06-04 16:51:42", "link": "http://arxiv.org/abs/2406.02483v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Keyword-Guided Adaptation of Automatic Speech Recognition", "abstract": "Automatic Speech Recognition (ASR) technology has made significant progress\nin recent years, providing accurate transcription across various domains.\nHowever, some challenges remain, especially in noisy environments and\nspecialized jargon. In this paper, we propose a novel approach for improved\njargon word recognition by contextual biasing Whisper-based models. We employ a\nkeyword spotting model that leverages the Whisper encoder representation to\ndynamically generate prompts for guiding the decoder during the transcription\nprocess. We introduce two approaches to effectively steer the decoder towards\nthese prompts: KG-Whisper, which is aimed at fine-tuning the Whisper decoder,\nand KG-Whisper-PT, which learns a prompt prefix. Our results show a significant\nimprovement in the recognition accuracy of specified keywords and in reducing\nthe overall word error rates. Specifically, in unseen language generalization,\nwe demonstrate an average WER improvement of 5.1% over Whisper.", "published": "2024-06-04 14:20:38", "link": "http://arxiv.org/abs/2406.02649v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "RepCNN: Micro-sized, Mighty Models for Wakeword Detection", "abstract": "Always-on machine learning models require a very low memory and compute\nfootprint. Their restricted parameter count limits the model's capacity to\nlearn, and the effectiveness of the usual training algorithms to find the best\nparameters. Here we show that a small convolutional model can be better trained\nby first refactoring its computation into a larger redundant multi-branched\narchitecture. Then, for inference, we algebraically re-parameterize the trained\nmodel into the single-branched form with fewer parameters for a lower memory\nfootprint and compute cost. Using this technique, we show that our always-on\nwake-word detector model, RepCNN, provides a good trade-off between latency and\naccuracy during inference. RepCNN re-parameterized models are 43% more accurate\nthan a uni-branch convolutional model while having the same runtime. RepCNN\nalso meets the accuracy of complex architectures like BC-ResNet, while having\n2x lesser peak memory usage and 10x faster runtime.", "published": "2024-06-04 16:14:19", "link": "http://arxiv.org/abs/2406.02652v2", "categories": ["eess.AS", "cs.AI", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Operational Latent Spaces", "abstract": "We investigate the construction of latent spaces through self-supervised\nlearning to support semantically meaningful operations. Analogous to\noperational amplifiers, these \"operational latent spaces\" (OpLaS) not only\ndemonstrate semantic structure such as clustering but also support common\ntransformational operations with inherent semantic meaning. Some operational\nlatent spaces are found to have arisen \"unintentionally\" in the progress toward\nsome (other) self-supervised learning objective, in which unintended but still\nuseful properties are discovered among the relationships of points in the\nspace. Other spaces may be constructed \"intentionally\" by developers\nstipulating certain kinds of clustering or transformations intended to produce\nthe desired structure. We focus on the intentional creation of operational\nlatent spaces via self-supervised learning, including the introduction of\nrotation operators via a novel \"FiLMR\" layer, which can be used to enable\nring-like symmetries found in some musical constructions.", "published": "2024-06-04 18:25:15", "link": "http://arxiv.org/abs/2406.02699v1", "categories": ["cs.LG", "cs.SD", "eess.AS", "I.2.4; J.5"], "primary_category": "cs.LG"}
{"title": "MaskSR: Masked Language Model for Full-band Speech Restoration", "abstract": "Speech restoration aims at restoring high quality speech in the presence of a\ndiverse set of distortions. Although several deep learning paradigms have been\nstudied for this task, the power of the recently emerging language models has\nnot been fully explored. In this paper, we propose MaskSR, a masked language\nmodel capable of restoring full-band 44.1 kHz speech jointly considering noise,\nreverb, clipping, and low bandwidth. MaskSR works with discrete acoustic tokens\nextracted using a pre-trained neural codec. During training, MaskSR is\noptimized to predict randomly masked tokens extracted from the high quality\ntarget speech, conditioned on the corrupted speech with various distortions.\nDuring inference, MaskSR reconstructs the target speech tokens with efficient\niterative sampling. Extensive experiments show that MaskSR obtains competitive\nresults on both the full-band speech restoration task and also on sub-tasks\ncompared with a wide range of models.", "published": "2024-06-04 08:23:57", "link": "http://arxiv.org/abs/2406.02092v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
