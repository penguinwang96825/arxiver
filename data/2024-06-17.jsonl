{"title": "Circular transformation of the European steel industry renders scrap metal a strategic resource", "abstract": "The steel industry is a major contributor to CO2 emissions, accounting for 7%\nof global emissions. The European steel industry is seeking to reduce its\nemissions by increasing the use of electric arc furnaces (EAFs), which can\nproduce steel from scrap, marking a major shift towards a circular steel\neconomy. Here, we show by combining trade with business intelligence data that\nthis shift requires a deep restructuring of the global and European scrap\ntrade, as well as a substantial scaling of the underlying business ecosystem.\nWe find that the scrap imports of European countries with major EAF\ninstallations have steadily decreased since 2007 while globally scrap trade\nstarted to increase recently. Our statistical modelling shows that every 1,000\ntonnes of EAF capacity installed is associated with an increase in annual\nimports of 550 tonnes and a decrease in annual exports of 1,000 tonnes of\nscrap, suggesting increased competition for scrap metal as countries ramp up\ntheir EAF capacity. Furthermore, each scrap company enables an increase of\naround 79,000 tonnes of EAF-based steel production per year in the EU. Taking\nthese relations as causal and extrapolating to the currently planned EAF\ncapacity, we find that an additional 730 (SD 140) companies might be required,\nemploying about 35,000 people (IQR 29,000-50,000) and generating an additional\nestimated turnover of USD 35 billion (IQR 27-48). Our results thus suggest that\nscrap metal is likely to become a strategic resource. They highlight the need\nfor a massive restructuring of the industry's supply networks and identify the\nresulting growth opportunities for companies.", "published": "2024-06-17 21:21:49", "link": "http://arxiv.org/abs/2406.12098v1", "categories": ["q-fin.TR", "physics.soc-ph"], "primary_category": "q-fin.TR"}
{"title": "Exploring Safety-Utility Trade-Offs in Personalized Language Models", "abstract": "As large language models (LLMs) become increasingly integrated into daily\napplications, it is essential to ensure they operate fairly across diverse user\ndemographics. In this work, we show that LLMs suffer from personalization bias,\nwhere their performance is impacted when they are personalized to a user's\nidentity. We quantify personalization bias by evaluating the performance of\nLLMs along two axes - safety and utility. We measure safety by examining how\nbenign LLM responses are to unsafe prompts with and without personalization. We\nmeasure utility by evaluating the LLM's performance on various tasks, including\ngeneral knowledge, mathematical abilities, programming, and reasoning skills.\nWe find that various LLMs, ranging from open-source models like Llama (Touvron\net al., 2023) and Mistral (Jiang et al., 2023) to API-based ones like GPT-3.5\nand GPT-4o (Ouyang et al., 2022), exhibit significant variance in performance\nin terms of safety-utility trade-offs depending on the user's identity.\nFinally, we discuss several strategies to mitigate personalization bias using\npreference tuning and prompt-based defenses.", "published": "2024-06-17 00:17:11", "link": "http://arxiv.org/abs/2406.11107v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text Grafting: Near-Distribution Weak Supervision for Minority Classes\n  in Text Classification", "abstract": "For extremely weak-supervised text classification, pioneer research generates\npseudo labels by mining texts similar to the class names from the raw corpus,\nwhich may end up with very limited or even no samples for the minority classes.\nRecent works have started to generate the relevant texts by prompting LLMs\nusing the class names or definitions; however, there is a high risk that LLMs\ncannot generate in-distribution (i.e., similar to the corpus where the text\nclassifier will be applied) data, leading to ungeneralizable classifiers. In\nthis paper, we combine the advantages of these two approaches and propose to\nbridge the gap via a novel framework, \\emph{text grafting}, which aims to\nobtain clean and near-distribution weak supervision for minority classes.\nSpecifically, we first use LLM-based logits to mine masked templates from the\nraw corpus, which have a high potential for data synthesis into the target\nminority class. Then, the templates are filled by state-of-the-art LLMs to\nsynthesize near-distribution texts falling into minority classes. Text grafting\nshows significant improvement over direct mining or synthesis on minority\nclasses. We also use analysis and case studies to comprehend the property of\ntext grafting.", "published": "2024-06-17 00:23:08", "link": "http://arxiv.org/abs/2406.11115v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Grammaticality Representation in ChatGPT as Compared to Linguists and\n  Laypeople", "abstract": "Large language models (LLMs) have demonstrated exceptional performance across\nvarious linguistic tasks. However, it remains uncertain whether LLMs have\ndeveloped human-like fine-grained grammatical intuition. This preregistered\nstudy (https://osf.io/t5nes) presents the first large-scale investigation of\nChatGPT's grammatical intuition, building upon a previous study that collected\nlaypeople's grammatical judgments on 148 linguistic phenomena that linguists\njudged to be grammatical, ungrammatical, or marginally grammatical (Sprouse,\nSchutze, & Almeida, 2013). Our primary focus was to compare ChatGPT with both\nlaypeople and linguists in the judgement of these linguistic constructions. In\nExperiment 1, ChatGPT assigned ratings to sentences based on a given reference\nsentence. Experiment 2 involved rating sentences on a 7-point scale, and\nExperiment 3 asked ChatGPT to choose the more grammatical sentence from a pair.\nOverall, our findings demonstrate convergence rates ranging from 73% to 95%\nbetween ChatGPT and linguists, with an overall point-estimate of 89%.\nSignificant correlations were also found between ChatGPT and laypeople across\nall tasks, though the correlation strength varied by task. We attribute these\nresults to the psychometric nature of the judgment tasks and the differences in\nlanguage processing styles between humans and LLMs.", "published": "2024-06-17 00:23:16", "link": "http://arxiv.org/abs/2406.11116v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dynamic Order Template Prediction for Generative Aspect-Based Sentiment\n  Analysis", "abstract": "Aspect-based sentiment analysis (ABSA) assesses sentiments towards specific\naspects within texts, resulting in detailed sentiment tuples. Previous ABSA\nmodels often use static templates to predict all of the elements in the tuples,\nand these models often fail to accurately capture dependencies between\nelements. Multi-view prompting method improves the performance of ABSA by\npredicting tuples with various templates and then ensembling the results.\nHowever, this method suffers from inefficiencies and out-of-distribution\nerrors. In this paper, we propose a Dynamic Order Template (DOT) method for\nABSA, which dynamically generates necessary views for each instance based on\ninstance-level entropy. Ensuring the diverse and relevant view generation, our\nproposed method improves F1-scores on ASQP and ACOS datasets while\nsignificantly reducing inference time.", "published": "2024-06-17 01:21:28", "link": "http://arxiv.org/abs/2406.11130v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Breaking Boundaries: Investigating the Effects of Model Editing on\n  Cross-linguistic Performance", "abstract": "The integration of pretrained language models (PLMs) like BERT and GPT has\nrevolutionized NLP, particularly for English, but it has also created\nlinguistic imbalances. This paper strategically identifies the need for\nlinguistic equity by examining several knowledge editing techniques in\nmultilingual contexts. We evaluate the performance of models such as Mistral,\nTowerInstruct, OpenHathi, Tamil-Llama, and Kan-Llama across languages including\nEnglish, German, French, Italian, Spanish, Hindi, Tamil, and Kannada. Our\nresearch identifies significant discrepancies in normal and merged models\nconcerning cross-lingual consistency. We employ strategies like 'each language\nfor itself' (ELFI) and 'each language for others' (ELFO) to stress-test these\nmodels. Our findings demonstrate the potential for LLMs to overcome linguistic\nbarriers, laying the groundwork for future research in achieving linguistic\ninclusivity in AI technologies.", "published": "2024-06-17 01:54:27", "link": "http://arxiv.org/abs/2406.11139v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Good are LLMs at Relation Extraction under Low-Resource Scenario?\n  Comprehensive Evaluation", "abstract": "Relation Extraction (RE) serves as a crucial technology for transforming\nunstructured text into structured information, especially within the framework\nof Knowledge Graph development. Its importance is emphasized by its essential\nrole in various downstream tasks. Besides the conventional RE methods which are\nbased on neural networks and pre-trained language models, large language models\n(LLMs) are also utilized in the research field of RE. However, on low-resource\nlanguages (LRLs), both conventional RE methods and LLM-based methods perform\npoorly on RE due to the data scarcity issues. To this end, this paper\nconstructs low-resource relation extraction datasets in 10 LRLs in three\nregions (Central Asia, Southeast Asia and Middle East). The corpora are\nconstructed by translating the original publicly available English RE datasets\n(NYT10, FewRel and CrossRE) using an effective multilingual machine\ntranslation. Then, we use the language perplexity (PPL) to filter out the\nlow-quality data from the translated datasets. Finally, we conduct an empirical\nstudy and validate the performance of several open-source LLMs on these\ngenerated LRL RE datasets.", "published": "2024-06-17 03:02:04", "link": "http://arxiv.org/abs/2406.11162v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Criminal Case Matching through Diverse Legal Factors", "abstract": "Criminal case matching endeavors to determine the relevance between different\ncriminal cases. Conventional methods predict the relevance solely based on\ninstance-level semantic features and neglect the diverse legal factors (LFs),\nwhich are associated with diverse court judgments. Consequently,\ncomprehensively representing a criminal case remains a challenge for these\napproaches. Moreover, extracting and utilizing these LFs for criminal case\nmatching face two challenges: (1) the manual annotations of LFs rely heavily on\nspecialized legal knowledge; (2) overlaps among LFs may potentially harm the\nmodel's performance. In this paper, we propose a two-stage framework named\nDiverse Legal Factor-enhanced Criminal Case Matching (DLF-CCM). Firstly,\nDLF-CCM employs a multi-task learning framework to pre-train an LF extraction\nnetwork on a large-scale legal judgment prediction dataset. In stage two,\nDLF-CCM introduces an LF de-redundancy module to learn shared LF and exclusive\nLFs. Moreover, an entropy-weighted fusion strategy is introduced to dynamically\nfuse the multiple relevance generated by all LFs. Experimental results validate\nthe effectiveness of DLF-CCM and show its significant improvements over\ncompetitive baselines. Code: https://github.com/jiezhao6/DLF-CCM.", "published": "2024-06-17 03:23:11", "link": "http://arxiv.org/abs/2406.11172v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BSRBF-KAN: A combination of B-splines and Radial Basis Functions in\n  Kolmogorov-Arnold Networks", "abstract": "In this paper, we introduce BSRBF-KAN, a Kolmogorov Arnold Network (KAN) that\ncombines B-splines and radial basis functions (RBFs) to fit input vectors\nduring data training. We perform experiments with BSRBF-KAN, multi-layer\nperception (MLP), and other popular KANs, including EfficientKAN, FastKAN,\nFasterKAN, and GottliebKAN over the MNIST and Fashion-MNIST datasets. BSRBF-KAN\nshows stability in 5 training runs with a competitive average accuracy of\n97.55% on MNIST and 89.33% on Fashion-MNIST and obtains convergence better than\nother networks. We expect BSRBF-KAN to open many combinations of mathematical\nfunctions to design KANs. Our repo is publicly available at:\nhttps://github.com/hoangthangta/BSRBF_KAN.", "published": "2024-06-17 03:26:02", "link": "http://arxiv.org/abs/2406.11173v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Retrieval-Augmented Feature Generation for Domain-Specific\n  Classification", "abstract": "Feature generation can significantly enhance learning outcomes, particularly\nfor tasks with limited data. An effective way to improve feature generation is\nby expanding the current feature space using existing features and enriching\nthe informational content. However, generating new, interpretable features in\napplication fields often requires domain-specific knowledge about the existing\nfeatures. This paper introduces a new method RAFG for generating reasonable and\nexplainable features specific to domain classification tasks. To generate new\nfeatures with interpretability in domain knowledge, we perform information\nretrieval on existing features to identify potential feature associations, and\nutilize these associations to generate meaningful features. Furthermore, we\ndevelop a Large Language Model (LLM)-based framework for feature generation\nwith reasoning to verify and filter features during the generation process.\nExperiments across several datasets in medical, economic, and geographic\ndomains show that our RAFG method produces high-quality, meaningful features\nand significantly improves classification performance compared with baseline\nmethods.", "published": "2024-06-17 03:29:14", "link": "http://arxiv.org/abs/2406.11177v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey on Human Preference Learning for Large Language Models", "abstract": "The recent surge of versatile large language models (LLMs) largely depends on\naligning increasingly capable foundation models with human intentions by\npreference learning, enhancing LLMs with excellent applicability and\neffectiveness in a wide range of contexts. Despite the numerous related studies\nconducted, a perspective on how human preferences are introduced into LLMs\nremains limited, which may prevent a deeper comprehension of the relationships\nbetween human preferences and LLMs as well as the realization of their\nlimitations. In this survey, we review the progress in exploring human\npreference learning for LLMs from a preference-centered perspective, covering\nthe sources and formats of preference feedback, the modeling and usage of\npreference signals, as well as the evaluation of the aligned LLMs. We first\ncategorize the human feedback according to data sources and formats. We then\nsummarize techniques for human preferences modeling and compare the advantages\nand disadvantages of different schools of models. Moreover, we present various\npreference usage methods sorted by the objectives to utilize human preference\nsignals. Finally, we summarize some prevailing approaches to evaluate LLMs in\nterms of alignment with human intentions and discuss our outlooks on the human\nintention alignment for LLMs.", "published": "2024-06-17 03:52:51", "link": "http://arxiv.org/abs/2406.11191v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Boundaries: Learning a Universal Entity Taxonomy across Datasets\n  and Languages for Open Named Entity Recognition", "abstract": "Open Named Entity Recognition (NER), which involves identifying arbitrary\ntypes of entities from arbitrary domains, remains challenging for Large\nLanguage Models (LLMs). Recent studies suggest that fine-tuning LLMs on\nextensive NER data can boost their performance. However, training directly on\nexisting datasets neglects their inconsistent entity definitions and redundant\ndata, limiting LLMs to dataset-specific learning and hindering out-of-domain\nadaptation. To address this, we present B2NERD, a compact dataset designed to\nguide LLMs' generalization in Open NER under a universal entity taxonomy.\nB2NERD is refined from 54 existing English and Chinese datasets using a\ntwo-step process. First, we detect inconsistent entity definitions across\ndatasets and clarify them by distinguishable label names to construct a\nuniversal taxonomy of 400+ entity types. Second, we address redundancy using a\ndata pruning strategy that selects fewer samples with greater category and\nsemantic diversity. Comprehensive evaluation shows that B2NERD significantly\nenhances LLMs' Open NER capabilities. Our B2NER models, trained on B2NERD,\noutperform GPT-4 by 6.8-12.0 F1 points and surpass previous methods in 3\nout-of-domain benchmarks across 15 datasets and 6 languages. The data, models,\nand code are publicly available at https://github.com/UmeanNever/B2NER.", "published": "2024-06-17 03:57:35", "link": "http://arxiv.org/abs/2406.11192v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MMNeuron: Discovering Neuron-Level Domain-Specific Interpretation in\n  Multimodal Large Language Model", "abstract": "Projecting visual features into word embedding space has become a significant\nfusion strategy adopted by Multimodal Large Language Models (MLLMs). However,\nits internal mechanisms have yet to be explored. Inspired by multilingual\nresearch, we identify domain-specific neurons in multimodal large language\nmodels. Specifically, we investigate the distribution of domain-specific\nneurons and the mechanism of how MLLMs process features from diverse domains.\nFurthermore, we propose a three-stage mechanism for language model modules in\nMLLMs when handling projected image features, and verify this hypothesis using\nlogit lens. Extensive experiments indicate that while current MLLMs exhibit\nVisual Question Answering (VQA) capability, they may not fully utilize\ndomain-specific information. Manipulating domain-specific neurons properly will\nresult in a 10% change of accuracy at most, shedding light on the development\nof cross-domain, all-encompassing MLLMs in the future. The source code is\navailable at https://github.com/Z1zs/MMNeuron.", "published": "2024-06-17 03:59:44", "link": "http://arxiv.org/abs/2406.11193v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "In-Context Editing: Learning Knowledge from Self-Induced Distributions", "abstract": "In scenarios where language models must incorporate new information\nefficiently without extensive retraining, traditional fine-tuning methods are\nprone to overfitting, degraded generalization, and unnatural language\ngeneration. To address these limitations, we introduce Consistent In-Context\nEditing (ICE), a novel approach leveraging the model's in-context learning\ncapability to optimize toward a contextual distribution rather than a one-hot\ntarget. ICE introduces a simple yet effective optimization framework for the\nmodel to internalize new knowledge by aligning its output distributions with\nand without additional context. This method enhances the robustness and\neffectiveness of gradient-based tuning methods, preventing overfitting and\npreserving the model's integrity. We analyze ICE across four critical aspects\nof knowledge editing: accuracy, locality, generalization, and linguistic\nquality, demonstrating its advantages. Experimental results confirm the\neffectiveness of ICE and demonstrate its potential for continual editing,\nensuring that the integrity of the model is preserved while updating\ninformation.", "published": "2024-06-17 04:00:04", "link": "http://arxiv.org/abs/2406.11194v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Problematic Tokens: Tokenizer Bias in Large Language Models", "abstract": "Recent advancements in large language models(LLMs), such as GPT-4 and GPT-4o,\nhave shown exceptional performance, especially in languages with abundant\nresources like English, thanks to extensive datasets that ensure robust\ntraining. Conversely, these models exhibit limitations when processing\nunder-resourced languages such as Chinese and Korean, where issues including\nhallucinatory responses remain prevalent. This paper traces the roots of these\ndisparities to the tokenization process inherent to these models. Specifically,\nit explores how the tokenizers vocabulary, often used to speed up the\ntokenization process and reduce tokens but constructed independently of the\nactual model training data, inadequately represents non-English languages. This\nmisrepresentation results in the propagation of under-trained or untrained\ntokens, which perpetuate biases and pose serious concerns related to data\nsecurity and ethical standards. We aim to dissect the tokenization mechanics of\nGPT-4o, illustrating how its simplified token-handling methods amplify these\nrisks and offer strategic solutions to mitigate associated security and ethical\nissues. Through this study, we emphasize the critical need to rethink\ntokenization frameworks to foster more equitable and secure AI technologies.\nThe code and data are available at https://github.com/yeyimilk/LLMGPT4o", "published": "2024-06-17 05:13:25", "link": "http://arxiv.org/abs/2406.11214v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ComperDial: Commonsense Persona-grounded Dialogue Dataset and Benchmark", "abstract": "We propose a new benchmark, ComperDial, which facilitates the training and\nevaluation of evaluation metrics for open-domain dialogue systems. ComperDial\nconsists of human-scored responses for 10,395 dialogue turns in 1,485\nconversations collected from 99 dialogue agents submitted to the Commonsense\nPersona-grounded Dialogue (CPD) challenge. As a result, for any dialogue, our\nbenchmark includes multiple diverse responses with variety of characteristics\nto ensure more robust evaluation of learned dialogue metrics. In addition to\nsingle-turn response scores, ComperDial also contains dialogue-level\nhuman-annotated scores, enabling joint assessment of multi-turn model responses\nthroughout a dialogue. Finally, building off ComperDial, we devise a new\nautomatic evaluation metric to measure the general similarity of\nmodel-generated dialogues to human conversations. Our experimental results\ndemonstrate that our novel metric, CPDScore is more correlated with human\njudgments than existing metrics. We release both ComperDial and CPDScore to the\ncommunity to accelerate development of automatic evaluation metrics for\nopen-domain dialogue systems.", "published": "2024-06-17 05:51:04", "link": "http://arxiv.org/abs/2406.11228v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What Kinds of Tokens Benefit from Distant Text? An Analysis on Long\n  Context Language Modeling", "abstract": "As the context length that large language models can handle continues to\nincrease, these models demonstrate an enhanced ability to utilize distant\ninformation for tasks such as language modeling. This capability contrasts with\nhuman reading and writing habits, where it is uncommon to remember and use\nparticularly distant information, except in cases of foreshadowing. In this\npaper, we aim to explore which kinds of words benefit more from long contexts\nin language models. By analyzing the changes in token probabilities with\nincreasing context length, we find that content words (e.g., nouns, adjectives)\nand the initial tokens of words benefit the most. Frequent patterns in the\ncontext (N-grams) also significantly impact predictions. Additionally, the\nmodel's prior knowledge plays a crucial role in influencing predictions,\nespecially for rare tokens. We also observe that language models become more\nconfident with longer contexts, resulting in sharper probability distributions.\nThis overconfidence may contribute to the increasing probabilities of tokens\nwith distant contextual information. We hope that our analysis will help the\ncommunity better understand long-text language modeling and contribute to the\ndesign of more reliable long-context models.", "published": "2024-06-17 06:07:29", "link": "http://arxiv.org/abs/2406.11238v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Machines Resonate with Humans? Evaluating the Emotional and Empathic\n  Comprehension of LMs", "abstract": "Empathy plays a pivotal role in fostering prosocial behavior, often triggered\nby the sharing of personal experiences through narratives. However, modeling\nempathy using NLP approaches remains challenging due to its deep\ninterconnection with human interaction dynamics. Previous approaches, which\ninvolve fine-tuning language models (LMs) on human-annotated empathic datasets,\nhave had limited success. In our pursuit of improving empathy understanding in\nLMs, we propose several strategies, including contrastive learning with masked\nLMs and supervised fine-tuning with large language models. While these methods\nshow improvements over previous methods, the overall results remain\nunsatisfactory. To better understand this trend, we performed an analysis which\nreveals a low agreement among annotators. This lack of consensus hinders\ntraining and highlights the subjective nature of the task. We also explore the\ncultural impact on annotations. To study this, we meticulously collected story\npairs in Urdu language and find that subjectivity in interpreting empathy among\nannotators appears to be independent of cultural background. Our systematic\nexploration of LMs' understanding of empathy reveals substantial opportunities\nfor further investigation in both task formulation and modeling.", "published": "2024-06-17 06:22:20", "link": "http://arxiv.org/abs/2406.11250v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dynamic Data Mixing Maximizes Instruction Tuning for Mixture-of-Experts", "abstract": "Mixture-of-Experts (MoE) models have shown remarkable capability in\ninstruction tuning, especially when the number of tasks scales. However,\nprevious methods simply merge all training tasks (e.g. creative writing,\ncoding, and mathematics) and apply fixed sampling weights, without considering\nthe importance of different tasks as the model training state changes. In this\nway, the most helpful data cannot be effectively distinguished, leading to\nsuboptimal model performance. To reduce the potential redundancies of datasets,\nwe make the first attempt and propose a novel dynamic data mixture for MoE\ninstruction tuning. Specifically, inspired by MoE's token routing preference,\nwe build dataset-level representations and then capture the subtle differences\namong datasets. Finally, we propose to dynamically adjust the sampling weight\nof datasets by their inter-redundancies, thus maximizing global performance\nunder a limited training budget. The experimental results on two MoE models\ndemonstrate the effectiveness of our approach on both downstream knowledge \\&\nreasoning tasks and open-ended queries. Code and models are available at\nhttps://github.com/Spico197/MoE-SFT .", "published": "2024-06-17 06:47:03", "link": "http://arxiv.org/abs/2406.11256v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SeRTS: Self-Rewarding Tree Search for Biomedical Retrieval-Augmented\n  Generation", "abstract": "Large Language Models (LLMs) have shown great potential in the biomedical\ndomain with the advancement of retrieval-augmented generation (RAG). However,\nexisting retrieval-augmented approaches face challenges in addressing diverse\nqueries and documents, particularly for medical knowledge queries, resulting in\nsub-optimal performance. To address these limitations, we propose a novel\nplug-and-play LLM-based retrieval method called Self-Rewarding Tree Search\n(SeRTS) based on Monte Carlo Tree Search (MCTS) and a self-rewarding paradigm.\nBy combining the reasoning capabilities of LLMs with the effectiveness of tree\nsearch, SeRTS boosts the zero-shot performance of retrieving high-quality and\ninformative results for RAG. We further enhance retrieval performance by\nfine-tuning LLMs with Proximal Policy Optimization (PPO) objectives using the\ntrajectories collected by SeRTS as feedback. Controlled experiments using the\nBioASQ-QA dataset with GPT-3.5-Turbo and LLama2-7b demonstrate that our method\nsignificantly improves the performance of the BM25 retriever and surpasses the\nstrong baseline of self-reflection in both efficiency and scalability.\nMoreover, SeRTS generates higher-quality feedback for PPO training than\nself-reflection. Our proposed method effectively adapts LLMs to document\nretrieval tasks, enhancing their ability to retrieve highly relevant documents\nfor RAG in the context of medical knowledge queries. This work presents a\nsignificant step forward in leveraging LLMs for accurate and comprehensive\nbiomedical question answering.", "published": "2024-06-17 06:48:31", "link": "http://arxiv.org/abs/2406.11258v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mitigating Large Language Model Hallucination with Faithful Finetuning", "abstract": "Large language models (LLMs) have demonstrated remarkable performance on\nvarious natural language processing tasks. However, they are prone to\ngenerating fluent yet untruthful responses, known as \"hallucinations\".\nHallucinations can lead to the spread of misinformation and cause harm in\ncritical applications. Mitigating hallucinations is challenging as they arise\nfrom factors such as noisy data, model overconfidence, lack of knowledge, and\nthe generation process itself. Recent efforts have attempted to address this\nissue through representation editing and decoding algorithms, reducing\nhallucinations without major structural changes or retraining. However, these\napproaches either implicitly edit LLMs' behavior in latent space or suppress\nthe tendency to output unfaithful results during decoding instead of explicitly\nmodeling on hallucination. In this work, we introduce Faithful Finetuning (F2),\na novel method that explicitly models the process of faithful question\nanswering through carefully designed loss functions during fine-tuning. We\nconduct extensive experiments on popular datasets and demonstrate that F2\nachieves significant improvements over vanilla models and baselines.", "published": "2024-06-17 07:16:07", "link": "http://arxiv.org/abs/2406.11267v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Skip-Layer Attention: Bridging Abstract and Detailed Dependencies in\n  Transformers", "abstract": "The Transformer architecture has significantly advanced deep learning,\nparticularly in natural language processing, by effectively managing long-range\ndependencies. However, as the demand for understanding complex relationships\ngrows, refining the Transformer's architecture becomes critical. This paper\nintroduces Skip-Layer Attention (SLA) to enhance Transformer models by enabling\ndirect attention between non-adjacent layers. This method improves the model's\nability to capture dependencies between high-level abstract features and\nlow-level details. By facilitating direct attention between these diverse\nfeature levels, our approach overcomes the limitations of current Transformers,\nwhich often rely on suboptimal intra-layer attention. Our implementation\nextends the Transformer's functionality by enabling queries in a given layer to\ninteract with keys and values from both the current layer and one preceding\nlayer, thus enhancing the diversity of multi-head attention without additional\ncomputational burden. Extensive experiments demonstrate that our enhanced\nTransformer model achieves superior performance in language modeling tasks,\nhighlighting the effectiveness of our skip-layer attention mechanism.", "published": "2024-06-17 07:24:38", "link": "http://arxiv.org/abs/2406.11274v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-training Large Language Models through Knowledge Detection", "abstract": "Large language models (LLMs) often necessitate extensive labeled datasets and\ntraining compute to achieve impressive performance across downstream tasks.\nThis paper explores a self-training paradigm, where the LLM autonomously\ncurates its own labels and selectively trains on unknown data samples\nidentified through a reference-free consistency method. Empirical evaluations\ndemonstrate significant improvements in reducing hallucination in generation\nacross multiple subjects. Furthermore, the selective training framework\nmitigates catastrophic forgetting in out-of-distribution benchmarks, addressing\na critical limitation in training LLMs. Our findings suggest that such an\napproach can substantially reduce the dependency on large labeled datasets,\npaving the way for more scalable and cost-effective language model training.", "published": "2024-06-17 07:25:09", "link": "http://arxiv.org/abs/2406.11275v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Small Agent Can Also Rock! Empowering Small Language Models as\n  Hallucination Detector", "abstract": "Hallucination detection is a challenging task for large language models\n(LLMs), and existing studies heavily rely on powerful closed-source LLMs such\nas GPT-4. In this paper, we propose an autonomous LLM-based agent framework,\ncalled HaluAgent, which enables relatively smaller LLMs (e.g. Baichuan2-Chat\n7B) to actively select suitable tools for detecting multiple hallucination\ntypes such as text, code, and mathematical expression. In HaluAgent, we\nintegrate the LLM, multi-functional toolbox, and design a fine-grained\nthree-stage detection framework along with memory mechanism. To facilitate the\neffectiveness of HaluAgent, we leverage existing Chinese and English datasets\nto synthesize detection trajectories for fine-tuning, which endows HaluAgent\nwith the capability for bilingual hallucination detection. Extensive\nexperiments demonstrate that only using 2K samples for tuning LLMs, HaluAgent\ncan perform hallucination detection on various types of tasks and datasets,\nachieving performance comparable to or even higher than GPT-4 without tool\nenhancements on both in-domain and out-of-domain datasets. We release our\ndataset and code at https://github.com/RUCAIBox/HaluAgent.", "published": "2024-06-17 07:30:05", "link": "http://arxiv.org/abs/2406.11277v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do Not Design, Learn: A Trainable Scoring Function for Uncertainty\n  Estimation in Generative LLMs", "abstract": "Uncertainty estimation (UE) of generative large language models (LLMs) is\ncrucial for evaluating the reliability of generated sequences. A significant\nsubset of UE methods utilize token probabilities to assess uncertainty,\naggregating multiple token probabilities into a single UE score using a scoring\nfunction. Existing scoring functions for probability-based UE, such as\nlength-normalized scoring and semantic contribution-based weighting, are\ndesigned to solve certain aspects of the problem but exhibit limitations,\nincluding the inability to handle biased probabilities and complex semantic\ndependencies between tokens. To address these issues, in this work, we propose\nLearnable Response Scoring (LARS) function, a novel scoring function that\nleverages supervised data to capture complex dependencies between tokens and\nprobabilities, thereby producing more reliable and calibrated response scores\nin computing the uncertainty of LLM generations. Our comprehensive experiments\nacross question-answering and arithmetical reasoning tasks with various\ndatasets demonstrate that LARS significantly outperforms existing scoring\nfunctions, achieving improvements of up to 16\\% AUROC score.", "published": "2024-06-17 07:30:40", "link": "http://arxiv.org/abs/2406.11278v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Systematic Survey of Text Summarization: From Statistical Methods to\n  Large Language Models", "abstract": "Text summarization research has undergone several significant transformations\nwith the advent of deep neural networks, pre-trained language models (PLMs),\nand recent large language models (LLMs). This survey thus provides a\ncomprehensive review of the research progress and evolution in text\nsummarization through the lens of these paradigm shifts. It is organized into\ntwo main parts: (1) a detailed overview of datasets, evaluation metrics, and\nsummarization methods before the LLM era, encompassing traditional statistical\nmethods, deep learning approaches, and PLM fine-tuning techniques, and (2) the\nfirst detailed examination of recent advancements in benchmarking, modeling,\nand evaluating summarization in the LLM era. By synthesizing existing\nliterature and presenting a cohesive overview, this survey also discusses\nresearch trends, open challenges, and proposes promising research directions in\nsummarization, aiming to guide researchers through the evolving landscape of\nsummarization research.", "published": "2024-06-17 07:52:32", "link": "http://arxiv.org/abs/2406.11289v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Empirical Investigation of Matrix Factorization Methods for\n  Pre-trained Transformers", "abstract": "The increasing size of transformer-based models in NLP makes the question of\ncompressing them important. In this work, we present a comprehensive analysis\nof factorization based model compression techniques. Specifically, we focus on\ncomparing straightforward low-rank factorization against the recently\nintroduced Monarch factorization, which exhibits impressive performance\npreservation on the GLUE benchmark. To mitigate stability issues associated\nwith low-rank factorization of the matrices in pre-trained transformers, we\nintroduce a staged factorization approach wherein layers are factorized one by\none instead of being factorized simultaneously. Through this strategy we\nsignificantly enhance the stability and reliability of the compression process.\nFurther, we introduce a simple block-wise low-rank factorization method, which\nhas a close relationship to Monarch factorization. Our experiments lead to the\nsurprising conclusion that straightforward low-rank factorization consistently\noutperforms Monarch factorization across both different compression ratios and\nsix different text classification tasks.", "published": "2024-06-17 08:14:23", "link": "http://arxiv.org/abs/2406.11307v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are Large Language Models True Healthcare Jacks-of-All-Trades?\n  Benchmarking Across Health Professions Beyond Physician Exams", "abstract": "Recent advancements in Large Language Models (LLMs) have demonstrated their\npotential in delivering accurate answers to questions about world knowledge.\nDespite this, existing benchmarks for evaluating LLMs in healthcare\npredominantly focus on medical doctors, leaving other critical healthcare\nprofessions underrepresented. To fill this research gap, we introduce the\nExaminations for Medical Personnel in Chinese (EMPEC), a pioneering large-scale\nhealthcare knowledge benchmark in traditional Chinese. EMPEC consists of\n157,803 exam questions across 124 subjects and 20 healthcare professions,\nincluding underrepresented occupations like Optometrists and Audiologists. Each\nquestion is tagged with its release time and source, ensuring relevance and\nauthenticity. We conducted extensive experiments on 17 LLMs, including\nproprietary, open-source models, general domain models and medical specific\nmodels, evaluating their performance under various settings. Our findings\nreveal that while leading models like GPT-4 achieve over 75\\% accuracy, they\nstill struggle with specialized fields and alternative medicine. Surprisingly,\ngeneral-purpose LLMs outperformed medical-specific models, and incorporating\nEMPEC's training data significantly enhanced performance. Additionally, the\nresults on questions released after the models' training cutoff date were\nconsistent with overall performance trends, suggesting that the models'\nperformance on the test set can predict their effectiveness in addressing\nunseen healthcare-related queries. The transition from traditional to\nsimplified Chinese characters had a negligible impact on model performance,\nindicating robust linguistic versatility. Our study underscores the importance\nof expanding benchmarks to cover a broader range of healthcare professions to\nbetter assess the applicability of LLMs in real-world healthcare scenarios.", "published": "2024-06-17 08:40:36", "link": "http://arxiv.org/abs/2406.11328v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-grained Controllable Text Generation through In-context Learning\n  with Feedback", "abstract": "We present a method for rewriting an input sentence to match specific values\nof nontrivial linguistic features, such as dependency depth. In contrast to\nearlier work, our method uses in-context learning rather than finetuning,\nmaking it applicable in use cases where data is sparse. We show that our model\nperforms accurate rewrites and matches the state of the art on rewriting\nsentences to a specified school grade level.", "published": "2024-06-17 08:55:48", "link": "http://arxiv.org/abs/2406.11338v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Systematic Analysis of Large Language Models as Soft Reasoners: The\n  Case of Syllogistic Inferences", "abstract": "The reasoning abilities of Large Language Models (LLMs) are becoming a\ncentral focus of study in NLP. In this paper, we consider the case of\nsyllogistic reasoning, an area of deductive reasoning studied extensively in\nlogic and cognitive psychology. Previous research has shown that pre-trained\nLLMs exhibit reasoning biases, such as $\\textit{content effects}$, avoid\nanswering that $\\textit{no conclusion follows}$, display human-like\ndifficulties, and struggle with multi-step reasoning. We contribute to this\nresearch line by systematically investigating the effects of chain-of-thought\nreasoning, in-context learning (ICL), and supervised fine-tuning (SFT) on\nsyllogistic reasoning, considering syllogisms with conclusions that support or\nviolate world knowledge, as well as ones with multiple premises. Crucially, we\ngo beyond the standard focus on accuracy, with an in-depth analysis of the\nconclusions generated by the models. Our results suggest that the behavior of\npre-trained LLMs can be explained by heuristics studied in cognitive science\nand that both ICL and SFT improve model performance on valid inferences,\nalthough only the latter mitigates most reasoning biases without harming model\nconsistency.", "published": "2024-06-17 08:59:04", "link": "http://arxiv.org/abs/2406.11341v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Quotation Attribution with Fictional Character Embeddings", "abstract": "Humans naturally attribute utterances of direct speech to their speaker in\nliterary works. When attributing quotes, we process contextual information but\nalso access mental representations of characters that we build and revise\nthroughout the narrative. Recent methods to automatically attribute such\nutterances have explored simulating human logic with deterministic rules or\nlearning new implicit rules with neural networks when processing contextual\ninformation. However, these systems inherently lack \\textit{character}\nrepresentations, which often leads to errors in more challenging examples of\nattribution: anaphoric and implicit quotes. In this work, we propose to augment\na popular quotation attribution system, BookNLP, with character embeddings that\nencode global stylistic information of characters derived from an off-the-shelf\nstylometric model, Universal Authorship Representation (UAR). We create DramaCV\n(Code and data can be found at\nhttps://github.com/deezer/character_embeddings_qa ), a corpus of English drama\nplays from the 15th to 20th century that we automatically annotate for\nAuthorship Verification of fictional characters utterances, and release two\nversions of UAR trained on DramaCV, that are tailored for literary characters\nanalysis. Then, through an extensive evaluation on 28 novels, we show that\ncombining BookNLP's contextual information with our proposed global character\nembeddings improves the identification of speakers for anaphoric and implicit\nquotes, reaching state-of-the-art performance.", "published": "2024-06-17 09:46:35", "link": "http://arxiv.org/abs/2406.11368v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating LLMs for Quotation Attribution in Literary Texts: A Case\n  Study of LLaMa3", "abstract": "Large Language Models (LLMs) have shown promising results in a variety of\nliterary tasks, often using complex memorized details of narration and\nfictional characters. In this work, we evaluate the ability of Llama-3 at\nattributing utterances of direct-speech to their speaker in novels. The LLM\nshows impressive results on a corpus of 28 novels, surpassing published results\nwith ChatGPT and encoder-based baselines by a large margin. We then validate\nthese results by assessing the impact of book memorization and annotation\ncontamination. We found that these types of memorization do not explain the\nlarge performance gain, making Llama-3 the new state-of-the-art for quotation\nattribution in English literature. We release publicly our code and data.", "published": "2024-06-17 09:56:46", "link": "http://arxiv.org/abs/2406.11380v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MetaGPT: Merging Large Language Models Using Model Exclusive Task\n  Arithmetic", "abstract": "The advent of large language models (LLMs) like GPT-4 has catalyzed the\nexploration of multi-task learning (MTL), in which a single model demonstrates\nproficiency across diverse tasks. Task arithmetic has emerged as a\ncost-effective approach for MTL. It enables performance enhancement across\nmultiple tasks by adding their corresponding task vectors to a pre-trained\nmodel. However, the current lack of a method that can simultaneously achieve\noptimal performance, computational efficiency, and data privacy limits their\napplication to LLMs. In this paper, we propose \\textbf{M}odel\n\\textbf{E}xclusive \\textbf{T}ask \\textbf{A}rithmetic for merging\n\\textbf{GPT}-scale models, which formalizes the objective of model merging into\na multi-task learning framework, aiming to minimize the average loss difference\nbetween the merged model and each individual task model. Since data privacy\nlimits the use of multi-task training data, we leverage LLMs' local linearity\nand task vectors' orthogonality to separate the data term and scaling\ncoefficients term and derive a model-exclusive task arithmetic method. Our\nproposed MetaGPT is data-agnostic and bypasses the heavy search process, making\nit cost-effective and easy to implement for LLMs.Extensive experiments\ndemonstrate that MetaGPT leads to improvements in task arithmetic and achieves\nstate-of-the-art performance on multiple tasks.", "published": "2024-06-17 10:12:45", "link": "http://arxiv.org/abs/2406.11385v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BAMBINO-LM: (Bilingual-)Human-Inspired Continual Pretraining of BabyLM", "abstract": "Children from bilingual backgrounds benefit from interactions with parents\nand teachers to re-acquire their heritage language. In this paper, we\ninvestigate how this insight from behavioral study can be incorporated into the\nlearning of small-scale language models. We introduce BAMBINO-LM, a continual\npre-training strategy for BabyLM that uses a novel combination of alternation\nand PPO-based perplexity reward induced from a parent Italian model. Upon\nevaluation on zero-shot classification tasks for English and Italian,\nBAMBINO-LM improves the Italian language capability of a BabyLM baseline. Our\nablation analysis demonstrates that employing both the alternation strategy and\nPPO-based modeling is key to this effectiveness gain. We also show that, as a\nside effect, the proposed method leads to a similar degradation in L1\neffectiveness as human children would have had in an equivalent learning\nscenario. Through its modeling and findings, BAMBINO-LM makes a focused\ncontribution to the pre-training of small-scale language models by first\ndeveloping a human-inspired strategy for pre-training and then showing that it\nresults in behaviours similar to that of humans.", "published": "2024-06-17 11:08:08", "link": "http://arxiv.org/abs/2406.11418v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automating Easy Read Text Segmentation", "abstract": "Easy Read text is one of the main forms of access to information for people\nwith reading difficulties. One of the key characteristics of this type of text\nis the requirement to split sentences into smaller grammatical segments, to\nfacilitate reading. Automated segmentation methods could foster the creation of\nEasy Read content, but their viability has yet to be addressed. In this work,\nwe study novel methods for the task, leveraging masked and generative language\nmodels, along with constituent parsing. We conduct comprehensive automatic and\nhuman evaluations in three languages, analysing the strengths and weaknesses of\nthe proposed alternatives, under scarce resource limitations. Our results\nhighlight the viability of automated Easy Read text segmentation and remaining\ndeficiencies compared to expert-driven human segmentation.", "published": "2024-06-17 12:25:25", "link": "http://arxiv.org/abs/2406.11464v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CrAM: Credibility-Aware Attention Modification in LLMs for Combating\n  Misinformation in RAG", "abstract": "Retrieval-Augmented Generation (RAG) can alleviate hallucinations of Large\nLanguage Models (LLMs) by referencing external documents. However, the\nmisinformation in external documents may mislead LLMs' generation. To address\nthis issue, we explore the task of \"credibility-aware RAG\", in which LLMs\nautomatically adjust the influence of retrieved documents based on their\ncredibility scores to counteract misinformation. To this end, we introduce a\nplug-and-play method named $\\textbf{Cr}$edibility-aware $\\textbf{A}$ttention\n$\\textbf{M}$odification (CrAM). CrAM identifies influential attention heads in\nLLMs and adjusts their attention weights based on the credibility of the\ndocuments, thereby reducing the impact of low-credibility documents.\nExperiments on Natual Questions and TriviaQA using Llama2-13B, Llama3-8B, and\nQwen1.5-7B show that CrAM improves the RAG performance of LLMs against\nmisinformation pollution by over 20%, even surpassing supervised fine-tuning\nmethods.", "published": "2024-06-17 13:01:12", "link": "http://arxiv.org/abs/2406.11497v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Counterfactual Debating with Preset Stances for Hallucination\n  Elimination of LLMs", "abstract": "Large Language Models (LLMs) excel in various natural language processing\ntasks but struggle with hallucination issues. Existing solutions have\nconsidered utilizing LLMs' inherent reasoning abilities to alleviate\nhallucination, such as self-correction and diverse sampling methods. However,\nthese methods often overtrust LLMs' initial answers due to inherent biases. The\nkey to alleviating this issue lies in overriding LLMs' inherent biases for\nanswer inspection. To this end, we propose a CounterFactual Multi-Agent Debate\n(CFMAD) framework. CFMAD presets the stances of LLMs to override their inherent\nbiases by compelling LLMs to generate justifications for a predetermined\nanswer's correctness. The LLMs with different predetermined stances are engaged\nwith a skeptical critic for counterfactual debate on the rationality of\ngenerated justifications. Finally, the debate process is evaluated by a\nthird-party judge to determine the final answer. Extensive experiments on four\ndatasets of three tasks demonstrate the superiority of CFMAD over existing\nmethods.", "published": "2024-06-17 13:21:23", "link": "http://arxiv.org/abs/2406.11514v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MEMLA: Enhancing Multilingual Knowledge Editing with Neuron-Masked\n  Low-Rank Adaptation", "abstract": "Knowledge editing aims to adjust the knowledge within large language models\n(LLMs) to prevent their responses from becoming obsolete or inaccurate.\nHowever, existing works on knowledge editing are primarily conducted in a\nsingle language, which is inadequate for multilingual language models. In this\npaper, we focus on multilingual knowledge editing (MKE), which requires\npropagating updates across multiple languages. This necessity poses a\nsignificant challenge for the task. Furthermore, the limited availability of a\ncomprehensive dataset for MKE exacerbates this challenge, hindering progress in\nthis area. Hence, we introduce the Multilingual Knowledge Editing Benchmark\n(MKEB), a novel dataset comprising 12 languages and providing a complete\nevaluation framework. Additionally, we propose a method that enhances\nMultilingual knowledge Editing with neuron-Masked Low-Rank Adaptation (MEMLA).\nSpecifically, we identify two categories of knowledge neurons to improve\nediting precision. Moreover, we perform LoRA-based editing with neuron masks to\nefficiently modify parameters and facilitate the propagation of updates across\nmultiple languages. Experiments demonstrate that our method outperforms\nexisting baselines and significantly enhances the multi-hop reasoning\ncapability of the edited model, with minimal impact on its downstream task\nperformance. The dataset and code will be made publicly available.", "published": "2024-06-17 14:03:50", "link": "http://arxiv.org/abs/2406.11566v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Error Span Annotation: A Balanced Approach for Human Evaluation of\n  Machine Translation", "abstract": "High-quality Machine Translation (MT) evaluation relies heavily on human\njudgments. Comprehensive error classification methods, such as Multidimensional\nQuality Metrics (MQM), are expensive as they are time-consuming and can only be\ndone by experts, whose availability may be limited especially for low-resource\nlanguages. On the other hand, just assigning overall scores, like Direct\nAssessment (DA), is simpler and faster and can be done by translators of any\nlevel, but is less reliable. In this paper, we introduce Error Span Annotation\n(ESA), a human evaluation protocol which combines the continuous rating of DA\nwith the high-level error severity span marking of MQM. We validate ESA by\ncomparing it to MQM and DA for 12 MT systems and one human reference\ntranslation (English to German) from WMT23. The results show that ESA offers\nfaster and cheaper annotations than MQM at the same quality level, without the\nrequirement of expensive MQM experts.", "published": "2024-06-17 14:20:47", "link": "http://arxiv.org/abs/2406.11580v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Style Transfer with Multi-iteration Preference Optimization", "abstract": "Numerous recent techniques for text style transfer characterize their\napproaches as variants of reinforcement learning and preference optimization.\nIn this work, we consider the relationship between these approaches and a class\nof optimization approaches developed primarily for (non-neural) statistical\nmachine translation, formerly known as `tuning'. Inspired by these techniques\nfrom the past, we improve upon established preference optimization approaches,\nincorporating multiple iterations of exploration and optimization, and choosing\ncontrastive examples by following a `hope' vs `fear' sampling strategy.\nCognizant of the difference between machine translation and style transfer,\nhowever, we further tailor our framework with a new pseudo-parallel generation\nmethod and a dynamic weighted reward aggregation method to tackle the lack of\nparallel data and the need for a multi-objective reward. We evaluate our model\non two commonly used text style transfer datasets. Through automatic and human\nevaluation results we show the effectiveness and the superiority of our model\ncompared to state-of-the-art baselines.", "published": "2024-06-17 14:20:53", "link": "http://arxiv.org/abs/2406.11581v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DELLA-Merging: Reducing Interference in Model Merging through\n  Magnitude-Based Sampling", "abstract": "With the proliferation of domain-specific models, model merging has emerged\nas a set of techniques that combine the capabilities of multiple models into\none that can multitask without the cost of additional training. In this paper,\nwe propose a new model merging technique, Drop and rEscaLe via sampLing with\nmAgnitude (DELLA-Merging), that employs a novel pruning technique, MAGPRUNE,\nwhich shows significant advantages over DARE and TIES. MAGPRUNE first ranks the\nparameters in order of their magnitude and assigns higher dropout probabilities\n(p) to parameters with lower ranks corresponding to lower magnitudes. To\napproximate the original embeddings, MAGPRUNE employs a rescaling operation on\nthe parameters that survive the random dropping by 1/(1 - p). On three\ndifferent expert models considered for merging (LM, Math, Code) and\ncorresponding benchmark datasets (AlpacaEval, GSM8K, MBPP), DELLA shows an\naverage improvement of 2.4 points over baseline methods employing delta\nparameter pruning (an improvement of 3.6 points over TIES, 1.2 points over\nDARE), and 11.1 points over the no-pruning baseline (TA). We release the source\ncode at: https://github.com/declare-lab/della.", "published": "2024-06-17 15:02:45", "link": "http://arxiv.org/abs/2406.11617v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Building Knowledge-Guided Lexica to Model Cultural Variation", "abstract": "Cultural variation exists between nations (e.g., the United States vs.\nChina), but also within regions (e.g., California vs. Texas, Los Angeles vs.\nSan Francisco). Measuring this regional cultural variation can illuminate how\nand why people think and behave differently. Historically, it has been\ndifficult to computationally model cultural variation due to a lack of training\ndata and scalability constraints. In this work, we introduce a new research\nproblem for the NLP community: How do we measure variation in cultural\nconstructs across regions using language? We then provide a scalable solution:\nbuilding knowledge-guided lexica to model cultural variation, encouraging\nfuture work at the intersection of NLP and cultural understanding. We also\nhighlight modern LLMs' failure to measure cultural variation or generate\nculturally varied language.", "published": "2024-06-17 15:05:43", "link": "http://arxiv.org/abs/2406.11622v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Many-Shot In-Context Learning Help LLMs as Evaluators? A Preliminary\n  Empirical Study", "abstract": "Utilizing Large Language Models (LLMs) as evaluators to assess the\nperformance of LLMs has garnered attention. However, this kind of evaluation\napproach is affected by potential biases within LLMs, raising concerns about\nthe accuracy and reliability of the evaluation results of LLMs. To address this\nproblem, we propose and study two many-shot In-Context Learning (ICL) prompt\ntemplates to help LLM evaluators mitigate potential biases: Many-Shot with\nReference (MSwR) and Many-Shot without Reference (MSoR). Specifically, the\nformer utilizes in-context examples with model-generated evaluation rationales\nas references, while the latter does not include these references. Using these\nprompt designs, we investigate the impact of increasing the number of\nin-context examples on the consistency and quality of the evaluation results.\nExperimental results show that advanced LLMs, such as GPT-4o, perform better in\nthe many-shot regime than in the zero-shot and few-shot regimes. Furthermore,\nwhen using GPT-4o as an evaluator in the many-shot regime, adopting MSwR as the\nprompt template performs better than MSoR.", "published": "2024-06-17 15:11:58", "link": "http://arxiv.org/abs/2406.11629v6", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Two-dimensional Zero-shot Dialogue State Tracking Evaluation Method\n  using GPT-4", "abstract": "Dialogue state tracking (DST) is evaluated by exact matching methods, which\nrely on large amounts of labeled data and ignore semantic consistency, leading\nto over-evaluation. Currently, leveraging large language models (LLM) in\nevaluating natural language processing tasks has achieved promising results.\nHowever, using LLM for DST evaluation is still under explored. In this paper,\nwe propose a two-dimensional zero-shot evaluation method for DST using GPT-4,\nwhich divides the evaluation into two dimensions: accuracy and completeness.\nFurthermore, we also design two manual reasoning paths in prompting to further\nimprove the accuracy of evaluation. Experimental results show that our method\nachieves better performance compared to the baselines, and is consistent with\ntraditional exact matching based methods.", "published": "2024-06-17 15:32:17", "link": "http://arxiv.org/abs/2406.11651v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ruby Teaming: Improving Quality Diversity Search with Memory for\n  Automated Red Teaming", "abstract": "We propose Ruby Teaming, a method that improves on Rainbow Teaming by\nincluding a memory cache as its third dimension. The memory dimension provides\ncues to the mutator to yield better-quality prompts, both in terms of attack\nsuccess rate (ASR) and quality diversity. The prompt archive generated by Ruby\nTeaming has an ASR of 74%, which is 20% higher than the baseline. In terms of\nquality diversity, Ruby Teaming outperforms Rainbow Teaming by 6% and 3% on\nShannon's Evenness Index (SEI) and Simpson's Diversity Index (SDI),\nrespectively.", "published": "2024-06-17 15:36:14", "link": "http://arxiv.org/abs/2406.11654v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cultural Conditioning or Placebo? On the Effectiveness of\n  Socio-Demographic Prompting", "abstract": "Socio-demographic prompting is a commonly employed approach to study cultural\nbiases in LLMs as well as for aligning models to certain cultures. In this\npaper, we systematically probe four LLMs (Llama 3, Mistral v0.2, GPT-3.5 Turbo\nand GPT-4) with prompts that are conditioned on culturally sensitive and\nnon-sensitive cues, on datasets that are supposed to be culturally sensitive\n(EtiCor and CALI) or neutral (MMLU and ETHICS). We observe that all models\nexcept GPT-4 show significant variations in their responses on both kinds of\ndatasets for both kinds of prompts, casting doubt on the robustness of the\nculturally-conditioned prompting as a method for eliciting cultural bias in\nmodels or as an alignment strategy. The work also calls rethinking the control\nexperiment design to tease apart the cultural conditioning of responses from\n\"placebo effect\", i.e., random perturbations of model responses due to\narbitrary tokens in the prompt.", "published": "2024-06-17 15:43:45", "link": "http://arxiv.org/abs/2406.11661v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"Not Aligned\" is Not \"Malicious\": Being Careful about Hallucinations of\n  Large Language Models' Jailbreak", "abstract": "\"Jailbreak\" is a major safety concern of Large Language Models (LLMs), which\noccurs when malicious prompts lead LLMs to produce harmful outputs, raising\nissues about the reliability and safety of LLMs. Therefore, an effective\nevaluation of jailbreaks is very crucial to develop its mitigation strategies.\nHowever, our research reveals that many jailbreaks identified by current\nevaluations may actually be hallucinations-erroneous outputs that are mistaken\nfor genuine safety breaches. This finding suggests that some perceived\nvulnerabilities might not represent actual threats, indicating a need for more\nprecise red teaming benchmarks. To address this problem, we propose the\n$\\textbf{B}$enchmark for reli$\\textbf{AB}$ilit$\\textbf{Y}$ and\njail$\\textbf{B}$reak ha$\\textbf{L}$l$\\textbf{U}$cination $\\textbf{E}$valuation\n(BabyBLUE). BabyBLUE introduces a specialized validation framework including\nvarious evaluators to enhance existing jailbreak benchmarks, ensuring outputs\nare useful malicious instructions. Additionally, BabyBLUE presents a new\ndataset as an augmentation to the existing red teaming benchmarks, specifically\naddressing hallucinations in jailbreaks, aiming to evaluate the true potential\nof jailbroken LLM outputs to cause harm to human society.", "published": "2024-06-17 15:51:01", "link": "http://arxiv.org/abs/2406.11668v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Endor: Hardware-Friendly Sparse Format for Offloaded LLM Inference", "abstract": "The increasing size of large language models (LLMs) challenges their usage on\nresource-constrained platforms. For example, memory on modern GPUs is\ninsufficient to hold LLMs that are hundreds of Gigabytes in size. Offloading is\na popular method to escape this constraint by storing weights of an LLM model\nto host CPU memory and SSD, then loading each weight to GPU before every use.\nIn our case study of offloaded inference, we found that due to the low\nbandwidth between storage devices and GPU, the latency of transferring large\nmodel weights from its offloaded location to GPU memory becomes the critical\nbottleneck with actual compute taking nearly 0% of runtime. To effectively\nreduce the weight transfer latency, we propose a novel sparse format that\ncompresses the unstructured sparse pattern of pruned LLM weights to non-zero\nvalues with high compression ratio and low decompression overhead. Endor\nachieves this by expressing the positions of non-zero elements with a bitmap.\nCompared to offloaded inference using the popular Huggingface Accelerate,\napplying Endor accelerates OPT-66B by 1.70x and Llama2-70B by 1.78x. When\ndirect weight transfer from SSD to GPU is leveraged, Endor achieves 2.25x\nspeedup on OPT-66B and 2.37x speedup on Llama2-70B.", "published": "2024-06-17 15:55:08", "link": "http://arxiv.org/abs/2406.11674v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HoLLMwood: Unleashing the Creativity of Large Language Models in\n  Screenwriting via Role Playing", "abstract": "Generative AI has demonstrated unprecedented creativity in the field of\ncomputer vision, yet such phenomena have not been observed in natural language\nprocessing. In particular, large language models (LLMs) can hardly produce\nwritten works at the level of human experts due to the extremely high\ncomplexity of literature writing. In this paper, we present HoLLMwood, an\nautomated framework for unleashing the creativity of LLMs and exploring their\npotential in screenwriting, which is a highly demanding task. Mimicking the\nhuman creative process, we assign LLMs to different roles involved in the\nreal-world scenario. In addition to the common practice of treating LLMs as\n${Writer}$, we also apply LLMs as ${Editor}$, who is responsible for providing\nfeedback and revision advice to ${Writer}$. Besides, to enrich the characters\nand deepen the plots, we introduce a role-playing mechanism and adopt LLMs as\n${Actors}$ that can communicate and interact with each other. Evaluations on\nautomatically generated screenplays show that HoLLMwood substantially\noutperforms strong baselines in terms of coherence, relevance, interestingness\nand overall quality.", "published": "2024-06-17 16:01:33", "link": "http://arxiv.org/abs/2406.11683v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tokenization Falling Short: On Subword Robustness in Large Language\n  Models", "abstract": "Language models typically tokenize raw text into sequences of subword\nidentifiers from a predefined vocabulary, a process inherently sensitive to\ntypographical errors, length variations, and largely oblivious to the internal\nstructure of tokens--issues we term the curse of tokenization. In this study,\nwe delve into these drawbacks and demonstrate that large language models (LLMs)\nremain susceptible to these problems. This study systematically investigates\nthese challenges and their impact on LLMs through three critical research\nquestions: (1) complex problem solving, (2) token structure probing, and (3)\nresilience to typographical variation. Our findings reveal that scaling model\nparameters can mitigate the issue of tokenization; however, LLMs still suffer\nfrom biases induced by typos and other text format variations. Our experiments\nshow that subword regularization such as BPE-dropout can mitigate this issue.\nWe release our evaluation code and data at https://github.com/FloatAI/TKEval.", "published": "2024-06-17 16:05:32", "link": "http://arxiv.org/abs/2406.11687v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Meta Reasoning for Large Language Models", "abstract": "We introduce Meta-Reasoning Prompting (MRP), a novel and efficient system\nprompting method for large language models (LLMs) inspired by human\nmeta-reasoning. Traditional in-context learning-based reasoning techniques,\nsuch as Tree-of-Thoughts, show promise but lack consistent state-of-the-art\nperformance across diverse tasks due to their specialized nature. MRP addresses\nthis limitation by guiding LLMs to dynamically select and apply different\nreasoning methods based on the specific requirements of each task, optimizing\nboth performance and computational efficiency. With MRP, LLM reasoning operates\nin two phases. Initially, the LLM identifies the most appropriate reasoning\nmethod using task input cues and objective descriptions of available methods.\nSubsequently, it applies the chosen method to complete the task. This dynamic\nstrategy mirrors human meta-reasoning, allowing the model to excel in a wide\nrange of problem domains. We evaluate the effectiveness of MRP through\ncomprehensive benchmarks. The results demonstrate that MRP achieves or\napproaches state-of-the-art performance across diverse tasks. MRP represents a\nsignificant advancement in enabling LLMs to identify cognitive challenges\nacross problems and leverage benefits across different reasoning approaches,\nenhancing their ability to handle diverse and complex problem domains\nefficiently. Every LLM deserves a Meta-Reasoning Prompting to unlock its full\npotential and ensure adaptability in an ever-evolving landscape of challenges\nand applications.", "published": "2024-06-17 16:14:11", "link": "http://arxiv.org/abs/2406.11698v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Multi-Agent Debate with Sparse Communication Topology", "abstract": "Multi-agent debate has proven effective in improving large language models\nquality for reasoning and factuality tasks. While various role-playing\nstrategies in multi-agent debates have been explored, in terms of the\ncommunication among agents, existing approaches adopt a brute force algorithm\n-- each agent can communicate with all other agents. In this paper, we\nsystematically investigate the effect of communication connectivity in\nmulti-agent systems. Our experiments on GPT and Mistral models reveal that\nmulti-agent debates leveraging sparse communication topology can achieve\ncomparable or superior performance while significantly reducing computational\ncosts. Furthermore, we extend the multi-agent debate framework to multimodal\nreasoning and alignment labeling tasks, showcasing its broad applicability and\neffectiveness. Our findings underscore the importance of communication\nconnectivity on enhancing the efficiency and effectiveness of the \"society of\nminds\" approach.", "published": "2024-06-17 17:33:09", "link": "http://arxiv.org/abs/2406.11776v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Safety Arithmetic: A Framework for Test-time Safety Alignment of\n  Language Models by Steering Parameters and Activations", "abstract": "Ensuring the safe alignment of large language models (LLMs) with human values\nis critical as they become integral to applications like translation and\nquestion answering. Current alignment methods struggle with dynamic user\nintentions and complex objectives, making models vulnerable to generating\nharmful content. We propose Safety Arithmetic, a training-free framework\nenhancing LLM safety across different scenarios: Base models, Supervised\nfine-tuned models (SFT), and Edited models. Safety Arithmetic involves Harm\nDirection Removal to avoid harmful content and Safety Alignment to promote safe\nresponses. Additionally, we present NoIntentEdit, a dataset highlighting edit\ninstances that could compromise model safety if used unintentionally. Our\nexperiments show that Safety Arithmetic significantly improves safety measures,\nreduces over-safety, and maintains model utility, outperforming existing\nmethods in ensuring safe content generation.", "published": "2024-06-17 17:48:13", "link": "http://arxiv.org/abs/2406.11801v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Do Large Language Models Acquire Factual Knowledge During\n  Pretraining?", "abstract": "Despite the recent observation that large language models (LLMs) can store\nsubstantial factual knowledge, there is a limited understanding of the\nmechanisms of how they acquire factual knowledge through pretraining. This work\naddresses this gap by studying how LLMs acquire factual knowledge during\npretraining. The findings reveal several important insights into the dynamics\nof factual knowledge acquisition during pretraining. First, counterintuitively,\nwe observe that pretraining on more data shows no significant improvement in\nthe model's capability to acquire and maintain factual knowledge. Next, there\nis a power-law relationship between training steps and forgetting of\nmemorization and generalization of factual knowledge, and LLMs trained with\nduplicated training data exhibit faster forgetting. Third, training LLMs with\nlarger batch sizes can enhance the models' robustness to forgetting. Overall,\nour observations suggest that factual knowledge acquisition in LLM pretraining\noccurs by progressively increasing the probability of factual knowledge\npresented in the pretraining data at each step. However, this increase is\ndiluted by subsequent forgetting. Based on this interpretation, we demonstrate\nthat we can provide plausible explanations for recently observed behaviors of\nLLMs, such as the poor performance of LLMs on long-tail knowledge and the\nbenefits of deduplicating the pretraining corpus.", "published": "2024-06-17 17:54:40", "link": "http://arxiv.org/abs/2406.11813v3", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Reframing linguistic bootstrapping as joint inference using\n  visually-grounded grammar induction models", "abstract": "Semantic and syntactic bootstrapping posit that children use their prior\nknowledge of one linguistic domain, say syntactic relations, to help later\nacquire another, such as the meanings of new words. Empirical results\nsupporting both theories may tempt us to believe that these are different\nlearning strategies, where one may precede the other. Here, we argue that they\nare instead both contingent on a more general learning strategy for language\nacquisition: joint learning. Using a series of neural visually-grounded grammar\ninduction models, we demonstrate that both syntactic and semantic bootstrapping\neffects are strongest when syntax and semantics are learnt simultaneously.\nJoint learning results in better grammar induction, realistic lexical category\nlearning, and better interpretations of novel sentence and verb meanings. Joint\nlearning makes language acquisition easier for learners by mutually\nconstraining the hypotheses spaces for both syntax and semantics. Studying the\ndynamics of joint inference over many input sources and modalities represents\nan important new direction for language modeling and learning research in both\ncognitive sciences and AI, as it may help us explain how language can be\nacquired in more constrained learning settings.", "published": "2024-06-17 18:01:06", "link": "http://arxiv.org/abs/2406.11977v1", "categories": ["cs.CL", "I.2.7; I.2.10; I.2.6; F.4.2"], "primary_category": "cs.CL"}
{"title": "FinTruthQA: A Benchmark Dataset for Evaluating the Quality of Financial\n  Information Disclosure", "abstract": "Accurate and transparent financial information disclosure is essential in\naccounting and finance, fostering trust and enabling informed investment\ndecisions that drive economic development. Among many information disclosure\nplatforms, the Chinese stock exchanges' investor interactive platform provides\na novel and interactive way for listed firms to disclose information of\ninterest to investors through an online question-and-answer (Q&A) format.\nHowever, it is common for listed firms to respond to questions with limited or\nno substantive information, and automatically evaluating the quality of\nfinancial information disclosure on large amounts of Q&A pairs is challenging.\nIn this study, our interdisciplinary team of AI and finance professionals\nproposed FinTruthQA, a benchmark designed to evaluate advanced natural language\nprocessing (NLP) techniques for the automatic quality assessment of information\ndisclosure in financial Q&A data. It comprises 6,000 real-world financial Q&A\nentries and each Q&A was manually annotated based on four key evaluation\ncriteria. We benchmarked various NLP techniques on FinTruthQA, including large\nlanguage models(LLMs). Experiments showed that existing NLP models have strong\npredictive ability for question identification and question relevance tasks,\nbut are suboptimal for answer readability and answer relevance tasks. By\nestablishing this benchmark, we provide a robust foundation for the automatic\nevaluation of information disclosure, demonstrating how AI can be leveraged for\nsocial good by promoting transparency, fairness, and investor protection in\nfinancial disclosure practices. FinTruthQA can be used by auditors, regulators,\nand financial analysts for real-time monitoring and data-driven\ndecision-making, as well as by researchers for advanced studies in accounting\nand finance, ultimately fostering greater trust and efficiency in the financial\nmarkets.", "published": "2024-06-17 18:25:02", "link": "http://arxiv.org/abs/2406.12009v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CItruS: Chunked Instruction-aware State Eviction for Long Sequence\n  Modeling", "abstract": "Long sequence modeling has gained broad interest as large language models\n(LLMs) continue to advance. Recent research has identified that a large portion\nof hidden states within the key-value caches of Transformer models can be\ndiscarded (also termed evicted) without affecting the perplexity performance in\ngenerating long sequences. However, we show that these methods, despite\npreserving perplexity performance, often drop information that is important for\nsolving downstream tasks, a problem which we call information neglect. To\naddress this issue, we introduce Chunked Instruction-aware State Eviction\n(CItruS), a novel modeling technique that integrates the attention preferences\nuseful for a downstream task into the eviction process of hidden states. In\naddition, we design a method for chunked sequence processing to further improve\nefficiency. Our training-free method exhibits superior performance on long\nsequence comprehension and retrieval tasks over several strong baselines under\nthe same memory budget, while preserving language modeling perplexity. The code\nand data have been released at https://github.com/ybai-nlp/CItruS.", "published": "2024-06-17 18:34:58", "link": "http://arxiv.org/abs/2406.12018v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unveiling and Mitigating Bias in Mental Health Analysis with Large\n  Language Models", "abstract": "The advancement of large language models (LLMs) has demonstrated strong\ncapabilities across various applications, including mental health analysis.\nHowever, existing studies have focused on predictive performance, leaving the\ncritical issue of fairness underexplored, posing significant risks to\nvulnerable populations. Despite acknowledging potential biases, previous works\nhave lacked thorough investigations into these biases and their impacts. To\naddress this gap, we systematically evaluate biases across seven social factors\n(e.g., gender, age, religion) using ten LLMs with different prompting methods\non eight diverse mental health datasets. Our results show that GPT-4 achieves\nthe best overall balance in performance and fairness among LLMs, although it\nstill lags behind domain-specific models like MentalRoBERTa in some cases.\nAdditionally, our tailored fairness-aware prompts can effectively mitigate bias\nin mental health predictions, highlighting the great potential for fair\nanalysis in this field.", "published": "2024-06-17 19:05:32", "link": "http://arxiv.org/abs/2406.12033v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learn Beyond The Answer: Training Language Models with Reflection for\n  Mathematical Reasoning", "abstract": "Supervised fine-tuning enhances the problem-solving abilities of language\nmodels across various mathematical reasoning tasks. To maximize such benefits,\nexisting research focuses on broadening the training set with various data\naugmentation techniques, which is effective for standard single-round\nquestion-answering settings. Our work introduces a novel technique aimed at\ncultivating a deeper understanding of the training problems at hand, enhancing\nperformance not only in standard settings but also in more complex scenarios\nthat require reflective thinking. Specifically, we propose reflective\naugmentation, a method that embeds problem reflection into each training\ninstance. It trains the model to consider alternative perspectives and engage\nwith abstractions and analogies, thereby fostering a thorough comprehension\nthrough reflective reasoning. Extensive experiments validate the achievement of\nour aim, underscoring the unique advantages of our method and its complementary\nnature relative to existing augmentation techniques.", "published": "2024-06-17 19:42:22", "link": "http://arxiv.org/abs/2406.12050v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "InternalInspector $I^2$: Robust Confidence Estimation in LLMs through\n  Internal States", "abstract": "Despite their vast capabilities, Large Language Models (LLMs) often struggle\nwith generating reliable outputs, frequently producing high-confidence\ninaccuracies known as hallucinations. Addressing this challenge, our research\nintroduces InternalInspector, a novel framework designed to enhance confidence\nestimation in LLMs by leveraging contrastive learning on internal states\nincluding attention states, feed-forward states, and activation states of all\nlayers. Unlike existing methods that primarily focus on the final activation\nstate, InternalInspector conducts a comprehensive analysis across all internal\nstates of every layer to accurately identify both correct and incorrect\nprediction processes. By benchmarking InternalInspector against existing\nconfidence estimation methods across various natural language understanding and\ngeneration tasks, including factual question answering, commonsense reasoning,\nand reading comprehension, InternalInspector achieves significantly higher\naccuracy in aligning the estimated confidence scores with the correctness of\nthe LLM's predictions and lower calibration error. Furthermore,\nInternalInspector excels at HaluEval, a hallucination detection benchmark,\noutperforming other internal-based confidence estimation methods in this task.", "published": "2024-06-17 19:46:05", "link": "http://arxiv.org/abs/2406.12053v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Models are Surprisingly Fragile to Drug Names in Biomedical\n  Benchmarks", "abstract": "Medical knowledge is context-dependent and requires consistent reasoning\nacross various natural language expressions of semantically equivalent phrases.\nThis is particularly crucial for drug names, where patients often use brand\nnames like Advil or Tylenol instead of their generic equivalents. To study\nthis, we create a new robustness dataset, RABBITS, to evaluate performance\ndifferences on medical benchmarks after swapping brand and generic drug names\nusing physician expert annotations.\n  We assess both open-source and API-based LLMs on MedQA and MedMCQA, revealing\na consistent performance drop ranging from 1-10\\%. Furthermore, we identify a\npotential source of this fragility as the contamination of test data in widely\nused pre-training datasets. All code is accessible at\nhttps://github.com/BittermanLab/RABBITS, and a HuggingFace leaderboard is\navailable at https://huggingface.co/spaces/AIM-Harvard/rabbits-leaderboard.", "published": "2024-06-17 20:09:24", "link": "http://arxiv.org/abs/2406.12066v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Satyrn: A Platform for Analytics Augmented Generation", "abstract": "Large language models (LLMs) are capable of producing documents, and\nretrieval augmented generation (RAG) has shown itself to be a powerful method\nfor improving accuracy without sacrificing fluency. However, not all\ninformation can be retrieved from text. We propose an approach that uses the\nanalysis of structured data to generate fact sets that are used to guide\ngeneration in much the same way that retrieved documents are used in RAG. This\nanalytics augmented generation (AAG) approach supports the ability to utilize\nstandard analytic techniques to generate facts that are then converted to text\nand passed to an LLM. We present a neurosymbolic platform, Satyrn, that\nleverages AAG to produce accurate, fluent, and coherent reports grounded in\nlarge scale databases. In our experiments, we find that Satyrn generates\nreports in which over 86% of claims are accurate while maintaining high levels\nof fluency and coherence, even when using smaller language models such as\nMistral-7B, as compared to GPT-4 Code Interpreter in which just 57% of claims\nare accurate.", "published": "2024-06-17 20:14:16", "link": "http://arxiv.org/abs/2406.12069v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "COMMUNITY-CROSS-INSTRUCT: Unsupervised Instruction Generation for\n  Aligning Large Language Models to Online Communities", "abstract": "Social scientists use surveys to probe the opinions and beliefs of\npopulations, but these methods are slow, costly, and prone to biases. Recent\nadvances in large language models (LLMs) enable the creating of computational\nrepresentations or \"digital twins\" of populations that generate human-like\nresponses mimicking the population's language, styles, and attitudes. We\nintroduce Community-Cross-Instruct, an unsupervised framework for aligning LLMs\nto online communities to elicit their beliefs. Given a corpus of a community's\nonline discussions, Community-Cross-Instruct automatically generates\ninstruction-output pairs by an advanced LLM to (1) finetune a foundational LLM\nto faithfully represent that community, and (2) evaluate the alignment of the\nfinetuned model to the community. We demonstrate the method's utility in\naccurately representing political and diet communities on Reddit. Unlike prior\nmethods requiring human-authored instructions, Community-Cross-Instruct\ngenerates instructions in a fully unsupervised manner, enhancing scalability\nand generalization across domains. This work enables cost-effective and\nautomated surveying of diverse online communities.", "published": "2024-06-17 20:20:47", "link": "http://arxiv.org/abs/2406.12074v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Decoding the Narratives: Analyzing Personal Drug Experiences Shared on\n  Reddit", "abstract": "Online communities such as drug-related subreddits serve as safe spaces for\npeople who use drugs (PWUD), fostering discussions on substance use\nexperiences, harm reduction, and addiction recovery. Users' shared narratives\non these forums provide insights into the likelihood of developing a substance\nuse disorder (SUD) and recovery potential. Our study aims to develop a\nmulti-level, multi-label classification model to analyze online user-generated\ntexts about substance use experiences. For this purpose, we first introduce a\nnovel taxonomy to assess the nature of posts, including their intended\nconnections (Inquisition or Disclosure), subjects (e.g., Recovery, Dependency),\nand specific objectives (e.g., Relapse, Quality, Safety). Using various\nmulti-label classification algorithms on a set of annotated data, we show that\nGPT-4, when prompted with instructions, definitions, and examples, outperformed\nall other models. We apply this model to label an additional 1,000 posts and\nanalyze the categories of linguistic expression used within posts in each\nclass. Our analysis shows that topics such as Safety, Combination of\nSubstances, and Mental Health see more disclosure, while discussions about\nphysiological Effects focus on harm reduction. Our work enriches the\nunderstanding of PWUD's experiences and informs the broader knowledge base on\nSUD and drug use.", "published": "2024-06-17 21:56:57", "link": "http://arxiv.org/abs/2406.12117v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AI \"News\" Content Farms Are Easy to Make and Hard to Detect: A Case\n  Study in Italian", "abstract": "Large Language Models (LLMs) are increasingly used as \"content farm\" models\n(CFMs), to generate synthetic text that could pass for real news articles. This\nis already happening even for languages that do not have high-quality\nmonolingual LLMs. We show that fine-tuning Llama (v1), mostly trained on\nEnglish, on as little as 40K Italian news articles, is sufficient for producing\nnews-like texts that native speakers of Italian struggle to identify as\nsynthetic.\n  We investigate three LLMs and three methods of detecting synthetic texts\n(log-likelihood, DetectGPT, and supervised classification), finding that they\nall perform better than human raters, but they are all impractical in the real\nworld (requiring either access to token likelihood information or a large\ndataset of CFM texts). We also explore the possibility of creating a proxy CFM:\nan LLM fine-tuned on a similar dataset to one used by the real \"content farm\".\nWe find that even a small amount of fine-tuning data suffices for creating a\nsuccessful detector, but we need to know which base LLM is used, which is a\nmajor challenge.\n  Our results suggest that there are currently no practical methods for\ndetecting synthetic news-like texts 'in the wild', while generating them is too\neasy. We highlight the urgency of more NLP research on this problem.", "published": "2024-06-17 22:19:00", "link": "http://arxiv.org/abs/2406.12128v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Gram2Vec: An Interpretable Document Vectorizer", "abstract": "We present Gram2Vec, a grammatical style embedding algorithm that embeds\ndocuments into a higher dimensional space by extracting the normalized relative\nfrequencies of grammatical features present in the text. Compared to neural\napproaches, Gram2Vec offers inherent interpretability based on how the feature\nvectors are generated. In our demo, we present a way to visualize a mapping of\nauthors to documents based on their Gram2Vec vectors and highlight the ability\nto drop or add features to view which authors make certain linguistic choices.\nNext, we use authorship attribution as an application to show how Gram2Vec can\nexplain why a document is attributed to a certain author, using cosine\nsimilarities between the Gram2Vec feature vectors to calculate the distances\nbetween candidate documents and a query document.", "published": "2024-06-17 22:42:14", "link": "http://arxiv.org/abs/2406.12131v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Intentions to Techniques: A Comprehensive Taxonomy and Challenges\n  in Text Watermarking for Large Language Models", "abstract": "With the rapid growth of Large Language Models (LLMs), safeguarding textual\ncontent against unauthorized use is crucial. Text watermarking offers a vital\nsolution, protecting both - LLM-generated and plain text sources. This paper\npresents a unified overview of different perspectives behind designing\nwatermarking techniques, through a comprehensive survey of the research\nliterature. Our work has two key advantages, (1) we analyze research based on\nthe specific intentions behind different watermarking techniques, evaluation\ndatasets used, watermarking addition, and removal methods to construct a\ncohesive taxonomy. (2) We highlight the gaps and open challenges in text\nwatermarking to promote research in protecting text authorship. This extensive\ncoverage and detailed analysis sets our work apart, offering valuable insights\ninto the evolving landscape of text watermarking in language models.", "published": "2024-06-17 00:09:31", "link": "http://arxiv.org/abs/2406.11106v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GoldCoin: Grounding Large Language Models in Privacy Laws via Contextual\n  Integrity Theory", "abstract": "Privacy issues arise prominently during the inappropriate transmission of\ninformation between entities. Existing research primarily studies privacy by\nexploring various privacy attacks, defenses, and evaluations within narrowly\npredefined patterns, while neglecting that privacy is not an isolated,\ncontext-free concept limited to traditionally sensitive data (e.g., social\nsecurity numbers), but intertwined with intricate social contexts that\ncomplicate the identification and analysis of potential privacy violations. The\nadvent of Large Language Models (LLMs) offers unprecedented opportunities for\nincorporating the nuanced scenarios outlined in privacy laws to tackle these\ncomplex privacy issues. However, the scarcity of open-source relevant case\nstudies restricts the efficiency of LLMs in aligning with specific legal\nstatutes. To address this challenge, we introduce a novel framework, GoldCoin,\ndesigned to efficiently ground LLMs in privacy laws for judicial assessing\nprivacy violations. Our framework leverages the theory of contextual integrity\nas a bridge, creating numerous synthetic scenarios grounded in relevant privacy\nstatutes (e.g., HIPAA), to assist LLMs in comprehending the complex contexts\nfor identifying privacy risks in the real world. Extensive experimental results\ndemonstrate that GoldCoin markedly enhances LLMs' capabilities in recognizing\nprivacy risks across real court cases, surpassing the baselines on different\njudicial tasks.", "published": "2024-06-17 02:27:32", "link": "http://arxiv.org/abs/2406.11149v2", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Aligning Large Language Models from Self-Reference AI Feedback with one\n  General Principle", "abstract": "In aligning large language models (LLMs), utilizing feedback from existing\nadvanced AI rather than humans is an important method to scale supervisory\nsignals. However, it is highly challenging for AI to understand human\nintentions and societal values, and provide accurate preference feedback based\non these. Current AI feedback methods rely on powerful LLMs, carefully designed\nspecific principles to describe human intentions, and are easily influenced by\nposition bias. To address these issues, we propose a self-reference-based AI\nfeedback framework that enables a 13B Llama2-Chat to provide high-quality\nfeedback under simple and general principles such as ``best for humanity``.\nSpecifically, we allow the AI to first respond to the user's instructions, then\ngenerate criticism of other answers based on its own response as a reference,\nand finally determine which answer better fits human preferences according to\nthe criticism. Additionally, we use a self-consistency method to further reduce\nthe impact of position bias, and employ semantic perplexity to calculate the\npreference strength differences between different answers. Experimental results\nshow that our method enables 13B and 70B Llama2-Chat annotators to provide\nhigh-quality preference feedback, and the policy models trained based on these\npreference data achieve significant advantages in benchmark datasets through\nreinforcement learning.", "published": "2024-06-17 03:51:46", "link": "http://arxiv.org/abs/2406.11190v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AvaTaR: Optimizing LLM Agents for Tool Usage via Contrastive Reasoning", "abstract": "Large language model (LLM) agents have demonstrated impressive capabilities\nin utilizing external tools and knowledge to boost accuracy and reduce\nhallucinations. However, developing prompting techniques that enable LLM agents\nto effectively use these tools and knowledge remains a heuristic and\nlabor-intensive task. Here, we introduce AvaTaR, a novel and automated\nframework that optimizes an LLM agent to effectively leverage provided tools,\nimproving performance on a given task. During optimization, we design a\ncomparator module to iteratively deliver insightful and comprehensive prompts\nto the LLM agent by contrastively reasoning between positive and negative\nexamples sampled from training data. We demonstrate AvaTaR on four complex\nmultimodal retrieval datasets featuring textual, visual, and relational\ninformation, and three general question-answering (QA) datasets. We find AvaTaR\nconsistently outperforms state-of-the-art approaches across all seven tasks,\nexhibiting strong generalization ability when applied to novel cases and\nachieving an average relative improvement of 14% on the Hit@1 metric for the\nretrieval datasets and 13% for the QA datasets. Code and dataset are available\nat https://github.com/zou-group/avatar.", "published": "2024-06-17 04:20:02", "link": "http://arxiv.org/abs/2406.11200v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Fine-Tuning or Fine-Failing? Debunking Performance Myths in Large\n  Language Models", "abstract": "Large Language Models (LLMs) have the unique capability to understand and\ngenerate human-like text from input queries. When fine-tuned, these models show\nenhanced performance on domain-specific queries. OpenAI highlights the process\nof fine-tuning, stating: \"To fine-tune a model, you are required to provide at\nleast 10 examples. We typically see clear improvements from fine-tuning on 50\nto 100 training examples, but the right number varies greatly based on the\nexact use case.\" This study extends this concept to the integration of LLMs\nwithin Retrieval-Augmented Generation (RAG) pipelines, which aim to improve\naccuracy and relevance by leveraging external corpus data for information\nretrieval. However, RAG's promise of delivering optimal responses often falls\nshort in complex query scenarios. This study aims to specifically examine the\neffects of fine-tuning LLMs on their ability to extract and integrate\ncontextual data to enhance the performance of RAG systems across multiple\ndomains. We evaluate the impact of fine-tuning on the LLMs' capacity for data\nextraction and contextual understanding by comparing the accuracy and\ncompleteness of fine-tuned models against baseline performances across datasets\nfrom multiple domains. Our findings indicate that fine-tuning resulted in a\ndecline in performance compared to the baseline models, contrary to the\nimprovements observed in standalone LLM applications as suggested by OpenAI.\nThis study highlights the need for vigorous investigation and validation of\nfine-tuned models for domain-specific tasks.", "published": "2024-06-17 04:35:17", "link": "http://arxiv.org/abs/2406.11201v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Building another Spanish dictionary, this time with GPT-4", "abstract": "We present the \"Spanish Built Factual Freectianary 2.0\" (Spanish-BFF-2) as\nthe second iteration of an AI-generated Spanish dictionary. Previously, we\ndeveloped the inaugural version of this unique free dictionary employing GPT-3.\nIn this study, we aim to improve the dictionary by using GPT-4-turbo instead.\nFurthermore, we explore improvements made to the initial version and compare\nthe performance of both models.", "published": "2024-06-17 05:25:56", "link": "http://arxiv.org/abs/2406.11218v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MiniConGTS: A Near Ultimate Minimalist Contrastive Grid Tagging Scheme\n  for Aspect Sentiment Triplet Extraction", "abstract": "Aspect Sentiment Triplet Extraction (ASTE) aims to co-extract the sentiment\ntriplets in a given corpus. Existing approaches within the\npretraining-finetuning paradigm tend to either meticulously craft complex\ntagging schemes and classification heads, or incorporate external semantic\naugmentation to enhance performance. In this study, we, for the first time,\nre-evaluate the redundancy in tagging schemes and the internal enhancement in\npretrained representations. We propose a method to improve and utilize\npretrained representations by integrating a minimalist tagging scheme and a\nnovel token-level contrastive learning strategy. The proposed approach\ndemonstrates comparable or superior performance compared to state-of-the-art\ntechniques while featuring a more compact design and reduced computational\noverhead. Additionally, we are the first to formally evaluate GPT-4's\nperformance in few-shot learning and Chain-of-Thought scenarios for this task.\nThe results demonstrate that the pretraining-finetuning paradigm remains highly\neffective even in the era of large language models.", "published": "2024-06-17 06:01:11", "link": "http://arxiv.org/abs/2406.11234v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SilverSpeak: Evading AI-Generated Text Detectors using Homoglyphs", "abstract": "The advent of Large Language Models (LLMs) has enabled the generation of text\nthat increasingly exhibits human-like characteristics. As the detection of such\ncontent is of significant importance, substantial research has been conducted\nwith the objective of developing reliable AI-generated text detectors. These\ndetectors have demonstrated promising results on test data, but recent research\nhas revealed that they can be circumvented by employing different techniques.\n  In this paper, we present homoglyph-based attacks (A $\\rightarrow$ Cyrillic\nA) as a means of circumventing existing detectors. We conduct a comprehensive\nevaluation to assess the effectiveness of these attacks on seven detectors,\nincluding ArguGPT, Binoculars, DetectGPT, Fast-DetectGPT, Ghostbuster, OpenAI's\ndetector, and watermarking techniques, on five different datasets. Our findings\ndemonstrate that homoglyph-based attacks can effectively circumvent\nstate-of-the-art detectors, leading them to classify all texts as either\nAI-generated or human-written (decreasing the average Matthews Correlation\nCoefficient from 0.64 to -0.01). Through further examination, we extract the\ntechnical justification underlying the success of the attacks, which varies\nacross detectors. Finally, we discuss the implications of these findings and\npotential defenses against such attacks.", "published": "2024-06-17 06:07:32", "link": "http://arxiv.org/abs/2406.11239v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FamiCom: Further Demystifying Prompts for Language Models with\n  Task-Agnostic Performance Estimation", "abstract": "Language models have shown impressive in-context-learning capabilities, which\nallow them to benefit from input prompts and perform better on downstream end\ntasks. Existing works investigate the mechanisms behind this observation, and\npropose label-agnostic prompt metrics that can better estimate end-task\nperformances. One popular approach is using perplexity as a way to measure\nmodels' familiarity with the prompt. While showing consistent improvements on\nin-domain tasks, we found that familiarity metrics such as perplexity cannot\naccurately estimate performance in complicated situations such as task or\ndomain transferring scenarios. In this work, we propose a revised measure\ncalled FamiCom, providing a more comprehensive measure for task-agnostic\nperformance estimation. Specifically, FamiCom combines familiarity with\n\\textit{complexity} -- the inherent difficulty of end tasks, which is an\nimportant factor missing from current metrics. Experiments show that FamiCom\nstrongly correlates with end-task performances, producing a 0.85 Spearman's\ncorrelation, versus 0.43 of familiarity-only ones'. We further apply FamiCom to\nautomatic prompt and demonstration selection, and outperform existing methods\nand baselines by more than 7.0% in accuracy.", "published": "2024-06-17 06:14:55", "link": "http://arxiv.org/abs/2406.11243v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Adversarial Style Augmentation via Large Language Model for Robust Fake\n  News Detection", "abstract": "The spread of fake news negatively impacts individuals and is regarded as a\nsignificant social challenge that needs to be addressed. A number of\nalgorithmic and insightful features have been identified for detecting fake\nnews. However, with the recent LLMs and their advanced generation capabilities,\nmany of the detectable features (e.g., style-conversion attacks) can be\naltered, making it more challenging to distinguish from real news. This study\nproposes adversarial style augmentation, AdStyle, to train a fake news detector\nthat remains robust against various style-conversion attacks. Our model's key\nmechanism is the careful use of LLMs to automatically generate a diverse yet\ncoherent range of style-conversion attack prompts. This improves the generation\nof prompts that are particularly difficult for the detector to handle.\nExperiments show that our augmentation strategy improves robustness and\ndetection performance when tested on fake news benchmark datasets.", "published": "2024-06-17 07:00:41", "link": "http://arxiv.org/abs/2406.11260v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Understanding the Collapse of LLMs in Model Editing", "abstract": "Despite significant progress in model editing methods, their application in\nreal-world scenarios remains challenging as they often cause large language\nmodels (LLMs) to collapse. Among them, ROME is particularly concerning, as it\ncould disrupt LLMs with only a single edit. In this paper, we study the root\ncauses of such collapse. Through extensive analysis, we identify two primary\nfactors that contribute to the collapse: i) inconsistent handling of prefixed\nand unprefixed keys in the parameter update equation may result in very small\ndenominators, causing excessively large parameter updates; ii) the subject of\ncollapse cases is usually the first token, whose unprefixed key distribution\nsignificantly differs from the prefixed key distribution in autoregressive\ntransformers, causing the aforementioned issue to materialize. To validate our\nfindings, we propose a simple yet effective approach: uniformly using prefixed\nkeys during editing phase and adding prefixes during testing phase to ensure\nthe consistency between training and testing. The experimental results show\nthat the proposed solution can prevent model collapse while maintaining the\neffectiveness of the edits.", "published": "2024-06-17 07:08:29", "link": "http://arxiv.org/abs/2406.11263v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Self and Cross-Model Distillation for LLMs: Effective Methods for\n  Refusal Pattern Alignment", "abstract": "Large Language Models (LLMs) like OpenAI's GPT series, Anthropic's Claude,\nand Meta's LLaMa have shown remarkable capabilities in text generation.\nHowever, their susceptibility to toxic prompts presents significant security\nchallenges. This paper investigates alignment techniques, including Supervised\nFine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF), to\nmitigate these risks. We conduct an empirical study on refusal patterns across\nnine LLMs, revealing that models with uniform refusal patterns, such as\nClaude3, exhibit higher security. Based on these findings, we propose\nself-distilling and cross-model distilling methods to enhance LLM security. Our\nresults show that these methods significantly improve refusal rates and reduce\nunsafe content, with cross-model distilling achieving refusal rates close to\nClaude3's 94.51%. These findings underscore the potential of distillation-based\nalignment in securing LLMs against toxic prompts.", "published": "2024-06-17 07:46:45", "link": "http://arxiv.org/abs/2406.11285v2", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "MFC-Bench: Benchmarking Multimodal Fact-Checking with Large\n  Vision-Language Models", "abstract": "Large vision-language models (LVLMs) have significantly improved multimodal\nreasoning tasks, such as visual question answering and image captioning. These\nmodels embed multimodal facts within their parameters, rather than relying on\nexternal knowledge bases to store factual information explicitly. However, the\ncontent discerned by LVLMs may deviate from factuality due to inherent bias or\nincorrect inference. To address this issue, we introduce MFC-Bench, a rigorous\nand comprehensive benchmark designed to evaluate the factual accuracy of LVLMs\nacross three stages of verdict prediction for MFC: Manipulation,\nOut-of-Context, and Veracity Classification. Through our evaluation on\nMFC-Bench, we benchmarked a dozen diverse and representative LVLMs, uncovering\nthat current models still fall short in multimodal fact-checking and\ndemonstrate insensitivity to various forms of manipulated content. We hope that\nMFC-Bench could raise attention to the trustworthy AI potentially assisted by\nLVLMs in the future. The MFC-Bench and accompanying resources are publicly\naccessible at https://github.com/wskbest/MFC-Bench, contributing to ongoing\nresearch in the multimodal fact-checking field.", "published": "2024-06-17 07:51:44", "link": "http://arxiv.org/abs/2406.11288v3", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Full-ECE: A Metric For Token-level Calibration on Large Language Models", "abstract": "Deep Neural Networks (DNNs) excel in various domains but face challenges in\nproviding accurate uncertainty estimates, which are crucial for high-stakes\napplications. Large Language Models (LLMs) have recently emerged as powerful\ntools, demonstrating exceptional performance in language tasks. However,\ntraditional calibration metrics such as Expected Calibration Error (ECE) and\nclasswise-ECE (cw-ECE) are inadequate for LLMs due to their vast vocabularies,\ndata complexity, and distributional focus. To address this, we propose a novel\ncalibration concept called full calibration and introduce its corresponding\nmetric, Full-ECE. Full-ECE evaluates the entire predicted probability\ndistribution, offering a more accurate and robust measure of calibration for\nLLMs.", "published": "2024-06-17 09:07:58", "link": "http://arxiv.org/abs/2406.11345v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "$\\texttt{MoE-RBench}$: Towards Building Reliable Language Models with\n  Sparse Mixture-of-Experts", "abstract": "Mixture-of-Experts (MoE) has gained increasing popularity as a promising\nframework for scaling up large language models (LLMs). However, the reliability\nassessment of MoE lags behind its surging applications. Moreover, when\ntransferred to new domains such as in fine-tuning MoE models sometimes\nunderperform their dense counterparts. Motivated by the research gap and\ncounter-intuitive phenomenon, we propose $\\texttt{MoE-RBench}$, the first\ncomprehensive assessment of SMoE reliability from three aspects: $\\textit{(i)}$\nsafety and hallucination, $\\textit{(ii)}$ resilience to adversarial attacks,\nand $\\textit{(iii)}$ out-of-distribution robustness. Extensive models and\ndatasets are tested to compare the MoE to dense networks from these reliability\ndimensions. Our empirical observations suggest that with appropriate\nhyperparameters, training recipes, and inference techniques, we can build the\nMoE model more reliably than the dense LLM. In particular, we find that the\nrobustness of SMoE is sensitive to the basic training settings. We hope that\nthis study can provide deeper insights into how to adapt the pre-trained MoE\nmodel to other tasks with higher-generation security, quality, and stability.\nCodes are available at https://github.com/UNITES-Lab/MoE-RBench", "published": "2024-06-17 09:17:05", "link": "http://arxiv.org/abs/2406.11353v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Refiner: Restructure Retrieval Content Efficiently to Advance\n  Question-Answering Capabilities", "abstract": "Large Language Models (LLMs) are limited by their parametric knowledge,\nleading to hallucinations in knowledge-extensive tasks. To address this,\nRetrieval-Augmented Generation (RAG) incorporates external document chunks to\nexpand LLM knowledge. Furthermore, compressing information from document chunks\nthrough extraction or summarization can improve LLM performance. Nonetheless,\nLLMs still struggle to notice and utilize scattered key information, a problem\nknown as the \"lost-in-the-middle\" syndrome. Therefore, we typically need to\nrestructure the content for LLM to recognize the key information. We propose\n$\\textit{Refiner}$, an end-to-end extract-and-restructure paradigm that\noperates in the post-retrieval process of RAG. $\\textit{Refiner}$ leverages a\nsingle decoder-only LLM to adaptively extract query-relevant contents verbatim\nalong with the necessary context, and section them based on their\ninterconnectedness, thereby highlights information distinction, and aligns\ndownstream LLMs with the original context effectively. Experiments show that a\ntrained $\\textit{Refiner}$ (with 7B parameters) exhibits significant gain to\ndownstream LLM in improving answer accuracy, and outperforms other\nstate-of-the-art advanced RAG and concurrent compressing approaches in various\nsingle-hop and multi-hop QA tasks. Notably, $\\textit{Refiner}$ achieves a 80.5%\ntokens reduction and a 1.6-7.0% improvement margin in multi-hop tasks compared\nto the next best solution. $\\textit{Refiner}$ is a plug-and-play solution that\ncan be seamlessly integrated with RAG systems, facilitating its application\nacross diverse open-source frameworks.", "published": "2024-06-17 09:25:10", "link": "http://arxiv.org/abs/2406.11357v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Boosting Scientific Concepts Understanding: Can Analogy from Teacher\n  Models Empower Student Models?", "abstract": "Analogical reasoning plays a critical role in human cognition, enabling us to\nunderstand new concepts by associating them with familiar ones. Previous\nresearch in the AI community has mainly focused on identifying and generating\nanalogies and then examining their quality under human evaluation, which\noverlooks the practical application of these analogies in real-world settings.\nInspired by the human education process, in this paper, we propose to\ninvestigate how analogies created by teacher language models (LMs) can assist\nstudent LMs in understanding scientific concepts, thereby aligning more closely\nwith practical scenarios. Our results suggest that free-form analogies can\nindeed aid LMs in understanding concepts. Additionally, analogies generated by\nstudent LMs can improve their own performance on scientific question answering,\ndemonstrating their capability to use analogies for self-learning new\nknowledge. Resources are available at https://github.com/siyuyuan/SCUA.", "published": "2024-06-17 09:51:38", "link": "http://arxiv.org/abs/2406.11375v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Language Models and Knowledge Graphs for Astronomical Entity\n  Disambiguation", "abstract": "This paper presents an experiment conducted during a hackathon, focusing on\nusing large language models (LLMs) and knowledge graph clustering to extract\nentities and relationships from astronomical text. The study demonstrates an\napproach to disambiguate entities that can appear in various contexts within\nthe astronomical domain. By collecting excerpts around specific entities and\nleveraging the GPT-4 language model, relevant entities and relationships are\nextracted. The extracted information is then used to construct a knowledge\ngraph, which is clustered using the Leiden algorithm. The resulting Leiden\ncommunities are utilized to identify the percentage of association of unknown\nexcerpts to each community, thereby enabling disambiguation. The experiment\nshowcases the potential of combining LLMs and knowledge graph clustering\ntechniques for information extraction in astronomical research. The results\nhighlight the effectiveness of the approach in identifying and disambiguating\nentities, as well as grouping them into meaningful clusters based on their\nrelationships.", "published": "2024-06-17 10:38:03", "link": "http://arxiv.org/abs/2406.11400v1", "categories": ["cs.CL", "astro-ph.IM"], "primary_category": "cs.CL"}
{"title": "Multimodal Structured Generation: CVPR's 2nd MMFM Challenge Technical\n  Report", "abstract": "Multimodal Foundation Models (MMFMs) have demonstrated strong performance in\nboth computer vision and natural language processing tasks. However, their\nperformance diminishes in tasks that require a high degree of integration\nbetween these modalities, such as document understanding. Moreover, finetuning\nthese models and deploying them requires significantly more compute and more\nengineering effort than unimodal models. In this work, we present Multimodal\nStructured Generation, a framework that forces (frozen) MMFMs to produce\noutputs in a strictly structured format by applying hard constraints directly\nto the output logits. This approach not only ensures that the model generates\nparseable outputs that downstream APIs can easily ingest but also allows us to\nforce the model to reason before answering, which significantly boosts\nperformance without the need for expensive fine-tuning. We demonstrate the\neffectiveness of our method through competitive results in the CVPR 2nd MMFM\nChallenge, highlighting that carefully designed lightweight engineering can\noutperform expensive and complicated modeling approaches. All of our scripts,\ndeployment steps, and evaluation results can be accessed in\nhttps://github.com/leloykun/MMFM-Challenge", "published": "2024-06-17 10:45:47", "link": "http://arxiv.org/abs/2406.11403v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "CodeGemma: Open Code Models Based on Gemma", "abstract": "This paper introduces CodeGemma, a collection of specialized open code models\nbuilt on top of Gemma, capable of a variety of code and natural language\ngeneration tasks. We release three model variants. CodeGemma 7B pretrained (PT)\nand instruction-tuned (IT) variants have remarkably resilient natural language\nunderstanding, excel in mathematical reasoning, and match code capabilities of\nother open models. CodeGemma 2B is a state-of-the-art code completion model\ndesigned for fast code infilling and open-ended generation in latency-sensitive\nsettings.", "published": "2024-06-17 10:54:35", "link": "http://arxiv.org/abs/2406.11409v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "HARE: HumAn pRiors, a key to small language model Efficiency", "abstract": "Human priors play a crucial role in efficiently utilizing data in deep\nlearning. However, with the development of large language models (LLMs), there\nis an increasing emphasis on scaling both model size and data volume, which\noften diminishes the importance of human priors in data construction.\nInfluenced by these trends, existing Small Language Models (SLMs) mainly rely\non web-scraped large-scale training data, neglecting the proper incorporation\nof human priors. This oversight limits the training efficiency of language\nmodels in resource-constrained settings. In this paper, we propose a principle\nto leverage human priors for data construction. This principle emphasizes\nachieving high-performance SLMs by training on a concise dataset that\naccommodates both semantic diversity and data quality consistency, while\navoiding benchmark data leakage. Following this principle, we train an SLM\nnamed HARE-1.1B. Extensive experiments on large-scale benchmark datasets\ndemonstrate that HARE-1.1B performs favorably against state-of-the-art SLMs,\nvalidating the effectiveness of the proposed principle. Additionally, this\nprovides new insights into efficient language model training in\nresource-constrained environments from the view of human priors.", "published": "2024-06-17 10:56:03", "link": "http://arxiv.org/abs/2406.11410v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluating the Efficacy of Open-Source LLMs in Enterprise-Specific RAG\n  Systems: A Comparative Study of Performance and Scalability", "abstract": "This paper presents an analysis of open-source large language models (LLMs)\nand their application in Retrieval-Augmented Generation (RAG) tasks, specific\nfor enterprise-specific data sets scraped from their websites. With the\nincreasing reliance on LLMs in natural language processing, it is crucial to\nevaluate their performance, accessibility, and integration within specific\norganizational contexts. This study examines various open-source LLMs, explores\ntheir integration into RAG frameworks using enterprise-specific data, and\nassesses the performance of different open-source embeddings in enhancing the\nretrieval and generation process. Our findings indicate that open-source LLMs,\ncombined with effective embedding techniques, can significantly improve the\naccuracy and efficiency of RAG systems, offering a viable alternative to\nproprietary solutions for enterprises.", "published": "2024-06-17 11:22:25", "link": "http://arxiv.org/abs/2406.11424v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Fusion Makes Perfection: An Efficient Multi-Grained Matching Approach\n  for Zero-Shot Relation Extraction", "abstract": "Predicting unseen relations that cannot be observed during the training phase\nis a challenging task in relation extraction. Previous works have made progress\nby matching the semantics between input instances and label descriptions.\nHowever, fine-grained matching often requires laborious manual annotation, and\nrich interactions between instances and label descriptions come with\nsignificant computational overhead. In this work, we propose an efficient\nmulti-grained matching approach that uses virtual entity matching to reduce\nmanual annotation cost, and fuses coarse-grained recall and fine-grained\nclassification for rich interactions with guaranteed inference speed.\nExperimental results show that our approach outperforms the previous State Of\nThe Art (SOTA) methods, and achieves a balance between inference efficiency and\nprediction accuracy in zero-shot relation extraction tasks. Our code is\navailable at https://github.com/longls777/EMMA.", "published": "2024-06-17 11:31:48", "link": "http://arxiv.org/abs/2406.11429v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression", "abstract": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability.", "published": "2024-06-17 11:35:16", "link": "http://arxiv.org/abs/2406.11430v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Super(ficial)-alignment: Strong Models May Deceive Weak Models in\n  Weak-to-Strong Generalization", "abstract": "Superalignment, where humans act as weak supervisors for superhuman models,\nhas become a crucial problem with the rapid development of Large Language\nModels (LLMs). Recent work has preliminarily studied this problem by using weak\nmodels to supervise strong models, and discovered that weakly supervised strong\nstudents can consistently outperform weak teachers towards the alignment\ntarget, leading to a weak-to-strong generalization phenomenon. However, we are\nconcerned that behind such a promising phenomenon, whether there exists an\nissue of weak-to-strong deception, where strong models deceive weak models by\nexhibiting well-aligned in areas known to weak models but producing misaligned\nbehaviors in cases weak models do not know. We take an initial step towards\nexploring this security issue in a specific but realistic multi-objective\nalignment case, where there may be some alignment targets conflicting with each\nother (e.g., helpfulness v.s. harmlessness). We aim to explore whether, in such\ncases, strong models might deliberately make mistakes in areas known to them\nbut unknown to weak models within one alignment dimension, in exchange for a\nhigher reward in another dimension. Through extensive experiments in both the\nreward modeling and preference optimization scenarios, we find: (1) The\nweak-to-strong deception phenomenon exists across all settings. (2) The\ndeception intensifies as the capability gap between weak and strong models\nincreases. (3) Bootstrapping with an intermediate model can mitigate the\ndeception to some extent, though its effectiveness remains limited. Our work\nhighlights the urgent need to pay more attention to the true reliability of\nsuperalignment.", "published": "2024-06-17 11:36:39", "link": "http://arxiv.org/abs/2406.11431v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Adaptive Reinforcement Learning Planning: Harnessing Large Language\n  Models for Complex Information Extraction", "abstract": "Existing research on large language models (LLMs) shows that they can solve\ninformation extraction tasks through multi-step planning. However, their\nextraction behavior on complex sentences and tasks is unstable, emerging issues\nsuch as false positives and missing elements. We observe that decomposing\ncomplex extraction tasks and extracting them step by step can effectively\nimprove LLMs' performance, and the extraction orders of entities significantly\naffect the final results of LLMs. This paper proposes a two-stage multi-step\nmethod for LLM-based information extraction and adopts the RL framework to\nexecute the multi-step planning. We regard sequential extraction as a Markov\ndecision process, build an LLM-based extraction environment, design a decision\nmodule to adaptively provide the optimal order for sequential entity extraction\non different sentences, and utilize the DDQN algorithm to train the decision\nmodel. We also design the rewards and evaluation metrics suitable for the\nextraction results of LLMs. We conduct extensive experiments on multiple public\ndatasets to demonstrate the effectiveness of our method in improving the\ninformation extraction capabilities of LLMs.", "published": "2024-06-17 12:11:01", "link": "http://arxiv.org/abs/2406.11455v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TRACE the Evidence: Constructing Knowledge-Grounded Reasoning Chains for\n  Retrieval-Augmented Generation", "abstract": "Retrieval-augmented generation (RAG) offers an effective approach for\naddressing question answering (QA) tasks. However, the imperfections of the\nretrievers in RAG models often result in the retrieval of irrelevant\ninformation, which could introduce noises and degrade the performance,\nespecially when handling multi-hop questions that require multiple steps of\nreasoning. To enhance the multi-hop reasoning ability of RAG models, we propose\nTRACE. TRACE constructs knowledge-grounded reasoning chains, which are a series\nof logically connected knowledge triples, to identify and integrate supporting\nevidence from the retrieved documents for answering questions. Specifically,\nTRACE employs a KG Generator to create a knowledge graph (KG) from the\nretrieved documents, and then uses an Autoregressive Reasoning Chain\nConstructor to build reasoning chains. Experimental results on three multi-hop\nQA datasets show that TRACE achieves an average performance improvement of up\nto 14.03% compared to using all the retrieved documents. Moreover, the results\nindicate that using reasoning chains as context, rather than the entire\ndocuments, is often sufficient to correctly answer questions.", "published": "2024-06-17 12:23:32", "link": "http://arxiv.org/abs/2406.11460v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Promises, Outlooks and Challenges of Diffusion Language Modeling", "abstract": "The modern autoregressive Large Language Models (LLMs) have achieved\noutstanding performance on NLP benchmarks, and they are deployed in the real\nworld. However, they still suffer from limitations of the autoregressive\ntraining paradigm. For example, autoregressive token generation is notably slow\nand can be prone to \\textit{exposure bias}. The diffusion-based language models\nwere proposed as an alternative to autoregressive generation to address some of\nthese limitations. We evaluate the recently proposed Score Entropy Discrete\nDiffusion (SEDD) approach and show it is a promising alternative to\nautoregressive generation but it has some short-comings too. We empirically\ndemonstrate the advantages and challenges of SEDD, and observe that SEDD\ngenerally matches autoregressive models in perplexity and on benchmarks such as\nHellaSwag, Arc or WinoGrande. Additionally, we show that in terms of inference\nlatency, SEDD can be up to 4.5$\\times$ more efficient than GPT-2. While SEDD\nallows conditioning on tokens at abitrary positions, SEDD appears slightly\nweaker than GPT-2 for conditional generation given short prompts. Finally, we\nreproduced the main results from the original SEDD paper.", "published": "2024-06-17 12:38:38", "link": "http://arxiv.org/abs/2406.11473v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "How Far Can In-Context Alignment Go? Exploring the State of In-Context\n  Alignment", "abstract": "Recent studies have demonstrated that In-Context Learning (ICL), through the\nuse of specific demonstrations, can align Large Language Models (LLMs) with\nhuman preferences known as In-Context Alignment (ICA), indicating that models\ncan comprehend human instructions without requiring parameter adjustments.\nHowever, the exploration of the mechanism and applicability of ICA remains\nlimited. In this paper, we begin by dividing the context text used in ICA into\nthree categories: format, system prompt, and example. Through ablation\nexperiments, we investigate the effectiveness of each part in enabling ICA to\nfunction effectively. We then examine how variants in these parts impact the\nmodel's alignment performance. Our findings indicate that the example part is\ncrucial for enhancing the model's alignment capabilities, with changes in\nexamples significantly affecting alignment performance. We also conduct a\ncomprehensive evaluation of ICA's zero-shot capabilities in various alignment\ntasks. The results indicate that compared to parameter fine-tuning methods, ICA\ndemonstrates superior performance in knowledge-based tasks and tool-use tasks.\nHowever, it still exhibits certain limitations in areas such as multi-turn\ndialogues and instruction following.", "published": "2024-06-17 12:38:48", "link": "http://arxiv.org/abs/2406.11474v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "How Can We Effectively Expand the Vocabulary of LLMs with 0.01GB of\n  Target Language Text?", "abstract": "Large language models (LLMs) have shown remarkable capabilities in many\nlanguages beyond English. Yet, LLMs require more inference steps when\ngenerating non-English text due to their reliance on English-centric tokenizers\nand vocabulary, resulting in higher usage costs to non-English speakers.\nVocabulary expansion with target language tokens is a widely used cross-lingual\nvocabulary adaptation approach to remedy this issue. Despite its effectiveness\nin inference speedup, previous work on vocabulary expansion has focused on\nhigh-resource settings assuming access to a substantial amount of target\nlanguage data to effectively initialize the embeddings of the new tokens and\nadapt the LLM to the target language. However, vocabulary expansion in\nlow-resource settings has yet to be explored. In this paper, we investigate\nvocabulary expansion in low-resource settings by considering embedding\ninitialization methods and continual pre-training strategies. Through extensive\nexperiments across typologically diverse languages, tasks and models, we\nestablish a set of strategies to perform vocabulary expansion for faster\ninference, maintaining competitive downstream performance to baselines with\nonly 30K sentences ($\\sim$0.01GB text data) from the target language.", "published": "2024-06-17 12:42:34", "link": "http://arxiv.org/abs/2406.11477v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Analysing zero-shot temporal relation extraction on clinical notes using\n  temporal consistency", "abstract": "This paper presents the first study for temporal relation extraction in a\nzero-shot setting focusing on biomedical text. We employ two types of prompts\nand five LLMs (GPT-3.5, Mixtral, Llama 2, Gemma, and PMC-LLaMA) to obtain\nresponses about the temporal relations between two events. Our experiments\ndemonstrate that LLMs struggle in the zero-shot setting performing worse than\nfine-tuned specialized models in terms of F1 score, showing that this is a\nchallenging task for LLMs. We further contribute a novel comprehensive temporal\nanalysis by calculating consistency scores for each LLM. Our findings reveal\nthat LLMs face challenges in providing responses consistent to the temporal\nproperties of uniqueness and transitivity. Moreover, we study the relation\nbetween the temporal consistency of an LLM and its accuracy and whether the\nlatter can be improved by solving temporal inconsistencies. Our analysis shows\nthat even when temporal consistency is achieved, the predictions can remain\ninaccurate.", "published": "2024-06-17 12:53:21", "link": "http://arxiv.org/abs/2406.11486v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "GeoGPT4V: Towards Geometric Multi-modal Large Language Models with\n  Geometric Image Generation", "abstract": "Large language models have seen widespread adoption in math problem-solving.\nHowever, in geometry problems that usually require visual aids for better\nunderstanding, even the most advanced multi-modal models currently still face\nchallenges in effectively using image information. High-quality data is crucial\nfor enhancing the geometric capabilities of multi-modal models, yet existing\nopen-source datasets and related efforts are either too challenging for direct\nmodel learning or suffer from misalignment between text and images. To overcome\nthis issue, we introduce a novel pipeline that leverages GPT-4 and GPT-4V to\ngenerate relatively basic geometry problems with aligned text and images,\nfacilitating model learning. We have produced a dataset of 4.9K geometry\nproblems and combined it with 19K open-source data to form our GeoGPT4V\ndataset. Experimental results demonstrate that the GeoGPT4V dataset\nsignificantly improves the geometry performance of various models on the\nMathVista and MathVision benchmarks. The code is available at\nhttps://github.com/Lanyu0303/GeoGPT4V_Project", "published": "2024-06-17 13:04:27", "link": "http://arxiv.org/abs/2406.11503v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Input Conditioned Graph Generation for Language Agents", "abstract": "Recent progress in Large Language Models (LLMs) and language agents has\ndemonstrated significant promise for various future applications across\nmultiple disciplines. While traditional approaches to language agents often\nrely on fixed, handcrafted designs, our research aims to develop both learnable\nand dynamic agents. Our method uses an existing framework that abstracts\nlanguage agents as graphs. Within this graph framework, we aim to learn a model\nthat can generate edges for every given input to the language agent. This\nallows us to generate edges that represent the flow of communication within the\ngraph based on the given input, thereby adjusting the internal communication of\na language agent. We learn to generate these edges using a pretrained LLM that\nis fine-tuned with reinforcement learning. This LLM can be fine-tuned on\nseveral datasets simultaneously, and we hypothesize that the model learns to\nadapt to these different domains during training, achieving good overall\nperformance when encountering data from different domains during deployment. We\ndemonstrate that our approach surpasses the previous static approach by nearly\n6% accuracy on a combined dataset of MMLU and CMMLU, and by more than 10% when\ntrained with a sparsity-inducing loss. It also performs superior in additional\nexperiments conducted with the MMLU and Mini Crossword Puzzles datasets. The\ncode is available at https://github.com/lukasVierling/DynamicGPTSwarm.", "published": "2024-06-17 13:53:15", "link": "http://arxiv.org/abs/2406.11555v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Extrinsic Evaluation of Cultural Competence in Large Language Models", "abstract": "Productive interactions between diverse users and language technologies\nrequire outputs from the latter to be culturally relevant and sensitive. Prior\nworks have evaluated models' knowledge of cultural norms, values, and\nartifacts, without considering how this knowledge manifests in downstream\napplications. In this work, we focus on extrinsic evaluation of cultural\ncompetence in two text generation tasks, open-ended question answering and\nstory generation. We quantitatively and qualitatively evaluate model outputs\nwhen an explicit cue of culture, specifically nationality, is perturbed in the\nprompts. Although we find that model outputs do vary when varying nationalities\nand feature culturally relevant words, we also find weak correlations between\ntext similarity of outputs for different countries and the cultural values of\nthese countries. Finally, we discuss important considerations in designing\ncomprehensive evaluation of cultural competence in user-facing tasks.", "published": "2024-06-17 14:03:27", "link": "http://arxiv.org/abs/2406.11565v3", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Mathematical Entities: Corpora and Benchmarks", "abstract": "Mathematics is a highly specialized domain with its own unique set of\nchallenges. Despite this, there has been relatively little research on natural\nlanguage processing for mathematical texts, and there are few mathematical\nlanguage resources aimed at NLP. In this paper, we aim to provide annotated\ncorpora that can be used to study the language of mathematics in different\ncontexts, ranging from fundamental concepts found in textbooks to advanced\nresearch mathematics. We preprocess the corpora with a neural parsing model and\nsome manual intervention to provide part-of-speech tags, lemmas, and dependency\ntrees. In total, we provide 182397 sentences across three corpora. We then aim\nto test and evaluate several noteworthy natural language processing models\nusing these corpora, to show how well they can adapt to the domain of\nmathematics and provide useful tools for exploring mathematical language. We\nevaluate several neural and symbolic models against benchmarks that we extract\nfrom the corpus metadata to show that terminology extraction and definition\nextraction do not easily generalize to mathematics, and that additional work is\nneeded to achieve good performance on these metrics. Finally, we provide a\nlearning assistant that grants access to the content of these corpora in a\ncontext-sensitive manner, utilizing text search and entity linking. Though our\ncorpora and benchmarks provide useful metrics for evaluating mathematical\nlanguage processing, further work is necessary to adapt models to mathematics\nin order to provide more effective learning assistants and apply NLP methods to\ndifferent mathematical domains.", "published": "2024-06-17 14:11:00", "link": "http://arxiv.org/abs/2406.11577v1", "categories": ["cs.CL", "math.HO"], "primary_category": "cs.CL"}
{"title": "Understanding \"Democratization\" in NLP and ML Research", "abstract": "Recent improvements in natural language processing (NLP) and machine learning\n(ML) and increased mainstream adoption have led to researchers frequently\ndiscussing the \"democratization\" of artificial intelligence. In this paper, we\nseek to clarify how democratization is understood in NLP and ML publications,\nthrough large-scale mixed-methods analyses of papers using the keyword\n\"democra*\" published in NLP and adjacent venues. We find that democratization\nis most frequently used to convey (ease of) access to or use of technologies,\nwithout meaningfully engaging with theories of democratization, while research\nusing other invocations of \"democra*\" tends to be grounded in theories of\ndeliberation and debate. Based on our findings, we call for researchers to\nenrich their use of the term democratization with appropriate theory, towards\ndemocratic technologies beyond superficial access.", "published": "2024-06-17 14:47:06", "link": "http://arxiv.org/abs/2406.11598v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Intrinsic Evaluation of Unlearning Using Parametric Knowledge Traces", "abstract": "The task of \"unlearning\" certain concepts in large language models (LLMs) has\nattracted immense attention recently, due to its importance in mitigating\nundesirable model behaviours, such as the generation of harmful, private, or\nincorrect information. Current protocols to evaluate unlearning methods largely\nrely on behavioral tests, without monitoring the presence of unlearned\nknowledge within the model's parameters. This residual knowledge can be\nadversarially exploited to recover the erased information post-unlearning. We\nargue that unlearning should also be evaluated internally, by considering\nchanges in the parametric knowledge traces of the unlearned concepts. To this\nend, we propose a general evaluation methodology that leverages vocabulary\nprojections to inspect concepts encoded in model parameters. We use this\napproach to localize \"concept vectors\" - parameter vectors that encode concrete\nconcepts - and construct ConceptVectors, a benchmark dataset containing\nhundreds of common concepts and their parametric knowledge traces within two\nopen-source LLMs. Evaluation on ConceptVectors shows that existing unlearning\nmethods minimally impact concept vectors and mostly suppress them during\ninference, while directly ablating these vectors demonstrably removes the\nassociated knowledge and significantly reduces the model's susceptibility to\nadversarial manipulation. Our results highlight limitations in behavioral-based\nunlearning evaluations and call for future work to include parameter-based\nevaluations. To support this, we release our code and benchmark at\nhttps://github.com/yihuaihong/ConceptVectors.", "published": "2024-06-17 15:00:35", "link": "http://arxiv.org/abs/2406.11614v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unveiling the Power of Source: Source-based Minimum Bayes Risk Decoding\n  for Neural Machine Translation", "abstract": "Maximum a posteriori decoding, a commonly used method for neural machine\ntranslation (NMT), aims to maximize the estimated posterior probability.\nHowever, high estimated probability does not always lead to high translation\nquality. Minimum Bayes Risk (MBR) decoding (\\citealp{kumar2004minimum}) offers\nan alternative by seeking hypotheses with the highest expected utility.\n  Inspired by Quality Estimation (QE) reranking which uses the QE model as a\nranker (\\citealp{fernandes-etal-2022-quality}), we propose source-based MBR\n(sMBR) decoding, a novel approach that utilizes quasi-sources (generated via\nparaphrasing or back-translation) as ``support hypotheses'' and a\nreference-free quality estimation metric as the utility function, marking the\nfirst work to solely use sources in MBR decoding. Experiments show that sMBR\noutperforms QE reranking and the standard MBR decoding. Our findings suggest\nthat sMBR is a promising approach for NMT decoding.", "published": "2024-06-17 15:13:52", "link": "http://arxiv.org/abs/2406.11632v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Base-Rate Effect on LLM Benchmark Performance: Disambiguating\n  Test-Taking Strategies from Benchmark Performance", "abstract": "Cloze testing is a common method for measuring the behavior of large language\nmodels on a number of benchmark tasks. Using the MMLU dataset, we show that the\nbase-rate probability (BRP) differences across answer tokens are significant\nand affect task performance ie. guess A if uncertain. We find that\ncounterfactual prompting does sufficiently mitigate the BRP effect. The BRP\neffect is found to have a similar effect to test taking strategies employed by\nhumans leading to the conflation of task performance and test-taking ability.\nWe propose the Nvr-X-MMLU task, a variation of MMLU, which helps to\ndisambiguate test-taking ability from task performance and reports the latter.", "published": "2024-06-17 15:14:10", "link": "http://arxiv.org/abs/2406.11634v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Can LLM be a Personalized Judge?", "abstract": "Ensuring that large language models (LLMs) reflect diverse user values and\npreferences is crucial as their user bases expand globally. It is therefore\nencouraging to see the growing interest in LLM personalization within the\nresearch community. However, current works often rely on the LLM-as-a-Judge\napproach for evaluation without thoroughly examining its validity. In this\npaper, we investigate the reliability of LLM-as-a-Personalized-Judge, asking\nLLMs to judge user preferences based on personas. Our findings suggest that\ndirectly applying LLM-as-a-Personalized-Judge is less reliable than previously\nassumed, showing low and inconsistent agreement with human ground truth. The\npersonas typically used are often overly simplistic, resulting in low\npredictive power. To address these issues, we introduce verbal uncertainty\nestimation into the LLM-as-a-Personalized-Judge pipeline, allowing the model to\nexpress low confidence on uncertain judgments. This adjustment leads to much\nhigher agreement (above 80%) on high-certainty samples for binary tasks.\nThrough human evaluation, we find that the LLM-as-a-Personalized-Judge achieves\ncomparable performance to third-party humans evaluation and even surpasses\nhuman performance on high-certainty samples. Our work indicates that\ncertainty-enhanced LLM-as-a-Personalized-Judge offers a promising direction for\ndeveloping more reliable and scalable methods for evaluating LLM\npersonalization.", "published": "2024-06-17 15:41:30", "link": "http://arxiv.org/abs/2406.11657v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Benchmarking of LLM Detection: Comparing Two Competing Approaches", "abstract": "This article gives an overview of the field of LLM text recognition.\nDifferent approaches and implemented detectors for the recognition of\nLLM-generated text are presented. In addition to discussing the\nimplementations, the article focuses on benchmarking the detectors. Although\nthere are numerous software products for the recognition of LLM-generated text,\nwith a focus on ChatGPT-like LLMs, the quality of the recognition (recognition\nrate) is not clear. Furthermore, while it can be seen that scientific\ncontributions presenting their novel approaches strive for some kind of\ncomparison with other approaches, the construction and independence of the\nevaluation dataset is often not comprehensible. As a result, discrepancies in\nthe performance evaluation of LLM detectors are often visible due to the\ndifferent benchmarking datasets. This article describes the creation of an\nevaluation dataset and uses this dataset to investigate the different\ndetectors. The selected detectors are benchmarked against each other.", "published": "2024-06-17 15:51:46", "link": "http://arxiv.org/abs/2406.11670v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TourRank: Utilizing Large Language Models for Documents Ranking with a\n  Tournament-Inspired Strategy", "abstract": "Large Language Models (LLMs) are increasingly employed in zero-shot documents\nranking, yielding commendable results. However, several significant challenges\nstill persist in LLMs for ranking: (1) LLMs are constrained by limited input\nlength, precluding them from processing a large number of documents\nsimultaneously; (2) The output document sequence is influenced by the input\norder of documents, resulting in inconsistent ranking outcomes; (3) Achieving a\nbalance between cost and ranking performance is challenging. To tackle these\nissues, we introduce a novel documents ranking method called TourRank, which is\ninspired by the sport tournaments, such as FIFA World Cup. Specifically, we 1)\novercome the limitation in input length and reduce the ranking latency by\nincorporating a multi-stage grouping strategy similar to the parallel group\nstage of sport tournaments; 2) improve the ranking performance and robustness\nto input orders by using a points system to ensemble multiple ranking results.\nWe test TourRank with different LLMs on the TREC DL datasets and the BEIR\nbenchmark. The experimental results demonstrate that TourRank delivers\nstate-of-the-art performance at a modest cost. The code of TourRank can be seen\non https://github.com/chenyiqun/TourRank.", "published": "2024-06-17 15:58:22", "link": "http://arxiv.org/abs/2406.11678v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "R-Eval: A Unified Toolkit for Evaluating Domain Knowledge of Retrieval\n  Augmented Large Language Models", "abstract": "Large language models have achieved remarkable success on general NLP tasks,\nbut they may fall short for domain-specific problems. Recently, various\nRetrieval-Augmented Large Language Models (RALLMs) are proposed to address this\nshortcoming. However, existing evaluation tools only provide a few baselines\nand evaluate them on various domains without mining the depth of domain\nknowledge. In this paper, we address the challenges of evaluating RALLMs by\nintroducing the R-Eval toolkit, a Python toolkit designed to streamline the\nevaluation of different RAG workflows in conjunction with LLMs. Our toolkit,\nwhich supports popular built-in RAG workflows and allows for the incorporation\nof customized testing data on the specific domain, is designed to be\nuser-friendly, modular, and extensible. We conduct an evaluation of 21 RALLMs\nacross three task levels and two representative domains, revealing significant\nvariations in the effectiveness of RALLMs across different tasks and domains.\nOur analysis emphasizes the importance of considering both task and domain\nrequirements when choosing a RAG workflow and LLM combination. We are committed\nto continuously maintaining our platform at https://github.com/THU-KEG/R-Eval\nto facilitate both the industry and the researchers.", "published": "2024-06-17 15:59:49", "link": "http://arxiv.org/abs/2406.11681v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Instruct, Not Assist: LLM-based Multi-Turn Planning and Hierarchical\n  Questioning for Socratic Code Debugging", "abstract": "Socratic questioning is an effective teaching strategy, encouraging critical\nthinking and problem-solving. The conversational capabilities of large language\nmodels (LLMs) show great potential for providing scalable, real-time student\nguidance. However, current LLMs often give away solutions directly, making them\nineffective instructors. We tackle this issue in the code debugging domain with\nTreeInstruct, an Instructor agent guided by a novel state space-based planning\nalgorithm. TreeInstruct asks probing questions to help students independently\nidentify and resolve errors. It estimates a student's conceptual and\nsyntactical knowledge to dynamically construct a question tree based on their\nresponses and current knowledge state, effectively addressing both independent\nand dependent mistakes concurrently in a multi-turn interaction setting. In\naddition to using an existing single-bug debugging benchmark, we construct a\nmore challenging multi-bug dataset of 150 coding problems, incorrect solutions,\nand bug fixes -- all carefully constructed and annotated by experts. Extensive\nevaluation shows TreeInstruct's state-of-the-art performance on both datasets,\nproving it to be a more effective instructor than baselines. Furthermore, a\nreal-world case study with five students of varying skill levels further\ndemonstrates TreeInstruct's ability to guide students to debug their code\nefficiently with minimal turns and highly Socratic questioning.", "published": "2024-06-17 16:28:21", "link": "http://arxiv.org/abs/2406.11709v4", "categories": ["cs.CL", "cs.MA"], "primary_category": "cs.CL"}
{"title": "1000 African Voices: Advancing inclusive multi-speaker multi-accent\n  speech synthesis", "abstract": "Recent advances in speech synthesis have enabled many useful applications\nlike audio directions in Google Maps, screen readers, and automated content\ngeneration on platforms like TikTok. However, these systems are mostly\ndominated by voices sourced from data-rich geographies with personas\nrepresentative of their source data. Although 3000 of the world's languages are\ndomiciled in Africa, African voices and personas are under-represented in these\nsystems. As speech synthesis becomes increasingly democratized, it is desirable\nto increase the representation of African English accents. We present Afro-TTS,\nthe first pan-African accented English speech synthesis system able to generate\nspeech in 86 African accents, with 1000 personas representing the rich\nphonological diversity across the continent for downstream application in\nEducation, Public Health, and Automated Content Creation. Speaker interpolation\nretains naturalness and accentedness, enabling the creation of new voices.", "published": "2024-06-17 16:46:10", "link": "http://arxiv.org/abs/2406.11727v2", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Interactive Evolution: A Neural-Symbolic Self-Training Framework For\n  Large Language Models", "abstract": "One of the primary driving forces contributing to the superior performance of\nLarge Language Models (LLMs) is the extensive availability of human-annotated\nnatural language data, which is used for alignment fine-tuning. This inspired\nresearchers to investigate self-training methods to mitigate the extensive\nreliance on human annotations. However, the current success of self-training\nhas been primarily observed in natural language scenarios, rather than in the\nincreasingly important neural-symbolic scenarios. To this end, we propose an\nenvironment-guided neural-symbolic self-training framework named ENVISIONS. It\naims to overcome two main challenges: (1) the scarcity of symbolic data, and\n(2) the limited proficiency of LLMs in processing symbolic language. Extensive\nevaluations conducted on three distinct domains demonstrate the effectiveness\nof our approach. Additionally, we have conducted a comprehensive analysis to\nuncover the factors contributing to ENVISIONS's success, thereby offering\nvaluable insights for future research in this area. Code will be available at\n\\url{https://github.com/xufangzhi/ENVISIONS}.", "published": "2024-06-17 16:52:56", "link": "http://arxiv.org/abs/2406.11736v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi-Layer Ranking with Large Language Models for News Source\n  Recommendation", "abstract": "To seek reliable information sources for news events, we introduce a novel\ntask of expert recommendation, which aims to identify trustworthy sources based\non their previously quoted statements. To achieve this, we built a novel\ndataset, called NewsQuote, consisting of 23,571 quote-speaker pairs sourced\nfrom a collection of news articles. We formulate the recommendation task as the\nretrieval of experts based on their likelihood of being associated with a given\nquery. We also propose a multi-layer ranking framework employing Large Language\nModels to improve the recommendation performance. Our results show that\nemploying an in-context learning based LLM ranker and a multi-layer\nranking-based filter significantly improve both the predictive quality and\nbehavioural quality of the recommender system.", "published": "2024-06-17 17:02:34", "link": "http://arxiv.org/abs/2406.11745v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "A Semantic-Aware Layer-Freezing Approach to Computation-Efficient\n  Fine-Tuning of Language Models", "abstract": "Finetuning language models (LMs) is crucial for adapting the models to\ndownstream data and tasks. However, full finetuning is usually costly. Existing\nwork, such as parameter-efficient finetuning (PEFT), often focuses on\n\\textit{how to finetune} but neglects the issue of \\textit{where to finetune}.\nAs a pioneering work on reducing the cost of backpropagation (at the layer\nlevel) by answering where to finetune, we conduct a semantic analysis of the LM\ninference process. We first propose using transition traces of the latent\nrepresentation to compute deviations (or loss). Then, using a derived formula\nof scaling law, we estimate the gain of each layer in reducing deviation (or\nloss). Further, we narrow down the scope for finetuning, and also, study the\ncost-benefit balance of LM finetuning. We perform extensive experiments across\nwell-known LMs and datasets. The results show that our approach is effective\nand efficient, and outperforms the existing baselines. Our approach is\northogonal to other techniques on improving finetuning efficiency, such as PEFT\nmethods, offering practical values on LM finetuning.", "published": "2024-06-17 17:13:08", "link": "http://arxiv.org/abs/2406.11753v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MDCR: A Dataset for Multi-Document Conditional Reasoning", "abstract": "The same real-life questions posed to different individuals may lead to\ndifferent answers based on their unique situations. For instance, whether a\nstudent is eligible for a scholarship depends on eligibility conditions, such\nas major or degree required. ConditionalQA was proposed to evaluate models'\ncapability of reading a document and answering eligibility questions,\nconsidering unmentioned conditions. However, it is limited to questions on\nsingle documents, neglecting harder cases that may require cross-document\nreasoning and optimization, for example, \"What is the maximum number of\nscholarships attainable?\" Such questions over multiple documents are not only\nmore challenging due to more context having to understand, but also because the\nmodel has to (1) explore all possible combinations of unmentioned conditions\nand (2) understand the relationship between conditions across documents, to\nreason about the optimal outcome. To evaluate models' capability of answering\nsuch questions, we propose a new dataset MDCR, which can reflect real-world\nchallenges and serve as a new test bed for complex conditional reasoning that\nrequires optimization. We evaluate this dataset using the most recent LLMs and\ndemonstrate their limitations in solving this task. We believe this dataset\nwill facilitate future research in answering optimization questions with\nunknown conditions.", "published": "2024-06-17 17:38:43", "link": "http://arxiv.org/abs/2406.11784v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DataComp-LM: In search of the next generation of training sets for\n  language models", "abstract": "We introduce DataComp for Language Models (DCLM), a testbed for controlled\ndataset experiments with the goal of improving language models. As part of\nDCLM, we provide a standardized corpus of 240T tokens extracted from Common\nCrawl, effective pretraining recipes based on the OpenLM framework, and a broad\nsuite of 53 downstream evaluations. Participants in the DCLM benchmark can\nexperiment with data curation strategies such as deduplication, filtering, and\ndata mixing at model scales ranging from 412M to 7B parameters. As a baseline\nfor DCLM, we conduct extensive experiments and find that model-based filtering\nis key to assembling a high-quality training set. The resulting dataset,\nDCLM-Baseline enables training a 7B parameter language model from scratch to\n64% 5-shot accuracy on MMLU with 2.6T training tokens. Compared to MAP-Neo, the\nprevious state-of-the-art in open-data language models, DCLM-Baseline\nrepresents a 6.6 percentage point improvement on MMLU while being trained with\n40% less compute. Our baseline model is also comparable to Mistral-7B-v0.3 and\nLlama 3 8B on MMLU (63% & 66%), and performs similarly on an average of 53\nnatural language understanding tasks while being trained with 6.6x less compute\nthan Llama 3 8B. Our results highlight the importance of dataset design for\ntraining language models and offer a starting point for further research on\ndata curation.", "published": "2024-06-17 17:42:57", "link": "http://arxiv.org/abs/2406.11794v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "RepLiQA: A Question-Answering Dataset for Benchmarking LLMs on Unseen\n  Reference Content", "abstract": "Large Language Models (LLMs) are trained on vast amounts of data, most of\nwhich is automatically scraped from the internet. This data includes\nencyclopedic documents that harbor a vast amount of general knowledge (e.g.,\nWikipedia) but also potentially overlap with benchmark datasets used for\nevaluating LLMs. Consequently, evaluating models on test splits that might have\nleaked into the training set is prone to misleading conclusions. To foster\nsound evaluation of language models, we introduce a new test dataset named\nRepLiQA, suited for question-answering and topic retrieval tasks. RepLiQA is a\ncollection of five splits of test sets, four of which have not been released to\nthe internet or exposed to LLM APIs prior to this publication. Each sample in\nRepLiQA comprises (1) a reference document crafted by a human annotator and\ndepicting an imaginary scenario (e.g., a news article) absent from the\ninternet; (2) a question about the document's topic; (3) a ground-truth answer\nderived directly from the information in the document; and (4) the paragraph\nextracted from the reference document containing the answer. As such, accurate\nanswers can only be generated if a model can find relevant content within the\nprovided document. We run a large-scale benchmark comprising several\nstate-of-the-art LLMs to uncover differences in performance across models of\nvarious types and sizes in a context-conditional language modeling setting.\nReleased splits of RepLiQA can be found here:\nhttps://huggingface.co/datasets/ServiceNow/repliqa.", "published": "2024-06-17 17:52:54", "link": "http://arxiv.org/abs/2406.11811v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On Efficient Language and Vision Assistants for Visually-Situated\n  Natural Language Understanding: What Matters in Reading and Reasoning", "abstract": "Recent advancements in language and vision assistants have showcased\nimpressive capabilities but suffer from a lack of transparency, limiting\nbroader research and reproducibility. While open-source models handle general\nimage tasks effectively, they face challenges with the high computational\ndemands of complex visually-situated text understanding. Such tasks often\nrequire increased token inputs and large vision modules to harness\nhigh-resolution information. Striking a balance between model size and data\nimportance remains an open question. This study aims to redefine the design of\nvision-language models by identifying key components and creating efficient\nmodels with constrained inference costs. By strategically formulating datasets,\noptimizing vision modules, and enhancing supervision techniques, we achieve\nsignificant improvements in inference throughput while maintaining high\nperformance. Extensive experiments across models ranging from 160M to 13B\nparameters offer insights into model optimization. We will fully open-source\nour codebase, models, and datasets at https://github.com/naver-ai/elva.", "published": "2024-06-17 17:57:30", "link": "http://arxiv.org/abs/2406.11823v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Language Modeling with Editable External Knowledge", "abstract": "When the world changes, so does the text that humans write about it. How do\nwe build language models that can be easily updated to reflect these changes?\nOne popular approach is retrieval-augmented generation, in which new documents\nare inserted into a knowledge base and retrieved during prediction for\ndownstream tasks. Most prior work on these systems have focused on improving\nbehavior during prediction through better retrieval or reasoning. This paper\nintroduces ERASE, which instead improves model behavior when new documents are\nacquired, by incrementally deleting or rewriting other entries in the knowledge\nbase each time a document is added. In two new benchmark datasets evaluating\nmodels' ability to answer questions about a stream of news articles or\nconversations, ERASE improves accuracy relative to conventional\nretrieval-augmented generation by 7-13% (Mixtral-8x7B) and 6-10% (Llama-3-8B)\nabsolute. Code and data are available at https://github.com/belindal/ERASE", "published": "2024-06-17 17:59:35", "link": "http://arxiv.org/abs/2406.11830v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Transcoders Find Interpretable LLM Feature Circuits", "abstract": "A key goal in mechanistic interpretability is circuit analysis: finding\nsparse subgraphs of models corresponding to specific behaviors or capabilities.\nHowever, MLP sublayers make fine-grained circuit analysis on transformer-based\nlanguage models difficult. In particular, interpretable features -- such as\nthose found by sparse autoencoders (SAEs) -- are typically linear combinations\nof extremely many neurons, each with its own nonlinearity to account for.\nCircuit analysis in this setting thus either yields intractably large circuits\nor fails to disentangle local and global behavior. To address this we explore\ntranscoders, which seek to faithfully approximate a densely activating MLP\nlayer with a wider, sparsely-activating MLP layer. We introduce a novel method\nfor using transcoders to perform weights-based circuit analysis through MLP\nsublayers. The resulting circuits neatly factorize into input-dependent and\ninput-invariant terms. We then successfully train transcoders on language\nmodels with 120M, 410M, and 1.4B parameters, and find them to perform at least\non par with SAEs in terms of sparsity, faithfulness, and\nhuman-interpretability. Finally, we apply transcoders to reverse-engineer\nunknown circuits in the model, and we obtain novel insights regarding the\n\"greater-than circuit\" in GPT2-small. Our results suggest that transcoders can\nprove effective in decomposing model computations involving MLPs into\ninterpretable circuits. Code is available at\nhttps://github.com/jacobdunefsky/transcoder_circuits/.", "published": "2024-06-17 17:49:00", "link": "http://arxiv.org/abs/2406.11944v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Prefixing Attention Sinks can Mitigate Activation Outliers for Large\n  Language Model Quantization", "abstract": "Despite recent advances in LLM quantization, activation quantization remains\nto be challenging due to the activation outliers. Conventional remedies, e.g.,\nmixing precisions for different channels, introduce extra overhead and reduce\nthe speedup. In this work, we develop a simple yet effective strategy to\nfacilitate per-tensor activation quantization by preventing the generation of\nproblematic tokens. Precisely, we propose a method to find a set of key-value\ncache, coined CushionCache, which mitigates outliers in subsequent tokens when\ninserted as a prefix. CushionCache works in two steps: First, we greedily\nsearch for a prompt token sequence that minimizes the maximum activation values\nin subsequent tokens. Then, we further tune the token cache to regularize the\nactivations of subsequent tokens to be more quantization-friendly. The proposed\nmethod successfully addresses activation outliers of LLMs, providing a\nsubstantial performance boost for per-tensor activation quantization methods.\nWe thoroughly evaluate our method over a wide range of models and benchmarks\nand find that it significantly surpasses the established baseline of per-tensor\nW8A8 quantization and can be seamlessly integrated with the recent activation\nquantization method.", "published": "2024-06-17 18:33:44", "link": "http://arxiv.org/abs/2406.12016v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "LiLiuM: eBay's Large Language Models for e-commerce", "abstract": "We introduce the LiLiuM series of large language models (LLMs): 1B, 7B, and\n13B parameter models developed 100% in-house to fit eBay's specific needs in\nthe e-commerce domain. This gives eBay full control over all aspects of the\nmodels including license, data, vocabulary, and architecture. We expect these\nmodels to be used as a foundation for fine-tuning and instruction-tuning,\neliminating dependencies to external models.\n  The LiLiuM LLMs have been trained on 3 trillion tokens of multilingual text\nfrom general and e-commerce domain. They perform similar to the popular LLaMA-2\nmodels on English natural language understanding (NLU) benchmarks. At the same\ntime, we outperform LLaMA-2 on non-English NLU tasks, machine translation and\non e-commerce specific downstream tasks.\n  As part of our data mixture, we utilize the newly released RedPajama-V2\ndataset for training and share our insights regarding data filtering and\ndeduplication. We also discuss in detail how to serialize structured data for\nuse in autoregressive language modeling. We provide insights on the effects of\nincluding code and parallel machine translation data in pre-training.\nFurthermore, we develop our own tokenizer and model vocabulary, customized\ntowards e-commerce. This way, we can achieve up to 34% speed-up in text\ngeneration on eBay-specific downstream tasks compared to LLaMA-2.\n  Finally, in relation to LLM pretraining, we show that checkpoint averaging\ncan further improve over the best individual model checkpoint.", "published": "2024-06-17 18:45:41", "link": "http://arxiv.org/abs/2406.12023v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Self-MoE: Towards Compositional Large Language Models with\n  Self-Specialized Experts", "abstract": "We present Self-MoE, an approach that transforms a monolithic LLM into a\ncompositional, modular system of self-specialized experts, named MiXSE (MiXture\nof Self-specialized Experts). Our approach leverages self-specialization, which\nconstructs expert modules using self-generated synthetic data, each equipping a\nshared base LLM with distinct domain-specific capabilities, activated via\nself-optimized routing. This allows for dynamic and capability-specific\nhandling of various target tasks, enhancing overall capabilities, without\nextensive human-labeled data and added parameters. Our empirical results reveal\nthat specializing LLMs may exhibit potential trade-offs in performances on\nnon-specialized tasks. On the other hand, our Self-MoE demonstrates substantial\nimprovements (6.5%p on average) over the base LLM across diverse benchmarks\nsuch as knowledge, reasoning, math, and coding. It also consistently\noutperforms other methods, including instance merging and weight merging, while\noffering better flexibility and interpretability by design with semantic\nexperts and routing. Our findings highlight the critical role of modularity,\nthe applicability of Self-MoE to multiple base LLMs, and the potential of\nself-improvement in achieving efficient, scalable, and adaptable systems.", "published": "2024-06-17 19:06:54", "link": "http://arxiv.org/abs/2406.12034v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MedCalc-Bench: Evaluating Large Language Models for Medical Calculations", "abstract": "As opposed to evaluating computation and logic-based reasoning, current\nbenchmarks for evaluating large language models (LLMs) in medicine are\nprimarily focused on question-answering involving domain knowledge and\ndescriptive reasoning. While such qualitative capabilities are vital to medical\ndiagnosis, in real-world scenarios, doctors frequently use clinical calculators\nthat follow quantitative equations and rule-based reasoning paradigms for\nevidence-based decision support. To this end, we propose MedCalc-Bench, a\nfirst-of-its-kind dataset focused on evaluating the medical calculation\ncapability of LLMs. MedCalc-Bench contains an evaluation set of over 1000\nmanually reviewed instances from 55 different medical calculation tasks. Each\ninstance in MedCalc-Bench consists of a patient note, a question requesting to\ncompute a specific medical value, a ground truth answer, and a step-by-step\nexplanation showing how the answer is obtained. While our evaluation results\nshow the potential of LLMs in this area, none of them are effective enough for\nclinical settings. Common issues include extracting the incorrect entities, not\nusing the correct equation or rules for a calculation task, or incorrectly\nperforming the arithmetic for the computation. We hope our study highlights the\nquantitative knowledge and reasoning gaps in LLMs within medical settings,\nencouraging future improvements of LLMs for various clinical calculation tasks.", "published": "2024-06-17 19:07:21", "link": "http://arxiv.org/abs/2406.12036v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "$\u03c4$-bench: A Benchmark for Tool-Agent-User Interaction in Real-World\n  Domains", "abstract": "Existing benchmarks do not test language agents on their interaction with\nhuman users or ability to follow domain-specific rules, both of which are vital\nfor deploying them in real world applications. We propose $\\tau$-bench, a\nbenchmark emulating dynamic conversations between a user (simulated by language\nmodels) and a language agent provided with domain-specific API tools and policy\nguidelines. We employ an efficient and faithful evaluation process that\ncompares the database state at the end of a conversation with the annotated\ngoal state. We also propose a new metric (pass^k) to evaluate the reliability\nof agent behavior over multiple trials. Our experiments show that even\nstate-of-the-art function calling agents (like gpt-4o) succeed on <50% of the\ntasks, and are quite inconsistent (pass^8 <25% in retail). Our findings point\nto the need for methods that can improve the ability of agents to act\nconsistently and follow rules reliably.", "published": "2024-06-17 19:33:08", "link": "http://arxiv.org/abs/2406.12045v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "WellDunn: On the Robustness and Explainability of Language Models and\n  Large Language Models in Identifying Wellness Dimensions", "abstract": "Language Models (LMs) are being proposed for mental health applications where\nthe heightened risk of adverse outcomes means predictive performance may not be\na sufficient litmus test of a model's utility in clinical practice. A model\nthat can be trusted for practice should have a correspondence between\nexplanation and clinical determination, yet no prior research has examined the\nattention fidelity of these models and their effect on ground truth\nexplanations. We introduce an evaluation design that focuses on the robustness\nand explainability of LMs in identifying Wellness Dimensions (WDs). We focus on\ntwo existing mental health and well-being datasets: (a) Multi-label\nClassification-based MultiWD, and (b) WellXplain for evaluating attention\nmechanism veracity against expert-labeled explanations. The labels are based on\nHalbert Dunn's theory of wellness, which gives grounding to our evaluation. We\nreveal four surprising results about LMs/LLMs: (1) Despite their human-like\ncapabilities, GPT-3.5/4 lag behind RoBERTa, and MedAlpaca, a fine-tuned LLM on\nWellXplain fails to deliver any remarkable improvements in performance or\nexplanations. (2) Re-examining LMs' predictions based on a confidence-oriented\nloss function reveals a significant performance drop. (3) Across all LMs/LLMs,\nthe alignment between attention and explanations remains low, with LLMs scoring\na dismal 0.0. (4) Most mental health-specific LMs/LLMs overlook domain-specific\nknowledge and undervalue explanations, causing these discrepancies. This study\nhighlights the need for further research into their consistency and\nexplanations in mental health and well-being.", "published": "2024-06-17 19:50:40", "link": "http://arxiv.org/abs/2406.12058v4", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Not Eliminate but Aggregate: Post-Hoc Control over Mixture-of-Experts to\n  Address Shortcut Shifts in Natural Language Understanding", "abstract": "Recent models for natural language understanding are inclined to exploit\nsimple patterns in datasets, commonly known as shortcuts. These shortcuts hinge\non spurious correlations between labels and latent features existing in the\ntraining data. At inference time, shortcut-dependent models are likely to\ngenerate erroneous predictions under distribution shifts, particularly when\nsome latent features are no longer correlated with the labels. To avoid this,\nprevious studies have trained models to eliminate the reliance on shortcuts. In\nthis study, we explore a different direction: pessimistically aggregating the\npredictions of a mixture-of-experts, assuming each expert captures relatively\ndifferent latent features. The experimental results demonstrate that our\npost-hoc control over the experts significantly enhances the model's robustness\nto the distribution shift in shortcuts. Besides, we show that our approach has\nsome practical advantages. We also analyze our model and provide results to\nsupport the assumption.", "published": "2024-06-17 20:00:04", "link": "http://arxiv.org/abs/2406.12060v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "When Reasoning Meets Information Aggregation: A Case Study with Sports\n  Narratives", "abstract": "Reasoning is most powerful when an LLM accurately aggregates relevant\ninformation. We examine the critical role of information aggregation in\nreasoning by requiring the LLM to analyze sports narratives. To succeed at this\ntask, an LLM must infer points from actions, identify related entities,\nattribute points accurately to players and teams, and compile key statistics to\ndraw conclusions. We conduct comprehensive experiments with real NBA basketball\ndata and present SportsGen, a new method to synthesize game narratives. By\nsynthesizing data, we can rigorously evaluate LLMs' reasoning capabilities\nunder complex scenarios with varying narrative lengths and density of\ninformation. Our findings show that most models, including GPT-4o, often fail\nto accurately aggregate basketball scores due to frequent scoring patterns.\nOpen-source models like Llama-3 further suffer from significant score\nhallucinations. Finally, the effectiveness of reasoning is influenced by\nnarrative complexity, information density, and domain-specific terms,\nhighlighting the challenges in analytical reasoning tasks.", "published": "2024-06-17 20:49:35", "link": "http://arxiv.org/abs/2406.12084v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Who's asking? User personas and the mechanics of latent misalignment", "abstract": "Despite investments in improving model safety, studies show that misaligned\ncapabilities remain latent in safety-tuned models. In this work, we shed light\non the mechanics of this phenomenon. First, we show that even when model\ngenerations are safe, harmful content can persist in hidden representations and\ncan be extracted by decoding from earlier layers. Then, we show that whether\nthe model divulges such content depends significantly on its perception of who\nit is talking to, which we refer to as user persona. In fact, we find\nmanipulating user persona to be even more effective for eliciting harmful\ncontent than direct attempts to control model refusal. We study both natural\nlanguage prompting and activation steering as control methods and show that\nactivation steering is significantly more effective at bypassing safety\nfilters. We investigate why certain personas break model safeguards and find\nthat they enable the model to form more charitable interpretations of otherwise\ndangerous queries. Finally, we show we can predict a persona's effect on\nrefusal given only the geometry of its steering vector.", "published": "2024-06-17 21:15:12", "link": "http://arxiv.org/abs/2406.12094v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Can LLMs Learn Macroeconomic Narratives from Social Media?", "abstract": "This study empirically tests the $\\textit{Narrative Economics}$ hypothesis,\nwhich posits that narratives (ideas that are spread virally and affect public\nbeliefs) can influence economic fluctuations. We introduce two curated datasets\ncontaining posts from X (formerly Twitter) which capture economy-related\nnarratives (Data will be shared upon paper acceptance). Employing Natural\nLanguage Processing (NLP) methods, we extract and summarize narratives from the\ntweets. We test their predictive power for $\\textit{macroeconomic}$ forecasting\nby incorporating the tweets' or the extracted narratives' representations in\ndownstream financial prediction tasks. Our work highlights the challenges in\nimproving macroeconomic models with narrative data, paving the way for the\nresearch community to realistically address this important challenge. From a\nscientific perspective, our investigation offers valuable insights and NLP\ntools for narrative extraction and summarization using Large Language Models\n(LLMs), contributing to future research on the role of narratives in economics.", "published": "2024-06-17 21:37:09", "link": "http://arxiv.org/abs/2406.12109v2", "categories": ["cs.CL", "cs.CE"], "primary_category": "cs.CL"}
{"title": "Efficient Sequential Decision Making with Large Language Models", "abstract": "This paper focuses on extending the success of large language models (LLMs)\nto sequential decision making. Existing efforts either (i) re-train or finetune\nLLMs for decision making, or (ii) design prompts for pretrained LLMs. The\nformer approach suffers from the computational burden of gradient updates, and\nthe latter approach does not show promising results. In this paper, we propose\na new approach that leverages online model selection algorithms to efficiently\nincorporate LLMs agents into sequential decision making. Statistically, our\napproach significantly outperforms both traditional decision making algorithms\nand vanilla LLM agents. Computationally, our approach avoids the need for\nexpensive gradient updates of LLMs, and throughout the decision making process,\nit requires only a small number of LLM calls. We conduct extensive experiments\nto verify the effectiveness of our proposed approach. As an example, on a\nlarge-scale Amazon dataset, our approach achieves more than a $6$x performance\ngain over baselines while calling LLMs in only $1.5$\\% of the time steps.", "published": "2024-06-17 22:13:22", "link": "http://arxiv.org/abs/2406.12125v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Bias in Text Embedding Models", "abstract": "Text embedding is becoming an increasingly popular AI methodology, especially\namong businesses, yet the potential of text embedding models to be biased is\nnot well understood. This paper examines the degree to which a selection of\npopular text embedding models are biased, particularly along gendered\ndimensions. More specifically, this paper studies the degree to which these\nmodels associate a list of given professions with gendered terms. The analysis\nreveals that text embedding models are prone to gendered biases but in varying\nways. Although there are certain inter-model commonalities, for instance,\ngreater association of professions like nurse, homemaker, and socialite with\nfemale identifiers, and greater association of professions like CEO, manager,\nand boss with male identifiers, not all models make the same gendered\nassociations for each occupation. Furthermore, the magnitude and directionality\nof bias can also vary on a model-by-model basis and depend on the particular\nwords models are prompted with. This paper demonstrates that gender bias\nafflicts text embedding models and suggests that businesses using this\ntechnology need to be mindful of the specific dimensions of this problem.", "published": "2024-06-17 22:58:36", "link": "http://arxiv.org/abs/2406.12138v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Code-Switching Red-Teaming: LLM Evaluation for Safety and Multilingual\n  Understanding", "abstract": "As large language models (LLMs) have advanced rapidly, concerns regarding\ntheir safety have become prominent. In this paper, we discover that\ncode-switching in red-teaming queries can effectively elicit undesirable\nbehaviors of LLMs, which are common practices in natural language. We introduce\na simple yet effective framework, CSRT, to synthesize code-switching\nred-teaming queries and investigate the safety and multilingual understanding\nof LLMs comprehensively. Through extensive experiments with ten\nstate-of-the-art LLMs and code-switching queries combining up to 10 languages,\nwe demonstrate that the CSRT significantly outperforms existing multilingual\nred-teaming techniques, achieving 46.7% more attacks than standard attacks in\nEnglish and being effective in conventional safety domains. We also examine the\nmultilingual ability of those LLMs to generate and understand code-switching\ntexts. Additionally, we validate the extensibility of the CSRT by generating\ncode-switching attack prompts with monolingual data. We finally conduct\ndetailed ablation studies exploring code-switching and propound unintended\ncorrelation between resource availability of languages and safety alignment in\nexisting multilingual LLMs.", "published": "2024-06-17 06:08:18", "link": "http://arxiv.org/abs/2406.15481v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "SegHist: A General Segmentation-based Framework for Chinese Historical\n  Document Text Line Detection", "abstract": "Text line detection is a key task in historical document analysis facing many\nchallenges of arbitrary-shaped text lines, dense texts, and text lines with\nhigh aspect ratios, etc. In this paper, we propose a general framework for\nhistorical document text detection (SegHist), enabling existing\nsegmentation-based text detection methods to effectively address the\nchallenges, especially text lines with high aspect ratios. Integrating the\nSegHist framework with the commonly used method DB++, we develop DB-SegHist.\nThis approach achieves SOTA on the CHDAC, MTHv2, and competitive results on\nHDRC datasets, with a significant improvement of 1.19% on the most challenging\nCHDAC dataset which features more text lines with high aspect ratios. Moreover,\nour method attains SOTA on rotated MTHv2 and rotated HDRC, demonstrating its\nrotational robustness. The code is available at\nhttps://github.com/LumionHXJ/SegHist.", "published": "2024-06-17 11:00:04", "link": "http://arxiv.org/abs/2406.15485v3", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "A Complete Survey on LLM-based AI Chatbots", "abstract": "The past few decades have witnessed an upsurge in data, forming the\nfoundation for data-hungry, learning-based AI technology. Conversational\nagents, often referred to as AI chatbots, rely heavily on such data to train\nlarge language models (LLMs) and generate new content (knowledge) in response\nto user prompts. With the advent of OpenAI's ChatGPT, LLM-based chatbots have\nset new standards in the AI community. This paper presents a complete survey of\nthe evolution and deployment of LLM-based chatbots in various sectors. We first\nsummarize the development of foundational chatbots, followed by the evolution\nof LLMs, and then provide an overview of LLM-based chatbots currently in use\nand those in the development phase. Recognizing AI chatbots as tools for\ngenerating new knowledge, we explore their diverse applications across various\nindustries. We then discuss the open challenges, considering how the data used\nto train the LLMs and the misuse of the generated knowledge can cause several\nissues. Finally, we explore the future outlook to augment their efficiency and\nreliability in numerous applications. By addressing key milestones and the\npresent-day context of LLM-based chatbots, our survey invites readers to delve\ndeeper into this realm, reflecting on how their next generation will reshape\nconversational AI.", "published": "2024-06-17 09:39:34", "link": "http://arxiv.org/abs/2406.16937v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Cultural Value Differences of LLMs: Prompt, Language, and Model Size", "abstract": "Our study aims to identify behavior patterns in cultural values exhibited by\nlarge language models (LLMs). The studied variants include question ordering,\nprompting language, and model size. Our experiments reveal that each tested LLM\ncan efficiently behave with different cultural values. More interestingly: (i)\nLLMs exhibit relatively consistent cultural values when presented with prompts\nin a single language. (ii) The prompting language e.g., Chinese or English, can\ninfluence the expression of cultural values. The same question can elicit\ndivergent cultural values when the same LLM is queried in a different language.\n(iii) Differences in sizes of the same model (e.g., Llama2-7B vs 13B vs 70B)\nhave a more significant impact on their demonstrated cultural values than model\ndifferences (e.g., Llama2 vs Mixtral). Our experiments reveal that query\nlanguage and model size of LLM are the main factors resulting in cultural value\ndifferences.", "published": "2024-06-17 12:35:33", "link": "http://arxiv.org/abs/2407.16891v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Investigating Annotator Bias in Large Language Models for Hate Speech\n  Detection", "abstract": "Data annotation, the practice of assigning descriptive labels to raw data, is\npivotal in optimizing the performance of machine learning models. However, it\nis a resource-intensive process susceptible to biases introduced by annotators.\nThe emergence of sophisticated Large Language Models (LLMs) presents a unique\nopportunity to modernize and streamline this complex procedure. While existing\nresearch extensively evaluates the efficacy of LLMs, as annotators, this paper\ndelves into the biases present in LLMs when annotating hate speech data. Our\nresearch contributes to understanding biases in four key categories: gender,\nrace, religion, and disability with four LLMs: GPT-3.5, GPT-4o, Llama-3.1 and\nGemma-2. Specifically targeting highly vulnerable groups within these\ncategories, we analyze annotator biases. Furthermore, we conduct a\ncomprehensive examination of potential factors contributing to these biases by\nscrutinizing the annotated data. We introduce our custom hate speech detection\ndataset, HateBiasNet, to conduct this research. Additionally, we perform the\nsame experiments on the ETHOS (Mollas et al. 2022) dataset also for comparative\nanalysis. This paper serves as a crucial resource, guiding researchers and\npractitioners in harnessing the potential of LLMs for data annotation, thereby\nfostering advancements in this critical field.", "published": "2024-06-17 00:18:31", "link": "http://arxiv.org/abs/2406.11109v5", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Are Large Language Models a Good Replacement of Taxonomies?", "abstract": "Large language models (LLMs) demonstrate an impressive ability to internalize\nknowledge and answer natural language questions. Although previous studies\nvalidate that LLMs perform well on general knowledge while presenting poor\nperformance on long-tail nuanced knowledge, the community is still doubtful\nabout whether the traditional knowledge graphs should be replaced by LLMs. In\nthis paper, we ask if the schema of knowledge graph (i.e., taxonomy) is made\nobsolete by LLMs. Intuitively, LLMs should perform well on common taxonomies\nand at taxonomy levels that are common to people. Unfortunately, there lacks a\ncomprehensive benchmark that evaluates the LLMs over a wide range of taxonomies\nfrom common to specialized domains and at levels from root to leaf so that we\ncan draw a confident conclusion. To narrow the research gap, we constructed a\nnovel taxonomy hierarchical structure discovery benchmark named TaxoGlimpse to\nevaluate the performance of LLMs over taxonomies. TaxoGlimpse covers ten\nrepresentative taxonomies from common to specialized domains with in-depth\nexperiments of different levels of entities in this taxonomy from root to leaf.\nOur comprehensive experiments of eighteen state-of-the-art LLMs under three\nprompting settings validate that LLMs can still not well capture the knowledge\nof specialized taxonomies and leaf-level entities.", "published": "2024-06-17 01:21:50", "link": "http://arxiv.org/abs/2406.11131v2", "categories": ["cs.CL", "cs.AI", "cs.DB"], "primary_category": "cs.CL"}
{"title": "RePrompt: Planning by Automatic Prompt Engineering for Large Language\n  Models Agents", "abstract": "In the past year, large language models (LLMs) have had remarkable success in\ndomains outside the traditional natural language processing, and their capacity\nis further expanded into the so-called LLM agents when connected with external\ntools. In all domains, the prompt to the LLMs has been shown to make a big\ndifference in what the LLM would generate and thus affect the performance of\nthe LLM agents. Therefore, automatic prompt engineering (APE) has become an\nimportant question for many researchers and users of LLMs. However, previous\nworks in APE rely on a final checker to evaluate the performance of the given\nprompt -- a requirement that is hard to meet in the case of LLM agents, where\nintermediate feedback is easier to obtain, and the final evaluation could be\nexpensive, inaccurate, or even missing. In this paper, we propose a novel\nmethod, \\textsc{RePrompt}, which does a ``gradient descent\"-like approach to\noptimize the step-by-step instructions in the prompts given to LLM agents,\nbased on the chat history obtained from interactions and reflections with LLM\nagents. By leveraging intermediate feedback, \\textsc{RePrompt} can optimize the\nprompt without the need for a final solution checker. We evaluate our approach\non PDDL generation, TravelPlanner, and Meeting Planning to show that our method\ncould generally improve performance for different reasoning tasks.", "published": "2024-06-17 01:23:11", "link": "http://arxiv.org/abs/2406.11132v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SUGARCREPE++ Dataset: Vision-Language Model Sensitivity to Semantic and\n  Lexical Alterations", "abstract": "Despite their remarkable successes, state-of-the-art large language models\n(LLMs), including vision-and-language models (VLMs) and unimodal language\nmodels (ULMs), fail to understand precise semantics. For example, semantically\nequivalent sentences expressed using different lexical compositions elicit\ndiverging representations. The degree of this divergence and its impact on\nencoded semantics is not very well understood. In this paper, we introduce the\nSUGARCREPE++ dataset to analyze the sensitivity of VLMs and ULMs to lexical and\nsemantic alterations. Each sample in SUGARCREPE++ dataset consists of an image\nand a corresponding triplet of captions: a pair of semantically equivalent but\nlexically different positive captions and one hard negative caption. This poses\na 3-way semantic (in)equivalence problem to the language models. We\ncomprehensively evaluate VLMs and ULMs that differ in architecture,\npre-training objectives and datasets to benchmark the performance of\nSUGARCREPE++ dataset. Experimental results highlight the difficulties of VLMs\nin distinguishing between lexical and semantic variations, particularly in\nobject attributes and spatial relations. Although VLMs with larger pre-training\ndatasets, model sizes, and multiple pre-training objectives achieve better\nperformance on SUGARCREPE++, there is a significant opportunity for\nimprovement. We show that all the models which achieve better performance on\ncompositionality datasets need not perform equally well on SUGARCREPE++,\nsignifying that compositionality alone may not be sufficient for understanding\nsemantic and lexical alterations. Given the importance of the property that the\nSUGARCREPE++ dataset targets, it serves as a new challenge to the\nvision-and-language community.", "published": "2024-06-17 03:22:20", "link": "http://arxiv.org/abs/2406.11171v2", "categories": ["cs.CV", "cs.CL", "cs.LG", "68T45, 68T50", "I.2.7; I.2.10"], "primary_category": "cs.CV"}
{"title": "Watch Every Step! LLM Agent Learning via Iterative Step-Level Process\n  Refinement", "abstract": "Large language model agents have exhibited exceptional performance across a\nrange of complex interactive tasks. Recent approaches have utilized tuning with\nexpert trajectories to enhance agent performance, yet they primarily\nconcentrate on outcome rewards, which may lead to errors or suboptimal actions\ndue to the absence of process supervision signals. In this paper, we introduce\nthe Iterative step-level Process Refinement (IPR) framework, which provides\ndetailed step-by-step guidance to enhance agent training. Specifically, we\nadopt the Monte Carlo method to estimate step-level rewards. During each\niteration, the agent explores along the expert trajectory and generates new\nactions. These actions are then evaluated against the corresponding step of\nexpert trajectory using step-level rewards. Such comparison helps identify\ndiscrepancies, yielding contrastive action pairs that serve as training data\nfor the agent. Our experiments on three complex agent tasks demonstrate that\nour framework outperforms a variety of strong baselines. Moreover, our\nanalytical findings highlight the effectiveness of IPR in augmenting action\nefficiency and its applicability to diverse models.", "published": "2024-06-17 03:29:13", "link": "http://arxiv.org/abs/2406.11176v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "WeatherQA: Can Multimodal Language Models Reason about Severe Weather?", "abstract": "Severe convective weather events, such as hail, tornadoes, and thunderstorms,\noften occur quickly yet cause significant damage, costing billions of dollars\nevery year. This highlights the importance of forecasting severe weather\nthreats hours in advance to better prepare meteorologists and residents in\nat-risk areas. Can modern large foundation models perform such forecasting?\nExisting weather benchmarks typically focus only on predicting time-series\nchanges in certain weather parameters (e.g., temperature, moisture) with\ntext-only features. In this work, we introduce WeatherQA, the first multimodal\ndataset designed for machines to reason about complex combinations of weather\nparameters (a.k.a., ingredients) and predict severe weather in real-world\nscenarios. The dataset includes over 8,000 (multi-images, text) pairs for\ndiverse severe weather events. Each pair contains rich information crucial for\nforecasting -- the images describe the ingredients capturing environmental\ninstability, surface observations, and radar reflectivity, and the text\ncontains forecast analyses written by human experts. With WeatherQA, we\nevaluate state-of-the-art vision language models, including GPT4, Claude3.5,\nGemini-1.5, and a fine-tuned Llama3-based VLM, by designing two challenging\ntasks: (1) multi-choice QA for predicting affected area and (2) classification\nof the development potential of severe convection. These tasks require deep\nunderstanding of domain knowledge (e.g., atmospheric dynamics) and complex\nreasoning over multimodal data (e.g., interactions between weather parameters).\nWe show a substantial gap between the strongest VLM, GPT4o, and human\nreasoning. Our comprehensive case study with meteorologists further reveals the\nweaknesses of the models, suggesting that better training and data integration\nare necessary to bridge this gap. WeatherQA link:\nhttps://github.com/chengqianma/WeatherQA.", "published": "2024-06-17 05:23:18", "link": "http://arxiv.org/abs/2406.11217v2", "categories": ["cs.AI", "cs.CL", "cs.CV", "physics.ao-ph"], "primary_category": "cs.AI"}
{"title": "Multimodal Needle in a Haystack: Benchmarking Long-Context Capability of\n  Multimodal Large Language Models", "abstract": "Multimodal Large Language Models (MLLMs) have shown significant promise in\nvarious applications, leading to broad interest from researchers and\npractitioners alike. However, a comprehensive evaluation of their long-context\ncapabilities remains underexplored. To address these gaps, we introduce the\nMultiModal Needle-in-a-haystack (MMNeedle) benchmark, specifically designed to\nassess the long-context capabilities of MLLMs. Besides multi-image input, we\nemploy image stitching to further increase the input context length, and\ndevelop a protocol to automatically generate labels for sub-image level\nretrieval. Essentially, MMNeedle evaluates MLLMs by stress-testing their\ncapability to locate a target sub-image (needle) within a set of images\n(haystack) based on textual instructions and descriptions of image contents.\nThis setup necessitates an advanced understanding of extensive visual contexts\nand effective information retrieval within long-context image inputs. With this\nbenchmark, we evaluate state-of-the-art MLLMs, encompassing both API-based and\nopen-source models. The findings reveal that GPT-4o consistently surpasses\nother models in long-context scenarios, but suffers from hallucination problems\nin negative samples, i.e., when needles are not in the haystacks. Our\ncomprehensive long-context evaluation of MLLMs also sheds lights on the\nconsiderable performance gap between API-based and open-source models. All the\ncode, data, and instructions required to reproduce the main results are\navailable at https://github.com/Wang-ML-Lab/multimodal-needle-in-a-haystack.", "published": "2024-06-17 05:54:06", "link": "http://arxiv.org/abs/2406.11230v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Enabling robots to follow abstract instructions and complete complex\n  dynamic tasks", "abstract": "Completing complex tasks in unpredictable settings like home kitchens\nchallenges robotic systems. These challenges include interpreting high-level\nhuman commands, such as \"make me a hot beverage\" and performing actions like\npouring a precise amount of water into a moving mug. To address these\nchallenges, we present a novel framework that combines Large Language Models\n(LLMs), a curated Knowledge Base, and Integrated Force and Visual Feedback\n(IFVF). Our approach interprets abstract instructions, performs long-horizon\ntasks, and handles various uncertainties. It utilises GPT-4 to analyse the\nuser's query and surroundings, then generates code that accesses a curated\ndatabase of functions during execution. It translates abstract instructions\ninto actionable steps. Each step involves generating custom code by employing\nretrieval-augmented generalisation to pull IFVF-relevant examples from the\nKnowledge Base. IFVF allows the robot to respond to noise and disturbances\nduring execution. We use coffee making and plate decoration to demonstrate our\napproach, including components ranging from pouring to drawer opening, each\nbenefiting from distinct feedback types and methods. This novel advancement\nmarks significant progress toward a scalable, efficient robotic framework for\ncompleting complex tasks in uncertain environments. Our findings are\nillustrated in an accompanying video and supported by an open-source GitHub\nrepository (released upon paper acceptance).", "published": "2024-06-17 05:55:35", "link": "http://arxiv.org/abs/2406.11231v1", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Probing the Decision Boundaries of In-context Learning in Large Language\n  Models", "abstract": "In-context learning is a key paradigm in large language models (LLMs) that\nenables them to generalize to new tasks and domains by simply prompting these\nmodels with a few exemplars without explicit parameter updates. Many attempts\nhave been made to understand in-context learning in LLMs as a function of model\nscale, pretraining data, and other factors. In this work, we propose a new\nmechanism to probe and understand in-context learning from the lens of decision\nboundaries for in-context binary classification. Decision boundaries are\nstraightforward to visualize and provide important information about the\nqualitative behavior of the inductive biases of standard classifiers. To our\nsurprise, we find that the decision boundaries learned by current LLMs in\nsimple binary classification tasks are often irregular and non-smooth,\nregardless of linear separability in the underlying task. This paper\ninvestigates the factors influencing these decision boundaries and explores\nmethods to enhance their generalizability. We assess various approaches,\nincluding training-free and fine-tuning methods for LLMs, the impact of model\narchitecture, and the effectiveness of active prompting techniques for\nsmoothing decision boundaries in a data-efficient manner. Our findings provide\na deeper understanding of in-context learning dynamics and offer practical\nimprovements for enhancing robustness and generalizability of in-context\nlearning.", "published": "2024-06-17 06:00:24", "link": "http://arxiv.org/abs/2406.11233v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Iterative Utility Judgment Framework via LLMs Inspired by Relevance in\n  Philosophy", "abstract": "Utility and topical relevance are critical measures in information retrieval\n(IR), reflecting system and user perspectives, respectively. While topical\nrelevance has long been emphasized, utility is a higher standard of relevance\nand is more useful for facilitating downstream tasks, e.g., in\nRetrieval-Augmented Generation (RAG). When we incorporate utility judgments\ninto RAG, we realize that the topical relevance, utility, and answering in RAG\nare closely related to the three types of relevance that Schutz discussed from\na philosophical perspective. They are topical relevance, interpretational\nrelevance, and motivational relevance, respectively. Inspired by the dynamic\niterations of the three types of relevance, we propose an Iterative utiliTy\njudgmEnt fraMework (ITEM) to promote each step of the cycle of RAG. We\nconducted extensive experiments on multi-grade passage retrieval and factoid\nquestion-answering datasets (i.e., TREC DL, WebAP, and NQ). Experimental\nresults demonstrate significant improvements in utility judgments, ranking of\ntopical relevance, and answer generation upon representative baselines,\nincluding multiple single-shot utility judging approaches. Our code and\nbenchmark can be found at https://anonymous.4open.science/r/ITEM-B486/.", "published": "2024-06-17 07:52:42", "link": "http://arxiv.org/abs/2406.11290v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Enhancing and Assessing Instruction-Following with Fine-Grained\n  Instruction Variants", "abstract": "The effective alignment of Large Language Models (LLMs) with precise\ninstructions is essential for their application in diverse real-world\nscenarios. Current methods focus on enhancing the diversity and complexity of\ntraining and evaluation samples, yet they fall short in accurately assessing\nLLMs' ability to follow similar instruction variants. We introduce an effective\ndata augmentation technique DeMoRecon that decomposes complex instructions into\nsimpler sub-components, modifies these, and reconstructs them into new\nvariants, thereby preserves the original instruction's context and complexity\nwhile introducing variability, which is critical for training and evaluating\nLLMs' instruction-following precision. Based on DeMoRecon, we developed the\nFGIV dataset which contains fine-grained instruction variants of 1,773 seed\ninstructions to both fine-tune and evaluate LLMs. Our findings show that LLMs\nfine-tuned with FGIV will gain significant performance boost on both ours and\ncommonly used instructions-following benchmarks.", "published": "2024-06-17 08:08:11", "link": "http://arxiv.org/abs/2406.11301v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "VideoVista: A Versatile Benchmark for Video Understanding and Reasoning", "abstract": "Despite significant breakthroughs in video analysis driven by the rapid\ndevelopment of large multimodal models (LMMs), there remains a lack of a\nversatile evaluation benchmark to comprehensively assess these models'\nperformance in video understanding and reasoning. To address this, we present\nVideoVista, a video QA benchmark that integrates challenges across diverse\ncontent categories, durations, and abilities. Specifically, VideoVista\ncomprises 25,000 questions derived from 3,400 videos spanning 14 categories\n(e.g., Howto, Film, and Entertainment) with durations ranging from a few\nseconds to over 10 minutes. Besides, it encompasses 19 types of understanding\ntasks (e.g., anomaly detection, interaction understanding) and 8 reasoning\ntasks (e.g., logical reasoning, causal reasoning). To achieve this, we present\nan automatic data construction framework, leveraging powerful GPT-4o alongside\nadvanced analysis tools (e.g., video splitting, object segmenting, and\ntracking). We also utilize this framework to construct training data to enhance\nthe capabilities of video-related LMMs (Video-LMMs). Through a comprehensive\nand quantitative evaluation of cutting-edge models, we reveal that: 1)\nVideo-LMMs face difficulties in fine-grained video tasks involving temporal\nlocation, object tracking, and anomaly detection; 2) Video-LMMs present\ninferior logical and relation reasoning abilities; 3) Open-source Video-LMMs'\nperformance is significantly lower than GPT-4o and Gemini-1.5, lagging by 20\npoints. This highlights the crucial role VideoVista will play in advancing LMMs\nthat can accurately understand videos and perform precise reasoning.", "published": "2024-06-17 08:09:00", "link": "http://arxiv.org/abs/2406.11303v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "GUICourse: From General Vision Language Models to Versatile GUI Agents", "abstract": "Utilizing Graphic User Interface (GUI) for human-computer interaction is\nessential for accessing a wide range of digital tools. Recent advancements in\nVision Language Models (VLMs) highlight the compelling potential to develop\nversatile agents to help humans finish GUI navigation tasks. However, current\nVLMs are challenged in terms of fundamental abilities (OCR and grounding) and\nGUI knowledge (the functions and control methods of GUI elements), preventing\nthem from becoming practical GUI agents. To solve these challenges, we\ncontribute GUICourse, a suite of datasets to train visual-based GUI agents from\ngeneral VLMs. First, we introduce the GUIEnv dataset to strengthen the OCR and\ngrounding capabilities of VLMs. Then, we introduce the GUIAct and GUIChat\ndatasets to enrich their knowledge of GUI components and interactions.\nExperiments demonstrate that our GUI agents have better performance on common\nGUI tasks than their baseline VLMs. Even the small-size GUI agent (with 3.1B\nparameters) can still work well on single-step and multi-step GUI tasks.\nFinally, we analyze the different varieties in the training stage of this agent\nby ablation study. Our source codes and datasets are released at\nhttps://github.com/yiye3/GUICourse.", "published": "2024-06-17 08:30:55", "link": "http://arxiv.org/abs/2406.11317v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "primary_category": "cs.AI"}
{"title": "Preserving Knowledge in Large Language Model with Model-Agnostic\n  Self-Decompression", "abstract": "Humans can retain old knowledge while learning new information, but Large\nLanguage Models (LLMs) often suffer from catastrophic forgetting when\npost-pretrained or supervised fine-tuned (SFT) on domain-specific data.\nMoreover, for Multimodal Large Language Models (MLLMs) which are composed of\nthe LLM base and visual projector (e.g. LLaVA), a significant decline in\nperformance on language benchmarks was observed compared to their\nsingle-modality counterparts. To address these challenges, we introduce a novel\nmodel-agnostic self-decompression method, Tree Generation (TG), that\ndecompresses knowledge within LLMs into the training corpus. This paper focuses\non TG-SFT, which can synthetically generate SFT data for the instruction tuning\nsteps. By incorporating the dumped corpus during SFT for MLLMs, we\nsignificantly reduce the forgetting problem.", "published": "2024-06-17 09:17:40", "link": "http://arxiv.org/abs/2406.11354v2", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Fairer Preferences Elicit Improved Human-Aligned Large Language Model\n  Judgments", "abstract": "Large language models (LLMs) have shown promising abilities as cost-effective\nand reference-free evaluators for assessing language generation quality. In\nparticular, pairwise LLM evaluators, which compare two generated texts and\ndetermine the preferred one, have been employed in a wide range of\napplications. However, LLMs exhibit preference biases and worrying sensitivity\nto prompt designs. In this work, we first reveal that the predictive preference\nof LLMs can be highly brittle and skewed, even with semantically equivalent\ninstructions. We find that fairer predictive preferences from LLMs consistently\nlead to judgments that are better aligned with humans. Motivated by this\nphenomenon, we propose an automatic Zero-shot Evaluation-oriented Prompt\nOptimization framework, ZEPO, which aims to produce fairer preference decisions\nand improve the alignment of LLM evaluators with human judgments. To this end,\nwe propose a zero-shot learning objective based on the preference decision\nfairness. ZEPO demonstrates substantial performance improvements over\nstate-of-the-art LLM evaluators, without requiring labeled data, on\nrepresentative meta-evaluation benchmarks. Our findings underscore the critical\ncorrelation between preference fairness and human alignment, positioning ZEPO\nas an efficient prompt optimizer for bridging the gap between LLM evaluators\nand human judgments.", "published": "2024-06-17 09:48:53", "link": "http://arxiv.org/abs/2406.11370v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Are Small Language Models Ready to Compete with Large Language Models\n  for Practical Applications?", "abstract": "The rapid rise of Language Models (LMs) has expanded their use in several\napplications. Yet, due to constraints of model size, associated cost, or\nproprietary restrictions, utilizing state-of-the-art (SOTA) LLMs is not always\nfeasible. With open, smaller LMs emerging, more applications can leverage their\ncapabilities, but selecting the right LM can be challenging as smaller LMs do\nnot perform well universally. This work tries to bridge this gap by proposing a\nframework to experimentally evaluate small, open LMs in practical settings\nthrough measuring semantic correctness of outputs across three practical\naspects: task types, application domains, and reasoning types, using diverse\nprompt styles. It also conducts an in-depth comparison of 10 small, open LMs to\nidentify the best LM and prompt style depending on specific application\nrequirements using the proposed framework. We also show that if selected\nappropriately, they can outperform SOTA LLMs like DeepSeek-v2, GPT-4o,\nGPT-4o-mini, Gemini-1.5-Pro, and even compete with GPT-4o.", "published": "2024-06-17 10:45:36", "link": "http://arxiv.org/abs/2406.11402v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "GigaSpeech 2: An Evolving, Large-Scale and Multi-domain ASR Corpus for\n  Low-Resource Languages with Automated Crawling, Transcription and Refinement", "abstract": "The evolution of speech technology has been spurred by the rapid increase in\ndataset sizes. Traditional speech models generally depend on a large amount of\nlabeled training data, which is scarce for low-resource languages. This paper\npresents GigaSpeech 2, a large-scale, multi-domain, multilingual speech\nrecognition corpus. It is designed for low-resource languages and does not rely\non paired speech and text data. GigaSpeech 2 comprises about 30,000 hours of\nautomatically transcribed speech, including Thai, Indonesian, and Vietnamese,\ngathered from unlabeled YouTube videos. We also introduce an automated pipeline\nfor data crawling, transcription, and label refinement. Specifically, this\npipeline uses Whisper for initial transcription and TorchAudio for forced\nalignment, combined with multi-dimensional filtering for data quality\nassurance. A modified Noisy Student Training is developed to further refine\nflawed pseudo labels iteratively, thus enhancing model performance.\nExperimental results on our manually transcribed evaluation set and two public\ntest sets from Common Voice and FLEURS confirm our corpus's high quality and\nbroad applicability. Notably, ASR models trained on GigaSpeech 2 can reduce the\nword error rate for Thai, Indonesian, and Vietnamese on our challenging and\nrealistic YouTube test set by 25% to 40% compared to the Whisper large-v3\nmodel, with merely 10% model parameters. Furthermore, our ASR models trained on\nGigaspeech 2 yield superior performance compared to commercial services. We\nbelieve that our newly introduced corpus and pipeline will open a new avenue\nfor low-resource speech recognition and significantly facilitate research in\nthis area.", "published": "2024-06-17 13:44:20", "link": "http://arxiv.org/abs/2406.11546v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "GECOBench: A Gender-Controlled Text Dataset and Benchmark for\n  Quantifying Biases in Explanations", "abstract": "Large pre-trained language models have become popular for many applications\nand form an important backbone of many downstream tasks in natural language\nprocessing (NLP). Applying 'explainable artificial intelligence' (XAI)\ntechniques to enrich such models' outputs is considered crucial for assuring\ntheir quality and shedding light on their inner workings. However, large\nlanguage models are trained on a plethora of data containing a variety of\nbiases, such as gender biases, affecting model weights and, potentially,\nbehavior. Currently, it is unclear to what extent such biases also impact model\nexplanations in possibly unfavorable ways. We create a gender-controlled text\ndataset, GECO, in which otherwise identical sentences appear in male and female\nforms. This gives rise to ground-truth 'world explanations' for gender\nclassification tasks, enabling the objective evaluation of the correctness of\nXAI methods. We also provide GECOBench, a rigorous quantitative evaluation\nframework benchmarking popular XAI methods, applying them to pre-trained\nlanguage models fine-tuned to different degrees. This allows us to investigate\nhow pre-training induces undesirable bias in model explanations and to what\nextent fine-tuning can mitigate such explanation bias. We show a clear\ndependency between explanation performance and the number of fine-tuned layers,\nwhere XAI methods are observed to particularly benefit from fine-tuning or\ncomplete retraining of embedding layers. Remarkably, this relationship holds\nfor models achieving similar classification performance on the same task. With\nthat, we highlight the utility of the proposed gender-controlled dataset and\nnovel benchmarking approach for research and development of novel XAI methods.\nAll code including dataset generation, model training, evaluation and\nvisualization is available at: https://github.com/braindatalab/gecobench", "published": "2024-06-17 13:44:37", "link": "http://arxiv.org/abs/2406.11547v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.LG"}
{"title": "Towards an End-to-End Framework for Invasive Brain Signal Decoding with\n  Large Language Models", "abstract": "In this paper, we introduce a groundbreaking end-to-end (E2E) framework for\ndecoding invasive brain signals, marking a significant advancement in the field\nof speech neuroprosthesis. Our methodology leverages the comprehensive\nreasoning abilities of large language models (LLMs) to facilitate direct\ndecoding. By fully integrating LLMs, we achieve results comparable to the\nstate-of-the-art cascade models. Our findings underscore the immense potential\nof E2E frameworks in speech neuroprosthesis, particularly as the technology\nbehind brain-computer interfaces (BCIs) and the availability of relevant\ndatasets continue to evolve. This work not only showcases the efficacy of\ncombining LLMs with E2E decoding for enhancing speech neuroprosthesis but also\nsets a new direction for future research in BCI applications, underscoring the\nimpact of LLMs in decoding complex neural signals for communication\nrestoration. Code will be made available at\nhttps://github.com/FsFrancis15/BrainLLM.", "published": "2024-06-17 14:04:18", "link": "http://arxiv.org/abs/2406.11568v1", "categories": ["cs.CL", "cs.SD", "eess.AS", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "Words in Motion: Extracting Interpretable Control Vectors for Motion\n  Transformers", "abstract": "Transformer-based models generate hidden states that are difficult to\ninterpret. In this work, we analyze hidden states and modify them at inference,\nwith a focus on motion forecasting. We use linear probing to analyze whether\ninterpretable features are embedded in hidden states. Our experiments reveal\nhigh probing accuracy, indicating latent space regularities with functionally\nimportant directions. Building on this, we use the directions between hidden\nstates with opposing features to fit control vectors. At inference, we add our\ncontrol vectors to hidden states and evaluate their impact on predictions.\nRemarkably, such modifications preserve the feasibility of predictions. We\nfurther refine our control vectors using sparse autoencoders (SAEs). This leads\nto more linear changes in predictions when scaling control vectors. Our\napproach enables mechanistic interpretation as well as zero-shot generalization\nto unseen dataset characteristics with negligible computational overhead.", "published": "2024-06-17 15:07:55", "link": "http://arxiv.org/abs/2406.11624v4", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "See It from My Perspective: How Language Affects Cultural Bias in Image\n  Understanding", "abstract": "Vision-language models (VLMs) can respond to queries about images in many\nlanguages. However, beyond language, culture affects how we see things. For\nexample, individuals from Western cultures focus more on the central figure in\nan image while individuals from East Asian cultures attend more to scene\ncontext. In this work, we characterize the Western bias of VLMs in image\nunderstanding and investigate the role that language plays in this disparity.\nWe evaluate VLMs across subjective and objective visual tasks with culturally\ndiverse images and annotations. We find that VLMs perform better on the Western\nsplit than on the East Asian split of each task. Through controlled\nexperimentation, we trace one source of this bias in image understanding to the\nlack of diversity in language model construction. While inference in a language\nnearer to a culture can lead to reductions in bias, we show it is much more\neffective when that language was well-represented during text-only\npre-training. Interestingly, this yields bias reductions even when prompting in\nEnglish. Our work highlights the importance of richer representation of all\nlanguages in building equitable VLMs.", "published": "2024-06-17 15:49:51", "link": "http://arxiv.org/abs/2406.11665v2", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "BLoB: Bayesian Low-Rank Adaptation by Backpropagation for Large Language\n  Models", "abstract": "Large Language Models (LLMs) often suffer from overconfidence during\ninference, particularly when adapted to downstream domain-specific tasks with\nlimited data. Previous work addresses this issue by employing approximate\nBayesian estimation after the LLMs are trained, enabling them to quantify\nuncertainty. However, such post-training approaches' performance is severely\nlimited by the parameters learned during training. In this paper, we go beyond\npost-training Bayesianization and propose Bayesian Low-Rank Adaptation by\nBackpropagation (BLoB), an algorithm that continuously and jointly adjusts both\nthe mean and covariance of LLM parameters throughout the whole fine-tuning\nprocess. Our empirical results verify the effectiveness of BLoB in terms of\ngeneralization and uncertainty estimation, when evaluated on both\nin-distribution and out-of-distribution data.", "published": "2024-06-17 15:55:38", "link": "http://arxiv.org/abs/2406.11675v5", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Knowledge-to-Jailbreak: One Knowledge Point Worth One Attack", "abstract": "Large language models (LLMs) have been increasingly applied to various\ndomains, which triggers increasing concerns about LLMs' safety on specialized\ndomains, e.g. medicine. However, testing the domain-specific safety of LLMs is\nchallenging due to the lack of domain knowledge-driven attacks in existing\nbenchmarks. To bridge this gap, we propose a new task, knowledge-to-jailbreak,\nwhich aims to generate jailbreaks from domain knowledge to evaluate the safety\nof LLMs when applied to those domains. We collect a large-scale dataset with\n12,974 knowledge-jailbreak pairs and fine-tune a large language model as\njailbreak-generator, to produce domain knowledge-specific jailbreaks.\nExperiments on 13 domains and 8 target LLMs demonstrate the effectiveness of\njailbreak-generator in generating jailbreaks that are both relevant to the\ngiven knowledge and harmful to the target LLMs. We also apply our method to an\nout-of-domain knowledge base, showing that jailbreak-generator can generate\njailbreaks that are comparable in harmfulness to those crafted by human\nexperts. Data and code: https://github.com/THU-KEG/Knowledge-to-Jailbreak/.", "published": "2024-06-17 15:59:59", "link": "http://arxiv.org/abs/2406.11682v1", "categories": ["cs.CL", "cs.AI", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Optimizing Instructions and Demonstrations for Multi-Stage Language\n  Model Programs", "abstract": "Language Model Programs, i.e. sophisticated pipelines of modular language\nmodel (LM) calls, are increasingly advancing NLP tasks, but they require\ncrafting prompts that are jointly effective for all modules. We study prompt\noptimization for LM programs, i.e. how to update these prompts to maximize a\ndownstream metric without access to module-level labels or gradients. To make\nthis tractable, we factorize our problem into optimizing the free-form\ninstructions and few-shot demonstrations of every module and introduce several\nstrategies to craft task-grounded instructions and navigate credit assignment\nacross modules. Our strategies include (i) program- and data-aware techniques\nfor proposing effective instructions, (ii) a stochastic mini-batch evaluation\nfunction for learning a surrogate model of our objective, and (iii) a\nmeta-optimization procedure in which we refine how LMs construct proposals over\ntime. Using these insights we develop MIPRO, a novel algorithm for optimizing\nLM programs. MIPRO outperforms baseline optimizers on five of seven diverse\nmulti-stage LM programs using a best-in-class open-source model (Llama-3-8B),\nby as high as 13% accuracy. We have released our new optimizers and benchmark\nin DSPy at http://dspy.ai", "published": "2024-06-17 16:12:03", "link": "http://arxiv.org/abs/2406.11695v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Nemotron-4 340B Technical Report", "abstract": "We release the Nemotron-4 340B model family, including Nemotron-4-340B-Base,\nNemotron-4-340B-Instruct, and Nemotron-4-340B-Reward. Our models are open\naccess under the NVIDIA Open Model License Agreement, a permissive model\nlicense that allows distribution, modification, and use of the models and its\noutputs. These models perform competitively to open access models on a wide\nrange of evaluation benchmarks, and were sized to fit on a single DGX H100 with\n8 GPUs when deployed in FP8 precision. We believe that the community can\nbenefit from these models in various research studies and commercial\napplications, especially for generating synthetic data to train smaller\nlanguage models. Notably, over 98% of data used in our model alignment process\nis synthetically generated, showcasing the effectiveness of these models in\ngenerating synthetic data. To further support open research and facilitate\nmodel development, we are also open-sourcing the synthetic data generation\npipeline used in our model alignment process.", "published": "2024-06-17 16:25:04", "link": "http://arxiv.org/abs/2406.11704v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Prompts as Auto-Optimized Training Hyperparameters: Training\n  Best-in-Class IR Models from Scratch with 10 Gold Labels", "abstract": "We develop a method for training small-scale (under 100M parameter) neural\ninformation retrieval models with as few as 10 gold relevance labels. The\nmethod depends on generating synthetic queries for documents using a language\nmodel (LM), and the key step is that we automatically optimize the LM prompt\nthat is used to generate these queries based on training quality. In\nexperiments with the BIRCO benchmark, we find that models trained with our\nmethod outperform RankZephyr and are competitive with RankLLama, both of which\nare 7B parameter models trained on over 100K labels. These findings point to\nthe power of automatic prompt optimization for synthetic dataset generation.", "published": "2024-06-17 16:25:55", "link": "http://arxiv.org/abs/2406.11706v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Measuring memorization in RLHF for code completion", "abstract": "Reinforcement learning with human feedback (RLHF) has become the dominant\nmethod to align large models to user preferences. Unlike fine-tuning, for which\nthere are many studies regarding training data memorization, it is not clear\nhow memorization is affected by or introduced in the RLHF alignment process.\nUnderstanding this relationship is important as real user data may be collected\nand used to align large models; if user data is memorized during RLHF and later\nregurgitated, this could raise privacy concerns. In addition to RLHF, other\nmethods such as Direct Preference Optimization (DPO) and $\\Psi$PO have gained\npopularity for learning directly from human preferences, removing the need for\noptimizing intermediary reward models with reinforcement learning. In this\nwork, we analyze how training data memorization can surface and propagate\nthrough each phase of RLHF and direct preference learning. We focus our study\non code completion models, as code completion is one of the most popular use\ncases for large language models. We find that RLHF significantly decreases the\nchance that data used for reward modeling and reinforcement learning is\nmemorized in comparison to directly fine-tuning on this data, but that examples\nalready memorized during the fine-tuning stage of RLHF, will, in the majority\nof cases, remain memorized after RLHF. In contrast, we find that aligning by\nlearning directly from human preference data via a special case of $\\Psi$PO,\nIdentity Preference Optimization (IPO), increases the likelihood that training\ndata is regurgitated compared to RLHF. Our work suggests that RLHF, as opposed\nto direct preference learning, is a safer way to mitigate the risk of\nregurgitating sensitive preference data when aligning large language models. We\nfind our conclusions are robust across multiple code completion datasets,\ntasks, and model scales.", "published": "2024-06-17 16:33:35", "link": "http://arxiv.org/abs/2406.11715v2", "categories": ["cs.LG", "cs.CL", "cs.SE"], "primary_category": "cs.LG"}
{"title": "Refusal in Language Models Is Mediated by a Single Direction", "abstract": "Conversational large language models are fine-tuned for both\ninstruction-following and safety, resulting in models that obey benign requests\nbut refuse harmful ones. While this refusal behavior is widespread across chat\nmodels, its underlying mechanisms remain poorly understood. In this work, we\nshow that refusal is mediated by a one-dimensional subspace, across 13 popular\nopen-source chat models up to 72B parameters in size. Specifically, for each\nmodel, we find a single direction such that erasing this direction from the\nmodel's residual stream activations prevents it from refusing harmful\ninstructions, while adding this direction elicits refusal on even harmless\ninstructions. Leveraging this insight, we propose a novel white-box jailbreak\nmethod that surgically disables refusal with minimal effect on other\ncapabilities. Finally, we mechanistically analyze how adversarial suffixes\nsuppress propagation of the refusal-mediating direction. Our findings\nunderscore the brittleness of current safety fine-tuning methods. More broadly,\nour work showcases how an understanding of model internals can be leveraged to\ndevelop practical methods for controlling model behavior.", "published": "2024-06-17 16:36:12", "link": "http://arxiv.org/abs/2406.11717v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "The Right Time Matters: Data Arrangement Affects Zero-Shot\n  Generalization in Instruction Tuning", "abstract": "Understanding alignment techniques begins with comprehending zero-shot\ngeneralization brought by instruction tuning, but little of the mechanism has\nbeen understood. Existing work has largely been confined to the task level,\nwithout considering that tasks are artificially defined and, to LLMs, merely\nconsist of tokens and representations. To bridge this gap, we investigate\nzero-shot generalization from the perspective of the data itself. We first\ndemonstrate that zero-shot generalization happens very early during instruction\ntuning, with loss serving as a stable indicator. Next, we investigate training\ndata arrangement through similarity and granularity perspectives, confirming\nthat the timing of exposure to certain training examples may greatly facilitate\ngeneralization on unseen tasks. Finally, we propose a more grounded training\ndata arrangement framework, Test-centric Multi-turn Arrangement, and show its\neffectiveness in promoting continual learning and further loss reduction. For\nthe first time, we show that zero-shot generalization during instruction tuning\nis a form of similarity-based generalization between training and test data at\nthe instance level. Our code is released at\nhttps://github.com/thunlp/Dynamics-of-Zero-Shot-Generalization.", "published": "2024-06-17 16:40:21", "link": "http://arxiv.org/abs/2406.11721v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "STAR: SocioTechnical Approach to Red Teaming Language Models", "abstract": "This research introduces STAR, a sociotechnical framework that improves on\ncurrent best practices for red teaming safety of large language models. STAR\nmakes two key contributions: it enhances steerability by generating\nparameterised instructions for human red teamers, leading to improved coverage\nof the risk surface. Parameterised instructions also provide more detailed\ninsights into model failures at no increased cost. Second, STAR improves signal\nquality by matching demographics to assess harms for specific groups, resulting\nin more sensitive annotations. STAR further employs a novel step of arbitration\nto leverage diverse viewpoints and improve label reliability, treating\ndisagreement not as noise but as a valuable contribution to signal quality.", "published": "2024-06-17 17:16:45", "link": "http://arxiv.org/abs/2406.11757v4", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC"], "primary_category": "cs.AI"}
{"title": "GAMA: A Large Audio-Language Model with Advanced Audio Understanding and\n  Complex Reasoning Abilities", "abstract": "Perceiving and understanding non-speech sounds and non-verbal speech is\nessential to making decisions that help us interact with our surroundings. In\nthis paper, we propose GAMA, a novel General-purpose Large Audio-Language Model\n(LALM) with Advanced Audio Understanding and Complex Reasoning Abilities. We\nbuild GAMA by integrating an LLM with multiple types of audio representations,\nincluding features from a custom Audio Q-Former, a multi-layer aggregator that\naggregates features from multiple layers of an audio encoder. We fine-tune GAMA\non a large-scale audio-language dataset, which augments it with audio\nunderstanding capabilities. Next, we propose CompA-R (Instruction-Tuning for\nComplex Audio Reasoning), a synthetically generated instruction-tuning (IT)\ndataset with instructions that require the model to perform complex reasoning\non the input audio. We instruction-tune GAMA with CompA-R to endow it with\ncomplex reasoning abilities, where we further add a soft prompt as input with\nhigh-level semantic evidence by leveraging event tags of the input audio.\nFinally, we also propose CompA-R-test, a human-labeled evaluation dataset for\nevaluating the capabilities of LALMs on open-ended audio question-answering\nthat requires complex reasoning. Through automated and expert human\nevaluations, we show that GAMA outperforms all other LALMs in literature on\ndiverse audio understanding tasks by margins of 1%-84%. Further, GAMA IT-ed on\nCompA-R proves to be superior in its complex reasoning and instruction\nfollowing capabilities.", "published": "2024-06-17 17:31:01", "link": "http://arxiv.org/abs/2406.11768v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Split, Unlearn, Merge: Leveraging Data Attributes for More Effective\n  Unlearning in LLMs", "abstract": "Large language models (LLMs) have shown to pose social and ethical risks such\nas generating toxic language or facilitating malicious use of hazardous\nknowledge. Machine unlearning is a promising approach to improve LLM safety by\ndirectly removing harmful behaviors and knowledge. In this paper, we propose\n\"SPlit, UNlearn, MerGE\" (SPUNGE), a framework that can be used with any\nunlearning method to amplify its effectiveness. SPUNGE leverages data\nattributes during unlearning by splitting unlearning data into subsets based on\nspecific attribute values, unlearning each subset separately, and merging the\nunlearned models. We empirically demonstrate that SPUNGE significantly improves\nthe performance of two recent unlearning methods on state-of-the-art LLMs while\nmaintaining their general capabilities on standard academic benchmarks.", "published": "2024-06-17 17:35:52", "link": "http://arxiv.org/abs/2406.11780v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "CELL your Model: Contrastive Explanations for Large Language Models", "abstract": "The advent of black-box deep neural network classification models has sparked\nthe need to explain their decisions. However, in the case of generative AI,\nsuch as large language models (LLMs), there is no class prediction to explain.\nRather, one can ask why an LLM output a particular response to a given prompt.\nIn this paper, we answer this question by proposing a contrastive explanation\nmethod requiring simply black-box/query access. Our explanations suggest that\nan LLM outputs a reply to a given prompt because if the prompt was slightly\nmodified, the LLM would have given a different response that is either less\npreferable or contradicts the original response. The key insight is that\ncontrastive explanations simply require a scoring function that has meaning to\nthe user and not necessarily a specific real valued quantity (viz. class\nlabel). To this end, we offer a novel budgeted algorithm, our main algorithmic\ncontribution, which intelligently creates contrasts based on such a scoring\nfunction while adhering to a query budget, necessary for longer contexts. We\nshow the efficacy of our method on important natural language tasks such as\nopen-text generation and chatbot conversations.", "published": "2024-06-17 17:39:10", "link": "http://arxiv.org/abs/2406.11785v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Iterative Length-Regularized Direct Preference Optimization: A Case\n  Study on Improving 7B Language Models to GPT-4 Level", "abstract": "Direct Preference Optimization (DPO), a standard method for aligning language\nmodels with human preferences, is traditionally applied to offline preferences.\nRecent studies show that DPO benefits from iterative training with online\npreferences labeled by a trained reward model. In this work, we identify a\npitfall of vanilla iterative DPO - improved response quality can lead to\nincreased verbosity. To address this, we introduce iterative length-regularized\nDPO (iLR-DPO) to penalize response length. Our empirical results show that\niLR-DPO can enhance a 7B model to perform on par with GPT-4 without increasing\nverbosity. Specifically, our 7B model achieves a $50.5\\%$ length-controlled win\nrate against $\\texttt{GPT-4 Preview}$ on AlpacaEval 2.0, and excels across\nstandard benchmarks including MT-Bench, Arena-Hard and OpenLLM Leaderboard.\nThese results demonstrate the effectiveness of iterative DPO in aligning\nlanguage models with human feedback.", "published": "2024-06-17 17:55:38", "link": "http://arxiv.org/abs/2406.11817v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "WPO: Enhancing RLHF with Weighted Preference Optimization", "abstract": "Reinforcement learning from human feedback (RLHF) is a promising solution to\nalign large language models (LLMs) more closely with human values. Off-policy\npreference optimization, where the preference data is obtained from other\nmodels, is widely adopted due to its cost efficiency and scalability. However,\noff-policy preference optimization often suffers from a distributional gap\nbetween the policy used for data collection and the target policy, leading to\nsuboptimal optimization. In this paper, we propose a novel strategy to mitigate\nthis problem by simulating on-policy learning with off-policy preference data.\nOur Weighted Preference Optimization (WPO) method adapts off-policy data to\nresemble on-policy data more closely by reweighting preference pairs according\nto their probability under the current policy. This method not only addresses\nthe distributional gap problem but also enhances the optimization process\nwithout incurring additional costs. We validate our method on instruction\nfollowing benchmarks including Alpaca Eval 2 and MT-bench. WPO not only\noutperforms Direct Preference Optimization (DPO) by up to 5.6% on Alpaca Eval 2\nbut also establishes a remarkable length-controlled winning rate against\nGPT-4-turbo of 76.7% based on Gemma-2-9b-it. We release the code and models at\nhttps://github.com/wzhouad/WPO.", "published": "2024-06-17 17:59:13", "link": "http://arxiv.org/abs/2406.11827v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "mDPO: Conditional Preference Optimization for Multimodal Large Language\n  Models", "abstract": "Direct preference optimization (DPO) has shown to be an effective method for\nlarge language model (LLM) alignment. Recent works have attempted to apply DPO\nto multimodal scenarios but have found it challenging to achieve consistent\nimprovement. Through a comparative experiment, we identify the unconditional\npreference problem in multimodal preference optimization, where the model\noverlooks the image condition. To address this problem, we propose mDPO, a\nmultimodal DPO objective that prevents the over-prioritization of language-only\npreferences by also optimizing image preference. Moreover, we introduce a\nreward anchor that forces the reward to be positive for chosen responses,\nthereby avoiding the decrease in their likelihood -- an intrinsic problem of\nrelative preference optimization. Experiments on two multimodal LLMs of\ndifferent sizes and three widely used benchmarks demonstrate that mDPO\neffectively addresses the unconditional preference problem in multimodal\npreference optimization and significantly improves model performance,\nparticularly in reducing hallucination.", "published": "2024-06-17 17:59:58", "link": "http://arxiv.org/abs/2406.11839v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Explainable assessment of financial experts' credibility by classifying\n  social media forecasts and checking the predictions with actual market data", "abstract": "Social media include diverse interaction metrics related to user popularity,\nthe most evident example being the number of user followers. The latter has\nraised concerns about the credibility of the posts by the most popular\ncreators. However, most existing approaches to assess credibility in social\nmedia strictly consider this problem a binary classification, often based on a\npriori information, without checking if actual real-world facts back the users'\ncomments. In addition, they do not provide automatic explanations of their\npredictions to foster their trustworthiness. In this work, we propose a\ncredibility assessment solution for financial creators in social media that\ncombines Natural Language Processing and Machine Learning. The reputation of\nthe contributors is assessed by automatically classifying their forecasts on\nasset values by type and verifying these predictions with actual market data to\napproximate their probability of success. The outcome of this verification is a\ncontinuous credibility score instead of a binary result, an entirely novel\ncontribution by this work. Moreover, social media metrics (i.e., user context)\nare exploited by calculating their correlation with the credibility rankings,\nproviding insights on the interest of the end-users in financial posts and\ntheir forecasts (i.e., drop or rise). Finally, the system provides natural\nlanguage explanations of its decisions based on a model-agnostic analysis of\nrelevant features.", "published": "2024-06-17 08:08:03", "link": "http://arxiv.org/abs/2406.11924v1", "categories": ["cs.SI", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.SI"}
{"title": "DocCGen: Document-based Controlled Code Generation", "abstract": "Recent developments show that Large Language Models (LLMs) produce\nstate-of-the-art performance on natural language (NL) to code generation for\nresource-rich general-purpose languages like C++, Java, and Python. However,\ntheir practical usage for structured domain-specific languages (DSLs) such as\nYAML, JSON is limited due to domain-specific schema, grammar, and\ncustomizations generally unseen by LLMs during pre-training. Efforts have been\nmade to mitigate this challenge via in-context learning through relevant\nexamples or by fine-tuning. However, it suffers from problems, such as limited\nDSL samples and prompt sensitivity but enterprises maintain good documentation\nof the DSLs. Therefore, we propose DocCGen, a framework that can leverage such\nrich knowledge by breaking the NL-to-Code generation task for structured code\nlanguages into a two-step process. First, it detects the correct libraries\nusing the library documentation that best matches the NL query. Then, it\nutilizes schema rules extracted from the documentation of these libraries to\nconstrain the decoding. We evaluate our framework for two complex structured\nlanguages, Ansible YAML and Bash command, consisting of two settings:\nOut-of-domain (OOD) and In-domain (ID). Our extensive experiments show that\nDocCGen consistently improves different-sized language models across all six\nevaluation metrics, reducing syntactic and semantic errors in structured code.\nWe plan to open-source the datasets and code to motivate research in\nconstrained code generation.", "published": "2024-06-17 08:34:57", "link": "http://arxiv.org/abs/2406.11925v2", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "A Critical Study of What Code-LLMs (Do Not) Learn", "abstract": "Large Language Models trained on code corpora (code-LLMs) have demonstrated\nimpressive performance in various coding assistance tasks. However, despite\ntheir increased size and training dataset, code-LLMs still have limitations\nsuch as suggesting codes with syntactic errors, variable misuse etc. Some\nstudies argue that code-LLMs perform well on coding tasks because they use\nself-attention and hidden representations to encode relations among input\ntokens. However, previous works have not studied what code properties are not\nencoded by code-LLMs. In this paper, we conduct a fine-grained analysis of\nattention maps and hidden representations of code-LLMs. Our study indicates\nthat code-LLMs only encode relations among specific subsets of input tokens.\nSpecifically, by categorizing input tokens into syntactic tokens and\nidentifiers, we found that models encode relations among syntactic tokens and\namong identifiers, but they fail to encode relations between syntactic tokens\nand identifiers. We also found that fine-tuned models encode these relations\npoorly compared to their pre-trained counterparts. Additionally, larger models\nwith billions of parameters encode significantly less information about code\nthan models with only a few hundred million parameters.", "published": "2024-06-17 13:11:17", "link": "http://arxiv.org/abs/2406.11930v1", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "From Crowdsourced Data to High-Quality Benchmarks: Arena-Hard and\n  BenchBuilder Pipeline", "abstract": "The rapid evolution of Large Language Models (LLMs) has outpaced the\ndevelopment of model evaluation, highlighting the need for continuous curation\nof new, challenging benchmarks. However, manual curation of high-quality,\nhuman-aligned benchmarks is expensive and time-consuming. To address this, we\nintroduce BenchBuilder, an automated pipeline that leverages LLMs to curate\nhigh-quality, open-ended prompts from large, crowd-sourced datasets, enabling\ncontinuous benchmark updates without human in the loop. We apply BenchBuilder\nto datasets such as Chatbot Arena and WildChat-1M, extracting challenging\nprompts and utilizing LLM-as-a-Judge for automatic model evaluation. To\nvalidate benchmark quality, we propose new metrics to measure a benchmark's\nalignment with human preferences and ability to separate models. We release\nArena-Hard-Auto, a benchmark consisting 500 challenging prompts curated by\nBenchBuilder. Arena-Hard-Auto provides 3x higher separation of model\nperformances compared to MT-Bench and achieves 98.6% correlation with human\npreference rankings, all at a cost of $20. Our work sets a new framework for\nthe scalable curation of automated benchmarks from extensive data.", "published": "2024-06-17 17:26:10", "link": "http://arxiv.org/abs/2406.11939v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Dialogue Action Tokens: Steering Language Models in Goal-Directed\n  Dialogue with a Multi-Turn Planner", "abstract": "We present an approach called Dialogue Action Tokens (DAT) that adapts\nlanguage model agents to plan goal-directed dialogues. The core idea is to\ntreat each utterance as an action, thereby converting dialogues into games\nwhere existing approaches such as reinforcement learning can be applied.\nSpecifically, we freeze a pretrained language model and train a small planner\nmodel that predicts a continuous action vector, used for controlled generation\nin each round. This design avoids the problem of language degradation under\nreward optimization. When evaluated on the Sotopia platform for social\nsimulations, the DAT-steered LLaMA model surpasses GPT-4's performance. We also\napply DAT to steer an attacker language model in a novel multi-turn red-teaming\nsetting, revealing a potential new attack surface.", "published": "2024-06-17 18:01:32", "link": "http://arxiv.org/abs/2406.11978v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SPA-VL: A Comprehensive Safety Preference Alignment Dataset for Vision\n  Language Model", "abstract": "The emergence of Vision Language Models (VLMs) has brought unprecedented\nadvances in understanding multimodal information. The combination of textual\nand visual semantics in VLMs is highly complex and diverse, making the safety\nalignment of these models challenging. Furthermore, due to the limited study on\nthe safety alignment of VLMs, there is a lack of large-scale, high-quality\ndatasets. To address these limitations, we propose a Safety Preference\nAlignment dataset for Vision Language Models named SPA-VL. In terms of breadth,\nSPA-VL covers 6 harmfulness domains, 13 categories, and 53 subcategories, and\ncontains 100,788 samples of the quadruple (question, image, chosen response,\nrejected response). In terms of depth, the responses are collected from 12\nopen-source (e.g., QwenVL) and closed-source (e.g., Gemini) VLMs to ensure\ndiversity. The construction of preference data is fully automated, and the\nexperimental results indicate that models trained with alignment techniques on\nthe SPA-VL dataset exhibit substantial improvements in harmlessness and\nhelpfulness while maintaining core capabilities. SPA-VL, as a large-scale,\nhigh-quality, and diverse dataset, represents a significant milestone in\nensuring that VLMs achieve both harmlessness and helpfulness.", "published": "2024-06-17 18:57:37", "link": "http://arxiv.org/abs/2406.12030v3", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Large Scale Transfer Learning for Tabular Data via Language Modeling", "abstract": "Tabular data -- structured, heterogeneous, spreadsheet-style data with rows\nand columns -- is widely used in practice across many domains. However, while\nrecent foundation models have reduced the need for developing task-specific\ndatasets and predictors in domains such as language modeling and computer\nvision, this transfer learning paradigm has not had similar impact in the\ntabular domain. In this work, we seek to narrow this gap and present TabuLa-8B,\na language model for tabular prediction. We define a process for extracting a\nlarge, high-quality training dataset from the TabLib corpus, proposing methods\nfor tabular data filtering and quality control. Using the resulting dataset,\nwhich comprises over 2.1B rows from over 4M unique tables, we fine-tune a Llama\n3-8B large language model (LLM) for tabular data prediction (classification and\nbinned regression) using a novel packing and attention scheme for tabular\nprediction. Through evaluation across a test suite of 329 datasets, we find\nthat TabuLa-8B has zero-shot accuracy on unseen tables that is over 15\npercentage points (pp) higher than random guessing, a feat that is not possible\nwith existing state-of-the-art tabular prediction models (e.g. XGBoost,\nTabPFN). In the few-shot setting (1-32 shots), without any fine-tuning on the\ntarget datasets, TabuLa-8B is 5-15 pp more accurate than XGBoost and TabPFN\nmodels that are explicitly trained on equal, or even up to 16x more data. We\nrelease our model, code, and data along with the publication of this paper.", "published": "2024-06-17 18:58:20", "link": "http://arxiv.org/abs/2406.12031v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Soft Prompting for Unlearning in Large Language Models", "abstract": "The widespread popularity of Large Language Models (LLMs), partly due to\ntheir unique ability to perform in-context learning, has also brought to light\nthe importance of ethical and safety considerations when deploying these\npre-trained models. In this work, we focus on investigating machine unlearning\nfor LLMs motivated by data protection regulations. In contrast to the growing\nliterature on fine-tuning methods to achieve unlearning, we focus on a\ncomparatively lightweight alternative called soft prompting to realize the\nunlearning of a subset of training data. With losses designed to enforce\nforgetting as well as utility preservation, our framework \\textbf{S}oft\n\\textbf{P}rompting for \\textbf{U}n\\textbf{l}earning (SPUL) learns prompt tokens\nthat can be appended to an arbitrary query to induce unlearning of specific\nexamples at inference time without updating LLM parameters. We conduct a\nrigorous evaluation of the proposed method and our results indicate that SPUL\ncan significantly improve the trade-off between utility and forgetting in the\ncontext of text classification and question answering with LLMs. We further\nvalidate our method using multiple LLMs to highlight the scalability of our\nframework and provide detailed insights into the choice of hyperparameters and\nthe influence of the size of unlearning data. Our implementation is available\nat \\url{https://github.com/karuna-bhaila/llm_unlearning}.", "published": "2024-06-17 19:11:40", "link": "http://arxiv.org/abs/2406.12038v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "UniGLM: Training One Unified Language Model for Text-Attributed Graph\n  Embedding", "abstract": "Representation learning on text-attributed graphs (TAGs), where nodes are\nrepresented by textual descriptions, is crucial for textual and relational\nknowledge systems and recommendation systems. Currently, state-of-the-art\nembedding methods for TAGs primarily focus on fine-tuning language models\n(e.g., BERT) using structure-aware training signals. While effective, these\nmethods are tailored for individual TAG and cannot generalize across various\ngraph scenarios. Given the shared textual space, leveraging multiple TAGs for\njoint fine-tuning, aligning text and graph structure from different aspects,\nwould be more beneficial. Motivated by this, we introduce a novel Unified Graph\nLanguage Model (UniGLM) framework, the first graph embedding model that\ngeneralizes well to both in-domain and cross-domain TAGs. Specifically, UniGLM\nis trained over multiple TAGs with different domains and scales using\nself-supervised contrastive learning. UniGLM includes an adaptive positive\nsample selection technique for identifying structurally similar nodes and a\nlazy contrastive module that is devised to accelerate training by minimizing\nrepetitive encoding calculations. Extensive empirical results across 9\nbenchmark TAGs demonstrate UniGLM's efficacy against leading embedding\nbaselines in terms of generalization (various downstream tasks and backbones)\nand transfer learning (in and out of domain scenarios). The code is available\nat https://github.com/NYUSHCS/UniGLM.", "published": "2024-06-17 19:45:21", "link": "http://arxiv.org/abs/2406.12052v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Is poisoning a real threat to LLM alignment? Maybe more so than you\n  think", "abstract": "Recent advancements in Reinforcement Learning with Human Feedback (RLHF) have\nsignificantly impacted the alignment of Large Language Models (LLMs). The\nsensitivity of reinforcement learning algorithms such as Proximal Policy\nOptimization (PPO) has led to new line work on Direct Policy Optimization\n(DPO), which treats RLHF in a supervised learning framework. The increased\npractical use of these RLHF methods warrants an analysis of their\nvulnerabilities. In this work, we investigate the vulnerabilities of DPO to\npoisoning attacks under different scenarios and compare the effectiveness of\npreference poisoning, a first of its kind. We comprehensively analyze DPO's\nvulnerabilities under different types of attacks, i.e., backdoor and\nnon-backdoor attacks, and different poisoning methods across a wide array of\nlanguage models, i.e., LLama 7B, Mistral 7B, and Gemma 7B. We find that unlike\nPPO-based methods, which, when it comes to backdoor attacks, require at least\n4\\% of the data to be poisoned to elicit harmful behavior, we exploit the true\nvulnerabilities of DPO more simply so we can poison the model with only as much\nas 0.5\\% of the data. We further investigate the potential reasons behind the\nvulnerability and how well this vulnerability translates into backdoor vs\nnon-backdoor attacks.", "published": "2024-06-17 21:06:00", "link": "http://arxiv.org/abs/2406.12091v3", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "End-to-end Text-to-SQL Generation within an Analytics Insight Engine", "abstract": "Recent advancements in Text-to-SQL have pushed database management systems\ntowards greater democratization of data access. Today's language models are at\nthe core of these advancements. They enable impressive Text-to-SQL generation\nas experienced in the development of Distyl AI's Analytics Insight Engine. Its\nearly deployment with enterprise customers has highlighted three core\nchallenges. First, data analysts expect support with authoring SQL queries of\nvery high complexity. Second, requests are ad-hoc and, as such, require low\nlatency. Finally, generation requires an understanding of domain-specific\nterminology and practices.\n  The design and implementation of our Text-to-SQL generation pipeline, powered\nby large language models, tackles these challenges. The core tenants of our\napproach rely on external knowledge that we extract in a pre-processing phase,\non retrieving the appropriate external knowledge at query generation time, and\non decomposing SQL query generation following a hierarchical CTE-based\nstructure. Finally, an adaptation framework leverages feedback to update the\nexternal knowledge, in turn improving query generation over time. We give an\noverview of our end-to-end approach and highlight the operators generating SQL\nduring inference.", "published": "2024-06-17 21:33:01", "link": "http://arxiv.org/abs/2406.12104v1", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Enhancing Text Classification through LLM-Driven Active Learning and\n  Human Annotation", "abstract": "In the context of text classification, the financial burden of annotation\nexercises for creating training data is a critical issue. Active learning\ntechniques, particularly those rooted in uncertainty sampling, offer a\ncost-effective solution by pinpointing the most instructive samples for manual\nannotation. Similarly, Large Language Models (LLMs) such as GPT-3.5 provide an\nalternative for automated annotation but come with concerns regarding their\nreliability. This study introduces a novel methodology that integrates human\nannotators and LLMs within an Active Learning framework. We conducted\nevaluations on three public datasets. IMDB for sentiment analysis, a Fake News\ndataset for authenticity discernment, and a Movie Genres dataset for\nmulti-label classification.The proposed framework integrates human annotation\nwith the output of LLMs, depending on the model uncertainty levels. This\nstrategy achieves an optimal balance between cost efficiency and classification\nperformance. The empirical results show a substantial decrease in the costs\nassociated with data annotation while either maintaining or improving model\naccuracy.", "published": "2024-06-17 21:45:48", "link": "http://arxiv.org/abs/2406.12114v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "A dual task learning approach to fine-tune a multilingual semantic\n  speech encoder for Spoken Language Understanding", "abstract": "Self-Supervised Learning is vastly used to efficiently represent speech for\nSpoken Language Understanding, gradually replacing conventional approaches.\nMeanwhile, textual SSL models are proposed to encode language-agnostic\nsemantics. SAMU-XLSR framework employed this semantic information to enrich\nmultilingual speech representations. A recent study investigated SAMU-XLSR\nin-domain semantic enrichment by specializing it on downstream transcriptions,\nleading to state-of-the-art results on a challenging SLU task. This study's\ninterest lies in the loss of multilingual performances and lack of\nspecific-semantics training induced by such specialization in close languages\nwithout any SLU implication. We also consider SAMU-XLSR's loss of initial\ncross-lingual abilities due to a separate SLU fine-tuning. Therefore, this\npaper proposes a dual task learning approach to improve SAMU-XLSR semantic\nenrichment while considering distant languages for multilingual and language\nportability experiments.", "published": "2024-06-17 23:07:53", "link": "http://arxiv.org/abs/2406.12141v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Self-Train Before You Transcribe", "abstract": "When there is a mismatch between the training and test domains, current\nspeech recognition systems show significant performance degradation.\nSelf-training methods, such as noisy student teacher training, can help address\nthis and enable the adaptation of models under such domain shifts. However,\nself-training typically requires a collection of unlabelled target domain data.\nFor settings where this is not practical, we investigate the benefit of\nperforming noisy student teacher training on recordings in the test set as a\ntest-time adaptation approach. Similarly to the dynamic evaluation approach in\nlanguage modelling, this enables the transfer of information across utterance\nboundaries and functions as a method of domain adaptation. A range of in-domain\nand out-of-domain datasets are used for experiments demonstrating large\nrelative gains of up to 32.2%. Interestingly, our method showed larger gains\nthan the typical self-training setup that utilises separate adaptation data.", "published": "2024-06-17 09:21:00", "link": "http://arxiv.org/abs/2406.12937v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Twin-Merging: Dynamic Integration of Modular Expertise in Model Merging", "abstract": "In the era of large language models, model merging is a promising way to\ncombine multiple task-specific models into a single multitask model without\nextra training. However, two challenges remain: (a) interference between\ndifferent models and (b) heterogeneous data during testing. Traditional model\nmerging methods often show significant performance gaps compared to fine-tuned\nmodels due to these issues. Additionally, a one-size-fits-all model lacks\nflexibility for diverse test data, leading to performance degradation. We show\nthat both shared and exclusive task-specific knowledge are crucial for merging\nperformance, but directly merging exclusive knowledge hinders overall\nperformance. In view of this, we propose Twin-Merging, a method that\nencompasses two principal stages: (1) modularizing knowledge into shared and\nexclusive components, with compression to reduce redundancy and enhance\nefficiency; (2) dynamically merging shared and task-specific knowledge based on\nthe input. This approach narrows the performance gap between merged and\nfine-tuned models and improves adaptability to heterogeneous data. Extensive\nexperiments on $20$ datasets for both language and vision tasks demonstrate the\neffectiveness of our method, showing an average improvement of $28.34\\%$ in\nabsolute normalized score for discriminative tasks and even surpassing the\nfine-tuned upper bound on the generative tasks. Our implementation is available\nin \\url{https://github.com/LZY-the-boys/Twin-Merging}", "published": "2024-06-17 02:31:55", "link": "http://arxiv.org/abs/2406.15479v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On Giant's Shoulders: Effortless Weak to Strong by Dynamic Logits Fusion", "abstract": "Efficient fine-tuning of large language models for task-specific applications\nis imperative, yet the vast number of parameters in these models makes their\ntraining increasingly challenging. Despite numerous proposals for effective\nmethods, a substantial memory overhead remains for gradient computations during\nupdates. \\thm{Can we fine-tune a series of task-specific small models and\ntransfer their knowledge directly to a much larger model without additional\ntraining?} In this paper, we explore weak-to-strong specialization using logit\narithmetic, facilitating a direct answer to this question. Existing\nweak-to-strong methods often employ a static knowledge transfer ratio and a\nsingle small model for transferring complex knowledge, which leads to\nsuboptimal performance. % To address this, To surmount these limitations, we\npropose a dynamic logit fusion approach that works with a series of\ntask-specific small models, each specialized in a different task. This method\nadaptively allocates weights among these models at each decoding step, learning\nthe weights through Kullback-Leibler divergence constrained optimization\nproblems. We conduct extensive experiments across various benchmarks in both\nsingle-task and multi-task settings, achieving leading results. By transferring\nexpertise from the 7B model to the 13B model, our method closes the performance\ngap by 96.4\\% in single-task scenarios and by 86.3\\% in multi-task scenarios\ncompared to full fine-tuning of the 13B model. Notably, we achieve surpassing\nperformance on unseen tasks. Moreover, we further demonstrate that our method\ncan effortlessly integrate in-context learning for single tasks and task\narithmetic for multi-task scenarios.", "published": "2024-06-17 03:07:41", "link": "http://arxiv.org/abs/2406.15480v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Duplicate Detection with GenAI", "abstract": "Customer data is often stored as records in Customer Relations Management\nsystems (CRMs). Data which is manually entered into such systems by one of more\nusers over time leads to data replication, partial duplication or fuzzy\nduplication. This in turn means that there no longer a single source of truth\nfor customers, contacts, accounts, etc. Downstream business processes become\nincreasing complex and contrived without a unique mapping between a record in a\nCRM and the target customer. Current methods to detect and de-duplicate records\nuse traditional Natural Language Processing techniques known as Entity\nMatching. In this paper we show how using the latest advancements in Large\nLanguage Models and Generative AI can vastly improve the identification and\nrepair of duplicated records. On common benchmark datasets we find an\nimprovement in the accuracy of data de-duplication rates from 30 percent using\nNLP techniques to almost 60 percent using our proposed method.", "published": "2024-06-17 06:42:13", "link": "http://arxiv.org/abs/2406.15483v1", "categories": ["cs.CL", "cs.DB", "cs.LG"], "primary_category": "cs.CL"}
{"title": "JobFair: A Framework for Benchmarking Gender Hiring Bias in Large\n  Language Models", "abstract": "The use of Large Language Models (LLMs) in hiring has led to legislative\nactions to protect vulnerable demographic groups. This paper presents a novel\nframework for benchmarking hierarchical gender hiring bias in Large Language\nModels (LLMs) for resume scoring, revealing significant issues of reverse\ngender hiring bias and overdebiasing. Our contributions are fourfold: Firstly,\nwe introduce a new construct grounded in labour economics, legal principles,\nand critiques of current bias benchmarks: hiring bias can be categorized into\ntwo types: Level bias (difference in the average outcomes between demographic\ncounterfactual groups) and Spread bias (difference in the variance of outcomes\nbetween demographic counterfactual groups); Level bias can be further\nsubdivided into statistical bias (i.e. changing with non-demographic content)\nand taste-based bias (i.e. consistent regardless of non-demographic content).\nSecondly, the framework includes rigorous statistical and computational hiring\nbias metrics, such as Rank After Scoring (RAS), Rank-based Impact Ratio,\nPermutation Test, and Fixed Effects Model. Thirdly, we analyze gender hiring\nbiases in ten state-of-the-art LLMs. Seven out of ten LLMs show significant\nbiases against males in at least one industry. An industry-effect regression\nreveals that the healthcare industry is the most biased against males.\nMoreover, we found that the bias performance remains invariant with resume\ncontent for eight out of ten LLMs. This indicates that the bias performance\nmeasured in this paper might apply to other resume datasets with different\nresume qualities. Fourthly, we provide a user-friendly demo and resume dataset\nto support the adoption and practical use of the framework, which can be\ngeneralized to other social traits and tasks.", "published": "2024-06-17 09:15:57", "link": "http://arxiv.org/abs/2406.15484v2", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "SampleAttention: Near-Lossless Acceleration of Long Context LLM\n  Inference with Adaptive Structured Sparse Attention", "abstract": "Large language models (LLMs) now support extremely long context windows, but\nthe quadratic complexity of vanilla attention results in significantly long\nTime-to-First-Token (TTFT) latency. Existing approaches to address this\ncomplexity require additional pretraining or finetuning, and often sacrifice\nmodel accuracy. In this paper, we first provide both theoretical and empirical\nfoundations for near-lossless sparse attention. We find dynamically capturing\nhead-specific sparse patterns at runtime with low overhead is crucial. To\naddress this, we propose SampleAttention, an adaptive structured and\nnear-lossless sparse attention. Leveraging observed significant sparse\npatterns, SampleAttention attends to a fixed percentage of adjacent tokens to\ncapture local window patterns, and employs a two-stage query-guided key-value\nfiltering approach, which adaptively select a minimum set of key-values with\nlow overhead, to capture column stripe patterns. Comprehensive evaluations show\nthat SampleAttention can seamlessly replace vanilla attention in off-the-shelf\nLLMs with nearly no accuracy loss, and reduces TTFT by up to $2.42\\times$\ncompared with FlashAttention.", "published": "2024-06-17 11:05:15", "link": "http://arxiv.org/abs/2406.15486v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Compress then Serve: Serving Thousands of LoRA Adapters with Little\n  Overhead", "abstract": "Fine-tuning large language models (LLMs) with low-rank adaptations (LoRAs)\nhas become common practice, often yielding numerous copies of the same LLM\ndiffering only in their LoRA updates. This paradigm presents challenges for\nsystems that serve real-time responses to queries that each involve a different\nLoRA. Prior works optimize the design of such systems but still require\ncontinuous loading and offloading of LoRAs, as it is infeasible to store\nthousands of LoRAs in GPU memory. To mitigate this issue, we investigate the\nefficacy of compression when serving LoRAs. We propose a method for the joint\ncompression of LoRAs into a shared basis paired with LoRA-specific scaling\nmatrices. We extend our algorithm to learn clusters of LoRAs that are amenable\nto joint compression, allowing it to scale gracefully to large LoRA\ncollections. Our experiments with up to 1000 LoRAs demonstrate that compressed\nLoRAs preserve performance while offering major throughput gains in realistic\nserving scenarios with over a thousand LoRAs, maintaining 80% of the throughput\nof serving a single LoRA.", "published": "2024-06-17 15:21:35", "link": "http://arxiv.org/abs/2407.00066v3", "categories": ["cs.DC", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.DC"}
{"title": "Exploring Fusion Techniques in Multimodal AI-Based Recruitment: Insights\n  from FairCVdb", "abstract": "Despite the large body of work on fairness-aware learning for individual\nmodalities like tabular data, images, and text, less work has been done on\nmultimodal data, which fuses various modalities for a comprehensive analysis.\nIn this work, we investigate the fairness and bias implications of multimodal\nfusion techniques in the context of multimodal AI-based recruitment systems\nusing the FairCVdb dataset. Our results show that early-fusion closely matches\nthe ground truth for both demographics, achieving the lowest MAEs by\nintegrating each modality's unique characteristics. In contrast, late-fusion\nleads to highly generalized mean scores and higher MAEs. Our findings emphasise\nthe significant potential of early-fusion for accurate and fair applications,\neven in the presence of demographic biases, compared to late-fusion. Future\nresearch could explore alternative fusion strategies and incorporate\nmodality-related fairness constraints to improve fairness. For code and\nadditional insights, visit:\nhttps://github.com/Swati17293/Multimodal-AI-Based-Recruitment-FairCVdb", "published": "2024-06-17 12:37:58", "link": "http://arxiv.org/abs/2407.16892v1", "categories": ["cs.CY", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CY"}
{"title": "Bridging Social Media and Search Engines: Dredge Words and the Detection\n  of Unreliable Domains", "abstract": "Proactive content moderation requires platforms to rapidly and continuously\nevaluate the credibility of websites. Leveraging the direct and indirect paths\nusers follow to unreliable websites, we develop a website credibility\nclassification and discovery system that integrates both webgraph and\nlarge-scale social media contexts. We additionally introduce the concept of\ndredge words, terms or phrases for which unreliable domains rank highly on\nsearch engines, and provide the first exploration of their usage on social\nmedia. Our graph neural networks that combine webgraph and social media\ncontexts generate to state-of-the-art results in website credibility\nclassification and significantly improves the top-k identification of\nunreliable domains. Additionally, we release a novel dataset of dredge words,\nhighlighting their strong connections to both social media and online commerce\nplatforms.", "published": "2024-06-17 11:22:04", "link": "http://arxiv.org/abs/2406.11423v3", "categories": ["cs.SI", "cs.AI", "cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.SI"}
{"title": "DiTTo-TTS: Diffusion Transformers for Scalable Text-to-Speech without\n  Domain-Specific Factors", "abstract": "Large-scale latent diffusion models (LDMs) excel in content generation across\nvarious modalities, but their reliance on phonemes and durations in\ntext-to-speech (TTS) limits scalability and access from other fields. While\nrecent studies show potential in removing these domain-specific factors,\nperformance remains suboptimal. In this work, we introduce DiTTo-TTS, a\nDiffusion Transformer (DiT)-based TTS model, to investigate whether LDM-based\nTTS can achieve state-of-the-art performance without domain-specific factors.\nThrough rigorous analysis and empirical exploration, we find that (1) DiT with\nminimal modifications outperforms U-Net, (2) variable-length modeling with a\nspeech length predictor significantly improves results over fixed-length\napproaches, and (3) conditions like semantic alignment in speech latent\nrepresentations are key to further enhancement. By scaling our training data to\n82K hours and the model size to 790M parameters, we achieve superior or\ncomparable zero-shot performance to state-of-the-art TTS models in naturalness,\nintelligibility, and speaker similarity, all without relying on domain-specific\nfactors. Speech samples are available at https://ditto-tts.github.io.", "published": "2024-06-17 11:25:57", "link": "http://arxiv.org/abs/2406.11427v2", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Spatially constrained vs. unconstrained filtering in neural\n  spatiospectral filters for multichannel speech enhancement", "abstract": "When using artificial neural networks for multichannel speech enhancement,\nfiltering is often achieved by estimating a complex-valued mask that is applied\nto all or one reference channel of the input signal. The estimation of this\nmask is based on the noisy multichannel signal and, hence, can exploit spatial\nand spectral cues simultaneously. While it has been shown that exploiting\nspatial and spectral cues jointly is beneficial for the speech enhancement\nresult, the mechanics of the interplay of the two inside the neural network are\nstill largely unknown. In this contribution, we investigate how two\nconceptually different neural spatiospectral filters (NSSFs) exploit spatial\ncues depending on the training target signal and show that, while one NSSF\nalways performs spatial filtering, the other one is selective in leveraging\nspatial information depending on the task at hand. These insights provide\nbetter understanding of the information the NSSFs use to make their prediction\nand, thus, allow to make informed decisions regarding their design and\ndeployment.", "published": "2024-06-17 09:52:41", "link": "http://arxiv.org/abs/2406.11376v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "An Exploration of Length Generalization in Transformer-Based Speech\n  Enhancement", "abstract": "The use of Transformer architectures has facilitated remarkable progress in\nspeech enhancement. Training Transformers using substantially long speech\nutterances is often infeasible as self-attention suffers from quadratic\ncomplexity. It is a critical and unexplored challenge for a Transformer-based\nspeech enhancement model to learn from short speech utterances and generalize\nto longer ones. In this paper, we conduct comprehensive experiments to explore\nthe length generalization problem in speech enhancement with Transformer. Our\nfindings first establish that position embedding provides an effective\ninstrument to alleviate the impact of utterance length on Transformer-based\nspeech enhancement. Specifically, we explore four different position embedding\nschemes to enable length generalization. The results confirm the superiority of\nrelative position embeddings (RPEs) over absolute PE (APEs) in length\ngeneralization.", "published": "2024-06-17 10:44:29", "link": "http://arxiv.org/abs/2406.11401v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Towards Intelligent Speech Assistants in Operating Rooms: A Multimodal\n  Model for Surgical Workflow Analysis", "abstract": "To develop intelligent speech assistants and integrate them seamlessly with\nintra-operative decision-support frameworks, accurate and efficient surgical\nphase recognition is a prerequisite. In this study, we propose a multimodal\nframework based on Gated Multimodal Units (GMU) and Multi-Stage Temporal\nConvolutional Networks (MS-TCN) to recognize surgical phases of port-catheter\nplacement operations. Our method merges speech and image models and uses them\nseparately in different surgical phases. Based on the evaluation of 28\noperations, we report a frame-wise accuracy of 92.65 $\\pm$ 3.52% and an\nF1-score of 92.30 $\\pm$ 3.82%. Our results show approximately 10% improvement\nin both metrics over previous work and validate the effectiveness of\nintegrating multimodal data for the surgical phase recognition task. We further\ninvestigate the contribution of individual data channels by comparing\nmono-modal models with multimodal models.", "published": "2024-06-17 12:47:04", "link": "http://arxiv.org/abs/2406.14576v1", "categories": ["eess.AS", "00b20"], "primary_category": "eess.AS"}
{"title": "Identification of Physical Properties in Acoustic Tubes Using\n  Physics-Informed Neural Networks", "abstract": "Physics-informed Neural Networks (PINNs) is a method for numerical simulation\nthat incorporates a loss function corresponding to the governing equations into\na neural network. While PINNs have been explored for their utility in inverse\nanalysis, their application in acoustic analysis remains limited. This study\npresents a method to identify loss parameters in acoustic tubes using PINNs. We\ncategorized the loss parameters into two groups: one dependent on the tube's\ndiameter and another constant, independent of it. The latter were set as the\ntrainable parameters of the neural network. The problem of identifying the loss\nparameter was formulated as an optimization problem, with the physical\nproperties being determined through this process. The neural network\narchitecture employed was based on our previously proposed ResoNet, which is\ndesigned for analyzing acoustic resonance. The efficacy of the proposed method\nis assessed through both forward and inverse analysis, specifically through the\nidentification of loss parameters. The findings demonstrate that it is feasible\nto accurately identify parameters that significantly impact the sound field\nunder analysis. By merely altering the governing equations in the loss\nfunction, this method could be adapted to various sound fields, suggesting its\npotential for broad application.", "published": "2024-06-17 00:41:26", "link": "http://arxiv.org/abs/2406.11119v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Self-Distillation Prototypes Network: Learning Robust Speaker\n  Representations without Supervision", "abstract": "Training speaker-discriminative and robust speaker verification systems\nwithout explicit speaker labels remains a persisting challenge. In this paper,\nwe propose a new self-supervised speaker verification approach,\nSelf-Distillation Prototypes Network (SDPN), which effectively facilitates\nself-supervised speaker representation learning. SDPN assigns the\nrepresentation of the augmented views of an utterance to the same prototypes as\nthe representation of the original view, thereby enabling effective knowledge\ntransfer between the views. Originally, due to the lack of negative pairs in\nthe SDPN training process, the network tends to align positive pairs very\nclosely in the embedding space, a phenomenon known as model collapse. To\nalleviate this problem, we introduce a diversity regularization term to\nembeddings in SDPN. Comprehensive experiments on the VoxCeleb datasets\ndemonstrate the superiority of SDPN in self-supervised speaker verification.\nSDPN sets a new state-of-the-art on the VoxCeleb1 speaker verification\nevaluation benchmark, achieving Equal Error Rate 1.80%, 1.99%, and 3.62% for\ntrial VoxCeleb1-O, VoxCeleb1-E and VoxCeleb1-H respectively, without using any\nspeaker labels in training.", "published": "2024-06-17 03:20:11", "link": "http://arxiv.org/abs/2406.11169v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SMRU: Split-and-Merge Recurrent-based UNet for Acoustic Echo\n  Cancellation and Noise Suppression", "abstract": "The proliferation of deep neural networks has spawned the rapid development\nof acoustic echo cancellation and noise suppression, and plenty of prior arts\nhave been proposed, which yield promising performance. Nevertheless, they\nrarely consider the deployment generality in different processing scenarios,\nsuch as edge devices, and cloud processing. To this end, this paper proposes a\ngeneral model, termed SMRU, to cover different application scenarios. The\nnovelty lies in two-fold. First, a multi-scale band split layer and band merge\nlayer are proposed to effectively fuse local frequency bands for lower\ncomplexity modeling. Besides, by simulating the multi-resolution feature\nmodeling characteristic of the classical UNet structure, a novel\nrecurrent-dominated UNet is devised. It consists of multiple variable frame\nrate blocks, each of which involves the causal time down-/up-sampling layer\nwith varying compression ratios and the dual-path structure for inter- and\nintra-band modeling. The model is configured from 50 M/s to 6.8 G/s in terms of\nMACs, and the experimental results show that the proposed approach yields\ncompetitive or even better performance over existing baselines, and has the\nfull potential to adapt to more general scenarios with varying complexity\nrequirements.", "published": "2024-06-17 03:28:08", "link": "http://arxiv.org/abs/2406.11175v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AnoPatch: Towards Better Consistency in Machine Anomalous Sound\n  Detection", "abstract": "Large pre-trained models have demonstrated dominant performances in multiple\nareas, where the consistency between pre-training and fine-tuning is the key to\nsuccess. However, few works reported satisfactory results of pre-trained models\nfor the machine anomalous sound detection (ASD) task. This may be caused by the\ninconsistency of the pre-trained model and the inductive bias of machine audio,\nresulting in inconsistency in data and architecture. Thus, we propose AnoPatch\nwhich utilizes a ViT backbone pre-trained on AudioSet and fine-tunes it on\nmachine audio. It is believed that machine audio is more related to audio\ndatasets than speech datasets, and modeling it from patch level suits the\nsparsity of machine audio. As a result, AnoPatch showcases state-of-the-art\n(SOTA) performances on the DCASE 2020 ASD dataset and the DCASE 2023 ASD\ndataset. We also compare multiple pre-trained models and empirically\ndemonstrate that better consistency yields considerable improvement.", "published": "2024-06-17 09:37:29", "link": "http://arxiv.org/abs/2406.11364v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AV-CrossNet: an Audiovisual Complex Spectral Mapping Network for Speech\n  Separation By Leveraging Narrow- and Cross-Band Modeling", "abstract": "Adding visual cues to audio-based speech separation can improve separation\nperformance. This paper introduces AV-CrossNet, an audiovisual (AV) system for\nspeech enhancement, target speaker extraction, and multi-talker speaker\nseparation. AV-CrossNet is extended from the CrossNet architecture, which is a\nrecently proposed network that performs complex spectral mapping for speech\nseparation by leveraging global attention and positional encoding. To\neffectively utilize visual cues, the proposed system incorporates pre-extracted\nvisual embeddings and employs a visual encoder comprising temporal\nconvolutional layers. Audio and visual features are fused in an early fusion\nlayer before feeding to AV-CrossNet blocks. We evaluate AV-CrossNet on multiple\ndatasets, including LRS, VoxCeleb, and COG-MHEAR challenge. Evaluation results\ndemonstrate that AV-CrossNet advances the state-of-the-art performance in all\naudiovisual tasks, even on untrained and mismatched datasets.", "published": "2024-06-17 15:04:15", "link": "http://arxiv.org/abs/2406.11619v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Sound event detection based on auxiliary decoder and maximum probability\n  aggregation for DCASE Challenge 2024 Task 4", "abstract": "In this report, we propose three novel methods for developing a sound event\ndetection (SED) model for the DCASE 2024 Challenge Task 4. First, we propose an\nauxiliary decoder attached to the final convolutional block to improve feature\nextraction capabilities while reducing dependency on embeddings from\npre-trained large models. The proposed auxiliary decoder operates independently\nfrom the main decoder, enhancing performance of the convolutional block during\nthe initial training stages by assigning a different weight strategy between\nmain and auxiliary decoder losses. Next, to address the time interval issue\nbetween the DESED and MAESTRO datasets, we propose maximum probability\naggregation (MPA) during the training step. The proposed MPA method enables the\nmodel's output to be aligned with soft labels of 1 s in the MAESTRO dataset.\nFinally, we propose a multi-channel input feature that employs various versions\nof logmel and MFCC features to generate time-frequency pattern. The\nexperimental results demonstrate the efficacy of these proposed methods in a\nview of improving SED performance by achieving a balanced enhancement across\ndifferent datasets and label types. Ultimately, this approach presents a\nsignificant step forward in developing more robust and flexible SED models", "published": "2024-06-17 06:41:12", "link": "http://arxiv.org/abs/2406.12721v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Performance Improvement of Language-Queried Audio Source Separation\n  Based on Caption Augmentation From Large Language Models for DCASE Challenge\n  2024 Task 9", "abstract": "We present a prompt-engineering-based text-augmentation approach applied to a\nlanguage-queried audio source separation (LASS) task. To enhance the\nperformance of LASS, the proposed approach utilizes large language models\n(LLMs) to generate multiple captions corresponding to each sentence of the\ntraining dataset. To this end, we first perform experiments to identify the\nmost effective prompts for caption augmentation with a smaller number of\ncaptions. A LASS model trained with these augmented captions demonstrates\nimproved performance on the DCASE 2024 Task 9 validation set compared to that\ntrained without augmentation. This study highlights the effectiveness of\nLLM-based caption augmentation in advancing language-queried audio source\nseparation.", "published": "2024-06-17 06:19:14", "link": "http://arxiv.org/abs/2406.11248v2", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MusicScore: A Dataset for Music Score Modeling and Generation", "abstract": "Music scores are written representations of music and contain rich\ninformation about musical components. The visual information on music scores\nincludes notes, rests, staff lines, clefs, dynamics, and articulations. This\nvisual information in music scores contains more semantic information than\naudio and symbolic representations of music. Previous music score datasets have\nlimited sizes and are mainly designed for optical music recognition (OMR).\nThere is a lack of research on creating a large-scale benchmark dataset for\nmusic modeling and generation. In this work, we propose MusicScore, a\nlarge-scale music score dataset collected and processed from the International\nMusic Score Library Project (IMSLP). MusicScore consists of image-text pairs,\nwhere the image is a page of a music score and the text is the metadata of the\nmusic. The metadata of MusicScore is extracted from the general information\nsection of the IMSLP pages. The metadata includes rich information about the\ncomposer, instrument, piece style, and genre of the music pieces. MusicScore is\ncurated into small, medium, and large scales of 400, 14k, and 200k image-text\npairs with varying diversity, respectively. We build a score generation system\nbased on a UNet diffusion model to generate visually readable music scores\nconditioned on text descriptions to benchmark the MusicScore dataset for music\nscore generation. MusicScore is released to the public at\nhttps://huggingface.co/datasets/ZheqiDAI/MusicScore.", "published": "2024-06-17 12:24:20", "link": "http://arxiv.org/abs/2406.11462v1", "categories": ["cs.MM", "cs.GR", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
