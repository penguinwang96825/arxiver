{"title": "Identifying and Measuring Token-Level Sentiment Bias in Pre-trained\n  Language Models with Prompts", "abstract": "Due to the superior performance, large-scale pre-trained language models\n(PLMs) have been widely adopted in many aspects of human society. However, we\nstill lack effective tools to understand the potential bias embedded in the\nblack-box models. Recent advances in prompt tuning show the possibility to\nexplore the internal mechanism of the PLMs. In this work, we propose two\ntoken-level sentiment tests: Sentiment Association Test (SAT) and Sentiment\nShift Test (SST) which utilize the prompt as a probe to detect the latent bias\nin the PLMs. Our experiments on the collection of sentiment datasets show that\nboth SAT and SST can identify sentiment bias in PLMs and SST is able to\nquantify the bias. The results also suggest that fine-tuning can possibly\naugment the existing bias in PLMs.", "published": "2022-04-15 02:01:31", "link": "http://arxiv.org/abs/2204.07289v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Where to Go for the Holidays: Towards Mixed-Type Dialogs for\n  Clarification of User Goals", "abstract": "Most dialog systems posit that users have figured out clear and specific\ngoals before starting an interaction. For example, users have determined the\ndeparture, the destination, and the travel time for booking a flight. However,\nin many scenarios, limited by experience and knowledge, users may know what\nthey need, but still struggle to figure out clear and specific goals by\ndetermining all the necessary slots.\n  In this paper, we identify this challenge and make a step forward by\ncollecting a new human-to-human mixed-type dialog corpus. It contains 5k dialog\nsessions and 168k utterances for 4 dialog types and 5 domains. Within each\nsession, an agent first provides user-goal-related knowledge to help figure out\nclear and specific goals, and then help achieve them.\n  Furthermore, we propose a mixed-type dialog model with a novel Prompt-based\ncontinual learning mechanism. Specifically, the mechanism enables the model to\ncontinually strengthen its ability on any specific type by utilizing existing\ndialog corpora effectively.", "published": "2022-04-15 02:30:45", "link": "http://arxiv.org/abs/2204.07299v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Training Entire-Space Models for Target-oriented Opinion Words\n  Extraction", "abstract": "Target-oriented opinion words extraction (TOWE) is a subtask of aspect-based\nsentiment analysis (ABSA). Given a sentence and an aspect term occurring in the\nsentence, TOWE extracts the corresponding opinion words for the aspect term.\nTOWE has two types of instance. In the first type, aspect terms are associated\nwith at least one opinion word, while in the second type, aspect terms do not\nhave corresponding opinion words. However, previous researches trained and\nevaluated their models with only the first type of instance, resulting in a\nsample selection bias problem. Specifically, TOWE models were trained with only\nthe first type of instance, while these models would be utilized to make\ninference on the entire space with both the first type of instance and the\nsecond type of instance. Thus, the generalization performance will be hurt.\nMoreover, the performance of these models on the first type of instance cannot\nreflect their performance on entire space. To validate the sample selection\nbias problem, four popular TOWE datasets containing only aspect terms\nassociated with at least one opinion word are extended and additionally include\naspect terms without corresponding opinion words. Experimental results on these\ndatasets show that training TOWE models on entire space will significantly\nimprove model performance and evaluating TOWE models only on the first type of\ninstance will overestimate model performance.", "published": "2022-04-15 05:39:25", "link": "http://arxiv.org/abs/2204.07337v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LaMemo: Language Modeling with Look-Ahead Memory", "abstract": "Although Transformers with fully connected self-attentions are powerful to\nmodel long-term dependencies, they are struggling to scale to long texts with\nthousands of words in language modeling. One of the solutions is to equip the\nmodel with a recurrence memory. However, existing approaches directly reuse\nhidden states from the previous segment that encodes contexts in a\nuni-directional way. As a result, this prohibits the memory to dynamically\ninteract with the current context that provides up-to-date information for\ntoken prediction. To remedy this issue, we propose Look-Ahead Memory (LaMemo)\nthat enhances the recurrence memory by incrementally attending to the\nright-side tokens, and interpolating with the old memory states to maintain\nlong-term information in the history. LaMemo embraces bi-directional attention\nand segment recurrence with an additional computation overhead only linearly\nproportional to the memory length. Experiments on widely used language modeling\nbenchmarks demonstrate its superiority over the baselines equipped with\ndifferent types of memory.", "published": "2022-04-15 06:11:25", "link": "http://arxiv.org/abs/2204.07341v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text Revision by On-the-Fly Representation Optimization", "abstract": "Text revision refers to a family of natural language generation tasks, where\nthe source and target sequences share moderate resemblance in surface form but\ndifferentiate in attributes, such as text formality and simplicity. Current\nstate-of-the-art methods formulate these tasks as sequence-to-sequence learning\nproblems, which rely on large-scale parallel training corpus. In this paper, we\npresent an iterative in-place editing approach for text revision, which\nrequires no parallel data. In this approach, we simply fine-tune a pre-trained\nTransformer with masked language modeling and attribute classification. During\ninference, the editing at each iteration is realized by two-step span\nreplacement. At the first step, the distributed representation of the text\noptimizes on the fly towards an attribute function. At the second step, a text\nspan is masked and another new one is proposed conditioned on the optimized\nrepresentation. The empirical experiments on two typical and important text\nrevision tasks, text formalization and text simplification, show the\neffectiveness of our approach. It achieves competitive and even better\nperformance than state-of-the-art supervised methods on text simplification,\nand gains better performance than strong unsupervised methods on text\nformalization \\footnote{Code and model are available at\n\\url{https://github.com/jingjingli01/OREO}}.", "published": "2022-04-15 07:38:08", "link": "http://arxiv.org/abs/2204.07359v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Role of Pre-trained Language Models in Word Ordering: A Case\n  Study with BART", "abstract": "Word ordering is a constrained language generation task taking unordered\nwords as input. Existing work uses linear models and neural networks for the\ntask, yet pre-trained language models have not been studied in word ordering,\nlet alone why they help. We use BART as an instance and show its effectiveness\nin the task. To explain why BART helps word ordering, we extend analysis with\nprobing and empirically identify that syntactic dependency knowledge in BART is\na reliable explanation. We also report performance gains with BART in the\nrelated partial tree linearization task, which readily extends our analysis.", "published": "2022-04-15 08:03:35", "link": "http://arxiv.org/abs/2204.07367v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ML_LTU at SemEval-2022 Task 4: T5 Towards Identifying Patronizing and\n  Condescending Language", "abstract": "This paper describes the system used by the Machine Learning Group of LTU in\nsubtask 1 of the SemEval-2022 Task 4: Patronizing and Condescending Language\n(PCL) Detection. Our system consists of finetuning a pretrained\nText-to-Text-Transfer Transformer (T5) and innovatively reducing its\nout-of-class predictions. The main contributions of this paper are 1) the\ndescription of the implementation details of the T5 model we used, 2) analysis\nof the successes & struggles of the model in this task, and 3) ablation studies\nbeyond the official submission to ascertain the relative importance of data\nsplit. Our model achieves an F1 score of 0.5452 on the official test set.", "published": "2022-04-15 12:00:25", "link": "http://arxiv.org/abs/2204.07432v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mixture of Experts for Biomedical Question Answering", "abstract": "Biomedical Question Answering (BQA) has attracted increasing attention in\nrecent years due to its promising application prospect. It is a challenging\ntask because the biomedical questions are professional and usually vary widely.\nExisting question answering methods answer all questions with a homogeneous\nmodel, leading to various types of questions competing for the shared\nparameters, which will confuse the model decision for each single type of\nquestions. In this paper, in order to alleviate the parameter competition\nproblem, we propose a Mixture-of-Expert (MoE) based question answering method\ncalled MoEBQA that decouples the computation for different types of questions\nby sparse routing. To be specific, we split a pretrained Transformer model into\nbottom and top blocks. The bottom blocks are shared by all the examples, aiming\nto capture the general features. The top blocks are extended to an MoE version\nthat consists of a series of independent experts, where each example is\nassigned to a few experts according to its underlying question type. MoEBQA\nautomatically learns the routing strategy in an end-to-end manner so that each\nexpert tends to deal with the question types it is expert in. We evaluate\nMoEBQA on three BQA datasets constructed based on real examinations. The\nresults show that our MoE extension significantly boosts the performance of\nquestion answering models and achieves new state-of-the-art performance. In\naddition, we elaborately analyze our MoE modules to reveal how MoEBQA works and\nfind that it can automatically group the questions into human-readable\nclusters.", "published": "2022-04-15 14:11:40", "link": "http://arxiv.org/abs/2204.07469v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Human Judgement as a Compass to Navigate Automatic Metrics for Formality\n  Transfer", "abstract": "Although text style transfer has witnessed rapid development in recent years,\nthere is as yet no established standard for evaluation, which is performed\nusing several automatic metrics, lacking the possibility of always resorting to\nhuman judgement. We focus on the task of formality transfer, and on the three\naspects that are usually evaluated: style strength, content preservation, and\nfluency. To cast light on how such aspects are assessed by common and new\nmetrics, we run a human-based evaluation and perform a rich correlation\nanalysis. We are then able to offer some recommendations on the use of such\nmetrics in formality transfer, also with an eye to their generalisability (or\nnot) to related tasks.", "published": "2022-04-15 17:15:52", "link": "http://arxiv.org/abs/2204.07549v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Chinese Idiom Paraphrasing", "abstract": "Idioms, are a kind of idiomatic expression in Chinese, most of which consist\nof four Chinese characters. Due to the properties of non-compositionality and\nmetaphorical meaning, Chinese Idioms are hard to be understood by children and\nnon-native speakers. This study proposes a novel task, denoted as Chinese Idiom\nParaphrasing (CIP). CIP aims to rephrase idioms-included sentences to\nnon-idiomatic ones under the premise of preserving the original sentence's\nmeaning. Since the sentences without idioms are easier handled by Chinese NLP\nsystems, CIP can be used to pre-process Chinese datasets, thereby facilitating\nand improving the performance of Chinese NLP tasks, e.g., machine translation\nsystem, Chinese idiom cloze, and Chinese idiom embeddings. In this study, CIP\ntask is treated as a special paraphrase generation task. To circumvent\ndifficulties in acquiring annotations, we first establish a large-scale CIP\ndataset based on human and machine collaboration, which consists of 115,530\nsentence pairs. We further deploy three baselines and two novel CIP approaches\nto deal with CIP problems. The results show that the proposed methods have\nbetter performances than the baselines based on the established CIP dataset.", "published": "2022-04-15 17:24:25", "link": "http://arxiv.org/abs/2204.07555v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Factuality in Text Simplification", "abstract": "Automated simplification models aim to make input texts more readable. Such\nmethods have the potential to make complex information accessible to a wider\naudience, e.g., providing access to recent medical literature which might\notherwise be impenetrable for a lay reader. However, such models risk\nintroducing errors into automatically simplified texts, for instance by\ninserting statements unsupported by the corresponding original text, or by\nomitting key information. Providing more readable but inaccurate versions of\ntexts may in many cases be worse than providing no such access at all. The\nproblem of factual accuracy (and the lack thereof) has received heightened\nattention in the context of summarization models, but the factuality of\nautomatically simplified texts has not been investigated. We introduce a\ntaxonomy of errors that we use to analyze both references drawn from standard\nsimplification datasets and state-of-the-art model outputs. We find that errors\noften appear in both that are not captured by existing evaluation metrics,\nmotivating a need for research into ensuring the factual accuracy of automated\nsimplification models.", "published": "2022-04-15 17:37:09", "link": "http://arxiv.org/abs/2204.07562v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "In-BoXBART: Get Instructions into Biomedical Multi-Task Learning", "abstract": "Single-task models have proven pivotal in solving specific tasks; however,\nthey have limitations in real-world applications where multi-tasking is\nnecessary and domain shifts are exhibited. Recently, instructional prompts have\nshown significant improvement towards multi-task generalization; however, the\neffect of instructional prompts and Multi-Task Learning (MTL) has not been\nsystematically studied in the biomedical domain. Motivated by this, this paper\nexplores the impact of instructional prompts for biomedical MTL. We introduce\nthe BoX, a collection of 32 instruction tasks for Biomedical NLP across (X)\nvarious categories. Using this meta-dataset, we propose a unified model termed\nIn-BoXBART, that can jointly learn all tasks of the BoX without any\ntask-specific modules. To the best of our knowledge, this is the first attempt\nto propose a unified model in the biomedical domain and use instructions to\nachieve generalization across several biomedical tasks. Experimental results\nindicate that the proposed model: 1) outperforms the single-task baseline by\n~3% and multi-task (without instruction) baseline by ~18% on an average, and 2)\nshows ~23% improvement compared to the single-task baseline in few-shot\nlearning (i.e., 32 instances per task) on an average. Our analysis indicates\nthat there is significant room for improvement across tasks in the BoX,\nimplying the scope for future research direction.", "published": "2022-04-15 18:06:22", "link": "http://arxiv.org/abs/2204.07600v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Finding Pareto Trade-offs in Fair and Accurate Detection of Toxic Speech", "abstract": "Optimizing NLP models for fairness poses many challenges. Lack of\ndifferentiable fairness measures prevents gradient-based loss training or\nrequires surrogate losses that diverge from the true metric of interest. In\naddition, competing objectives (e.g., accuracy vs. fairness) often require\nmaking trade-offs based on stakeholder preferences, but stakeholders may not\nknow their preferences before seeing system performance under different\ntrade-off settings. To address these challenges, we begin by formulating a\ndifferentiable version of a popular fairness measure, Accuracy Parity, to\nprovide balanced accuracy across demographic groups. Next, we show how\nmodel-agnostic, HyperNetwork optimization can efficiently train arbitrary NLP\nmodel architectures to learn Pareto-optimal trade-offs between competing\nmetrics. Focusing on the task of toxic language detection, we show the\ngenerality and efficacy of our methods across two datasets, three neural\narchitectures, and three fairness losses.", "published": "2022-04-15 22:11:25", "link": "http://arxiv.org/abs/2204.07661v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Spanish Abstract Meaning Representation: Annotation of a General Corpus", "abstract": "The Abstract Meaning Representation (AMR) formalism, designed originally for\nEnglish, has been adapted to a number of languages. We build on previous work\nproposing the annotation of AMR in Spanish, which resulted in the release of 50\nSpanish AMR annotations for the fictional text \"The Little Prince.\" In this\nwork, we present the first sizable, general annotation project for Spanish\nAbstract Meaning Representation. Our approach to annotation makes use of\nSpanish rolesets from the AnCora-Net lexicon and extends English AMR with\nsemantic features specific to Spanish. In addition to our guidelines, we\nrelease an annotated corpus (586 annotations total, for 486 unique sentences)\nof multiple genres of documents from the \"Abstract Meaning Representation 2.0 -\nFour Translations\" sembank. This corpus is commonly used for evaluation of AMR\nparsing and generation, but does not include gold AMRs; we hope that providing\ngold annotations for this dataset can result in a more complete approach to\ncross-lingual AMR parsing. Finally, we perform a disagreement analysis and\ndiscuss the implications of our work on the adaptability of AMR to languages\nother than English.", "published": "2022-04-15 22:26:04", "link": "http://arxiv.org/abs/2204.07663v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CILDA: Contrastive Data Augmentation using Intermediate Layer Knowledge\n  Distillation", "abstract": "Knowledge distillation (KD) is an efficient framework for compressing\nlarge-scale pre-trained language models. Recent years have seen a surge of\nresearch aiming to improve KD by leveraging Contrastive Learning, Intermediate\nLayer Distillation, Data Augmentation, and Adversarial Training. In this work,\nwe propose a learning based data augmentation technique tailored for knowledge\ndistillation, called CILDA. To the best of our knowledge, this is the first\ntime that intermediate layer representations of the main task are used in\nimproving the quality of augmented samples. More precisely, we introduce an\naugmentation technique for KD based on intermediate layer matching using\ncontrastive loss to improve masked adversarial data augmentation. CILDA\noutperforms existing state-of-the-art KD approaches on the GLUE benchmark, as\nwell as in an out-of-domain evaluation.", "published": "2022-04-15 23:16:37", "link": "http://arxiv.org/abs/2204.07674v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MoEBERT: from BERT to Mixture-of-Experts via Importance-Guided\n  Adaptation", "abstract": "Pre-trained language models have demonstrated superior performance in various\nnatural language processing tasks. However, these models usually contain\nhundreds of millions of parameters, which limits their practicality because of\nlatency requirements in real-world applications. Existing methods train small\ncompressed models via knowledge distillation. However, performance of these\nsmall models drops significantly compared with the pre-trained models due to\ntheir reduced model capacity. We propose MoEBERT, which uses a\nMixture-of-Experts structure to increase model capacity and inference speed. We\ninitialize MoEBERT by adapting the feed-forward neural networks in a\npre-trained model into multiple experts. As such, representation power of the\npre-trained model is largely retained. During inference, only one of the\nexperts is activated, such that speed can be improved. We also propose a\nlayer-wise distillation method to train MoEBERT. We validate the efficiency and\neffectiveness of MoEBERT on natural language understanding and question\nanswering tasks. Results show that the proposed method outperforms existing\ntask-specific distillation algorithms. For example, our method outperforms\nprevious approaches by over 2% on the MNLI (mismatched) dataset. Our code is\npublicly available at https://github.com/SimiaoZuo/MoEBERT.", "published": "2022-04-15 23:19:37", "link": "http://arxiv.org/abs/2204.07675v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DialAug: Mixing up Dialogue Contexts in Contrastive Learning for Robust\n  Conversational Modeling", "abstract": "Retrieval-based conversational systems learn to rank response candidates for\na given dialogue context by computing the similarity between their vector\nrepresentations. However, training on a single textual form of the multi-turn\ncontext limits the ability of a model to learn representations that generalize\nto natural perturbations seen during inference. In this paper we propose a\nframework that incorporates augmented versions of a dialogue context into the\nlearning objective. We utilize contrastive learning as an auxiliary objective\nto learn robust dialogue context representations that are invariant to\nperturbations injected through the augmentation method. We experiment with four\nbenchmark dialogue datasets and demonstrate that our framework combines well\nwith existing augmentation methods and can significantly improve over baseline\nBERT-based ranking architectures. Furthermore, we propose a novel data\naugmentation method, ConMix, that adds token level perturbations through\nstochastic mixing of tokens from other contexts in the batch. We show that our\nproposed augmentation method outperforms previous data augmentation approaches,\nand provides dialogue representations that are more robust to common\nperturbations seen during inference.", "published": "2022-04-15 23:39:41", "link": "http://arxiv.org/abs/2204.07679v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Incremental Prompting: Episodic Memory Prompt for Lifelong Event\n  Detection", "abstract": "Lifelong event detection aims to incrementally update a model with new event\ntypes and data while retaining the capability on previously learned old types.\nOne critical challenge is that the model would catastrophically forget old\ntypes when continually trained on new data. In this paper, we introduce\nEpisodic Memory Prompts (EMP) to explicitly preserve the learned task-specific\nknowledge. Our method adopts continuous prompt for each task and they are\noptimized to instruct the model prediction and learn event-specific\nrepresentation. The EMPs learned in previous tasks are carried along with the\nmodel in subsequent tasks, and can serve as a memory module that keeps the old\nknowledge and transferring to new tasks. Experiment results demonstrate the\neffectiveness of our method. Furthermore, we also conduct a comprehensive\nanalysis of the new and old event types in lifelong learning.", "published": "2022-04-15 00:21:31", "link": "http://arxiv.org/abs/2204.07275v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Characterizing the Efficiency vs. Accuracy Trade-off for Long-Context\n  NLP Models", "abstract": "With many real-world applications of Natural Language Processing (NLP)\ncomprising of long texts, there has been a rise in NLP benchmarks that measure\nthe accuracy of models that can handle longer input sequences. However, these\nbenchmarks do not consider the trade-offs between accuracy, speed, and power\nconsumption as input sizes or model sizes are varied. In this work, we perform\na systematic study of this accuracy vs. efficiency trade-off on two widely used\nlong-sequence models - Longformer-Encoder-Decoder (LED) and Big Bird - during\nfine-tuning and inference on four datasets from the SCROLLS benchmark. To study\nhow this trade-off differs across hyperparameter settings, we compare the\nmodels across four sequence lengths (1024, 2048, 3072, 4096) and two model\nsizes (base and large) under a fixed resource budget. We find that LED\nconsistently achieves better accuracy at lower energy costs than Big Bird. For\nsummarization, we find that increasing model size is more energy efficient than\nincreasing sequence length for higher accuracy. However, this comes at the cost\nof a large drop in inference speed. For question answering, we find that\nsmaller models are both more efficient and more accurate due to the larger\ntraining batch sizes possible under a fixed resource budget.", "published": "2022-04-15 01:52:45", "link": "http://arxiv.org/abs/2204.07288v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Cross-Modal Understanding in Visual Dialog via Contrastive\n  Learning", "abstract": "Visual Dialog is a challenging vision-language task since the visual dialog\nagent needs to answer a series of questions after reasoning over both the image\ncontent and dialog history. Though existing methods try to deal with the\ncross-modal understanding in visual dialog, they are still not enough in\nranking candidate answers based on their understanding of visual and textual\ncontexts. In this paper, we analyze the cross-modal understanding in visual\ndialog based on the vision-language pre-training model VD-BERT and propose a\nnovel approach to improve the cross-modal understanding for visual dialog,\nnamed ICMU. ICMU enhances cross-modal understanding by distinguishing different\npulled inputs (i.e. pulled images, questions or answers) based on four-way\ncontrastive learning. In addition, ICMU exploits the single-turn visual\nquestion answering to enhance the visual dialog model's cross-modal\nunderstanding to handle a multi-turn visually-grounded conversation.\nExperiments show that the proposed approach improves the visual dialog model's\ncross-modal understanding and brings satisfactory gain to the VisDial dataset.", "published": "2022-04-15 02:36:52", "link": "http://arxiv.org/abs/2204.07302v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Vision-and-Language Pretrained Models: A Survey", "abstract": "Pretrained models have produced great success in both Computer Vision (CV)\nand Natural Language Processing (NLP). This progress leads to learning joint\nrepresentations of vision and language pretraining by feeding visual and\nlinguistic contents into a multi-layer transformer, Visual-Language Pretrained\nModels (VLPMs). In this paper, we present an overview of the major advances\nachieved in VLPMs for producing joint representations of vision and language.\nAs the preliminaries, we briefly describe the general task definition and\ngenetic architecture of VLPMs. We first discuss the language and vision data\nencoding methods and then present the mainstream VLPM structure as the core\ncontent. We further summarise several essential pretraining and fine-tuning\nstrategies. Finally, we highlight three future directions for both CV and NLP\nresearchers to provide insightful guidance.", "published": "2022-04-15 07:33:06", "link": "http://arxiv.org/abs/2204.07356v5", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Is Surprisal in Issue Trackers Actionable?", "abstract": "Background. From information theory, surprisal is a measurement of how\nunexpected an event is. Statistical language models provide a probabilistic\napproximation of natural languages, and because surprisal is constructed with\nthe probability of an event occuring, it is therefore possible to determine the\nsurprisal associated with English sentences. The issues and pull requests of\nsoftware repository issue trackers give insight into the development process\nand likely contain the surprising events of this process.\n  Objective. Prior works have identified that unusual events in software\nrepositories are of interest to developers, and use simple code metrics-based\nmethods for detecting them. In this study we will propose a new method for\nunusual event detection in software repositories using surprisal. With the\nability to find surprising issues and pull requests, we intend to further\nanalyse them to determine if they actually hold importance in a repository, or\nif they pose a significant challenge to address. If it is possible to find bad\nsurprises early, or before they cause additional troubles, it is plausible that\neffort, cost and time will be saved as a result.\n  Method. After extracting the issues and pull requests from 5000 of the most\npopular software repositories on GitHub, we will train a language model to\nrepresent these issues. We will measure their perceived importance in the\nrepository, measure their resolution difficulty using several analogues,\nmeasure the surprisal of each, and finally generate inferential statistics to\ndescribe any correlations.", "published": "2022-04-15 07:49:40", "link": "http://arxiv.org/abs/2204.07363v1", "categories": ["cs.CL", "cs.SE", "H.3.3; I.2.7"], "primary_category": "cs.CL"}
{"title": "A Personalized Dialogue Generator with Implicit User Persona Detection", "abstract": "Current works in the generation of personalized dialogue primarily contribute\nto the agent presenting a consistent personality and driving a more informative\nresponse. However, we found that the generated responses from most previous\nmodels tend to be self-centered, with little care for the user in the dialogue.\nMoreover, we consider that human-like conversation is essentially built based\non inferring information about the persona of the other party. Motivated by\nthis, we propose a novel personalized dialogue generator by detecting an\nimplicit user persona. Because it is hard to collect a large number of detailed\npersonas for each user, we attempted to model the user's potential persona and\nits representation from dialogue history, with no external knowledge. The\nperception and fader variables were conceived using conditional variational\ninference. The two latent variables simulate the process of people being aware\nof each other's persona and producing a corresponding expression in\nconversation. Finally, posterior-discriminated regularization was presented to\nenhance the training procedure. Empirical studies demonstrate that, compared to\nstate-of-the-art methods, our approach is more concerned with the user's\npersona and achieves a considerable boost across the evaluations.", "published": "2022-04-15 08:12:10", "link": "http://arxiv.org/abs/2204.07372v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ERGO: Event Relational Graph Transformer for Document-level Event\n  Causality Identification", "abstract": "Document-level Event Causality Identification (DECI) aims to identify causal\nrelations between event pairs in a document. It poses a great challenge of\nacross-sentence reasoning without clear causal indicators. In this paper, we\npropose a novel Event Relational Graph TransfOrmer (ERGO) framework for DECI,\nwhich improves existing state-of-the-art (SOTA) methods upon two aspects.\nFirst, we formulate DECI as a node classification problem by constructing an\nevent relational graph, without the needs of prior knowledge or tools. Second,\nERGO seamlessly integrates event-pair relation classification and global\ninference, which leverages a Relational Graph Transformer (RGT) to capture the\npotential causal chain. Besides, we introduce edge-building strategies and\nadaptive focal loss to deal with the massive false positives caused by common\nspurious correlation. Extensive experiments on two benchmark datasets show that\nERGO significantly outperforms previous SOTA methods (13.1% F1 gains on\naverage). We have conducted extensive quantitative analysis and case studies to\nprovide insights for future research directions (Section 4.8).", "published": "2022-04-15 12:12:16", "link": "http://arxiv.org/abs/2204.07434v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Political Communities on Twitter: Case Study of the 2022 French\n  Presidential Election", "abstract": "With the significant increase in users on social media platforms, a new means\nof political campaigning has appeared. Twitter and Facebook are now notable\ncampaigning tools during elections. Indeed, the candidates and their parties\nnow take to the internet to interact and spread their ideas. In this paper, we\naim to identify political communities formed on Twitter during the 2022 French\npresidential election and analyze each respective community. We create a\nlarge-scale Twitter dataset containing 1.2 million users and 62.6 million\ntweets that mention keywords relevant to the election. We perform community\ndetection on a retweet graph of users and propose an in-depth analysis of the\nstance of each community. Finally, we attempt to detect offensive tweets and\nautomatic bots, comparing across communities in order to gain insight into each\ncandidate's supporter demographics and online campaign strategy.", "published": "2022-04-15 12:18:16", "link": "http://arxiv.org/abs/2204.07436v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Stretching Sentence-pair NLI Models to Reason over Long Documents and\n  Clusters", "abstract": "Natural Language Inference (NLI) has been extensively studied by the NLP\ncommunity as a framework for estimating the semantic relation between sentence\npairs. While early work identified certain biases in NLI models, recent\nadvancements in modeling and datasets demonstrated promising performance. In\nthis work, we further explore the direct zero-shot applicability of NLI models\nto real applications, beyond the sentence-pair setting they were trained on.\nFirst, we analyze the robustness of these models to longer and out-of-domain\ninputs. Then, we develop new aggregation methods to allow operating over full\ndocuments, reaching state-of-the-art performance on the ContractNLI dataset.\nInterestingly, we find NLI scores to provide strong retrieval signals, leading\nto more relevant evidence extractions compared to common similarity-based\nmethods. Finally, we go further and investigate whole document clusters to\nidentify both discrepancies and consensus among sources. In a test case, we\nfind real inconsistencies between Wikipedia pages in different languages about\nthe same topic.", "published": "2022-04-15 12:56:39", "link": "http://arxiv.org/abs/2204.07447v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Pre-trained Language Models with Syntactic Dependency\n  Prediction Task for Chinese Semantic Error Recognition", "abstract": "Existing Chinese text error detection mainly focuses on spelling and simple\ngrammatical errors. These errors have been studied extensively and are\nrelatively simple for humans. On the contrary, Chinese semantic errors are\nunderstudied and more complex that humans cannot easily recognize. The task of\nthis paper is Chinese Semantic Error Recognition (CSER), a binary\nclassification task to determine whether a sentence contains semantic errors.\nThe current research has no effective method to solve this task. In this paper,\nwe inherit the model structure of BERT and design several syntax-related\npre-training tasks so that the model can learn syntactic knowledge. Our\npre-training tasks consider both the directionality of the dependency structure\nand the diversity of the dependency relationship. Due to the lack of a\npublished dataset for CSER, we build a high-quality dataset for CSER for the\nfirst time named Corpus of Chinese Linguistic Semantic Acceptability (CoCLSA).\nThe experimental results on the CoCLSA show that our methods outperform\nuniversal pre-trained models and syntax-infused models.", "published": "2022-04-15 13:55:32", "link": "http://arxiv.org/abs/2204.07464v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Polling Latent Opinions: A Method for Computational Sociolinguistics\n  Using Transformer Language Models", "abstract": "Text analysis of social media for sentiment, topic analysis, and other\nanalysis depends initially on the selection of keywords and phrases that will\nbe used to create the research corpora. However, keywords that researchers\nchoose may occur infrequently, leading to errors that arise from using small\nsamples. In this paper, we use the capacity for memorization, interpolation,\nand extrapolation of Transformer Language Models such as the GPT series to\nlearn the linguistic behaviors of a subgroup within larger corpora of Yelp\nreviews. We then use prompt-based queries to generate synthetic text that can\nbe analyzed to produce insights into specific opinions held by the populations\nthat the models were trained on. Once learned, more specific sentiment queries\ncan be made of the model with high levels of accuracy when compared to\ntraditional keyword searches. We show that even in cases where a specific\nkeyphrase is limited or not present at all in the training corpora, the GPT is\nable to accurately generate large volumes of text that have the correct\nsentiment.", "published": "2022-04-15 14:33:58", "link": "http://arxiv.org/abs/2204.07483v2", "categories": ["cs.CL", "cs.CY", "K.4.m"], "primary_category": "cs.CL"}
{"title": "Improving Passage Retrieval with Zero-Shot Question Generation", "abstract": "We propose a simple and effective re-ranking method for improving passage\nretrieval in open question answering. The re-ranker re-scores retrieved\npassages with a zero-shot question generation model, which uses a pre-trained\nlanguage model to compute the probability of the input question conditioned on\na retrieved passage. This approach can be applied on top of any retrieval\nmethod (e.g. neural or keyword-based), does not require any domain- or\ntask-specific training (and therefore is expected to generalize better to data\ndistribution shifts), and provides rich cross-attention between query and\npassage (i.e. it must explain every token in the question). When evaluated on a\nnumber of open-domain retrieval datasets, our re-ranker improves strong\nunsupervised retrieval models by 6%-18% absolute and strong supervised models\nby up to 12% in terms of top-20 passage retrieval accuracy. We also obtain new\nstate-of-the-art results on full open-domain question answering by simply\nadding the new re-ranker to existing models with no further changes.", "published": "2022-04-15 14:51:41", "link": "http://arxiv.org/abs/2204.07496v4", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Understanding Game-Playing Agents with Natural Language Annotations", "abstract": "We present a new dataset containing 10K human-annotated games of Go and show\nhow these natural language annotations can be used as a tool for model\ninterpretability. Given a board state and its associated comment, our approach\nuses linear probing to predict mentions of domain-specific terms (e.g., ko,\natari) from the intermediate state representations of game-playing agents like\nAlphaGo Zero. We find these game concepts are nontrivially encoded in two\ndistinct policy networks, one trained via imitation learning and another\ntrained via reinforcement learning. Furthermore, mentions of domain-specific\nterms are most easily predicted from the later layers of both models,\nsuggesting that these policy networks encode high-level abstractions similar to\nthose used in the natural language annotations.", "published": "2022-04-15 16:11:08", "link": "http://arxiv.org/abs/2204.07531v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Summarization with Graphical Elements", "abstract": "Automatic text summarization has experienced substantial progress in recent\nyears. With this progress, the question has arisen whether the types of\nsummaries that are typically generated by automatic summarization models align\nwith users' needs. Ter Hoeve et al (2020) answer this question negatively.\nAmongst others, they recommend focusing on generating summaries with more\ngraphical elements. This is in line with what we know from the\npsycholinguistics literature about how humans process text. Motivated from\nthese two angles, we propose a new task: summarization with graphical elements,\nand we verify that these summaries are helpful for a critical mass of people.\nWe collect a high quality human labeled dataset to support research into the\ntask. We present a number of baseline methods that show that the task is\ninteresting and challenging. Hence, with this work we hope to inspire a new\nline of research within the automatic summarization community.", "published": "2022-04-15 17:16:41", "link": "http://arxiv.org/abs/2204.07551v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluation Benchmarks for Spanish Sentence Representations", "abstract": "Due to the success of pre-trained language models, versions of languages\nother than English have been released in recent years. This fact implies the\nneed for resources to evaluate these models. In the case of Spanish, there are\nfew ways to systematically assess the models' quality. In this paper, we narrow\nthe gap by building two evaluation benchmarks. Inspired by previous work\n(Conneau and Kiela, 2018; Chen et al., 2019), we introduce Spanish SentEval and\nSpanish DiscoEval, aiming to assess the capabilities of stand-alone and\ndiscourse-aware sentence representations, respectively. Our benchmarks include\nconsiderable pre-existing and newly constructed datasets that address different\ntasks from various domains. In addition, we evaluate and analyze the most\nrecent pre-trained Spanish language models to exhibit their capabilities and\nlimitations. As an example, we discover that for the case of discourse\nevaluation tasks, mBERT, a language model trained on multiple languages,\nusually provides a richer latent representation than models trained only with\ndocuments in Spanish. We hope our contribution will motivate a fairer, more\ncomparable, and less cumbersome way to evaluate future Spanish language models.", "published": "2022-04-15 17:53:05", "link": "http://arxiv.org/abs/2204.07571v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "mGPT: Few-Shot Learners Go Multilingual", "abstract": "Recent studies report that autoregressive language models can successfully\nsolve many NLP tasks via zero- and few-shot learning paradigms, which opens up\nnew possibilities for using the pre-trained language models. This paper\nintroduces two autoregressive GPT-like models with 1.3 billion and 13 billion\nparameters trained on 60 languages from 25 language families using Wikipedia\nand Colossal Clean Crawled Corpus. We reproduce the GPT-3 architecture using\nGPT-2 sources and the sparse attention mechanism; Deepspeed and Megatron\nframeworks allow us to parallelize the training and inference steps\neffectively. The resulting models show performance on par with the recently\nreleased XGLM models by Facebook, covering more languages and enhancing NLP\npossibilities for low resource languages of CIS countries and Russian small\nnations. We detail the motivation for the choices of the architecture design,\nthoroughly describe the data preparation pipeline, and train five small\nversions of the model to choose the most optimal multilingual tokenization\nstrategy. We measure the model perplexity in all covered languages and evaluate\nit on the wide spectre of multilingual tasks, including classification,\ngenerative, sequence labeling and knowledge probing. The models were evaluated\nwith the zero-shot and few-shot methods. Furthermore, we compared the\nclassification tasks with the state-of-the-art multilingual model XGLM. source\ncode and the mGPT XL model are publicly released.", "published": "2022-04-15 13:02:33", "link": "http://arxiv.org/abs/2204.07580v2", "categories": ["cs.CL", "cs.AI", "68-06, 68-04, 68T50, 68T01", "I.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "Learning to Adapt Domain Shifts of Moral Values via Instance Weighting", "abstract": "Classifying moral values in user-generated text from social media is critical\nin understanding community cultures and interpreting user behaviors of social\nmovements. Moral values and language usage can change across the social\nmovements; however, text classifiers are usually trained in source domains of\nexisting social movements and tested in target domains of new social issues\nwithout considering the variations. In this study, we examine domain shifts of\nmoral values and language usage, quantify the effects of domain shifts on the\nmorality classification task, and propose a neural adaptation framework via\ninstance weighting to improve cross-domain classification tasks. The\nquantification analysis suggests a strong correlation between morality shifts,\nlanguage usage, and classification performance. We evaluate the neural\nadaptation framework on a public Twitter data across 7 social movements and\ngain classification improvements up to 12.1\\%. Finally, we release a new data\nof the COVID-19 vaccine labeled with moral values and evaluate our approach on\nthe new target domain. For the case study of the COVID-19 vaccine, our\nadaptation framework achieves up to 5.26\\% improvements over neural baselines.", "published": "2022-04-15 18:15:41", "link": "http://arxiv.org/abs/2204.07603v2", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Detection of sepsis during emergency department triage using machine\n  learning", "abstract": "Sepsis is a life-threatening condition with organ dysfunction and is a\nleading cause of death and critical illness worldwide. Even a few hours of\ndelay in the treatment of sepsis results in increased mortality. Early\ndetection of sepsis during emergency department triage would allow early\ninitiation of lab analysis, antibiotic administration, and other sepsis\ntreatment protocols. The purpose of this study was to compare sepsis detection\nperformance at ED triage (prior to the use of laboratory diagnostics) of the\nstandard sepsis screening algorithm (SIRS with source of infection) and a\nmachine learning algorithm trained on EHR triage data. A machine learning model\n(KATE Sepsis) was developed using patient encounters with triage data from\n16participating hospitals. KATE Sepsis and standard screening were\nretrospectively evaluated on the adult population of 512,949 medical records.\nKATE Sepsis demonstrates an AUC of 0.9423 (0.9401 - 0.9441) with sensitivity of\n71.09% (70.12% - 71.98%) and specificity of 94.81% (94.75% - 94.87%). Standard\nscreening demonstrates an AUC of 0.6826 (0.6774 - 0.6878) with sensitivity of\n40.8% (39.71% - 41.86%) and specificity of 95.72% (95.68% - 95.78%). The KATE\nSepsis model trained to detect sepsis demonstrates 77.67% (75.78% -79.42%)\nsensitivity in detecting severe sepsis and 86.95% (84.2% - 88.81%) sensitivity\nin detecting septic shock. The standard screening protocol demonstrates 43.06%\n(41% - 45.87%) sensitivity in detecting severe sepsis and40% (36.55% - 43.26%)\nsensitivity in detecting septic shock. Future research should focus on the\nprospective impact of KATE Sepsis on administration of antibiotics, readmission\nrate, morbidity and mortality.", "published": "2022-04-15 21:57:08", "link": "http://arxiv.org/abs/2204.07657v6", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Semantic Structure based Query Graph Prediction for Question Answering\n  over Knowledge Graph", "abstract": "Building query graphs from natural language questions is an important step in\ncomplex question answering over knowledge graph (Complex KGQA). In general, a\nquestion can be correctly answered if its query graph is built correctly and\nthe right answer is then retrieved by issuing the query graph against the KG.\nTherefore, this paper focuses on query graph generation from natural language\nquestions. Existing approaches for query graph generation ignore the semantic\nstructure of a question, resulting in a large number of noisy query graph\ncandidates that undermine prediction accuracies. In this paper, we define six\nsemantic structures from common questions in KGQA and develop a novel\nStructure-BERT to predict the semantic structure of a question. By doing so, we\ncan first filter out noisy candidate query graphs, and then rank the remaining\ncandidates with a BERT-based ranking model. Extensive experiments on two\npopular benchmarks MetaQA and WebQuestionsSP (WSP) demonstrate the\neffectiveness of our method as compared to state-of-the-arts.", "published": "2022-04-15 20:35:00", "link": "http://arxiv.org/abs/2204.10194v6", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Automated speech tools for helping communities process restricted-access\n  corpora for language revival efforts", "abstract": "Many archival recordings of speech from endangered languages remain\nunannotated and inaccessible to community members and language learning\nprograms. One bottleneck is the time-intensive nature of annotation. An even\nnarrower bottleneck occurs for recordings with access constraints, such as\nlanguage that must be vetted or filtered by authorised community members before\nannotation can begin. We propose a privacy-preserving workflow to widen both\nbottlenecks for recordings where speech in the endangered language is\nintermixed with a more widely-used language such as English for meta-linguistic\ncommentary and questions (e.g. What is the word for 'tree'?). We integrate\nvoice activity detection (VAD), spoken language identification (SLI), and\nautomatic speech recognition (ASR) to transcribe the metalinguistic content,\nwhich an authorised person can quickly scan to triage recordings that can be\nannotated by people with lower levels of access. We report work-in-progress\nprocessing 136 hours archival audio containing a mix of English and Muruwari.\nOur collaborative work with the Muruwari custodian of the archival materials\nshow that this workflow reduces metalanguage transcription time by 20% even\ngiven only minimal amounts of annotated training data: 10 utterances per\nlanguage for SLI and for ASR at most 39 minutes, and possibly as little as 39\nseconds.", "published": "2022-04-15 00:05:23", "link": "http://arxiv.org/abs/2204.07272v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Saga: A Platform for Continuous Construction and Serving of Knowledge At\n  Scale", "abstract": "We introduce Saga, a next-generation knowledge construction and serving\nplatform for powering knowledge-based applications at industrial scale. Saga\nfollows a hybrid batch-incremental design to continuously integrate billions of\nfacts about real-world entities and construct a central knowledge graph that\nsupports multiple production use cases with diverse requirements around data\nfreshness, accuracy, and availability. In this paper, we discuss the unique\nchallenges associated with knowledge graph construction at industrial scale,\nand review the main components of Saga and how they address these challenges.\nFinally, we share lessons-learned from a wide array of production use cases\npowered by Saga.", "published": "2022-04-15 03:20:30", "link": "http://arxiv.org/abs/2204.07309v1", "categories": ["cs.DB", "cs.AI", "cs.CL"], "primary_category": "cs.DB"}
{"title": "XDBERT: Distilling Visual Information to BERT from Cross-Modal Systems\n  to Improve Language Understanding", "abstract": "Transformer-based models are widely used in natural language understanding\n(NLU) tasks, and multimodal transformers have been effective in visual-language\ntasks. This study explores distilling visual information from pretrained\nmultimodal transformers to pretrained language encoders. Our framework is\ninspired by cross-modal encoders' success in visual-language tasks while we\nalter the learning objective to cater to the language-heavy characteristics of\nNLU. After training with a small number of extra adapting steps and finetuned,\nthe proposed XDBERT (cross-modal distilled BERT) outperforms pretrained-BERT in\ngeneral language understanding evaluation (GLUE), situations with adversarial\ngenerations (SWAG) benchmarks, and readability benchmarks. We analyze the\nperformance of XDBERT on GLUE to show that the improvement is likely visually\ngrounded.", "published": "2022-04-15 03:44:00", "link": "http://arxiv.org/abs/2204.07316v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Email Spam Detection Using Hierarchical Attention Hybrid Deep Learning\n  Method", "abstract": "Email is one of the most widely used ways to communicate, with millions of\npeople and businesses relying on it to communicate and share knowledge and\ninformation on a daily basis. Nevertheless, the rise in email users has\noccurred a dramatic increase in spam emails in recent years. Processing and\nmanaging emails properly for individuals and companies are getting increasingly\ndifficult. This article proposes a novel technique for email spam detection\nthat is based on a combination of convolutional neural networks, gated\nrecurrent units, and attention mechanisms. During system training, the network\nis selectively focused on necessary parts of the email text. The usage of\nconvolution layers to extract more meaningful, abstract, and generalizable\nfeatures by hierarchical representation is the major contribution of this\nstudy. Additionally, this contribution incorporates cross-dataset evaluation,\nwhich enables the generation of more independent performance results from the\nmodel's training dataset. According to cross-dataset evaluation results, the\nproposed technique advances the results of the present attention-based\ntechniques by utilizing temporal convolutions, which give us more flexible\nreceptive field sizes are utilized. The suggested technique's findings are\ncompared to those of state-of-the-art models and show that our approach\noutperforms them.", "published": "2022-04-15 09:02:36", "link": "http://arxiv.org/abs/2204.07390v2", "categories": ["cs.CL", "cs.LG", "cs.NE", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Towards Fine-grained Causal Reasoning and QA", "abstract": "Understanding causality is key to the success of NLP applications, especially\nin high-stakes domains. Causality comes in various perspectives such as enable\nand prevent that, despite their importance, have been largely ignored in the\nliterature. This paper introduces a novel fine-grained causal reasoning dataset\nand presents a series of novel predictive tasks in NLP, such as causality\ndetection, event causality extraction, and Causal QA. Our dataset contains\nhuman annotations of 25K cause-effect event pairs and 24K question-answering\npairs within multi-sentence samples, where each can have multiple causal\nrelationships. Through extensive experiments and analysis, we show that the\ncomplex relations in our dataset bring unique challenges to state-of-the-art\nmethods across all three tasks and highlight potential research opportunities,\nespecially in developing \"causal-thinking\" methods.", "published": "2022-04-15 10:12:46", "link": "http://arxiv.org/abs/2204.07408v1", "categories": ["cs.CL", "cs.AI", "cs.LO"], "primary_category": "cs.CL"}
{"title": "COTS: Collaborative Two-Stream Vision-Language Pre-Training Model for\n  Cross-Modal Retrieval", "abstract": "Large-scale single-stream pre-training has shown dramatic performance in\nimage-text retrieval. Regrettably, it faces low inference efficiency due to\nheavy attention layers. Recently, two-stream methods like CLIP and ALIGN with\nhigh inference efficiency have also shown promising performance, however, they\nonly consider instance-level alignment between the two streams (thus there is\nstill room for improvement). To overcome these limitations, we propose a novel\nCOllaborative Two-Stream vision-language pretraining model termed COTS for\nimage-text retrieval by enhancing cross-modal interaction. In addition to\ninstance level alignment via momentum contrastive learning, we leverage two\nextra levels of cross-modal interactions in our COTS: (1) Token-level\ninteraction - a masked visionlanguage modeling (MVLM) learning objective is\ndevised without using a cross-stream network module, where variational\nautoencoder is imposed on the visual encoder to generate visual tokens for each\nimage. (2) Task-level interaction - a KL-alignment learning objective is\ndevised between text-to-image and image-to-text retrieval tasks, where the\nprobability distribution per task is computed with the negative queues in\nmomentum contrastive learning. Under a fair comparison setting, our COTS\nachieves the highest performance among all two-stream methods and comparable\nperformance (but with 10,800X faster in inference) w.r.t. the latest\nsingle-stream methods. Importantly, our COTS is also applicable to\ntext-to-video retrieval, yielding new state-ofthe-art on the widely-used\nMSR-VTT dataset.", "published": "2022-04-15 12:34:47", "link": "http://arxiv.org/abs/2204.07441v2", "categories": ["cs.CV", "cs.CL", "cs.IR"], "primary_category": "cs.CV"}
{"title": "Improving Rare Word Recognition with LM-aware MWER Training", "abstract": "Language models (LMs) significantly improve the recognition accuracy of\nend-to-end (E2E) models on words rarely seen during training, when used in\neither the shallow fusion or the rescoring setups. In this work, we introduce\nLMs in the learning of hybrid autoregressive transducer (HAT) models in the\ndiscriminative training framework, to mitigate the training versus inference\ngap regarding the use of LMs. For the shallow fusion setup, we use LMs during\nboth hypotheses generation and loss computation, and the LM-aware MWER-trained\nmodel achieves 10\\% relative improvement over the model trained with standard\nMWER on voice search test sets containing rare words. For the rescoring setup,\nwe learn a small neural module to generate per-token fusion weights in a\ndata-dependent manner. This model achieves the same rescoring WER as regular\nMWER-trained model, but without the need for sweeping fusion weights.", "published": "2022-04-15 17:19:41", "link": "http://arxiv.org/abs/2204.07553v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Streaming Align-Refine for Non-autoregressive Deliberation", "abstract": "We propose a streaming non-autoregressive (non-AR) decoding algorithm to\ndeliberate the hypothesis alignment of a streaming RNN-T model. Our algorithm\nfacilitates a simple greedy decoding procedure, and at the same time is capable\nof producing the decoding result at each frame with limited right context, thus\nenjoying both high efficiency and low latency. These advantages are achieved by\nconverting the offline Align-Refine algorithm to be streaming-compatible, with\na novel transformer decoder architecture that performs local self-attentions\nfor both text and audio, and a time-aligned cross-attention at each layer.\nFurthermore, we perform discriminative training of our model with the minimum\nword error rate (MWER) criterion, which has not been done in the non-AR\ndecoding literature. Experiments on voice search datasets and Librispeech show\nthat with reasonable right context, our streaming model performs as well as the\noffline counterpart, and discriminative training leads to further WER gain when\nthe first-pass model has small capacity.", "published": "2022-04-15 17:24:39", "link": "http://arxiv.org/abs/2204.07556v1", "categories": ["cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "It is Okay to Not Be Okay: Overcoming Emotional Bias in Affective Image\n  Captioning by Contrastive Data Collection", "abstract": "Datasets that capture the connection between vision, language, and affection\nare limited, causing a lack of understanding of the emotional aspect of human\nintelligence. As a step in this direction, the ArtEmis dataset was recently\nintroduced as a large-scale dataset of emotional reactions to images along with\nlanguage explanations of these chosen emotions. We observed a significant\nemotional bias towards instance-rich emotions, making trained neural speakers\nless accurate in describing under-represented emotions. We show that collecting\nnew data, in the same way, is not effective in mitigating this emotional bias.\nTo remedy this problem, we propose a contrastive data collection approach to\nbalance ArtEmis with a new complementary dataset such that a pair of similar\nimages have contrasting emotions (one positive and one negative). We collected\n260,533 instances using the proposed method, we combine them with ArtEmis,\ncreating a second iteration of the dataset. The new combined dataset, dubbed\nArtEmis v2.0, has a balanced distribution of emotions with explanations\nrevealing more fine details in the associated painting. Our experiments show\nthat neural speakers trained on the new dataset improve CIDEr and METEOR\nevaluation metrics by 20% and 7%, respectively, compared to the biased dataset.\nFinally, we also show that the performance per emotion of neural speakers is\nimproved across all the emotion categories, significantly on under-represented\nemotions. The collected dataset and code are available at\nhttps://artemisdataset-v2.org.", "published": "2022-04-15 22:08:45", "link": "http://arxiv.org/abs/2204.07660v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Just Fine-tune Twice: Selective Differential Privacy for Large Language\n  Models", "abstract": "Protecting large language models from privacy leakage is becoming\nincreasingly crucial with their wide adoption in real-world products. Yet\napplying differential privacy (DP), a canonical notion with provable privacy\nguarantees for machine learning models, to those models remains challenging due\nto the trade-off between model utility and privacy loss. Utilizing the fact\nthat sensitive information in language data tends to be sparse, Shi et al.\n(2021) formalized a DP notion extension called Selective Differential Privacy\n(SDP) to protect only the sensitive tokens defined by a policy function.\nHowever, their algorithm only works for RNN-based models. In this paper, we\ndevelop a novel framework, Just Fine-tune Twice (JFT), that achieves SDP for\nstate-of-the-art large transformer-based models. Our method is easy to\nimplement: it first fine-tunes the model with redacted in-domain data, and then\nfine-tunes it again with the original in-domain data using a private training\nmechanism. Furthermore, we study the scenario of imperfect implementation of\npolicy functions that misses sensitive tokens and develop systematic methods to\nhandle it. Experiments show that our method achieves strong utility compared to\nprevious baselines. We also analyze the SDP privacy guarantee empirically with\nthe canary insertion attack.", "published": "2022-04-15 22:36:55", "link": "http://arxiv.org/abs/2204.07667v3", "categories": ["cs.CL", "cs.AI", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Speaker-Aware Mixture of Mixtures Training for Weakly Supervised Speaker\n  Extraction", "abstract": "Dominant researches adopt supervised training for speaker extraction, while\nthe scarcity of ideally clean corpus and channel mismatch problem are rarely\nconsidered. To this end, we propose speaker-aware mixture of mixtures training\n(SAMoM), utilizing the consistency of speaker identity among target source,\nenrollment utterance and target estimate to weakly supervise the training of a\ndeep speaker extractor. In SAMoM, the input is constructed by mixing up\ndifferent speaker-aware mixtures (SAMs), each contains multiple speakers with\ntheir identities known and enrollment utterances available. Informed by\nenrollment utterances, target speech is extracted from the input one by one,\nsuch that the estimated targets can approximate the original SAMs after a remix\nin accordance with the identity consistency. Moreover, using SAMoM in a\nsemi-supervised setting with a certain amount of clean sources enables\napplication in noisy scenarios. Extensive experiments on Libri2Mix show that\nthe proposed method achieves promising results without access to any clean\nsources (11.06dB SI-SDRi). With a domain adaptation, our approach even\noutperformed supervised framework in a cross-domain evaluation on AISHELL-1.", "published": "2022-04-15 08:22:35", "link": "http://arxiv.org/abs/2204.07375v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "BYOL for Audio: Exploring Pre-trained General-purpose Audio\n  Representations", "abstract": "Pre-trained models are essential as feature extractors in modern machine\nlearning systems in various domains. In this study, we hypothesize that\nrepresentations effective for general audio tasks should provide multiple\naspects of robust features of the input sound. For recognizing sounds\nregardless of perturbations such as varying pitch or timbre, features should be\nrobust to these perturbations. For serving the diverse needs of tasks such as\nrecognition of emotions or music genres, representations should provide\nmultiple aspects of information, such as local and global features. To\nimplement our principle, we propose a self-supervised learning method:\nBootstrap Your Own Latent (BYOL) for Audio (BYOL-A, pronounced \"viola\"). BYOL-A\npre-trains representations of the input sound invariant to audio data\naugmentations, which makes the learned representations robust to the\nperturbations of sounds. Whereas the BYOL-A encoder combines local and global\nfeatures and calculates their statistics to make the representation provide\nmulti-aspect information. As a result, the learned representations should\nprovide robust and multi-aspect information to serve various needs of diverse\ntasks. We evaluated the general audio task performance of BYOL-A compared to\nprevious state-of-the-art methods, and BYOL-A demonstrated generalizability\nwith the best average result of 72.4% and the best VoxCeleb1 result of 57.6%.\nExtensive ablation experiments revealed that the BYOL-A encoder architecture\ncontributes to most performance, and the final critical portion resorts to the\nBYOL framework and BYOL-A augmentations. Our code is available online at\nhttps://github.com/nttcslab/byol-a for future studies.", "published": "2022-04-15 09:44:10", "link": "http://arxiv.org/abs/2204.07402v2", "categories": ["eess.AS", "cs.SD", "68T07"], "primary_category": "eess.AS"}
{"title": "Improving Frame-Online Neural Speech Enhancement with Overlapped-Frame\n  Prediction", "abstract": "Frame-online speech enhancement systems in the short-time Fourier transform\n(STFT) domain usually have an algorithmic latency equal to the window size due\nto the use of overlap-add in the inverse STFT (iSTFT). This algorithmic latency\nallows the enhancement models to leverage future contextual information up to a\nlength equal to the window size. However, this information is only partially\nleveraged by current frame-online systems. To fully exploit it, we propose an\noverlapped-frame prediction technique for deep learning based frame-online\nspeech enhancement, where at each frame our deep neural network (DNN) predicts\nthe current and several past frames that are necessary for overlap-add, instead\nof only predicting the current frame. In addition, we propose a loss function\nto account for the scale difference between predicted and oracle target\nsignals. Experiments on a noisy-reverberant speech enhancement task show the\neffectiveness of the proposed algorithms.", "published": "2022-04-15 17:40:32", "link": "http://arxiv.org/abs/2204.07566v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Anomalous Sound Detection Based on Machine Activity Detection", "abstract": "We have developed an unsupervised anomalous sound detection method for\nmachine condition monitoring that utilizes an auxiliary task -- detecting when\nthe target machine is active. First, we train a model that detects machine\nactivity by using normal data with machine activity labels and then use the\nactivity-detection error as the anomaly score for a given sound clip if we have\naccess to the ground-truth activity labels in the inference phase. If these\nlabels are not available, the anomaly score is calculated through outlier\ndetection on the embedding vectors obtained by the activity-detection model.\nSolving this auxiliary task enables the model to learn the difference between\nthe target machine sounds and similar background noise, which makes it possible\nto identify small deviations in the target sounds. Experimental results showed\nthat the proposed method improves the anomaly-detection performance of the\nconventional method complementarily by means of an ensemble.", "published": "2022-04-15 07:23:32", "link": "http://arxiv.org/abs/2204.07353v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Deep CardioSound-An Ensembled Deep Learning Model for Heart Sound\n  MultiLabelling", "abstract": "Heart sound diagnosis and classification play an essential role in detecting\ncardiovascular disorders, especially when the remote diagnosis becomes standard\nclinical practice. Most of the current work is designed for single category\nbased heard sound classification tasks. To further extend the landscape of the\nautomatic heart sound diagnosis landscape, this work proposes a deep multilabel\nlearning model that can automatically annotate heart sound recordings with\nlabels from different label groups, including murmur's timing, pitch, grading,\nquality, and shape. Our experiment results show that the proposed method has\nachieved outstanding performance on the holdout data for the multi-labelling\ntask with sensitivity=0.990, specificity=0.999, F1=0.990 at the segments level,\nand an overall accuracy=0.969 at the patient's recording level.", "published": "2022-04-15 11:13:11", "link": "http://arxiv.org/abs/2204.07420v2", "categories": ["cs.SD", "cs.CV", "eess.AS", "eess.IV"], "primary_category": "cs.SD"}
