{"title": "Leveraging Pretrained Word Embeddings for Part-of-Speech Tagging of Code\n  Switching Data", "abstract": "Linguistic Code Switching (CS) is a phenomenon that occurs when multilingual\nspeakers alternate between two or more languages/dialects within a single\nconversation. Processing CS data is especially challenging in intra-sentential\ndata given state-of-the-art monolingual NLP technologies since such\ntechnologies are geared toward the processing of one language at a time. In\nthis paper, we address the problem of Part-of-Speech tagging (POS) in the\ncontext of linguistic code switching (CS). We explore leveraging multiple\nneural network architectures to measure the impact of different pre-trained\nembeddings methods on POS tagging CS data. We investigate the landscape in four\nCS language pairs, Spanish-English, Hindi-English, Modern Standard Arabic-\nEgyptian Arabic dialect (MSA-EGY), and Modern Standard Arabic- Levantine Arabic\ndialect (MSA-LEV). Our results show that multilingual embedding (e.g., MSA-EGY\nand MSA-LEV) helps closely related languages (EGY/LEV) but adds noise to the\nlanguages that are distant (SPA/HIN). Finally, we show that our proposed models\noutperform state-of-the-art CS taggers for MSA-EGY language pair.", "published": "2019-05-31 00:08:27", "link": "http://arxiv.org/abs/1905.13359v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rewarding Smatch: Transition-Based AMR Parsing with Reinforcement\n  Learning", "abstract": "Our work involves enriching the Stack-LSTM transition-based AMR parser\n(Ballesteros and Al-Onaizan, 2017) by augmenting training with Policy Learning\nand rewarding the Smatch score of sampled graphs. In addition, we also combined\nseveral AMR-to-text alignments with an attention mechanism and we supplemented\nthe parser with pre-processed concept identification, named entities and\ncontextualized embeddings. We achieve a highly competitive performance that is\ncomparable to the best published results. We show an in-depth study ablating\neach of the new components of the parser", "published": "2019-05-31 01:15:36", "link": "http://arxiv.org/abs/1905.13370v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Open Information Extraction via Iterative Rank-Aware Learning", "abstract": "Open information extraction (IE) is the task of extracting open-domain\nassertions from natural language sentences. A key step in open IE is confidence\nmodeling, ranking the extractions based on their estimated quality to adjust\nprecision and recall of extracted assertions. We found that the extraction\nlikelihood, a confidence measure used by current supervised open IE systems, is\nnot well calibrated when comparing the quality of assertions extracted from\ndifferent sentences. We propose an additional binary classification loss to\ncalibrate the likelihood to make it more globally comparable, and an iterative\nlearning process, where extractions generated by the open IE model are\nincrementally included as training samples to help the model learn from trial\nand error. Experiments on OIE2016 demonstrate the effectiveness of our method.\nCode and data are available at https://github.com/jzbjyb/oie_rank.", "published": "2019-05-31 05:02:50", "link": "http://arxiv.org/abs/1905.13413v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-Grained Spoiler Detection from Large-Scale Review Corpora", "abstract": "This paper presents computational approaches for automatically detecting\ncritical plot twists in reviews of media products. First, we created a\nlarge-scale book review dataset that includes fine-grained spoiler annotations\nat the sentence-level, as well as book and (anonymized) user information.\nSecond, we carefully analyzed this dataset, and found that: spoiler language\ntends to be book-specific; spoiler distributions vary greatly across books and\nreview authors; and spoiler sentences tend to jointly appear in the latter part\nof reviews. Third, inspired by these findings, we developed an end-to-end\nneural network architecture to detect spoiler sentences in review corpora.\nQuantitative and qualitative results demonstrate that the proposed method\nsubstantially outperforms existing baselines.", "published": "2019-05-31 05:06:00", "link": "http://arxiv.org/abs/1905.13416v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Symbol Emergence as an Interpersonal Multimodal Categorization", "abstract": "This study focuses on category formation for individual agents and the\ndynamics of symbol emergence in a multi-agent system through semiotic\ncommunication. Semiotic communication is defined, in this study, as the\ngeneration and interpretation of signs associated with the categories formed\nthrough the agent's own sensory experience or by exchange of signs with other\nagents. From the viewpoint of language evolution and symbol emergence,\norganization of a symbol system in a multi-agent system is considered as a\nbottom-up and dynamic process, where individual agents share the meaning of\nsigns and categorize sensory experience. A constructive computational model can\nexplain the mutual dependency of the two processes and has mathematical support\nthat guarantees a symbol system's emergence and sharing within the multi-agent\nsystem. In this paper, we describe a new computational model that represents\nsymbol emergence in a two-agent system based on a probabilistic generative\nmodel for multimodal categorization. It models semiotic communication via a\nprobabilistic rejection based on the receiver's own belief. We have found that\nthe dynamics by which cognitively independent agents create a symbol system\nthrough their semiotic communication can be regarded as the inference process\nof a hidden variable in an interpersonal multimodal categorizer, if we define\nthe rejection probability based on the Metropolis-Hastings algorithm. The\nvalidity of the proposed model and algorithm for symbol emergence is also\nverified in an experiment with two agents observing daily objects in the\nreal-world environment. The experimental results demonstrate that our model\nreproduces the phenomena of symbol emergence, which does not require a teacher\nwho would know a pre-existing symbol system. Instead, the multi-agent system\ncan form and use a symbol system without having pre-existing categories.", "published": "2019-05-31 07:03:03", "link": "http://arxiv.org/abs/1905.13443v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Effective writing style imitation via combinatorial paraphrasing", "abstract": "Stylometry can be used to profile or deanonymize authors against their will\nbased on writing style. Style transfer provides a defence. Current techniques\ntypically use either encoder-decoder architectures or rule-based algorithms.\nCrucially, style transfer must reliably retain original semantic content to be\nactually deployable. We conduct a multifaceted evaluation of three\nstate-of-the-art encoder-decoder style transfer techniques, and show that all\nfail at semantic retainment. In particular, they do not produce appropriate\nparaphrases, but only retain original content in the trivial case of exactly\nreproducing the text. To mitigate this problem we propose ParChoice: a\ntechnique based on the combinatorial application of multiple paraphrasing\nalgorithms. ParChoice strongly outperforms the encoder-decoder baselines in\nsemantic retainment. Additionally, compared to baselines that achieve\nnon-negligible semantic retainment, ParChoice has superior style transfer\nperformance. We also apply ParChoice to multi-author style imitation (not\nconsidered by prior work), where we achieve up to 75% imitation success among\nfive authors. Furthermore, when compared to two state-of-the-art rule-based\nstyle transfer techniques, ParChoice has markedly better semantic retainment.\nCombining ParChoice with the best performing rule-based baseline (Mutant-X)\nalso reaches the highest style transfer success on the Brennan-Greenstadt and\nExtended-Brennan-Greenstadt corpora, with much less impact on original meaning\nthan when using the rule-based baseline techniques alone. Finally, we highlight\na critical problem that afflicts all current style transfer techniques: the\nadversary can use the same technique for thwarting style transfer via\nadversarial training. We show that adding randomness to style transfer helps to\nmitigate the effectiveness of adversarial training.", "published": "2019-05-31 08:42:27", "link": "http://arxiv.org/abs/1905.13464v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating an Effective Character-level Embedding in Korean Sentence\n  Classification", "abstract": "Different from the writing systems of many Romance and Germanic languages,\nsome languages or language families show complex conjunct forms in character\ncomposition. For such cases where the conjuncts consist of the components\nrepresenting consonant(s) and vowel, various character encoding schemes can be\nadopted beyond merely making up a one-hot vector. However, there has been\nlittle work done on intra-language comparison regarding performances using each\nrepresentation. In this study, utilizing the Korean language which is\ncharacter-rich and agglutinative, we investigate an encoding scheme that is the\nmost effective among Jamo-level one-hot, character-level one-hot,\ncharacter-level dense, and character-level multi-hot. Classification\nperformance with each scheme is evaluated on two corpora: one on binary\nsentiment analysis of movie reviews, and the other on multi-class\nidentification of intention types. The result displays that the character-level\nfeatures show higher performance in general, although the Jamo-level features\nmay show compatibility with the attention-based models if guaranteed adequate\nparameter set size.", "published": "2019-05-31 14:54:46", "link": "http://arxiv.org/abs/1905.13656v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do Human Rationales Improve Machine Explanations?", "abstract": "Work on \"learning with rationales\" shows that humans providing explanations\nto a machine learning system can improve the system's predictive accuracy.\nHowever, this work has not been connected to work in \"explainable AI\" which\nconcerns machines explaining their reasoning to humans. In this work, we show\nthat learning with rationales can also improve the quality of the machine's\nexplanations as evaluated by human judges. Specifically, we present experiments\nshowing that, for CNN- based text classification, explanations generated using\n\"supervised attention\" are judged superior to explanations generated using\nnormal unsupervised attention.", "published": "2019-05-31 16:49:57", "link": "http://arxiv.org/abs/1905.13714v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Thinking Slow about Latency Evaluation for Simultaneous Machine\n  Translation", "abstract": "Simultaneous machine translation attempts to translate a source sentence\nbefore it is finished being spoken, with applications to translation of spoken\nlanguage for live streaming and conversation. Since simultaneous systems trade\nquality to reduce latency, having an effective and interpretable latency metric\nis crucial. We introduce a variant of the recently proposed Average Lagging\n(AL) metric, which we call Differentiable Average Lagging (DAL). It\ndistinguishes itself by being differentiable and internally consistent to its\nunderlying mathematical model.", "published": "2019-05-31 19:57:49", "link": "http://arxiv.org/abs/1906.00048v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving the Similarity Measure of Determinantal Point Processes for\n  Extractive Multi-Document Summarization", "abstract": "The most important obstacles facing multi-document summarization include\nexcessive redundancy in source descriptions and the looming shortage of\ntraining data. These obstacles prevent encoder-decoder models from being used\ndirectly, but optimization-based methods such as determinantal point processes\n(DPPs) are known to handle them well. In this paper we seek to strengthen a\nDPP-based method for extractive multi-document summarization by presenting a\nnovel similarity measure inspired by capsule networks. The approach measures\nredundancy between a pair of sentences based on surface form and semantic\ninformation. We show that our DPP system with improved similarity measure\nperforms competitively, outperforming strong summarization baselines on\nbenchmark datasets. Our findings are particularly meaningful for summarizing\ndocuments created by multiple authors containing redundant yet lexically\ndiverse expressions.", "published": "2019-05-31 20:45:15", "link": "http://arxiv.org/abs/1906.00072v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Scoring Sentence Singletons and Pairs for Abstractive Summarization", "abstract": "When writing a summary, humans tend to choose content from one or two\nsentences and merge them into a single summary sentence. However, the\nmechanisms behind the selection of one or multiple source sentences remain\npoorly understood. Sentence fusion assumes multi-sentence input; yet sentence\nselection methods only work with single sentences and not combinations of them.\nThere is thus a crucial gap between sentence selection and fusion to support\nsummarizing by both compressing single sentences and fusing pairs. This paper\nattempts to bridge the gap by ranking sentence singletons and pairs together in\na unified space. Our proposed framework attempts to model human methodology by\nselecting either a single sentence or a pair of sentences, then compressing or\nfusing the sentence(s) to produce a summary sentence. We conduct extensive\nexperiments on both single- and multi-document summarization datasets and\nreport findings on sentence selection and abstraction.", "published": "2019-05-31 20:58:08", "link": "http://arxiv.org/abs/1906.00077v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Examining Structure of Word Embeddings with PCA", "abstract": "In this paper we compare structure of Czech word embeddings for English-Czech\nneural machine translation (NMT), word2vec and sentiment analysis. We show that\nalthough it is possible to successfully predict part of speech (POS) tags from\nword embeddings of word2vec and various translation models, not all of the\nembedding spaces show the same structure. The information about POS is present\nin word2vec embeddings, but the high degree of organization by POS in the NMT\ndecoder suggests that this information is more important for machine\ntranslation and therefore the NMT model represents it in more direct way. Our\nmethod is based on correlation of principal component analysis (PCA) dimensions\nwith categorical linguistic data. We also show that further examining\nhistograms of classes along the principal component is important to understand\nthe structure of representation of information in embeddings.", "published": "2019-05-31 22:47:56", "link": "http://arxiv.org/abs/1906.00114v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-modal Discriminative Model for Vision-and-Language Navigation", "abstract": "Vision-and-Language Navigation (VLN) is a natural language grounding task\nwhere agents have to interpret natural language instructions in the context of\nvisual scenes in a dynamic environment to achieve prescribed navigation goals.\nSuccessful agents must have the ability to parse natural language of varying\nlinguistic styles, ground them in potentially unfamiliar scenes, plan and react\nwith ambiguous environmental feedback. Generalization ability is limited by the\namount of human annotated data. In particular, \\emph{paired} vision-language\nsequence data is expensive to collect. We develop a discriminator that\nevaluates how well an instruction explains a given path in VLN task using\nmulti-modal alignment. Our study reveals that only a small fraction of the\nhigh-quality augmented data from \\citet{Fried:2018:Speaker}, as scored by our\ndiscriminator, is useful for training VLN agents with similar performance on\npreviously unseen environments. We also show that a VLN agent warm-started with\npre-trained components from the discriminator outperforms the benchmark success\nrates of 35.5 by 10\\% relative measure on previously unseen environments.", "published": "2019-05-31 00:07:24", "link": "http://arxiv.org/abs/1905.13358v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Constructive Type-Logical Supertagging with Self-Attention Networks", "abstract": "We propose a novel application of self-attention networks towards grammar\ninduction. We present an attention-based supertagger for a refined type-logical\ngrammar, trained on constructing types inductively. In addition to achieving a\nhigh overall type accuracy, our model is able to learn the syntax of the\ngrammar's type system along with its denotational semantics. This lifts the\nclosed world assumption commonly made by lexicalized grammar supertaggers,\ngreatly enhancing its generalization potential. This is evidenced both by its\nadequate accuracy over sparse word types and its ability to correctly construct\ncomplex types never seen during training, which, to the best of our knowledge,\nwas as of yet unaccomplished.", "published": "2019-05-31 05:16:15", "link": "http://arxiv.org/abs/1905.13418v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Content Word-based Sentence Decoding and Evaluating for Open-domain\n  Neural Response Generation", "abstract": "Various encoder-decoder models have been applied to response generation in\nopen-domain dialogs, but a majority of conventional models directly learn a\nmapping from lexical input to lexical output without explicitly modeling\nintermediate representations. Utilizing language hierarchy and modeling\nintermediate information have been shown to benefit many language understanding\nand generation tasks. Motivated by Broca's aphasia, we propose to use a content\nword sequence as an intermediate representation for open-domain response\ngeneration. Experimental results show that the proposed method improves content\nrelatedness of produced responses, and our models can often choose correct\ngrammar for generated content words. Meanwhile, instead of evaluating complete\nsentences, we propose to compute conventional metrics on content word\nsequences, which is a better indicator of content relevance.", "published": "2019-05-31 06:36:19", "link": "http://arxiv.org/abs/1905.13438v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Attention Is (not) All You Need for Commonsense Reasoning", "abstract": "The recently introduced BERT model exhibits strong performance on several\nlanguage understanding benchmarks. In this paper, we describe a simple\nre-implementation of BERT for commonsense reasoning. We show that the\nattentions produced by BERT can be directly utilized for tasks such as the\nPronoun Disambiguation Problem and Winograd Schema Challenge. Our proposed\nattention-guided commonsense reasoning method is conceptually simple yet\nempirically powerful. Experimental analysis on multiple datasets demonstrates\nthat our proposed system performs remarkably well on all cases while\noutperforming the previously reported state of the art by a margin. While\nresults suggest that BERT seems to implicitly learn to establish complex\nrelationships between entities, solving commonsense reasoning tasks might\nrequire more than unsupervised models learned from huge text corpora.", "published": "2019-05-31 10:27:58", "link": "http://arxiv.org/abs/1905.13497v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Using Natural Language Processing to Develop an Automated Orthodontic\n  Diagnostic System", "abstract": "We work on the task of automatically designing a treatment plan from the\nfindings included in the medical certificate written by the dentist. To develop\nan artificial intelligence system that deals with free-form certificates\nwritten by dentists, we annotate the findings and utilized the natural language\nprocessing approach. As a result of the experiment using 990 certificates,\n0.585 F1-score was achieved for the task of extracting orthodontic problems\nfrom findings, and 0.584 correlation coefficient with the human ranking was\nachieved for the treatment prioritization task.", "published": "2019-05-31 13:13:51", "link": "http://arxiv.org/abs/1905.13601v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GSN: A Graph-Structured Network for Multi-Party Dialogues", "abstract": "Existing neural models for dialogue response generation assume that\nutterances are sequentially organized. However, many real-world dialogues\ninvolve multiple interlocutors (i.e., multi-party dialogues), where the\nassumption does not hold as utterances from different interlocutors can occur\n\"in parallel.\" This paper generalizes existing sequence-based models to a\nGraph-Structured neural Network (GSN) for dialogue modeling. The core of GSN is\na graph-based encoder that can model the information flow along the\ngraph-structured dialogues (two-party sequential dialogues are a special case).\nExperimental results show that GSN significantly outperforms existing\nsequence-based models.", "published": "2019-05-31 14:30:49", "link": "http://arxiv.org/abs/1905.13637v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Spotting Collective Behaviour of Online Frauds in Customer Reviews", "abstract": "Online reviews play a crucial role in deciding the quality before purchasing\nany product. Unfortunately, spammers often take advantage of online review\nforums by writing fraud reviews to promote/demote certain products. It may turn\nout to be more detrimental when such spammers collude and collectively inject\nspam reviews as they can take complete control of users' sentiment due to the\nvolume of fraud reviews they inject. Group spam detection is thus more\nchallenging than individual-level fraud detection due to unclear definition of\na group, variation of inter-group dynamics, scarcity of labeled group-level\nspam data, etc. Here, we propose DeFrauder, an unsupervised method to detect\nonline fraud reviewer groups. It first detects candidate fraud groups by\nleveraging the underlying product review graph and incorporating several\nbehavioral signals which model multi-faceted collaboration among reviewers. It\nthen maps reviewers into an embedding space and assigns a spam score to each\ngroup such that groups comprising spammers with highly similar behavioral\ntraits achieve high spam score. While comparing with five baselines on four\nreal-world datasets (two of them were curated by us), DeFrauder shows superior\nperformance by outperforming the best baseline with 17.11% higher NDCG@50 (on\naverage) across datasets.", "published": "2019-05-31 14:49:30", "link": "http://arxiv.org/abs/1905.13649v6", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Entropy Minimization In Emergent Languages", "abstract": "There is growing interest in studying the languages that emerge when neural\nagents are jointly trained to solve tasks requiring communication through a\ndiscrete channel. We investigate here the information-theoretic complexity of\nsuch languages, focusing on the basic two-agent, one-exchange setup. We find\nthat, under common training procedures, the emergent languages are subject to\nan entropy minimization pressure that has also been detected in human language,\nwhereby the mutual information between the communicating agent's inputs and the\nmessages is minimized, within the range afforded by the need for successful\ncommunication. That is, emergent languages are (nearly) as simple as the task\nthey are developed for allow them to be. This pressure is amplified as we\nincrease communication channel discreteness. Further, we observe that stronger\ndiscrete-channel-driven entropy minimization leads to representations with\nincreased robustness to overfitting and adversarial attacks. We conclude by\ndiscussing the implications of our findings for the study of natural and\nartificial communication systems.", "published": "2019-05-31 15:54:41", "link": "http://arxiv.org/abs/1905.13687v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Visual Understanding and Narration: A Deeper Understanding and\n  Explanation of Visual Scenes", "abstract": "We describe the task of Visual Understanding and Narration, in which a robot\n(or agent) generates text for the images that it collects when navigating its\nenvironment, by answering open-ended questions, such as 'what happens, or might\nhave happened, here?'", "published": "2019-05-31 19:12:55", "link": "http://arxiv.org/abs/1906.00038v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "OK-VQA: A Visual Question Answering Benchmark Requiring External\n  Knowledge", "abstract": "Visual Question Answering (VQA) in its ideal form lets us study reasoning in\nthe joint space of vision and language and serves as a proxy for the AI task of\nscene understanding. However, most VQA benchmarks to date are focused on\nquestions such as simple counting, visual attributes, and object detection that\ndo not require reasoning or knowledge beyond what is in the image. In this\npaper, we address the task of knowledge-based visual question answering and\nprovide a benchmark, called OK-VQA, where the image content is not sufficient\nto answer the questions, encouraging methods that rely on external knowledge\nresources. Our new dataset includes more than 14,000 questions that require\nexternal knowledge to answer. We show that the performance of the\nstate-of-the-art VQA models degrades drastically in this new setting. Our\nanalysis shows that our knowledge-based VQA task is diverse, difficult, and\nlarge compared to previous knowledge-based VQA datasets. We hope that this\ndataset enables researchers to open up new avenues for research in this domain.\nSee http://okvqa.allenai.org to download and browse the dataset.", "published": "2019-05-31 20:29:01", "link": "http://arxiv.org/abs/1906.00067v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Emotional Embeddings: Refining Word Embeddings to Capture Emotional\n  Content of Words", "abstract": "Word embeddings are one of the most useful tools in any modern natural\nlanguage processing expert's toolkit. They contain various types of information\nabout each word which makes them the best way to represent the terms in any NLP\ntask. But there are some types of information that cannot be learned by these\nmodels. Emotional information of words are one of those. In this paper, we\npresent an approach to incorporate emotional information of words into these\nmodels. We accomplish this by adding a secondary training stage which uses an\nemotional lexicon and a psychological model of basic emotions. We show that\nfitting an emotional model into pre-trained word vectors can increase the\nperformance of these models in emotional similarity metrics. Retrained models\nperform better than their original counterparts from 13% improvement for\nWord2Vec model, to 29% for GloVe vectors. This is the first such model\npresented in the literature, and although preliminary, these emotion sensitive\nmodels can open the way to increase performance in variety of emotion detection\ntechniques.", "published": "2019-05-31 22:46:03", "link": "http://arxiv.org/abs/1906.00112v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Can We Derive Explicit and Implicit Bias from Corpus?", "abstract": "Language is a popular resource to mine speakers' attitude bias, supposing\nthat speakers' statements represent their bias on concepts. However, psychology\nstudies show that people's explicit bias in statements can be different from\ntheir implicit bias in mind. Although both explicit and implicit bias are\nuseful for different applications, current automatic techniques do not\ndistinguish them. Inspired by psychological measurements of explicit and\nimplicit bias, we develop an automatic language-based technique to reproduce\npsychological measurements on large population. By connecting each\npsychological measurement with the statements containing the certain\ncombination of special words, we derive explicit and implicit bias by\nunderstanding the sentiment of corresponding category of statements. Extensive\nexperiments on English and Chinese serious media (Wikipedia) and non-serious\nmedia (social media) show that our method successfully reproduce the\nsmall-scale psychological observations on large population and achieve new\nfindings.", "published": "2019-05-31 00:36:21", "link": "http://arxiv.org/abs/1905.13364v1", "categories": ["cs.SI", "cs.CL", "cs.CY"], "primary_category": "cs.SI"}
{"title": "Audio Caption in a Car Setting with a Sentence-Level Loss", "abstract": "Captioning has attracted much attention in image and video understanding\nwhile a small amount of work examines audio captioning. This paper contributes\na Mandarin-annotated dataset for audio captioning within a car scene. A\nsentence-level loss is proposed to be used in tandem with a GRU encoder-decoder\nmodel to generate captions with higher semantic similarity to human\nannotations. We evaluate the model on the newly-proposed Car dataset, a\npreviously published Mandarin Hospital dataset and the Joint dataset,\nindicating its generalization capability across different scenes. An\nimprovement in all metrics can be observed, including classical natural\nlanguage generation (NLG) metrics, sentence richness and human evaluation\nratings. However, though detailed audio captions can now be automatically\ngenerated, human annotations still outperform model captions on many aspects.", "published": "2019-05-31 07:30:15", "link": "http://arxiv.org/abs/1905.13448v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MultiQA: An Empirical Investigation of Generalization and Transfer in\n  Reading Comprehension", "abstract": "A large number of reading comprehension (RC) datasets has been created\nrecently, but little analysis has been done on whether they generalize to one\nanother, and the extent to which existing datasets can be leveraged for\nimproving performance on new ones. In this paper, we conduct such an\ninvestigation over ten RC datasets, training on one or more source RC datasets,\nand evaluating generalization, as well as transfer to a target RC dataset. We\nanalyze the factors that contribute to generalization, and show that training\non a source RC dataset and transferring to a target dataset substantially\nimproves performance, even in the presence of powerful contextual\nrepresentations from BERT (Devlin et al., 2019). We also find that training on\nmultiple source RC datasets leads to robust generalization and transfer, and\ncan reduce the cost of example collection for a new RC dataset. Following our\nanalysis, we propose MultiQA, a BERT-based model, trained on multiple RC\ndatasets, which leads to state-of-the-art performance on five RC datasets. We\nshare our infrastructure for the benefit of the research community.", "published": "2019-05-31 08:05:31", "link": "http://arxiv.org/abs/1905.13453v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Crowdsourcing and Validating Event-focused Emotion Corpora for German\n  and English", "abstract": "Sentiment analysis has a range of corpora available across multiple\nlanguages. For emotion analysis, the situation is more limited, which hinders\npotential research on cross-lingual modeling and the development of predictive\nmodels for other languages. In this paper, we fill this gap for German by\nconstructing deISEAR, a corpus designed in analogy to the well-established\nEnglish ISEAR emotion dataset. Motivated by Scherer's appraisal theory, we\nimplement a crowdsourcing experiment which consists of two steps. In step 1,\nparticipants create descriptions of emotional events for a given emotion. In\nstep 2, five annotators assess the emotion expressed by the texts. We show that\ntransferring an emotion classification model from the original English ISEAR to\nthe German crowdsourced deISEAR via machine translation does not, on average,\ncause a performance drop.", "published": "2019-05-31 13:54:54", "link": "http://arxiv.org/abs/1905.13618v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Table2Vec: Neural Word and Entity Embeddings for Table Population and\n  Retrieval", "abstract": "Tables contain valuable knowledge in a structured form. We employ neural\nlanguage modeling approaches to embed tabular data into vector spaces.\nSpecifically, we consider different table elements, such caption, column\nheadings, and cells, for training word and entity embeddings. These embeddings\nare then utilized in three particular table-related tasks, row population,\ncolumn population, and table retrieval, by incorporating them into existing\nretrieval models as additional semantic similarity signals. Evaluation results\nshow that table embeddings can significantly improve upon the performance of\nstate-of-the-art baselines.", "published": "2019-05-31 19:22:29", "link": "http://arxiv.org/abs/1906.00041v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Multimodal Ensemble Approach to Incorporate Various Types of Clinical\n  Notes for Predicting Readmission", "abstract": "Electronic Health Records (EHRs) have been heavily used to predict various\ndownstream clinical tasks such as readmission or mortality. One of the\nmodalities in EHRs, clinical notes, has not been fully explored for these tasks\ndue to its unstructured and inexplicable nature. Although recent advances in\ndeep learning (DL) enables models to extract interpretable features from\nunstructured data, they often require a large amount of training data. However,\nmany tasks in medical domains inherently consist of small sample data with\nlengthy documents; for a kidney transplant as an example, data from only a few\nthousand of patients are available and each patient's document consists of a\ncouple of millions of words in major hospitals. Thus, complex DL methods cannot\nbe applied to these kinds of domains. In this paper, we present a comprehensive\nensemble model using vector space modeling and topic modeling. Our proposed\nmodel is evaluated on the readmission task of kidney transplant patients and\nimproves 0.0211 in terms of c-statistics from the previous state-of-the-art\napproach using structured data, while typical DL methods fail to beat this\napproach. The proposed architecture provides the interpretable score for each\nfeature from both modalities, structured and unstructured data, which is shown\nto be meaningful through a physician's evaluation.", "published": "2019-05-31 20:25:06", "link": "http://arxiv.org/abs/1906.01498v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Real-Time Adversarial Attacks", "abstract": "In recent years, many efforts have demonstrated that modern machine learning\nalgorithms are vulnerable to adversarial attacks, where small, but carefully\ncrafted, perturbations on the input can make them fail. While these attack\nmethods are very effective, they only focus on scenarios where the target model\ntakes static input, i.e., an attacker can observe the entire original sample\nand then add a perturbation at any point of the sample. These attack approaches\nare not applicable to situations where the target model takes streaming input,\ni.e., an attacker is only able to observe past data points and add\nperturbations to the remaining (unobserved) data points of the input. In this\npaper, we propose a real-time adversarial attack scheme for machine learning\nmodels with streaming inputs.", "published": "2019-05-31 03:32:10", "link": "http://arxiv.org/abs/1905.13399v2", "categories": ["cs.CR", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
{"title": "Increasing Compactness Of Deep Learning Based Speech Enhancement Models\n  With Parameter Pruning And Quantization Techniques", "abstract": "Most recent studies on deep learning based speech enhancement (SE) focused on\nimproving denoising performance. However, successful SE applications require\nstriking a desirable balance between denoising performance and computational\ncost in real scenarios. In this study, we propose a novel parameter pruning\n(PP) technique, which removes redundant channels in a neural network. In\naddition, a parameter quantization (PQ) technique was applied to reduce the\nsize of a neural network by representing weights with fewer cluster centroids.\nBecause the techniques are derived based on different concepts, the PP and PQ\ncan be integrated to provide even more compact SE models. The experimental\nresults show that the PP and PQ techniques produce a compacted SE model with a\nsize of only 10.03% compared to that of the original model, resulting in minor\nperformance losses of 1.43% (from 0.70 to 0.69) for STOI and 3.24% (from 1.85\nto 1.79) for PESQ. The promising results suggest that the PP and PQ techniques\ncan be used in a SE system in devices with limited storage and computation\nresources.", "published": "2019-05-31 04:07:20", "link": "http://arxiv.org/abs/1906.01078v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
