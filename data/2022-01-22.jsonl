{"title": "Chinese Word Segmentation with Heterogeneous Graph Neural Network", "abstract": "In recent years, deep learning has achieved significant success in the\nChinese word segmentation (CWS) task. Most of these methods improve the\nperformance of CWS by leveraging external information, e.g., words, sub-words,\nsyntax. However, existing approaches fail to effectively integrate the\nmulti-level linguistic information and also ignore the structural feature of\nthe external information. Therefore, in this paper, we proposed a framework to\nimprove CWS, named HGNSeg. It exploits multi-level external information\nsufficiently with the pre-trained language model and heterogeneous graph neural\nnetwork. The experimental results on six benchmark datasets (e.g., Bakeoff\n2005, Bakeoff 2008) validate that our approach can effectively improve the\nperformance of Chinese word segmentation. Importantly, in cross-domain\nscenarios, our method also shows a strong ability to alleviate the\nout-of-vocabulary (OOV) problem.", "published": "2022-01-22 06:25:56", "link": "http://arxiv.org/abs/2201.08975v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leaf: Multiple-Choice Question Generation", "abstract": "Testing with quiz questions has proven to be an effective way to assess and\nimprove the educational process. However, manually creating quizzes is tedious\nand time-consuming. To address this challenge, we present Leaf, a system for\ngenerating multiple-choice questions from factual text. In addition to being\nvery well suited for the classroom, Leaf could also be used in an industrial\nsetting, e.g., to facilitate onboarding and knowledge sharing, or as a\ncomponent of chatbots, question answering systems, or Massive Open Online\nCourses (MOOCs). The code and the demo are available on\nhttps://github.com/KristiyanVachev/Leaf-Question-Generation.", "published": "2022-01-22 10:17:53", "link": "http://arxiv.org/abs/2201.09012v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Visual Information Guided Zero-Shot Paraphrase Generation", "abstract": "Zero-shot paraphrase generation has drawn much attention as the large-scale\nhigh-quality paraphrase corpus is limited. Back-translation, also known as the\npivot-based method, is typical to this end. Several works leverage different\ninformation as \"pivot\" such as language, semantic representation and so on. In\nthis paper, we explore using visual information such as image as the \"pivot\" of\nback-translation. Different with the pipeline back-translation method, we\npropose visual information guided zero-shot paraphrase generation (ViPG) based\nonly on paired image-caption data. It jointly trains an image captioning model\nand a paraphrasing model and leverage the image captioning model to guide the\ntraining of the paraphrasing model. Both automatic evaluation and human\nevaluation show our model can generate paraphrase with good relevancy, fluency\nand diversity, and image is a promising kind of pivot for zero-shot paraphrase\ngeneration.", "published": "2022-01-22 18:10:39", "link": "http://arxiv.org/abs/2201.09107v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Question rewriting? Assessing its importance for conversational question\n  answering", "abstract": "In conversational question answering, systems must correctly interpret the\ninterconnected interactions and generate knowledgeable answers, which may\nrequire the retrieval of relevant information from a background repository.\nRecent approaches to this problem leverage neural language models, although\ndifferent alternatives can be considered in terms of modules for (a)\nrepresenting user questions in context, (b) retrieving the relevant background\ninformation, and (c) generating the answer. This work presents a conversational\nquestion answering system designed specifically for the Search-Oriented\nConversational AI (SCAI) shared task, and reports on a detailed analysis of its\nquestion rewriting module. In particular, we considered different variations of\nthe question rewriting module to evaluate the influence on the subsequent\ncomponents, and performed a careful analysis of the results obtained with the\nbest system configuration. Our system achieved the best performance in the\nshared task and our analysis emphasizes the importance of the conversation\ncontext representation for the overall system performance.", "published": "2022-01-22 23:31:25", "link": "http://arxiv.org/abs/2201.09146v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Causal effect of racial bias in data and machine learning algorithms on\n  user persuasiveness & discriminatory decision making: An Empirical Study", "abstract": "Language data and models demonstrate various types of bias, be it ethnic,\nreligious, gender, or socioeconomic. AI/NLP models, when trained on the\nracially biased dataset, AI/NLP models instigate poor model explainability,\ninfluence user experience during decision making and thus further magnifies\nsocietal biases, raising profound ethical implications for society. The\nmotivation of the study is to investigate how AI systems imbibe bias from data\nand produce unexplainable discriminatory outcomes and influence an individual's\narticulateness of system outcome due to the presence of racial bias features in\ndatasets. The design of the experiment involves studying the counterfactual\nimpact of racial bias features present in language datasets and its associated\neffect on the model outcome. A mixed research methodology is adopted to\ninvestigate the cross implication of biased model outcome on user experience,\neffect on decision-making through controlled lab experimentation. The findings\nprovide foundation support for correlating the implication of carry-over an\nartificial intelligence model solving NLP task due to biased concept presented\nin the dataset. Further, the research outcomes justify the negative influence\non users' persuasiveness that leads to alter the decision-making quotient of an\nindividual when trying to rely on the model outcome to act. The paper bridges\nthe gap across the harm caused in establishing poor customer trustworthiness\ndue to an inequitable system design and provides strong support for\nresearchers, policymakers, and data scientists to build responsible AI\nframeworks within organizations.", "published": "2022-01-22 08:26:09", "link": "http://arxiv.org/abs/2202.00471v3", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Solvability of orbit-finite systems of linear equations", "abstract": "We study orbit-finite systems of linear equations, in the setting of sets\nwith atoms. Our principal contribution is a decision procedure for solvability\nof such systems. The procedure works for every field (and even commutative\nring) under mild effectiveness assumptions, and reduces a given orbit-finite\nsystem to a number of finite ones: exponentially many in general, but\npolynomially many when atom dimension of input systems is fixed. Towards\nobtaining the procedure we push further the theory of vector spaces generated\nby orbit-finite sets, and show that each such vector space admits an\norbit-finite basis. This fundamental property is a key tool in our development,\nbut should be also of wider interest.", "published": "2022-01-22 14:35:30", "link": "http://arxiv.org/abs/2201.09060v2", "categories": ["cs.CL", "cs.FL", "cs.LO"], "primary_category": "cs.CL"}
{"title": "A Causal Lens for Controllable Text Generation", "abstract": "Controllable text generation concerns two fundamental tasks of wide\napplications, namely generating text of given attributes (i.e.,\nattribute-conditional generation), and minimally editing existing text to\npossess desired attributes (i.e., text attribute transfer). Extensive prior\nwork has largely studied the two problems separately, and developed different\nconditional models which, however, are prone to producing biased text (e.g.,\nvarious gender stereotypes). This paper proposes to formulate controllable text\ngeneration from a principled causal perspective which models the two tasks with\na unified framework. A direct advantage of the causal formulation is the use of\nrich causality tools to mitigate generation biases and improve control. We\ntreat the two tasks as interventional and counterfactual causal inference based\non a structural causal model, respectively. We then apply the framework to the\nchallenging practical setting where confounding factors (that induce spurious\ncorrelations) are observable only on a small fraction of data. Experiments show\nsignificant superiority of the causal approach over previous conditional models\nfor improved control accuracy and reduced bias.", "published": "2022-01-22 19:31:43", "link": "http://arxiv.org/abs/2201.09119v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "A Noise-Robust Self-supervised Pre-training Model Based Speech\n  Representation Learning for Automatic Speech Recognition", "abstract": "Wav2vec2.0 is a popular self-supervised pre-training framework for learning\nspeech representations in the context of automatic speech recognition (ASR). It\nwas shown that wav2vec2.0 has a good robustness against the domain shift, while\nthe noise robustness is still unclear. In this work, we therefore first analyze\nthe noise robustness of wav2vec2.0 via experiments. We observe that wav2vec2.0\npre-trained on noisy data can obtain good representations and thus improve the\nASR performance on the noisy test set, which however brings a performance\ndegradation on the clean test set. To avoid this issue, in this work we propose\nan enhanced wav2vec2.0 model. Specifically, the noisy speech and the\ncorresponding clean version are fed into the same feature encoder, where the\nclean speech provides training targets for the model. Experimental results\nreveal that the proposed method can not only improve the ASR performance on the\nnoisy test set which surpasses the original wav2vec2.0, but also ensure a tiny\nperformance decrease on the clean test set. In addition, the effectiveness of\nthe proposed method is demonstrated under different types of noise conditions.", "published": "2022-01-22 00:22:47", "link": "http://arxiv.org/abs/2201.08930v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Supervised and Self-supervised Pretraining Based COVID-19 Detection\n  Using Acoustic Breathing/Cough/Speech Signals", "abstract": "In this work, we propose a bi-directional long short-term memory (BiLSTM)\nnetwork based COVID-19 detection method using breath/speech/cough signals. By\nusing the acoustic signals to train the network, respectively, we can build\nindividual models for three tasks, whose parameters are averaged to obtain an\naverage model, which is then used as the initialization for the BiLSTM model\ntraining of each task. This initialization method can significantly improve the\nperformance on the three tasks, which surpasses the official baseline results.\nBesides, we also utilize a public pre-trained model wav2vec2.0 and pre-train it\nusing the official DiCOVA datasets. This wav2vec2.0 model is utilized to\nextract high-level features of the sound as the model input to replace\nconventional mel-frequency cepstral coefficients (MFCC) features. Experimental\nresults reveal that using high-level features together with MFCC features can\nimprove the performance. To further improve the performance, we also deploy\nsome preprocessing techniques like silent segment removal, amplitude\nnormalization and time-frequency mask. The proposed detection model is\nevaluated on the DiCOVA dataset and results show that our method achieves an\narea under curve (AUC) score of 88.44% on blind test in the fusion track.", "published": "2022-01-22 01:22:20", "link": "http://arxiv.org/abs/2201.08934v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Exploring auditory acoustic features for the diagnosis of the Covid-19", "abstract": "The current outbreak of a coronavirus, has quickly escalated to become a\nserious global problem that has now been declared a Public Health Emergency of\nInternational Concern by the World Health Organization. Infectious diseases\nknow no borders, so when it comes to controlling outbreaks, timing is\nabsolutely essential. It is so important to detect threats as early as\npossible, before they spread. After a first successful DiCOVA challenge, the\norganisers released second DiCOVA challenge with the aim of diagnosing COVID-19\nthrough the use of breath, cough and speech audio samples. This work presents\nthe details of the automatic system for COVID-19 detection using breath, cough\nand speech recordings. We developed different front-end auditory acoustic\nfeatures along with a bidirectional Long Short-Term Memory (bi-LSTM) as\nclassifier. The results are promising and have demonstrated the high\ncomplementary behaviour among the auditory acoustic features in the Breathing,\nCough and Speech tracks giving an AUC of 86.60% on the test set.", "published": "2022-01-22 18:22:38", "link": "http://arxiv.org/abs/2201.09110v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "NAS-VAD: Neural Architecture Search for Voice Activity Detection", "abstract": "Various neural network-based approaches have been proposed for more robust\nand accurate voice activity detection (VAD). Manual design of such neural\narchitectures is an error-prone and time-consuming process, which prompted the\ndevelopment of neural architecture search (NAS) that automatically design and\noptimize network architectures. While NAS has been successfully applied to\nimprove performance in a variety of tasks, it has not yet been exploited in the\nVAD domain. In this paper, we present the first work that utilizes NAS\napproaches on the VAD task. To effectively search architectures for the VAD\ntask, we propose a modified macro structure and a new search space with a much\nbroader range of operations that includes attention operations. The results\nshow that the network structures found by the propose NAS framework outperform\nprevious manually designed state-of-the-art VAD models in various noise-added\nand real-world-recorded datasets. We also show that the architectures searched\non a particular dataset achieve improved generalization performance on unseen\naudio datasets. Our code and models are available at\nhttps://github.com/daniel03c1/NAS_VAD.", "published": "2022-01-22 12:06:41", "link": "http://arxiv.org/abs/2201.09032v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Artificial Intelligence for Suicide Assessment using Audiovisual Cues: A\n  Review", "abstract": "Death by suicide is the seventh leading death cause worldwide. The recent\nadvancement in Artificial Intelligence (AI), specifically AI applications in\nimage and voice processing, has created a promising opportunity to\nrevolutionize suicide risk assessment. Subsequently, we have witnessed\nfast-growing literature of research that applies AI to extract audiovisual\nnon-verbal cues for mental illness assessment. However, the majority of the\nrecent works focus on depression, despite the evident difference between\ndepression symptoms and suicidal behavior and non-verbal cues. This paper\nreviews recent works that study suicide ideation and suicide behavior detection\nthrough audiovisual feature analysis, mainly suicidal voice/speech acoustic\nfeatures analysis and suicidal visual cues. Automatic suicide assessment is a\npromising research direction that is still in the early stages. Accordingly,\nthere is a lack of large datasets that can be used to train machine learning\nand deep learning models proven to be effective in other, similar tasks.", "published": "2022-01-22 21:17:19", "link": "http://arxiv.org/abs/2201.09130v2", "categories": ["cs.AI", "cs.CV", "cs.CY", "cs.HC", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.AI"}
