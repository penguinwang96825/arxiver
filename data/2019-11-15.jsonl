{"title": "Using natural language processing to extract health-related causality\n  from Twitter messages", "abstract": "Twitter messages (tweets) contain various types of information, which include\nhealth-related information. Analysis of health-related tweets would help us\nunderstand health conditions and concerns encountered in our daily life. In\nthis work, we evaluated an approach to extracting causal relations from tweets\nusing natural language processing (NLP) techniques. We focused on three\nhealth-related topics: stress\", \"insomnia\", and \"headache\". We proposed a set\nof lexico-syntactic patterns based on dependency parser outputs to extract\ncausal information. A large dataset consisting of 24 million tweets were used.\nThe results show that our approach achieved an average precision between 74.59%\nand 92.27%. Analysis of extracted relations revealed interesting findings about\nhealth-related in Twitter.", "published": "2019-11-15 06:30:25", "link": "http://arxiv.org/abs/1911.06488v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bootstrapping NLU Models with Multi-task Learning", "abstract": "Bootstrapping natural language understanding (NLU) systems with minimal\ntraining data is a fundamental challenge of extending digital assistants like\nAlexa and Siri to a new language. A common approach that is adapted in digital\nassistants when responding to a user query is to process the input in a\npipeline manner where the first task is to predict the domain, followed by the\ninference of intent and slots. However, this cascaded approach instigates error\npropagation and prevents information sharing among these tasks. Further, the\nuse of words as the atomic units of meaning as done in many studies might lead\nto coverage problems for morphologically rich languages such as German and\nFrench when data is limited. We address these issues by introducing a\ncharacter-level unified neural architecture for joint modeling of the domain,\nintent, and slot classification. We compose word-embeddings from characters and\njointly optimize all classification tasks via multi-task learning. In our\nresults, we show that the proposed architecture is an optimal choice for\nbootstrapping NLU systems in low-resource settings thus saving time, cost and\nhuman effort.", "published": "2019-11-15 14:47:02", "link": "http://arxiv.org/abs/1911.06673v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating robustness of language models for chief complaint extraction\n  from patient-generated text", "abstract": "Automated classification of chief complaints from patient-generated text is a\ncritical first step in developing scalable platforms to triage patients without\nhuman intervention. In this work, we evaluate several approaches to chief\ncomplaint classification using a novel Chief Complaint (CC) Dataset that\ncontains ~200,000 patient-generated reasons-for-visit entries mapped to a set\nof 795 discrete chief complaints. We examine the use of several fine-tuned\nbidirectional transformer (BERT) models trained on both unrelated texts as well\nas on the CC dataset. We contrast this performance with a TF-IDF baseline. Our\nevaluation has three components: (1) a random test hold-out from the original\ndataset; (2) a \"misspelling set,\" consisting of a hand-selected subset of the\ntest set, where every entry has at least one misspelling; (3) a separate\nexperimenter-generated free-text set. We find that the TF-IDF model performs\nsignificantly better than the strongest BERT-based model on the test (best BERT\nPR-AUC $0.3597 \\pm 0.0041$ vs TF-IDF PR-AUC $0.3878 \\pm 0.0148$, $p=7\\cdot\n10^{-5}$), and is statistically comparable to the misspelling sets (best BERT\nPR-AUC $0.2579 \\pm 0.0079$ vs TF-IDF PR-AUC $0.2733 \\pm 0.0130$, $p=0.06$).\nHowever, when examining model predictions on experimenter-generated queries,\nsome concerns arise about TF-IDF baseline's robustness. Our results suggest\nthat in certain tasks, simple language embedding baselines may be very\nperformant; however, truly understanding their robustness requires further\nanalysis.", "published": "2019-11-15 23:37:41", "link": "http://arxiv.org/abs/1911.06915v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Accuracy-Enhanced Stemming Algorithm for Arabic Information Retrieval", "abstract": "This paper provides a method for indexing and retrieving Arabic texts, based\non natural language processing. Our approach exploits the notion of template in\nword stemming and replaces the words by their stems. This technique has proven\nto be effective since it has returned significant relevant retrieval results by\ndecreasing silence during the retrieval phase. Series of experiments have been\nconducted to test the performance of the proposed algorithm ESAIR (Enhanced\nStemmer for Arabic Information Retrieval). The results obtained indicate that\nthe algorithm extracts the exact root with an accuracy rate up to 96% and\nhence, improving information retrieval.", "published": "2019-11-15 20:17:06", "link": "http://arxiv.org/abs/1911.08249v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Distant Supervised Relation Extraction by Dynamic Neural\n  Network", "abstract": "Distant Supervised Relation Extraction (DSRE) is usually formulated as a\nproblem of classifying a bag of sentences that contain two query entities, into\nthe predefined relation classes. Most existing methods consider those relation\nclasses as distinct semantic categories while ignoring their potential\nconnection to query entities. In this paper, we propose to leverage this\nconnection to improve the relation extraction accuracy. Our key ideas are\ntwofold: (1) For sentences belonging to the same relation class, the expression\nstyle, i.e. words choice, can vary according to the query entities. To account\nfor this style shift, the model should adjust its parameters in accordance with\nentity types. (2) Some relation classes are semantically similar, and the\nentity types appear in one relation may also appear in others. Therefore, it\ncan be trained cross different relation classes and further enhance those\nclasses with few samples, i.e., long-tail classes. To unify these two\narguments, we developed a novel Dynamic Neural Network for Relation Extraction\n(DNNRE). The network adopts a novel dynamic parameter generator that\ndynamically generates the network parameters according to the query entity\ntypes and relation classes. By using this mechanism, the network can\nsimultaneously handle the style shift problem and enhance the prediction\naccuracy for long-tail classes. Through our experimental study, we demonstrate\nthe effectiveness of the proposed method and show that it can achieve superior\nperformance over the state-of-the-art methods.", "published": "2019-11-15 06:31:13", "link": "http://arxiv.org/abs/1911.06489v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Experiments in Detecting Persuasion Techniques in the News", "abstract": "Many recent political events, like the 2016 US Presidential elections or the\n2018 Brazilian elections have raised the attention of institutions and of the\ngeneral public on the role of Internet and social media in influencing the\noutcome of these events. We argue that a safe democracy is one in which\ncitizens have tools to make them aware of propaganda campaigns. We propose a\nnovel task: performing fine-grained analysis of texts by detecting all\nfragments that contain propaganda techniques as well as their type. We further\ndesign a novel multi-granularity neural network, and we show that it\noutperforms several strong BERT-based baselines.", "published": "2019-11-15 07:14:35", "link": "http://arxiv.org/abs/1911.06815v1", "categories": ["cs.CL", "cs.IR", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Assigning Medical Codes at the Encounter Level by Paying Attention to\n  Documents", "abstract": "The vast majority of research in computer assisted medical coding focuses on\ncoding at the document level, but a substantial proportion of medical coding in\nthe real world involves coding at the level of clinical encounters, each of\nwhich is typically represented by a potentially large set of documents. We\nintroduce encounter-level document attention networks, which use hierarchical\nattention to explicitly take the hierarchical structure of encounter\ndocumentation into account. Experimental evaluation demonstrates improvements\nin coding accuracy as well as facilitation of human reviewers in their ability\nto identify which documents within an encounter play a role in determining the\nencounter level codes.", "published": "2019-11-15 19:40:57", "link": "http://arxiv.org/abs/1911.06848v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Subword Level Language Model for Bangla Language", "abstract": "Language models are at the core of natural language processing. The ability\nto represent natural language gives rise to its applications in numerous NLP\ntasks including text classification, summarization, and translation. Research\nin this area is very limited in Bangla due to the scarcity of resources, except\nfor some count-based models and very recent neural language models being\nproposed, which are all based on words and limited in practical tasks due to\ntheir high perplexity. This paper attempts to approach this issue of perplexity\nand proposes a subword level neural language model with the AWD-LSTM\narchitecture and various other techniques suitable for training in Bangla\nlanguage. The model is trained on a corpus of Bangla newspaper articles of an\nappreciable size consisting of more than 28.5 million word tokens. The\nperformance comparison with various other models depicts the significant\nreduction in perplexity the proposed model provides, reaching as low as 39.84,\nin just 20 epochs.", "published": "2019-11-15 08:22:33", "link": "http://arxiv.org/abs/1911.07613v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploiting Token and Path-based Representations of Code for Identifying\n  Security-Relevant Commits", "abstract": "Public vulnerability databases such as CVE and NVD account for only 60% of\nsecurity vulnerabilities present in open-source projects, and are known to\nsuffer from inconsistent quality. Over the last two years, there has been\nconsiderable growth in the number of known vulnerabilities across projects\navailable in various repositories such as NPM and Maven Central. Such an\nincreasing risk calls for a mechanism to infer the presence of security threats\nin a timely manner. We propose novel hierarchical deep learning models for the\nidentification of security-relevant commits from either the commit diff or the\nsource code for the Java classes. By comparing the performance of our model\nagainst code2vec, a state-of-the-art model that learns from path-based\nrepresentations of code, and a logistic regression baseline, we show that deep\nlearning models show promising results in identifying security-related commits.\nWe also conduct a comparative analysis of how various deep learning models\nlearn across different input representations and the effect of regularization\non the generalization of our models.", "published": "2019-11-15 03:16:12", "link": "http://arxiv.org/abs/1911.07620v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Independent and automatic evaluation of acoustic-to-articulatory\n  inversion models", "abstract": "Reconstruction of articulatory trajectories from the acoustic speech signal\nhas been proposed for improving speech recognition and text-to-speech\nsynthesis. However, to be useful in these settings, articulatory reconstruction\nmust be speaker independent. Furthermore, as most research focuses on single,\nsmall datasets with few speakers, robust articulatory reconstrucion could\nprofit from combining datasets. Standard evaluation measures such as root mean\nsquare error and Pearson correlation are inappropriate for evaluating the\nspeaker-independence of models or the usefulness of combining datasets. We\npresent a new evaluation for articulatory reconstruction which is independent\nof the articulatory data set used for training: the phone discrimination ABX\ntask. We use the ABX measure to evaluate a Bi-LSTM based model trained on 3\ndatasets (14 speakers), and show that it gives information complementary to the\nstandard measures, and enables us to evaluate the effects of dataset merging,\nas well as the speaker independence of the model.", "published": "2019-11-15 11:33:50", "link": "http://arxiv.org/abs/1911.06573v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "CatGAN: Category-aware Generative Adversarial Networks with Hierarchical\n  Evolutionary Learning for Category Text Generation", "abstract": "Generating multiple categories of texts is a challenging task and draws more\nand more attention. Since generative adversarial nets (GANs) have shown\ncompetitive results on general text generation, they are extended for category\ntext generation in some previous works. However, the complicated model\nstructures and learning strategies limit their performance and exacerbate the\ntraining instability. This paper proposes a category-aware GAN (CatGAN) which\nconsists of an efficient category-aware model for category text generation and\na hierarchical evolutionary learning algorithm for training our model. The\ncategory-aware model directly measures the gap between real samples and\ngenerated samples on each category, then reducing this gap will guide the model\nto generate high-quality category samples. The Gumbel-Softmax relaxation\nfurther frees our model from complicated learning strategies for updating\nCatGAN on discrete data. Moreover, only focusing on the sample quality normally\nleads the mode collapse problem, thus a hierarchical evolutionary learning\nalgorithm is introduced to stabilize the training procedure and obtain the\ntrade-off between quality and diversity while training CatGAN. Experimental\nresults demonstrate that CatGAN outperforms most of the existing\nstate-of-the-art methods.", "published": "2019-11-15 14:03:30", "link": "http://arxiv.org/abs/1911.06641v2", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Towards Personalized Dialog Policies for Conversational Skill Discovery", "abstract": "Many businesses and consumers are extending the capabilities of voice-based\nservices such as Amazon Alexa, Google Home, Microsoft Cortana, and Apple Siri\nto create custom voice experiences (also known as skills). As the number of\nthese experiences increases, a key problem is the discovery of skills that can\nbe used to address a user's request. In this paper, we focus on conversational\nskill discovery and present a conversational agent which engages in a dialog\nwith users to help them find the skills that fulfill their needs. To this end,\nwe start with a rule-based agent and improve it by using reinforcement\nlearning. In this way, we enable the agent to adapt to different user\nattributes and conversational styles as it interacts with users. We evaluate\nour approach in a real production setting by deploying the agent to interact\nwith real users, and show the effectiveness of the conversational agent in\nhelping users find the skills that serve their request.", "published": "2019-11-15 16:56:17", "link": "http://arxiv.org/abs/1911.06747v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CNN-based Dual-Chain Models for Knowledge Graph Learning", "abstract": "Knowledge graph learning plays a critical role in integrating domain specific\nknowledge bases when deploying machine learning and data mining models in\npractice. Existing methods on knowledge graph learning primarily focus on\nmodeling the relations among entities as translations among the relations and\nentities, and many of these methods are not able to handle zero-shot problems,\nwhen new entities emerge. In this paper, we present a new convolutional neural\nnetwork (CNN)-based dual-chain model. Different from translation based methods,\nin our model, interactions among relations and entities are directly captured\nvia CNN over their embeddings. Moreover, a secondary chain of learning is\nconducted simultaneously to incorporate additional information and to enable\nbetter performance. We also present an extension of this model, which\nincorporates descriptions of entities and learns a second set of entity\nembeddings from the descriptions. As a result, the extended model is able to\neffectively handle zero-shot problems. We conducted comprehensive experiments,\ncomparing our methods with 15 methods on 8 benchmark datasets. Extensive\nexperimental results demonstrate that our proposed methods achieve or\noutperform the state-of-the-art results on knowledge graph learning, and\noutperform other methods on zero-shot problems. In addition, our methods\napplied to real-world biomedical data are able to produce results that conform\nto expert domain knowledge.", "published": "2019-11-15 23:24:17", "link": "http://arxiv.org/abs/1911.06910v2", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Selection-based Question Answering of an MOOC", "abstract": "e-Yantra Robotics Competition (eYRC) is a unique Robotics Competition hosted\nby IIT Bombay that is actually an Embedded Systems and Robotics MOOC.\nRegistrations have been growing exponentially in each year from 4500 in 2012 to\nover 34000 in 2019. In this 5-month long competition students learn complex\nskills under severe time pressure and have access to a discussion forum to post\ndoubts about the learning material. Responding to questions in real-time is a\nchallenge for project staff. Here, we illustrate the advantage of Deep Learning\nfor real-time question answering in the eYRC discussion forum. We illustrate\nthe advantage of Transformer based contextual embedding mechanisms such as\nBidirectional Encoder Representation From Transformer (BERT) over word\nembedding mechanisms such as Word2Vec. We propose a weighted similarity metric\nas a measure of matching and find it more reliable than Content-Content or\nTitle-Title similarities alone. The automation of replying to questions has\nbrought the turn around response time(TART) down from a minimum of 21 mins to a\nminimum of 0.3 secs.", "published": "2019-11-15 09:20:32", "link": "http://arxiv.org/abs/1911.07629v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Deep Long Audio Inpainting", "abstract": "Long (> 200 ms) audio inpainting, to recover a long missing part in an audio\nsegment, could be widely applied to audio editing tasks and transmission loss\nrecovery. It is a very challenging problem due to the high dimensional, complex\nand non-correlated audio features. While deep learning models have made\ntremendous progress in image and video inpainting, audio inpainting did not\nattract much attention. In this work, we take a pioneering step, exploring the\npossibility of adapting deep learning frameworks from various domains inclusive\nof audio synthesis and image inpainting for audio inpainting. Also, as the\nfirst to systematically analyze factors affecting audio inpainting performance,\nwe explore how factors ranging from mask size, receptive field and audio\nrepresentation could affect the performance. We also set up a benchmark for\nlong audio inpainting. The code will be available on GitHub upon accepted.", "published": "2019-11-15 04:42:29", "link": "http://arxiv.org/abs/1911.06476v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Sample Drop Detection for Distant-speech Recognition with Asynchronous\n  Devices Distributed in Space", "abstract": "In many applications of multi-microphone multi-device processing, the\nsynchronization among different input channels can be affected by the lack of a\ncommon clock and isolated drops of samples. In this work, we address the issue\nof sample drop detection in the context of a conversational speech scenario,\nrecorded by a set of microphones distributed in space. The goal is to design a\nneural-based model that given a short window in the time domain, detects\nwhether one or more devices have been subjected to a sample drop event. The\ncandidate time windows are selected from a set of large time intervals,\npossibly including a sample drop, and by using a preprocessing step. The latter\nis based on the application of normalized cross-correlation between signals\nacquired by different devices. The architecture of the neural network relies on\na CNN-LSTM encoder, followed by multi-head attention. The experiments are\nconducted using both artificial and real data. Our proposed approach obtained\nF1 score of 88% on an evaluation set extracted from the CHiME-5 corpus. A\ncomparable performance was found in a larger set of experiments conducted on a\nset of multi-channel artificial scenes.", "published": "2019-11-15 15:56:43", "link": "http://arxiv.org/abs/1911.06713v1", "categories": ["cs.SD", "eess.AS", "I.2.7"], "primary_category": "cs.SD"}
{"title": "Adaptive Multi-scale Detection of Acoustic Events", "abstract": "The goal of acoustic (or sound) events detection (AED or SED) is to predict\nthe temporal position of target events in given audio segments. This task plays\na significant role in safety monitoring, acoustic early warning and other\nscenarios. However, the deficiency of data and diversity of acoustic event\nsources make the AED task a tough issue, especially for prevalent data-driven\nmethods. In this paper, we start by analyzing acoustic events according to\ntheir time-frequency domain properties, showing that different acoustic events\nhave different time-frequency scale characteristics. Inspired by the analysis,\nwe propose an adaptive multi-scale detection (AdaMD) method. By taking\nadvantage of the hourglass neural network and gated recurrent unit (GRU)\nmodule, our AdaMD produces multiple predictions at different temporal and\nfrequency resolutions. An adaptive training algorithm is subsequently adopted\nto combine multi-scale predictions to enhance its overall capability.\nExperimental results on Detection and Classification of Acoustic Scenes and\nEvents 2017 (DCASE 2017) Task 2, DCASE 2016 Task 3 and DCASE 2017 Task 3\ndemonstrate that the AdaMD outperforms published state-of-the-art competitors\nin terms of the metrics of event error rate (ER) and F1-score. The verification\nexperiment on our collected factory mechanical dataset also proves the\nnoise-resistant capability of the AdaMD, providing the possibility for it to be\ndeployed in the complex environment.", "published": "2019-11-15 21:20:03", "link": "http://arxiv.org/abs/1911.06878v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Cross-modal supervised learning for better acoustic representations", "abstract": "Obtaining large-scale human-labeled datasets to train acoustic representation\nmodels is a very challenging task. On the contrary, we can easily collect data\nwith machine-generated labels. In this work, we propose to exploit\nmachine-generated labels to learn better acoustic representations, based on the\nsynchronization between vision and audio. Firstly, we collect a large-scale\nvideo dataset with 15 million samples, which totally last 16,320 hours. Each\nvideo is 3 to 5 seconds in length and annotated automatically by publicly\navailable visual and audio classification models. Secondly, we train various\nclassical convolutional neural networks (CNNs) including VGGish, ResNet 50 and\nMobilenet v2. We also make several improvements to VGGish and achieve better\nresults. Finally, we transfer our models on three external standard benchmarks\nfor audio classification task, and achieve significant performance boost over\nthe state-of-the-art results. Models and codes are available at:\nhttps://github.com/Deeperjia/vgg-like-audio-models.", "published": "2019-11-15 02:23:23", "link": "http://arxiv.org/abs/1911.07917v2", "categories": ["cs.CV", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Generative Audio Synthesis with a Parametric Model", "abstract": "Use a parametric representation of audio to train a generative model in the\ninterest of obtaining more flexible control over the generated sound.", "published": "2019-11-15 20:59:30", "link": "http://arxiv.org/abs/1911.08335v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
