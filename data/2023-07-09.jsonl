{"title": "Optimal Transport Posterior Alignment for Cross-lingual Semantic Parsing", "abstract": "Cross-lingual semantic parsing transfers parsing capability from a\nhigh-resource language (e.g., English) to low-resource languages with scarce\ntraining data. Previous work has primarily considered silver-standard data\naugmentation or zero-shot methods, however, exploiting few-shot gold data is\ncomparatively unexplored. We propose a new approach to cross-lingual semantic\nparsing by explicitly minimizing cross-lingual divergence between probabilistic\nlatent variables using Optimal Transport. We demonstrate how this direct\nguidance improves parsing from natural languages using fewer examples and less\ntraining. We evaluate our method on two datasets, MTOP and MultiATIS++SQL,\nestablishing state-of-the-art results under a few-shot cross-lingual regime.\nAblation studies further reveal that our method improves performance even\nwithout parallel input translations. In addition, we show that our model better\ncaptures cross-lingual structure in the latent space to improve semantic\nrepresentation similarity.", "published": "2023-07-09 04:52:31", "link": "http://arxiv.org/abs/2307.04096v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards cross-language prosody transfer for dialog", "abstract": "Speech-to-speech translation systems today do not adequately support use for\ndialog purposes. In particular, nuances of speaker intent and stance can be\nlost due to improper prosody transfer. We present an exploration of what needs\nto be done to overcome this. First, we developed a data collection protocol in\nwhich bilingual speakers re-enact utterances from an earlier conversation in\ntheir other language, and used this to collect an English-Spanish corpus, so\nfar comprising 1871 matched utterance pairs. Second, we developed a simple\nprosodic dissimilarity metric based on Euclidean distance over a broad set of\nprosodic features. We then used these to investigate cross-language prosodic\ndifferences, measure the likely utility of three simple baseline models, and\nidentify phenomena which will require more powerful modeling. Our findings\nshould inform future research on cross-language prosody and the design of\nspeech-to-speech translation systems capable of effective prosody transfer.", "published": "2023-07-09 08:32:14", "link": "http://arxiv.org/abs/2307.04123v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automated Essay Scoring in Argumentative Writing: DeBERTeachingAssistant", "abstract": "Automated Essay scoring has been explored as a research and industry problem\nfor over 50 years. It has drawn a lot of attention from the NLP community\nbecause of its clear educational value as a research area that can engender the\ncreation of valuable time-saving tools for educators around the world. Yet,\nthese tools are generally focused on detecting good grammar, spelling mistakes,\nand organization quality but tend to fail at incorporating persuasiveness\nfeatures in their final assessment. The responsibility to give actionable\nfeedback to the student to improve the strength of their arguments is left\nsolely on the teacher's shoulders. In this work, we present a transformer-based\narchitecture capable of achieving above-human accuracy in annotating\nargumentative writing discourse elements for their persuasiveness quality and\nwe expand on planned future work investigating the explainability of our model\nso that actionable feedback can be offered to the student and thus potentially\nenable a partnership between the teacher's advice and the machine's advice.", "published": "2023-07-09 23:02:19", "link": "http://arxiv.org/abs/2307.04276v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Coding at Scale: Design and Deployment of a Nationwide System\n  for Normalizing Referrals in the Chilean Public Healthcare System", "abstract": "The disease coding task involves assigning a unique identifier from a\ncontrolled vocabulary to each disease mentioned in a clinical document. This\ntask is relevant since it allows information extraction from unstructured data\nto perform, for example, epidemiological studies about the incidence and\nprevalence of diseases in a determined context. However, the manual coding\nprocess is subject to errors as it requires medical personnel to be competent\nin coding rules and terminology. In addition, this process consumes a lot of\ntime and energy, which could be allocated to more clinically relevant tasks.\nThese difficulties can be addressed by developing computational systems that\nautomatically assign codes to diseases. In this way, we propose a two-step\nsystem for automatically coding diseases in referrals from the Chilean public\nhealthcare system. Specifically, our model uses a state-of-the-art NER model\nfor recognizing disease mentions and a search engine system based on\nElasticsearch for assigning the most relevant codes associated with these\ndisease mentions. The system's performance was evaluated on referrals manually\ncoded by clinical experts. Our system obtained a MAP score of 0.63 for the\nsubcategory level and 0.83 for the category level, close to the best-performing\nmodels in the literature. This system could be a support tool for health\nprofessionals, optimizing the coding and management process. Finally, to\nguarantee reproducibility, we publicly release the code of our models and\nexperiments.", "published": "2023-07-09 16:19:35", "link": "http://arxiv.org/abs/2307.05560v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Augmenters at SemEval-2023 Task 1: Enhancing CLIP in Handling\n  Compositionality and Ambiguity for Zero-Shot Visual WSD through Prompt\n  Augmentation and Text-To-Image Diffusion", "abstract": "This paper describes our zero-shot approaches for the Visual Word Sense\nDisambiguation (VWSD) Task in English. Our preliminary study shows that the\nsimple approach of matching candidate images with the phrase using CLIP suffers\nfrom the many-to-many nature of image-text pairs. We find that the CLIP text\nencoder may have limited abilities in capturing the compositionality in natural\nlanguage. Conversely, the descriptive focus of the phrase varies from instance\nto instance. We address these issues in our two systems, Augment-CLIP and\nStable Diffusion Sampling (SD Sampling). Augment-CLIP augments the text prompt\nby generating sentences that contain the context phrase with the help of large\nlanguage models (LLMs). We further explore CLIP models in other languages, as\nthe an ambiguous word may be translated into an unambiguous one in the other\nlanguage. SD Sampling uses text-to-image Stable Diffusion to generate multiple\nimages from the given phrase, increasing the likelihood that a subset of images\nmatch the one that paired with the text.", "published": "2023-07-09 22:39:37", "link": "http://arxiv.org/abs/2307.05564v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessing the efficacy of large language models in generating accurate\n  teacher responses", "abstract": "(Tack et al., 2023) organized the shared task hosted by the 18th Workshop on\nInnovative Use of NLP for Building Educational Applications on generation of\nteacher language in educational dialogues. Following the structure of the\nshared task, in this study, we attempt to assess the generative abilities of\nlarge language models in providing informative and helpful insights to\nstudents, thereby simulating the role of a knowledgeable teacher. To this end,\nwe present an extensive evaluation of several benchmarking generative models,\nincluding GPT-4 (few-shot, in-context learning), fine-tuned GPT-2, and\nfine-tuned DialoGPT. Additionally, to optimize for pedagogical quality, we\nfine-tuned the Flan-T5 model using reinforcement learning. Our experimental\nfindings on the Teacher-Student Chatroom Corpus subset indicate the efficacy of\nGPT-4 over other fine-tuned models, measured using BERTScore and DialogRPT.\n  We hypothesize that several dataset characteristics, including sampling,\nrepresentativeness, and dialog completeness, pose significant challenges to\nfine-tuning, thus contributing to the poor generalizability of the fine-tuned\nmodels. Finally, we note the need for these generative models to be evaluated\nwith a metric that relies not only on dialog coherence and matched language\nmodeling distribution but also on the model's ability to showcase pedagogical\nskills.", "published": "2023-07-09 22:32:46", "link": "http://arxiv.org/abs/2307.04274v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DebateKG: Automatic Policy Debate Case Creation with Semantic Knowledge\n  Graphs", "abstract": "Recent work within the Argument Mining community has shown the applicability\nof Natural Language Processing systems for solving problems found within\ncompetitive debate. One of the most important tasks within competitive debate\nis for debaters to create high quality debate cases. We show that effective\ndebate cases can be constructed using constrained shortest path traversals on\nArgumentative Semantic Knowledge Graphs. We study this potential in the context\nof a type of American Competitive Debate, called Policy Debate, which already\nhas a large scale dataset targeting it called DebateSum. We significantly\nimprove upon DebateSum by introducing 53180 new examples, as well as further\nuseful metadata for every example, to the dataset. We leverage the txtai\nsemantic search and knowledge graph toolchain to produce and contribute 9\nsemantic knowledge graphs built on this dataset. We create a unique method for\nevaluating which knowledge graphs are better in the context of producing policy\ndebate cases. A demo which automatically generates debate cases, along with all\nother code and the Knowledge Graphs, are open-sourced and made available to the\npublic here: https://huggingface.co/spaces/Hellisotherpeople/DebateKG", "published": "2023-07-09 04:19:19", "link": "http://arxiv.org/abs/2307.04090v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Dream Content Discovery from Reddit with an Unsupervised Mixed-Method\n  Approach", "abstract": "Dreaming is a fundamental but not fully understood part of human experience\nthat can shed light on our thought patterns. Traditional dream analysis\npractices, while popular and aided by over 130 unique scales and rating\nsystems, have limitations. Mostly based on retrospective surveys or lab\nstudies, they struggle to be applied on a large scale or to show the importance\nand connections between different dream themes. To overcome these issues, we\ndeveloped a new, data-driven mixed-method approach for identifying topics in\nfree-form dream reports through natural language processing. We tested this\nmethod on 44,213 dream reports from Reddit's r/Dreams subreddit, where we found\n217 topics, grouped into 22 larger themes: the most extensive collection of\ndream topics to date. We validated our topics by comparing it to the\nwidely-used Hall and van de Castle scale. Going beyond traditional scales, our\nmethod can find unique patterns in different dream types (like nightmares or\nrecurring dreams), understand topic importance and connections, and observe\nchanges in collective dream experiences over time and around major events, like\nthe COVID-19 pandemic and the recent Russo-Ukrainian war. We envision that the\napplications of our method will provide valuable insights into the intricate\nnature of dreaming.", "published": "2023-07-09 13:24:58", "link": "http://arxiv.org/abs/2307.04167v1", "categories": ["cs.CY", "cs.CL", "physics.soc-ph", "H.4.0; K.4.0"], "primary_category": "cs.CY"}
{"title": "Can Generative Large Language Models Perform ASR Error Correction?", "abstract": "ASR error correction is an interesting option for post processing speech\nrecognition system outputs. These error correction models are usually trained\nin a supervised fashion using the decoding results of a target ASR system. This\napproach can be computationally intensive and the model is tuned to a specific\nASR system. Recently generative large language models (LLMs) have been applied\nto a wide range of natural language processing tasks, as they can operate in a\nzero-shot or few shot fashion. In this paper we investigate using ChatGPT, a\ngenerative LLM, for ASR error correction. Based on the ASR N-best output, we\npropose both unconstrained and constrained, where a member of the N-best list\nis selected, approaches. Additionally, zero and 1-shot settings are evaluated.\nExperiments show that this generative LLM approach can yield performance gains\nfor two different state-of-the-art ASR architectures, transducer and\nattention-encoder-decoder based, and multiple test sets.", "published": "2023-07-09 13:38:25", "link": "http://arxiv.org/abs/2307.04172v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Self-Adaptive Sampling for Efficient Video Question-Answering on\n  Image--Text Models", "abstract": "Video question-answering is a fundamental task in the field of video\nunderstanding. Although current vision--language models (VLMs) equipped with\nVideo Transformers have enabled temporal modeling and yielded superior results,\nthey are at the cost of huge computational power and thus too expensive to\ndeploy in real-time application scenarios. An economical workaround only\nsamples a small portion of frames to represent the main content of that video\nand tune an image--text model on these sampled frames. Recent video\nunderstanding models usually randomly sample a set of frames or clips,\nregardless of internal correlations between their visual contents, nor their\nrelevance to the problem. We argue that such kinds of aimless sampling may omit\nthe key frames from which the correct answer can be deduced, and the situation\ngets worse when the sampling sparsity increases, which always happens as the\nvideo lengths increase. To mitigate this issue, we propose two frame sampling\nstrategies, namely the most domain frames (MDF) and most implied frames (MIF),\nto maximally preserve those frames that are most likely vital to the given\nquestions. MDF passively minimizes the risk of key frame omission in a\nbootstrap manner, while MIS actively searches key frames customized for each\nvideo--question pair with the assistance of auxiliary models. The experimental\nresults on three public datasets from three advanced VLMs (CLIP, GIT and\nAll-in-one) demonstrate that our proposed strategies can boost the performance\nfor image-text pretrained models. The source codes pertaining to the method\nproposed in this paper are publicly available at\nhttps://github.com/declare-lab/sas-vqa.", "published": "2023-07-09 14:54:30", "link": "http://arxiv.org/abs/2307.04192v4", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "ChatGPT in the Age of Generative AI and Large Language Models: A Concise\n  Survey", "abstract": "ChatGPT is a large language model (LLM) created by OpenAI that has been\ncarefully trained on a large amount of data. It has revolutionized the field of\nnatural language processing (NLP) and has pushed the boundaries of LLM\ncapabilities. ChatGPT has played a pivotal role in enabling widespread public\ninteraction with generative artificial intelligence (GAI) on a large scale. It\nhas also sparked research interest in developing similar technologies and\ninvestigating their applications and implications. In this paper, our primary\ngoal is to provide a concise survey on the current lines of research on ChatGPT\nand its evolution. We considered both the glass box and black box views of\nChatGPT, encompassing the components and foundational elements of the\ntechnology, as well as its applications, impacts, and implications. The glass\nbox approach focuses on understanding the inner workings of the technology, and\nthe black box approach embraces it as a complex system, and thus examines its\ninputs, outputs, and effects. This paves the way for a comprehensive\nexploration of the technology and provides a road map for further research and\nexperimentation. We also lay out essential foundational literature on LLMs and\nGAI in general and their connection with ChatGPT. This overview sheds light on\nexisting and missing research lines in the emerging field of LLMs, benefiting\nboth public users and developers. Furthermore, the paper delves into the broad\nspectrum of applications and significant concerns in fields such as education,\nresearch, healthcare, finance, etc.", "published": "2023-07-09 19:28:46", "link": "http://arxiv.org/abs/2307.04251v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Review of feedback in Automated Essay Scoring", "abstract": "The first automated essay scoring system was developed 50 years ago.\nAutomated essay scoring systems are developing into systems with richer\nfunctions than the previous simple scoring systems. Its purpose is not only to\nscore essays but also as a learning tool to improve the writing skill of users.\nFeedback is the most important aspect of making an automated essay scoring\nsystem useful in real life. The importance of feedback was already emphasized\nin the first AES system. This paper reviews research on feedback including\ndifferent feedback types and essay traits on automated essay scoring. We also\nreviewed the latest case studies of the automated essay scoring system that\nprovides feedback.", "published": "2023-07-09 11:04:13", "link": "http://arxiv.org/abs/2307.05553v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; I.2.2; K.3.1"], "primary_category": "cs.CL"}
{"title": "A Personalized Reinforcement Learning Summarization Service for Learning\n  Structure from Unstructured Data", "abstract": "The exponential growth of textual data has created a crucial need for tools\nthat assist users in extracting meaningful insights. Traditional document\nsummarization approaches often fail to meet individual user requirements and\nlack structure for efficient information processing. To address these\nlimitations, we propose Summation, a hierarchical personalized concept-based\nsummarization approach. It synthesizes documents into a concise hierarchical\nconcept map and actively engages users by learning and adapting to their\npreferences. Using a Reinforcement Learning algorithm, Summation generates\npersonalized summaries for unseen documents on specific topics. This framework\nenhances comprehension, enables effective navigation, and empowers users to\nextract meaningful insights from large document collections aligned with their\nunique requirements.", "published": "2023-07-09 01:19:08", "link": "http://arxiv.org/abs/2307.05696v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Disentangling Societal Inequality from Model Biases: Gender Inequality\n  in Divorce Court Proceedings", "abstract": "Divorce is the legal dissolution of a marriage by a court. Since this is\nusually an unpleasant outcome of a marital union, each party may have reasons\nto call the decision to quit which is generally documented in detail in the\ncourt proceedings. Via a substantial corpus of 17,306 court proceedings, this\npaper investigates gender inequality through the lens of divorce court\nproceedings. While emerging data sources (e.g., public court records) on\nsensitive societal issues hold promise in aiding social science research,\nbiases present in cutting-edge natural language processing (NLP) methods may\ninterfere with or affect such studies. We thus require a thorough analysis of\npotential gaps and limitations present in extant NLP resources. In this paper,\non the methodological side, we demonstrate that existing NLP resources required\nseveral non-trivial modifications to quantify societal inequalities. On the\nsubstantive side, we find that while a large number of court cases perhaps\nsuggest changing norms in India where women are increasingly challenging\npatriarchy, AI-powered analyses of these court proceedings indicate striking\ngender inequality with women often subjected to domestic violence.", "published": "2023-07-09 02:31:56", "link": "http://arxiv.org/abs/2307.10200v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "FILM: How can Few-Shot Image Classification Benefit from Pre-Trained\n  Language Models?", "abstract": "Few-shot learning aims to train models that can be generalized to novel\nclasses with only a few samples. Recently, a line of works are proposed to\nenhance few-shot learning with accessible semantic information from class\nnames. However, these works focus on improving existing modules such as visual\nprototypes and feature extractors of the standard few-shot learning framework.\nThis limits the full potential use of semantic information. In this paper, we\npropose a novel few-shot learning framework that uses pre-trained language\nmodels based on contrastive learning. To address the challenge of alignment\nbetween visual features and textual embeddings obtained from text-based\npre-trained language model, we carefully design the textual branch of our\nframework and introduce a metric module to generalize the cosine similarity.\nFor better transferability, we let the metric module adapt to different\nfew-shot tasks and adopt MAML to train the model via bi-level optimization.\nMoreover, we conduct extensive experiments on multiple benchmarks to\ndemonstrate the effectiveness of our method.", "published": "2023-07-09 08:07:43", "link": "http://arxiv.org/abs/2307.04114v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.MM"], "primary_category": "cs.LG"}
{"title": "IANS: Intelligibility-aware Null-steering Beamforming for\n  Dual-Microphone Arrays", "abstract": "Beamforming techniques are popular in speech-related applications due to\ntheir effective spatial filtering capabilities. Nonetheless, conventional\nbeamforming techniques generally depend heavily on either the target's\ndirection-of-arrival (DOA), relative transfer function (RTF) or covariance\nmatrix. This paper presents a new approach, the intelligibility-aware\nnull-steering (IANS) beamforming framework, which uses the STOI-Net\nintelligibility prediction model to improve speech intelligibility without\nprior knowledge of the speech signal parameters mentioned earlier. The IANS\nframework combines a null-steering beamformer (NSBF) to generate a set of\nbeamformed outputs, and STOI-Net, to determine the optimal result. Experimental\nresults indicate that IANS can produce intelligibility-enhanced signals using a\nsmall dual-microphone array. The results are comparable to those obtained by\nnull-steering beamformers with given knowledge of DOAs.", "published": "2023-07-09 14:04:58", "link": "http://arxiv.org/abs/2307.04179v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
