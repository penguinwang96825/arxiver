{"title": "What time is it? Temporal Analysis of Novels", "abstract": "Recognizing the flow of time in a story is a crucial aspect of understanding\nit. Prior work related to time has primarily focused on identifying temporal\nexpressions or relative sequencing of events, but here we propose\ncomputationally annotating each line of a book with wall clock times, even in\nthe absence of explicit time-descriptive phrases. To do so, we construct a data\nset of hourly time phrases from 52,183 fictional books. We then construct a\ntime-of-day classification model that achieves an average error of 2.27 hours.\nFurthermore, we show that by analyzing a book in whole using dynamic\nprogramming of breakpoints, we can roughly partition a book into segments that\neach correspond to a particular time-of-day. This approach improves upon\nbaselines by over two hours. Finally, we apply our model to a corpus of\nliterature categorized by different periods in history, to show interesting\ntrends of hourly activity throughout the past. Among several observations we\nfind that the fraction of events taking place past 10 P.M jumps past 1880 -\ncoincident with the advent of the electric light bulb and city lights.", "published": "2020-11-09 01:11:55", "link": "http://arxiv.org/abs/2011.04124v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Summarization of Open-Domain Podcast Episodes", "abstract": "We present implementation details of our abstractive summarizers that achieve\ncompetitive results on the Podcast Summarization task of TREC 2020. A concise\ntextual summary that captures important information is crucial for users to\ndecide whether to listen to the podcast. Prior work focuses primarily on\nlearning contextualized representations. Instead, we investigate several\nless-studied aspects of neural abstractive summarization, including (i) the\nimportance of selecting important segments from transcripts to serve as input\nto the summarizer; (ii) striking a balance between the amount and quality of\ntraining instances; (iii) the appropriate summary length and start/end points.\nWe highlight the design considerations behind our system and offer key insights\ninto the strengths and weaknesses of neural abstractive systems. Our results\nsuggest that identifying important segments from transcripts to use as input to\nan abstractive summarizer is advantageous for summarizing long documents. Our\nbest system achieves a quality rating of 1.559 judged by NIST evaluators---an\nabsolute increase of 0.268 (+21%) over the creator descriptions.", "published": "2020-11-09 01:31:05", "link": "http://arxiv.org/abs/2011.04132v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CxGBERT: BERT meets Construction Grammar", "abstract": "While lexico-semantic elements no doubt capture a large amount of linguistic\ninformation, it has been argued that they do not capture all information\ncontained in text. This assumption is central to constructionist approaches to\nlanguage which argue that language consists of constructions, learned pairings\nof a form and a function or meaning that are either frequent or have a meaning\nthat cannot be predicted from its component parts. BERT's training objectives\ngive it access to a tremendous amount of lexico-semantic information, and while\nBERTology has shown that BERT captures certain important linguistic dimensions,\nthere have been no studies exploring the extent to which BERT might have access\nto constructional information. In this work we design several probes and\nconduct extensive experiments to answer this question. Our results allow us to\nconclude that BERT does indeed have access to a significant amount of\ninformation, much of which linguists typically call constructional information.\nThe impact of this observation is potentially far-reaching as it provides\ninsights into what deep learning methods learn from text, while also showing\nthat information contained in constructions is redundantly encoded in\nlexico-semantics.", "published": "2020-11-09 01:39:52", "link": "http://arxiv.org/abs/2011.04134v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"What Do You Mean by That?\" A Parser-Independent Interactive Approach\n  for Enhancing Text-to-SQL", "abstract": "In Natural Language Interfaces to Databases systems, the text-to-SQL\ntechnique allows users to query databases by using natural language questions.\nThough significant progress in this area has been made recently, most parsers\nmay fall short when they are deployed in real systems. One main reason stems\nfrom the difficulty of fully understanding the users' natural language\nquestions. In this paper, we include human in the loop and present a novel\nparser-independent interactive approach (PIIA) that interacts with users using\nmulti-choice questions and can easily work with arbitrary parsers. Experiments\nwere conducted on two cross-domain datasets, the WikiSQL and the more complex\nSpider, with five state-of-the-art parsers. These demonstrated that PIIA is\ncapable of enhancing the text-to-SQL performance with limited interaction turns\nby using both simulation and human evaluation.", "published": "2020-11-09 02:14:33", "link": "http://arxiv.org/abs/2011.04151v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Chapter Captor: Text Segmentation in Novels", "abstract": "Books are typically segmented into chapters and sections, representing\ncoherent subnarratives and topics. We investigate the task of predicting\nchapter boundaries, as a proxy for the general task of segmenting long texts.\nWe build a Project Gutenberg chapter segmentation data set of 9,126 English\nnovels, using a hybrid approach combining neural inference and rule matching to\nrecognize chapter title headers in books, achieving an F1-score of 0.77 on this\ntask. Using this annotated data as ground truth after removing structural cues,\nwe present cut-based and neural methods for chapter segmentation, achieving an\nF1-score of 0.453 on the challenging task of exact break prediction over\nbook-length documents. Finally, we reveal interesting historical trends in the\nchapter structure of novels.", "published": "2020-11-09 02:56:54", "link": "http://arxiv.org/abs/2011.04163v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient End-to-End Speech Recognition Using Performers in Conformers", "abstract": "On-device end-to-end speech recognition poses a high requirement on model\nefficiency. Most prior works improve the efficiency by reducing model sizes. We\npropose to reduce the complexity of model architectures in addition to model\nsizes. More specifically, we reduce the floating-point operations in conformer\nby replacing the transformer module with a performer. The proposed\nattention-based efficient end-to-end speech recognition model yields\ncompetitive performance on the LibriSpeech corpus with 10 millions of\nparameters and linear computation complexity. The proposed model also\noutperforms previous lightweight end-to-end models by about 20% relatively in\nword error rate.", "published": "2020-11-09 05:22:57", "link": "http://arxiv.org/abs/2011.04196v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pointing to Subwords for Generating Function Names in Source Code", "abstract": "We tackle the task of automatically generating a function name from source\ncode. Existing generators face difficulties in generating low-frequency or\nout-of-vocabulary subwords. In this paper, we propose two strategies for\ncopying low-frequency or out-of-vocabulary subwords in inputs. Our best\nperforming model showed an improvement over the conventional method in terms of\nour modified F1 and accuracy on the Java-small and Java-large datasets.", "published": "2020-11-09 08:17:17", "link": "http://arxiv.org/abs/2011.04241v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BERT-JAM: Boosting BERT-Enhanced Neural Machine Translation with Joint\n  Attention", "abstract": "BERT-enhanced neural machine translation (NMT) aims at leveraging\nBERT-encoded representations for translation tasks. A recently proposed\napproach uses attention mechanisms to fuse Transformer's encoder and decoder\nlayers with BERT's last-layer representation and shows enhanced performance.\nHowever, their method doesn't allow for the flexible distribution of attention\nbetween the BERT representation and the encoder/decoder representation. In this\nwork, we propose a novel BERT-enhanced NMT model called BERT-JAM which improves\nupon existing models from two aspects: 1) BERT-JAM uses joint-attention modules\nto allow the encoder/decoder layers to dynamically allocate attention between\ndifferent representations, and 2) BERT-JAM allows the encoder/decoder layers to\nmake use of BERT's intermediate representations by composing them using a gated\nlinear unit (GLU). We train BERT-JAM with a novel three-phase optimization\nstrategy that progressively unfreezes different components of BERT-JAM. Our\nexperiments show that BERT-JAM achieves SOTA BLEU scores on multiple\ntranslation tasks.", "published": "2020-11-09 09:30:37", "link": "http://arxiv.org/abs/2011.04266v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Character-level Representations Improve DRS-based Semantic Parsing Even\n  in the Age of BERT", "abstract": "We combine character-level and contextual language model representations to\nimprove performance on Discourse Representation Structure parsing. Character\nrepresentations can easily be added in a sequence-to-sequence model in either\none encoder or as a fully separate encoder, with improvements that are robust\nto different language models, languages and data sets. For English, these\nimprovements are larger than adding individual sources of linguistic\ninformation or adding non-contextual embeddings. A new method of analysis based\non semantic tags demonstrates that the character-level representations improve\nperformance across a subset of selected semantic phenomena.", "published": "2020-11-09 10:24:12", "link": "http://arxiv.org/abs/2011.04308v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Low-Resource Adaptation of Neural NLP Models", "abstract": "Real-world applications of natural language processing (NLP) are challenging.\nNLP models rely heavily on supervised machine learning and require large\namounts of annotated data. These resources are often based on language data\navailable in large quantities, such as English newswire. However, in real-world\napplications of NLP, the textual resources vary across several dimensions, such\nas language, dialect, topic, and genre. It is challenging to find annotated\ndata of sufficient amount and quality. The objective of this thesis is to\ninvestigate methods for dealing with such low-resource scenarios in information\nextraction and natural language understanding. To this end, we study distant\nsupervision and sequential transfer learning in various low-resource settings.\nWe develop and adapt neural NLP models to explore a number of research\nquestions concerning NLP tasks with minimal or no training data.", "published": "2020-11-09 12:13:55", "link": "http://arxiv.org/abs/2011.04372v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Positional Artefacts Propagate Through Masked Language Model Embeddings", "abstract": "In this work, we demonstrate that the contextualized word vectors derived\nfrom pretrained masked language model-based encoders share a common, perhaps\nundesirable pattern across layers. Namely, we find cases of persistent outlier\nneurons within BERT and RoBERTa's hidden state vectors that consistently bear\nthe smallest or largest values in said vectors. In an attempt to investigate\nthe source of this information, we introduce a neuron-level analysis method,\nwhich reveals that the outliers are closely related to information captured by\npositional embeddings. We also pre-train the RoBERTa-base models from scratch\nand find that the outliers disappear without using positional embeddings. These\noutliers, we find, are the major cause of anisotropy of encoders' raw vector\nspaces, and clipping them leads to increased similarity across vectors. We\ndemonstrate this in practice by showing that clipped vectors can more\naccurately distinguish word senses, as well as lead to better sentence\nembeddings when mean pooling. In three supervised tasks, we find that clipping\ndoes not affect the performance.", "published": "2020-11-09 12:49:39", "link": "http://arxiv.org/abs/2011.04393v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Synonym Knowledge Enhanced Reader for Chinese Idiom Reading\n  Comprehension", "abstract": "Machine reading comprehension (MRC) is the task that asks a machine to answer\nquestions based on a given context. For Chinese MRC, due to the non-literal and\nnon-compositional semantic characteristics, Chinese idioms pose unique\nchallenges for machines to understand. Previous studies tend to treat idioms\nseparately without fully exploiting the relationship among them. In this paper,\nwe first define the concept of literal meaning coverage to measure the\nconsistency between semantics and literal meanings for Chinese idioms. With the\ndefinition, we prove that the literal meanings of many idioms are far from\ntheir semantics, and we also verify that the synonymic relationship can\nmitigate this inconsistency, which would be beneficial for idiom comprehension.\nFurthermore, to fully utilize the synonymic relationship, we propose the\nsynonym knowledge enhanced reader. Specifically, for each idiom, we first\nconstruct a synonym graph according to the annotations from a high-quality\nsynonym dictionary or the cosine similarity between the pre-trained idiom\nembeddings and then incorporate the graph attention network and gate mechanism\nto encode the graph. Experimental results on ChID, a large-scale Chinese idiom\nreading comprehension dataset, show that our model achieves state-of-the-art\nperformance.", "published": "2020-11-09 15:28:53", "link": "http://arxiv.org/abs/2011.04499v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "VisBERT: Hidden-State Visualizations for Transformers", "abstract": "Explainability and interpretability are two important concepts, the absence\nof which can and should impede the application of well-performing neural\nnetworks to real-world problems. At the same time, they are difficult to\nincorporate into the large, black-box models that achieve state-of-the-art\nresults in a multitude of NLP tasks. Bidirectional Encoder Representations from\nTransformers (BERT) is one such black-box model. It has become a staple\narchitecture to solve many different NLP tasks and has inspired a number of\nrelated Transformer models. Understanding how these models draw conclusions is\ncrucial for both their improvement and application. We contribute to this\nchallenge by presenting VisBERT, a tool for visualizing the contextual token\nrepresentations within BERT for the task of (multi-hop) Question Answering.\nInstead of analyzing attention weights, we focus on the hidden states resulting\nfrom each encoder block within the BERT model. This way we can observe how the\nsemantic representations are transformed throughout the layers of the model.\nVisBERT enables users to get insights about the model's internal state and to\nexplore its inference steps or potential shortcomings. The tool allows us to\nidentify distinct phases in BERT's transformations that are similar to a\ntraditional NLP pipeline and offer insights during failed predictions.", "published": "2020-11-09 15:37:43", "link": "http://arxiv.org/abs/2011.04507v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Action State Update Approach to Dialogue Management", "abstract": "Utterance interpretation is one of the main functions of a dialogue manager,\nwhich is the key component of a dialogue system. We propose the action state\nupdate approach (ASU) for utterance interpretation, featuring a statistically\ntrained binary classifier used to detect dialogue state update actions in the\ntext of a user utterance. Our goal is to interpret referring expressions in\nuser input without a domain-specific natural language understanding component.\nFor training the model, we use active learning to automatically select\nsimulated training examples. With both user-simulated and interactive human\nevaluations, we show that the ASU approach successfully interprets user\nutterances in a dialogue system, including those with referring expressions.", "published": "2020-11-09 18:49:41", "link": "http://arxiv.org/abs/2011.04637v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CLAR: A Cross-Lingual Argument Regularizer for Semantic Role Labeling", "abstract": "Semantic role labeling (SRL) identifies predicate-argument structure(s) in a\ngiven sentence. Although different languages have different argument\nannotations, polyglot training, the idea of training one model on multiple\nlanguages, has previously been shown to outperform monolingual baselines,\nespecially for low resource languages. In fact, even a simple combination of\ndata has been shown to be effective with polyglot training by representing the\ndistant vocabularies in a shared representation space. Meanwhile, despite the\ndissimilarity in argument annotations between languages, certain argument\nlabels do share common semantic meaning across languages (e.g. adjuncts have\nmore or less similar semantic meaning across languages). To leverage such\nsimilarity in annotation space across languages, we propose a method called\nCross-Lingual Argument Regularizer (CLAR). CLAR identifies such linguistic\nannotation similarity across languages and exploits this information to map the\ntarget language arguments using a transformation of the space on which source\nlanguage arguments lie. By doing so, our experimental results show that CLAR\nconsistently improves SRL performance on multiple languages over monolingual\nand polyglot baselines for low resource languages.", "published": "2020-11-09 20:16:57", "link": "http://arxiv.org/abs/2011.04732v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EstBERT: A Pretrained Language-Specific BERT for Estonian", "abstract": "This paper presents EstBERT, a large pretrained transformer-based\nlanguage-specific BERT model for Estonian. Recent work has evaluated\nmultilingual BERT models on Estonian tasks and found them to outperform the\nbaselines. Still, based on existing studies on other languages, a\nlanguage-specific BERT model is expected to improve over the multilingual ones.\nWe first describe the EstBERT pretraining process and then present the results\nof the models based on finetuned EstBERT for multiple NLP tasks, including POS\nand morphological tagging, named entity recognition and text classification.\nThe evaluation results show that the models based on EstBERT outperform\nmultilingual BERT models on five tasks out of six, providing further evidence\ntowards a view that training language-specific BERT models are still useful,\neven when multilingual models are available.", "published": "2020-11-09 21:33:53", "link": "http://arxiv.org/abs/2011.04784v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AI Stories: An Interactive Narrative System for Children", "abstract": "AI Stories is a proposed interactive dialogue system, that lets children\nco-create narrative worlds through conversation. Over the next three years this\nsystem will be developed and tested within pediatric wards, where it offers a\nuseful resource between the gap of education and play. Telling and making\nstories is a fundamental part of language play, and its chatty and nonsensical\nqualities are important; therefore, the prologued usage an automated system\noffers is a benefit to children. In this paper I will present the current state\nof this project, in its more experimental and general guise. Conceptually\nstory-telling through dialogue relates to the preprint interpretation of story,\nbeyond the static and linear medium, where stories were performative, temporal,\nand social.", "published": "2020-11-09 08:17:22", "link": "http://arxiv.org/abs/2011.04242v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CapWAP: Captioning with a Purpose", "abstract": "The traditional image captioning task uses generic reference captions to\nprovide textual information about images. Different user populations, however,\nwill care about different visual aspects of images. In this paper, we propose a\nnew task, Captioning with a Purpose (CapWAP). Our goal is to develop systems\nthat can be tailored to be useful for the information needs of an intended\npopulation, rather than merely provide generic information about an image. In\nthis task, we use question-answer (QA) pairs---a natural expression of\ninformation need---from users, instead of reference captions, for both training\nand post-inference evaluation. We show that it is possible to use reinforcement\nlearning to directly optimize for the intended information need, by rewarding\noutputs that allow a question answering model to provide correct answers to\nsampled user questions. We convert several visual question answering datasets\ninto CapWAP datasets, and demonstrate that under a variety of scenarios our\npurposeful captioning system learns to anticipate and fulfill specific\ninformation needs better than its generic counterparts, as measured by QA\nperformance on user questions from unseen images, when using the caption alone\nas context.", "published": "2020-11-09 09:23:55", "link": "http://arxiv.org/abs/2011.04264v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Bangla Text Classification using Transformers", "abstract": "Text classification has been one of the earliest problems in NLP. Over time\nthe scope of application areas has broadened and the difficulty of dealing with\nnew areas (e.g., noisy social media content) has increased. The problem-solving\nstrategy switched from classical machine learning to deep learning algorithms.\nOne of the recent deep neural network architecture is the Transformer. Models\ndesigned with this type of network and its variants recently showed their\nsuccess in many downstream natural language processing tasks, especially for\nresource-rich languages, e.g., English. However, these models have not been\nexplored fully for Bangla text classification tasks. In this work, we fine-tune\nmultilingual transformer models for Bangla text classification tasks in\ndifferent domains, including sentiment analysis, emotion detection, news\ncategorization, and authorship attribution. We obtain the state of the art\nresults on six benchmark datasets, improving upon the previous results by 5-29%\naccuracy across different tasks.", "published": "2020-11-09 14:12:07", "link": "http://arxiv.org/abs/2011.04446v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Automated Discovery of Mathematical Definitions in Text with Deep Neural\n  Networks", "abstract": "Automatic definition extraction from texts is an important task that has\nnumerous applications in several natural language processing fields such as\nsummarization, analysis of scientific texts, automatic taxonomy generation,\nontology generation, concept identification, and question answering. For\ndefinitions that are contained within a single sentence, this problem can be\nviewed as a binary classification of sentences into definitions and\nnon-definitions. In this paper, we focus on automatic detection of one-sentence\ndefinitions in mathematical texts, which are difficult to separate from\nsurrounding text. We experiment with several data representations, which\ninclude sentence syntactic structure and word embeddings, and apply deep\nlearning methods such as the Convolutional Neural Network (CNN) and the Long\nShort-Term Memory network (LSTM), in order to identify mathematical\ndefinitions. Our experiments demonstrate the superiority of CNN and its\ncombination with LSTM, when applied on the syntactically-enriched input\nrepresentation. We also present a new dataset for definition extraction from\nmathematical texts. We demonstrate that this dataset is beneficial for training\nsupervised models aimed at extraction of mathematical definitions. Our\nexperiments with different domains demonstrate that mathematical definitions\nrequire special treatment, and that using cross-domain learning is inefficient\nfor that task.", "published": "2020-11-09 15:57:53", "link": "http://arxiv.org/abs/2011.04521v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Refer, Reuse, Reduce: Generating Subsequent References in Visual and\n  Conversational Contexts", "abstract": "Dialogue participants often refer to entities or situations repeatedly within\na conversation, which contributes to its cohesiveness. Subsequent references\nexploit the common ground accumulated by the interlocutors and hence have\nseveral interesting properties, namely, they tend to be shorter and reuse\nexpressions that were effective in previous mentions. In this paper, we tackle\nthe generation of first and subsequent references in visually grounded\ndialogue. We propose a generation model that produces referring utterances\ngrounded in both the visual and the conversational context. To assess the\nreferring effectiveness of its output, we also implement a reference resolution\nsystem. Our experiments and analyses show that the model produces better, more\neffective referring utterances than a model not grounded in the dialogue\ncontext, and generates subsequent references that exhibit linguistic patterns\nakin to humans.", "published": "2020-11-09 16:53:54", "link": "http://arxiv.org/abs/2011.04554v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Generating Image Descriptions via Sequential Cross-Modal Alignment\n  Guided by Human Gaze", "abstract": "When speakers describe an image, they tend to look at objects before\nmentioning them. In this paper, we investigate such sequential cross-modal\nalignment by modelling the image description generation process\ncomputationally. We take as our starting point a state-of-the-art image\ncaptioning system and develop several model variants that exploit information\nfrom human gaze patterns recorded during language production. In particular, we\npropose the first approach to image description generation where visual\nprocessing is modelled $\\textit{sequentially}$. Our experiments and analyses\nconfirm that better descriptions can be obtained by exploiting gaze-driven\nattention and shed light on human cognitive processes by comparing different\nways of aligning the gaze modality with language production. We find that\nprocessing gaze data sequentially leads to descriptions that are better aligned\nto those produced by speakers, more diverse, and more natural${-}$particularly\nwhen gaze is encoded with a dedicated recurrent component.", "published": "2020-11-09 17:45:32", "link": "http://arxiv.org/abs/2011.04592v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Scaling Hidden Markov Language Models", "abstract": "The hidden Markov model (HMM) is a fundamental tool for sequence modeling\nthat cleanly separates the hidden state from the emission structure. However,\nthis separation makes it difficult to fit HMMs to large datasets in modern NLP,\nand they have fallen out of use due to very poor performance compared to fully\nobserved models. This work revisits the challenge of scaling HMMs to language\nmodeling datasets, taking ideas from recent approaches to neural modeling. We\npropose methods for scaling HMMs to massive state spaces while maintaining\nefficient exact inference, a compact parameterization, and effective\nregularization. Experiments show that this approach leads to models that are\nmore accurate than previous HMM and n-gram-based methods, making progress\ntowards the performance of state-of-the-art neural models.", "published": "2020-11-09 18:51:55", "link": "http://arxiv.org/abs/2011.04640v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Adversarial Semantic Collisions", "abstract": "We study semantic collisions: texts that are semantically unrelated but\njudged as similar by NLP models. We develop gradient-based approaches for\ngenerating semantic collisions and demonstrate that state-of-the-art models for\nmany tasks which rely on analyzing the meaning and similarity of texts--\nincluding paraphrase identification, document retrieval, response suggestion,\nand extractive summarization-- are vulnerable to semantic collisions. For\nexample, given a target query, inserting a crafted collision into an irrelevant\ndocument can shift its retrieval rank from 1000 to top 3. We show how to\ngenerate semantic collisions that evade perplexity-based filtering and discuss\nother potential mitigations. Our code is available at\nhttps://github.com/csong27/collision-bert.", "published": "2020-11-09 20:42:01", "link": "http://arxiv.org/abs/2011.04743v1", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Language Through a Prism: A Spectral Approach for Multiscale Language\n  Representations", "abstract": "Language exhibits structure at different scales, ranging from subwords to\nwords, sentences, paragraphs, and documents. To what extent do deep models\ncapture information at these scales, and can we force them to better capture\nstructure across this hierarchy? We approach this question by focusing on\nindividual neurons, analyzing the behavior of their activations at different\ntimescales. We show that signal processing provides a natural framework for\nseparating structure across scales, enabling us to 1) disentangle\nscale-specific information in existing embeddings and 2) train models to learn\nmore about particular scales. Concretely, we apply spectral filters to the\nactivations of a neuron across an input, producing filtered embeddings that\nperform well on part of speech tagging (word-level), dialog speech acts\nclassification (utterance-level), or topic classification (document-level),\nwhile performing poorly on the other tasks. We also present a prism layer for\ntraining models, which uses spectral filters to constrain different neurons to\nmodel structure at different scales. Our proposed BERT + Prism model can better\npredict masked tokens using long-range context and produces multiscale\nrepresentations that perform better at utterance- and document-level tasks. Our\nmethods are general and readily applicable to other domains besides language,\nsuch as images, audio, and video.", "published": "2020-11-09 23:17:43", "link": "http://arxiv.org/abs/2011.04823v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Text Classification through Glyph-aware Disentangled Character Embedding\n  and Semantic Sub-character Augmentation", "abstract": "We propose a new character-based text classification framework for\nnon-alphabetic languages, such as Chinese and Japanese. Our framework consists\nof a variational character encoder (VCE) and character-level text classifier.\nThe VCE is composed of a $\\beta$-variational auto-encoder ($\\beta$-VAE) that\nlearns the proposed glyph-aware disentangled character embedding (GDCE). Since\nour GDCE provides zero-mean unit-variance character embeddings that are\ndimensionally independent, it is applicable for our interpretable data\naugmentation, namely, semantic sub-character augmentation (SSA). In this paper,\nwe evaluated our framework using Japanese text classification tasks at the\ndocument- and sentence-level. We confirmed that our GDCE and SSA not only\nprovided embedding interpretability but also improved the classification\nperformance. Our proposal achieved a competitive result to the state-of-the-art\nmodel while also providing model interpretability. Our code is available on\nhttps://github.com/IyatomiLab/GDCE-SSA", "published": "2020-11-09 04:38:02", "link": "http://arxiv.org/abs/2011.04184v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Gated Recurrent Fusion with Joint Training Framework for Robust\n  End-to-End Speech Recognition", "abstract": "The joint training framework for speech enhancement and recognition methods\nhave obtained quite good performances for robust end-to-end automatic speech\nrecognition (ASR). However, these methods only utilize the enhanced feature as\nthe input of the speech recognition component, which are affected by the speech\ndistortion problem. In order to address this problem, this paper proposes a\ngated recurrent fusion (GRF) method with joint training framework for robust\nend-to-end ASR. The GRF algorithm is used to dynamically combine the noisy and\nenhanced features. Therefore, the GRF can not only remove the noise signals\nfrom the enhanced features, but also learn the raw fine structures from the\nnoisy features so that it can alleviate the speech distortion. The proposed\nmethod consists of speech enhancement, GRF and speech recognition. Firstly, the\nmask based speech enhancement network is applied to enhance the input speech.\nSecondly, the GRF is applied to address the speech distortion problem. Thirdly,\nto improve the performance of ASR, the state-of-the-art speech transformer\nalgorithm is used as the speech recognition component. Finally, the joint\ntraining framework is utilized to optimize these three components,\nsimultaneously. Our experiments are conducted on an open-source Mandarin speech\ncorpus called AISHELL-1. Experimental results show that the proposed method\nachieves the relative character error rate (CER) reduction of 10.04\\% over the\nconventional joint enhancement and transformer method only using the enhanced\nfeatures. Especially for the low signal-to-noise ratio (0 dB), our proposed\nmethod can achieves better performances with 12.67\\% CER reduction, which\nsuggests the potential of our proposed method.", "published": "2020-11-09 08:52:05", "link": "http://arxiv.org/abs/2011.04249v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Knowledge Distillation for Singing Voice Detection", "abstract": "Singing Voice Detection (SVD) has been an active area of research in music\ninformation retrieval (MIR). Currently, two deep neural network-based methods,\none based on CNN and the other on RNN, exist in literature that learn optimized\nfeatures for the voice detection (VD) task and achieve state-of-the-art\nperformance on common datasets. Both these models have a huge number of\nparameters (1.4M for CNN and 65.7K for RNN) and hence not suitable for\ndeployment on devices like smartphones or embedded sensors with limited\ncapacity in terms of memory and computation power. The most popular method to\naddress this issue is known as knowledge distillation in deep learning\nliterature (in addition to model compression) where a large pre-trained network\nknown as the teacher is used to train a smaller student network. Given the wide\napplications of SVD in music information retrieval, to the best of our\nknowledge, model compression for practical deployment has not yet been\nexplored. In this paper, efforts have been made to investigate this issue using\nboth conventional as well as ensemble knowledge distillation techniques.", "published": "2020-11-09 10:14:37", "link": "http://arxiv.org/abs/2011.04297v2", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Masked Proxy Loss For Text-Independent Speaker Verification", "abstract": "Open-set speaker recognition can be regarded as a metric learning problem,\nwhich is to maximize inter-class variance and minimize intra-class variance.\nSupervised metric learning can be categorized into entity-based learning and\nproxy-based learning. Most of the existing metric learning objectives like\nContrastive, Triplet, Prototypical, GE2E, etc all belong to the former\ndivision, the performance of which is either highly dependent on sample mining\nstrategy or restricted by insufficient label information in the mini-batch.\nProxy-based losses mitigate both shortcomings, however, fine-grained\nconnections among entities are either not or indirectly leveraged. This paper\nproposes a Masked Proxy (MP) loss which directly incorporates both proxy-based\nrelationships and pair-based relationships. We further propose Multinomial\nMasked Proxy (MMP) loss to leverage the hardness of speaker pairs. These\nmethods have been applied to evaluate on VoxCeleb test set and reach\nstate-of-the-art Equal Error Rate(EER).", "published": "2020-11-09 15:16:29", "link": "http://arxiv.org/abs/2011.04491v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Artificial Intelligence Decision Support for Medical Triage", "abstract": "Applying state-of-the-art machine learning and natural language processing on\napproximately one million of teleconsultation records, we developed a triage\nsystem, now certified and in use at the largest European telemedicine provider.\nThe system evaluates care alternatives through interactions with patients via a\nmobile application. Reasoning on an initial set of provided symptoms, the\ntriage application generates AI-powered, personalized questions to better\ncharacterize the problem and recommends the most appropriate point of care and\ntime frame for a consultation. The underlying technology was developed to meet\nthe needs for performance, transparency, user acceptance and ease of use,\ncentral aspects to the adoption of AI-based decision support systems. Providing\nsuch remote guidance at the beginning of the chain of care has significant\npotential for improving cost efficiency, patient experience and outcomes. Being\nremote, always available and highly scalable, this service is fundamental in\nhigh demand situations, such as the current COVID-19 outbreak.", "published": "2020-11-09 16:45:01", "link": "http://arxiv.org/abs/2011.04548v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Speaker De-identification System using Autoencoders and Adversarial\n  Training", "abstract": "The fast increase of web services and mobile apps, which collect personal\ndata from users, increases the risk that their privacy may be severely\ncompromised. In particular, the increasing variety of spoken language\ninterfaces and voice assistants empowered by the vertiginous breakthroughs in\nDeep Learning are prompting important concerns in the European Union to\npreserve speech data privacy. For instance, an attacker can record speech from\nusers and impersonate them to get access to systems requiring voice\nidentification. Hacking speaker profiles from users is also possible by means\nof existing technology to extract speaker, linguistic (e.g., dialect) and\nparalinguistic features (e.g., age) from the speech signal. In order to\nmitigate these weaknesses, in this paper, we propose a speaker\nde-identification system based on adversarial training and autoencoders in\norder to suppress speaker, gender, and accent information from speech.\nExperimental results show that combining adversarial learning and autoencoders\nincrease the equal error rate of a speaker verification system while preserving\nthe intelligibility of the anonymized spoken content.", "published": "2020-11-09 19:22:05", "link": "http://arxiv.org/abs/2011.04696v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Personalized Query Rewriting in Conversational AI Agents", "abstract": "Spoken language understanding (SLU) systems in conversational AI agents often\nexperience errors in the form of misrecognitions by automatic speech\nrecognition (ASR) or semantic gaps in natural language understanding (NLU).\nThese errors easily translate to user frustrations, particularly so in\nrecurrent events e.g. regularly toggling an appliance, calling a frequent\ncontact, etc. In this work, we propose a query rewriting approach by leveraging\nusers' historically successful interactions as a form of memory. We present a\nneural retrieval model and a pointer-generator network with hierarchical\nattention and show that they perform significantly better at the query\nrewriting task with the aforementioned user memories than without. We also\nhighlight how our approach with the proposed models leverages the structural\nand semantic diversity in ASR's output towards recovering users' intents.", "published": "2020-11-09 20:45:39", "link": "http://arxiv.org/abs/2011.04748v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "An Analysis of Dataset Overlap on Winograd-Style Tasks", "abstract": "The Winograd Schema Challenge (WSC) and variants inspired by it have become\nimportant benchmarks for common-sense reasoning (CSR). Model performance on the\nWSC has quickly progressed from chance-level to near-human using neural\nlanguage models trained on massive corpora. In this paper, we analyze the\neffects of varying degrees of overlap between these training corpora and the\ntest instances in WSC-style tasks. We find that a large number of test\ninstances overlap considerably with the corpora on which state-of-the-art\nmodels are (pre)trained, and that a significant drop in classification accuracy\noccurs when we evaluate models on instances with minimal overlap. Based on\nthese results, we develop the KnowRef-60K dataset, which consists of over 60k\npronoun disambiguation problems scraped from web data. KnowRef-60K is the\nlargest corpus to date for WSC-style common-sense reasoning and exhibits a\nsignificantly lower proportion of overlaps with current pretraining corpora.", "published": "2020-11-09 21:11:17", "link": "http://arxiv.org/abs/2011.04767v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Data Augmentation For Children's Speech Recognition -- The \"Ethiopian\"\n  System For The SLT 2021 Children Speech Recognition Challenge", "abstract": "This paper presents the \"Ethiopian\" system for the SLT 2021 Children Speech\nRecognition Challenge. Various data processing and augmentation techniques are\nproposed to tackle children's speech recognition problem, especially the lack\nof the children's speech recognition training data issue. Detailed experiments\nare designed and conducted to show the effectiveness of each technique, across\ndifferent speech recognition toolkits and model architectures. Step by step, we\nexplain how we come up with our final system, which provides the\nstate-of-the-art results in the SLT 2021 Children Speech Recognition Challenge,\nwith 21.66% CER on the Track 1 evaluation set (4th place overall), and 16.53%\nCER on the Track 2 evaluation set (1st place overall). Post-challenge analysis\nshows that our system actually achieves 18.82% CER on the Track 1 evaluation\nset, but we submitted the wrong version to the challenge organizer for Track 1.", "published": "2020-11-09 16:44:47", "link": "http://arxiv.org/abs/2011.04547v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Musical analysis of Stravinski's \"The Rite of Spring\" based on\n  computational methods", "abstract": "Stravinski's \"The Rite of Spring\" is one of the most well-known pieces from\nthe classical contemporary music repertoire. However, its analysis has aroused\ndifferent opinions within its construction and compositional foundations. In\nthis sense, I here proposed my own manual analysis and a computational approach\nwhich aims to find a similar analysis, giving the opportunity of discovering\nnew possible points of view and supplying the current deficiencies of the\nMusi\"c Computing common analysis systems.", "published": "2020-11-09 17:13:21", "link": "http://arxiv.org/abs/2011.04568v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "FRILL: A Non-Semantic Speech Embedding for Mobile Devices", "abstract": "Learned speech representations can drastically improve performance on tasks\nwith limited labeled data. However, due to their size and complexity, learned\nrepresentations have limited utility in mobile settings where run-time\nperformance can be a significant bottleneck. In this work, we propose a class\nof lightweight non-semantic speech embedding models that run efficiently on\nmobile devices based on the recently proposed TRILL speech embedding. We\ncombine novel architectural modifications with existing speed-up techniques to\ncreate embedding models that are fast enough to run in real-time on a mobile\ndevice and exhibit minimal performance degradation on a benchmark of\nnon-semantic speech tasks. One such model (FRILL) is 32x faster on a Pixel 1\nsmartphone and 40% the size of TRILL, with an average decrease in accuracy of\nonly 2%. To our knowledge, FRILL is the highest-quality non-semantic embedding\ndesigned for use on mobile devices. Furthermore, we demonstrate that these\nrepresentations are useful for mobile health tasks such as non-speech human\nsounds detection and face-masked speech detection. Our models and code are\npublicly available.", "published": "2020-11-09 18:07:06", "link": "http://arxiv.org/abs/2011.04609v5", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Benchmarking LF-MMI, CTC and RNN-T Criteria for Streaming ASR", "abstract": "In this work, to measure the accuracy and efficiency for a latency-controlled\nstreaming automatic speech recognition (ASR) application, we perform\ncomprehensive evaluations on three popular training criteria: LF-MMI, CTC and\nRNN-T. In transcribing social media videos of 7 languages with training data\n3K-14K hours, we conduct large-scale controlled experimentation across each\ncriterion using identical datasets and encoder model architecture. We find that\nRNN-T has consistent wins in ASR accuracy, while CTC models excel at inference\nefficiency. Moreover, we selectively examine various modeling strategies for\ndifferent training criteria, including modeling units, encoder architectures,\npre-training, etc. Given such large-scale real-world streaming ASR application,\nto our best knowledge, we present the first comprehensive benchmark on these\nthree widely used training criteria across a great many languages.", "published": "2020-11-09 21:34:38", "link": "http://arxiv.org/abs/2011.04785v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "STOI-Net: A Deep Learning based Non-Intrusive Speech Intelligibility\n  Assessment Model", "abstract": "The calculation of most objective speech intelligibility assessment metrics\nrequires clean speech as a reference. Such a requirement may limit the\napplicability of these metrics in real-world scenarios. To overcome this\nlimitation, we propose a deep learning-based non-intrusive speech\nintelligibility assessment model, namely STOI-Net. The input and output of\nSTOI-Net are speech spectral features and predicted STOI scores, respectively.\nThe model is formed by the combination of a convolutional neural network and\nbidirectional long short-term memory (CNN-BLSTM) architecture with a\nmultiplicative attention mechanism. Experimental results show that the STOI\nscore estimated by STOI-Net has a good correlation with the actual STOI score\nwhen tested with noisy and enhanced speech utterances. The correlation values\nare 0.97 and 0.83, respectively, for the seen test condition (the test speakers\nand noise types are involved in the training set) and the unseen test condition\n(the test speakers and noise types are not involved in the training set). The\nresults confirm the capability of STOI-Net to accurately predict the STOI\nscores without referring to clean speech.", "published": "2020-11-09 09:57:10", "link": "http://arxiv.org/abs/2011.04292v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "COVID-19 Patient Detection from Telephone Quality Speech Data", "abstract": "In this paper, we try to investigate the presence of cues about the COVID-19\ndisease in the speech data. We use an approach that is similar to speaker\nrecognition. Each sentence is represented as super vectors of short term Mel\nfilter bank features for each phoneme. These features are used to learn a\ntwo-class classifier to separate the COVID-19 speech from normal. Experiments\non a small dataset collected from YouTube videos show that an SVM classifier on\nthis dataset is able to achieve an accuracy of 88.6% and an F1-Score of 92.7%.\nFurther investigation reveals that some phone classes, such as nasals, stops,\nand mid vowels can distinguish the two classes better than the others.", "published": "2020-11-09 10:16:08", "link": "http://arxiv.org/abs/2011.04299v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Efficient Training Data Generation for Phase-Based DOA Estimation", "abstract": "Deep learning (DL) based direction of arrival (DOA) estimation is an active\nresearch topic and currently represents the state-of-the-art. Usually, DL-based\nDOA estimators are trained with recorded data or computationally expensive\ngenerated data. Both data types require significant storage and excessive time\nto, respectively, record or generate. We propose a low complexity online data\ngeneration method to train DL models with a phase-based feature input. The data\ngeneration method models the phases of the microphone signals in the frequency\ndomain by employing a deterministic model for the direct path and a statistical\nmodel for the late reverberation of the room transfer function. By an\nevaluation using data from measured room impulse responses, we demonstrate that\na model trained with the proposed training data generation method performs\ncomparably to models trained with data generated based on the source-image\nmethod.", "published": "2020-11-09 14:25:03", "link": "http://arxiv.org/abs/2011.04456v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Informed Source Extraction With Application to Acoustic Echo Reduction", "abstract": "Informed speaker extraction aims to extract a target speech signal from a\nmixture of sources given prior knowledge about the desired speaker. Recent deep\nlearning-based methods leverage a speaker discriminative model that maps a\nreference snippet uttered by the target speaker into a single embedding vector\nthat encapsulates the characteristics of the target speaker. However, such\nmodeling deliberately neglects the time-varying properties of the reference\nsignal. In this work, we assume that a reference signal is available that is\ntemporally correlated with the target signal. To take this correlation into\naccount, we propose a time-varying source discriminative model that captures\nthe temporal dynamics of the reference signal. We also show that existing\nmethods and the proposed method can be generalized to non-speech sources as\nwell. Experimental results demonstrate that the proposed method significantly\nimproves the extraction performance when applied in an acoustic echo reduction\nscenario.", "published": "2020-11-09 17:13:23", "link": "http://arxiv.org/abs/2011.04569v4", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "An Empirical Study of Visual Features for DNN based Audio-Visual Speech\n  Enhancement in Multi-talker Environments", "abstract": "Audio-visual speech enhancement (AVSE) methods use both audio and visual\nfeatures for the task of speech enhancement and the use of visual features has\nbeen shown to be particularly effective in multi-speaker scenarios. In the\nmajority of deep neural network (DNN) based AVSE methods, the audio and visual\ndata are first processed separately using different sub-networks, and then the\nlearned features are fused to utilize the information from both modalities.\nThere have been various studies on suitable audio input features and network\narchitectures, however, to the best of our knowledge, there is no published\nstudy that has investigated which visual features are best suited for this\nspecific task. In this work, we perform an empirical study of the most commonly\nused visual features for DNN based AVSE, the pre-processing requirements for\neach of these features, and investigate their influence on the performance. Our\nstudy shows that despite the overall better performance of embedding-based\nfeatures, their computationally intensive pre-processing make their use\ndifficult in low resource systems. For such systems, optical flow or raw\npixels-based features might be better suited.", "published": "2020-11-09 11:48:14", "link": "http://arxiv.org/abs/2011.04359v1", "categories": ["eess.AS", "cs.CV", "cs.LG", "cs.SD", "eess.IV"], "primary_category": "eess.AS"}
