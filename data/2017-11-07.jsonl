{"title": "Extractive Multi-document Summarization Using Multilayer Networks", "abstract": "Huge volumes of textual information has been produced every single day. In\norder to organize and understand such large datasets, in recent years,\nsummarization techniques have become popular. These techniques aims at finding\nrelevant, concise and non-redundant content from such a big data. While network\nmethods have been adopted to model texts in some scenarios, a systematic\nevaluation of multilayer network models in the multi-document summarization\ntask has been limited to a few studies. Here, we evaluate the performance of a\nmultilayer-based method to select the most relevant sentences in the context of\nan extractive multi document summarization (MDS) task. In the adopted model,\nnodes represent sentences and edges are created based on the number of shared\nwords between sentences. Differently from previous studies in multi-document\nsummarization, we make a distinction between edges linking sentences from\ndifferent documents (inter-layer) and those connecting sentences from the same\ndocument (intra-layer). As a proof of principle, our results reveal that such a\ndiscrimination between intra- and inter-layer in a multilayered representation\nis able to improve the quality of the generated summaries. This piece of\ninformation could be used to improve current statistical methods and related\ntextual models.", "published": "2017-11-07 17:01:55", "link": "http://arxiv.org/abs/1711.02608v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Non-Autoregressive Neural Machine Translation", "abstract": "Existing approaches to neural machine translation condition each output word\non previously generated outputs. We introduce a model that avoids this\nautoregressive property and produces its outputs in parallel, allowing an order\nof magnitude lower latency during inference. Through knowledge distillation,\nthe use of input token fertilities as a latent variable, and policy gradient\nfine-tuning, we achieve this at a cost of as little as 2.0 BLEU points relative\nto the autoregressive Transformer network used as a teacher. We demonstrate\nsubstantial cumulative improvements associated with each of the three aspects\nof our training strategy, and validate our approach on IWSLT 2016\nEnglish-German and two WMT language pairs. By sampling fertilities in parallel\nat inference time, our non-autoregressive model achieves near-state-of-the-art\nperformance of 29.8 BLEU on WMT 2016 English-Romanian.", "published": "2017-11-07 04:42:48", "link": "http://arxiv.org/abs/1711.02281v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unbounded cache model for online language modeling with open vocabulary", "abstract": "Recently, continuous cache models were proposed as extensions to recurrent\nneural network language models, to adapt their predictions to local changes in\nthe data distribution. These models only capture the local context, of up to a\nfew thousands tokens. In this paper, we propose an extension of continuous\ncache models, which can scale to larger contexts. In particular, we use a large\nscale non-parametric memory component that stores all the hidden activations\nseen in the past. We leverage recent advances in approximate nearest neighbor\nsearch and quantization algorithms to store millions of representations while\nsearching them efficiently. We conduct extensive experiments showing that our\napproach significantly improves the perplexity of pre-trained language models\non new distributions, and can scale efficiently to much larger contexts than\npreviously proposed local cache models.", "published": "2017-11-07 16:51:52", "link": "http://arxiv.org/abs/1711.02604v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Quality-Efficiency Trade-offs in Machine Learning for Text Processing", "abstract": "Data mining, machine learning, and natural language processing are powerful\ntechniques that can be used together to extract information from large texts.\nDepending on the task or problem at hand, there are many different approaches\nthat can be used. The methods available are continuously being optimized, but\nnot all these methods have been tested and compared in a set of problems that\ncan be solved using supervised machine learning algorithms. The question is\nwhat happens to the quality of the methods if we increase the training data\nsize from, say, 100 MB to over 1 GB? Moreover, are quality gains worth it when\nthe rate of data processing diminishes? Can we trade quality for time\nefficiency and recover the quality loss by just being able to process more\ndata? We attempt to answer these questions in a general way for text processing\ntasks, considering the trade-offs involving training data size, learning time,\nand quality obtained. We propose a performance trade-off framework and apply it\nto three important text processing problems: Named Entity Recognition,\nSentiment Analysis and Document Classification. These problems were also chosen\nbecause they have different levels of object granularity: words, paragraphs,\nand documents. For each problem, we selected several supervised machine\nlearning algorithms and we evaluated the trade-offs of them on large publicly\navailable data sets (news, reviews, patents). To explore these trade-offs, we\nuse different data subsets of increasing size ranging from 50 MB to several GB.\nWe also consider the impact of the data set and the evaluation technique. We\nfind that the results do not change significantly and that most of the time the\nbest algorithms is the fastest. However, we also show that the results for\nsmall data (say less than 100 MB) are different from the results for big data\nand in those cases the best algorithm is much harder to determine.", "published": "2017-11-07 05:43:34", "link": "http://arxiv.org/abs/1711.02295v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Non-uniform time-scaling of Carnatic music transients", "abstract": "Gamakas are an integral aspect of Carnatic Music, a form of classical music\nprevalent in South India. They are used in ragas, which may be seen as melodic\nscales and/or a set of characteristic melodic phrases. Gamakas exhibit\ncontinuous pitch variation often spanning several semitones. In this paper, we\nstudy how gamakas scale with tempo and propose a novel approach to change the\ntempo of Carnatic music pieces. The music signal is viewed as consisting of\nconstant-pitch segments and transients. The transients show continuous pitch\nvariation and we consider their analyses from a theoretical stand-point. We\nnext observe the non-uniform ratios of time-scaling of constant-pitch segments,\ntransients and silence in excerpts from nine concert renditions of varnams in\nsix ragas. The results indicate that the changing tempo of Carnatic music does\nnot change the duration of transients significantly. We report listening tests\non our algorithm to slow down Carnatic music that is consistent with this\nobservation.", "published": "2017-11-07 07:11:34", "link": "http://arxiv.org/abs/1711.02318v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "End-to-end learning for music audio tagging at scale", "abstract": "The lack of data tends to limit the outcomes of deep learning research,\nparticularly when dealing with end-to-end learning stacks processing raw data\nsuch as waveforms. In this study, 1.2M tracks annotated with musical labels are\navailable to train our end-to-end models. This large amount of data allows us\nto unrestrictedly explore two different design paradigms for music\nauto-tagging: assumption-free models - using waveforms as input with very small\nconvolutional filters; and models that rely on domain knowledge - log-mel\nspectrograms with a convolutional neural network designed to learn timbral and\ntemporal features. Our work focuses on studying how these two types of deep\narchitectures perform when datasets of variable size are available for\ntraining: the MagnaTagATune (25k songs), the Million Song Dataset (240k songs),\nand a private dataset of 1.2M songs. Our experiments suggest that music domain\nassumptions are relevant when not enough training data are available, thus\nshowing how waveform-based models outperform spectrogram-based ones in\nlarge-scale data scenarios.", "published": "2017-11-07 15:10:39", "link": "http://arxiv.org/abs/1711.02520v4", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The ACCompanion v0.1: An Expressive Accompaniment System", "abstract": "In this paper we present a preliminary version of the ACCompanion, an\nexpressive accompaniment system for MIDI input. The system uses a probabilistic\nmonophonic score follower to track the position of the soloist in the score,\nand a linear Gaussian model to compute tempo updates. The expressiveness of the\nsystem is powered by the Basis-Mixer, a state-of-the-art computational model of\nexpressive music performance. The system allows for expressive dynamics, timing\nand articulation.", "published": "2017-11-07 12:13:30", "link": "http://arxiv.org/abs/1711.02427v1", "categories": ["cs.SD", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
