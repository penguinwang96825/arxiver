{"title": "Evaluating Neuron Interpretation Methods of NLP Models", "abstract": "Neuron Interpretation has gained traction in the field of interpretability,\nand have provided fine-grained insights into what a model learns and how\nlanguage knowledge is distributed amongst its different components. However,\nthe lack of evaluation benchmark and metrics have led to siloed progress within\nthese various methods, with very little work comparing them and highlighting\ntheir strengths and weaknesses. The reason for this discrepancy is the\ndifficulty of creating ground truth datasets, for example, many neurons within\na given model may learn the same phenomena, and hence there may not be one\ncorrect answer. Moreover, a learned phenomenon may spread across several\nneurons that work together -- surfacing these to create a gold standard\nchallenging. In this work, we propose an evaluation framework that measures the\ncompatibility of a neuron analysis method with other methods. We hypothesize\nthat the more compatible a method is with the majority of the methods, the more\nconfident one can be about its performance. We systematically evaluate our\nproposed framework and present a comparative analysis of a large set of neuron\ninterpretation methods. We make the evaluation framework available to the\ncommunity. It enables the evaluation of any new method using 20 concepts and\nacross three pre-trained models.The code is released at\nhttps://github.com/fdalvi/neuron-comparative-analysis", "published": "2023-01-30 02:04:35", "link": "http://arxiv.org/abs/2301.12608v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring the Constructicon: Linguistic Analysis of a Computational CxG", "abstract": "Recent work has formulated the task for computational construction grammar as\nproducing a constructicon given a corpus of usage. Previous work has evaluated\nthese unsupervised grammars using both internal metrics (for example, Minimum\nDescription Length) and external metrics (for example, performance on a\ndialectology task). This paper instead takes a linguistic approach to\nevaluation, first learning a constructicon and then analyzing its contents from\na linguistic perspective. This analysis shows that a learned constructicon can\nbe divided into nine major types of constructions, of which Verbal and Nominal\nare the most common. The paper also shows that both the token and type\nfrequency of constructions can be used to model variation across registers and\ndialects.", "published": "2023-01-30 03:51:08", "link": "http://arxiv.org/abs/2301.12642v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "REPLUG: Retrieval-Augmented Black-Box Language Models", "abstract": "We introduce REPLUG, a retrieval-augmented language modeling framework that\ntreats the language model (LM) as a black box and augments it with a tuneable\nretrieval model. Unlike prior retrieval-augmented LMs that train language\nmodels with special cross attention mechanisms to encode the retrieved text,\nREPLUG simply prepends retrieved documents to the input for the frozen\nblack-box LM. This simple design can be easily applied to any existing\nretrieval and language models. Furthermore, we show that the LM can be used to\nsupervise the retrieval model, which can then find documents that help the LM\nmake better predictions. Our experiments demonstrate that REPLUG with the tuned\nretriever significantly improves the performance of GPT-3 (175B) on language\nmodeling by 6.3%, as well as the performance of Codex on five-shot MMLU by\n5.1%.", "published": "2023-01-30 04:18:09", "link": "http://arxiv.org/abs/2301.12652v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UzbekTagger: The rule-based POS tagger for Uzbek language", "abstract": "This research paper presents a part-of-speech (POS) annotated dataset and\ntagger tool for the low-resource Uzbek language. The dataset includes 12 tags,\nwhich were used to develop a rule-based POS-tagger tool. The corpus text used\nin the annotation process was made sure to be balanced over 20 different fields\nin order to ensure its representativeness. Uzbek being an agglutinative\nlanguage so the most of the words in an Uzbek sentence are formed by adding\nsuffixes. This nature of it makes the POS-tagging task difficult to find the\nstems of words and the right part-of-speech they belong to. The methodology\nproposed in this research is the stemming of the words with an affix/suffix\nstripping approach including database of the stem forms of the words in the\nUzbek language. The tagger tool was tested on the annotated dataset and showed\nhigh accuracy in identifying and tagging parts of speech in Uzbek text. This\nnewly presented dataset and tagger tool can be used for a variety of natural\nlanguage processing tasks such as language modeling, machine translation, and\ntext-to-speech synthesis. The presented dataset is the first of its kind to be\nmade publicly available for Uzbek, and the POS-tagger tool created can also be\nused as a pivot to use as a base for other closely-related Turkic languages.", "published": "2023-01-30 07:40:45", "link": "http://arxiv.org/abs/2301.12711v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-Tuning Deteriorates General Textual Out-of-Distribution Detection\n  by Distorting Task-Agnostic Features", "abstract": "Detecting out-of-distribution (OOD) inputs is crucial for the safe deployment\nof natural language processing (NLP) models. Though existing methods,\nespecially those based on the statistics in the feature space of fine-tuned\npre-trained language models (PLMs), are claimed to be effective, their\neffectiveness on different types of distribution shifts remains underexplored.\nIn this work, we take the first step to comprehensively evaluate the mainstream\ntextual OOD detection methods for detecting semantic and non-semantic shifts.\nWe find that: (1) no existing method behaves well in both settings; (2)\nfine-tuning PLMs on in-distribution data benefits detecting semantic shifts but\nseverely deteriorates detecting non-semantic shifts, which can be attributed to\nthe distortion of task-agnostic features. To alleviate the issue, we present a\nsimple yet effective general OOD score named GNOME that integrates the\nconfidence scores derived from the task-agnostic and task-specific\nrepresentations. Experiments show that GNOME works well in both semantic and\nnon-semantic shift scenarios, and further brings significant improvement on two\ncross-task benchmarks where both kinds of shifts simultaneously take place. Our\ncode is available at https://github.com/lancopku/GNOME.", "published": "2023-01-30 08:01:13", "link": "http://arxiv.org/abs/2301.12715v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Response-act Guided Reinforced Dialogue Generation for Mental Health\n  Counseling", "abstract": "Virtual Mental Health Assistants (VMHAs) have become a prevalent method for\nreceiving mental health counseling in the digital healthcare space. An\nassistive counseling conversation commences with natural open-ended topics to\nfamiliarize the client with the environment and later converges into more\nfine-grained domain-specific topics. Unlike other conversational systems, which\nare categorized as open-domain or task-oriented systems, VMHAs possess a hybrid\nconversational flow. These counseling bots need to comprehend various aspects\nof the conversation, such as dialogue-acts, intents, etc., to engage the client\nin an effective conversation. Although the surge in digital health research\nhighlights applications of many general-purpose response generation systems,\nthey are barely suitable in the mental health domain -- the prime reason is the\nlack of understanding in mental health counseling. Moreover, in general,\ndialogue-act guided response generators are either limited to a template-based\nparadigm or lack appropriate semantics. To this end, we propose READER -- a\nREsponse-Act guided reinforced Dialogue genERation model for the mental health\ncounseling conversations. READER is built on transformer to jointly predict a\npotential dialogue-act d(t+1) for the next utterance (aka response-act) and to\ngenerate an appropriate response u(t+1). Through the\ntransformer-reinforcement-learning (TRL) with Proximal Policy Optimization\n(PPO), we guide the response generator to abide by d(t+1) and ensure the\nsemantic richness of the responses via BERTScore in our reward computation. We\nevaluate READER on HOPE, a benchmark counseling conversation dataset and\nobserve that it outperforms several baselines across several evaluation metrics\n-- METEOR, ROUGE, and BERTScore. We also furnish extensive qualitative and\nquantitative analyses on results, including error analysis, human evaluation,\netc.", "published": "2023-01-30 08:53:35", "link": "http://arxiv.org/abs/2301.12729v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Far Can It Go?: On Intrinsic Gender Bias Mitigation for Text\n  Classification", "abstract": "To mitigate gender bias in contextualized language models, different\nintrinsic mitigation strategies have been proposed, alongside many bias\nmetrics. Considering that the end use of these language models is for\ndownstream tasks like text classification, it is important to understand how\nthese intrinsic bias mitigation strategies actually translate to fairness in\ndownstream tasks and the extent of this. In this work, we design a probe to\ninvestigate the effects that some of the major intrinsic gender bias mitigation\nstrategies have on downstream text classification tasks. We discover that\ninstead of resolving gender bias, intrinsic mitigation techniques and metrics\nare able to hide it in such a way that significant gender information is\nretained in the embeddings. Furthermore, we show that each mitigation technique\nis able to hide the bias from some of the intrinsic bias measures but not all,\nand each intrinsic bias measure can be fooled by some mitigation techniques,\nbut not all. We confirm experimentally, that none of the intrinsic mitigation\ntechniques used without any other fairness intervention is able to consistently\nimpact extrinsic bias. We recommend that intrinsic bias mitigation techniques\nshould be combined with other fairness interventions for downstream tasks.", "published": "2023-01-30 13:05:48", "link": "http://arxiv.org/abs/2301.12855v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Robustness of Prompt-based Semantic Parsing with Large Pre-trained\n  Language Model: An Empirical Study on Codex", "abstract": "Semantic parsing is a technique aimed at constructing a structured\nrepresentation of the meaning of a natural-language question. Recent\nadvancements in few-shot language models trained on code have demonstrated\nsuperior performance in generating these representations compared to\ntraditional unimodal language models, which are trained on downstream tasks.\nDespite these advancements, existing fine-tuned neural semantic parsers are\nsusceptible to adversarial attacks on natural-language inputs. While it has\nbeen established that the robustness of smaller semantic parsers can be\nenhanced through adversarial training, this approach is not feasible for large\nlanguage models in real-world scenarios, as it requires both substantial\ncomputational resources and expensive human annotation on in-domain semantic\nparsing data. This paper presents the first empirical study on the adversarial\nrobustness of a large prompt-based language model of code, \\codex. Our results\ndemonstrate that the state-of-the-art (SOTA) code-language models are\nvulnerable to carefully crafted adversarial examples. To address this\nchallenge, we propose methods for improving robustness without the need for\nsignificant amounts of labeled data or heavy computational resources.", "published": "2023-01-30 13:21:00", "link": "http://arxiv.org/abs/2301.12868v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Active Learning for Multilingual Semantic Parser", "abstract": "Current multilingual semantic parsing (MSP) datasets are almost all collected\nby translating the utterances in the existing datasets from the resource-rich\nlanguage to the target language. However, manual translation is costly. To\nreduce the translation effort, this paper proposes the first active learning\nprocedure for MSP (AL-MSP). AL-MSP selects only a subset from the existing\ndatasets to be translated. We also propose a novel selection method that\nprioritizes the examples diversifying the logical form structures with more\nlexical choices, and a novel hyperparameter tuning method that needs no extra\nannotation cost. Our experiments show that AL-MSP significantly reduces\ntranslation costs with ideal selection methods. Our selection method with\nproper hyperparameters yields better parsing performance than the other\nbaselines on two multilingual datasets.", "published": "2023-01-30 14:19:29", "link": "http://arxiv.org/abs/2301.12920v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using n-aksaras to model Sanskrit and Sanskrit-adjacent texts", "abstract": "Despite -- or perhaps because of -- their simplicity, n-grams, or contiguous\nsequences of tokens, have been used with great success in computational\nlinguistics since their introduction in the late 20th century. Recast as\nk-mers, or contiguous sequences of monomers, they have also found applications\nin computational biology. When applied to the analysis of texts, n-grams\nusually take the form of sequences of words. But if we try to apply this model\nto the analysis of Sanskrit texts, we are faced with the arduous task of,\nfirstly, resolving sandhi to split a phrase into words, and, secondly,\nsplitting long compounds into their components. This paper presents a simpler\nmethod of tokenizing a Sanskrit text for n-grams, by using n-aksaras, or\ncontiguous sequences of aksaras. This model reduces the need for sandhi\nresolution, making it much easier to use on raw text. It is also possible to\nuse this model on Sanskrit-adjacent texts, e.g., a Tamil commentary on a\nSanskrit text. As a test case, the commentaries on Amarakosa 1.0.1 have been\nmodelled as n-aksaras, showing patterns of text reuse across ten centuries and\nnine languages. Some initial observations are made concerning Buddhist\ncommentarial practices.", "published": "2023-01-30 15:17:06", "link": "http://arxiv.org/abs/2301.12969v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Representation biases in sentence transformers", "abstract": "Variants of the BERT architecture specialised for producing full-sentence\nrepresentations often achieve better performance on downstream tasks than\nsentence embeddings extracted from vanilla BERT. However, there is still little\nunderstanding of what properties of inputs determine the properties of such\nrepresentations. In this study, we construct several sets of sentences with\npre-defined lexical and syntactic structures and show that SOTA sentence\ntransformers have a strong nominal-participant-set bias: cosine similarities\nbetween pairs of sentences are more strongly determined by the overlap in the\nset of their noun participants than by having the same predicates, lengthy\nnominal modifiers, or adjuncts. At the same time, the precise\nsyntactic-thematic functions of the participants are largely irrelevant.", "published": "2023-01-30 16:35:23", "link": "http://arxiv.org/abs/2301.13039v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Secret of Metaphor on Expressing Stronger Emotion", "abstract": "Metaphors are proven to have stronger emotional impact than literal\nexpressions. Although this conclusion is shown to be promising in benefiting\nvarious NLP applications, the reasons behind this phenomenon are not well\nstudied. This paper conducts the first study in exploring how metaphors convey\nstronger emotion than their literal counterparts. We find that metaphors are\ngenerally more specific than literal expressions. The more specific property of\nmetaphor can be one of the reasons for metaphors' superiority in emotion\nexpression. When we compare metaphors with literal expressions with the same\nspecificity level, the gap of emotion expressing ability between both reduces\nsignificantly. In addition, we observe specificity is crucial in literal\nlanguage as well, as literal language can express stronger emotion by making it\nmore specific.", "published": "2023-01-30 16:36:02", "link": "http://arxiv.org/abs/2301.13042v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contextual Dynamic Prompting for Response Generation in Task-oriented\n  Dialog Systems", "abstract": "Response generation is one of the critical components in task-oriented dialog\nsystems. Existing studies have shown that large pre-trained language models can\nbe adapted to this task. The typical paradigm of adapting such extremely large\nlanguage models would be by fine-tuning on the downstream tasks which is not\nonly time-consuming but also involves significant resources and access to\nfine-tuning data. Prompting (Schick and Sch\\\"utze, 2020) has been an\nalternative to fine-tuning in many NLP tasks. In our work, we explore the idea\nof using prompting for response generation in task-oriented dialog systems.\nSpecifically, we propose an approach that performs contextual dynamic prompting\nwhere the prompts are learnt from dialog contexts. We aim to distill useful\nprompting signals from the dialog context. On experiments with MultiWOZ 2.2\ndataset (Zang et al., 2020), we show that contextual dynamic prompts improve\nresponse generation in terms of combined score (Mehri et al., 2019) by 3\nabsolute points, and a massive 20 points when dialog states are incorporated.\nFurthermore, human annotation on these conversations found that agents which\nincorporate context were preferred over agents with vanilla prefix-tuning.", "published": "2023-01-30 20:26:02", "link": "http://arxiv.org/abs/2301.13268v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adaptive Machine Translation with Large Language Models", "abstract": "Consistency is a key requirement of high-quality translation. It is\nespecially important to adhere to pre-approved terminology and adapt to\ncorrected translations in domain-specific projects. Machine translation (MT)\nhas achieved significant progress in the area of domain adaptation. However,\nreal-time adaptation remains challenging. Large-scale language models (LLMs)\nhave recently shown interesting capabilities of in-context learning, where they\nlearn to replicate certain input-output text generation patterns, without\nfurther fine-tuning. By feeding an LLM at inference time with a prompt that\nconsists of a list of translation pairs, it can then simulate the domain and\nstyle characteristics. This work aims to investigate how we can utilize\nin-context learning to improve real-time adaptive MT. Our extensive experiments\nshow promising results at translation time. For example, LLMs can adapt to a\nset of in-domain sentence pairs and/or terminology while translating a new\nsentence. We observe that the translation quality with few-shot in-context\nlearning can surpass that of strong encoder-decoder MT systems, especially for\nhigh-resource languages. Moreover, we investigate whether we can combine MT\nfrom strong encoder-decoder models with fuzzy matches, which can further\nimprove translation quality, especially for less supported languages. We\nconduct our experiments across five diverse language pairs, namely\nEnglish-to-Arabic (EN-AR), English-to-Chinese (EN-ZH), English-to-French\n(EN-FR), English-to-Kinyarwanda (EN-RW), and English-to-Spanish (EN-ES).", "published": "2023-01-30 21:17:15", "link": "http://arxiv.org/abs/2301.13294v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LongEval: Guidelines for Human Evaluation of Faithfulness in Long-form\n  Summarization", "abstract": "While human evaluation remains best practice for accurately judging the\nfaithfulness of automatically-generated summaries, few solutions exist to\naddress the increased difficulty and workload when evaluating long-form\nsummaries. Through a survey of 162 papers on long-form summarization, we first\nshed light on current human evaluation practices surrounding long-form\nsummaries. We find that 73% of these papers do not perform any human evaluation\non model-generated summaries, while other works face new difficulties that\nmanifest when dealing with long documents (e.g., low inter-annotator\nagreement). Motivated by our survey, we present LongEval, a set of guidelines\nfor human evaluation of faithfulness in long-form summaries that addresses the\nfollowing challenges: (1) How can we achieve high inter-annotator agreement on\nfaithfulness scores? (2) How can we minimize annotator workload while\nmaintaining accurate faithfulness scores? and (3) Do humans benefit from\nautomated alignment between summary and source snippets? We deploy LongEval in\nannotation studies on two long-form summarization datasets in different domains\n(SQuALITY and PubMed), and we find that switching to a finer granularity of\njudgment (e.g., clause-level) reduces inter-annotator variance in faithfulness\nscores (e.g., std-dev from 18.5 to 6.8). We also show that scores from a\npartial annotation of fine-grained units highly correlates with scores from a\nfull annotation workload (0.89 Kendall's tau using 50% judgments). We release\nour human judgments, annotation templates, and our software as a Python library\nfor future research.", "published": "2023-01-30 21:31:48", "link": "http://arxiv.org/abs/2301.13298v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Speak from Text: Zero-Shot Multilingual Text-to-Speech with\n  Unsupervised Text Pretraining", "abstract": "While neural text-to-speech (TTS) has achieved human-like natural synthetic\nspeech, multilingual TTS systems are limited to resource-rich languages due to\nthe need for paired text and studio-quality audio data. This paper proposes a\nmethod for zero-shot multilingual TTS using text-only data for the target\nlanguage. The use of text-only data allows the development of TTS systems for\nlow-resource languages for which only textual resources are available, making\nTTS accessible to thousands of languages. Inspired by the strong cross-lingual\ntransferability of multilingual language models, our framework first performs\nmasked language model pretraining with multilingual text-only data. Then we\ntrain this model with a paired data in a supervised manner, while freezing a\nlanguage-aware embedding layer. This allows inference even for languages not\nincluded in the paired data but present in the text-only data. Evaluation\nresults demonstrate highly intelligible zero-shot TTS with a character error\nrate of less than 12% for an unseen language.", "published": "2023-01-30 00:53:50", "link": "http://arxiv.org/abs/2301.12596v3", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Knowledge Distillation $\\approx$ Label Smoothing: Fact or Fallacy?", "abstract": "Originally proposed as a method for knowledge transfer from one model to\nanother, some recent studies have suggested that knowledge distillation (KD) is\nin fact a form of regularization. Perhaps the strongest argument of all for\nthis new perspective comes from its apparent similarities with label smoothing\n(LS). Here we re-examine this stated equivalence between the two methods by\ncomparing the predictive confidences of the models they train. Experiments on\nfour text classification tasks involving models of different sizes show that:\n(a) In most settings, KD and LS drive model confidence in completely opposite\ndirections, and (b) In KD, the student inherits not only its knowledge but also\nits confidence from the teacher, reinforcing the classical knowledge transfer\nview.", "published": "2023-01-30 02:05:24", "link": "http://arxiv.org/abs/2301.12609v4", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Zero-shot Clarifying Question Generation for Conversational Search", "abstract": "A long-standing challenge for search and conversational assistants is query\nintention detection in ambiguous queries. Asking clarifying questions in\nconversational search has been widely studied and considered an effective\nsolution to resolve query ambiguity. Existing work have explored various\napproaches for clarifying question ranking and generation. However, due to the\nlack of real conversational search data, they have to use artificial datasets\nfor training, which limits their generalizability to real-world search\nscenarios. As a result, the industry has shown reluctance to implement them in\nreality, further suspending the availability of real conversational search\ninteraction data. The above dilemma can be formulated as a cold start problem\nof clarifying question generation and conversational search in general.\nFurthermore, even if we do have large-scale conversational logs, it is not\nrealistic to gather training data that can comprehensively cover all possible\nqueries and topics in open-domain search scenarios. The risk of fitting bias\nwhen training a clarifying question retrieval/generation model on\nincomprehensive dataset is thus another important challenge.\n  In this work, we innovatively explore generating clarifying questions in a\nzero-shot setting to overcome the cold start problem and we propose a\nconstrained clarifying question generation system which uses both question\ntemplates and query facets to guide the effective and precise question\ngeneration. The experiment results show that our method outperforms existing\nstate-of-the-art zero-shot baselines by a large margin. Human annotations to\nour model outputs also indicate our method generates 25.2\\% more natural\nquestions, 18.1\\% more useful questions, 6.1\\% less unnatural and 4\\% less\nuseless questions.", "published": "2023-01-30 04:43:02", "link": "http://arxiv.org/abs/2301.12660v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "KG-BERTScore: Incorporating Knowledge Graph into BERTScore for\n  Reference-Free Machine Translation Evaluation", "abstract": "BERTScore is an effective and robust automatic metric for referencebased\nmachine translation evaluation. In this paper, we incorporate multilingual\nknowledge graph into BERTScore and propose a metric named KG-BERTScore, which\nlinearly combines the results of BERTScore and bilingual named entity matching\nfor reference-free machine translation evaluation. From the experimental\nresults on WMT19 QE as a metric without references shared tasks, our metric\nKG-BERTScore gets higher overall correlation with human judgements than the\ncurrent state-of-the-art metrics for reference-free machine translation\nevaluation.1 Moreover, the pre-trained multilingual model used by KG-BERTScore\nand the parameter for linear combination are also studied in this paper.", "published": "2023-01-30 07:12:15", "link": "http://arxiv.org/abs/2301.12699v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Crawling the Internal Knowledge-Base of Language Models", "abstract": "Language models are trained on large volumes of text, and as a result their\nparameters might contain a significant body of factual knowledge. Any\ndownstream task performed by these models implicitly builds on these facts, and\nthus it is highly desirable to have means for representing this body of\nknowledge in an interpretable way. However, there is currently no mechanism for\nsuch a representation. Here, we propose to address this goal by extracting a\nknowledge-graph of facts from a given language model. We describe a procedure\nfor ``crawling'' the internal knowledge-base of a language model. Specifically,\ngiven a seed entity, we expand a knowledge-graph around it. The crawling\nprocedure is decomposed into sub-tasks, realized through specially designed\nprompts that control for both precision (i.e., that no wrong facts are\ngenerated) and recall (i.e., the number of facts generated). We evaluate our\napproach on graphs crawled starting from dozens of seed entities, and show it\nyields high precision graphs (82-92%), while emitting a reasonable number of\nfacts per entity.", "published": "2023-01-30 12:03:36", "link": "http://arxiv.org/abs/2301.12810v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Finding the Law: Enhancing Statutory Article Retrieval via Graph Neural\n  Networks", "abstract": "Statutory article retrieval (SAR), the task of retrieving statute law\narticles relevant to a legal question, is a promising application of legal text\nprocessing. In particular, high-quality SAR systems can improve the work\nefficiency of legal professionals and provide basic legal assistance to\ncitizens in need at no cost. Unlike traditional ad-hoc information retrieval,\nwhere each document is considered a complete source of information, SAR deals\nwith texts whose full sense depends on complementary information from the\ntopological organization of statute law. While existing works ignore these\ndomain-specific dependencies, we propose a novel graph-augmented dense statute\nretriever (G-DSR) model that incorporates the structure of legislation via a\ngraph neural network to improve dense retrieval performance. Experimental\nresults show that our approach outperforms strong retrieval baselines on a\nreal-world expert-annotated SAR dataset.", "published": "2023-01-30 12:59:09", "link": "http://arxiv.org/abs/2301.12847v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "GE-Blender: Graph-Based Knowledge Enhancement for Blender", "abstract": "Although the great success of open-domain dialogue generation, unseen\nentities can have a large impact on the dialogue generation task. It leads to\nperformance degradation of the model in the dialog generation. Previous\nresearches used retrieved knowledge of seen entities as the auxiliary data to\nenhance the representation of the model. Nevertheless, logical explanation of\nunseen entities remains unexplored, such as possible co-occurrence or\nsemantically similar words of them and their entity category. In this work, we\npropose an approach to address the challenge above. We construct a graph by\nextracting entity nodes in them, enhancing the representation of the context of\nthe unseen entity with the entity's 1-hop surrounding nodes. Furthermore, We\nadded the named entity tag prediction task to apply the problem that the unseen\nentity does not exist in the graph. We conduct our experiments on an open\ndataset Wizard of Wikipedia and the empirical results indicate that our\napproach outperforms the state-of-the-art approaches on Wizard of Wikipedia.", "published": "2023-01-30 13:00:20", "link": "http://arxiv.org/abs/2301.12850v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "N-Gram Nearest Neighbor Machine Translation", "abstract": "Nearest neighbor machine translation augments the Autoregressive\nTranslation~(AT) with $k$-nearest-neighbor retrieval, by comparing the\nsimilarity between the token-level context representations of the target tokens\nin the query and the datastore. However, the token-level representation may\nintroduce noise when translating ambiguous words, or fail to provide accurate\nretrieval results when the representation generated by the model contains\nindistinguishable context information, e.g., Non-Autoregressive\nTranslation~(NAT) models. In this paper, we propose a novel $n$-gram nearest\nneighbor retrieval method that is model agnostic and applicable to both AT and\nNAT models. Specifically, we concatenate the adjacent $n$-gram hidden\nrepresentations as the key, while the tuple of corresponding target tokens is\nthe value. In inference, we propose tailored decoding algorithms for AT and NAT\nmodels respectively. We demonstrate that the proposed method consistently\noutperforms the token-level method on both AT and NAT models as well on general\nas on domain adaptation translation tasks. On domain adaptation, the proposed\nmethod brings $1.03$ and $2.76$ improvements regarding the average BLEU score\non AT and NAT models respectively.", "published": "2023-01-30 13:19:19", "link": "http://arxiv.org/abs/2301.12866v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Red teaming ChatGPT via Jailbreaking: Bias, Robustness, Reliability and\n  Toxicity", "abstract": "Recent breakthroughs in natural language processing (NLP) have permitted the\nsynthesis and comprehension of coherent text in an open-ended way, therefore\ntranslating the theoretical algorithms into practical applications. The large\nlanguage models (LLMs) have significantly impacted businesses such as report\nsummarization software and copywriters. Observations indicate, however, that\nLLMs may exhibit social prejudice and toxicity, posing ethical and societal\ndangers of consequences resulting from irresponsibility. Large-scale benchmarks\nfor accountable LLMs should consequently be developed. Although several\nempirical investigations reveal the existence of a few ethical difficulties in\nadvanced LLMs, there is little systematic examination and user study of the\nrisks and harmful behaviors of current LLM usage. To further educate future\nefforts on constructing ethical LLMs responsibly, we perform a qualitative\nresearch method called ``red teaming'' on OpenAI's ChatGPT\\footnote{In this\npaper, ChatGPT refers to the version released on Dec 15th.} to better\nunderstand the practical features of ethical dangers in recent LLMs. We analyze\nChatGPT comprehensively from four perspectives: 1) \\textit{Bias} 2)\n\\textit{Reliability} 3) \\textit{Robustness} 4) \\textit{Toxicity}. In accordance\nwith our stated viewpoints, we empirically benchmark ChatGPT on multiple sample\ndatasets. We find that a significant number of ethical risks cannot be\naddressed by existing benchmarks, and hence illustrate them via additional case\nstudies. In addition, we examine the implications of our findings on AI ethics\nand harmal behaviors of ChatGPT, as well as future problems and practical\ndesign considerations for responsible LLMs. We believe that our findings may\ngive light on future efforts to determine and mitigate the ethical hazards\nposed by machines in LLM applications.", "published": "2023-01-30 13:20:48", "link": "http://arxiv.org/abs/2301.12867v4", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Quantifying Context Mixing in Transformers", "abstract": "Self-attention weights and their transformed variants have been the main\nsource of information for analyzing token-to-token interactions in\nTransformer-based models. But despite their ease of interpretation, these\nweights are not faithful to the models' decisions as they are only one part of\nan encoder, and other components in the encoder layer can have considerable\nimpact on information mixing in the output representations. In this work, by\nexpanding the scope of analysis to the whole encoder block, we propose Value\nZeroing, a novel context mixing score customized for Transformers that provides\nus with a deeper understanding of how information is mixed at each encoder\nlayer. We demonstrate the superiority of our context mixing score over other\nanalysis methods through a series of complementary evaluations with different\nviewpoints based on linguistically informed rationales, probing, and\nfaithfulness analysis.", "published": "2023-01-30 15:19:02", "link": "http://arxiv.org/abs/2301.12971v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Human Word Association based model for topic detection in social\n  networks", "abstract": "With the widespread use of social networks, detecting the topics discussed on\nthese platforms has become a significant challenge. Current approaches\nprimarily rely on frequent pattern mining or semantic relations, often\nneglecting the structure of the language. Language structural methods aim to\ndiscover the relationships between words and how humans understand them.\nTherefore, this paper introduces a topic detection framework for social\nnetworks based on the concept of imitating the mental ability of word\nassociation. This framework employs the Human Word Association method and\nincludes a specially designed extraction algorithm. The performance of this\nmethod is evaluated using the FA-CUP dataset, a benchmark in the field of topic\ndetection. The results indicate that the proposed method significantly improves\ntopic detection compared to other methods, as evidenced by Topic-recall and the\nkeyword F1 measure. Additionally, to assess the applicability and\ngeneralizability of the proposed method, a dataset of Telegram posts in the\nPersian language is used. The results demonstrate that this method outperforms\nother topic detection methods.", "published": "2023-01-30 17:10:34", "link": "http://arxiv.org/abs/2301.13066v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Communication Drives the Emergence of Language Universals in Neural\n  Agents: Evidence from the Word-order/Case-marking Trade-off", "abstract": "Artificial learners often behave differently from human learners in the\ncontext of neural agent-based simulations of language emergence and change. A\ncommon explanation is the lack of appropriate cognitive biases in these\nlearners. However, it has also been proposed that more naturalistic settings of\nlanguage learning and use could lead to more human-like results. We investigate\nthis latter account focusing on the word-order/case-marking trade-off, a widely\nattested language universal that has proven particularly hard to simulate. We\npropose a new Neural-agent Language Learning and Communication framework\n(NeLLCom) where pairs of speaking and listening agents first learn a miniature\nlanguage via supervised learning, and then optimize it for communication via\nreinforcement learning. Following closely the setup of earlier human\nexperiments, we succeed in replicating the trade-off with the new framework\nwithout hard-coding specific biases in the agents. We see this as an essential\nstep towards the investigation of language universals with neural learners.", "published": "2023-01-30 17:22:33", "link": "http://arxiv.org/abs/2301.13083v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Alternating Updates for Efficient Transformers", "abstract": "It has been well established that increasing scale in deep transformer\nnetworks leads to improved quality and performance. However, this increase in\nscale often comes with prohibitive increases in compute cost and inference\nlatency. We introduce Alternating Updates (AltUp), a simple-to-implement method\nto increase a model's capacity without the computational burden. AltUp enables\nthe widening of the learned representation, i.e., the token embedding, while\nonly incurring a negligible increase in latency. AltUp achieves this by working\non a subblock of the widened representation at each layer and using a\npredict-and-correct mechanism to update the inactivated blocks. We present\nextensions of AltUp, such as its applicability to the sequence dimension, and\ndemonstrate how AltUp can be synergistically combined with existing approaches,\nsuch as Sparse Mixture-of-Experts models, to obtain efficient models with even\nhigher capacity. Our experiments on benchmark transformer models and language\ntasks demonstrate the consistent effectiveness of AltUp on a diverse set of\nscenarios. Notably, on SuperGLUE and SQuAD benchmarks, AltUp enables up to\n$87\\%$ speedup relative to the dense baselines at the same accuracy.", "published": "2023-01-30 22:06:05", "link": "http://arxiv.org/abs/2301.13310v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "ChatGPT or Human? Detect and Explain. Explaining Decisions of Machine\n  Learning Model for Detecting Short ChatGPT-generated Text", "abstract": "ChatGPT has the ability to generate grammatically flawless and\nseemingly-human replies to different types of questions from various domains.\nThe number of its users and of its applications is growing at an unprecedented\nrate. Unfortunately, use and abuse come hand in hand. In this paper, we study\nwhether a machine learning model can be effectively trained to accurately\ndistinguish between original human and seemingly human (that is,\nChatGPT-generated) text, especially when this text is short. Furthermore, we\nemploy an explainable artificial intelligence framework to gain insight into\nthe reasoning behind the model trained to differentiate between\nChatGPT-generated and human-generated text. The goal is to analyze model's\ndecisions and determine if any specific patterns or characteristics can be\nidentified. Our study focuses on short online reviews, conducting two\nexperiments comparing human-generated and ChatGPT-generated text. The first\nexperiment involves ChatGPT text generated from custom queries, while the\nsecond experiment involves text generated by rephrasing original\nhuman-generated reviews. We fine-tune a Transformer-based model and use it to\nmake predictions, which are then explained using SHAP. We compare our model\nwith a perplexity score-based approach and find that disambiguation between\nhuman and ChatGPT-generated reviews is more challenging for the ML model when\nusing rephrased text. However, our proposed approach still achieves an accuracy\nof 79%. Using explainability, we observe that ChatGPT's writing is polite,\nwithout specific details, using fancy and atypical vocabulary, impersonal, and\ntypically it does not express feelings.", "published": "2023-01-30 08:06:08", "link": "http://arxiv.org/abs/2301.13852v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Specializing Smaller Language Models towards Multi-Step Reasoning", "abstract": "The surprising ability of Large Language Models (LLMs) to perform well on\ncomplex reasoning with only few-shot chain-of-thought prompts is believed to\nemerge only in very large-scale models (100+ billion parameters). We show that\nsuch abilities can, in fact, be distilled down from GPT-3.5 ($\\ge$ 175B) to T5\nvariants ($\\le$ 11B). We propose model specialization, to specialize the\nmodel's ability towards a target task. The hypothesis is that large models\n(commonly viewed as larger than 100B) have strong modeling power, but are\nspread on a large spectrum of tasks. Small models (commonly viewed as smaller\nthan 10B) have limited model capacity, but if we concentrate their capacity on\na specific target task, the model can achieve a decent improved performance. We\nuse multi-step math reasoning as our testbed because it is a very typical\nemergent ability. We show two important aspects of model abilities: (1). there\nexists a very complex balance/ tradeoff between language models'\nmulti-dimensional abilities; (2). by paying the price of decreased generic\nability, we can clearly lift up the scaling curve of models smaller than 10B\ntowards a specialized multi-step math reasoning ability. We further give\ncomprehensive discussions about important design choices for better\ngeneralization, including the tuning data format, the start model checkpoint,\nand a new model selection method. We hope our practice and discoveries can\nserve as an important attempt towards specialized smaller models in the new\nresearch paradigm set by LLMs.", "published": "2023-01-30 08:51:19", "link": "http://arxiv.org/abs/2301.12726v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Knowledge Transfer from Pre-trained Language Models to Cif-based Speech\n  Recognizers via Hierarchical Distillation", "abstract": "Large-scale pre-trained language models (PLMs) have shown great potential in\nnatural language processing tasks. Leveraging the capabilities of PLMs to\nenhance automatic speech recognition (ASR) systems has also emerged as a\npromising research direction. However, previous works may be limited by the\ninflexible structures of PLMs and the insufficient utilization of PLMs. To\nalleviate these problems, we propose the hierarchical knowledge distillation\n(HKD) on the continuous integrate-and-fire (CIF) based ASR models. To transfer\nknowledge from PLMs to the ASR models, HKD employs cross-modal knowledge\ndistillation with contrastive loss at the acoustic level and knowledge\ndistillation with regression loss at the linguistic level. Compared with the\noriginal CIF-based model, our method achieves 15% and 9% relative error rate\nreduction on the AISHELL-1 and LibriSpeech datasets, respectively.", "published": "2023-01-30 15:44:55", "link": "http://arxiv.org/abs/2301.13003v2", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Can an AI Win Ghana's National Science and Maths Quiz? An AI Grand\n  Challenge for Education", "abstract": "There is a lack of enough qualified teachers across Africa which hampers\nefforts to provide adequate learning support such as educational question\nanswering (EQA) to students. An AI system that can enable students to ask\nquestions via text or voice and get instant answers will make high-quality\neducation accessible. Despite advances in the field of AI, there exists no\nrobust benchmark or challenge to enable building such an (EQA) AI within the\nAfrican context. Ghana's National Science and Maths Quiz competition (NSMQ) is\nthe perfect competition to evaluate the potential of such an AI due to its wide\ncoverage of scientific fields, variety of question types, highly competitive\nnature, and live, real-world format. The NSMQ is a Jeopardy-style annual live\nquiz competition in which 3 teams of 2 students compete by answering questions\nacross biology, chemistry, physics, and math in 5 rounds over 5 progressive\nstages until a winning team is crowned for that year. In this position paper,\nwe propose the NSMQ AI Grand Challenge, an AI Grand Challenge for Education\nusing Ghana's National Science and Maths Quiz competition (NSMQ) as a case\nstudy. Our proposed grand challenge is to \"Build an AI to compete live in\nGhana's National Science and Maths Quiz (NSMQ) competition and win - performing\nbetter than the best contestants in all rounds and stages of the competition.\"\nWe describe the competition, and key technical challenges to address along with\nideas from recent advances in machine learning that could be leveraged to solve\nthis challenge. This position paper is a first step towards conquering such a\nchallenge and importantly, making advances in AI for education in the African\ncontext towards democratizing high-quality education across Africa.", "published": "2023-01-30 17:28:33", "link": "http://arxiv.org/abs/2301.13089v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "LEXTREME: A Multi-Lingual and Multi-Task Benchmark for the Legal Domain", "abstract": "Lately, propelled by the phenomenal advances around the transformer\narchitecture, the legal NLP field has enjoyed spectacular growth. To measure\nprogress, well curated and challenging benchmarks are crucial. However, most\nbenchmarks are English only and in legal NLP specifically there is no\nmultilingual benchmark available yet. Additionally, many benchmarks are\nsaturated, with the best models clearly outperforming the best humans and\nachieving near perfect scores. We survey the legal NLP literature and select 11\ndatasets covering 24 languages, creating LEXTREME. To provide a fair\ncomparison, we propose two aggregate scores, one based on the datasets and one\non the languages. The best baseline (XLM-R large) achieves both a dataset\naggregate score a language aggregate score of 61.3. This indicates that\nLEXTREME is still very challenging and leaves ample room for improvement. To\nmake it easy for researchers and practitioners to use, we release LEXTREME on\nhuggingface together with all the code required to evaluate models and a public\nWeights and Biases project with all the runs.", "published": "2023-01-30 18:05:08", "link": "http://arxiv.org/abs/2301.13126v3", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2"], "primary_category": "cs.CL"}
{"title": "Protein Representation Learning via Knowledge Enhanced Primary Structure\n  Modeling", "abstract": "Protein representation learning has primarily benefited from the remarkable\ndevelopment of language models (LMs). Accordingly, pre-trained protein models\nalso suffer from a problem in LMs: a lack of factual knowledge. The recent\nsolution models the relationships between protein and associated knowledge\nterms as the knowledge encoding objective. However, it fails to explore the\nrelationships at a more granular level, i.e., the token level. To mitigate\nthis, we propose Knowledge-exploited Auto-encoder for Protein (KeAP), which\nperforms token-level knowledge graph exploration for protein representation\nlearning. In practice, non-masked amino acids iteratively query the associated\nknowledge tokens to extract and integrate helpful information for restoring\nmasked amino acids via attention. We show that KeAP can consistently outperform\nthe previous counterpart on 9 representative downstream applications, sometimes\nsurpassing it by large margins. These results suggest that KeAP provides an\nalternative yet effective way to perform knowledge enhanced protein\nrepresentation learning.", "published": "2023-01-30 18:33:15", "link": "http://arxiv.org/abs/2301.13154v2", "categories": ["cs.LG", "cs.CL", "cs.MM"], "primary_category": "cs.LG"}
{"title": "Advancing Radiograph Representation Learning with Masked Record Modeling", "abstract": "Modern studies in radiograph representation learning rely on either\nself-supervision to encode invariant semantics or associated radiology reports\nto incorporate medical expertise, while the complementarity between them is\nbarely noticed. To explore this, we formulate the self- and report-completion\nas two complementary objectives and present a unified framework based on masked\nrecord modeling (MRM). In practice, MRM reconstructs masked image patches and\nmasked report tokens following a multi-task scheme to learn knowledge-enhanced\nsemantic representations. With MRM pre-training, we obtain pre-trained models\nthat can be well transferred to various radiography tasks. Specifically, we\nfind that MRM offers superior performance in label-efficient fine-tuning. For\ninstance, MRM achieves 88.5% mean AUC on CheXpert using 1% labeled data,\noutperforming previous R$^2$L methods with 100% labels. On NIH ChestX-ray, MRM\noutperforms the best performing counterpart by about 3% under small labeling\nratios. Besides, MRM surpasses self- and report-supervised pre-training in\nidentifying the pneumonia type and the pneumothorax area, sometimes by large\nmargins.", "published": "2023-01-30 18:33:32", "link": "http://arxiv.org/abs/2301.13155v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "ArchiSound: Audio Generation with Diffusion", "abstract": "The recent surge in popularity of diffusion models for image generation has\nbrought new attention to the potential of these models in other areas of media\ngeneration. One area that has yet to be fully explored is the application of\ndiffusion models to audio generation. Audio generation requires an\nunderstanding of multiple aspects, such as the temporal dimension, long term\nstructure, multiple layers of overlapping sounds, and the nuances that only\ntrained listeners can detect. In this work, we investigate the potential of\ndiffusion models for audio generation. We propose a set of models to tackle\nmultiple aspects, including a new method for text-conditional latent audio\ndiffusion with stacked 1D U-Nets, that can generate multiple minutes of music\nfrom a textual description. For each model, we make an effort to maintain\nreasonable inference speed, targeting real-time on a single consumer GPU. In\naddition to trained models, we provide a collection of open source libraries\nwith the hope of simplifying future work in the field. Samples can be found at\nhttps://bit.ly/audio-diffusion. Codes are at\nhttps://github.com/archinetai/audio-diffusion-pytorch.", "published": "2023-01-30 20:23:26", "link": "http://arxiv.org/abs/2301.13267v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Proxy-based Zero-Shot Entity Linking by Effective Candidate Retrieval", "abstract": "A recent advancement in the domain of biomedical Entity Linking is the\ndevelopment of powerful two-stage algorithms, an initial candidate retrieval\nstage that generates a shortlist of entities for each mention, followed by a\ncandidate ranking stage. However, the effectiveness of both stages are\ninextricably dependent on computationally expensive components. Specifically,\nin candidate retrieval via dense representation retrieval it is important to\nhave hard negative samples, which require repeated forward passes and nearest\nneighbour searches across the entire entity label set throughout training. In\nthis work, we show that pairing a proxy-based metric learning loss with an\nadversarial regularizer provides an efficient alternative to hard negative\nsampling in the candidate retrieval stage. In particular, we show competitive\nperformance on the recall@1 metric, thereby providing the option to leave out\nthe expensive candidate ranking step. Finally, we demonstrate how the model can\nbe used in a zero-shot setting to discover out of knowledge base biomedical\nentities.", "published": "2023-01-30 22:43:21", "link": "http://arxiv.org/abs/2301.13318v1", "categories": ["cs.LG", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "Real-Time Acoustic Perception for Automotive Applications", "abstract": "In recent years the automotive industry has been strongly promoting the\ndevelopment of smart cars, equipped with multi-modal sensors to gather\ninformation about the surroundings, in order to aid human drivers or make\nautonomous decisions. While the focus has mostly been on visual sensors, also\nacoustic events are crucial to detect situations that require a change in the\ndriving behavior, such as a car honking, or the sirens of approaching emergency\nvehicles. In this paper, we summarize the results achieved so far in the Marie\nSklodowska-Curie Actions (MSCA) European Industrial Doctorates (EID) project\nIntelligent Ultra Low-Power Signal Processing for Automotive (I-SPOT). On the\nalgorithmic side, the I-SPOT Project aims to enable detecting, localizing and\ntracking environmental audio signals by jointly developing microphone array\nprocessing and deep learning techniques that specifically target automotive\napplications. Data generation software has been developed to cover the I-SPOT\ntarget scenarios and research challenges. This tool is currently being used to\ndevelop low-complexity deep learning techniques for emergency sound detection.\nOn the hardware side, the goal impels workflows for hardware-algorithm\nco-design to ease the generation of architectures that are sufficiently\nflexible towards algorithmic evolutions without giving up on efficiency, as\nwell as enable rapid feedback of hardware implications of algorithmic decision.\nThis is pursued though a hierarchical workflow that breaks the\nhardware-algorithm design space into reasonable subsets, which has been tested\nfor operator-level optimizations on state-of-the-art robust sound source\nlocalization for edge devices. Further, several open challenges towards an\nend-to-end system are clarified for the next stage of I-SPOT.", "published": "2023-01-30 12:00:20", "link": "http://arxiv.org/abs/2301.12808v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "MYRiAD: A Multi-Array Room Acoustic Database", "abstract": "In the development of acoustic signal processing algorithms, their evaluation\nin various acoustic environments is of utmost importance. In order to advance\nevaluation in realistic and reproducible scenarios, several high-quality\nacoustic databases have been developed over the years. In this paper, we\npresent another complementary database of acoustic recordings, referred to as\nthe Multi-arraY Room Acoustic Database (MYRiAD). The MYRiAD database is unique\nin its diversity of microphone configurations suiting a wide range of\nenhancement and reproduction applications (such as assistive hearing,\nteleconferencing, or sound zoning), the acoustics of the two recording spaces,\nand the variety of contained signals including 1214 room impulse responses\n(RIRs), reproduced speech, music, and stationary noise, as well as recordings\nof live cocktail parties held in both rooms. The microphone configurations\ncomprise a dummy head (DH) with in-ear omnidirectional microphones, two\nbehind-the-ear (BTE) pieces equipped with 2 omnidirectional microphones each, 5\nexternal omnidirectional microphones (XMs), and two concentric circular\nmicrophone arrays (CMAs) consisting of 12 omnidirectional microphones in total.\nThe two recording spaces, namely the SONORA Audio Laboratory (SAL) and the\nAlamire Interactive Laboratory (AIL), have reverberation times of 2.1s and\n0.5s, respectively. Audio signals were reproduced using 10 movable loudspeakers\nin the SAL and a built-in array of 24 loudspeakers in the AIL. MATLAB and\nPython scripts are included for accessing the signals as well as microphone and\nloudspeaker coordinates. The database is publicly available at [1].", "published": "2023-01-30 16:54:03", "link": "http://arxiv.org/abs/2301.13057v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion\n  Models", "abstract": "Large-scale multimodal generative modeling has created milestones in\ntext-to-image and text-to-video generation. Its application to audio still lags\nbehind for two main reasons: the lack of large-scale datasets with high-quality\ntext-audio pairs, and the complexity of modeling long continuous audio data. In\nthis work, we propose Make-An-Audio with a prompt-enhanced diffusion model that\naddresses these gaps by 1) introducing pseudo prompt enhancement with a\ndistill-then-reprogram approach, it alleviates data scarcity with orders of\nmagnitude concept compositions by using language-free audios; 2) leveraging\nspectrogram autoencoder to predict the self-supervised audio representation\ninstead of waveforms. Together with robust contrastive language-audio\npretraining (CLAP) representations, Make-An-Audio achieves state-of-the-art\nresults in both objective and subjective benchmark evaluation. Moreover, we\npresent its controllability and generalization for X-to-Audio with \"No Modality\nLeft Behind\", for the first time unlocking the ability to generate\nhigh-definition, high-fidelity audios given a user-defined modality input.\nAudio samples are available at https://Text-to-Audio.github.io", "published": "2023-01-30 04:44:34", "link": "http://arxiv.org/abs/2301.12661v1", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DanceAnyWay: Synthesizing Beat-Guided 3D Dances with Randomized Temporal\n  Contrastive Learning", "abstract": "We present DanceAnyWay, a generative learning method to synthesize\nbeat-guided dances of 3D human characters synchronized with music. Our method\nlearns to disentangle the dance movements at the beat frames from the dance\nmovements at all the remaining frames by operating at two hierarchical levels.\nAt the coarser \"beat\" level, it encodes the rhythm, pitch, and melody\ninformation of the input music via dedicated feature representations only at\nthe beat frames. It leverages them to synthesize the beat poses of the target\ndances using a sequence-to-sequence learning framework. At the finer\n\"repletion\" level, our method encodes similar rhythm, pitch, and melody\ninformation from all the frames of the input music via dedicated feature\nrepresentations. It generates the full dance sequences by combining the\nsynthesized beat and repletion poses and enforcing plausibility through an\nadversarial learning framework. Our training paradigm also enforces\nfine-grained diversity in the synthesized dances through a randomized temporal\ncontrastive loss, which ensures different segments of the dance sequences have\ndifferent movements and avoids motion freezing or collapsing to repetitive\nmovements. We evaluate the performance of our approach through extensive\nexperiments on the benchmark AIST++ dataset and observe improvements of about\n7%-12% in motion quality metrics and 1.5%-4% in motion diversity metrics over\nthe current baselines, respectively. We also conducted a user study to evaluate\nthe visual quality of our synthesized dances. We note that, on average, the\nsamples generated by our method were about 9-48% more preferred by the\nparticipants and had a 4-27% better five-point Likert-scale score over the best\navailable current baseline in terms of motion quality and synchronization. Our\nsource code and project page are available at\nhttps://github.com/aneeshbhattacharya/DanceAnyWay.", "published": "2023-01-30 22:20:24", "link": "http://arxiv.org/abs/2303.03870v3", "categories": ["cs.SD", "cs.GR", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SingSong: Generating musical accompaniments from singing", "abstract": "We present SingSong, a system that generates instrumental music to accompany\ninput vocals, potentially offering musicians and non-musicians alike an\nintuitive new way to create music featuring their own voice. To accomplish\nthis, we build on recent developments in musical source separation and audio\ngeneration. Specifically, we apply a state-of-the-art source separation\nalgorithm to a large corpus of music audio to produce aligned pairs of vocals\nand instrumental sources. Then, we adapt AudioLM (Borsos et al., 2022) -- a\nstate-of-the-art approach for unconditional audio generation -- to be suitable\nfor conditional \"audio-to-audio\" generation tasks, and train it on the\nsource-separated (vocal, instrumental) pairs. In a pairwise comparison with the\nsame vocal inputs, listeners expressed a significant preference for\ninstrumentals generated by SingSong compared to those from a strong retrieval\nbaseline.\n  Sound examples at https://g.co/magenta/singsong", "published": "2023-01-30 04:53:23", "link": "http://arxiv.org/abs/2301.12662v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "GibbsDDRM: A Partially Collapsed Gibbs Sampler for Solving Blind Inverse\n  Problems with Denoising Diffusion Restoration", "abstract": "Pre-trained diffusion models have been successfully used as priors in a\nvariety of linear inverse problems, where the goal is to reconstruct a signal\nfrom noisy linear measurements. However, existing approaches require knowledge\nof the linear operator. In this paper, we propose GibbsDDRM, an extension of\nDenoising Diffusion Restoration Models (DDRM) to a blind setting in which the\nlinear measurement operator is unknown. GibbsDDRM constructs a joint\ndistribution of the data, measurements, and linear operator by using a\npre-trained diffusion model for the data prior, and it solves the problem by\nposterior sampling with an efficient variant of a Gibbs sampler. The proposed\nmethod is problem-agnostic, meaning that a pre-trained diffusion model can be\napplied to various inverse problems without fine-tuning. In experiments, it\nachieved high performance on both blind image deblurring and vocal\ndereverberation tasks, despite the use of simple generic priors for the\nunderlying linear operators.", "published": "2023-01-30 06:27:48", "link": "http://arxiv.org/abs/2301.12686v2", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
