{"title": "Distributional Models and Deep Learning Embeddings: Combining the Best\n  of Both Worlds", "abstract": "There are two main approaches to the distributed representation of words:\nlow-dimensional deep learning embeddings and high-dimensional distributional\nmodels, in which each dimension corresponds to a context word. In this paper,\nwe combine these two approaches by learning embeddings based on\ndistributional-model vectors - as opposed to one-hot vectors as is standardly\ndone in deep learning. We show that the combined approach has better\nperformance on a word relatedness judgment task.", "published": "2013-12-19 14:18:14", "link": "http://arxiv.org/abs/1312.5559v3", "categories": ["cs.CL", "I.2.6; I.2.7"], "primary_category": "cs.CL"}
{"title": "Word Emdeddings through Hellinger PCA", "abstract": "Word embeddings resulting from neural language models have been shown to be\nsuccessful for a large variety of NLP tasks. However, such architecture might\nbe difficult to train and time-consuming. Instead, we propose to drastically\nsimplify the word embeddings computation through a Hellinger PCA of the word\nco-occurence matrix. We compare those new word embeddings with some well-known\nembeddings on NER and movie review tasks and show that we can reach similar or\neven better performance. Although deep learning is not really necessary for\ngenerating good word embeddings, we show that it can provide an easy way to\nadapt embeddings to specific tasks.", "published": "2013-12-19 13:31:11", "link": "http://arxiv.org/abs/1312.5542v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
