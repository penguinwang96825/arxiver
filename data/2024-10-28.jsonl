{"title": "Modeling and Replication of the Prepayment Option of Mortgages including Behavioral Uncertainty", "abstract": "Prepayment risk embedded in fixed-rate mortgages forms a significant fraction\nof a financial institution's exposure, and it receives particular attention\nbecause of the magnitude of the underlying market. The embedded prepayment\noption (EPO) bears the same interest rate risk as an exotic interest rate swap\n(IRS) with a suitable stochastic notional. We investigate the effect of\nrelaxing the assumption of a deterministic relationship between the market\ninterest rate incentive and the prepayment rate. A non-hedgeable risk factor is\nmodeled to capture the uncertainty in mortgage owners' behavior, leading to an\nincomplete market. We prove under natural assumptions that including behavioral\nuncertainty reduces the exposure's value. We statically replicate the exposure\nresulting from the EPO with IRSs and swaptions, and we show that a replication\nbased on swaps solely cannot easily control the right tail of the exposure\ndistribution, while including swaptions enables that. The replication framework\nis flexible and focuses on different regions in the exposure distribution.\nSince a non-hedgeable risk factor entails the existence of multiple equivalent\nmartingale measures, pricing and optimal replication are not unique. We\ninvestigate the effect of a market price of risk misspecification and we\nprovide a methodology to generate robust hedging strategies. Such strategies,\nobtained as solutions to a saddle-point problem, allow us to bound the exposure\nagainst a misspecification of the pricing measure.", "published": "2024-10-28 15:12:11", "link": "http://arxiv.org/abs/2410.21110v1", "categories": ["q-fin.CP"], "primary_category": "q-fin.CP"}
{"title": "Liquidity Jump, Liquidity Diffusion, and Crypto Wash Trading", "abstract": "We develop a new framework to detect wash trading in crypto assets through\nreal-time liquidity fluctuation. We propose that short-term price jumps in\ncrypto assets results from wash trading-induced liquidity fluctuation, and\nconstruct two complementary liquidity measures, liquidity jump (size of\nfluctuation) and liquidity diffusion (volatility of fluctuation), to capture\nthe behavioral signature of wash trading. Using US stocks as a benchmark, we\ndemonstrate that joint elevation in both liquidity metrics indicates wash\ntrading in crypto assets. A simulated regulatory treatment that removes likely\nwash trades confirms this dynamic: it reduces liquidity diffusion significantly\nwhile leaving liquidity jump largely unaffected. These findings align with a\ntheoretical model in which manipulative traders amplify both the level and\nvariance of price pressure, whereas passive investors affect only the level.\nOur model offers practical tools for investors to assess market quality and for\nregulators to monitor manipulation risk on crypto exchanges without oversight.", "published": "2024-10-28 11:58:38", "link": "http://arxiv.org/abs/2411.05803v4", "categories": ["q-fin.RM", "q-fin.CP", "q-fin.ST", "q-fin.TR"], "primary_category": "q-fin.RM"}
{"title": "Do LLM Personas Dream of Bull Markets? Comparing Human and AI Investment Strategies Through the Lens of the Five-Factor Model", "abstract": "Large Language Models (LLMs) have demonstrated the ability to adopt a\npersonality and behave in a human-like manner. There is a large body of\nresearch that investigates the behavioural impacts of personality in less\nobvious areas such as investment attitudes or creative decision making. In this\nstudy, we investigated whether an LLM persona with a specific Big Five\npersonality profile would perform an investment task similarly to a human with\nthe same personality traits. We used a simulated investment task to determine\nif these results could be generalised into actual behaviours. In this simulated\nenvironment, our results show these personas produced meaningful behavioural\ndifferences in all assessed categories, with these behaviours generally being\nconsistent with expectations derived from human research. We found that LLMs\nare able to generalise traits into expected behaviours in three areas: learning\nstyle, impulsivity and risk appetite while environmental attitudes could not be\naccurately represented. In addition, we showed that LLMs produce behaviour that\nis more reflective of human behaviour in a simulation environment compared to a\nsurvey environment.", "published": "2024-10-28 02:50:41", "link": "http://arxiv.org/abs/2411.05801v1", "categories": ["q-fin.ST", "cs.AI", "cs.CY", "q-fin.GN"], "primary_category": "q-fin.ST"}
{"title": "Visualizing attention zones in machine reading comprehension models", "abstract": "The attention mechanism plays an important role in the machine reading\ncomprehension (MRC) model. Here, we describe a pipeline for building an MRC\nmodel with a pretrained language model and visualizing the effect of each\nattention zone in different layers, which can indicate the explainability of\nthe model. With the presented protocol and accompanying code, researchers can\neasily visualize the relevance of each attention zone in the MRC model. This\napproach can be generalized to other pretrained language models.", "published": "2024-10-28 01:20:18", "link": "http://arxiv.org/abs/2410.20652v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SHARE: Shared Memory-Aware Open-Domain Long-Term Dialogue Dataset\n  Constructed from Movie Script", "abstract": "Shared memories between two individuals strengthen their bond and are crucial\nfor facilitating their ongoing conversations. This study aims to make long-term\ndialogue more engaging by leveraging these shared memories. To this end, we\nintroduce a new long-term dialogue dataset named SHARE, constructed from movie\nscripts, which are a rich source of shared memories among various\nrelationships. Our dialogue dataset contains the summaries of persona\ninformation and events of two individuals, as explicitly revealed in their\nconversation, along with implicitly extractable shared memories. We also\nintroduce EPISODE, a long-term dialogue framework based on SHARE that utilizes\nshared experiences between individuals. Through experiments using SHARE, we\ndemonstrate that shared memories between two individuals make long-term\ndialogues more engaging and sustainable, and that EPISODE effectively manages\nshared memories during dialogue. Our new dataset is publicly available at\nhttps://anonymous.4open.science/r/SHARE-AA1E/SHARE.json.", "published": "2024-10-28 02:41:33", "link": "http://arxiv.org/abs/2410.20682v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Combining Domain-Specific Models and LLMs for Automated Disease\n  Phenotyping from Survey Data", "abstract": "This exploratory pilot study investigated the potential of combining a\ndomain-specific model, BERN2, with large language models (LLMs) to enhance\nautomated disease phenotyping from research survey data. Motivated by the need\nfor efficient and accurate methods to harmonize the growing volume of survey\ndata with standardized disease ontologies, we employed BERN2, a biomedical\nnamed entity recognition and normalization model, to extract disease\ninformation from the ORIGINS birth cohort survey data. After rigorously\nevaluating BERN2's performance against a manually curated ground truth dataset,\nwe integrated various LLMs using prompt engineering, Retrieval-Augmented\nGeneration (RAG), and Instructional Fine-Tuning (IFT) to refine the model's\noutputs. BERN2 demonstrated high performance in extracting and normalizing\ndisease mentions, and the integration of LLMs, particularly with Few Shot\nInference and RAG orchestration, further improved accuracy. This approach,\nespecially when incorporating structured examples, logical reasoning prompts,\nand detailed context, offers a promising avenue for developing tools to enable\nefficient cohort profiling and data harmonization across large, heterogeneous\nresearch datasets.", "published": "2024-10-28 02:55:03", "link": "http://arxiv.org/abs/2410.20695v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating LLMs for Targeted Concept Simplification for Domain-Specific\n  Texts", "abstract": "One useful application of NLP models is to support people in reading complex\ntext from unfamiliar domains (e.g., scientific articles). Simplifying the\nentire text makes it understandable but sometimes removes important details. On\nthe contrary, helping adult readers understand difficult concepts in context\ncan enhance their vocabulary and knowledge. In a preliminary human study, we\nfirst identify that lack of context and unfamiliarity with difficult concepts\nis a major reason for adult readers' difficulty with domain-specific text. We\nthen introduce \"targeted concept simplification,\" a simplification task for\nrewriting text to help readers comprehend text containing unfamiliar concepts.\nWe also introduce WikiDomains, a new dataset of 22k definitions from 13\nacademic domains paired with a difficult concept within each definition. We\nbenchmark the performance of open-source and commercial LLMs and a simple\ndictionary baseline on this task across human judgments of ease of\nunderstanding and meaning preservation. Interestingly, our human judges\npreferred explanations about the difficult concept more than simplification of\nthe concept phrase. Further, no single model achieved superior performance\nacross all quality dimensions, and automated metrics also show low correlations\nwith human evaluations of concept simplification ($\\sim0.2$), opening up rich\navenues for research on personalized human reading comprehension support.", "published": "2024-10-28 05:56:51", "link": "http://arxiv.org/abs/2410.20763v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are LLM-Judges Robust to Expressions of Uncertainty? Investigating the\n  effect of Epistemic Markers on LLM-based Evaluation", "abstract": "In line with the principle of honesty, there has been a growing effort to\ntrain large language models (LLMs) to generate outputs containing epistemic\nmarkers. However, evaluation in the presence of epistemic markers has been\nlargely overlooked, raising a critical question: Could the use of epistemic\nmarkers in LLM-generated outputs lead to unintended negative consequences? To\naddress this, we present EMBER, a benchmark designed to assess the robustness\nof LLM-judges to epistemic markers in both single and pairwise evaluation\nsettings. Our findings, based on evaluations using EMBER, reveal that all\ntested LLM-judges, including GPT-4o, show a notable lack of robustness in the\npresence of epistemic markers. Specifically, we observe a negative bias toward\nepistemic markers, with a stronger bias against markers expressing uncertainty.\nThis suggests that LLM-judges are influenced by the presence of these markers\nand do not focus solely on the correctness of the content.", "published": "2024-10-28 06:21:43", "link": "http://arxiv.org/abs/2410.20774v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Decoding Reading Goals from Eye Movements", "abstract": "Readers can have different goals with respect to the text that they are\nreading. Can these goals be decoded from their eye movements over the text? In\nthis work, we examine for the first time whether it is possible to distinguish\nbetween two types of common reading goals: information seeking and ordinary\nreading for comprehension. Using large-scale eye tracking data, we address this\ntask with a wide range of models that cover different architectural and data\nrepresentation strategies, and further introduce a new model ensemble. We find\nthat transformer-based models with scanpath representations coupled with\nlanguage modeling solve it most successfully, and that accurate predictions can\nbe made in real time, long before the participant finished reading the text. We\nfurther introduce a new method for model performance analysis based on mixed\neffect modeling. Combining this method with rich textual annotations reveals\nkey properties of textual items and participants that contribute to the\ndifficulty of the task, and improves our understanding of the variability in\neye movement patterns across the two reading regimes.", "published": "2024-10-28 06:40:03", "link": "http://arxiv.org/abs/2410.20779v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rephrasing natural text data with different languages and quality levels\n  for Large Language Model pre-training", "abstract": "Recently published work on rephrasing natural text data for pre-training LLMs\nhas shown promising results when combining the original dataset with the\nsynthetically rephrased data. We build upon previous work by replicating\nexisting results on C4 and extending them with our optimized rephrasing\npipeline to the English, German, Italian, and Spanish Oscar subsets of\nCulturaX. Our pipeline leads to increased performance on standard evaluation\nbenchmarks in both the mono- and multilingual setup. In addition, we provide a\ndetailed study of our pipeline, investigating the choice of the base dataset\nand LLM for the rephrasing, as well as the relationship between the model size\nand the performance after pre-training. By exploring data with different\nperceived quality levels, we show that gains decrease with higher quality.\nFurthermore, we find the difference in performance between model families to be\nbigger than between different model sizes. This highlights the necessity for\ndetailed tests before choosing an LLM to rephrase large amounts of data.\nMoreover, we investigate the effect of pre-training with synthetic data on\nsupervised fine-tuning. Here, we find increasing but inconclusive results that\nhighly depend on the used benchmark. These results (again) highlight the need\nfor better benchmarking setups. In summary, we show that rephrasing\nmultilingual and low-quality data is a very promising direction to extend LLM\npre-training data.", "published": "2024-10-28 07:30:05", "link": "http://arxiv.org/abs/2410.20796v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NewTerm: Benchmarking Real-Time New Terms for Large Language Models with\n  Annual Updates", "abstract": "Despite their remarkable abilities in various tasks, large language models\n(LLMs) still struggle with real-time information (e.g., new facts and terms)\ndue to the knowledge cutoff in their development process. However, existing\nbenchmarks focus on outdated content and limited fields, facing difficulties in\nreal-time updating and leaving new terms unexplored. To address this problem,\nwe propose an adaptive benchmark, NewTerm, for real-time evaluation of new\nterms. We design a highly automated construction method to ensure high-quality\nbenchmark construction with minimal human effort, allowing flexible updates for\nreal-time information. Empirical results on various LLMs demonstrate over 20%\nperformance reduction caused by new terms. Additionally, while updates to the\nknowledge cutoff of LLMs can cover some of the new terms, they are unable to\ngeneralize to more distant new terms. We also analyze which types of terms are\nmore challenging and why LLMs struggle with new terms, paving the way for\nfuture research. Finally, we construct NewTerm 2022 and 2023 to evaluate the\nnew terms updated each year and will continue updating annually. The benchmark\nand codes can be found at https://github.com/hexuandeng/NewTerm.", "published": "2024-10-28 08:02:23", "link": "http://arxiv.org/abs/2410.20814v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Zeno's Paradox of `Low-Resource' Languages", "abstract": "The disparity in the languages commonly studied in Natural Language\nProcessing (NLP) is typically reflected by referring to languages as low vs\nhigh-resourced. However, there is limited consensus on what exactly qualifies\nas a `low-resource language.' To understand how NLP papers define and study\n`low resource' languages, we qualitatively analyzed 150 papers from the ACL\nAnthology and popular speech-processing conferences that mention the keyword\n`low-resource.' Based on our analysis, we show how several interacting axes\ncontribute to `low-resourcedness' of a language and why that makes it difficult\nto track progress for each individual language. We hope our work (1) elicits\nexplicit definitions of the terminology when it is used in papers and (2)\nprovides grounding for the different axes to consider when connoting a language\nas low-resource.", "published": "2024-10-28 08:05:34", "link": "http://arxiv.org/abs/2410.20817v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLMs are Biased Evaluators But Not Biased for Retrieval Augmented\n  Generation", "abstract": "Recent studies have demonstrated that large language models (LLMs) exhibit\nsignificant biases in evaluation tasks, particularly in preferentially rating\nand favoring self-generated content. However, the extent to which this bias\nmanifests in fact-oriented tasks, especially within retrieval-augmented\ngeneration (RAG) frameworks-where keyword extraction and factual accuracy take\nprecedence over stylistic elements-remains unclear. Our study addresses this\nknowledge gap by simulating two critical phases of the RAG framework. In the\nfirst phase, we access the suitability of human-authored versus model-generated\npassages, emulating the pointwise reranking process. The second phase involves\nconducting pairwise reading comprehension tests to simulate the generation\nprocess. Contrary to previous findings indicating a self-preference in rating\ntasks, our results reveal no significant self-preference effect in RAG\nframeworks. Instead, we observe that factual accuracy significantly influences\nLLMs' output, even in the absence of prior knowledge. Our research contributes\nto the ongoing discourse on LLM biases and their implications for RAG-based\nsystem, offering insights that may inform the development of more robust and\nunbiased LLM systems.", "published": "2024-10-28 08:32:09", "link": "http://arxiv.org/abs/2410.20833v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Simple Yet Effective Corpus Construction Framework for Indonesian\n  Grammatical Error Correction", "abstract": "Currently, the majority of research in grammatical error correction (GEC) is\nconcentrated on universal languages, such as English and Chinese. Many\nlow-resource languages lack accessible evaluation corpora. How to efficiently\nconstruct high-quality evaluation corpora for GEC in low-resource languages has\nbecome a significant challenge. To fill these gaps, in this paper, we present a\nframework for constructing GEC corpora. Specifically, we focus on Indonesian as\nour research language and construct an evaluation corpus for Indonesian GEC\nusing the proposed framework, addressing the limitations of existing evaluation\ncorpora in Indonesian. Furthermore, we investigate the feasibility of utilizing\nexisting large language models (LLMs), such as GPT-3.5-Turbo and GPT-4, to\nstreamline corpus annotation efforts in GEC tasks. The results demonstrate\nsignificant potential for enhancing the performance of LLMs in low-resource\nlanguage settings. Our code and corpus can be obtained from\nhttps://github.com/GKLMIP/GEC-Construction-Framework.", "published": "2024-10-28 08:44:56", "link": "http://arxiv.org/abs/2410.20838v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reward Modeling with Weak Supervision for Language Models", "abstract": "Recent advancements in large language models (LLMs) have led to their\nincreased application across various tasks, with reinforcement learning from\nhuman feedback (RLHF) being a crucial part of their training to align responses\nwith user intentions. In the RLHF process, a reward model is trained using\nresponses preferences determined by human labelers or AI systems, which then\nrefines the LLM through reinforcement learning. This work introduces weak\nsupervision as a strategy to extend RLHF datasets and enhance reward model\nperformance. Weak supervision employs noisy or imprecise data labeling,\nreducing reliance on expensive manually labeled data. By analyzing RLHF\ndatasets to identify heuristics that correlate with response preference, we\nwrote simple labeling functions and then calibrated a label model to weakly\nannotate unlabeled data. Our evaluation show that while weak supervision\nsignificantly benefits smaller datasets by improving reward model performance,\nits effectiveness decreases with larger, originally labeled datasets.\nAdditionally, using an LLM to generate and then weakly label responses offers a\npromising method for extending preference data.", "published": "2024-10-28 09:37:58", "link": "http://arxiv.org/abs/2410.20869v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AutoRAG: Automated Framework for optimization of Retrieval Augmented\n  Generation Pipeline", "abstract": "Using LLMs (Large Language Models) in conjunction with external documents has\nmade RAG (Retrieval-Augmented Generation) an essential technology. Numerous\ntechniques and modules for RAG are being researched, but their performance can\nvary across different datasets. Finding RAG modules that perform well on\nspecific datasets is challenging. In this paper, we propose the AutoRAG\nframework, which automatically identifies suitable RAG modules for a given\ndataset. AutoRAG explores and approximates the optimal combination of RAG\nmodules for the dataset. Additionally, we share the results of optimizing a\ndataset using AutoRAG. All experimental results and data are publicly available\nand can be accessed through our GitHub repository\nhttps://github.com/Marker-Inc-Korea/AutoRAG_ARAGOG_Paper .", "published": "2024-10-28 09:55:52", "link": "http://arxiv.org/abs/2410.20878v1", "categories": ["cs.CL", "H.4.0"], "primary_category": "cs.CL"}
{"title": "NeuGPT: Unified multi-modal Neural GPT", "abstract": "This paper introduces NeuGPT, a groundbreaking multi-modal language\ngeneration model designed to harmonize the fragmented landscape of neural\nrecording research. Traditionally, studies in the field have been\ncompartmentalized by signal type, with EEG, MEG, ECoG, SEEG, fMRI, and fNIRS\ndata being analyzed in isolation. Recognizing the untapped potential for\ncross-pollination and the adaptability of neural signals across varying\nexperimental conditions, we set out to develop a unified model capable of\ninterfacing with multiple modalities. Drawing inspiration from the success of\npre-trained large models in NLP, computer vision, and speech processing, NeuGPT\nis architected to process a diverse array of neural recordings and interact\nwith speech and text data. Our model mainly focus on brain-to-text decoding,\nimproving SOTA from 6.94 to 12.92 on BLEU-1 and 6.93 to 13.06 on ROUGE-1F. It\ncan also simulate brain signals, thereby serving as a novel neural interface.\nCode is available at\n\\href{https://github.com/NeuSpeech/NeuGPT}{NeuSpeech/NeuGPT\n(https://github.com/NeuSpeech/NeuGPT) .}", "published": "2024-10-28 10:53:22", "link": "http://arxiv.org/abs/2410.20916v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Long Sequence Modeling with Attention Tensorization: From Sequence to\n  Tensor Learning", "abstract": "As the demand for processing extended textual data grows, the ability to\nhandle long-range dependencies and maintain computational efficiency is more\ncritical than ever. One of the key issues for long-sequence modeling using\nattention-based model is the mismatch between the limited-range modeling power\nof full attention and the long-range token dependency in the input sequence. In\nthis work, we propose to scale up the attention receptive field by tensorizing\nlong input sequences into compact tensor representations followed by attention\non each transformed dimension. The resulting Tensorized Attention can be\nadopted as efficient transformer backbones to extend input context length with\nimproved memory and time efficiency. We show that the proposed attention\ntensorization encodes token dependencies as a multi-hop attention process, and\nis equivalent to Kronecker decomposition of full attention. Extensive\nexperiments show that tensorized attention can be used to adapt pretrained LLMs\nwith improved efficiency. Notably, Llama-8B with tensorization is trained under\n32,768 context length and can steadily extrapolate to 128k length during\ninference with $11\\times$ speedup, compared to full attention with\nFlashAttention-2.", "published": "2024-10-28 11:08:57", "link": "http://arxiv.org/abs/2410.20926v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Autoformalize Mathematical Statements by Symbolic Equivalence and\n  Semantic Consistency", "abstract": "Autoformalization, the task of automatically translating natural language\ndescriptions into a formal language, poses a significant challenge across\nvarious domains, especially in mathematics. Recent advancements in large\nlanguage models (LLMs) have unveiled their promising capabilities to formalize\neven competition-level math problems. However, we observe a considerable\ndiscrepancy between pass@1 and pass@k accuracies in LLM-generated\nformalizations. To address this gap, we introduce a novel framework that scores\nand selects the best result from k autoformalization candidates based on two\ncomplementary self-consistency methods: symbolic equivalence and semantic\nconsistency. Elaborately, symbolic equivalence identifies the logical\nhomogeneity among autoformalization candidates using automated theorem provers,\nand semantic consistency evaluates the preservation of the original meaning by\ninformalizing the candidates and computing the similarity between the\nembeddings of the original and informalized texts. Our extensive experiments on\nthe MATH and miniF2F datasets demonstrate that our approach significantly\nenhances autoformalization accuracy, achieving up to 0.22-1.35x relative\nimprovements across various LLMs and baseline methods.", "published": "2024-10-28 11:37:39", "link": "http://arxiv.org/abs/2410.20936v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Attacking Misinformation Detection Using Adversarial Examples Generated\n  by Language Models", "abstract": "We investigate the challenge of generating adversarial examples to test the\nrobustness of text classification algorithms detecting low-credibility content,\nincluding propaganda, false claims, rumours and hyperpartisan news. We focus on\nsimulation of content moderation by setting realistic limits on the number of\nqueries an attacker is allowed to attempt. Within our solution (TREPAT),\ninitial rephrasings are generated by large language models with prompts\ninspired by meaning-preserving NLP tasks, e.g. text simplification and style\ntransfer. Subsequently, these modifications are decomposed into small changes,\napplied through beam search procedure until the victim classifier changes its\ndecision. The evaluation confirms the superiority of our approach in the\nconstrained scenario, especially in case of long input text (news articles),\nwhere exhaustive search is not feasible.", "published": "2024-10-28 11:46:30", "link": "http://arxiv.org/abs/2410.20940v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is GPT-4 Less Politically Biased than GPT-3.5? A Renewed Investigation\n  of ChatGPT's Political Biases", "abstract": "This work investigates the political biases and personality traits of\nChatGPT, specifically comparing GPT-3.5 to GPT-4. In addition, the ability of\nthe models to emulate political viewpoints (e.g., liberal or conservative\npositions) is analyzed. The Political Compass Test and the Big Five Personality\nTest were employed 100 times for each scenario, providing statistically\nsignificant results and an insight into the results correlations. The responses\nwere analyzed by computing averages, standard deviations, and performing\nsignificance tests to investigate differences between GPT-3.5 and GPT-4.\nCorrelations were found for traits that have been shown to be interdependent in\nhuman studies. Both models showed a progressive and libertarian political bias,\nwith GPT-4's biases being slightly, but negligibly, less pronounced.\nSpecifically, on the Political Compass, GPT-3.5 scored -6.59 on the economic\naxis and -6.07 on the social axis, whereas GPT-4 scored -5.40 and -4.73. In\ncontrast to GPT-3.5, GPT-4 showed a remarkable capacity to emulate assigned\npolitical viewpoints, accurately reflecting the assigned quadrant\n(libertarian-left, libertarian-right, authoritarian-left, authoritarian-right)\nin all four tested instances. On the Big Five Personality Test, GPT-3.5 showed\nhighly pronounced Openness and Agreeableness traits (O: 85.9%, A: 84.6%). Such\npronounced traits correlate with libertarian views in human studies. While\nGPT-4 overall exhibited less pronounced Big Five personality traits, it did\nshow a notably higher Neuroticism score. Assigned political orientations\ninfluenced Openness, Agreeableness, and Conscientiousness, again reflecting\ninterdependencies observed in human studies. Finally, we observed that test\nsequencing affected ChatGPT's responses and the observed correlations,\nindicating a form of contextual memory.", "published": "2024-10-28 13:32:52", "link": "http://arxiv.org/abs/2410.21008v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Frequency matters: Modeling irregular morphological patterns in Spanish\n  with Transformers", "abstract": "Over the past decade, various studies have addressed how speakers solve the\nso-called `The Paradigm Cell Filling Problem' (PCFP) \\citep{ackerman2009parts}\nacross different languages. The PCFP addresses a fundamental question in\nmorphological processing: how do speakers accurately generate inflected forms\nof words when presented with incomplete paradigms? This problem is particularly\nsalient when modeling complex inflectional systems. We focus on Spanish verbal\nparadigms, where certain verbs follow an irregular L-shaped pattern, where the\nfirst-person singular present indicative stem matches the stem used throughout\nthe present subjunctive mood. We formulate the problem as a morphological\nreinflection task. Specifically, we investigate the role of input frequency in\nthe acquisition of regular versus irregular L-shaped patterns in transformer\nmodels. By systematically manipulating the input distributions and analyzing\nmodel behavior, we reveal four key findings: 1) Models perform better on\nL-shaped verbs compared to regular verbs, especially in uneven frequency\nconditions; 2) Robust primacy effects are observed, but no consistent recency\neffects; 3) Memorization becomes more prominent as the proportion of L-shaped\nverbs increases; 4) There is a tendency to regularize L-shaped verbs when their\nconsonant alternation pairs are rare or absent in the training data.", "published": "2024-10-28 13:36:46", "link": "http://arxiv.org/abs/2410.21013v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic Component Analysis: Discovering Patterns in Short Texts Beyond\n  Topics", "abstract": "Topic modeling is a key method in text analysis, but existing approaches are\nlimited by assuming one topic per document or fail to scale efficiently for\nlarge, noisy datasets of short texts. We introduce Semantic Component Analysis\n(SCA), a novel topic modeling technique that overcomes these limitations by\ndiscovering multiple, nuanced semantic components beyond a single topic in\nshort texts which we accomplish by introducing a decomposition step to the\nclustering-based topic modeling framework. We evaluate SCA on Twitter datasets\nin English, Hausa and Chinese. It achieves competetive coherence and diversity\ncompared to BERTopic, while uncovering at least double the semantic components\nand maintaining a noise rate close to zero. Furthermore, SCA is scalable and\neffective across languages, including an underrepresented one.", "published": "2024-10-28 14:09:52", "link": "http://arxiv.org/abs/2410.21054v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CRAT: A Multi-Agent Framework for Causality-Enhanced Reflective and\n  Retrieval-Augmented Translation with Large Language Models", "abstract": "Large language models (LLMs) have shown great promise in machine translation,\nbut they still struggle with contextually dependent terms, such as new or\ndomain-specific words. This leads to inconsistencies and errors that are\ndifficult to address. Existing solutions often depend on manual identification\nof such terms, which is impractical given the complexity and evolving nature of\nlanguage. While Retrieval-Augmented Generation (RAG) could provide some\nassistance, its application to translation is limited by issues such as\nhallucinations from information overload. In this paper, we propose CRAT, a\nnovel multi-agent translation framework that leverages RAG and\ncausality-enhanced self-reflection to address these challenges. This framework\nconsists of several specialized agents: the Unknown Terms Identification agent\ndetects unknown terms within the context, the Knowledge Graph (KG) Constructor\nagent extracts relevant internal knowledge about these terms and retrieves\nbilingual information from external sources, the Causality-enhanced Judge agent\nvalidates the accuracy of the information, and the Translator agent\nincorporates the refined information into the final output. This automated\nprocess allows for more precise and consistent handling of key terms during\ntranslation. Our results show that CRAT significantly improves translation\naccuracy, particularly in handling context-sensitive terms and emerging\nvocabulary.", "published": "2024-10-28 14:29:11", "link": "http://arxiv.org/abs/2410.21067v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Current State-of-the-Art of Bias Detection and Mitigation in Machine\n  Translation for African and European Languages: a Review", "abstract": "Studying bias detection and mitigation methods in natural language processing\nand the particular case of machine translation is highly relevant, as societal\nstereotypes might be reflected or reinforced by these systems. In this paper,\nwe analyze the state-of-the-art with a particular focus on European and African\nlanguages. We show how the majority of the work in this field concentrates on\nfew languages, and that there is potential for future research to cover also\nthe less investigated languages to contribute to more diversity in the research\nfield.", "published": "2024-10-28 15:28:50", "link": "http://arxiv.org/abs/2410.21126v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "uOttawa at LegalLens-2024: Transformer-based Classification Experiments", "abstract": "This paper presents the methods used for LegalLens-2024 shared task, which\nfocused on detecting legal violations within unstructured textual data and\nassociating these violations with potentially affected individuals. The shared\ntask included two subtasks: A) Legal Named Entity Recognition (L-NER) and B)\nLegal Natural Language Inference (L-NLI). For subtask A, we utilized the spaCy\nlibrary, while for subtask B, we employed a combined model incorporating\nRoBERTa and CNN. Our results were 86.3% in the L-NER subtask and 88.25% in the\nL-NLI subtask. Overall, our paper demonstrates the effectiveness of transformer\nmodels in addressing complex tasks in the legal domain. The source code for our\nimplementation is publicly available at\nhttps://github.com/NimaMeghdadi/uOttawa-at-LegalLens-2024-Transformer-based-Classification", "published": "2024-10-28 15:42:45", "link": "http://arxiv.org/abs/2410.21139v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SciER: An Entity and Relation Extraction Dataset for Datasets, Methods,\n  and Tasks in Scientific Documents", "abstract": "Scientific information extraction (SciIE) is critical for converting\nunstructured knowledge from scholarly articles into structured data (entities\nand relations). Several datasets have been proposed for training and validating\nSciIE models. However, due to the high complexity and cost of annotating\nscientific texts, those datasets restrict their annotations to specific parts\nof paper, such as abstracts, resulting in the loss of diverse entity mentions\nand relations in context. In this paper, we release a new entity and relation\nextraction dataset for entities related to datasets, methods, and tasks in\nscientific articles. Our dataset contains 106 manually annotated full-text\nscientific publications with over 24k entities and 12k relations. To capture\nthe intricate use and interactions among entities in full texts, our dataset\ncontains a fine-grained tag set for relations. Additionally, we provide an\nout-of-distribution test set to offer a more realistic evaluation. We conduct\ncomprehensive experiments, including state-of-the-art supervised models and our\nproposed LLM-based baselines, and highlight the challenges presented by our\ndataset, encouraging the development of innovative models to further the field\nof SciIE.", "published": "2024-10-28 15:56:49", "link": "http://arxiv.org/abs/2410.21155v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are BabyLMs Second Language Learners?", "abstract": "This paper describes a linguistically-motivated approach to the 2024 edition\nof the BabyLM Challenge (Warstadt et al. 2023). Rather than pursuing a first\nlanguage learning (L1) paradigm, we approach the challenge from a second\nlanguage (L2) learning perspective. In L2 learning, there is a stronger focus\non learning explicit linguistic information, such as grammatical notions,\ndefinitions of words or different ways of expressing a meaning. This makes L2\nlearning potentially more efficient and concise. We approximate this using data\nfrom Wiktionary, grammar examples either generated by an LLM or sourced from\ngrammar books, and paraphrase data. We find that explicit information about\nword meaning (in our case, Wiktionary) does not boost model performance, while\ngrammatical information can give a small improvement. The most impactful data\ningredient is sentence paraphrases, with our two best models being trained on\n1) a mix of paraphrase data and data from the BabyLM pretraining dataset, and\n2) exclusively paraphrase data.", "published": "2024-10-28 17:52:15", "link": "http://arxiv.org/abs/2410.21254v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Arithmetic Without Algorithms: Language Models Solve Math With a Bag of\n  Heuristics", "abstract": "Do large language models (LLMs) solve reasoning tasks by learning robust\ngeneralizable algorithms, or do they memorize training data? To investigate\nthis question, we use arithmetic reasoning as a representative task. Using\ncausal analysis, we identify a subset of the model (a circuit) that explains\nmost of the model's behavior for basic arithmetic logic and examine its\nfunctionality. By zooming in on the level of individual circuit neurons, we\ndiscover a sparse set of important neurons that implement simple heuristics.\nEach heuristic identifies a numerical input pattern and outputs corresponding\nanswers. We hypothesize that the combination of these heuristic neurons is the\nmechanism used to produce correct arithmetic answers. To test this, we\ncategorize each neuron into several heuristic types-such as neurons that\nactivate when an operand falls within a certain range-and find that the\nunordered combination of these heuristic types is the mechanism that explains\nmost of the model's accuracy on arithmetic prompts. Finally, we demonstrate\nthat this mechanism appears as the main source of arithmetic accuracy early in\ntraining. Overall, our experimental results across several LLMs show that LLMs\nperform arithmetic using neither robust algorithms nor memorization; rather,\nthey rely on a \"bag of heuristics\".", "published": "2024-10-28 17:59:06", "link": "http://arxiv.org/abs/2410.21272v1", "categories": ["cs.CL", "68T5", "I.2.7"], "primary_category": "cs.CL"}
{"title": "A Survey on Automatic Credibility Assessment of Textual Credibility\n  Signals in the Era of Large Language Models", "abstract": "In the current era of social media and generative AI, an ability to\nautomatically assess the credibility of online social media content is of\ntremendous importance. Credibility assessment is fundamentally based on\naggregating credibility signals, which refer to small units of information,\nsuch as content factuality, bias, or a presence of persuasion techniques, into\nan overall credibility score. Credibility signals provide a more granular, more\neasily explainable and widely utilizable information in contrast to currently\npredominant fake news detection, which utilizes various (mostly latent)\nfeatures. A growing body of research on automatic credibility assessment and\ndetection of credibility signals can be characterized as highly fragmented and\nlacking mutual interconnections. This issue is even more prominent due to a\nlack of an up-to-date overview of research works on automatic credibility\nassessment. In this survey, we provide such systematic and comprehensive\nliterature review of 175 research papers while focusing on textual credibility\nsignals and Natural Language Processing (NLP), which undergoes a significant\nadvancement due to Large Language Models (LLMs). While positioning the NLP\nresearch into the context of other multidisciplinary research works, we tackle\nwith approaches for credibility assessment as well as with 9 categories of\ncredibility signals (we provide a thorough analysis for 3 of them, namely: 1)\nfactuality, subjectivity and bias, 2) persuasion techniques and logical\nfallacies, and 3) claims and veracity). Following the description of the\nexisting methods, datasets and tools, we identify future challenges and\nopportunities, while paying a specific attention to recent rapid development of\ngenerative AI.", "published": "2024-10-28 17:51:08", "link": "http://arxiv.org/abs/2410.21360v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SpeechQE: Estimating the Quality of Direct Speech Translation", "abstract": "Recent advances in automatic quality estimation for machine translation have\nexclusively focused on written language, leaving the speech modality\nunderexplored. In this work, we formulate the task of quality estimation for\nspeech translation (SpeechQE), construct a benchmark, and evaluate a family of\nsystems based on cascaded and end-to-end architectures. In this process, we\nintroduce a novel end-to-end system leveraging pre-trained text LLM. Results\nsuggest that end-to-end approaches are better suited to estimating the quality\nof direct speech translation than using quality estimation systems designed for\ntext in cascaded systems. More broadly, we argue that quality estimation of\nspeech translation needs to be studied as a separate problem from that of text,\nand release our data and models to guide further research in this space.", "published": "2024-10-28 19:50:04", "link": "http://arxiv.org/abs/2410.21485v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RoBIn: A Transformer-Based Model For Risk Of Bias Inference With Machine\n  Reading Comprehension", "abstract": "Objective: Scientific publications play a crucial role in uncovering\ninsights, testing novel drugs, and shaping healthcare policies. Accessing the\nquality of publications requires evaluating their Risk of Bias (RoB), a process\ntypically conducted by human reviewers. In this study, we introduce a new\ndataset for machine reading comprehension and RoB assessment and present RoBIn\n(Risk of Bias Inference), an innovative model crafted to automate such\nevaluation. The model employs a dual-task approach, extracting evidence from a\ngiven context and assessing the RoB based on the gathered evidence. Methods: We\nuse data from the Cochrane Database of Systematic Reviews (CDSR) as ground\ntruth to label open-access clinical trial publications from PubMed. This\nprocess enabled us to develop training and test datasets specifically for\nmachine reading comprehension and RoB inference. Additionally, we created\nextractive (RoBInExt) and generative (RoBInGen) Transformer-based approaches to\nextract relevant evidence and classify the RoB effectively. Results: RoBIn is\nevaluated across various settings and benchmarked against state-of-the-art\nmethods for RoB inference, including large language models in multiple\nscenarios. In most cases, the best-performing RoBIn variant surpasses\ntraditional machine learning and LLM-based approaches, achieving an ROC AUC of\n0.83. Conclusion: Based on the evidence extracted from clinical trial reports,\nRoBIn performs a binary classification to decide whether the trial is at a low\nRoB or a high/unclear RoB. We found that both RoBInGen and RoBInExt are robust\nand have the best results in many settings.", "published": "2024-10-28 20:03:56", "link": "http://arxiv.org/abs/2410.21495v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SandboxAQ's submission to MRL 2024 Shared Task on Multi-lingual\n  Multi-task Information Retrieval", "abstract": "This paper explores the problems of Question Answering (QA) and Named Entity\nRecognition (NER) in five diverse languages. We tested five Large Language\nModels with various prompting methods, including zero-shot, chain-of-thought\nreasoning, and translation techniques. Our results show that while some models\nconsistently outperform others, their effectiveness varies significantly across\ntasks and languages. We saw that advanced prompting techniques generally\nimproved QA performance but had mixed results for NER; and we observed that\nlanguage difficulty patterns differed between tasks. Our findings highlight the\nneed for task-specific approaches in multilingual NLP and suggest that current\nmodels may develop different linguistic competencies for different tasks.", "published": "2024-10-28 20:15:45", "link": "http://arxiv.org/abs/2410.21501v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CARMO: Dynamic Criteria Generation for Context-Aware Reward Modelling", "abstract": "Reward modeling in large language models is susceptible to reward hacking,\ncausing models to latch onto superficial features such as the tendency to\ngenerate lists or unnecessarily long responses. In reinforcement learning from\nhuman feedback (RLHF) and more generally during post-training flawed reward\nsignals often lead to outputs that optimize for these spurious correlates\ninstead of genuine quality or correctness. We propose Context-Aware Reward\nModeling (CARMO), a novel approach that first generates dynamic,\ncontext-relevant criteria to ground the reward model before producing reward\nscores. Unlike prior methods that rely on static rubrics, CARMO leverages large\nlanguage models (LLMs) to adaptively create evaluation criteria such as logical\nconsistency, clarity, and depth tailored to the user query. Our theoretical\nanalysis shows that such criteria generation can mitigate reward hacking. We\nfurther demonstrate that CARMO can be distilled into smaller models, reducing\nthe computational cost of alignment. We establish a new state-of-the-art\nperformance in zero-shot settings for generative models, achieving a 2.1\\%\nimprovement on Reward Bench. Furthermore, alignment performed on the\nCARMO-curated preference dataset achieves 22.5\\% and 21.1\\% LC-WR and WR,\nrespectively, on Mistral-Base (7B).", "published": "2024-10-28 21:18:49", "link": "http://arxiv.org/abs/2410.21545v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SubjECTive-QA: Measuring Subjectivity in Earnings Call Transcripts' QA\n  Through Six-Dimensional Feature Analysis", "abstract": "Fact-checking is extensively studied in the context of misinformation and\ndisinformation, addressing objective inaccuracies. However, a softer form of\nmisinformation involves responses that are factually correct but lack certain\nfeatures such as clarity and relevance. This challenge is prevalent in formal\nQuestion-Answer (QA) settings such as press conferences in finance, politics,\nsports, and other domains, where subjective answers can obscure transparency.\nDespite this, there is a lack of manually annotated datasets for subjective\nfeatures across multiple dimensions. To address this gap, we introduce\nSubjECTive-QA, a human annotated dataset on Earnings Call Transcripts' (ECTs)\nQA sessions as the answers given by company representatives are often open to\nsubjective interpretations and scrutiny. The dataset includes 49,446\nannotations for long-form QA pairs across six features: Assertive, Cautious,\nOptimistic, Specific, Clear, and Relevant. These features are carefully\nselected to encompass the key attributes that reflect the tone of the answers\nprovided during QA sessions across different domain. Our findings are that the\nbest-performing Pre-trained Language Model (PLM), RoBERTa-base, has similar\nweighted F1 scores to Llama-3-70b-Chat on features with lower subjectivity,\nsuch as Relevant and Clear, with a mean difference of 2.17% in their weighted\nF1 scores. The models perform significantly better on features with higher\nsubjectivity, such as Specific and Assertive, with a mean difference of 10.01%\nin their weighted F1 scores. Furthermore, testing SubjECTive-QA's\ngeneralizability using QAs from White House Press Briefings and Gaggles yields\nan average weighted F1 score of 65.97% using our best models for each feature,\ndemonstrating broader applicability beyond the financial domain. SubjECTive-QA\nis publicly available under the CC BY 4.0 license", "published": "2024-10-28 01:17:34", "link": "http://arxiv.org/abs/2410.20651v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Relaxed Recursive Transformers: Effective Parameter Sharing with\n  Layer-wise LoRA", "abstract": "Large language models (LLMs) are expensive to deploy. Parameter sharing\noffers a possible path towards reducing their size and cost, but its\neffectiveness in modern LLMs remains fairly limited. In this work, we revisit\n\"layer tying\" as form of parameter sharing in Transformers, and introduce novel\nmethods for converting existing LLMs into smaller \"Recursive Transformers\" that\nshare parameters across layers, with minimal loss of performance. Here, our\nRecursive Transformers are efficiently initialized from standard pretrained\nTransformers, but only use a single block of unique layers that is then\nrepeated multiple times in a loop. We further improve performance by\nintroducing Relaxed Recursive Transformers that add flexibility to the layer\ntying constraint via depth-wise low-rank adaptation (LoRA) modules, yet still\npreserve the compactness of the overall model. We show that our recursive\nmodels (e.g., recursive Gemma 1B) outperform both similar-sized vanilla\npretrained models (such as TinyLlama 1.1B and Pythia 1B) and knowledge\ndistillation baselines -- and can even recover most of the performance of the\noriginal \"full-size\" model (e.g., Gemma 2B with no shared parameters). Finally,\nwe propose Continuous Depth-wise Batching, a promising new inference paradigm\nenabled by the Recursive Transformer when paired with early exiting. In a\ntheoretical analysis, we show that this has the potential to lead to\nsignificant (2-3x) gains in inference throughput.", "published": "2024-10-28 02:15:45", "link": "http://arxiv.org/abs/2410.20672v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Relation-based Counterfactual Data Augmentation and Contrastive Learning\n  for Robustifying Natural Language Inference Models", "abstract": "Although pre-trained language models show good performance on various natural\nlanguage processing tasks, they often rely on non-causal features and patterns\nto determine the outcome. For natural language inference tasks, previous\nresults have shown that even a model trained on a large number of data fails to\nperform well on counterfactually revised data, indicating that the model is not\nrobustly learning the semantics of the classes. In this paper, we propose a\nmethod in which we use token-based and sentence-based augmentation methods to\ngenerate counterfactual sentence pairs that belong to each class, and apply\ncontrastive learning to help the model learn the difference between sentence\npairs of different classes with similar contexts. Evaluation results with\ncounterfactually-revised dataset and general NLI datasets show that the\nproposed method can improve the performance and robustness of the NLI model.", "published": "2024-10-28 03:43:25", "link": "http://arxiv.org/abs/2410.20710v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SEG:Seeds-Enhanced Iterative Refinement Graph Neural Network for Entity\n  Alignment", "abstract": "Entity alignment is crucial for merging knowledge across knowledge graphs, as\nit matches entities with identical semantics. The standard method matches these\nentities based on their embedding similarities using semi-supervised learning.\nHowever, diverse data sources lead to non-isomorphic neighborhood structures\nfor aligned entities, complicating alignment, especially for less common and\nsparsely connected entities. This paper presents a soft label propagation\nframework that integrates multi-source data and iterative seed enhancement,\naddressing scalability challenges in handling extensive datasets where scale\ncomputing excels. The framework uses seeds for anchoring and selects optimal\nrelationship pairs to create soft labels rich in neighborhood features and\nsemantic relationship data. A bidirectional weighted joint loss function is\nimplemented, which reduces the distance between positive samples and\ndifferentially processes negative samples, taking into account the\nnon-isomorphic neighborhood structures. Our method outperforms existing\nsemi-supervised approaches, as evidenced by superior results on multiple\ndatasets, significantly improving the quality of entity alignment.", "published": "2024-10-28 04:50:46", "link": "http://arxiv.org/abs/2410.20733v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Gender Bias in LLM-generated Interview Responses", "abstract": "LLMs have emerged as a promising tool for assisting individuals in diverse\ntext-generation tasks, including job-related texts. However, LLM-generated\nanswers have been increasingly found to exhibit gender bias. This study\nevaluates three LLMs (GPT-3.5, GPT-4, Claude) to conduct a multifaceted audit\nof LLM-generated interview responses across models, question types, and jobs,\nand their alignment with two gender stereotypes. Our findings reveal that\ngender bias is consistent, and closely aligned with gender stereotypes and the\ndominance of jobs. Overall, this study contributes to the systematic\nexamination of gender bias in LLM-generated interview responses, highlighting\nthe need for a mindful approach to mitigate such biases in related\napplications.", "published": "2024-10-28 05:08:08", "link": "http://arxiv.org/abs/2410.20739v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Plan*RAG: Efficient Test-Time Planning for Retrieval Augmented\n  Generation", "abstract": "We introduce Plan*RAG, a novel framework that enables structured multi-hop\nreasoning in retrieval-augmented generation (RAG) through test-time reasoning\nplan generation. While existing approaches such as ReAct maintain reasoning\nchains within the language model's context window, we observe that this often\nleads to plan fragmentation and execution failures. Our key insight is that by\nisolating the reasoning plan as a directed acyclic graph (DAG) outside the LM's\nworking memory, we can enable (1) systematic exploration of reasoning paths,\n(2) atomic subqueries enabling precise retrievals and grounding, and (3)\nefficiency through parallel execution and bounded context window utilization.\nMoreover, Plan*RAG's modular design allows it to be integrated with existing\nRAG methods, thus providing a practical solution to improve current RAG\nsystems. On standard multi-hop reasoning benchmarks, Plan*RAG consistently\nachieves improvements over recently proposed methods such as RQ-RAG and\nSelf-RAG, while maintaining comparable computational costs.", "published": "2024-10-28 05:35:04", "link": "http://arxiv.org/abs/2410.20753v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Static and Dynamic Attention Framework for Multi Turn Dialogue\n  Generation", "abstract": "Recently, research on open domain dialogue systems have attracted extensive\ninterests of academic and industrial researchers. The goal of an open domain\ndialogue system is to imitate humans in conversations. Previous works on single\nturn conversation generation have greatly promoted the research of open domain\ndialogue systems. However, understanding multiple single turn conversations is\nnot equal to the understanding of multi turn dialogue due to the coherent and\ncontext dependent properties of human dialogue. Therefore, in open domain multi\nturn dialogue generation, it is essential to modeling the contextual semantics\nof the dialogue history, rather than only according to the last utterance.\nPrevious research had verified the effectiveness of the hierarchical recurrent\nencoder-decoder framework on open domain multi turn dialogue generation.\nHowever, using RNN-based model to hierarchically encoding the utterances to\nobtain the representation of dialogue history still face the problem of a\nvanishing gradient. To address this issue, in this paper, we proposed a static\nand dynamic attention-based approach to model the dialogue history and then\ngenerate open domain multi turn dialogue responses. Experimental results on\nUbuntu and Opensubtitles datasets verify the effectiveness of the proposed\nstatic and dynamic attention-based approach on automatic and human evaluation\nmetrics in various experimental settings. Meanwhile, we also empirically verify\nthe performance of combining the static and dynamic attentions on open domain\nmulti turn dialogue generation.", "published": "2024-10-28 06:05:34", "link": "http://arxiv.org/abs/2410.20766v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SCULPT: Systematic Tuning of Long Prompts", "abstract": "Prompt optimization is essential for effective utilization of large language\nmodels (LLMs) across diverse tasks. While existing optimization methods are\neffective in optimizing short prompts, they struggle with longer, more complex\nones, often risking information loss and being sensitive to small\nperturbations. To address these challenges, we propose SCULPT (Systematic\nTuning of Long Prompts), a framework that treats prompt optimization as a\nhierarchical tree refinement problem. SCULPT represents prompts as tree\nstructures, enabling targeted modifications while preserving contextual\nintegrity. It employs a Critic-Actor framework that generates reflections and\napplies actions to refine the prompt. Evaluations demonstrate SCULPT's\neffectiveness on long prompts, its robustness to adversarial perturbations, and\nits ability to generate high-performing prompts even without any initial\nhuman-written prompt. Compared to existing state of the art methods, SCULPT\nconsistently improves LLM performance by preserving essential task information\nwhile applying structured refinements. Both qualitative and quantitative\nanalyses show that SCULPT produces more stable and interpretable prompt\nmodifications, ensuring better generalization across tasks.", "published": "2024-10-28 07:10:10", "link": "http://arxiv.org/abs/2410.20788v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Deep Learning for Medical Text Processing: BERT Model Fine-Tuning and\n  Comparative Study", "abstract": "This paper proposes a medical literature summary generation method based on\nthe BERT model to address the challenges brought by the current explosion of\nmedical information. By fine-tuning and optimizing the BERT model, we develop\nan efficient summary generation system that can quickly extract key information\nfrom medical literature and generate coherent, accurate summaries. In the\nexperiment, we compared various models, including Seq-Seq, Attention,\nTransformer, and BERT, and demonstrated that the improved BERT model offers\nsignificant advantages in the Rouge and Recall metrics. Furthermore, the\nresults of this study highlight the potential of knowledge distillation\ntechniques to further enhance model performance. The system has demonstrated\nstrong versatility and efficiency in practical applications, offering a\nreliable tool for the rapid screening and analysis of medical literature.", "published": "2024-10-28 07:17:45", "link": "http://arxiv.org/abs/2410.20792v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Fine-Grained and Multi-Dimensional Metrics for Document-Level Machine\n  Translation", "abstract": "Large language models (LLMs) have excelled in various NLP tasks, including\nmachine translation (MT), yet most studies focus on sentence-level translation.\nThis work investigates the inherent capability of instruction-tuned LLMs for\ndocument-level translation (docMT). Unlike prior approaches that require\nspecialized techniques, we evaluate LLMs by directly prompting them to\ntranslate entire documents in a single pass. Our results show that this method\nimproves translation quality compared to translating sentences separately, even\nwithout document-level fine-tuning. However, this advantage is not reflected in\nBLEU scores, which often favor sentence-based translations. We propose using\nthe LLM-as-a-judge paradigm for evaluation, where GPT-4 is used to assess\ndocument coherence, accuracy, and fluency in a more nuanced way than\nn-gram-based metrics. Overall, our work demonstrates that instruction-tuned\nLLMs can effectively leverage document context for translation. However, we\ncaution against using BLEU scores for evaluating docMT, as they often provide\nmisleading outcomes, failing to capture the quality of document-level\ntranslation. Code and the outputs from GPT4-as-a-judge are available at\nhttps://github.com/EIT-NLP/BLEUless_DocMT", "published": "2024-10-28 11:49:58", "link": "http://arxiv.org/abs/2410.20941v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FACT: Examining the Effectiveness of Iterative Context Rewriting for\n  Multi-fact Retrieval", "abstract": "Large Language Models (LLMs) are proficient at retrieving single facts from\nextended contexts, yet they struggle with tasks requiring the simultaneous\nretrieval of multiple facts, especially during generation. This paper\nidentifies a novel \"lost-in-the-middle\" phenomenon, where LLMs progressively\nlose track of critical information throughout the generation process, resulting\nin incomplete or inaccurate retrieval. To address this challenge, we introduce\nFind All Crucial Texts (FACT), an iterative retrieval method that refines\ncontext through successive rounds of rewriting. This approach enables models to\ncapture essential facts incrementally, which are often overlooked in\nsingle-pass retrieval. Experiments demonstrate that FACT substantially enhances\nmulti-fact retrieval performance across various tasks, though improvements are\nless notable in general-purpose QA scenarios. Our findings shed light on the\nlimitations of LLMs in multi-fact retrieval and underscore the need for more\nresilient long-context retrieval strategies.", "published": "2024-10-28 13:36:41", "link": "http://arxiv.org/abs/2410.21012v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Transferable Post-training via Inverse Value Learning", "abstract": "As post-training processes utilize increasingly large datasets and base\nmodels continue to grow in size, the computational demands and implementation\nchallenges of existing algorithms are escalating significantly. In this paper,\nwe propose modeling the changes at the logits level during post-training using\na separate neural network (i.e., the value network). After training this\nnetwork on a small base model using demonstrations, this network can be\nseamlessly integrated with other pre-trained models during inference, enables\nthem to achieve similar capability enhancements. We systematically investigate\nthe best practices for this paradigm in terms of pre-training weights and\nconnection schemes. We demonstrate that the resulting value network has broad\ntransferability across pre-trained models of different parameter sizes within\nthe same family, models undergoing continuous pre-training within the same\nfamily, and models with different vocabularies across families. In certain\ncases, it can achieve performance comparable to full-parameter fine-tuning.\nFurthermore, we explore methods to enhance the transferability of the value\nmodel and prevent overfitting to the base model used during training.", "published": "2024-10-28 13:48:43", "link": "http://arxiv.org/abs/2410.21027v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time", "abstract": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, at the 1.3B parameters scale,\ndiffusion models, even without caching, can generate tokens at a rate that is\nup to 8 times faster than AR models employing KV-caching, and we anticipate\nfurther improvements with the inclusion of caching. Moreover, we demonstrate\nthe efficacy of our approach for diffusion language models with up to 860M\nparameters.", "published": "2024-10-28 13:56:30", "link": "http://arxiv.org/abs/2410.21035v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Sorting Out the Bad Seeds: Automatic Classification of Cryptocurrency\n  Abuse Reports", "abstract": "Abuse reporting services collect reports about abuse victims have suffered.\nAccurate classification of the submitted reports is fundamental to analyzing\nthe prevalence and financial impact of different abuse types (e.g., sextortion,\ninvestment, romance). Current classification approaches are problematic because\nthey require the reporter to select the abuse type from a list, assuming the\nreporter has the necessary experience for the classification, which we show is\nfrequently not the case, or require manual classification by analysts, which\ndoes not scale. To address these issues, this paper presents a novel approach\nto classify cryptocurrency abuse reports automatically. We first build a\ntaxonomy of 19 frequently reported abuse types. Given as input the textual\ndescription written by the reporter, our classifier leverages a large language\nmodel (LLM) to interpret the text and assign it an abuse type in our taxonomy.\nWe collect 290K cryptocurrency abuse reports from two popular reporting\nservices: BitcoinAbuse and BBB's ScamTracker. We build ground truth datasets\nfor 20K of those reports and use them to evaluate three designs for our\nLLM-based classifier and four LLMs, as well as a supervised ML classifier used\nas a baseline. Our LLM-based classifier achieves a precision of 0.92, a recall\nof 0.87, and an F1 score of 0.89, compared to an F1 score of 0.55 for the\nbaseline. We demonstrate our classifier in two applications: providing\nfinancial loss statistics for fine-grained abuse types and generating tagged\naddresses for cryptocurrency analysis platforms.", "published": "2024-10-28 13:58:04", "link": "http://arxiv.org/abs/2410.21041v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Stealthy Jailbreak Attacks on Large Language Models via Benign Data\n  Mirroring", "abstract": "Large language model (LLM) safety is a critical issue, with numerous studies\nemploying red team testing to enhance model security. Among these, jailbreak\nmethods explore potential vulnerabilities by crafting malicious prompts that\ninduce model outputs contrary to safety alignments. Existing black-box\njailbreak methods often rely on model feedback, repeatedly submitting queries\nwith detectable malicious instructions during the attack search process.\nAlthough these approaches are effective, the attacks may be intercepted by\ncontent moderators during the search process. We propose an improved transfer\nattack method that guides malicious prompt construction by locally training a\nmirror model of the target black-box model through benign data distillation.\nThis method offers enhanced stealth, as it does not involve submitting\nidentifiable malicious instructions to the target model during the search\nphase. Our approach achieved a maximum attack success rate of 92%, or a\nbalanced value of 80% with an average of 1.5 detectable jailbreak queries per\nsample against GPT-3.5 Turbo on a subset of AdvBench. These results underscore\nthe need for more robust defense mechanisms.", "published": "2024-10-28 14:48:05", "link": "http://arxiv.org/abs/2410.21083v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Action Recognition in Surveillance Videos", "abstract": "The growing demand for surveillance in public spaces presents significant\nchallenges due to the shortage of human resources. Current AI-based video\nsurveillance systems heavily rely on core computer vision models that require\nextensive finetuning, which is particularly difficult in surveillance settings\ndue to limited datasets and difficult setting (viewpoint, low quality, etc.).\nIn this work, we propose leveraging Large Vision-Language Models (LVLMs), known\nfor their strong zero and few-shot generalization, to tackle video\nunderstanding tasks in surveillance. Specifically, we explore VideoLLaMA2, a\nstate-of-the-art LVLM, and an improved token-level sampling method,\nSelf-Reflective Sampling (Self-ReS). Our experiments on the UCF-Crime dataset\nshow that VideoLLaMA2 represents a significant leap in zero-shot performance,\nwith 20% boost over the baseline. Self-ReS additionally increases zero-shot\naction recognition performance to 44.6%. These results highlight the potential\nof LVLMs, paired with improved sampling techniques, for advancing surveillance\nvideo analysis in diverse scenarios.", "published": "2024-10-28 15:13:53", "link": "http://arxiv.org/abs/2410.21113v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Towards Unifying Evaluation of Counterfactual Explanations: Leveraging\n  Large Language Models for Human-Centric Assessments", "abstract": "As machine learning models evolve, maintaining transparency demands more\nhuman-centric explainable AI techniques. Counterfactual explanations, with\nroots in human reasoning, identify the minimal input changes needed to obtain a\ngiven output and, hence, are crucial for supporting decision-making. Despite\ntheir importance, the evaluation of these explanations often lacks grounding in\nuser studies and remains fragmented, with existing metrics not fully capturing\nhuman perspectives. To address this challenge, we developed a diverse set of 30\ncounterfactual scenarios and collected ratings across 8 evaluation metrics from\n206 respondents. Subsequently, we fine-tuned different Large Language Models\n(LLMs) to predict average or individual human judgment across these metrics.\nOur methodology allowed LLMs to achieve an accuracy of up to 63% in zero-shot\nevaluations and 85% (over a 3-classes prediction) with fine-tuning across all\nmetrics. The fine-tuned models predicting human ratings offer better\ncomparability and scalability in evaluating different counterfactual\nexplanation frameworks.", "published": "2024-10-28 15:33:37", "link": "http://arxiv.org/abs/2410.21131v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Palisade -- Prompt Injection Detection Framework", "abstract": "The advent of Large Language Models LLMs marks a milestone in Artificial\nIntelligence, altering how machines comprehend and generate human language.\nHowever, LLMs are vulnerable to malicious prompt injection attacks, where\ncrafted inputs manipulate the models behavior in unintended ways, compromising\nsystem integrity and causing incorrect outcomes. Conventional detection methods\nrely on static, rule-based approaches, which often fail against sophisticated\nthreats like abnormal token sequences and alias substitutions, leading to\nlimited adaptability and higher rates of false positives and false\nnegatives.This paper proposes a novel NLP based approach for prompt injection\ndetection, emphasizing accuracy and optimization through a layered input\nscreening process. In this framework, prompts are filtered through three\ndistinct layers rule-based, ML classifier, and companion LLM before reaching\nthe target model, thereby minimizing the risk of malicious interaction.Tests\nshow the ML classifier achieves the highest accuracy among individual layers,\nyet the multi-layer framework enhances overall detection accuracy by reducing\nfalse negatives. Although this increases false positives, it minimizes the risk\nof overlooking genuine injected prompts, thus prioritizing security.This\nmulti-layered detection approach highlights LLM vulnerabilities and provides a\ncomprehensive framework for future research, promoting secure interactions\nbetween humans and AI systems.", "published": "2024-10-28 15:47:03", "link": "http://arxiv.org/abs/2410.21146v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "M2rc-Eval: Massively Multilingual Repository-level Code Completion\n  Evaluation", "abstract": "Repository-level code completion has drawn great attention in software\nengineering, and several benchmark datasets have been introduced. However,\nexisting repository-level code completion benchmarks usually focus on a limited\nnumber of languages (<5), which cannot evaluate the general code intelligence\nabilities across different languages for existing code Large Language Models\n(LLMs). Besides, the existing benchmarks usually report overall average scores\nof different languages, where the fine-grained abilities in different\ncompletion scenarios are ignored. Therefore, to facilitate the research of code\nLLMs in multilingual scenarios, we propose a massively multilingual\nrepository-level code completion benchmark covering 18 programming languages\n(called M2RC-EVAL), and two types of fine-grained annotations (i.e.,\nbucket-level and semantic-level) on different completion scenarios are\nprovided, where we obtain these annotations based on the parsed abstract syntax\ntree. Moreover, we also curate a massively multilingual instruction corpora\nM2RC- INSTRUCT dataset to improve the repository-level code completion\nabilities of existing code LLMs. Comprehensive experimental results demonstrate\nthe effectiveness of our M2RC-EVAL and M2RC-INSTRUCT.", "published": "2024-10-28 15:58:41", "link": "http://arxiv.org/abs/2410.21157v1", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "LoRA vs Full Fine-tuning: An Illusion of Equivalence", "abstract": "Fine-tuning is a crucial paradigm for adapting pre-trained large language\nmodels to downstream tasks. Recently, methods like Low-Rank Adaptation (LoRA)\nhave been shown to match the performance of fully fine-tuned models on various\ntasks with an extreme reduction in the number of trainable parameters. Even in\nsettings where both methods learn similarly accurate models, \\emph{are their\nlearned solutions really equivalent?} We study how different fine-tuning\nmethods change pre-trained models by analyzing the model's weight matrices\nthrough the lens of their spectral properties. We find that full fine-tuning\nand LoRA yield weight matrices whose singular value decompositions exhibit very\ndifferent structure; moreover, the fine-tuned models themselves show distinct\ngeneralization behaviors when tested outside the adaptation task's\ndistribution. More specifically, we first show that the weight matrices trained\nwith LoRA have new, high-ranking singular vectors, which we call \\emph{intruder\ndimensions}. Intruder dimensions do not appear during full fine-tuning. Second,\nwe show that LoRA models with intruder dimensions, despite achieving similar\nperformance to full fine-tuning on the target task, become worse models of the\npre-training distribution and adapt less robustly to multiple tasks\nsequentially. Higher-rank, rank-stabilized LoRA models closely mirror full\nfine-tuning, even when performing on par with lower-rank LoRA models on the\nsame tasks. These results suggest that models updated with LoRA and full\nfine-tuning access different parts of parameter space, even when they perform\nequally on the fine-tuned distribution. We conclude by examining why intruder\ndimensions appear in LoRA fine-tuned models, why they are undesirable, and how\ntheir effects can be minimized.", "published": "2024-10-28 17:14:01", "link": "http://arxiv.org/abs/2410.21228v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "LongReward: Improving Long-context Large Language Models with AI\n  Feedback", "abstract": "Though significant advancements have been achieved in developing long-context\nlarge language models (LLMs), the compromised quality of LLM-synthesized data\nfor supervised fine-tuning (SFT) often affects the long-context performance of\nSFT models and leads to inherent limitations. In principle, reinforcement\nlearning (RL) with appropriate reward signals can further enhance models'\ncapacities. However, how to obtain reliable rewards in long-context scenarios\nremains unexplored. To this end, we propose LongReward, a novel method that\nutilizes an off-the-shelf LLM to provide rewards for long-context model\nresponses from four human-valued dimensions: helpfulness, logicality,\nfaithfulness, and completeness, each with a carefully designed assessment\npipeline. By combining LongReward and offline RL algorithm DPO, we are able to\neffectively improve long-context SFT models. Our experiments indicate that\nLongReward not only significantly improves models' long-context performance but\nalso enhances their ability to follow short instructions. We also find that\nlong-context DPO with LongReward and conventional short-context DPO can be used\ntogether without hurting either one's performance.", "published": "2024-10-28 17:50:42", "link": "http://arxiv.org/abs/2410.21252v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "EoRA: Training-free Compensation for Compressed LLM with Eigenspace\n  Low-Rank Approximation", "abstract": "In this work, we re-formulate the model compression problem into the\ncustomized compensation problem: Given a compressed model, we aim to introduce\nresidual low-rank paths to compensate for compression errors under customized\nrequirements from users (e.g., tasks, compression ratios), resulting in greater\nflexibility in balancing accuracy and overhead(inference and model size)\nwithout being bound to fixed compression formats. However, naively applying SVD\nto derive residual paths causes suboptimal utilization of the low-rank\nrepresentation capacity. Instead, we propose Training-free Eigenspace Low-Rank\nApproximation (EoRA), a method that directly minimizes compression-induced\nerrors without requiring gradient-based training, achieving fast optimization\nin minutes using a small amount of calibration data. EoRA projects compression\nerrors into the eigenspace of input activations, leveraging eigenvalues to\neffectively prioritize the reconstruction of high-importance error components.\nMoreover, EoRA can be seamlessly integrated with fine-tuning and quantization\nto further improve effectiveness and efficiency. EoRA consistently outperforms\nprevious methods in compensating errors for compressed LLaMA2/3 models on\nvarious tasks, such as language generation, commonsense reasoning, and math\nreasoning tasks (e.g., 31.31%/12.88% and 9.69% improvements on\nARC-Easy/ARC-Challenge and MathQA when compensating LLaMA3-8B that is quantized\nto 4-bit and pruned to 2:4 sparsity). EoRA offers a scalable, training-free\nsolution to compensate for compression errors, making it a powerful tool to\ndeploy LLMs more flexibly. Code is available at https://github.com/NVlabs/EoRA.", "published": "2024-10-28 17:59:03", "link": "http://arxiv.org/abs/2410.21271v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fine-tuned Large Language Models (LLMs): Improved Prompt Injection\n  Attacks Detection", "abstract": "Large language models (LLMs) are becoming a popular tool as they have\nsignificantly advanced in their capability to tackle a wide range of\nlanguage-based tasks. However, LLMs applications are highly vulnerable to\nprompt injection attacks, which poses a critical problem. These attacks target\nLLMs applications through using carefully designed input prompts to divert the\nmodel from adhering to original instruction, thereby it could execute\nunintended actions. These manipulations pose serious security threats which\npotentially results in data leaks, biased outputs, or harmful responses. This\nproject explores the security vulnerabilities in relation to prompt injection\nattacks. To detect whether a prompt is vulnerable or not, we follows two\napproaches: 1) a pre-trained LLM, and 2) a fine-tuned LLM. Then, we conduct a\nthorough analysis and comparison of the classification performance. Firstly, we\nuse pre-trained XLM-RoBERTa model to detect prompt injections using test\ndataset without any fine-tuning and evaluate it by zero-shot classification.\nThen, this proposed work will apply supervised fine-tuning to this pre-trained\nLLM using a task-specific labeled dataset from deepset in huggingface, and this\nfine-tuned model achieves impressive results with 99.13\\% accuracy, 100\\%\nprecision, 98.33\\% recall and 99.15\\% F1-score thorough rigorous\nexperimentation and evaluation. We observe that our approach is highly\nefficient in detecting prompt injection attacks.", "published": "2024-10-28 00:36:21", "link": "http://arxiv.org/abs/2410.21337v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Language Model Benchmarks in Medical Tasks", "abstract": "With the increasing application of large language models (LLMs) in the\nmedical domain, evaluating these models' performance using benchmark datasets\nhas become crucial. This paper presents a comprehensive survey of various\nbenchmark datasets employed in medical LLM tasks. These datasets span multiple\nmodalities including text, image, and multimodal benchmarks, focusing on\ndifferent aspects of medical knowledge such as electronic health records\n(EHRs), doctor-patient dialogues, medical question-answering, and medical image\ncaptioning. The survey categorizes the datasets by modality, discussing their\nsignificance, data structure, and impact on the development of LLMs for\nclinical tasks such as diagnosis, report generation, and predictive decision\nsupport. Key benchmarks include MIMIC-III, MIMIC-IV, BioASQ, PubMedQA, and\nCheXpert, which have facilitated advancements in tasks like medical report\ngeneration, clinical summarization, and synthetic data generation. The paper\nsummarizes the challenges and opportunities in leveraging these benchmarks for\nadvancing multimodal medical intelligence, emphasizing the need for datasets\nwith a greater degree of language diversity, structured omics data, and\ninnovative approaches to synthesis. This work also provides a foundation for\nfuture research in the application of LLMs in medicine, contributing to the\nevolving field of medical artificial intelligence.", "published": "2024-10-28 11:07:33", "link": "http://arxiv.org/abs/2410.21348v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLMCBench: Benchmarking Large Language Model Compression for Efficient\n  Deployment", "abstract": "Although large language models (LLMs) have demonstrated their strong\nintelligence ability, the high demand for computation and storage hinders their\npractical application. To this end, many model compression techniques are\nproposed to increase the efficiency of LLMs. However, current researches only\nvalidate their methods on limited models, datasets, metrics, etc, and still\nlack a comprehensive evaluation under more general scenarios. So it is still a\nquestion of which model compression approach we should use under a specific\ncase. To mitigate this gap, we present the Large Language Model Compression\nBenchmark (LLMCBench), a rigorously designed benchmark with an in-depth\nanalysis for LLM compression algorithms. We first analyze the actual model\nproduction requirements and carefully design evaluation tracks and metrics.\nThen, we conduct extensive experiments and comparison using multiple mainstream\nLLM compression approaches. Finally, we perform an in-depth analysis based on\nthe evaluation and provide useful insight for LLM compression design. We hope\nour LLMCBench can contribute insightful suggestions for LLM compression\nalgorithm design and serve as a foundation for future research. Our code is\navailable at https://github.com/AboveParadise/LLMCBench.", "published": "2024-10-28 14:45:01", "link": "http://arxiv.org/abs/2410.21352v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Causal Interventions on Causal Paths: Mapping GPT-2's Reasoning From\n  Syntax to Semantics", "abstract": "While interpretability research has shed light on some internal algorithms\nutilized by transformer-based LLMs, reasoning in natural language, with its\ndeep contextuality and ambiguity, defies easy categorization. As a result,\nformulating clear and motivating questions for circuit analysis that rely on\nwell-defined in-domain and out-of-domain examples required for causal\ninterventions is challenging. Although significant work has investigated\ncircuits for specific tasks, such as indirect object identification (IOI),\ndeciphering natural language reasoning through circuits remains difficult due\nto its inherent complexity. In this work, we take initial steps to characterize\ncausal reasoning in LLMs by analyzing clear-cut cause-and-effect sentences like\n\"I opened an umbrella because it started raining,\" where causal interventions\nmay be possible through carefully crafted scenarios using GPT-2 small. Our\nfindings indicate that causal syntax is localized within the first 2-3 layers,\nwhile certain heads in later layers exhibit heightened sensitivity to\nnonsensical variations of causal sentences. This suggests that models may infer\nreasoning by (1) detecting syntactic cues and (2) isolating distinct heads in\nthe final layers that focus on semantic relationships.", "published": "2024-10-28 15:37:56", "link": "http://arxiv.org/abs/2410.21353v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Energy-Based Diffusion Language Models for Text Generation", "abstract": "Despite remarkable progress in autoregressive language models, alternative\ngenerative paradigms beyond left-to-right generation are still being actively\nexplored. Discrete diffusion models, with the capacity for parallel generation,\nhave recently emerged as a promising alternative. Unfortunately, these models\nstill underperform the autoregressive counterparts, with the performance gap\nincreasing when reducing the number of sampling steps. Our analysis reveals\nthat this degradation is a consequence of an imperfect approximation used by\ndiffusion models. In this work, we propose Energy-based Diffusion Language\nModel (EDLM), an energy-based model operating at the full sequence level for\neach diffusion step, introduced to improve the underlying approximation used by\ndiffusion models. More specifically, we introduce an EBM in a residual form,\nand show that its parameters can be obtained by leveraging a pretrained\nautoregressive model or by finetuning a bidirectional transformer via noise\ncontrastive estimation. We also propose an efficient generation algorithm via\nparallel important sampling. Comprehensive experiments on language modeling\nbenchmarks show that our model can consistently outperform state-of-the-art\ndiffusion models by a significant margin, and approaches autoregressive models'\nperplexity. We further show that, without any generation performance drop, our\nframework offers a 1.3$\\times$ sampling speedup over existing diffusion models.\nReproduced code is available at\nhttps://github.com/MinkaiXu/Energy-Diffusion-LLM.", "published": "2024-10-28 17:25:56", "link": "http://arxiv.org/abs/2410.21357v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CT2C-QA: Multimodal Question Answering over Chinese Text, Table and\n  Chart", "abstract": "Multimodal Question Answering (MMQA) is crucial as it enables comprehensive\nunderstanding and accurate responses by integrating insights from diverse data\nrepresentations such as tables, charts, and text. Most existing researches in\nMMQA only focus on two modalities such as image-text QA, table-text QA and\nchart-text QA, and there remains a notable scarcity in studies that investigate\nthe joint analysis of text, tables, and charts. In this paper, we present\nC$\\text{T}^2$C-QA, a pioneering Chinese reasoning-based QA dataset that\nincludes an extensive collection of text, tables, and charts, meticulously\ncompiled from 200 selectively sourced webpages. Our dataset simulates real\nwebpages and serves as a great test for the capability of the model to analyze\nand reason with multimodal data, because the answer to a question could appear\nin various modalities, or even potentially not exist at all. Additionally, we\npresent AED (\\textbf{A}llocating, \\textbf{E}xpert and \\textbf{D}esicion), a\nmulti-agent system implemented through collaborative deployment, information\ninteraction, and collective decision-making among different agents.\nSpecifically, the Assignment Agent is in charge of selecting and activating\nexpert agents, including those proficient in text, tables, and charts. The\nDecision Agent bears the responsibility of delivering the final verdict,\ndrawing upon the analytical insights provided by these expert agents. We\nexecute a comprehensive analysis, comparing AED with various state-of-the-art\nmodels in MMQA, including GPT-4. The experimental outcomes demonstrate that\ncurrent methodologies, including GPT-4, are yet to meet the benchmarks set by\nour dataset.", "published": "2024-10-28 18:13:14", "link": "http://arxiv.org/abs/2410.21414v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Language Models for Manufacturing", "abstract": "The rapid advances in Large Language Models (LLMs) have the potential to\ntransform manufacturing industry, offering new opportunities to optimize\nprocesses, improve efficiency, and drive innovation. This paper provides a\ncomprehensive exploration of the integration of LLMs into the manufacturing\ndomain, focusing on their potential to automate and enhance various aspects of\nmanufacturing, from product design and development to quality control, supply\nchain optimization, and talent management. Through extensive evaluations across\nmultiple manufacturing tasks, we demonstrate the remarkable capabilities of\nstate-of-the-art LLMs, such as GPT-4V, in understanding and executing complex\ninstructions, extracting valuable insights from vast amounts of data, and\nfacilitating knowledge sharing. We also delve into the transformative potential\nof LLMs in reshaping manufacturing education, automating coding processes,\nenhancing robot control systems, and enabling the creation of immersive,\ndata-rich virtual environments through the industrial metaverse. By\nhighlighting the practical applications and emerging use cases of LLMs in\nmanufacturing, this paper aims to provide a valuable resource for\nprofessionals, researchers, and decision-makers seeking to harness the power of\nthese technologies to address real-world challenges, drive operational\nexcellence, and unlock sustainable growth in an increasingly competitive\nlandscape.", "published": "2024-10-28 18:13:47", "link": "http://arxiv.org/abs/2410.21418v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "UFT: Unifying Fine-Tuning of SFT and RLHF/DPO/UNA through a Generalized\n  Implicit Reward Function", "abstract": "By pretraining on trillions of tokens, an LLM gains the capability of text\ngeneration. However, to enhance its utility and reduce potential harm, SFT and\nalignment are applied sequentially to the pretrained model. Due to the\ndiffering nature and objective functions of SFT and alignment, catastrophic\nforgetting has become a significant issue. To address this, we introduce\nUnified Fine-Tuning (UFT), which integrates SFT and alignment into a single\ntraining stage using the same objective and loss functions through an implicit\nreward function. Our experimental results demonstrate that UFT outperforms SFT\non instruction-tuning data alone. Moreover, when combining instruction-tuning\ndata with alignment data, UFT effectively prevents catastrophic forgetting\nacross these two stages and shows a clear advantage over sequentially applying\nSFT and alignment. This is evident in the significant improvements observed in\nthe \\textbf{ifeval} task for instruction-following and the \\textbf{truthful-qa}\ntask for factuality. The proposed general fine-tuning framework UFT establishes\nan effective and efficient pretraining-UFT paradigm for LLM training.", "published": "2024-10-28 18:34:25", "link": "http://arxiv.org/abs/2410.21438v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference", "abstract": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV.", "published": "2024-10-28 19:08:12", "link": "http://arxiv.org/abs/2410.21465v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Estimating Causal Effects of Text Interventions Leveraging LLMs", "abstract": "Quantifying the effects of textual interventions in social systems, such as\nreducing anger in social media posts to see its impact on engagement, is\nchallenging. Real-world interventions are often infeasible, necessitating\nreliance on observational data. Traditional causal inference methods, typically\ndesigned for binary or discrete treatments, are inadequate for handling the\ncomplex, high-dimensional textual data. This paper addresses these challenges\nby proposing CausalDANN, a novel approach to estimate causal effects using text\ntransformations facilitated by large language models (LLMs). Unlike existing\nmethods, our approach accommodates arbitrary textual interventions and\nleverages text-level classifiers with domain adaptation ability to produce\nrobust effect estimates against domain shifts, even when only the control group\nis observed. This flexibility in handling various text interventions is a key\nadvancement in causal estimation for textual data, offering opportunities to\nbetter understand human behaviors and develop effective interventions within\nsocial systems.", "published": "2024-10-28 19:19:35", "link": "http://arxiv.org/abs/2410.21474v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FATH: Authentication-based Test-time Defense against Indirect Prompt\n  Injection Attacks", "abstract": "Large language models (LLMs) have been widely deployed as the backbone with\nadditional tools and text information for real-world applications. However,\nintegrating external information into LLM-integrated applications raises\nsignificant security concerns. Among these, prompt injection attacks are\nparticularly threatening, where malicious instructions injected in the external\ntext information can exploit LLMs to generate answers as the attackers desire.\nWhile both training-time and test-time defense methods have been developed to\nmitigate such attacks, the unaffordable training costs associated with\ntraining-time methods and the limited effectiveness of existing test-time\nmethods make them impractical. This paper introduces a novel test-time defense\nstrategy, named Formatting AuThentication with Hash-based tags (FATH). Unlike\nexisting approaches that prevent LLMs from answering additional instructions in\nexternal text, our method implements an authentication system, requiring LLMs\nto answer all received instructions with a security policy and selectively\nfilter out responses to user instructions as the final output. To achieve this,\nwe utilize hash-based authentication tags to label each response, facilitating\naccurate identification of responses according to the user's instructions and\nimproving the robustness against adaptive attacks. Comprehensive experiments\ndemonstrate that our defense method can effectively defend against indirect\nprompt injection attacks, achieving state-of-the-art performance under Llama3\nand GPT3.5 models across various attack methods. Our code is released at:\nhttps://github.com/Jayfeather1024/FATH", "published": "2024-10-28 20:02:47", "link": "http://arxiv.org/abs/2410.21492v2", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Efficient Training of Sparse Autoencoders for Large Language Models via\n  Layer Groups", "abstract": "Sparse AutoEnocders (SAEs) have recently been employed as an unsupervised\napproach for understanding the inner workings of Large Language Models (LLMs).\nThey reconstruct the model's activations with a sparse linear combination of\ninterpretable features. However, training SAEs is computationally intensive,\nespecially as models grow in size and complexity. To address this challenge, we\npropose a novel training strategy that reduces the number of trained SAEs from\none per layer to one for a given group of contiguous layers. Our experimental\nresults on Pythia 160M highlight a speedup of up to 6x without compromising the\nreconstruction quality and performance on downstream tasks. Therefore, layer\nclustering presents an efficient approach to train SAEs in modern LLMs.", "published": "2024-10-28 20:23:30", "link": "http://arxiv.org/abs/2410.21508v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLM-Forest: Ensemble Learning of LLMs with Graph-Augmented Prompts for\n  Data Imputation", "abstract": "Missing data imputation is a critical challenge in various domains, such as\nhealthcare and finance, where data completeness is vital for accurate analysis.\nLarge language models (LLMs), trained on vast corpora, have shown strong\npotential in data generation, making them a promising tool for data imputation.\nHowever, challenges persist in designing effective prompts for a\nfinetuning-free process and in mitigating the risk of LLM hallucinations. To\naddress these issues, we propose a novel framework, LLM-Forest, which\nintroduces a \"forest\" of few-shot learning LLM \"trees\" with confidence-based\nweighted voting, inspired by ensemble learning (Random Forest). This framework\nis established on a new concept of bipartite information graphs to identify\nhigh-quality relevant neighboring entries with both feature and value\ngranularity. Extensive experiments on 9 real-world datasets demonstrate the\neffectiveness and efficiency of LLM-Forest.", "published": "2024-10-28 20:42:46", "link": "http://arxiv.org/abs/2410.21520v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Not All LLM-Generated Data Are Equal: Rethinking Data Weighting in Text\n  Classification", "abstract": "Synthetic data augmentation via large language models (LLMs) allows\nresearchers to leverage additional training data, thus enhancing the\nperformance of downstream tasks, especially when real-world data is scarce.\nHowever, the generated data can deviate from the real-world data, and this\nmisalignment can bring deficient outcomes while applying the trained model to\napplications. Therefore, we proposed efficient weighted-loss approaches to\nalign synthetic data with real-world distribution by emphasizing high-quality\nand diversified data generated by LLMs with using merely a little real-world\ndata. We empirically assessed the effectiveness of our method on multiple text\nclassification tasks, and the results showed leveraging our approaches on a\nBERT-level model robustly outperformed standard cross-entropy and other data\nweighting approaches, providing potential solutions to effectively leveraging\nsynthetic data from any suitable data generator for model training.", "published": "2024-10-28 20:53:49", "link": "http://arxiv.org/abs/2410.21526v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Semantic Search Evaluation", "abstract": "We propose a novel method for evaluating the performance of a content search\nsystem that measures the semantic match between a query and the results\nreturned by the search system. We introduce a metric called \"on-topic rate\" to\nmeasure the percentage of results that are relevant to the query. To achieve\nthis, we design a pipeline that defines a golden query set, retrieves the top K\nresults for each query, and sends calls to GPT 3.5 with formulated prompts. Our\nsemantic evaluation pipeline helps identify common failure patterns and goals\nagainst the metric for relevance improvements.", "published": "2024-10-28 21:25:38", "link": "http://arxiv.org/abs/2410.21549v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Thank You, Stingray: Multilingual Large Language Models Can Not (Yet)\n  Disambiguate Cross-Lingual Word Sense", "abstract": "Multilingual large language models (LLMs) have gained prominence, but\nconcerns arise regarding their reliability beyond English. This study addresses\nthe gap in cross-lingual semantic evaluation by introducing a novel benchmark\nfor cross-lingual sense disambiguation, StingrayBench. In this paper, we\ndemonstrate using false friends -- words that are orthographically similar but\nhave completely different meanings in two languages -- as a possible approach\nto pinpoint the limitation of cross-lingual sense disambiguation in LLMs. We\ncollect false friends in four language pairs, namely Indonesian-Malay,\nIndonesian-Tagalog, Chinese-Japanese, and English-German; and challenge LLMs to\ndistinguish the use of them in context. In our analysis of various models, we\nobserve they tend to be biased toward higher-resource languages. We also\npropose new metrics for quantifying the cross-lingual sense bias and\ncomprehension based on our benchmark. Our work contributes to developing more\ndiverse and inclusive language modeling, promoting fairer access for the wider\nmultilingual community.", "published": "2024-10-28 22:09:43", "link": "http://arxiv.org/abs/2410.21573v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Perspective for Adapting Generalist AI to Specialized Medical AI\n  Applications and Their Challenges", "abstract": "The integration of Large Language Models (LLMs) into medical applications has\nsparked widespread interest across the healthcare industry, from drug discovery\nand development to clinical decision support, assisting telemedicine, medical\ndevices, and healthcare insurance applications. This perspective paper aims to\ndiscuss the inner workings of building LLM-powered medical AI applications and\nintroduces a comprehensive framework for their development. We review existing\nliterature and outline the unique challenges of applying LLMs in specialized\nmedical contexts. Additionally, we introduce a three-step framework to organize\nmedical LLM research activities: 1) Modeling: breaking down complex medical\nworkflows into manageable steps for developing medical-specific models; 2)\nOptimization: optimizing the model performance with crafted prompts and\nintegrating external knowledge and tools, and 3) System engineering:\ndecomposing complex tasks into subtasks and leveraging human expertise for\nbuilding medical AI applications. Furthermore, we offer a detailed use case\nplaybook that describes various LLM-powered medical AI applications, such as\noptimizing clinical trial design, enhancing clinical decision support, and\nadvancing medical imaging analysis. Finally, we discuss various challenges and\nconsiderations for building medical AI applications with LLMs, such as handling\nhallucination issues, data ownership and compliance, privacy, intellectual\nproperty considerations, compute cost, sustainability issues, and responsible\nAI requirements.", "published": "2024-10-28 22:30:06", "link": "http://arxiv.org/abs/2411.00024v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Guide-LLM: An Embodied LLM Agent and Text-Based Topological Map for\n  Robotic Guidance of People with Visual Impairments", "abstract": "Navigation presents a significant challenge for persons with visual\nimpairments (PVI). While traditional aids such as white canes and guide dogs\nare invaluable, they fall short in delivering detailed spatial information and\nprecise guidance to desired locations. Recent developments in large language\nmodels (LLMs) and vision-language models (VLMs) offer new avenues for enhancing\nassistive navigation. In this paper, we introduce Guide-LLM, an embodied\nLLM-based agent designed to assist PVI in navigating large indoor environments.\nOur approach features a novel text-based topological map that enables the LLM\nto plan global paths using a simplified environmental representation, focusing\non straight paths and right-angle turns to facilitate navigation. Additionally,\nwe utilize the LLM's commonsense reasoning for hazard detection and\npersonalized path planning based on user preferences. Simulated experiments\ndemonstrate the system's efficacy in guiding PVI, underscoring its potential as\na significant advancement in assistive technology. The results highlight\nGuide-LLM's ability to offer efficient, adaptive, and personalized navigation\nassistance, pointing to promising advancements in this field.", "published": "2024-10-28 01:58:21", "link": "http://arxiv.org/abs/2410.20666v2", "categories": ["cs.RO", "cs.AI", "cs.CL"], "primary_category": "cs.RO"}
{"title": "Simple Is Effective: The Roles of Graphs and Large Language Models in\n  Knowledge-Graph-Based Retrieval-Augmented Generation", "abstract": "Large Language Models (LLMs) demonstrate strong reasoning abilities but face\nlimitations such as hallucinations and outdated knowledge. Knowledge Graph\n(KG)-based Retrieval-Augmented Generation (RAG) addresses these issues by\ngrounding LLM outputs in structured external knowledge from KGs. However,\ncurrent KG-based RAG frameworks still struggle to optimize the trade-off\nbetween retrieval effectiveness and efficiency in identifying a suitable amount\nof relevant graph information for the LLM to digest. We introduce SubgraphRAG,\nextending the KG-based RAG framework that retrieves subgraphs and leverages\nLLMs for reasoning and answer prediction. Our approach innovatively integrates\na lightweight multilayer perceptron with a parallel triple-scoring mechanism\nfor efficient and flexible subgraph retrieval while encoding directional\nstructural distances to enhance retrieval effectiveness. The size of retrieved\nsubgraphs can be flexibly adjusted to match the query's need and the downstream\nLLM's capabilities. This design strikes a balance between model complexity and\nreasoning power, enabling scalable and generalizable retrieval processes.\nNotably, based on our retrieved subgraphs, smaller LLMs like\nLlama3.1-8B-Instruct deliver competitive results with explainable reasoning,\nwhile larger models like GPT-4o achieve state-of-the-art accuracy compared with\nprevious baselines -- all without fine-tuning. Extensive evaluations on the\nWebQSP and CWQ benchmarks highlight SubgraphRAG's strengths in efficiency,\naccuracy, and reliability by reducing hallucinations and improving response\ngrounding.", "published": "2024-10-28 04:39:32", "link": "http://arxiv.org/abs/2410.20724v4", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ElectionSim: Massive Population Election Simulation Powered by Large\n  Language Model Driven Agents", "abstract": "The massive population election simulation aims to model the preferences of\nspecific groups in particular election scenarios. It has garnered significant\nattention for its potential to forecast real-world social trends. Traditional\nagent-based modeling (ABM) methods are constrained by their ability to\nincorporate complex individual background information and provide interactive\nprediction results. In this paper, we introduce ElectionSim, an innovative\nelection simulation framework based on large language models, designed to\nsupport accurate voter simulations and customized distributions, together with\nan interactive platform to dialogue with simulated voters. We present a\nmillion-level voter pool sampled from social media platforms to support\naccurate individual simulation. We also introduce PPE, a poll-based\npresidential election benchmark to assess the performance of our framework\nunder the U.S. presidential election scenario. Through extensive experiments\nand analyses, we demonstrate the effectiveness and robustness of our framework\nin U.S. presidential election simulations.", "published": "2024-10-28 05:25:50", "link": "http://arxiv.org/abs/2410.20746v3", "categories": ["cs.CL", "cs.CY", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Matryoshka: Learning to Drive Black-Box LLMs with LLMs", "abstract": "Despite the impressive generative abilities of black-box large language\nmodels (LLMs), their inherent opacity hinders further advancements in\ncapabilities such as reasoning, planning, and personalization. Existing works\naim to enhance LLM capabilities via domain-specific adaptation or in-context\nlearning, which require additional training on accessible model parameters, an\ninfeasible option for black-box LLMs. To address this challenge, we introduce\nMatryoshika, a lightweight white-box LLM controller that guides a large-scale\nblack-box LLM generator by decomposing complex tasks into a series of\nintermediate outputs. Specifically, we consider the black-box LLM as an\nenvironment, with Matryoshika serving as a policy to provide intermediate\nguidance through prompts for driving the black-box LLM. Matryoshika is trained\nto pivot the outputs of the black-box LLM aligning with preferences during\niterative interaction, which enables controllable multi-turn generation and\nself-improvement in optimizing intermediate guidance. Empirical evaluations on\nthree diverse tasks demonstrate that Matryoshika effectively enhances the\ncapabilities of black-box LLMs in complex, long-horizon tasks, including\nreasoning, planning, and personalization. By leveraging this pioneering\ncontroller-generator framework to mitigate dependence on model parameters,\nMatryoshika provides a transparent and practical solution for improving\nblack-box LLMs through controllable multi-turn generation using white-box LLMs.", "published": "2024-10-28 05:28:51", "link": "http://arxiv.org/abs/2410.20749v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "MrT5: Dynamic Token Merging for Efficient Byte-level Language Models", "abstract": "Models that rely on subword tokenization have significant drawbacks, such as\nsensitivity to character-level noise like spelling errors and inconsistent\ncompression rates across different languages and scripts. While character- or\nbyte-level models like ByT5 attempt to address these concerns, they have not\ngained widespread adoption -- processing raw byte streams without tokenization\nresults in significantly longer sequence lengths, making training and inference\ninefficient. This work introduces MrT5 (MergeT5), a more efficient variant of\nByT5 that integrates a token deletion mechanism in its encoder to dynamically\nshorten the input sequence length. After processing through a fixed number of\nencoder layers, a learned delete gate determines which tokens are to be removed\nand which are to be retained for subsequent layers. MrT5 effectively \"merges\"\ncritical information from deleted tokens into a more compact sequence,\nleveraging contextual information from the remaining tokens. In continued\npre-training experiments, we find that MrT5 can achieve significant gains in\ninference runtime with minimal effect on performance, as measured by\nbits-per-byte. Additionally, with multilingual training, MrT5 adapts to the\northographic characteristics of each language, learning language-specific\ncompression rates. Furthermore, MrT5 shows comparable accuracy to ByT5 on\ndownstream evaluations such as XNLI, TyDi QA, and character-level tasks while\nreducing sequence lengths by up to 75%. Our approach presents a solution to the\npractical limitations of existing byte-level models.", "published": "2024-10-28 06:14:12", "link": "http://arxiv.org/abs/2410.20771v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "KD-LoRA: A Hybrid Approach to Efficient Fine-Tuning with LoRA and\n  Knowledge Distillation", "abstract": "Large language models (LLMs) have demonstrated remarkable performance across\nvarious downstream tasks. However, the high computational and memory\nrequirements of LLMs are a major bottleneck. To address this,\nparameter-efficient fine-tuning (PEFT) methods such as low-rank adaptation\n(LoRA) have been proposed to reduce computational costs while ensuring minimal\nloss in performance. Additionally, knowledge distillation (KD) has been a\npopular choice for obtaining compact student models from teacher models. In\nthis work, we present KD-LoRA, a novel fine-tuning method that combines LoRA\nwith KD. Our results demonstrate that KD-LoRA achieves performance comparable\nto full fine-tuning (FFT) and LoRA while significantly reducing resource\nrequirements. Specifically, KD-LoRA retains 98% of LoRA's performance on the\nGLUE benchmark, while being 40% more compact. Additionally, KD-LoRA reduces GPU\nmemory usage by 30% compared to LoRA, while decreasing inference time by 30%\ncompared to both FFT and LoRA. We evaluate KD-LoRA across three encoder-only\nmodels: BERT, RoBERTa, and DeBERTaV3. Code is available at\nhttps://github.com/rambodazimi/KD-LoRA.", "published": "2024-10-28 06:38:24", "link": "http://arxiv.org/abs/2410.20777v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Graph-based Uncertainty Metrics for Long-form Language Model Outputs", "abstract": "Recent advancements in Large Language Models (LLMs) have significantly\nimproved text generation capabilities, but these systems are still known to\nhallucinate, and granular uncertainty estimation for long-form LLM generations\nremains challenging. In this work, we propose Graph Uncertainty -- which\nrepresents the relationship between LLM generations and claims within them as a\nbipartite graph and estimates the claim-level uncertainty with a family of\ngraph centrality metrics. Under this view, existing uncertainty estimation\nmethods based on the concept of self-consistency can be viewed as using degree\ncentrality as an uncertainty measure, and we show that more sophisticated\nalternatives such as closeness centrality provide consistent gains at\nclaim-level uncertainty estimation. Moreover, we present uncertainty-aware\ndecoding techniques that leverage both the graph structure and uncertainty\nestimates to improve the factuality of LLM generations by preserving only the\nmost reliable claims. Compared to existing methods, our graph-based uncertainty\nmetrics lead to an average of 6.8% relative gains on AUPRC across various\nlong-form generation settings, and our end-to-end system provides consistent\n2-4% gains in factuality over existing decoding techniques while significantly\nimproving the informativeness of generated responses.", "published": "2024-10-28 06:47:25", "link": "http://arxiv.org/abs/2410.20783v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Bridging the Gap between Expert and Language Models: Concept-guided\n  Chess Commentary Generation and Evaluation", "abstract": "Deep learning-based expert models have reached superhuman performance in\ndecision-making domains such as chess and Go. However, it is under-explored to\nexplain or comment on given decisions although it is important for model\nexplainability and human education. The outputs of expert models are accurate,\nbut yet difficult to interpret for humans. On the other hand, large language\nmodels (LLMs) can produce fluent commentary but are prone to hallucinations due\nto their limited decision-making capabilities. To bridge this gap between\nexpert models and LLMs, we focus on chess commentary as a representative task\nof explaining complex decision-making processes through language and address\nboth the generation and evaluation of commentary. We introduce Concept-guided\nChess Commentary generation (CCC) for producing commentary and GPT-based Chess\nCommentary Evaluation (GCC-Eval) for assessing it. CCC integrates the\ndecision-making strengths of expert models with the linguistic fluency of LLMs\nthrough prioritized, concept-based explanations. GCC-Eval leverages expert\nknowledge to evaluate chess commentary based on informativeness and linguistic\nquality. Experimental results, validated by both human judges and GCC-Eval,\ndemonstrate that CCC generates commentary which is accurate, informative, and\nfluent.", "published": "2024-10-28 07:59:34", "link": "http://arxiv.org/abs/2410.20811v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "DeTeCtive: Detecting AI-generated Text via Multi-Level Contrastive\n  Learning", "abstract": "Current techniques for detecting AI-generated text are largely confined to\nmanual feature crafting and supervised binary classification paradigms. These\nmethodologies typically lead to performance bottlenecks and unsatisfactory\ngeneralizability. Consequently, these methods are often inapplicable for\nout-of-distribution (OOD) data and newly emerged large language models (LLMs).\nIn this paper, we revisit the task of AI-generated text detection. We argue\nthat the key to accomplishing this task lies in distinguishing writing styles\nof different authors, rather than simply classifying the text into\nhuman-written or AI-generated text. To this end, we propose DeTeCtive, a\nmulti-task auxiliary, multi-level contrastive learning framework. DeTeCtive is\ndesigned to facilitate the learning of distinct writing styles, combined with a\ndense information retrieval pipeline for AI-generated text detection. Our\nmethod is compatible with a range of text encoders. Extensive experiments\ndemonstrate that our method enhances the ability of various text encoders in\ndetecting AI-generated text across multiple benchmarks and achieves\nstate-of-the-art results. Notably, in OOD zero-shot evaluation, our method\noutperforms existing approaches by a large margin. Moreover, we find our method\nboasts a Training-Free Incremental Adaptation (TFIA) capability towards OOD\ndata, further enhancing its efficacy in OOD detection scenarios. We will\nopen-source our code and models in hopes that our work will spark new thoughts\nin the field of AI-generated text detection, ensuring safe application of LLMs\nand enhancing compliance. Our code is available at\nhttps://github.com/heyongxin233/DeTeCtive.", "published": "2024-10-28 12:34:49", "link": "http://arxiv.org/abs/2410.20964v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Retrieval-Enhanced Mutation Mastery: Augmenting Zero-Shot Prediction of\n  Protein Language Model", "abstract": "Enzyme engineering enables the modification of wild-type proteins to meet\nindustrial and research demands by enhancing catalytic activity, stability,\nbinding affinities, and other properties. The emergence of deep learning\nmethods for protein modeling has demonstrated superior results at lower costs\ncompared to traditional approaches such as directed evolution and rational\ndesign. In mutation effect prediction, the key to pre-training deep learning\nmodels lies in accurately interpreting the complex relationships among protein\nsequence, structure, and function. This study introduces a retrieval-enhanced\nprotein language model for comprehensive analysis of native properties from\nsequence and local structural interactions, as well as evolutionary properties\nfrom retrieved homologous sequences. The state-of-the-art performance of the\nproposed ProtREM is validated on over 2 million mutants across 217 assays from\nan open benchmark (ProteinGym). We also conducted post-hoc analyses of the\nmodel's ability to improve the stability and binding affinity of a VHH\nantibody. Additionally, we designed 10 new mutants on a DNA polymerase and\nconducted wet-lab experiments to evaluate their enhanced activity at higher\ntemperatures. Both in silico and experimental evaluations confirmed that our\nmethod provides reliable predictions of mutation effects, offering an auxiliary\ntool for biologists aiming to evolve existing enzymes. The implementation is\npublicly available at https://github.com/tyang816/ProtREM.", "published": "2024-10-28 15:28:51", "link": "http://arxiv.org/abs/2410.21127v1", "categories": ["cs.CL", "cs.AI", "q-bio.QM"], "primary_category": "cs.CL"}
{"title": "Document Parsing Unveiled: Techniques, Challenges, and Prospects for\n  Structured Information Extraction", "abstract": "Document parsing is essential for converting unstructured and semi-structured\ndocuments-such as contracts, academic papers, and invoices-into structured,\nmachine-readable data. Document parsing extract reliable structured data from\nunstructured inputs, providing huge convenience for numerous applications.\nEspecially with recent achievements in Large Language Models, document parsing\nplays an indispensable role in both knowledge base construction and training\ndata generation. This survey presents a comprehensive review of the current\nstate of document parsing, covering key methodologies, from modular pipeline\nsystems to end-to-end models driven by large vision-language models. Core\ncomponents such as layout detection, content extraction (including text,\ntables, and mathematical expressions), and multi-modal data integration are\nexamined in detail. Additionally, this paper discusses the challenges faced by\nmodular document parsing systems and vision-language models in handling complex\nlayouts, integrating multiple modules, and recognizing high-density text. It\nemphasizes the importance of developing larger and more diverse datasets and\noutlines future research directions.", "published": "2024-10-28 16:11:35", "link": "http://arxiv.org/abs/2410.21169v3", "categories": ["cs.MM", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.MM"}
{"title": "Belief in the Machine: Investigating Epistemological Blind Spots of\n  Language Models", "abstract": "As language models (LMs) become integral to fields like healthcare, law, and\njournalism, their ability to differentiate between fact, belief, and knowledge\nis essential for reliable decision-making. Failure to grasp these distinctions\ncan lead to significant consequences in areas such as medical diagnosis, legal\njudgments, and dissemination of fake news. Despite this, current literature has\nlargely focused on more complex issues such as theory of mind, overlooking more\nfundamental epistemic challenges. This study systematically evaluates the\nepistemic reasoning capabilities of modern LMs, including GPT-4, Claude-3, and\nLlama-3, using a new dataset, KaBLE, consisting of 13,000 questions across 13\ntasks. Our results reveal key limitations. First, while LMs achieve 86%\naccuracy on factual scenarios, their performance drops significantly with false\nscenarios, particularly in belief-related tasks. Second, LMs struggle with\nrecognizing and affirming personal beliefs, especially when those beliefs\ncontradict factual data, which raises concerns for applications in healthcare\nand counseling, where engaging with a person's beliefs is critical. Third, we\nidentify a salient bias in how LMs process first-person versus third-person\nbeliefs, performing better on third-person tasks (80.7%) compared to\nfirst-person tasks (54.4%). Fourth, LMs lack a robust understanding of the\nfactive nature of knowledge, namely, that knowledge inherently requires truth.\nFifth, LMs rely on linguistic cues for fact-checking and sometimes bypass the\ndeeper reasoning. These findings highlight significant concerns about current\nLMs' ability to reason about truth, belief, and knowledge while emphasizing the\nneed for advancements in these areas before broad deployment in critical\nsectors.", "published": "2024-10-28 16:38:20", "link": "http://arxiv.org/abs/2410.21195v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "BongLLaMA: LLaMA for Bangla Language", "abstract": "Bangla (or \"Bengali\") is a language spoken by approximately 240 million\nnative speakers and around 300 million people worldwide. Despite being the 5th\nlargest spoken language in the world, Bangla is still a \"low-resource\"\nlanguage, and existing pretrained language models often struggle to perform\nwell on Bangla Language Processing (BLP) tasks. This work addresses this gap by\nintroducing BongLLaMA (i.e., Bangla-LLaMA), an open-source large language model\nfine-tuned exclusively on large Bangla corpora and instruction-tuning datasets.\nWe present our methodology, data augmentation techniques, fine-tuning details,\nand comprehensive benchmarking results showcasing the utility of BongLLaMA on\nBLP tasks. We believe BongLLaMA will serve as the new standard baseline for\nBangla Language Models and, thus, facilitate future benchmarking studies\nfocused on this widely-spoken yet \"low-resource\" language. All BongLLaMA models\nare available for public use at https://huggingface.co/BanglaLLM.", "published": "2024-10-28 16:44:02", "link": "http://arxiv.org/abs/2410.21200v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "HoPE: A Novel Positional Encoding Without Long-Term Decay for Enhanced\n  Context Awareness and Extrapolation", "abstract": "Many positional encodings (PEs) are designed to exhibit long-term decay,\nbased on an entrenched and long-standing inductive opinion: tokens farther away\nfrom the current position carry less relevant information. We argue that\nlong-term decay is outdated in the era of LLMs, as LLMs are now applied to\ntasks demanding precise retrieval of in-context information from arbitrary\npositions. Firstly, we present empirical analyses on various PEs, demonstrating\nthat models inherently learn attention with only a local-decay pattern while\nforming a U-shape pattern globally, contradicting the principle of long-term\ndecay. Furthermore, we conduct a detailed analysis of rotary position encoding\n(RoPE, a prevalent relative positional encoding in LLMs), and found that the\nU-shape attention is caused by some learned components, which are also the key\nfactor limiting RoPE's expressiveness and extrapolation.Inspired by these\ninsights, we propose High-frequency rotary Position Encoding (HoPE). HoPE\nreplaces the specific components in RoPE with position-independent ones,\nretaining only high-frequency signals, which also breaks the principle of\nlong-term decay in theory. HoPE achieves two major advantages: (1) Without\nconstraints imposed by long-term decay, contradictory factors that limit\nspontaneous attention optimization and model extrapolation performance are\nremoved. (2) Components representing positions and semantics are are optimized.\nThese enhances model's context awareness and extrapolation, as validated by\nextensive experiments.", "published": "2024-10-28 17:01:52", "link": "http://arxiv.org/abs/2410.21216v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Flaming-hot Initiation with Regular Execution Sampling for Large\n  Language Models", "abstract": "Since the release of ChatGPT, large language models (LLMs) have demonstrated\nremarkable capabilities across various domains. A key challenge in developing\nthese general capabilities is efficiently sourcing diverse, high-quality data.\nThis becomes especially critical in reasoning-related tasks with sandbox\ncheckers, such as math or code, where the goal is to generate correct solutions\nto specific problems with higher probability. In this work, we introduce\nFlaming-hot Initiation with Regular Execution (FIRE) sampling, a simple yet\nhighly effective method to efficiently find good responses. Our empirical\nfindings show that FIRE sampling enhances inference-time generation quality and\nalso benefits training in the alignment stage. Furthermore, we explore how FIRE\nsampling improves performance by promoting diversity and analyze the impact of\nemploying FIRE at different positions within a response.", "published": "2024-10-28 17:30:01", "link": "http://arxiv.org/abs/2410.21236v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Zero-Shot Dense Retrieval with Embeddings from Relevance Feedback", "abstract": "Building effective dense retrieval systems remains difficult when relevance\nsupervision is not available. Recent work has looked to overcome this challenge\nby using a Large Language Model (LLM) to generate hypothetical documents that\ncan be used to find the closest real document. However, this approach relies\nsolely on the LLM to have domain-specific knowledge relevant to the query,\nwhich may not be practical. Furthermore, generating hypothetical documents can\nbe inefficient as it requires the LLM to generate a large number of tokens for\neach query. To address these challenges, we introduce Real Document Embeddings\nfrom Relevance Feedback (ReDE-RF). Inspired by relevance feedback, ReDE-RF\nproposes to re-frame hypothetical document generation as a relevance estimation\ntask, using an LLM to select which documents should be used for nearest\nneighbor search. Through this re-framing, the LLM no longer needs\ndomain-specific knowledge but only needs to judge what is relevant.\nAdditionally, relevance estimation only requires the LLM to output a single\ntoken, thereby improving search latency. Our experiments show that ReDE-RF\nconsistently surpasses state-of-the-art zero-shot dense retrieval methods\nacross a wide range of low-resource retrieval datasets while also making\nsignificant improvements in latency per-query.", "published": "2024-10-28 17:40:40", "link": "http://arxiv.org/abs/2410.21242v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "FinTeamExperts: Role Specialized MOEs For Financial Analysis", "abstract": "Large Language Models (LLMs), such as ChatGPT, Phi3 and Llama-3, are leading\na significant leap in AI, as they can generalize knowledge from their training\nto new tasks without fine-tuning. However, their application in the financial\ndomain remains relatively limited. The financial field is inherently complex,\nrequiring a deep understanding across various perspectives, from macro, micro\neconomic trend to quantitative analysis. Motivated by this complexity, a\nmixture of expert LLMs tailored to specific financial domains could offer a\nmore comprehensive understanding for intricate financial tasks. In this paper,\nwe present the FinTeamExperts, a role-specialized LLM framework structured as a\nMixture of Experts (MOEs) for financial analysis. The framework simulates a\ncollaborative team setting by training each model to specialize in distinct\nroles: Macro Analysts, Micro analysts, and Quantitative Analysts. This\nrole-specific specialization enhances the model's ability to integrate their\ndomain-specific expertise. We achieve this by training three 8-billion\nparameter models on different corpus, each dedicated to excelling in specific\nfinance-related roles. We then instruct-tune FinTeamExperts on downstream tasks\nto align with practical financial tasks. The experimental results show that\nFinTeamExperts outperform all models of the same size and larger on three out\nof four datasets. On the fourth dataset, which presents a more complex task,\nFinTeamExperts still surpass all models of the same size. This highlights the\nsuccess of our role-based specialization approach and the continued training\napproach for FinTeamExperts.", "published": "2024-10-28 00:40:55", "link": "http://arxiv.org/abs/2410.21338v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TransformLLM: Adapting Large Language Models via LLM-Transformed Reading\n  Comprehension Text", "abstract": "Large Language Models (LLMs) have shown promise in highly-specialized\ndomains, however challenges are still present in aspects of accuracy and costs.\nThese limitations restrict the usage of existing models in domain-specific\ntasks. While fine-tuning pre-trained models have shown promising results, this\nprocess can be computationally expensive and require massive datasets of the\nspecialized application in hand. In this work, we bridge that gap. We have\ndeveloped Phi-2-Legal and Mistral-Legal-7B, which are language models\nspecifically designed for legal applications. These models are based on Phi-2\nand Mistral-7B-v0.1, and have gone through continued pre-training with over 500\nmillion tokens of legal texts. Our innovative approach significantly improves\ncapabilities in legal tasks by using Large Language Models (LLMs) to convert\nraw training data into reading comprehension text. Our legal LLMs have\ndemonstrated superior performance in legal benchmarks, even outperforming\nmodels trained on much larger datasets with more resources. This work\nemphasizes the effectiveness of continued pre-training on domain-specific\ntexts, while using affordable LLMs for data conversion, which gives these\nmodels domain expertise while retaining general language understanding\ncapabilities. While this work uses the legal domain as a test case, our method\ncan be scaled and applied to any pre-training dataset, resulting in significant\nimprovements across different tasks. These findings underscore the potential of\ndomain-adaptive pre-training and reading comprehension for the development of\nhighly effective domain-specific language models.", "published": "2024-10-28 19:32:18", "link": "http://arxiv.org/abs/2410.21479v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AiSciVision: A Framework for Specializing Large Multimodal Models in\n  Scientific Image Classification", "abstract": "Trust and interpretability are crucial for the use of Artificial Intelligence\n(AI) in scientific research, but current models often operate as black boxes\noffering limited transparency and justifications for their outputs. We\nintroduce AiSciVision, a framework that specializes Large Multimodal Models\n(LMMs) into interactive research partners and classification models for image\nclassification tasks in niche scientific domains. Our framework uses two key\ncomponents: (1) Visual Retrieval-Augmented Generation (VisRAG) and (2)\ndomain-specific tools utilized in an agentic workflow. To classify a target\nimage, AiSciVision first retrieves the most similar positive and negative\nlabeled images as context for the LMM. Then the LMM agent actively selects and\napplies tools to manipulate and inspect the target image over multiple rounds,\nrefining its analysis before making a final prediction. These VisRAG and\ntooling components are designed to mirror the processes of domain experts, as\nhumans often compare new data to similar examples and use specialized tools to\nmanipulate and inspect images before arriving at a conclusion. Each inference\nproduces both a prediction and a natural language transcript detailing the\nreasoning and tool usage that led to the prediction. We evaluate AiSciVision on\nthree real-world scientific image classification datasets: detecting the\npresence of aquaculture ponds, diseased eelgrass, and solar panels. Across\nthese datasets, our method outperforms fully supervised models in low and\nfull-labeled data settings. AiSciVision is actively deployed in real-world use,\nspecifically for aquaculture research, through a dedicated web application that\ndisplays and allows the expert users to converse with the transcripts. This\nwork represents a crucial step toward AI systems that are both interpretable\nand effective, advancing their use in scientific research and scientific\ndiscovery.", "published": "2024-10-28 19:35:47", "link": "http://arxiv.org/abs/2410.21480v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Can Large Language Models Act as Symbolic Reasoners?", "abstract": "The performance of Large language models (LLMs) across a broad range of\ndomains has been impressive but have been critiqued as not being able to reason\nabout their process and conclusions derived. This is to explain the conclusions\ndraw, and also for determining a plan or strategy for their approach. This\npaper explores the current research in investigating symbolic reasoning and\nLLMs, and whether an LLM can inherently provide some form of reasoning or\nwhether supporting components are necessary, and, if there is evidence for a\nreasoning capability, is this evident in a specific domain or is this a general\ncapability? In addition, this paper aims to identify the current research gaps\nand future trends of LLM explainability, presenting a review of the literature,\nidentifying current research into this topic and suggests areas for future\nwork.", "published": "2024-10-28 20:01:50", "link": "http://arxiv.org/abs/2410.21490v1", "categories": ["cs.CL", "cs.AI", "cs.ET"], "primary_category": "cs.CL"}
{"title": "L3Ms -- Lagrange Large Language Models", "abstract": "Supervised fine-tuning (SFT) and alignment of large language models (LLMs)\nare key steps in providing a good user experience. However, the concept of an\nappropriate alignment is inherently application-dependent, and current methods\noften rely on heuristic choices to drive optimization. In this work, we\nformulate SFT and alignment as a constrained optimization problem: the LLM is\nfine-tuned on a task while being required to meet application-specific\nrequirements, without resorting to heuristics. To solve this, we propose\nLagrange Large Language Models (L3Ms), which employ logarithmic barriers to\nenforce the constraints. This approach allows for the customization of L3Ms\nacross diverse applications while avoiding heuristic-driven processes. We\nexperimentally demonstrate the versatility and efficacy of L3Ms in achieving\ntailored alignments for various applications.", "published": "2024-10-28 21:02:13", "link": "http://arxiv.org/abs/2410.21533v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "MultiTok: Variable-Length Tokenization for Efficient LLMs Adapted from\n  LZW Compression", "abstract": "Large language models have drastically changed the prospects of AI by\nintroducing technologies for more complex natural language processing. However,\ncurrent methodologies to train such LLMs require extensive resources including\nbut not limited to large amounts of data, expensive machinery, and lengthy\ntraining. To solve this problem, this paper proposes a new tokenization method\ninspired by universal Lempel-Ziv-Welch data compression that compresses\nrepetitive phrases into multi-word tokens. With MultiTok as a new tokenizing\ntool, we show that language models are able to be trained notably more\nefficiently while offering a similar accuracy on more succinct and compressed\ntraining data. In fact, our results demonstrate that MultiTok achieves a\ncomparable performance to the BERT and GPT-2 standards as both a stand-alone\ntokenizer and an add-on to existing tokenizers while also providing close to\n2.5x faster training with more than 30% less training data.", "published": "2024-10-28 21:24:51", "link": "http://arxiv.org/abs/2410.21548v2", "categories": ["cs.CL", "cs.IT", "cs.LG", "math.IT"], "primary_category": "cs.CL"}
{"title": "Can Large Language Models Replace Data Scientists in Biomedical\n  Research?", "abstract": "Data science plays a critical role in biomedical research, but it requires\nprofessionals with expertise in coding and medical data analysis. Large\nlanguage models (LLMs) have shown great potential in supporting medical tasks\nand performing well in general coding tests. However, existing evaluations fail\nto assess their capability in biomedical data science, particularly in handling\ndiverse data types such as genomics and clinical datasets. To address this gap,\nwe developed a benchmark of data science coding tasks derived from the analyses\nof 39 published studies. This benchmark comprises 293 coding tasks (128 in\nPython and 165 in R) performed on real-world TCGA-type genomics and clinical\ndata. Our findings reveal that the vanilla prompting of LLMs yields suboptimal\nperformances due to drawbacks in following input instructions, understanding\ntarget data, and adhering to standard analysis practices. Next, we benchmarked\nsix cutting-edge LLMs and advanced adaptation methods, finding two methods to\nbe particularly effective: chain-of-thought prompting, which provides a\nstep-by-step plan for data analysis, which led to a 21% code accuracy\nimprovement (56.6% versus 35.3%); and self-reflection, enabling LLMs to refine\nthe buggy code iteratively, yielding an 11% code accuracy improvement (45.5%\nversus 34.3%). Building on these insights, we developed a platform that\nintegrates LLMs into the data science workflow for medical professionals. In a\nuser study with five medical professionals, we found that while LLMs cannot\nfully automate programming tasks, they significantly streamline the programming\nprocess. We found that 80% of their submitted code solutions were incorporated\nfrom LLM-generated code, with up to 96% reuse in some cases. Our analysis\nhighlights the potential of LLMs to enhance data science efficiency in\nbiomedical research when integrated into expert workflows.", "published": "2024-10-28 22:48:06", "link": "http://arxiv.org/abs/2410.21591v2", "categories": ["cs.AI", "cs.CL", "q-bio.GN", "q-bio.QM"], "primary_category": "cs.AI"}
{"title": "Reducing the Scope of Language Models with Circuit Breakers", "abstract": "Language models are now deployed in a wide variety of user-facing\napplications, often for specific purposes like answering questions about\ndocumentation or acting as coding assistants. As these models are intended for\nparticular purposes, they should not be able to answer irrelevant queries like\nrequests for poetry or questions about physics, or even worse, queries that can\nonly be answered by humans like sensitive company policies. Instead we would\nlike them to only answer queries corresponding to desired behavior and refuse\nall other requests, which we refer to as scoping. We find that, despite the use\nof system prompts, two representative language models can be poorly scoped and\nrespond to queries they should not be addressing. We then conduct a\ncomprehensive empirical evaluation of methods which could be used for scoping\nthe behavior of language models. Among many other results, we show that a\nrecently-proposed method for general alignment, Circuit Breakers (CB), can be\nadapted to scope language models to very specific tasks like sentiment analysis\nor summarization or even tasks with finer-grained scoping (e.g. summarizing\nonly news articles). When compared to standard methods like fine-tuning or\npreference learning, CB is more robust both for out of distribution tasks, and\nto adversarial prompting techniques. We also show that layering SFT and CB\ntogether often results in the best of both worlds: improved performance only on\nrelevant queries, while rejecting irrelevant ones.", "published": "2024-10-28 23:06:57", "link": "http://arxiv.org/abs/2410.21597v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Actor-Critic Approach to Boosting Text-to-SQL Large Language Model", "abstract": "Text-To-SQL (T2S) conversion based on large language models (LLMs) has found\na wide range of applications, by leveraging the capabilities of LLMs in\ninterpreting the query intent expressed in natural language. Existing research\nfocuses on suitable representations for data schema and/or questions,\ntask-specific instructions and representative examples, and complicated\ninference pipelines. All these methods are empirical and task specific, without\na theoretical bound on performance. In this paper, we propose a simple,\ngeneral, and performance guaranteed T2S enhancement approach called\nActor-Critic (AC). Specifically, we design two roles using the same LLM: an\nActor to produce SQL queries and a Critic to evaluate the produced SQL. If the\nCritic believes the produced SQL is wrong, it notifies the Actor to reproduce\nthe SQL and perform evaluation again. By this simple iterative process,\nexpected performance can be derived in theory. We conducted extensive\nexperiments on the Spider and related datasets with eleven LLMs, and\ndemonstrated that the Actor-Critic method consistently improves the performance\nof T2S, thus serving as a general enhancement approach for T2S conversion.", "published": "2024-10-28 15:22:35", "link": "http://arxiv.org/abs/2410.22082v1", "categories": ["cs.DB", "cs.CL", "cs.HC"], "primary_category": "cs.DB"}
{"title": "Survey of User Interface Design and Interaction Techniques in Generative\n  AI Applications", "abstract": "The applications of generative AI have become extremely impressive, and the\ninterplay between users and AI is even more so. Current human-AI interaction\nliterature has taken a broad look at how humans interact with generative AI,\nbut it lacks specificity regarding the user interface designs and patterns used\nto create these applications. Therefore, we present a survey that\ncomprehensively presents taxonomies of how a human interacts with AI and the\nuser interaction patterns designed to meet the needs of a variety of relevant\nuse cases. We focus primarily on user-guided interactions, surveying\ninteractions that are initiated by the user and do not include any implicit\nsignals given by the user. With this survey, we aim to create a compendium of\ndifferent user-interaction patterns that can be used as a reference for\ndesigners and developers alike. In doing so, we also strive to lower the entry\nbarrier for those attempting to learn more about the design of generative AI\napplications.", "published": "2024-10-28 23:10:06", "link": "http://arxiv.org/abs/2410.22370v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.HC"}
{"title": "Systematically Analyzing Prompt Injection Vulnerabilities in Diverse LLM\n  Architectures", "abstract": "This study systematically analyzes the vulnerability of 36 large language\nmodels (LLMs) to various prompt injection attacks, a technique that leverages\ncarefully crafted prompts to elicit malicious LLM behavior. Across 144 prompt\ninjection tests, we observed a strong correlation between model parameters and\nvulnerability, with statistical analyses, such as logistic regression and\nrandom forest feature analysis, indicating that parameter size and architecture\nsignificantly influence susceptibility. Results revealed that 56 percent of\ntests led to successful prompt injections, emphasizing widespread vulnerability\nacross various parameter sizes, with clustering analysis identifying distinct\nvulnerability profiles associated with specific model configurations.\nAdditionally, our analysis uncovered correlations between certain prompt\ninjection techniques, suggesting potential overlaps in vulnerabilities. These\nfindings underscore the urgent need for robust, multi-layered defenses in LLMs\ndeployed across critical infrastructure and sensitive industries. Successful\nprompt injection attacks could result in severe consequences, including data\nbreaches, unauthorized access, or misinformation. Future research should\nexplore multilingual and multi-step defenses alongside adaptive mitigation\nstrategies to strengthen LLM security in diverse, real-world environments.", "published": "2024-10-28 18:55:21", "link": "http://arxiv.org/abs/2410.23308v1", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Device-Directed Speech Detection for Follow-up Conversations Using Large\n  Language Models", "abstract": "Follow-up conversations with virtual assistants (VAs) enable a user to\nseamlessly interact with a VA without the need to repeatedly invoke it using a\nkeyword (after the first query). Therefore, accurate Device-directed Speech\nDetection (DDSD) from the follow-up queries is critical for enabling\nnaturalistic user experience. To this end, we explore the notion of Large\nLanguage Models (LLMs) and model the first query when making inference about\nthe follow-ups (based on the ASR-decoded text), via prompting of a pretrained\nLLM, or by adapting a binary classifier on top of the LLM. In doing so, we also\nexploit the ASR uncertainty when designing the LLM prompts. We show on the\nreal-world dataset of follow-up conversations that this approach yields large\ngains (20-40% reduction in false alarms at 10% fixed false rejects) due to the\njoint modeling of the previous speech context and ASR uncertainty, compared to\nwhen follow-ups are modeled alone.", "published": "2024-10-28 19:43:43", "link": "http://arxiv.org/abs/2411.00023v2", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "CycleResearcher: Improving Automated Research via Automated Review", "abstract": "The automation of scientific discovery has been a long-standing goal within\nthe research community, driven by the potential to accelerate knowledge\ncreation. While significant progress has been made using commercial large\nlanguage models (LLMs) as research assistants or idea generators, the\npossibility of automating the entire research process with open-source LLMs\nremains largely unexplored. This paper explores the feasibility of using\nopen-source post-trained LLMs as autonomous agents capable of performing the\nfull cycle of automated research and review, from literature review and\nmanuscript preparation to peer review and paper refinement. Our iterative\npreference training framework consists of CycleResearcher, which conducts\nresearch tasks, and CycleReviewer, which simulates the peer review process,\nproviding iterative feedback via reinforcement learning. To train these models,\nwe develop two new datasets, Review-5k and Research-14k, reflecting real-world\nmachine learning research and peer review dynamics. Our results demonstrate\nthat CycleReviewer achieves promising performance with a 26.89\\% reduction in\nmean absolute error (MAE) compared to individual human reviewers in predicting\npaper scores, indicating the potential of LLMs to effectively assist\nexpert-level research evaluation. In research, the papers generated by the\nCycleResearcher model achieved a score of 5.36 in simulated peer reviews,\nshowing some competitiveness in terms of simulated review scores compared to\nthe preprint level of 5.24 from human experts, while still having room for\nimprovement compared to the accepted paper level of 5.69. This work represents\na significant step toward fully automated scientific inquiry, providing ethical\nsafeguards and exploring AI-driven research capabilities. The code, dataset and\nmodel weight are released at https://wengsyx.github.io/Researcher/.", "published": "2024-10-28 08:10:21", "link": "http://arxiv.org/abs/2411.00816v3", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AutoGLM: Autonomous Foundation Agents for GUIs", "abstract": "We present AutoGLM, a new series in the ChatGLM family, designed to serve as\nfoundation agents for autonomous control of digital devices through Graphical\nUser Interfaces (GUIs). While foundation models excel at acquiring human\nknowledge, they often struggle with decision-making in dynamic real-world\nenvironments, limiting their progress toward artificial general intelligence.\nThis limitation underscores the importance of developing foundation agents\ncapable of learning through autonomous environmental interactions by\nreinforcing existing models. Focusing on Web Browser and Phone as\nrepresentative GUI scenarios, we have developed AutoGLM as a practical\nfoundation agent system for real-world GUI interactions. Our approach\nintegrates a comprehensive suite of techniques and infrastructures to create\ndeployable agent systems suitable for user delivery. Through this development,\nwe have derived two key insights: First, the design of an appropriate\n\"intermediate interface\" for GUI control is crucial, enabling the separation of\nplanning and grounding behaviors, which require distinct optimization for\nflexibility and accuracy respectively. Second, we have developed a novel\nprogressive training framework that enables self-evolving online curriculum\nreinforcement learning for AutoGLM. Our evaluations demonstrate AutoGLM's\neffectiveness across multiple domains. For web browsing, AutoGLM achieves a\n55.2% success rate on VAB-WebArena-Lite (improving to 59.1% with a second\nattempt) and 96.2% on OpenTable evaluation tasks. In Android device control,\nAutoGLM attains a 36.2% success rate on AndroidLab (VAB-Mobile) and 89.7% on\ncommon tasks in popular Chinese APPs.", "published": "2024-10-28 17:05:10", "link": "http://arxiv.org/abs/2411.00820v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.HC"}
{"title": "Can Machines Think Like Humans? A Behavioral Evaluation of LLM-Agents in\n  Dictator Games", "abstract": "As Large Language Model (LLM)-based agents increasingly undertake real-world\ntasks and engage with human society, how well do we understand their behaviors?\nWe (1) investigate how LLM agents' prosocial behaviors -- a fundamental social\nnorm -- can be induced by different personas and benchmarked against human\nbehaviors; and (2) introduce a behavioral and social science approach to\nevaluate LLM agents' decision-making. We explored how different personas and\nexperimental framings affect these AI agents' altruistic behavior in dictator\ngames and compared their behaviors within the same LLM family, across various\nfamilies, and with human behaviors. The findings reveal substantial variations\nand inconsistencies among LLMs and notable differences compared to human\nbehaviors. Merely assigning a human-like identity to LLMs does not produce\nhuman-like behaviors. Despite being trained on extensive human-generated data,\nthese AI agents are unable to capture the internal processes of human\ndecision-making. Their alignment with human is highly variable and dependent on\nspecific model architectures and prompt formulations; even worse, such\ndependence does not follow a clear pattern. LLMs can be useful task-specific\ntools but are not yet intelligent human-like agents.", "published": "2024-10-28 17:47:41", "link": "http://arxiv.org/abs/2410.21359v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "econ.GN", "q-fin.EC"], "primary_category": "cs.CL"}
{"title": "Simultaneous Diarization and Separation of Meetings through the\n  Integration of Statistical Mixture Models", "abstract": "We propose an approach for simultaneous diarization and separation of meeting\ndata. It consists of a complex Angular Central Gaussian Mixture Model (cACGMM)\nfor speech source separation, and a von-Mises-Fisher Mixture Model (VMFMM) for\ndiarization in a joint statistical framework. Through the integration, both\nspatial and spectral information are exploited for diarization and separation.\nWe also develop a method for counting the number of active speakers in a\nsegment of a meeting to support block-wise processing. While the total number\nof speakers in a meeting may be known, it is usually not known on a per-segment\nlevel. With the proposed speaker counting, joint diarization and source\nseparation can be done segment-by-segment, and the permutation problem across\nsegments is solved, thus allowing for block-online processing in the future.\nExperimental results on the LibriCSS meeting corpus show that the integrated\napproach outperforms a cascaded approach of diarization and speech enhancement\nin terms of WER, both on a per-segment and on a per-meeting level.", "published": "2024-10-28 18:59:54", "link": "http://arxiv.org/abs/2410.21455v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Data-Efficient Low-Complexity Acoustic Scene Classification via\n  Distilling and Progressive Pruning", "abstract": "The goal of the acoustic scene classification (ASC) task is to classify\nrecordings into one of the predefined acoustic scene classes. However, in\nreal-world scenarios, ASC systems often encounter challenges such as recording\ndevice mismatch, low-complexity constraints, and the limited availability of\nlabeled data. To alleviate these issues, in this paper, a data-efficient and\nlow-complexity ASC system is built with a new model architecture and better\ntraining strategies. Specifically, we firstly design a new low-complexity\narchitecture named Rep-Mobile by integrating multi-convolution branches which\ncan be reparameterized at inference. Compared to other models, it achieves\nbetter performance and less computational complexity. Then we apply the\nknowledge distillation strategy and provide a comparison of the data efficiency\nof the teacher model with different architectures. Finally, we propose a\nprogressive pruning strategy, which involves pruning the model multiple times\nin small amounts, resulting in better performance compared to a single step\npruning. Experiments are conducted on the TAU dataset. With Rep-Mobile and\nthese training strategies, our proposed ASC system achieves the\nstate-of-the-art (SOTA) results so far, while also winning the first place with\na significant advantage over others in the DCASE2024 Challenge.", "published": "2024-10-28 06:31:20", "link": "http://arxiv.org/abs/2410.20775v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ST-ITO: Controlling Audio Effects for Style Transfer with Inference-Time\n  Optimization", "abstract": "Audio production style transfer is the task of processing an input to impart\nstylistic elements from a reference recording. Existing approaches often train\na neural network to estimate control parameters for a set of audio effects.\nHowever, these approaches are limited in that they can only control a fixed set\nof effects, where the effects must be differentiable or otherwise employ\nspecialized training techniques. In this work, we introduce ST-ITO, Style\nTransfer with Inference-Time Optimization, an approach that instead searches\nthe parameter space of an audio effect chain at inference. This method enables\ncontrol of arbitrary audio effect chains, including unseen and\nnon-differentiable effects. Our approach employs a learned metric of audio\nproduction style, which we train through a simple and scalable self-supervised\npretraining strategy, along with a gradient-free optimizer. Due to the limited\nexisting evaluation methods for audio production style transfer, we introduce a\nmulti-part benchmark to evaluate audio production style metrics and style\ntransfer systems. This evaluation demonstrates that our audio representation\nbetter captures attributes related to audio production and enables expressive\nstyle transfer via control of arbitrary audio effects.", "published": "2024-10-28 17:24:37", "link": "http://arxiv.org/abs/2410.21233v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Enhancing TTS Stability in Hebrew using Discrete Semantic Units", "abstract": "This study introduces a refined approach to Text-to-Speech (TTS) generation\nthat significantly enhances sampling stability across languages, with a\nparticular focus on Hebrew. By leveraging discrete semantic units with higher\nphonetic correlation obtained from a self-supervised model, our method\naddresses the inherent instability often encountered in TTS systems, especially\nthose dealing with non-diacriticized scripts like Hebrew. Utilizing HuBERT\ncodes, our model generates discrete representations that are optimized for TTS\ntasks, thereby reducing the dependency on diacritic-based text processing. This\nadvancement not only simplifies the language modeling process but also improves\nthe robustness and shows controllability of the speech output due to\ndisentenglement properties of the semantic units. The inclusion of a speaker\nembedding in the vocoder further aids in capturing the unique vocal\ncharacteristics of the speaker, contributing to the naturalness of the\nsynthesized speech. Our experimental results demonstrate that this approach not\nonly maintains high performance in Hebrew but also shows adaptability to\nEnglish, underscoring its effectiveness in enhancing stability in TTS systems\nuniversally. Our method, named LOTHM (Language of The Hebrew Man), outperforms\nexisting methods in terms of stability while achieving naturalness and speaker\nsimilarity on par with previous methods, making it a compelling choice for\nfuture speech synthesis applications. Samples can be found in our page\npages.cs.huji.ac.il/adiyoss-lab/LoTHM .", "published": "2024-10-28 20:16:30", "link": "http://arxiv.org/abs/2410.21502v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Mitigating Unauthorized Speech Synthesis for Voice Protection", "abstract": "With just a few speech samples, it is possible to perfectly replicate a\nspeaker's voice in recent years, while malicious voice exploitation (e.g.,\ntelecom fraud for illegal financial gain) has brought huge hazards in our daily\nlives. Therefore, it is crucial to protect publicly accessible speech data that\ncontains sensitive information, such as personal voiceprints. Most previous\ndefense methods have focused on spoofing speaker verification systems in timbre\nsimilarity but the synthesized deepfake speech is still of high quality. In\nresponse to the rising hazards, we devise an effective, transferable, and\nrobust proactive protection technology named Pivotal Objective Perturbation\n(POP) that applies imperceptible error-minimizing noises on original speech\nsamples to prevent them from being effectively learned for text-to-speech (TTS)\nsynthesis models so that high-quality deepfake speeches cannot be generated. We\nconduct extensive experiments on state-of-the-art (SOTA) TTS models utilizing\nobjective and subjective metrics to comprehensively evaluate our proposed\nmethod. The experimental results demonstrate outstanding effectiveness and\ntransferability across various models. Compared to the speech unclarity score\nof 21.94% from voice synthesizers trained on samples without protection,\nPOP-protected samples significantly increase it to 127.31%. Moreover, our\nmethod shows robustness against noise reduction and data augmentation\ntechniques, thereby greatly reducing potential hazards.", "published": "2024-10-28 05:16:37", "link": "http://arxiv.org/abs/2410.20742v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "An Ensemble Approach to Music Source Separation: A Comparative Analysis\n  of Conventional and Hierarchical Stem Separation", "abstract": "Music source separation (MSS) is a task that involves isolating individual\nsound sources, or stems, from mixed audio signals. This paper presents an\nensemble approach to MSS, combining several state-of-the-art architectures to\nachieve superior separation performance across traditional Vocal, Drum, and\nBass (VDB) stems, as well as expanding into second-level hierarchical\nseparation for sub-stems like kick, snare, lead vocals, and background vocals.\nOur method addresses the limitations of relying on a single model by utilising\nthe complementary strengths of various models, leading to more balanced results\nacross stems. For stem selection, we used the harmonic mean of Signal-to-Noise\nRatio (SNR) and Signal-to-Distortion Ratio (SDR), ensuring that extreme values\ndo not skew the results and that both metrics are weighted effectively. In\naddition to consistently high performance across the VDB stems, we also\nexplored second-level hierarchical separation, revealing important insights\ninto the complexities of MSS and how factors like genre and instrumentation can\ninfluence model performance. While the second-level separation results show\nroom for improvement, the ability to isolate sub-stems marks a significant\nadvancement. Our findings pave the way for further research in MSS,\nparticularly in expanding model capabilities beyond VDB and improving niche\nstem separations such as guitar and piano.", "published": "2024-10-28 06:18:12", "link": "http://arxiv.org/abs/2410.20773v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Atrial Fibrillation Detection System via Acoustic Sensing for Mobile\n  Phones", "abstract": "Atrial fibrillation (AF) is characterized by irregular electrical impulses\noriginating in the atria, which can lead to severe complications and even\ndeath. Due to the intermittent nature of the AF, early and timely monitoring of\nAF is critical for patients to prevent further exacerbation of the condition.\nAlthough ambulatory ECG Holter monitors provide accurate monitoring, the high\ncost of these devices hinders their wider adoption. Current mobile-based AF\ndetection systems offer a portable solution, however, these systems have\nvarious applicability issues such as being easily affected by environmental\nfactors and requiring significant user effort. To overcome the above\nlimitations, we present MobileAF, a novel smartphone-based AF detection system\nusing speakers and microphones. In order to capture minute cardiac activities,\nwe propose a multi-channel pulse wave probing method. In addition, we enhance\nthe signal quality by introducing a three-stage pulse wave purification\npipeline. What's more, a ResNet-based network model is built to implement\naccurate and reliable AF detection. We collect data from 23 participants\nutilizing our data collection application on the smartphone. Extensive\nexperimental results demonstrate the superior performance of our system, with\n97.9% accuracy, 96.8% precision, 97.2% recall, 98.3% specificity, and 97.0% F1\nscore.", "published": "2024-10-28 09:14:14", "link": "http://arxiv.org/abs/2410.20852v1", "categories": ["cs.SD", "cs.CE", "eess.AS", "q-bio.QM"], "primary_category": "cs.SD"}
{"title": "SepMamba: State-space models for speaker separation using Mamba", "abstract": "Deep learning-based single-channel speaker separation has improved\nsignificantly in recent years largely due to the introduction of the\ntransformer-based attention mechanism. However, these improvements come at the\nexpense of intense computational demands, precluding their use in many\npractical applications. As a computationally efficient alternative with similar\nmodeling capabilities, Mamba was recently introduced. We propose SepMamba, a\nU-Net-based architecture composed primarily of bidirectional Mamba layers. We\nfind that our approach outperforms similarly-sized prominent models - including\ntransformer-based models - on the WSJ0 2-speaker dataset while enjoying a\nsignificant reduction in computational cost, memory usage, and forward pass\ntime. We additionally report strong results for causal variants of SepMamba.\nOur approach provides a computationally favorable alternative to\ntransformer-based architectures for deep speech separation.", "published": "2024-10-28 13:20:53", "link": "http://arxiv.org/abs/2410.20997v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "OmniSep: Unified Omni-Modality Sound Separation with Query-Mixup", "abstract": "The scaling up has brought tremendous success in the fields of vision and\nlanguage in recent years. When it comes to audio, however, researchers\nencounter a major challenge in scaling up the training data, as most natural\naudio contains diverse interfering signals. To address this limitation, we\nintroduce Omni-modal Sound Separation (OmniSep), a novel framework capable of\nisolating clean soundtracks based on omni-modal queries, encompassing both\nsingle-modal and multi-modal composed queries. Specifically, we introduce the\nQuery-Mixup strategy, which blends query features from different modalities\nduring training. This enables OmniSep to optimize multiple modalities\nconcurrently, effectively bringing all modalities under a unified framework for\nsound separation. We further enhance this flexibility by allowing queries to\ninfluence sound separation positively or negatively, facilitating the retention\nor removal of specific sounds as desired. Finally, OmniSep employs a\nretrieval-augmented approach known as Query-Aug, which enables open-vocabulary\nsound separation. Experimental evaluations on MUSIC, VGGSOUND-CLEAN+, and\nMUSIC-CLEAN+ datasets demonstrate effectiveness of OmniSep, achieving\nstate-of-the-art performance in text-, image-, and audio-queried sound\nseparation tasks. For samples and further information, please visit the demo\npage at \\url{https://omnisep.github.io/}.", "published": "2024-10-28 17:58:15", "link": "http://arxiv.org/abs/2410.21269v1", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Knowledge Distillation for Real-Time Classification of Early Media in\n  Voice Communications", "abstract": "This paper investigates the industrial setting of real-time classification of\nearly media exchanged during the initialization phase of voice calls. We\nexplore the application of state-of-the-art audio tagging models and highlight\nsome limitations when applied to the classification of early media. While most\nexisting approaches leverage convolutional neural networks, we propose a novel\napproach for low-resource requirements based on gradient-boosted trees. Our\napproach not only demonstrates a substantial improvement in runtime\nperformance, but also exhibits a comparable accuracy. We show that leveraging\nknowledge distillation and class aggregation techniques to train a simpler and\nsmaller model accelerates the classification of early media in voice calls. We\nprovide a detailed analysis of the results on a proprietary and publicly\navailable dataset, regarding accuracy and runtime performance. We additionally\nreport a case study of the achieved performance improvements at a regional data\ncenter in India.", "published": "2024-10-28 19:32:17", "link": "http://arxiv.org/abs/2410.21478v1", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS", "I.2.0"], "primary_category": "cs.SD"}
{"title": "A Novel Score-CAM based Denoiser for Spectrographic Signature Extraction\n  without Ground Truth", "abstract": "Sonar based audio classification techniques are a growing area of research in\nthe field of underwater acoustics. Usually, underwater noise picked up by\npassive sonar transducers contains all types of signals that travel through the\nocean and is transformed into spectrographic images. As a result, the\ncorresponding spectrograms intended to display the temporal-frequency data of a\ncertain object often include the tonal regions of abundant extraneous noise\nthat can effectively interfere with a 'contact'. So, a majority of\nspectrographic samples extracted from underwater audio signals are rendered\nunusable due to their clutter and lack the required indistinguishability\nbetween different objects. With limited clean true data for supervised\ntraining, creating classification models for these audio signals is severely\nbottlenecked.\n  This paper derives several new techniques to combat this problem by\ndeveloping a novel Score-CAM based denoiser to extract an object's signature\nfrom noisy spectrographic data without being given any ground truth data. In\nparticular, this paper proposes a novel generative adversarial network\narchitecture for learning and producing spectrographic training data in similar\ndistributions to low-feature spectrogram inputs. In addition, this paper also a\ngeneralizable class activation mapping based denoiser for different\ndistributions of acoustic data, even real-world data distributions. Utilizing\nthese novel architectures and proposed denoising techniques, these experiments\ndemonstrate state-of-the-art noise reduction accuracy and improved\nclassification accuracy than current audio classification standards. As such,\nthis approach has applications not only to audio data but for countless data\ndistributions used all around the world for machine learning.", "published": "2024-10-28 21:40:46", "link": "http://arxiv.org/abs/2410.21557v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio Classification of Low Feature Spectrograms Utilizing Convolutional\n  Neural Networks", "abstract": "Modern day audio signal classification techniques lack the ability to\nclassify low feature audio signals in the form of spectrographic temporal\nfrequency data representations. Additionally, currently utilized techniques\nrely on full diverse data sets that are often not representative of real-world\ndistributions. This paper derives several first-of-its-kind machine learning\nmethodologies to analyze these low feature audio spectrograms given data\ndistributions that may have normalized, skewed, or even limited training sets.\nIn particular, this paper proposes several novel customized convolutional\narchitectures to extract identifying features using binary, one-class, and\nsiamese approaches to identify the spectrographic signature of a given audio\nsignal. Utilizing these novel convolutional architectures as well as the\nproposed classification methods, these experiments demonstrate state-of-the-art\nclassification accuracy and improved efficiency than traditional audio\nclassification methods.", "published": "2024-10-28 21:48:57", "link": "http://arxiv.org/abs/2410.21561v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multilingual Standalone Trustworthy Voice-Based Social Network for\n  Disaster Situations", "abstract": "In disaster scenarios, effective communication is crucial, yet language\nbarriers often hinder timely and accurate information dissemination,\nexacerbating vulnerabilities and complicating response efforts. This paper\npresents a novel, multilingual, voice-based social network specifically\ndesigned to address these challenges. The proposed system integrates advanced\nartificial intelligence (AI) with blockchain technology to enable secure,\nasynchronous voice communication across multiple languages. The application\noperates independently of external servers, ensuring reliability even in\ncompromised environments by functioning offline through local networks. Key\nfeatures include AI-driven real-time translation of voice messages, ensuring\nseamless cross-linguistic communication, and blockchain-enabled storage for\nsecure, immutable records of all interactions, safeguarding message integrity.\nDesigned for cross-platform use, the system offers consistent performance\nacross devices, from mobile phones to desktops, making it highly adaptable in\ndiverse disaster situations. Evaluation metrics demonstrate high accuracy in\nspeech recognition and translation, low latency, and user satisfaction,\nvalidating the system's effectiveness in enhancing communication during crises.\nThis solution represents a significant advancement in disaster communication,\nbridging language gaps to support more inclusive and efficient emergency\nresponse.", "published": "2024-10-28 03:24:37", "link": "http://arxiv.org/abs/2411.08889v1", "categories": ["cs.HC", "cs.AI", "cs.SD", "eess.AS", "I.2.7; K.4.4"], "primary_category": "cs.HC"}
