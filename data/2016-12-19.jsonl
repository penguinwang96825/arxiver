{"title": "Neural Multi-Source Morphological Reinflection", "abstract": "We explore the task of multi-source morphological reinflection, which\ngeneralizes the standard, single-source version. The input consists of (i) a\ntarget tag and (ii) multiple pairs of source form and source tag for a lemma.\nThe motivation is that it is beneficial to have access to more than one source\nform since different source forms can provide complementary information, e.g.,\ndifferent stems. We further present a novel extension to the encoder- decoder\nrecurrent neural architecture, consisting of multiple encoders, to better solve\nthe task. We show that our new architecture outperforms single-source\nreinflection models and publish our dataset for multi-source morphological\nreinflection to facilitate future research.", "published": "2016-12-19 02:21:24", "link": "http://arxiv.org/abs/1612.06027v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Boosting Neural Machine Translation", "abstract": "Training efficiency is one of the main problems for Neural Machine\nTranslation (NMT). Deep networks need for very large data as well as many\ntraining iterations to achieve state-of-the-art performance. This results in\nvery high computation cost, slowing down research and industrialisation. In\nthis paper, we propose to alleviate this problem with several training methods\nbased on data boosting and bootstrap with no modifications to the neural\nnetwork. It imitates the learning process of humans, which typically spend more\ntime when learning \"difficult\" concepts than easier ones. We experiment on an\nEnglish-French translation task showing accuracy improvements of up to 1.63\nBLEU while saving 20% of training time.", "published": "2016-12-19 11:49:49", "link": "http://arxiv.org/abs/1612.06138v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Machine Translation from Simplified Translations", "abstract": "Text simplification aims at reducing the lexical, grammatical and structural\ncomplexity of a text while keeping the same meaning. In the context of machine\ntranslation, we introduce the idea of simplified translations in order to boost\nthe learning ability of deep neural translation models. We conduct preliminary\nexperiments showing that translation complexity is actually reduced in a\ntranslation of a source bi-text compared to the target reference of the bi-text\nwhile using a neural machine translation (NMT) system learned on the exact same\nbi-text. Based on knowledge distillation idea, we then train an NMT system\nusing the simplified bi-text, and show that it outperforms the initial system\nthat was built over the reference data set. Performance is further boosted when\nboth reference and automatic translations are used to learn the network. We\nperform an elementary analysis of the translated corpus and report accuracy\nresults of the proposed approach on English-to-French and English-to-German\ntranslation tasks.", "published": "2016-12-19 11:50:58", "link": "http://arxiv.org/abs/1612.06139v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Domain Control for Neural Machine Translation", "abstract": "Machine translation systems are very sensitive to the domains they were\ntrained on. Several domain adaptation techniques have been deeply studied. We\npropose a new technique for neural machine translation (NMT) that we call\ndomain control which is performed at runtime using a unique neural network\ncovering multiple domains. The presented approach shows quality improvements\nwhen compared to dedicated domains translating on any of the covered domains\nand even on out-of-domain data. In addition, model parameters do not need to be\nre-estimated for each domain, making this effective to real use cases.\nEvaluation is carried out on English-to-French translation for two different\ntesting scenarios. We first consider the case where an end-user performs\ntranslations on a known domain. Secondly, we consider the scenario where the\ndomain is not known and predicted at the sentence level before translating.\nResults show consistent accuracy improvements for both conditions.", "published": "2016-12-19 11:51:35", "link": "http://arxiv.org/abs/1612.06140v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Domain specialization: a post-training domain adaptation for Neural\n  Machine Translation", "abstract": "Domain adaptation is a key feature in Machine Translation. It generally\nencompasses terminology, domain and style adaptation, especially for human\npost-editing workflows in Computer Assisted Translation (CAT). With Neural\nMachine Translation (NMT), we introduce a new notion of domain adaptation that\nwe call \"specialization\" and which is showing promising results both in the\nlearning speed and in adaptation accuracy. In this paper, we propose to explore\nthis approach under several perspectives.", "published": "2016-12-19 11:52:08", "link": "http://arxiv.org/abs/1612.06141v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Empirical Study of Adequate Vision Span for Attention-Based Neural\n  Machine Translation", "abstract": "Recently, the attention mechanism plays a key role to achieve high\nperformance for Neural Machine Translation models. However, as it computes a\nscore function for the encoder states in all positions at each decoding step,\nthe attention model greatly increases the computational complexity. In this\npaper, we investigate the adequate vision span of attention models in the\ncontext of machine translation, by proposing a novel attention framework that\nis capable of reducing redundant score computation dynamically. The term\n\"vision span\" means a window of the encoder states considered by the attention\nmodel in one step. In our experiments, we found that the average window size of\nvision span can be reduced by over 50% with modest loss in accuracy on\nEnglish-Japanese and German-English translation tasks.% This results indicate\nthat the conventional attention mechanism performs a significant amount of\nredundant computation.", "published": "2016-12-19 04:23:22", "link": "http://arxiv.org/abs/1612.06043v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Tweet Representations using Temporal and User Context", "abstract": "In this work we propose a novel representation learning model which computes\nsemantic representations for tweets accurately. Our model systematically\nexploits the chronologically adjacent tweets ('context') from users' Twitter\ntimelines for this task. Further, we make our model user-aware so that it can\ndo well in modeling the target tweet by exploiting the rich knowledge about the\nuser such as the way the user writes the post and also summarizing the topics\non which the user writes. We empirically demonstrate that the proposed models\noutperform the state-of-the-art models in predicting the user profile\nattributes like spouse, education and job by 19.66%, 2.27% and 2.22%\nrespectively.", "published": "2016-12-19 07:06:34", "link": "http://arxiv.org/abs/1612.06062v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A recurrent neural network without chaos", "abstract": "We introduce an exceptionally simple gated recurrent neural network (RNN)\nthat achieves performance comparable to well-known gated architectures, such as\nLSTMs and GRUs, on the word-level language modeling task. We prove that our\nmodel has simple, predicable and non-chaotic dynamics. This stands in stark\ncontrast to more standard gated architectures, whose underlying dynamical\nsystems exhibit chaotic behavior.", "published": "2016-12-19 14:59:14", "link": "http://arxiv.org/abs/1612.06212v1", "categories": ["cs.NE", "cs.CL", "cs.LG"], "primary_category": "cs.NE"}
{"title": "Talk it up or play it down? (Un)expected correlations between\n  (de-)emphasis and recurrence of discussion points in consequential U.S.\n  economic policy meetings", "abstract": "In meetings where important decisions get made, what items receive more\nattention may influence the outcome. We examine how different types of\nrhetorical (de-)emphasis -- including hedges, superlatives, and contrastive\nconjunctions -- correlate with what gets revisited later, controlling for item\nfrequency and speaker. Our data consists of transcripts of recurring meetings\nof the Federal Reserve's Open Market Committee (FOMC), where important aspects\nof U.S. monetary policy are decided on. Surprisingly, we find that words\nappearing in the context of hedging, which is usually considered a way to\nexpress uncertainty, are more likely to be repeated in subsequent meetings,\nwhile strong emphasis indicated by superlatives has a slightly negative effect\non word recurrence in subsequent meetings. We also observe interesting patterns\nin how these effects vary depending on social factors such as status and gender\nof the speaker. For instance, the positive effects of hedging are more\npronounced for female speakers than for male speakers.", "published": "2016-12-19 21:00:02", "link": "http://arxiv.org/abs/1612.06391v1", "categories": ["cs.SI", "cs.CL", "physics.soc-ph"], "primary_category": "cs.SI"}
