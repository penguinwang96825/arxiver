{"title": "Hierarchical Neural Networks for Sequential Sentence Classification in\n  Medical Scientific Abstracts", "abstract": "Prevalent models based on artificial neural network (ANN) for sentence\nclassification often classify sentences in isolation without considering the\ncontext in which sentences appear. This hampers the traditional sentence\nclassification approaches to the problem of sequential sentence classification,\nwhere structured prediction is needed for better overall classification\nperformance. In this work, we present a hierarchical sequential labeling\nnetwork to make use of the contextual information within surrounding sentences\nto help classify the current sentence. Our model outperforms the\nstate-of-the-art results by 2%-3% on two benchmarking datasets for sequential\nsentence classification in medical scientific abstracts.", "published": "2018-08-19 05:06:28", "link": "http://arxiv.org/abs/1808.06161v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Source-Critical Reinforcement Learning for Transferring Spoken Language\n  Understanding to a New Language", "abstract": "To deploy a spoken language understanding (SLU) model to a new language,\nlanguage transferring is desired to avoid the trouble of acquiring and labeling\na new big SLU corpus. Translating the original SLU corpus into the target\nlanguage is an attractive strategy. However, SLU corpora consist of plenty of\nsemantic labels (slots), which general-purpose translators cannot handle well,\nnot to mention additional culture differences. This paper focuses on the\nlanguage transferring task given a tiny in-domain parallel SLU corpus. The\nin-domain parallel corpus can be used as the first adaptation on the general\ntranslator. But more importantly, we show how to use reinforcement learning\n(RL) to further finetune the adapted translator, where translated sentences\nwith more proper slot tags receive higher rewards. We evaluate our approach on\nChinese to English language transferring for SLU systems. The experimental\nresults show that the generated English SLU corpus via adaptation and\nreinforcement learning gives us over 97% in the slot F1 score and over 84%\naccuracy in domain classification. It demonstrates the effectiveness of the\nproposed language transferring method. Compared with naive translation, our\nproposed method improves domain classification accuracy by relatively 22%, and\nthe slot filling F1 score by relatively more than 71%.", "published": "2018-08-19 05:49:46", "link": "http://arxiv.org/abs/1808.06167v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adapting the Neural Encoder-Decoder Framework from Single to\n  Multi-Document Summarization", "abstract": "Generating a text abstract from a set of documents remains a challenging\ntask. The neural encoder-decoder framework has recently been exploited to\nsummarize single documents, but its success can in part be attributed to the\navailability of large parallel data automatically acquired from the Web. In\ncontrast, parallel data for multi-document summarization are scarce and costly\nto obtain. There is a pressing need to adapt an encoder-decoder model trained\non single-document summarization data to work with multiple-document input. In\nthis paper, we present an initial investigation into a novel adaptation method.\nIt exploits the maximal marginal relevance method to select representative\nsentences from multi-document input, and leverages an abstractive\nencoder-decoder model to fuse disparate sentences to an abstractive summary.\nThe adaptation method is robust and itself requires no training data. Our\nsystem compares favorably to state-of-the-art extractive and abstractive\napproaches judged by automatic metrics and human assessors.", "published": "2018-08-19 14:43:09", "link": "http://arxiv.org/abs/1808.06218v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Detection of Vague Words and Sentences in Privacy Policies", "abstract": "Website privacy policies represent the single most important source of\ninformation for users to gauge how their personal data are collected, used and\nshared by companies. However, privacy policies are often vague and people\nstruggle to understand the content. Their opaqueness poses a significant\nchallenge to both users and policy regulators. In this paper, we seek to\nidentify vague content in privacy policies. We construct the first corpus of\nhuman-annotated vague words and sentences and present empirical studies on\nautomatic vagueness detection. In particular, we investigate context-aware and\ncontext-agnostic models for predicting vague words, and explore\nauxiliary-classifier generative adversarial networks for characterizing\nsentence vagueness. Our experimental results demonstrate the effectiveness of\nproposed approaches. Finally, we provide suggestions for resolving vagueness\nand improving the usability of privacy policies.", "published": "2018-08-19 15:12:19", "link": "http://arxiv.org/abs/1808.06219v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SentencePiece: A simple and language independent subword tokenizer and\n  detokenizer for Neural Text Processing", "abstract": "This paper describes SentencePiece, a language-independent subword tokenizer\nand detokenizer designed for Neural-based text processing, including Neural\nMachine Translation. It provides open-source C++ and Python implementations for\nsubword units. While existing subword segmentation tools assume that the input\nis pre-tokenized into word sequences, SentencePiece can train subword models\ndirectly from raw sentences, which allows us to make a purely end-to-end and\nlanguage independent system. We perform a validation experiment of NMT on\nEnglish-Japanese machine translation, and find that it is possible to achieve\ncomparable accuracy to direct subword training from raw sentences. We also\ncompare the performance of subword training and segmentation with various\nconfigurations. SentencePiece is available under the Apache 2 license at\nhttps://github.com/google/sentencepiece.", "published": "2018-08-19 16:49:06", "link": "http://arxiv.org/abs/1808.06226v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lexicosyntactic Inference in Neural Models", "abstract": "We investigate neural models' ability to capture lexicosyntactic inferences:\ninferences triggered by the interaction of lexical and syntactic information.\nWe take the task of event factuality prediction as a case study and build a\nfactuality judgment dataset for all English clause-embedding verbs in various\nsyntactic contexts. We use this dataset, which we make publicly available, to\nprobe the behavior of current state-of-the-art neural systems, showing that\nthese systems make certain systematic errors that are clearly visible through\nthe lens of factuality prediction.", "published": "2018-08-19 17:45:51", "link": "http://arxiv.org/abs/1808.06232v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Machine Translation of Text from Non-Native Speakers", "abstract": "Neural Machine Translation (NMT) systems are known to degrade when confronted\nwith noisy data, especially when the system is trained only on clean data. In\nthis paper, we show that augmenting training data with sentences containing\nartificially-introduced grammatical errors can make the system more robust to\nsuch errors. In combination with an automatic grammar error correction system,\nwe can recover 1.5 BLEU out of 2.4 BLEU lost due to grammatical errors. We also\npresent a set of Spanish translations of the JFLEG grammar error correction\ncorpus, which allows for testing NMT robustness to real grammatical errors.", "published": "2018-08-19 22:42:06", "link": "http://arxiv.org/abs/1808.06267v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "XL-NBT: A Cross-lingual Neural Belief Tracking Framework", "abstract": "Task-oriented dialog systems are becoming pervasive, and many companies\nheavily rely on them to complement human agents for customer service in call\ncenters. With globalization, the need for providing cross-lingual customer\nsupport becomes more urgent than ever. However, cross-lingual support poses\ngreat challenges---it requires a large amount of additional annotated data from\nnative speakers. In order to bypass the expensive human annotation and achieve\nthe first step towards the ultimate goal of building a universal dialog system,\nwe set out to build a cross-lingual state tracking framework. Specifically, we\nassume that there exists a source language with dialog belief tracking\nannotations while the target languages have no annotated dialog data of any\nform. Then, we pre-train a state tracker for the source language as a teacher,\nwhich is able to exploit easy-to-access parallel data. We then distill and\ntransfer its own knowledge to the student state tracker in target languages. We\nspecifically discuss two types of common parallel resources: bilingual corpus\nand bilingual dictionary, and design different transfer learning strategies\naccordingly. Experimentally, we successfully use English state tracker as the\nteacher to transfer its knowledge to both Italian and German trackers and\nachieve promising results.", "published": "2018-08-19 19:08:10", "link": "http://arxiv.org/abs/1808.06244v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Linked Recurrent Neural Networks", "abstract": "Recurrent Neural Networks (RNNs) have been proven to be effective in modeling\nsequential data and they have been applied to boost a variety of tasks such as\ndocument classification, speech recognition and machine translation. Most of\nexisting RNN models have been designed for sequences assumed to be identically\nand independently distributed (i.i.d). However, in many real-world\napplications, sequences are naturally linked. For example, web documents are\nconnected by hyperlinks; and genes interact with each other. On the one hand,\nlinked sequences are inherently not i.i.d., which poses tremendous challenges\nto existing RNN models. On the other hand, linked sequences offer link\ninformation in addition to the sequential information, which enables\nunprecedented opportunities to build advanced RNN models. In this paper, we\nstudy the problem of RNN for linked sequences. In particular, we introduce a\nprincipled approach to capture link information and propose a linked Recurrent\nNeural Network (LinkedRNN), which models sequential and link information\ncoherently. We conduct experiments on real-world datasets from multiple domains\nand the experimental results validate the effectiveness of the proposed\nframework.", "published": "2018-08-19 06:21:58", "link": "http://arxiv.org/abs/1808.06170v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
