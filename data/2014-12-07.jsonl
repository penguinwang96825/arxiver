{"title": "Learning Word Representations from Relational Graphs", "abstract": "Attributes of words and relations between two words are central to numerous\ntasks in Artificial Intelligence such as knowledge representation, similarity\nmeasurement, and analogy detection. Often when two words share one or more\nattributes in common, they are connected by some semantic relations. On the\nother hand, if there are numerous semantic relations between two words, we can\nexpect some of the attributes of one of the words to be inherited by the other.\nMotivated by this close connection between attributes and relations, given a\nrelational graph in which words are inter- connected via numerous semantic\nrelations, we propose a method to learn a latent representation for the\nindividual words. The proposed method considers not only the co-occurrences of\nwords as done by existing approaches for word representation learning, but also\nthe semantic relations in which two words co-occur. To evaluate the accuracy of\nthe word representations learnt using the proposed method, we use the learnt\nword representations to solve semantic word analogy problems. Our experimental\nresults show that it is possible to learn better word representations by using\nsemantic semantics between words.", "published": "2014-12-07 17:49:53", "link": "http://arxiv.org/abs/1412.2378v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
