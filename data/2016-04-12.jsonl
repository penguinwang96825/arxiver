{"title": "Disfluency Detection using a Bidirectional LSTM", "abstract": "We introduce a new approach for disfluency detection using a Bidirectional\nLong-Short Term Memory neural network (BLSTM). In addition to the word\nsequence, the model takes as input pattern match features that were developed\nto reduce sensitivity to vocabulary size in training, which lead to improved\nperformance over the word sequence alone. The BLSTM takes advantage of explicit\nrepair states in addition to the standard reparandum states. The final output\nleverages integer linear programming to incorporate constraints of disfluency\nstructure. In experiments on the Switchboard corpus, the model achieves\nstate-of-the-art performance for both the standard disfluency detection task\nand the correction detection task. Analysis shows that the model has better\ndetection of non-repetition disfluencies, which tend to be much harder to\ndetect.", "published": "2016-04-12 02:34:00", "link": "http://arxiv.org/abs/1604.03209v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving sentence compression by learning to predict gaze", "abstract": "We show how eye-tracking corpora can be used to improve sentence compression\nmodels, presenting a novel multi-task learning algorithm based on multi-layer\nLSTMs. We obtain performance competitive with or better than state-of-the-art\napproaches.", "published": "2016-04-12 11:57:05", "link": "http://arxiv.org/abs/1604.03357v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Attributes as Semantic Units between Natural Language and Visual\n  Recognition", "abstract": "Impressive progress has been made in the fields of computer vision and\nnatural language processing. However, it remains a challenge to find the best\npoint of interaction for these very different modalities. In this chapter we\ndiscuss how attributes allow us to exchange information between the two\nmodalities and in this way lead to an interaction on a semantic level.\nSpecifically we discuss how attributes allow using knowledge mined from\nlanguage resources for recognizing novel visual categories, how we can generate\nsentence description about images and video, how we can ground natural language\nin visual content, and finally, how we can answer natural language questions\nabout images.", "published": "2016-04-12 05:23:26", "link": "http://arxiv.org/abs/1604.03249v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Applying Ontological Modeling on Quranic Nature Domain", "abstract": "The holy Quran is the holy book of the Muslims. It contains information about\nmany domains. Often people search for particular concepts of holy Quran based\non the relations among concepts. An ontological modeling of holy Quran can be\nuseful in such a scenario. In this paper, we have modeled nature related\nconcepts of holy Quran using OWL (Web Ontology Language) / RDF (Resource\nDescription Framework). Our methodology involves identifying nature related\nconcepts mentioned in holy Quran and identifying relations among those\nconcepts. These concepts and relations are represented as classes/instances and\nproperties of an OWL ontology. Later, in the result section it is shown that,\nusing the Ontological model, SPARQL queries can retrieve verses and concepts of\ninterest. Thus, this modeling helps semantic search and query on the holy\nQuran. In this work, we have used English translation of the holy Quran by\nSahih International, Protege OWL Editor and for querying we have used SPARQL.", "published": "2016-04-12 09:27:00", "link": "http://arxiv.org/abs/1604.03318v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Video Description using Bidirectional Recurrent Neural Networks", "abstract": "Although traditionally used in the machine translation field, the\nencoder-decoder framework has been recently applied for the generation of video\nand image descriptions. The combination of Convolutional and Recurrent Neural\nNetworks in these models has proven to outperform the previous state of the\nart, obtaining more accurate video descriptions. In this work we propose\npushing further this model by introducing two contributions into the encoding\nstage. First, producing richer image representations by combining object and\nlocation information from Convolutional Neural Networks and second, introducing\nBidirectional Recurrent Neural Networks for capturing both forward and backward\ntemporal relationships in the input frames.", "published": "2016-04-12 13:09:01", "link": "http://arxiv.org/abs/1604.03390v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
