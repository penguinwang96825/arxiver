{"title": "Predictions For Pre-training Language Models", "abstract": "Language model pre-training has proven to be useful in many language\nunderstanding tasks. In this paper, we investigate whether it is still helpful\nto add the self-training method in the pre-training step and the fine-tuning\nstep. Towards this goal, we propose a learning framework that making best use\nof the unlabel data on the low-resource and high-resource labeled dataset. In\nindustry NLP applications, we have large amounts of data produced by users or\ncustomers. Our learning framework is based on this large amounts of unlabel\ndata. First, We use the model fine-tuned on manually labeled dataset to predict\npseudo labels for the user-generated unlabeled data. Then we use the pseudo\nlabels to supervise the task-specific training on the large amounts of\nuser-generated data. We consider this task-specific training step on pseudo\nlabels as a pre-training step for the next fine-tuning step. At last, we\nfine-tune on the manually labeled dataset upon the pre-trained model. In this\nwork, we first empirically show that our method is able to solidly improve the\nperformance by 3.6%, when the manually labeled fine-tuning dataset is\nrelatively small. Then we also show that our method still is able to improve\nthe performance further by 0.2%, when the manually labeled fine-tuning dataset\nis relatively large enough. We argue that our method make the best use of the\nunlabel data, which is superior to either pre-training or self-training alone.", "published": "2020-11-18 01:35:01", "link": "http://arxiv.org/abs/2011.09031v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Diverse and Non-redundant Answer Set Extraction on Community QA based on\n  DPPs", "abstract": "In community-based question answering (CQA) platforms, it takes time for a\nuser to get useful information from among many answers. Although one solution\nis an answer ranking method, the user still needs to read through the\ntop-ranked answers carefully. This paper proposes a new task of selecting a\ndiverse and non-redundant answer set rather than ranking the answers. Our\nmethod is based on determinantal point processes (DPPs), and it calculates the\nanswer importance and similarity between answers by using BERT. We built a\ndataset focusing on a Japanese CQA site, and the experiments on this dataset\ndemonstrated that the proposed method outperformed several baseline methods.", "published": "2020-11-18 07:33:03", "link": "http://arxiv.org/abs/2011.09140v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Document-Level Sentiment Analysis with User and Product\n  Context", "abstract": "Past work that improves document-level sentiment analysis by encoding user\nand product information has been limited to considering only the text of the\ncurrent review. We investigate incorporating additional review text available\nat the time of sentiment prediction that may prove meaningful for guiding\nprediction. Firstly, we incorporate all available historical review text\nbelonging to the author of the review in question. Secondly, we investigate the\ninclusion of historical reviews associated with the current product (written by\nother users). We achieve this by explicitly storing representations of reviews\nwritten by the same user and about the same product and force the model to\nmemorize all reviews for one particular user and product. Additionally, we drop\nthe hierarchical architecture used in previous work to enable words in the text\nto directly attend to each other. Experiment results on IMDB, Yelp 2013 and\nYelp 2014 datasets show improvement to state-of-the-art of more than 2\npercentage points in the best case.", "published": "2020-11-18 10:59:14", "link": "http://arxiv.org/abs/2011.09210v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the use of Self-supervised Pre-trained Acoustic and Linguistic\n  Features for Continuous Speech Emotion Recognition", "abstract": "Pre-training for feature extraction is an increasingly studied approach to\nget better continuous representations of audio and text content. In the present\nwork, we use wav2vec and camemBERT as self-supervised learned models to\nrepresent our data in order to perform continuous emotion recognition from\nspeech (SER) on AlloSat, a large French emotional database describing the\nsatisfaction dimension, and on the state of the art corpus SEWA focusing on\nvalence, arousal and liking dimensions. To the authors' knowledge, this paper\npresents the first study showing that the joint use of wav2vec and BERT-like\npre-trained features is very relevant to deal with continuous SER task, usually\ncharacterized by a small amount of labeled training data. Evaluated by the\nwell-known concordance correlation coefficient (CCC), our experiments show that\nwe can reach a CCC value of 0.825 instead of 0.592 when using MFCC in\nconjunction with word2vec word embedding on the AlloSat dataset.", "published": "2020-11-18 11:10:29", "link": "http://arxiv.org/abs/2011.09212v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Ubiqus English-Inuktitut System for WMT20", "abstract": "This paper describes Ubiqus' submission to the WMT20 English-Inuktitut shared\nnews translation task. Our main system, and only submission, is based on a\nmultilingual approach, jointly training a Transformer model on several\nagglutinative languages. The English-Inuktitut translation task is challenging\nat every step, from data selection, preparation and tokenization to quality\nevaluation down the line. Difficulties emerge both because of the peculiarities\nof the Inuktitut language as well as the low-resource context.", "published": "2020-11-18 12:49:17", "link": "http://arxiv.org/abs/2011.09249v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Master Thesis: Neural Sign Language Translation by Learning Tokenization", "abstract": "In this thesis, we propose a multitask learning based method to improve\nNeural Sign Language Translation (NSLT) consisting of two parts, a tokenization\nlayer and Neural Machine Translation (NMT). The tokenization part focuses on\nhow Sign Language (SL) videos should be represented to be fed into the other\npart. It has not been studied elaborately whereas NMT research has attracted\nseveral researchers contributing enormous advancements. Up to now, there are\ntwo main input tokenization levels, namely frame-level and gloss-level\ntokenization. Glosses are world-like intermediate presentation and unique to\nSLs. Therefore, we aim to develop a generic sign-level tokenization layer so\nthat it is applicable to other domains without further effort. We begin with\ninvestigating current tokenization approaches and explain their weaknesses with\nseveral experiments. To provide a solution, we adapt Transfer Learning,\nMultitask Learning and Unsupervised Domain Adaptation into this research to\nleverage additional supervision. We succeed in enabling knowledge transfer\nbetween SLs and improve translation quality by 5 points in BLEU-4 and 8 points\nin ROUGE scores. Secondly, we show the effects of body parts by extensive\nexperiments in all the tokenization approaches. Apart from these, we adopt\n3D-CNNs to improve efficiency in terms of time and space. Lastly, we discuss\nthe advantages of sign-level tokenization over gloss-level tokenization. To sum\nup, our proposed method eliminates the need for gloss level annotation to\nobtain higher scores by providing additional supervision by utilizing weak\nsupervision sources.", "published": "2020-11-18 13:59:36", "link": "http://arxiv.org/abs/2011.09289v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LAVA: Latent Action Spaces via Variational Auto-encoding for Dialogue\n  Policy Optimization", "abstract": "Reinforcement learning (RL) can enable task-oriented dialogue systems to\nsteer the conversation towards successful task completion. In an end-to-end\nsetting, a response can be constructed in a word-level sequential decision\nmaking process with the entire system vocabulary as action space. Policies\ntrained in such a fashion do not require expert-defined action spaces, but they\nhave to deal with large action spaces and long trajectories, making RL\nimpractical. Using the latent space of a variational model as action space\nalleviates this problem. However, current approaches use an uninformed prior\nfor training and optimize the latent distribution solely on the context. It is\ntherefore unclear whether the latent representation truly encodes the\ncharacteristics of different actions. In this paper, we explore three ways of\nleveraging an auxiliary task to shape the latent variable distribution: via\npre-training, to obtain an informed prior, and via multitask learning. We\nchoose response auto-encoding as the auxiliary task, as this captures the\ngenerative factors of dialogue responses while requiring low computational cost\nand neither additional data nor labels. Our approach yields a more\naction-characterized latent representations which support end-to-end dialogue\npolicy optimization and achieves state-of-the-art success rates. These results\nwarrant a more wide-spread use of RL in end-to-end dialogue models.", "published": "2020-11-18 16:23:30", "link": "http://arxiv.org/abs/2011.09378v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Out-of-Task Training for Dialog State Tracking Models", "abstract": "Dialog state tracking (DST) suffers from severe data sparsity. While many\nnatural language processing (NLP) tasks benefit from transfer learning and\nmulti-task learning, in dialog these methods are limited by the amount of\navailable data and by the specificity of dialog applications. In this work, we\nsuccessfully utilize non-dialog data from unrelated NLP tasks to train dialog\nstate trackers. This opens the door to the abundance of unrelated NLP corpora\nto mitigate the data sparsity issue inherent to DST.", "published": "2020-11-18 16:23:30", "link": "http://arxiv.org/abs/2011.09379v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Topology of Word Embeddings: Singularities Reflect Polysemy", "abstract": "The manifold hypothesis suggests that word vectors live on a submanifold\nwithin their ambient vector space. We argue that we should, more accurately,\nexpect them to live on a pinched manifold: a singular quotient of a manifold\nobtained by identifying some of its points. The identified, singular points\ncorrespond to polysemous words, i.e. words with multiple meanings. Our point of\nview suggests that monosemous and polysemous words can be distinguished based\non the topology of their neighbourhoods. We present two kinds of empirical\nevidence to support this point of view: (1) We introduce a topological measure\nof polysemy based on persistent homology that correlates well with the actual\nnumber of meanings of a word. (2) We propose a simple, topologically motivated\nsolution to the SemEval-2010 task on Word Sense Induction & Disambiguation that\nproduces competitive results.", "published": "2020-11-18 17:21:51", "link": "http://arxiv.org/abs/2011.09413v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EasyTransfer -- A Simple and Scalable Deep Transfer Learning Platform\n  for NLP Applications", "abstract": "The literature has witnessed the success of leveraging Pre-trained Language\nModels (PLMs) and Transfer Learning (TL) algorithms to a wide range of Natural\nLanguage Processing (NLP) applications, yet it is not easy to build an\neasy-to-use and scalable TL toolkit for this purpose. To bridge this gap, the\nEasyTransfer platform is designed to develop deep TL algorithms for NLP\napplications. EasyTransfer is backended with a high-performance and scalable\nengine for efficient training and inference, and also integrates comprehensive\ndeep TL algorithms, to make the development of industrial-scale TL applications\neasier. In EasyTransfer, the built-in data and model parallelism strategies,\ncombined with AI compiler optimization, show to be 4.0x faster than the\ncommunity version of distributed training. EasyTransfer supports various NLP\nmodels in the ModelZoo, including mainstream PLMs and multi-modality models. It\nalso features various in-house developed TL algorithms, together with the\nAppZoo for NLP applications. The toolkit is convenient for users to quickly\nstart model training, evaluation, and online deployment. EasyTransfer is\ncurrently deployed at Alibaba to support a variety of business scenarios,\nincluding item recommendation, personalized search, conversational question\nanswering, etc. Extensive experiments on real-world datasets and online\napplications show that EasyTransfer is suitable for online production with\ncutting-edge performance for various applications. The source code of\nEasyTransfer is released at Github (https://github.com/alibaba/EasyTransfer).", "published": "2020-11-18 18:41:27", "link": "http://arxiv.org/abs/2011.09463v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Predicting metrical patterns in Spanish poetry with language models", "abstract": "In this paper, we compare automated metrical pattern identification systems\navailable for Spanish against extensive experiments done by fine-tuning\nlanguage models trained on the same task. Despite being initially conceived as\na model suitable for semantic tasks, our results suggest that BERT-based models\nretain enough structural information to perform reasonably well for Spanish\nscansion.", "published": "2020-11-18 22:33:09", "link": "http://arxiv.org/abs/2011.09567v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ontology-based and User-focused Automatic Text Summarization (OATS):\n  Using COVID-19 Risk Factors as an Example", "abstract": "This paper proposes a novel Ontology-based and user-focused Automatic Text\nSummarization (OATS) system, in the setting where the goal is to automatically\ngenerate text summarization from unstructured text by extracting sentences\ncontaining the information that aligns to the user's focus. OATS consists of\ntwo modules: ontology-based topic identification and user-focused text\nsummarization; it first utilizes an ontology-based approach to identify\nrelevant documents to user's interest, and then takes advantage of the answers\nextracted from a question answering model using questions specified from users\nfor the generation of text summarization. To support the fight against the\nCOVID-19 pandemic, we used COVID-19 risk factors as an example to demonstrate\nthe proposed OATS system with the aim of helping the medical community\naccurately identify relevant scientific literature and efficiently review the\ninformation that addresses risk factors related to COVID-19.", "published": "2020-11-18 20:15:01", "link": "http://arxiv.org/abs/2012.02028v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sequence-Level Mixed Sample Data Augmentation", "abstract": "Despite their empirical success, neural networks still have difficulty\ncapturing compositional aspects of natural language. This work proposes a\nsimple data augmentation approach to encourage compositional behavior in neural\nmodels for sequence-to-sequence problems. Our approach, SeqMix, creates new\nsynthetic examples by softly combining input/output sequences from the training\nset. We connect this approach to existing techniques such as SwitchOut and word\ndropout, and show that these techniques are all approximating variants of a\nsingle objective. SeqMix consistently yields approximately 1.0 BLEU improvement\non five different translation datasets over strong Transformer baselines. On\ntasks that require strong compositional generalization such as SCAN and\nsemantic parsing, SeqMix also offers further improvements.", "published": "2020-11-18 02:18:04", "link": "http://arxiv.org/abs/2011.09039v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Hierarchical Multi-Modal Encoder for Moment Localization in Video\n  Corpus", "abstract": "Identifying a short segment in a long video that semantically matches a text\nquery is a challenging task that has important application potentials in\nlanguage-based video search, browsing, and navigation. Typical retrieval\nsystems respond to a query with either a whole video or a pre-defined video\nsegment, but it is challenging to localize undefined segments in untrimmed and\nunsegmented videos where exhaustively searching over all possible segments is\nintractable. The outstanding challenge is that the representation of a video\nmust account for different levels of granularity in the temporal domain. To\ntackle this problem, we propose the HierArchical Multi-Modal EncodeR (HAMMER)\nthat encodes a video at both the coarse-grained clip level and the fine-grained\nframe level to extract information at different scales based on multiple\nsubtasks, namely, video retrieval, segment temporal localization, and masked\nlanguage modeling. We conduct extensive experiments to evaluate our model on\nmoment localization in video corpus on ActivityNet Captions and TVR datasets.\nOur approach outperforms the previous methods as well as strong baselines,\nestablishing new state-of-the-art for this task.", "published": "2020-11-18 02:42:36", "link": "http://arxiv.org/abs/2011.09046v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Do Fine-tuned Commonsense Language Models Really Generalize?", "abstract": "Recently, transformer-based methods such as RoBERTa and GPT-3 have led to\nsignificant experimental advances in natural language processing tasks such as\nquestion answering and commonsense reasoning. The latter is typically evaluated\nthrough multiple benchmarks framed as multiple-choice instances of the former.\nAccording to influential leaderboards hosted by the Allen Institute (evaluating\nstate-of-the-art performance on commonsense reasoning benchmarks), models based\non such transformer methods are approaching human-like performance and have\naverage accuracy well over 80% on many benchmarks. Since these are commonsense\nbenchmarks, a model that generalizes on commonsense reasoning should not\nexperience much performance loss across multiple commonsense benchmarks. In\nthis paper, we study the generalization issue in detail by designing and\nconducting a rigorous scientific study. Using five common benchmarks, multiple\ncontrols and statistical analysis, we find clear evidence that fine-tuned\ncommonsense language models still do not generalize well, even with moderate\nchanges to the experimental setup, and may, in fact, be susceptible to dataset\nbias. We also perform selective studies, including qualitative and consistency\nanalyses, to gain deeper insight into the problem.", "published": "2020-11-18 08:52:49", "link": "http://arxiv.org/abs/2011.09159v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "A Definition and a Test for Human-Level Artificial Intelligence", "abstract": "Despite recent advances of AI research in many application-specific domains,\nwe do not know how to build a human-level artificial intelligence (HLAI). We\nconjecture that learning from others' experience with the language is the\nessential characteristic that distinguishes human intelligence from the rest.\nHumans can update the action-value function with the verbal description as if\nthey experience states, actions, and corresponding rewards sequences firsthand.\nIn this paper, we present a classification of intelligence according to how\nindividual agents learn and propose a definition and a test for HLAI. The main\nidea is that language acquisition without explicit rewards can be a sufficient\ntest for HLAI.", "published": "2020-11-18 17:10:02", "link": "http://arxiv.org/abs/2011.09410v5", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Palomino-Ochoa at SemEval-2020 Task 9: Robust System based on\n  Transformer for Code-Mixed Sentiment Classification", "abstract": "We present a transfer learning system to perform a mixed Spanish-English\nsentiment classification task. Our proposal uses the state-of-the-art language\nmodel BERT and embed it within a ULMFiT transfer learning pipeline. This\ncombination allows us to predict the polarity detection of code-mixed\n(English-Spanish) tweets. Thus, among 29 submitted systems, our approach\n(referred to as dplominop) is ranked 4th on the Sentimix Spanglish test set of\nSemEval 2020 Task 9. In fact, our system yields the weighted-F1 score value of\n0.755 which can be easily reproduced -- the source code and implementation\ndetails are made available.", "published": "2020-11-18 18:25:58", "link": "http://arxiv.org/abs/2011.09448v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Tie Your Embeddings Down: Cross-Modal Latent Spaces for End-to-end\n  Spoken Language Understanding", "abstract": "End-to-end (E2E) spoken language understanding (SLU) systems can infer the\nsemantics of a spoken utterance directly from an audio signal. However,\ntraining an E2E system remains a challenge, largely due to the scarcity of\npaired audio-semantics data. In this paper, we treat an E2E system as a\nmulti-modal model, with audio and text functioning as its two modalities, and\nuse a cross-modal latent space (CMLS) architecture, where a shared latent space\nis learned between the `acoustic' and `text' embeddings. We propose using\ndifferent multi-modal losses to explicitly guide the acoustic embeddings to be\ncloser to the text embeddings, obtained from a semantically powerful\npre-trained BERT model. We train the CMLS model on two publicly available E2E\ndatasets, across different cross-modal losses and show that our proposed\ntriplet loss function achieves the best performance. It achieves a relative\nimprovement of 1.4% and 4% respectively over an E2E model without a cross-modal\nspace and a relative improvement of 0.7% and 1% over a previously published\nCMLS model using $L_2$ loss. The gains are higher for a smaller, more\ncomplicated E2E dataset, demonstrating the efficacy of using an efficient\ncross-modal loss function, especially when there is limited E2E training data\navailable.", "published": "2020-11-18 02:32:42", "link": "http://arxiv.org/abs/2011.09044v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Inspecting state of the art performance and NLP metrics in image-based\n  medical report generation", "abstract": "Several deep learning architectures have been proposed over the last years to\ndeal with the problem of generating a written report given an imaging exam as\ninput. Most works evaluate the generated reports using standard Natural\nLanguage Processing (NLP) metrics (e.g. BLEU, ROUGE), reporting significant\nprogress. In this article, we contrast this progress by comparing state of the\nart (SOTA) models against weak baselines. We show that simple and even naive\napproaches yield near SOTA performance on most traditional NLP metrics. We\nconclude that evaluation methods in this task should be further studied towards\ncorrectly measuring clinical accuracy, ideally involving physicians to\ncontribute to this end.", "published": "2020-11-18 13:09:12", "link": "http://arxiv.org/abs/2011.09257v3", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "I.2.7; I.4.9; J.3"], "primary_category": "cs.CL"}
{"title": "Combining Prosodic, Voice Quality and Lexical Features to Automatically\n  Detect Alzheimer's Disease", "abstract": "Alzheimer's Disease (AD) is nowadays the most common form of dementia, and\nits automatic detection can help to identify symptoms at early stages, so that\npreventive actions can be carried out. Moreover, non-intrusive techniques based\non spoken data are crucial for the development of AD automatic detection\nsystems. In this light, this paper is presented as a contribution to the ADReSS\nChallenge, aiming at improving AD automatic detection from spontaneous speech.\nTo this end, recordings from 108 participants, which are age-, gender-, and AD\ncondition-balanced, have been used as training set to perform two different\ntasks: classification into AD/non-AD conditions, and regression over the\nMini-Mental State Examination (MMSE) scores. Both tasks have been performed\nextracting 28 features from speech -- based on prosody and voice quality -- and\n51 features from the transcriptions -- based on lexical and turn-taking\ninformation. Our results achieved up to 87.5 % of classification accuracy using\na Random Forest classifier, and 4.54 of RMSE using a linear regression with\nstochastic gradient descent over the provided test set. This shows promising\nresults in the automatic detection of Alzheimer's Disease through speech and\nlexical features.", "published": "2020-11-18 13:37:27", "link": "http://arxiv.org/abs/2011.09272v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Sequence-to-Sequence Approach to Dialogue State Tracking", "abstract": "This paper is concerned with dialogue state tracking (DST) in a task-oriented\ndialogue system. Building a DST module that is highly effective is still a\nchallenging issue, although significant progresses have been made recently.\nThis paper proposes a new approach to dialogue state tracking, referred to as\nSeq2Seq-DU, which formalizes DST as a sequence-to-sequence problem. Seq2Seq-DU\nemploys two BERT-based encoders to respectively encode the utterances in the\ndialogue and the descriptions of schemas, an attender to calculate attentions\nbetween the utterance embeddings and the schema embeddings, and a decoder to\ngenerate pointers to represent the current state of dialogue. Seq2Seq-DU has\nthe following advantages. It can jointly model intents, slots, and slot values;\nit can leverage the rich representations of utterances and schemas based on\nBERT; it can effectively deal with categorical and non-categorical slots, and\nunseen schemas. In addition, Seq2Seq-DU can also be used in the NLU (natural\nlanguage understanding) module of a dialogue system. Experimental results on\nbenchmark datasets in different settings (SGD, MultiWOZ2.2, MultiWOZ2.1,\nWOZ2.0, DSTC2, M2M, SNIPS, and ATIS) show that Seq2Seq-DU outperforms the\nexisting methods.", "published": "2020-11-18 21:42:44", "link": "http://arxiv.org/abs/2011.09553v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Clustering-based Automatic Construction of Legal Entity Knowledge Base\n  from Contracts", "abstract": "In contract analysis and contract automation, a knowledge base (KB) of legal\nentities is fundamental for performing tasks such as contract verification,\ncontract generation and contract analytic. However, such a KB does not always\nexist nor can be produced in a short time. In this paper, we propose a\nclustering-based approach to automatically generate a reliable knowledge base\nof legal entities from given contracts without any supplemental references. The\nproposed method is robust to different types of errors brought by\npre-processing such as Optical Character Recognition (OCR) and Named Entity\nRecognition (NER), as well as editing errors such as typos. We evaluate our\nmethod on a dataset that consists of 800 real contracts with various qualities\nfrom 15 clients. Compared to the collected ground-truth data, our method is\nable to recall 84\\% of the knowledge.", "published": "2020-11-18 17:51:27", "link": "http://arxiv.org/abs/2012.01942v2", "categories": ["cs.CL", "cs.AI", "cs.DS", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multi-Channel Automatic Speech Recognition Using Deep Complex Unet", "abstract": "The front-end module in multi-channel automatic speech recognition (ASR)\nsystems mainly use microphone array techniques to produce enhanced signals in\nnoisy conditions with reverberation and echos. Recently, neural network (NN)\nbased front-end has shown promising improvement over the conventional signal\nprocessing methods. In this paper, we propose to adopt the architecture of deep\ncomplex Unet (DCUnet) - a powerful complex-valued Unet-structured speech\nenhancement model - as the front-end of the multi-channel acoustic model, and\nintegrate them in a multi-task learning (MTL) framework along with cascaded\nframework for comparison. Meanwhile, we investigate the proposed methods with\nseveral training strategies to improve the recognition accuracy on the\n1000-hours real-world XiaoMi smart speaker data with echos. Experiments show\nthat our proposed DCUnet-MTL method brings about 12.2% relative character error\nrate (CER) reduction compared with the traditional approach with array\nprocessing plus single-channel acoustic model. It also achieves superior\nperformance than the recently proposed neural beamforming method.", "published": "2020-11-18 04:17:30", "link": "http://arxiv.org/abs/2011.09081v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "WPD++: An Improved Neural Beamformer for Simultaneous Speech Separation\n  and Dereverberation", "abstract": "This paper aims at eliminating the interfering speakers' speech, additive\nnoise, and reverberation from the noisy multi-talker speech mixture that\nbenefits automatic speech recognition (ASR) backend. While the recently\nproposed Weighted Power minimization Distortionless response (WPD) beamformer\ncan perform separation and dereverberation simultaneously, the noise\ncancellation component still has the potential to progress. We propose an\nimproved neural WPD beamformer called \"WPD++\" by an enhanced beamforming module\nin the conventional WPD and a multi-objective loss function for the joint\ntraining. The beamforming module is improved by utilizing the spatio-temporal\ncorrelation. A multi-objective loss, including the complex spectra domain\nscale-invariant signal-to-noise ratio (C-Si-SNR) and the magnitude domain mean\nsquare error (Mag-MSE), is properly designed to make multiple constraints on\nthe enhanced speech and the desired power of the dry clean signal. Joint\ntraining is conducted to optimize the complex-valued mask estimator and the\nWPD++ beamformer in an end-to-end way. The results show that the proposed WPD++\noutperforms several state-of-the-art beamformers on the enhanced speech quality\nand word error rate (WER) of ASR.", "published": "2020-11-18 09:06:03", "link": "http://arxiv.org/abs/2011.09162v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "CAA-Net: Conditional Atrous CNNs with Attention for Explainable\n  Device-robust Acoustic Scene Classification", "abstract": "Acoustic Scene Classification (ASC) aims to classify the environment in which\nthe audio signals are recorded. Recently, Convolutional Neural Networks (CNNs)\nhave been successfully applied to ASC. However, the data distributions of the\naudio signals recorded with multiple devices are different. There has been\nlittle research on the training of robust neural networks on acoustic scene\ndatasets recorded with multiple devices, and on explaining the operation of the\ninternal layers of the neural networks. In this article, we focus on training\nand explaining device-robust CNNs on multi-device acoustic scene data. We\npropose conditional atrous CNNs with attention for multi-device ASC. Our\nproposed system contains an ASC branch and a device classification branch, both\nmodelled by CNNs. We visualise and analyse the intermediate layers of the\natrous CNNs. A time-frequency attention mechanism is employed to analyse the\ncontribution of each time-frequency bin of the feature maps in the CNNs. On the\nDetection and Classification of Acoustic Scenes and Events (DCASE) 2018 ASC\ndataset, recorded with three devices, our proposed model performs significantly\nbetter than CNNs trained on single-device data.", "published": "2020-11-18 14:12:44", "link": "http://arxiv.org/abs/2011.09299v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Context-aware RNNLM Rescoring for Conversational Speech Recognition", "abstract": "Conversational speech recognition is regarded as a challenging task due to\nits free-style speaking and long-term contextual dependencies. Prior work has\nexplored the modeling of long-range context through RNNLM rescoring with\nimproved performance. To further take advantage of the persisted nature during\na conversation, such as topics or speaker turn, we extend the rescoring\nprocedure to a new context-aware manner. For RNNLM training, we capture the\ncontextual dependencies by concatenating adjacent sentences with various tag\nwords, such as speaker or intention information. For lattice rescoring, the\nlattice of adjacent sentences are also connected with the first-pass decoded\nresult by tag words. Besides, we also adopt a selective concatenation strategy\nbased on tf-idf, making the best use of contextual similarity to improve\ntranscription performance. Results on four different conversation test sets\nshow that our approach yields up to 13.1% and 6% relative char-error-rate (CER)\nreduction compared with 1st-pass decoding and common lattice-rescoring,\nrespectively.", "published": "2020-11-18 14:18:59", "link": "http://arxiv.org/abs/2011.09301v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Vertical-Horizontal Structured Attention for Generating Music with\n  Chords", "abstract": "In this paper, we propose a lightweight music-generating model based on\nvariational autoencoder (VAE) with structured attention. Generating music is\ndifferent from generating text because the melodies with chords give listeners\ndistinguished polyphonic feelings. In a piece of music, a chord consisting of\nmultiple notes comes from either the mixture of multiple instruments or the\ncombination of multiple keys of a single instrument. We focus our study on the\nlatter. Our model captures not only the temporal relations along time but the\nstructure relations between keys. Experimental results show that our model has\na better performance than baseline MusicVAE in capturing notes in a chord.\nBesides, our method accords with music theory since it maintains the\nconfiguration of the circle of fifths, distinguishes major and minor keys from\ninterval vectors, and manifests meaningful structures between music phrases.", "published": "2020-11-18 04:08:42", "link": "http://arxiv.org/abs/2011.09078v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Expanding Access to Music Technology -- Rapid Prototyping Accessible\n  Instrument Solutions For Musicians With Intellectual Disabilities", "abstract": "Using open-source and creative coding frameworks, a teamof artist-engineers\nfrom Portland Community College work-ing with artists who experience\nIntellectual/Developmentaldisabilities prototyped an ensemble of adapted\ninstrumentsand synthesizers that facilitate real-time in-key collabora-tion.\nThe instruments employ a variety of sensors, send-ing the resulting musical\ncontrols to software sound gener-ators via MIDI. Careful consideration was\ngiven to the bal-ance between freedom of expression, and curating the pos-sible\nsonic outcomes as adaptation. Evaluation of adaptedinstrument design may differ\ngreatly from frameworks forevaluating traditional instruments or products\nintended formass-market, though the results of such focused and indi-vidualised\ndesign have a variety of possible applications.", "published": "2020-11-18 07:48:12", "link": "http://arxiv.org/abs/2011.09143v1", "categories": ["cs.SD", "cs.HC", "eess.AS", "H.5.5"], "primary_category": "cs.SD"}
