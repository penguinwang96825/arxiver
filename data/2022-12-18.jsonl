{"title": "Low-Resource Authorship Style Transfer: Can Non-Famous Authors Be\n  Imitated?", "abstract": "Authorship style transfer involves altering text to match the style of a\ntarget author whilst preserving the original meaning. Existing unsupervised\napproaches like STRAP have largely focused on style transfer to target authors\nwith many examples of their writing style in books, speeches, or other\npublished works. This high-resource training data requirement (often greater\nthan 100,000 words) makes these approaches primarily useful for style transfer\nto published authors, politicians, or other well-known figures and authorship\nstyles, while style transfer to non-famous authors has not been well-studied.\nWe introduce the low-resource authorship style transfer task, a more\nchallenging class of authorship style transfer where only a limited amount of\ntext in the target author's style may exist. In our experiments, we\nspecifically choose source and target authors from Reddit and style transfer\ntheir Reddit posts, limiting ourselves to just 16 posts (on average ~500 words)\nof the target author's style. Style transfer accuracy is typically measured by\nhow often a classifier or human judge will classify an output as written by the\ntarget author. Recent authorship representations models excel at authorship\nidentification even with just a few writing samples, making automatic\nevaluation of this task possible for the first time through evaluation metrics\nwe propose. Our results establish an in-context learning technique we develop\nas the strongest baseline, though we find current approaches do not yet achieve\nmastery of this challenging task. We release our data and implementations to\nencourage further investigation.", "published": "2022-12-18 01:57:30", "link": "http://arxiv.org/abs/2212.08986v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PoE: a Panel of Experts for Generalized Automatic Dialogue Assessment", "abstract": "Chatbots are expected to be knowledgeable across multiple domains, e.g. for\ndaily chit-chat, exchange of information, and grounding in emotional\nsituations. To effectively measure the quality of such conversational agents, a\nmodel-based automatic dialogue evaluation metric (ADEM) is expected to perform\nwell across multiple domains. Despite significant progress, an ADEM that works\nwell in one domain does not necessarily generalize to another. This calls for a\ndedicated network architecture for domain generalization. To tackle the\nmulti-domain dialogue evaluation task, we propose a Panel of Experts (PoE), a\nmultitask network that consists of a shared transformer encoder and a\ncollection of lightweight adapters. The shared encoder captures the general\nknowledge of dialogues across domains, while each adapter specializes in one\nspecific domain and serves as a domain expert. To validate the idea, we\nconstruct a high-quality multi-domain dialogue dataset leveraging data\naugmentation and pseudo-labeling. The PoE network is comprehensively assessed\non 16 dialogue evaluation datasets spanning a wide range of dialogue domains.\nIt achieves state-of-the-art performance in terms of mean Spearman correlation\nover all the evaluation datasets. It exhibits better zero-shot generalization\nthan existing state-of-the-art ADEMs and the ability to easily adapt to new\ndomains with few-shot transfer learning.", "published": "2022-12-18 02:26:50", "link": "http://arxiv.org/abs/2212.08992v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentence-level Feedback Generation for English Language Learners: Does\n  Data Augmentation Help?", "abstract": "In this paper, we present strong baselines for the task of Feedback Comment\nGeneration for Writing Learning. Given a sentence and an error span, the task\nis to generate a feedback comment explaining the error. Sentences and feedback\ncomments are both in English. We experiment with LLMs and also create multiple\npseudo datasets for the task, investigating how it affects the performance of\nour system. We present our results for the task along with extensive analysis\nof the generated comments with the aim of aiding future studies in feedback\ncomment generation for English language learners.", "published": "2022-12-18 03:53:44", "link": "http://arxiv.org/abs/2212.08999v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Coreference Resolution based on Reinforcement Learning", "abstract": "The target of a coreference resolution system is to cluster all mentions that\nrefer to the same entity in a given context. All coreference resolution systems\nneed to solve two subtasks; one task is to detect all of the potential\nmentions, and the other is to learn the linking of an antecedent for each\npossible mention. In this paper, we propose a reinforcement learning\nactor-critic-based neural coreference resolution system, which can achieve both\nmention detection and mention clustering by leveraging an actor-critic deep\nreinforcement learning technique and a joint training algorithm. We experiment\non the BERT model to generate different input span representations. Our model\nwith the BERT span representation achieves the state-of-the-art performance\namong the models on the CoNLL-2012 Shared Task English Test Set.", "published": "2022-12-18 07:36:35", "link": "http://arxiv.org/abs/2212.09028v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Better Choice: Entire-space Datasets for Aspect Sentiment Triplet\n  Extraction", "abstract": "Aspect sentiment triplet extraction (ASTE) aims to extract aspect term,\nsentiment and opinion term triplets from sentences. Since the initial datasets\nused to evaluate models on ASTE had flaws, several studies later corrected the\ninitial datasets and released new versions of the datasets independently. As a\nresult, different studies select different versions of datasets to evaluate\ntheir methods, which makes ASTE-related works hard to follow. In this paper, we\nanalyze the relation between different versions of datasets and suggest that\nthe entire-space version should be used for ASTE. Besides the sentences\ncontaining triplets and the triplets in the sentences, the entire-space version\nadditionally includes the sentences without triplets and the aspect terms which\ndo not belong to any triplets. Hence, the entire-space version is consistent\nwith real-world scenarios and evaluating models on the entire-space version can\nbetter reflect the models' performance in real-world scenarios. In addition,\nexperimental results show that evaluating models on non-entire-space datasets\ninflates the performance of existing models and models trained on the\nentire-space version can obtain better performance.", "published": "2022-12-18 09:57:40", "link": "http://arxiv.org/abs/2212.09052v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Digital \"Echo Chambers\": The Role of Viewpoint Diversity in\n  Political Discussion", "abstract": "Increasingly taking place in online spaces, modern political conversations\nare typically perceived to be unproductively affirming -- siloed in so called\n``echo chambers'' of exclusively like-minded discussants. Yet, to date we lack\nsufficient means to measure viewpoint diversity in conversations. To this end,\nin this paper, we operationalize two viewpoint metrics proposed for recommender\nsystems and adapt them to the context of social media conversations. This is\nthe first study to apply these two metrics (Representation and Fragmentation)\nto real world data and to consider the implications for online conversations\nspecifically. We apply these measures to two topics -- daylight savings time\n(DST), which serves as a control, and the more politically polarized topic of\nimmigration. We find that the diversity scores for both Fragmentation and\nRepresentation are lower for immigration than for DST. Further, we find that\nwhile pro-immigrant views receive consistent pushback on the platform,\nanti-immigrant views largely operate within echo chambers. We observe less\nsevere yet similar patterns for DST. Taken together, Representation and\nFragmentation paint a meaningful and important new picture of viewpoint\ndiversity.", "published": "2022-12-18 10:18:15", "link": "http://arxiv.org/abs/2212.09056v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Let's Negotiate! A Survey of Negotiation Dialogue Systems", "abstract": "Negotiation is one of the crucial abilities in human communication, and there\nhas been a resurgent research interest in negotiation dialogue systems\nrecently, which goal is to empower intelligent agents with such ability that\ncan efficiently help humans resolve conflicts or reach beneficial agreements.\nAlthough there have been many explorations in negotiation dialogue systems, a\nsystematic review of this task has to date remained notably absent. To this\nend, we aim to fill this gap by reviewing contemporary studies in the emerging\nfield of negotiation dialogue systems, covering benchmarks, evaluations, and\nmethodologies. Furthermore, we also discuss potential future directions,\nincluding multi-modal, multi-party, and cross-cultural negotiation scenarios.\nOur goal is to provide the community with a systematic overview of negotiation\ndialogue systems and to inspire future research.", "published": "2022-12-18 12:03:53", "link": "http://arxiv.org/abs/2212.09072v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PVGRU: Generating Diverse and Relevant Dialogue Responses via\n  Pseudo-Variational Mechanism", "abstract": "We investigate response generation for multi-turn dialogue in\ngenerative-based chatbots. Existing generative models based on RNNs (Recurrent\nNeural Networks) usually employ the last hidden state to summarize the\nsequences, which makes models unable to capture the subtle variability observed\nin different dialogues and cannot distinguish the differences between dialogues\nthat are similar in composition. In this paper, we propose a Pseudo-Variational\nGated Recurrent Unit (PVGRU) component without posterior knowledge through\nintroducing a recurrent summarizing variable into the GRU, which can aggregate\nthe accumulated distribution variations of subsequences. PVGRU can perceive the\nsubtle semantic variability through summarizing variables that are optimized by\nthe devised distribution consistency and reconstruction objectives. In\naddition, we build a Pseudo-Variational Hierarchical Dialogue (PVHD) model\nbased on PVGRU. Experimental results demonstrate that PVGRU can broadly improve\nthe diversity and relevance of responses on two benchmark datasets.", "published": "2022-12-18 13:36:07", "link": "http://arxiv.org/abs/2212.09086v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Continual Knowledge Distillation for Neural Machine Translation", "abstract": "While many parallel corpora are not publicly accessible for data copyright,\ndata privacy and competitive differentiation reasons, trained translation\nmodels are increasingly available on open platforms. In this work, we propose a\nmethod called continual knowledge distillation to take advantage of existing\ntranslation models to improve one model of interest. The basic idea is to\nsequentially transfer knowledge from each trained model to the distilled model.\nExtensive experiments on Chinese-English and German-English datasets show that\nour method achieves significant and consistent improvements over strong\nbaselines under both homogeneous and heterogeneous trained model settings and\nis robust to malicious models.", "published": "2022-12-18 14:41:13", "link": "http://arxiv.org/abs/2212.09097v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LaSQuE: Improved Zero-Shot Classification from Explanations Through\n  Quantifier Modeling and Curriculum Learning", "abstract": "A hallmark of human intelligence is the ability to learn new concepts purely\nfrom language. Several recent approaches have explored training machine\nlearning models via natural language supervision. However, these approaches\nfall short in leveraging linguistic quantifiers (such as 'always' or 'rarely')\nand mimicking humans in compositionally learning complex tasks. Here, we\npresent LaSQuE, a method that can learn zero-shot classifiers from language\nexplanations by using three new strategies - (1) modeling the semantics of\nlinguistic quantifiers in explanations (including exploiting ordinal strength\nrelationships, such as 'always' > 'likely'), (2) aggregating information from\nmultiple explanations using an attention-based mechanism, and (3) model\ntraining via curriculum learning. With these strategies, LaSQuE outperforms\nprior work, showing an absolute gain of up to 7% in generalizing to unseen\nreal-world classification tasks.", "published": "2022-12-18 15:10:05", "link": "http://arxiv.org/abs/2212.09104v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CAPSTONE: Curriculum Sampling for Dense Retrieval with Document\n  Expansion", "abstract": "The dual-encoder has become the de facto architecture for dense retrieval.\nTypically, it computes the latent representations of the query and document\nindependently, thus failing to fully capture the interactions between the query\nand document. To alleviate this, recent research has focused on obtaining\nquery-informed document representations. During training, it expands the\ndocument with a real query, but during inference, it replaces the real query\nwith a generated one. This inconsistency between training and inference causes\nthe dense retrieval model to prioritize query information while disregarding\nthe document when computing the document representation. Consequently, it\nperforms even worse than the vanilla dense retrieval model because its\nperformance heavily relies on the relevance between the generated queries and\nthe real query.In this paper, we propose a curriculum sampling strategy that\nutilizes pseudo queries during training and progressively enhances the\nrelevance between the generated query and the real query. By doing so, the\nretrieval model learns to extend its attention from the document alone to both\nthe document and query, resulting in high-quality query-informed document\nrepresentations. Experimental results on both in-domain and out-of-domain\ndatasets demonstrate that our approach outperforms previous dense retrieval\nmodels.", "published": "2022-12-18 15:57:46", "link": "http://arxiv.org/abs/2212.09114v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Discontinuous Constituency Parsing with Mildly\n  Context-Sensitive Grammars", "abstract": "We study grammar induction with mildly context-sensitive grammars for\nunsupervised discontinuous parsing. Using the probabilistic linear context-free\nrewriting system (LCFRS) formalism, our approach fixes the rule structure in\nadvance and focuses on parameter learning with maximum likelihood. To reduce\nthe computational complexity of both parsing and parameter estimation, we\nrestrict the grammar formalism to LCFRS-2 (i.e., binary LCFRS with fan-out two)\nand further discard rules that require O(n^6) time to parse, reducing inference\nto O(n^5). We find that using a large number of nonterminals is beneficial and\nthus make use of tensor decomposition-based rank-space dynamic programming with\nan embedding-based parameterization of rule probabilities to scale up the\nnumber of nonterminals. Experiments on German and Dutch show that our approach\nis able to induce linguistically meaningful trees with continuous and\ndiscontinuous structures", "published": "2022-12-18 18:10:45", "link": "http://arxiv.org/abs/2212.09140v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Retriever-Augmented Language Models Reason? The Blame Game Between\n  the Retriever and the Language Model", "abstract": "Augmenting pretrained language models with retrievers has shown promise in\neffectively solving common NLP problems, such as language modeling and question\nanswering. In this paper, we evaluate the strengths and weaknesses of popular\nretriever-augmented language models, namely kNN-LM, REALM, DPR + FiD,\nContriever + ATLAS, and Contriever + Flan-T5, in reasoning over retrieved\nstatements across different tasks. Our findings indicate that the simple\nsimilarity metric employed by retrievers is insufficient for retrieving all the\nnecessary statements for reasoning. Additionally, the language models do not\nexhibit strong reasoning even when provided with only the required statements.\nFurthermore, when combined with imperfect retrievers, the performance of the\nlanguage models becomes even worse, e.g., Flan-T5's performance drops by 28.6%\nwhen retrieving 5 statements using Contriever. While larger language models\nimprove performance, there is still a substantial room for enhancement. Our\nfurther analysis indicates that multihop retrieve-and-read is promising for\nlarge language models like GPT-3.5, but does not generalize to other language\nmodels like Flan-T5-xxl.", "published": "2022-12-18 19:27:41", "link": "http://arxiv.org/abs/2212.09146v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rainproof: An Umbrella To Shield Text Generators From\n  Out-Of-Distribution Data", "abstract": "Implementing effective control mechanisms to ensure the proper functioning\nand security of deployed NLP models, from translation to chatbots, is\nessential. A key ingredient to ensure safe system behaviour is\nOut-Of-Distribution (OOD) detection, which aims to detect whether an input\nsample is statistically far from the training distribution. Although OOD\ndetection is a widely covered topic in classification tasks, most methods rely\non hidden features output by the encoder. In this work, we focus on leveraging\nsoft-probabilities in a black-box framework, i.e. we can access the\nsoft-predictions but not the internal states of the model. Our contributions\ninclude: (i) RAINPROOF a Relative informAItioN Projection OOD detection\nframework; and (ii) a more operational evaluation setting for OOD detection.\nSurprisingly, we find that OOD detection is not necessarily aligned with\ntask-specific measures. The OOD detector may filter out samples well processed\nby the model and keep samples that are not, leading to weaker performance. Our\nresults show that RAINPROOF provides OOD detection methods more aligned with\ntask-specific performance metrics than traditional OOD detectors.", "published": "2022-12-18 21:22:28", "link": "http://arxiv.org/abs/2212.09171v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Don't Forget Your ABC's: Evaluating the State-of-the-Art in\n  Chat-Oriented Dialogue Systems", "abstract": "Despite tremendous advancements in dialogue systems, stable evaluation still\nrequires human judgments producing notoriously high-variance metrics due to\ntheir inherent subjectivity. Moreover, methods and labels in dialogue\nevaluation are not fully standardized, especially for open-domain chats, with a\nlack of work to compare and assess the validity of those approaches. The use of\ninconsistent evaluation can misinform the performance of a dialogue system,\nwhich becomes a major hurdle to enhance it. Thus, a dimensional evaluation of\nchat-oriented open-domain dialogue systems that reliably measures several\naspects of dialogue capabilities is desired. This paper presents a novel human\nevaluation method to estimate the rates of many dialogue system behaviors. Our\nmethod is used to evaluate four state-of-the-art open-domain dialogue systems\nand compared with existing approaches. The analysis demonstrates that our\nbehavior method is more suitable than alternative Likert-style or comparative\napproaches for dimensional evaluation of these systems.", "published": "2022-12-18 22:07:55", "link": "http://arxiv.org/abs/2212.09180v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language model acceptability judgements are not always robust to context", "abstract": "Targeted syntactic evaluations of language models ask whether models show\nstable preferences for syntactically acceptable content over minimal-pair\nunacceptable inputs. Most targeted syntactic evaluation datasets ask models to\nmake these judgements with just a single context-free sentence as input. This\ndoes not match language models' training regime, in which input sentences are\nalways highly contextualized by the surrounding corpus. This mismatch raises an\nimportant question: how robust are models' syntactic judgements in different\ncontexts? In this paper, we investigate the stability of language models'\nperformance on targeted syntactic evaluations as we vary properties of the\ninput context: the length of the context, the types of syntactic phenomena it\ncontains, and whether or not there are violations of grammaticality. We find\nthat model judgements are generally robust when placed in randomly sampled\nlinguistic contexts. However, they are substantially unstable for contexts\ncontaining syntactic structures matching those in the critical test content.\nAmong all tested models (GPT-2 and five variants of OPT), we significantly\nimprove models' judgements by providing contexts with matching syntactic\nstructures, and conversely significantly worsen them using unacceptable\ncontexts with matching but violated syntactic structures. This effect is\namplified by the length of the context, except for unrelated inputs. We show\nthat these changes in model performance are not explainable by simple features\nmatching the context and the test inputs, such as lexical overlap and\ndependency overlap. This sensitivity to highly specific syntactic features of\nthe context can only be explained by the models' implicit in-context learning\nabilities.", "published": "2022-12-18 00:11:06", "link": "http://arxiv.org/abs/2212.08979v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Robust Semantic Frame Parsing Pipeline on a New Complex Twitter\n  Dataset", "abstract": "Most recent semantic frame parsing systems for spoken language understanding\n(SLU) are designed based on recurrent neural networks. These systems display\ndecent performance on benchmark SLU datasets such as ATIS or SNIPS, which\ncontain short utterances with relatively simple patterns. However, the current\nsemantic frame parsing models lack a mechanism to handle out-of-distribution\n(\\emph{OOD}) patterns and out-of-vocabulary (\\emph{OOV}) tokens. In this paper,\nwe introduce a robust semantic frame parsing pipeline that can handle both\n\\emph{OOD} patterns and \\emph{OOV} tokens in conjunction with a new complex\nTwitter dataset that contains long tweets with more \\emph{OOD} patterns and\n\\emph{OOV} tokens. The new pipeline demonstrates much better results in\ncomparison to state-of-the-art baseline SLU models on both the SNIPS dataset\nand the new Twitter dataset (Our new Twitter dataset can be downloaded from\nhttps://1drv.ms/u/s!AroHb-W6_OAlavK4begsDsMALfE?e=c8f2XX ). Finally, we also\nbuild an E2E application to demo the feasibility of our algorithm and show why\nit is useful in real application.", "published": "2022-12-18 01:59:49", "link": "http://arxiv.org/abs/2212.08987v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Impact of Sentiment Analysis in Fake Review Detection", "abstract": "Fake review identification is an important topic and has gained the interest\nof experts all around the world. Identifying fake reviews is challenging for\nresearchers, and there are several primary challenges to fake review detection.\nWe propose developing an initial research paper for investigating fake reviews\nby using sentiment analysis. Ten research papers are identified that show fake\nreviews, and they discuss currently available solutions for predicting or\ndetecting fake reviews. They also show the distribution of fake and truthful\nreviews through the analysis of sentiment. We summarize and compare previous\nstudies related to fake reviews. We highlight the most significant challenges\nin the sentiment evaluation process and demonstrate that there is a significant\nimpact on sentiment scores used to identify fake feedback.", "published": "2022-12-18 03:17:47", "link": "http://arxiv.org/abs/2212.08995v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Synthesis and Evaluation of a Domain-specific Large Data Set for\n  Dungeons & Dragons", "abstract": "This paper introduces the Forgotten Realms Wiki (FRW) data set and domain\nspecific natural language generation using FRW along with related analyses.\nForgotten Realms is the de-facto default setting of the popular open ended\ntabletop fantasy role playing game, Dungeons & Dragons. The data set was\nextracted from the Forgotten Realms Fandom wiki consisting of more than over\n45,200 articles. The FRW data set is constituted of 11 sub-data sets in a\nnumber of formats: raw plain text, plain text annotated by article title,\ndirected link graphs, wiki info-boxes annotated by the wiki article title,\nPoincar\\'e embedding of first link graph, multiple Word2Vec and Doc2Vec models\nof the corpus. This is the first data set of this size for the Dungeons &\nDragons domain. We then present a pairwise similarity comparison benchmark\nwhich utilizes similarity measures. In addition, we perform D&D domain specific\nnatural language generation using the corpus and evaluate the named entity\nclassification with respect to the lore of Forgotten Realms.", "published": "2022-12-18 12:54:45", "link": "http://arxiv.org/abs/2212.09080v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Rethinking the Role of Scale for In-Context Learning: An\n  Interpretability-based Case Study at 66 Billion Scale", "abstract": "Language models have been shown to perform better with an increase in scale\non a wide variety of tasks via the in-context learning paradigm. In this paper,\nwe investigate the hypothesis that the ability of a large language model to\nin-context learn-perform a task is not uniformly spread across all of its\nunderlying components. Using a 66 billion parameter language model (OPT-66B)\nacross a diverse set of 14 downstream tasks, we find this is indeed the case:\n$\\sim$70% of attention heads and $\\sim$20% of feed forward networks can be\nremoved with minimal decline in task performance. We find substantial overlap\nin the set of attention heads (un)important for in-context learning across\ntasks and number of in-context examples. We also address our hypothesis through\na task-agnostic lens, finding that a small set of attention heads in OPT-66B\nscore highly on their ability to perform primitive induction operations\nassociated with in-context learning, namely, prefix matching and copying. These\ninduction heads overlap with task-specific important heads, reinforcing\narguments by Olsson et al. (arXiv:2209.11895) regarding induction head\ngenerality to more sophisticated behaviors associated with in-context learning.\nOverall, our study provides several insights that indicate large language\nmodels may be under-trained for in-context learning and opens up questions on\nhow to pre-train language models to more effectively perform in-context\nlearning.", "published": "2022-12-18 14:36:07", "link": "http://arxiv.org/abs/2212.09095v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Recall, Expand and Multi-Candidate Cross-Encode: Fast and Accurate\n  Ultra-Fine Entity Typing", "abstract": "Ultra-fine entity typing (UFET) predicts extremely free-formed types (e.g.,\npresident, politician) of a given entity mention (e.g., Joe Biden) in context.\nState-of-the-art (SOTA) methods use the cross-encoder (CE) based architecture.\nCE concatenates the mention (and its context) with each type and feeds the\npairs into a pretrained language model (PLM) to score their relevance. It\nbrings deeper interaction between mention and types to reach better performance\nbut has to perform N (type set size) forward passes to infer types of a single\nmention. CE is therefore very slow in inference when the type set is large\n(e.g., N = 10k for UFET). To this end, we propose to perform entity typing in a\nrecall-expand-filter manner. The recall and expand stages prune the large type\nset and generate K (K is typically less than 256) most relevant type candidates\nfor each mention. At the filter stage, we use a novel model called MCCE to\nconcurrently encode and score these K candidates in only one forward pass to\nobtain the final type prediction. We investigate different variants of MCCE and\nextensive experiments show that MCCE under our paradigm reaches SOTA\nperformance on ultra-fine entity typing and is thousands of times faster than\nthe cross-encoder. We also found MCCE is very effective in fine-grained (130\ntypes) and coarse-grained (9 types) entity typing. Our code is available at\n\\url{https://github.com/modelscope/AdaSeq/tree/master/examples/MCCE}.", "published": "2022-12-18 16:42:52", "link": "http://arxiv.org/abs/2212.09125v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On Isotropy, Contextualization and Learning Dynamics of\n  Contrastive-based Sentence Representation Learning", "abstract": "Incorporating contrastive learning objectives in sentence representation\nlearning (SRL) has yielded significant improvements on many sentence-level NLP\ntasks. However, it is not well understood why contrastive learning works for\nlearning sentence-level semantics. In this paper, we aim to help guide future\ndesigns of sentence representation learning methods by taking a closer look at\ncontrastive SRL through the lens of isotropy, contextualization and learning\ndynamics. We interpret its successes through the geometry of the representation\nshifts and show that contrastive learning brings isotropy, and drives high\nintra-sentence similarity: when in the same sentence, tokens converge to\nsimilar positions in the semantic space. We also find that what we formalize as\n\"spurious contextualization\" is mitigated for semantically meaningful tokens,\nwhile augmented for functional ones. We find that the embedding space is\ndirected towards the origin during training, with more areas now better\ndefined. We ablate these findings by observing the learning dynamics with\ndifferent training temperatures, batch sizes and pooling methods.", "published": "2022-12-18 21:11:49", "link": "http://arxiv.org/abs/2212.09170v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Task Preferences across Languages on Community Question Answering\n  Platforms", "abstract": "With the steady emergence of community question answering (CQA) platforms\nlike Quora, StackExchange, and WikiHow, users now have an unprecedented access\nto information on various kind of queries and tasks. Moreover, the rapid\nproliferation and localization of these platforms spanning geographic and\nlinguistic boundaries offer a unique opportunity to study the task requirements\nand preferences of users in different socio-linguistic groups. In this study,\nwe implement an entity-embedding model trained on a large longitudinal dataset\nof multi-lingual and task-oriented question-answer pairs to uncover and\nquantify the (i) prevalence and distribution of various online tasks across\nlinguistic communities, and (ii) emerging and receding trends in task\npopularity over time in these communities. Our results show that there exists\nsubstantial variance in task preference as well as popularity trends across\nlinguistic communities on the platform. Findings from this study will help Q&A\nplatforms better curate and personalize content for non-English users, while\nalso offering valuable insights to businesses looking to target non-English\nspeaking communities online.", "published": "2022-12-18 09:37:20", "link": "http://arxiv.org/abs/2212.09045v1", "categories": ["cs.CL", "cs.CY", "cs.HC", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Chatbots in a Botnet World", "abstract": "Question-and-answer formats provide a novel experimental platform for\ninvestigating cybersecurity questions. Unlike previous chatbots, the latest\nChatGPT model from OpenAI supports an advanced understanding of complex coding\nquestions. The research demonstrates thirteen coding tasks that generally\nqualify as stages in the MITRE ATT&CK framework, ranging from credential access\nto defense evasion. With varying success, the experimental prompts generate\nexamples of keyloggers, logic bombs, obfuscated worms, and payment-fulfilled\nransomware. The empirical results illustrate cases that support the broad gain\nof functionality, including self-replication and self-modification, evasion,\nand strategic understanding of complex cybersecurity goals. One surprising\nfeature of ChatGPT as a language-only model centers on its ability to spawn\ncoding approaches that yield images that obfuscate or embed executable\nprogramming steps or links.", "published": "2022-12-18 16:08:40", "link": "http://arxiv.org/abs/2212.11126v2", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "BEATs: Audio Pre-Training with Acoustic Tokenizers", "abstract": "The massive growth of self-supervised learning (SSL) has been witnessed in\nlanguage, vision, speech, and audio domains over the past few years. While\ndiscrete label prediction is widely adopted for other modalities, the\nstate-of-the-art audio SSL models still employ reconstruction loss for\npre-training. Compared with reconstruction loss, semantic-rich discrete label\nprediction encourages the SSL model to abstract the high-level audio semantics\nand discard the redundant details as in human perception. However, a\nsemantic-rich acoustic tokenizer for general audio pre-training is usually not\nstraightforward to obtain, due to the continuous property of audio and\nunavailable phoneme sequences like speech. To tackle this challenge, we propose\nBEATs, an iterative audio pre-training framework to learn Bidirectional Encoder\nrepresentation from Audio Transformers, where an acoustic tokenizer and an\naudio SSL model are optimized by iterations. In the first iteration, we use\nrandom projection as the acoustic tokenizer to train an audio SSL model in a\nmask and label prediction manner. Then, we train an acoustic tokenizer for the\nnext iteration by distilling the semantic knowledge from the pre-trained or\nfine-tuned audio SSL model. The iteration is repeated with the hope of mutual\npromotion of the acoustic tokenizer and audio SSL model. The experimental\nresults demonstrate our acoustic tokenizers can generate discrete labels with\nrich audio semantics and our audio SSL models achieve state-of-the-art results\nacross various audio classification benchmarks, even outperforming previous\nmodels that use more training data and model parameters significantly.\nSpecifically, we set a new state-of-the-art mAP 50.6% on AudioSet-2M for\naudio-only models without using any external data, and 98.1% accuracy on\nESC-50. The code and pre-trained models are available at https://aka.ms/beats.", "published": "2022-12-18 10:41:55", "link": "http://arxiv.org/abs/2212.09058v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Fast FullSubNet: Accelerate Full-band and Sub-band Fusion Model for\n  Single-channel Speech Enhancement", "abstract": "FullSubNet is our recently proposed real-time single-channel speech\nenhancement network that achieves outstanding performance on the Deep Noise\nSuppression (DNS) Challenge dataset. A number of variants of FullSubNet have\nbeen proposed, but they all focus on the structure design towards better\nperformance and are rarely concerned with computational efficiency. For many\nspeech enhancement applications, a key feature is that system runs on a\nreal-time, latency-sensitive, battery-powered platform, which strictly limits\nthe algorithm latency and computational complexity. In this work, we propose a\nnew architecture named Fast FullSubNet dedicated to accelerating the\ncomputation of FullSubNet. Specifically, Fast FullSubNet processes sub-band\nspeech spectra in the mel-frequency domain by using cascaded linear-to-mel\nfull-band, sub-band, and mel-to-linear full-band models such that frequencies\ninvolved in the sub-band computation are vastly reduced. After that, a\ndown-sampling operation is proposed for the sub-band input sequence to further\nreduce the computational complexity along the time axis. Experimental results\nshow that, compared to FullSubNet, Fast FullSubNet has only 13\\% computational\ncomplexity and 16\\% processing time, and achieves comparable or even better\nperformance. Code and audio samples are available at\nhttps://github.com/Audio-WestlakeU/FullSubNet.", "published": "2022-12-18 05:41:33", "link": "http://arxiv.org/abs/2212.09019v2", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "A Review of Speech-centric Trustworthy Machine Learning: Privacy,\n  Safety, and Fairness", "abstract": "Speech-centric machine learning systems have revolutionized many leading\ndomains ranging from transportation and healthcare to education and defense,\nprofoundly changing how people live, work, and interact with each other.\nHowever, recent studies have demonstrated that many speech-centric ML systems\nmay need to be considered more trustworthy for broader deployment.\nSpecifically, concerns over privacy breaches, discriminating performance, and\nvulnerability to adversarial attacks have all been discovered in ML research\nfields. In order to address the above challenges and risks, a significant\nnumber of efforts have been made to ensure these ML systems are trustworthy,\nespecially private, safe, and fair. In this paper, we conduct the first\ncomprehensive survey on speech-centric trustworthy ML topics related to\nprivacy, safety, and fairness. In addition to serving as a summary report for\nthe research community, we point out several promising future research\ndirections to inspire the researchers who wish to explore further in this area.", "published": "2022-12-18 04:21:35", "link": "http://arxiv.org/abs/2212.09006v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Exploring Workplace Behaviors through Speaking Patterns using\n  Large-scale Multimodal Wearable Recordings: A Study of Healthcare Providers", "abstract": "Interpersonal spoken communication is central to human interaction and the\nexchange of information. Such interactive processes involve not only speech and\nspoken language but also non-verbal cues such as hand gestures, facial\nexpressions, and nonverbal vocalization, that are used to express feelings and\nprovide feedback. These multimodal communication signals carry a variety of\ninformation about the people: traits like gender and age as well as about\nphysical and psychological states and behavior. This work uses wearable\nmultimodal sensors to investigate interpersonal communication behaviors\nfocusing on speaking patterns among healthcare providers with a focus on\nnurses. We analyze longitudinal data collected from $99$ nurses in a large\nhospital setting over ten weeks. The results indicate that speaking pattern\ndifferences across shift schedules and working units. Moreover, results show\nthat speaking patterns combined with physiological measures can be used to\npredict affect measures and life satisfaction scores. The implementation of\nthis work can be accessed at https://github.com/usc-sail/tiles-audio-arousal.", "published": "2022-12-18 14:01:35", "link": "http://arxiv.org/abs/2212.09090v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
