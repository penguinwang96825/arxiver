{"title": "Goal-Oriented Script Construction", "abstract": "The knowledge of scripts, common chains of events in stereotypical scenarios,\nis a valuable asset for task-oriented natural language understanding systems.\nWe propose the Goal-Oriented Script Construction task, where a model produces a\nsequence of steps to accomplish a given goal. We pilot our task on the first\nmultilingual script learning dataset supporting 18 languages collected from\nwikiHow, a website containing half a million how-to articles. For baselines, we\nconsider both a generation-based approach using a language model and a\nretrieval-based approach by first retrieving the relevant steps from a large\ncandidate pool and then ordering them. We show that our task is practical,\nfeasible but challenging for state-of-the-art Transformer models, and that our\nmethods can be readily deployed for various other datasets and domains with\ndecent zero-shot performance.", "published": "2021-07-28 06:39:31", "link": "http://arxiv.org/abs/2107.13189v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Arabic aspect sentiment polarity classification using BERT", "abstract": "Aspect-based sentiment analysis(ABSA) is a textual analysis methodology that\ndefines the polarity of opinions on certain aspects related to specific\ntargets. The majority of research on ABSA is in English, with a small amount of\nwork available in Arabic. Most previous Arabic research has relied on deep\nlearning models that depend primarily on context-independent word embeddings\n(e.g.word2vec), where each word has a fixed representation independent of its\ncontext. This article explores the modeling capabilities of contextual\nembeddings from pre-trained language models, such as BERT, and making use of\nsentence pair input on Arabic aspect sentiment polarity classification task. In\nparticular, we develop a simple but effective BERT-based neural baseline to\nhandle this task. Our BERT architecture with a simple linear classification\nlayer surpassed the state-of-the-art works, according to the experimental\nresults on three different Arabic datasets. Achieving an accuracy of 89.51% on\nthe Arabic hotel reviews dataset, 73% on the Human annotated book reviews\ndataset, and 85.73% on the Arabic news dataset.", "published": "2021-07-28 11:34:00", "link": "http://arxiv.org/abs/2107.13290v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Scale Feature and Metric Learning for Relation Extraction", "abstract": "Existing methods in relation extraction have leveraged the lexical features\nin the word sequence and the syntactic features in the parse tree. Though\neffective, the lexical features extracted from the successive word sequence may\nintroduce some noise that has little or no meaningful content. Meanwhile, the\nsyntactic features are usually encoded via graph convolutional networks which\nhave restricted receptive field. To address the above limitations, we propose a\nmulti-scale feature and metric learning framework for relation extraction.\nSpecifically, we first develop a multi-scale convolutional neural network to\naggregate the non-successive mainstays in the lexical sequence. We also design\na multi-scale graph convolutional network which can increase the receptive\nfield towards specific syntactic roles. Moreover, we present a multi-scale\nmetric learning paradigm to exploit both the feature-level relation between\nlexical and syntactic features and the sample-level relation between instances\nwith the same or different classes. We conduct extensive experiments on three\nreal world datasets for various types of relation extraction tasks. The results\ndemonstrate that our model significantly outperforms the state-of-the-art\napproaches.", "published": "2021-07-28 15:14:36", "link": "http://arxiv.org/abs/2107.13425v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Robustness Against Natural Language Word Substitutions", "abstract": "Robustness against word substitutions has a well-defined and widely\nacceptable form, i.e., using semantically similar words as substitutions, and\nthus it is considered as a fundamental stepping-stone towards broader\nrobustness in natural language processing. Previous defense methods capture\nword substitutions in vector space by using either $l_2$-ball or\nhyper-rectangle, which results in perturbation sets that are not inclusive\nenough or unnecessarily large, and thus impedes mimicry of worst cases for\nrobust training. In this paper, we introduce a novel \\textit{Adversarial Sparse\nConvex Combination} (ASCC) method. We model the word substitution attack space\nas a convex hull and leverages a regularization term to enforce perturbation\ntowards an actual substitution, thus aligning our modeling better with the\ndiscrete textual space. Based on the ASCC method, we further propose\nASCC-defense, which leverages ASCC to generate worst-case perturbations and\nincorporates adversarial training towards robustness. Experiments show that\nASCC-defense outperforms the current state-of-the-arts in terms of robustness\non two prevailing NLP tasks, \\emph{i.e.}, sentiment analysis and natural\nlanguage inference, concerning several attacks across multiple model\narchitectures. Besides, we also envision a new class of defense towards\nrobustness in NLP, where our robustly trained word vectors can be plugged into\na normally trained model and enforce its robustness without applying any other\ndefense techniques.", "published": "2021-07-28 17:55:08", "link": "http://arxiv.org/abs/2107.13541v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Abusive Albanian", "abstract": "The ever growing usage of social media in the recent years has had a direct\nimpact on the increased presence of hate speech and offensive speech in online\nplatforms. Research on effective detection of such content has mainly focused\non English and a few other widespread languages, while the leftover majority\nfail to have the same work put into them and thus cannot benefit from the\nsteady advancements made in the field. In this paper we present \\textsc{Shaj},\nan annotated Albanian dataset for hate speech and offensive speech that has\nbeen constructed from user-generated content on various social media platforms.\nIts annotation follows the hierarchical schema introduced in OffensEval. The\ndataset is tested using three different classification models, the best of\nwhich achieves an F1 score of 0.77 for the identification of offensive\nlanguage, 0.64 F1 score for the automatic categorization of offensive types and\nlastly, 0.52 F1 score for the offensive language target identification.", "published": "2021-07-28 18:47:32", "link": "http://arxiv.org/abs/2107.13592v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating Text Simplification Evaluation", "abstract": "Modern text simplification (TS) heavily relies on the availability of gold\nstandard data to build machine learning models. However, existing studies show\nthat parallel TS corpora contain inaccurate simplifications and incorrect\nalignments. Additionally, evaluation is usually performed by using metrics such\nas BLEU or SARI to compare system output to the gold standard. A major\nlimitation is that these metrics do not match human judgements and the\nperformance on different datasets and linguistic phenomena vary greatly.\nFurthermore, our research shows that the test and training subsets of parallel\ndatasets differ significantly. In this work, we investigate existing TS\ncorpora, providing new insights that will motivate the improvement of existing\nstate-of-the-art TS evaluation methods. Our contributions include the analysis\nof TS corpora based on existing modifications used for simplification and an\nempirical study on TS models performance by using better-distributed datasets.\nWe demonstrate that by improving the distribution of TS datasets, we can build\nmore robust TS models.", "published": "2021-07-28 22:49:32", "link": "http://arxiv.org/abs/2107.13662v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to solve complex tasks by growing knowledge culturally across\n  generations", "abstract": "Knowledge built culturally across generations allows humans to learn far more\nthan an individual could glean from their own experience in a lifetime.\nCultural knowledge in turn rests on language: language is the richest record of\nwhat previous generations believed, valued, and practiced, and how these\nevolved over time. The power and mechanisms of language as a means of cultural\nlearning, however, are not well understood, and as a result, current AI systems\ndo not leverage language as a means for cultural knowledge transmission. Here,\nwe take a first step towards reverse-engineering cultural learning through\nlanguage. We developed a suite of complex tasks in the form of minimalist-style\nvideo games, which we deployed in an iterated learning paradigm. Human\nparticipants were limited to only two attempts (two lives) to beat each game\nand were allowed to write a message to a future participant who read the\nmessage before playing. Knowledge accumulated gradually across generations,\nallowing later generations to advance further in the games and perform more\nefficient actions. Multigenerational learning followed a strikingly similar\ntrajectory to individuals learning alone with an unlimited number of lives.\nSuccessive generations of learners were able to succeed by expressing distinct\ntypes of knowledge in natural language: the dynamics of the environment,\nvaluable goals, dangerous risks, and strategies for success. The video game\nparadigm we pioneer here is thus a rich test bed for developing AI systems\ncapable of acquiring and transmitting cultural knowledge.", "published": "2021-07-28 14:09:40", "link": "http://arxiv.org/abs/2107.13377v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Domain-matched Pre-training Tasks for Dense Retrieval", "abstract": "Pre-training on larger datasets with ever increasing model size is now a\nproven recipe for increased performance across almost all NLP tasks. A notable\nexception is information retrieval, where additional pre-training has so far\nfailed to produce convincing results. We show that, with the right pre-training\nsetup, this barrier can be overcome. We demonstrate this by pre-training large\nbi-encoder models on 1) a recently released set of 65 million synthetically\ngenerated questions, and 2) 200 million post-comment pairs from a preexisting\ndataset of Reddit conversations made available by pushshift.io. We evaluate on\na set of information retrieval and dialogue retrieval benchmarks, showing\nsubstantial improvements over supervised baselines.", "published": "2021-07-28 19:13:00", "link": "http://arxiv.org/abs/2107.13602v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Sentiment Analysis of the COVID-related r/Depression Posts", "abstract": "Reddit.com is a popular social media platform among young people. Reddit\nusers share their stories to seek support from other users, especially during\nthe Covid-19 pandemic. Messages posted on Reddit and their content have\nprovided researchers with opportunity to analyze public concerns. In this\nstudy, we analyzed sentiments of COVID-related messages posted on r/Depression.\nOur study poses the following questions: a) What are the common topics that the\nReddit users discuss? b) Can we use these topics to classify sentiments of the\nposts? c) What matters concern people more during the pandemic?\n  Key Words: Sentiment Classification, Depression, COVID-19, Reddit, LDA, BERT", "published": "2021-07-28 15:47:26", "link": "http://arxiv.org/abs/2108.06215v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "cs.SI", "I.2; I.2.7"], "primary_category": "cs.IR"}
{"title": "Towards Emotion-Aware Agents For Negotiation Dialogues", "abstract": "Negotiation is a complex social interaction that encapsulates emotional\nencounters in human decision-making. Virtual agents that can negotiate with\nhumans are useful in pedagogy and conversational AI. To advance the development\nof such agents, we explore the prediction of two important subjective goals in\na negotiation - outcome satisfaction and partner perception. Specifically, we\nanalyze the extent to which emotion attributes extracted from the negotiation\nhelp in the prediction, above and beyond the individual difference variables.\nWe focus on a recent dataset in chat-based negotiations, grounded in a\nrealistic camping scenario. We study three degrees of emotion dimensions -\nemoticons, lexical, and contextual by leveraging affective lexicons and a\nstate-of-the-art deep learning architecture. Our insights will be helpful in\ndesigning adaptive negotiation agents that interact through realistic\ncommunication interfaces.", "published": "2021-07-28 04:42:36", "link": "http://arxiv.org/abs/2107.13165v1", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods\n  in Natural Language Processing", "abstract": "This paper surveys and organizes research works in a new paradigm in natural\nlanguage processing, which we dub \"prompt-based learning\". Unlike traditional\nsupervised learning, which trains a model to take in an input x and predict an\noutput y as P(y|x), prompt-based learning is based on language models that\nmodel the probability of text directly. To use these models to perform\nprediction tasks, the original input x is modified using a template into a\ntextual string prompt x' that has some unfilled slots, and then the language\nmodel is used to probabilistically fill the unfilled information to obtain a\nfinal string x, from which the final output y can be derived. This framework is\npowerful and attractive for a number of reasons: it allows the language model\nto be pre-trained on massive amounts of raw text, and by defining a new\nprompting function the model is able to perform few-shot or even zero-shot\nlearning, adapting to new scenarios with few or no labeled data. In this paper\nwe introduce the basics of this promising paradigm, describe a unified set of\nmathematical notations that can cover a wide variety of existing work, and\norganize existing work along several dimensions, e.g.the choice of pre-trained\nmodels, prompts, and tuning strategies. To make the field more accessible to\ninterested beginners, we not only make a systematic review of existing works\nand a highly structured typology of prompt-based concepts, but also release\nother resources, e.g., a website http://pretrain.nlpedia.ai/ including\nconstantly-updated survey, and paperlist.", "published": "2021-07-28 18:09:46", "link": "http://arxiv.org/abs/2107.13586v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CycleGAN-based Non-parallel Speech Enhancement with an Adaptive\n  Attention-in-attention Mechanism", "abstract": "Non-parallel training is a difficult but essential task for DNN-based speech\nenhancement methods, for the lack of adequate noisy and paired clean speech\ncorpus in many real scenarios. In this paper, we propose a novel adaptive\nattention-in-attention CycleGAN (AIA-CycleGAN) for non-parallel speech\nenhancement. In previous CycleGAN-based non-parallel speech enhancement\nmethods, the limited mapping ability of the generator may cause performance\ndegradation and insufficient feature learning. To alleviate this degradation,\nwe propose an integration of adaptive time-frequency attention (ATFA) and\nadaptive hierarchical attention (AHA) to form an attention-in-attention (AIA)\nmodule for more flexible feature learning during the mapping procedure. More\nspecifically, ATFA can capture the long-range temporal-spectral contextual\ninformation for more effective feature representations, while AHA can flexibly\naggregate different AFTA's intermediate output feature maps by adaptive\nattention weights depending on the global context. Numerous experimental\nresults demonstrate that the proposed approach achieves consistently more\nsuperior performance over previous GAN-based and CycleGAN-based methods in\nnon-parallel training. Moreover, experiments in parallel training verify that\nthe proposed AIA-CycleGAN also outperforms most advanced GAN-based and Non-GAN\nbased speech enhancement approaches, especially in maintaining speech integrity\nand reducing speech distortion.", "published": "2021-07-28 02:55:24", "link": "http://arxiv.org/abs/2107.13143v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "On Perceived Emotion in Expressive Piano Performance: Further\n  Experimental Evidence for the Relevance of Mid-level Perceptual Features", "abstract": "Despite recent advances in audio content-based music emotion recognition, a\nquestion that remains to be explored is whether an algorithm can reliably\ndiscern emotional or expressive qualities between different performances of the\nsame piece. In the present work, we analyze several sets of features on their\neffectiveness in predicting arousal and valence of six different performances\n(by six famous pianists) of Bach's Well-Tempered Clavier Book 1. These features\ninclude low-level acoustic features, score-based features, features extracted\nusing a pre-trained emotion model, and Mid-level perceptual features. We\ncompare their predictive power by evaluating them on several experiments\ndesigned to test performance-wise or piece-wise variations of emotion. We find\nthat Mid-level features show significant contribution in performance-wise\nvariation of both arousal and valence -- even better than the pre-trained\nemotion model. Our findings add to the evidence of Mid-level perceptual\nfeatures being an important representation of musical attributes for several\ntasks -- specifically, in this case, for capturing the expressive aspects of\nmusic that manifest as perceived emotion of a musical performance.", "published": "2021-07-28 09:22:18", "link": "http://arxiv.org/abs/2107.13231v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Deep learning based cough detection camera using enhanced features", "abstract": "Coughing is a typical symptom of COVID-19. To detect and localize coughing\nsounds remotely, a convolutional neural network (CNN) based deep learning model\nwas developed in this work and integrated with a sound camera for the\nvisualization of the cough sounds. The cough detection model is a binary\nclassifier of which the input is a two second acoustic feature and the output\nis one of two inferences (Cough or Others). Data augmentation was performed on\nthe collected audio files to alleviate class imbalance and reflect various\nbackground noises in practical environments. For effective featuring of the\ncough sound, conventional features such as spectrograms, mel-scaled\nspectrograms, and mel-frequency cepstral coefficients (MFCC) were reinforced by\nutilizing their velocity (V) and acceleration (A) maps in this work. VGGNet,\nGoogLeNet, and ResNet were simplified to binary classifiers, and were named\nV-net, G-net, and R-net, respectively. To find the best combination of features\nand networks, training was performed for a total of 39 cases and the\nperformance was confirmed using the test F1 score. Finally, a test F1 score of\n91.9% (test accuracy of 97.2%) was achieved from G-net with the MFCC-V-A\nfeature (named Spectroflow), an acoustic feature effective for use in cough\ndetection. The trained cough detection model was integrated with a sound camera\n(i.e., one that visualizes sound sources using a beamforming microphone array).\nIn a pilot test, the cough detection camera detected coughing sounds with an F1\nscore of 90.0% (accuracy of 96.0%), and the cough location in the camera image\nwas tracked in real time.", "published": "2021-07-28 10:44:20", "link": "http://arxiv.org/abs/2107.13260v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Don't Separate, Learn to Remix: End-to-End Neural Remixing with Joint\n  Optimization", "abstract": "The task of manipulating the level and/or effects of individual instruments\nto recompose a mixture of recordings, or remixing, is common across a variety\nof applications such as music production, audio-visual post-production,\npodcasts, and more. This process, however, traditionally requires access to\nindividual source recordings, restricting the creative process. To work around\nthis, source separation algorithms can separate a mixture into its respective\ncomponents. Then, a user can adjust their levels and mix them back together.\nThis two-step approach, however, still suffers from audible artifacts and\nmotivates further work. In this work, we learn to remix music directly by\nre-purposing Conv-TasNet, a well-known source separation model, into two neural\nremixing architectures. To do this, we use an explicit loss term that directly\nmeasures remix quality and jointly optimize it with a separation loss. We\nevaluate our methods using the Slakh and MUSDB18 datasets and report remixing\nperformance as well as the impact on source separation as a byproduct. Our\nresults suggest that learning-to-remix significantly outperforms a strong\nseparation baseline and is particularly useful for small volume changes.", "published": "2021-07-28 20:45:55", "link": "http://arxiv.org/abs/2107.13634v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Visual Domain Transfer Learning Approach for Heartbeat Sound\n  Classification", "abstract": "Heart disease is the most common reason for human mortality that causes\nalmost one-third of deaths throughout the world. Detecting the disease early\nincreases the chances of survival of the patient and there are several ways a\nsign of heart disease can be detected early. This research proposes to convert\ncleansed and normalized heart sound into visual mel scale spectrograms and then\nusing visual domain transfer learning approaches to automatically extract\nfeatures and categorize between heart sounds. Some of the previous studies\nfound that the spectrogram of various types of heart sounds is visually\ndistinguishable to human eyes, which motivated this study to experiment on\nvisual domain classification approaches for automated heart sound\nclassification. It will use convolution neural network-based architectures i.e.\nResNet, MobileNetV2, etc as the automated feature extractors from spectrograms.\nThese well-accepted models in the image domain showed to learn generalized\nfeature representations of cardiac sounds collected from different environments\nwith varying amplitude and noise levels. Model evaluation criteria used were\ncategorical accuracy, precision, recall, and AUROC as the chosen dataset is\nunbalanced. The proposed approach has been implemented on datasets A and B of\nthe PASCAL heart sound collection and resulted in ~ 90% categorical accuracy\nand AUROC of ~0.97 for both sets.", "published": "2021-07-28 09:41:38", "link": "http://arxiv.org/abs/2107.13237v2", "categories": ["eess.AS", "cs.CV", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Detection of squawks in respiratory sounds of mechanically ventilated\n  COVID-19 patients", "abstract": "Mechanically ventilated patients typically exhibit abnormal respiratory\nsounds. Squawks are short inspiratory adventitious sounds that may occur in\npatients with pneumonia, such as COVID-19 patients. In this work we devised a\nmethod for squawk detection in mechanically ventilated patients by developing\nalgorithms for respiratory cycle estimation, squawk candidate identification,\nfeature extraction, and clustering. The best classifier reached an F1 of 0.48\nat the sound file level and an F1 of 0.66 at the recording session level. These\npreliminary results are promising, as they were obtained in noisy environments.\nThis method will give health professionals a new feature to assess the\npotential deterioration of critically ill patients.", "published": "2021-07-28 18:46:29", "link": "http://arxiv.org/abs/2107.13591v1", "categories": ["physics.med-ph", "cs.SD", "eess.AS"], "primary_category": "physics.med-ph"}
{"title": "Proposal-based Few-shot Sound Event Detection for Speech and\n  Environmental Sounds with Perceivers", "abstract": "Many applications involve detecting and localizing specific sound events\nwithin long, untrimmed documents, including keyword spotting, medical\nobservation, and bioacoustic monitoring for conservation. Deep learning\ntechniques often set the state-of-the-art for these tasks. However, for some\ntypes of events, there is insufficient labeled data to train such models. In\nthis paper, we propose a region proposal-based approach to few-shot sound event\ndetection utilizing the Perceiver architecture. Motivated by a lack of suitable\nbenchmark datasets, we generate two new few-shot sound event localization\ndatasets: \"Vox-CASE,\" using clips of celebrity speech as the sound event, and\n\"ESC-CASE,\" using environmental sound events. Our highest performing proposed\nfew-shot approaches achieve 0.483 and 0.418 F1-score, respectively, with 5-shot\n5-way tasks on these two datasets. These represent relative improvements of\n72.5% and 11.2% over strong proposal-free few-shot sound event detection\nbaselines.", "published": "2021-07-28 19:46:55", "link": "http://arxiv.org/abs/2107.13616v2", "categories": ["eess.AS", "cs.NE", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Estimating Respiratory Rate From Breath Audio Obtained Through Wearable\n  Microphones", "abstract": "Respiratory rate (RR) is a clinical metric used to assess overall health and\nphysical fitness. An individual's RR can change from their baseline due to\nchronic illness symptoms (e.g., asthma, congestive heart failure), acute\nillness (e.g., breathlessness due to infection), and over the course of the day\ndue to physical exhaustion during heightened exertion. Remote estimation of RR\ncan offer a cost-effective method to track disease progression and\ncardio-respiratory fitness over time. This work investigates a model-driven\napproach to estimate RR from short audio segments obtained after physical\nexertion in healthy adults. Data was collected from 21 individuals using\nmicrophone-enabled, near-field headphones before, during, and after strenuous\nexercise. RR was manually annotated by counting perceived inhalations and\nexhalations. A multi-task Long-Short Term Memory (LSTM) network with\nconvolutional layers was implemented to process mel-filterbank energies,\nestimate RR in varying background noise conditions, and predict heavy\nbreathing, indicated by an RR of more than 25 breaths per minute. The\nmulti-task model performs both classification and regression tasks and\nleverages a mixture of loss functions. It was observed that RR can be estimated\nwith a concordance correlation coefficient (CCC) of 0.76 and a mean squared\nerror (MSE) of 0.2, demonstrating that audio can be a viable signal for\napproximating RR.", "published": "2021-07-28 17:24:44", "link": "http://arxiv.org/abs/2107.14028v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Squeeze-Excitation Convolutional Recurrent Neural Networks for\n  Audio-Visual Scene Classification", "abstract": "The use of multiple and semantically correlated sources can provide\ncomplementary information to each other that may not be evident when working\nwith individual modalities on their own. In this context, multi-modal models\ncan help producing more accurate and robust predictions in machine learning\ntasks where audio-visual data is available. This paper presents a multi-modal\nmodel for automatic scene classification that exploits simultaneously auditory\nand visual information. The proposed approach makes use of two separate\nnetworks which are respectively trained in isolation on audio and visual data,\nso that each network specializes in a given modality. The visual subnetwork is\na pre-trained VGG16 model followed by a bidiretional recurrent layer, while the\nresidual audio subnetwork is based on stacked squeeze-excitation convolutional\nblocks trained from scratch. After training each subnetwork, the fusion of\ninformation from the audio and visual streams is performed at two different\nstages. The early fusion stage combines features resulting from the last\nconvolutional block of the respective subnetworks at different time steps to\nfeed a bidirectional recurrent structure. The late fusion stage combines the\noutput of the early fusion stage with the independent predictions provided by\nthe two subnetworks, resulting in the final prediction. We evaluate the method\nusing the recently published TAU Audio-Visual Urban Scenes 2021, which contains\nsynchronized audio and video recordings from 12 European cities in 10 different\nscene classes. The proposed model has been shown to provide an excellent\ntrade-off between prediction performance (86.5%) and system complexity (15M\nparameters) in the evaluation results of the DCASE 2021 Challenge.", "published": "2021-07-28 06:10:10", "link": "http://arxiv.org/abs/2107.13180v1", "categories": ["cs.MM", "cs.CV", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.MM"}
{"title": "Pitch-Informed Instrument Assignment Using a Deep Convolutional Network\n  with Multiple Kernel Shapes", "abstract": "This paper proposes a deep convolutional neural network for performing\nnote-level instrument assignment. Given a polyphonic multi-instrumental music\nsignal along with its ground truth or predicted notes, the objective is to\nassign an instrumental source for each note. This problem is addressed as a\npitch-informed classification task where each note is analysed individually. We\nalso propose to utilise several kernel shapes in the convolutional layers in\norder to facilitate learning of efficient timbre-discriminative feature maps.\nExperiments on the MusicNet dataset using 7 instrument classes show that our\napproach is able to achieve an average F-score of 0.904 when the original\nmulti-pitch annotations are used as the pitch information for the system, and\nthat it also excels if the note information is provided using third-party\nmulti-pitch estimation algorithms. We also include ablation studies\ninvestigating the effects of the use of multiple kernel shapes and comparing\ndifferent input representations for the audio and the note-related information.", "published": "2021-07-28 19:48:09", "link": "http://arxiv.org/abs/2107.13617v1", "categories": ["cs.SD", "cs.IR", "cs.LG", "cs.NE", "eess.AS"], "primary_category": "cs.SD"}
