{"title": "Cross-Lingual Sentiment Analysis Without (Good) Translation", "abstract": "Current approaches to cross-lingual sentiment analysis try to leverage the\nwealth of labeled English data using bilingual lexicons, bilingual vector space\nembeddings, or machine translation systems. Here we show that it is possible to\nuse a single linear transformation, with as few as 2000 word pairs, to capture\nfine-grained sentiment relationships between words in a cross-lingual setting.\nWe apply these cross-lingual sentiment models to a diverse set of tasks to\ndemonstrate their functionality in a non-English context. By effectively\nleveraging English sentiment knowledge without the need for accurate\ntranslation, we can analyze and extract features from other languages with\nscarce data at a very low cost, thus making sentiment and related analyses for\nmany languages inexpensive.", "published": "2017-07-06 03:50:38", "link": "http://arxiv.org/abs/1707.01626v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Embedded Deep Learning based Word Prediction", "abstract": "Recent developments in deep learning with application to language modeling\nhave led to success in tasks of text processing, summarizing and machine\ntranslation. However, deploying huge language models for mobile device such as\non-device keyboards poses computation as a bottle-neck due to their puny\ncomputation capacities. In this work we propose an embedded deep learning based\nword prediction method that optimizes run-time memory and also provides a real\ntime prediction environment. Our model size is 7.40MB and has average\nprediction time of 6.47 ms. We improve over the existing methods for word\nprediction in terms of key stroke savings and word prediction rate.", "published": "2017-07-06 07:39:06", "link": "http://arxiv.org/abs/1707.01662v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Simple Approach to Learn Polysemous Word Embeddings", "abstract": "Many NLP applications require disambiguating polysemous words. Existing\nmethods that learn polysemous word vector representations involve first\ndetecting various senses and optimizing the sense-specific embeddings\nseparately, which are invariably more involved than single sense learning\nmethods such as word2vec. Evaluating these methods is also problematic, as\nrigorous quantitative evaluations in this space is limited, especially when\ncompared with single-sense embeddings. In this paper, we propose a simple\nmethod to learn a word representation, given any context. Our method only\nrequires learning the usual single sense representation, and coefficients that\ncan be learnt via a single pass over the data. We propose several new test sets\nfor evaluating word sense induction, relevance detection, and contextual word\nsimilarity, significantly supplementing the currently available tests. Results\non these and other tests show that while our method is embarrassingly simple,\nit achieves excellent results when compared to the state of the art models for\nunsupervised polysemous word representation learning.", "published": "2017-07-06 13:54:01", "link": "http://arxiv.org/abs/1707.01793v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Single-Queue Decoding for Neural Machine Translation", "abstract": "Neural machine translation models rely on the beam search algorithm for\ndecoding. In practice, we found that the quality of hypotheses in the search\nspace is negatively affected owing to the fixed beam size. To mitigate this\nproblem, we store all hypotheses in a single priority queue and use a universal\nscore function for hypothesis selection. The proposed algorithm is more\nflexible as the discarded hypotheses can be revisited in a later step. We\nfurther design a penalty function to punish the hypotheses that tend to produce\na final translation that is much longer or shorter than expected. Despite its\nsimplicity, we show that the proposed decoding algorithm is able to select\nhypotheses with better qualities and improve the translation performance.", "published": "2017-07-06 15:12:55", "link": "http://arxiv.org/abs/1707.01830v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Role of Text Preprocessing in Neural Network Architectures: An\n  Evaluation Study on Text Categorization and Sentiment Analysis", "abstract": "Text preprocessing is often the first step in the pipeline of a Natural\nLanguage Processing (NLP) system, with potential impact in its final\nperformance. Despite its importance, text preprocessing has not received much\nattention in the deep learning literature. In this paper we investigate the\nimpact of simple text preprocessing decisions (particularly tokenizing,\nlemmatizing, lowercasing and multiword grouping) on the performance of a\nstandard neural text classifier. We perform an extensive evaluation on standard\nbenchmarks from text categorization and sentiment analysis. While our\nexperiments show that a simple tokenization of input text is generally\nadequate, they also highlight significant degrees of variability across\npreprocessing techniques. This reveals the importance of paying attention to\nthis usually-overlooked step in the pipeline, particularly when comparing\ndifferent models. Finally, our evaluation provides insights into the best\npreprocessing practices for training word embeddings.", "published": "2017-07-06 13:31:13", "link": "http://arxiv.org/abs/1707.01780v3", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Higher-order Relation Schema Induction using Tensor Factorization with\n  Back-off and Aggregation", "abstract": "Relation Schema Induction (RSI) is the problem of identifying type signatures\nof arguments of relations from unlabeled text. Most of the previous work in\nthis area have focused only on binary RSI, i.e., inducing only the subject and\nobject type signatures per relation. However, in practice, many relations are\nhigh-order, i.e., they have more than two arguments and inducing type\nsignatures of all arguments is necessary. For example, in the sports domain,\ninducing a schema win(WinningPlayer, OpponentPlayer, Tournament, Location) is\nmore informative than inducing just win(WinningPlayer, OpponentPlayer). We\nrefer to this problem as Higher-order Relation Schema Induction (HRSI). In this\npaper, we propose Tensor Factorization with Back-off and Aggregation (TFBA), a\nnovel framework for the HRSI problem. To the best of our knowledge, this is the\nfirst attempt at inducing higher-order relation schemata from unlabeled text.\nUsing the experimental analysis on three real world datasets, we show how TFBA\nhelps in dealing with sparsity and induce higher order schemata.", "published": "2017-07-06 18:02:12", "link": "http://arxiv.org/abs/1707.01917v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Long-Term Memory Networks for Question Answering", "abstract": "Question answering is an important and difficult task in the natural language\nprocessing domain, because many basic natural language processing tasks can be\ncast into a question answering task. Several deep neural network architectures\nhave been developed recently, which employ memory and inference components to\nmemorize and reason over text information, and generate answers to questions.\nHowever, a major drawback of many such models is that they are capable of only\ngenerating single-word answers. In addition, they require large amount of\ntraining data to generate accurate answers. In this paper, we introduce the\nLong-Term Memory Network (LTMN), which incorporates both an external memory\nmodule and a Long Short-Term Memory (LSTM) module to comprehend the input data\nand generate multi-word answers. The LTMN model can be trained end-to-end using\nback-propagation and requires minimal supervision. We test our model on two\nsynthetic data sets (based on Facebook's bAbI data set) and the real-world\nStanford question answering data set, and show that it can achieve\nstate-of-the-art performance.", "published": "2017-07-06 20:48:42", "link": "http://arxiv.org/abs/1707.01961v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Cross-linguistic differences and similarities in image descriptions", "abstract": "Automatic image description systems are commonly trained and evaluated on\nlarge image description datasets. Recently, researchers have started to collect\nsuch datasets for languages other than English. An unexplored question is how\ndifferent these datasets are from English and, if there are any differences,\nwhat causes them to differ. This paper provides a cross-linguistic comparison\nof Dutch, English, and German image descriptions. We find that these\ndescriptions are similar in many respects, but the familiarity of crowd workers\nwith the subjects of the images has a noticeable influence on description\nspecificity.", "published": "2017-07-06 11:53:41", "link": "http://arxiv.org/abs/1707.01736v2", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "An Interactive Tool for Natural Language Processing on Clinical Text", "abstract": "Natural Language Processing (NLP) systems often make use of machine learning\ntechniques that are unfamiliar to end-users who are interested in analyzing\nclinical records. Although NLP has been widely used in extracting information\nfrom clinical text, current systems generally do not support model revision\nbased on feedback from domain experts.\n  We present a prototype tool that allows end users to visualize and review the\noutputs of an NLP system that extracts binary variables from clinical text. Our\ntool combines multiple visualizations to help the users understand these\nresults and make any necessary corrections, thus forming a feedback loop and\nhelping improve the accuracy of the NLP models. We have tested our prototype in\na formative think-aloud user study with clinicians and researchers involved in\ncolonoscopy research. Results from semi-structured interviews and a System\nUsability Scale (SUS) analysis show that the users are able to quickly start\nrefining NLP models, despite having very little or no experience with machine\nlearning. Observations from these sessions suggest revisions to the interface\nto better support review workflow and interpretation of results.", "published": "2017-07-06 17:44:15", "link": "http://arxiv.org/abs/1707.01890v2", "categories": ["cs.HC", "cs.CL", "cs.IR"], "primary_category": "cs.HC"}
