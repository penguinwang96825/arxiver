{"title": "No Questions Asked: Effects of Transparency on Stablecoin Liquidity During the Collapse of Silicon Valley Bank", "abstract": "Fiat-pegged stablecoins are by nature exposed to spillover effects during\nmarket turmoil in Traditional Finance (TradFi). We observe a difference in\nTradFi market shocks impact between various stablecoins, in particular, USD\nCoin (USDC) and Tether USDT (USDT), the former with a higher reporting\nfrequency and transparency than the latter. We investigate this, using top USDC\nand USDT liquidity pools in Uniswap, by adapting the Marginal Cost of Immediacy\n(MCI) measure to Uniswap's Automated Market Maker, and then conducting\nDifference-in-Differences analysis on MCI and Total Value Locked (TVL) in USD,\nas well as measuring liquidity concentration across different providers.\nResults show that the Silicon Valley Bank (SVB) event reduced USDC's TVL\ndominance over USDT, increased USDT's liquidity cost relative to USDC, and\nliquidity provision remained concentrated with pool-specific trends. These\nfindings reveal a flight-to-safety behavior and counterintuitive effects of\nstablecoin transparency: USDC's frequent and detailed disclosures led to swift\nmarket reactions, while USDT's opacity and less frequent reporting provided a\nsafety net against immediate impacts.", "published": "2024-07-16 13:33:57", "link": "http://arxiv.org/abs/2407.11716v1", "categories": ["q-fin.TR", "cs.CE"], "primary_category": "q-fin.TR"}
{"title": "Ancient Korean Archive Translation: Comparison Analysis on Statistical\n  phrase alignment, LLM in-context learning, and inter-methodological approach", "abstract": "This study aims to compare three methods for translating ancient texts with\nsparse corpora: (1) the traditional statistical translation method of phrase\nalignment, (2) in-context LLM learning, and (3) proposed inter methodological\napproach - statistical machine translation method using sentence piece tokens\nderived from unified set of source-target corpus. The performance of the\nproposed approach in this study is 36.71 in BLEU score, surpassing the scores\nof SOLAR-10.7B context learning and the best existing Seq2Seq model. Further\nanalysis and discussion are presented.", "published": "2024-07-16 04:26:21", "link": "http://arxiv.org/abs/2407.11368v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Estimating Agreement by Chance for Sequence Annotation", "abstract": "In the field of natural language processing, correction of performance\nassessment for chance agreement plays a crucial role in evaluating the\nreliability of annotations. However, there is a notable dearth of research\nfocusing on chance correction for assessing the reliability of sequence\nannotation tasks, despite their widespread prevalence in the field. To address\nthis gap, this paper introduces a novel model for generating random\nannotations, which serves as the foundation for estimating chance agreement in\nsequence annotation tasks. Utilizing the proposed randomization model and a\nrelated comparison approach, we successfully derive the analytical form of the\ndistribution, enabling the computation of the probable location of each\nannotated text segment and subsequent chance agreement estimation. Through a\ncombination simulation and corpus-based evaluation, we successfully assess its\napplicability and validate its accuracy and efficacy.", "published": "2024-07-16 04:32:47", "link": "http://arxiv.org/abs/2407.11371v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "InvAgent: A Large Language Model based Multi-Agent System for Inventory\n  Management in Supply Chains", "abstract": "Supply chain management (SCM) involves coordinating the flow of goods,\ninformation, and finances across various entities to deliver products\nefficiently. Effective inventory management is crucial in today's volatile and\nuncertain world. Previous research has demonstrated the superiority of\nheuristic methods and reinforcement learning applications in inventory\nmanagement. However, the application of large language models (LLMs) as\nautonomous agents in multi-agent systems for inventory management remains\nunderexplored. This study introduces a novel approach using LLMs to manage\nmulti-agent inventory systems. Leveraging their zero-shot learning\ncapabilities, our model, InvAgent, enhances resilience and improves efficiency\nacross the supply chain network. Our contributions include utilizing LLMs for\nzero-shot learning to enable adaptive and informed decision-making without\nprior training, providing explainability and clarity through chain-of-thought,\nand demonstrating dynamic adaptability to varying demand scenarios while\nreducing costs and preventing stockouts. Extensive evaluations across different\nscenarios highlight the efficiency of our model in SCM.", "published": "2024-07-16 04:55:17", "link": "http://arxiv.org/abs/2407.11384v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting the Impact of Pursuing Modularity for Code Generation", "abstract": "Modular programming, which aims to construct the final program by integrating\nsmaller, independent building blocks, has been regarded as a desirable practice\nin software development. However, with the rise of recent code generation\nagents built upon large language models (LLMs), a question emerges: is this\ntraditional practice equally effective for these new tools? In this work, we\nassess the impact of modularity in code generation by introducing a novel\nmetric for its quantitative measurement. Surprisingly, unlike conventional\nwisdom on the topic, we find that modularity is not a core factor for improving\nthe performance of code generation models. We also explore potential\nexplanations for why LLMs do not exhibit a preference for modular code compared\nto non-modular code.", "published": "2024-07-16 05:48:24", "link": "http://arxiv.org/abs/2407.11406v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Representation Bias in Political Sample Simulations with Large Language\n  Models", "abstract": "This study seeks to identify and quantify biases in simulating political\nsamples with Large Language Models, specifically focusing on vote choice and\npublic opinion. Using the GPT-3.5-Turbo model, we leverage data from the\nAmerican National Election Studies, German Longitudinal Election Study, Zuobiao\nDataset, and China Family Panel Studies to simulate voting behaviors and public\nopinions. This methodology enables us to examine three types of representation\nbias: disparities based on the the country's language, demographic groups, and\npolitical regime types. The findings reveal that simulation performance is\ngenerally better for vote choice than for public opinions, more accurate in\nEnglish-speaking countries, more effective in bipartisan systems than in\nmulti-partisan systems, and stronger in democratic settings than in\nauthoritarian regimes. These results contribute to enhancing our understanding\nand developing strategies to mitigate biases in AI applications within the\nfield of computational social science.", "published": "2024-07-16 05:52:26", "link": "http://arxiv.org/abs/2407.11409v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SPINACH: SPARQL-Based Information Navigation for Challenging Real-World\n  Questions", "abstract": "Large Language Models (LLMs) have led to significant improvements in the\nKnowledge Base Question Answering (KBQA) task. However, datasets used in KBQA\nstudies do not capture the true complexity of KBQA tasks. They either have\nsimple questions, use synthetically generated logical forms, or are based on\nsmall knowledge base (KB) schemas.\n  We introduce the SPINACH dataset, an expert-annotated KBQA dataset collected\nfrom discussions on Wikidata's \"Request a Query\" forum with 320\ndecontextualized question-SPARQL pairs. The complexity of these in-the-wild\nqueries calls for a KBQA system that can dynamically explore large and often\nincomplete schemas and reason about them, as it is infeasible to create a\ncomprehensive training dataset.\n  We also introduce an in-context learning KBQA agent, also called SPINACH,\nthat mimics how a human expert would write SPARQLs to handle challenging\nquestions. SPINACH achieves a new state of the art on the QALD-7, QALD-9 Plus\nand QALD-10 datasets by 31.0%, 27.0%, and 10.0% in $F_1$, respectively, and\ncoming within 1.6% of the fine-tuned LLaMA SOTA model on WikiWebQuestions. On\nour new SPINACH dataset, the SPINACH agent outperforms all baselines, including\nthe best GPT-4-based KBQA agent, by at least 38.1% in $F_1$.", "published": "2024-07-16 06:18:21", "link": "http://arxiv.org/abs/2407.11417v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "States Hidden in Hidden States: LLMs Emerge Discrete State\n  Representations Implicitly", "abstract": "Large Language Models (LLMs) exhibit various emergent abilities. Among these\nabilities, some might reveal the internal working mechanisms of models. In this\npaper, we uncover a novel emergent capability in models: the intrinsic ability\nto perform extended sequences of calculations without relying on\nchain-of-thought step-by-step solutions. Remarkably, the most advanced models\ncan directly output the results of two-digit number additions with lengths\nextending up to 15 addends. We hypothesize that the model emerges Implicit\nDiscrete State Representations (IDSRs) within its hidden states and performs\nsymbolic calculations internally. To test this hypothesis, we design a sequence\nof experiments that look into the hidden states. Specifically, we first confirm\nthat IDSRs exist. Then, we provide interesting observations about the formation\nof IDSRs from layer, digit, and sequence perspectives. Finally, we confirm that\nmodels indeed use IDSRs to produce the final answers. However, we also discover\nthat these state representations are far from lossless in current open-sourced\nmodels, leading to inaccuracies in their final performance. Our work presents a\nnovel exploration of LLMs' symbolic calculation abilities and the underlying\nmechanisms.", "published": "2024-07-16 06:27:22", "link": "http://arxiv.org/abs/2407.11421v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Trust No Bot: Discovering Personal Disclosures in Human-LLM\n  Conversations in the Wild", "abstract": "Measuring personal disclosures made in human-chatbot interactions can provide\na better understanding of users' AI literacy and facilitate privacy research\nfor large language models (LLMs). We run an extensive, fine-grained analysis on\nthe personal disclosures made by real users to commercial GPT models,\ninvestigating the leakage of personally identifiable and sensitive information.\nTo understand the contexts in which users disclose to chatbots, we develop a\ntaxonomy of tasks and sensitive topics, based on qualitative and quantitative\nanalysis of naturally occurring conversations. We discuss these potential\nprivacy harms and observe that: (1) personally identifiable information (PII)\nappears in unexpected contexts such as in translation or code editing (48% and\n16% of the time, respectively) and (2) PII detection alone is insufficient to\ncapture the sensitive topics that are common in human-chatbot interactions,\nsuch as detailed sexual preferences or specific drug use habits. We believe\nthat these high disclosure rates are of significant importance for researchers\nand data curators, and we call for the design of appropriate nudging mechanisms\nto help users moderate their interactions.", "published": "2024-07-16 07:05:31", "link": "http://arxiv.org/abs/2407.11438v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Scientific QA System with Verifiable Answers", "abstract": "In this paper, we introduce the VerifAI project, a pioneering open-source\nscientific question-answering system, designed to provide answers that are not\nonly referenced but also automatically vetted and verifiable. The components of\nthe system are (1) an Information Retrieval system combining semantic and\nlexical search techniques over scientific papers (PubMed), (2) a\nRetrieval-Augmented Generation (RAG) module using fine-tuned generative model\n(Mistral 7B) and retrieved articles to generate claims with references to the\narticles from which it was derived, and (3) a Verification engine, based on a\nfine-tuned DeBERTa and XLM-RoBERTa models on Natural Language Inference task\nusing SciFACT dataset. The verification engine cross-checks the generated claim\nand the article from which the claim was derived, verifying whether there may\nhave been any hallucinations in generating the claim. By leveraging the\nInformation Retrieval and RAG modules, Verif.ai excels in generating factual\ninformation from a vast array of scientific sources. At the same time, the\nVerification engine rigorously double-checks this output, ensuring its accuracy\nand reliability. This dual-stage process plays a crucial role in acquiring and\nconfirming factual information, significantly enhancing the information\nlandscape. Our methodology could significantly enhance scientists'\nproductivity, concurrently fostering trust in applying generative language\nmodels within scientific domains, where hallucinations and misinformation are\nunacceptable.", "published": "2024-07-16 08:21:02", "link": "http://arxiv.org/abs/2407.11485v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AdaptEval: Evaluating Large Language Models on Domain Adaptation for\n  Text Summarization", "abstract": "Despite the advances in the abstractive summarization task using Large\nLanguage Models (LLM), there is a lack of research that asses their abilities\nto easily adapt to different domains. We evaluate the domain adaptation\nabilities of a wide range of LLMs on the summarization task across various\ndomains in both fine-tuning and in-context learning settings. We also present\nAdaptEval, the first domain adaptation evaluation suite. AdaptEval includes a\ndomain benchmark and a set of metrics to facilitate the analysis of domain\nadaptation. Our results demonstrate that LLMs exhibit comparable performance in\nthe in-context learning setting, regardless of their parameter scale.", "published": "2024-07-16 10:50:39", "link": "http://arxiv.org/abs/2407.11591v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ECoh: Turn-level Coherence Evaluation for Multilingual Dialogues", "abstract": "Despite being heralded as the new standard for dialogue evaluation, the\nclosed-source nature of GPT-4 poses challenges for the community. Motivated by\nthe need for lightweight, open source, and multilingual dialogue evaluators,\nthis paper introduces GenResCoh (Generated Responses targeting Coherence).\nGenResCoh is a novel LLM generated dataset comprising over 130k negative and\npositive responses and accompanying explanations seeded from XDailyDialog and\nXPersona covering English, French, German, Italian, and Chinese. Leveraging\nGenResCoh, we propose ECoh (Evaluation of Coherence), a family of evaluators\ntrained to assess response coherence across multiple languages. Experimental\nresults demonstrate that ECoh achieves multilingual detection capabilities\nsuperior to the teacher model (GPT-3.5-Turbo) on GenResCoh, despite being based\non a much smaller architecture. Furthermore, the explanations provided by ECoh\nclosely align in terms of quality with those generated by the teacher model.", "published": "2024-07-16 12:28:30", "link": "http://arxiv.org/abs/2407.11660v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MINI-LLM: Memory-Efficient Structured Pruning for Large Language Models", "abstract": "As Large Language Models (LLMs) grow dramatically in size, there is an\nincreasing trend in compressing and speeding up these models. Previous studies\nhave highlighted the usefulness of gradients for importance scoring in neural\nnetwork compressing, especially in pruning medium-size networks. However, the\nsubstantial memory requirements involved in calculating gradients with\nbackpropagation impede the utilization of gradients in guiding LLM pruning. As\na result, most pruning strategies for LLMs rely on gradient-free criteria, such\nas weight magnitudes or a mix of magnitudes and activations. In this paper, we\ndevise a hybrid pruning criterion, which appropriately integrates magnitude,\nactivation, and gradient to capitalize on feature map sensitivity for pruning\nLLMs. To overcome memory requirement barriers, we estimate gradients using only\nforward passes. Based on this, we propose a Memory-effIcieNt structured prunIng\nprocedure for LLMs (MINI-LLM) to remove no-critical channels and\nmulti-attention heads. Experimental results demonstrate the superior\nperformance of MINI-LLM over existing gradient-free methods on three LLMs:\nLLaMA, BLOOM, and OPT across various downstream tasks (classification,\nmultiple-choice, and generation), while MINI-LLM maintains a GPU memory\nfootprint akin to gradient-free methods.", "published": "2024-07-16 12:59:44", "link": "http://arxiv.org/abs/2407.11681v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Are LLMs Mitigating Stereotyping Harms? Learning from Search Engine\n  Studies", "abstract": "With the widespread availability of LLMs since the release of ChatGPT and\nincreased public scrutiny, commercial model development appears to have focused\ntheir efforts on 'safety' training concerning legal liabilities at the expense\nof social impact evaluation. This mimics a similar trend which we could observe\nfor search engine autocompletion some years prior. We draw on scholarship from\nNLP and search engine auditing and present a novel evaluation task in the style\nof autocompletion prompts to assess stereotyping in LLMs. We assess LLMs by\nusing four metrics, namely refusal rates, toxicity, sentiment and regard, with\nand without safety system prompts. Our findings indicate an improvement to\nstereotyping outputs with the system prompt, but overall a lack of attention by\nLLMs under study to certain harms classified as toxic, particularly for prompts\nabout peoples/ethnicities and sexual orientation. Mentions of intersectional\nidentities trigger a disproportionate amount of stereotyping. Finally, we\ndiscuss the implications of these findings about stereotyping harms in light of\nthe coming intermingling of LLMs and search and the choice of stereotyping\nmitigation policy to adopt. We address model builders, academics, NLP\npractitioners and policy makers, calling for accountability and awareness\nconcerning stereotyping harms, be it for training data curation, leader board\ndesign and usage, or social impact measurement.", "published": "2024-07-16 14:04:35", "link": "http://arxiv.org/abs/2407.11733v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Robust Utility-Preserving Text Anonymization Based on Large Language\n  Models", "abstract": "Text anonymization is crucial for sharing sensitive data while maintaining\nprivacy. Existing techniques face the emerging challenges of re-identification\nattack ability of Large Language Models (LLMs), which have shown advanced\ncapability in memorizing detailed information and patterns as well as\nconnecting disparate pieces of information. In defending against LLM-based\nre-identification attacks, anonymization could jeopardize the utility of the\nresulting anonymized data in downstream tasks -- the trade-off between privacy\nand data utility requires deeper understanding within the context of LLMs. This\npaper proposes a framework composed of three LLM-based components -- a privacy\nevaluator, a utility evaluator, and an optimization component, which work\ncollaboratively to perform anonymization. To provide a practical model for\nlarge-scale and real-time environments, we distill the anonymization\ncapabilities into a lightweight model using Direct Preference Optimization\n(DPO). Extensive experiments demonstrate that the proposed models outperform\nbaseline models, showing robustness in reducing the risk of re-identification\nwhile preserving greater data utility in downstream tasks. Our code and dataset\nare available at https://github.com/UKPLab/arxiv2024-rupta.", "published": "2024-07-16 14:28:56", "link": "http://arxiv.org/abs/2407.11770v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Educational Personalized Learning Path Planning with Large Language\n  Models", "abstract": "Educational Personalized Learning Path Planning (PLPP) aims to tailor\nlearning experiences to individual learners' needs, enhancing learning\nefficiency and engagement. Despite its potential, traditional PLPP systems\noften lack adaptability, interactivity, and transparency. This paper proposes a\nnovel approach integrating Large Language Models (LLMs) with prompt engineering\nto address these challenges. By designing prompts that incorporate\nlearner-specific information, our method guides LLMs like LLama-2-70B and GPT-4\nto generate personalized, coherent, and pedagogically sound learning paths. We\nconducted experiments comparing our method with a baseline approach across\nvarious metrics, including accuracy, user satisfaction, and the quality of\nlearning paths. The results show significant improvements in all areas,\nparticularly with GPT-4, demonstrating the effectiveness of prompt engineering\nin enhancing PLPP. Additional long-term impact analysis further validates our\nmethod's potential to improve learner performance and retention. This research\nhighlights the promise of LLMs and prompt engineering in advancing personalized\neducation.", "published": "2024-07-16 14:32:56", "link": "http://arxiv.org/abs/2407.11773v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Novel Lexicon for the Moral Foundation of Liberty", "abstract": "The moral value of liberty is a central concept in our inference system when\nit comes to taking a stance towards controversial social issues such as vaccine\nhesitancy, climate change, or the right to abortion. Here, we propose a novel\nLiberty lexicon evaluated on more than 3,000 manually annotated data both in\nin- and out-of-domain scenarios. As a result of this evaluation, we produce a\ncombined lexicon that constitutes the main outcome of this work. This final\nlexicon incorporates information from an ensemble of lexicons that have been\ngenerated using word embedding similarity (WE) and compositional semantics\n(CS). Our key contributions include enriching the liberty annotations,\ndeveloping a robust liberty lexicon for broader application, and revealing the\ncomplexity of expressions related to liberty across different platforms.\nThrough the evaluation, we show that the difficulty of the task calls for\ndesigning approaches that combine knowledge, in an effort of improving the\nrepresentations of learning systems.", "published": "2024-07-16 15:49:05", "link": "http://arxiv.org/abs/2407.11862v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Localizing and Mitigating Errors in Long-form Question Answering", "abstract": "Long-form question answering (LFQA) aims to provide thorough and in-depth\nanswers to complex questions, enhancing comprehension. However, such detailed\nresponses are prone to hallucinations and factual inconsistencies, challenging\ntheir faithful evaluation. This work introduces HaluQuestQA, the first\nhallucination dataset with localized error annotations for human-written and\nmodel-generated LFQA answers. HaluQuestQA comprises 698 QA pairs with 1.8k\nspan-level error annotations for five different error types by expert\nannotators, along with preference judgments. Using our collected data, we\nthoroughly analyze the shortcomings of long-form answers and find that they\nlack comprehensiveness and provide unhelpful references. We train an automatic\nfeedback model on this dataset that predicts error spans with incomplete\ninformation and provides associated explanations. Finally, we propose a\nprompt-based approach, Error-informed refinement, that uses signals from the\nlearned feedback model to refine generated answers, which we show reduces\nerrors and improves answer quality across multiple models. Furthermore, humans\nfind answers generated by our approach comprehensive and highly prefer them\n(84%) over the baseline answers.", "published": "2024-07-16 17:23:16", "link": "http://arxiv.org/abs/2407.11930v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NeedleBench: Can LLMs Do Retrieval and Reasoning in 1 Million Context\n  Window?", "abstract": "In evaluating the long-context capabilities of large language models (LLMs),\nidentifying content relevant to a user's query from original long documents is\na crucial prerequisite for any LLM to answer questions based on long text. We\npresent NeedleBench, a framework consisting of a series of progressively more\nchallenging tasks for assessing bilingual long-context capabilities, spanning\nmultiple length intervals (4k, 8k, 32k, 128k, 200k, 1000k, and beyond) and\ndifferent depth ranges, allowing the strategic insertion of critical data\npoints in different text depth zones to rigorously test the retrieval and\nreasoning capabilities of models in diverse contexts. We use the NeedleBench\nframework to assess how well the leading open-source models can identify key\ninformation relevant to the question and apply that information to reasoning in\nbilingual long texts. Furthermore, we propose the Ancestral Trace Challenge\n(ATC) to mimic the complexity of logical reasoning challenges that are likely\nto be present in real-world long-context tasks, providing a simple method for\nevaluating LLMs in dealing with complex long-context situations. Our results\nsuggest that current LLMs have significant room for improvement in practical\nlong-context applications, as they struggle with the complexity of logical\nreasoning challenges that are likely to be present in real-world long-context\ntasks. All codes and resources are available at OpenCompass:\nhttps://github.com/open-compass/opencompass.", "published": "2024-07-16 17:59:06", "link": "http://arxiv.org/abs/2407.11963v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Identifying Speakers in Dialogue Transcripts: A Text-based Approach\n  Using Pretrained Language Models", "abstract": "We introduce an approach to identifying speaker names in dialogue\ntranscripts, a crucial task for enhancing content accessibility and\nsearchability in digital media archives. Despite the advancements in speech\nrecognition, the task of text-based speaker identification (SpeakerID) has\nreceived limited attention, lacking large-scale, diverse datasets for effective\nmodel training. Addressing these gaps, we present a novel, large-scale dataset\nderived from the MediaSum corpus, encompassing transcripts from a wide range of\nmedia sources. We propose novel transformer-based models tailored for\nSpeakerID, leveraging contextual cues within dialogues to accurately attribute\nspeaker names. Through extensive experiments, our best model achieves a great\nprecision of 80.3\\%, setting a new benchmark for SpeakerID. The data and code\nare publicly available here:\n\\url{https://github.com/adobe-research/speaker-identification}", "published": "2024-07-16 18:03:58", "link": "http://arxiv.org/abs/2407.12094v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MASIVE: Open-Ended Affective State Identification in English and Spanish", "abstract": "In the field of emotion analysis, much NLP research focuses on identifying a\nlimited number of discrete emotion categories, often applied across languages.\nThese basic sets, however, are rarely designed with textual data in mind, and\nculture, language, and dialect can influence how particular emotions are\ninterpreted. In this work, we broaden our scope to a practically unbounded set\nof \\textit{affective states}, which includes any terms that humans use to\ndescribe their experiences of feeling. We collect and publish MASIVE, a dataset\nof Reddit posts in English and Spanish containing over 1,000 unique affective\nstates each. We then define the new problem of \\textit{affective state\nidentification} for language generation models framed as a masked span\nprediction task. On this task, we find that smaller finetuned multilingual\nmodels outperform much larger LLMs, even on region-specific Spanish affective\nstates. Additionally, we show that pretraining on MASIVE improves model\nperformance on existing emotion benchmarks. Finally, through machine\ntranslation experiments, we find that native speaker-written data is vital to\ngood performance on this task.", "published": "2024-07-16 21:43:47", "link": "http://arxiv.org/abs/2407.12196v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reliable Reasoning Beyond Natural Language", "abstract": "Despite their linguistic competence, Large Language models (LLMs) often\nexhibit limitations in their ability to reason reliably and flexibly. To\naddress this, we propose a neurosymbolic approach that prompts LLMs to extract\nand encode all relevant information from a problem statement as logical code\nstatements, and then use a logic programming language (Prolog) to conduct the\niterative computations of explicit deductive reasoning. Our approach\nsignificantly enhances the performance of LLMs on the standard mathematical\nreasoning benchmark, GSM8k, and the Navigate dataset from the BIG-bench\ndataset. Additionally, we introduce a novel dataset, the Non-Linear Reasoning\n(NLR) dataset, consisting of 55 unique word problems that target the\nshortcomings of the next token prediction paradigm of LLMs and require complex\nnon-linear reasoning but only basic arithmetic skills to solve. Our findings\ndemonstrate that the integration of Prolog enables LLMs to achieve high\nperformance on the NLR dataset, which even the most advanced language models\n(including GPT4) fail to solve using text only.", "published": "2024-07-16 04:34:18", "link": "http://arxiv.org/abs/2407.11373v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Oscars of AI Theater: A Survey on Role-Playing with Language Models", "abstract": "This survey explores the burgeoning field of role-playing with language\nmodels, focusing on their development from early persona-based models to\nadvanced character-driven simulations facilitated by Large Language Models\n(LLMs). Initially confined to simple persona consistency due to limited model\ncapabilities, role-playing tasks have now expanded to embrace complex character\nportrayals involving character consistency, behavioral alignment, and overall\nattractiveness. We provide a comprehensive taxonomy of the critical components\nin designing these systems, including data, models and alignment, agent\narchitecture and evaluation. This survey not only outlines the current\nmethodologies and challenges, such as managing dynamic personal profiles and\nachieving high-level persona consistency but also suggests avenues for future\nresearch in improving the depth and realism of role-playing applications. The\ngoal is to guide future research by offering a structured overview of current\nmethodologies and identifying potential areas for improvement. Related\nresources and papers are available at\nhttps://github.com/nuochenpku/Awesome-Role-Play-Papers.", "published": "2024-07-16 08:20:39", "link": "http://arxiv.org/abs/2407.11484v9", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Fine-Tuning Medical Language Models for Enhanced Long-Contextual\n  Understanding and Domain Expertise", "abstract": "Large Language Models (LLMs) have been widely applied in various professional\nfields. By fine-tuning the models using domain specific question and answer\ndatasets, the professional domain knowledge and Q\\&A abilities of these models\nhave significantly improved, for example, medical professional LLMs that use\nfine-tuning of doctor-patient Q\\&A data exhibit extraordinary disease\ndiagnostic abilities. However, we observed that despite improvements in\nspecific domain knowledge, the performance of medical LLM in long-context\nunderstanding has significantly declined, especially compared to general\nlanguage models with similar parameters. The purpose of this study is to\ninvestigate the phenomenon of reduced performance in understanding long-context\nin medical LLM. We designed a series of experiments to conduct open-book\nprofessional knowledge exams on all models to evaluate their ability to read\nlong-context. By adjusting the proportion and quantity of general data and\nmedical data in the process of fine-tuning, we can determine the best data\ncomposition to optimize the professional model and achieve a balance between\nlong-context performance and specific domain knowledge.", "published": "2024-07-16 09:37:20", "link": "http://arxiv.org/abs/2407.11536v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "How Personality Traits Influence Negotiation Outcomes? A Simulation\n  based on Large Language Models", "abstract": "Psychological evidence reveals the influence of personality traits on\ndecision-making. For instance, agreeableness is generally associated with\npositive outcomes in negotiations, whereas neuroticism is often linked to less\nfavorable outcomes. This paper introduces a simulation framework centered on\nLarge Language Model (LLM) agents endowed with synthesized personality traits.\nThe agents negotiate within bargaining domains and possess customizable\npersonalities and objectives. The experimental results show that the behavioral\ntendencies of LLM-based simulations could reproduce behavioral patterns\nobserved in human negotiations. The contribution is twofold. First, we propose\na simulation methodology that investigates the alignment between the linguistic\nand economic capabilities of LLM agents. Secondly, we offer empirical insights\ninto the strategic impact of Big-Five personality traits on the outcomes of\nbilateral negotiations. We also provide a case study based on synthesized\nbargaining dialogues to reveal intriguing behaviors, including deceitful and\ncompromising behaviors.", "published": "2024-07-16 09:52:51", "link": "http://arxiv.org/abs/2407.11549v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference", "abstract": "Large Language Models have excelled in various domains but face efficiency\nchallenges due to the growing Key-Value (KV) cache required for long-sequence\ninference. Recent efforts aim to reduce KV cache size by evicting vast\nnon-critical cache elements during runtime while preserving generation quality.\nHowever, these methods typically allocate compression budgets uniformly across\nall attention heads, ignoring the unique attention patterns of each head. In\nthis paper, we establish a theoretical loss upper bound between pre- and\npost-eviction attention output, explaining the optimization target of prior\ncache eviction methods, while guiding the optimization of adaptive budget\nallocation. Base on this, we propose {\\it Ada-KV}, the first head-wise adaptive\nbudget allocation strategy. It offers plug-and-play benefits, enabling seamless\nintegration with prior cache eviction methods. Extensive evaluations on 13\ndatasets from Ruler and 16 datasets from LongBench, all conducted under both\nquestion-aware and question-agnostic scenarios, demonstrate substantial quality\nimprovements over existing methods.", "published": "2024-07-16 09:53:32", "link": "http://arxiv.org/abs/2407.11550v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Comprehensive Evaluation of Large Language Models on Temporal Event\n  Forecasting", "abstract": "Recently, Large Language Models (LLMs) have demonstrated great potential in\nvarious data mining tasks, such as knowledge question answering, mathematical\nreasoning, and commonsense reasoning. However, the reasoning capability of LLMs\non temporal event forecasting has been under-explored. To systematically\ninvestigate their abilities in temporal event forecasting, we conduct a\ncomprehensive evaluation of LLM-based methods for temporal event forecasting.\nDue to the lack of a high-quality dataset that involves both graph and textual\ndata, we first construct a benchmark dataset, named MidEast-TE-mini. Based on\nthis dataset, we design a series of baseline methods, characterized by various\ninput formats and retrieval augmented generation(RAG) modules. From extensive\nexperiments, we find that directly integrating raw texts into the input of LLMs\ndoes not enhance zero-shot extrapolation performance. In contrast,\nincorporating raw texts in specific complex events and fine-tuning LLMs\nsignificantly improves performance. Moreover, enhanced with retrieval modules,\nLLM can effectively capture temporal relational patterns hidden in historical\nevents. Meanwhile, issues such as popularity bias and the long-tail problem\nstill persist in LLMs, particularly in the RAG-based method. These findings not\nonly deepen our understanding of LLM-based event forecasting methods but also\nhighlight several promising research directions.We consider that this\ncomprehensive evaluation, along with the identified research opportunities,\nwill significantly contribute to future research on temporal event forecasting\nthrough LLMs.", "published": "2024-07-16 11:58:54", "link": "http://arxiv.org/abs/2407.11638v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "CCoE: A Compact and Efficient LLM Framework with Multi-Expert\n  Collaboration for Resource-Limited Settings", "abstract": "Large Language Models (LLMs) have achieved exceptional performance across\ndiverse domains through training on massive datasets. However, scaling LLMs to\nsupport multiple downstream domain applications remains a significant\nchallenge, especially under resource constraints. Existing approaches often\nstruggle to balance performance across multiple domains with resource\nefficiency, limiting their broader applicability. To address this, we introduce\nthe CCoE architecture, a modular framework that seamlessly integrates\ndomain-specific experts into a unified LLM. By leveraging independently trained\nexpert subnetworks on a shared backbone partition, CCoE achieves\nstate-of-the-art performance while significantly reducing the resource\nrequirements for multi-expert deployments. Furthermore, rule-based gating and\nexpert planning in CCoE enable flexible task allocation, promoting expert\ncollaboration to handle complex reasoning tasks. CCoE not only reduces\ninference costs but also provides a flexible and scalable solution for\nintegrating domain expertise across diverse applications. Experiments on five\ndomains demonstrate that CCoE achieves comparable performance to current\ndomain-specific LLMs. Moreover, compared to existing multi-domain model\nensemble methods, CCoE reduces memory usage by 61.3%, while improving inference\nefficiency by 0.76x over parameter-efficient multi-expert integration\napproaches.", "published": "2024-07-16 13:03:58", "link": "http://arxiv.org/abs/2407.11686v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Vectoring Languages", "abstract": "Recent breakthroughs in large language models (LLM) have stirred up global\nattention, and the research has been accelerating non-stop since then.\nPhilosophers and psychologists have also been researching the structure of\nlanguage for decades, but they are having a hard time finding a theory that\ndirectly benefits from the breakthroughs of LLMs. In this article, we propose a\nnovel structure of language that reflects well on the mechanisms behind\nlanguage models and go on to show that this structure is also better at\ncapturing the diverse nature of language compared to previous methods. An\nanalogy of linear algebra is adapted to strengthen the basis of this\nperspective. We further argue about the difference between this perspective and\nthe design philosophy for current language models. Lastly, we discuss how this\nperspective can lead us to research directions that may accelerate the\nimprovements of science fastest.", "published": "2024-07-16 14:25:55", "link": "http://arxiv.org/abs/2407.11766v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sharif-MGTD at SemEval-2024 Task 8: A Transformer-Based Approach to\n  Detect Machine Generated Text", "abstract": "Detecting Machine-Generated Text (MGT) has emerged as a significant area of\nstudy within Natural Language Processing. While language models generate text,\nthey often leave discernible traces, which can be scrutinized using either\ntraditional feature-based methods or more advanced neural language models. In\nthis research, we explore the effectiveness of fine-tuning a RoBERTa-base\ntransformer, a powerful neural architecture, to address MGT detection as a\nbinary classification task. Focusing specifically on Subtask A\n(Monolingual-English) within the SemEval-2024 competition framework, our\nproposed system achieves an accuracy of 78.9% on the test dataset, positioning\nus at 57th among participants. Our study addresses this challenge while\nconsidering the limited hardware resources, resulting in a system that excels\nat identifying human-written texts but encounters challenges in accurately\ndiscerning MGTs.", "published": "2024-07-16 14:33:01", "link": "http://arxiv.org/abs/2407.11774v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SwitchCIT: Switching for Continual Instruction Tuning", "abstract": "Large language models (LLMs) and multimodal models (MMs) have exhibited\nimpressive capabilities in various domains, particularly in general language\nunderstanding and visual reasoning. However, these models, trained on massive\ndata, may not be finely optimized for specific tasks triggered by instructions.\nContinual instruction tuning is crucial to adapt a large model to evolving\ntasks and domains, ensuring their effectiveness and relevance across a wide\nrange of applications. In the context of continual instruction tuning, where\nmodels are sequentially trained on different tasks, catastrophic forgetting can\noccur, leading to performance degradation on previously learned tasks. This\nwork addresses the catastrophic forgetting in continual instruction learning\nthrough a switching mechanism for routing computations to parameter-efficient\ntuned models. We demonstrate the effectiveness of our method through\nexperiments on continual instruction tuning of different natural language\ngeneration tasks and vision-language tasks. We also showcase the advantages of\nour proposed method in terms of efficiency, scalability, portability, and\nprivacy preservation.", "published": "2024-07-16 14:37:33", "link": "http://arxiv.org/abs/2407.11780v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GPT Assisted Annotation of Rhetorical and Linguistic Features for\n  Interpretable Propaganda Technique Detection in News Text", "abstract": "While the use of machine learning for the detection of propaganda techniques\nin text has garnered considerable attention, most approaches focus on\n\"black-box\" solutions with opaque inner workings. Interpretable approaches\nprovide a solution, however, they depend on careful feature engineering and\ncostly expert annotated data. Additionally, language features specific to\npropagandistic text are generally the focus of rhetoricians or linguists, and\nthere is no data set labeled with such features suitable for machine learning.\nThis study codifies 22 rhetorical and linguistic features identified in\nliterature related to the language of persuasion for the purpose of annotating\nan existing data set labeled with propaganda techniques. To help human experts\nannotate natural language sentences with these features, RhetAnn, a web\napplication, was specifically designed to minimize an otherwise considerable\nmental effort. Finally, a small set of annotated data was used to fine-tune\nGPT-3.5, a generative large language model (LLM), to annotate the remaining\ndata while optimizing for financial cost and classification accuracy. This\nstudy demonstrates how combining a small number of human annotated examples\nwith GPT can be an effective strategy for scaling the annotation process at a\nfraction of the cost of traditional annotation relying solely on human experts.\nThe results are on par with the best performing model at the time of writing,\nnamely GPT-4, at 10x less the cost. Our contribution is a set of features,\ntheir properties, definitions, and examples in a machine-readable format, along\nwith the code for RhetAnn and the GPT prompts and fine-tuning procedures for\nadvancing state-of-the-art interpretable propaganda technique detection.", "published": "2024-07-16 15:15:39", "link": "http://arxiv.org/abs/2407.11827v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LoFTI: Localization and Factuality Transfer to Indian Locales", "abstract": "Large language models (LLMs) encode vast amounts of world knowledge acquired\nvia training on large web-scale datasets crawled from the internet. However,\nthese datasets typically exhibit a geographical bias towards English-speaking\nWestern countries. This results in LLMs producing biased or hallucinated\nresponses to queries that require answers localized to other geographical\nregions. In this work, we introduce a new benchmark named LoFTI (Localization\nand Factuality Transfer to Indian Locales) that can be used to evaluate an\nLLM's localization and factual text transfer capabilities. LoFTI consists of\nfactual statements about entities in source and target locations; the source\nlocations are spread across the globe and the target locations are all within\nIndia with varying degrees of hyperlocality (country, states, cities). The\nentities span a wide variety of categories. We use LoFTI to evaluate Mixtral,\nGPT-4 and two other Mixtral-based approaches well-suited to the task of\nlocalized factual transfer. We demonstrate that LoFTI is a high-quality\nevaluation benchmark and all the models, including GPT-4, produce skewed\nresults across varying levels of hyperlocality.", "published": "2024-07-16 15:20:43", "link": "http://arxiv.org/abs/2407.11833v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Preemptive Detection and Correction of Misaligned Actions in LLM Agents", "abstract": "Deploying LLM-based agents in real-life applications often faces a critical\nchallenge: the misalignment between agents' behavior and user intent. Such\nmisalignment may lead agents to unintentionally execute critical actions that\ncarry negative outcomes (e.g., accidentally triggering a \"buy-now\" in web\nshopping), resulting in undesirable or even irreversible consequences. Although\naddressing these issues is crucial, the preemptive detection and correction of\nmisaligned actions remains relatively underexplored. To fill this gap, we\nintroduce InferAct, a novel approach that leverages the belief reasoning\nability of LLMs, grounded in Theory-of-Mind, to detect misaligned actions\nbefore execution. Once the misalignment is detected, InferAct alerts users for\ntimely correction, preventing adverse outcomes and enhancing the reliability of\nLLM agents' decision-making processes. Experiments on three widely used tasks\ndemonstrate that InferAct achieves up to 20% improvements on Marco-F1 against\nbaselines in misaligned action detection. An in-depth evaluation of\nmisalignment correction further highlights InferAct's effectiveness in\nimproving agent alignment.", "published": "2024-07-16 15:24:44", "link": "http://arxiv.org/abs/2407.11843v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in\n  Grammatical Error Detection", "abstract": "Grammatical Error Detection (GED) methods rely heavily on human annotated\nerror corpora. However, these annotations are unavailable in many low-resource\nlanguages. In this paper, we investigate GED in this context. Leveraging the\nzero-shot cross-lingual transfer capabilities of multilingual pre-trained\nlanguage models, we train a model using data from a diverse set of languages to\ngenerate synthetic errors in other languages. These synthetic error corpora are\nthen used to train a GED model. Specifically we propose a two-stage fine-tuning\npipeline where the GED model is first fine-tuned on multilingual synthetic data\nfrom target languages followed by fine-tuning on human-annotated GED corpora\nfrom source languages. This approach outperforms current state-of-the-art\nannotation-free GED methods. We also analyse the errors produced by our method\nand other strong baselines, finding that our approach produces errors that are\nmore diverse and more similar to human errors.", "published": "2024-07-16 15:35:15", "link": "http://arxiv.org/abs/2407.11854v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluating Task-Oriented Dialogue Consistency through Constraint\n  Satisfaction", "abstract": "Task-oriented dialogues must maintain consistency both within the dialogue\nitself, ensuring logical coherence across turns, and with the conversational\ndomain, accurately reflecting external knowledge. We propose to conceptualize\ndialogue consistency as a Constraint Satisfaction Problem (CSP), wherein\nvariables represent segments of the dialogue referencing the conversational\ndomain, and constraints among variables reflect dialogue properties, including\nlinguistic, conversational, and domain-based aspects. To demonstrate the\nfeasibility of the approach, we utilize a CSP solver to detect inconsistencies\nin dialogues re-lexicalized by an LLM. Our findings indicate that: (i) CSP is\neffective to detect dialogue inconsistencies; and (ii) consistent dialogue\nre-lexicalization is challenging for state-of-the-art LLMs, achieving only a\n0.15 accuracy rate when compared to a CSP solver. Furthermore, through an\nablation study, we reveal that constraints derived from domain knowledge pose\nthe greatest difficulty in being respected. We argue that CSP captures core\nproperties of dialogue consistency that have been poorly considered by\napproaches based on component pipelines.", "published": "2024-07-16 15:38:41", "link": "http://arxiv.org/abs/2407.11857v1", "categories": ["cs.CL", "cs.SC"], "primary_category": "cs.CL"}
{"title": "What's Wrong? Refining Meeting Summaries with LLM Feedback", "abstract": "Meeting summarization has become a critical task since digital encounters\nhave become a common practice. Large language models (LLMs) show great\npotential in summarization, offering enhanced coherence and context\nunderstanding compared to traditional methods. However, they still struggle to\nmaintain relevance and avoid hallucination. We introduce a multi-LLM correction\napproach for meeting summarization using a two-phase process that mimics the\nhuman review process: mistake identification and summary refinement. We release\nQMSum Mistake, a dataset of 200 automatically generated meeting summaries\nannotated by humans on nine error types, including structural, omission, and\nirrelevance errors. Our experiments show that these errors can be identified\nwith high accuracy by an LLM. We transform identified mistakes into actionable\nfeedback to improve the quality of a given summary measured by relevance,\ninformativeness, conciseness, and coherence. This post-hoc refinement\neffectively improves summary quality by leveraging multiple LLMs to validate\noutput quality. Our multi-LLM approach for meeting summarization shows\npotential for similar complex text generation tasks requiring robustness,\naction planning, and discussion towards a goal.", "published": "2024-07-16 17:10:16", "link": "http://arxiv.org/abs/2407.11919v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Rethinking Transformer-based Multi-document Summarization: An Empirical\n  Investigation", "abstract": "The utilization of Transformer-based models prospers the growth of\nmulti-document summarization (MDS). Given the huge impact and widespread\nadoption of Transformer-based models in various natural language processing\ntasks, investigating their performance and behaviors in the context of MDS\nbecomes crucial for advancing the field and enhancing the quality of summary.\nTo thoroughly examine the behaviours of Transformer-based MDS models, this\npaper presents five empirical studies on (1) measuring the impact of document\nboundary separators quantitatively; (2) exploring the effectiveness of\ndifferent mainstream Transformer structures; (3) examining the sensitivity of\nthe encoder and decoder; (4) discussing different training strategies; and (5)\ndiscovering the repetition in a summary generation. The experimental results on\nprevalent MDS datasets and eleven evaluation metrics show the influence of\ndocument boundary separators, the granularity of different level features and\ndifferent model training strategies. The results also reveal that the decoder\nexhibits greater sensitivity to noises compared to the encoder. This\nunderscores the important role played by the decoder, suggesting a potential\ndirection for future research in MDS. Furthermore, the experimental results\nindicate that the repetition problem in the generated summaries has\ncorrelations with the high uncertainty scores.", "published": "2024-07-16 17:42:37", "link": "http://arxiv.org/abs/2407.11948v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GoldFinch: High Performance RWKV/Transformer Hybrid with Linear Pre-Fill\n  and Extreme KV-Cache Compression", "abstract": "We introduce GoldFinch, a hybrid Linear Attention/Transformer sequence model\nthat uses a new technique to efficiently generate a highly compressed and\nreusable KV-Cache in linear time and space with respect to sequence length.\nGoldFinch stacks our new GOLD transformer on top of an enhanced version of the\nFinch (RWKV-6) architecture. We train up to 1.5B parameter class models of the\nFinch, Llama, and GoldFinch architectures, and find dramatically improved\nmodeling performance relative to both Finch and Llama. Our cache size savings\nincrease linearly with model layer count, ranging from 756-2550 times smaller\nthan the traditional transformer cache for common sizes, enabling inference of\nextremely large context lengths even on limited hardware. Although\nautoregressive generation has O(n) time complexity per token because of\nattention, pre-fill computation of the entire initial cache state for a\nsubmitted context costs only O(1) time per token due to the use of a recurrent\nneural network (RNN) to generate this cache. We release our trained weights and\ntraining code under the Apache 2.0 license for community use.", "published": "2024-07-16 18:00:00", "link": "http://arxiv.org/abs/2407.12077v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Better RAG using Relevant Information Gain", "abstract": "A common way to extend the memory of large language models (LLMs) is by\nretrieval augmented generation (RAG), which inserts text retrieved from a\nlarger memory into an LLM's context window. However, the context window is\ntypically limited to several thousand tokens, which limits the number of\nretrieved passages that can inform a model's response. For this reason, it's\nimportant to avoid occupying context window space with redundant information by\nensuring a degree of diversity among retrieved passages. At the same time, the\ninformation should also be relevant to the current task. Most prior methods\nthat encourage diversity among retrieved results, such as Maximal Marginal\nRelevance (MMR), do so by incorporating an objective that explicitly trades off\ndiversity and relevance. We propose a novel simple optimization metric based on\nrelevant information gain, a probabilistic measure of the total information\nrelevant to a query for a set of retrieved results. By optimizing this metric,\ndiversity organically emerges from our system. When used as a drop-in\nreplacement for the retrieval component of a RAG system, this method yields\nstate-of-the-art performance on question answering tasks from the Retrieval\nAugmented Generation Benchmark (RGB), outperforming existing metrics that\ndirectly optimize for relevance and diversity.", "published": "2024-07-16 18:09:21", "link": "http://arxiv.org/abs/2407.12101v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLMs-in-the-loop Part-1: Expert Small AI Models for Bio-Medical Text\n  Translation", "abstract": "Machine translation is indispensable in healthcare for enabling the global\ndissemination of medical knowledge across languages. However, complex medical\nterminology poses unique challenges to achieving adequate translation quality\nand accuracy. This study introduces a novel \"LLMs-in-the-loop\" approach to\ndevelop supervised neural machine translation models optimized specifically for\nmedical texts. While large language models (LLMs) have demonstrated powerful\ncapabilities, this research shows that small, specialized models trained on\nhigh-quality in-domain (mostly synthetic) data can outperform even vastly\nlarger LLMs.\n  Custom parallel corpora in six languages were compiled from scientific\narticles, synthetically generated clinical documents, and medical texts. Our\nLLMs-in-the-loop methodology employs synthetic data generation, rigorous\nevaluation, and agent orchestration to enhance performance. We developed small\nmedical translation models using the MarianMT base model. We introduce a new\nmedical translation test dataset to standardize evaluation in this domain.\nAssessed using BLEU, METEOR, ROUGE, and BERT scores on this test set, our\nMarianMT-based models outperform Google Translate, DeepL, and GPT-4-Turbo.\n  Results demonstrate that our LLMs-in-the-loop approach, combined with\nfine-tuning high-quality, domain-specific data, enables specialized models to\noutperform general-purpose and some larger systems. This research, part of a\nbroader series on expert small models, paves the way for future\nhealthcare-related AI developments, including deidentification and bio-medical\nentity extraction models. Our study underscores the potential of tailored\nneural translation models and the LLMs-in-the-loop methodology to advance the\nfield through improved data generation, evaluation, agent, and modeling\ntechniques.", "published": "2024-07-16 19:32:23", "link": "http://arxiv.org/abs/2407.12126v2", "categories": ["cs.CL", "cs.AI", "68T35"], "primary_category": "cs.CL"}
{"title": "Predicting Emotion Intensity in Polish Political Texts: Comparing\n  Supervised Models and Large Language Models in a Resource-Poor Language", "abstract": "This study explores the use of large language models (LLMs) to predict\nemotion intensity in Polish political texts, a resource-poor language context.\nThe research compares the performance of several LLMs against a supervised\nmodel trained on an annotated corpus of 10,000 social media texts, evaluated\nfor the intensity of emotions by expert judges. The findings indicate that\nwhile the supervised model generally outperforms LLMs, offering higher accuracy\nand lower variance, LLMs present a viable alternative, especially given the\nhigh costs associated with data annotation. The study highlights the potential\nof LLMs in low-resource language settings and underscores the need for further\nresearch on emotion intensity prediction and its application across different\nlanguages and continuous features. The implications suggest a nuanced\ndecision-making process to choose the right approach to emotion prediction for\nresearchers and practitioners based on resource availability and the specific\nrequirements of their tasks.", "published": "2024-07-16 19:53:14", "link": "http://arxiv.org/abs/2407.12141v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SELF-GUIDE: Better Task-Specific Instruction Following via\n  Self-Synthetic Finetuning", "abstract": "Large language models (LLMs) hold the promise of solving diverse tasks when\nprovided with appropriate natural language prompts. However, prompting often\nleads models to make predictions with lower accuracy compared to finetuning a\nmodel with ample training data. On the other hand, while finetuning LLMs on\ntask-specific data generally improves their performance, abundant annotated\ndatasets are not available for all tasks. Previous work has explored generating\ntask-specific data from state-of-the-art LLMs and using this data to finetune\nsmaller models, but this approach requires access to a language model other\nthan the one being trained, which introduces cost, scalability challenges, and\nlegal hurdles associated with continuously relying on more powerful LLMs. In\nresponse to these, we propose SELF-GUIDE, a multi-stage mechanism in which we\nsynthesize task-specific input-output pairs from the student LLM, then use\nthese input-output pairs to finetune the student LLM itself. In our empirical\nevaluation of the Natural Instructions V2 benchmark, we find that SELF-GUIDE\nimproves the performance of LLM by a substantial margin. Specifically, we\nreport an absolute improvement of approximately 15% for classification tasks\nand 18% for generation tasks in the benchmark's metrics. This sheds light on\nthe promise of self-synthesized data guiding LLMs towards becoming\ntask-specific experts without any external learning signals.", "published": "2024-07-16 04:41:58", "link": "http://arxiv.org/abs/2407.12874v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ReFeR: Improving Evaluation and Reasoning through Hierarchy of Models", "abstract": "Assessing the quality of outputs generated by generative models, such as\nlarge language models and vision language models, presents notable challenges.\nTraditional methods for evaluation typically rely on either human assessments,\nwhich are resource-intensive, or automatic metrics that often show a low\ncorrelation with human judgment. Another common approach is to use deep\nlearning systems, which not only consume a substantial amount of compute and\ntime but also require extensive training data. In this study, we introduce a\ntuning-free framework called ReFeR, designed to evaluate generative outputs,\nincluding both text and images, by leveraging a 2-level hierarchy of LLMs and\nVLMs themselves. We rigorously evaluate our framework, ReFeR, across four\ndiverse evaluation tasks. The framework not only improves the accuracy of these\nevaluations, surpassing previous benchmarks but also generates constructive\nfeedback. Interestingly, the framework is also applicable to reasoning tasks.\nExperiments on four reasoning tasks demonstrate superior collective reasoning\nabilities of the framework. We present two variants of the framework:\nReFeR-Turbo, optimized for accelerated performance, and ReFeR-Lite, offering a\nmore cost-effective solution. ReFeR-Lite is $\\sim7.7\\times$ more efficient\nwhile being comparably accurate to ReFeR-Turbo. We make code, data and PIP\npackage publicly available. See this PIP URL\nhttps://pypi.org/project/refer-agents/ and this Git URL\nhttps://github.com/yaswanth-iitkgp/ReFeR_Code .", "published": "2024-07-16 08:25:26", "link": "http://arxiv.org/abs/2407.12877v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Do LLMs have Consistent Values?", "abstract": "Large Language Models (LLM) technology is constantly improving towards\nhuman-like dialogue. Values are a basic driving force underlying human\nbehavior, but little research has been done to study the values exhibited in\ntext generated by LLMs. Here we study this question by turning to the rich\nliterature on value structure in psychology. We ask whether LLMs exhibit the\nsame value structure that has been demonstrated in humans, including the\nranking of values, and correlation between values. We show that the results of\nthis analysis depend on how the LLM is prompted, and that under a particular\nprompting strategy (referred to as \"Value Anchoring\") the agreement with human\ndata is quite compelling. Our results serve both to improve our understanding\nof values in LLMs, as well as introduce novel methods for assessing consistency\nin LLM responses.", "published": "2024-07-16 08:58:00", "link": "http://arxiv.org/abs/2407.12878v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "BinaryAlign: Word Alignment as Binary Sequence Labeling", "abstract": "Real world deployments of word alignment are almost certain to cover both\nhigh and low resource languages. However, the state-of-the-art for this task\nrecommends a different model class depending on the availability of gold\nalignment training data for a particular language pair. We propose BinaryAlign,\na novel word alignment technique based on binary sequence labeling that\noutperforms existing approaches in both scenarios, offering a unifying approach\nto the task. Additionally, we vary the specific choice of multilingual\nfoundation model, perform stratified error analysis over alignment error type,\nand explore the performance of BinaryAlign on non-English language pairs. We\nmake our source code publicly available.", "published": "2024-07-16 15:11:06", "link": "http://arxiv.org/abs/2407.12881v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Beyond Binary: Multiclass Paraphasia Detection with Generative\n  Pretrained Transformers and End-to-End Models", "abstract": "Aphasia is a language disorder that can lead to speech errors known as\nparaphasias, which involve the misuse, substitution, or invention of words.\nAutomatic paraphasia detection can help those with Aphasia by facilitating\nclinical assessment and treatment planning options. However, most automatic\nparaphasia detection works have focused solely on binary detection, which\ninvolves recognizing only the presence or absence of a paraphasia. Multiclass\nparaphasia detection represents an unexplored area of research that focuses on\nidentifying multiple types of paraphasias and where they occur in a given\nspeech segment. We present novel approaches that use a generative pretrained\ntransformer (GPT) to identify paraphasias from transcripts as well as two\nend-to-end approaches that focus on modeling both automatic speech recognition\n(ASR) and paraphasia classification as multiple sequences vs. a single\nsequence. We demonstrate that a single sequence model outperforms GPT baselines\nfor multiclass paraphasia detection.", "published": "2024-07-16 03:24:51", "link": "http://arxiv.org/abs/2407.11345v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Pilot Study of GSLM-based Simulation of Foreign Accentuation Only\n  Using Native Speech Corpora", "abstract": "We propose a method of simulating the human process of foreign accentuation\nusing Generative Spoken Language Model (GSLM) only with native speech corpora.\nWhen one listens to spoken words of a foreign language and repeats them, the\nrepeated speech is often with the accent of that listener's L1. This is said to\nbe because the spoken words are mentally represented as a sequence of\nphonological units of the L1, and those units are used for oral reproduction.\nWe simulate this process by inputting speech of language A into GSLM of\nlanguage B to add B's accent onto the input speech. The process of running ASR\nof the L1 for foreign input speech and giving the ASR result to TTS of the L1\ncan be viewed as a naive implementation of this approach. The results of our\nexperiments show that the synthesized accent of the output speech is highly\nnatural, compared to real samples of A generated by speakers whose L1 is B, and\nthat the degree of accentuation is controllable.", "published": "2024-07-16 04:29:00", "link": "http://arxiv.org/abs/2407.11370v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "CIC-BART-SSA: Controllable Image Captioning with Structured Semantic\n  Augmentation", "abstract": "Controllable Image Captioning (CIC) aims at generating natural language\ndescriptions for an image, conditioned on information provided by end users,\ne.g., regions, entities or events of interest. However, available\nimage-language datasets mainly contain captions that describe the entirety of\nan image, making them ineffective for training CIC models that can potentially\nattend to any subset of regions or relationships. To tackle this challenge, we\npropose a novel, fully automatic method to sample additional focused and\nvisually grounded captions using a unified structured semantic representation\nbuilt on top of the existing set of captions associated with an image. We\nleverage Abstract Meaning Representation (AMR), a cross-lingual graph-based\nsemantic formalism, to encode all possible spatio-semantic relations between\nentities, beyond the typical spatial-relations-only focus of current methods.\nWe use this Structured Semantic Augmentation (SSA) framework to augment\nexisting image-caption datasets with the grounded controlled captions,\nincreasing their spatial and semantic diversity and focal coverage. We then\ndevelop a new model, CIC-BART-SSA, specifically tailored for the CIC task, that\nsources its control signals from SSA-diversified datasets. We empirically show\nthat, compared to SOTA CIC models, CIC-BART-SSA generates captions that are\nsuperior in diversity and text quality, are competitive in controllability,\nand, importantly, minimize the gap between broad and highly focused controlled\ncaptioning performance by efficiently generalizing to the challenging highly\nfocused scenarios. Code is available at\nhttps://github.com/SamsungLabs/CIC-BART-SSA.", "published": "2024-07-16 05:26:12", "link": "http://arxiv.org/abs/2407.11393v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Semantic Operators: A Declarative Model for Rich, AI-based Data\n  Processing", "abstract": "The semantic capabilities of large language models (LLMs) have the potential\nto enable rich analytics and reasoning over vast knowledge corpora.\nUnfortunately, existing systems either empirically optimize expensive\nLLM-powered operations with no performance guarantees, or serve a limited set\nof row-wise LLM operations, providing limited robustness, expressiveness and\nusability. We introduce semantic operators, the first formalism for declarative\nand general-purpose AI-based transformations based on natural language\nspecifications (e.g., filtering, sorting, joining or aggregating records using\nnatural language criteria). Each operator opens a rich space for execution\nplans, similar to relational operators. Our model specifies the expected\nbehavior of each operator with a high-quality gold algorithm, and we develop an\noptimization framework that reduces cost, while providing accuracy guarantees\nwith respect to a gold algorithm. Using this approach, we propose several novel\noptimizations to accelerate semantic filtering, joining, group-by and top-k\noperations by up to $1,000\\times$. We implement semantic operators in the LOTUS\nsystem and demonstrate LOTUS' effectiveness on real, bulk-semantic processing\napplications, including fact-checking, biomedical multi-label classification,\nsearch, and topic analysis. We show that the semantic operator model is\nexpressive, capturing state-of-the-art AI pipelines in a few operator calls,\nand making it easy to express new pipelines that match or exceed quality of\nrecent LLM-based analytic systems by up to $170\\%$, while offering accuracy\nguarantees. Overall, LOTUS programs match or exceed the accuracy of\nstate-of-the-art AI pipelines for each task while running up to $3.6\\times$\nfaster than the highest-quality baselines. LOTUS is publicly available at\nhttps://github.com/lotus-data/lotus.", "published": "2024-07-16 06:19:14", "link": "http://arxiv.org/abs/2407.11418v3", "categories": ["cs.DB", "cs.AI", "cs.CL"], "primary_category": "cs.DB"}
{"title": "Beyond Correctness: Benchmarking Multi-dimensional Code Generation for\n  Large Language Models", "abstract": "In recent years, researchers have proposed numerous benchmarks to evaluate\nthe impressive coding capabilities of large language models (LLMs). However,\ncurrent benchmarks primarily assess the accuracy of LLM-generated code, while\nneglecting other critical dimensions that also significantly impact code\nquality in real-world development. Moreover, relying exclusively on correctness\nas the guiding metric renders LLMs susceptible to data contamination.\nTherefore, this paper proposes the RACE benchmark, which comprehensively\nevaluates the quality of code generated by LLMs across 4 dimensions:\nReadability, mAintainability, Correctness, and Efficiency. Specifically,\nconsidering the demand-dependent nature of dimensions beyond correctness, we\ndesign various types of user requirements for each dimension to assess the\nmodel's ability to generate correct code that also meets user demands. We\nanalyze 28 representative LLMs based on RACE and find that: 1) current\ncorrectness-centric benchmarks fail to capture the multifaceted requirements of\ncode in real-world scenarios, while RACE provides a comprehensive evaluation\nthat reveals the defects of LLMs across multiple dimensions; 2) the RACE\nbenchmark serves as an effective tool for resisting the risk of data\ncontamination; 3) even the most advanced code LLMs still encounter significant\nchallenges in customized requirements involving complex instructions; 4) most\nLLMs exhibit an inherent preference for specific coding style. These findings\nhighlight the need for a multidimensional evaluation of code LLMs, emphasizing\nmetrics beyond correctness for real-world applications. Future efforts should\naim to develop novel learning algorithms to enhance code generation under\nvaried constraints and improve coverage and usability for diverse user needs.", "published": "2024-07-16 08:08:48", "link": "http://arxiv.org/abs/2407.11470v2", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "MMSD-Net: Towards Multi-modal Stuttering Detection", "abstract": "Stuttering is a common speech impediment that is caused by irregular\ndisruptions in speech production, affecting over 70 million people across the\nworld. Standard automatic speech processing tools do not take speech ailments\ninto account and are thereby not able to generate meaningful results when\npresented with stuttered speech as input. The automatic detection of stuttering\nis an integral step towards building efficient, context-aware speech processing\nsystems. While previous approaches explore both statistical and neural\napproaches for stuttering detection, all of these methods are uni-modal in\nnature. This paper presents MMSD-Net, the first multi-modal neural framework\nfor stuttering detection. Experiments and results demonstrate that\nincorporating the visual signal significantly aids stuttering detection, and\nour model yields an improvement of 2-17% in the F1-score over existing\nstate-of-the-art uni-modal approaches.", "published": "2024-07-16 08:26:59", "link": "http://arxiv.org/abs/2407.11492v1", "categories": ["cs.SD", "cs.CL", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Reasoning with Large Language Models, a Survey", "abstract": "Scaling up language models to billions of parameters has opened up\npossibilities for in-context learning, allowing instruction tuning and few-shot\nlearning on tasks that the model was not specifically trained for. This has\nachieved breakthrough performance on language tasks such as translation,\nsummarization, and question-answering. Furthermore, in addition to these\nassociative \"System 1\" tasks, recent advances in Chain-of-thought prompt\nlearning have demonstrated strong \"System 2\" reasoning abilities, answering a\nquestion in the field of artificial general intelligence whether LLMs can\nreason. The field started with the question whether LLMs can solve grade school\nmath word problems. This paper reviews the rapidly expanding field of\nprompt-based reasoning with LLMs. Our taxonomy identifies different ways to\ngenerate, evaluate, and control multi-step reasoning. We provide an in-depth\ncoverage of core approaches and open problems, and we propose a research agenda\nfor the near future. Finally, we highlight the relation between reasoning and\nprompt-based learning, and we discuss the relation between reasoning,\nsequential decision processes, and reinforcement learning. We find that\nself-improvement, self-reflection, and some metacognitive abilities of the\nreasoning processes are possible through the judicious use of prompts. True\nself-improvement and self-reasoning, to go from reasoning with LLMs to\nreasoning by LLMs, remains future work.", "published": "2024-07-16 08:49:35", "link": "http://arxiv.org/abs/2407.11511v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "The Foundations of Tokenization: Statistical and Computational Concerns", "abstract": "Tokenization - the practice of converting strings of characters from an\nalphabet into sequences of tokens over a vocabulary - is a critical step in the\nNLP pipeline. The use of token representations is widely credited with\nincreased model performance but is also the source of many undesirable\nbehaviors, such as spurious ambiguity or inconsistency. Despite its recognized\nimportance as a standard representation method in NLP, the theoretical\nunderpinnings of tokenization are not yet fully understood. In particular, the\nimpact of tokenization on language model estimation has been investigated\nprimarily through empirical means. The present paper contributes to addressing\nthis theoretical gap by proposing a unified formal framework for representing\nand analyzing tokenizer models. Based on the category of stochastic maps, this\nframework enables us to establish general conditions for a principled use of\ntokenizers and, most importantly, the necessary and sufficient conditions for a\ntokenizer model to preserve the consistency of statistical estimators. In\naddition, we discuss statistical and computational concerns crucial for\ndesigning and implementing tokenizer models, such as inconsistency, ambiguity,\nfiniteness, and sequentiality. The framework and results advanced in this paper\ncontribute to building robust theoretical foundations for representations in\nneural language modeling that can inform future theoretical and empirical\nresearch.", "published": "2024-07-16 11:12:28", "link": "http://arxiv.org/abs/2407.11606v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Large Language Models as Misleading Assistants in Conversation", "abstract": "Large Language Models (LLMs) are able to provide assistance on a wide range\nof information-seeking tasks. However, model outputs may be misleading, whether\nunintentionally or in cases of intentional deception. We investigate the\nability of LLMs to be deceptive in the context of providing assistance on a\nreading comprehension task, using LLMs as proxies for human users. We compare\noutcomes of (1) when the model is prompted to provide truthful assistance, (2)\nwhen it is prompted to be subtly misleading, and (3) when it is prompted to\nargue for an incorrect answer. Our experiments show that GPT-4 can effectively\nmislead both GPT-3.5-Turbo and GPT-4, with deceptive assistants resulting in up\nto a 23% drop in accuracy on the task compared to when a truthful assistant is\nused. We also find that providing the user model with additional context from\nthe passage partially mitigates the influence of the deceptive model. This work\nhighlights the ability of LLMs to produce misleading information and the\neffects this may have in real-world situations.", "published": "2024-07-16 14:45:22", "link": "http://arxiv.org/abs/2407.11789v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "PipeInfer: Accelerating LLM Inference using Asynchronous Pipelined\n  Speculation", "abstract": "Inference of Large Language Models (LLMs) across computer clusters has become\na focal point of research in recent times, with many acceleration techniques\ntaking inspiration from CPU speculative execution. These techniques reduce\nbottlenecks associated with memory bandwidth, but also increase end-to-end\nlatency per inference run, requiring high speculation acceptance rates to\nimprove performance. Combined with a variable rate of acceptance across tasks,\nspeculative inference techniques can result in reduced performance.\nAdditionally, pipeline-parallel designs require many user requests to maintain\nmaximum utilization. As a remedy, we propose PipeInfer, a pipelined speculative\nacceleration technique to reduce inter-token latency and improve system\nutilization for single-request scenarios while also improving tolerance to low\nspeculation acceptance rates and low-bandwidth interconnects. PipeInfer\nexhibits up to a 2.15$\\times$ improvement in generation speed over standard\nspeculative inference. PipeInfer achieves its improvement through Continuous\nAsynchronous Speculation and Early Inference Cancellation, the former improving\nlatency and generation speed by running single-token inference simultaneously\nwith several speculative runs, while the latter improves speed and latency by\nskipping the computation of invalidated runs, even in the middle of inference.", "published": "2024-07-16 14:52:02", "link": "http://arxiv.org/abs/2407.11798v2", "categories": ["cs.CL", "cs.DC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Scaling Sign Language Translation", "abstract": "Sign language translation (SLT) addresses the problem of translating\ninformation from a sign language in video to a spoken language in text.\nExisting studies, while showing progress, are often limited to narrow domains\nand/or few sign languages and struggle with open-domain tasks. In this paper,\nwe push forward the frontier of SLT by scaling pretraining data, model size,\nand number of translation directions. We perform large-scale SLT pretraining on\ndifferent data including 1) noisy multilingual YouTube SLT data, 2) parallel\ntext corpora, and 3) SLT data augmented by translating video captions to other\nlanguages with off-the-shelf machine translation models. We unify different\npretraining tasks with task-specific prompts under the encoder-decoder\narchitecture, and initialize the SLT model with pretrained (m/By)T5 models\nacross model sizes. SLT pretraining results on How2Sign and FLEURS-ASL#0 (ASL\nto 42 spoken languages) demonstrate the significance of data/model scaling and\ncross-lingual cross-modal transfer, as well as the feasibility of zero-shot\nSLT. We finetune the pretrained SLT models on 5 downstream open-domain SLT\nbenchmarks covering 5 sign languages. Experiments show substantial quality\nimprovements over the vanilla baselines, surpassing the previous\nstate-of-the-art (SOTA) by wide margins.", "published": "2024-07-16 15:36:58", "link": "http://arxiv.org/abs/2407.11855v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Does Refusal Training in LLMs Generalize to the Past Tense?", "abstract": "Refusal training is widely used to prevent LLMs from generating harmful,\nundesirable, or illegal outputs. We reveal a curious generalization gap in the\ncurrent refusal training approaches: simply reformulating a harmful request in\nthe past tense (e.g., \"How to make a Molotov cocktail?\" to \"How did people make\na Molotov cocktail?\") is often sufficient to jailbreak many state-of-the-art\nLLMs. We systematically evaluate this method on Llama-3 8B, Claude-3.5 Sonnet,\nGPT-3.5 Turbo, Gemma-2 9B, Phi-3-Mini, GPT-4o mini, GPT-4o, o1-mini,\no1-preview, and R2D2 models using GPT-3.5 Turbo as a reformulation model. For\nexample, the success rate of this simple attack on GPT-4o increases from 1%\nusing direct requests to 88% using 20 past tense reformulation attempts on\nharmful requests from JailbreakBench with GPT-4 as a jailbreak judge.\nInterestingly, we also find that reformulations in the future tense are less\neffective, suggesting that refusal guardrails tend to consider past historical\nquestions more benign than hypothetical future questions. Moreover, our\nexperiments on fine-tuning GPT-3.5 Turbo show that defending against past\nreformulations is feasible when past tense examples are explicitly included in\nthe fine-tuning data. Overall, our findings highlight that the widely used\nalignment techniques -- such as SFT, RLHF, and adversarial training -- employed\nto align the studied models can be brittle and do not always generalize as\nintended. We provide code and jailbreak artifacts at\nhttps://github.com/tml-epfl/llm-past-tense.", "published": "2024-07-16 17:59:55", "link": "http://arxiv.org/abs/2407.11969v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Private prediction for large-scale synthetic text generation", "abstract": "We present an approach for generating differentially private synthetic text\nusing large language models (LLMs), via private prediction. In the private\nprediction framework, we only require the output synthetic data to satisfy\ndifferential privacy guarantees. This is in contrast to approaches that train a\ngenerative model on potentially sensitive user-supplied source data and seek to\nensure the model itself is safe to release.\n  We prompt a pretrained LLM with source data, but ensure that next-token\npredictions are made with differential privacy guarantees. Previous work in\nthis paradigm reported generating a small number of examples (<10) at\nreasonable privacy levels, an amount of data that is useful only for downstream\nin-context learning or prompting. In contrast, we make changes that allow us to\ngenerate thousands of high-quality synthetic data points, greatly expanding the\nset of potential applications. Our improvements come from an improved privacy\nanalysis and a better private selection mechanism, which makes use of the\nequivalence between the softmax layer for sampling tokens in LLMs and the\nexponential mechanism. Furthermore, we introduce a novel use of public\npredictions via the sparse vector technique, in which we do not pay privacy\ncosts for tokens that are predictable without sensitive data; we find this to\nbe particularly effective for structured data.", "published": "2024-07-16 18:28:40", "link": "http://arxiv.org/abs/2407.12108v2", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "GPT-4V Cannot Generate Radiology Reports Yet", "abstract": "GPT-4V's purported strong multimodal abilities raise interests in using it to\nautomate radiology report writing, but there lacks thorough evaluations. In\nthis work, we perform a systematic evaluation of GPT-4V in generating radiology\nreports on two chest X-ray report datasets: MIMIC-CXR and IU X-Ray. We attempt\nto directly generate reports using GPT-4V through different prompting\nstrategies and find that it fails terribly in both lexical metrics and clinical\nefficacy metrics. To understand the low performance, we decompose the task into\ntwo steps: 1) the medical image reasoning step of predicting medical condition\nlabels from images; and 2) the report synthesis step of generating reports from\n(groundtruth) conditions. We show that GPT-4V's performance in image reasoning\nis consistently low across different prompts. In fact, the distributions of\nmodel-predicted labels remain constant regardless of which groundtruth\nconditions are present on the image, suggesting that the model is not\ninterpreting chest X-rays meaningfully. Even when given groundtruth conditions\nin report synthesis, its generated reports are less correct and less\nnatural-sounding than a finetuned LLaMA-2. Altogether, our findings cast doubt\non the viability of using GPT-4V in a radiology workflow.", "published": "2024-07-16 21:03:14", "link": "http://arxiv.org/abs/2407.12176v4", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "A Language Modeling Approach to Diacritic-Free Hebrew TTS", "abstract": "We tackle the task of text-to-speech (TTS) in Hebrew. Traditional Hebrew\ncontains Diacritics, which dictate the way individuals should pronounce given\nwords, however, modern Hebrew rarely uses them. The lack of diacritics in\nmodern Hebrew results in readers expected to conclude the correct pronunciation\nand understand which phonemes to use based on the context. This imposes a\nfundamental challenge on TTS systems to accurately map between text-to-speech.\nIn this work, we propose to adopt a language modeling Diacritics-Free approach,\nfor the task of Hebrew TTS. The model operates on discrete speech\nrepresentations and is conditioned on a word-piece tokenizer. We optimize the\nproposed method using in-the-wild weakly supervised data and compare it to\nseveral diacritic-based TTS systems. Results suggest the proposed method is\nsuperior to the evaluated baselines considering both content preservation and\nnaturalness of the generated speech. Samples can be found under the following\nlink: pages.cs.huji.ac.il/adiyoss-lab/HebTTS/", "published": "2024-07-16 22:43:49", "link": "http://arxiv.org/abs/2407.12206v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Exploring the Use of Abusive Generative AI Models on Civitai", "abstract": "The rise of generative AI is transforming the landscape of digital imagery,\nand exerting a significant influence on online creative communities. This has\nled to the emergence of AI-Generated Content (AIGC) social platforms, such as\nCivitai. These distinctive social platforms allow users to build and share\ntheir own generative AI models, thereby enhancing the potential for more\ndiverse artistic expression. Designed in the vein of social networks, they also\nprovide artists with the means to showcase their creations (generated from the\nmodels), engage in discussions, and obtain feedback, thus nurturing a sense of\ncommunity. Yet, this openness also raises concerns about the abuse of such\nplatforms, e.g., using models to disseminate deceptive deepfakes or infringe\nupon copyrights. To explore this, we conduct the first comprehensive empirical\nstudy of an AIGC social platform, focusing on its use for generating abusive\ncontent. As an exemplar, we construct a comprehensive dataset covering Civitai,\nthe largest available AIGC social platform. Based on this dataset of 87K models\nand 2M images, we explore the characteristics of content and discuss strategies\nfor moderation to better govern these platforms.", "published": "2024-07-16 06:18:03", "link": "http://arxiv.org/abs/2407.12876v2", "categories": ["cs.SI", "cs.AI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Large Visual-Language Models Are Also Good Classifiers: A Study of\n  In-Context Multimodal Fake News Detection", "abstract": "Large visual-language models (LVLMs) exhibit exceptional performance in\nvisual-language reasoning across diverse cross-modal benchmarks. Despite these\nadvances, recent research indicates that Large Language Models (LLMs), like\nGPT-3.5-turbo, underachieve compared to well-trained smaller models, such as\nBERT, in Fake News Detection (FND), prompting inquiries into LVLMs' efficacy in\nFND tasks. Although performance could improve through fine-tuning LVLMs, the\nsubstantial parameters and requisite pre-trained weights render it a\nresource-heavy endeavor for FND applications. This paper initially assesses the\nFND capabilities of two notable LVLMs, CogVLM and GPT4V, in comparison to a\nsmaller yet adeptly trained CLIP model in a zero-shot context. The findings\ndemonstrate that LVLMs can attain performance competitive with that of the\nsmaller model. Next, we integrate standard in-context learning (ICL) with\nLVLMs, noting improvements in FND performance, though limited in scope and\nconsistency. To address this, we introduce the \\textbf{I}n-context\n\\textbf{M}ultimodal \\textbf{F}ake \\textbf{N}ews \\textbf{D}etection (IMFND)\nframework, enriching in-context examples and test inputs with predictions and\ncorresponding probabilities from a well-trained smaller model. This strategic\nintegration directs the LVLMs' focus towards news segments associated with\nhigher probabilities, thereby improving their analytical accuracy. The\nexperimental results suggest that the IMFND framework significantly boosts the\nFND efficiency of LVLMs, achieving enhanced accuracy over the standard ICL\napproach across three publicly available FND datasets.", "published": "2024-07-16 09:28:23", "link": "http://arxiv.org/abs/2407.12879v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cross-Modal Augmentation for Few-Shot Multimodal Fake News Detection", "abstract": "The nascent topic of fake news requires automatic detection methods to\nquickly learn from limited annotated samples. Therefore, the capacity to\nrapidly acquire proficiency in a new task with limited guidance, also known as\nfew-shot learning, is critical for detecting fake news in its early stages.\nExisting approaches either involve fine-tuning pre-trained language models\nwhich come with a large number of parameters, or training a complex neural\nnetwork from scratch with large-scale annotated datasets. This paper presents a\nmultimodal fake news detection model which augments multimodal features using\nunimodal features. For this purpose, we introduce Cross-Modal Augmentation\n(CMA), a simple approach for enhancing few-shot multimodal fake news detection\nby transforming n-shot classification into a more robust (n $\\times$ z)-shot\nproblem, where z represents the number of supplementary features. The proposed\nCMA achieves SOTA results over three benchmark datasets, utilizing a\nsurprisingly simple linear probing method to classify multimodal fake news with\nonly a few training samples. Furthermore, our method is significantly more\nlightweight than prior approaches, particularly in terms of the number of\ntrainable parameters and epoch times. The code is available here:\n\\url{https://github.com/zgjiangtoby/FND_fewshot}", "published": "2024-07-16 09:32:11", "link": "http://arxiv.org/abs/2407.12880v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "InstructAV: Instruction Fine-tuning Large Language Models for Authorship\n  Verification", "abstract": "Large Language Models (LLMs) have demonstrated remarkable proficiency in a\nwide range of NLP tasks. However, when it comes to authorship verification (AV)\ntasks, which involve determining whether two given texts share the same\nauthorship, even advanced models like ChatGPT exhibit notable limitations. This\npaper introduces a novel approach, termed InstructAV, for authorship\nverification. This approach utilizes LLMs in conjunction with a\nparameter-efficient fine-tuning (PEFT) method to simultaneously improve\naccuracy and explainability. The distinctiveness of InstructAV lies in its\nability to align classification decisions with transparent and understandable\nexplanations, representing a significant progression in the field of authorship\nverification. Through comprehensive experiments conducted across various\ndatasets, InstructAV demonstrates its state-of-the-art performance on the AV\ntask, offering high classification accuracy coupled with enhanced explanation\nreliability.", "published": "2024-07-16 16:27:01", "link": "http://arxiv.org/abs/2407.12882v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive\n  Retrieval", "abstract": "Existing retrieval benchmarks primarily consist of information-seeking\nqueries (e.g., aggregated questions from search engines) where keyword or\nsemantic-based retrieval is usually sufficient. However, many complex\nreal-world queries require in-depth reasoning to identify relevant documents\nthat go beyond surface form matching. For example, finding documentation for a\ncoding question requires understanding the logic and syntax of the functions\ninvolved. To better benchmark retrieval on such challenging queries, we\nintroduce BRIGHT, the first text retrieval benchmark that requires intensive\nreasoning to retrieve relevant documents. Our dataset consists of 1,384\nreal-world queries spanning diverse domains, such as economics, psychology,\nmathematics, and coding. These queries are drawn from naturally occurring and\ncarefully curated human data. Extensive evaluation reveals that even\nstate-of-the-art retrieval models perform poorly on BRIGHT. The leading model\non the MTEB leaderboard (Muennighoff et al., 2023) SFR-Embedding-Mistral (Meng\net al., 2024), which achieves a score of 59.0 nDCG@10,1 produces a score of\nnDCG@10 of 18.3 on BRIGHT. We show that incorporating explicit reasoning about\nthe query improves retrieval performance by up to 12.2 points. Moreover,\nincorporating retrieved documents from the top-performing retriever boosts\nquestion-answering performance. We believe that BRIGHT paves the way for future\nresearch on retrieval systems in more realistic and challenging settings.", "published": "2024-07-16 17:58:27", "link": "http://arxiv.org/abs/2407.12883v4", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Whitening Not Recommended for Classification Tasks in LLMs", "abstract": "Sentence embedding is a cornerstone in NLP. Whitening has been claimed to be\nan effective operation to improve embedding quality obtained from Large\nLanguage Models (LLMs). However, we find that the efficacy of whitening is\nmodel-dependent and task-dependent. In particular, whitening degenerates\nembeddings for classification tasks. The conclusion is supported by extensive\nexperiments. We also explored a variety of whitening operations, including PCA,\nZCA, PCA-Cor, ZCA-Cor and Cholesky whitenings. A by-product of our research is\nembedding evaluation platform for LLMs called SentEval+.", "published": "2024-07-16 22:48:30", "link": "http://arxiv.org/abs/2407.12886v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Continuous Embedding Attacks via Clipped Inputs in Jailbreaking Large\n  Language Models", "abstract": "Security concerns for large language models (LLMs) have recently escalated,\nfocusing on thwarting jailbreaking attempts in discrete prompts. However, the\nexploration of jailbreak vulnerabilities arising from continuous embeddings has\nbeen limited, as prior approaches primarily involved appending discrete or\ncontinuous suffixes to inputs. Our study presents a novel channel for\nconducting direct attacks on LLM inputs, eliminating the need for suffix\naddition or specific questions provided that the desired output is predefined.\nWe additionally observe that extensive iterations often lead to overfitting,\ncharacterized by repetition in the output. To counteract this, we propose a\nsimple yet effective strategy named CLIP. Our experiments show that for an\ninput length of 40 at iteration 1000, applying CLIP improves the ASR from 62%\nto 83%", "published": "2024-07-16 20:53:00", "link": "http://arxiv.org/abs/2407.13796v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Performance Evaluation of Lightweight Open-source Large Language Models\n  in Pediatric Consultations: A Comparative Analysis", "abstract": "Large language models (LLMs) have demonstrated potential applications in\nmedicine, yet data privacy and computational burden limit their deployment in\nhealthcare institutions. Open-source and lightweight versions of LLMs emerge as\npotential solutions, but their performance, particularly in pediatric settings\nremains underexplored. In this cross-sectional study, 250 patient consultation\nquestions were randomly selected from a public online medical forum, with 10\nquestions from each of 25 pediatric departments, spanning from December 1,\n2022, to October 30, 2023. Two lightweight open-source LLMs, ChatGLM3-6B and\nVicuna-7B, along with a larger-scale model, Vicuna-13B, and the widely-used\nproprietary ChatGPT-3.5, independently answered these questions in Chinese\nbetween November 1, 2023, and November 7, 2023. To assess reproducibility, each\ninquiry was replicated once. We found that ChatGLM3-6B demonstrated higher\naccuracy and completeness than Vicuna-13B and Vicuna-7B (P < .001), but all\nwere outperformed by ChatGPT-3.5. ChatGPT-3.5 received the highest ratings in\naccuracy (65.2%) compared to ChatGLM3-6B (41.2%), Vicuna-13B (11.2%), and\nVicuna-7B (4.4%). Similarly, in completeness, ChatGPT-3.5 led (78.4%), followed\nby ChatGLM3-6B (76.0%), Vicuna-13B (34.8%), and Vicuna-7B (22.0%) in highest\nratings. ChatGLM3-6B matched ChatGPT-3.5 in readability, both outperforming\nVicuna models (P < .001). In terms of empathy, ChatGPT-3.5 outperformed the\nlightweight LLMs (P < .001). In safety, all models performed comparably well (P\n> .05), with over 98.4% of responses being rated as safe. Repetition of\ninquiries confirmed these findings. In conclusion, Lightweight LLMs demonstrate\npromising application in pediatric healthcare. However, the observed gap\nbetween lightweight and large-scale proprietary LLMs underscores the need for\ncontinued development efforts.", "published": "2024-07-16 03:35:09", "link": "http://arxiv.org/abs/2407.15862v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY", "68M20 (Primary) 62G10 (Secondary)"], "primary_category": "cs.LG"}
{"title": "LiteGPT: Large Vision-Language Model for Joint Chest X-ray Localization\n  and Classification Task", "abstract": "Vision-language models have been extensively explored across a wide range of\ntasks, achieving satisfactory performance; however, their application in\nmedical imaging remains underexplored. In this work, we propose a unified\nframework - LiteGPT - for the medical imaging. We leverage multiple pre-trained\nvisual encoders to enrich information and enhance the performance of\nvision-language models. To the best of our knowledge, this is the first study\nto utilize vision-language models for the novel task of joint localization and\nclassification in medical images. Besides, we are pioneers in providing\nbaselines for disease localization in chest X-rays. Finally, we set new\nstate-of-the-art performance in the image classification task on the\nwell-benchmarked VinDr-CXR dataset. All code and models are publicly available\nonline: https://github.com/leduckhai/LiteGPT", "published": "2024-07-16 02:19:02", "link": "http://arxiv.org/abs/2407.12064v1", "categories": ["eess.IV", "cs.CL", "cs.CV", "cs.LG", "cs.MM"], "primary_category": "eess.IV"}
{"title": "Team HYU ASML ROBOVOX SP Cup 2024 System Description", "abstract": "This report describes the submission of HYU ASML team to the IEEE Signal\nProcessing Cup 2024 (SP Cup 2024). This challenge, titled \"ROBOVOX: Far-Field\nSpeaker Recognition by a Mobile Robot,\" focuses on speaker recognition using a\nmobile robot in noisy and reverberant conditions. Our solution combines the\nresult of deep residual neural networks and time-delay neural network-based\nspeaker embedding models. These models were trained on a diverse dataset that\nincludes French speech. To account for the challenging evaluation environment\ncharacterized by high noise, reverberation, and short speech conditions, we\nfocused on data augmentation and training speech duration for the speaker\nembedding model. Our submission achieved second place on the SP Cup 2024 public\nleaderboard, with a detection cost function of 0.5245 and an equal error rate\nof 6.46%.", "published": "2024-07-16 04:05:50", "link": "http://arxiv.org/abs/2407.11365v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "VoxBlink2: A 100K+ Speaker Recognition Corpus and the Open-Set\n  Speaker-Identification Benchmark", "abstract": "In this paper, we provide a large audio-visual speaker recognition dataset,\nVoxBlink2, which includes approximately 10M utterances with videos from 110K+\nspeakers in the wild. This dataset represents a significant expansion over the\nVoxBlink dataset, encompassing a broader diversity of speakers and scenarios by\nthe grace of an optimized data collection pipeline. Afterward, we explore the\nimpact of training strategies, data scale, and model complexity on speaker\nverification and finally establish a new single-model state-of-the-art EER at\n0.170% and minDCF at 0.006% on the VoxCeleb1-O test set. Such remarkable\nresults motivate us to explore speaker recognition from a new challenging\nperspective. We raise the Open-Set Speaker-Identification task, which is\ndesigned to either match a probe utterance with a known gallery speaker or\ncategorize it as an unknown query. Associated with this task, we design\nconcrete benchmark and evaluation protocols. The data and model resources can\nbe found in http://voxblink2.github.io.", "published": "2024-07-16 08:49:30", "link": "http://arxiv.org/abs/2407.11510v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "The VoicePrivacy 2022 Challenge: Progress and Perspectives in Voice\n  Anonymisation", "abstract": "The VoicePrivacy Challenge promotes the development of voice anonymisation\nsolutions for speech technology. In this paper we present a systematic overview\nand analysis of the second edition held in 2022. We describe the voice\nanonymisation task and datasets used for system development and evaluation,\npresent the different attack models used for evaluation, and the associated\nobjective and subjective metrics. We describe three anonymisation baselines,\nprovide a summary description of the anonymisation systems developed by\nchallenge participants, and report objective and subjective evaluation results\nfor all. In addition, we describe post-evaluation analyses and a summary of\nrelated work reported in the open literature. Results show that solutions based\non voice conversion better preserve utility, that an alternative which combines\nautomatic speech recognition with synthesis achieves greater privacy, and that\na privacy-utility trade-off remains inherent to current anonymisation\nsolutions. Finally, we present our ideas and priorities for future VoicePrivacy\nChallenge editions.", "published": "2024-07-16 08:51:37", "link": "http://arxiv.org/abs/2407.11516v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "MUSA: Multi-lingual Speaker Anonymization via Serial Disentanglement", "abstract": "Speaker anonymization is an effective privacy protection solution designed to\nconceal the speaker's identity while preserving the linguistic content and\npara-linguistic information of the original speech. While most prior studies\nfocus solely on a single language, an ideal speaker anonymization system should\nbe capable of handling multiple languages. This paper proposes MUSA, a\nMulti-lingual Speaker Anonymization approach that employs a serial\ndisentanglement strategy to perform a step-by-step disentanglement from a\nglobal time-invariant representation to a temporal time-variant representation.\nBy utilizing semantic distillation and self-supervised speaker distillation,\nthe serial disentanglement strategy can avoid strong inductive biases and\nexhibit superior generalization performance across different languages.\nMeanwhile, we propose a straightforward anonymization strategy that employs\nempty embedding with zero values to simulate the speaker identity concealment\nprocess, eliminating the need for conversion to a pseudo-speaker identity and\nthereby reducing the complexity of speaker anonymization process. Experimental\nresults on VoicePrivacy official datasets and multi-lingual datasets\ndemonstrate that MUSA can effectively protect speaker privacy while preserving\nlinguistic content and para-linguistic information.", "published": "2024-07-16 11:48:00", "link": "http://arxiv.org/abs/2407.11629v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Semantic Communication for the Internet of Sounds: Architecture, Design\n  Principles, and Challenges", "abstract": "The Internet of Sounds (IoS) combines sound sensing, processing, and\ntransmission techniques, enabling collaboration among diverse sound devices. To\nachieve perceptual quality of sound synchronization in the IoS, it is necessary\nto precisely synchronize three critical factors: sound quality, timing, and\nbehavior control. However, conventional bit-oriented communication, which\nfocuses on bit reproduction, may not be able to fulfill these synchronization\nrequirements under dynamic channel conditions. One promising approach to\naddress the synchronization challenges of the IoS is through the use of\nsemantic communication (SC) that can capture and leverage the logical\nrelationships in its source data. Consequently, in this paper, we propose an\nIoS-centric SC framework with a transceiver design. The designed encoder\nextracts semantic information from diverse sources and transmits it to IoS\nlisteners. It can also distill important semantic information to reduce\ntransmission latency for timing synchronization. At the receiver's end, the\ndecoder employs context- and knowledge-based reasoning techniques to\nreconstruct and integrate sounds, which achieves sound quality synchronization\nacross diverse communication environments. Moreover, by periodically sharing\nknowledge, SC models of IoS devices can be updated to optimize their\nsynchronization behavior. Finally, we explore several open issues on\nmathematical models, resource allocation, and cross-layer protocols.", "published": "2024-07-16 22:10:14", "link": "http://arxiv.org/abs/2407.12203v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Investigating the Effect of Label Topology and Training Criterion on ASR\n  Performance and Alignment Quality", "abstract": "The ongoing research scenario for automatic speech recognition (ASR)\nenvisions a clear division between end-to-end approaches and classic modular\nsystems. Even though a high-level comparison between the two approaches in\nterms of their requirements and (dis)advantages is commonly addressed, a closer\ncomparison under similar conditions is not readily available in the literature.\nIn this work, we present a comparison focused on the label topology and\ntraining criterion. We compare two discriminative alignment models with hidden\nMarkov model (HMM) and connectionist temporal classification topology, and two\nfirst-order label context ASR models utilizing factored HMM and strictly\nmonotonic recurrent neural network transducer, respectively. We use different\nmeasurements for the evaluation of the alignment quality, and compare word\nerror rate and real time factor of our best systems. Experiments are conducted\non the LibriSpeech 960h and Switchboard 300h tasks.", "published": "2024-07-16 12:02:47", "link": "http://arxiv.org/abs/2407.11641v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Vibravox: A Dataset of French Speech Captured with Body-conduction Audio\n  Sensors", "abstract": "Vibravox is a dataset compliant with the General Data Protection Regulation\n(GDPR) containing audio recordings using five different body-conduction audio\nsensors: two in-ear microphones, two bone conduction vibration pickups, and a\nlaryngophone. The dataset also includes audio data from an airborne microphone\nused as a reference. The Vibravox corpus contains 45 hours per sensor of speech\nsamples and physiological sounds recorded by 188 participants under different\nacoustic conditions imposed by a high order ambisonics 3D spatializer.\nAnnotations about the recording conditions and linguistic transcriptions are\nalso included in the corpus. We conducted a series of experiments on various\nspeech-related tasks, including speech recognition, speech enhancement, and\nspeaker verification. These experiments were carried out using state-of-the-art\nmodels to evaluate and compare their performances on signals captured by the\ndifferent audio sensors offered by the Vibravox dataset, with the aim of\ngaining a better grasp of their individual characteristics.", "published": "2024-07-16 15:16:10", "link": "http://arxiv.org/abs/2407.11828v4", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Disentangled Acoustic Fields For Multimodal Physical Scene Understanding", "abstract": "We study the problem of multimodal physical scene understanding, where an\nembodied agent needs to find fallen objects by inferring object properties,\ndirection, and distance of an impact sound source. Previous works adopt\nfeed-forward neural networks to directly regress the variables from sound,\nleading to poor generalization and domain adaptation issues. In this paper, we\nillustrate that learning a disentangled model of acoustic formation, referred\nto as disentangled acoustic field (DAF), to capture the sound generation and\npropagation process, enables the embodied agent to construct a spatial\nuncertainty map over where the objects may have fallen. We demonstrate that our\nanalysis-by-synthesis framework can jointly infer sound properties by\nexplicitly decomposing and factorizing the latent space of the disentangled\nmodel. We further show that the spatial uncertainty map can significantly\nimprove the success rate for the localization of fallen objects by proposing\nmultiple plausible exploration locations.", "published": "2024-07-16 02:54:54", "link": "http://arxiv.org/abs/2407.11333v1", "categories": ["cs.RO", "cs.SD", "eess.AS"], "primary_category": "cs.RO"}
{"title": "Statistics-aware Audio-visual Deepfake Detector", "abstract": "In this paper, we propose an enhanced audio-visual deep detection method.\nRecent methods in audio-visual deepfake detection mostly assess the\nsynchronization between audio and visual features. Although they have shown\npromising results, they are based on the maximization/minimization of isolated\nfeature distances without considering feature statistics. Moreover, they rely\non cumbersome deep learning architectures and are heavily dependent on\nempirically fixed hyperparameters. Herein, to overcome these limitations, we\npropose: (1) a statistical feature loss to enhance the discrimination\ncapability of the model, instead of relying solely on feature distances; (2)\nusing the waveform for describing the audio as a replacement of frequency-based\nrepresentations; (3) a post-processing normalization of the fakeness score; (4)\nthe use of shallower network for reducing the computational complexity.\nExperiments on the DFDC and FakeAVCeleb datasets demonstrate the relevance of\nthe proposed method.", "published": "2024-07-16 12:15:41", "link": "http://arxiv.org/abs/2407.11650v2", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Universal Sound Separation with Self-Supervised Audio Masked Autoencoder", "abstract": "Universal sound separation (USS) is a task of separating mixtures of\narbitrary sound sources. Typically, universal separation models are trained\nfrom scratch in a supervised manner, using labeled data. Self-supervised\nlearning (SSL) is an emerging deep learning approach that leverages unlabeled\ndata to obtain task-agnostic representations, which can benefit many downstream\ntasks. In this paper, we propose integrating a self-supervised pre-trained\nmodel, namely the audio masked autoencoder (A-MAE), into a universal sound\nseparation system to enhance its separation performance. We employ two\nstrategies to utilize SSL embeddings: freezing or updating the parameters of\nA-MAE during fine-tuning. The SSL embeddings are concatenated with the\nshort-time Fourier transform (STFT) to serve as input features for the\nseparation model. We evaluate our methods on the AudioSet dataset, and the\nexperimental results indicate that the proposed methods successfully enhance\nthe separation performance of a state-of-the-art ResUNet-based USS model.", "published": "2024-07-16 14:11:44", "link": "http://arxiv.org/abs/2407.11745v2", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
