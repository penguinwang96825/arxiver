{"title": "Interactive Attention for Neural Machine Translation", "abstract": "Conventional attention-based Neural Machine Translation (NMT) conducts\ndynamic alignment in generating the target sentence. By repeatedly reading the\nrepresentation of source sentence, which keeps fixed after generated by the\nencoder (Bahdanau et al., 2015), the attention mechanism has greatly enhanced\nstate-of-the-art NMT. In this paper, we propose a new attention mechanism,\ncalled INTERACTIVE ATTENTION, which models the interaction between the decoder\nand the representation of source sentence during translation by both reading\nand writing operations. INTERACTIVE ATTENTION can keep track of the interaction\nhistory and therefore improve the translation performance. Experiments on NIST\nChinese-English translation task show that INTERACTIVE ATTENTION can achieve\nsignificant improvements over both the previous attention-based NMT baseline\nand some state-of-the-art variants of attention-based NMT (i.e., coverage\nmodels (Tu et al., 2016)). And neural machine translator with our INTERACTIVE\nATTENTION can outperform the open source attention-based NMT system Groundhog\nby 4.22 BLEU points and the open source phrase-based system Moses by 3.94 BLEU\npoints averagely on multiple test sets.", "published": "2016-10-17 08:33:20", "link": "http://arxiv.org/abs/1610.05011v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Machine Translation Advised by Statistical Machine Translation", "abstract": "Neural Machine Translation (NMT) is a new approach to machine translation\nthat has made great progress in recent years. However, recent studies show that\nNMT generally produces fluent but inadequate translations (Tu et al. 2016b; Tu\net al. 2016a; He et al. 2016; Tu et al. 2017). This is in contrast to\nconventional Statistical Machine Translation (SMT), which usually yields\nadequate but non-fluent translations. It is natural, therefore, to leverage the\nadvantages of both models for better translations, and in this work we propose\nto incorporate SMT model into NMT framework. More specifically, at each\ndecoding step, SMT offers additional recommendations of generated words based\non the decoding information from NMT (e.g., the generated partial translation\nand attention history). Then we employ an auxiliary classifier to score the SMT\nrecommendations and a gating function to combine the SMT recommendations with\nNMT generations, both of which are jointly trained within the NMT architecture\nin an end-to-end manner. Experimental results on Chinese-English translation\nshow that the proposed approach achieves significant and consistent\nimprovements over state-of-the-art NMT and SMT systems on multiple NIST test\nsets.", "published": "2016-10-17 14:50:58", "link": "http://arxiv.org/abs/1610.05150v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pre-Translation for Neural Machine Translation", "abstract": "Recently, the development of neural machine translation (NMT) has\nsignificantly improved the translation quality of automatic machine\ntranslation. While most sentences are more accurate and fluent than\ntranslations by statistical machine translation (SMT)-based systems, in some\ncases, the NMT system produces translations that have a completely different\nmeaning. This is especially the case when rare words occur.\n  When using statistical machine translation, it has already been shown that\nsignificant gains can be achieved by simplifying the input in a preprocessing\nstep. A commonly used example is the pre-reordering approach.\n  In this work, we used phrase-based machine translation to pre-translate the\ninput into the target language. Then a neural machine translation system\ngenerates the final hypothesis using the pre-translation. Thereby, we use\neither only the output of the phrase-based machine translation (PBMT) system or\na combination of the PBMT output and the source sentence.\n  We evaluate the technique on the English to German translation task. Using\nthis approach we are able to outperform the PBMT system as well as the baseline\nneural MT system by up to 2 BLEU points. We analyzed the influence of the\nquality of the initial system on the final result.", "published": "2016-10-17 18:14:24", "link": "http://arxiv.org/abs/1610.05243v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "End-to-end attention-based distant speech recognition with Highway LSTM", "abstract": "End-to-end attention-based models have been shown to be competitive\nalternatives to conventional DNN-HMM models in the Speech Recognition Systems.\nIn this paper, we extend existing end-to-end attention-based models that can be\napplied for Distant Speech Recognition (DSR) task. Specifically, we propose an\nend-to-end attention-based speech recognizer with multichannel input that\nperforms sequence prediction directly at the character level. To gain a better\nperformance, we also incorporate Highway long short-term memory (HLSTM) which\noutperforms previous models on AMI distant speech recognition task.", "published": "2016-10-17 21:16:56", "link": "http://arxiv.org/abs/1610.05361v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cached Long Short-Term Memory Neural Networks for Document-Level\n  Sentiment Classification", "abstract": "Recently, neural networks have achieved great success on sentiment\nclassification due to their ability to alleviate feature engineering. However,\none of the remaining challenges is to model long texts in document-level\nsentiment classification under a recurrent architecture because of the\ndeficiency of the memory unit. To address this problem, we present a Cached\nLong Short-Term Memory neural networks (CLSTM) to capture the overall semantic\ninformation in long texts. CLSTM introduces a cache mechanism, which divides\nmemory into several groups with different forgetting rates and thus enables the\nnetwork to keep sentiment information better within a recurrent unit. The\nproposed CLSTM outperforms the state-of-the-art models on three publicly\navailable document-level sentiment analysis datasets.", "published": "2016-10-17 07:28:06", "link": "http://arxiv.org/abs/1610.04989v1", "categories": ["cs.CL", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Achieving Human Parity in Conversational Speech Recognition", "abstract": "Conversational speech recognition has served as a flagship speech recognition\ntask since the release of the Switchboard corpus in the 1990s. In this paper,\nwe measure the human error rate on the widely used NIST 2000 test set, and find\nthat our latest automated system has reached human parity. The error rate of\nprofessional transcribers is 5.9% for the Switchboard portion of the data, in\nwhich newly acquainted pairs of people discuss an assigned topic, and 11.3% for\nthe CallHome portion where friends and family members have open-ended\nconversations. In both cases, our automated system establishes a new state of\nthe art, and edges past the human benchmark, achieving error rates of 5.8% and\n11.0%, respectively. The key to our system's performance is the use of various\nconvolutional and LSTM acoustic model architectures, combined with a novel\nspatial smoothing method and lattice-free MMI acoustic training, multiple\nrecurrent neural network language modeling approaches, and a systematic use of\nsystem combination.", "published": "2016-10-17 18:40:50", "link": "http://arxiv.org/abs/1610.05256v2", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
