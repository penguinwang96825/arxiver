{"title": "A Novel Graph-based Multi-modal Fusion Encoder for Neural Machine\n  Translation", "abstract": "Multi-modal neural machine translation (NMT) aims to translate source\nsentences into a target language paired with images. However, dominant\nmulti-modal NMT models do not fully exploit fine-grained semantic\ncorrespondences between semantic units of different modalities, which have\npotential to refine multi-modal representation learning. To deal with this\nissue, in this paper, we propose a novel graph-based multi-modal fusion encoder\nfor NMT. Specifically, we first represent the input sentence and image using a\nunified multi-modal graph, which captures various semantic relationships\nbetween multi-modal semantic units (words and visual objects). We then stack\nmultiple graph-based multi-modal fusion layers that iteratively perform\nsemantic interactions to learn node representations. Finally, these\nrepresentations provide an attention-based context vector for the decoder. We\nevaluate our proposed encoder on the Multi30K datasets. Experimental results\nand in-depth analysis show the superiority of our multi-modal NMT model.", "published": "2020-07-17 04:06:09", "link": "http://arxiv.org/abs/2007.08742v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Constructing a Family Tree of Ten Indo-European Languages with\n  Delexicalized Cross-linguistic Transfer Patterns", "abstract": "It is reasonable to hypothesize that the divergence patterns formulated by\nhistorical linguists and typologists reflect constraints on human languages,\nand are thus consistent with Second Language Acquisition (SLA) in a certain\nway. In this paper, we validate this hypothesis on ten Indo-European languages.\nWe formalize the delexicalized transfer as interpretable tree-to-string and\ntree-to-tree patterns which can be automatically induced from web data by\napplying neural syntactic parsing and grammar induction technologies. This\nallows us to quantitatively probe cross-linguistic transfer and extend\ninquiries of SLA. We extend existing works which utilize mixed features and\nsupport the agreement between delexicalized cross-linguistic transfer and the\nphylogenetic structure resulting from the historical-comparative paradigm.", "published": "2020-07-17 15:56:54", "link": "http://arxiv.org/abs/2007.09076v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards an Automated SOAP Note: Classifying Utterances from Medical\n  Conversations", "abstract": "Summaries generated from medical conversations can improve recall and\nunderstanding of care plans for patients and reduce documentation burden for\ndoctors. Recent advancements in automatic speech recognition (ASR) and natural\nlanguage understanding (NLU) offer potential solutions to generate these\nsummaries automatically, but rigorous quantitative baselines for benchmarking\nresearch in this domain are lacking. In this paper, we bridge this gap for two\ntasks: classifying utterances from medical conversations according to (i) the\nSOAP section and (ii) the speaker role. Both are fundamental building blocks\nalong the path towards an end-to-end, automated SOAP note for medical\nconversations. We provide details on a dataset that contains human and ASR\ntranscriptions of medical conversations and corresponding machine learning\noptimized SOAP notes. We then present a systematic analysis in which we adapt\nan existing deep learning architecture to the two aforementioned tasks. The\nresults suggest that modelling context in a hierarchical manner, which captures\nboth word and utterance level context, yields substantial improvements on both\nclassification tasks. Additionally, we develop and analyze a modular method for\nadapting our model to ASR output.", "published": "2020-07-17 04:19:30", "link": "http://arxiv.org/abs/2007.08749v3", "categories": ["cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Knowledge-Based Video Question Answering with Unsupervised Scene\n  Descriptions", "abstract": "To understand movies, humans constantly reason over the dialogues and actions\nshown in specific scenes and relate them to the overall storyline already seen.\nInspired by this behaviour, we design ROLL, a model for knowledge-based video\nstory question answering that leverages three crucial aspects of movie\nunderstanding: dialog comprehension, scene reasoning, and storyline recalling.\nIn ROLL, each of these tasks is in charge of extracting rich and diverse\ninformation by 1) processing scene dialogues, 2) generating unsupervised video\nscene descriptions, and 3) obtaining external knowledge in a weakly supervised\nfashion. To answer a given question correctly, the information generated by\neach inspired-cognitive task is encoded via Transformers and fused through a\nmodality weighting mechanism, which balances the information from the different\nsources. Exhaustive evaluation demonstrates the effectiveness of our approach,\nwhich yields a new state-of-the-art on two challenging video question answering\ndatasets: KnowIT VQA and TVQA+.", "published": "2020-07-17 04:26:38", "link": "http://arxiv.org/abs/2007.08751v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Task-Level Curriculum Learning for Non-Autoregressive Neural Machine\n  Translation", "abstract": "Non-autoregressive translation (NAT) achieves faster inference speed but at\nthe cost of worse accuracy compared with autoregressive translation (AT). Since\nAT and NAT can share model structure and AT is an easier task than NAT due to\nthe explicit dependency on previous target-side tokens, a natural idea is to\ngradually shift the model training from the easier AT task to the harder NAT\ntask. To smooth the shift from AT training to NAT training, in this paper, we\nintroduce semi-autoregressive translation (SAT) as intermediate tasks. SAT\ncontains a hyperparameter k, and each k value defines a SAT task with different\ndegrees of parallelism. Specially, SAT covers AT and NAT as its special cases:\nit reduces to AT when k = 1 and to NAT when k = N (N is the length of target\nsentence). We design curriculum schedules to gradually shift k from 1 to N,\nwith different pacing functions and number of tasks trained at the same time.\nWe called our method as task-level curriculum learning for NAT (TCL-NAT).\nExperiments on IWSLT14 De-En, IWSLT16 En-De, WMT14 En-De and De-En datasets\nshow that TCL-NAT achieves significant accuracy improvements over previous NAT\nbaselines and reduces the performance gap between NAT and AT models to 1-2 BLEU\npoints, demonstrating the effectiveness of our proposed method.", "published": "2020-07-17 06:06:54", "link": "http://arxiv.org/abs/2007.08772v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Compositional Generalization in Semantic Parsing: Pre-training vs.\n  Specialized Architectures", "abstract": "While mainstream machine learning methods are known to have limited ability\nto compositionally generalize, new architectures and techniques continue to be\nproposed to address this limitation. We investigate state-of-the-art techniques\nand architectures in order to assess their effectiveness in improving\ncompositional generalization in semantic parsing tasks based on the SCAN and\nCFQ datasets. We show that masked language model (MLM) pre-training rivals\nSCAN-inspired architectures on primitive holdout splits. On a more complex\ncompositional task, we show that pre-training leads to significant improvements\nin performance vs. comparable non-pre-trained models, whereas architectures\nproposed to encourage compositional generalization on SCAN or in the area of\nalgorithm learning fail to lead to significant improvements. We establish a new\nstate of the art on the CFQ compositional generalization benchmark using MLM\npre-training together with an intermediate representation.", "published": "2020-07-17 13:34:49", "link": "http://arxiv.org/abs/2007.08970v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Neural Named Entity Recognition for Kazakh", "abstract": "We present several neural networks to address the task of named entity\nrecognition for morphologically complex languages (MCL). Kazakh is a\nmorphologically complex language in which each root/stem can produce hundreds\nor thousands of variant word forms. This nature of the language could lead to a\nserious data sparsity problem, which may prevent the deep learning models from\nbeing well trained for under-resourced MCLs. In order to model the MCLs' words\neffectively, we introduce root and entity tag embedding plus tensor layer to\nthe neural networks. The effects of those are significant for improving NER\nmodel performance of MCLs. The proposed models outperform state-of-the-art\nincluding character-based approaches, and can be potentially applied to other\nmorphologically complex languages.", "published": "2020-07-17 16:45:22", "link": "http://arxiv.org/abs/2007.13626v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Toward Givenness Hierarchy Theoretic Natural Language Generation", "abstract": "Language-capable interactive robots participating in dialogues with human\ninterlocutors must be able to naturally and efficiently communicate about the\nentities in their environment. A key aspect of such communication is the use of\nanaphoric language. The linguistic theory of the Givenness Hierarchy(GH)\nsuggests that humans use anaphora based on the cognitive statuses their\nreferents have in the minds of their interlocutors. In previous work,\nresearchers presented GH-theoretic approaches to robot anaphora understanding.\nIn this paper we describe how the GH might need to be used quite differently to\nfacilitate robot anaphora generation.", "published": "2020-07-17 17:51:29", "link": "http://arxiv.org/abs/2007.16009v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi-Perspective Semantic Information Retrieval in the Biomedical\n  Domain", "abstract": "Information Retrieval (IR) is the task of obtaining pieces of data (such as\ndocuments) that are relevant to a particular query or need from a large\nrepository of information. IR is a valuable component of several downstream\nNatural Language Processing (NLP) tasks. Practically, IR is at the heart of\nmany widely-used technologies like search engines. While probabilistic ranking\nfunctions like the Okapi BM25 function have been utilized in IR systems since\nthe 1970's, modern neural approaches pose certain advantages compared to their\nclassical counterparts. In particular, the release of BERT (Bidirectional\nEncoder Representations from Transformers) has had a significant impact in the\nNLP community by demonstrating how the use of a Masked Language Model trained\non a large corpus of data can improve a variety of downstream NLP tasks,\nincluding sentence classification and passage re-ranking. IR Systems are also\nimportant in the biomedical and clinical domains. Given the increasing amount\nof scientific literature across biomedical domain, the ability find answers to\nspecific clinical queries from a repository of millions of articles is a matter\nof practical value to medical professionals. Moreover, there are\ndomain-specific challenges present, including handling clinical jargon and\nevaluating the similarity or relatedness of various medical symptoms when\ndetermining the relevance between a query and a sentence. This work presents\ncontributions to several aspects of the Biomedical Semantic Information\nRetrieval domain. First, it introduces Multi-Perspective Sentence Relevance, a\nnovel methodology of utilizing BERT-based models for contextual IR. The system\nis evaluated using the BioASQ Biomedical IR Challenge. Finally, practical\ncontributions in the form of a live IR system for medics and a proposed\nchallenge on the Living Systematic Review clinical task are provided.", "published": "2020-07-17 21:05:44", "link": "http://arxiv.org/abs/2008.01526v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Training with reduced precision of a support vector machine model for\n  text classification", "abstract": "This paper presents the impact of using quantization on the efficiency of\nmulti-class text classification in the training process of a support vector\nmachine (SVM). This work is focused on comparing the efficiency of SVM model\ntrained using reduced precision with its original form. The main advantage of\nusing quantization is decrease in computation time and in memory footprint on\nthe dedicated hardware platform which supports low precision computation like\nGPU (16-bit) or FPGA (any bit-width). The paper presents the impact of a\nprecision reduction of the SVM training process on text classification\naccuracy. The implementation of the CPU was performed using the OpenMP library.\nAdditionally, the results of the implementation of the GPU using double, single\nand half precision are presented.", "published": "2020-07-17 11:59:30", "link": "http://arxiv.org/abs/2007.08657v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Neural Architecture Search For LF-MMI Trained Time Delay Neural Networks", "abstract": "Deep neural networks (DNNs) based automatic speech recognition (ASR) systems\nare often designed using expert knowledge and empirical evaluation. In this\npaper, a range of neural architecture search (NAS) techniques are used to\nautomatically learn two types of hyper-parameters of state-of-the-art factored\ntime delay neural networks (TDNNs): i) the left and right splicing context\noffsets; and ii) the dimensionality of the bottleneck linear projection at each\nhidden layer. These include the DARTS method integrating architecture selection\nwith lattice-free MMI (LF-MMI) TDNN training; Gumbel-Softmax and pipelined\nDARTS reducing the confusion over candidate architectures and improving the\ngeneralization of architecture selection; and Penalized DARTS incorporating\nresource constraints to adjust the trade-off between performance and system\ncomplexity. Parameter sharing among candidate architectures allows efficient\nsearch over up to $7^{28}$ different TDNN systems. Experiments conducted on the\n300-hour Switchboard corpus suggest the auto-configured systems consistently\noutperform the baseline LF-MMI TDNN systems using manual network design or\nrandom architecture search after LHUC speaker adaptation and RNNLM rescoring.\nAbsolute word error rate (WER) reductions up to 1.0\\% and relative model size\nreduction of 28\\% were obtained. Consistent performance improvements were also\nobtained on a UASpeech disordered speech recognition task using the proposed\nNAS approaches.", "published": "2020-07-17 08:32:11", "link": "http://arxiv.org/abs/2007.08818v4", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SummPip: Unsupervised Multi-Document Summarization with Sentence Graph\n  Compression", "abstract": "Obtaining training data for multi-document summarization (MDS) is time\nconsuming and resource-intensive, so recent neural models can only be trained\nfor limited domains. In this paper, we propose SummPip: an unsupervised method\nfor multi-document summarization, in which we convert the original documents to\na sentence graph, taking both linguistic and deep representation into account,\nthen apply spectral clustering to obtain multiple clusters of sentences, and\nfinally compress each cluster to generate the final summary. Experiments on\nMulti-News and DUC-2004 datasets show that our method is competitive to\nprevious unsupervised methods and is even comparable to the neural supervised\napproaches. In addition, human evaluation shows our system produces consistent\nand complete summaries compared to human written ones.", "published": "2020-07-17 13:01:15", "link": "http://arxiv.org/abs/2007.08954v2", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "WordCraft: An Environment for Benchmarking Commonsense Agents", "abstract": "The ability to quickly solve a wide range of real-world tasks requires a\ncommonsense understanding of the world. Yet, how to best extract such knowledge\nfrom natural language corpora and integrate it with reinforcement learning (RL)\nagents remains an open challenge. This is partly due to the lack of lightweight\nsimulation environments that sufficiently reflect the semantics of the real\nworld and provide knowledge sources grounded with respect to observations in an\nRL environment. To better enable research on agents making use of commonsense\nknowledge, we propose WordCraft, an RL environment based on Little Alchemy 2.\nThis lightweight environment is fast to run and built upon entities and\nrelations inspired by real-world semantics. We evaluate several representation\nlearning methods on this new benchmark and propose a new method for integrating\nknowledge graphs with an RL agent.", "published": "2020-07-17 18:40:46", "link": "http://arxiv.org/abs/2007.09185v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "CTC-Segmentation of Large Corpora for German End-to-end Speech\n  Recognition", "abstract": "Recent end-to-end Automatic Speech Recognition (ASR) systems demonstrated the\nability to outperform conventional hybrid DNN/ HMM ASR. Aside from\narchitectural improvements in those systems, those models grew in terms of\ndepth, parameters and model capacity. However, these models also require more\ntraining data to achieve comparable performance.\n  In this work, we combine freely available corpora for German speech\nrecognition, including yet unlabeled speech data, to a big dataset of over\n$1700$h of speech data. For data preparation, we propose a two-stage approach\nthat uses an ASR model pre-trained with Connectionist Temporal Classification\n(CTC) to boot-strap more training data from unsegmented or unlabeled training\ndata. Utterances are then extracted from label probabilities obtained from the\nnetwork trained with CTC to determine segment alignments. With this training\ndata, we trained a hybrid CTC/attention Transformer model that achieves\n$12.8\\%$ WER on the Tuda-DE test set, surpassing the previous baseline of\n$14.4\\%$ of conventional hybrid DNN/HMM ASR.", "published": "2020-07-17 17:38:08", "link": "http://arxiv.org/abs/2007.09127v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Streaming ResLSTM with Causal Mean Aggregation for Device-Directed\n  Utterance Detection", "abstract": "In this paper, we propose a streaming model to distinguish voice queries\nintended for a smart-home device from background speech. The proposed model\nconsists of multiple CNN layers with residual connections, followed by a\nstacked LSTM architecture. The streaming capability is achieved by using\nunidirectional LSTM layers and a causal mean aggregation layer to form the\nfinal utterance-level prediction up to the current frame. In order to avoid\nredundant computation during online streaming inference, we use a caching\nmechanism for every convolution operation. Experimental results on a\ndevice-directed vs. non device-directed task show that the proposed model\nyields an equal error rate reduction of 41% compared to our previous best model\non this task. Furthermore, we show that the proposed model is able to\naccurately predict earlier in time compared to the attention-based models.", "published": "2020-07-17 21:30:11", "link": "http://arxiv.org/abs/2007.09245v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SkipConvNet: Skip Convolutional Neural Network for Speech\n  Dereverberation using Optimally Smoothed Spectral Mapping", "abstract": "The reliability of using fully convolutional networks (FCNs) has been\nsuccessfully demonstrated by recent studies in many speech applications. One of\nthe most popular variants of these FCNs is the `U-Net', which is an\nencoder-decoder network with skip connections. In this study, we propose\n`SkipConvNet' where we replace each skip connection with multiple convolutional\nmodules to provide decoder with intuitive feature maps rather than encoder's\noutput to improve the learning capacity of the network. We also propose the use\nof optimal smoothing of power spectral density (PSD) as a pre-processing step,\nwhich helps to further enhance the efficiency of the network. To evaluate our\nproposed system, we use the REVERB challenge corpus to assess the performance\nof various enhancement approaches under the same conditions. We focus solely on\nmonitoring improvements in speech quality and their contribution to improving\nthe efficiency of back-end speech systems, such as speech recognition and\nspeaker verification, trained on only clean speech. Experimental findings show\nthat the proposed system consistently outperforms other approaches.", "published": "2020-07-17 17:43:00", "link": "http://arxiv.org/abs/2007.09131v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Speech2Video Synthesis with 3D Skeleton Regularization and Expressive\n  Body Poses", "abstract": "In this paper, we propose a novel approach to convert given speech audio to a\nphoto-realistic speaking video of a specific person, where the output video has\nsynchronized, realistic, and expressive rich body dynamics. We achieve this by\nfirst generating 3D skeleton movements from the audio sequence using a\nrecurrent neural network (RNN), and then synthesizing the output video via a\nconditional generative adversarial network (GAN). To make the skeleton movement\nrealistic and expressive, we embed the knowledge of an articulated 3D human\nskeleton and a learned dictionary of personal speech iconic gestures into the\ngeneration process in both learning and testing pipelines. The former prevents\nthe generation of unreasonable body distortion, while the later helps our model\nquickly learn meaningful body movement through a few recorded videos. To\nproduce photo-realistic and high-resolution video with motion details, we\npropose to insert part attention mechanisms in the conditional GAN, where each\ndetailed part, e.g. head and hand, is automatically zoomed in to have their own\ndiscriminators. To validate our approach, we collect a dataset with 20\nhigh-quality videos from 1 male and 1 female model reading various documents\nunder different topics. Compared with previous SoTA pipelines handling similar\ntasks, our approach achieves better results by a user study.", "published": "2020-07-17 19:30:14", "link": "http://arxiv.org/abs/2007.09198v5", "categories": ["cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Deep multi-metric learning for text-independent speaker verification", "abstract": "Text-independent speaker verification is an important artificial intelligence\nproblem that has a wide spectrum of applications, such as criminal\ninvestigation, payment certification, and interest-based customer services. The\npurpose of text-independent speaker verification is to determine whether two\ngiven uncontrolled utterances originate from the same speaker or not.\nExtracting speech features for each speaker using deep neural networks is a\npromising direction to explore and a straightforward solution is to train the\ndiscriminative feature extraction network by using a metric learning loss\nfunction. However, a single loss function often has certain limitations. Thus,\nwe use deep multi-metric learning to address the problem and introduce three\ndifferent losses for this problem, i.e., triplet loss, n-pair loss and angular\nloss. The three loss functions work in a cooperative way to train a feature\nextraction network equipped with Residual connections and\nsqueeze-and-excitation attention. We conduct experiments on the large-scale\n\\texttt{VoxCeleb2} dataset, which contains over a million utterances from over\n$6,000$ speakers, and the proposed deep neural network obtains an equal error\nrate of $3.48\\%$, which is a very competitive result. Codes for both training\nand testing and pretrained models are available at\n\\url{https://github.com/GreatJiweix/DmmlTiSV}, which is the first publicly\navailable code repository for large-scale text-independent speaker verification\nwith performance on par with the state-of-the-art systems.", "published": "2020-07-17 13:19:44", "link": "http://arxiv.org/abs/2007.10479v1", "categories": ["eess.AS", "cs.CV", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Self-Supervised Learning of Context-Aware Pitch Prosody Representations", "abstract": "In music and speech, meaning is derived at multiple levels of context.\nAffect, for example, can be inferred both by a short sound token and by sonic\npatterns over a longer temporal window such as an entire recording. In this\nletter, we focus on inferring meaning from this dichotomy of contexts. We show\nhow contextual representations of short sung vocal lines can be implicitly\nlearned from fundamental frequency ($F_0$) and thus be used as a meaningful\nfeature space for downstream Music Information Retrieval (MIR) tasks. We\npropose three self-supervised deep learning paradigms which leverage pseudotask\nlearning of these two levels of context to produce latent representation\nspaces. We evaluate the usefulness of these representations by embedding unseen\npitch contours into each space and conducting downstream classification tasks.\nOur results show that contextual representation can enhance downstream\nclassification by as much as 15\\% as compared to using traditional statistical\ncontour features.", "published": "2020-07-17 15:41:00", "link": "http://arxiv.org/abs/2007.09060v4", "categories": ["cs.SD", "cs.CV", "cs.IR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
