{"title": "Controllable Data Augmentation for Few-Shot Text Mining with\n  Chain-of-Thought Attribute Manipulation", "abstract": "Prompting large language models (LLMs) for data augmentation has recently\nbecome a common practice in few-shot NLP tasks. In this paper, we propose\nChain-of-Thought Attribute Manipulation (CoTAM), a novel approach that\ngenerates new data from existing examples by only tweaking in the\nuser-provided, task-specific attribute, e.g., sentiment polarity or topic in\nmovie reviews. Instead of conventional latent representation controlling, we\nleverage the chain-of-thought prompting to directly edit the text in three\nsteps, (1) attribute decomposition, (2) manipulation proposal, and (3) sentence\nreconstruction. Extensive results on various tasks, such as text (pair)\nclassification, aspect-based sentiment analysis, and conditional text\ngeneration, verify the superiority of CoTAM over other LLM-based augmentation\nmethods with the same number of training examples for both fine-tuning and\nin-context learning. Remarkably, the 2D visualization of the augmented dataset\nusing principal component analysis revealed a human-recognizable decision\nboundary that is likely hinted by the attribute manipulation, demonstrating the\npotential of our proposed approach.", "published": "2023-07-14 00:10:03", "link": "http://arxiv.org/abs/2307.07099v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MMSD2.0: Towards a Reliable Multi-modal Sarcasm Detection System", "abstract": "Multi-modal sarcasm detection has attracted much recent attention.\nNevertheless, the existing benchmark (MMSD) has some shortcomings that hinder\nthe development of reliable multi-modal sarcasm detection system: (1) There are\nsome spurious cues in MMSD, leading to the model bias learning; (2) The\nnegative samples in MMSD are not always reasonable. To solve the aforementioned\nissues, we introduce MMSD2.0, a correction dataset that fixes the shortcomings\nof MMSD, by removing the spurious cues and re-annotating the unreasonable\nsamples. Meanwhile, we present a novel framework called multi-view CLIP that is\ncapable of leveraging multi-grained cues from multiple perspectives (i.e.,\ntext, image, and text-image interaction view) for multi-modal sarcasm\ndetection. Extensive experiments show that MMSD2.0 is a valuable benchmark for\nbuilding reliable multi-modal sarcasm detection systems and multi-view CLIP can\nsignificantly outperform the previous best baselines.", "published": "2023-07-14 03:22:51", "link": "http://arxiv.org/abs/2307.07135v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving BERT with Hybrid Pooling Network and Drop Mask", "abstract": "Transformer-based pre-trained language models, such as BERT, achieve great\nsuccess in various natural language understanding tasks. Prior research found\nthat BERT captures a rich hierarchy of linguistic information at different\nlayers. However, the vanilla BERT uses the same self-attention mechanism for\neach layer to model the different contextual features. In this paper, we\npropose a HybridBERT model which combines self-attention and pooling networks\nto encode different contextual features in each layer. Additionally, we propose\na simple DropMask method to address the mismatch between pre-training and\nfine-tuning caused by excessive use of special mask tokens during Masked\nLanguage Modeling pre-training. Experiments show that HybridBERT outperforms\nBERT in pre-training with lower loss, faster training speed (8% relative),\nlower memory cost (13% relative), and also in transfer learning with 1.5%\nrelative higher accuracies on downstream tasks. Additionally, DropMask improves\naccuracies of BERT on downstream tasks across various masking rates.", "published": "2023-07-14 10:20:08", "link": "http://arxiv.org/abs/2307.07258v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MorphPiece : A Linguistic Tokenizer for Large Language Models", "abstract": "Tokenization is a critical part of modern NLP pipelines. However,\ncontemporary tokenizers for Large Language Models are based on statistical\nanalysis of text corpora, without much consideration to the linguistic\nfeatures. I propose a linguistically motivated tokenization scheme, MorphPiece,\nwhich is based partly on morphological segmentation of the underlying text. A\nGPT-style causal language model trained on this tokenizer (called MorphGPT)\nshows comparable or superior performance on a variety of supervised and\nunsupervised NLP tasks, compared to the OpenAI GPT-2 model. Specifically I\nevaluated MorphGPT on language modeling tasks, zero-shot performance on GLUE\nBenchmark with various prompt templates, massive text embedding benchmark\n(MTEB) for supervised and unsupervised performance, and lastly with another\nmorphological tokenization scheme (FLOTA, Hoffmann et al., 2022) and find that\nthe model trained on MorphPiece outperforms GPT-2 on most evaluations, at times\nwith considerable margin, despite being trained for about half the training\niterations.", "published": "2023-07-14 10:35:04", "link": "http://arxiv.org/abs/2307.07262v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are words equally surprising in audio and audio-visual comprehension?", "abstract": "We report a controlled study investigating the effect of visual information\n(i.e., seeing the speaker) on spoken language comprehension. We compare the ERP\nsignature (N400) associated with each word in audio-only and audio-visual\npresentations of the same verbal stimuli. We assess the extent to which\nsurprisal measures (which quantify the predictability of words in their lexical\ncontext) are generated on the basis of different types of language models\n(specifically n-gram and Transformer models) that predict N400 responses for\neach word. Our results indicate that cognitive effort differs significantly\nbetween multimodal and unimodal settings. In addition, our findings suggest\nthat while Transformer-based models, which have access to a larger lexical\ncontext, provide a better fit in the audio-only setting, 2-gram language models\nare more effective in the multimodal setting. This highlights the significant\nimpact of local lexical context on cognitive processing in a multimodal\nenvironment.", "published": "2023-07-14 11:17:37", "link": "http://arxiv.org/abs/2307.07277v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using Large Language Models for Zero-Shot Natural Language Generation\n  from Knowledge Graphs", "abstract": "In any system that uses structured knowledge graph (KG) data as its\nunderlying knowledge representation, KG-to-text generation is a useful tool for\nturning parts of the graph data into text that can be understood by humans.\nRecent work has shown that models that make use of pretraining on large amounts\nof text data can perform well on the KG-to-text task even with relatively small\nsets of training data on the specific graph-to-text task. In this paper, we\nbuild on this concept by using large language models to perform zero-shot\ngeneration based on nothing but the model's understanding of the triple\nstructure from what it can read. We show that ChatGPT achieves near\nstate-of-the-art performance on some measures of the WebNLG 2020 challenge, but\nfalls behind on others. Additionally, we compare factual, counter-factual and\nfictional statements, and show that there is a significant connection between\nwhat the LLM already knows about the data it is parsing and the quality of the\noutput text.", "published": "2023-07-14 12:45:03", "link": "http://arxiv.org/abs/2307.07312v2", "categories": ["cs.CL", "68T50", "I.2.7; I.2.4"], "primary_category": "cs.CL"}
{"title": "QontSum: On Contrasting Salient Content for Query-focused Summarization", "abstract": "Query-focused summarization (QFS) is a challenging task in natural language\nprocessing that generates summaries to address specific queries. The broader\nfield of Generative Information Retrieval (Gen-IR) aims to revolutionize\ninformation extraction from vast document corpora through generative\napproaches, encompassing Generative Document Retrieval (GDR) and Grounded\nAnswer Retrieval (GAR). This paper highlights the role of QFS in Grounded\nAnswer Generation (GAR), a key subdomain of Gen-IR that produces human-readable\nanswers in direct correspondence with queries, grounded in relevant documents.\nIn this study, we propose QontSum, a novel approach for QFS that leverages\ncontrastive learning to help the model attend to the most relevant regions of\nthe input document. We evaluate our approach on a couple of benchmark datasets\nfor QFS and demonstrate that it either outperforms existing state-of-the-art or\nexhibits a comparable performance with considerably reduced computational cost\nthrough enhancements in the fine-tuning stage, rather than relying on\nlarge-scale pre-training experiments, which is the focus of current SOTA.\nMoreover, we conducted a human study and identified improvements in the\nrelevance of generated summaries to the posed queries without compromising\nfluency. We further conduct an error analysis study to understand our model's\nlimitations and propose avenues for future research.", "published": "2023-07-14 19:25:35", "link": "http://arxiv.org/abs/2307.07586v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Domain Adaptation using Lexical Transformations and Label\n  Injection for Twitter Data", "abstract": "Domain adaptation is an important and widely studied problem in natural\nlanguage processing. A large body of literature tries to solve this problem by\nadapting models trained on the source domain to the target domain. In this\npaper, we instead solve this problem from a dataset perspective. We modify the\nsource domain dataset with simple lexical transformations to reduce the domain\nshift between the source dataset distribution and the target dataset\ndistribution. We find that models trained on the transformed source domain\ndataset performs significantly better than zero-shot models. Using our proposed\ntransformations to convert standard English to tweets, we reach an unsupervised\npart-of-speech (POS) tagging accuracy of 92.14% (from 81.54% zero shot\naccuracy), which is only slightly below the supervised performance of 94.45%.\nWe also use our proposed transformations to synthetically generate tweets and\naugment the Twitter dataset to achieve state-of-the-art performance for POS\ntagging.", "published": "2023-07-14 13:19:07", "link": "http://arxiv.org/abs/2307.10210v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sensi-BERT: Towards Sensitivity Driven Fine-Tuning for\n  Parameter-Efficient BERT", "abstract": "Large pre-trained language models have recently gained significant traction\ndue to their improved performance on various down-stream tasks like text\nclassification and question answering, requiring only few epochs of\nfine-tuning. However, their large model sizes often prohibit their applications\non resource-constrained edge devices. Existing solutions of yielding\nparameter-efficient BERT models largely rely on compute-exhaustive training and\nfine-tuning. Moreover, they often rely on additional compute heavy models to\nmitigate the performance gap. In this paper, we present Sensi-BERT, a\nsensitivity driven efficient fine-tuning of BERT models that can take an\noff-the-shelf pre-trained BERT model and yield highly parameter-efficient\nmodels for downstream tasks. In particular, we perform sensitivity analysis to\nrank each individual parameter tensor, that then is used to trim them\naccordingly during fine-tuning for a given parameter or FLOPs budget. Our\nexperiments show the efficacy of Sensi-BERT across different downstream tasks\nincluding MNLI, QQP, QNLI, SST-2 and SQuAD, showing better performance at\nsimilar or smaller parameter budget compared to various alternatives.", "published": "2023-07-14 17:24:15", "link": "http://arxiv.org/abs/2307.11764v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do not Mask Randomly: Effective Domain-adaptive Pre-training by Masking\n  In-domain Keywords", "abstract": "We propose a novel task-agnostic in-domain pre-training method that sits\nbetween generic pre-training and fine-tuning. Our approach selectively masks\nin-domain keywords, i.e., words that provide a compact representation of the\ntarget domain. We identify such keywords using KeyBERT (Grootendorst, 2020). We\nevaluate our approach using six different settings: three datasets combined\nwith two distinct pre-trained language models (PLMs). Our results reveal that\nthe fine-tuned PLMs adapted using our in-domain pre-training strategy\noutperform PLMs that used in-domain pre-training with random masking as well as\nthose that followed the common pre-train-then-fine-tune paradigm. Further, the\noverhead of identifying in-domain keywords is reasonable, e.g., 7-15% of the\npre-training time (for two epochs) for BERT Large (Devlin et al., 2019).", "published": "2023-07-14 05:09:04", "link": "http://arxiv.org/abs/2307.07160v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Drive Like a Human: Rethinking Autonomous Driving with Large Language\n  Models", "abstract": "In this paper, we explore the potential of using a large language model (LLM)\nto understand the driving environment in a human-like manner and analyze its\nability to reason, interpret, and memorize when facing complex scenarios. We\nargue that traditional optimization-based and modular autonomous driving (AD)\nsystems face inherent performance limitations when dealing with long-tail\ncorner cases. To address this problem, we propose that an ideal AD system\nshould drive like a human, accumulating experience through continuous driving\nand using common sense to solve problems. To achieve this goal, we identify\nthree key abilities necessary for an AD system: reasoning, interpretation, and\nmemorization. We demonstrate the feasibility of employing an LLM in driving\nscenarios by building a closed-loop system to showcase its comprehension and\nenvironment-interaction abilities. Our extensive experiments show that the LLM\nexhibits the impressive ability to reason and solve long-tailed cases,\nproviding valuable insights for the development of human-like autonomous\ndriving. The related code are available at\nhttps://github.com/PJLab-ADG/DriveLikeAHuman .", "published": "2023-07-14 05:18:34", "link": "http://arxiv.org/abs/2307.07162v1", "categories": ["cs.RO", "cs.CL"], "primary_category": "cs.RO"}
{"title": "Learning to Retrieve In-Context Examples for Large Language Models", "abstract": "Large language models (LLMs) have demonstrated their ability to learn\nin-context, allowing them to perform various tasks based on a few input-output\nexamples. However, the effectiveness of in-context learning is heavily reliant\non the quality of the selected examples. In this paper, we propose a novel\nframework to iteratively train dense retrievers that can identify high-quality\nin-context examples for LLMs. Our framework initially trains a reward model\nbased on LLM feedback to evaluate the quality of candidate examples, followed\nby knowledge distillation to train a bi-encoder based dense retriever. Our\nexperiments on a suite of $30$ tasks demonstrate that our framework\nsignificantly enhances in-context learning performance. Furthermore, we show\nthe generalization ability of our framework to unseen tasks during training. An\nin-depth analysis reveals that our model improves performance by retrieving\nexamples with similar patterns, and the gains are consistent across LLMs of\nvarying sizes. The code and data are available at\nhttps://github.com/microsoft/LMOps/tree/main/llm_retriever .", "published": "2023-07-14 05:23:08", "link": "http://arxiv.org/abs/2307.07164v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Certified Robustness for Large Language Models with Self-Denoising", "abstract": "Although large language models (LLMs) have achieved great success in vast\nreal-world applications, their vulnerabilities towards noisy inputs have\nsignificantly limited their uses, especially in high-stake environments. In\nthese contexts, it is crucial to ensure that every prediction made by large\nlanguage models is stable, i.e., LLM predictions should be consistent given\nminor differences in the input. This largely falls into the study of certified\nrobust LLMs, i.e., all predictions of LLM are certified to be correct in a\nlocal region around the input. Randomized smoothing has demonstrated great\npotential in certifying the robustness and prediction stability of LLMs.\nHowever, randomized smoothing requires adding noise to the input before model\nprediction, and its certification performance depends largely on the model's\nperformance on corrupted data. As a result, its direct application to LLMs\nremains challenging and often results in a small certification radius. To\naddress this issue, we take advantage of the multitasking nature of LLMs and\npropose to denoise the corrupted inputs with LLMs in a self-denoising manner.\nDifferent from previous works like denoised smoothing, which requires training\na separate model to robustify LLM, our method enjoys far better efficiency and\nflexibility. Our experiment results show that our method outperforms the\nexisting certification methods under both certified robustness and empirical\nrobustness. The codes are available at\nhttps://github.com/UCSB-NLP-Chang/SelfDenoise.", "published": "2023-07-14 05:40:24", "link": "http://arxiv.org/abs/2307.07171v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Dialogue Agents 101: A Beginner's Guide to Critical Ingredients for\n  Designing Effective Conversational Systems", "abstract": "Sharing ideas through communication with peers is the primary mode of human\ninteraction. Consequently, extensive research has been conducted in the area of\nconversational AI, leading to an increase in the availability and diversity of\nconversational tasks, datasets, and methods. However, with numerous tasks being\nexplored simultaneously, the current landscape of conversational AI becomes\nfragmented. Therefore, initiating a well-thought-out model for a dialogue agent\ncan pose significant challenges for a practitioner. Towards highlighting the\ncritical ingredients needed for a practitioner to design a dialogue agent from\nscratch, the current study provides a comprehensive overview of the primary\ncharacteristics of a dialogue agent, the supporting tasks, their corresponding\nopen-domain datasets, and the methods used to benchmark these datasets. We\nobserve that different methods have been used to tackle distinct dialogue\ntasks. However, building separate models for each task is costly and does not\nleverage the correlation among the several tasks of a dialogue agent. As a\nresult, recent trends suggest a shift towards building unified foundation\nmodels. To this end, we propose UNIT, a UNified dIalogue dataseT constructed\nfrom conversations of existing datasets for different dialogue tasks capturing\nthe nuances for each of them. We also examine the evaluation strategies used to\nmeasure the performance of dialogue agents and highlight the scope for future\nresearch in the area of conversational AI.", "published": "2023-07-14 10:05:47", "link": "http://arxiv.org/abs/2307.07255v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "C3: Zero-shot Text-to-SQL with ChatGPT", "abstract": "This paper proposes a ChatGPT-based zero-shot Text-to-SQL method, dubbed C3,\nwhich achieves 82.3\\% in terms of execution accuracy on the holdout test set of\nSpider and becomes the state-of-the-art zero-shot Text-to-SQL method on the\nSpider Challenge. C3 consists of three key components: Clear Prompting (CP),\nCalibration with Hints (CH), and Consistent Output (CO), which are\ncorresponding to the model input, model bias and model output respectively. It\nprovides a systematic treatment for zero-shot Text-to-SQL. Extensive\nexperiments have been conducted to verify the effectiveness and efficiency of\nour proposed method.", "published": "2023-07-14 12:30:41", "link": "http://arxiv.org/abs/2307.07306v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Gloss Attention for Gloss-free Sign Language Translation", "abstract": "Most sign language translation (SLT) methods to date require the use of gloss\nannotations to provide additional supervision information, however, the\nacquisition of gloss is not easy. To solve this problem, we first perform an\nanalysis of existing models to confirm how gloss annotations make SLT easier.\nWe find that it can provide two aspects of information for the model, 1) it can\nhelp the model implicitly learn the location of semantic boundaries in\ncontinuous sign language videos, 2) it can help the model understand the sign\nlanguage video globally. We then propose \\emph{gloss attention}, which enables\nthe model to keep its attention within video segments that have the same\nsemantics locally, just as gloss helps existing models do. Furthermore, we\ntransfer the knowledge of sentence-to-sentence similarity from the natural\nlanguage model to our gloss attention SLT network (GASLT) to help it understand\nsign language videos at the sentence level. Experimental results on multiple\nlarge-scale sign language datasets show that our proposed GASLT model\nsignificantly outperforms existing methods. Our code is provided in\n\\url{https://github.com/YinAoXiong/GASLT}.", "published": "2023-07-14 14:07:55", "link": "http://arxiv.org/abs/2307.07361v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "A scoping review on multimodal deep learning in biomedical images and\n  texts", "abstract": "Computer-assisted diagnostic and prognostic systems of the future should be\ncapable of simultaneously processing multimodal data. Multimodal deep learning\n(MDL), which involves the integration of multiple sources of data, such as\nimages and text, has the potential to revolutionize the analysis and\ninterpretation of biomedical data. However, it only caught researchers'\nattention recently. To this end, there is a critical need to conduct a\nsystematic review on this topic, identify the limitations of current work, and\nexplore future directions. In this scoping review, we aim to provide a\ncomprehensive overview of the current state of the field and identify key\nconcepts, types of studies, and research gaps with a focus on biomedical images\nand texts joint learning, mainly because these two were the most commonly\navailable data types in MDL research. This study reviewed the current uses of\nmultimodal deep learning on five tasks: (1) Report generation, (2) Visual\nquestion answering, (3) Cross-modal retrieval, (4) Computer-aided diagnosis,\nand (5) Semantic segmentation. Our results highlight the diverse applications\nand potential of MDL and suggest directions for future research in the field.\nWe hope our review will facilitate the collaboration of natural language\nprocessing (NLP) and medical imaging communities and support the next\ngeneration of decision-making and computer-assisted diagnostic system\ndevelopment.", "published": "2023-07-14 14:08:54", "link": "http://arxiv.org/abs/2307.07362v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Composition-contrastive Learning for Sentence Embeddings", "abstract": "Vector representations of natural language are ubiquitous in search\napplications. Recently, various methods based on contrastive learning have been\nproposed to learn textual representations from unlabelled data; by maximizing\nalignment between minimally-perturbed embeddings of the same text, and\nencouraging a uniform distribution of embeddings across a broader corpus.\nDifferently, we propose maximizing alignment between texts and a composition of\ntheir phrasal constituents. We consider several realizations of this objective\nand elaborate the impact on representations in each case. Experimental results\non semantic textual similarity tasks show improvements over baselines that are\ncomparable with state-of-the-art approaches. Moreover, this work is the first\nto do so without incurring costs in auxiliary training objectives or additional\nnetwork parameters.", "published": "2023-07-14 14:39:35", "link": "http://arxiv.org/abs/2307.07380v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Rank Your Summaries: Enhancing Bengali Text Summarization via\n  Ranking-based Approach", "abstract": "With the increasing need for text summarization techniques that are both\nefficient and accurate, it becomes crucial to explore avenues that enhance the\nquality and precision of pre-trained models specifically tailored for\nsummarizing Bengali texts. When it comes to text summarization tasks, there are\nnumerous pre-trained transformer models at one's disposal. Consequently, it\nbecomes quite a challenge to discern the most informative and relevant summary\nfor a given text among the various options generated by these pre-trained\nsummarization models. This paper aims to identify the most accurate and\ninformative summary for a given text by utilizing a simple but effective\nranking-based approach that compares the output of four different pre-trained\nBengali text summarization models. The process begins by carrying out\npreprocessing of the input text that involves eliminating unnecessary elements\nsuch as special characters and punctuation marks. Next, we utilize four\npre-trained summarization models to generate summaries, followed by applying a\ntext ranking algorithm to identify the most suitable summary. Ultimately, the\nsummary with the highest ranking score is chosen as the final one. To evaluate\nthe effectiveness of this approach, the generated summaries are compared\nagainst human-annotated summaries using standard NLG metrics such as BLEU,\nROUGE, BERTScore, WIL, WER, and METEOR. Experimental results suggest that by\nleveraging the strengths of each pre-trained transformer model and combining\nthem using a ranking-based approach, our methodology significantly improves the\naccuracy and effectiveness of the Bengali text summarization.", "published": "2023-07-14 15:07:20", "link": "http://arxiv.org/abs/2307.07392v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "HuCurl: Human-induced Curriculum Discovery", "abstract": "We introduce the problem of curriculum discovery and describe a curriculum\nlearning framework capable of discovering effective curricula in a curriculum\nspace based on prior knowledge about sample difficulty. Using annotation\nentropy and loss as measures of difficulty, we show that (i): the\ntop-performing discovered curricula for a given model and dataset are often\nnon-monotonic as opposed to monotonic curricula in existing literature, (ii):\nthe prevailing easy-to-hard or hard-to-easy transition curricula are often at\nthe risk of underperforming, and (iii): the curricula discovered for smaller\ndatasets and models perform well on larger datasets and models respectively.\nThe proposed framework encompasses some of the existing curriculum learning\napproaches and can discover curricula that outperform them across several NLP\ntasks.", "published": "2023-07-14 15:41:43", "link": "http://arxiv.org/abs/2307.07412v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Towards Generalizable Detection of Urgency of Discussion Forum Posts", "abstract": "Students who take an online course, such as a MOOC, use the course's\ndiscussion forum to ask questions or reach out to instructors when encountering\nan issue. However, reading and responding to students' questions is difficult\nto scale because of the time needed to consider each message. As a result,\ncritical issues may be left unresolved, and students may lose the motivation to\ncontinue in the course. To help address this problem, we build predictive\nmodels that automatically determine the urgency of each forum post, so that\nthese posts can be brought to instructors' attention. This paper goes beyond\nprevious work by predicting not just a binary decision cut-off but a post's\nlevel of urgency on a 7-point scale. First, we train and cross-validate several\nmodels on an original data set of 3,503 posts from MOOCs at University of\nPennsylvania. Second, to determine the generalizability of our models, we test\ntheir performance on a separate, previously published data set of 29,604 posts\nfrom MOOCs at Stanford University. While the previous work on post urgency used\nonly one data set, we evaluated the prediction across different data sets and\ncourses. The best-performing model was a support vector regressor trained on\nthe Universal Sentence Encoder embeddings of the posts, achieving an RMSE of\n1.1 on the training set and 1.4 on the test set. Understanding the urgency of\nforum posts enables instructors to focus their time more effectively and, as a\nresult, better support student learning.", "published": "2023-07-14 20:21:50", "link": "http://arxiv.org/abs/2307.07614v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Othering and low status framing of immigrant cuisines in US restaurant\n  reviews and large language models", "abstract": "Identifying implicit attitudes toward food can mitigate social prejudice due\nto food's salience as a marker of ethnic identity. Stereotypes about food are\nrepresentational harms that may contribute to racialized discourse and\nnegatively impact economic outcomes for restaurants. Understanding the presence\nof representational harms in online corpora in particular is important, given\nthe increasing use of large language models (LLMs) for text generation and\ntheir tendency to reproduce attitudes in their training data. Through careful\nlinguistic analyses, we evaluate social theories about attitudes toward\nimmigrant cuisine in a large-scale study of framing differences in 2.1M English\nlanguage Yelp reviews. Controlling for factors such as restaurant price and\nneighborhood racial diversity, we find that immigrant cuisines are more likely\nto be othered using socially constructed frames of authenticity (e.g.,\n\"authentic,\" \"traditional\"), and that non-European cuisines (e.g., Indian,\nMexican) in particular are described as more exotic compared to European ones\n(e.g., French). We also find that non-European cuisines are more likely to be\ndescribed as cheap and dirty, even after controlling for price, and even among\nthe most expensive restaurants. Finally, we show that reviews generated by LLMs\nreproduce similar framing tendencies, pointing to the downstream retention of\nthese representational harms. Our results corroborate social theories of\ngastronomic stereotyping, revealing racialized evaluative processes and\nlinguistic strategies through which they manifest.", "published": "2023-07-14 22:25:39", "link": "http://arxiv.org/abs/2307.07645v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mitigating Bias in Conversations: A Hate Speech Classifier and Debiaser\n  with Prompts", "abstract": "Discriminatory language and biases are often present in hate speech during\nconversations, which usually lead to negative impacts on targeted groups such\nas those based on race, gender, and religion. To tackle this issue, we propose\nan approach that involves a two-step process: first, detecting hate speech\nusing a classifier, and then utilizing a debiasing component that generates\nless biased or unbiased alternatives through prompts. We evaluated our approach\non a benchmark dataset and observed reduction in negativity due to hate speech\ncomments. The proposed method contributes to the ongoing efforts to reduce\nbiases in online discourse and promote a more inclusive and fair environment\nfor communication.", "published": "2023-07-14 13:33:28", "link": "http://arxiv.org/abs/2307.10213v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fairness of ChatGPT and the Role Of Explainable-Guided Prompts", "abstract": "Our research investigates the potential of Large-scale Language Models\n(LLMs), specifically OpenAI's GPT, in credit risk assessment-a binary\nclassification task. Our findings suggest that LLMs, when directed by\njudiciously designed prompts and supplemented with domain-specific knowledge,\ncan parallel the performance of traditional Machine Learning (ML) models.\nIntriguingly, they achieve this with significantly less data-40 times less,\nutilizing merely 20 data points compared to the ML's 800. LLMs particularly\nexcel in minimizing false positives and enhancing fairness, both being vital\naspects of risk analysis. While our results did not surpass those of classical\nML models, they underscore the potential of LLMs in analogous tasks, laying a\ngroundwork for future explorations into harnessing the capabilities of LLMs in\ndiverse ML tasks.", "published": "2023-07-14 09:20:16", "link": "http://arxiv.org/abs/2307.11761v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Similarity-based Memory Enhanced Joint Entity and Relation Extraction", "abstract": "Document-level joint entity and relation extraction is a challenging\ninformation extraction problem that requires a unified approach where a single\nneural network performs four sub-tasks: mention detection, coreference\nresolution, entity classification, and relation extraction. Existing methods\noften utilize a sequential multi-task learning approach, in which the arbitral\ndecomposition causes the current task to depend only on the previous one,\nmissing the possible existence of the more complex relationships between them.\nIn this paper, we present a multi-task learning framework with bidirectional\nmemory-like dependency between tasks to address those drawbacks and perform the\njoint problem more accurately. Our empirical studies show that the proposed\napproach outperforms the existing methods and achieves state-of-the-art results\non the BioCreative V CDR corpus.", "published": "2023-07-14 12:26:56", "link": "http://arxiv.org/abs/2307.11762v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Switching Head-Tail Funnel UNITER for Dual Referring Expression\n  Comprehension with Fetch-and-Carry Tasks", "abstract": "This paper describes a domestic service robot (DSR) that fetches everyday\nobjects and carries them to specified destinations according to free-form\nnatural language instructions. Given an instruction such as \"Move the bottle on\nthe left side of the plate to the empty chair,\" the DSR is expected to identify\nthe bottle and the chair from multiple candidates in the environment and carry\nthe target object to the destination. Most of the existing multimodal language\nunderstanding methods are impractical in terms of computational complexity\nbecause they require inferences for all combinations of target object\ncandidates and destination candidates. We propose Switching Head-Tail Funnel\nUNITER, which solves the task by predicting the target object and the\ndestination individually using a single model. Our method is validated on a\nnewly-built dataset consisting of object manipulation instructions and semi\nphoto-realistic images captured in a standard Embodied AI simulator. The\nresults show that our method outperforms the baseline method in terms of\nlanguage comprehension accuracy. Furthermore, we conduct physical experiments\nin which a DSR delivers standardized everyday objects in a standardized\ndomestic environment as requested by instructions with referring expressions.\nThe experimental results show that the object grasping and placing actions are\nachieved with success rates of more than 90%.", "published": "2023-07-14 05:27:56", "link": "http://arxiv.org/abs/2307.07166v1", "categories": ["cs.RO", "cs.CL", "cs.CV"], "primary_category": "cs.RO"}
{"title": "Replay to Remember: Continual Layer-Specific Fine-tuning for German\n  Speech Recognition", "abstract": "While Automatic Speech Recognition (ASR) models have shown significant\nadvances with the introduction of unsupervised or self-supervised training\ntechniques, these improvements are still only limited to a subsection of\nlanguages and speakers. Transfer learning enables the adaptation of large-scale\nmultilingual models to not only low-resource languages but also to more\nspecific speaker groups. However, fine-tuning on data from new domains is\nusually accompanied by a decrease in performance on the original domain.\nTherefore, in our experiments, we examine how well the performance of\nlarge-scale ASR models can be approximated for smaller domains, with our own\ndataset of German Senior Voice Commands (SVC-de), and how much of the general\nspeech recognition performance can be preserved by selectively freezing parts\nof the model during training. To further increase the robustness of the ASR\nmodel to vocabulary and speakers outside of the fine-tuned domain, we apply\nExperience Replay for continual learning. By adding only a fraction of data\nfrom the original domain, we are able to reach Word-Error-Rates (WERs) below\n5\\% on the new domain, while stabilizing performance for general speech\nrecognition at acceptable WERs.", "published": "2023-07-14 11:20:22", "link": "http://arxiv.org/abs/2307.07280v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Towards dialect-inclusive recognition in a low-resource language: are\n  balanced corpora the answer?", "abstract": "ASR systems are generally built for the spoken 'standard', and their\nperformance declines for non-standard dialects/varieties. This is a problem for\na language like Irish, where there is no single spoken standard, but rather\nthree major dialects: Ulster (Ul), Connacht (Co) and Munster (Mu). As a\ndiagnostic to quantify the effect of the speaker's dialect on recognition\nperformance, 12 ASR systems were trained, firstly using baseline\ndialect-balanced training corpora, and then using modified versions of the\nbaseline corpora, where dialect-specific materials were either subtracted or\nadded. Results indicate that dialect-balanced corpora do not yield a similar\nperformance across the dialects: the Ul dialect consistently underperforms,\nwhereas Mu yields lowest WERs. There is a close relationship between Co and Mu\ndialects, but one that is not symmetrical. These results will guide future\ncorpus collection and system building strategies to optimise for cross-dialect\nperformance equity.", "published": "2023-07-14 12:18:38", "link": "http://arxiv.org/abs/2307.07295v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Hybrid moderation in the newsroom: Recommending featured posts to\n  content moderators", "abstract": "Online news outlets are grappling with the moderation of user-generated\ncontent within their comment section. We present a recommender system based on\nranking class probabilities to support and empower the moderator in choosing\nfeatured posts, a time-consuming task. By combining user and textual content\nfeatures we obtain an optimal classification F1-score of 0.44 on the test set.\nFurthermore, we observe an optimum mean NDCG@5 of 0.87 on a large set of\nvalidation articles. As an expert evaluation, content moderators assessed the\noutput of a random selection of articles by choosing comments to feature based\non the recommendations, which resulted in a NDCG score of 0.83. We conclude\nthat first, adding text features yields the best score and second, while\nchoosing featured content remains somewhat subjective, content moderators found\nsuitable comments in all but one evaluated recommendations. We end the paper by\nanalyzing our best-performing model, a step towards transparency and\nexplainability in hybrid content moderation.", "published": "2023-07-14 12:51:12", "link": "http://arxiv.org/abs/2307.07317v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "How Different Is Stereotypical Bias Across Languages?", "abstract": "Recent studies have demonstrated how to assess the stereotypical bias in\npre-trained English language models. In this work, we extend this branch of\nresearch in multiple different dimensions by systematically investigating (a)\nmono- and multilingual models of (b) different underlying architectures with\nrespect to their bias in (c) multiple different languages. To that end, we make\nuse of the English StereoSet data set (Nadeem et al., 2021), which we\nsemi-automatically translate into German, French, Spanish, and Turkish. We find\nthat it is of major importance to conduct this type of analysis in a\nmultilingual setting, as our experiments show a much more nuanced picture as\nwell as notable differences from the English-only analysis. The main takeaways\nfrom our analysis are that mGPT-2 (partly) shows surprising anti-stereotypical\nbehavior across languages, English (monolingual) models exhibit the strongest\nbias, and the stereotypes reflected in the data set are least present in\nTurkish models. Finally, we release our codebase alongside the translated data\nsets and practical guidelines for the semi-automatic translation to encourage a\nfurther extension of our work to other languages.", "published": "2023-07-14 13:17:11", "link": "http://arxiv.org/abs/2307.07331v1", "categories": ["cs.CL", "cs.CY", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Towards spoken dialect identification of Irish", "abstract": "The Irish language is rich in its diversity of dialects and accents. This\ncompounds the difficulty of creating a speech recognition system for the\nlow-resource language, as such a system must contend with a high degree of\nvariability with limited corpora. A recent study investigating dialect bias in\nIrish ASR found that balanced training corpora gave rise to unequal dialect\nperformance, with performance for the Ulster dialect being consistently worse\nthan for the Connacht or Munster dialects. Motivated by this, the present\nexperiments investigate spoken dialect identification of Irish, with a view to\nincorporating such a system into the speech recognition pipeline. Two acoustic\nclassification models are tested, XLS-R and ECAPA-TDNN, in conjunction with a\ntext-based classifier using a pretrained Irish-language BERT model. The\nECAPA-TDNN, particularly a model pretrained for language identification on the\nVoxLingua107 dataset, performed best overall, with an accuracy of 73%. This was\nfurther improved to 76% by fusing the model's outputs with the text-based\nmodel. The Ulster dialect was most accurately identified, with an accuracy of\n94%, however the model struggled to disambiguate between the Connacht and\nMunster dialects, suggesting a more nuanced approach may be necessary to\nrobustly distinguish between the dialects of Irish.", "published": "2023-07-14 16:03:09", "link": "http://arxiv.org/abs/2307.07436v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Population Expansion for Training Language Models with Private Federated\n  Learning", "abstract": "Federated learning (FL) combined with differential privacy (DP) offers\nmachine learning (ML) training with distributed devices and with a formal\nprivacy guarantee. With a large population of devices, FL with DP produces a\nperformant model in a timely manner. However, for applications with a smaller\npopulation, not only does the model utility degrade as the DP noise is\ninversely proportional to population, but also the training latency increases\nsince waiting for enough clients to become available from a smaller pool is\nslower. In this work, we thus propose expanding the population based on domain\nadaptation techniques to speed up the training and improves the final model\nquality when training with small populations. We empirically demonstrate that\nour techniques can improve the utility by 13% to 30% on real-world language\nmodeling datasets.", "published": "2023-07-14 16:59:08", "link": "http://arxiv.org/abs/2307.07477v1", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "HYTREL: Hypergraph-enhanced Tabular Data Representation Learning", "abstract": "Language models pretrained on large collections of tabular data have\ndemonstrated their effectiveness in several downstream tasks. However, many of\nthese models do not take into account the row/column permutation invariances,\nhierarchical structure, etc. that exist in tabular data. To alleviate these\nlimitations, we propose HYTREL, a tabular language model, that captures the\npermutation invariances and three more structural properties of tabular data by\nusing hypergraphs - where the table cells make up the nodes and the cells\noccurring jointly together in each row, column, and the entire table are used\nto form three different types of hyperedges. We show that HYTREL is maximally\ninvariant under certain conditions for tabular data, i.e., two tables obtain\nthe same representations via HYTREL iff the two tables are identical up to\npermutations. Our empirical results demonstrate that HYTREL consistently\noutperforms other competitive baselines on four downstream tasks with minimal\npretraining, illustrating the advantages of incorporating the inductive biases\nassociated with tabular data into the representations. Finally, our qualitative\nanalyses showcase that HYTREL can assimilate the table structures to generate\nrobust representations for the cells, rows, columns, and the entire table.", "published": "2023-07-14 05:41:22", "link": "http://arxiv.org/abs/2307.08623v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Understanding Multi-Turn Toxic Behaviors in Open-Domain Chatbots", "abstract": "Recent advances in natural language processing and machine learning have led\nto the development of chatbot models, such as ChatGPT, that can engage in\nconversational dialogue with human users. However, the ability of these models\nto generate toxic or harmful responses during a non-toxic multi-turn\nconversation remains an open research question. Existing research focuses on\nsingle-turn sentence testing, while we find that 82\\% of the individual\nnon-toxic sentences that elicit toxic behaviors in a conversation are\nconsidered safe by existing tools. In this paper, we design a new attack,\n\\toxicbot, by fine-tuning a chatbot to engage in conversation with a target\nopen-domain chatbot. The chatbot is fine-tuned with a collection of crafted\nconversation sequences. Particularly, each conversation begins with a sentence\nfrom a crafted prompt sentences dataset. Our extensive evaluation shows that\nopen-domain chatbot models can be triggered to generate toxic responses in a\nmulti-turn conversation. In the best scenario, \\toxicbot achieves a 67\\%\nactivation rate. The conversation sequences in the fine-tuning stage help\ntrigger the toxicity in a conversation, which allows the attack to bypass two\ndefense methods. Our findings suggest that further research is needed to\naddress chatbot toxicity in a dynamic interactive environment. The proposed\n\\toxicbot can be used by both industry and researchers to develop methods for\ndetecting and mitigating toxic responses in conversational dialogue and improve\nthe robustness of chatbots for end users.", "published": "2023-07-14 03:58:42", "link": "http://arxiv.org/abs/2307.09579v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Large Language Models Understand and Can be Enhanced by Emotional\n  Stimuli", "abstract": "Emotional intelligence significantly impacts our daily behaviors and\ninteractions. Although Large Language Models (LLMs) are increasingly viewed as\na stride toward artificial general intelligence, exhibiting impressive\nperformance in numerous tasks, it is still uncertain if LLMs can genuinely\ngrasp psychological emotional stimuli. Understanding and responding to\nemotional cues gives humans a distinct advantage in problem-solving. In this\npaper, we take the first step towards exploring the ability of LLMs to\nunderstand emotional stimuli. To this end, we first conduct automatic\nexperiments on 45 tasks using various LLMs, including Flan-T5-Large, Vicuna,\nLlama 2, BLOOM, ChatGPT, and GPT-4. Our tasks span deterministic and generative\napplications that represent comprehensive evaluation scenarios. Our automatic\nexperiments show that LLMs have a grasp of emotional intelligence, and their\nperformance can be improved with emotional prompts (which we call\n\"EmotionPrompt\" that combines the original prompt with emotional stimuli),\ne.g., 8.00% relative performance improvement in Instruction Induction and 115%\nin BIG-Bench. In addition to those deterministic tasks that can be\nautomatically evaluated using existing metrics, we conducted a human study with\n106 participants to assess the quality of generative tasks using both vanilla\nand emotional prompts. Our human study results demonstrate that EmotionPrompt\nsignificantly boosts the performance of generative tasks (10.9% average\nimprovement in terms of performance, truthfulness, and responsibility metrics).\nWe provide an in-depth discussion regarding why EmotionPrompt works for LLMs\nand the factors that may influence its performance. We posit that EmotionPrompt\nheralds a novel avenue for exploring interdisciplinary knowledge for human-LLMs\ninteraction.", "published": "2023-07-14 00:57:12", "link": "http://arxiv.org/abs/2307.11760v7", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "A Topical Approach to Capturing Customer Insight In Social Media", "abstract": "The age of social media has opened new opportunities for businesses. This\nflourishing wealth of information is outside traditional channels and\nframeworks of classical marketing research, including that of Marketing Mix\nModeling (MMM). Textual data, in particular, poses many challenges that data\nanalysis practitioners must tackle. Social media constitute massive,\nheterogeneous, and noisy document sources. Industrial data acquisition\nprocesses include some amount of ETL. However, the variability of noise in the\ndata and the heterogeneity induced by different sources create the need for\nad-hoc tools. Put otherwise, customer insight extraction in fully unsupervised,\nnoisy contexts is an arduous task. This research addresses the challenge of\nfully unsupervised topic extraction in noisy, Big Data contexts. We present\nthree approaches we built on the Variational Autoencoder framework: the\nEmbedded Dirichlet Process, the Embedded Hierarchical Dirichlet Process, and\nthe time-aware Dynamic Embedded Dirichlet Process. These nonparametric\napproaches concerning topics present the particularity of determining word\nembeddings and topic embeddings. These embeddings do not require transfer\nlearning, but knowledge transfer remains possible. We test these approaches on\nbenchmark and automotive industry-related datasets from a real-world use case.\nWe show that our models achieve equal to better performance than\nstate-of-the-art methods and that the field of topic modeling would benefit\nfrom improved evaluation metrics.", "published": "2023-07-14 11:15:28", "link": "http://arxiv.org/abs/2307.11775v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Reproducing the Acoustic Velocity Vectors in a Spherical Listening\n  Region", "abstract": "Acoustic velocity vectors (AVVs) are related to the human's perception of\nsound at low frequencies and are widely used in Ambisonics. This paper proposes\na spatial sound field reproduction algorithm called velocity matching, which\nreproduces the AVVs in the spherical listening region by matching the AVVs'\nspherical harmonic coefficients. Using the sound field translation formula, the\nspherical harmonic coefficients of the AVVs are derived from the spherical\nharmonic coefficients of the pressure, which can be measured by a higher-order\nmicrophone array. Unlike algorithms that only control the AVVs at discrete\nsweet spots, the proposed velocity matching algorithm manipulates the AVVs in\nthe whole spherical listening region and allows the listener to move beyond the\nsweet spots. Simulations show the proposed velocity matching algorithm\naccurately reproduces the AVVs in the spherical listening region and requires\nfewer number of loudspeakers than pressure matching algorithm.", "published": "2023-07-14 07:30:59", "link": "http://arxiv.org/abs/2307.07200v5", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Low Rank Properties for Estimating Microphones Start Time and Sources\n  Emission Time", "abstract": "Uncertainty in timing information pertaining to the start time of microphone\nrecordings and sources' emission time pose significant challenges in various\napplications, such as joint microphones and sources localization. Traditional\noptimization methods, which directly estimate this unknown timing information\n(UTIm), often fall short compared to approaches exploiting the low-rank\nproperty (LRP). LRP encompasses an additional low-rank structure, facilitating\na linear constraint on UTIm to help formulate related low-rank structure\ninformation. This method allows us to attain globally optimal solutions for\nUTIm, given proper initialization. However, the initialization process often\ninvolves randomness, leading to suboptimal, local minimum values. This paper\npresents a novel, combined low-rank approximation (CLRA) method designed to\nmitigate the effects of this random initialization. We introduce three new LRP\nvariants, underpinned by mathematical proof, which allow the UTIm to draw on a\nricher pool of low-rank structural information. Utilizing this augmented\nlow-rank structural information from both LRP and the proposed variants, we\nformulate four linear constraints on the UTIm. Employing the proposed CLRA\nalgorithm, we derive global optimal solutions for the UTIm via these four\nlinear constraints.Experimental results highlight the superior performance of\nour method over existing state-of-the-art approaches, measured in terms of both\nthe recovery number and reduced estimation errors of UTIm.", "published": "2023-07-14 00:05:10", "link": "http://arxiv.org/abs/2307.07096v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Mega-TTS 2: Boosting Prompting Mechanisms for Zero-Shot Speech Synthesis", "abstract": "Zero-shot text-to-speech (TTS) aims to synthesize voices with unseen speech\nprompts, which significantly reduces the data and computation requirements for\nvoice cloning by skipping the fine-tuning process. However, the prompting\nmechanisms of zero-shot TTS still face challenges in the following aspects: 1)\nprevious works of zero-shot TTS are typically trained with single-sentence\nprompts, which significantly restricts their performance when the data is\nrelatively sufficient during the inference stage. 2) The prosodic information\nin prompts is highly coupled with timbre, making it untransferable to each\nother. This paper introduces Mega-TTS 2, a generic prompting mechanism for\nzero-shot TTS, to tackle the aforementioned challenges. Specifically, we design\na powerful acoustic autoencoder that separately encodes the prosody and timbre\ninformation into the compressed latent space while providing high-quality\nreconstructions. Then, we propose a multi-reference timbre encoder and a\nprosody latent language model (P-LLM) to extract useful information from\nmulti-sentence prompts. We further leverage the probabilities derived from\nmultiple P-LLM outputs to produce transferable and controllable prosody.\nExperimental results demonstrate that Mega-TTS 2 could not only synthesize\nidentity-preserving speech with a short prompt of an unseen speaker from\narbitrary sources but consistently outperform the fine-tuning method when the\nvolume of data ranges from 10 seconds to 5 minutes. Furthermore, our method\nenables to transfer various speaking styles to the target timbre in a\nfine-grained and controlled manner. Audio samples can be found in\nhttps://boostprompt.github.io/boostprompt/.", "published": "2023-07-14 08:21:25", "link": "http://arxiv.org/abs/2307.07218v4", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "AudioInceptionNeXt: TCL AI LAB Submission to EPIC-SOUND\n  Audio-Based-Interaction-Recognition Challenge 2023", "abstract": "This report presents the technical details of our submission to the 2023\nEpic-Kitchen EPIC-SOUNDS Audio-Based Interaction Recognition Challenge. The\ntask is to learn the mapping from audio samples to their corresponding action\nlabels. To achieve this goal, we propose a simple yet effective single-stream\nCNN-based architecture called AudioInceptionNeXt that operates on the\ntime-frequency log-mel-spectrogram of the audio samples. Motivated by the\ndesign of the InceptionNeXt, we propose parallel multi-scale depthwise\nseparable convolutional kernels in the AudioInceptionNeXt block, which enable\nthe model to learn the time and frequency information more effectively. The\nlarge-scale separable kernels capture the long duration of activities and the\nglobal frequency semantic information, while the small-scale separable kernels\ncapture the short duration of activities and local details of frequency\ninformation. Our approach achieved 55.43% of top-1 accuracy on the challenge\ntest set, ranked as 1st on the public leaderboard. Codes are available\nanonymously at https://github.com/StevenLauHKHK/AudioInceptionNeXt.git.", "published": "2023-07-14 10:39:05", "link": "http://arxiv.org/abs/2307.07265v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Representation Learning With Hidden Unit Clustering For Low Resource\n  Speech Applications", "abstract": "The representation learning of speech, without textual resources, is an area\nof significant interest for many low resource speech applications. In this\npaper, we describe an approach to self-supervised representation learning from\nraw audio using a hidden unit clustering (HUC) framework. The input to the\nmodel consists of audio samples that are windowed and processed with 1-D\nconvolutional layers. The learned \"time-frequency\" representations from the\nconvolutional neural network (CNN) module are further processed with long short\nterm memory (LSTM) layers which generate a contextual vector representation for\nevery windowed segment. The HUC framework, allowing the categorization of the\nrepresentations into a small number of phoneme-like units, is used to train the\nmodel for learning semantically rich speech representations. The targets\nconsist of phoneme-like pseudo labels for each audio segment and these are\ngenerated with an iterative k-means algorithm. We explore techniques that\nimprove the speaker invariance of the learned representations and illustrate\nthe effectiveness of the proposed approach on two settings, i) completely\nunsupervised speech applications on the sub-tasks described as part of the\nZeroSpeech 2021 challenge and ii) semi-supervised automatic speech recognition\n(ASR) applications on the TIMIT dataset and on the GramVaani challenge Hindi\ndataset. In these experiments, we achieve state-of-art results for various\nZeroSpeech tasks. Further, on the ASR experiments, the HUC representations are\nshown to improve significantly over other established benchmarks based on\nWav2vec, HuBERT and Best-RQ.", "published": "2023-07-14 13:02:10", "link": "http://arxiv.org/abs/2307.07325v1", "categories": ["eess.AS", "cs.AI", "cs.LG"], "primary_category": "eess.AS"}
