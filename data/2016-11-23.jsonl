{"title": "ATR4S: Toolkit with State-of-the-art Automatic Terms Recognition Methods\n  in Scala", "abstract": "Automatically recognized terminology is widely used for various\ndomain-specific texts processing tasks, such as machine translation,\ninformation retrieval or sentiment analysis. However, there is still no\nagreement on which methods are best suited for particular settings and,\nmoreover, there is no reliable comparison of already developed methods. We\nbelieve that one of the main reasons is the lack of state-of-the-art methods\nimplementations, which are usually non-trivial to recreate. In order to address\nthese issues, we present ATR4S, an open-source software written in Scala that\ncomprises more than 15 methods for automatic terminology recognition (ATR) and\nimplements the whole pipeline from text document preprocessing, to term\ncandidates collection, term candidates scoring, and finally, term candidates\nranking. It is highly scalable, modular and configurable tool with support of\nautomatic caching. We also compare 10 state-of-the-art methods on 7 open\ndatasets by average precision and processing time. Experimental comparison\nreveals that no single method demonstrates best average precision for all\ndatasets and that other available tools for ATR do not contain the best\nmethods.", "published": "2016-11-23 14:14:52", "link": "http://arxiv.org/abs/1611.07804v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Emergent Predication Structure in Hidden State Vectors of Neural Readers", "abstract": "A significant number of neural architectures for reading comprehension have\nrecently been developed and evaluated on large cloze-style datasets. We present\nexperiments supporting the emergence of \"predication structure\" in the hidden\nstate vectors of these readers. More specifically, we provide evidence that the\nhidden state vectors represent atomic formulas $\\Phi[c]$ where $\\Phi$ is a\nsemantic property (predicate) and $c$ is a constant symbol entity identifier.", "published": "2016-11-23 19:51:34", "link": "http://arxiv.org/abs/1611.07954v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adaptive Feature Abstraction for Translating Video to Text", "abstract": "Previous models for video captioning often use the output from a specific\nlayer of a Convolutional Neural Network (CNN) as video features. However, the\nvariable context-dependent semantics in the video may make it more appropriate\nto adaptively select features from the multiple CNN layers. We propose a new\napproach for generating adaptive spatiotemporal representations of videos for\nthe captioning task. A novel attention mechanism is developed, that adaptively\nand sequentially focuses on different layers of CNN features (levels of feature\n\"abstraction\"), as well as local spatiotemporal regions of the feature maps at\neach layer. The proposed approach is evaluated on three benchmark datasets:\nYouTube2Text, M-VAD and MSR-VTT. Along with visualizing the results and how the\nmodel works, these experiments quantitatively demonstrate the effectiveness of\nthe proposed adaptive spatiotemporal feature abstraction for translating videos\nto sentences with rich semantics.", "published": "2016-11-23 15:21:48", "link": "http://arxiv.org/abs/1611.07837v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Learning Generic Sentence Representations Using Convolutional Neural\n  Networks", "abstract": "We propose a new encoder-decoder approach to learn distributed sentence\nrepresentations that are applicable to multiple purposes. The model is learned\nby using a convolutional neural network as an encoder to map an input sentence\ninto a continuous vector, and using a long short-term memory recurrent neural\nnetwork as a decoder. Several tasks are considered, including sentence\nreconstruction and future sentence prediction. Further, a hierarchical\nencoder-decoder model is proposed to encode a sentence to predict multiple\nfuture sentences. By training our models on a large collection of novels, we\nobtain a highly generic convolutional sentence encoder that performs well in\npractice. Experimental results on several benchmark datasets, and across a\nbroad range of applications, demonstrate the superiority of the proposed model\nover competing methods.", "published": "2016-11-23 17:32:23", "link": "http://arxiv.org/abs/1611.07897v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Scalable Bayesian Learning of Recurrent Neural Networks for Language\n  Modeling", "abstract": "Recurrent neural networks (RNNs) have shown promising performance for\nlanguage modeling. However, traditional training of RNNs using back-propagation\nthrough time often suffers from overfitting. One reason for this is that\nstochastic optimization (used for large training sets) does not provide good\nestimates of model uncertainty. This paper leverages recent advances in\nstochastic gradient Markov Chain Monte Carlo (also appropriate for large\ntraining sets) to learn weight uncertainty in RNNs. It yields a principled\nBayesian learning algorithm, adding gradient noise during training (enhancing\nexploration of the model-parameter space) and model averaging when testing.\nExtensive experiments on various RNN models and across a broad range of\napplications demonstrate the superiority of the proposed approach over\nstochastic optimization.", "published": "2016-11-23 23:40:50", "link": "http://arxiv.org/abs/1611.08034v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Semantic Compositional Networks for Visual Captioning", "abstract": "A Semantic Compositional Network (SCN) is developed for image captioning, in\nwhich semantic concepts (i.e., tags) are detected from the image, and the\nprobability of each tag is used to compose the parameters in a long short-term\nmemory (LSTM) network. The SCN extends each weight matrix of the LSTM to an\nensemble of tag-dependent weight matrices. The degree to which each member of\nthe ensemble is used to generate an image caption is tied to the\nimage-dependent probability of the corresponding tag. In addition to captioning\nimages, we also extend the SCN to generate captions for video clips. We\nqualitatively analyze semantic composition in SCNs, and quantitatively evaluate\nthe algorithm on three benchmark datasets: COCO, Flickr30k, and Youtube2Text.\nExperimental results show that the proposed method significantly outperforms\nprior state-of-the-art approaches, across multiple evaluation metrics.", "published": "2016-11-23 21:22:22", "link": "http://arxiv.org/abs/1611.08002v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
