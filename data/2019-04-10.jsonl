{"title": "A Variational Approach to Weakly Supervised Document-Level Multi-Aspect\n  Sentiment Classification", "abstract": "In this paper, we propose a variational approach to weakly supervised\ndocument-level multi-aspect sentiment classification. Instead of using\nuser-generated ratings or annotations provided by domain experts, we use\ntarget-opinion word pairs as \"supervision.\" These word pairs can be extracted\nby using dependency parsers and simple rules. Our objective is to predict an\nopinion word given a target word while our ultimate goal is to learn a\nsentiment polarity classifier to predict the sentiment polarity of each aspect\ngiven a document. By introducing a latent variable, i.e., the sentiment\npolarity, to the objective function, we can inject the sentiment polarity\nclassifier to the objective via the variational lower bound. We can learn a\nsentiment polarity classifier by optimizing the lower bound. We show that our\nmethod can outperform weakly supervised baselines on TripAdvisor and\nBeerAdvocate datasets and can be comparable to the state-of-the-art supervised\nmethod with hundreds of labels per aspect.", "published": "2019-04-10 08:24:06", "link": "http://arxiv.org/abs/1904.05055v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NLPR@SRPOL at SemEval-2019 Task 6 and Task 5: Linguistically enhanced\n  deep learning offensive sentence classifier", "abstract": "The paper presents a system developed for the SemEval-2019 competition Task 5\nhat-Eval Basile et al. (2019) (team name: LU Team) and Task 6 OffensEval\nZampieri et al. (2019b) (team name: NLPR@SRPOL), where we achieved 2nd position\nin Subtask C. The system combines in an ensemble several models (LSTM,\nTransformer, OpenAI's GPT, Random forest, SVM) with various embeddings (custom,\nELMo, fastText, Universal Encoder) together with additional linguistic features\n(number of blacklisted words, special characters, etc.). The system works with\na multi-tier blacklist and a large corpus of crawled data, annotated for\ngeneral offensiveness. In the paper we do an extensive analysis of our results\nand show how the combination of features and embedding affect the performance\nof the models.", "published": "2019-04-10 12:56:50", "link": "http://arxiv.org/abs/1904.05152v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Simple BERT Models for Relation Extraction and Semantic Role Labeling", "abstract": "We present simple BERT-based models for relation extraction and semantic role\nlabeling. In recent years, state-of-the-art performance has been achieved using\nneural models by incorporating lexical and syntactic features such as\npart-of-speech tags and dependency trees. In this paper, extensive experiments\non datasets for these two tasks show that without using any external features,\na simple BERT-based model can achieve state-of-the-art performance. To our\nknowledge, we are the first to successfully apply BERT in this manner. Our\nmodels provide strong baselines for future research.", "published": "2019-04-10 15:52:13", "link": "http://arxiv.org/abs/1904.05255v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CNM: An Interpretable Complex-valued Network for Matching", "abstract": "This paper seeks to model human language by the mathematical framework of\nquantum physics. With the well-designed mathematical formulations in quantum\nphysics, this framework unifies different linguistic units in a single\ncomplex-valued vector space, e.g. words as particles in quantum states and\nsentences as mixed systems. A complex-valued network is built to implement this\nframework for semantic matching. With well-constrained complex-valued\ncomponents, the network admits interpretations to explicit physical meanings.\nThe proposed complex-valued network for matching (CNM) achieves comparable\nperformances to strong CNN and RNN baselines on two benchmarking question\nanswering (QA) datasets.", "published": "2019-04-10 16:59:29", "link": "http://arxiv.org/abs/1904.05298v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Harvey Mudd College at SemEval-2019 Task 4: The Clint Buchanan\n  Hyperpartisan News Detector", "abstract": "We investigate the recently developed Bidirectional Encoder Representations\nfrom Transformers (BERT) model for the hyperpartisan news detection task. Using\na subset of hand-labeled articles from SemEval as a validation set, we test the\nperformance of different parameters for BERT models. We find that accuracy from\ntwo different BERT models using different proportions of the articles is\nconsistently high, with our best-performing model on the validation set\nachieving 85% accuracy and the best-performing model on the test set achieving\n77%. We further determined that our model exhibits strong consistency, labeling\nindependent slices of the same article identically. Finally, we find that\nrandomizing the order of word pieces dramatically reduces validation accuracy\n(to approximately 60%), but that shuffling groups of four or more word pieces\nmaintains an accuracy of about 80%, indicating the model mainly gains value\nfrom local context.", "published": "2019-04-10 17:43:51", "link": "http://arxiv.org/abs/1905.01962v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cross-lingual Visual Verb Sense Disambiguation", "abstract": "Recent work has shown that visual context improves cross-lingual sense\ndisambiguation for nouns. We extend this line of work to the more challenging\ntask of cross-lingual verb sense disambiguation, introducing the MultiSense\ndataset of 9,504 images annotated with English, German, and Spanish verbs. Each\nimage in MultiSense is annotated with an English verb and its translation in\nGerman or Spanish. We show that cross-lingual verb sense disambiguation models\nbenefit from visual context, compared to unimodal baselines. We also show that\nthe verb sense predicted by our best disambiguation model can improve the\nresults of a text-only machine translation system when used for a multimodal\ntranslation task.", "published": "2019-04-10 09:43:06", "link": "http://arxiv.org/abs/1904.05092v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Advances in Natural Language Question Answering: A Review", "abstract": "Question Answering has recently received high attention from artificial\nintelligence communities due to the advancements in learning technologies.\nEarly question answering models used rule-based approaches and moved to the\nstatistical approach to address the vastly available information. However,\nstatistical approaches are shown to underperform in handling the dynamic nature\nand the variation of language. Therefore, learning models have shown the\ncapability of handling the dynamic nature and variations in language. Many deep\nlearning methods have been introduced to question answering. Most of the deep\nlearning approaches have shown to achieve higher results compared to machine\nlearning and statistical methods. The dynamic nature of language has profited\nfrom the nonlinear learning in deep learning. This has created prominent\nsuccess and a spike in work on question answering. This paper discusses the\nsuccesses and challenges in question answering question answering systems and\ntechniques that are used in these challenges.", "published": "2019-04-10 16:26:51", "link": "http://arxiv.org/abs/1904.05276v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ClinicalBERT: Modeling Clinical Notes and Predicting Hospital\n  Readmission", "abstract": "Clinical notes contain information about patients that goes beyond structured\ndata like lab values and medications. However, clinical notes have been\nunderused relative to structured data, because notes are high-dimensional and\nsparse. This work develops and evaluates representations of clinical notes\nusing bidirectional transformers (ClinicalBERT). ClinicalBERT uncovers\nhigh-quality relationships between medical concepts as judged by humans.\nClinicalBert outperforms baselines on 30-day hospital readmission prediction\nusing both discharge summaries and the first few days of notes in the intensive\ncare unit. Code and model parameters are available.", "published": "2019-04-10 17:53:13", "link": "http://arxiv.org/abs/1904.05342v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Corpora Generation for Grammatical Error Correction", "abstract": "Grammatical Error Correction (GEC) has been recently modeled using the\nsequence-to-sequence framework. However, unlike sequence transduction problems\nsuch as machine translation, GEC suffers from the lack of plentiful parallel\ndata. We describe two approaches for generating large parallel datasets for GEC\nusing publicly available Wikipedia data. The first method extracts\nsource-target pairs from Wikipedia edit histories with minimal filtration\nheuristics, while the second method introduces noise into Wikipedia sentences\nvia round-trip translation through bridge languages. Both strategies yield\nsimilar sized parallel corpora containing around 4B tokens. We employ an\niterative decoding strategy that is tailored to the loosely supervised nature\nof our constructed corpora. We demonstrate that neural GEC models trained using\neither type of corpora give similar performance. Fine-tuning these models on\nthe Lang-8 corpus and ensembling allows us to surpass the state of the art on\nboth the CoNLL-2014 benchmark and the JFLEG task. We provide systematic\nanalysis that compares the two approaches to data generation and highlights the\neffectiveness of ensembling.", "published": "2019-04-10 05:47:15", "link": "http://arxiv.org/abs/1904.05780v1", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "BAG: Bi-directional Attention Entity Graph Convolutional Network for\n  Multi-hop Reasoning Question Answering", "abstract": "Multi-hop reasoning question answering requires deep comprehension of\nrelationships between various documents and queries. We propose a\nBi-directional Attention Entity Graph Convolutional Network (BAG), leveraging\nrelationships between nodes in an entity graph and attention information\nbetween a query and the entity graph, to solve this task. Graph convolutional\nnetworks are used to obtain a relation-aware representation of nodes for entity\ngraphs built from documents with multi-level features. Bidirectional attention\nis then applied on graphs and queries to generate a query-aware nodes\nrepresentation, which will be used for the final prediction. Experimental\nevaluation shows BAG achieves state-of-the-art accuracy performance on the\nQAngaroo WIKIHOP dataset.", "published": "2019-04-10 01:40:08", "link": "http://arxiv.org/abs/1904.04969v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AMRec: An Intelligent System for Academic Method Recommendation", "abstract": "Finding new academic Methods for research problems is the key task in a\nresearcher's research career. It is usually very difficult for new researchers\nto find good Methods for their research problems since they lack of research\nexperiences. In order to help researchers carry out their researches in a more\nconvenient way, we describe a novel recommendation system called AMRec to\nrecommend new academic Methods for research problems in this paper. Our\nproposed system first extracts academic concepts (Tasks and Methods) and their\nrelations from academic literatures, and then leverages the regularized matrix\nfactorization Method for academic Method recommendation. Preliminary evaluation\nresults verify the effectiveness of our proposed system.", "published": "2019-04-10 03:49:37", "link": "http://arxiv.org/abs/1904.04995v1", "categories": ["cs.IR", "cs.CL", "cs.DL"], "primary_category": "cs.IR"}
{"title": "Better Word Embeddings by Disentangling Contextual n-Gram Information", "abstract": "Pre-trained word vectors are ubiquitous in Natural Language Processing\napplications. In this paper, we show how training word embeddings jointly with\nbigram and even trigram embeddings, results in improved unigram embeddings. We\nclaim that training word embeddings along with higher n-gram embeddings helps\nin the removal of the contextual information from the unigrams, resulting in\nbetter stand-alone word embeddings. We empirically show the validity of our\nhypothesis by outperforming other competing word representation models by a\nsignificant margin on a wide variety of tasks. We make our models publicly\navailable.", "published": "2019-04-10 07:44:06", "link": "http://arxiv.org/abs/1904.05033v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Detecting Cybersecurity Events from Noisy Short Text", "abstract": "It is very critical to analyze messages shared over social networks for cyber\nthreat intelligence and cyber-crime prevention. In this study, we propose a\nmethod that leverages both domain-specific word embeddings and task-specific\nfeatures to detect cyber security events from tweets. Our model employs a\nconvolutional neural network (CNN) and a long short-term memory (LSTM)\nrecurrent neural network which takes word level meta-embeddings as inputs and\nincorporates contextual embeddings to classify noisy short text. We collected a\nnew dataset of cyber security related tweets from Twitter and manually\nannotated a subset of 2K of them. We experimented with this dataset and\nconcluded that the proposed model outperforms both traditional and neural\nbaselines. The results suggest that our method works well for detecting cyber\nsecurity events from noisy short text.", "published": "2019-04-10 08:23:31", "link": "http://arxiv.org/abs/1904.05054v2", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "From Semi-supervised to Almost-unsupervised Speech Recognition with\n  Very-low Resource by Jointly Learning Phonetic Structures from Audio and Text\n  Embeddings", "abstract": "Producing a large amount of annotated speech data for training ASR systems\nremains difficult for more than 95% of languages all over the world which are\nlow-resourced. However, we note human babies start to learn the language by the\nsounds (or phonetic structures) of a small number of exemplar words, and\n\"generalize\" such knowledge to other words without hearing a large amount of\ndata. We initiate some preliminary work in this direction. Audio Word2Vec is\nused to learn the phonetic structures from spoken words (signal segments),\nwhile another autoencoder is used to learn the phonetic structures from text\nwords. The relationships among the above two can be learned jointly, or\nseparately after the above two are well trained. This relationship can be used\nin speech recognition with very low resource. In the initial experiments on the\nTIMIT dataset, only 2.1 hours of speech data (in which 2500 spoken words were\nannotated and the rest unlabeled) gave a word error rate of 44.6%, and this\nnumber can be reduced to 34.2% if 4.1 hr of speech data (in which 20000 spoken\nwords were annotated) were given. These results are not satisfactory, but a\ngood starting point.", "published": "2019-04-10 09:16:24", "link": "http://arxiv.org/abs/1904.05078v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "What's in a Name? Reducing Bias in Bios without Access to Protected\n  Attributes", "abstract": "There is a growing body of work that proposes methods for mitigating bias in\nmachine learning systems. These methods typically rely on access to protected\nattributes such as race, gender, or age. However, this raises two significant\nchallenges: (1) protected attributes may not be available or it may not be\nlegal to use them, and (2) it is often desirable to simultaneously consider\nmultiple protected attributes, as well as their intersections. In the context\nof mitigating bias in occupation classification, we propose a method for\ndiscouraging correlation between the predicted probability of an individual's\ntrue occupation and a word embedding of their name. This method leverages the\nsocietal biases that are encoded in word embeddings, eliminating the need for\naccess to protected attributes. Crucially, it only requires access to\nindividuals' names at training time and not at deployment time. We evaluate two\nvariations of our proposed method using a large-scale dataset of online\nbiographies. We find that both variations simultaneously reduce race and gender\nbiases, with almost no reduction in the classifier's overall true positive\nrate.", "published": "2019-04-10 15:10:37", "link": "http://arxiv.org/abs/1904.05233v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Deep Neural Networks Ensemble for Detecting Medication Mentions in\n  Tweets", "abstract": "Objective: After years of research, Twitter posts are now recognized as an\nimportant source of patient-generated data, providing unique insights into\npopulation health. A fundamental step to incorporating Twitter data in\npharmacoepidemiological research is to automatically recognize medication\nmentions in tweets. Given that lexical searches for medication names may fail\ndue to misspellings or ambiguity with common words, we propose a more advanced\nmethod to recognize them. Methods: We present Kusuri, an Ensemble Learning\nclassifier, able to identify tweets mentioning drug products and dietary\nsupplements. Kusuri (\"medication\" in Japanese) is composed of two modules.\nFirst, four different classifiers (lexicon-based, spelling-variant-based,\npattern-based and one based on a weakly-trained neural network) are applied in\nparallel to discover tweets potentially containing medication names. Second, an\nensemble of deep neural networks encoding morphological, semantical and\nlong-range dependencies of important words in the tweets discovered is used to\nmake the final decision. Results: On a balanced (50-50) corpus of 15,005\ntweets, Kusuri demonstrated performances close to human annotators with 93.7%\nF1-score, the best score achieved thus far on this corpus. On a corpus made of\nall tweets posted by 113 Twitter users (98,959 tweets, with only 0.26%\nmentioning medications), Kusuri obtained 76.3% F1-score. There is not a prior\ndrug extraction system that compares running on such an extremely unbalanced\ndataset. Conclusion: The system identifies tweets mentioning drug names with\nperformance high enough to ensure its usefulness and ready to be integrated in\nlarger natural language processing systems.", "published": "2019-04-10 17:18:17", "link": "http://arxiv.org/abs/1904.05308v2", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Grounded Unsupervised Universal Part-of-Speech Tagger for Low-Resource\n  Languages", "abstract": "Unsupervised part of speech (POS) tagging is often framed as a clustering\nproblem, but practical taggers need to \\textit{ground} their clusters as well.\nGrounding generally requires reference labeled data, a luxury a low-resource\nlanguage might not have. In this work, we describe an approach for low-resource\nunsupervised POS tagging that yields fully grounded output and requires no\nlabeled training data. We find the classic method of Brown et al. (1992)\nclusters well in our use case and employ a decipherment-based approach to\ngrounding. This approach presumes a sequence of cluster IDs is a `ciphertext'\nand seeks a POS tag-to-cluster ID mapping that will reveal the POS sequence. We\nshow intrinsically that, despite the difficulty of the task, we obtain\nreasonable performance across a variety of languages. We also show\nextrinsically that incorporating our POS tagger into a name tagger leads to\nstate-of-the-art tagging performance in Sinhalese and Kinyarwanda, two\nlanguages with nearly no labeled POS data available. We further demonstrate our\ntagger's utility by incorporating it into a true `zero-resource' variant of the\nMalopa (Ammar et al., 2016) dependency parser model that removes the current\nreliance on multilingual resources and gold POS tags for new languages.\nExperiments show that including our tagger makes up much of the accuracy lost\nwhen gold POS tags are unavailable.", "published": "2019-04-10 20:22:31", "link": "http://arxiv.org/abs/1904.05426v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Distributed Deep Learning Strategies For Automatic Speech Recognition", "abstract": "In this paper, we propose and investigate a variety of distributed deep\nlearning strategies for automatic speech recognition (ASR) and evaluate them\nwith a state-of-the-art Long short-term memory (LSTM) acoustic model on the\n2000-hour Switchboard (SWB2000), which is one of the most widely used datasets\nfor ASR performance benchmark. We first investigate what are the proper\nhyper-parameters (e.g., learning rate) to enable the training with sufficiently\nlarge batch size without impairing the model accuracy. We then implement\nvarious distributed strategies, including Synchronous (SYNC), Asynchronous\nDecentralized Parallel SGD (ADPSGD) and the hybrid of the two HYBRID, to study\ntheir runtime/accuracy trade-off. We show that we can train the LSTM model\nusing ADPSGD in 14 hours with 16 NVIDIA P100 GPUs to reach a 7.6% WER on the\nHub5- 2000 Switchboard (SWB) test set and a 13.1% WER on the CallHome (CH) test\nset. Furthermore, we can train the model using HYBRID in 11.5 hours with 32\nNVIDIA V100 GPUs without loss in accuracy.", "published": "2019-04-10 01:00:26", "link": "http://arxiv.org/abs/1904.04956v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Generating Animations from Screenplays", "abstract": "Automatically generating animation from natural language text finds\napplication in a number of areas e.g. movie script writing, instructional\nvideos, and public safety. However, translating natural language text into\nanimation is a challenging task. Existing text-to-animation systems can handle\nonly very simple sentences, which limits their applications. In this paper, we\ndevelop a text-to-animation system which is capable of handling complex\nsentences. We achieve this by introducing a text simplification step into the\nprocess. Building on an existing animation generation system for screenwriting,\nwe create a robust NLP pipeline to extract information from screenplays and map\nthem to the system's knowledge base. We develop a set of linguistic\ntransformation rules that simplify complex sentences. Information extracted\nfrom the simplified sentences is used to generate a rough storyboard and video\ndepicting the text. Our sentence simplification module outperforms existing\nsystems in terms of BLEU and SARI metrics.We further evaluated our system via a\nuser study: 68 % participants believe that our system generates reasonable\nanimation from input screenplays.", "published": "2019-04-10 21:04:54", "link": "http://arxiv.org/abs/1904.05440v1", "categories": ["cs.CL", "cs.AI", "cs.GR", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Expectation-Maximization for Speech Source Separation Using Convolutive\n  Transfer Function", "abstract": "This paper addresses the problem of under-determinded speech source\nseparation from multichannel microphone singals, i.e. the convolutive mixtures\nof multiple sources. The time-domain signals are first transformed to the\nshort-time Fourier transform (STFT) domain. To represent the room filters in\nthe STFT domain, instead of the widely-used narrowband assumption, we propose\nto use a more accurate model, i.e. the convolutive transfer function (CTF). At\neach frequency band, the CTF coefficients of the mixing filters and the STFT\ncoefficients of the sources are jointly estimated by maximizing the likelihood\nof the microphone signals, which is resolved by an Expectation-Maximization\n(EM) algorithm. Experiments show that the proposed method provides very\nsatisfactory performance under highly reverberant environments.", "published": "2019-04-10 15:38:43", "link": "http://arxiv.org/abs/1904.05249v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Autoencoder-Based Articulatory-to-Acoustic Mapping for Ultrasound Silent\n  Speech Interfaces", "abstract": "When using ultrasound video as input, Deep Neural Network-based Silent Speech\nInterfaces usually rely on the whole image to estimate the spectral parameters\nrequired for the speech synthesis step. Although this approach is quite\nstraightforward, and it permits the synthesis of understandable speech, it has\nseveral disadvantages as well. Besides the inability to capture the relations\nbetween close regions (i.e. pixels) of the image, this pixel-by-pixel\nrepresentation of the image is also quite uneconomical. It is easy to see that\na significant part of the image is irrelevant for the spectral parameter\nestimation task as the information stored by the neighbouring pixels is\nredundant, and the neural network is quite large due to the large number of\ninput features. To resolve these issues, in this study we train an autoencoder\nneural network on the ultrasound image; the estimation of the spectral speech\nparameters is done by a second DNN, using the activations of the bottleneck\nlayer of the autoencoder network as features. In our experiments, the proposed\nmethod proved to be more efficient than the standard approach: the measured\nnormalized mean squared error scores were lower, while the correlation values\nwere higher in each case. Based on the result of a listening test, the\nsynthesized utterances also sounded more natural to native speakers. A further\nadvantage of our proposed approach is that, due to the (relatively) small size\nof the bottleneck layer, we can utilize several consecutive ultrasound images\nduring estimation without a significant increase in the network size, while\nsignificantly increasing the accuracy of parameter estimation.", "published": "2019-04-10 15:57:07", "link": "http://arxiv.org/abs/1904.05259v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "An Interactive Musical Prediction System with Mixture Density Recurrent\n  Neural Networks", "abstract": "This paper is about creating digital musical instruments where a predictive\nneural network model is integrated into the interactive system. Rather than\npredicting symbolic music (e.g., MIDI notes), we suggest that predicting future\ncontrol data from the user and precise temporal information can lead to new and\ninteresting interactive possibilities. We propose that a mixture density\nrecurrent neural network (MDRNN) is an appropriate model for this task. The\npredictions can be used to fill-in control data when the user stops performing,\nor as a kind of filter on the user's own input. We present an interactive MDRNN\nprediction server that allows rapid prototyping of new NIMEs featuring\npredictive musical interaction by recording datasets, training MDRNN models,\nand experimenting with interaction modes. We illustrate our system with several\nexample NIMEs applying this idea. Our evaluation shows that real-time\npredictive interaction is viable even on single-board computers and that small\nmodels are appropriate for small datasets.", "published": "2019-04-10 05:50:15", "link": "http://arxiv.org/abs/1904.05009v1", "categories": ["cs.SD", "cs.HC", "cs.NE", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Neuralogram: A Deep Neural Network Based Representation for Audio\n  Signals", "abstract": "We propose the Neuralogram -- a deep neural network based representation for\nunderstanding audio signals which, as the name suggests, transforms an audio\nsignal to a dense, compact representation based upon embeddings learned via a\nneural architecture. Through a series of probing signals, we show how our\nrepresentation can encapsulate pitch, timbre and rhythm-based information, and\nother attributes. This representation suggests a method for revealing\nmeaningful relationships in arbitrarily long audio signals that are not readily\nrepresented by existing algorithms. This has the potential for numerous\napplications in audio understanding, music recommendation, meta-data extraction\nto name a few.", "published": "2019-04-10 09:04:18", "link": "http://arxiv.org/abs/1904.05073v1", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Framework for Multi-f0 Modeling in SATB Choir Recordings", "abstract": "Fundamental frequency (f0) modeling is an important but relatively unexplored\naspect of choir singing. Performance evaluation as well as auditory analysis of\nsinging, whether individually or in a choir, often depend on extracting f0\ncontours for the singing voice. However, due to the large number of singers,\nsinging at a similar frequency range, extracting the exact individual pitch\ncontours from choir recordings is a challenging task. In this paper, we address\nthis task and develop a methodology for modeling pitch contours of SATB choir\nrecordings. A typical SATB choir consists of four parts, each covering a\ndistinct range of pitches and often with multiple singers each. We first\nevaluate some state-of-the-art multi-f0 estimation systems for the particular\ncase of choirs with a single singer per part, and observe that the pitch of\nindividual singers can be estimated to a relatively high degree of accuracy. We\nobserve, however, that the scenario of multiple singers for each choir part\n(i.e. unison singing) is far more challenging. In this work we propose a\nmethodology based on combining a multi-f0 estimation methodology based on deep\nlearning followed by a set of traditional DSP techniques to model f0 and its\ndispersion instead of a single f0 trajectory for each choir part. We present\nand discuss our observations and test our framework with different singer\nconfigurations.", "published": "2019-04-10 09:35:50", "link": "http://arxiv.org/abs/1904.05086v1", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio-noise Power Spectral Density Estimation Using Long Short-term\n  Memory", "abstract": "We propose a method using a long short-term memory (LSTM) network to estimate\nthe noise power spectral density (PSD) of single-channel audio signals\nrepresented in the short time Fourier transform (STFT) domain. An LSTM network\ncommon to all frequency bands is trained, which processes each frequency band\nindividually by mapping the noisy STFT magnitude sequence to its corresponding\nnoise PSD sequence. Unlike deep-learning-based speech enhancement methods that\nlearn the full-band spectral structure of speech segments, the proposed method\nexploits the sub-band STFT magnitude evolution of noise with a long time\ndependency, in the spirit of the unsupervised noise estimators described in the\nliterature. Speaker- and speech-independent experiments with different types of\nnoise show that the proposed method outperforms the unsupervised estimators,\nand generalizes well to noise types that are not present in the training set.", "published": "2019-04-10 13:14:11", "link": "http://arxiv.org/abs/1904.05166v1", "categories": ["eess.SP", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
{"title": "Acoustic Scene Classification by Implicitly Identifying Distinct Sound\n  Events", "abstract": "In this paper, we propose a new strategy for acoustic scene classification\n(ASC) , namely recognizing acoustic scenes through identifying distinct sound\nevents. This differs from existing strategies, which focus on characterizing\nglobal acoustical distributions of audio or the temporal evolution of\nshort-term audio features, without analysis down to the level of sound events.\nTo identify distinct sound events for each scene, we formulate ASC in a\nmulti-instance learning (MIL) framework, where each audio recording is mapped\ninto a bag-of-instances representation. Here, instances can be seen as\nhigh-level representations for sound events inside a scene. We also propose a\nMIL neural networks model, which implicitly identifies distinct instances\n(i.e., sound events). Furthermore, we propose two specially designed modules\nthat model the multi-temporal scale and multi-modal natures of the sound events\nrespectively. The experiments were conducted on the official development set of\nthe DCASE2018 Task1 Subtask B, and our best-performing model improves over the\nofficial baseline by 9.4% (68.3% vs 58.9%) in terms of classification accuracy.\nThis study indicates that recognizing acoustic scenes by identifying distinct\nsound events is effective and paves the way for future studies that combine\nthis strategy with previous ones.", "published": "2019-04-10 14:19:34", "link": "http://arxiv.org/abs/1904.05204v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Compact and Discriminative Feature Based on Auditory Summary\n  Statistics for Acoustic Scene Classification", "abstract": "One of the biggest challenges of acoustic scene classification (ASC) is to\nfind proper features to better represent and characterize environmental sounds.\nEnvironmental sounds generally involve more sound sources while exhibiting less\nstructure in temporal spectral representations. However, the background of an\nacoustic scene exhibits temporal homogeneity in acoustic properties, suggesting\nit could be characterized by distribution statistics rather than temporal\ndetails. In this work, we investigated using auditory summary statistics as the\nfeature for ASC tasks. The inspiration comes from a recent neuroscience study,\nwhich shows the human auditory system tends to perceive sound textures through\ntime-averaged statistics. Based on these statistics, we further proposed to use\nlinear discriminant analysis to eliminate redundancies among these statistics\nwhile keeping the discriminative information, providing an extreme com-pact\nrepresentation for acoustic scenes. Experimental results show the outstanding\nperformance of the proposed feature over the conventional handcrafted features.", "published": "2019-04-10 15:31:09", "link": "http://arxiv.org/abs/1904.05243v1", "categories": ["cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "RawNet: Fast End-to-End Neural Vocoder", "abstract": "Neural network-based vocoders have recently demonstrated the powerful ability\nto synthesize high-quality speech. These models usually generate samples by\nconditioning on spectral features, such as Mel-spectrogram and fundamental\nfrequency, which is crucial to speech synthesis. However, the feature\nextraction procession tends to depend heavily on human knowledge resulting in a\nless expressive description of the origin audio. In this work, we proposed\nRawNet, a complete end-to-end neural vocoder following the auto-encoder\nstructure for speaker-dependent and -independent speech synthesis. It\nautomatically learns to extract features and recover audio using neural\nnetworks, which include a coder network to capture a higher representation of\nthe input audio and an autoregressive voder network to restore the audio in a\nsample-by-sample manner. The coder and voder are jointly trained directly on\nthe raw waveform without any human-designed features. The experimental results\nshow that RawNet achieves a better speech quality using a simplified model\narchitecture and obtains a faster speech generation speed at the inference\nstage.", "published": "2019-04-10 10:25:25", "link": "http://arxiv.org/abs/1904.05351v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "One-shot Voice Conversion by Separating Speaker and Content\n  Representations with Instance Normalization", "abstract": "Recently, voice conversion (VC) without parallel data has been successfully\nadapted to multi-target scenario in which a single model is trained to convert\nthe input voice to many different speakers. However, such model suffers from\nthe limitation that it can only convert the voice to the speakers in the\ntraining data, which narrows down the applicable scenario of VC. In this paper,\nwe proposed a novel one-shot VC approach which is able to perform VC by only an\nexample utterance from source and target speaker respectively, and the source\nand target speaker do not even need to be seen during training. This is\nachieved by disentangling speaker and content representations with instance\nnormalization (IN). Objective and subjective evaluation shows that our model is\nable to generate the voice similar to target speaker. In addition to the\nperformance measurement, we also demonstrate that this model is able to learn\nmeaningful speaker representations without any supervision.", "published": "2019-04-10 16:22:18", "link": "http://arxiv.org/abs/1904.05742v4", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
