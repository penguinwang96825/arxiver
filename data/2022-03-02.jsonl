{"title": "Do Prompts Solve NLP Tasks Using Natural Language?", "abstract": "Thanks to the advanced improvement of large pre-trained language models,\nprompt-based fine-tuning is shown to be effective on a variety of downstream\ntasks. Though many prompting methods have been investigated, it remains unknown\nwhich type of prompts are the most effective among three types of prompts\n(i.e., human-designed prompts, schema prompts and null prompts). In this work,\nwe empirically compare the three types of prompts under both few-shot and\nfully-supervised settings. Our experimental results show that schema prompts\nare the most effective in general. Besides, the performance gaps tend to\ndiminish when the scale of training data grows large.", "published": "2022-03-02 07:20:59", "link": "http://arxiv.org/abs/2203.00902v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Past Mistake is the Future Wisdom: Error-driven Contrastive\n  Probability Optimization for Chinese Spell Checking", "abstract": "Chinese Spell Checking (CSC) aims to detect and correct Chinese spelling\nerrors, which are mainly caused by the phonological or visual similarity.\nRecently, pre-trained language models (PLMs) promote the progress of CSC task.\nHowever, there exists a gap between the learned knowledge of PLMs and the goal\nof CSC task. PLMs focus on the semantics in text and tend to correct the\nerroneous characters to semantically proper or commonly used ones, but these\naren't the ground-truth corrections. To address this issue, we propose an\nError-driven COntrastive Probability Optimization (ECOPO) framework for CSC\ntask. ECOPO refines the knowledge representations of PLMs, and guides the model\nto avoid predicting these common characters through an error-driven way.\nParticularly, ECOPO is model-agnostic and it can be combined with existing CSC\nmethods to achieve better performance. Extensive experiments and detailed\nanalyses on SIGHAN datasets demonstrate that ECOPO is simple yet effective.", "published": "2022-03-02 09:58:56", "link": "http://arxiv.org/abs/2203.00991v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey on Aspect-Based Sentiment Analysis: Tasks, Methods, and\n  Challenges", "abstract": "As an important fine-grained sentiment analysis problem, aspect-based\nsentiment analysis (ABSA), aiming to analyze and understand people's opinions\nat the aspect level, has been attracting considerable interest in the last\ndecade. To handle ABSA in different scenarios, various tasks are introduced for\nanalyzing different sentiment elements and their relations, including the\naspect term, aspect category, opinion term, and sentiment polarity. Unlike\nearly ABSA works focusing on a single sentiment element, many compound ABSA\ntasks involving multiple elements have been studied in recent years for\ncapturing more complete aspect-level sentiment information. However, a\nsystematic review of various ABSA tasks and their corresponding solutions is\nstill lacking, which we aim to fill in this survey. More specifically, we\nprovide a new taxonomy for ABSA which organizes existing studies from the axes\nof concerned sentiment elements, with an emphasis on recent advances of\ncompound ABSA tasks. From the perspective of solutions, we summarize the\nutilization of pre-trained language models for ABSA, which improved the\nperformance of ABSA to a new stage. Besides, techniques for building more\npractical ABSA systems in cross-domain/lingual scenarios are discussed.\nFinally, we review some emerging topics and discuss some open challenges to\noutlook potential future directions of ABSA.", "published": "2022-03-02 12:01:46", "link": "http://arxiv.org/abs/2203.01054v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mukayese: Turkish NLP Strikes Back", "abstract": "Having sufficient resources for language X lifts it from the under-resourced\nlanguages class, but not necessarily from the under-researched class. In this\npaper, we address the problem of the absence of organized benchmarks in the\nTurkish language. We demonstrate that languages such as Turkish are left behind\nthe state-of-the-art in NLP applications. As a solution, we present Mukayese, a\nset of NLP benchmarks for the Turkish language that contains several NLP tasks.\nWe work on one or more datasets for each benchmark and present two or more\nbaselines. Moreover, we present four new benchmarking datasets in Turkish for\nlanguage modeling, sentence segmentation, and spell checking. All datasets and\nbaselines are available under: https://github.com/alisafaya/mukayese", "published": "2022-03-02 16:18:44", "link": "http://arxiv.org/abs/2203.01215v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "py-irt: A Scalable Item Response Theory Library for Python", "abstract": "py-irt is a Python library for fitting Bayesian Item Response Theory (IRT)\nmodels. py-irt estimates latent traits of subjects and items, making it\nappropriate for use in IRT tasks as well as ideal-point models. py-irt is built\non top of the Pyro and PyTorch frameworks and uses GPU-accelerated training to\nscale to large data sets. Code, documentation, and examples can be found at\nhttps://github.com/nd-ball/py-irt. py-irt can be installed from the GitHub page\nor the Python Package Index (PyPI).", "published": "2022-03-02 18:09:46", "link": "http://arxiv.org/abs/2203.01282v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TSAM: A Two-Stream Attention Model for Causal Emotion Entailment", "abstract": "Causal Emotion Entailment (CEE) aims to discover the potential causes behind\nan emotion in a conversational utterance. Previous works formalize CEE as\nindependent utterance pair classification problems, with emotion and speaker\ninformation neglected. From a new perspective, this paper considers CEE in a\njoint framework. We classify multiple utterances synchronously to capture the\ncorrelations between utterances in a global view and propose a Two-Stream\nAttention Model (TSAM) to effectively model the speaker's emotional influences\nin the conversational history. Specifically, the TSAM comprises three modules:\nEmotion Attention Network (EAN), Speaker Attention Network (SAN), and\ninteraction module. The EAN and SAN incorporate emotion and speaker information\nin parallel, and the subsequent interaction module effectively interchanges\nrelevant information between the EAN and SAN via a mutual BiAffine\ntransformation. Extensive experimental results demonstrate that our model\nachieves new State-Of-The-Art (SOTA) performance and outperforms baselines\nremarkably.", "published": "2022-03-02 02:11:41", "link": "http://arxiv.org/abs/2203.00819v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SelfKG: Self-Supervised Entity Alignment in Knowledge Graphs", "abstract": "Entity alignment, aiming to identify equivalent entities across different\nknowledge graphs (KGs), is a fundamental problem for constructing Web-scale\nKGs. Over the course of its development, the label supervision has been\nconsidered necessary for accurate alignments. Inspired by the recent progress\nof self-supervised learning, we explore the extent to which we can get rid of\nsupervision for entity alignment. Commonly, the label information (positive\nentity pairs) is used to supervise the process of pulling the aligned entities\nin each positive pair closer. However, our theoretical analysis suggests that\nthe learning of entity alignment can actually benefit more from pushing\nunlabeled negative pairs far away from each other than pulling labeled positive\npairs close. By leveraging this discovery, we develop the self-supervised\nlearning objective for entity alignment. We present SelfKG with efficient\nstrategies to optimize this objective for aligning entities without label\nsupervision. Extensive experiments on benchmark datasets demonstrate that\nSelfKG without supervision can match or achieve comparable results with\nstate-of-the-art supervised baselines. The performance of SelfKG suggests that\nself-supervised learning offers great potential for entity alignment in KGs.\nThe code and data are available at https://github.com/THUDM/SelfKG.", "published": "2022-03-02 11:40:37", "link": "http://arxiv.org/abs/2203.01044v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Discontinuous Constituency and BERT: A Case Study of Dutch", "abstract": "In this paper, we set out to quantify the syntactic capacity of BERT in the\nevaluation regime of non-context free patterns, as occurring in Dutch. We\ndevise a test suite based on a mildly context-sensitive formalism, from which\nwe derive grammars that capture the linguistic phenomena of control verb\nnesting and verb raising. The grammars, paired with a small lexicon, provide us\nwith a large collection of naturalistic utterances, annotated with verb-subject\npairings, that serve as the evaluation test bed for an attention-based span\nselection probe. Our results, backed by extensive analysis, suggest that the\nmodels investigated fail in the implicit acquisition of the dependencies\nexamined.", "published": "2022-03-02 12:30:21", "link": "http://arxiv.org/abs/2203.01063v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Large-Scale Hate Speech Detection with Cross-Domain Transfer", "abstract": "The performance of hate speech detection models relies on the datasets on\nwhich the models are trained. Existing datasets are mostly prepared with a\nlimited number of instances or hate domains that define hate topics. This\nhinders large-scale analysis and transfer learning with respect to hate\ndomains. In this study, we construct large-scale tweet datasets for hate speech\ndetection in English and a low-resource language, Turkish, consisting of\nhuman-labeled 100k tweets per each. Our datasets are designed to have equal\nnumber of tweets distributed over five domains. The experimental results\nsupported by statistical tests show that Transformer-based language models\noutperform conventional bag-of-words and neural models by at least 5% in\nEnglish and 10% in Turkish for large-scale hate speech detection. The\nperformance is also scalable to different training sizes, such that 98% of\nperformance in English, and 97% in Turkish, are recovered when 20% of training\ninstances are used. We further examine the generalization ability of\ncross-domain transfer among hate domains. We show that 96% of the performance\nof a target domain in average is recovered by other domains for English, and\n92% for Turkish. Gender and religion are more successful to generalize to other\ndomains, while sports fail most.", "published": "2022-03-02 14:02:54", "link": "http://arxiv.org/abs/2203.01111v2", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Towards Contextual Spelling Correction for Customization of End-to-end\n  Speech Recognition Systems", "abstract": "Contextual biasing is an important and challenging task for end-to-end\nautomatic speech recognition (ASR) systems, which aims to achieve better\nrecognition performance by biasing the ASR system to particular context phrases\nsuch as person names, music list, proper nouns, etc. Existing methods mainly\ninclude contextual LM biasing and adding bias encoder into end-to-end ASR\nmodels. In this work, we introduce a novel approach to do contextual biasing by\nadding a contextual spelling correction model on top of the end-to-end ASR\nsystem. We incorporate contextual information into a sequence-to-sequence\nspelling correction model with a shared context encoder. Our proposed model\nincludes two different mechanisms: autoregressive (AR) and non-autoregressive\n(NAR). We propose filtering algorithms to handle large-size context lists, and\nperformance balancing mechanisms to control the biasing degree of the model. We\ndemonstrate the proposed model is a general biasing solution which is\ndomain-insensitive and can be adopted in different scenarios. Experiments show\nthat the proposed method achieves as much as 51% relative word error rate (WER)\nreduction over ASR system and outperforms traditional biasing methods. Compared\nto the AR solution, the proposed NAR model reduces model size by 43.2% and\nspeeds up inference by 2.1 times.", "published": "2022-03-02 06:00:48", "link": "http://arxiv.org/abs/2203.00888v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Parameter-Efficient Mixture-of-Experts Architecture for Pre-trained\n  Language Models", "abstract": "Recently, Mixture-of-Experts (short as MoE) architecture has achieved\nremarkable success in increasing the model capacity of large-scale language\nmodels. However, MoE requires incorporating significantly more parameters than\nthe base model being extended. In this paper, we propose building a\nparameter-efficient MoE architecture by sharing information among experts. We\nadopt the matrix product operator (MPO, a tensor decomposition from quantum\nmany-body physics) to reconstruct the parameter matrix in the expert layer and\nincrease model capacity for pre-trained language models by sharing parameters\nof the central tensor (containing the core information) among different experts\nwhile enabling the specificity through the auxiliary tensors (complementing the\ncentral tensor) of different experts. To address the unbalanced optimization\nissue, we further design the gradient mask strategy for the MPO-based MoE\narchitecture. Extensive experiments based on T5 and GPT-2 show improved\nperformance and efficiency of the pre-trained language model (27.2x reduction\nin total parameters for the superior model performance, compared with the\nSwitch Transformers). Our code is publicly available at\nhttps://github.com/RUCAIBox/MPOE.", "published": "2022-03-02 13:44:49", "link": "http://arxiv.org/abs/2203.01104v4", "categories": ["cs.CL", "cs.AI", "cs.LG", "quant-ph"], "primary_category": "cs.CL"}
{"title": "Providing Insights for Open-Response Surveys via End-to-End\n  Context-Aware Clustering", "abstract": "Teachers often conduct surveys in order to collect data from a predefined\ngroup of students to gain insights into topics of interest. When analyzing\nsurveys with open-ended textual responses, it is extremely time-consuming,\nlabor-intensive, and difficult to manually process all the responses into an\ninsightful and comprehensive report. In the analysis step, traditionally, the\nteacher has to read each of the responses and decide on how to group them in\norder to extract insightful information. Even though it is possible to group\nthe responses only using certain keywords, such an approach would be limited\nsince it not only fails to account for embedded contexts but also cannot detect\npolysemous words or phrases and semantics that are not expressible in single\nwords. In this work, we present a novel end-to-end context-aware framework that\nextracts, aggregates, and abbreviates embedded semantic patterns in\nopen-response survey data. Our framework relies on a pre-trained natural\nlanguage model in order to encode the textual data into semantic vectors. The\nencoded vectors then get clustered either into an optimally tuned number of\ngroups or into a set of groups with pre-specified titles. In the former case,\nthe clusters are then further analyzed to extract a representative set of\nkeywords or summary sentences that serve as the labels of the clusters. In our\nframework, for the designated clusters, we finally provide context-aware\nwordclouds that demonstrate the semantically prominent keywords within each\ngroup. Honoring user privacy, we have successfully built the on-device\nimplementation of our framework suitable for real-time analysis on mobile\ndevices and have tested it on a synthetic dataset. Our framework reduces the\ncosts at-scale by automating the process of extracting the most insightful\ninformation pieces from survey data.", "published": "2022-03-02 18:24:10", "link": "http://arxiv.org/abs/2203.01294v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Recent, rapid advancement in visual question answering architecture: a\n  review", "abstract": "Understanding visual question answering is going to be crucial for numerous\nhuman activities. However, it presents major challenges at the heart of the\nartificial intelligence endeavor. This paper presents an update on the rapid\nadvancements in visual question answering using images that have occurred in\nthe last couple of years. Tremendous growth in research on improving visual\nquestion answering system architecture has been published recently, showing the\nimportance of multimodal architectures. Several points on the benefits of\nvisual question answering are mentioned in the review paper by Manmadhan et al.\n(2020), on which the present article builds, including subsequent updates in\nthe field.", "published": "2022-03-02 03:39:53", "link": "http://arxiv.org/abs/2203.01322v4", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "LILE: Look In-Depth before Looking Elsewhere -- A Dual Attention Network\n  using Transformers for Cross-Modal Information Retrieval in Histopathology\n  Archives", "abstract": "The volume of available data has grown dramatically in recent years in many\napplications. Furthermore, the age of networks that used multiple modalities\nseparately has practically ended. Therefore, enabling bidirectional\ncross-modality data retrieval capable of processing has become a requirement\nfor many domains and disciplines of research. This is especially true in the\nmedical field, as data comes in a multitude of types, including various types\nof images and reports as well as molecular data. Most contemporary works apply\ncross attention to highlight the essential elements of an image or text in\nrelation to the other modalities and try to match them together. However,\nregardless of their importance in their own modality, these approaches usually\nconsider features of each modality equally. In this study, self-attention as an\nadditional loss term will be proposed to enrich the internal representation\nprovided into the cross attention module. This work suggests a novel\narchitecture with a new loss term to help represent images and texts in the\njoint latent space. Experiment results on two benchmark datasets, i.e. MS-COCO\nand ARCH, show the effectiveness of the proposed method.", "published": "2022-03-02 22:42:20", "link": "http://arxiv.org/abs/2203.01445v2", "categories": ["cs.CV", "cs.CL", "cs.LG", "eess.IV"], "primary_category": "cs.CV"}
{"title": "Adversarial Robustness of Neural-Statistical Features in Detection of\n  Generative Transformers", "abstract": "The detection of computer-generated text is an area of rapidly increasing\nsignificance as nascent generative models allow for efficient creation of\ncompelling human-like text, which may be abused for the purposes of spam,\ndisinformation, phishing, or online influence campaigns. Past work has studied\ndetection of current state-of-the-art models, but despite a developing threat\nlandscape, there has been minimal analysis of the robustness of detection\nmethods to adversarial attacks. To this end, we evaluate neural and non-neural\napproaches on their ability to detect computer-generated text, their robustness\nagainst text adversarial attacks, and the impact that successful adversarial\nattacks have on human judgement of text quality. We find that while statistical\nfeatures underperform neural features, statistical features provide additional\nadversarial robustness that can be leveraged in ensemble detection models. In\nthe process, we find that previously effective complex phrasal features for\ndetection of computer-generated text hold little predictive power against\ncontemporary generative models, and identify promising statistical features to\nuse instead. Finally, we pioneer the usage of $\\Delta$MAUVE as a proxy measure\nfor human judgement of adversarial text quality.", "published": "2022-03-02 16:46:39", "link": "http://arxiv.org/abs/2203.07983v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "High-Modality Multimodal Transformer: Quantifying Modality & Interaction\n  Heterogeneity for High-Modality Representation Learning", "abstract": "Many real-world problems are inherently multimodal, from spoken language,\ngestures, and paralinguistics humans use to communicate, to force,\nproprioception, and visual sensors on robots. While there has been an explosion\nof interest in multimodal learning, these methods are focused on a small set of\nmodalities primarily in language, vision, and audio. In order to accelerate\ngeneralization towards diverse and understudied modalities, this paper studies\nefficient representation learning for high-modality scenarios involving a large\nset of diverse modalities. Since adding new models for every new modality\nbecomes prohibitively expensive, a critical technical challenge is\nheterogeneity quantification: how can we measure which modalities encode\nsimilar information and interactions in order to permit parameter sharing with\nprevious modalities? This paper proposes two new information theoretic metrics\nfor heterogeneity quantification: (1) modality heterogeneity studies how\nsimilar 2 modalities {X1,X2} are by measuring how much information can be\ntransferred from X1 to X2, while (2) interaction heterogeneity studies how\nsimilarly pairs of modalities {X1,X2}, {X3,X4} interact by measuring how much\ninformation can be transferred from fusing {X1,X2} to {X3,X4}. We show the\nimportance of these 2 proposed metrics as a way to automatically prioritize the\nfusion of modalities that contain unique information or interactions. The\nresult is a single model, HighMMT, that scales up to 10 modalities (text,\nimage, audio, video, sensors, proprioception, speech, time-series, sets, and\ntables) and 15 tasks from 5 research areas. Not only does HighMMT outperform\nprior methods on the tradeoff between performance and efficiency, it also\ndemonstrates a crucial scaling behavior: performance continues to improve with\neach modality added, and it transfers to entirely new modalities and tasks\nduring fine-tuning.", "published": "2022-03-02 18:56:20", "link": "http://arxiv.org/abs/2203.01311v4", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.MM"], "primary_category": "cs.LG"}
{"title": "MuSE-SVS: Multi-Singer Emotional Singing Voice Synthesizer that Controls\n  Emotional Intensity", "abstract": "We propose a multi-singer emotional singing voice synthesizer, Muse-SVS, that\nexpresses emotion at various intensity levels by controlling subtle changes in\npitch, energy, and phoneme duration while accurately following the score. To\ncontrol multiple style attributes while avoiding loss of fidelity and\nexpressiveness due to interference between attributes, Muse-SVS represents all\nattributes and their relations together by a joint embedding in a unified\nembedding space. Muse-SVS can express emotional intensity levels not included\nin the training data through embedding interpolation and extrapolation. We also\npropose a statistical pitch predictor to express pitch variance according to\nemotional intensity, and a context-aware residual duration predictor to prevent\nthe accumulation of variances in phoneme duration, which is crucial for\nsynchronization with instrumental parts. In addition, we propose a novel\nASPP-Transformer, which combines atrous spatial pyramid pooling (ASPP) and\nTransformer, to improve fidelity and expressiveness by referring to broad\ncontexts. In experiments, Muse-SVS exhibited improved fidelity, expressiveness,\nand synchronization performance compared with baseline models. The\nvisualization results show that Muse-SVS effectively express the variance in\npitch, energy, and phoneme duration according to emotional intensity. To the\nbest of our knowledge, Muse-SVS is the first neural SVS capable of controlling\nemotional intensity.", "published": "2022-03-02 08:26:50", "link": "http://arxiv.org/abs/2203.00931v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Multi-Scale Time-Frequency Spectrogram Discriminator for GAN-based\n  Non-Autoregressive TTS", "abstract": "The generative adversarial network (GAN) has shown its outstanding capability\nin improving Non-Autoregressive TTS (NAR-TTS) by adversarially training it with\nan extra model that discriminates between the real and the generated speech. To\nmaximize the benefits of GAN, it is crucial to find a powerful discriminator\nthat can capture rich distinguishable information. In this paper, we propose a\nmulti-scale time-frequency spectrogram discriminator to help NAR-TTS generate\nhigh-fidelity Mel-spectrograms. It treats the spectrogram as a 2D image to\nexploit the correlation among different components in the time-frequency\ndomain. And a U-Net-based model structure is employed to discriminate at\ndifferent scales to capture both coarse-grained and fine-grained information.\nWe conduct subjective tests to evaluate the proposed approach. Both multi-scale\nand time-frequency discriminating bring significant improvement in the\nnaturalness and fidelity. When combining the neural vocoder, it is shown more\neffective and concise than fine-tuning the vocoder. Finally, we visualize the\ndiscriminating maps to compare their difference to verify the effectiveness of\nmulti-scale discriminating.", "published": "2022-03-02 13:10:08", "link": "http://arxiv.org/abs/2203.01080v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speaker Adaption with Intuitive Prosodic Features for Statistical\n  Parametric Speech Synthesis", "abstract": "In this paper, we propose a method of speaker adaption with intuitive\nprosodic features for statistical parametric speech synthesis. The intuitive\nprosodic features employed in this method include pitch, pitch range, speech\nrate and energy considering that they are directly related with the overall\nprosodic characteristics of different speakers. The intuitive prosodic features\nare extracted at utterance-level or speaker-level, and are further integrated\ninto the existing speaker-encoding-based and speaker-embedding-based adaptation\nframeworks respectively. The acoustic models are sequence-to-sequence ones\nbased on Tacotron2. Intuitive prosodic features are concatenated with text\nencoder outputs and speaker vectors for decoding acoustic features.Experimental\nresults have demonstrated that our proposed methods can achieve better\nobjective and subjective performance than the baseline methods without\nintuitive prosodic features. Besides, the proposed speaker adaption method with\nutterance-level prosodic features has achieved the best similarity of synthetic\nspeech among all compared methods.", "published": "2022-03-02 09:00:31", "link": "http://arxiv.org/abs/2203.00951v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio Self-supervised Learning: A Survey", "abstract": "Inspired by the humans' cognitive ability to generalise knowledge and skills,\nSelf-Supervised Learning (SSL) targets at discovering general representations\nfrom large-scale data without requiring human annotations, which is an\nexpensive and time consuming task. Its success in the fields of computer vision\nand natural language processing have prompted its recent adoption into the\nfield of audio and speech processing. Comprehensive reviews summarising the\nknowledge in audio SSL are currently missing. To fill this gap, in the present\nwork, we provide an overview of the SSL methods used for audio and speech\nprocessing applications. Herein, we also summarise the empirical works that\nexploit the audio modality in multi-modal SSL frameworks, and the existing\nsuitable benchmarks to evaluate the power of SSL in the computer audition\ndomain. Finally, we discuss some open problems and point out the future\ndirections on the development of audio SSL.", "published": "2022-03-02 15:58:29", "link": "http://arxiv.org/abs/2203.01205v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
