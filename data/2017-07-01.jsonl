{"title": "Synthetic Data for Neural Machine Translation of Spoken-Dialects", "abstract": "In this paper, we introduce a novel approach to generate synthetic data for\ntraining Neural Machine Translation systems. The proposed approach transforms a\ngiven parallel corpus between a written language and a target language to a\nparallel corpus between a spoken dialect variant and the target language. Our\napproach is language independent and can be used to generate data for any\nvariant of the source language such as slang or spoken dialect or even for a\ndifferent language that is closely related to the source language.\n  The proposed approach is based on local embedding projection of distributed\nrepresentations which utilizes monolingual embeddings to transform parallel\ndata across language variants. We report experimental results on Levantine to\nEnglish translation using Neural Machine Translation. We show that the\ngenerated data can improve a very large scale system by more than 2.8 Bleu\npoints using synthetic spoken data which shows that it can be used to provide a\nreliable translation system for a spoken dialect that does not have sufficient\nparallel data.", "published": "2017-07-01 01:21:22", "link": "http://arxiv.org/abs/1707.00079v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Attention using a Fixed-Size Memory Representation", "abstract": "The standard content-based attention mechanism typically used in\nsequence-to-sequence models is computationally expensive as it requires the\ncomparison of large encoder and decoder states at each time step. In this work,\nwe propose an alternative attention mechanism based on a fixed size memory\nrepresentation that is more efficient. Our technique predicts a compact set of\nK attention contexts during encoding and lets the decoder compute an efficient\nlookup that does not need to consult the memory. We show that our approach\nperforms on-par with the standard attention mechanism while yielding inference\nspeedups of 20% for real-world translation tasks and more for tasks with longer\nsequences. By visualizing attention scores we demonstrate that our models learn\ndistinct, meaningful alignments.", "published": "2017-07-01 08:16:24", "link": "http://arxiv.org/abs/1707.00110v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Heterogeneous Supervision for Relation Extraction: A Representation\n  Learning Approach", "abstract": "Relation extraction is a fundamental task in information extraction. Most\nexisting methods have heavy reliance on annotations labeled by human experts,\nwhich are costly and time-consuming. To overcome this drawback, we propose a\nnovel framework, REHession, to conduct relation extractor learning using\nannotations from heterogeneous information source, e.g., knowledge base and\ndomain heuristics. These annotations, referred as heterogeneous supervision,\noften conflict with each other, which brings a new challenge to the original\nrelation extraction task: how to infer the true label from noisy labels for a\ngiven instance. Identifying context information as the backbone of both\nrelation extraction and true label discovery, we adopt embedding techniques to\nlearn the distributed representations of context, which bridges all components\nwith mutual enhancement in an iterative fashion. Extensive experimental results\ndemonstrate the superiority of REHession over the state-of-the-art.", "published": "2017-07-01 15:23:23", "link": "http://arxiv.org/abs/1707.00166v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Content-Based Weak Supervision for Ad-Hoc Re-Ranking", "abstract": "One challenge with neural ranking is the need for a large amount of\nmanually-labeled relevance judgments for training. In contrast with prior work,\nwe examine the use of weak supervision sources for training that yield pseudo\nquery-document pairs that already exhibit relevance (e.g., newswire\nheadline-content pairs and encyclopedic heading-paragraph pairs). We also\npropose filtering techniques to eliminate training samples that are too far out\nof domain using two techniques: a heuristic-based approach and novel supervised\nfilter that re-purposes a neural ranker. Using several leading neural ranking\narchitectures and multiple weak supervision datasets, we show that these\nsources of training pairs are effective on their own (outperforming prior weak\nsupervision techniques), and that filtering can further improve performance.", "published": "2017-07-01 18:42:29", "link": "http://arxiv.org/abs/1707.00189v3", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Rank-1 Constrained Multichannel Wiener Filter for Speech Recognition in\n  Noisy Environments", "abstract": "Multichannel linear filters, such as the Multichannel Wiener Filter (MWF) and\nthe Generalized Eigenvalue (GEV) beamformer are popular signal processing\ntechniques which can improve speech recognition performance. In this paper, we\npresent an experimental study on these linear filters in a specific speech\nrecognition task, namely the CHiME-4 challenge, which features real recordings\nin multiple noisy environments. Specifically, the rank-1 MWF is employed for\nnoise reduction and a new constant residual noise power constraint is derived\nwhich enhances the recognition performance. To fulfill the underlying rank-1\nassumption, the speech covariance matrix is reconstructed based on eigenvectors\nor generalized eigenvectors. Then the rank-1 constrained MWF is evaluated with\nalternative multichannel linear filters under the same framework, which\ninvolves a Bidirectional Long Short-Term Memory (BLSTM) network for mask\nestimation. The proposed filter outperforms alternative ones, leading to a 40%\nrelative Word Error Rate (WER) reduction compared with the baseline Weighted\nDelay and Sum (WDAS) beamformer on the real test set, and a 15% relative WER\nreduction compared with the GEV-BAN method. The results also suggest that the\nspeech recognition accuracy correlates more with the Mel-frequency cepstral\ncoefficients (MFCC) feature variance than with the noise reduction or the\nspeech distortion level.", "published": "2017-07-01 20:50:33", "link": "http://arxiv.org/abs/1707.00201v2", "categories": ["cs.SD", "cs.CL"], "primary_category": "cs.SD"}
{"title": "SAM: Semantic Attribute Modulation for Language Modeling and Style\n  Variation", "abstract": "This paper presents a Semantic Attribute Modulation (SAM) for language\nmodeling and style variation. The semantic attribute modulation includes\nvarious document attributes, such as titles, authors, and document categories.\nWe consider two types of attributes, (title attributes and category\nattributes), and a flexible attribute selection scheme by automatically scoring\nthem via an attribute attention mechanism. The semantic attributes are embedded\ninto the hidden semantic space as the generation inputs. With the attributes\nproperly harnessed, our proposed SAM can generate interpretable texts with\nregard to the input attributes. Qualitative analysis, including word semantic\nanalysis and attention values, shows the interpretability of SAM. On several\ntypical text datasets, we empirically demonstrate the superiority of the\nSemantic Attribute Modulated language model with different combinations of\ndocument attributes. Moreover, we present a style variation for the lyric\ngeneration using SAM, which shows a strong connection between the style\nvariation and the semantic attributes.", "published": "2017-07-01 09:00:28", "link": "http://arxiv.org/abs/1707.00117v3", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Sample-efficient Actor-Critic Reinforcement Learning with Supervised\n  Data for Dialogue Management", "abstract": "Deep reinforcement learning (RL) methods have significant potential for\ndialogue policy optimisation. However, they suffer from a poor performance in\nthe early stages of learning. This is especially problematic for on-line\nlearning with real users. Two approaches are introduced to tackle this problem.\nFirstly, to speed up the learning process, two sample-efficient neural networks\nalgorithms: trust region actor-critic with experience replay (TRACER) and\nepisodic natural actor-critic with experience replay (eNACER) are presented.\nFor TRACER, the trust region helps to control the learning step size and avoid\ncatastrophic model changes. For eNACER, the natural gradient identifies the\nsteepest ascent direction in policy space to speed up the convergence. Both\nmodels employ off-policy learning with experience replay to improve\nsample-efficiency. Secondly, to mitigate the cold start issue, a corpus of\ndemonstration data is utilised to pre-train the models prior to on-line\nreinforcement learning. Combining these two approaches, we demonstrate a\npractical approach to learn deep RL-based dialogue policies and demonstrate\ntheir effectiveness in a task-oriented information seeking domain.", "published": "2017-07-01 09:56:31", "link": "http://arxiv.org/abs/1707.00130v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Efficient Correlated Topic Modeling with Topic Embedding", "abstract": "Correlated topic modeling has been limited to small model and problem sizes\ndue to their high computational cost and poor scaling. In this paper, we\npropose a new model which learns compact topic embeddings and captures topic\ncorrelations through the closeness between the topic vectors. Our method\nenables efficient inference in the low-dimensional embedding space, reducing\nprevious cubic or quadratic time complexity to linear w.r.t the topic size. We\nfurther speedup variational inference with a fast sampler to exploit sparsity\nof topic occurrence. Extensive experiments show that our approach is capable of\nhandling model and data scales which are several orders of magnitude larger\nthan existing correlation results, without sacrificing modeling quality by\nproviding competitive or superior performance in document classification and\nretrieval.", "published": "2017-07-01 21:10:15", "link": "http://arxiv.org/abs/1707.00206v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
