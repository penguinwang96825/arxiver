{"title": "'Just because you are right, doesn't mean I am wrong': Overcoming a\n  Bottleneck in the Development and Evaluation of Open-Ended Visual Question\n  Answering (VQA) Tasks", "abstract": "GQA~\\citep{hudson2019gqa} is a dataset for real-world visual reasoning and\ncompositional question answering. We found that many answers predicted by the\nbest vision-language models on the GQA dataset do not match the ground-truth\nanswer but still are semantically meaningful and correct in the given context.\nIn fact, this is the case with most existing visual question answering (VQA)\ndatasets where they assume only one ground-truth answer for each question. We\npropose Alternative Answer Sets (AAS) of ground-truth answers to address this\nlimitation, which is created automatically using off-the-shelf NLP tools. We\nintroduce a semantic metric based on AAS and modify top VQA solvers to support\nmultiple plausible answers for a question. We implement this approach on the\nGQA dataset and show the performance improvements. Code and data are available\nin this link \\url{https://github.com/luomancs/alternative_answer_set.git}.", "published": "2021-03-28 00:07:08", "link": "http://arxiv.org/abs/2103.15022v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Hallucination and Predictive Uncertainty in Conditional Language\n  Generation", "abstract": "Despite improvements in performances on different natural language generation\ntasks, deep neural models are prone to hallucinating facts that are incorrect\nor nonexistent. Different hypotheses are proposed and examined separately for\ndifferent tasks, but no systematic explanations are available across these\ntasks. In this study, we draw connections between hallucinations and predictive\nuncertainty in conditional language generation. We investigate their\nrelationship in both image captioning and data-to-text generation and propose a\nsimple extension to beam search to reduce hallucination. Our analysis shows\nthat higher predictive uncertainty corresponds to a higher chance of\nhallucination. Epistemic uncertainty is more indicative of hallucination than\naleatoric or total uncertainties. It helps to achieve better results of trading\nperformance in standard metric for less hallucination with the proposed beam\nsearch variant.", "published": "2021-03-28 00:32:27", "link": "http://arxiv.org/abs/2103.15025v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "InsertGNN: Can Graph Neural Networks Outperform Humans in TOEFL Sentence\n  Insertion Problem?", "abstract": "Sentence insertion is an interesting NLP problem but received insufficient\nattention. Existing approaches in sentence ordering, text coherence, and\nquestion answering are neither suitable nor good enough at solving it. To\nbridge this gap, we propose InsertGNN, a simple yet effective model that\nrepresents the problem as a graph and adopts a hierarchical graph neural\nnetwork (GNN) to learn the connection between sentences. We evaluate our method\nin our newly collected TOEFL dataset and further verify its effectiveness on\nthe larger arXiv dataset using cross-domain learning. Extensive experiments\ndemonstrate that InsertGNN outperforms all baselines by a large margin with an\naccuracy of 70\\%, rivaling the average human test scores.", "published": "2021-03-28 06:50:31", "link": "http://arxiv.org/abs/2103.15066v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PENELOPIE: Enabling Open Information Extraction for the Greek Language\n  through Machine Translation", "abstract": "In this paper we present our submission for the EACL 2021 SRW; a methodology\nthat aims at bridging the gap between high and low-resource languages in the\ncontext of Open Information Extraction, showcasing it on the Greek language.\nThe goals of this paper are twofold: First, we build Neural Machine Translation\n(NMT) models for English-to-Greek and Greek-to-English based on the Transformer\narchitecture. Second, we leverage these NMT models to produce English\ntranslations of Greek text as input for our NLP pipeline, to which we apply a\nseries of pre-processing and triple extraction tasks. Finally, we\nback-translate the extracted triples to Greek. We conduct an evaluation of both\nour NMT and OIE methods on benchmark datasets and demonstrate that our approach\noutperforms the current state-of-the-art for the Greek natural language.", "published": "2021-03-28 08:01:58", "link": "http://arxiv.org/abs/2103.15075v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TransICD: Transformer Based Code-wise Attention Model for Explainable\n  ICD Coding", "abstract": "International Classification of Disease (ICD) coding procedure which refers\nto tagging medical notes with diagnosis codes has been shown to be effective\nand crucial to the billing system in medical sector. Currently, ICD codes are\nassigned to a clinical note manually which is likely to cause many errors.\nMoreover, training skilled coders also requires time and human resources.\nTherefore, automating the ICD code determination process is an important task.\nWith the advancement of artificial intelligence theory and computational\nhardware, machine learning approach has emerged as a suitable solution to\nautomate this process. In this project, we apply a transformer-based\narchitecture to capture the interdependence among the tokens of a document and\nthen use a code-wise attention mechanism to learn code-specific representations\nof the entire document. Finally, they are fed to separate dense layers for\ncorresponding code prediction. Furthermore, to handle the imbalance in the code\nfrequency of clinical datasets, we employ a label distribution aware margin\n(LDAM) loss function. The experimental results on the MIMIC-III dataset show\nthat our proposed model outperforms other baselines by a significant margin. In\nparticular, our best setting achieves a micro-AUC score of 0.923 compared to\n0.868 of bidirectional recurrent neural networks. We also show that by using\nthe code-wise attention mechanism, the model can provide more insights about\nits prediction, and thus it can support clinicians to make reliable decisions.\nOur code is available online (https://github.com/biplob1ly/TransICD)", "published": "2021-03-28 05:34:32", "link": "http://arxiv.org/abs/2104.10652v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PnG BERT: Augmented BERT on Phonemes and Graphemes for Neural TTS", "abstract": "This paper introduces PnG BERT, a new encoder model for neural TTS. This\nmodel is augmented from the original BERT model, by taking both phoneme and\ngrapheme representations of text as input, as well as the word-level alignment\nbetween them. It can be pre-trained on a large text corpus in a self-supervised\nmanner, and fine-tuned in a TTS task. Experimental results show that a neural\nTTS model using a pre-trained PnG BERT as its encoder yields more natural\nprosody and more accurate pronunciation than a baseline model using only\nphoneme input with no pre-training. Subjective side-by-side preference\nevaluations show that raters have no statistically significant preference\nbetween the speech synthesized using a PnG BERT and ground truth recordings\nfrom professional speakers.", "published": "2021-03-28 06:24:00", "link": "http://arxiv.org/abs/2103.15060v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Quantifying Bias in Automatic Speech Recognition", "abstract": "Automatic speech recognition (ASR) systems promise to deliver objective\ninterpretation of human speech. Practice and recent evidence suggests that the\nstate-of-the-art (SotA) ASRs struggle with the large variation in speech due to\ne.g., gender, age, speech impairment, race, and accents. Many factors can cause\nthe bias of an ASR system. Our overarching goal is to uncover bias in ASR\nsystems to work towards proactive bias mitigation in ASR. This paper is a first\nstep towards this goal and systematically quantifies the bias of a Dutch SotA\nASR system against gender, age, regional accents and non-native accents. Word\nerror rates are compared, and an in-depth phoneme-level error analysis is\nconducted to understand where bias is occurring. We primarily focus on bias due\nto articulation differences in the dataset. Based on our findings, we suggest\nbias mitigation strategies for ASR development.", "published": "2021-03-28 12:52:03", "link": "http://arxiv.org/abs/2103.15122v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Quantum Bose-Einstein Statistics for Indistinguishable Concepts in Human\n  Language", "abstract": "We investigate the hypothesis that within a combination of a 'number concept'\nplus a 'substantive concept', such as 'eleven animals,' the identity and\nindistinguishability present on the level of the concepts, i.e., all eleven\nanimals are identical and indistinguishable, gives rise to a statistical\nstructure of the Bose-Einstein type similar to how Bose-Einstein statistics is\npresent for identical and indistinguishable quantum particles. We proceed by\nidentifying evidence for this hypothesis by extracting the statistical data\nfrom the World-Wide-Web utilizing the Google Search tool. By using the\nKullback-Leibler divergence method, we then compare the obtained distribution\nwith the Maxwell-Boltzmann as well as with the Bose-Einstein distributions and\nshow that the Bose-Einstein's provides a better fit as compared to the\nMaxwell-Boltzmanns.", "published": "2021-03-28 13:07:12", "link": "http://arxiv.org/abs/2103.15125v1", "categories": ["q-bio.NC", "cs.CL", "quant-ph"], "primary_category": "q-bio.NC"}
{"title": "Libri-adhoc40: A dataset collected from synchronized ad-hoc microphone\n  arrays", "abstract": "Recently, there is a research trend on ad-hoc microphone arrays. However,\nmost research was conducted on simulated data. Although some data sets were\ncollected with a small number of distributed devices, they were not\nsynchronized which hinders the fundamental theoretical research to ad-hoc\nmicrophone arrays. To address this issue, this paper presents a synchronized\nspeech corpus, named Libri-adhoc40, which collects the replayed Librispeech\ndata from loudspeakers by ad-hoc microphone arrays of 40 strongly synchronized\ndistributed nodes in a real office environment. Besides, to provide the\nevaluation target for speech frontend processing and other applications, we\nalso recorded the replayed speech in an anechoic chamber. We trained several\nmulti-device speech recognition systems on both the Libri-adhoc40 dataset and a\nsimulated dataset. Experimental results demonstrate the validness of the\nproposed corpus which can be used as a benchmark to reflect the trend and\ndifference of the models with different ad-hoc microphone arrays. The dataset\nis online available at https://github.com/ISmallFish/Libri-adhoc40.", "published": "2021-03-28 12:46:31", "link": "http://arxiv.org/abs/2103.15118v3", "categories": ["eess.AS"], "primary_category": "eess.AS"}
