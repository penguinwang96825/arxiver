{"title": "Enhancing Incremental Summarization with Structured Representations", "abstract": "Large language models (LLMs) often struggle with processing extensive input\ncontexts, which can lead to redundant, inaccurate, or incoherent summaries.\nRecent methods have used unstructured memory to incrementally process these\ncontexts, but they still suffer from information overload due to the volume of\nunstructured data handled. In our study, we introduce structured knowledge\nrepresentations ($GU_{json}$), which significantly improve summarization\nperformance by 40% and 14% across two public datasets. Most notably, we propose\nthe Chain-of-Key strategy ($CoK_{json}$) that dynamically updates or augments\nthese representations with new information, rather than recreating the\nstructured memory for each new source. This method further enhances performance\nby 7% and 4% on the datasets.", "published": "2024-07-21 00:23:33", "link": "http://arxiv.org/abs/2407.15021v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Training Zero-Shot Generalizable End-to-End Task-Oriented Dialog System\n  Without Turn-level Dialog Annotations", "abstract": "Task-oriented dialogue (TOD) systems enable users to achieve their goals\nthrough natural language interactions. Traditionally, these systems have relied\non turn-level manually annotated metadata, such as dialogue states and policy\nannotations, which are expensive, time-consuming, and often inconsistent or\nerror-prone. This dependence limits the potential to leverage vast amounts of\nreadily available conversational data for training TOD systems. Additionally, a\ncritical challenge in TOD system design is determining when and how to access\nand integrate information from external sources. Current approaches typically\nexpect this information to be provided alongside the dialogue context, rather\nthan learning to identify and retrieve it autonomously. While pre-trained large\nlanguage models (LLMs) have been used to develop TOD systems, their potential\nto train such systems without laborious annotations remains largely unexplored.\nThis work employs multi-task instruction fine-tuning to create more efficient\nand scalable TOD systems that can effectively leverage natural language\nconversational data without manual annotations, while autonomously managing\nexternal information retrieval. Our extensive experimental evaluations, using\nthree diverse TOD datasets and three LLMs of varying sizes, demonstrate that\nour approach can generalize to new, unseen domains. Notably, our approach\noutperforms both state-of-the-art models trained on annotated data and\nbillion-scale parameter off-the-shelf ChatGPT models.", "published": "2024-07-21 04:52:38", "link": "http://arxiv.org/abs/2407.15055v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A multi-level multi-label text classification dataset of 19th century\n  Ottoman and Russian literary and critical texts", "abstract": "This paper introduces a multi-level, multi-label text classification dataset\ncomprising over 3000 documents. The dataset features literary and critical\ntexts from 19th-century Ottoman Turkish and Russian. It is the first study to\napply large language models (LLMs) to this dataset, sourced from prominent\nliterary periodicals of the era. The texts have been meticulously organized and\nlabeled. This was done according to a taxonomic framework that takes into\naccount both their structural and semantic attributes. Articles are categorized\nand tagged with bibliometric metadata by human experts. We present baseline\nclassification results using a classical bag-of-words (BoW) naive Bayes model\nand three modern LLMs: multilingual BERT, Falcon, and Llama-v2. We found that\nin certain cases, Bag of Words (BoW) outperforms Large Language Models (LLMs),\nemphasizing the need for additional research, especially in low-resource\nlanguage settings. This dataset is expected to be a valuable resource for\nresearchers in natural language processing and machine learning, especially for\nhistorical and low-resource languages. The dataset is publicly available^1.", "published": "2024-07-21 12:14:45", "link": "http://arxiv.org/abs/2407.15136v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-grained Gender Control in Machine Translation with Large Language\n  Models", "abstract": "In machine translation, the problem of ambiguously gendered input has been\npointed out, where the gender of an entity is not available in the source\nsentence. To address this ambiguity issue, the task of controlled translation\nthat takes the gender of the ambiguous entity as additional input have been\nproposed. However, most existing works have only considered a simplified setup\nof one target gender for input. In this paper, we tackle controlled translation\nin a more realistic setting of inputs with multiple entities and propose\nGender-of-Entity (GoE) prompting method for LLMs. Our proposed method instructs\nthe model with fine-grained entity-level gender information to translate with\ncorrect gender inflections. By utilizing four evaluation benchmarks, we\ninvestigate the controlled translation capability of LLMs in multiple\ndimensions and find that LLMs reach state-of-the-art performance in controlled\ntranslation. Furthermore, we discover an emergence of gender interference\nphenomenon when controlling the gender of multiple entities. Finally, we\naddress the limitations of existing gender accuracy evaluation metrics and\npropose leveraging LLMs as an evaluator for gender inflection in machine\ntranslation.", "published": "2024-07-21 13:15:00", "link": "http://arxiv.org/abs/2407.15154v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey on Employing Large Language Models for Text-to-SQL Tasks", "abstract": "The increasing volume of data in relational databases and the expertise\nneeded for writing SQL queries pose challenges for users to access and analyze\ndata. Text-to-SQL (Text2SQL) solves the issues by utilizing natural language\nprocessing (NLP) techniques to convert natural language into SQL queries. With\nthe development of Large Language Models (LLMs), a range of LLM-based Text2SQL\nmethods have emerged. This survey provides a comprehensive review of LLMs in\nText2SQL tasks. We review benchmark datasets, prompt engineering methods,\nfine-tuning methods, and base models in LLM-based Text2SQL methods. We provide\ninsights in each part and discuss future directions in this field.", "published": "2024-07-21 14:48:23", "link": "http://arxiv.org/abs/2407.15186v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Two eyes, Two views, and finally, One summary! Towards Multi-modal\n  Multi-tasking Knowledge-Infused Medical Dialogue Summarization", "abstract": "We often summarize a multi-party conversation in two stages: chunking with\nhomogeneous units and summarizing the chunks. Thus, we hypothesize that there\nexists a correlation between homogeneous speaker chunking and overall\nsummarization tasks. In this work, we investigate the effectiveness of a\nmulti-faceted approach that simultaneously produces summaries of medical\nconcerns, doctor impressions, and an overall view. We introduce a multi-modal,\nmulti-tasking, knowledge-infused medical dialogue summary generation\n(MMK-Summation) model, which is incorporated with adapter-based fine-tuning\nthrough a gated mechanism for multi-modal information integration. The model,\nMMK-Summation, takes dialogues as input, extracts pertinent external knowledge\nbased on the context, integrates the knowledge and visual cues from the\ndialogues into the textual content, and ultimately generates concise summaries\nencompassing medical concerns, doctor impressions, and a comprehensive\noverview. The introduced model surpasses multiple baselines and traditional\nsummarization models across all evaluation metrics (including human\nevaluation), which firmly demonstrates the efficacy of the knowledge-guided\nmulti-tasking, multimodal medical conversation summarization. The code is\navailable at https://github.com/NLP-RL/MMK-Summation.", "published": "2024-07-21 18:00:10", "link": "http://arxiv.org/abs/2407.15237v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "XAI meets LLMs: A Survey of the Relation between Explainable AI and\n  Large Language Models", "abstract": "In this survey, we address the key challenges in Large Language Models (LLM)\nresearch, focusing on the importance of interpretability. Driven by increasing\ninterest from AI and business sectors, we highlight the need for transparency\nin LLMs. We examine the dual paths in current LLM research and eXplainable\nArtificial Intelligence (XAI): enhancing performance through XAI and the\nemerging focus on model interpretability. Our paper advocates for a balanced\napproach that values interpretability equally with functional advancements.\nRecognizing the rapid development in LLM research, our survey includes both\npeer-reviewed and preprint (arXiv) papers, offering a comprehensive overview of\nXAI's role in LLM research. We conclude by urging the research community to\nadvance both LLM and XAI fields together.", "published": "2024-07-21 19:23:45", "link": "http://arxiv.org/abs/2407.15248v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fact-Aware Multimodal Retrieval Augmentation for Accurate Medical\n  Radiology Report Generation", "abstract": "Multimodal foundation models hold significant potential for automating\nradiology report generation, thereby assisting clinicians in diagnosing cardiac\ndiseases. However, generated reports often suffer from serious factual\ninaccuracy. In this paper, we introduce a fact-aware multimodal\nretrieval-augmented pipeline in generating accurate radiology reports\n(FactMM-RAG). We first leverage RadGraph to mine factual report pairs, then\nintegrate factual knowledge to train a universal multimodal retriever. Given a\nradiology image, our retriever can identify high-quality reference reports to\naugment multimodal foundation models, thus enhancing the factual completeness\nand correctness of report generation. Experiments on two benchmark datasets\nshow that our multimodal retriever outperforms state-of-the-art retrievers on\nboth language generation and radiology-specific metrics, up to 6.5% and 2%\nscore in F1CheXbert and F1RadGraph. Further analysis indicates that employing\nour factually-informed training strategy imposes an effective supervision\nsignal, without relying on explicit diagnostic label guidance, and successfully\npropagates fact-aware capabilities from the multimodal retriever to the\nmultimodal foundation model in radiology report generation.", "published": "2024-07-21 21:04:28", "link": "http://arxiv.org/abs/2407.15268v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense\n  Persona Knowledge Linking", "abstract": "Understanding rich dialogues often requires NLP systems to access relevant\ncommonsense persona knowledge, but retrieving this knowledge is challenging due\nto complex contexts and the implicit nature of commonsense. This paper presents\nour approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,\naddressing the critical need for integrating persona and commonsense knowledge\nin open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that\nleverages Large Language Models to generate high-quality synthetic datasets for\ntraining commonsense persona knowledge linkers. To demonstrate the efficacy of\nour approach, we present SynCPKL, a new dataset specifically designed for this\ntask. Our experiments validate the effectiveness of SynCPKL for training\ncommonsense persona knowledge linkers. Additionally, our top-performing model,\nDerberta-SynCPKL, secured first place in the CPKL challenge by a 16%\nimprovement in F1 score. We released both SynCPKL and Derberta-SynCPKL at\nhttps://github.com/irislin1006/CPKL.", "published": "2024-07-21 22:07:14", "link": "http://arxiv.org/abs/2407.15281v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Intrinsic Self-correction for Enhanced Morality: An Analysis of Internal\n  Mechanisms and the Superficial Hypothesis", "abstract": "Large Language Models (LLMs) are capable of producing content that\nperpetuates stereotypes, discrimination, and toxicity. The recently proposed\nmoral self-correction is a computationally efficient method for reducing\nharmful content in the responses of LLMs. However, the process of how injecting\nself-correction instructions can modify the behavior of LLMs remains\nunder-explored. In this paper, we explore the effectiveness of moral\nself-correction by answering three research questions: (1) In what scenarios\ndoes moral self-correction work? (2) What are the internal mechanisms of LLMs,\ne.g., hidden states, that are influenced by moral self-correction instructions?\n(3) Is intrinsic moral self-correction actually superficial in terms of reduced\nimmorality in hidden states?\n  We argue that self-correction can help LLMs find a shortcut to more morally\ncorrect output, rather than truly reducing the immorality stored in hidden\nstates. Through empirical investigation with tasks of language generation and\nmulti-choice question answering, we conclude:(i) LLMs exhibit good performance\nacross both tasks, and self-correction instructions are particularly beneficial\nwhen the correct answer is already top-ranked; (ii) The morality levels in\nintermediate hidden states are strong indicators as to whether one instruction\nwould be more effective than another; (iii) Based on our analysis of\nintermediate hidden states and task case studies of self-correction behaviors,\nwe are first to propose the hypothesis that intrinsic moral self-correction is\nin fact superficial.", "published": "2024-07-21 22:50:11", "link": "http://arxiv.org/abs/2407.15286v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Answer, Assemble, Ace: Understanding How LMs Answer Multiple Choice\n  Questions", "abstract": "Multiple-choice question answering (MCQA) is a key competence of performant\ntransformer language models that is tested by mainstream benchmarks. However,\nrecent evidence shows that models can have quite a range of performance,\nparticularly when the task format is diversified slightly (such as by shuffling\nanswer choice order). In this work we ask: how do successful models perform\nformatted MCQA? We employ vocabulary projection and activation patching methods\nto localize key hidden states that encode relevant information for predicting\nthe correct answer. We find that the prediction of a specific answer symbol is\ncausally attributed to a few middle layers, and specifically their multi-head\nself-attention mechanisms. We show that subsequent layers increase the\nprobability of the predicted answer symbol in vocabulary space, and that this\nprobability increase is associated with a sparse set of attention heads with\nunique roles. We additionally uncover differences in how different models\nadjust to alternative symbols. Finally, we demonstrate that a synthetic task\ncan disentangle sources of model error to pinpoint when a model has learned\nformatted MCQA, and show that logit differences between answer choice tokens\ncontinue to grow over the course of training.", "published": "2024-07-21 00:10:23", "link": "http://arxiv.org/abs/2407.15018v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "End-to-End Video Question Answering with Frame Scoring Mechanisms and\n  Adaptive Sampling", "abstract": "Video Question Answering (VideoQA) has emerged as a challenging frontier in\nthe field of multimedia processing, requiring intricate interactions between\nvisual and textual modalities. Simply uniformly sampling frames or\nindiscriminately aggregating frame-level visual features often falls short in\ncapturing the nuanced and relevant contexts of videos to well perform VideoQA.\nTo mitigate these issues, we propose VidF4, a novel VideoQA framework equipped\nwith tailored frame selection strategy for effective and efficient VideoQA. We\npropose three frame-scoring mechanisms that consider both question relevance\nand inter-frame similarity to evaluate the importance of each frame for a given\nquestion on the video. Furthermore, we design a differentiable adaptive frame\nsampling mechanism to facilitate end-to-end training for the frame selector and\nanswer generator. The experimental results across three widely adopted\nbenchmarks demonstrate that our model consistently outperforms existing VideoQA\nmethods, establishing a new SOTA across NExT-QA (+0.3%), STAR (+0.9%), and TVQA\n(+1.0%). Furthermore, through both quantitative and qualitative analyses, we\nvalidate the effectiveness of each design choice.", "published": "2024-07-21 04:09:37", "link": "http://arxiv.org/abs/2407.15047v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Relational Database Augmented Large Language Model", "abstract": "Large language models (LLMs) excel in many natural language processing (NLP)\ntasks. However, since LLMs can only incorporate new knowledge through training\nor supervised fine-tuning processes, they are unsuitable for applications that\ndemand precise, up-to-date, and private information not available in the\ntraining corpora. This precise, up-to-date, and private information is\ntypically stored in relational databases. Thus, a promising solution is to\naugment LLMs with the inclusion of relational databases as external memory.\nThis can ensure the timeliness, correctness, and consistency of data, and\nassist LLMs in performing complex arithmetic operations beyond their inherent\ncapabilities. However, bridging the gap between LLMs and relational databases\nis challenging. It requires the awareness of databases and data values stored\nin databases to select correct databases and issue correct SQL queries.\nBesides, it is necessary for the external memory to be independent of the LLM\nto meet the needs of real-world applications. We introduce a novel LLM-agnostic\nmemory architecture comprising a database selection memory, a data value\nmemory, and relational databases. And we design an elegant pipeline to retrieve\ninformation from it. Besides, we carefully design the prompts to instruct the\nLLM to maximize the framework's potential. To evaluate our method, we compose a\nnew dataset with various types of questions. Experimental results show that our\nframework enables LLMs to effectively answer database-related questions, which\nis beyond their direct ability.", "published": "2024-07-21 06:19:10", "link": "http://arxiv.org/abs/2407.15071v1", "categories": ["cs.DB", "cs.CL"], "primary_category": "cs.DB"}
{"title": "Multi-Agent Causal Discovery Using Large Language Models", "abstract": "Causal discovery aims to identify causal relationships between variables and\nis a critical research area in machine learning. Traditional methods focus on\nstatistical or machine learning algorithms to uncover causal links from\nstructured data, often overlooking the valuable contextual information provided\nby metadata. Large language models (LLMs) have shown promise in creating\nunified causal discovery frameworks by incorporating both structured data and\nmetadata. However, their potential in multi-agent settings remains largely\nunexplored. To address this gap, we introduce the Multi-Agent Causal Discovery\nFramework (MAC), which consists of two key modules: the Debate-Coding Module\n(DCM) and the Meta-Debate Module (MDM). The DCM begins with a multi-agent\ndebating and coding process, where agents use both structured data and metadata\nto collaboratively select the most suitable statistical causal discovery (SCD)\nmethod. The selected SCD is then applied to the structured data to generate an\ninitial causal graph. This causal graph is transformed into causal metadata\nthrough the Meta Fusion mechanism. With all the metadata, MDM then refines the\ncausal structure by leveraging a multi-agent debating framework. Extensive\nexperiments across five datasets demonstrate that MAC outperforms both\ntraditional statistical causal discovery methods and existing LLM-based\napproaches, achieving state-of-the-art performance.", "published": "2024-07-21 06:21:47", "link": "http://arxiv.org/abs/2407.15073v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "DOPRA: Decoding Over-accumulation Penalization and Re-allocation in\n  Specific Weighting Layer", "abstract": "In this work, we introduce DOPRA, a novel approach designed to mitigate\nhallucinations in multi-modal large language models (MLLMs). Unlike existing\nsolutions that typically involve costly supplementary training data or the\nintegration of external knowledge sources, DOPRA innovatively addresses\nhallucinations by decoding specific weighted layer penalties and\nredistribution, offering an economical and effective solution without\nadditional resources. DOPRA is grounded in unique insights into the intrinsic\nmechanisms controlling hallucinations within MLLMs, especially the models'\ntendency to over-rely on a subset of summary tokens in the self-attention\nmatrix, neglecting critical image-related information. This phenomenon is\nparticularly pronounced in certain strata. To counteract this over-reliance,\nDOPRA employs a strategy of weighted overlay penalties and redistribution in\nspecific layers, such as the 12th layer, during the decoding process.\nFurthermore, DOPRA includes a retrospective allocation process that re-examines\nthe sequence of generated tokens, allowing the algorithm to reallocate token\nselection to better align with the actual image content, thereby reducing the\nincidence of hallucinatory descriptions in auto-generated captions. Overall,\nDOPRA represents a significant step forward in improving the output quality of\nMLLMs by systematically reducing hallucinations through targeted adjustments\nduring the decoding process.", "published": "2024-07-21 11:54:49", "link": "http://arxiv.org/abs/2407.15130v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ReAttention: Training-Free Infinite Context with Finite Attention Scope", "abstract": "The long-context capability of the Large Language Models (LLM) has made\nsignificant breakthroughs, but the maximum supported context length in length\nextrapolation remains a critical bottleneck limiting their practical\napplications. The constraint of context length in LLMs arises from the\nself-attention mechanism, which cannot effectively and efficiently capture the\nsemantic relationships within infinitely long contexts via the limited\npre-trained positional information and attention scope. In this work, we\npropose ReAttention, a training-free approach enabling LLM based on the\nself-attention mechanism to support an infinite context with a finite attention\nscope under sufficient memory resources. ReAttention performs the\nposition-agnostic top-$k$ attention before the ordinary position-aware\nself-attention, freeing LLMs from the length extrapolation issue. We validate\nthe performance of ReAttention on the LongBench, L-Eval, and InfiniteBench and\ndemonstrate that it is on par with traditional methods. Furthermore, we also\napply ReAttention on mainstream LLMs, including LLaMA3.1-8B and\nMistral-v0.3-7B, enabling them to support context lengths of at least 1M and\neven expanding the context length of LLaMA3.2-3B-chat by 128$\\times$ to 4M\nwithout any further training in Needle-In-A-Haystack tests. We also improve the\nefficiency of ReAttention with Triton and achieve an efficient extrapolation\nwithout additional overhead. The code is available at\nhttps://github.com/OpenMOSS/ReAttention.", "published": "2024-07-21 14:23:37", "link": "http://arxiv.org/abs/2407.15176v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Community-Centric Perspective for Characterizing and Detecting\n  Anti-Asian Violence-Provoking Speech", "abstract": "Violence-provoking speech -- speech that implicitly or explicitly promotes\nviolence against the members of the targeted community, contributed to a\nmassive surge in anti-Asian crimes during the pandemic. While previous works\nhave characterized and built tools for detecting other forms of harmful speech,\nlike fear speech and hate speech, our work takes a community-centric approach\nto studying anti-Asian violence-provoking speech. Using data from ~420k Twitter\nposts spanning a 3-year duration (January 1, 2020 to February 1, 2023), we\ndevelop a codebook to characterize anti-Asian violence-provoking speech and\ncollect a community-crowdsourced dataset to facilitate its large-scale\ndetection using state-of-the-art classifiers. We contrast the capabilities of\nnatural language processing classifiers, ranging from BERT-based to LLM-based\nclassifiers, in detecting violence-provoking speech with their capabilities to\ndetect anti-Asian hateful speech. In contrast to prior work that has\ndemonstrated the effectiveness of such classifiers in detecting hateful speech\n($F_1 = 0.89$), our work shows that accurate and reliable detection of\nviolence-provoking speech is a challenging task ($F_1 = 0.69$). We discuss the\nimplications of our findings, particularly the need for proactive interventions\nto support Asian communities during public health crises. The resources related\nto the study are available at\nhttps://claws-lab.github.io/violence-provoking-speech/.", "published": "2024-07-21 17:27:17", "link": "http://arxiv.org/abs/2407.15227v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "The Hitchhiker's Guide to Human Alignment with *PO", "abstract": "With the growing utilization of large language models (LLMs) across domains,\nalignment towards human preferences has become one of the most critical aspects\nof training models. At the forefront of state-of-the-art human alignment\nmethods are preference optimization methods (*PO). However, prior research has\noften concentrated on identifying the best-performing method, typically\ninvolving a grid search over hyperparameters, which can be impractical for\ngeneral practitioners. In this paper, we aim to identify the algorithm that,\nwhile being performant, is simultaneously more robust to varying\nhyperparameters, thereby increasing the likelihood of achieving better results.\nWe focus on a realistic out-of-distribution (OOD) scenario that mirrors\nreal-world applications of human alignment, offering practical insights into\nthe strengths and weaknesses of these methods. Furthermore, to better\nunderstand the shortcomings of generations from the different methods, we\nanalyze the model generations through the lens of KL divergence of the SFT\nmodel and the response length statistics. Our analysis reveals that the widely\nadopted DPO method consistently produces lengthy responses of inferior quality\nthat are very close to the SFT responses. Motivated by these findings, we\npropose an embarrassingly simple extension to the DPO algorithm, LN-DPO,\nresulting in more concise responses without sacrificing quality compared to the\npolicy obtained by vanilla DPO.", "published": "2024-07-21 17:35:20", "link": "http://arxiv.org/abs/2407.15229v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Audio-visual training for improved grounding in video-text LLMs", "abstract": "Recent advances in multimodal LLMs, have led to several video-text models\nbeing proposed for critical video-related tasks. However, most of the previous\nworks support visual input only, essentially muting the audio signal in the\nvideo. Few models that support both audio and visual input, are not explicitly\ntrained on audio data. Hence, the effect of audio towards video understanding\nis largely unexplored. To this end, we propose a model architecture that\nhandles audio-visual inputs explicitly. We train our model with both audio and\nvisual data from a video instruction-tuning dataset. Comparison with\nvision-only baselines, and other audio-visual models showcase that training on\naudio data indeed leads to improved grounding of responses. For better\nevaluation of audio-visual models, we also release a human-annotated benchmark\ndataset, with audio-aware question-answer pairs.", "published": "2024-07-21 03:59:14", "link": "http://arxiv.org/abs/2407.15046v1", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "When Can Transformers Count to n?", "abstract": "Large language models based on the transformer architectures can solve highly\ncomplex tasks. But are there simple tasks that such models cannot solve? Here\nwe focus on very simple counting tasks, that involve counting how many times a\ntoken in the vocabulary have appeared in a string. We show that if the\ndimension of the transformer state is linear in the context length, this task\ncan be solved. However, the solution we propose does not scale beyond this\nlimit, and we provide theoretical arguments for why it is likely impossible for\na size limited transformer to implement this task. Our empirical results\ndemonstrate the same phase-transition in performance, as anticipated by the\ntheoretical argument. Our results demonstrate the importance of understanding\nhow transformers can solve simple tasks.", "published": "2024-07-21 13:31:02", "link": "http://arxiv.org/abs/2407.15160v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Decoding Multilingual Moral Preferences: Unveiling LLM's Biases Through\n  the Moral Machine Experiment", "abstract": "Large language models (LLMs) increasingly find their way into the most\ndiverse areas of our everyday lives. They indirectly influence people's\ndecisions or opinions through their daily use. Therefore, understanding how and\nwhich moral judgements these LLMs make is crucial. However, morality is not\nuniversal and depends on the cultural background. This raises the question of\nwhether these cultural preferences are also reflected in LLMs when prompted in\ndifferent languages or whether moral decision-making is consistent across\ndifferent languages. So far, most research has focused on investigating the\ninherent values of LLMs in English. While a few works conduct multilingual\nanalyses of moral bias in LLMs in a multilingual setting, these analyses do not\ngo beyond atomic actions. To the best of our knowledge, a multilingual analysis\nof moral bias in dilemmas has not yet been conducted.\n  To address this, our paper builds on the moral machine experiment (MME) to\ninvestigate the moral preferences of five LLMs, Falcon, Gemini, Llama, GPT, and\nMPT, in a multilingual setting and compares them with the preferences collected\nfrom humans belonging to different cultures. To accomplish this, we generate\n6500 scenarios of the MME and prompt the models in ten languages on which\naction to take. Our analysis reveals that all LLMs inhibit different moral\nbiases to some degree and that they not only differ from the human preferences\nbut also across multiple languages within the models themselves. Moreover, we\nfind that almost all models, particularly Llama 3, divert greatly from human\nvalues and, for instance, prefer saving fewer people over saving more.", "published": "2024-07-21 14:48:13", "link": "http://arxiv.org/abs/2407.15184v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "TAGCOS: Task-agnostic Gradient Clustered Coreset Selection for\n  Instruction Tuning Data", "abstract": "Instruction tuning has achieved unprecedented success in NLP, turning large\nlanguage models into versatile chatbots. However, the increasing variety and\nvolume of instruction datasets demand significant computational resources. To\naddress this, it is essential to extract a small and highly informative subset\n(i.e., Coreset) that achieves comparable performance to the full dataset.\nAchieving this goal poses non-trivial challenges: 1) data selection requires\naccurate data representations that reflect the training samples' quality, 2)\nconsidering the diverse nature of instruction datasets, and 3) ensuring the\nefficiency of the coreset selection algorithm for large models. To address\nthese challenges, we propose Task-Agnostic Gradient Clustered COreset Selection\n(TAGCOS). Specifically, we leverage sample gradients as the data\nrepresentations, perform clustering to group similar data, and apply an\nefficient greedy algorithm for coreset selection. Experimental results show\nthat our algorithm, selecting only 5% of the data, surpasses other unsupervised\nmethods and achieves performance close to that of the full dataset.", "published": "2024-07-21 17:59:20", "link": "http://arxiv.org/abs/2407.15235v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Weak-to-Strong Compositional Learning from Generative Models for\n  Language-based Object Detection", "abstract": "Vision-language (VL) models often exhibit a limited understanding of complex\nexpressions of visual objects (e.g., attributes, shapes, and their relations),\ngiven complex and diverse language queries. Traditional approaches attempt to\nimprove VL models using hard negative synthetic text, but their effectiveness\nis limited. In this paper, we harness the exceptional compositional\nunderstanding capabilities of generative foundational models. We introduce a\nnovel method for structured synthetic data generation aimed at enhancing the\ncompositional understanding of VL models in language-based object detection.\nOur framework generates densely paired positive and negative triplets (image,\ntext descriptions, and bounding boxes) in both image and text domains. By\nleveraging these synthetic triplets, we transform 'weaker' VL models into\n'stronger' models in terms of compositional understanding, a process we call\n\"Weak-to-Strong Compositional Learning\" (WSCL). To achieve this, we propose a\nnew compositional contrastive learning formulation that discovers semantics and\nstructures in complex descriptions from synthetic triplets. As a result, VL\nmodels trained with our synthetic data generation exhibit a significant\nperformance boost in the Omnilabel benchmark by up to +5AP and the D3 benchmark\nby +6.9AP upon existing baselines.", "published": "2024-07-21 23:43:24", "link": "http://arxiv.org/abs/2407.15296v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Towards Automated Data Sciences with Natural Language and SageCopilot:\n  Practices and Lessons Learned", "abstract": "While the field of NL2SQL has made significant advancements in translating\nnatural language instructions into executable SQL scripts for data querying and\nprocessing, achieving full automation within the broader data science pipeline\n- encompassing data querying, analysis, visualization, and reporting - remains\na complex challenge. This study introduces SageCopilot, an advanced,\nindustry-grade system system that automates the data science pipeline by\nintegrating Large Language Models (LLMs), Autonomous Agents (AutoAgents), and\nLanguage User Interfaces (LUIs). Specifically, SageCopilot incorporates a\ntwo-phase design: an online component refining users' inputs into executable\nscripts through In-Context Learning (ICL) and running the scripts for results\nreporting & visualization, and an offline preparing demonstrations requested by\nICL in the online phase. A list of trending strategies such as Chain-of-Thought\nand prompt-tuning have been used to augment SageCopilot for enhanced\nperformance. Through rigorous testing and comparative analysis against\nprompt-based solutions, SageCopilot has been empirically validated to achieve\nsuperior end-to-end performance in generating or executing scripts and offering\nresults with visualization, backed by real-world datasets. Our in-depth\nablation studies highlight the individual contributions of various components\nand strategies used by SageCopilot to the end-to-end correctness for data\nsciences.", "published": "2024-07-21 08:58:18", "link": "http://arxiv.org/abs/2407.21040v1", "categories": ["cs.AI", "cs.CL", "cs.DB", "cs.SE"], "primary_category": "cs.AI"}
{"title": "They Look Like Each Other: Case-based Reasoning for Explainable\n  Depression Detection on Twitter using Large Language Models", "abstract": "Depression is a common mental health issue that requires prompt diagnosis and\ntreatment. Despite the promise of social media data for depression detection,\nthe opacity of employed deep learning models hinders interpretability and\nraises bias concerns. We address this challenge by introducing ProtoDep, a\nnovel, explainable framework for Twitter-based depression detection. ProtoDep\nleverages prototype learning and the generative power of Large Language Models\nto provide transparent explanations at three levels: (i) symptom-level\nexplanations for each tweet and user, (ii) case-based explanations comparing\nthe user to similar individuals, and (iii) transparent decision-making through\nclassification weights. Evaluated on five benchmark datasets, ProtoDep achieves\nnear state-of-the-art performance while learning meaningful prototypes. This\nmulti-faceted approach offers significant potential to enhance the reliability\nand transparency of depression detection on social media, ultimately aiding\nmental health professionals in delivering more informed care.", "published": "2024-07-21 20:13:50", "link": "http://arxiv.org/abs/2407.21041v1", "categories": ["cs.CL", "cs.AI", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Failures to Find Transferable Image Jailbreaks Between Vision-Language\n  Models", "abstract": "The integration of new modalities into frontier AI systems offers exciting\ncapabilities, but also increases the possibility such systems can be\nadversarially manipulated in undesirable ways. In this work, we focus on a\npopular class of vision-language models (VLMs) that generate text outputs\nconditioned on visual and textual inputs. We conducted a large-scale empirical\nstudy to assess the transferability of gradient-based universal image\n``jailbreaks\" using a diverse set of over 40 open-parameter VLMs, including 18\nnew VLMs that we publicly release. Overall, we find that transferable\ngradient-based image jailbreaks are extremely difficult to obtain. When an\nimage jailbreak is optimized against a single VLM or against an ensemble of\nVLMs, the jailbreak successfully jailbreaks the attacked VLM(s), but exhibits\nlittle-to-no transfer to any other VLMs; transfer is not affected by whether\nthe attacked and target VLMs possess matching vision backbones or language\nmodels, whether the language model underwent instruction-following and/or\nsafety-alignment training, or many other factors. Only two settings display\npartially successful transfer: between identically-pretrained and\nidentically-initialized VLMs with slightly different VLM training data, and\nbetween different training checkpoints of a single VLM. Leveraging these\nresults, we then demonstrate that transfer can be significantly improved\nagainst a specific target VLM by attacking larger ensembles of\n``highly-similar\" VLMs. These results stand in stark contrast to existing\nevidence of universal and transferable text jailbreaks against language models\nand transferable adversarial attacks against image classifiers, suggesting that\nVLMs may be more robust to gradient-based transfer attacks.", "published": "2024-07-21 16:27:24", "link": "http://arxiv.org/abs/2407.15211v2", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Overview of Speaker Modeling and Its Applications: From the Lens of Deep\n  Speaker Representation Learning", "abstract": "Speaker individuality information is among the most critical elements within\nspeech signals. By thoroughly and accurately modeling this information, it can\nbe utilized in various intelligent speech applications, such as speaker\nrecognition, speaker diarization, speech synthesis, and target speaker\nextraction. In this overview, we present a comprehensive review of neural\napproaches to speaker representation learning from both theoretical and\npractical perspectives. Theoretically, we discuss speaker encoders ranging from\nsupervised to self-supervised learning algorithms, standalone models to large\npretrained models, pure speaker embedding learning to joint optimization with\ndownstream tasks, and efforts toward interpretability. Practically, we\nsystematically examine approaches for robustness and effectiveness, introduce\nand compare various open-source toolkits in the field. Through the systematic\nand comprehensive review of the relevant literature, research activities, and\nresources, we provide a clear reference for researchers in the speaker\ncharacterization and modeling field, as well as for those who wish to apply\nspeaker modeling techniques to specific downstream tasks.", "published": "2024-07-21 14:54:54", "link": "http://arxiv.org/abs/2407.15188v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MusiConGen: Rhythm and Chord Control for Transformer-Based Text-to-Music\n  Generation", "abstract": "Existing text-to-music models can produce high-quality audio with great\ndiversity. However, textual prompts alone cannot precisely control temporal\nmusical features such as chords and rhythm of the generated music. To address\nthis challenge, we introduce MusiConGen, a temporally-conditioned\nTransformer-based text-to-music model that builds upon the pretrained MusicGen\nframework. Our innovation lies in an efficient finetuning mechanism, tailored\nfor consumer-grade GPUs, that integrates automatically-extracted rhythm and\nchords as the condition signal. During inference, the condition can either be\nmusical features extracted from a reference audio signal, or be user-defined\nsymbolic chord sequence, BPM, and textual prompts. Our performance evaluation\non two datasets -- one derived from extracted features and the other from\nuser-created inputs -- demonstrates that MusiConGen can generate realistic\nbacking track music that aligns well with the specified conditions. We\nopen-source the code and model checkpoints, and provide audio examples online,\nhttps://musicongen.github.io/musicongen_demo/.", "published": "2024-07-21 05:27:53", "link": "http://arxiv.org/abs/2407.15060v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Explainability Paths for Sustained Artistic Practice with AI", "abstract": "The development of AI-driven generative audio mirrors broader AI trends,\noften prioritizing immediate accessibility at the expense of explainability.\nConsequently, integrating such tools into sustained artistic practice remains a\nsignificant challenge. In this paper, we explore several paths to improve\nexplainability, drawing primarily from our research-creation practice in\ntraining and implementing generative audio models. As practical provisions for\nimproved explainability, we highlight human agency over training materials, the\nviability of small-scale datasets, the facilitation of the iterative creative\nprocess, and the integration of interactive machine learning as a mapping tool.\nImportantly, these steps aim to enhance human agency over generative AI systems\nnot only during model inference, but also when curating and preprocessing\ntraining data as well as during the training phase of models.", "published": "2024-07-21 16:48:14", "link": "http://arxiv.org/abs/2407.15216v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
