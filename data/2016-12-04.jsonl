{"title": "CER: Complementary Entity Recognition via Knowledge Expansion on Large\n  Unlabeled Product Reviews", "abstract": "Product reviews contain a lot of useful information about product features\nand customer opinions. One important product feature is the complementary\nentity (products) that may potentially work together with the reviewed product.\nKnowing complementary entities of the reviewed product is very important\nbecause customers want to buy compatible products and avoid incompatible ones.\nIn this paper, we address the problem of Complementary Entity Recognition\n(CER). Since no existing method can solve this problem, we first propose a\nnovel unsupervised method to utilize syntactic dependency paths to recognize\ncomplementary entities. Then we expand category-level domain knowledge about\ncomplementary entities using only a few general seed verbs on a large amount of\nunlabeled reviews. The domain knowledge helps the unsupervised method to adapt\nto different products and greatly improves the precision of the CER task. The\nadvantage of the proposed method is that it does not require any labeled data\nfor training. We conducted experiments on 7 popular products with about 1200\nreviews in total to demonstrate that the proposed approach is effective.", "published": "2016-12-04 00:22:44", "link": "http://arxiv.org/abs/1612.01039v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Symbolic Machines: Learning Semantic Parsers on Freebase with\n  Weak Supervision (Short Version)", "abstract": "Extending the success of deep neural networks to natural language\nunderstanding and symbolic reasoning requires complex operations and external\nmemory. Recent neural program induction approaches have attempted to address\nthis problem, but are typically limited to differentiable memory, and\nconsequently cannot scale beyond small synthetic tasks. In this work, we\npropose the Manager-Programmer-Computer framework, which integrates neural\nnetworks with non-differentiable memory to support abstract, scalable and\nprecise operations through a friendly neural computer interface. Specifically,\nwe introduce a Neural Symbolic Machine, which contains a sequence-to-sequence\nneural \"programmer\", and a non-differentiable \"computer\" that is a Lisp\ninterpreter with code assist. To successfully apply REINFORCE for training, we\naugment it with approximate gold programs found by an iterative maximum\nlikelihood training process. NSM is able to learn a semantic parser from weak\nsupervision over a large knowledge base. It achieves new state-of-the-art\nperformance on WebQuestionsSP, a challenging semantic parsing dataset, with\nweak supervision. Compared to previous approaches, NSM is end-to-end, therefore\ndoes not rely on feature engineering or domain specific knowledge.", "published": "2016-12-04 22:29:32", "link": "http://arxiv.org/abs/1612.01197v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
