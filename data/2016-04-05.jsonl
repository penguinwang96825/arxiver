{"title": "Modeling Relational Information in Question-Answer Pairs with\n  Convolutional Neural Networks", "abstract": "In this paper, we propose convolutional neural networks for learning an\noptimal representation of question and answer sentences. Their main aspect is\nthe use of relational information given by the matches between words from the\ntwo members of the pair. The matches are encoded as embeddings with additional\nparameters (dimensions), which are tuned by the network. These allows for\nbetter capturing interactions between questions and answers, resulting in a\nsignificant boost in accuracy. We test our models on two widely used answer\nsentence selection benchmarks. The results clearly show the effectiveness of\nour relational information, which allows our relatively simple network to\napproach the state of the art.", "published": "2016-04-05 08:50:27", "link": "http://arxiv.org/abs/1604.01178v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Character-Level Neural Translation for Multilingual Media Monitoring in\n  the SUMMA Project", "abstract": "The paper steps outside the comfort-zone of the traditional NLP tasks like\nautomatic speech recognition (ASR) and machine translation (MT) to addresses\ntwo novel problems arising in the automated multilingual news monitoring:\nsegmentation of the TV and radio program ASR transcripts into individual\nstories, and clustering of the individual stories coming from various sources\nand languages into storylines. Storyline clustering of stories covering the\nsame events is an essential task for inquisitorial media monitoring. We address\nthese two problems jointly by engaging the low-dimensional semantic\nrepresentation capabilities of the sequence to sequence neural translation\nmodels. To enable joint multi-task learning for multilingual neural translation\nof morphologically rich languages we replace the attention mechanism with the\nsliding-window mechanism and operate the sequence to sequence neural\ntranslation model on the character-level rather than on the word-level. The\nstory segmentation and storyline clustering problem is tackled by examining the\nlow-dimensional vectors produced as a side-product of the neural translation\nprocess. The results of this paper describe a novel approach to the automatic\nstory segmentation and storyline clustering problem.", "published": "2016-04-05 11:34:11", "link": "http://arxiv.org/abs/1604.01221v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A new TAG Formalism for Tamil and Parser Analytics", "abstract": "Tree adjoining grammar (TAG) is specifically suited for morph rich and\nagglutinated languages like Tamil due to its psycho linguistic features and\nparse time dependency and morph resolution. Though TAG and LTAG formalisms have\nbeen known for about 3 decades, efforts on designing TAG Syntax for Tamil have\nnot been entirely successful due to the complexity of its specification and the\nrich morphology of Tamil language. In this paper we present a minimalistic TAG\nfor Tamil without much morphological considerations and also introduce a parser\nimplementation with some obvious variations from the XTAG system", "published": "2016-04-05 12:42:31", "link": "http://arxiv.org/abs/1604.01235v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RIGA at SemEval-2016 Task 8: Impact of Smatch Extensions and\n  Character-Level Neural Translation on AMR Parsing Accuracy", "abstract": "Two extensions to the AMR smatch scoring script are presented. The first\nextension com-bines the smatch scoring script with the C6.0 rule-based\nclassifier to produce a human-readable report on the error patterns frequency\nobserved in the scored AMR graphs. This first extension results in 4% gain over\nthe state-of-art CAMR baseline parser by adding to it a manually crafted\nwrapper fixing the identified CAMR parser errors. The second extension combines\na per-sentence smatch with an en-semble method for selecting the best AMR graph\namong the set of AMR graphs for the same sentence. This second modification\nau-tomatically yields further 0.4% gain when ap-plied to outputs of two\nnondeterministic AMR parsers: a CAMR+wrapper parser and a novel character-level\nneural translation AMR parser. For AMR parsing task the character-level neural\ntranslation attains surprising 7% gain over the carefully optimized word-level\nneural translation. Overall, we achieve smatch F1=62% on the SemEval-2016\nofficial scor-ing set and F1=67% on the LDC2015E86 test set.", "published": "2016-04-05 14:46:18", "link": "http://arxiv.org/abs/1604.01278v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mental Lexicon Growth Modelling Reveals the Multiplexity of the English\n  Language", "abstract": "In this work we extend previous analyses of linguistic networks by adopting a\nmulti-layer network framework for modelling the human mental lexicon, i.e. an\nabstract mental repository where words and concepts are stored together with\ntheir linguistic patterns. Across a three-layer linguistic multiplex, we model\nEnglish words as nodes and connect them according to (i) phonological\nsimilarities, (ii) synonym relationships and (iii) free word associations. Our\nmain aim is to exploit this multi-layered structure to explore the influence of\nphonological and semantic relationships on lexicon assembly over time. We\npropose a model of lexicon growth which is driven by the phonological layer:\nwords are suggested according to different orderings of insertion (e.g. shorter\nword length, highest frequency, semantic multiplex features) and accepted or\nrejected subject to constraints. We then measure times of network assembly and\ncompare these to empirical data about the age of acquisition of words. In\nagreement with empirical studies in psycholinguistics, our results provide\nquantitative evidence for the hypothesis that word acquisition is driven by\nfeatures at multiple levels of organisation within language.", "published": "2016-04-05 13:04:35", "link": "http://arxiv.org/abs/1604.01243v1", "categories": ["physics.soc-ph", "cs.CL", "cs.SI"], "primary_category": "physics.soc-ph"}
{"title": "Learning to Generate Posters of Scientific Papers", "abstract": "Researchers often summarize their work in the form of posters. Posters\nprovide a coherent and efficient way to convey core ideas from scientific\npapers. Generating a good scientific poster, however, is a complex and time\nconsuming cognitive task, since such posters need to be readable, informative,\nand visually aesthetic. In this paper, for the first time, we study the\nchallenging problem of learning to generate posters from scientific papers. To\nthis end, a data-driven framework, that utilizes graphical models, is proposed.\nSpecifically, given content to display, the key elements of a good poster,\nincluding panel layout and attributes of each panel, are learned and inferred\nfrom data. Then, given inferred layout and attributes, composition of graphical\nelements within each panel is synthesized. To learn and validate our model, we\ncollect and make public a Poster-Paper dataset, which consists of scientific\npapers and corresponding posters with exhaustively labelled panels and\nattributes. Qualitative and quantitative results indicate the effectiveness of\nour approach.", "published": "2016-04-05 11:18:04", "link": "http://arxiv.org/abs/1604.01219v1", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.MM", "stat.ML"], "primary_category": "cs.AI"}
{"title": "Feature extraction using Latent Dirichlet Allocation and Neural\n  Networks: A case study on movie synopses", "abstract": "Feature extraction has gained increasing attention in the field of machine\nlearning, as in order to detect patterns, extract information, or predict\nfuture observations from big data, the urge of informative features is crucial.\nThe process of extracting features is highly linked to dimensionality reduction\nas it implies the transformation of the data from a sparse high-dimensional\nspace, to higher level meaningful abstractions. This dissertation employs\nNeural Networks for distributed paragraph representations, and Latent Dirichlet\nAllocation to capture higher level features of paragraph vectors. Although\nNeural Networks for distributed paragraph representations are considered the\nstate of the art for extracting paragraph vectors, we show that a quick topic\nanalysis model such as Latent Dirichlet Allocation can provide meaningful\nfeatures too. We evaluate the two methods on the CMU Movie Summary Corpus, a\ncollection of 25,203 movie plot summaries extracted from Wikipedia. Finally,\nfor both approaches, we use K-Nearest Neighbors to discover similar movies, and\nplot the projected representations using T-Distributed Stochastic Neighbor\nEmbedding to depict the context similarities. These similarities, expressed as\nmovie distances, can be used for movies recommendation. The recommended movies\nof this approach are compared with the recommended movies from IMDB, which use\na collaborative filtering recommendation approach, to show that our two models\ncould constitute either an alternative or a supplementary recommendation\napproach.", "published": "2016-04-05 14:32:48", "link": "http://arxiv.org/abs/1604.01272v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
