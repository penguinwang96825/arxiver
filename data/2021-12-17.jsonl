{"title": "NILC-Metrix: assessing the complexity of written and spoken language in\n  Brazilian Portuguese", "abstract": "This paper presents and makes publicly available the NILC-Metrix, a\ncomputational system comprising 200 metrics proposed in studies on discourse,\npsycholinguistics, cognitive and computational linguistics, to assess textual\ncomplexity in Brazilian Portuguese (BP). These metrics are relevant for\ndescriptive analysis and the creation of computational models and can be used\nto extract information from various linguistic levels of written and spoken\nlanguage. The metrics in NILC-Metrix were developed during the last 13 years,\nstarting in 2008 with Coh-Metrix-Port, a tool developed within the scope of the\nPorSimples project. Coh-Metrix-Port adapted some metrics to BP from the\nCoh-Metrix tool that computes metrics related to cohesion and coherence of\ntexts in English. After the end of PorSimples in 2010, new metrics were added\nto the initial 48 metrics of Coh-Metrix-Port. Given the large number of\nmetrics, we present them following an organisation similar to the metrics of\nCoh-Metrix v3.0 to facilitate comparisons made with metrics in Portuguese and\nEnglish. In this paper, we illustrate the potential of NILC-Metrix by\npresenting three applications: (i) a descriptive analysis of the differences\nbetween children's film subtitles and texts written for Elementary School I and\nII (Final Years); (ii) a new predictor of textual complexity for the corpus of\noriginal and simplified texts of the PorSimples project; (iii) a complexity\nprediction model for school grades, using transcripts of children's story\nnarratives told by teenagers. For each application, we evaluate which groups of\nmetrics are more discriminative, showing their contribution for each task.", "published": "2021-12-17 16:51:00", "link": "http://arxiv.org/abs/2201.03445v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Joint Chinese Word Segmentation and Part-of-speech Tagging via Two-stage\n  Span Labeling", "abstract": "Chinese word segmentation and part-of-speech tagging are necessary tasks in\nterms of computational linguistics and application of natural language\nprocessing. Many re-searchers still debate the demand for Chinese word\nsegmentation and part-of-speech tagging in the deep learning era. Nevertheless,\nresolving ambiguities and detecting unknown words are challenging problems in\nthis field. Previous studies on joint Chinese word segmentation and\npart-of-speech tagging mainly follow the character-based tagging model focusing\non modeling n-gram features. Unlike previous works, we propose a neural model\nnamed SpanSegTag for joint Chinese word segmentation and part-of-speech tagging\nfollowing the span labeling in which the probability of each n-gram being the\nword and the part-of-speech tag is the main problem. We use the biaffine\noperation over the left and right boundary representations of consecutive\ncharacters to model the n-grams. Our experiments show that our BERT-based model\nSpanSegTag achieved competitive performances on the CTB5, CTB6, and UD, or\nsignificant improvements on CTB7 and CTB9 benchmark datasets compared with the\ncurrent state-of-the-art method using BERT or ZEN encoders.", "published": "2021-12-17 12:59:02", "link": "http://arxiv.org/abs/2112.09488v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Challenge Dataset of Cognates and False Friend Pairs from Indian\n  Languages", "abstract": "Cognates are present in multiple variants of the same text across different\nlanguages (e.g., \"hund\" in German and \"hound\" in English language mean \"dog\").\nThey pose a challenge to various Natural Language Processing (NLP) applications\nsuch as Machine Translation, Cross-lingual Sense Disambiguation, Computational\nPhylogenetics, and Information Retrieval. A possible solution to address this\nchallenge is to identify cognates across language pairs. In this paper, we\ndescribe the creation of two cognate datasets for twelve Indian languages,\nnamely Sanskrit, Hindi, Assamese, Oriya, Kannada, Gujarati, Tamil, Telugu,\nPunjabi, Bengali, Marathi, and Malayalam. We digitize the cognate data from an\nIndian language cognate dictionary and utilize linked Indian language Wordnets\nto generate cognate sets. Additionally, we use the Wordnet data to create a\nFalse Friends' dataset for eleven language pairs. We also evaluate the efficacy\nof our dataset using previously available baseline cognate detection\napproaches. We also perform a manual evaluation with the help of lexicographers\nand release the curated gold-standard dataset with this paper.", "published": "2021-12-17 14:23:43", "link": "http://arxiv.org/abs/2112.09526v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Topic-Aware Encoding for Extractive Summarization", "abstract": "Document summarization provides an instrument for faster understanding the\ncollection of text documents and has several real-life applications. With the\ngrowth of online text data, numerous summarization models have been proposed\nrecently. The Sequence-to-Sequence (Seq2Seq) based neural summarization model\nis the most widely used in the summarization field due to its high performance.\nThis is because semantic information and structure information in the text is\nadequately considered when encoding. However, the existing extractive\nsummarization models pay little attention to and use the central topic\ninformation to assist the generation of summaries, which leads to models not\nensuring the generated summary under the primary topic. A lengthy document can\nspan several topics, and a single summary cannot do justice to all the topics.\nTherefore, the key to generating a high-quality summary is determining the\ncentral topic and building a summary based on it, especially for a long\ndocument. We propose a topic-aware encoding for document summarization to deal\nwith this issue. This model effectively combines syntactic-level and\ntopic-level information to build a comprehensive sentence representation.\nSpecifically, a neural topic model is added in the neural-based sentence-level\nrepresentation learning to adequately consider the central topic information\nfor capturing the critical content in the original document. The experimental\nresults on three public datasets show that our model outperforms the\nstate-of-the-art models.", "published": "2021-12-17 15:26:37", "link": "http://arxiv.org/abs/2112.09572v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transcribing Natural Languages for The Deaf via Neural Editing Programs", "abstract": "This work studies the task of glossification, of which the aim is to em\ntranscribe natural spoken language sentences for the Deaf (hard-of-hearing)\ncommunity to ordered sign language glosses. Previous sequence-to-sequence\nlanguage models trained with paired sentence-gloss data often fail to capture\nthe rich connections between the two distinct languages, leading to\nunsatisfactory transcriptions. We observe that despite different grammars,\nglosses effectively simplify sentences for the ease of deaf communication,\nwhile sharing a large portion of vocabulary with sentences. This has motivated\nus to implement glossification by executing a collection of editing actions,\ne.g. word addition, deletion, and copying, called editing programs, on their\nnatural spoken language counterparts. Specifically, we design a new neural\nagent that learns to synthesize and execute editing programs, conditioned on\nsentence contexts and partial editing results. The agent is trained to imitate\nminimal editing programs, while exploring more widely the program space via\npolicy gradients to optimize sequence-wise transcription quality. Results show\nthat our approach outperforms previous glossification models by a large margin.", "published": "2021-12-17 16:21:49", "link": "http://arxiv.org/abs/2112.09600v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reasoning Chain Based Adversarial Attack for Multi-hop Question\n  Answering", "abstract": "Recent years have witnessed impressive advances in challenging multi-hop QA\ntasks. However, these QA models may fail when faced with some disturbance in\nthe input text and their interpretability for conducting multi-hop reasoning\nremains uncertain. Previous adversarial attack works usually edit the whole\nquestion sentence, which has limited effect on testing the entity-based\nmulti-hop inference ability. In this paper, we propose a multi-hop reasoning\nchain based adversarial attack method. We formulate the multi-hop reasoning\nchains starting from the query entity to the answer entity in the constructed\ngraph, which allows us to align the question to each reasoning hop and thus\nattack any hop. We categorize the questions into different reasoning types and\nadversarially modify part of the question corresponding to the selected\nreasoning hop to generate the distracting sentence. We test our adversarial\nscheme on three QA models on HotpotQA dataset. The results demonstrate\nsignificant performance reduction on both answer and supporting facts\nprediction, verifying the effectiveness of our reasoning chain based attack\nmethod for multi-hop reasoning models and the vulnerability of them. Our\nadversarial re-training further improves the performance and robustness of\nthese models.", "published": "2021-12-17 18:03:14", "link": "http://arxiv.org/abs/2112.09658v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Explain, Edit, and Understand: Rethinking User Study Design for\n  Evaluating Model Explanations", "abstract": "In attempts to \"explain\" predictions of machine learning models, researchers\nhave proposed hundreds of techniques for attributing predictions to features\nthat are deemed important. While these attributions are often claimed to hold\nthe potential to improve human \"understanding\" of the models, surprisingly\nlittle work explicitly evaluates progress towards this aspiration. In this\npaper, we conduct a crowdsourcing study, where participants interact with\ndeception detection models that have been trained to distinguish between\ngenuine and fake hotel reviews. They are challenged both to simulate the model\non fresh reviews, and to edit reviews with the goal of lowering the probability\nof the originally predicted class. Successful manipulations would lead to an\nadversarial example. During the training (but not the test) phase, input spans\nare highlighted to communicate salience. Through our evaluation, we observe\nthat for a linear bag-of-words model, participants with access to the feature\ncoefficients during training are able to cause a larger reduction in model\nconfidence in the testing phase when compared to the no-explanation control.\nFor the BERT-based classifier, popular local explanations do not improve their\nability to reduce the model confidence over the no-explanation case.\nRemarkably, when the explanation for the BERT model is given by the (global)\nattributions of a linear model trained to imitate the BERT model, people can\neffectively manipulate the model.", "published": "2021-12-17 18:29:56", "link": "http://arxiv.org/abs/2112.09669v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can we Fix the Scope for Coreference? Problems and Solutions for\n  Benchmarks beyond OntoNotes", "abstract": "Current work on automatic coreference resolution has focused on the OntoNotes\nbenchmark dataset, due to both its size and consistency. However many aspects\nof the OntoNotes annotation scheme are not well understood by NLP\npractitioners, including the treatment of generic NPs, noun modifiers,\nindefinite anaphora, predication and more. These often lead to counterintuitive\nclaims, results and system behaviors. This opinion piece aims to highlight some\nof the problems with the OntoNotes rendition of coreference, and to propose a\nway forward relying on three principles: 1. a focus on semantics, not\nmorphosyntax; 2. cross-linguistic generalizability; and 3. a separation of\nidentity and scope, which can resolve old problems involving temporal and modal\ndomain consistency.", "published": "2021-12-17 20:00:54", "link": "http://arxiv.org/abs/2112.09742v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Best of Both Worlds: A Hybrid Approach for Multi-Hop Explanation with\n  Declarative Facts", "abstract": "Language-enabled AI systems can answer complex, multi-hop questions to high\naccuracy, but supporting answers with evidence is a more challenging task which\nis important for the transparency and trustworthiness to users. Prior work in\nthis area typically makes a trade-off between efficiency and accuracy;\nstate-of-the-art deep neural network systems are too cumbersome to be useful in\nlarge-scale applications, while the fastest systems lack reliability. In this\nwork, we integrate fast syntactic methods with powerful semantic methods for\nmulti-hop explanation generation based on declarative facts. Our best system,\nwhich learns a lightweight operation to simulate multi-hop reasoning over\npieces of evidence and fine-tunes language models to re-rank generated\nexplanation chains, outperforms a purely syntactic baseline from prior work by\nup to 7% in gold explanation retrieval rate.", "published": "2021-12-17 20:27:48", "link": "http://arxiv.org/abs/2201.02740v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Neural Architectures for Biological Inter-Sentence Relation Extraction", "abstract": "We introduce a family of deep-learning architectures for inter-sentence\nrelation extraction, i.e., relations where the participants are not necessarily\nin the same sentence. We apply these architectures to an important use case in\nthe biomedical domain: assigning biological context to biochemical events. In\nthis work, biological context is defined as the type of biological system\nwithin which the biochemical event is observed. The neural architectures encode\nand aggregate multiple occurrences of the same candidate context mentions to\ndetermine whether it is the correct context for a particular event mention. We\npropose two broad types of architectures: the first type aggregates multiple\ninstances that correspond to the same candidate context with respect to event\nmention before emitting a classification; the second type independently\nclassifies each instance and uses the results to vote for the final class, akin\nto an ensemble approach. Our experiments show that the proposed neural\nclassifiers are competitive and some achieve better performance than previous\nstate of the art traditional machine learning methods without the need for\nfeature engineering. Our analysis shows that the neural methods particularly\nimprove precision compared to traditional machine learning classifiers and also\ndemonstrates how the difficulty of inter-sentence relation extraction increases\nas the distance between the event and context mentions increase.", "published": "2021-12-17 02:32:58", "link": "http://arxiv.org/abs/2112.09288v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "KGBoost: A Classification-based Knowledge Base Completion Method with\n  Negative Sampling", "abstract": "Knowledge base completion is formulated as a binary classification problem in\nthis work, where an XGBoost binary classifier is trained for each relation\nusing relevant links in knowledge graphs (KGs). The new method, named KGBoost,\nadopts a modularized design and attempts to find hard negative samples so as to\ntrain a powerful classifier for missing link prediction. We conduct experiments\non multiple benchmark datasets, and demonstrate that KGBoost outperforms\nstate-of-the-art methods across most datasets. Furthermore, as compared with\nmodels trained by end-to-end optimization, KGBoost works well under the\nlow-dimensional setting so as to allow a smaller model size.", "published": "2021-12-17 06:19:37", "link": "http://arxiv.org/abs/2112.09340v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Expedition: A System for the Unsupervised Learning of a Hierarchy of\n  Concepts", "abstract": "We present a system for bottom-up cumulative learning of myriad concepts\ncorresponding to meaningful character strings, and their part-related and\nprediction edges. The learning is self-supervised in that the concepts\ndiscovered are used as predictors as well as targets of prediction. We devise\nan objective for segmenting with the learned concepts, derived from comparing\nto a baseline prediction system, that promotes making and using larger\nconcepts, which in turn allows for predicting larger spans of text, and we\ndescribe a simple technique to promote exploration, i.e. trying out newly\ngenerated concepts in the segmentation process. We motivate and explain a\nlayering of the concepts, to help separate the (conditional) distributions\nlearnt among concepts. The layering of the concepts roughly corresponds to a\npart-whole concept hierarchy. With rudimentary segmentation and learning\nalgorithms, the system is promising in that it acquires many concepts (tens of\nthousands in our small-scale experiments), and it learns to segment text well:\nwhen fed with English text with spaces removed, starting at the character\nlevel, much of what is learned respects word or phrase boundaries, and over\ntime the average number of \"bad\" splits within segmentations, i.e. splits\ninside words, decreases as larger concepts are discovered and the system learns\nwhen to use them during segmentation. We report on promising experiments when\nthe input text is converted to binary and the system begins with only two\nconcepts, \"0\" and \"1\". The system is transparent, in the sense that it is easy\nto tell what the concepts learned correspond to, and which ones are active in a\nsegmentation, or how the system \"sees\" its input. We expect this framework to\nbe extensible and we discuss the current limitations and a number of directions\nfor enhancing the learning and inference capabilities.", "published": "2021-12-17 06:49:18", "link": "http://arxiv.org/abs/2112.09348v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A Multimodal Approach for Automatic Mania Assessment in Bipolar Disorder", "abstract": "Bipolar disorder is a mental health disorder that causes mood swings that\nrange from depression to mania. Diagnosis of bipolar disorder is usually done\nbased on patient interviews, and reports obtained from the caregivers of the\npatients. Subsequently, the diagnosis depends on the experience of the expert,\nand it is possible to have confusions of the disorder with other mental\ndisorders. Automated processes in the diagnosis of bipolar disorder can help\nproviding quantitative indicators, and allow easier observations of the\npatients for longer periods. Furthermore, the need for remote treatment and\ndiagnosis became especially important during the COVID-19 pandemic. In this\nthesis, we create a multimodal decision system based on recordings of the\npatient in acoustic, linguistic, and visual modalities. The system is trained\non the Bipolar Disorder corpus. Comprehensive analysis of unimodal and\nmultimodal systems, as well as various fusion techniques are performed. Besides\nprocessing entire patient sessions using unimodal features, a task-level\ninvestigation of the clips is studied. Using acoustic, linguistic, and visual\nfeatures in a multimodal fusion system, we achieved a 64.8% unweighted average\nrecall score, which improves the state-of-the-art performance achieved on this\ndataset.", "published": "2021-12-17 12:09:01", "link": "http://arxiv.org/abs/2112.09467v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sparsifying Sparse Representations for Passage Retrieval by Top-$k$\n  Masking", "abstract": "Sparse lexical representation learning has demonstrated much progress in\nimproving passage retrieval effectiveness in recent models such as DeepImpact,\nuniCOIL, and SPLADE. This paper describes a straightforward yet effective\napproach for sparsifying lexical representations for passage retrieval,\nbuilding on SPLADE by introducing a top-$k$ masking scheme to control sparsity\nand a self-learning method to coax masked representations to mimic unmasked\nrepresentations. A basic implementation of our model is competitive with more\nsophisticated approaches and achieves a good balance between effectiveness and\nefficiency. The simplicity of our methods opens the door for future\nexplorations in lexical representation learning for passage retrieval.", "published": "2021-12-17 16:59:40", "link": "http://arxiv.org/abs/2112.09628v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Sublinear Time Approximation of Text Similarity Matrices", "abstract": "We study algorithms for approximating pairwise similarity matrices that arise\nin natural language processing. Generally, computing a similarity matrix for\n$n$ data points requires $\\Omega(n^2)$ similarity computations. This quadratic\nscaling is a significant bottleneck, especially when similarities are computed\nvia expensive functions, e.g., via transformer models. Approximation methods\nreduce this quadratic complexity, often by using a small subset of exactly\ncomputed similarities to approximate the remainder of the complete pairwise\nsimilarity matrix.\n  Significant work focuses on the efficient approximation of positive\nsemidefinite (PSD) similarity matrices, which arise e.g., in kernel methods.\nHowever, much less is understood about indefinite (non-PSD) similarity\nmatrices, which often arise in NLP. Motivated by the observation that many of\nthese matrices are still somewhat close to PSD, we introduce a generalization\nof the popular Nystr\\\"{o}m method to the indefinite setting. Our algorithm can\nbe applied to any similarity matrix and runs in sublinear time in the size of\nthe matrix, producing a rank-$s$ approximation with just $O(ns)$ similarity\ncomputations.\n  We show that our method, along with a simple variant of CUR decomposition,\nperforms very well in approximating a variety of similarity matrices arising in\nNLP tasks. We demonstrate high accuracy of the approximated similarity matrices\nin the downstream tasks of document classification, sentence similarity, and\ncross-document coreference.", "published": "2021-12-17 17:04:34", "link": "http://arxiv.org/abs/2112.09631v3", "categories": ["cs.LG", "cs.CL", "F.2.1"], "primary_category": "cs.LG"}
{"title": "Incomplete Knowledge Graph Alignment", "abstract": "Knowledge graph (KG) alignment - the task of recognizing entities referring\nto the same thing in different KGs - is recognized as one of the most important\noperations in the field of KG construction and completion. However, existing\nalignment techniques often assume that the input KGs are complete and\nisomorphic, which is not true due to the real-world heterogeneity in the\ndomain, size, and sparsity. In this work, we address the problem of aligning\nincomplete KGs with representation learning. Our KG embedding framework\nexploits two feature channels: transitivity-based and proximity-based. The\nformer captures the consistency constraints between entities via translation\npaths, while the latter captures the neighbourhood structure of KGs via\nattention guided relation-aware graph neural network. The two feature channels\nare jointly learned to exchange important features between the input KGs while\nenforcing the output representations of the input KGs in the same embedding\nspace. Also, we develop a missing links detector that discovers and recovers\nthe missing links in the input KGs during the training process, which helps\nmitigate the incompleteness issue and thus improve the compatibility of the\nlearned representations. The embeddings then are fused to generate the\nalignment result, and the high-confidence matched node pairs are updated to the\npre-aligned supervision data to improve the embeddings gradually. Empirical\nresults show that our model is more accurate than the SOTA and is robust\nagainst different levels of incompleteness.", "published": "2021-12-17 00:41:28", "link": "http://arxiv.org/abs/2112.09266v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Overview of the HASOC Subtrack at FIRE 2021: Hate Speech and Offensive\n  Content Identification in English and Indo-Aryan Languages", "abstract": "The widespread of offensive content online such as hate speech poses a\ngrowing societal problem. AI tools are necessary for supporting the moderation\nprocess at online platforms. For the evaluation of these identification tools,\ncontinuous experimentation with data sets in different languages are necessary.\nThe HASOC track (Hate Speech and Offensive Content Identification) is dedicated\nto develop benchmark data for this purpose. This paper presents the HASOC\nsubtrack for English, Hindi, and Marathi. The data set was assembled from\nTwitter. This subtrack has two sub-tasks. Task A is a binary classification\nproblem (Hate and Not Offensive) offered for all three languages. Task B is a\nfine-grained classification problem for three classes (HATE) Hate speech,\nOFFENSIVE and PROFANITY offered for English and Hindi. Overall, 652 runs were\nsubmitted by 65 teams. The performance of the best classification algorithms\nfor task A are F1 measures 0.91, 0.78 and 0.83 for Marathi, Hindi and English,\nrespectively. This overview presents the tasks and the data development as well\nas the detailed results. The systems submitted to the competition applied a\nvariety of technologies. The best performing algorithms were mainly variants of\ntransformer architectures.", "published": "2021-12-17 03:28:54", "link": "http://arxiv.org/abs/2112.09301v1", "categories": ["cs.CL", "cs.AI", "cs.SI"], "primary_category": "cs.CL"}
{"title": "WebGPT: Browser-assisted question-answering with human feedback", "abstract": "We fine-tune GPT-3 to answer long-form questions using a text-based\nweb-browsing environment, which allows the model to search and navigate the\nweb. By setting up the task so that it can be performed by humans, we are able\nto train models on the task using imitation learning, and then optimize answer\nquality with human feedback. To make human evaluation of factual accuracy\neasier, models must collect references while browsing in support of their\nanswers. We train and evaluate our models on ELI5, a dataset of questions asked\nby Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior\ncloning, and then performing rejection sampling against a reward model trained\nto predict human preferences. This model's answers are preferred by humans 56%\nof the time to those of our human demonstrators, and 69% of the time to the\nhighest-voted answer from Reddit.", "published": "2021-12-17 05:43:43", "link": "http://arxiv.org/abs/2112.09332v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Continual Learning for Monolingual End-to-End Automatic Speech\n  Recognition", "abstract": "Adapting Automatic Speech Recognition (ASR) models to new domains results in\na deterioration of performance on the original domain(s), a phenomenon called\nCatastrophic Forgetting (CF). Even monolingual ASR models cannot be extended to\nnew accents, dialects, topics, etc. without suffering from CF, making them\nunable to be continually enhanced without storing all past data. Fortunately,\nContinual Learning (CL) methods, which aim to enable continual adaptation while\novercoming CF, can be used. In this paper, we implement an extensive number of\nCL methods for End-to-End ASR and test and compare their ability to extend a\nmonolingual Hybrid CTC-Transformer model across four new tasks. We find that\nthe best performing CL method closes the gap between the fine-tuned model\n(lower bound) and the model trained jointly on all tasks (upper bound) by more\nthan 40%, while requiring access to only 0.6% of the original data.", "published": "2021-12-17 10:47:17", "link": "http://arxiv.org/abs/2112.09427v4", "categories": ["eess.AS", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "eess.AS"}
{"title": "An ensemble deep learning technique for detecting suicidal ideation from\n  posts in social media platforms", "abstract": "Suicidal ideation detection from social media is an evolving research with\ngreat challenges. Many of the people who have the tendency to suicide share\ntheir thoughts and opinions through social media platforms. As part of many\nresearches it is observed that the publicly available posts from social media\ncontain valuable criteria to effectively detect individuals with suicidal\nthoughts. The most difficult part to prevent suicide is to detect and\nunderstand the complex risk factors and warning signs that may lead to suicide.\nThis can be achieved by identifying the sudden changes in a user behavior\nautomatically. Natural language processing techniques can be used to collect\nbehavioral and textual features from social media interactions and these\nfeatures can be passed to a specially designed framework to detect anomalies in\nhuman interactions that are indicators of suicidal intentions. We can achieve\nfast detection of suicidal ideation using deep learning and/or machine learning\nbased classification approaches. For such a purpose, we can employ the\ncombination of LSTM and CNN models to detect such emotions from posts of the\nusers. In order to improve the accuracy, some approaches like using more data\nfor training, using attention model to improve the efficiency of existing\nmodels etc. could be done. This paper proposes a LSTM-Attention-CNN combined\nmodel to analyze social media submissions to detect any underlying suicidal\nintentions. During evaluations, the proposed model demonstrated an accuracy of\n90.3 percent and an F1-score of 92.6 percent, which is greater than the\nbaseline models.", "published": "2021-12-17 15:34:03", "link": "http://arxiv.org/abs/2112.10609v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.IR"}
{"title": "JTubeSpeech: corpus of Japanese speech collected from YouTube for speech\n  recognition and speaker verification", "abstract": "In this paper, we construct a new Japanese speech corpus called\n\"JTubeSpeech.\" Although recent end-to-end learning requires large-size speech\ncorpora, open-sourced such corpora for languages other than English have not\nyet been established. In this paper, we describe the construction of a corpus\nfrom YouTube videos and subtitles for speech recognition and speaker\nverification. Our method can automatically filter the videos and subtitles with\nalmost no language-dependent processes. We consistently employ Connectionist\nTemporal Classification (CTC)-based techniques for automatic speech recognition\n(ASR) and a speaker variation-based method for automatic speaker verification\n(ASV). We build 1) a large-scale Japanese ASR benchmark with more than 1,300\nhours of data and 2) 900 hours of data for Japanese ASV.", "published": "2021-12-17 05:09:44", "link": "http://arxiv.org/abs/2112.09323v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Dialog+ in Broadcasting: First Field Tests Using Deep-Learning-Based\n  Dialogue Enhancement", "abstract": "Difficulties in following speech due to loud background sounds are common in\nbroadcasting. Object-based audio, e.g., MPEG-H Audio solves this problem by\nproviding a user-adjustable speech level. While object-based audio is gaining\nmomentum, transitioning to it requires time and effort. Also, lots of content\nexists, produced and archived outside the object-based workflows. To address\nthis, Fraunhofer IIS has developed a deep-learning solution called Dialog+,\ncapable of enabling speech level personalization also for content with only the\nfinal audio tracks available. This paper reports on public field tests\nevaluating Dialog+, conducted together with Westdeutscher Rundfunk (WDR) and\nBayerischer Rundfunk (BR), starting from September 2020. To our knowledge,\nthese are the first large-scale tests of this kind. As part of one of these, a\nsurvey with more than 2,000 participants showed that 90% of the people above 60\nyears old have problems in understanding speech in TV \"often\" or \"very often\".\nOverall, 83% of the participants liked the possibility to switch to Dialog+,\nincluding those who do not normally struggle with speech intelligibility.\nDialog+ introduces a clear benefit for the audience, filling the gap between\nobject-based broadcasting and traditionally produced material.", "published": "2021-12-17 13:04:42", "link": "http://arxiv.org/abs/2112.09494v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Linguistic and Gender Variation in Speech Emotion Recognition using\n  Spectral Features", "abstract": "This work explores the effect of gender and linguistic-based vocal variations\non the accuracy of emotive expression classification. Emotive expressions are\nconsidered from the perspective of spectral features in speech (Mel-frequency\nCepstral Coefficient, Melspectrogram, Spectral Contrast). Emotions are\nconsidered from the perspective of Basic Emotion Theory. A convolutional neural\nnetwork is utilised to classify emotive expressions in emotive audio datasets\nin English, German, and Italian. Vocal variations for spectral features\nassessed by (i) a comparative analysis identifying suitable spectral features,\n(ii) the classification performance for mono, multi and cross-lingual emotive\ndata and (iii) an empirical evaluation of a machine learning model to assess\nthe effects of gender and linguistic variation on classification accuracy. The\nresults showed that spectral features provide a potential avenue for increasing\nemotive expression classification. Additionally, the accuracy of emotive\nexpression classification was high within mono and cross-lingual emotive data,\nbut poor in multi-lingual data. Similarly, there were differences in\nclassification accuracy between gender populations. These results demonstrate\nthe importance of accounting for population differences to enable accurate\nspeech emotion recognition.", "published": "2021-12-17 16:14:51", "link": "http://arxiv.org/abs/2112.09596v2", "categories": ["cs.SD", "eess.AS", "91C99", "I.2; J.2"], "primary_category": "cs.SD"}
{"title": "MIDI-DDSP: Detailed Control of Musical Performance via Hierarchical\n  Modeling", "abstract": "Musical expression requires control of both what notes are played, and how\nthey are performed. Conventional audio synthesizers provide detailed expressive\ncontrols, but at the cost of realism. Black-box neural audio synthesis and\nconcatenative samplers can produce realistic audio, but have few mechanisms for\ncontrol. In this work, we introduce MIDI-DDSP a hierarchical model of musical\ninstruments that enables both realistic neural audio synthesis and detailed\nuser control. Starting from interpretable Differentiable Digital Signal\nProcessing (DDSP) synthesis parameters, we infer musical notes and high-level\nproperties of their expressive performance (such as timbre, vibrato, dynamics,\nand articulation). This creates a 3-level hierarchy (notes, performance,\nsynthesis) that affords individuals the option to intervene at each level, or\nutilize trained priors (performance given notes, synthesis given performance)\nfor creative assistance. Through quantitative experiments and listening tests,\nwe demonstrate that this hierarchy can reconstruct high-fidelity audio,\naccurately predict performance attributes for a note sequence, independently\nmanipulate the attributes of a given performance, and as a complete system,\ngenerate realistic audio from a novel note sequence. By utilizing an\ninterpretable hierarchy, with multiple levels of granularity, MIDI-DDSP opens\nthe door to assistive tools to empower individuals across a diverse range of\nmusical experience.", "published": "2021-12-17 04:15:42", "link": "http://arxiv.org/abs/2112.09312v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Interpreting Audiograms with Multi-stage Neural Networks", "abstract": "Audiograms are a particular type of line charts representing individuals'\nhearing level at various frequencies. They are used by audiologists to diagnose\nhearing loss, and further select and tune appropriate hearing aids for\ncustomers. There have been several projects such as Autoaudio that aim to\naccelerate this process through means of machine learning. But all existing\nmodels at their best can only detect audiograms in images and classify them\ninto general categories. They are unable to extract hearing level information\nfrom detected audiograms by interpreting the marks, axis, and lines. To address\nthis issue, we propose a Multi-stage Audiogram Interpretation Network (MAIN)\nthat directly reads hearing level data from photos of audiograms. We also\nestablished Open Audiogram, an open dataset of audiogram images with\nannotations of marks and axes on which we trained and evaluated our proposed\nmodel. Experiments show that our model is feasible and reliable.", "published": "2021-12-17 07:27:39", "link": "http://arxiv.org/abs/2112.09357v1", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Discretization and Re-synthesis: an alternative method to solve the\n  Cocktail Party Problem", "abstract": "Deep learning based models have significantly improved the performance of\nspeech separation with input mixtures like the cocktail party. Prominent\nmethods (e.g., frequency-domain and time-domain speech separation) usually\nbuild regression models to predict the ground-truth speech from the mixture,\nusing the masking-based design and the signal-level loss criterion (e.g., MSE\nor SI-SNR). This study demonstrates, for the first time, that the\nsynthesis-based approach can also perform well on this problem, with great\nflexibility and strong potential. Specifically, we propose a novel speech\nseparation/enhancement model based on the recognition of discrete symbols, and\nconvert the paradigm of the speech separation/enhancement related tasks from\nregression to classification. By utilizing the synthesis model with the input\nof discrete symbols, after the prediction of discrete symbol sequence, each\ntarget speech could be re-synthesized. Evaluation results based on the\nWSJ0-2mix and VCTK-noisy corpora in various settings show that our proposed\nmethod can steadily synthesize the separated speech with high speech quality\nand without any interference, which is difficult to avoid in regression-based\nmethods. In addition, with negligible loss of listening quality, the speaker\nconversion of enhanced/separated speech could be easily realized through our\nmethod.", "published": "2021-12-17 08:35:40", "link": "http://arxiv.org/abs/2112.09382v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio Retrieval with Natural Language Queries: A Benchmark Study", "abstract": "The objectives of this work are cross-modal text-audio and audio-text\nretrieval, in which the goal is to retrieve the audio content from a pool of\ncandidates that best matches a given written description and vice versa.\nText-audio retrieval enables users to search large databases through an\nintuitive interface: they simply issue free-form natural language descriptions\nof the sound they would like to hear. To study the tasks of text-audio and\naudio-text retrieval, which have received limited attention in the existing\nliterature, we introduce three challenging new benchmarks. We first construct\ntext-audio and audio-text retrieval benchmarks from the AudioCaps and Clotho\naudio captioning datasets. Additionally, we introduce the SoundDescs benchmark,\nwhich consists of paired audio and natural language descriptions for a diverse\ncollection of sounds that are complementary to those found in AudioCaps and\nClotho. We employ these three benchmarks to establish baselines for cross-modal\ntext-audio and audio-text retrieval, where we demonstrate the benefits of\npre-training on diverse audio tasks. We hope that our benchmarks will inspire\nfurther research into audio retrieval with free-form text queries. Code, audio\nfeatures for all datasets used, and the SoundDescs dataset are publicly\navailable at https://github.com/akoepke/audio-retrieval-benchmark.", "published": "2021-12-17 10:25:14", "link": "http://arxiv.org/abs/2112.09418v2", "categories": ["eess.AS", "cs.IR", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Soundify: Matching Sound Effects to Video", "abstract": "In the art of video editing, sound helps add character to an object and\nimmerse the viewer within a space. Through formative interviews with\nprofessional editors (N=10), we found that the task of adding sounds to video\ncan be challenging. This paper presents Soundify, a system that assists editors\nin matching sounds to video. Given a video, Soundify identifies matching\nsounds, synchronizes the sounds to the video, and dynamically adjusts panning\nand volume to create spatial audio. In a human evaluation study (N=889), we\nshow that Soundify is capable of matching sounds to video out-of-the-box for a\ndiverse range of audio categories. In a within-subjects expert study (N=12), we\ndemonstrate the usefulness of Soundify in helping video editors match sounds to\nvideo with lighter workload, reduced task completion time, and improved\nusability.", "published": "2021-12-17 19:22:01", "link": "http://arxiv.org/abs/2112.09726v4", "categories": ["cs.SD", "cs.CV", "cs.HC", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
