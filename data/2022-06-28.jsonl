{"title": "Few-Shot Fine-Grained Entity Typing with Automatic Label Interpretation\n  and Instance Generation", "abstract": "We study the problem of few-shot Fine-grained Entity Typing (FET), where only\na few annotated entity mentions with contexts are given for each entity type.\nRecently, prompt-based tuning has demonstrated superior performance to standard\nfine-tuning in few-shot scenarios by formulating the entity type classification\ntask as a ''fill-in-the-blank'' problem. This allows effective utilization of\nthe strong language modeling capability of Pre-trained Language Models (PLMs).\nDespite the success of current prompt-based tuning approaches, two major\nchallenges remain: (1) the verbalizer in prompts is either manually designed or\nconstructed from external knowledge bases, without considering the target\ncorpus and label hierarchy information, and (2) current approaches mainly\nutilize the representation power of PLMs, but have not explored their\ngeneration power acquired through extensive general-domain pre-training. In\nthis work, we propose a novel framework for few-shot FET consisting of two\nmodules: (1) an entity type label interpretation module automatically learns to\nrelate type labels to the vocabulary by jointly leveraging few-shot instances\nand the label hierarchy, and (2) a type-based contextualized instance generator\nproduces new instances based on given instances to enlarge the training set for\nbetter generalization. On three benchmark datasets, our model outperforms\nexisting methods by significant margins. Code can be found at\nhttps://github.com/teapot123/Fine-Grained-Entity-Typing.", "published": "2022-06-28 04:05:40", "link": "http://arxiv.org/abs/2206.13746v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CC-Riddle: A Question Answering Dataset of Chinese Character Riddles", "abstract": "The Chinese character riddle is a unique form of cultural entertainment\nspecific to the Chinese language. It typically comprises two parts: the riddle\ndescription and the solution. The solution to the riddle is a single character,\nwhile the riddle description primarily describes the glyph of the solution,\noccasionally supplemented with its explanation and pronunciation. Solving\nChinese character riddles is a challenging task that demands understanding of\ncharacter glyph, general knowledge, and a grasp of figurative language. In this\npaper, we construct a \\textbf{C}hinese \\textbf{C}haracter riddle dataset named\nCC-Riddle, which covers the majority of common simplified Chinese characters.\nThe construction process is a combination of web crawling, language model\ngeneration and manual filtering. In generation stage, we input the Chinese\nphonetic alphabet, glyph and meaning of the solution character into the\ngeneration model, which then produces multiple riddle descriptions. The\ngenerated riddles are then manually filtered and the final CC-Riddle dataset is\ncomposed of both human-written riddles and these filtered, generated riddles.\nIn order to assess the performance of language models on the task of solving\ncharacter riddles, we use retrieval-based, generative and multiple-choice QA\nstrategies to test three language models: BERT, ChatGPT and ChatGLM. The test\nresults reveal that current language models still struggle to solve Chinese\ncharacter riddles. CC-Riddle is publicly available at\n\\url{https://github.com/pku0xff/CC-Riddle}.", "published": "2022-06-28 06:23:13", "link": "http://arxiv.org/abs/2206.13778v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Dependency Parsing with Backtracking using Deep Reinforcement Learning", "abstract": "Greedy algorithms for NLP such as transition based parsing are prone to error\npropagation. One way to overcome this problem is to allow the algorithm to\nbacktrack and explore an alternative solution in cases where new evidence\ncontradicts the solution explored so far. In order to implement such a\nbehavior, we use reinforcement learning and let the algorithm backtrack in\ncases where such an action gets a better reward than continuing to explore the\ncurrent solution. We test this idea on both POS tagging and dependency parsing\nand show that backtracking is an effective means to fight against error\npropagation.", "published": "2022-06-28 11:45:42", "link": "http://arxiv.org/abs/2206.13914v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analysis of Individual Conversational Volatility in Tandem\n  Telecollaboration for Second Language Learning", "abstract": "Second language learning can be enabled by tandem collaboration where\nstudents are grouped into video conference calls while learning the native\nlanguage of other student(s) on the calls. This places students in an online\nenvironment where the more outgoing can actively contribute and engage in\ndialogue while those more shy and unsure of their second language skills can\nsit back and coast through the calls. We have built and deployed the L2L system\nwhich records timings of conversational utterances from all participants in a\ncall. We generate visualisations including participation rates and timelines\nfor each student in each call and present these on a dashboard. We have\nrecently developed a measure called personal conversational volatility for how\ndynamic has been each student's contribution to the dialogue in each call. We\npresent an analysis of conversational volatility measures for a sample of 19\nindividual English-speaking students from our University who are learning\nFrenchm, in each of 86 tandem telecollaboration calls over one teaching\nsemester. Our analysis shows there is a need to look into the nature of the\ninteractions and see if the choices of discussion topics assigned to them were\ntoo difficult for some students and that may have influenced their engagement\nin some way.", "published": "2022-06-28 12:34:00", "link": "http://arxiv.org/abs/2206.13965v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MACSA: A Multimodal Aspect-Category Sentiment Analysis Dataset with\n  Multimodal Fine-grained Aligned Annotations", "abstract": "Multimodal fine-grained sentiment analysis has recently attracted increasing\nattention due to its broad applications. However, the existing multimodal\nfine-grained sentiment datasets most focus on annotating the fine-grained\nelements in text but ignore those in images, which leads to the fine-grained\nelements in visual content not receiving the full attention they deserve. In\nthis paper, we propose a new dataset, the Multimodal Aspect-Category Sentiment\nAnalysis (MACSA) dataset, which contains more than 21K text-image pairs. The\ndataset provides fine-grained annotations for both textual and visual content\nand firstly uses the aspect category as the pivot to align the fine-grained\nelements between the two modalities. Based on our dataset, we propose the\nMultimodal ACSA task and a multimodal graph-based aligned model (MGAM), which\nadopts a fine-grained cross-modal fusion method. Experimental results show that\nour method can facilitate the baseline comparison for future research on this\ncorpus. We will make the dataset and code publicly available.", "published": "2022-06-28 12:49:16", "link": "http://arxiv.org/abs/2206.13969v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Joint Generator-Ranker Learning for Natural Language Generation", "abstract": "Generate-then-rank is a widely used mechanism for text generation, where a\ngenerator produces multiple text candidates and a ranker chooses the best one\namong the text candidates. However, existing methods usually train the\ngenerator and the ranker individually, neglecting the mutual feedback that\ncould further enhance the generation quality. To tackle this limitation, we\npropose JGR, a novel joint training algorithm that integrates the generator and\nthe ranker in a single framework. JGR optimizes the generator with a hybrid\nobjective that combines data likelihood and ranker reward, and trains the\nranker with a contrastive loss that compares the generator outputs. By\niteratively updating the generator and the ranker, JGR can effectively\nharmonize their learning and enhance their quality jointly. We evaluate JGR on\nvarious text generation tasks and demonstrate that it surpasses existing\nmethods on four public datasets across three common generation scenarios. Our\ncode and models are publicly available at\nhttps://github.com/microsoft/ProphetNet/tree/master/JGR.", "published": "2022-06-28 12:58:30", "link": "http://arxiv.org/abs/2206.13974v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Proton: Probing Schema Linking Information from Pre-trained Language\n  Models for Text-to-SQL Parsing", "abstract": "The importance of building text-to-SQL parsers which can be applied to new\ndatabases has long been acknowledged, and a critical step to achieve this goal\nis schema linking, i.e., properly recognizing mentions of unseen columns or\ntables when generating SQLs. In this work, we propose a novel framework to\nelicit relational structures from large-scale pre-trained language models\n(PLMs) via a probing procedure based on Poincar\\'e distance metric, and use the\ninduced relations to augment current graph-based parsers for better schema\nlinking. Compared with commonly-used rule-based methods for schema linking, we\nfound that probing relations can robustly capture semantic correspondences,\neven when surface forms of mentions and entities differ. Moreover, our probing\nprocedure is entirely unsupervised and requires no additional parameters.\nExtensive experiments show that our framework sets new state-of-the-art\nperformance on three benchmarks. We empirically verify that our probing\nprocedure can indeed find desired relational structures through qualitative\nanalysis. Our code can be found at\nhttps://github.com/AlibabaResearch/DAMO-ConvAI.", "published": "2022-06-28 14:05:25", "link": "http://arxiv.org/abs/2206.14017v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Lexical Gender Inference: A Scalable Methodology using Online\n  Databases", "abstract": "This paper presents a new method for automatically detecting words with\nlexical gender in large-scale language datasets. Currently, the evaluation of\ngender bias in natural language processing relies on manually compiled lexicons\nof gendered expressions, such as pronouns ('he', 'she', etc.) and nouns with\nlexical gender ('mother', 'boyfriend', 'policewoman', etc.). However, manual\ncompilation of such lists can lead to static information if they are not\nperiodically updated and often involve value judgments by individual annotators\nand researchers. Moreover, terms not included in the list fall out of the range\nof analysis. To address these issues, we devised a scalable, dictionary-based\nmethod to automatically detect lexical gender that can provide a dynamic,\nup-to-date analysis with high coverage. Our approach reaches over 80% accuracy\nin determining the lexical gender of nouns retrieved randomly from a Wikipedia\nsample and when testing on a list of gendered words used in previous research.", "published": "2022-06-28 14:57:26", "link": "http://arxiv.org/abs/2206.14055v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Simplifying Dataflow Dialogue Design", "abstract": "In \\citep{andreas2020task-oriented}, a dataflow (DF) based dialogue system\nwas introduced, showing clear advantages compared to many commonly used current\nsystems. This was accompanied by the release of SMCalFlow, a practically\nrelevant, manually annotated dataset, more detailed and much larger than any\ncomparable dialogue dataset. Despite these remarkable contributions, the\ncommunity has not shown further interest in this direction. What are the\nreasons for this lack of interest? And how can the community be encouraged to\nengage in research in this direction?\n  One explanation may be the perception that this approach is too complex -\nboth the the annotation and the system. This paper argues that this perception\nis wrong: 1) Suggestions for a simplified format for the annotation of the\ndataset are presented, 2) An implementation of the DF execution engine is\nreleased\\footnote{https://github.com/telepathylabsai/OpenDF}, which can serve\nas a sandbox allowing researchers to easily implement, and experiment with, new\nDF dialogue designs. The hope is that these contributions will help engage more\npractitioners in exploring new ideas and designs for DF based dialogue systems.", "published": "2022-06-28 16:36:08", "link": "http://arxiv.org/abs/2206.14125v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Creation and Analysis of an International Corpus of Privacy Laws", "abstract": "The landscape of privacy laws and regulations around the world is complex and\never-changing. National and super-national laws, agreements, decrees, and other\ngovernment-issued rules form a patchwork that companies must follow to operate\ninternationally. To examine the status and evolution of this patchwork, we\nintroduce the Government Privacy Instructions Corpus, or GPI Corpus, of 1,043\nprivacy laws, regulations, and guidelines, covering 182 jurisdictions. This\ncorpus enables a large-scale quantitative and qualitative examination of legal\nfoci on privacy. We examine the temporal distribution of when GPIs were created\nand illustrate the dramatic increase in privacy legislation over the past 50\nyears, although a finer-grained examination reveals that the rate of increase\nvaries depending on the personal data types that GPIs address. Our exploration\nalso demonstrates that most privacy laws respectively address relatively few\npersonal data types, showing that comprehensive privacy legislation remains\nrare. Additionally, topic modeling results show the prevalence of common themes\nin GPIs, such as finance, healthcare, and telecommunications. Finally, we\nrelease the corpus to the research community to promote further study.", "published": "2022-06-28 17:36:12", "link": "http://arxiv.org/abs/2206.14169v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BertNet: Harvesting Knowledge Graphs with Arbitrary Relations from\n  Pretrained Language Models", "abstract": "It is crucial to automatically construct knowledge graphs (KGs) of diverse\nnew relations to support knowledge discovery and broad applications. Previous\nKG construction methods, based on either crowdsourcing or text mining, are\noften limited to a small predefined set of relations due to manual cost or\nrestrictions in text corpus. Recent research proposed to use pretrained\nlanguage models (LMs) as implicit knowledge bases that accept knowledge queries\nwith prompts. Yet, the implicit knowledge lacks many desirable properties of a\nfull-scale symbolic KG, such as easy access, navigation, editing, and quality\nassurance. In this paper, we propose a new approach of harvesting massive KGs\nof arbitrary relations from pretrained LMs. With minimal input of a relation\ndefinition (a prompt and a few shot of example entity pairs), the approach\nefficiently searches in the vast entity pair space to extract diverse accurate\nknowledge of the desired relation. We develop an effective search-and-rescore\nmechanism for improved efficiency and accuracy. We deploy the approach to\nharvest KGs of over 400 new relations from different LMs. Extensive human and\nautomatic evaluations show our approach manages to extract diverse accurate\nknowledge, including tuples of complex relations (e.g., \"A is capable of but\nnot good at B\"). The resulting KGs as a symbolic interpretation of the source\nLMs also reveal new insights into the LMs' knowledge capacities.", "published": "2022-06-28 19:46:29", "link": "http://arxiv.org/abs/2206.14268v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NERDA-Con: Extending NER models for Continual Learning -- Integrating\n  Distinct Tasks and Updating Distribution Shifts", "abstract": "With increasing applications in areas such as biomedical information\nextraction pipelines and social media analytics, Named Entity Recognition (NER)\nhas become an indispensable tool for knowledge extraction. However, with the\ngradual shift in language structure and vocabulary, NERs are plagued with\ndistribution shifts, making them redundant or not as profitable without\nre-training. Re-training NERs based on Large Language Models (LLMs) from\nscratch over newly acquired data poses economic disadvantages. In contrast,\nre-training only with newly acquired data will result in Catastrophic\nForgetting of previously acquired knowledge. Therefore, we propose NERDA-Con, a\npipeline for training NERs with LLM bases by incorporating the concept of\nElastic Weight Consolidation (EWC) into the NER fine-tuning NERDA pipeline. As\nwe believe our work has implications to be utilized in the pipeline of\ncontinual learning and NER, we open-source our code as well as provide the\nfine-tuning library of the same name NERDA-Con at\nhttps://github.com/SupritiVijay/NERDA-Con and\nhttps://pypi.org/project/NERDA-Con/.", "published": "2022-06-28 03:22:55", "link": "http://arxiv.org/abs/2206.14607v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Principal Phrase Mining", "abstract": "Extracting frequent words from a collection of texts is commonly performed in\nmany subjects. However, as useful as it is to obtain a collection of commonly\noccurring words from texts, there is a need for more specific information to be\nobtained from texts in the form of most commonly occurring phrases. Despite\nthis need, extracting frequent phrases is not commonly done due to inherent\ncomplications, the most significant being double-counting. Double-counting\noccurs when words or phrases are counted when they appear inside longer phrases\nthat themselves are also counted, resulting in a selection of mostly\nmeaningless phrases that are frequent only because they occur inside frequent\nsuper phrases. Several papers have been written on phrase mining that describe\nsolutions to this issue; however, they either require a list of so-called\nquality phrases to be available to the extracting process, or they require\nhuman interaction to identify those quality phrases during the process. We\npresent here a method that eliminates double-counting via a unique\nrectification process that does not require lists of quality phrases. In the\ncontext of a set of texts, we define a principal phrase as a phrase that does\nnot cross punctuation marks, does not start with a stop word, with the\nexception of the stop words \"not\" and \"no\", does not end with a stop word, is\nfrequent within those texts without being double counted, and is meaningful to\nthe user. Our method identifies such principal phrases independently without\nhuman input, and enables their extraction from any texts within a reasonable\namount of time.", "published": "2022-06-28 04:11:31", "link": "http://arxiv.org/abs/2206.13748v2", "categories": ["cs.CL", "stat.AP"], "primary_category": "cs.CL"}
{"title": "Adaptive Multi-view Rule Discovery for Weakly-Supervised Compatible\n  Products Prediction", "abstract": "On e-commerce platforms, predicting if two products are compatible with each\nother is an important functionality to achieve trustworthy product\nrecommendation and search experience for consumers. However, accurately\npredicting product compatibility is difficult due to the heterogeneous product\ndata and the lack of manually curated training data. We study the problem of\ndiscovering effective labeling rules that can enable weakly-supervised product\ncompatibility prediction. We develop AMRule, a multi-view rule discovery\nframework that can (1) adaptively and iteratively discover novel rulers that\ncan complement the current weakly-supervised model to improve compatibility\nprediction; (2) discover interpretable rules from both structured attribute\ntables and unstructured product descriptions. AMRule adaptively discovers\nlabeling rules from large-error instances via a boosting-style strategy, the\nhigh-quality rules can remedy the current model's weak spots and refine the\nmodel iteratively. For rule discovery from structured product attributes, we\ngenerate composable high-order rules from decision trees; and for rule\ndiscovery from unstructured product descriptions, we generate prompt-based\nrules from a pre-trained language model. Experiments on 4 real-world datasets\nshow that AMRule outperforms the baselines by 5.98% on average and improves\nrule quality and rule proposal efficiency.", "published": "2022-06-28 04:11:58", "link": "http://arxiv.org/abs/2206.13749v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "On the Impact of Noises in Crowd-Sourced Data for Speech Translation", "abstract": "Training speech translation (ST) models requires large and high-quality\ndatasets. MuST-C is one of the most widely used ST benchmark datasets. It\ncontains around 400 hours of speech-transcript-translation data for each of the\neight translation directions. This dataset passes several quality-control\nfilters during creation. However, we find that MuST-C still suffers from three\nmajor quality issues: audio-text misalignment, inaccurate translation, and\nunnecessary speaker's name. What are the impacts of these data quality issues\nfor model development and evaluation? In this paper, we propose an automatic\nmethod to fix or filter the above quality issues, using English-German (En-De)\ntranslation as an example. Our experiments show that ST models perform better\non clean test sets, and the rank of proposed models remains consistent across\ndifferent test sets. Besides, simply removing misaligned data points from the\ntraining set does not lead to a better ST model.", "published": "2022-06-28 05:01:06", "link": "http://arxiv.org/abs/2206.13756v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Flexible text generation for counterfactual fairness probing", "abstract": "A common approach for testing fairness issues in text-based classifiers is\nthrough the use of counterfactuals: does the classifier output change if a\nsensitive attribute in the input is changed? Existing counterfactual generation\nmethods typically rely on wordlists or templates, producing simple\ncounterfactuals that don't take into account grammar, context, or subtle\nsensitive attribute references, and could miss issues that the wordlist\ncreators had not considered. In this paper, we introduce a task for generating\ncounterfactuals that overcomes these shortcomings, and demonstrate how large\nlanguage models (LLMs) can be leveraged to make progress on this task. We show\nthat this LLM-based method can produce complex counterfactuals that existing\nmethods cannot, comparing the performance of various counterfactual generation\nmethods on the Civil Comments dataset and showing their value in evaluating a\ntoxicity classifier.", "published": "2022-06-28 05:07:20", "link": "http://arxiv.org/abs/2206.13757v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Link the World: Improving Open-domain Conversation with Dynamic\n  Spatiotemporal-aware Knowledge", "abstract": "Making chatbots world aware in a conversation like a human is a crucial\nchallenge, where the world may contain dynamic knowledge and spatiotemporal\nstate. Several recent advances have tried to link the dialog system to a static\nknowledge base or search engine, but they do not contain all the world\ninformation needed for conversations. In contrast, we propose a new method to\nimprove the dialogue system using spatiotemporal aware dynamic knowledge. We\nutilize service information as a way for the dialogue system to link the world.\nThe system actively builds a request according to the dialog context and\nspatiotemporal state to get service information and then generates world aware\nresponses. To implement this method, we collect DuSinc, an open-domain\nhuman-human dialogue dataset, where a participant can access the service to get\nthe information needed for dialogue responses. Through automatic and human\nevaluations, we found that service information significantly improves the\nconsistency, informativeness, factuality, and engagingness of the dialogue\nsystem, making it behave more like a human. Compared to the pre-trained models\nwithout spatiotemporal aware dynamic knowledge, the overall session-level score\nwas improved by 60.87\\%. The collection dataset and methods will be\nopen-sourced.", "published": "2022-06-28 13:41:48", "link": "http://arxiv.org/abs/2206.14000v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Placing (Historical) Facts on a Timeline: A Classification cum Coref\n  Resolution Approach", "abstract": "A timeline provides one of the most effective ways to visualize the important\nhistorical facts that occurred over a period of time, presenting the insights\nthat may not be so apparent from reading the equivalent information in textual\nform. By leveraging generative adversarial learning for important sentence\nclassification and by assimilating knowledge based tags for improving the\nperformance of event coreference resolution we introduce a two staged system\nfor event timeline generation from multiple (historical) text documents. We\ndemonstrate our results on two manually annotated historical text documents.\nOur results can be extremely helpful for historians, in advancing research in\nhistory and in understanding the socio-political landscape of a country as\nreflected in the writings of famous personas.", "published": "2022-06-28 15:36:44", "link": "http://arxiv.org/abs/2206.14089v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "The NLP Sandbox: an efficient model-to-data system to enable federated\n  and unbiased evaluation of clinical NLP models", "abstract": "Objective The evaluation of natural language processing (NLP) models for\nclinical text de-identification relies on the availability of clinical notes,\nwhich is often restricted due to privacy concerns. The NLP Sandbox is an\napproach for alleviating the lack of data and evaluation frameworks for NLP\nmodels by adopting a federated, model-to-data approach. This enables unbiased\nfederated model evaluation without the need for sharing sensitive data from\nmultiple institutions. Materials and Methods We leveraged the Synapse\ncollaborative framework, containerization software, and OpenAPI generator to\nbuild the NLP Sandbox (nlpsandbox.io). We evaluated two state-of-the-art NLP\nde-identification focused annotation models, Philter and NeuroNER, using data\nfrom three institutions. We further validated model performance using data from\nan external validation site. Results We demonstrated the usefulness of the NLP\nSandbox through de-identification clinical model evaluation. The external\ndeveloper was able to incorporate their model into the NLP Sandbox template and\nprovide user experience feedback. Discussion We demonstrated the feasibility of\nusing the NLP Sandbox to conduct a multi-site evaluation of clinical text\nde-identification models without the sharing of data. Standardized model and\ndata schemas enable smooth model transfer and implementation. To generalize the\nNLP Sandbox, work is required on the part of data owners and model developers\nto develop suitable and standardized schemas and to adapt their data or model\nto fit the schemas. Conclusions The NLP Sandbox lowers the barrier to utilizing\nclinical data for NLP model evaluation and facilitates federated, multi-site,\nunbiased evaluation of NLP models.", "published": "2022-06-28 17:47:56", "link": "http://arxiv.org/abs/2206.14181v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Simple and Effective Knowledge-Driven Query Expansion for QA-Based\n  Product Attribute Extraction", "abstract": "A key challenge in attribute value extraction (AVE) from e-commerce sites is\nhow to handle a large number of attributes for diverse products. Although this\nchallenge is partially addressed by a question answering (QA) approach which\nfinds a value in product data for a given query (attribute), it does not work\neffectively for rare and ambiguous queries. We thus propose simple\nknowledge-driven query expansion based on possible answers (values) of a query\n(attribute) for QA-based AVE. We retrieve values of a query (attribute) from\nthe training data to expand the query. We train a model with two tricks,\nknowledge dropout and knowledge token mixing, which mimic the imperfection of\nthe value knowledge in testing. Experimental results on our cleaned version of\nAliExpress dataset show that our method improves the performance of AVE (+6.08\nmacro F1), especially for rare and ambiguous attributes (+7.82 and +6.86 macro\nF1, respectively).", "published": "2022-06-28 19:43:57", "link": "http://arxiv.org/abs/2206.14264v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Kwame for Science: An AI Teaching Assistant Based on Sentence-BERT for\n  Science Education in West Africa", "abstract": "Africa has a high student-to-teacher ratio which limits students' access to\nteachers. Consequently, students struggle to get answers to their questions. In\nthis work, we extended Kwame, our previous AI teaching assistant, adapted it\nfor science education, and deployed it as a web app. Kwame for Science answers\nquestions of students based on the Integrated Science subject of the West\nAfrican Senior Secondary Certificate Examination (WASSCE). Kwame for Science is\na Sentence-BERT-based question-answering web app that displays 3 paragraphs as\nanswers along with a confidence score in response to science questions.\nAdditionally, it displays the top 5 related past exam questions and their\nanswers in addition to the 3 paragraphs. Our preliminary evaluation of the\nKwame for Science with a 2.5-week real-world deployment showed a top 3 accuracy\nof 87.5% (n=56) with 190 users across 11 countries. Kwame for Science will\nenable the delivery of scalable, cost-effective, and quality remote education\nto millions of people across Africa.", "published": "2022-06-28 02:27:23", "link": "http://arxiv.org/abs/2206.13703v2", "categories": ["cs.CL", "cs.CY", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Bengali Common Voice Speech Dataset for Automatic Speech Recognition", "abstract": "Bengali is one of the most spoken languages in the world with over 300\nmillion speakers globally. Despite its popularity, research into the\ndevelopment of Bengali speech recognition systems is hindered due to the lack\nof diverse open-source datasets. As a way forward, we have crowdsourced the\nBengali Common Voice Speech Dataset, which is a sentence-level automatic speech\nrecognition corpus. Collected on the Mozilla Common Voice platform, the dataset\nis part of an ongoing campaign that has led to the collection of over 400 hours\nof data in 2 months and is growing rapidly. Our analysis shows that this\ndataset has more speaker, phoneme, and environmental diversity compared to the\nOpenSLR Bengali ASR dataset, the largest existing open-source Bengali speech\ndataset. We present insights obtained from the dataset and discuss key\nlinguistic challenges that need to be addressed in future versions.\nAdditionally, we report the current performance of a few Automatic Speech\nRecognition (ASR) algorithms and set a benchmark for future research.", "published": "2022-06-28 14:52:08", "link": "http://arxiv.org/abs/2206.14053v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Collecting high-quality adversarial data for machine reading\n  comprehension tasks with humans and models in the loop", "abstract": "We present our experience as annotators in the creation of high-quality,\nadversarial machine-reading-comprehension data for extractive QA for Task 1 of\nthe First Workshop on Dynamic Adversarial Data Collection (DADC). DADC is an\nemergent data collection paradigm with both models and humans in the loop. We\nset up a quasi-experimental annotation design and perform quantitative analyses\nacross groups with different numbers of annotators focusing on successful\nadversarial attacks, cost analysis, and annotator confidence correlation. We\nfurther perform a qualitative analysis of our perceived difficulty of the task\ngiven the different topics of the passages in our dataset and conclude with\nrecommendations and suggestions that might be of value to people working on\nfuture DADC tasks and related annotation interfaces.", "published": "2022-06-28 20:01:18", "link": "http://arxiv.org/abs/2206.14272v1", "categories": ["cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Bottleneck Low-rank Transformers for Low-resource Spoken Language\n  Understanding", "abstract": "End-to-end spoken language understanding (SLU) systems benefit from\npretraining on large corpora, followed by fine-tuning on application-specific\ndata. The resulting models are too large for on-edge applications. For\ninstance, BERT-based systems contain over 110M parameters. Observing the model\nis overparameterized, we propose lean transformer structure where the dimension\nof the attention mechanism is automatically reduced using group sparsity. We\npropose a variant where the learned attention subspace is transferred to an\nattention bottleneck layer. In a low-resource setting and without pre-training,\nthe resulting compact SLU model achieves accuracies competitive with\npre-trained large models.", "published": "2022-06-28 23:08:32", "link": "http://arxiv.org/abs/2206.14318v1", "categories": ["cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Attention-based conditioning methods using variable frame rate for\n  style-robust speaker verification", "abstract": "We propose an approach to extract speaker embeddings that are robust to\nspeaking style variations in text-independent speaker verification. Typically,\nspeaker embedding extraction includes training a DNN for speaker classification\nand using the bottleneck features as speaker representations. Such a network\nhas a pooling layer to transform frame-level to utterance-level features by\ncalculating statistics over all utterance frames, with equal weighting.\nHowever, self-attentive embeddings perform weighted pooling such that the\nweights correspond to the importance of the frames in a speaker classification\ntask. Entropy can capture acoustic variability due to speaking style\nvariations. Hence, an entropy-based variable frame rate vector is proposed as\nan external conditioning vector for the self-attention layer to provide the\nnetwork with information that can address style effects. This work explores\nfive different approaches to conditioning. The best conditioning approach,\nconcatenation with gating, provided statistically significant improvements over\nthe x-vector baseline in 12/23 tasks and was the same as the baseline in 11/23\ntasks when using the UCLA speaker variability database. It also significantly\noutperformed self-attention without conditioning in 9/23 tasks and was worse in\n1/23. The method also showed significant improvements in multi-speaker\nscenarios of SITW.", "published": "2022-06-28 01:14:09", "link": "http://arxiv.org/abs/2206.13680v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Learning from human perception to improve automatic speaker verification\n  in style-mismatched conditions", "abstract": "Our prior experiments show that humans and machines seem to employ different\napproaches to speaker discrimination, especially in the presence of speaking\nstyle variability. The experiments examined read versus conversational speech.\nListeners focused on speaker-specific idiosyncrasies while \"telling speakers\ntogether\", and on relative distances in a shared acoustic space when \"telling\nspeakers apart\". However, automatic speaker verification (ASV) systems use the\nsame loss function irrespective of target or non-target trials. To improve ASV\nperformance in the presence of style variability, insights learnt from human\nperception are used to design a new training loss function that we refer to as\n\"CllrCE loss\". CllrCE loss uses both speaker-specific idiosyncrasies and\nrelative acoustic distances between speakers to train the ASV system. When\nusing the UCLA speaker variability database, in the x-vector and conditioning\nsetups, CllrCE loss results in significant relative improvements in EER by\n1-66%, and minDCF by 1-31% and 1-56%, respectively, when compared to the\nx-vector baseline. Using the SITW evaluation tasks, which involve different\nconversational speech tasks, the proposed loss combined with self-attention\nconditioning results in significant relative improvements in EER by 2-5% and\nminDCF by 6-12% over baseline. In the SITW case, performance improvements were\nconsistent only with conditioning.", "published": "2022-06-28 01:24:38", "link": "http://arxiv.org/abs/2206.13684v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Tiny-Sepformer: A Tiny Time-Domain Transformer Network for Speech\n  Separation", "abstract": "Time-domain Transformer neural networks have proven their superiority in\nspeech separation tasks. However, these models usually have a large number of\nnetwork parameters, thus often encountering the problem of GPU memory\nexplosion. In this paper, we proposed Tiny-Sepformer, a tiny version of\nTransformer network for speech separation. We present two techniques to reduce\nthe model parameters and memory consumption: (1) Convolution-Attention (CA)\nblock, spliting the vanilla Transformer to two paths, multi-head attention and\n1D depthwise separable convolution, (2) parameter sharing, sharing the layer\nparameters within the CA block. In our experiments, Tiny-Sepformer could\ngreatly reduce the model size, and achieves comparable separation performance\nwith vanilla Sepformer on WSJ0-2/3Mix datasets.", "published": "2022-06-28 01:46:37", "link": "http://arxiv.org/abs/2206.13689v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Exploring linguistic feature and model combination for speech\n  recognition based automatic AD detection", "abstract": "Early diagnosis of Alzheimer's disease (AD) is crucial in facilitating\npreventive care and delay progression. Speech based automatic AD screening\nsystems provide a non-intrusive and more scalable alternative to other clinical\nscreening techniques. Scarcity of such specialist data leads to uncertainty in\nboth model selection and feature learning when developing such systems. To this\nend, this paper investigates the use of feature and model combination\napproaches to improve the robustness of domain fine-tuning of BERT and Roberta\npre-trained text encoders on limited data, before the resulting embedding\nfeatures being fed into an ensemble of backend classifiers to produce the final\nAD detection decision via majority voting. Experiments conducted on the\nADReSS20 Challenge dataset suggest consistent performance improvements were\nobtained using model and feature combination in system development.\nState-of-the-art AD detection accuracies of 91.67 percent and 93.75 percent\nwere obtained using manual and ASR speech transcripts respectively on the\nADReSS20 test set consisting of 48 elderly speakers.", "published": "2022-06-28 05:09:01", "link": "http://arxiv.org/abs/2206.13758v2", "categories": ["cs.LG", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Interrelate Training and Searching: A Unified Online Clustering\n  Framework for Speaker Diarization", "abstract": "For online speaker diarization, samples arrive incrementally, and the overall\ndistribution of the samples is invisible. Moreover, in most existing\nclustering-based methods, the training objective of the embedding extractor is\nnot designed specially for clustering. To improve online speaker diarization\nperformance, we propose a unified online clustering framework, which provides\nan interactive manner between embedding extractors and clustering algorithms.\nSpecifically, the framework consists of two highly coupled parts:\nclustering-guided recurrent training (CGRT) and truncated beam searching\nclustering (TBSC). The CGRT introduces the clustering algorithm into the\ntraining process of embedding extractors, which could provide not only\ncluster-aware information for the embedding extractor, but also crucial\nparameters for the clustering process afterward. And with these parameters,\nwhich contain preliminary information of the metric space, the TBSC penalizes\nthe probability score of each cluster, in order to output more accurate\nclustering results in online fashion with low latency. With the above\ninnovations, our proposed online clustering system achieves 14.48\\% DER with\ncollar 0.25 at 2.5s latency on the AISHELL-4, while the DER of the offline\nagglomerative hierarchical clustering is 14.57\\%.", "published": "2022-06-28 05:10:20", "link": "http://arxiv.org/abs/2206.13760v1", "categories": ["eess.AS", "cs.MM"], "primary_category": "eess.AS"}
{"title": "A Hierarchical Speaker Representation Framework for One-shot Singing\n  Voice Conversion", "abstract": "Typically, singing voice conversion (SVC) depends on an embedding vector,\nextracted from either a speaker lookup table (LUT) or a speaker recognition\nnetwork (SRN), to model speaker identity. However, singing contains more\nexpressive speaker characteristics than conversational speech. It is suspected\nthat a single embedding vector may only capture averaged and coarse-grained\nspeaker characteristics, which is insufficient for the SVC task. To this end,\nthis work proposes a novel hierarchical speaker representation framework for\nSVC, which can capture fine-grained speaker characteristics at different\ngranularity. It consists of an up-sampling stream and three down-sampling\nstreams. The up-sampling stream transforms the linguistic features into audio\nsamples, while one down-sampling stream of the three operates in the reverse\ndirection. It is expected that the temporal statistics of each down-sampling\nblock can represent speaker characteristics at different granularity, which\nwill be engaged in the up-sampling blocks to enhance the speaker modeling.\nExperiment results verify that the proposed method outperforms both the LUT and\nSRN based SVC systems. Moreover, the proposed system supports the one-shot SVC\nwith only a few seconds of reference audio.", "published": "2022-06-28 05:19:31", "link": "http://arxiv.org/abs/2206.13762v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Algorithms for audio inpainting based on probabilistic nonnegative\n  matrix factorization", "abstract": "Audio inpainting, i.e., the task of restoring missing or occluded audio\nsignal samples, usually relies on sparse representations or autoregressive\nmodeling. In this paper, we propose to structure the spectrogram with\nnonnegative matrix factorization (NMF) in a probabilistic framework. First, we\ntreat the missing samples as latent variables, and derive two\nexpectation-maximization algorithms for estimating the parameters of the model,\ndepending on whether we formulate the problem in the time- or time-frequency\ndomain. Then, we treat the missing samples as parameters, and we address this\nnovel problem by deriving an alternating minimization scheme. We assess the\npotential of these algorithms for the task of restoring short- to middle-length\ngaps in music signals. Experiments reveal great convergence properties of the\nproposed methods, as well as competitive performance when compared to\nstate-of-the-art audio inpainting techniques.", "published": "2022-06-28 05:38:35", "link": "http://arxiv.org/abs/2206.13768v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Two Methods for Spoofing-Aware Speaker Verification: Multi-Layer\n  Perceptron Score Fusion Model and Integrated Embedding Projector", "abstract": "The use of deep neural networks (DNN) has dramatically elevated the\nperformance of automatic speaker verification (ASV) over the last decade.\nHowever, ASV systems can be easily neutralized by spoofing attacks. Therefore,\nthe Spoofing-Aware Speaker Verification (SASV) challenge is designed and held\nto promote development of systems that can perform ASV considering spoofing\nattacks by integrating ASV and spoofing countermeasure (CM) systems. In this\npaper, we propose two back-end systems: multi-layer perceptron score fusion\nmodel (MSFM) and integrated embedding projector (IEP). The MSFM, score fusion\nback-end system, derived SASV score utilizing ASV and CM scores and embeddings.\nOn the other hand,IEP combines ASV and CM embeddings into SASV embedding and\ncalculates final SASV score based on the cosine similarity. We effectively\nintegrated ASV and CM systems through proposed MSFM and IEP and achieved the\nSASV equal error rates 0.56%, 1.32% on the official evaluation trials of the\nSASV 2022 challenge.", "published": "2022-06-28 07:42:56", "link": "http://arxiv.org/abs/2206.13807v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speaker Verification in Multi-Speaker Environments Using Temporal\n  Feature Fusion", "abstract": "Verifying the identity of a speaker is crucial in modern human-machine\ninterfaces, e.g., to ensure privacy protection or to enable biometric\nauthentication. Classical speaker verification (SV) approaches estimate a\nfixed-dimensional embedding from a speech utterance that encodes the speaker's\nvoice characteristics. A speaker is verified if his/her voice embedding is\nsufficiently similar to the embedding of the claimed speaker. However, such\napproaches assume that only a single speaker exists in the input. The presence\nof concurrent speakers is likely to have detrimental effects on the\nperformance. To address SV in a multi-speaker environment, we propose an\nend-to-end deep learning-based SV system that detects whether the target\nspeaker exists within an input or not. First, an embedding is estimated from a\nreference utterance to represent the target's characteristics. Second,\nframe-level features are estimated from the input mixture. The reference\nembedding is then fused frame-wise with the mixture's features to allow\ndistinguishing the target from other speakers on a frame basis. Finally, the\nfused features are used to predict whether the target speaker is active in the\nspeech segment or not. Experimental evaluation shows that the proposed method\noutperforms the x-vector in multi-speaker conditions.", "published": "2022-06-28 07:45:40", "link": "http://arxiv.org/abs/2206.13808v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "RetrieverTTS: Modeling Decomposed Factors for Text-Based Speech\n  Insertion", "abstract": "This paper proposes a new \"decompose-and-edit\" paradigm for the text-based\nspeech insertion task that facilitates arbitrary-length speech insertion and\neven full sentence generation. In the proposed paradigm, global and local\nfactors in speech are explicitly decomposed and separately manipulated to\nachieve high speaker similarity and continuous prosody. Specifically, we\nproposed to represent the global factors by multiple tokens, which are\nextracted by cross-attention operation and then injected back by link-attention\noperation. Due to the rich representation of global factors, we manage to\nachieve high speaker similarity in a zero-shot manner. In addition, we\nintroduce a prosody smoothing task to make the local prosody factor\ncontext-aware and therefore achieve satisfactory prosody continuity. We further\nachieve high voice quality with an adversarial training stage. In the\nsubjective test, our method achieves state-of-the-art performance in both\nnaturalness and similarity. Audio samples can be found at\nhttps://ydcustc.github.io/retrieverTTS-demo/.", "published": "2022-06-28 10:02:41", "link": "http://arxiv.org/abs/2206.13865v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Expressive, Variable, and Controllable Duration Modelling in TTS", "abstract": "Duration modelling has become an important research problem once more with\nthe rise of non-attention neural text-to-speech systems. The current approaches\nlargely fall back to relying on previous statistical parametric speech\nsynthesis technology for duration prediction, which poorly models the\nexpressiveness and variability in speech. In this paper, we propose two\nalternate approaches to improve duration modelling. First, we propose a\nduration model conditioned on phrasing that improves the predicted durations\nand provides better modelling of pauses. We show that the duration model\nconditioned on phrasing improves the naturalness of speech over our baseline\nduration model. Second, we also propose a multi-speaker duration model called\nCauliflow, that uses normalising flows to predict durations that better match\nthe complex target duration distribution. Cauliflow performs on par with our\nother proposed duration model in terms of naturalness, whilst providing\nvariable durations for the same prompt and variable levels of expressiveness.\nLastly, we propose to condition Cauliflow on parameters that provide an\nintuitive control of the pacing and pausing in the synthesised speech in a\nnovel way.", "published": "2022-06-28 17:21:53", "link": "http://arxiv.org/abs/2206.14165v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Volume-Independent Music Matching by Frequency Spectrum Comparison", "abstract": "Often, I hear a piece of music and wonder what the name of the piece is.\nIndeed, there are applications such as Shazam app that provides music matching.\nHowever, the limitations of those apps are that the same piece performed by the\nsame musician cannot be identified if it is not the same recording. Shazam\nidentifies the recording of it, not the music. This is because Shazam matches\nthe variation in volume, not the frequencies of the sound. This research\nattempts to match music the way humans understand it: by the frequency spectrum\nof music, not the volume variation. Essentially, the idea is to precompute the\nfrequency spectrums of all the music in the database, then take the unknown\npiece and try to match its frequency spectrum against every segment of every\nmusic in the database. I did it by matching the frequency spectrum of the\nunknown piece to our database by sliding the window by 0.1 seconds and\ncalculating the error by taking Absolute value, normalizing the audio,\nsubtracting the normalized arrays, and taking the sum of absolute differences.\nThe segment that shows the least error is considered the candidate for the\nmatch. The matching performance proved to be dependent on the complexity of the\nmusic. Matching simple music, such as single note pieces, was successful.\nHowever, more complex pieces, such as Chopins Ballade 4, were not successful,\nthat is, the algorithm could not produce low error values in any of the music\nin the database. I suspect that it has to do with having too many notes:\nmismatches in the higher harmonics added up to a significant amount of errors,\nwhich swamps the calculations.", "published": "2022-06-28 16:58:13", "link": "http://arxiv.org/abs/2206.15426v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Dummy Prototypical Networks for Few-Shot Open-Set Keyword Spotting", "abstract": "Keyword spotting is the task of detecting a keyword in streaming audio.\nConventional keyword spotting targets predefined keywords classification, but\nthere is growing attention in few-shot (query-by-example) keyword spotting,\ne.g., N-way classification given M-shot support samples. Moreover, in\nreal-world scenarios, there can be utterances from unexpected categories\n(open-set) which need to be rejected rather than classified as one of the N\nclasses. Combining the two needs, we tackle few-shot open-set keyword spotting\nwith a new benchmark setting, named splitGSC. We propose episode-known dummy\nprototypes based on metric learning to detect an open-set better and introduce\na simple and powerful approach, Dummy Prototypical Networks (D-ProtoNets). Our\nD-ProtoNets shows clear margins compared to recent few-shot open-set\nrecognition (FSOSR) approaches in the suggested splitGSC. We also verify our\nmethod on a standard benchmark, miniImageNet, and D-ProtoNets shows the\nstate-of-the-art open-set detection rate in FSOSR.", "published": "2022-06-28 01:56:24", "link": "http://arxiv.org/abs/2206.13691v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Domain Agnostic Few-shot Learning for Speaker Verification", "abstract": "Deep learning models for verification systems often fail to generalize to new\nusers and new environments, even though they learn highly discriminative\nfeatures. To address this problem, we propose a few-shot domain generalization\nframework that learns to tackle distribution shift for new users and new\ndomains. Our framework consists of domain-specific and domain-aggregation\nnetworks, which are the experts on specific and combined domains, respectively.\nBy using these networks, we generate episodes that mimic the presence of both\nnovel users and novel domains in the training phase to eventually produce\nbetter generalization. To save memory, we reduce the number of domain-specific\nnetworks by clustering similar domains together. Upon extensive evaluation on\nartificially generated noise domains, we can explicitly show generalization\nability of our framework. In addition, we apply our proposed methods to the\nexisting competitive architecture on the standard benchmark, which shows\nfurther performance improvements.", "published": "2022-06-28 02:22:47", "link": "http://arxiv.org/abs/2206.13700v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Personalized Keyword Spotting through Multi-task Learning", "abstract": "Keyword spotting (KWS) plays an essential role in enabling speech-based user\ninteraction on smart devices, and conventional KWS (C-KWS) approaches have\nconcentrated on detecting user-agnostic pre-defined keywords. However, in\npractice, most user interactions come from target users enrolled in the device\nwhich motivates to construct personalized keyword spotting. We design two\npersonalized KWS tasks; (1) Target user Biased KWS (TB-KWS) and (2) Target user\nOnly KWS (TO-KWS). To solve the tasks, we propose personalized keyword spotting\nthrough multi-task learning (PK-MTL) that consists of multi-task learning and\ntask-adaptation. First, we introduce applying multi-task learning on keyword\nspotting and speaker verification to leverage user information to the keyword\nspotting system. Next, we design task-specific scoring functions to adapt to\nthe personalized KWS tasks thoroughly. We evaluate our framework on\nconventional and personalized scenarios, and the results show that PK-MTL can\ndramatically reduce the false alarm rate, especially in various practical\nscenarios.", "published": "2022-06-28 02:48:34", "link": "http://arxiv.org/abs/2206.13708v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Comparison of Speech Representations for the MOS Prediction System", "abstract": "Automatic methods to predict Mean Opinion Score (MOS) of listeners have been\nresearched to assure the quality of Text-to-Speech systems. Many previous\nstudies focus on architectural advances (e.g. MBNet, LDNet, etc.) to capture\nrelations between spectral features and MOS in a more effective way and\nachieved high accuracy. However, the optimal representation in terms of\ngeneralization capability still largely remains unknown. To this end, we\ncompare the performance of Self-Supervised Learning (SSL) features obtained by\nthe wav2vec framework to that of spectral features such as magnitude of\nspectrogram and melspectrogram. Moreover, we propose to combine the SSL\nfeatures and features which we believe to retain essential information to the\nautomatic MOS to compensate each other for their drawbacks. We conduct\ncomprehensive experiments on a large-scale listening test corpus collected from\npast Blizzard and Voice Conversion Challenges. We found that the wav2vec\nfeature set showed the best generalization even though the given ground-truth\nwas not always reliable. Furthermore, we found that the combinations performed\nthe best and analyzed how they bridged the gap between spectral and the wav2vec\nfeature sets.", "published": "2022-06-28 08:18:18", "link": "http://arxiv.org/abs/2206.13817v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "QTI Submission to DCASE 2021: residual normalization for\n  device-imbalanced acoustic scene classification with efficient design", "abstract": "This technical report describes the details of our TASK1A submission of the\nDCASE2021 challenge. The goal of the task is to design an audio scene\nclassification system for device-imbalanced datasets under the constraints of\nmodel complexity. This report introduces four methods to achieve the goal.\nFirst, we propose Residual Normalization, a novel feature normalization method\nthat uses instance normalization with a shortcut path to discard unnecessary\ndevice-specific information without losing useful information for\nclassification. Second, we design an efficient architecture, BC-ResNet-Mod, a\nmodified version of the baseline architecture with a limited receptive field.\nThird, we exploit spectrogram-to-spectrogram translation from one to multiple\ndevices to augment training data. Finally, we utilize three model compression\nschemes: pruning, quantization, and knowledge distillation to reduce model\ncomplexity. The proposed system achieves an average test accuracy of 76.3% in\nTAU Urban Acoustic Scenes 2020 Mobile, development dataset with 315k\nparameters, and average test accuracy of 75.3% after compression to 61.0KB of\nnon-zero parameters. We extend this work to [1].", "published": "2022-06-28 11:42:52", "link": "http://arxiv.org/abs/2206.13909v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Show Me Your Face, And I'll Tell You How You Speak", "abstract": "When we speak, the prosody and content of the speech can be inferred from the\nmovement of our lips. In this work, we explore the task of lip to speech\nsynthesis, i.e., learning to generate speech given only the lip movements of a\nspeaker where we focus on learning accurate lip to speech mappings for multiple\nspeakers in unconstrained, large vocabulary settings. We capture the speaker's\nvoice identity through their facial characteristics, i.e., age, gender,\nethnicity and condition them along with the lip movements to generate speaker\nidentity aware speech. To this end, we present a novel method \"Lip2Speech\",\nwith key design choices to achieve accurate lip to speech synthesis in\nunconstrained scenarios. We also perform various experiments and extensive\nevaluation using quantitative, qualitative metrics and human evaluation.", "published": "2022-06-28 13:52:47", "link": "http://arxiv.org/abs/2206.14009v1", "categories": ["cs.CV", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.CV"}
