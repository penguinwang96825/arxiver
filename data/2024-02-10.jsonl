{"title": "History, Development, and Principles of Large Language Models-An\n  Introductory Survey", "abstract": "Language models serve as a cornerstone in natural language processing (NLP),\nutilizing mathematical methods to generalize language laws and knowledge for\nprediction and generation. Over extensive research spanning decades, language\nmodeling has progressed from initial statistical language models (SLMs) to the\ncontemporary landscape of large language models (LLMs). Notably, the swift\nevolution of LLMs has reached the ability to process, understand, and generate\nhuman-level text. Nevertheless, despite the significant advantages that LLMs\noffer in improving both work and personal lives, the limited understanding\namong general practitioners about the background and principles of these models\nhampers their full potential. Notably, most LLM reviews focus on specific\naspects and utilize specialized language, posing a challenge for practitioners\nlacking relevant background knowledge. In light of this, this survey aims to\npresent a comprehensible overview of LLMs to assist a broader audience. It\nstrives to facilitate a comprehensive understanding by exploring the historical\nbackground of language models and tracing their evolution over time. The survey\nfurther investigates the factors influencing the development of LLMs,\nemphasizing key contributions. Additionally, it concentrates on elucidating the\nunderlying principles of LLMs, equipping audiences with essential theoretical\nknowledge. The survey also highlights the limitations of existing work and\npoints out promising future directions.", "published": "2024-02-10 01:18:15", "link": "http://arxiv.org/abs/2402.06853v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating Consistency in Query-Based Meeting Summarization: A\n  Comparative Study of Different Embedding Methods", "abstract": "With more and more advanced data analysis techniques emerging, people will\nexpect these techniques to be applied in more complex tasks and solve problems\nin our daily lives. Text Summarization is one of famous applications in Natural\nLanguage Processing (NLP) field. It aims to automatically generate summary with\nimportant information based on a given context, which is important when you\nhave to deal with piles of documents. Summarization techniques can help capture\nkey points in a short time and bring convenience in works. One of applicable\nsituation is meeting summarization, especially for important meeting that tend\nto be long, complicated, multi-topic and multi-person. Therefore, when people\nwant to review specific content from a meeting, it will be hard and\ntime-consuming to find the related spans in the meeting transcript. However,\nmost of previous works focus on doing summarization for newsletters, scientific\narticles...etc, which have a clear document structure and an official format.\nFor the documents with complex structure like transcripts, we think those works\nare not quite suitable for meeting summarization. Besides, the consistency of\nsummary is another issue common to be discussed in NLP field. To conquer\nchallenges of meeting summarization, we are inspired by \"QMSum: A New Benchmark\nfor Query-based Multi-domain Meeting Summarization\" proposed by Microsoft and\nwe also propose our Locater model designed to extract relevant spans based on\ngiven transcript and query, which are then summarized by Summarizer model.\nFurthermore, we perform a comparative study by applying different word\nembedding techniques to improve summary consistency.", "published": "2024-02-10 08:25:30", "link": "http://arxiv.org/abs/2402.06907v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TL;DR Progress: Multi-faceted Literature Exploration in Text\n  Summarization", "abstract": "This paper presents TL;DR Progress, a new tool for exploring the literature\non neural text summarization. It organizes 514~papers based on a comprehensive\nannotation scheme for text summarization approaches and enables fine-grained,\nfaceted search. Each paper was manually annotated to capture aspects such as\nevaluation metrics, quality dimensions, learning paradigms, challenges\naddressed, datasets, and document domains. In addition, a succinct indicative\nsummary is provided for each paper, consisting of automatically extracted\ncontextual factors, issues, and proposed solutions. The tool is available\nonline at https://www.tldr-progress.de, a demo video at\nhttps://youtu.be/uCVRGFvXUj8", "published": "2024-02-10 09:16:56", "link": "http://arxiv.org/abs/2402.06913v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Thorough Examination of Decoding Methods in the Era of LLMs", "abstract": "Decoding methods play an indispensable role in converting language models\nfrom next-token predictors into practical task solvers. Prior research on\ndecoding methods, primarily focusing on task-specific models, may not extend to\nthe current era of general-purpose large language models (LLMs). Moreover, the\nrecent influx of decoding strategies has further complicated this landscape.\nThis paper provides a comprehensive and multifaceted analysis of various\ndecoding methods within the context of LLMs, evaluating their performance,\nrobustness to hyperparameter changes, and decoding speeds across a wide range\nof tasks, models, and deployment environments. Our findings reveal that\ndecoding method performance is notably task-dependent and influenced by factors\nsuch as alignment, model size, and quantization. Intriguingly, sensitivity\nanalysis exposes that certain methods achieve superior performance at the cost\nof extensive hyperparameter tuning, highlighting the trade-off between\nattaining optimal results and the practicality of implementation in varying\ncontexts.", "published": "2024-02-10 11:14:53", "link": "http://arxiv.org/abs/2402.06925v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LiFi: Lightweight Controlled Text Generation with Fine-Grained Control\n  Codes", "abstract": "In the rapidly evolving field of text generation, the demand for more precise\ncontrol mechanisms has become increasingly apparent. To address this need, we\npresent a novel methodology, LIFI, which offers a lightweight approach with\nfine-grained control for controlled text generation. Unlike previous studies\nthat train pre-trained language models to follow discrete, categorical, and\nexclusive control codes, LIFI learns controlled text generation under the\nguidance of continuous, relative, and nonexclusive control codes. These\nfine-grained codes are automatically derived from an attribute classifier,\ninitially trained with a small amount of labeled data and subsequently employed\nto label abundant unlabeled data, thus garnering more extensive supervision\nsignals. Moreover, to achieve efficient control, we incorporate the\nfine-grained control codes with adapters, a parameter- and compute-efficient\nway to steer a pre-trained language model. We evaluate LIFI on two conventional\ntasks -- sentiment control and topic control -- and one newly proposed task --\nstylistic novel writing. Comprehensive experimental results validate the\neffectiveness of our proposed methods, demonstrating substantial performance\nimprovements over existing baselines.", "published": "2024-02-10 11:53:48", "link": "http://arxiv.org/abs/2402.06930v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Should I try multiple optimizers when fine-tuning pre-trained\n  Transformers for NLP tasks? Should I tune their hyperparameters?", "abstract": "NLP research has explored different neural model architectures and sizes,\ndatasets, training objectives, and transfer learning techniques. However, the\nchoice of optimizer during training has not been explored as extensively.\nTypically, some variant of Stochastic Gradient Descent (SGD) is employed,\nselected among numerous variants, using unclear criteria, often with minimal or\nno tuning of the optimizer's hyperparameters. Experimenting with five GLUE\ndatasets, two models (DistilBERT and DistilRoBERTa), and seven popular\noptimizers (SGD, SGD with Momentum, Adam, AdaMax, Nadam, AdamW, and AdaBound),\nwe find that when the hyperparameters of the optimizers are tuned, there is no\nsubstantial difference in test performance across the five more elaborate\n(adaptive) optimizers, despite differences in training loss. Furthermore,\ntuning just the learning rate is in most cases as good as tuning all the\nhyperparameters. Hence, we recommend picking any of the best-behaved adaptive\noptimizers (e.g., Adam) and tuning only its learning rate. When no\nhyperparameter can be tuned, SGD with Momentum is the best choice.", "published": "2024-02-10 13:26:14", "link": "http://arxiv.org/abs/2402.06948v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ChemLLM: A Chemical Large Language Model", "abstract": "Large language models (LLMs) have made impressive progress in chemistry\napplications. However, the community lacks an LLM specifically designed for\nchemistry. The main challenges are two-fold: firstly, most chemical data and\nscientific knowledge are stored in structured databases, which limits the\nmodel's ability to sustain coherent dialogue when used directly. Secondly,\nthere is an absence of objective and fair benchmark that encompass most\nchemistry tasks. Here, we introduce ChemLLM, a comprehensive framework that\nfeatures the first LLM dedicated to chemistry. It also includes ChemData, a\ndataset specifically designed for instruction tuning, and ChemBench, a robust\nbenchmark covering nine essential chemistry tasks. ChemLLM is adept at\nperforming various tasks across chemical disciplines with fluid dialogue\ninteraction. Notably, ChemLLM achieves results comparable to GPT-4 on the core\nchemical tasks and demonstrates competitive performance with LLMs of similar\nsize in general scenarios. ChemLLM paves a new path for exploration in chemical\nstudies, and our method of incorporating structured chemical knowledge into\ndialogue systems sets a new standard for developing LLMs in various scientific\nfields. Codes, Datasets, and Model weights are publicly accessible at\nhttps://hf.co/AI4Chem", "published": "2024-02-10 01:11:59", "link": "http://arxiv.org/abs/2402.06852v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Can LLMs Recognize Toxicity? A Structured Investigation Framework and\n  Toxicity Metric", "abstract": "In the pursuit of developing Large Language Models (LLMs) that adhere to\nsocietal standards, it is imperative to detect the toxicity in the generated\ntext. The majority of existing toxicity metrics rely on encoder models trained\non specific toxicity datasets, which are susceptible to out-of-distribution\n(OOD) problems and depend on the dataset's definition of toxicity. In this\npaper, we introduce a robust metric grounded on LLMs to flexibly measure\ntoxicity according to the given definition. We first analyze the toxicity\nfactors, followed by an examination of the intrinsic toxic attributes of LLMs\nto ascertain their suitability as evaluators. Finally, we evaluate the\nperformance of our metric with detailed analysis. Our empirical results\ndemonstrate outstanding performance in measuring toxicity within verified\nfactors, improving on conventional metrics by 12 points in the F1 score. Our\nfindings also indicate that upstream toxicity significantly influences\ndownstream metrics, suggesting that LLMs are unsuitable for toxicity\nevaluations within unverified factors.", "published": "2024-02-10 07:55:27", "link": "http://arxiv.org/abs/2402.06900v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "NLP for Knowledge Discovery and Information Extraction from Energetics\n  Corpora", "abstract": "We present a demonstration of the utility of NLP for aiding research into\nenergetic materials and associated systems. The NLP method enables machine\nunderstanding of textual data, offering an automated route to knowledge\ndiscovery and information extraction from energetics text. We apply three\nestablished unsupervised NLP models: Latent Dirichlet Allocation, Word2Vec, and\nthe Transformer to a large curated dataset of energetics-related scientific\narticles. We demonstrate that each NLP algorithm is capable of identifying\nenergetic topics and concepts, generating a language model which aligns with\nSubject Matter Expert knowledge. Furthermore, we present a document\nclassification pipeline for energetics text. Our classification pipeline\nachieves 59-76\\% accuracy depending on the NLP model used, with the highest\nperforming Transformer model rivaling inter-annotator agreement metrics. The\nNLP approaches studied in this work can identify concepts germane to energetics\nand therefore hold promise as a tool for accelerating energetics research\nefforts and energetics material development.", "published": "2024-02-10 14:43:08", "link": "http://arxiv.org/abs/2402.06964v1", "categories": ["cs.CL", "cond-mat.mtrl-sci"], "primary_category": "cs.CL"}
{"title": "Instruct Once, Chat Consistently in Multiple Rounds: An Efficient Tuning\n  Framework for Dialogue", "abstract": "Tuning language models for dialogue generation has been a prevalent paradigm\nfor building capable dialogue agents. Yet, traditional tuning narrowly views\ndialogue generation as resembling other language generation tasks, ignoring the\nrole disparities between two speakers and the multi-round interactive process\nthat dialogues ought to be. Such a manner often leads to unsatisfactory chat\nconsistency for the built agent. In this work, we emphasize the interactive,\ncommunicative nature of dialogue and argue that it is more feasible to model\nthe speaker roles of agent and user separately, enabling the agent to adhere to\nits role consistently. With this in mind, we propose an efficient Multi-round\nInteractive Dialogue Tuning (Midi-Tuning) framework. It models the agent and\nuser individually with two adapters built upon large language models. The\nadapters make use of respective utterances round by round in alternating order\nand they are tuned via a round-level memory caching mechanism. Extensive\nexperiments demonstrate that, our framework performs superior to traditional\nfine-tuning and harbors the tremendous potential for improving dialogue\nconsistency.", "published": "2024-02-10 14:52:52", "link": "http://arxiv.org/abs/2402.06967v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Semi-Supervised Learning for Bilingual Lexicon Induction", "abstract": "We consider the problem of aligning two sets of continuous word\nrepresentations, corresponding to languages, to a common space in order to\ninfer a bilingual lexicon. It was recently shown that it is possible to infer\nsuch lexicon, without using any parallel data, by aligning word embeddings\ntrained on monolingual data. Such line of work is called unsupervised bilingual\ninduction. By wondering whether it was possible to gain experience in the\nprogressive learning of several languages, we asked ourselves to what extent we\ncould integrate the knowledge of a given set of languages when learning a new\none, without having parallel data for the latter. In other words, while keeping\nthe core problem of unsupervised learning in the latest step, we allowed the\naccess to other corpora of idioms, hence the name semi-supervised. This led us\nto propose a novel formulation, considering the lexicon induction as a ranking\nproblem for which we used recent tools of this machine learning field. Our\nexperiments on standard benchmarks, inferring dictionary from English to more\nthan 20 languages, show that our approach consistently outperforms existing\nstate of the art benchmark. In addition, we deduce from this new scenario\nseveral relevant conclusions allowing a better understanding of the alignment\nphenomenon.", "published": "2024-02-10 19:27:22", "link": "http://arxiv.org/abs/2402.07028v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sentinels of the Stream: Unleashing Large Language Models for Dynamic\n  Packet Classification in Software Defined Networks -- Position Paper", "abstract": "With the release of OpenAI's ChatGPT, the field of large language models\n(LLM) saw an increase of academic interest in GPT based chat assistants. In the\nnext few months multiple accesible large language models were released that\nincluded Meta's LLama models and Mistral AI's Mistral and Mixtral MoE models.\nThese models are available openly for a wide array of purposes with a wide\nspectrum of licenses. These LLMs have found their use in a different number of\nfields like code development, SQL generation etc. In this work we propose our\nplan to explore the applicability of large language model in the domain of\nnetwork security. We plan to create Sentinel, a LLM, to analyse network packet\ncontents and pass a judgment on it's threat level. This work is a preliminary\nreport that will lay our plan for our future endeavors.", "published": "2024-02-10 04:47:58", "link": "http://arxiv.org/abs/2402.07950v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "DAEDRA: A language model for predicting outcomes in passive\n  pharmacovigilance reporting", "abstract": "Over the recent years, the emergence of large language models (LLMs) has\ngiven rise to a proliferation of domain-specific models that are intended to\nreflect the particularities of linguistic context and content as a correlate of\nthe originating domain. This paper details the conception, design, training and\nevaluation of DAEDRA, a LLM designed to detect regulatory-relevant outcomes\n(mortality, ER attendance and hospitalisation) in adverse event reports\nelicited through passive reporting (PR). While PR is a highly cost-efficient\nway of eliciting information from a wide and diverse audience -- typically\nincluding not only physicians and healthcare providers but also patients,\nfamily members and other lay stakeholders --, this diversity makes PR corpora\ndifficult to analyse. Generic language models may not capture the complex\nclinical dimensions while specific clinical or biomedical models may not\nperform well on lay reports. To evaluate the utility of a subdomain-specific\nlanguage model, an adaptive training approach was adapted, wherein base\nlanguage model candidates were evaluated on a subset of the corpus, and the\nbest performer was trained on the entire corpus. This yielded a small but\nsignificant improvement in $F_1$ (+1%), precision (+2.5%) and recall (+3.8%),\nat a relatively low training cost and a single-day training time.\nSubdomain-specific LLMs continue to be viable options for better results when\nanalysing highly specialised corpora.", "published": "2024-02-10 16:48:45", "link": "http://arxiv.org/abs/2402.10951v1", "categories": ["cs.CL", "cs.LG", "68T50", "I.2.7; J.3"], "primary_category": "cs.CL"}
{"title": "Generating Chain-of-Thoughts with a Pairwise-Comparison Approach to\n  Searching for the Most Promising Intermediate Thought", "abstract": "To improve the ability of the large language model (LLMs) to tackle complex\nreasoning problems, chain-of-thoughts (CoT) methods were proposed to guide LLMs\nto reason step-by-step, enabling problem solving from simple to complex.\nState-of-the-art methods for generating such a chain involve interactive\ncollaboration, where the learner generates candidate intermediate thoughts,\nevaluated by the LLM, guiding the generation of subsequent thoughts. However, a\nwidespread yet understudied problem is that the evaluation from the LLM is\ntypically noisy and unreliable, potentially misleading the generation process\nin selecting promising intermediate thoughts. In this paper, motivated by\nVapnik's principle, we use pairwise-comparison evaluation instead of point-wise\nscoring to search for promising intermediate thoughts with the noisy feedback\nfrom the LLM. In each round, we randomly pair intermediate thoughts and\ndirectly prompt the LLM to select the more promising one from each pair,\nallowing us to identify the most promising thoughts through an iterative\nprocess. To further alleviate the noise in the comparison, we incorporate\ntechniques from ensemble learning and dueling bandits, proposing two variants\nof the algorithm. Experiments on three real-world tasks demonstrate the\neffectiveness of our proposed algorithm and verify the rationale of the\npairwise comparison mechanism.", "published": "2024-02-10 09:51:03", "link": "http://arxiv.org/abs/2402.06918v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "OpenFedLLM: Training Large Language Models on Decentralized Private Data\n  via Federated Learning", "abstract": "Trained on massive publicly available data, large language models (LLMs) have\ndemonstrated tremendous success across various fields. While more data\ncontributes to better performance, a disconcerting reality is that high-quality\npublic data will be exhausted in a few years. In this paper, we offer a\npotential next step for contemporary LLMs: collaborative and privacy-preserving\nLLM training on the underutilized distributed private data via federated\nlearning (FL), where multiple data owners collaboratively train a shared model\nwithout transmitting raw data. To achieve this, we build a concise, integrated,\nand research-friendly framework/codebase, named OpenFedLLM. It covers federated\ninstruction tuning for enhancing instruction-following capability, federated\nvalue alignment for aligning with human values, and 7 representative FL\nalgorithms. Besides, OpenFedLLM supports training on diverse domains, where we\ncover 8 training datasets; and provides comprehensive evaluations, where we\ncover 30+ evaluation metrics. Through extensive experiments, we observe that\nall FL algorithms outperform local training on training LLMs, demonstrating a\nclear performance improvement across a variety of settings. Notably, in a\nfinancial benchmark, Llama2-7B fine-tuned by applying any FL algorithm can\noutperform GPT-4 by a significant margin while the model obtained through\nindividual training cannot, demonstrating strong motivation for clients to\nparticipate in FL. The code is available at\nhttps://github.com/rui-ye/OpenFedLLM.", "published": "2024-02-10 13:50:11", "link": "http://arxiv.org/abs/2402.06954v1", "categories": ["cs.LG", "cs.CL", "cs.DC", "cs.MA"], "primary_category": "cs.LG"}
{"title": "SpeechCLIP+: Self-supervised multi-task representation learning for\n  speech via CLIP and speech-image data", "abstract": "The recently proposed visually grounded speech model SpeechCLIP is an\ninnovative framework that bridges speech and text through images via CLIP\nwithout relying on text transcription. On this basis, this paper introduces two\nextensions to SpeechCLIP. First, we apply the Continuous Integrate-and-Fire\n(CIF) module to replace a fixed number of CLS tokens in the cascaded\narchitecture. Second, we propose a new hybrid architecture that merges the\ncascaded and parallel architectures of SpeechCLIP into a multi-task learning\nframework. Our experimental evaluation is performed on the Flickr8k and\nSpokenCOCO datasets. The results show that in the speech keyword extraction\ntask, the CIF-based cascaded SpeechCLIP model outperforms the previous cascaded\nSpeechCLIP model using a fixed number of CLS tokens. Furthermore, through our\nhybrid architecture, cascaded task learning boosts the performance of the\nparallel branch in image-speech retrieval tasks.", "published": "2024-02-10 14:26:42", "link": "http://arxiv.org/abs/2402.06959v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Event-Keyed Summarization", "abstract": "We introduce event-keyed summarization (EKS), a novel task that marries\ntraditional summarization and document-level event extraction, with the goal of\ngenerating a contextualized summary for a specific event, given a document and\nan extracted event structure. We introduce a dataset for this task, MUCSUM,\nconsisting of summaries of all events in the classic MUC-4 dataset, along with\na set of baselines that comprises both pretrained LM standards in the\nsummarization literature, as well as larger frontier models. We show that\nablations that reduce EKS to traditional summarization or structure-to-text\nyield inferior summaries of target events and that MUCSUM is a robust benchmark\nfor this task. Lastly, we conduct a human evaluation of both reference and\nmodel summaries, and provide some detailed analysis of the results.", "published": "2024-02-10 15:32:53", "link": "http://arxiv.org/abs/2402.06973v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Rational Analysis of the Speech-to-Song Illusion", "abstract": "The speech-to-song illusion is a robust psychological phenomenon whereby a\nspoken sentence sounds increasingly more musical as it is repeated. Despite\ndecades of research, a complete formal account of this transformation is still\nlacking, and some of its nuanced characteristics, namely, that certain phrases\nappear to transform while others do not, is not well understood. Here we\nprovide a formal account of this phenomenon, by recasting it as a statistical\ninference whereby a rational agent attempts to decide whether a sequence of\nutterances is more likely to have been produced in a song or speech. Using this\napproach and analyzing song and speech corpora, we further introduce a novel\nprose-to-lyrics illusion that is purely text-based. In this illusion, simply\nduplicating written sentences makes them appear more like song lyrics. We\nprovide robust evidence for this new illusion in both human participants and\nlarge language models.", "published": "2024-02-10 16:54:28", "link": "http://arxiv.org/abs/2402.06992v1", "categories": ["q-bio.NC", "cs.AI", "cs.CL", "stat.AP"], "primary_category": "q-bio.NC"}
{"title": "A Tale of Tails: Model Collapse as a Change of Scaling Laws", "abstract": "As AI model size grows, neural scaling laws have become a crucial tool to\npredict the improvements of large models when increasing capacity and the size\nof original (human or natural) training data. Yet, the widespread use of\npopular models means that the ecosystem of online data and text will co-evolve\nto progressively contain increased amounts of synthesized data. In this paper\nwe ask: How will the scaling laws change in the inevitable regime where\nsynthetic data makes its way into the training corpus? Will future models,\nstill improve, or be doomed to degenerate up to total (model) collapse? We\ndevelop a theoretical framework of model collapse through the lens of scaling\nlaws. We discover a wide range of decay phenomena, analyzing loss of scaling,\nshifted scaling with number of generations, the ''un-learning\" of skills, and\ngrokking when mixing human and synthesized data. Our theory is validated by\nlarge-scale experiments with a transformer on an arithmetic task and text\ngeneration using the large language model Llama2.", "published": "2024-02-10 21:06:34", "link": "http://arxiv.org/abs/2402.07043v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "$L^*LM$: Learning Automata from Examples using Natural Language Oracles", "abstract": "Expert demonstrations have proven an easy way to indirectly specify complex\ntasks. Recent algorithms even support extracting unambiguous formal\nspecifications, e.g. deterministic finite automata (DFA), from demonstrations.\nUnfortunately, these techniques are generally not sample efficient. In this\nwork, we introduce $L^*LM$, an algorithm for learning DFAs from both\ndemonstrations and natural language. Due to the expressivity of natural\nlanguage, we observe a significant improvement in the data efficiency of\nlearning DFAs from expert demonstrations. Technically, $L^*LM$ leverages large\nlanguage models to answer membership queries about the underlying task. This is\nthen combined with recent techniques for transforming learning from\ndemonstrations into a sequence of labeled example learning problems. In our\nexperiments, we observe the two modalities complement each other, yielding a\npowerful few-shot learner.", "published": "2024-02-10 21:46:34", "link": "http://arxiv.org/abs/2402.07051v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.FL"], "primary_category": "cs.LG"}
{"title": "Understanding the Progression of Educational Topics via Semantic\n  Matching", "abstract": "Education systems are dynamically changing to accommodate technological\nadvances, industrial and societal needs, and to enhance students' learning\njourneys. Curriculum specialists and educators constantly revise taught\nsubjects across educational grades to identify gaps, introduce new learning\ntopics, and enhance the learning outcomes. This process is usually done within\nthe same subjects (e.g. math) or across related subjects (e.g. math and\nphysics) considering the same and different educational levels, leading to\nmassive multi-layer comparisons. Having nuanced data about subjects, topics,\nand learning outcomes structured within a dataset, empowers us to leverage data\nscience to better understand the progression of various learning topics. In\nthis paper, Bidirectional Encoder Representations from Transformers (BERT)\ntopic modeling was used to extract topics from the curriculum, which were then\nused to identify relationships between subjects, track their progression, and\nidentify conceptual gaps. We found that grouping learning outcomes by common\ntopics helped specialists reduce redundancy and introduce new concepts in the\ncurriculum. We built a dashboard to avail the methodology to curriculum\nspecials. Finally, we tested the validity of the approach with subject matter\nexperts.", "published": "2024-02-10 08:24:29", "link": "http://arxiv.org/abs/2403.05553v1", "categories": ["cs.CY", "cs.CL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "GenTranslate: Large Language Models are Generative Multilingual Speech\n  and Machine Translators", "abstract": "Recent advances in large language models (LLMs) have stepped forward the\ndevelopment of multilingual speech and machine translation by its reduced\nrepresentation errors and incorporated external knowledge. However, both\ntranslation tasks typically utilize beam search decoding and top-1 hypothesis\nselection for inference. These techniques struggle to fully exploit the rich\ninformation in the diverse N-best hypotheses, making them less optimal for\ntranslation tasks that require a single, high-quality output sequence. In this\npaper, we propose a new generative paradigm for translation tasks, namely\n\"GenTranslate\", which builds upon LLMs to generate better results from the\ndiverse translation versions in N-best list. Leveraging the rich linguistic\nknowledge and strong reasoning abilities of LLMs, our new paradigm can\nintegrate the rich information in N-best candidates to generate a\nhigher-quality translation result. Furthermore, to support LLM finetuning, we\nbuild and release a HypoTranslate dataset that contains over 592K\nhypotheses-translation pairs in 11 languages. Experiments on various speech and\nmachine translation benchmarks (e.g., FLEURS, CoVoST-2, WMT) demonstrate that\nour GenTranslate significantly outperforms the state-of-the-art model.", "published": "2024-02-10 07:20:49", "link": "http://arxiv.org/abs/2402.06894v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Gemini Goes to Med School: Exploring the Capabilities of Multimodal\n  Large Language Models on Medical Challenge Problems & Hallucinations", "abstract": "Large language models have the potential to be valuable in the healthcare\nindustry, but it's crucial to verify their safety and effectiveness through\nrigorous evaluation. For this purpose, we comprehensively evaluated both\nopen-source LLMs and Google's new multimodal LLM called Gemini across Medical\nreasoning, hallucination detection, and Medical Visual Question Answering\ntasks. While Gemini showed competence, it lagged behind state-of-the-art models\nlike MedPaLM 2 and GPT-4 in diagnostic accuracy. Additionally, Gemini achieved\nan accuracy of 61.45\\% on the medical VQA dataset, significantly lower than\nGPT-4V's score of 88\\%. Our analysis revealed that Gemini is highly susceptible\nto hallucinations, overconfidence, and knowledge gaps, which indicate risks if\ndeployed uncritically. We also performed a detailed analysis by medical subject\nand test type, providing actionable feedback for developers and clinicians. To\nmitigate risks, we applied prompting strategies that improved performance.\nAdditionally, we facilitated future research and development by releasing a\nPython module for medical LLM evaluation and establishing a dedicated\nleaderboard on Hugging Face for medical domain LLMs. Python module can be found\nat https://github.com/promptslab/RosettaEval", "published": "2024-02-10 19:08:28", "link": "http://arxiv.org/abs/2402.07023v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Analysis of Self-Supervised Speech Models on Children's Speech and\n  Infant Vocalizations", "abstract": "To understand why self-supervised learning (SSL) models have empirically\nachieved strong performances on several speech-processing downstream tasks,\nnumerous studies have focused on analyzing the encoded information of the SSL\nlayer representations in adult speech. Limited work has investigated how\npre-training and fine-tuning affect SSL models encoding children's speech and\nvocalizations. In this study, we aim to bridge this gap by probing SSL models\non two relevant downstream tasks: (1) phoneme recognition (PR) on the speech of\nadults, older children (8-10 years old), and younger children (1-4 years old),\nand (2) vocalization classification (VC) distinguishing cry, fuss, and babble\nfor infants under 14 months old. For younger children's PR, the superiority of\nfine-tuned SSL models is largely due to their ability to learn features that\nrepresent older children's speech and then adapt those features to the speech\nof younger children. For infant VC, SSL models pre-trained on large-scale home\nrecordings learn to leverage phonetic representations at middle layers, and\nthereby enhance the performance of this task.", "published": "2024-02-10 05:20:50", "link": "http://arxiv.org/abs/2402.06888v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Cacophony: An Improved Contrastive Audio-Text Model", "abstract": "Despite recent advancements, audio-text models still lag behind their\nimage-text counterparts in scale and performance. In this paper, we propose to\nimprove both the data scale and the training procedure of audio-text\ncontrastive models. Specifically, we craft a large-scale audio-text dataset\ncontaining 13,000 hours of text-labeled audio, using pretrained language models\nto process noisy text descriptions and automatic captioning to obtain text\ndescriptions for unlabeled audio samples. We first train on audio-only data\nwith a masked autoencoder (MAE) objective, which allows us to benefit from the\nscalability of unlabeled audio datasets. We then train a contrastive model with\nan auxiliary captioning objective with the audio encoder initialized from the\nMAE model. Our final model, which we name Cacophony, achieves state-of-the-art\nperformance on audio-text retrieval tasks, and exhibits competitive results on\nthe HEAR benchmark and other downstream tasks such as zero-shot classification.", "published": "2024-02-10 16:34:26", "link": "http://arxiv.org/abs/2402.06986v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Implementation of Kalman Filter Approach for Active Noise Control by\n  Using MATLAB: Dynamic Noise Cancellation", "abstract": "This article offers an elaborate description of a Kalman filter code employed\nin the active control system. Conventional active noise management methods\nusually employ an adaptive filter, such as the filtered reference least mean\nsquare (FxLMS) algorithm, to adjust to changes in the primary noise and\nacoustic environment. Nevertheless, the slow convergence characteristics of the\nFxLMS algorithm typically impact the effectiveness of reducing dynamic noise.\nHence, this study suggests employing the Kalman filter in the active noise\ncontrol (ANC) system to enhance the efficacy of noise reduction for dynamic\nnoise. The ANC application effectively utilizes the Kalman filter with a novel\ndynamic ANC model. The numerical simulation revealed that the proposed Kalman\nfilter exhibits superior convergence performance compared to the FxLMS\nalgorithm for handling dynamic noise. The code is available on\n\\href{https://github.com/ShiDongyuan/Kalman_Filter_for_ANC.git}{GitHub} and\n\\href{https://www.mathworks.com/matlabcentral/fileexchange/159311-kalman-filter-for-active-noise-control}{MathWorks}.", "published": "2024-02-10 07:41:24", "link": "http://arxiv.org/abs/2402.06896v1", "categories": ["eess.SY", "cs.SY", "eess.AS", "eess.SP"], "primary_category": "eess.SY"}
{"title": "CochCeps-Augment: A Novel Self-Supervised Contrastive Learning Using\n  Cochlear Cepstrum-based Masking for Speech Emotion Recognition", "abstract": "Self-supervised learning (SSL) for automated speech recognition in terms of\nits emotional content, can be heavily degraded by the presence noise, affecting\nthe efficiency of modeling the intricate temporal and spectral informative\nstructures of speech. Recently, SSL on large speech datasets, as well as new\naudio-specific SSL proxy tasks, such as, temporal and frequency masking, have\nemerged, yielding superior performance compared to classic approaches drawn\nfrom the image augmentation domain. Our proposed contribution builds upon this\nsuccessful paradigm by introducing CochCeps-Augment, a novel bio-inspired\nmasking augmentation task for self-supervised contrastive learning of speech\nrepresentations. Specifically, we utilize the newly introduced bio-inspired\ncochlear cepstrogram (CCGRAM) to derive noise robust representations of input\nspeech, that are then further refined through a self-supervised learning\nscheme. The latter employs SimCLR to generate contrastive views of a CCGRAM\nthrough masking of its angle and quefrency dimensions. Our experimental\napproach and validations on the emotion recognition K-EmoCon benchmark dataset,\nfor the first time via a speaker-independent approach, features unsupervised\npre-training, linear probing and fine-tuning. Our results potentiate\nCochCeps-Augment to serve as a standard tool in speech emotion recognition\nanalysis, showing the added value of incorporating bio-inspired masking as an\ninformative augmentation task for self-supervision. Our code for implementing\nCochCeps-Augment will be made available at:\nhttps://github.com/GiannisZgs/CochCepsAugment.", "published": "2024-02-10 11:13:13", "link": "http://arxiv.org/abs/2402.06923v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Speech motion anomaly detection via cross-modal translation of 4D motion\n  fields from tagged MRI", "abstract": "Understanding the relationship between tongue motion patterns during speech\nand their resulting speech acoustic outcomes -- i.e., articulatory-acoustic\nrelation -- is of great importance in assessing speech quality and developing\ninnovative treatment and rehabilitative strategies. This is especially\nimportant when evaluating and detecting abnormal articulatory features in\npatients with speech-related disorders. In this work, we aim to develop a\nframework for detecting speech motion anomalies in conjunction with their\ncorresponding speech acoustics. This is achieved through the use of a deep\ncross-modal translator trained on data from healthy individuals only, which\nbridges the gap between 4D motion fields obtained from tagged MRI and 2D\nspectrograms derived from speech acoustic data. The trained translator is used\nas an anomaly detector, by measuring the spectrogram reconstruction quality on\nhealthy individuals or patients. In particular, the cross-modal translator is\nlikely to yield limited generalization capabilities on patient data, which\nincludes unseen out-of-distribution patterns and demonstrates subpar\nperformance, when compared with healthy individuals.~A one-class SVM is then\nused to distinguish the spectrograms of healthy individuals from those of\npatients. To validate our framework, we collected a total of 39 paired tagged\nMRI and speech waveforms, consisting of data from 36 healthy individuals and 3\ntongue cancer patients. We used both 3D convolutional and transformer-based\ndeep translation models, training them on the healthy training set and then\napplying them to both the healthy and patient testing sets. Our framework\ndemonstrates a capability to detect abnormal patient data, thereby illustrating\nits potential in enhancing the understanding of the articulatory-acoustic\nrelation for both healthy individuals and patients.", "published": "2024-02-10 16:16:24", "link": "http://arxiv.org/abs/2402.06984v1", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS", "eess.IV"], "primary_category": "cs.SD"}
