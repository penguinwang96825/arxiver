{"title": "Adversarially Constructed Evaluation Sets Are More Challenging, but May\n  Not Be Fair", "abstract": "More capable language models increasingly saturate existing task benchmarks,\nin some cases outperforming humans. This has left little headroom with which to\nmeasure further progress. Adversarial dataset creation has been proposed as a\nstrategy to construct more challenging datasets, and two common approaches are:\n(1) filtering out easy examples and (2) model-in-the-loop data collection. In\nthis work, we study the impact of applying each approach to create more\nchallenging evaluation datasets. We adapt the AFLite algorithm to filter\nevaluation data, and run experiments against 18 different adversary models. We\nfind that AFLite indeed selects more challenging examples, lowering the\nperformance of evaluated models more as stronger adversary models are used.\nHowever, the resulting ranking of models can also be unstable and highly\nsensitive to the choice of adversary model used. Moreover, AFLite oversamples\nexamples with low annotator agreement, meaning that model comparisons hinge on\nthe most contentiously labeled examples. Smaller-scale experiments on the\nadversarially collected datasets ANLI and AdversarialQA show similar findings,\nbroadly lowering performance with stronger adversaries while disproportionately\naffecting the adversary model.", "published": "2021-11-16 01:45:26", "link": "http://arxiv.org/abs/2111.08181v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Meeting Summarization with Pre-training and Clustering Methods", "abstract": "Automatic meeting summarization is becoming increasingly popular these days.\nThe ability to automatically summarize meetings and to extract key information\ncould greatly increase the efficiency of our work and life. In this paper, we\nexperiment with different approaches to improve the performance of query-based\nmeeting summarization. We started with HMNet\\cite{hmnet}, a hierarchical\nnetwork that employs both a word-level transformer and a turn-level\ntransformer, as the baseline. We explore the effectiveness of pre-training the\nmodel with a large news-summarization dataset. We investigate adding the\nembeddings of queries as a part of the input vectors for query-based\nsummarization. Furthermore, we experiment with extending the\nlocate-then-summarize approach of QMSum\\cite{qmsum} with an intermediate\nclustering step. Lastly, we compare the performance of our baseline models with\nBART, a state-of-the-art language model that is effective for summarization. We\nachieved improved performance by adding query embeddings to the input of the\nmodel, by using BART as an alternative language model, and by using clustering\nmethods to extract key information at utterance level before feeding the text\ninto summarization models.", "published": "2021-11-16 03:14:40", "link": "http://arxiv.org/abs/2111.08210v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Few-Shot Self-Rationalization with Natural Language Prompts", "abstract": "Self-rationalization models that predict task labels and generate free-text\nelaborations for their predictions could enable more intuitive interaction with\nNLP systems. These models are, however, currently trained with a large amount\nof human-written free-text explanations for each task which hinders their\nbroader usage. We propose to study a more realistic setting of\nself-rationalization using few training examples. We present FEB -- a\nstandardized collection of four existing English-language datasets and\nassociated metrics. We identify the right prompting approach by extensively\nexploring natural language prompts on FEB. Then, by using this prompt and\nscaling the model size, we demonstrate that making progress on few-shot\nself-rationalization is possible. We show there is still ample room for\nimprovement in this task: the average plausibility of generated explanations\nassessed by human annotators is at most 51% (with GPT-3), while plausibility of\nhuman explanations is 76%. We hope that FEB and our proposed approach will spur\nthe community to take on the few-shot self-rationalization challenge.", "published": "2021-11-16 08:21:40", "link": "http://arxiv.org/abs/2111.08284v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The role of attraction-repulsion dynamics in simulating the emergence of\n  inflectional class systems", "abstract": "Dynamic models of paradigm change can elucidate how the simplest of processes\nmay lead to unexpected outcomes, and thereby can reveal new potential\nexplanations for observed linguistic phenomena. Ackerman & Malouf (2015)\npresent a model in which inflectional systems reduce in disorder through the\naction of an attraction-only dynamic, in which lexemes only ever grow more\nsimilar to one another over time. Here we emphasise that: (1) Attraction-only\nmodels cannot evolve the structured diversity which characterises true\ninflectional systems, because they inevitably remove all variation; and (2)\nModels with both attraction and repulsion enable the emergence of systems that\nare strikingly reminiscent of morphomic structure such as inflection classes.\nThus, just one small ingredient -- change based on dissimilarity -- separates\nmodels that tend inexorably to uniformity, and which therefore are implausible\nfor inflectional morphology, from those which evolve stable, morphome-like\nstructure. These models have the potential to alter how we attempt to account\nfor morphological complexity.", "published": "2021-11-16 13:39:26", "link": "http://arxiv.org/abs/2111.08465v1", "categories": ["cs.CL", "J.5"], "primary_category": "cs.CL"}
{"title": "Coral: An Approach for Conversational Agents in Mental Health\n  Applications", "abstract": "It may be difficult for some individuals to open up and share their thoughts\nand feelings in front of a mental health expert. For those who are more at ease\nwith a virtual agent, conversational agents can serve as an intermediate step\nin the right direction. The conversational agent must therefore be empathetic\nand able to conduct free-flowing conversations. To this effect, we present an\napproach for creating a generative empathetic open-domain chatbot that can be\nused for mental health applications. We leverage large scale pre-training and\nempathetic conversational data to make the responses more empathetic in nature\nand a multi-turn dialogue arrangement to maintain context. Our models achieve\nstate-of-the-art results on the Empathetic Dialogues test set.", "published": "2021-11-16 15:15:58", "link": "http://arxiv.org/abs/2111.08545v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Document AI: Benchmarks, Models and Applications", "abstract": "Document AI, or Document Intelligence, is a relatively new research topic\nthat refers to the techniques for automatically reading, understanding, and\nanalyzing business documents. It is an important research direction for natural\nlanguage processing and computer vision. In recent years, the popularity of\ndeep learning technology has greatly advanced the development of Document AI,\nsuch as document layout analysis, visual information extraction, document\nvisual question answering, document image classification, etc. This paper\nbriefly reviews some of the representative models, tasks, and benchmark\ndatasets. Furthermore, we also introduce early-stage heuristic rule-based\ndocument analysis, statistical machine learning algorithms, and deep learning\napproaches especially pre-training methods. Finally, we look into future\ndirections for Document AI research.", "published": "2021-11-16 16:43:07", "link": "http://arxiv.org/abs/2111.08609v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "User Response and Sentiment Prediction for Automatic Dialogue Evaluation", "abstract": "Automatic evaluation is beneficial for open-domain dialog system development.\nHowever, standard word-overlap metrics (BLEU, ROUGE) do not correlate well with\nhuman judgements of open-domain dialog systems. In this work we propose to use\nthe sentiment of the next user utterance for turn or dialog level evaluation.\nSpecifically we propose three methods: one that predicts the next sentiment\ndirectly, and two others that predict the next user utterance using an\nutterance or a feedback generator model and then classify its sentiment.\nExperiments show our model outperforming existing automatic evaluation metrics\non both written and spoken open-domain dialogue datasets.", "published": "2021-11-16 22:19:17", "link": "http://arxiv.org/abs/2111.08808v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Attention-based Multi-hypothesis Fusion for Speech Summarization", "abstract": "Speech summarization, which generates a text summary from speech, can be\nachieved by combining automatic speech recognition (ASR) and text summarization\n(TS). With this cascade approach, we can exploit state-of-the-art models and\nlarge training datasets for both subtasks, i.e., Transformer for ASR and\nBidirectional Encoder Representations from Transformers (BERT) for TS. However,\nASR errors directly affect the quality of the output summary in the cascade\napproach. We propose a cascade speech summarization model that is robust to ASR\nerrors and that exploits multiple hypotheses generated by ASR to attenuate the\neffect of ASR errors on the summary. We investigate several schemes to combine\nASR hypotheses. First, we propose using the sum of sub-word embedding vectors\nweighted by their posterior values provided by an ASR system as an input to a\nBERT-based TS system. Then, we introduce a more general scheme that uses an\nattention-based fusion module added to a pre-trained BERT module to align and\ncombine several ASR hypotheses. Finally, we perform speech summarization\nexperiments on the How2 dataset and a newly assembled TED-based dataset that we\nwill release with this paper. These experiments show that retraining the\nBERT-based TS system with these schemes can improve summarization performance\nand that the attention-based fusion module is particularly effective.", "published": "2021-11-16 03:00:29", "link": "http://arxiv.org/abs/2111.08201v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual\n  Concepts", "abstract": "Most existing methods in vision language pre-training rely on object-centric\nfeatures extracted through object detection and make fine-grained alignments\nbetween the extracted features and texts. It is challenging for these methods\nto learn relations among multiple objects. To this end, we propose a new method\ncalled X-VLM to perform `multi-grained vision language pre-training.' The key\nto learning multi-grained alignments is to locate visual concepts in the image\ngiven the associated texts, and in the meantime align the texts with the visual\nconcepts, where the alignments are in multi-granularity. Experimental results\nshow that X-VLM effectively leverages the learned multi-grained alignments to\nmany downstream vision language tasks and consistently outperforms\nstate-of-the-art methods.", "published": "2021-11-16 07:55:26", "link": "http://arxiv.org/abs/2111.08276v3", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Multi-Vector Models with Textual Guidance for Fine-Grained Scientific\n  Document Similarity", "abstract": "We present a new scientific document similarity model based on matching\nfine-grained aspects of texts. To train our model, we exploit a\nnaturally-occurring source of supervision: sentences in the full-text of papers\nthat cite multiple papers together (co-citations). Such co-citations not only\nreflect close paper relatedness, but also provide textual descriptions of how\nthe co-cited papers are related. This novel form of textual supervision is used\nfor learning to match aspects across papers. We develop multi-vector\nrepresentations where vectors correspond to sentence-level aspects of\ndocuments, and present two methods for aspect matching: (1) A fast method that\nonly matches single aspects, and (2) a method that makes sparse multiple\nmatches with an Optimal Transport mechanism that computes an Earth Mover's\nDistance between aspects. Our approach improves performance on document\nsimilarity tasks in four datasets. Further, our fast single-match method\nachieves competitive results, paving the way for applying fine-grained\nsimilarity to large scientific corpora. Code, data, and models available at:\nhttps://github.com/allenai/aspire", "published": "2021-11-16 11:12:30", "link": "http://arxiv.org/abs/2111.08366v3", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "STAMP 4 NLP -- An Agile Framework for Rapid Quality-Driven NLP\n  Applications Development", "abstract": "The progress in natural language processing (NLP) research over the last\nyears, offers novel business opportunities for companies, as automated user\ninteraction or improved data analysis. Building sophisticated NLP applications\nrequires dealing with modern machine learning (ML) technologies, which impedes\nenterprises from establishing successful NLP projects. Our experience in\napplied NLP research projects shows that the continuous integration of research\nprototypes in production-like environments with quality assurance builds trust\nin the software and shows convenience and usefulness regarding the business\ngoal. We introduce STAMP 4 NLP as an iterative and incremental process model\nfor developing NLP applications. With STAMP 4 NLP, we merge software\nengineering principles with best practices from data science. Instantiating our\nprocess model allows efficiently creating prototypes by utilizing templates,\nconventions, and implementations, enabling developers and data scientists to\nfocus on the business goals. Due to our iterative-incremental approach,\nbusinesses can deploy an enhanced version of the prototype to their software\nenvironment after every iteration, maximizing potential business value and\ntrust early and avoiding the cost of successful yet never deployed experiments.", "published": "2021-11-16 12:20:47", "link": "http://arxiv.org/abs/2111.08408v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Generative Pre-Trained Transformer for Design Concept Generation: An\n  Exploration", "abstract": "Novel concepts are essential for design innovation and can be generated with\nthe aid of data stimuli and computers. However, current generative design\nalgorithms focus on diagrammatic or spatial concepts that are either too\nabstract to understand or too detailed for early phase design exploration. This\npaper explores the uses of generative pre-trained transformers (GPT) for\nnatural language design concept generation. Our experiments involve the use of\nGPT-2 and GPT-3 for different creative reasonings in design tasks. Both show\nreasonably good performance for verbal design concept generation.", "published": "2021-11-16 14:12:08", "link": "http://arxiv.org/abs/2111.08489v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving the robustness and accuracy of biomedical language models\n  through adversarial training", "abstract": "Deep transformer neural network models have improved the predictive accuracy\nof intelligent text processing systems in the biomedical domain. They have\nobtained state-of-the-art performance scores on a wide variety of biomedical\nand clinical Natural Language Processing (NLP) benchmarks. However, the\nrobustness and reliability of these models has been less explored so far.\nNeural NLP models can be easily fooled by adversarial samples, i.e. minor\nchanges to input that preserve the meaning and understandability of the text\nbut force the NLP system to make erroneous decisions. This raises serious\nconcerns about the security and trust-worthiness of biomedical NLP systems,\nespecially when they are intended to be deployed in real-world use cases. We\ninvestigated the robustness of several transformer neural language models, i.e.\nBioBERT, SciBERT, BioMed-RoBERTa, and Bio-ClinicalBERT, on a wide range of\nbiomedical and clinical text processing tasks. We implemented various\nadversarial attack methods to test the NLP systems in different attack\nscenarios. Experimental results showed that the biomedical NLP models are\nsensitive to adversarial samples; their performance dropped in average by 21\nand 18.9 absolute percent on character-level and word-level adversarial noise,\nrespectively. Conducting extensive adversarial training experiments, we\nfine-tuned the NLP models on a mixture of clean samples and adversarial inputs.\nResults showed that adversarial training is an effective defense mechanism\nagainst adversarial noise; the models robustness improved in average by 11.3\nabsolute percent. In addition, the models performance on clean data increased\nin average by 2.4 absolute present, demonstrating that adversarial training can\nboost generalization abilities of biomedical NLP systems.", "published": "2021-11-16 14:58:05", "link": "http://arxiv.org/abs/2111.08529v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Interpreting Language Models Through Knowledge Graph Extraction", "abstract": "Transformer-based language models trained on large text corpora have enjoyed\nimmense popularity in the natural language processing community and are\ncommonly used as a starting point for downstream tasks. While these models are\nundeniably useful, it is a challenge to quantify their performance beyond\ntraditional accuracy metrics. In this paper, we compare BERT-based language\nmodels through snapshots of acquired knowledge at sequential stages of the\ntraining process. Structured relationships from training corpora may be\nuncovered through querying a masked language model with probing tasks. We\npresent a methodology to unveil a knowledge acquisition timeline by generating\nknowledge graph extracts from cloze \"fill-in-the-blank\" statements at various\nstages of RoBERTa's early training. We extend this analysis to a comparison of\npretrained variations of BERT models (DistilBERT, BERT-base, RoBERTa). This\nwork proposes a quantitative framework to compare language models through\nknowledge graph extraction (GED, Graph2Vec) and showcases a part-of-speech\nanalysis (POSOR) to identify the linguistic strengths of each model variant.\nUsing these metrics, machine learning practitioners can compare models,\ndiagnose their models' behavioral strengths and weaknesses, and identify new\ntargeted datasets to improve model performance.", "published": "2021-11-16 15:18:01", "link": "http://arxiv.org/abs/2111.08546v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "NVIDIA NeMo Neural Machine Translation Systems for English-German and\n  English-Russian News and Biomedical Tasks at WMT21", "abstract": "This paper provides an overview of NVIDIA NeMo's neural machine translation\nsystems for the constrained data track of the WMT21 News and Biomedical Shared\nTranslation Tasks. Our news task submissions for English-German (En-De) and\nEnglish-Russian (En-Ru) are built on top of a baseline transformer-based\nsequence-to-sequence model. Specifically, we use a combination of 1) checkpoint\naveraging 2) model scaling 3) data augmentation with backtranslation and\nknowledge distillation from right-to-left factorized models 4) finetuning on\ntest sets from previous years 5) model ensembling 6) shallow fusion decoding\nwith transformer language models and 7) noisy channel re-ranking. Additionally,\nour biomedical task submission for English-Russian uses a biomedically biased\nvocabulary and is trained from scratch on news task data, medically relevant\ntext curated from the news task dataset, and biomedical data provided by the\nshared task. Our news system achieves a sacreBLEU score of 39.5 on the WMT'20\nEn-De test set outperforming the best submission from last year's task of 38.8.\nOur biomedical task Ru-En and En-Ru systems reach BLEU scores of 43.8 and 40.3\nrespectively on the WMT'20 Biomedical Task Test set, outperforming the previous\nyear's best submissions.", "published": "2021-11-16 17:24:11", "link": "http://arxiv.org/abs/2111.08634v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DataCLUE: A Benchmark Suite for Data-centric NLP", "abstract": "Data-centric AI has recently proven to be more effective and\nhigh-performance, while traditional model-centric AI delivers fewer and fewer\nbenefits. It emphasizes improving the quality of datasets to achieve better\nmodel performance. This field has significant potential because of its great\npracticability and getting more and more attention. However, we have not seen\nsignificant research progress in this field, especially in NLP. We propose\nDataCLUE, which is the first Data-Centric benchmark applied in NLP field. We\nalso provide three simple but effective baselines to foster research in this\nfield (improve Macro-F1 up to 5.7% point). In addition, we conduct\ncomprehensive experiments with human annotators and show the hardness of\nDataCLUE. We also try an advanced method: the forgetting informed bootstrapping\nlabel correction method. All the resources related to DataCLUE, including\ndatasets, toolkit, leaderboard, and baselines, is available online at\nhttps://github.com/CLUEbenchmark/DataCLUE", "published": "2021-11-16 17:30:56", "link": "http://arxiv.org/abs/2111.08647v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Comparative Study on Transfer Learning and Distance Metrics in\n  Semantic Clustering over the COVID-19 Tweets", "abstract": "This paper is a comparison study in the context of Topic Detection on\nCOVID-19 data. There are various approaches for Topic Detection, among which\nthe Clustering approach is selected in this paper. Clustering requires distance\nand calculating distance needs embedding. The aim of this research is to\nsimultaneously study the three factors of embedding methods, distance metrics\nand clustering methods and their interaction. A dataset including one-month\ntweets collected with COVID-19-related hashtags is used for this study. Five\nmethods, from earlier to new methods, are selected among the embedding methods:\nWord2Vec, fastText, GloVe, BERT and T5. Five clustering methods are\ninvestigated in this paper that are: k-means, DBSCAN, OPTICS, spectral and\nJarvis-Patrick. Euclidian distance and Cosine distance as the most important\ndistance metrics in this field are also examined. First, more than 7,500 tests\nare performed to tune the parameters. Then, all the different combinations of\nembedding methods with distance metrics and clustering methods are investigated\nby silhouette metric. The number of these combinations is 50 cases. First, the\nresults of these 50 tests are examined. Then, the rank of each method is taken\ninto account in all the tests of that method. Finally, the major variables of\nthe research (embedding methods, distance metrics and clustering methods) are\nstudied separately. Averaging is performed over the control variables to\nneutralize their effect. The experimental results show that T5 strongly\noutperforms other embedding methods in terms of silhouette metric. In terms of\ndistance metrics, cosine distance is weakly better. DBSCAN is also superior to\nother methods in terms of clustering methods.", "published": "2021-11-16 17:44:24", "link": "http://arxiv.org/abs/2111.08658v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Facilitating reflection in teletandem through automatically generated\n  conversation metrics and playback video", "abstract": "This pilot study focuses on a tool called L2L that allows second language\n(L2) learners to visualise and analyse their Zoom interactions with native\nspeakers. L2L uses the Zoom transcript to automatically generate conversation\nmetrics and its playback feature with timestamps allows students to replay any\nchosen portion of the conversation for post-session reflection and self-review.\nThis exploratory study investigates a seven-week teletandem project, where\nundergraduate students from an Irish University learning French (B2) interacted\nwith their peers from a French University learning English (B2+) via Zoom. The\ndata collected from a survey (N=43) and semi-structured interviews (N=35) show\nthat the quantitative conversation metrics and qualitative review of the\nsynchronous content helped raise students' confidence levels while engaging\nwith native speakers. Furthermore, it allowed them to set tangible goals to\nimprove their participation, and be more aware of what, why and how they are\nlearning.", "published": "2021-11-16 21:33:07", "link": "http://arxiv.org/abs/2111.08788v2", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Solving Linear Algebra by Program Synthesis", "abstract": "We solve MIT's Linear Algebra 18.06 course and Columbia University's\nComputational Linear Algebra COMS3251 courses with perfect accuracy by\ninteractive program synthesis. This surprisingly strong result is achieved by\nturning the course questions into programming tasks and then running the\nprograms to produce the correct answers. We use OpenAI Codex with zero-shot\nlearning, without providing any examples in the prompts, to synthesize code\nfrom questions. We quantify the difference between the original question text\nand the transformed question text that yields a correct answer. Since all\nCOMS3251 questions are not available online the model is not overfitting. We go\nbeyond just generating code for questions with numerical answers by\ninteractively generating code that also results visually pleasing plots as\noutput. Finally, we automatically generate new questions given a few sample\nquestions which may be used as new course content. This work is a significant\nstep forward in solving quantitative math problems and opens the door for\nsolving many university level STEM courses by machine.", "published": "2021-11-16 01:16:43", "link": "http://arxiv.org/abs/2111.08171v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "CoCA-MDD: A Coupled Cross-Attention based Framework for Streaming\n  Mispronunciation Detection and Diagnosis", "abstract": "Mispronunciation detection and diagnosis (MDD) is a popular research focus in\ncomputer-aided pronunciation training (CAPT) systems. End-to-end (e2e)\napproaches are becoming dominant in MDD. However an e2e MDD model usually\nrequires entire speech utterances as input context, which leads to significant\ntime latency especially for long paragraphs. We propose a streaming e2e MDD\nmodel called CoCA-MDD. We utilize conv-transformer structure to encode input\nspeech in a streaming manner. A coupled cross-attention (CoCA) mechanism is\nproposed to integrate frame-level acoustic features with encoded reference\nlinguistic features. CoCA also enables our model to perform mispronunciation\nclassification with whole utterances. The proposed model allows system fusion\nbetween the streaming output and mispronunciation classification output for\nfurther performance enhancement. We evaluate CoCA-MDD on publicly available\ncorpora. CoCA-MDD achieves F1 scores of 57.03% and 60.78% for streaming and\nfusion modes respectively on L2-ARCTIC. For phone-level pronunciation scoring,\nCoCA-MDD achieves 0.58 Pearson correlation coefficient (PCC) value on\nSpeechOcean762.", "published": "2021-11-16 02:17:49", "link": "http://arxiv.org/abs/2111.08191v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Solving Probability and Statistics Problems by Program Synthesis", "abstract": "We solve university level probability and statistics questions by program\nsynthesis using OpenAI's Codex, a Transformer trained on text and fine-tuned on\ncode. We transform course problems from MIT's 18.05 Introduction to Probability\nand Statistics and Harvard's STAT110 Probability into programming tasks. We\nthen execute the generated code to get a solution. Since these course questions\nare grounded in probability, we often aim to have Codex generate probabilistic\nprograms that simulate a large number of probabilistic dependencies to compute\nits solution. Our approach requires prompt engineering to transform the\nquestion from its original form to an explicit, tractable form that results in\na correct program and solution. To estimate the amount of work needed to\ntranslate an original question into its tractable form, we measure the\nsimilarity between original and transformed questions. Our work is the first to\nintroduce a new dataset of university-level probability and statistics problems\nand solve these problems in a scalable fashion using the program synthesis\ncapabilities of large language models.", "published": "2021-11-16 07:34:25", "link": "http://arxiv.org/abs/2111.08267v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.PL"], "primary_category": "cs.LG"}
{"title": "Literature-Augmented Clinical Outcome Prediction", "abstract": "We present BEEP (Biomedical Evidence-Enhanced Predictions), a novel approach\nfor clinical outcome prediction that retrieves patient-specific medical\nliterature and incorporates it into predictive models. Based on each individual\npatient's clinical notes, we train language models (LMs) to find relevant\npapers and fuse them with information from notes to predict outcomes such as\nin-hospital mortality. We develop methods to retrieve literature based on\nnoisy, information-dense patient notes, and to augment existing outcome\nprediction models with retrieved papers in a manner that maximizes predictive\naccuracy. Our approach boosts predictive performance on three important\nclinical tasks in comparison to strong recent LM baselines, increasing F1 by up\nto 5 points and precision@Top-K by a large margin of over 25%.", "published": "2021-11-16 11:19:02", "link": "http://arxiv.org/abs/2111.08374v3", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Integrated Semantic and Phonetic Post-correction for Chinese Speech\n  Recognition", "abstract": "Due to the recent advances of natural language processing, several works have\napplied the pre-trained masked language model (MLM) of BERT to the\npost-correction of speech recognition. However, existing pre-trained models\nonly consider the semantic correction while the phonetic features of words is\nneglected. The semantic-only post-correction will consequently decrease the\nperformance since homophonic errors are fairly common in Chinese ASR. In this\npaper, we proposed a novel approach to collectively exploit the contextualized\nrepresentation and the phonetic information between the error and its replacing\ncandidates to alleviate the error rate of Chinese ASR. Our experiment results\non real world speech recognition datasets showed that our proposed method has\nevidently lower CER than the baseline model, which utilized a pre-trained BERT\nMLM as the corrector.", "published": "2021-11-16 11:55:27", "link": "http://arxiv.org/abs/2111.08400v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "CVSS-BERT: Explainable Natural Language Processing to Determine the\n  Severity of a Computer Security Vulnerability from its Description", "abstract": "When a new computer security vulnerability is publicly disclosed, only a\ntextual description of it is available. Cybersecurity experts later provide an\nanalysis of the severity of the vulnerability using the Common Vulnerability\nScoring System (CVSS). Specifically, the different characteristics of the\nvulnerability are summarized into a vector (consisting of a set of metrics),\nfrom which a severity score is computed. However, because of the high number of\nvulnerabilities disclosed everyday this process requires lot of manpower, and\nseveral days may pass before a vulnerability is analyzed. We propose to\nleverage recent advances in the field of Natural Language Processing (NLP) to\ndetermine the CVSS vector and the associated severity score of a vulnerability\nfrom its textual description in an explainable manner. To this purpose, we\ntrained multiple BERT classifiers, one for each metric composing the CVSS\nvector. Experimental results show that our trained classifiers are able to\ndetermine the value of the metrics of the CVSS vector with high accuracy. The\nseverity score computed from the predicted CVSS vector is also very close to\nthe real severity score attributed by a human expert. For explainability\npurpose, gradient-based input saliency method was used to determine the most\nrelevant input words for a given prediction made by our classifiers. Often, the\ntop relevant words include terms in agreement with the rationales of a human\ncybersecurity expert, making the explanation comprehensible for end-users.", "published": "2021-11-16 14:31:09", "link": "http://arxiv.org/abs/2111.08510v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "WikiContradiction: Detecting Self-Contradiction Articles on Wikipedia", "abstract": "While Wikipedia has been utilized for fact-checking and claim verification to\ndebunk misinformation and disinformation, it is essential to either improve\narticle quality and rule out noisy articles. Self-contradiction is one of the\nlow-quality article types in Wikipedia. In this work, we propose a task of\ndetecting self-contradiction articles in Wikipedia. Based on the\n\"self-contradictory\" template, we create a novel dataset for the\nself-contradiction detection task. Conventional contradiction detection focuses\non comparing pairs of sentences or claims, but self-contradiction detection\nneeds to further reason the semantics of an article and simultaneously learn\nthe contradiction-aware comparison from all pairs of sentences. Therefore, we\npresent the first model, Pairwise Contradiction Neural Network (PCNN), to not\nonly effectively identify self-contradiction articles, but also highlight the\nmost contradiction pairs of contradiction sentences. The main idea of PCNN is\ntwo-fold. First, to mitigate the effect of data scarcity on self-contradiction\narticles, we pre-train the module of pairwise contradiction learning using SNLI\nand MNLI benchmarks. Second, we select top-K sentence pairs with the highest\ncontradiction probability values and model their correlation to determine\nwhether the corresponding article belongs to self-contradiction. Experiments\nconducted on the proposed WikiContradiction dataset exhibit that PCNN can\ngenerate promising performance and comprehensively highlight the sentence pairs\nthe contradiction locates.", "published": "2021-11-16 15:12:37", "link": "http://arxiv.org/abs/2111.08543v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Words of Wisdom: Representational Harms in Learning From AI\n  Communication", "abstract": "Many educational technologies use artificial intelligence (AI) that presents\ngenerated or produced language to the learner. We contend that all language,\nincluding all AI communication, encodes information about the identity of the\nhuman or humans who contributed to crafting the language. With AI\ncommunication, however, the user may index identity information that does not\nmatch the source. This can lead to representational harms if language\nassociated with one cultural group is presented as \"standard\" or \"neutral\", if\nthe language advantages one group over another, or if the language reinforces\nnegative stereotypes. In this work, we discuss a case study using a Visual\nQuestion Generation (VQG) task involving gathering crowdsourced data from\ntargeted demographic groups. Generated questions will be presented to human\nevaluators to understand how they index the identity behind the language,\nwhether and how they perceive any representational harms, and how they would\nideally address any such harms caused by AI communication. We reflect on the\neducational applications of this work as well as the implications for equality,\ndiversity, and inclusion (EDI).", "published": "2021-11-16 15:59:49", "link": "http://arxiv.org/abs/2111.08581v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.HC"}
{"title": "Who Decides if AI is Fair? The Labels Problem in Algorithmic Auditing", "abstract": "Labelled \"ground truth\" datasets are routinely used to evaluate and audit AI\nalgorithms applied in high-stakes settings. However, there do not exist widely\naccepted benchmarks for the quality of labels in these datasets. We provide\nempirical evidence that quality of labels can significantly distort the results\nof algorithmic audits in real-world settings. Using data annotators typically\nhired by AI firms in India, we show that fidelity of the ground truth data can\nlead to spurious differences in performance of ASRs between urban and rural\npopulations. After a rigorous, albeit expensive, label cleaning process, these\ndisparities between groups disappear. Our findings highlight how trade-offs\nbetween label quality and data annotation costs can complicate algorithmic\naudits in practice. They also emphasize the need for development of\nconsensus-driven, widely accepted benchmarks for label quality.", "published": "2021-11-16 19:00:03", "link": "http://arxiv.org/abs/2111.08723v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SALSA-Lite: A Fast and Effective Feature for Polyphonic Sound Event\n  Localization and Detection with Microphone Arrays", "abstract": "Polyphonic sound event localization and detection (SELD) has many practical\napplications in acoustic sensing and monitoring. However, the development of\nreal-time SELD has been limited by the demanding computational requirement of\nmost recent SELD systems. In this work, we introduce SALSA-Lite, a fast and\neffective feature for polyphonic SELD using microphone array inputs. SALSA-Lite\nis a lightweight variation of a previously proposed SALSA feature for\npolyphonic SELD. SALSA, which stands for Spatial Cue-Augmented Log-Spectrogram,\nconsists of multichannel log-spectrograms stacked channelwise with the\nnormalized principal eigenvectors of the spectrotemporally corresponding\nspatial covariance matrices. In contrast to SALSA, which uses eigenvector-based\nspatial features, SALSA-Lite uses normalized inter-channel phase differences as\nspatial features, allowing a 30-fold speedup compared to the original SALSA\nfeature. Experimental results on the TAU-NIGENS Spatial Sound Events 2021\ndataset showed that the SALSA-Lite feature achieved competitive performance\ncompared to the full SALSA feature, and significantly outperformed the\ntraditional feature set of multichannel log-mel spectrograms with generalized\ncross-correlation spectra. Specifically, using SALSA-Lite features increased\nlocalization-dependent F1 score and class-dependent localization recall by 15%\nand 5%, respectively, compared to using multichannel log-mel spectrograms with\ngeneralized cross-correlation spectra.", "published": "2021-11-16 02:28:13", "link": "http://arxiv.org/abs/2111.08192v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "An Exploratory Study on Perceptual Spaces of the Singing Voice", "abstract": "Sixty participants provided dissimilarity ratings between various singing\ntechniques. Multidimensional scaling, class averaging and clustering techniques\nwere used to analyse timbral spaces and how they change between different\nsingers, genders and registers. Clustering analysis showed that ground-truth\nsimilarity and silhouette scores that were not significantly different between\ngender or register conditions, while similarity scores were positively\ncorrelated with participants' instrumental abilities and task comprehension.\nParticipant feedback showed how a revised study design might mitigate noise in\nour data, leading to more detailed statistical results. Timbre maps and class\ndistance analysis showed us which singing techniques remained similar to one\nanother across gender and register conditions. This research provides insight\ninto how the timbre space of singing changes under different conditions,\nhighlights the subjectivity of perception between participants, and provides\ngeneralised timbre maps for regularisation in machine learning.", "published": "2021-11-16 02:48:25", "link": "http://arxiv.org/abs/2111.08196v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Detecting acoustic reflectors using a robot's ego-noise", "abstract": "In this paper, we propose a method to estimate the proximity of an acoustic\nreflector, e.g., a wall, using ego-noise, i.e., the noise produced by the\nmoving parts of a listening robot. This is achieved by estimating the times of\narrival of acoustic echoes reflected from the surface. Simulated experiments\nshow that the proposed nonintrusive approach is capable of accurately\nestimating the distance of a reflector up to 1 meter and outperforms a\npreviously proposed intrusive approach under loud ego-noise conditions. The\nproposed method is helped by a probabilistic echo detector that estimates\nwhether or not an acoustic reflector is within a short range of the robotic\nplatform. This preliminary investigation paves the way towards a new kind of\ncollision avoidance system that would purely rely on audio sensors rather than\nconventional proximity sensors.", "published": "2021-11-16 09:56:03", "link": "http://arxiv.org/abs/2111.08327v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "S-DCCRN: Super Wide Band DCCRN with learnable complex feature for speech\n  enhancement", "abstract": "In speech enhancement, complex neural network has shown promising performance\ndue to their effectiveness in processing complex-valued spectrum. Most of the\nrecent speech enhancement approaches mainly focus on wide-band signal with a\nsampling rate of 16K Hz. However, research on super wide band (e.g., 32K Hz) or\neven full-band (48K) denoising is still lacked due to the difficulty of\nmodeling more frequency bands and particularly high frequency components. In\nthis paper, we extend our previous deep complex convolution recurrent neural\nnetwork (DCCRN) substantially to a super wide band version -- S-DCCRN, to\nperform speech denoising on speech of 32K Hz sampling rate. We first employ a\ncascaded sub-band and full-band processing module, which consists of two\nsmall-footprint DCCRNs -- one operates on sub-band signal and one operates on\nfull-band signal, aiming at benefiting from both local and global frequency\ninformation. Moreover, instead of simply adopting the STFT feature as input, we\nuse a complex feature encoder trained in an end-to-end manner to refine the\ninformation of different frequency bands. We also use a complex feature decoder\nto revert the feature to time-frequency domain. Finally, a learnable spectrum\ncompression method is adopted to adjust the energy of different frequency\nbands, which is beneficial for neural network learning. The proposed model,\nS-DCCRN, has surpassed PercepNet as well as several competitive models and\nachieves state-of-the-art performance in terms of speech quality and\nintelligibility. Ablation studies also demonstrate the effectiveness of\ndifferent contributions.", "published": "2021-11-16 11:34:34", "link": "http://arxiv.org/abs/2111.08387v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Unsupervised Speech Enhancement with speech recognition embedding and\n  disentanglement losses", "abstract": "Speech enhancement has recently achieved great success with various deep\nlearning methods. However, most conventional speech enhancement systems are\ntrained with supervised methods that impose two significant challenges. First,\na majority of training datasets for speech enhancement systems are synthetic.\nWhen mixing clean speech and noisy corpora to create the synthetic datasets,\ndomain mismatches occur between synthetic and real-world recordings of noisy\nspeech or audio. Second, there is a trade-off between increasing speech\nenhancement performance and degrading speech recognition (ASR) performance.\nThus, we propose an unsupervised loss function to tackle those two problems.\nOur function is developed by extending the MixIT loss function with speech\nrecognition embedding and disentanglement loss. Our results show that the\nproposed function effectively improves the speech enhancement performance\ncompared to a baseline trained in a supervised way on the noisy VoxCeleb\ndataset. While fully unsupervised training is unable to exceed the\ncorresponding baseline, with joint super- and unsupervised training, the system\nis able to achieve similar speech quality and better ASR performance than the\nbest supervised baseline.", "published": "2021-11-16 18:23:47", "link": "http://arxiv.org/abs/2111.08678v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Zero-shot Singing Technique Conversion", "abstract": "In this paper we propose modifications to the neural network framework,\nAutoVC for the task of singing technique conversion. This includes utilising a\npretrained singing technique encoder which extracts technique information, upon\nwhich a decoder is conditioned during training. By swapping out a source\nsinger's technique information for that of the target's during conversion, the\ninput spectrogram is reconstructed with the target's technique. We document the\nbeneficial effects of omitting the latent loss, the importance of sequential\ntraining, and our process for fine-tuning the bottleneck. We also conducted a\nlistening study where participants rate the specificity of technique-converted\nvoices as well as their naturalness. From this we are able to conclude how\neffective the technique conversions are and how different conditions affect\nthem, while assessing the model's ability to reconstruct its input data.", "published": "2021-11-16 23:53:17", "link": "http://arxiv.org/abs/2111.08839v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Video Background Music Generation with Controllable Music Transformer", "abstract": "In this work, we address the task of video background music generation. Some\nprevious works achieve effective music generation but are unable to generate\nmelodious music tailored to a particular video, and none of them considers the\nvideo-music rhythmic consistency. To generate the background music that matches\nthe given video, we first establish the rhythmic relations between video and\nbackground music. In particular, we connect timing, motion speed, and motion\nsaliency from video with beat, simu-note density, and simu-note strength from\nmusic, respectively. We then propose CMT, a Controllable Music Transformer that\nenables local control of the aforementioned rhythmic features and global\ncontrol of the music genre and instruments. Objective and subjective\nevaluations show that the generated background music has achieved satisfactory\ncompatibility with the input videos, and at the same time, impressive music\nquality. Code and models are available at\nhttps://github.com/wzk1015/video-bgm-generation.", "published": "2021-11-16 11:28:31", "link": "http://arxiv.org/abs/2111.08380v1", "categories": ["cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "Single-channel speech separation using Soft-minimum Permutation\n  Invariant Training", "abstract": "The goal of speech separation is to extract multiple speech sources from a\nsingle microphone recording. Recently, with the advancement of deep learning\nand availability of large datasets, speech separation has been formulated as a\nsupervised learning problem. These approaches aim to learn discriminative\npatterns of speech, speakers, and background noise using a supervised learning\nalgorithm, typically a deep neural network. A long-lasting problem in\nsupervised speech separation is finding the correct label for each separated\nspeech signal, referred to as label permutation ambiguity. Permutation\nambiguity refers to the problem of determining the output-label assignment\nbetween the separated sources and the available single-speaker speech labels.\nFinding the best output-label assignment is required for calculation of\nseparation error, which is later used for updating parameters of the model.\nRecently, Permutation Invariant Training (PIT) has been shown to be a promising\nsolution in handling the label ambiguity problem. However, the overconfident\nchoice of the output-label assignment by PIT results in a sub-optimal trained\nmodel. In this work, we propose a probabilistic optimization framework to\naddress the inefficiency of PIT in finding the best output-label assignment.\nOur proposed method entitled trainable Soft-minimum PIT is then employed on the\nsame Long-Short Term Memory (LSTM) architecture used in Permutation Invariant\nTraining (PIT) speech separation method. The results of our experiments show\nthat the proposed method outperforms conventional PIT speech separation\nsignificantly (p-value $ < 0.01$) by +1dB in Signal to Distortion Ratio (SDR)\nand +1.5dB in Signal to Interference Ratio (SIR).", "published": "2021-11-16 17:25:05", "link": "http://arxiv.org/abs/2111.08635v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
