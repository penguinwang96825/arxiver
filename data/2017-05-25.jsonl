{"title": "Max-Cosine Matching Based Neural Models for Recognizing Textual\n  Entailment", "abstract": "Recognizing textual entailment is a fundamental task in a variety of text\nmining or natural language processing applications. This paper proposes a\nsimple neural model for RTE problem. It first matches each word in the\nhypothesis with its most-similar word in the premise, producing an augmented\nrepresentation of the hypothesis conditioned on the premise as a sequence of\nword pairs. The LSTM model is then used to model this augmented sequence, and\nthe final output from the LSTM is fed into a softmax layer to make the\nprediction. Besides the base model, in order to enhance its performance, we\nalso proposed three techniques: the integration of multiple word-embedding\nlibrary, bi-way integration, and ensemble based on model averaging.\nExperimental results on the SNLI dataset have shown that the three techniques\nare effective in boosting the predicative accuracy and that our method\noutperforms several state-of-the-state ones.", "published": "2017-05-25 05:45:42", "link": "http://arxiv.org/abs/1705.09054v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Jointly Learning Sentence Embeddings and Syntax with Unsupervised\n  Tree-LSTMs", "abstract": "We introduce a neural network that represents sentences by composing their\nwords according to induced binary parse trees. We use Tree-LSTM as our\ncomposition function, applied along a tree structure found by a fully\ndifferentiable natural language chart parser. Our model simultaneously\noptimises both the composition function and the parser, thus eliminating the\nneed for externally-provided parse trees which are normally required for\nTree-LSTM. It can therefore be seen as a tree-based RNN that is unsupervised\nwith respect to the parse trees. As it is fully differentiable, our model is\neasily trained with an off-the-shelf gradient descent method and\nbackpropagation. We demonstrate that it achieves better performance compared to\nvarious supervised Tree-LSTM architectures on a textual entailment task and a\nreverse dictionary task.", "published": "2017-05-25 14:09:48", "link": "http://arxiv.org/abs/1705.09189v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Structured Text Representations", "abstract": "In this paper, we focus on learning structure-aware document representations\nfrom data without recourse to a discourse parser or additional annotations.\nDrawing inspiration from recent efforts to empower neural networks with a\nstructural bias, we propose a model that can encode a document while\nautomatically inducing rich structural dependencies. Specifically, we embed a\ndifferentiable non-projective parsing algorithm into a neural model and use\nattention mechanisms to incorporate the structural biases. Experimental\nevaluation across different tasks and datasets shows that the proposed model\nachieves state-of-the-art results on document modeling tasks while inducing\nintermediate structures which are both interpretable and meaningful.", "published": "2017-05-25 14:54:07", "link": "http://arxiv.org/abs/1705.09207v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Neural Models for Documents with Metadata", "abstract": "Most real-world document collections involve various types of metadata, such\nas author, source, and date, and yet the most commonly-used approaches to\nmodeling text corpora ignore this information. While specialized models have\nbeen developed for particular applications, few are widely used in practice, as\ncustomization typically requires derivation of a custom inference algorithm. In\nthis paper, we build on recent advances in variational inference methods and\npropose a general neural framework, based on topic models, to enable flexible\nincorporation of metadata and allow for rapid exploration of alternative\nmodels. Our approach achieves strong performance, with a manageable tradeoff\nbetween perplexity, coherence, and sparsity. Finally, we demonstrate the\npotential of our framework through an exploration of a corpus of articles about\nUS immigration.", "published": "2017-05-25 18:00:03", "link": "http://arxiv.org/abs/1705.09296v2", "categories": ["stat.ML", "cs.CL"], "primary_category": "stat.ML"}
{"title": "Deriving Neural Architectures from Sequence and Graph Kernels", "abstract": "The design of neural architectures for structured objects is typically guided\nby experimental insights rather than a formal process. In this work, we appeal\nto kernels over combinatorial structures, such as sequences and graphs, to\nderive appropriate neural operations. We introduce a class of deep recurrent\nneural operations and formally characterize their associated kernel spaces. Our\nrecurrent modules compare the input to virtual reference objects (cf. filters\nin CNN) via the kernels. Similar to traditional neural operations, these\nreference objects are parameterized and directly optimized in end-to-end\ntraining. We empirically evaluate the proposed class of neural architectures on\nstandard applications such as language modeling and molecular graph regression,\nachieving state-of-the-art results across these applications.", "published": "2017-05-25 03:58:10", "link": "http://arxiv.org/abs/1705.09037v3", "categories": ["cs.NE", "cs.CL", "cs.LG"], "primary_category": "cs.NE"}
