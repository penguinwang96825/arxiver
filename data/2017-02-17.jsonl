{"title": "Experiment Segmentation in Scientific Discourse as Clause-level\n  Structured Prediction using Recurrent Neural Networks", "abstract": "We propose a deep learning model for identifying structure within experiment\nnarratives in scientific literature. We take a sequence labeling approach to\nthis problem, and label clauses within experiment narratives to identify the\ndifferent parts of the experiment. Our dataset consists of paragraphs taken\nfrom open access PubMed papers labeled with rhetorical information as a result\nof our pilot annotation. Our model is a Recurrent Neural Network (RNN) with\nLong Short-Term Memory (LSTM) cells that labels clauses. The clause\nrepresentations are computed by combining word representations using a novel\nattention mechanism that involves a separate RNN. We compare this model against\nLSTMs where the input layer has simple or no attention and a feature rich CRF\nmodel. Furthermore, we describe how our work could be useful for information\nextraction from scientific literature.", "published": "2017-02-17 15:39:21", "link": "http://arxiv.org/abs/1702.05398v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analysis and Optimization of fastText Linear Text Classifier", "abstract": "The paper [1] shows that simple linear classifier can compete with complex\ndeep learning algorithms in text classification applications. Combining bag of\nwords (BoW) and linear classification techniques, fastText [1] attains same or\nonly slightly lower accuracy than deep learning algorithms [2-9] that are\norders of magnitude slower. We proved formally that fastText can be transformed\ninto a simpler equivalent classifier, which unlike fastText does not have any\nhidden layer. We also proved that the necessary and sufficient dimensionality\nof the word vector embedding space is exactly the number of document classes.\nThese results help constructing more optimal linear text classifiers with\nguaranteed maximum classification capabilities. The results are proven exactly\nby pure formal algebraic methods without attracting any empirical data.", "published": "2017-02-17 22:10:28", "link": "http://arxiv.org/abs/1702.05531v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "soc2seq: Social Embedding meets Conversation Model", "abstract": "While liking or upvoting a post on a mobile app is easy to do, replying with\na written note is much more difficult, due to both the cognitive load of coming\nup with a meaningful response as well as the mechanics of entering the text.\nHere we present a novel textual reply generation model that goes beyond the\ncurrent auto-reply and predictive text entry models by taking into account the\ncontent preferences of the user, the idiosyncrasies of their conversational\nstyle, and even the structure of their social graph. Specifically, we have\ndeveloped two types of models for personalized user interactions: a\ncontent-based conversation model, which makes use of location together with\nuser information, and a social-graph-based conversation model, which combines\ncontent-based conversation models with social graphs.", "published": "2017-02-17 20:26:50", "link": "http://arxiv.org/abs/1702.05512v3", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Be Precise or Fuzzy: Learning the Meaning of Cardinals and Quantifiers\n  from Vision", "abstract": "People can refer to quantities in a visual scene by using either exact\ncardinals (e.g. one, two, three) or natural language quantifiers (e.g. few,\nmost, all). In humans, these two processes underlie fairly different cognitive\nand neural mechanisms. Inspired by this evidence, the present study proposes\ntwo models for learning the objective meaning of cardinals and quantifiers from\nvisual scenes containing multiple objects. We show that a model capitalizing on\na 'fuzzy' measure of similarity is effective for learning quantifiers, whereas\nthe learning of exact cardinals is better accomplished when information about\nnumber is provided.", "published": "2017-02-17 09:26:10", "link": "http://arxiv.org/abs/1702.05270v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
