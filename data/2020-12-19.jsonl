{"title": "Breaking Writer's Block: Low-cost Fine-tuning of Natural Language\n  Generation Models", "abstract": "It is standard procedure these days to solve Information Extraction task by\nfine-tuning large pre-trained language models. This is not the case for\ngeneration task, which relies on a variety of techniques for controlled\nlanguage generation. In this paper, we describe a system that fine-tunes a\nnatural language generation model for the problem of solving Writer's Block.\nThe fine-tuning changes the conditioning to also include the right context in\naddition to the left context, as well as an optional list of entities, the\nsize, the genre and a summary of the paragraph that the human author wishes to\ngenerate. Our proposed fine-tuning obtains excellent results, even with a small\nnumber of epochs and a total cost of USD 150. The system can be accessed as a\nweb-service, and all the code is released. A video showcasing the interface and\nthe model is also available.", "published": "2020-12-19 11:19:11", "link": "http://arxiv.org/abs/2101.03216v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Uncertainty-Aware Label Refinement for Sequence Labeling", "abstract": "Conditional random fields (CRF) for label decoding has become ubiquitous in\nsequence labeling tasks. However, the local label dependencies and inefficient\nViterbi decoding have always been a problem to be solved. In this work, we\nintroduce a novel two-stage label decoding framework to model long-term label\ndependencies, while being much more computationally efficient. A base model\nfirst predicts draft labels, and then a novel two-stream self-attention model\nmakes refinements on these draft predictions based on long-range label\ndependencies, which can achieve parallel decoding for a faster prediction. In\naddition, in order to mitigate the side effects of incorrect draft labels,\nBayesian neural networks are used to indicate the labels with a high\nprobability of being wrong, which can greatly assist in preventing error\npropagation. The experimental results on three sequence labeling benchmarks\ndemonstrated that the proposed method not only outperformed the CRF-based\nmethods but also greatly accelerated the inference process.", "published": "2020-12-19 06:56:59", "link": "http://arxiv.org/abs/2012.10608v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FraCaS: Temporal Analysis", "abstract": "In this paper, we propose an implementation of temporal semantics which is\nsuitable for inference problems. This implementation translates syntax trees to\nlogical formulas, suitable for consumption by the Coq proof assistant. We\nsupport several phenomena including: temporal references, temporal adverbs,\naspectual classes and progressives. We apply these semantics to the complete\nFraCaS testsuite. We obtain an accuracy of 81 percent overall and 73 percent\nfor problems explicitly marked as related to temporal reference.", "published": "2020-12-19 11:55:46", "link": "http://arxiv.org/abs/2012.10668v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lexically-constrained Text Generation through Commonsense Knowledge\n  Extraction and Injection", "abstract": "Conditional text generation has been a challenging task that is yet to see\nhuman-level performance from state-of-the-art models. In this work, we\nspecifically focus on the Commongen benchmark, wherein the aim is to generate a\nplausible sentence for a given set of input concepts. Despite advances in other\ntasks, large pre-trained language models that are fine-tuned on this dataset\noften produce sentences that are syntactically correct but qualitatively\ndeviate from a human understanding of common sense. Furthermore, generated\nsequences are unable to fulfill such lexical requirements as matching\npart-of-speech and full concept coverage. In this paper, we explore how\ncommonsense knowledge graphs can enhance model performance, with respect to\ncommonsense reasoning and lexically-constrained decoding. We propose strategies\nfor enhancing the semantic correctness of the generated text, which we\naccomplish through: extracting commonsense relations from Conceptnet, injecting\nthese relations into the Unified Language Model (UniLM) through attention\nmechanisms, and enforcing the aforementioned lexical requirements through\noutput constraints. By performing several ablations, we find that commonsense\ninjection enables the generation of sentences that are more aligned with human\nunderstanding, while remaining compliant with lexical requirements.", "published": "2020-12-19 23:23:40", "link": "http://arxiv.org/abs/2012.10813v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Finding Sparse Structures for Domain Specific Neural Machine Translation", "abstract": "Neural machine translation often adopts the fine-tuning approach to adapt to\nspecific domains. However, nonrestricted fine-tuning can easily degrade on the\ngeneral domain and over-fit to the target domain. To mitigate the issue, we\npropose Prune-Tune, a novel domain adaptation method via gradual pruning. It\nlearns tiny domain-specific sub-networks during fine-tuning on new domains.\nPrune-Tune alleviates the over-fitting and the degradation problem without\nmodel modification. Furthermore, Prune-Tune is able to sequentially learn a\nsingle network with multiple disjoint domain-specific sub-networks for multiple\ndomains. Empirical experiment results show that Prune-Tune outperforms several\nstrong competitors in the target domain test set without sacrificing the\nquality on the general domain in both single and multi-domain settings. The\nsource code and data are available at https://github.com/ohlionel/Prune-Tune.", "published": "2020-12-19 03:33:27", "link": "http://arxiv.org/abs/2012.10586v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning by Fixing: Solving Math Word Problems with Weak Supervision", "abstract": "Previous neural solvers of math word problems (MWPs) are learned with full\nsupervision and fail to generate diverse solutions. In this paper, we address\nthis issue by introducing a \\textit{weakly-supervised} paradigm for learning\nMWPs. Our method only requires the annotations of the final answers and can\ngenerate various solutions for a single problem. To boost weakly-supervised\nlearning, we propose a novel \\textit{learning-by-fixing} (LBF) framework, which\ncorrects the misperceptions of the neural network via symbolic reasoning.\nSpecifically, for an incorrect solution tree generated by the neural network,\nthe \\textit{fixing} mechanism propagates the error from the root node to the\nleaf nodes and infers the most probable fix that can be executed to get the\ndesired answer. To generate more diverse solutions, \\textit{tree\nregularization} is applied to guide the efficient shrinkage and exploration of\nthe solution space, and a \\textit{memory buffer} is designed to track and save\nthe discovered various fixes for each problem. Experimental results on the\nMath23K dataset show the proposed LBF framework significantly outperforms\nreinforcement learning baselines in weakly-supervised learning. Furthermore, it\nachieves comparable top-1 and much better top-3/5 answer accuracies than\nfully-supervised methods, demonstrating its strength in producing diverse\nsolutions.", "published": "2020-12-19 03:10:21", "link": "http://arxiv.org/abs/2012.10582v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "T-GAP: Learning to Walk across Time for Temporal Knowledge Graph\n  Completion", "abstract": "Temporal knowledge graphs (TKGs) inherently reflect the transient nature of\nreal-world knowledge, as opposed to static knowledge graphs. Naturally,\nautomatic TKG completion has drawn much research interests for a more realistic\nmodeling of relational reasoning. However, most of the existing mod-els for TKG\ncompletion extend static KG embeddings that donot fully exploit TKG structure,\nthus lacking in 1) account-ing for temporally relevant events already residing\nin the lo-cal neighborhood of a query, and 2) path-based inference that\nfacilitates multi-hop reasoning and better interpretability. In this paper, we\npropose T-GAP, a novel model for TKG completion that maximally utilizes both\ntemporal information and graph structure in its encoder and decoder. T-GAP\nencodes query-specific substructure of TKG by focusing on the temporal\ndisplacement between each event and the query times-tamp, and performs\npath-based inference by propagating attention through the graph. Our empirical\nexperiments demonstrate that T-GAP not only achieves superior performance\nagainst state-of-the-art baselines, but also competently generalizes to queries\nwith unseen timestamps. Through extensive qualitative analyses, we also show\nthat T-GAP enjoys from transparent interpretability, and follows human\nintuition in its reasoning process.", "published": "2020-12-19 04:45:32", "link": "http://arxiv.org/abs/2012.10595v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "On (Emergent) Systematic Generalisation and Compositionality in Visual\n  Referential Games with Straight-Through Gumbel-Softmax Estimator", "abstract": "The drivers of compositionality in artificial languages that emerge when two\n(or more) agents play a non-visual referential game has been previously\ninvestigated using approaches based on the REINFORCE algorithm and the (Neural)\nIterated Learning Model. Following the more recent introduction of the\n\\textit{Straight-Through Gumbel-Softmax} (ST-GS) approach, this paper\ninvestigates to what extent the drivers of compositionality identified so far\nin the field apply in the ST-GS context and to what extent do they translate\ninto (emergent) systematic generalisation abilities, when playing a visual\nreferential game. Compositionality and the generalisation abilities of the\nemergent languages are assessed using topographic similarity and zero-shot\ncompositional tests. Firstly, we provide evidence that the test-train split\nstrategy significantly impacts the zero-shot compositional tests when dealing\nwith visual stimuli, whilst it does not when dealing with symbolic ones.\nSecondly, empirical evidence shows that using the ST-GS approach with small\nbatch sizes and an overcomplete communication channel improves compositionality\nin the emerging languages. Nevertheless, while shown robust with symbolic\nstimuli, the effect of the batch size is not so clear-cut when dealing with\nvisual stimuli. Our results also show that not all overcomplete communication\nchannels are created equal. Indeed, while increasing the maximum sentence\nlength is found to be beneficial to further both compositionality and\ngeneralisation abilities, increasing the vocabulary size is found detrimental.\nFinally, a lack of correlation between the language compositionality at\ntraining-time and the agents' generalisation abilities is observed in the\ncontext of discriminative referential games with visual stimuli. This is\nsimilar to previous observations in the field using the generative variant with\nsymbolic stimuli.", "published": "2020-12-19 20:40:09", "link": "http://arxiv.org/abs/2012.10776v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Non-uniform FIR Digital Filter Bank for Hearing Aid Application Using\n  Frequency Response Masking Technique: A Review", "abstract": "Hearing aid is an electroacoustic device used to selectively amplify the\naudio sounds with an aim to make speech more intelligible for a hearing\nimpaired person. Filter bank is one of the important parts of digital hearing\naid where the sub band gains of each filter can be tuned to compensate an\nindividuals unique hearing loss pattern. As the human perception is based on\nthe logarithmic scale, nonuniform filter bank outperforms uniform filter bank.\nThe main advantage of nonuniform filer bank is that it requires less number of\nsub-band filters, hence resulted in low hardware complexity and cost. Much\neffort has been devoted to design these nonuniform filter banks for hearing aid\napplications. This paper aimed to provide a review of previous researches based\non nonuniform finite impulse response (FIR) digital filter bank for hearing aid\napplication using frequency response masking (FRM) technique. By reviewing\nfilter banks, we try to find the difference between fixed and variable band\nfilter bank and to give an insight about which method is more suitable for\nmatching most common types of hearing loss. Papers which involved methods of\ndesign, theoretical computation and simulation results of filter bank have been\nreviewed.", "published": "2020-12-19 11:26:02", "link": "http://arxiv.org/abs/2012.10663v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DCCRGAN: Deep Complex Convolution Recurrent Generator Adversarial\n  Network for Speech Enhancement", "abstract": "Generative adversarial network (GAN) still exists some problems in dealing\nwith speech enhancement (SE) task. Some GAN-based systems adopt the same\nstructure from Pixel-to-Pixel directly without special optimization. The\nimportance of the generator network has not been fully explored. Other related\nresearches change the generator network but operate in the time-frequency\ndomain, which ignores the phase mismatch problem. In order to solve these\nproblems, a deep complex convolution recurrent GAN (DCCRGAN) structure is\nproposed in this paper. The complex module builds the correlation between\nmagnitude and phase of the waveform and has been proved to be effective. The\nproposed structure is trained in an end-to-end way. Different LSTM layers are\nused in the generator network to sufficiently explore the speech enhancement\nperformance of DCCRGAN. The experimental results confirm that the proposed\nDCCRGAN outperforms the state-of-the-art GAN-based SE systems.", "published": "2020-12-19 16:28:52", "link": "http://arxiv.org/abs/2012.10732v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
