{"title": "Deep Lexical Hypothesis: Identifying personality structure in natural\n  language", "abstract": "Recent advances in natural language processing (NLP) have produced general\nmodels that can perform complex tasks such as summarizing long passages and\ntranslating across languages. Here, we introduce a method to extract adjective\nsimilarities from language models as done with survey-based ratings in\ntraditional psycholexical studies but using millions of times more text in a\nnatural setting. The correlational structure produced through this method is\nhighly similar to that of self- and other-ratings of 435 terms reported by\nSaucier and Goldberg (1996a). The first three unrotated factors produced using\nNLP are congruent with those in survey data, with coefficients of 0.89, 0.79,\nand 0.79. This structure is robust to many modeling decisions: adjective set,\nincluding those with 1,710 terms (Goldberg, 1982) and 18,000 terms (Allport &\nOdbert, 1936); the query used to extract correlations; and language model.\nNotably, Neuroticism and Openness are only weakly and inconsistently recovered.\nThis is a new source of signal that is closer to the original (semantic) vision\nof the Lexical Hypothesis. The method can be applied where surveys cannot: in\ndozens of languages simultaneously, with tens of thousands of items, on\nhistorical text, and at extremely large scale for little cost. The code is made\npublic to facilitate reproduction and fast iteration in new directions of\nresearch.", "published": "2022-03-04 02:06:10", "link": "http://arxiv.org/abs/2203.02092v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SimKGC: Simple Contrastive Knowledge Graph Completion with Pre-trained\n  Language Models", "abstract": "Knowledge graph completion (KGC) aims to reason over known facts and infer\nthe missing links. Text-based methods such as KGBERT (Yao et al., 2019) learn\nentity representations from natural language descriptions, and have the\npotential for inductive KGC. However, the performance of text-based methods\nstill largely lag behind graph embedding-based methods like TransE (Bordes et\nal., 2013) and RotatE (Sun et al., 2019b). In this paper, we identify that the\nkey issue is efficient contrastive learning. To improve the learning\nefficiency, we introduce three types of negatives: in-batch negatives,\npre-batch negatives, and self-negatives which act as a simple form of hard\nnegatives. Combined with InfoNCE loss, our proposed model SimKGC can\nsubstantially outperform embedding-based methods on several benchmark datasets.\nIn terms of mean reciprocal rank (MRR), we advance the state-of-the-art by +19%\non WN18RR, +6.8% on the Wikidata5M transductive setting, and +22% on the\nWikidata5M inductive setting. Thorough analyses are conducted to gain insights\ninto each component. Our code is available at\nhttps://github.com/intfloat/SimKGC .", "published": "2022-03-04 07:36:30", "link": "http://arxiv.org/abs/2203.02167v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ClarET: Pre-training a Correlation-Aware Context-To-Event Transformer\n  for Event-Centric Generation and Classification", "abstract": "Generating new events given context with correlated ones plays a crucial role\nin many event-centric reasoning tasks. Existing works either limit their scope\nto specific scenarios or overlook event-level correlations. In this paper, we\npropose to pre-train a general Correlation-aware context-to-Event Transformer\n(ClarET) for event-centric reasoning. To achieve this, we propose three novel\nevent-centric objectives, i.e., whole event recovering, contrastive\nevent-correlation encoding and prompt-based event locating, which highlight\nevent-level correlations with effective training. The proposed ClarET is\napplicable to a wide range of event-centric reasoning scenarios, considering\nits versatility of (i) event-correlation types (e.g., causal, temporal,\ncontrast), (ii) application formulations (i.e., generation and classification),\nand (iii) reasoning types (e.g., abductive, counterfactual and ending\nreasoning). Empirical fine-tuning results, as well as zero- and few-shot\nlearning, on 9 benchmarks (5 generation and 4 classification tasks covering 4\nreasoning types with diverse event correlations), verify its effectiveness and\ngeneralization ability.", "published": "2022-03-04 10:11:15", "link": "http://arxiv.org/abs/2203.02225v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IISERB Brains at SemEval 2022 Task 6: A Deep-learning Framework to\n  Identify Intended Sarcasm in English", "abstract": "This paper describes the system architectures and the models submitted by our\nteam \"IISERBBrains\" to SemEval 2022 Task 6 competition. We contested for all\nthree sub-tasks floated for the English dataset. On the leader-board, wegot19th\nrank out of43 teams for sub-taskA, the 8th rank out of22 teams for sub-task\nB,and13th rank out of 16 teams for sub-taskC. Apart from the submitted results\nand models, we also report the other models and results that we obtained\nthrough our experiments after organizers published the gold labels of their\nevaluation data", "published": "2022-03-04 11:23:54", "link": "http://arxiv.org/abs/2203.02244v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Plain Toxic: Detection of Inappropriate Statements on Flammable\n  Topics for the Russian Language", "abstract": "Toxicity on the Internet, such as hate speech, offenses towards particular\nusers or groups of people, or the use of obscene words, is an acknowledged\nproblem. However, there also exist other types of inappropriate messages which\nare usually not viewed as toxic, e.g. as they do not contain explicit offences.\nSuch messages can contain covered toxicity or generalizations, incite harmful\nactions (crime, suicide, drug use), provoke \"heated\" discussions. Such messages\nare often related to particular sensitive topics, e.g. on politics, sexual\nminorities, social injustice which more often than other topics, e.g. cars or\ncomputing, yield toxic emotional reactions. At the same time, clearly not all\nmessages within such flammable topics are inappropriate.\n  Towards this end, in this work, we present two text collections labelled\naccording to binary notion of inapropriateness and a multinomial notion of\nsensitive topic. Assuming that the notion of inappropriateness is common among\npeople of the same culture, we base our approach on human intuitive\nunderstanding of what is not acceptable and harmful. To objectivise the notion\nof inappropriateness, we define it in a data-driven way though crowdsourcing.\nNamely we run a large-scale annotation study asking workers if a given chatbot\ntextual statement could harm reputation of a company created it. Acceptably\nhigh values of inter-annotator agreement suggest that the notion of\ninappropriateness exists and can be uniformly understood by different people.\nTo define the notion of sensitive topics in an objective way we use on\nguidelines suggested commonly by specialists of legal and PR department of a\nlarge public company as potentially harmful.", "published": "2022-03-04 15:59:06", "link": "http://arxiv.org/abs/2203.02392v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Continuous Rating as Reliable Human Evaluation of Simultaneous Speech\n  Translation", "abstract": "Simultaneous speech translation (SST) can be evaluated on simulated online\nevents where human evaluators watch subtitled videos and continuously express\ntheir satisfaction by pressing buttons (so called Continuous Rating).\nContinuous Rating is easy to collect, but little is known about its\nreliability, or relation to comprehension of foreign language document by SST\nusers. In this paper, we contrast Continuous Rating with factual questionnaires\non judges with different levels of source language knowledge. Our results show\nthat Continuous Rating is easy and reliable SST quality assessment if the\njudges have at least limited knowledge of the source language. Our study\nindicates users' preferences on subtitle layout and presentation style and,\nmost importantly, provides a significant evidence that users with advanced\nsource language knowledge prefer low latency over fewer re-translations.", "published": "2022-03-04 17:41:39", "link": "http://arxiv.org/abs/2203.02458v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Simultaneous to Streaming Machine Translation by Leveraging\n  Streaming History", "abstract": "Simultaneous Machine Translation is the task of incrementally translating an\ninput sentence before it is fully available. Currently, simultaneous\ntranslation is carried out by translating each sentence independently of the\npreviously translated text. More generally, Streaming MT can be understood as\nan extension of Simultaneous MT to the incremental translation of a continuous\ninput text stream. In this work, a state-of-the-art simultaneous sentence-level\nMT system is extended to the streaming setup by leveraging the streaming\nhistory. Extensive empirical results are reported on IWSLT Translation Tasks,\nshowing that leveraging the streaming history leads to significant quality\ngains. In particular, the proposed system proves to compare favorably to the\nbest performing systems.", "published": "2022-03-04 17:41:45", "link": "http://arxiv.org/abs/2203.02459v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LiteTransformerSearch: Training-free Neural Architecture Search for\n  Efficient Language Models", "abstract": "The Transformer architecture is ubiquitously used as the building block of\nlarge-scale autoregressive language models. However, finding architectures with\nthe optimal trade-off between task performance (perplexity) and hardware\nconstraints like peak memory utilization and latency is non-trivial. This is\nexacerbated by the proliferation of various hardware. We leverage the somewhat\nsurprising empirical observation that the number of decoder parameters in\nautoregressive Transformers has a high rank correlation with task performance,\nirrespective of the architecture topology. This observation organically induces\na simple Neural Architecture Search (NAS) algorithm that uses decoder\nparameters as a proxy for perplexity without need for any model training. The\nsearch phase of our training-free algorithm, dubbed Lightweight Transformer\nSearch (LTS), can be run directly on target devices since it does not require\nGPUs. Using on-target-device measurements, LTS extracts the Pareto-frontier of\nperplexity versus any hardware performance cost. We evaluate LTS on diverse\ndevices from ARM CPUs to NVIDIA GPUs and two popular autoregressive Transformer\nbackbones: GPT-2 and Transformer-XL. Results show that the perplexity of\n16-layer GPT-2 and Transformer-XL can be achieved with up to 1.5x, 2.5x faster\nruntime and 1.2x, 2.0x lower peak memory utilization. When evaluated in zero\nand one-shot settings, LTS Pareto-frontier models achieve higher average\naccuracy compared to the 350M parameter OPT across 14 tasks, with up to 1.6x\nlower latency. LTS extracts the Pareto-frontier in under 3 hours while running\non a commodity laptop. We effectively remove the carbon footprint of hundreds\nof GPU hours of training during search, offering a strong simple baseline for\nfuture NAS methods in autoregressive language modeling.", "published": "2022-03-04 02:10:43", "link": "http://arxiv.org/abs/2203.02094v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Continual Few-shot Relation Learning via Embedding Space Regularization\n  and Data Augmentation", "abstract": "Existing continual relation learning (CRL) methods rely on plenty of labeled\ntraining data for learning a new task, which can be hard to acquire in real\nscenario as getting large and representative labeled data is often expensive\nand time-consuming. It is therefore necessary for the model to learn novel\nrelational patterns with very few labeled data while avoiding catastrophic\nforgetting of previous task knowledge. In this paper, we formulate this\nchallenging yet practical problem as continual few-shot relation learning\n(CFRL). Based on the finding that learning for new emerging few-shot tasks\noften results in feature distributions that are incompatible with previous\ntasks' learned distributions, we propose a novel method based on embedding\nspace regularization and data augmentation. Our method generalizes to new\nfew-shot tasks and avoids catastrophic forgetting of previous tasks by\nenforcing extra constraints on the relational embeddings and by adding extra\n{relevant} data in a self-supervised manner. With extensive experiments we\ndemonstrate that our method can significantly outperform previous\nstate-of-the-art methods in CFRL task settings.", "published": "2022-03-04 05:19:09", "link": "http://arxiv.org/abs/2203.02135v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "GCNet: Graph Completion Network for Incomplete Multimodal Learning in\n  Conversation", "abstract": "Conversations have become a critical data format on social media platforms.\nUnderstanding conversation from emotion, content and other aspects also\nattracts increasing attention from researchers due to its widespread\napplication in human-computer interaction. In real-world environments, we often\nencounter the problem of incomplete modalities, which has become a core issue\nof conversation understanding. To address this problem, researchers propose\nvarious methods. However, existing approaches are mainly designed for\nindividual utterances rather than conversational data, which cannot fully\nexploit temporal and speaker information in conversations. To this end, we\npropose a novel framework for incomplete multimodal learning in conversations,\ncalled \"Graph Complete Network (GCNet)\", filling the gap of existing works. Our\nGCNet contains two well-designed graph neural network-based modules, \"Speaker\nGNN\" and \"Temporal GNN\", to capture temporal and speaker dependencies. To make\nfull use of complete and incomplete data, we jointly optimize classification\nand reconstruction tasks in an end-to-end manner. To verify the effectiveness\nof our method, we conduct experiments on three benchmark conversational\ndatasets. Experimental results demonstrate that our GCNet is superior to\nexisting state-of-the-art approaches in incomplete multimodal learning. Code is\navailable at https://github.com/zeroQiaoba/GCNet.", "published": "2022-03-04 08:13:18", "link": "http://arxiv.org/abs/2203.02177v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "EAG: Extract and Generate Multi-way Aligned Corpus for Complete\n  Multi-lingual Neural Machine Translation", "abstract": "Complete Multi-lingual Neural Machine Translation (C-MNMT) achieves superior\nperformance against the conventional MNMT by constructing multi-way aligned\ncorpus, i.e., aligning bilingual training examples from different language\npairs when either their source or target sides are identical. However, since\nexactly identical sentences from different language pairs are scarce, the power\nof the multi-way aligned corpus is limited by its scale. To handle this\nproblem, this paper proposes \"Extract and Generate\" (EAG), a two-step approach\nto construct large-scale and high-quality multi-way aligned corpus from\nbilingual data. Specifically, we first extract candidate aligned examples by\npairing the bilingual examples from different language pairs with highly\nsimilar source or target sentences; and then generate the final aligned\nexamples from the candidates with a well-trained generation model. With this\ntwo-step pipeline, EAG can construct a large-scale and multi-way aligned corpus\nwhose diversity is almost identical to the original bilingual corpus.\nExperiments on two publicly available datasets i.e., WMT-5 and OPUS-100, show\nthat the proposed method achieves significant improvements over strong\nbaselines, with +1.1 and +1.4 BLEU points improvements on the two datasets\nrespectively.", "published": "2022-03-04 08:21:27", "link": "http://arxiv.org/abs/2203.02180v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "In the Service of Online Order: Tackling Cyber-Bullying with Machine\n  Learning and Affect Analysis", "abstract": "One of the burning problems lately in Japan has been cyber-bullying, or\nslandering and bullying people online. The problem has been especially noticed\non unofficial Web sites of Japanese schools. Volunteers consisting of school\npersonnel and PTA (Parent-Teacher Association) members have started Online\nPatrol to spot malicious contents within Web forums and blogs. In practise,\nOnline Patrol assumes reading through the whole Web contents, which is a task\ndifficult to perform manually. With this paper we introduce a research intended\nto help PTA members perform Online Patrol more efficiently. We aim to develop a\nset of tools that can automatically detect malicious entries and report them to\nPTA members. First, we collected cyber-bullying data from unofficial school Web\nsites. Then we performed analysis of this data in two ways. Firstly, we\nanalysed the entries with a multifaceted affect analysis system in order to\nfind distinctive features for cyber-bullying and apply them to a machine\nlearning classifier. Secondly, we applied a SVM based machine learning method\nto train a classifier for detection of cyber-bullying. The system was able to\nclassify cyber-bullying entries with 88.2% of balanced F-score.", "published": "2022-03-04 03:13:45", "link": "http://arxiv.org/abs/2203.02116v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Training language models to follow instructions with human feedback", "abstract": "Making language models bigger does not inherently make them better at\nfollowing a user's intent. For example, large language models can generate\noutputs that are untruthful, toxic, or simply not helpful to the user. In other\nwords, these models are not aligned with their users. In this paper, we show an\navenue for aligning language models with user intent on a wide range of tasks\nby fine-tuning with human feedback. Starting with a set of labeler-written\nprompts and prompts submitted through the OpenAI API, we collect a dataset of\nlabeler demonstrations of the desired model behavior, which we use to fine-tune\nGPT-3 using supervised learning. We then collect a dataset of rankings of model\noutputs, which we use to further fine-tune this supervised model using\nreinforcement learning from human feedback. We call the resulting models\nInstructGPT. In human evaluations on our prompt distribution, outputs from the\n1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3,\ndespite having 100x fewer parameters. Moreover, InstructGPT models show\nimprovements in truthfulness and reductions in toxic output generation while\nhaving minimal performance regressions on public NLP datasets. Even though\nInstructGPT still makes simple mistakes, our results show that fine-tuning with\nhuman feedback is a promising direction for aligning language models with human\nintent.", "published": "2022-03-04 07:04:42", "link": "http://arxiv.org/abs/2203.02155v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MM-DFN: Multimodal Dynamic Fusion Network for Emotion Recognition in\n  Conversations", "abstract": "Emotion Recognition in Conversations (ERC) has considerable prospects for\ndeveloping empathetic machines. For multimodal ERC, it is vital to understand\ncontext and fuse modality information in conversations. Recent graph-based\nfusion methods generally aggregate multimodal information by exploring unimodal\nand cross-modal interactions in a graph. However, they accumulate redundant\ninformation at each layer, limiting the context understanding between\nmodalities. In this paper, we propose a novel Multimodal Dynamic Fusion Network\n(MM-DFN) to recognize emotions by fully understanding multimodal conversational\ncontext. Specifically, we design a new graph-based dynamic fusion module to\nfuse multimodal contextual features in a conversation. The module reduces\nredundancy and enhances complementarity between modalities by capturing the\ndynamics of contextual information in different semantic spaces. Extensive\nexperiments on two public benchmark datasets demonstrate the effectiveness and\nsuperiority of MM-DFN.", "published": "2022-03-04 15:42:53", "link": "http://arxiv.org/abs/2203.02385v1", "categories": ["cs.CL", "cs.AI", "cs.MM"], "primary_category": "cs.CL"}
{"title": "OCR quality affects perceived usefulness of historical newspaper\n  clippings -- a user study", "abstract": "Effects of Optical Character Recognition (OCR) quality on historical\ninformation retrieval have so far been studied in data-oriented scenarios\nregarding the effectiveness of retrieval results. Such studies have either\nfocused on the effects of artificially degraded OCR quality (see, e.g., [1-2])\nor utilized test collections containing texts based on authentic low quality\nOCR data (see, e.g., [3]). In this paper the effects of OCR quality are studied\nin a user-oriented information retrieval setting. Thirty-two users evaluated\nsubjectively query results of six topics each (out of 30 topics) based on\npre-formulated queries using a simulated work task setting. To the best of our\nknowledge our simulated work task experiment is the first one showing\nempirically that users' subjective relevance assessments of retrieved documents\nare affected by a change in the quality of optically read text. Users of\nhistorical newspaper collections have so far commented effects of OCR'ed data\nquality mainly in impressionistic ways, and controlled user environments for\nstudying effects of OCR quality on users' relevance assessments of the\nretrieval results have so far been missing. To remedy this The National Library\nof Finland (NLF) set up an experimental query environment for the contents of\none Finnish historical newspaper, Uusi Suometar 1869-1918, to be able to\ncompare users' evaluation of search results of two different OCR qualities for\ndigitized newspaper articles. The query interface was able to present the same\nunderlying document for the user based on two alternatives: either based on the\nlower OCR quality, or based on the higher OCR quality, and the choice was\nrandomized. The users did not know about quality differences in the article\ntexts they evaluated. The main result of the study is that improved optical\ncharacter recognition quality affects perceived usefulness of historical\nnewspaper articles significantly. The mean average evaluation score for the\nimproved OCR results was 7.94% higher than the mean average evaluation score of\nthe old OCR results.", "published": "2022-03-04 11:49:54", "link": "http://arxiv.org/abs/2203.03557v1", "categories": ["cs.IR", "cs.CL", "cs.DL"], "primary_category": "cs.IR"}
{"title": "Selective Pseudo-labeling and Class-wise Discriminative Fusion for Sound\n  Event Detection", "abstract": "In recent years, exploring effective sound separation (SSep) techniques to\nimprove overlapping sound event detection (SED) attracts more and more\nattention. Creating accurate separation signals to avoid the catastrophic error\naccumulation during SED model training is very important and challenging. In\nthis study, we first propose a novel selective pseudo-labeling approach, termed\nSPL, to produce high confidence separated target events from blind sound\nseparation outputs. These target events are then used to fine-tune the original\nSED model that pre-trained on the sound mixtures in a multi-objective learning\nstyle. Then, to further leverage the SSep outputs, a class-wise discriminative\nfusion is proposed to improve the final SED performances, by combining multiple\nframe-level event predictions of both sound mixtures and their separated\nsignals. All experiments are performed on the public DCASE 2021 Task 4 dataset,\nand results show that our approaches significantly outperforms the official\nbaseline, the collar-based F 1, PSDS1 and PSDS2 performances are improved from\n44.3%, 37.3% and 54.9% to 46.5%, 44.5% and 75.4%, respectively.", "published": "2022-03-04 08:54:20", "link": "http://arxiv.org/abs/2203.02191v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "PercepNet+: A Phase and SNR Aware PercepNet for Real-Time Speech\n  Enhancement", "abstract": "PercepNet, a recent extension of the RNNoise, an efficient, high-quality and\nreal-time full-band speech enhancement technique, has shown promising\nperformance in various public deep noise suppression tasks. This paper proposes\na new approach, named PercepNet+, to further extend the PercepNet with four\nsignificant improvements. First, we introduce a phase-aware structure to\nleverage the phase information into PercepNet, by adding the complex features\nand complex subband gains as the deep network input and output respectively.\nThen, a signal-to-noise ratio (SNR) estimator and an SNR switched\npost-processing are specially designed to alleviate the over attenuation (OA)\nthat appears in high SNR conditions of the original PercepNet. Moreover, the\nGRU layer is replaced by TF-GRU to model both temporal and frequency\ndependencies. Finally, we propose to integrate the loss of complex subband\ngain, SNR, pitch filtering strength, and an OA loss in a multi-objective\nlearning manner to further improve the speech enhancement performance.\nExperimental results show that, the proposed PercepNet+ outperforms the\noriginal PercepNet significantly in terms of both PESQ and STOI, without\nincreasing the model size too much.", "published": "2022-03-04 12:23:09", "link": "http://arxiv.org/abs/2203.02263v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "On the relevance of language in speaker recognition", "abstract": "This paper presents a new database collected from a bilingual speakers set\n(49), in two different languages: Spanish and Catalan. Phonetically there are\nsignificative differences between both languages. These differences have let us\nto establish several conclusions on the relevance of language in speaker\nrecognition, using two methods: vector quantization and covariance matrices", "published": "2022-03-04 10:31:19", "link": "http://arxiv.org/abs/2203.01992v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MANNER: Multi-view Attention Network for Noise Erasure", "abstract": "In the field of speech enhancement, time domain methods have difficulties in\nachieving both high performance and efficiency. Recently, dual-path models have\nbeen adopted to represent long sequential features, but they still have limited\nrepresentations and poor memory efficiency. In this study, we propose\nMulti-view Attention Network for Noise ERasure (MANNER) consisting of a\nconvolutional encoder-decoder with a multi-view attention block, applied to the\ntime-domain signals. MANNER efficiently extracts three different\nrepresentations from noisy speech and estimates high-quality clean speech. We\nevaluated MANNER on the VoiceBank-DEMAND dataset in terms of five objective\nspeech quality metrics. Experimental results show that MANNER achieves\nstate-of-the-art performance while efficiently processing noisy speech.", "published": "2022-03-04 08:27:35", "link": "http://arxiv.org/abs/2203.02181v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Look\\&Listen: Multi-Modal Correlation Learning for Active Speaker\n  Detection and Speech Enhancement", "abstract": "Active speaker detection and speech enhancement have become two increasingly\nattractive topics in audio-visual scenario understanding. According to their\nrespective characteristics, the scheme of independently designed architecture\nhas been widely used in correspondence to each single task. This may lead to\nthe representation learned by the model being task-specific, and inevitably\nresult in the lack of generalization ability of the feature based on\nmulti-modal modeling. More recent studies have shown that establishing\ncross-modal relationship between auditory and visual stream is a promising\nsolution for the challenge of audio-visual multi-task learning. Therefore, as a\nmotivation to bridge the multi-modal associations in audio-visual tasks, a\nunified framework is proposed to achieve target speaker detection and speech\nenhancement with joint learning of audio-visual modeling in this study.", "published": "2022-03-04 09:53:19", "link": "http://arxiv.org/abs/2203.02216v2", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Integrating Statistical Uncertainty into Neural Network-Based Speech\n  Enhancement", "abstract": "Speech enhancement in the time-frequency domain is often performed by\nestimating a multiplicative mask to extract clean speech. However, most neural\nnetwork-based methods perform point estimation, i.e., their output consists of\na single mask. In this paper, we study the benefits of modeling uncertainty in\nneural network-based speech enhancement. For this, our neural network is\ntrained to map a noisy spectrogram to the Wiener filter and its associated\nvariance, which quantifies uncertainty, based on the maximum a posteriori (MAP)\ninference of spectral coefficients. By estimating the distribution instead of\nthe point estimate, one can model the uncertainty associated with each\nestimate. We further propose to use the estimated Wiener filter and its\nuncertainty to build an approximate MAP (A-MAP) estimator of spectral\nmagnitudes, which in turn is combined with the MAP inference of spectral\ncoefficients to form a hybrid loss function to jointly reinforce the\nestimation. Experimental results on different datasets show that the proposed\nmethod can not only capture the uncertainty associated with the estimated\nfilters, but also yield a higher enhancement performance over comparable models\nthat do not take uncertainty into account.", "published": "2022-03-04 12:55:46", "link": "http://arxiv.org/abs/2203.02288v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Freeform Body Motion Generation from Speech", "abstract": "People naturally conduct spontaneous body motions to enhance their speeches\nwhile giving talks. Body motion generation from speech is inherently difficult\ndue to the non-deterministic mapping from speech to body motions. Most existing\nworks map speech to motion in a deterministic way by conditioning on certain\nstyles, leading to sub-optimal results. Motivated by studies in linguistics, we\ndecompose the co-speech motion into two complementary parts: pose modes and\nrhythmic dynamics. Accordingly, we introduce a novel freeform motion generation\nmodel (FreeMo) by equipping a two-stream architecture, i.e., a pose mode branch\nfor primary posture generation, and a rhythmic motion branch for rhythmic\ndynamics synthesis. On one hand, diverse pose modes are generated by\nconditional sampling in a latent space, guided by speech semantics. On the\nother hand, rhythmic dynamics are synced with the speech prosody. Extensive\nexperiments demonstrate the superior performance against several baselines, in\nterms of motion diversity, quality and syncing with speech. Code and\npre-trained models will be publicly available through\nhttps://github.com/TheTempAccount/Co-Speech-Motion-Generation.", "published": "2022-03-04 13:03:22", "link": "http://arxiv.org/abs/2203.02291v1", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "iSTFTNet: Fast and Lightweight Mel-Spectrogram Vocoder Incorporating\n  Inverse Short-Time Fourier Transform", "abstract": "In recent text-to-speech synthesis and voice conversion systems, a\nmel-spectrogram is commonly applied as an intermediate representation, and the\nnecessity for a mel-spectrogram vocoder is increasing. A mel-spectrogram\nvocoder must solve three inverse problems: recovery of the original-scale\nmagnitude spectrogram, phase reconstruction, and frequency-to-time conversion.\nA typical convolutional mel-spectrogram vocoder solves these problems jointly\nand implicitly using a convolutional neural network, including temporal\nupsampling layers, when directly calculating a raw waveform. Such an approach\nallows skipping redundant processes during waveform synthesis (e.g., the direct\nreconstruction of high-dimensional original-scale spectrograms). By contrast,\nthe approach solves all problems in a black box and cannot effectively employ\nthe time-frequency structures existing in a mel-spectrogram. We thus propose\niSTFTNet, which replaces some output-side layers of the mel-spectrogram vocoder\nwith the inverse short-time Fourier transform (iSTFT) after sufficiently\nreducing the frequency dimension using upsampling layers, reducing the\ncomputational cost from black-box modeling and avoiding redundant estimations\nof high-dimensional spectrograms. During our experiments, we applied our ideas\nto three HiFi-GAN variants and made the models faster and more lightweight with\na reasonable speech quality. Audio samples are available at\nhttps://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/istftnet/.", "published": "2022-03-04 16:05:48", "link": "http://arxiv.org/abs/2203.02395v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Ontological Learning from Weak Labels", "abstract": "Ontologies encompass a formal representation of knowledge through the\ndefinition of concepts or properties of a domain, and the relationships between\nthose concepts. In this work, we seek to investigate whether using this\nontological information will improve learning from weakly labeled data, which\nare easier to collect since it requires only the presence or absence of an\nevent to be known. We use the AudioSet ontology and dataset, which contains\naudio clips weakly labeled with the ontology concepts and the ontology\nproviding the \"Is A\" relations between the concepts. We first re-implemented\nthe model proposed by soundevent_ontology with modification to fit the\nmulti-label scenario and then expand on that idea by using a Graph\nConvolutional Network (GCN) to model the ontology information to learn the\nconcepts. We find that the baseline Siamese does not perform better by\nincorporating ontology information in the weak and multi-label scenario, but\nthat the GCN does capture the ontology knowledge better for weak, multi-labeled\ndata. In our experiments, we also investigate how different modules can\ntolerate noises introduced from weak labels and better incorporate ontology\ninformation. Our best Siamese-GCN model achieves mAP=0.45 and AUC=0.87 for\nlower-level concepts and mAP=0.72 and AUC=0.86 for higher-level concepts, which\nis an improvement over the baseline Siamese but about the same as our models\nthat do not use ontology information.", "published": "2022-03-04 18:29:46", "link": "http://arxiv.org/abs/2203.02483v1", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
