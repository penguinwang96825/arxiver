{"title": "Multilingual Neural Machine Translation With Soft Decoupled Encoding", "abstract": "Multilingual training of neural machine translation (NMT) systems has led to\nimpressive accuracy improvements on low-resource languages. However, there are\nstill significant challenges in efficiently learning word representations in\nthe face of paucity of data. In this paper, we propose Soft Decoupled Encoding\n(SDE), a multilingual lexicon encoding framework specifically designed to share\nlexical-level information intelligently without requiring heuristic\npreprocessing such as pre-segmenting the data. SDE represents a word by its\nspelling through a character encoding, and its semantic meaning through a\nlatent embedding space shared by all languages. Experiments on a standard\ndataset of four low-resource languages show consistent improvements over strong\nmultilingual NMT baselines, with gains of up to 2 BLEU on one of the tested\nlanguages, achieving the new state-of-the-art on all four language pairs.", "published": "2019-02-09 21:47:05", "link": "http://arxiv.org/abs/1902.03499v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contextual Recurrent Neural Networks", "abstract": "There is an implicit assumption that by unfolding recurrent neural networks\n(RNN) in finite time, the misspecification of choosing a zero value for the\ninitial hidden state is mitigated by later time steps. This assumption has been\nshown to work in practice and alternative initialization may be suggested but\noften overlooked. In this paper, we propose a method of parameterizing the\ninitial hidden state of an RNN. The resulting architecture, referred to as a\nContextual RNN, can be trained end-to-end. The performance on an associative\nretrieval task is found to improve by conditioning the RNN initial hidden state\non contextual information from the input sequence. Furthermore, we propose a\nnovel method of conditionally generating sequences using the hidden state\nparameterization of Contextual RNN.", "published": "2019-02-09 17:41:56", "link": "http://arxiv.org/abs/1902.03455v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Network connectivity dynamics affect the evolution of culturally\n  transmitted variants", "abstract": "The distribution of cultural variants in a population is shaped by both\nneutral evolutionary dynamics and by selection pressures, which include several\nindividual cognitive biases, demographic factors and social network structures.\nThe temporal dynamics of social network connectivity, i.e. the order in which\nindividuals in a population interact with each other, has been largely\nunexplored. In this paper we investigate how, in a fully connected social\nnetwork, connectivity dynamics, alone and in interaction with different\ncognitive biases, affect the evolution of cultural variants. Using agent-based\ncomputer simulations, we manipulate population connectivity dynamics (early,\nmiddle and late full-population connectivity); content bias, or a preference\nfor high-quality variants; coordination bias, or whether agents tend to use\nself-produced variants (egocentric bias), or to switch to variants observed in\nothers (allocentric bias); and memory size, or the number of items that agents\ncan store in their memory. We show that connectivity dynamics affect the\ntime-course of variant spread, with lower connectivity slowing down convergence\nof the population onto a single cultural variant. We also show that, compared\nto a neutral evolutionary model, content bias accelerates convergence and\namplifies the effects of connectivity dynamics, whilst larger memory size and\ncoordination bias, especially egocentric bias, slow down convergence.\nFurthermore, connectivity dynamics affect the frequency of high quality\nvariants (adaptiveness), with late connectivity populations showing bursts of\nrapid change in adaptiveness followed by periods of relatively slower change,\nand early connectivity populations following a single-peak evolutionary\ndynamic. In this way, we provide for the first time a direct connection between\nthe order of agents' interactions and punctuational evolution.", "published": "2019-02-09 15:53:14", "link": "http://arxiv.org/abs/1902.06598v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Generative Moment Matching Network-based Random Modulation Post-filter\n  for DNN-based Singing Voice Synthesis and Neural Double-tracking", "abstract": "This paper proposes a generative moment matching network (GMMN)-based\npost-filter that provides inter-utterance pitch variation for deep neural\nnetwork (DNN)-based singing voice synthesis. The natural pitch variation of a\nhuman singing voice leads to a richer musical experience and is used in\ndouble-tracking, a recording method in which two performances of the same\nphrase are recorded and mixed to create a richer, layered sound. However,\nsinging voices synthesized using conventional DNN-based methods never vary\nbecause the synthesis process is deterministic and only one waveform is\nsynthesized from one musical score. To address this problem, we use a GMMN to\nmodel the variation of the modulation spectrum of the pitch contour of natural\nsinging voices and add a randomized inter-utterance variation to the pitch\ncontour generated by conventional DNN-based singing voice synthesis.\nExperimental evaluations suggest that 1) our approach can provide perceptible\ninter-utterance pitch variation while preserving speech quality. We extend our\napproach to double-tracking, and the evaluation demonstrates that 2) GMMN-based\nneural double-tracking is perceptually closer to natural double-tracking than\nconventional signal processing-based artificial double-tracking is.", "published": "2019-02-09 07:49:42", "link": "http://arxiv.org/abs/1902.03389v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.MM", "cs.NE", "eess.AS"], "primary_category": "cs.SD"}
