{"title": "Semantic Refinement GRU-based Neural Language Generation for Spoken\n  Dialogue Systems", "abstract": "Natural language generation (NLG) plays a critical role in spoken dialogue\nsystems. This paper presents a new approach to NLG by using recurrent neural\nnetworks (RNN), in which a gating mechanism is applied before RNN computation.\nThis allows the proposed model to generate appropriate sentences. The RNN-based\ngenerator can be learned from unaligned data by jointly training sentence\nplanning and surface realization to produce natural language responses. The\nmodel was extensively evaluated on four different NLG domains. The results show\nthat the proposed generator achieved better performance on all the NLG domains\ncompared to previous generators.", "published": "2017-06-01 00:37:46", "link": "http://arxiv.org/abs/1706.00134v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Natural Language Generation for Spoken Dialogue System using RNN\n  Encoder-Decoder Networks", "abstract": "Natural language generation (NLG) is a critical component in a spoken\ndialogue system. This paper presents a Recurrent Neural Network based\nEncoder-Decoder architecture, in which an LSTM-based decoder is introduced to\nselect, aggregate semantic elements produced by an attention mechanism over the\ninput elements, and to produce the required utterances. The proposed generator\ncan be jointly trained both sentence planning and surface realization to\nproduce natural language sentences. The proposed model was extensively\nevaluated on four different NLG datasets. The experimental results showed that\nthe proposed generators not only consistently outperform the previous methods\nacross all the NLG domains but also show an ability to generalize from a new,\nunseen domain and learn from multi-domain datasets.", "published": "2017-06-01 01:06:17", "link": "http://arxiv.org/abs/1706.00139v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Polish Read Speech Corpus for Speech Tools and Services", "abstract": "This paper describes the speech processing activities conducted at the Polish\nconsortium of the CLARIN project. The purpose of this segment of the project\nwas to develop specific tools that would allow for automatic and semi-automatic\nprocessing of large quantities of acoustic speech data. The tools include the\nfollowing: grapheme-to-phoneme conversion, speech-to-text alignment, voice\nactivity detection, speaker diarization, keyword spotting and automatic speech\ntranscription. Furthermore, in order to develop these tools, a large\nhigh-quality studio speech corpus was recorded and released under an open\nlicense, to encourage development in the area of Polish speech research.\nAnother purpose of the corpus was to serve as a reference for studies in\nphonetics and pronunciation. All the tools and resources were released on the\nthe Polish CLARIN website. This paper discusses the current status and future\nplans for the project.", "published": "2017-06-01 10:27:07", "link": "http://arxiv.org/abs/1706.00245v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using of heterogeneous corpora for training of an ASR system", "abstract": "The paper summarizes the development of the LVCSR system built as a part of\nthe Pashto speech-translation system at the SCALE (Summer Camp for Applied\nLanguage Exploration) 2015 workshop on \"Speech-to-text-translation for\nlow-resource languages\". The Pashto language was chosen as a good \"proxy\"\nlow-resource language, exhibiting multiple phenomena which make the\nspeech-recognition and and speech-to-text-translation systems development hard.\n  Even when the amount of data is seemingly sufficient, given the fact that the\ndata originates from multiple sources, the preliminary experiments reveal that\nthere is little to no benefit in merging (concatenating) the corpora and more\nelaborate ways of making use of all of the data must be worked out.\n  This paper concentrates only on the LVCSR part and presents a range of\ndifferent techniques that were found to be useful in order to benefit from\nmultiple different corpora", "published": "2017-06-01 14:30:19", "link": "http://arxiv.org/abs/1706.00321v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Morph-fitting: Fine-Tuning Word Vector Spaces with Simple\n  Language-Specific Rules", "abstract": "Morphologically rich languages accentuate two properties of distributional\nvector space models: 1) the difficulty of inducing accurate representations for\nlow-frequency word forms; and 2) insensitivity to distinct lexical relations\nthat have similar distributional signatures. These effects are detrimental for\nlanguage understanding systems, which may infer that 'inexpensive' is a\nrephrasing for 'expensive' or may not associate 'acquire' with 'acquires'. In\nthis work, we propose a novel morph-fitting procedure which moves past the use\nof curated semantic lexicons for improving distributional vector spaces.\nInstead, our method injects morphological constraints generated using simple\nlanguage-specific rules, pulling inflectional forms of the same word close\ntogether and pushing derivational antonyms far apart. In intrinsic evaluation\nover four languages, we show that our approach: 1) improves low-frequency word\nestimates; and 2) boosts the semantic quality of the entire word vector\ncollection. Finally, we show that morph-fitted vectors yield large gains in the\ndownstream task of dialogue state tracking, highlighting the importance of\nmorphology for tackling long-tail phenomena in language understanding tasks.", "published": "2017-06-01 16:31:20", "link": "http://arxiv.org/abs/1706.00377v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NMTPY: A Flexible Toolkit for Advanced Neural Machine Translation\n  Systems", "abstract": "In this paper, we present nmtpy, a flexible Python toolkit based on Theano\nfor training Neural Machine Translation and other neural sequence-to-sequence\narchitectures. nmtpy decouples the specification of a network from the training\nand inference utilities to simplify the addition of a new architecture and\nreduce the amount of boilerplate code to be written. nmtpy has been used for\nLIUM's top-ranked submissions to WMT Multimodal Machine Translation and News\nTranslation tasks in 2016 and 2017.", "published": "2017-06-01 18:57:39", "link": "http://arxiv.org/abs/1706.00457v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Machine Assisted Analysis of Vowel Length Contrasts in Wolof", "abstract": "Growing digital archives and improving algorithms for automatic analysis of\ntext and speech create new research opportunities for fundamental research in\nphonetics. Such empirical approaches allow statistical evaluation of a much\nlarger set of hypothesis about phonetic variation and its conditioning factors\n(among them geographical / dialectal variants). This paper illustrates this\nvision and proposes to challenge automatic methods for the analysis of a not\neasily observable phenomenon: vowel length contrast. We focus on Wolof, an\nunder-resourced language from Sub-Saharan Africa. In particular, we propose\nmultiple features to make a fine evaluation of the degree of length contrast\nunder different factors such as: read vs semi spontaneous speech ; standard vs\ndialectal Wolof. Our measures made fully automatically on more than 20k vowel\ntokens show that our proposed features can highlight different degrees of\ncontrast for each vowel considered. We notably show that contrast is weaker in\nsemi-spontaneous speech and in a non standard semi-spontaneous dialect.", "published": "2017-06-01 19:20:50", "link": "http://arxiv.org/abs/1706.00465v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Function Assistant: A Tool for NL Querying of APIs", "abstract": "In this paper, we describe Function Assistant, a lightweight Python-based\ntoolkit for querying and exploring source code repositories using natural\nlanguage. The toolkit is designed to help end-users of a target API quickly\nfind information about functions through high-level natural language queries\nand descriptions. For a given text query and background API, the tool finds\ncandidate functions by performing a translation from the text to known\nrepresentations in the API using the semantic parsing approach of Richardson\nand Kuhn (2017). Translations are automatically learned from example text-code\npairs in example APIs. The toolkit includes features for building translation\npipelines and query engines for arbitrary source code projects. To explore this\nlast feature, we perform new experiments on 27 well-known Python projects\nhosted on Github.", "published": "2017-06-01 19:26:32", "link": "http://arxiv.org/abs/1706.00468v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Morphological Embeddings for Named Entity Recognition in Morphologically\n  Rich Languages", "abstract": "In this work, we present new state-of-the-art results of 93.59,% and 79.59,%\nfor Turkish and Czech named entity recognition based on the model of (Lample et\nal., 2016). We contribute by proposing several schemes for representing the\nmorphological analysis of a word in the context of named entity recognition. We\nshow that a concatenation of this representation with the word and character\nembeddings improves the performance. The effect of these representation schemes\non the tagging performance is also investigated.", "published": "2017-06-01 21:59:47", "link": "http://arxiv.org/abs/1706.00506v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep Learning for Hate Speech Detection in Tweets", "abstract": "Hate speech detection on Twitter is critical for applications like\ncontroversial event extraction, building AI chatterbots, content\nrecommendation, and sentiment analysis. We define this task as being able to\nclassify a tweet as racist, sexist or neither. The complexity of the natural\nlanguage constructs makes this task very challenging. We perform extensive\nexperiments with multiple deep learning architectures to learn semantic word\nembeddings to handle this complexity. Our experiments on a benchmark dataset of\n16K annotated tweets show that such deep learning methods outperform\nstate-of-the-art char/word n-gram methods by ~18 F1 points.", "published": "2017-06-01 07:25:22", "link": "http://arxiv.org/abs/1706.00188v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Learning to Compute Word Embeddings On the Fly", "abstract": "Words in natural language follow a Zipfian distribution whereby some words\nare frequent but most are rare. Learning representations for words in the \"long\ntail\" of this distribution requires enormous amounts of data. Representations\nof rare words trained directly on end tasks are usually poor, requiring us to\npre-train embeddings on external data, or treat all rare words as\nout-of-vocabulary words with a unique representation. We provide a method for\npredicting embeddings of rare words on the fly from small amounts of auxiliary\ndata with a network trained end-to-end for the downstream task. We show that\nthis improves results against baselines where embeddings are trained on the end\ntask for reading comprehension, recognizing textual entailment and language\nmodeling.", "published": "2017-06-01 13:12:15", "link": "http://arxiv.org/abs/1706.00286v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Teaching Machines to Describe Images via Natural Language Feedback", "abstract": "Robots will eventually be part of every household. It is thus critical to\nenable algorithms to learn from and be guided by non-expert users. In this\npaper, we bring a human in the loop, and enable a human teacher to give\nfeedback to a learning agent in the form of natural language. We argue that a\ndescriptive sentence can provide a much stronger learning signal than a numeric\nreward in that it can easily point to where the mistakes are and how to correct\nthem. We focus on the problem of image captioning in which the quality of the\noutput can easily be judged by non-experts. We propose a hierarchical\nphrase-based captioning model trained with policy gradients, and design a\nfeedback network that provides reward to the learner by conditioning on the\nhuman-provided feedback. We show that by exploiting descriptive feedback our\nmodel learns to perform better than when given independently written human\ncaptions.", "published": "2017-06-01 00:24:55", "link": "http://arxiv.org/abs/1706.00130v2", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Transfer Learning for Speech Recognition on a Budget", "abstract": "End-to-end training of automated speech recognition (ASR) systems requires\nmassive data and compute resources. We explore transfer learning based on model\nadaptation as an approach for training ASR models under constrained GPU memory,\nthroughput and training data. We conduct several systematic experiments\nadapting a Wav2Letter convolutional neural network originally trained for\nEnglish ASR to the German language. We show that this technique allows faster\ntraining on consumer-grade resources while requiring less training data in\norder to achieve the same accuracy, thereby lowering the cost of training ASR\nmodels in other languages. Model introspection revealed that small adaptations\nto the network's weights were sufficient for good performance, especially for\ninner layers.", "published": "2017-06-01 13:33:54", "link": "http://arxiv.org/abs/1706.00290v1", "categories": ["cs.LG", "cs.CL", "cs.NE", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Discovering Discrete Latent Topics with Neural Variational Inference", "abstract": "Topic models have been widely explored as probabilistic generative models of\ndocuments. Traditional inference methods have sought closed-form derivations\nfor updating the models, however as the expressiveness of these models grows,\nso does the difficulty of performing fast and accurate inference over their\nparameters. This paper presents alternative neural approaches to topic\nmodelling by providing parameterisable distributions over topics which permit\ntraining by backpropagation in the framework of neural variational inference.\nIn addition, with the help of a stick-breaking construction, we propose a\nrecurrent network that is able to discover a notionally unbounded number of\ntopics, analogous to Bayesian non-parametric topic models. Experimental results\non the MXM Song Lyrics, 20NewsGroups and Reuters News datasets demonstrate the\neffectiveness and efficiency of these neural topic models.", "published": "2017-06-01 15:55:42", "link": "http://arxiv.org/abs/1706.00359v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Semantic Specialisation of Distributional Word Vector Spaces using\n  Monolingual and Cross-Lingual Constraints", "abstract": "We present Attract-Repel, an algorithm for improving the semantic quality of\nword vectors by injecting constraints extracted from lexical resources.\nAttract-Repel facilitates the use of constraints from mono- and cross-lingual\nresources, yielding semantically specialised cross-lingual vector spaces. Our\nevaluation shows that the method can make use of existing cross-lingual\nlexicons to construct high-quality vector spaces for a plethora of different\nlanguages, facilitating semantic transfer from high- to lower-resource ones.\nThe effectiveness of our approach is demonstrated with state-of-the-art results\non semantic similarity datasets in six languages. We next show that\nAttract-Repel-specialised vectors boost performance in the downstream task of\ndialogue state tracking (DST) across multiple languages. Finally, we show that\ncross-lingual vector spaces produced by our algorithm facilitate the training\nof multilingual DST models, which brings further performance improvements.", "published": "2017-06-01 16:29:47", "link": "http://arxiv.org/abs/1706.00374v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
