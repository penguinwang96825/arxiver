{"title": "Towards NLP with Deep Learning: Convolutional Neural Networks and\n  Recurrent Neural Networks for Offensive Language Identification in Social\n  Media", "abstract": "This short paper presents the design decisions taken and challenges\nencountered in completing SemEval Task 6, which poses the problem of\nidentifying and categorizing offensive language in tweets. Our proposed\nsolutions explore Deep Learning techniques, Linear Support Vector\nclassification and Random Forests to identify offensive tweets, to classify\noffenses as targeted or untargeted and eventually to identify the target\nsubject type.", "published": "2019-03-02 09:42:54", "link": "http://arxiv.org/abs/1903.00665v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Predicting and interpreting embeddings for out of vocabulary words in\n  downstream tasks", "abstract": "We propose a novel way to handle out of vocabulary (OOV) words in downstream\nnatural language processing (NLP) tasks. We implement a network that predicts\nuseful embeddings for OOV words based on their morphology and on the context in\nwhich they appear. Our model also incorporates an attention mechanism\nindicating the focus allocated to the left context words, the right context\nwords or the word's characters, hence making the prediction more interpretable.\nThe model is a ``drop-in'' module that is jointly trained with the downstream\ntask's neural network, thus producing embeddings specialized for the task at\nhand. When the task is mostly syntactical, we observe that our model aims most\nof its attention on surface form characters. On the other hand, for tasks more\nsemantical, the network allocates more attention to the surrounding words. In\nall our tests, the module helps the network to achieve better performances in\ncomparison to the use of simple random embeddings.", "published": "2019-03-02 15:32:39", "link": "http://arxiv.org/abs/1903.00724v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Weakly Labelled AudioSet Tagging with Attention Neural Networks", "abstract": "Audio tagging is the task of predicting the presence or absence of sound\nclasses within an audio clip. Previous work in audio tagging focused on\nrelatively small datasets limited to recognising a small number of sound\nclasses. We investigate audio tagging on AudioSet, which is a dataset\nconsisting of over 2 million audio clips and 527 classes. AudioSet is weakly\nlabelled, in that only the presence or absence of sound classes is known for\neach clip, while the onset and offset times are unknown. To address the\nweakly-labelled audio tagging problem, we propose attention neural networks as\na way to attend the most salient parts of an audio clip. We bridge the\nconnection between attention neural networks and multiple instance learning\n(MIL) methods, and propose decision-level and feature-level attention neural\nnetworks for audio tagging. We investigate attention neural networks modeled by\ndifferent functions, depths and widths. Experiments on AudioSet show that the\nfeature-level attention neural network achieves a state-of-the-art mean average\nprecision (mAP) of 0.369, outperforming the best multiple instance learning\n(MIL) method of 0.317 and Google's deep neural network baseline of 0.314. In\naddition, we discover that the audio tagging performance on AudioSet embedding\nfeatures has a weak correlation with the number of training samples and the\nquality of labels of each sound class.", "published": "2019-03-02 21:13:59", "link": "http://arxiv.org/abs/1903.00765v6", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Making Sense of Audio Vibration for Liquid Height Estimation in Robotic\n  Pouring", "abstract": "In this paper, we focus on the challenging perception problem in robotic\npouring. Most of the existing approaches either leverage visual or haptic\ninformation. However, these techniques may suffer from poor generalization\nperformances on opaque containers or concerning measuring precision. To tackle\nthese drawbacks, we propose to make use of audio vibration sensing and design a\ndeep neural network PouringNet to predict the liquid height from the audio\nfragment during the robotic pouring task. PouringNet is trained on our\ncollected real-world pouring dataset with multimodal sensing data, which\ncontains more than 3000 recordings of audio, force feedback, video and\ntrajectory data of the human hand that performs the pouring task. Each record\nrepresents a complete pouring procedure. We conduct several evaluations on\nPouringNet with our dataset and robotic hardware. The results demonstrate that\nour PouringNet generalizes well across different liquid containers, positions\nof the audio receiver, initial liquid heights and types of liquid, and\nfacilitates a more robust and accurate audio-based perception for robotic\npouring.", "published": "2019-03-02 08:09:46", "link": "http://arxiv.org/abs/1903.00650v2", "categories": ["cs.RO", "cs.SD", "eess.AS"], "primary_category": "cs.RO"}
