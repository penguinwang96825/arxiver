{"title": "A Resource for Computational Experiments on Mapudungun", "abstract": "We present a resource for computational experiments on Mapudungun, a\npolysynthetic indigenous language spoken in Chile with upwards of 200 thousand\nspeakers. We provide 142 hours of culturally significant conversations in the\ndomain of medical treatment. The conversations are fully transcribed and\ntranslated into Spanish. The transcriptions also include annotations for\ncode-switching and non-standard pronunciations. We also provide baseline\nresults on three core NLP tasks: speech recognition, speech synthesis, and\nmachine translation between Spanish and Mapudungun. We further explore other\napplications for which the corpus will be suitable, including the study of\ncode-switching, historical orthography change, linguistic structure, and\nsociological and anthropological studies.", "published": "2019-12-04 02:26:39", "link": "http://arxiv.org/abs/1912.01772v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Acquiring Knowledge from Pre-trained Model to Neural Machine Translation", "abstract": "Pre-training and fine-tuning have achieved great success in the natural\nlanguage process field. The standard paradigm of exploiting them includes two\nsteps: first, pre-training a model, e.g. BERT, with a large scale unlabeled\nmonolingual data. Then, fine-tuning the pre-trained model with labeled data\nfrom downstream tasks. However, in neural machine translation (NMT), we address\nthe problem that the training objective of the bilingual task is far different\nfrom the monolingual pre-trained model. This gap leads that only using\nfine-tuning in NMT can not fully utilize prior language knowledge. In this\npaper, we propose an APT framework for acquiring knowledge from the pre-trained\nmodel to NMT. The proposed approach includes two modules: 1). a dynamic fusion\nmechanism to fuse task-specific features adapted from general knowledge into\nNMT network, 2). a knowledge distillation paradigm to learn language knowledge\ncontinuously during the NMT training process. The proposed approach could\nintegrate suitable knowledge from pre-trained models to improve the NMT.\nExperimental results on WMT English to German, German to English and Chinese to\nEnglish machine translation tasks show that our model outperforms strong\nbaselines and the fine-tuning counterparts.", "published": "2019-12-04 02:54:18", "link": "http://arxiv.org/abs/1912.01774v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Relation Extraction Using Syntactic Indicators and Sentential\n  Contexts", "abstract": "State-of-the-art methods for relation extraction consider the sentential\ncontext by modeling the entire sentence. However, syntactic indicators, certain\nphrases or words like prepositions that are more informative than other words\nand may be beneficial for identifying semantic relations. Other approaches\nusing fixed text triggers capture such information but ignore the lexical\ndiversity. To leverage both syntactic indicators and sentential contexts, we\npropose an indicator-aware approach for relation extraction. Firstly, we\nextract syntactic indicators under the guidance of syntactic knowledge. Then we\nconstruct a neural network to incorporate both syntactic indicators and the\nentire sentences into better relation representations. By this way, the\nproposed model alleviates the impact of noisy information from entire sentences\nand breaks the limit of text triggers. Experiments on the SemEval-2010 Task 8\nbenchmark dataset show that our model significantly outperforms the\nstate-of-the-art methods.", "published": "2019-12-04 09:16:36", "link": "http://arxiv.org/abs/1912.01858v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Machine Translation: A Review and Survey", "abstract": "The field of machine translation (MT), the automatic translation of written\ntext from one natural language into another, has experienced a major paradigm\nshift in recent years. Statistical MT, which mainly relies on various\ncount-based models and which used to dominate MT research for decades, has\nlargely been superseded by neural machine translation (NMT), which tackles\ntranslation with a single neural network. In this work we will trace back the\norigins of modern NMT architectures to word and sentence embeddings and earlier\nexamples of the encoder-decoder network family. We will conclude with a survey\nof recent trends in the field.", "published": "2019-12-04 15:16:03", "link": "http://arxiv.org/abs/1912.02047v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Exploration of Data Augmentation and Sampling Techniques for\n  Domain-Agnostic Question Answering", "abstract": "To produce a domain-agnostic question answering model for the Machine Reading\nQuestion Answering (MRQA) 2019 Shared Task, we investigate the relative\nbenefits of large pre-trained language models, various data sampling\nstrategies, as well as query and context paraphrases generated by\nback-translation. We find a simple negative sampling technique to be\nparticularly effective, even though it is typically used for datasets that\ninclude unanswerable questions, such as SQuAD 2.0. When applied in conjunction\nwith per-domain sampling, our XLNet (Yang et al., 2019)-based submission\nachieved the second best Exact Match and F1 in the MRQA leaderboard\ncompetition.", "published": "2019-12-04 17:48:58", "link": "http://arxiv.org/abs/1912.02145v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cooperative Reasoning on Knowledge Graph and Corpus: A\n  Multi-agentReinforcement Learning Approach", "abstract": "Knowledge-graph-based reasoning has drawn a lot of attention due to its\ninterpretability. However, previous methods suffer from the incompleteness of\nthe knowledge graph, namely the interested link or entity that can be missing\nin the knowledge graph(explicit missing). Also, most previous models assume the\ndistance between the target and source entity is short, which is not true on a\nreal-world KG like Freebase(implicit missing). The sensitivity to the\nincompleteness of KG and the incapability to capture the long-distance link\nbetween entities have limited the performance of these models on large KG. In\nthis paper, we propose a model that leverages the text corpus to cure such\nlimitations, either the explicit or implicit missing links. We model the\nquestion answering on KG as a cooperative task between two agents, a knowledge\ngraph reasoning agent and an information extraction agent. Each agent learns\nits skill to complete its own task, hopping on KG or select knowledge from the\ncorpus, via maximizing the reward for correctly answering the question. The\nreasoning agent decides how to find an equivalent path for the given entity and\nrelation. The extraction agent provide shortcut for long-distance target entity\nor provide missing relations for explicit missing links with messages from the\nreasoning agent. Through such cooperative reward design, our model can augment\nthe incomplete KG strategically while not introduce much unnecessary noise that\ncould enlarge the search space and lower the performance.", "published": "2019-12-04 19:00:54", "link": "http://arxiv.org/abs/1912.02206v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Implicit Knowledge in Argumentative Texts: An Annotated Corpus", "abstract": "When speaking or writing, people omit information that seems clear and\nevident, such that only part of the message is expressed in words. Especially\nin argumentative texts it is very common that (important) parts of the argument\nare implied and omitted. We hypothesize that for argument analysis it will be\nbeneficial to reconstruct this implied information. As a starting point for\nfilling such knowledge gaps, we build a corpus consisting of high-quality human\nannotations of missing and implied information in argumentative texts. To learn\nmore about the characteristics of both the argumentative texts and the added\ninformation, we further annotate the data with semantic clause types and\ncommonsense knowledge relations. The outcome of our work is a carefully\nde-signed and richly annotated dataset, for which we then provide an in-depth\nanalysis by investigating characteristic distributions and correlations of the\nassigned labels. We reveal interesting patterns and intersections between the\nannotation categories and properties of our dataset, which enable insights into\nthe characteristics of both argumentative texts and implicit knowledge in terms\nof structural features and semantic information. The results of our analysis\ncan help to assist automated argument analysis and can guide the process of\nrevealing implicit information in argumentative texts automatically.", "published": "2019-12-04 14:43:46", "link": "http://arxiv.org/abs/1912.10161v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Building a Multilingual Sememe Knowledge Base: Predicting\n  Sememes for BabelNet Synsets", "abstract": "A sememe is defined as the minimum semantic unit of human languages. Sememe\nknowledge bases (KBs), which contain words annotated with sememes, have been\nsuccessfully applied to many NLP tasks. However, existing sememe KBs are built\non only a few languages, which hinders their widespread utilization. To address\nthe issue, we propose to build a unified sememe KB for multiple languages based\non BabelNet, a multilingual encyclopedic dictionary. We first build a dataset\nserving as the seed of the multilingual sememe KB. It manually annotates\nsememes for over $15$ thousand synsets (the entries of BabelNet). Then, we\npresent a novel task of automatic sememe prediction for synsets, aiming to\nexpand the seed dataset into a usable KB. We also propose two simple and\neffective models, which exploit different information of synsets. Finally, we\nconduct quantitative and qualitative analyses to explore important factors and\ndifficulties in the task. All the source code and data of this work can be\nobtained on https://github.com/thunlp/BabelNet-Sememe-Prediction.", "published": "2019-12-04 04:39:32", "link": "http://arxiv.org/abs/1912.01795v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PDC -- a probabilistic distributional clustering algorithm: a case study\n  on suicide articles in PubMed", "abstract": "The need to organize a large collection in a manner that facilitates human\ncomprehension is crucial given the ever-increasing volumes of information. In\nthis work, we present PDC (probabilistic distributional clustering), a novel\nalgorithm that, given a document collection, computes disjoint term sets\nrepresenting topics in the collection. The algorithm relies on probabilities of\nword co-occurrences to partition the set of terms appearing in the collection\nof documents into disjoint groups of related terms. In this work, we also\npresent an environment to visualize the computed topics in the term space and\nretrieve the most related PubMed articles for each group of terms. We\nillustrate the algorithm by applying it to PubMed documents on the topic of\nsuicide. Suicide is a major public health problem identified as the tenth\nleading cause of death in the US. In this application, our goal is to provide a\nglobal view of the mental health literature pertaining to the subject of\nsuicide, and through this, to help create a rich environment of multifaceted\ndata to guide health care researchers in their endeavor to better understand\nthe breadth, depth and scope of the problem. We demonstrate the usefulness of\nthe proposed algorithm by providing a web portal that allows mental health\nresearchers to peruse the suicide-related literature in PubMed.", "published": "2019-12-04 16:07:25", "link": "http://arxiv.org/abs/1912.02077v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Keyword Aware Influential Community Search in Large Attributed Graphs", "abstract": "We introduce a novel keyword-aware influential community query KICQ that\nfinds the most influential communities from an attributed graph, where an\ninfluential community is defined as a closely connected group of vertices\nhaving some dominance over other groups of vertices with the expertise (a set\nof keywords) matching with the query terms (words or phrases). We first design\nthe KICQ that facilitates users to issue an influential CS query intuitively by\nusing a set of query terms, and predicates (AND or OR). In this context, we\npropose a novel word-embedding based similarity model that enables semantic\ncommunity search, which substantially alleviates the limitations of exact\nkeyword based community search. Next, we propose a new influence measure for a\ncommunity that considers both the cohesiveness and influence of the community\nand eliminates the need for specifying values of internal parameters of a\nnetwork. Finally, we propose two efficient algorithms for searching influential\ncommunities in large attributed graphs. We present detailed experiments and a\ncase study to demonstrate the effectiveness and efficiency of the proposed\napproaches.", "published": "2019-12-04 16:59:40", "link": "http://arxiv.org/abs/1912.02114v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Integrating Knowledge into End-to-End Speech Recognition from External\n  Text-Only Data", "abstract": "Attention-based encoder-decoder (AED) models have achieved promising\nperformance in speech recognition. However, because of the end-to-end training,\nan AED model is usually trained with speech-text paired data. It is challenging\nto incorporate external text-only data into AED models. Another issue of the\nAED model is that it does not use the right context of a text token while\npredicting the token. To alleviate the above two issues, we propose a unified\nmethod called LST (Learn Spelling from Teachers) to integrate knowledge into an\nAED model from the external text-only data and leverage the whole context in a\nsentence. The method is divided into two stages. First, in the representation\nstage, a language model is trained on the text. It can be seen as that the\nknowledge in the text is compressed into the LM. Then, at the transferring\nstage, the knowledge is transferred to the AED model via teacher-student\nlearning. To further use the whole context of the text sentence, we propose an\nLM called causal cloze completer (COR), which estimates the probability of a\ntoken, given both the left context and the right context of it. Therefore, with\nLST training, the AED model can leverage the whole context in the sentence.\nDifferent from fusion based methods, which use LM during decoding, the proposed\nmethod does not increase any extra complexity at the inference stage. We\nconduct experiments on two scales of public Chinese datasets AISHELL-1 and\nAISHELL-2. The experimental results demonstrate the effectiveness of leveraging\nexternal text-only data and the whole context in a sentence with our proposed\nmethod, compared with baseline hybrid systems and AED model based systems.", "published": "2019-12-04 03:12:27", "link": "http://arxiv.org/abs/1912.01777v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Towards Constructing a Corpus for Studying the Effects of Treatments and\n  Substances Reported in PubMed Abstracts", "abstract": "We present the construction of an annotated corpus of PubMed abstracts\nreporting about positive, negative or neutral effects of treatments or\nsubstances. Our ultimate goal is to annotate one sentence (rationale) for each\nabstract and to use this resource as a training set for text classification of\neffects discussed in PubMed abstracts. Currently, the corpus consists of 750\nabstracts. We describe the automatic processing that supports the corpus\nconstruction, the manual annotation activities and some features of the medical\nlanguage in the abstracts selected for the annotated corpus. It turns out that\nrecognizing the terminology and the abbreviations is key for determining the\nrationale sentence. The corpus will be applied to improve our classifier, which\ncurrently has accuracy of 78.80% achieved with normalization of the abstract\nterms based on UMLS concepts from specific semantic groups and an SVM with a\nlinear kernel. Finally, we discuss some other possible applications of this\ncorpus.", "published": "2019-12-04 07:22:32", "link": "http://arxiv.org/abs/1912.01831v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "PitchNet: Unsupervised Singing Voice Conversion with Pitch Adversarial\n  Network", "abstract": "Singing voice conversion is to convert a singer's voice to another one's\nvoice without changing singing content. Recent work shows that unsupervised\nsinging voice conversion can be achieved with an autoencoder-based approach\n[1]. However, the converted singing voice can be easily out of key, showing\nthat the existing approach cannot model the pitch information precisely. In\nthis paper, we propose to advance the existing unsupervised singing voice\nconversion method proposed in [1] to achieve more accurate pitch translation\nand flexible pitch manipulation. Specifically, the proposed PitchNet added an\nadversarially trained pitch regression network to enforce the encoder network\nto learn pitch invariant phoneme representation, and a separate module to feed\npitch extracted from the source audio to the decoder network. Our evaluation\nshows that the proposed method can greatly improve the quality of the converted\nsinging voice (2.92 vs 3.75 in MOS). We also demonstrate that the pitch of\nconverted singing can be easily controlled during generation by changing the\nlevels of the extracted pitch before passing it to the decoder network.", "published": "2019-12-04 08:56:13", "link": "http://arxiv.org/abs/1912.01852v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Scalable Bayesian Preference Learning for Crowds", "abstract": "We propose a scalable Bayesian preference learning method for jointly\npredicting the preferences of individuals as well as the consensus of a crowd\nfrom pairwise labels. Peoples' opinions often differ greatly, making it\ndifficult to predict their preferences from small amounts of personal data.\nIndividual biases also make it harder to infer the consensus of a crowd when\nthere are few labels per item. We address these challenges by combining matrix\nfactorisation with Gaussian processes, using a Bayesian approach to account for\nuncertainty arising from noisy and sparse data. Our method exploits input\nfeatures, such as text embeddings and user metadata, to predict preferences for\nnew items and users that are not in the training set. As previous solutions\nbased on Gaussian processes do not scale to large numbers of users, items or\npairwise labels, we propose a stochastic variational inference approach that\nlimits computational and memory costs. Our experiments on a recommendation task\nshow that our method is competitive with previous approaches despite our\nscalable inference approximation. We demonstrate the method's scalability on a\nnatural language processing task with thousands of users and items, and show\nimprovements over the state of the art on this task. We make our software\npublicly available for future work.", "published": "2019-12-04 13:56:38", "link": "http://arxiv.org/abs/1912.01987v2", "categories": ["cs.LG", "cs.CL", "cs.HC", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Plug and Play Language Models: A Simple Approach to Controlled Text\n  Generation", "abstract": "Large transformer-based language models (LMs) trained on huge text corpora\nhave shown unparalleled generation capabilities. However, controlling\nattributes of the generated language (e.g. switching topic or sentiment) is\ndifficult without modifying the model architecture or fine-tuning on\nattribute-specific data and entailing the significant cost of retraining. We\npropose a simple alternative: the Plug and Play Language Model (PPLM) for\ncontrollable language generation, which combines a pretrained LM with one or\nmore simple attribute classifiers that guide text generation without any\nfurther training of the LM. In the canonical scenario we present, the attribute\nmodels are simple classifiers consisting of a user-specified bag of words or a\nsingle learned layer with 100,000 times fewer parameters than the LM. Sampling\nentails a forward and backward pass in which gradients from the attribute model\npush the LM's hidden activations and thus guide the generation. Model samples\ndemonstrate control over a range of topics and sentiment styles, and extensive\nautomated and human annotated evaluations show attribute alignment and fluency.\nPPLMs are flexible in that any combination of differentiable attribute models\nmay be used to steer text generation, which will allow for diverse and creative\napplications beyond the examples given in this paper.", "published": "2019-12-04 18:32:15", "link": "http://arxiv.org/abs/1912.02164v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AMUSED: A Multi-Stream Vector Representation Method for Use in Natural\n  Dialogue", "abstract": "The problem of building a coherent and non-monotonous conversational agent\nwith proper discourse and coverage is still an area of open research. Current\narchitectures only take care of semantic and contextual information for a given\nquery and fail to completely account for syntactic and external knowledge which\nare crucial for generating responses in a chit-chat system. To overcome this\nproblem, we propose an end to end multi-stream deep learning architecture which\nlearns unified embeddings for query-response pairs by leveraging contextual\ninformation from memory networks and syntactic information by incorporating\nGraph Convolution Networks (GCN) over their dependency parse. A stream of this\nnetwork also utilizes transfer learning by pre-training a bidirectional\ntransformer to extract semantic representation for each input sentence and\nincorporates external knowledge through the the neighborhood of the entities\nfrom a Knowledge Base (KB). We benchmark these embeddings on next sentence\nprediction task and significantly improve upon the existing techniques.\nFurthermore, we use AMUSED to represent query and responses along with its\ncontext to develop a retrieval based conversational agent which has been\nvalidated by expert linguists to have comprehensive engagement with humans.", "published": "2019-12-04 17:35:03", "link": "http://arxiv.org/abs/1912.10160v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Cross-Language Aphasia Detection using Optimal Transport Domain\n  Adaptation", "abstract": "Multi-language speech datasets are scarce and often have small sample sizes\nin the medical domain. Robust transfer of linguistic features across languages\ncould improve rates of early diagnosis and therapy for speakers of low-resource\nlanguages when detecting health conditions from speech. We utilize\nout-of-domain, unpaired, single-speaker, healthy speech data for training\nmultiple Optimal Transport (OT) domain adaptation systems. We learn mappings\nfrom other languages to English and detect aphasia from linguistic\ncharacteristics of speech, and show that OT domain adaptation improves aphasia\ndetection over unilingual baselines for French (6% increased F1) and Mandarin\n(5% increased F1). Further, we show that adding aphasic data to the domain\nadaptation system significantly increases performance for both French and\nMandarin, increasing the F1 scores further (10% and 8% increase in F1 scores\nfor French and Mandarin, respectively, over unilingual baselines).", "published": "2019-12-04 19:48:54", "link": "http://arxiv.org/abs/1912.04370v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
