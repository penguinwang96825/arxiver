{"title": "Robust Subgraph Generation Improves Abstract Meaning Representation\n  Parsing", "abstract": "The Abstract Meaning Representation (AMR) is a representation for open-domain\nrich semantics, with potential use in fields like event extraction and machine\ntranslation. Node generation, typically done using a simple dictionary lookup,\nis currently an important limiting factor in AMR parsing. We propose a small\nset of actions that derive AMR subgraphs by transformations on spans of text,\nwhich allows for more robust learning of this stage. Our set of construction\nactions generalize better than the previous approach, and can be learned with a\nsimple classifier. We improve on the previous state-of-the-art result for AMR\nparsing, boosting end-to-end performance by 3 F$_1$ on both the LDC2013E117 and\nLDC2014T12 datasets.", "published": "2015-06-10 00:40:12", "link": "http://arxiv.org/abs/1506.03139v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A cognitive neural architecture able to learn and communicate through\n  natural language", "abstract": "Communicative interactions involve a kind of procedural knowledge that is\nused by the human brain for processing verbal and nonverbal inputs and for\nlanguage production. Although considerable work has been done on modeling human\nlanguage abilities, it has been difficult to bring them together to a\ncomprehensive tabula rasa system compatible with current knowledge of how\nverbal information is processed in the brain. This work presents a cognitive\nsystem, entirely based on a large-scale neural architecture, which was\ndeveloped to shed light on the procedural knowledge involved in language\nelaboration. The main component of this system is the central executive, which\nis a supervising system that coordinates the other components of the working\nmemory. In our model, the central executive is a neural network that takes as\ninput the neural activation states of the short-term memory and yields as\noutput mental actions, which control the flow of information among the working\nmemory components through neural gating mechanisms. The proposed system is\ncapable of learning to communicate through natural language starting from\ntabula rasa, without any a priori knowledge of the structure of phrases,\nmeaning of words, role of the different classes of words, only by interacting\nwith a human through a text-based interface, using an open-ended incremental\nlearning process. It is able to learn nouns, verbs, adjectives, pronouns and\nother word classes, and to use them in expressive language. The model was\nvalidated on a corpus of 1587 input sentences, based on literature on early\nlanguage assessment, at the level of about 4-years old child, and produced 521\noutput sentences, expressing a broad range of language processing\nfunctionalities.", "published": "2015-06-10 09:25:59", "link": "http://arxiv.org/abs/1506.03229v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Combining Temporal Information and Topic Modeling for Cross-Document\n  Event Ordering", "abstract": "Building unified timelines from a collection of written news articles\nrequires cross-document event coreference resolution and temporal relation\nextraction. In this paper we present an approach event coreference resolution\naccording to: a) similar temporal information, and b) similar semantic\narguments. Temporal information is detected using an automatic temporal\ninformation system (TIPSem), while semantic information is represented by means\nof LDA Topic Modeling. The evaluation of our approach shows that it obtains the\nhighest Micro-average F-score results in the SemEval2015 Task 4: TimeLine:\nCross-Document Event Ordering (25.36\\% for TrackB, 23.15\\% for SubtrackB), with\nan improvement of up to 6\\% in comparison to the other systems. However, our\nexperiment also showed some draw-backs in the Topic Modeling approach that\ndegrades performance of the system.", "published": "2015-06-10 11:28:31", "link": "http://arxiv.org/abs/1506.03257v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Paraphrase Database to Compositional Paraphrase Model and Back", "abstract": "The Paraphrase Database (PPDB; Ganitkevitch et al., 2013) is an extensive\nsemantic resource, consisting of a list of phrase pairs with (heuristic)\nconfidence estimates. However, it is still unclear how it can best be used, due\nto the heuristic nature of the confidences and its necessarily incomplete\ncoverage. We propose models to leverage the phrase pairs from the PPDB to build\nparametric paraphrase models that score paraphrase pairs more accurately than\nthe PPDB's internal scores while simultaneously improving its coverage. They\nallow for learning phrase embeddings as well as improved word embeddings.\nMoreover, we introduce two new, manually annotated datasets to evaluate\nshort-phrase paraphrasing models. Using our paraphrase model trained using\nPPDB, we achieve state-of-the-art results on standard word and bigram\nsimilarity tasks and beat strong baselines on our new short phrase paraphrase\ntasks.", "published": "2015-06-10 21:29:28", "link": "http://arxiv.org/abs/1506.03487v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unveiling the Dreams of Word Embeddings: Towards Language-Driven Image\n  Generation", "abstract": "We introduce language-driven image generation, the task of generating an\nimage visualizing the semantic contents of a word embedding, e.g., given the\nword embedding of grasshopper, we generate a natural image of a grasshopper. We\nimplement a simple method based on two mapping functions. The first takes as\ninput a word embedding (as produced, e.g., by the word2vec toolkit) and maps it\nonto a high-level visual space (e.g., the space defined by one of the top\nlayers of a Convolutional Neural Network). The second function maps this\nabstract visual representation to pixel space, in order to generate the target\nimage. Several user studies suggest that the current system produces images\nthat capture general visual properties of the concepts encoded in the word\nembedding, such as color or typical environment, and are sufficient to\ndiscriminate between general categories of objects.", "published": "2015-06-10 22:57:20", "link": "http://arxiv.org/abs/1506.03500v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Teaching Machines to Read and Comprehend", "abstract": "Teaching machines to read natural language documents remains an elusive\nchallenge. Machine reading systems can be tested on their ability to answer\nquestions posed on the contents of documents that they have seen, but until now\nlarge scale training and test datasets have been missing for this type of\nevaluation. In this work we define a new methodology that resolves this\nbottleneck and provides large scale supervised reading comprehension data. This\nallows us to develop a class of attention based deep neural networks that learn\nto read real documents and answer complex questions with minimal prior\nknowledge of language structure.", "published": "2015-06-10 14:54:39", "link": "http://arxiv.org/abs/1506.03340v3", "categories": ["cs.CL", "cs.AI", "cs.NE"], "primary_category": "cs.CL"}
