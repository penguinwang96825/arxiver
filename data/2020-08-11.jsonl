{"title": "A Parallel Evaluation Data Set of Software Documentation with Document\n  Structure Annotation", "abstract": "This paper accompanies the software documentation data set for machine\ntranslation, a parallel evaluation data set of data originating from the SAP\nHelp Portal, that we released to the machine translation community for research\npurposes. It offers the possibility to tune and evaluate machine translation\nsystems in the domain of corporate software documentation and contributes to\nthe availability of a wider range of evaluation scenarios. The data set\ncomprises of the language pairs English to Hindi, Indonesian, Malay and Thai,\nand thus also increases the test coverage for the many low-resource language\npairs. Unlike most evaluation data sets that consist of plain parallel text,\nthe segments in this data set come with additional metadata that describes\nstructural information of the document context. We provide insights into the\norigin and creation, the particularities and characteristics of the data set as\nwell as machine translation results.", "published": "2020-08-11 06:50:23", "link": "http://arxiv.org/abs/2008.04550v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Neural Generative Model for Joint Learning Topics and Topic-Specific\n  Word Embeddings", "abstract": "We propose a novel generative model to explore both local and global context\nfor joint learning topics and topic-specific word embeddings. In particular, we\nassume that global latent topics are shared across documents, a word is\ngenerated by a hidden semantic vector encoding its contextual semantic meaning,\nand its context words are generated conditional on both the hidden semantic\nvector and global latent topics. Topics are trained jointly with the word\nembeddings. The trained model maps words to topic-dependent embeddings, which\nnaturally addresses the issue of word polysemy. Experimental results show that\nthe proposed model outperforms the word-level embedding methods in both word\nsimilarity evaluation and word sense disambiguation. Furthermore, the model\nalso extracts more coherent topics compared with existing neural topic models\nor other models for joint learning of topics and word embeddings. Finally, the\nmodel can be easily integrated with existing deep contextualized word embedding\nlearning methods to further improve the performance of downstream tasks such as\nsentiment classification.", "published": "2020-08-11 13:54:11", "link": "http://arxiv.org/abs/2008.04702v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hybrid Ranking Network for Text-to-SQL", "abstract": "In this paper, we study how to leverage pre-trained language models in\nText-to-SQL. We argue that previous approaches under utilize the base language\nmodels by concatenating all columns together with the NL question and feeding\nthem into the base language model in the encoding stage. We propose a neat\napproach called Hybrid Ranking Network (HydraNet) which breaks down the problem\ninto column-wise ranking and decoding and finally assembles the column-wise\noutputs into a SQL query by straightforward rules. In this approach, the\nencoder is given a NL question and one individual column, which perfectly\naligns with the original tasks BERT/RoBERTa is trained on, and hence we avoid\nany ad-hoc pooling or additional encoding layers which are necessary in prior\napproaches. Experiments on the WikiSQL dataset show that the proposed approach\nis very effective, achieving the top place on the leaderboard.", "published": "2020-08-11 15:01:52", "link": "http://arxiv.org/abs/2008.04759v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting Low Resource Status of Indian Languages in Machine\n  Translation", "abstract": "Indian language machine translation performance is hampered due to the lack\nof large scale multi-lingual sentence aligned corpora and robust benchmarks.\nThrough this paper, we provide and analyse an automated framework to obtain\nsuch a corpus for Indian language neural machine translation (NMT) systems. Our\npipeline consists of a baseline NMT system, a retrieval module, and an\nalignment module that is used to work with publicly available websites such as\npress releases by the government. The main contribution towards this effort is\nto obtain an incremental method that uses the above pipeline to iteratively\nimprove the size of the corpus as well as improve each of the components of our\nsystem. Through our work, we also evaluate the design choices such as the\nchoice of pivoting language and the effect of iterative incremental increase in\ncorpus size. Our work in addition to providing an automated framework also\nresults in generating a relatively larger corpus as compared to existing\ncorpora that are available for Indian languages. This corpus helps us obtain\nsubstantially improved results on the publicly available WAT evaluation\nbenchmark and other standard evaluation benchmarks.", "published": "2020-08-11 17:05:13", "link": "http://arxiv.org/abs/2008.04860v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Sockeye 2 Neural Machine Translation Toolkit at AMTA 2020", "abstract": "We present Sockeye 2, a modernized and streamlined version of the Sockeye\nneural machine translation (NMT) toolkit. New features include a simplified\ncode base through the use of MXNet's Gluon API, a focus on state of the art\nmodel architectures, distributed mixed precision training, and efficient CPU\ndecoding with 8-bit quantization. These improvements result in faster training\nand inference, higher automatic metric scores, and a shorter path from research\nto production.", "published": "2020-08-11 17:42:26", "link": "http://arxiv.org/abs/2008.04885v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Paraphrase Generation as Zero-Shot Multilingual Translation:\n  Disentangling Semantic Similarity from Lexical and Syntactic Diversity", "abstract": "Recent work has shown that a multilingual neural machine translation (NMT)\nmodel can be used to judge how well a sentence paraphrases another sentence in\nthe same language (Thompson and Post, 2020); however, attempting to generate\nparaphrases from such a model using standard beam search produces trivial\ncopies or near copies. We introduce a simple paraphrase generation algorithm\nwhich discourages the production of n-grams that are present in the input. Our\napproach enables paraphrase generation in many languages from a single\nmultilingual NMT model. Furthermore, the amount of lexical diversity between\nthe input and output can be controlled at generation time. We conduct a human\nevaluation to compare our method to a paraphraser trained on the large English\nsynthetic paraphrase database ParaBank 2 (Hu et al., 2019c) and find that our\nmethod produces paraphrases that better preserve meaning and are more\ngramatical, for the same level of lexical diversity. Additional smaller human\nassessments demonstrate our approach also works in two non-English languages.", "published": "2020-08-11 18:05:34", "link": "http://arxiv.org/abs/2008.04935v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Context Reinforced Neural Topic Modeling over Short Texts", "abstract": "As one of the prevalent topic mining tools, neural topic modeling has\nattracted a lot of interests for the advantages of high efficiency in training\nand strong generalisation abilities. However, due to the lack of context in\neach short text, the existing neural topic models may suffer from feature\nsparsity on such documents. To alleviate this issue, we propose a Context\nReinforced Neural Topic Model (CRNTM), whose characteristics can be summarized\nas follows. Firstly, by assuming that each short text covers only a few salient\ntopics, CRNTM infers the topic for each word in a narrow range. Secondly, our\nmodel exploits pre-trained word embeddings by treating topics as multivariate\nGaussian distributions or Gaussian mixture distributions in the embedding\nspace. Extensive experiments on two benchmark datasets validate the\neffectiveness of the proposed model on both topic discovery and text\nclassification.", "published": "2020-08-11 06:41:53", "link": "http://arxiv.org/abs/2008.04545v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "A Comparison of Synthetic Oversampling Methods for Multi-class Text\n  Classification", "abstract": "The authors compared oversampling methods for the problem of multi-class\ntopic classification. The SMOTE algorithm underlies one of the most popular\noversampling methods. It consists in choosing two examples of a minority class\nand generating a new example based on them. In the paper, the authors compared\nthe basic SMOTE method with its two modifications (Borderline SMOTE and ADASYN)\nand random oversampling technique on the example of one of text classification\ntasks. The paper discusses the k-nearest neighbor algorithm, the support vector\nmachine algorithm and three types of neural networks (feedforward network, long\nshort-term memory (LSTM) and bidirectional LSTM). The authors combine these\nmachine learning algorithms with different text representations and compared\nsynthetic oversampling methods. In most cases, the use of oversampling\ntechniques can significantly improve the quality of classification. The authors\nconclude that for this task, the quality of the KNN and SVM algorithms is more\ninfluenced by class imbalance than neural networks.", "published": "2020-08-11 11:41:53", "link": "http://arxiv.org/abs/2008.04636v1", "categories": ["cs.CL", "cs.LG", "68T50", "I.7.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "Real-Time Sign Language Detection using Human Pose Estimation", "abstract": "We propose a lightweight real-time sign language detection model, as we\nidentify the need for such a case in videoconferencing. We extract optical flow\nfeatures based on human pose estimation and, using a linear classifier, show\nthese features are meaningful with an accuracy of 80%, evaluated on the DGS\nCorpus. Using a recurrent model directly on the input, we see improvements of\nup to 91% accuracy, while still working under 4ms. We describe a demo\napplication to sign language detection in the browser in order to demonstrate\nits usage possibility in videoconferencing applications.", "published": "2020-08-11 11:42:03", "link": "http://arxiv.org/abs/2008.04637v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Transformer with Bidirectional Decoder for Speech Recognition", "abstract": "Attention-based models have made tremendous progress on end-to-end automatic\nspeech recognition(ASR) recently. However, the conventional transformer-based\napproaches usually generate the sequence results token by token from left to\nright, leaving the right-to-left contexts unexploited. In this work, we\nintroduce a bidirectional speech transformer to utilize the different\ndirectional contexts simultaneously. Specifically, the outputs of our proposed\ntransformer include a left-to-right target, and a right-to-left target. In\ninference stage, we use the introduced bidirectional beam search method, which\ncan not only generate left-to-right candidates but also generate right-to-left\ncandidates, and determine the best hypothesis by the score.\n  To demonstrate our proposed speech transformer with a bidirectional\ndecoder(STBD), we conduct extensive experiments on the AISHELL-1 dataset. The\nresults of experiments show that STBD achieves a 3.6\\% relative CER\nreduction(CERR) over the unidirectional speech transformer baseline. Besides,\nthe strongest model in this paper called STBD-Big can achieve 6.64\\% CER on the\ntest set, without language model rescoring and any extra data augmentation\nstrategies.", "published": "2020-08-11 02:12:42", "link": "http://arxiv.org/abs/2008.04481v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Topic Adaptation and Prototype Encoding for Few-Shot Visual Storytelling", "abstract": "Visual Storytelling~(VIST) is a task to tell a narrative story about a\ncertain topic according to the given photo stream. The existing studies focus\non designing complex models, which rely on a huge amount of human-annotated\ndata. However, the annotation of VIST is extremely costly and many topics\ncannot be covered in the training dataset due to the long-tail topic\ndistribution. In this paper, we focus on enhancing the generalization ability\nof the VIST model by considering the few-shot setting. Inspired by the way\nhumans tell a story, we propose a topic adaptive storyteller to model the\nability of inter-topic generalization. In practice, we apply the gradient-based\nmeta-learning algorithm on multi-modal seq2seq models to endow the model the\nability to adapt quickly from topic to topic. Besides, We further propose a\nprototype encoding structure to model the ability of intra-topic derivation.\nSpecifically, we encode and restore the few training story text to serve as a\nreference to guide the generation at inference time. Experimental results show\nthat topic adaptation and prototype encoding structure mutually bring benefit\nto the few-shot model on BLEU and METEOR metric. The further case study shows\nthat the stories generated after few-shot adaptation are more relative and\nexpressive.", "published": "2020-08-11 03:55:11", "link": "http://arxiv.org/abs/2008.04504v1", "categories": ["cs.CL", "cs.CV", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On Learning Language-Invariant Representations for Universal Machine\n  Translation", "abstract": "The goal of universal machine translation is to learn to translate between\nany pair of languages, given a corpus of paired translated documents for\n\\emph{a small subset} of all pairs of languages. Despite impressive empirical\nresults and an increasing interest in massively multilingual models,\ntheoretical analysis on translation errors made by such universal machine\ntranslation models is only nascent. In this paper, we formally prove certain\nimpossibilities of this endeavour in general, as well as prove positive results\nin the presence of additional (but natural) structure of data.\n  For the former, we derive a lower bound on the translation error in the\nmany-to-many translation setting, which shows that any algorithm aiming to\nlearn shared sentence representations among multiple language pairs has to make\na large translation error on at least one of the translation tasks, if no\nassumption on the structure of the languages is made. For the latter, we show\nthat if the paired documents in the corpus follow a natural\n\\emph{encoder-decoder} generative process, we can expect a natural notion of\n``generalization'': a linear number of language pairs, rather than quadratic,\nsuffices to learn a good representation. Our theory also explains what kinds of\nconnection graphs between pairs of languages are better suited: ones with\nlonger paths result in worse sample complexity in terms of the total number of\ndocuments per language pair needed. We believe our theoretical insights and\nimplications contribute to the future algorithmic design of universal machine\ntranslation.", "published": "2020-08-11 04:45:33", "link": "http://arxiv.org/abs/2008.04510v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Neural PLDA Modeling for End-to-End Speaker Verification", "abstract": "While deep learning models have made significant advances in supervised\nclassification problems, the application of these models for out-of-set\nverification tasks like speaker recognition has been limited to deriving\nfeature embeddings. The state-of-the-art x-vector PLDA based speaker\nverification systems use a generative model based on probabilistic linear\ndiscriminant analysis (PLDA) for computing the verification score. Recently, we\nhad proposed a neural network approach for backend modeling in speaker\nverification called the neural PLDA (NPLDA) where the likelihood ratio score of\nthe generative PLDA model is posed as a discriminative similarity function and\nthe learnable parameters of the score function are optimized using a\nverification cost. In this paper, we extend this work to achieve joint\noptimization of the embedding neural network (x-vector network) with the NPLDA\nnetwork in an end-to-end (E2E) fashion. This proposed end-to-end model is\noptimized directly from the acoustic features with a verification cost function\nand during testing, the model directly outputs the likelihood ratio score. With\nvarious experiments using the NIST speaker recognition evaluation (SRE) 2018\nand 2019 datasets, we show that the proposed E2E model improves significantly\nover the x-vector PLDA baseline speaker verification system.", "published": "2020-08-11 05:54:54", "link": "http://arxiv.org/abs/2008.04527v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Investigation of End-To-End Speaker-Attributed ASR for Continuous\n  Multi-Talker Recordings", "abstract": "Recently, an end-to-end (E2E) speaker-attributed automatic speech recognition\n(SA-ASR) model was proposed as a joint model of speaker counting, speech\nrecognition and speaker identification for monaural overlapped speech. It\nshowed promising results for simulated speech mixtures consisting of various\nnumbers of speakers. However, the model required prior knowledge of speaker\nprofiles to perform speaker identification, which significantly limited the\napplication of the model. In this paper, we extend the prior work by addressing\nthe case where no speaker profile is available. Specifically, we perform\nspeaker counting and clustering by using the internal speaker representations\nof the E2E SA-ASR model to diarize the utterances of the speakers whose\nprofiles are missing from the speaker inventory. We also propose a simple\nmodification to the reference labels of the E2E SA-ASR training which helps\nhandle continuous multi-talker recordings well. We conduct a comprehensive\ninvestigation of the original E2E SA-ASR and the proposed method on the\nmonaural LibriCSS dataset. Compared to the original E2E SA-ASR with relevant\nspeaker profiles, the proposed method achieves a close performance without any\nprior speaker knowledge. We also show that the source-target attention in the\nE2E SA-ASR model provides information about the start and end times of the\nhypotheses.", "published": "2020-08-11 06:41:55", "link": "http://arxiv.org/abs/2008.04546v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DensE: An Enhanced Non-commutative Representation for Knowledge Graph\n  Embedding with Adaptive Semantic Hierarchy", "abstract": "Capturing the composition patterns of relations is a vital task in knowledge\ngraph completion. It also serves as a fundamental step towards multi-hop\nreasoning over learned knowledge. Previously, several rotation-based\ntranslational methods have been developed to model composite relations using\nthe product of a series of complex-valued diagonal matrices. However, these\nmethods tend to make several oversimplified assumptions on the composite\nrelations, e.g., forcing them to be commutative, independent from entities and\nlacking semantic hierarchy. To systematically tackle these problems, we have\ndeveloped a novel knowledge graph embedding method, named DensE, to provide an\nimproved modeling scheme for the complex composition patterns of relations. In\nparticular, our method decomposes each relation into an SO(3) group-based\nrotation operator and a scaling operator in the three dimensional (3-D)\nEuclidean space. This design principle leads to several advantages of our\nmethod: (1) For composite relations, the corresponding diagonal relation\nmatrices can be non-commutative, reflecting a predominant scenario in real\nworld applications; (2) Our model preserves the natural interaction between\nrelational operations and entity embeddings; (3) The scaling operation provides\nthe modeling power for the intrinsic semantic hierarchical structure of\nentities; (4) The enhanced expressiveness of DensE is achieved with high\ncomputational efficiency in terms of both parameter size and training time; and\n(5) Modeling entities in Euclidean space instead of quaternion space keeps the\ndirect geometrical interpretations of relational patterns. Experimental results\non multiple benchmark knowledge graphs show that DensE outperforms the current\nstate-of-the-art models for missing link prediction, especially on composite\nrelations.", "published": "2020-08-11 06:45:50", "link": "http://arxiv.org/abs/2008.04548v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Spectrum and Prosody Conversion for Cross-lingual Voice Conversion with\n  CycleGAN", "abstract": "Cross-lingual voice conversion aims to change source speaker's voice to sound\nlike that of target speaker, when source and target speakers speak different\nlanguages. It relies on non-parallel training data from two different\nlanguages, hence, is more challenging than mono-lingual voice conversion.\nPrevious studies on cross-lingual voice conversion mainly focus on spectral\nconversion with a linear transformation for F0 transfer. However, as an\nimportant prosodic factor, F0 is inherently hierarchical, thus it is\ninsufficient to just use a linear method for conversion. We propose the use of\ncontinuous wavelet transform (CWT) decomposition for F0 modeling. CWT provides\na way to decompose a signal into different temporal scales that explain prosody\nin different time resolutions. We also propose to train two CycleGAN pipelines\nfor spectrum and prosody mapping respectively. In this way, we eliminate the\nneed for parallel data of any two languages and any alignment techniques.\nExperimental results show that our proposed Spectrum-Prosody-CycleGAN framework\noutperforms the Spectrum-CycleGAN baseline in subjective evaluation. To our\nbest knowledge, this is the first study of prosody in cross-lingual voice\nconversion.", "published": "2020-08-11 07:29:55", "link": "http://arxiv.org/abs/2008.04562v3", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "LTIatCMU at SemEval-2020 Task 11: Incorporating Multi-Level Features for\n  Multi-Granular Propaganda Span Identification", "abstract": "In this paper we describe our submission for the task of Propaganda Span\nIdentification in news articles. We introduce a BERT-BiLSTM based span-level\npropaganda classification model that identifies which token spans within the\nsentence are indicative of propaganda. The \"multi-granular\" model incorporates\nlinguistic knowledge at various levels of text granularity, including word,\nsentence and document level syntactic, semantic and pragmatic affect features,\nwhich significantly improve model performance, compared to its\nlanguage-agnostic variant. To facilitate better representation learning, we\nalso collect a corpus of 10k news articles, and use it for fine-tuning the\nmodel. The final model is a majority-voting ensemble which learns different\npropaganda class boundaries by leveraging different subsets of incorporated\nknowledge and attains $4^{th}$ position on the test leaderboard. Our final\nmodel and code is released at https://github.com/sopu/PropagandaSemEval2020.", "published": "2020-08-11 16:14:47", "link": "http://arxiv.org/abs/2008.04820v2", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Compact Speaker Embedding: lrx-vector", "abstract": "Deep neural networks (DNN) have recently been widely used in speaker\nrecognition systems, achieving state-of-the-art performance on various\nbenchmarks. The x-vector architecture is especially popular in this research\ncommunity, due to its excellent performance and manageable computational\ncomplexity. In this paper, we present the lrx-vector system, which is the\nlow-rank factorized version of the x-vector embedding network. The primary\nobjective of this topology is to further reduce the memory requirement of the\nspeaker recognition system. We discuss the deployment of knowledge distillation\nfor training the lrx-vector system and compare against low-rank factorization\nwith SVD. On the VOiCES 2019 far-field corpus we were able to reduce the\nweights by 28% compared to the full-rank x-vector system while keeping the\nrecognition rate constant (1.83% EER).", "published": "2020-08-11 21:32:16", "link": "http://arxiv.org/abs/2008.05011v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Modeling Prosodic Phrasing with Multi-Task Learning in Tacotron-based\n  TTS", "abstract": "Tacotron-based end-to-end speech synthesis has shown remarkable voice\nquality. However, the rendering of prosody in the synthesized speech remains to\nbe improved, especially for long sentences, where prosodic phrasing errors can\noccur frequently. In this paper, we extend the Tacotron-based speech synthesis\nframework to explicitly model the prosodic phrase breaks. We propose a\nmulti-task learning scheme for Tacotron training, that optimizes the system to\npredict both Mel spectrum and phrase breaks. To our best knowledge, this is the\nfirst implementation of multi-task learning for Tacotron based TTS with a\nprosodic phrasing model. Experiments show that our proposed training scheme\nconsistently improves the voice quality for both Chinese and Mongolian systems.", "published": "2020-08-11 07:57:29", "link": "http://arxiv.org/abs/2008.05284v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Exploring Aligned Lyrics-Informed Singing Voice Separation", "abstract": "In this paper, we propose a method of utilizing aligned lyrics as additional\ninformation to improve the performance of singing voice separation. We have\ncombined the highway network-based lyrics encoder into Open-unmix separation\nnetwork and show that the model trained with the aligned lyrics indeed results\nin a better performance than the model that was not informed. The question now\nremains whether the increase of performance is actually due to the phonetic\ncontents that lie in the informed aligned lyrics or not. To this end, we\ninvestigated the source of performance increase in multifaceted ways by\nobserving the change of performance when incorrect lyrics were given to the\nmodel. Experiment results show that the model can use not only just vocal\nactivity information but also the phonetic contents from the aligned lyrics.", "published": "2020-08-11 02:16:12", "link": "http://arxiv.org/abs/2008.04482v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Acoustic effects of medical, cloth, and transparent face masks on speech\n  signals", "abstract": "Face masks muffle speech and make communication more difficult, especially\nfor people with hearing loss. This study examines the acoustic attenuation\ncaused by different face masks, including medical, cloth, and transparent\nmasks, using a head-shaped loudspeaker and a live human talker. The results\nsuggest that all masks attenuate frequencies above 1 kHz, that attenuation is\ngreatest in front of the talker, and that there is substantial variation\nbetween mask types, especially cloth masks with different materials and weaves.\nTransparent masks have poor acoustic performance compared to both medical and\ncloth masks. Most masks have little effect on lapel microphones, suggesting\nthat existing sound reinforcement and assistive listening systems may be\neffective for verbal communication with masks.", "published": "2020-08-11 05:33:42", "link": "http://arxiv.org/abs/2008.04521v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Unsupervised Learning For Sequence-to-sequence Text-to-speech For\n  Low-resource Languages", "abstract": "Recently, sequence-to-sequence models with attention have been successfully\napplied in Text-to-speech (TTS). These models can generate near-human speech\nwith a large accurately-transcribed speech corpus. However, preparing such a\nlarge data-set is both expensive and laborious. To alleviate the problem of\nheavy data demand, we propose a novel unsupervised pre-training mechanism in\nthis paper. Specifically, we first use Vector-quantization\nVariational-Autoencoder (VQ-VAE) to ex-tract the unsupervised linguistic units\nfrom large-scale, publicly found, and untranscribed speech. We then pre-train\nthe sequence-to-sequence TTS model by using the<unsupervised linguistic units,\naudio>pairs. Finally, we fine-tune the model with a small amount of<text,\naudio>paired data from the target speaker. As a result, both objective and\nsubjective evaluations show that our proposed method can synthesize more\nintelligible and natural speech with the same amount of paired training data.\nBesides, we extend our proposed method to the hypothesized low-resource\nlanguages and verify the effectiveness of the method using objective\nevaluation.", "published": "2020-08-11 06:48:36", "link": "http://arxiv.org/abs/2008.04549v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "EEG-based Auditory Attention Decoding: Towards Neuro-Steered Hearing\n  Devices", "abstract": "People suffering from hearing impairment often have difficulties\nparticipating in conversations in so-called `cocktail party' scenarios with\nmultiple people talking simultaneously. Although advanced algorithms exist to\nsuppress background noise in these situations, a hearing device also needs\ninformation on which of these speakers the user actually aims to attend to. The\ncorrect (attended) speaker can then be enhanced using this information, and all\nother speakers can be treated as background noise. Recent neuroscientific\nadvances have shown that it is possible to determine the focus of auditory\nattention from non-invasive neurorecording techniques, such as\nelectroencephalography (EEG). Based on these new insights, a multitude of\nauditory attention decoding (AAD) algorithms have been proposed, which could,\ncombined with the appropriate speaker separation algorithms and miniaturized\nEEG sensor devices, lead to so-called neuro-steered hearing devices. In this\npaper, we provide a broad review and a statistically grounded comparative study\nof EEG-based AAD algorithms and address the main signal processing challenges\nin this field.", "published": "2020-08-11 08:02:54", "link": "http://arxiv.org/abs/2008.04569v3", "categories": ["eess.SP", "eess.AS"], "primary_category": "eess.SP"}
{"title": "Alzheimer's Dementia Detection from Audio and Text Modalities", "abstract": "Automatic detection of Alzheimer's dementia by speech processing is enhanced\nwhen features of both the acoustic waveform and the content are extracted.\nAudio and text transcription have been widely used in health-related tasks, as\nspectral and prosodic speech features, as well as semantic and linguistic\ncontent, convey information about various diseases. Hence, this paper describes\nthe joint work of the GTM-UVIGO research group and acceXible startup to the\nADDReSS challenge at INTERSPEECH 2020. The submitted systems aim to detect\npatterns of Alzheimer's disease from both the patient's voice and message\ntranscription. Six different systems have been built and compared: four of them\nare speech-based and the other two systems are text-based. The x-vector,\ni-vector, and statistical speech-based functionals features are evaluated. As a\nlower speaking fluency is a common pattern in patients with Alzheimer's\ndisease, rhythmic features are also proposed. For transcription analysis, two\nsystems are proposed: one uses GloVe word embedding features and the other uses\nseveral features extracted by language modelling. Several intra-modality and\ninter-modality score fusion strategies are investigated. The performance of\nsingle modality and multimodal systems are presented. The achieved results are\npromising, outperforming the results achieved by the ADDReSS's baseline\nsystems.", "published": "2020-08-11 10:34:22", "link": "http://arxiv.org/abs/2008.04617v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Transfer Learning for Improving Singing-voice Detection in Polyphonic\n  Instrumental Music", "abstract": "Detecting singing-voice in polyphonic instrumental music is critical to music\ninformation retrieval. To train a robust vocal detector, a large dataset marked\nwith vocal or non-vocal label at frame-level is essential. However, frame-level\nlabeling is time-consuming and labor expensive, resulting there is little\nwell-labeled dataset available for singing-voice detection (S-VD). Hence, we\npropose a data augmentation method for S-VD by transfer learning. In this\nstudy, clean speech clips with voice activity endpoints and separate\ninstrumental music clips are artificially added together to simulate polyphonic\nvocals to train a vocal/non-vocal detector. Due to the different articulation\nand phonation between speaking and singing, the vocal detector trained with the\nartificial dataset does not match well with the polyphonic music which is\nsinging vocals together with the instrumental accompaniments. To reduce this\nmismatch, transfer learning is used to transfer the knowledge learned from the\nartificial speech-plus-music training set to a small but matched polyphonic\ndataset, i.e., singing vocals with accompaniments. By transferring the related\nknowledge to make up for the lack of well-labeled training data in S-VD, the\nproposed data augmentation method by transfer learning can improve S-VD\nperformance with an F-score improvement from 89.5% to 93.2%.", "published": "2020-08-11 12:22:17", "link": "http://arxiv.org/abs/2008.04658v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "S-vectors and TESA: Speaker Embeddings and a Speaker Authenticator Based\n  on Transformer Encoder", "abstract": "One of the most popular speaker embeddings is x-vectors, which are obtained\nfrom an architecture that gradually builds a larger temporal context with\nlayers. In this paper, we propose to derive speaker embeddings from\nTransformer's encoder trained for speaker classification. Self-attention, on\nwhich Transformer's encoder is built, attends to all the features over the\nentire utterance and might be more suitable in capturing the speaker\ncharacteristics in an utterance. We refer to the speaker embeddings obtained\nfrom the proposed speaker classification model as s-vectors to emphasize that\nthey are obtained from an architecture that heavily relies on self-attention.\nThrough experiments, we demonstrate that s-vectors perform better than\nx-vectors. In addition to the s-vectors, we also propose a new architecture\nbased on Transformer's encoder for speaker verification as a replacement for\nspeaker verification based on conventional probabilistic linear discriminant\nanalysis (PLDA). This architecture is inspired by the next sentence prediction\ntask of bidirectional encoder representations from Transformers (BERT), and we\nfeed the s-vectors of two utterances to verify whether they belong to the same\nspeaker. We name this architecture the Transformer encoder speaker\nauthenticator (TESA). Our experiments show that the performance of s-vectors\nwith TESA is better than s-vectors with conventional PLDA-based speaker\nverification.", "published": "2020-08-11 12:23:21", "link": "http://arxiv.org/abs/2008.04659v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Bunched LPCNet : Vocoder for Low-cost Neural Text-To-Speech Systems", "abstract": "LPCNet is an efficient vocoder that combines linear prediction and deep\nneural network modules to keep the computational complexity low. In this work,\nwe present two techniques to further reduce it's complexity, aiming for a\nlow-cost LPCNet vocoder-based neural Text-to-Speech (TTS) System. These\ntechniques are: 1) Sample-bunching, which allows LPCNet to generate more than\none audio sample per inference; and 2) Bit-bunching, which reduces the\ncomputations in the final layer of LPCNet. With the proposed bunching\ntechniques, LPCNet, in conjunction with a Deep Convolutional TTS (DCTTS)\nacoustic model, shows a 2.19x improvement over the baseline run-time when\nrunning on a mobile device, with a less than 0.1 decrease in TTS mean opinion\nscore (MOS).", "published": "2020-08-11 08:15:45", "link": "http://arxiv.org/abs/2008.04574v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Why Did the x-Vector System Miss a Target Speaker? Impact of Acoustic\n  Mismatch Upon Target Score on VoxCeleb Data", "abstract": "Modern automatic speaker verification (ASV) relies heavily on machine\nlearning implemented through deep neural networks. It can be difficult to\ninterpret the output of these black boxes. In line with interpretative machine\nlearning, we model the dependency of ASV detection score upon acoustic mismatch\nof the enrollment and test utterances. We aim to identify mismatch factors that\nexplain target speaker misses (false rejections). We use distance in the first-\nand second-order statistics of selected acoustic features as the predictors in\na linear mixed effects model, while a standard Kaldi x-vector system forms our\nASV black-box. Our results on the VoxCeleb data reveal the most prominent\nmismatch factor to be in F0 mean, followed by mismatches associated with\nformant frequencies. Our findings indicate that x-vector systems lack\nrobustness to intra-speaker variations.", "published": "2020-08-11 08:33:20", "link": "http://arxiv.org/abs/2008.04578v1", "categories": ["eess.AS", "cs.CY", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Surgical Mask Detection with Convolutional Neural Networks and Data\n  Augmentations on Spectrograms", "abstract": "In many fields of research, labeled datasets are hard to acquire. This is\nwhere data augmentation promises to overcome the lack of training data in the\ncontext of neural network engineering and classification tasks. The idea here\nis to reduce model over-fitting to the feature distribution of a small\nunder-descriptive training dataset. We try to evaluate such data augmentation\ntechniques to gather insights in the performance boost they provide for several\nconvolutional neural networks on mel-spectrogram representations of audio data.\nWe show the impact of data augmentation on the binary classification task of\nsurgical mask detection in samples of human voice (ComParE Challenge 2020).\nAlso we consider four varying architectures to account for augmentation\nrobustness. Results show that most of the baselines given by ComParE are\noutperformed.", "published": "2020-08-11 09:02:47", "link": "http://arxiv.org/abs/2008.04590v1", "categories": ["eess.AS", "cs.CV", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "PlugSonic: a web- and mobile-based platform for binaural audio and sonic\n  narratives", "abstract": "PlugSonic is a suite of web- and mobile-based applications for the curation\nand experience of binaural interactive soundscapes and sonic narratives. It was\ndeveloped as part of the PLUGGY EU project (Pluggable Social Platform for\nHeritage Awareness and Participation) and consists of two main applications:\nPlugSonic Sample, to edit and apply audio effects, and PlugSonic Soundscape, to\ncreate and experience binaural soundscapes. The audio processing within\nPlugSonic is based on the Web Audio API and the 3D Tune-In Toolkit, while the\nexploration of soundscapes in a physical space is obtained using Apple's ARKit.\nIn this paper we present the design choices, the user involvement processes and\nthe implementation details. The main goal of PlugSonic is technology\ndemocratisation; PlugSonic users - whether institutions or citizens - are all\ngiven the instruments needed to create, process and experience 3D soundscapes\nand sonic narrative; without the need for specific devices, external tools\n(software and/or hardware), specialised knowledge or custom development. The\nevaluation, which was conducted with inexperienced users on three tasks -\ncreation, curation and experience - demonstrates how PlugSonic is indeed a\nsimple, effective, yet powerful tool.", "published": "2020-08-11 11:42:49", "link": "http://arxiv.org/abs/2008.04638v1", "categories": ["cs.SD", "cs.HC", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Content-based Music Similarity with Triplet Networks", "abstract": "We explore the feasibility of using triplet neural networks to embed songs\nbased on content-based music similarity. Our network is trained using triplets\nof songs such that two songs by the same artist are embedded closer to one\nanother than to a third song by a different artist. We compare two models that\nare trained using different ways of picking this third song: at random vs.\nbased on shared genre labels. Our experiments are conducted using songs from\nthe Free Music Archive and use standard audio features. The initial results\nshow that shallow Siamese networks can be used to embed music for a simple\nartist retrieval task.", "published": "2020-08-11 18:10:02", "link": "http://arxiv.org/abs/2008.04938v2", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "PoCoNet: Better Speech Enhancement with Frequency-Positional Embeddings,\n  Semi-Supervised Conversational Data, and Biased Loss", "abstract": "Neural network applications generally benefit from larger-sized models, but\nfor current speech enhancement models, larger scale networks often suffer from\ndecreased robustness to the variety of real-world use cases beyond what is\nencountered in training data. We introduce several innovations that lead to\nbetter large neural networks for speech enhancement. The novel PoCoNet\narchitecture is a convolutional neural network that, with the use of\nfrequency-positional embeddings, is able to more efficiently build\nfrequency-dependent features in the early layers. A semi-supervised method\nhelps increase the amount of conversational training data by pre-enhancing\nnoisy datasets, improving performance on real recordings. A new loss function\nbiased towards preserving speech quality helps the optimization better match\nhuman perceptual opinions on speech quality. Ablation experiments and objective\nand human opinion metrics show the benefits of the proposed improvements.", "published": "2020-08-11 01:24:45", "link": "http://arxiv.org/abs/2008.04470v1", "categories": ["eess.AS", "cs.LG", "cs.NE", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
