{"title": "Robust Dialogue Utterance Rewriting as Sequence Tagging", "abstract": "The task of dialogue rewriting aims to reconstruct the latest dialogue\nutterance by copying the missing content from the dialogue context. Until now,\nthe existing models for this task suffer from the robustness issue, i.e.,\nperformances drop dramatically when testing on a different domain. We address\nthis robustness issue by proposing a novel sequence-tagging-based model so that\nthe search space is significantly reduced, yet the core of this task is still\nwell covered. As a common issue of most tagging models for text generation, the\nmodel's outputs may lack fluency. To alleviate this issue, we inject the loss\nsignal from BLEU or GPT-2 under a REINFORCE framework. Experiments show huge\nimprovements of our model over the current state-of-the-art systems on domain\ntransfer.", "published": "2020-12-29 00:05:35", "link": "http://arxiv.org/abs/2012.14535v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding and Improving Lexical Choice in Non-Autoregressive\n  Translation", "abstract": "Knowledge distillation (KD) is essential for training non-autoregressive\ntranslation (NAT) models by reducing the complexity of the raw data with an\nautoregressive teacher model. In this study, we empirically show that as a side\neffect of this training, the lexical choice errors on low-frequency words are\npropagated to the NAT model from the teacher model. To alleviate this problem,\nwe propose to expose the raw data to NAT models to restore the useful\ninformation of low-frequency words, which are missed in the distilled data. To\nthis end, we introduce an extra Kullback-Leibler divergence term derived by\ncomparing the lexical choice of NAT model and that embedded in the raw data.\nExperimental results across language pairs and model architectures demonstrate\nthe effectiveness and universality of the proposed approach. Extensive analyses\nconfirm our claim that our approach improves performance by reducing the\nlexical choice errors on low-frequency words. Encouragingly, our approach\npushes the SOTA NAT performance on the WMT14 English-German and WMT16\nRomanian-English datasets up to 27.8 and 33.8 BLEU points, respectively. The\nsource code will be released.", "published": "2020-12-29 03:18:50", "link": "http://arxiv.org/abs/2012.14583v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is human scoring the best criteria for summary evaluation?", "abstract": "Normally, summary quality measures are compared with quality scores produced\nby human annotators. A higher correlation with human scores is considered to be\na fair indicator of a better measure. We discuss observations that cast doubt\non this view. We attempt to show a possibility of an alternative indicator.\nGiven a family of measures, we explore a criterion of selecting the best\nmeasure not relying on correlations with human scores. Our observations for the\nBLANC family of measures suggest that the criterion is universal across very\ndifferent styles of summaries.", "published": "2020-12-29 04:48:52", "link": "http://arxiv.org/abs/2012.14602v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UniK-QA: Unified Representations of Structured and Unstructured\n  Knowledge for Open-Domain Question Answering", "abstract": "We study open-domain question answering with structured, unstructured and\nsemi-structured knowledge sources, including text, tables, lists and knowledge\nbases. Departing from prior work, we propose a unifying approach that\nhomogenizes all sources by reducing them to text and applies the\nretriever-reader model which has so far been limited to text sources only. Our\napproach greatly improves the results on knowledge-base QA tasks by 11 points,\ncompared to latest graph-based methods. More importantly, we demonstrate that\nour unified knowledge (UniK-QA) model is a simple and yet effective way to\ncombine heterogeneous sources of knowledge, advancing the state-of-the-art\nresults on two popular question answering benchmarks, NaturalQuestions and\nWebQuestions, by 3.5 and 2.6 points, respectively.\n  The code of UniK-QA is available at:\nhttps://github.com/facebookresearch/UniK-QA.", "published": "2020-12-29 05:14:08", "link": "http://arxiv.org/abs/2012.14610v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multiple Structural Priors Guided Self Attention Network for Language\n  Understanding", "abstract": "Self attention networks (SANs) have been widely utilized in recent NLP\nstudies. Unlike CNNs or RNNs, standard SANs are usually position-independent,\nand thus are incapable of capturing the structural priors between sequences of\nwords. Existing studies commonly apply one single mask strategy on SANs for\nincorporating structural priors while failing at modeling more abundant\nstructural information of texts. In this paper, we aim at introducing multiple\ntypes of structural priors into SAN models, proposing the Multiple Structural\nPriors Guided Self Attention Network (MS-SAN) that transforms different\nstructural priors into different attention heads by using a novel multi-mask\nbased multi-head attention mechanism. In particular, we integrate two\ncategories of structural priors, including the sequential order and the\nrelative position of words. For the purpose of capturing the latent\nhierarchical structure of the texts, we extract these information not only from\nthe word contexts but also from the dependency syntax trees. Experimental\nresults on two tasks show that MS-SAN achieves significant improvements against\nother strong baselines.", "published": "2020-12-29 07:30:03", "link": "http://arxiv.org/abs/2012.14642v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Interpretable NLG for Task-oriented Dialogue Systems with Heterogeneous\n  Rendering Machines", "abstract": "End-to-end neural networks have achieved promising performances in natural\nlanguage generation (NLG). However, they are treated as black boxes and lack\ninterpretability. To address this problem, we propose a novel framework,\nheterogeneous rendering machines (HRM), that interprets how neural generators\nrender an input dialogue act (DA) into an utterance. HRM consists of a renderer\nset and a mode switcher. The renderer set contains multiple decoders that vary\nin both structure and functionality. For every generation step, the mode\nswitcher selects an appropriate decoder from the renderer set to generate an\nitem (a word or a phrase). To verify the effectiveness of our method, we have\nconducted extensive experiments on 5 benchmark datasets. In terms of automatic\nmetrics (e.g., BLEU), our model is competitive with the current\nstate-of-the-art method. The qualitative analysis shows that our model can\ninterpret the rendering process of neural generators well. Human evaluation\nalso confirms the interpretability of our proposed approach.", "published": "2020-12-29 07:41:48", "link": "http://arxiv.org/abs/2012.14645v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Theoretical Analysis of the Repetition Problem in Text Generation", "abstract": "Text generation tasks, including translation, summarization, language models,\nand etc. see rapid growth during recent years. Despite the remarkable\nachievements, the repetition problem has been observed in nearly all text\ngeneration models undermining the generation performance extensively. To solve\nthe repetition problem, many methods have been proposed, but there is no\nexisting theoretical analysis to show why this problem happens and how it is\nresolved. In this paper, we propose a new framework for theoretical analysis\nfor the repetition problem. We first define the Average Repetition Probability\n(ARP) to characterize the repetition problem quantitatively. Then, we conduct\nan extensive analysis of the Markov generation model and derive several upper\nbounds of the average repetition probability with intuitive understanding. We\nshow that most of the existing methods are essentially minimizing the upper\nbounds explicitly or implicitly. Grounded on our theory, we show that the\nrepetition problem is, unfortunately, caused by the traits of our language\nitself. One major reason is attributed to the fact that there exist too many\nwords predicting the same word as the subsequent word with high probability.\nConsequently, it is easy to go back to that word and form repetitions and we\ndub it as the high inflow problem. Furthermore, we derive a concentration bound\nof the average repetition probability for a general generation model. Finally,\nbased on the theoretical upper bounds, we propose a novel rebalanced encoding\napproach to alleviate the high inflow problem. The experimental results show\nthat our theoretical framework is applicable in general generation models and\nour proposed rebalanced encoding approach alleviates the repetition problem\nsignificantly. The source code of this paper can be obtained from\nhttps://github.com/fuzihaofzh/repetition-problem-nlg.", "published": "2020-12-29 08:51:47", "link": "http://arxiv.org/abs/2012.14660v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CascadeBERT: Accelerating Inference of Pre-trained Language Models via\n  Calibrated Complete Models Cascade", "abstract": "Dynamic early exiting aims to accelerate the inference of pre-trained\nlanguage models (PLMs) by emitting predictions in internal layers without\npassing through the entire model. In this paper, we empirically analyze the\nworking mechanism of dynamic early exiting and find that it faces a performance\nbottleneck under high speed-up ratios. On one hand, the PLMs' representations\nin shallow layers lack high-level semantic information and thus are not\nsufficient for accurate predictions. On the other hand, the exiting decisions\nmade by internal classifiers are unreliable, leading to wrongly emitted early\npredictions. We instead propose a new framework for accelerating the inference\nof PLMs, CascadeBERT, which dynamically selects proper-sized and complete\nmodels in a cascading manner, providing comprehensive representations for\npredictions. We further devise a difficulty-aware objective, encouraging the\nmodel to output the class probability that reflects the real difficulty of each\ninstance for a more reliable cascading mechanism. Experimental results show\nthat CascadeBERT can achieve an overall 15\\% improvement under 4$\\times$\nspeed-up compared with existing dynamic early exiting methods on six\nclassification tasks, yielding more calibrated and accurate predictions.", "published": "2020-12-29 09:43:50", "link": "http://arxiv.org/abs/2012.14682v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Code Summarization with Structure-induced Transformer", "abstract": "Code summarization (CS) is becoming a promising area in recent language\nunderstanding, which aims to generate sensible human language automatically for\nprogramming language in the format of source code, serving in the most\nconvenience of programmer developing. It is well known that programming\nlanguages are highly structured. Thus previous works attempt to apply\nstructure-based traversal (SBT) or non-sequential models like Tree-LSTM and\ngraph neural network (GNN) to learn structural program semantics. However, it\nis surprising that incorporating SBT into advanced encoder like Transformer\ninstead of LSTM has been shown no performance gain, which lets GNN become the\nonly rest means modeling such necessary structural clue in source code. To\nrelease such inconvenience, we propose structure-induced Transformer, which\nencodes sequential code inputs with multi-view structural clues in terms of a\nnewly-proposed structure-induced self-attention mechanism. Extensive\nexperiments show that our proposed structure-induced Transformer helps achieve\nnew state-of-the-art results on benchmarks.", "published": "2020-12-29 11:37:43", "link": "http://arxiv.org/abs/2012.14710v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document\n  Understanding", "abstract": "Pre-training of text and layout has proved effective in a variety of\nvisually-rich document understanding tasks due to its effective model\narchitecture and the advantage of large-scale unlabeled scanned/digital-born\ndocuments. We propose LayoutLMv2 architecture with new pre-training tasks to\nmodel the interaction among text, layout, and image in a single multi-modal\nframework. Specifically, with a two-stream multi-modal Transformer encoder,\nLayoutLMv2 uses not only the existing masked visual-language modeling task but\nalso the new text-image alignment and text-image matching tasks, which make it\nbetter capture the cross-modality interaction in the pre-training stage.\nMeanwhile, it also integrates a spatial-aware self-attention mechanism into the\nTransformer architecture so that the model can fully understand the relative\npositional relationship among different text blocks. Experiment results show\nthat LayoutLMv2 outperforms LayoutLM by a large margin and achieves new\nstate-of-the-art results on a wide variety of downstream visually-rich document\nunderstanding tasks, including FUNSD (0.7895 $\\to$ 0.8420), CORD (0.9493 $\\to$\n0.9601), SROIE (0.9524 $\\to$ 0.9781), Kleister-NDA (0.8340 $\\to$ 0.8520),\nRVL-CDIP (0.9443 $\\to$ 0.9564), and DocVQA (0.7295 $\\to$ 0.8672). We made our\nmodel and code publicly available at \\url{https://aka.ms/layoutlmv2}.", "published": "2020-12-29 13:01:52", "link": "http://arxiv.org/abs/2012.14740v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dialogue Response Selection with Hierarchical Curriculum Learning", "abstract": "We study the learning of a matching model for dialogue response selection.\nMotivated by the recent finding that models trained with random negative\nsamples are not ideal in real-world scenarios, we propose a hierarchical\ncurriculum learning framework that trains the matching model in an\n\"easy-to-difficult\" scheme. Our learning framework consists of two\ncomplementary curricula: (1) corpus-level curriculum (CC); and (2)\ninstance-level curriculum (IC). In CC, the model gradually increases its\nability in finding the matching clues between the dialogue context and a\nresponse candidate. As for IC, it progressively strengthens the model's ability\nin identifying the mismatching information between the dialogue context and a\nresponse candidate. Empirical studies on three benchmark datasets with three\nstate-of-the-art matching models demonstrate that the proposed learning\nframework significantly improves the model performance across various\nevaluation metrics.", "published": "2020-12-29 14:06:41", "link": "http://arxiv.org/abs/2012.14756v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CMV-BERT: Contrastive multi-vocab pretraining of BERT", "abstract": "In this work, we represent CMV-BERT, which improves the pretraining of a\nlanguage model via two ingredients: (a) contrastive learning, which is well\nstudied in the area of computer vision; (b) multiple vocabularies, one of which\nis fine-grained and the other is coarse-grained. The two methods both provide\ndifferent views of an original sentence, and both are shown to be beneficial.\nDownstream tasks demonstrate our proposed CMV-BERT are effective in improving\nthe pretrained language models.", "published": "2020-12-29 14:23:50", "link": "http://arxiv.org/abs/2012.14763v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Adversarial Examples in Chinese Texts Using Sentence-Pieces", "abstract": "Adversarial attacks in texts are mostly substitution-based methods that\nreplace words or characters in the original texts to achieve success attacks.\nRecent methods use pre-trained language models as the substitutes generator.\nWhile in Chinese, such methods are not applicable since words in Chinese\nrequire segmentations first. In this paper, we propose a pre-train language\nmodel as the substitutes generator using sentence-pieces to craft adversarial\nexamples in Chinese. The substitutions in the generated adversarial examples\nare not characters or words but \\textit{'pieces'}, which are more natural to\nChinese readers. Experiments results show that the generated adversarial\nsamples can mislead strong target models and remain fluent and semantically\npreserved.", "published": "2020-12-29 14:28:07", "link": "http://arxiv.org/abs/2012.14769v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Hierarchical Transformer with Speaker Modeling for Emotion Recognition\n  in Conversation", "abstract": "Emotion Recognition in Conversation (ERC) is a more challenging task than\nconventional text emotion recognition. It can be regarded as a personalized and\ninteractive emotion recognition task, which is supposed to consider not only\nthe semantic information of text but also the influences from speakers. The\ncurrent method models speakers' interactions by building a relation between\nevery two speakers. However, this fine-grained but complicated modeling is\ncomputationally expensive, hard to extend, and can only consider local context.\nTo address this problem, we simplify the complicated modeling to a binary\nversion: Intra-Speaker and Inter-Speaker dependencies, without identifying\nevery unique speaker for the targeted speaker. To better achieve the simplified\ninteraction modeling of speakers in Transformer, which shows excellent ability\nto settle long-distance dependency, we design three types of masks and\nrespectively utilize them in three independent Transformer blocks. The designed\nmasks respectively model the conventional context modeling, Intra-Speaker\ndependency, and Inter-Speaker dependency. Furthermore, different speaker-aware\ninformation extracted by Transformer blocks diversely contributes to the\nprediction, and therefore we utilize the attention mechanism to automatically\nweight them. Experiments on two ERC datasets indicate that our model is\nefficacious to achieve better performance.", "published": "2020-12-29 14:47:35", "link": "http://arxiv.org/abs/2012.14781v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DRS at MRP 2020: Dressing up Discourse Representation Structures as\n  Graphs", "abstract": "Discourse Representation Theory (DRT) is a formal account for representing\nthe meaning of natural language discourse. Meaning in DRT is modeled via a\nDiscourse Representation Structure (DRS), a meaning representation with a\nmodel-theoretic interpretation, which is usually depicted as nested boxes. In\ncontrast, a directed labeled graph is a common data structure used to encode\nsemantics of natural language texts. The paper describes the procedure of\ndressing up DRSs as directed labeled graphs to include DRT as a new framework\nin the 2020 shared task on Cross-Framework and Cross-Lingual Meaning\nRepresentation Parsing. Since one of the goals of the shared task is to\nencourage unified models for several semantic graph frameworks, the conversion\nprocedure was biased towards making the DRT graph framework somewhat similar to\nother graph-based meaning representation frameworks.", "published": "2020-12-29 16:36:49", "link": "http://arxiv.org/abs/2012.14837v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "The Parallel Meaning Bank: A Framework for Semantically Annotating\n  Multiple Languages", "abstract": "This paper gives a general description of the ideas behind the Parallel\nMeaning Bank, a framework with the aim to provide an easy way to annotate\ncompositional semantics for texts written in languages other than English. The\nannotation procedure is semi-automatic, and comprises seven layers of\nlinguistic information: segmentation, symbolisation, semantic tagging, word\nsense disambiguation, syntactic structure, thematic role labelling, and\nco-reference. New languages can be added to the meaning bank as long as the\ndocuments are based on translations from English, but also introduce new\ninteresting challenges on the linguistics assumptions underlying the Parallel\nMeaning Bank.", "published": "2020-12-29 17:04:10", "link": "http://arxiv.org/abs/2012.14854v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Transformer Feed-Forward Layers Are Key-Value Memories", "abstract": "Feed-forward layers constitute two-thirds of a transformer model's\nparameters, yet their role in the network remains under-explored. We show that\nfeed-forward layers in transformer-based language models operate as key-value\nmemories, where each key correlates with textual patterns in the training\nexamples, and each value induces a distribution over the output vocabulary. Our\nexperiments show that the learned patterns are human-interpretable, and that\nlower layers tend to capture shallow patterns, while upper layers learn more\nsemantic ones. The values complement the keys' input patterns by inducing\noutput distributions that concentrate probability mass on tokens likely to\nappear immediately after each pattern, particularly in the upper layers.\nFinally, we demonstrate that the output of a feed-forward layer is a\ncomposition of its memories, which is subsequently refined throughout the\nmodel's layers via residual connections to produce the final output\ndistribution.", "published": "2020-12-29 19:12:05", "link": "http://arxiv.org/abs/2012.14913v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WikiTableT: A Large-Scale Data-to-Text Dataset for Generating Wikipedia\n  Article Sections", "abstract": "Datasets for data-to-text generation typically focus either on multi-domain,\nsingle-sentence generation or on single-domain, long-form generation. In this\nwork, we cast generating Wikipedia sections as a data-to-text generation task\nand create a large-scale dataset, WikiTableT, that pairs Wikipedia sections\nwith their corresponding tabular data and various metadata. WikiTableT contains\nmillions of instances, covering a broad range of topics, as well as a variety\nof flavors of generation tasks with different levels of flexibility. We\nbenchmark several training and decoding strategies on WikiTableT. Our\nqualitative analysis shows that the best approaches can generate fluent and\nhigh quality texts but they struggle with coherence and factuality, showing the\npotential for our dataset to inspire future work on long-form generation.", "published": "2020-12-29 19:35:34", "link": "http://arxiv.org/abs/2012.14919v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Natural Language Attacks in a Hard Label Black Box Setting", "abstract": "We study an important and challenging task of attacking natural language\nprocessing models in a hard label black box setting. We propose a\ndecision-based attack strategy that crafts high quality adversarial examples on\ntext classification and entailment tasks. Our proposed attack strategy\nleverages population-based optimization algorithm to craft plausible and\nsemantically similar adversarial examples by observing only the top label\npredicted by the target model. At each iteration, the optimization procedure\nallow word replacements that maximizes the overall semantic similarity between\nthe original and the adversarial text. Further, our approach does not rely on\nusing substitute models or any kind of training data. We demonstrate the\nefficacy of our proposed approach through extensive experimentation and\nablation studies on five state-of-the-art target models across seven benchmark\ndatasets. In comparison to attacks proposed in prior literature, we are able to\nachieve a higher success rate with lower word perturbation percentage that too\nin a highly restricted setting.", "published": "2020-12-29 22:01:38", "link": "http://arxiv.org/abs/2012.14956v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can You be More Social? Injecting Politeness and Positivity into\n  Task-Oriented Conversational Agents", "abstract": "Goal-oriented conversational agents are becoming prevalent in our daily\nlives. For these systems to engage users and achieve their goals, they need to\nexhibit appropriate social behavior as well as provide informative replies that\nguide users through tasks. The first component of the research in this paper\napplies statistical modeling techniques to understand conversations between\nusers and human agents for customer service. Analyses show that social language\nused by human agents is associated with greater users' responsiveness and task\ncompletion. The second component of the research is the construction of a\nconversational agent model capable of injecting social language into an agent's\nresponses while still preserving content. The model uses a sequence-to-sequence\ndeep learning architecture, extended with a social language understanding\nelement. Evaluation in terms of content preservation and social language level\nusing both human judgment and automatic linguistic measures shows that the\nmodel can generate responses that enable agents to address users' issues in a\nmore socially appropriate way.", "published": "2020-12-29 08:22:48", "link": "http://arxiv.org/abs/2012.14653v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "RADDLE: An Evaluation Benchmark and Analysis Platform for Robust\n  Task-oriented Dialog Systems", "abstract": "For task-oriented dialog systems to be maximally useful, it must be able to\nprocess conversations in a way that is (1) generalizable with a small number of\ntraining examples for new task domains, and (2) robust to user input in various\nstyles, modalities or domains. In pursuit of these goals, we introduce the\nRADDLE benchmark, a collection of corpora and tools for evaluating the\nperformance of models across a diverse set of domains. By including tasks with\nlimited training data, RADDLE is designed to favor and encourage models with a\nstrong generalization ability. RADDLE also includes a diagnostic checklist that\nfacilitates detailed robustness analysis in aspects such as language\nvariations, speech errors, unseen entities, and out-of-domain utterances. We\nevaluate recent state-of-the-art systems based on pre-training and fine-tuning,\nand find that grounded pre-training on heterogeneous dialog corpora performs\nbetter than training a separate model per domain. Overall, existing models are\nless than satisfactory in robustness evaluation, which suggests opportunities\nfor future improvement.", "published": "2020-12-29 08:58:49", "link": "http://arxiv.org/abs/2012.14666v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Faster Re-translation Using Non-Autoregressive Model For Simultaneous\n  Neural Machine Translation", "abstract": "Recently, simultaneous translation has gathered a lot of attention since it\nenables compelling applications such as subtitle translation for a live event\nor real-time video-call translation. Some of these translation applications\nallow editing of partial translation giving rise to re-translation approaches.\nThe current re-translation approaches are based on autoregressive sequence\ngeneration models (ReTA), which generate tar-get tokens in the (partial)\ntranslation sequentially. The multiple re-translations with sequential\ngeneration inReTAmodelslead to an increased inference time gap between the\nincoming source input and the corresponding target output as the source input\ngrows. Besides, due to the large number of inference operations involved, the\nReTA models are not favorable for resource-constrained devices. In this work,\nwe propose a faster re-translation system based on a non-autoregressive\nsequence generation model (FReTNA) to overcome the aforementioned limitations.\nWe evaluate the proposed model on multiple translation tasks and our model\nreduces the inference times by several orders and achieves a competitive\nBLEUscore compared to the ReTA and streaming (Wait-k) models.The proposed model\nreduces the average computation time by a factor of 20 when compared to the\nReTA model by incurring a small drop in the translation quality. It also\noutperforms the streaming-based Wait-k model both in terms of computation time\n(1.5 times lower) and translation quality.", "published": "2020-12-29 09:43:27", "link": "http://arxiv.org/abs/2012.14681v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Understanding and Improving Encoder Layer Fusion in Sequence-to-Sequence\n  Learning", "abstract": "Encoder layer fusion (EncoderFusion) is a technique to fuse all the encoder\nlayers (instead of the uppermost layer) for sequence-to-sequence (Seq2Seq)\nmodels, which has proven effective on various NLP tasks. However, it is still\nnot entirely clear why and when EncoderFusion should work. In this paper, our\nmain contribution is to take a step further in understanding EncoderFusion.\nMany of previous studies believe that the success of EncoderFusion comes from\nexploiting surface and syntactic information embedded in lower encoder layers.\nUnlike them, we find that the encoder embedding layer is more important than\nother intermediate encoder layers. In addition, the uppermost decoder layer\nconsistently pays more attention to the encoder embedding layer across NLP\ntasks. Based on this observation, we propose a simple fusion method,\nSurfaceFusion, by fusing only the encoder embedding layer for the softmax\nlayer. Experimental results show that SurfaceFusion outperforms EncoderFusion\non several NLP benchmarks, including machine translation, text summarization,\nand grammatical error correction. It obtains the state-of-the-art performance\non WMT16 Romanian-English and WMT14 English-French translation tasks. Extensive\nanalyses reveal that SurfaceFusion learns more expressive bilingual word\nembeddings by building a closer relationship between relevant source and target\nembedding. Source code is freely available at\nhttps://github.com/SunbowLiu/SurfaceFusion.", "published": "2020-12-29 14:26:59", "link": "http://arxiv.org/abs/2012.14768v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Combining Semilattices and Semimodules", "abstract": "We describe the canonical weak distributive law $\\delta \\colon \\mathcal S\n\\mathcal P \\to \\mathcal P \\mathcal S$ of the powerset monad $\\mathcal P$ over\nthe $S$-left-semimodule monad $\\mathcal S$, for a class of semirings $S$. We\nshow that the composition of $\\mathcal P$ with $\\mathcal S$ by means of such\n$\\delta$ yields almost the monad of convex subsets previously introduced by\nJacobs: the only difference consists in the absence in Jacobs's monad of the\nempty convex set. We provide a handy characterisation of the canonical weak\nlifting of $\\mathcal P$ to $\\mathbb{EM}(\\mathcal S)$ as well as an algebraic\ntheory for the resulting composed monad. Finally, we restrict the composed\nmonad to finitely generated convex subsets and we show that it is presented by\nan algebraic theory combining semimodules and semilattices with bottom, which\nare the algebras for the finite powerset monad $\\mathcal P_f$.", "published": "2020-12-29 14:44:13", "link": "http://arxiv.org/abs/2012.14778v3", "categories": ["cs.CL", "math.LO"], "primary_category": "cs.CL"}
{"title": "Dialogue Graph Modeling for Conversational Machine Reading", "abstract": "Conversational Machine Reading (CMR) aims at answering questions in a\ncomplicated manner. Machine needs to answer questions through interactions with\nusers based on given rule document, user scenario and dialogue history, and ask\nquestions to clarify if necessary. In this paper, we propose a dialogue graph\nmodeling framework to improve the understanding and reasoning ability of\nmachine on CMR task. There are three types of graph in total. Specifically,\nDiscourse Graph is designed to learn explicitly and extract the discourse\nrelation among rule texts as well as the extra knowledge of scenario;\nDecoupling Graph is used for understanding local and contextualized connection\nwithin rule texts. And finally a global graph for fusing the information\ntogether and reply to the user with our final decision being either\n\"Yes/No/Irrelevant\" or to ask a follow-up question to clarify.", "published": "2020-12-29 16:08:36", "link": "http://arxiv.org/abs/2012.14827v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Few-Shot Text Ranking with Meta Adapted Synthetic Weak Supervision", "abstract": "The effectiveness of Neural Information Retrieval (Neu-IR) often depends on a\nlarge scale of in-domain relevance training signals, which are not always\navailable in real-world ranking scenarios. To democratize the benefits of\nNeu-IR, this paper presents MetaAdaptRank, a domain adaptive learning method\nthat generalizes Neu-IR models from label-rich source domains to few-shot\ntarget domains. Drawing on source-domain massive relevance supervision,\nMetaAdaptRank contrastively synthesizes a large number of weak supervision\nsignals for target domains and meta-learns to reweight these synthetic \"weak\"\ndata based on their benefits to the target-domain ranking accuracy of Neu-IR\nmodels. Experiments on three TREC benchmarks in the web, news, and biomedical\ndomains show that MetaAdaptRank significantly improves the few-shot ranking\naccuracy of Neu-IR models. Further analyses indicate that MetaAdaptRank thrives\nfrom both its contrastive weak data synthesis and meta-reweighted data\nselection. The code and data of this paper can be obtained from\nhttps://github.com/thunlp/MetaAdaptRank.", "published": "2020-12-29 17:28:53", "link": "http://arxiv.org/abs/2012.14862v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "YASO: A Targeted Sentiment Analysis Evaluation Dataset for Open-Domain\n  Reviews", "abstract": "Current TSA evaluation in a cross-domain setup is restricted to the small set\nof review domains available in existing datasets. Such an evaluation is\nlimited, and may not reflect true performance on sites like Amazon or Yelp that\nhost diverse reviews from many domains. To address this gap, we present YASO -\na new TSA evaluation dataset of open-domain user reviews. YASO contains 2,215\nEnglish sentences from dozens of review domains, annotated with target terms\nand their sentiment. Our analysis verifies the reliability of these\nannotations, and explores the characteristics of the collected data. Benchmark\nresults using five contemporary TSA systems show there is ample room for\nimprovement on this challenging new dataset. YASO is available at\nhttps://github.com/IBM/yaso-tsa.", "published": "2020-12-29 00:25:15", "link": "http://arxiv.org/abs/2012.14541v2", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Generating Query Focused Summaries from Query-Free Resources", "abstract": "The availability of large-scale datasets has driven the development of neural\nmodels that create generic summaries from single or multiple documents. In this\nwork we consider query focused summarization (QFS), a task for which training\ndata in the form of queries, documents, and summaries is not readily available.\nWe propose to decompose QFS into (1) query modeling (i.e., finding supportive\nevidence within a set of documents for a query) and (2) conditional language\nmodeling (i.e., summary generation). We introduce MaRGE, a Masked ROUGE\nRegression framework for evidence estimation and ranking which relies on a\nunified representation for summaries and queries, so that summaries in generic\ndata can be converted into proxy queries for learning a query model.\nExperiments across QFS benchmarks and query types show that our model achieves\nstate-of-the-art performance despite learning from weak supervision.", "published": "2020-12-29 14:39:35", "link": "http://arxiv.org/abs/2012.14774v2", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Few-Shot Named Entity Recognition: A Comprehensive Study", "abstract": "This paper presents a comprehensive study to efficiently build named entity\nrecognition (NER) systems when a small number of in-domain labeled data is\navailable. Based upon recent Transformer-based self-supervised pre-trained\nlanguage models (PLMs), we investigate three orthogonal schemes to improve the\nmodel generalization ability for few-shot settings: (1) meta-learning to\nconstruct prototypes for different entity types, (2) supervised pre-training on\nnoisy web data to extract entity-related generic representations and (3)\nself-training to leverage unlabeled in-domain data. Different combinations of\nthese schemes are also considered. We perform extensive empirical comparisons\non 10 public NER datasets with various proportions of labeled data, suggesting\nuseful insights for future research. Our experiments show that (i) in the\nfew-shot learning setting, the proposed NER schemes significantly improve or\noutperform the commonly used baseline, a PLM-based linear classifier fine-tuned\non domain labels; (ii) We create new state-of-the-art results on both few-shot\nand training-free settings compared with existing methods. We will release our\ncode and pre-trained models for reproducible research.", "published": "2020-12-29 23:43:16", "link": "http://arxiv.org/abs/2012.14978v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Data-driven audio recognition: a supervised dictionary approach", "abstract": "Machine hearing is an emerging area. Motivated by the need of a principled\nframework across domain applications for machine listening, we propose a\ngeneric and data-driven representation learning approach. For this sake, a\nnovel and efficient supervised dictionary learning method is presented.\nExperiments are performed on both computational auditory scene (East Anglia and\nRouen) and synthetic music chord recognition datasets. Obtained results show\nthat our method is capable to reach state-of-the-art hand-crafted features for\nboth applications", "published": "2020-12-29 14:21:06", "link": "http://arxiv.org/abs/2012.14761v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Detection of Lexical Stress Errors in Non-Native (L2) English with Data\n  Augmentation and Attention", "abstract": "This paper describes two novel complementary techniques that improve the\ndetection of lexical stress errors in non-native (L2) English speech:\nattention-based feature extraction and data augmentation based on Neural\nText-To-Speech (TTS). In a classical approach, audio features are usually\nextracted from fixed regions of speech such as the syllable nucleus. We propose\nan attention-based deep learning model that automatically derives optimal\nsyllable-level representation from frame-level and phoneme-level audio\nfeatures. Training this model is challenging because of the limited amount of\nincorrect stress patterns. To solve this problem, we propose to augment the\ntraining set with incorrectly stressed words generated with Neural TTS.\nCombining both techniques achieves 94.8% precision and 49.2% recall for the\ndetection of incorrectly stressed words in L2 English speech of Slavic and\nBaltic speakers.", "published": "2020-12-29 14:59:49", "link": "http://arxiv.org/abs/2012.14788v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Bayesian HMM clustering of x-vector sequences (VBx) in speaker\n  diarization: theory, implementation and analysis on standard tasks", "abstract": "The recently proposed VBx diarization method uses a Bayesian hidden Markov\nmodel to find speaker clusters in a sequence of x-vectors. In this work we\nperform an extensive comparison of performance of the VBx diarization with\nother approaches in the literature and we show that VBx achieves superior\nperformance on three of the most popular datasets for evaluating diarization:\nCALLHOME, AMI and DIHARDII datasets. Further, we present for the first time the\nderivation and update formulae for the VBx model, focusing on the efficiency\nand simplicity of this model as compared to the previous and more complex BHMM\nmodel working on frame-by-frame standard Cepstral features. Together with this\npublication, we release the recipe for training the x-vector extractors used in\nour experiments on both wide and narrowband data, and the VBx recipes that\nattain state-of-the-art performance on all three datasets. Besides, we point\nout the lack of a standardized evaluation protocol for AMI dataset and we\npropose a new protocol for both Beamformed and Mix-Headset audios based on the\nofficial AMI partitions and transcriptions.", "published": "2020-12-29 21:41:19", "link": "http://arxiv.org/abs/2012.14952v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Detecting COVID-19 from Breathing and Coughing Sounds using Deep Neural\n  Networks", "abstract": "The COVID-19 pandemic has affected the world unevenly; while industrial\neconomies have been able to produce the tests necessary to track the spread of\nthe virus and mostly avoided complete lockdowns, developing countries have\nfaced issues with testing capacity. In this paper, we explore the usage of deep\nlearning models as a ubiquitous, low-cost, pre-testing method for detecting\nCOVID-19 from audio recordings of breathing or coughing taken with mobile\ndevices or via the web. We adapt an ensemble of Convolutional Neural Networks\nthat utilise raw breathing and coughing audio and spectrograms to classify if a\nspeaker is infected with COVID-19 or not. The different models are obtained via\nautomatic hyperparameter tuning using Bayesian Optimisation combined with\nHyperBand. The proposed method outperforms a traditional baseline approach by a\nlarge margin. Ultimately, it achieves an Unweighted Average Recall (UAR) of\n74.9%, or an Area Under ROC Curve (AUC) of 80.7% by ensembling neural networks,\nconsidering the best test set result across breathing and coughing in a\nstrictly subject independent manner. In isolation, breathing sounds thereby\nappear slightly better suited than coughing ones (76.1% vs 73.7% UAR).", "published": "2020-12-29 01:14:17", "link": "http://arxiv.org/abs/2012.14553v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "68T11", "I.2; I.5; J.3"], "primary_category": "cs.SD"}
