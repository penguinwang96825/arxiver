{"title": "Robust forward investment and consumption under drift and volatility uncertainties: A randomization approach", "abstract": "This paper studies robust forward investment and consumption preferences and\noptimal strategies for a risk-averse and ambiguity-averse agent in an\nincomplete financial market with drift and volatility uncertainties. We focus\non non-zero volatility and constant relative risk aversion (CRRA) forward\npreferences. Given the non-convexity of the Hamiltonian with respect to\nuncertain volatilities, we first construct robust randomized forward\npreferences through endogenous randomization in an auxiliary market. We derive\nthe corresponding optimal and robust investment and consumption strategies.\nFurthermore, we show that such forward preferences and strategies, developed in\nthe auxiliary market, remain optimal and robust in the physical market,\noffering a comprehensive framework for forward investment and consumption under\nmodel uncertainty.", "published": "2024-10-02 09:48:28", "link": "http://arxiv.org/abs/2410.01378v1", "categories": ["q-fin.PM", "q-fin.MF", "60H30, 91G10"], "primary_category": "q-fin.PM"}
{"title": "Distilling Analysis from Generative Models for Investment Decisions", "abstract": "Professionals' decisions are the focus of every field. For example,\npoliticians' decisions will influence the future of the country, and stock\nanalysts' decisions will impact the market. Recognizing the influential role of\nprofessionals' perspectives, inclinations, and actions in shaping\ndecision-making processes and future trends across multiple fields, we propose\nthree tasks for modeling these decisions in the financial market. To facilitate\nthis, we introduce a novel dataset, A3, designed to simulate professionals'\ndecision-making processes. While we find current models present challenges in\nforecasting professionals' behaviors, particularly in making trading decisions,\nthe proposed Chain-of-Decision approach demonstrates promising improvements. It\nintegrates an opinion-generator-in-the-loop to provide subjective analysis\nbased on each news item, further enhancing the proposed tasks' performance.", "published": "2024-10-02 01:39:42", "link": "http://arxiv.org/abs/2410.07225v1", "categories": ["q-fin.ST", "cs.CL", "cs.LG"], "primary_category": "q-fin.ST"}
{"title": "Mean field equilibrium asset pricing model under partial observation: An exponential quadratic Gaussian approach", "abstract": "This paper studies an asset pricing model in a partially observable market\nwith a large number of heterogeneous agents using the mean field game theory.\nIn this model, we assume that investors can only observe stock prices and must\ninfer the risk premium from these observations when determining trading\nstrategies. We characterize the equilibrium risk premium in such a market\nthrough a solution to the mean field backward stochastic differential equation\n(BSDE). Specifically, the solution to the mean field BSDE can be expressed\nsemi-analytically by employing an exponential quadratic Gaussian framework. We\nthen construct the risk premium process, which cannot be observed directly by\ninvestors, endogenously using the Kalman-Bucy filtering theory. In addition, we\ninclude a simple numerical simulation to visualize the dynamics of our market\nmodel.", "published": "2024-10-02 09:09:34", "link": "http://arxiv.org/abs/2410.01352v2", "categories": ["q-fin.PR", "q-fin.TR", "49N80, 91B51, 60H10"], "primary_category": "q-fin.PR"}
{"title": "Document Classification using File Names", "abstract": "Rapid document classification is critical in several time-sensitive\napplications like digital forensics and large-scale media classification.\nTraditional approaches that rely on heavy-duty deep learning models fall short\ndue to high inference times over vast input datasets and computational\nresources associated with analyzing whole documents. In this paper, we present\na method using lightweight supervised learning models, combined with a TF-IDF\nfeature extraction-based tokenization method, to accurately and efficiently\nclassify documents based solely on file names, that substantially reduces\ninference time. Our results indicate that file name classifiers can process\nmore than 90% of in-scope documents with 99.63% and 96.57% accuracy when tested\non two datasets, while being 442x faster than more complex models such as DiT.\nOur method offers a crucial solution to efficiently process vast document\ndatasets in critical scenarios, enabling fast and more reliable document\nclassification.", "published": "2024-10-02 01:42:19", "link": "http://arxiv.org/abs/2410.01166v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GADFA: Generator-Assisted Decision-Focused Approach for Opinion\n  Expressing Timing Identification", "abstract": "The advancement of text generation models has granted us the capability to\nproduce coherent and convincing text on demand. Yet, in real-life\ncircumstances, individuals do not continuously generate text or voice their\nopinions. For instance, consumers pen product reviews after weighing the merits\nand demerits of a product, and professional analysts issue reports following\nsignificant news releases. In essence, opinion expression is typically prompted\nby particular reasons or signals. Despite long-standing developments in opinion\nmining, the appropriate timing for expressing an opinion remains largely\nunexplored. To address this deficit, our study introduces an innovative task -\nthe identification of news-triggered opinion expressing timing. We ground this\ntask in the actions of professional stock analysts and develop a novel dataset\nfor investigation. Our approach is decision-focused, leveraging text generation\nmodels to steer the classification model, thus enhancing overall performance.\nOur experimental findings demonstrate that the text generated by our model\ncontributes fresh insights from various angles, effectively aiding in\nidentifying the optimal timing for opinion expression.", "published": "2024-10-02 01:54:46", "link": "http://arxiv.org/abs/2410.01169v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unifying the Scope of Bridging Anaphora Types in English: Bridging\n  Annotations in ARRAU and GUM", "abstract": "Comparing bridging annotations across coreference resources is difficult,\nlargely due to a lack of standardization across definitions and annotation\nschemas and narrow coverage of disparate text domains across resources. To\nalleviate domain coverage issues and consolidate schemas, we compare guidelines\nand use interpretable predictive models to examine the bridging instances\nannotated in the GUM, GENTLE and ARRAU corpora. Examining these cases, we find\nthat there is a large difference in types of phenomena annotated as bridging.\nBeyond theoretical results, we release a harmonized, subcategorized version of\nthe test sets of GUM, GENTLE and the ARRAU Wall Street Journal data to promote\nmeaningful and reliable evaluation of bridging resolution across domains.", "published": "2024-10-02 01:56:28", "link": "http://arxiv.org/abs/2410.01170v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Multilingual Retrieval Augmented Generation for Culturally-Sensitive\n  Tasks: A Benchmark for Cross-lingual Robustness", "abstract": "The paradigm of retrieval-augmented generated (RAG) helps mitigate\nhallucinations of large language models (LLMs). However, RAG also introduces\nbiases contained within the retrieved documents. These biases can be amplified\nin scenarios which are multilingual and culturally-sensitive, such as\nterritorial disputes. In this paper, we introduce BordIRLines, a benchmark\nconsisting of 720 territorial dispute queries paired with 14k Wikipedia\ndocuments across 49 languages. To evaluate LLMs' cross-lingual robustness for\nthis task, we formalize several modes for multilingual retrieval. Our\nexperiments on several LLMs reveal that retrieving multilingual documents best\nimproves response consistency and decreases geopolitical bias over using purely\nin-language documents, showing how incorporating diverse perspectives improves\nrobustness. Also, querying in low-resource languages displays a much wider\nvariance in the linguistic distribution of response citations. Our further\nexperiments and case studies investigate how cross-lingual RAG is affected by\naspects from IR to document contents. We release our benchmark and code to\nsupport further research towards ensuring equitable information access across\nlanguages at https://huggingface.co/datasets/borderlines/bordirlines.", "published": "2024-10-02 01:59:07", "link": "http://arxiv.org/abs/2410.01171v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Gold Panning in Vocabulary: An Adaptive Method for Vocabulary Expansion\n  of Domain-Specific LLMs", "abstract": "While Large Language Models (LLMs) demonstrate impressive generation\nabilities, they frequently struggle when it comes to specialized domains due to\ntheir limited domain-specific knowledge. Studies on domain-specific LLMs resort\nto expanding the vocabulary before fine-tuning on domain-specific corpus,\naiming to decrease the sequence length and enhance efficiency during decoding,\nwithout thoroughly investigating the results of vocabulary expansion to LLMs\nover different domains. Our pilot study reveals that expansion with only a\nsubset of the entire vocabulary may lead to superior performance. Guided by the\ndiscovery, this paper explores how to identify a vocabulary subset to achieve\nthe optimal results. We introduce VEGAD, an adaptive method that automatically\nidentifies valuable words from a given domain vocabulary. Our method has been\nvalidated through experiments on three Chinese datasets, demonstrating its\neffectiveness. Additionally, we have undertaken comprehensive analyses of the\nmethod. The selection of a optimal subset for expansion has shown to enhance\nperformance on both domain-specific tasks and general tasks, showcasing the\npotential of VEGAD.", "published": "2024-10-02 02:47:39", "link": "http://arxiv.org/abs/2410.01188v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "StringLLM: Understanding the String Processing Capability of Large\n  Language Models", "abstract": "String processing, which mainly involves the analysis and manipulation of\nstrings, is a fundamental component of modern computing. Despite the\nsignificant advancements of large language models (LLMs) in various natural\nlanguage processing (NLP) tasks, their capability in string processing remains\nunderexplored and underdeveloped. To bridge this gap, we present a\ncomprehensive study of LLMs' string processing capability. In particular, we\nfirst propose StringLLM, a method to construct datasets for benchmarking string\nprocessing capability of LLMs. We use StringLLM to build a series of datasets,\nreferred to as StringBench. It encompasses a wide range of string processing\ntasks, allowing us to systematically evaluate LLMs' performance in this area.\nOur evaluations indicate that LLMs struggle with accurately processing strings\ncompared to humans. To uncover the underlying reasons for this limitation, we\nconduct an in-depth analysis and subsequently propose an effective approach\nthat significantly enhances LLMs' string processing capability via fine-tuning.\nThis work provides a foundation for future research to understand LLMs' string\nprocessing capability. Our code and data are available at\nhttps://github.com/wxl-lxw/StringLLM.", "published": "2024-10-02 03:23:29", "link": "http://arxiv.org/abs/2410.01208v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Training Data Attribution for Large Language Models with\n  Fitting Error Consideration", "abstract": "The black-box nature of large language models (LLMs) poses challenges in\ninterpreting results, impacting issues such as data intellectual property\nprotection and hallucination tracing. Training data attribution (TDA) methods\nare considered effective solutions to address these challenges. Most recent TDA\nmethods rely on influence functions, assuming the model achieves minimized\nempirical risk. However, achieving this criterion is difficult, and sourcing\naccuracy can be compromised by fitting errors during model training. In this\npaper, we introduce a novel TDA method called Debias and Denoise Attribution\n(DDA), which enhances influence functions by addressing fitting errors.\nSpecifically, the debias strategy seeks to improve the performance of influence\nfunctions by eliminating the knowledge bias present in the base model before\nfine-tuning, while the denoise strategy aims to reduce discrepancies in\ninfluence scores arising from varying degrees of fitting during the training\nprocess through smoothing techniques. Experimental results demonstrate that our\nmethod significantly outperforms existing approaches, achieving an averaged AUC\nof 91.64%. Moreover, DDA exhibits strong generality and scalability across\nvarious sources and different-scale models like LLaMA2, QWEN2, and Mistral.", "published": "2024-10-02 07:14:26", "link": "http://arxiv.org/abs/2410.01285v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Endless Jailbreaks with Bijection Learning", "abstract": "Despite extensive safety measures, LLMs are vulnerable to adversarial inputs,\nor jailbreaks, which can elicit unsafe behaviors. In this work, we introduce\nbijection learning, a powerful attack algorithm which automatically fuzzes LLMs\nfor safety vulnerabilities using randomly-generated encodings whose complexity\ncan be tightly controlled. We leverage in-context learning to teach models\nbijective encodings, pass encoded queries to the model to bypass built-in\nsafety mechanisms, and finally decode responses back into English. Our attack\nis extremely effective on a wide range of frontier language models. Moreover,\nby controlling complexity parameters such as number of key-value mappings in\nthe encodings, we find a close relationship between the capability level of the\nattacked LLM and the average complexity of the most effective bijection\nattacks. Our work highlights that new vulnerabilities in frontier models can\nemerge with scale: more capable models are more severely jailbroken by\nbijection attacks.", "published": "2024-10-02 07:40:56", "link": "http://arxiv.org/abs/2410.01294v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assisted Data Annotation for Business Process Information Extraction\n  from Textual Documents", "abstract": "Machine-learning based generation of process models from natural language\ntext process descriptions provides a solution for the time-intensive and\nexpensive process discovery phase. Many organizations have to carry out this\nphase, before they can utilize business process management and its benefits.\nYet, research towards this is severely restrained by an apparent lack of large\nand high-quality datasets. This lack of data can be attributed to, among other\nthings, an absence of proper tool assistance for dataset creation, resulting in\nhigh workloads and inferior data quality. We explore two assistance features to\nsupport dataset creation, a recommendation system for identifying process\ninformation in the text and visualization of the current state of already\nidentified process information as a graphical business process model. A\ncontrolled user study with 31 participants shows that assisting dataset\ncreators with recommendations lowers all aspects of workload, up to $-51.0\\%$,\nand significantly improves annotation quality, up to $+38.9\\%$. We make all\ndata and code available to encourage further research on additional novel\nassistance strategies.", "published": "2024-10-02 09:14:39", "link": "http://arxiv.org/abs/2410.01356v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CrowdCounter: A benchmark type-specific multi-target counterspeech\n  dataset", "abstract": "Counterspeech presents a viable alternative to banning or suspending users\nfor hate speech while upholding freedom of expression. However, writing\neffective counterspeech is challenging for moderators/users. Hence, developing\nsuggestion tools for writing counterspeech is the need of the hour. One\ncritical challenge in developing such a tool is the lack of quality and\ndiversity of the responses in the existing datasets. Hence, we introduce a new\ndataset - CrowdCounter containing 3,425 hate speech-counterspeech pairs\nspanning six different counterspeech types (empathy, humor, questioning,\nwarning, shaming, contradiction), which is the first of its kind. The design of\nour annotation platform itself encourages annotators to write type-specific,\nnon-redundant and high-quality counterspeech. We evaluate two frameworks for\ngenerating counterspeech responses - vanilla and type-controlled prompts -\nacross four large language models. In terms of metrics, we evaluate the\nresponses using relevance, diversity and quality. We observe that Flan-T5 is\nthe best model in the vanilla framework across different models. Type-specific\nprompts enhance the relevance of the responses, although they might reduce the\nlanguage quality. DialoGPT proves to be the best at following the instructions\nand generating the type-specific counterspeech accurately.", "published": "2024-10-02 10:24:51", "link": "http://arxiv.org/abs/2410.01400v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Question-guided Knowledge Graph Re-scoring and Injection for Knowledge\n  Graph Question Answering", "abstract": "Knowledge graph question answering (KGQA) involves answering natural language\nquestions by leveraging structured information stored in a knowledge graph.\nTypically, KGQA initially retrieve a targeted subgraph from a large-scale\nknowledge graph, which serves as the basis for reasoning models to address\nqueries. However, the retrieved subgraph inevitably brings distraction\ninformation for knowledge utilization, impeding the model's ability to perform\naccurate reasoning. To address this issue, we propose a Question-guided\nKnowledge Graph Re-scoring method (Q-KGR) to eliminate noisy pathways for the\ninput question, thereby focusing specifically on pertinent factual knowledge.\nMoreover, we introduce Knowformer, a parameter-efficient method for injecting\nthe re-scored knowledge graph into large language models to enhance their\nability to perform factual reasoning. Extensive experiments on multiple KGQA\nbenchmarks demonstrate the superiority of our method over existing systems.", "published": "2024-10-02 10:27:07", "link": "http://arxiv.org/abs/2410.01401v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can We Further Elicit Reasoning in LLMs? Critic-Guided Planning with\n  Retrieval-Augmentation for Solving Challenging Tasks", "abstract": "State-of-the-art large language models (LLMs) exhibit impressive\nproblem-solving capabilities but may struggle with complex reasoning and\nfactual correctness. Existing methods harness the strengths of chain-of-thought\nand retrieval-augmented generation (RAG) to decompose a complex problem into\nsimpler steps and apply retrieval to improve factual correctness. These methods\nwork well on straightforward reasoning tasks but often falter on challenging\ntasks such as competitive programming and mathematics, due to frequent\nreasoning errors and irrelevant knowledge retrieval. To address this, we\nintroduce Critic-guided planning with Retrieval-augmentation, CR-Planner, a\nnovel framework that leverages fine-tuned critic models to guide both reasoning\nand retrieval processes through planning. CR-Planner solves a problem by\niteratively selecting and executing sub-goals. Initially, it identifies the\nmost promising sub-goal from reasoning, query generation, and retrieval, guided\nby rewards given by a critic model named sub-goal critic. It then executes this\nsub-goal through sampling and selecting the optimal output based on evaluations\nfrom another critic model named execution critic. This iterative process,\ninformed by retrieved information and critic models, enables CR-Planner to\neffectively navigate the solution space towards the final answer. We employ\nMonte Carlo Tree Search to collect the data for training the critic models,\nallowing for a systematic exploration of action sequences and their long-term\nimpacts. We validate CR-Planner on challenging domain-knowledge-intensive and\nreasoning-heavy tasks, including competitive programming, theorem-driven math\nreasoning, and complex domain retrieval problems. Our experiments demonstrate\nthat CR-Planner significantly outperforms baselines, highlighting its\neffectiveness in addressing challenging problems by improving both reasoning\nand retrieval.", "published": "2024-10-02 11:26:02", "link": "http://arxiv.org/abs/2410.01428v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Little Goes a Long Way: Efficient Long Context Training and Inference\n  with Partial Contexts", "abstract": "Training and serving long-context large language models (LLMs) incurs\nsubstantial overhead. To address this, two critical steps are often required: a\npretrained LLM typically undergoes a separate stage for context length\nextension by training on long-context data, followed by architectural\nmodifications to reduce the overhead of KV cache during serving. This paper\nargues that integrating length extension with a GPU-friendly KV cache reduction\narchitecture not only reduces training overhead during length extension, but\nalso achieves better long-context performance. This leads to our proposed\nLongGen, which finetunes a pretrained LLM into an efficient architecture during\nlength extension. LongGen builds on three key insights: (1) Sparse attention\npatterns, such as window attention (attending to recent tokens), attention sink\n(initial ones), and blockwise sparse attention (strided token blocks) are\nwell-suited for building efficient long-context models, primarily due to their\nGPU-friendly memory access patterns, enabling efficiency gains not just\ntheoretically but in practice as well. (2) It is essential for the model to\nhave direct access to all tokens. A hybrid architecture with 1/3 full attention\nlayers and 2/3 efficient ones achieves a balanced trade-off between efficiency\nand long-context performance. (3) Lightweight training on 5B long-context data\nis sufficient to extend the hybrid model's context length from 4K to 128K.\n  We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its\neffectiveness across different scales. During training with 128K-long contexts,\nLongGen achieves 1.55x training speedup and reduces wall-clock time by 36%,\ncompared to a full-attention baseline. During inference, LongGen reduces KV\ncache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding\nspeedup.", "published": "2024-10-02 12:35:53", "link": "http://arxiv.org/abs/2410.01485v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Small Language Models Also Work With Small Vocabularies: Probing the\n  Linguistic Abilities of Grapheme- and Phoneme-Based Baby Llamas", "abstract": "Recent work investigates whether LMs learn human-like linguistic\ngeneralizations and representations from developmentally plausible amounts of\ndata. Yet, the basic linguistic units processed in these LMs are determined by\nsubword-based tokenization, which limits their validity as models of learning\nat and below the word level. In this paper, we explore the potential of\ntokenization-free, phoneme- and grapheme-based language models. We demonstrate\nthat small models based on the Llama architecture can achieve strong linguistic\nperformance on standard syntactic and novel lexical/phonetic benchmarks when\ntrained with character-level vocabularies. We further show that phoneme-based\nmodels almost match grapheme-based models in standard tasks and novel\nevaluations. Our findings suggest a promising direction for creating more\nlinguistically plausible language models that are better suited for\ncomputational studies of language acquisition and processing.", "published": "2024-10-02 12:36:08", "link": "http://arxiv.org/abs/2410.01487v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extending Context Window of Large Language Models from a Distributional\n  Perspective", "abstract": "Scaling the rotary position embedding (RoPE) has become a common method for\nextending the context window of RoPE-based large language models (LLMs).\nHowever, existing scaling methods often rely on empirical approaches and lack a\nprofound understanding of the internal distribution within RoPE, resulting in\nsuboptimal performance in extending the context window length. In this paper,\nwe propose to optimize the context window extending task from the view of\nrotary angle distribution. Specifically, we first estimate the distribution of\nthe rotary angles within the model and analyze the extent to which length\nextension perturbs this distribution. Then, we present a novel extension\nstrategy that minimizes the disturbance between rotary angle distributions to\nmaintain consistency with the pre-training phase, enhancing the model's\ncapability to generalize to longer sequences. Experimental results compared to\nthe strong baseline methods demonstrate that our approach reduces by up to 72%\nof the distributional disturbance when extending LLaMA2's context window to 8k,\nand reduces by up to 32% when extending to 16k. On the LongBench-E benchmark,\nour method achieves an average improvement of up to 4.33% over existing\nstate-of-the-art methods. Furthermore, Our method maintains the model's\nperformance on the Hugging Face Open LLM benchmark after context window\nextension, with only an average performance fluctuation ranging from -0.12 to\n+0.22.", "published": "2024-10-02 12:40:11", "link": "http://arxiv.org/abs/2410.01490v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PersonaMath: Boosting Mathematical Reasoning via Persona-Driven Data\n  Augmentation", "abstract": "While closed-source Large Language Models (LLMs) demonstrate strong\nmathematical problem-solving abilities, open-source models still face\nchallenges with such tasks. To bridge this gap, we propose a data augmentation\napproach and introduce PersonaMathQA, a dataset derived from MATH and GSM8K, on\nwhich we train the PersonaMath models. Our approach consists of two stages: the\nfirst stage focuses on learning from Persona Diversification, and the second\nstage emphasizes learning from Reflection. In the first stage, we regenerate\ndetailed chain-of-thought (CoT) solutions as instructions using a closed-source\nLLM and introduce a persona-driven data augmentation technique. This technique\ninnovatively classifies personas based on occupations, significantly enhancing\nthe dataset's diversity and quality. In the second stage, we incorporate\nreflection to fully leverage more challenging and valuable questions.\nEvaluation of our PersonaMath models on MATH and GSM8K reveals that the\nPersonaMath-7B model (based on Qwen2.5-7B) achieves an accuracy of 61.2% on\nMATH and 87.8% on GSM8K, surpassing all baseline methods and achieving\nstate-of-the-art performance. Notably, our dataset contains only 128.9K data\npoints-merely 32.6% of MetaMathQA and 49.5% of MathInstruct-yet our model\noutperforms these baselines, demonstrating the high quality and diversity of\nour dataset, which enables more efficient model training. We open-source the\nPersonaMathQA dataset, PersonaMath models, and our code for public usage.", "published": "2024-10-02 12:57:12", "link": "http://arxiv.org/abs/2410.01504v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Intent Detection in the Age of LLMs", "abstract": "Intent detection is a critical component of task-oriented dialogue systems\n(TODS) which enables the identification of suitable actions to address user\nutterances at each dialog turn. Traditional approaches relied on\ncomputationally efficient supervised sentence transformer encoder models, which\nrequire substantial training data and struggle with out-of-scope (OOS)\ndetection. The emergence of generative large language models (LLMs) with\nintrinsic world knowledge presents new opportunities to address these\nchallenges. In this work, we adapt 7 SOTA LLMs using adaptive in-context\nlearning and chain-of-thought prompting for intent detection, and compare their\nperformance with contrastively fine-tuned sentence transformer (SetFit) models\nto highlight prediction quality and latency tradeoff. We propose a hybrid\nsystem using uncertainty based routing strategy to combine the two approaches\nthat along with negative data augmentation results in achieving the best of\nboth worlds ( i.e. within 2% of native LLM accuracy with 50% less latency). To\nbetter understand LLM OOS detection capabilities, we perform controlled\nexperiments revealing that this capability is significantly influenced by the\nscope of intent labels and the size of the label space. We also introduce a\ntwo-step approach utilizing internal LLM representations, demonstrating\nempirical gains in OOS detection accuracy and F1-score by >5% for the\nMistral-7B model.", "published": "2024-10-02 15:01:55", "link": "http://arxiv.org/abs/2410.01627v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DeIDClinic: A Multi-Layered Framework for De-identification of Clinical\n  Free-text Data", "abstract": "De-identification is important in protecting patients' privacy for healthcare\ntext analytics. The MASK framework is one of the best on the de-identification\nshared task organised by n2c2/i2b2 challenges. This work enhances the MASK\nframework by integrating ClinicalBERT, a deep learning model specifically\nfine-tuned on clinical texts, alongside traditional de-identification methods\nlike dictionary lookup and rule-based approaches. The system effectively\nidentifies and either redacts or replaces sensitive identifiable entities\nwithin clinical documents, while also allowing users to customise the masked\ndocuments according to their specific needs. The integration of ClinicalBERT\nsignificantly improves the performance of entity recognition, achieving 0.9732\nF1-score, especially for common entities such as names, dates, and locations.\n  A risk assessment feature has also been developed, which analyses the\nuniqueness of context within documents to classify them into risk levels,\nguiding further de-identification efforts. While the system demonstrates strong\noverall performance, this work highlights areas for future improvement,\nincluding handling more complex entity occurrences and enhancing the system's\nadaptability to different clinical settings.", "published": "2024-10-02 15:16:02", "link": "http://arxiv.org/abs/2410.01648v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Exploration of Self-Supervised Mutual Information Alignment for\n  Multi-Task Settings", "abstract": "There is a growing need for pluralistic alignment methods that can steer\nlanguage models towards individual attributes and preferences. One such method,\nSelf-Supervised Alignment with Mutual Information (SAMI), uses conditional\nmutual information to encourage the connection between behavioral preferences\nand model responses. We conduct two experiments exploring SAMI in multi-task\nsettings. First, we compare SAMI to Direct Preference Optimization (DPO) on a\nmulti-task benchmark (MT-Bench), using a stronger model to generate training\ndata for a weaker one across diverse categories (humanities, STEM, extraction,\ncoding, math, reasoning, and roleplay). Our results indicate that one iteration\nof SAMI has a 57% win rate against DPO, with significant variation in\nperformance between task categories. Second, we examine SAMI's impact on\nmathematical accuracy (GSM-8K) relative to supervised fine-tuning (SFT). While\nSAMI increases zero-shot performance by 1.1%, SFT is more effective with a 3.2%\nboost. However, SAMI shows interesting scaling trends. When given 10 attempts,\nSAMI improves accuracy by 3.9%, while SFT achieves a 10.1% increase. Combining\nSAMI with SFT yields an additional improvement of 1.3% in multi-attempt\nsettings, though single-attempt accuracy remains unchanged.", "published": "2024-10-02 16:15:04", "link": "http://arxiv.org/abs/2410.01704v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Visual Perception in Text Strings", "abstract": "Understanding visual semantics embedded in consecutive characters is a\ncrucial capability for both large language models (LLMs) and multi-modal large\nlanguage models (MLLMs). This type of artifact possesses the unique\ncharacteristic that identical information can be readily formulated in both\ntexts and images, making them a significant proxy for analyzing modern LLMs'\nand MLLMs' capabilities in modality-agnostic vision understanding. In this\nwork, we select ASCII art as a representative artifact, where the lines and\nbrightness used to depict each concept are rendered by characters, and we frame\nthe problem as an ASCII art recognition task. We benchmark model performance on\nthis task by constructing an evaluation dataset with an elaborate\ncategorization tree and also collect a training set to elicit the models'\nvisual perception ability. Through a comprehensive analysis of dozens of\nmodels, results reveal that although humans can achieve nearly 100% accuracy,\nthe state-of-the-art LLMs and MLLMs lag far behind. Models are capable of\nrecognizing concepts depicted in the ASCII arts given only text inputs\nindicated by over 60% accuracy for some concepts, but most of them achieves\nmerely around 30% accuracy when averaged across all categories. When provided\nwith images as inputs, GPT-4o gets 82.68%, outperforming the strongest\nopen-source MLLM by 21.95%. Although models favor different kinds of ASCII art\ndepending on the modality provided, none of the MLLMs successfully benefit when\nboth modalities are supplied simultaneously. Moreover, supervised fine-tuning\nhelps improve models' accuracy especially when provided with the image\nmodality, but also highlights the need for better training techniques to\nenhance the information fusion among modalities.", "published": "2024-10-02 16:46:01", "link": "http://arxiv.org/abs/2410.01733v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Quantifying Generalization Complexity for Large Language Models", "abstract": "While large language models (LLMs) have shown exceptional capabilities in\nunderstanding complex queries and performing sophisticated tasks, their\ngeneralization abilities are often deeply entangled with memorization,\nnecessitating more precise evaluation. To address this challenge, we introduce\nScylla, a dynamic evaluation framework that quantitatively measures the\ngeneralization abilities of LLMs. Scylla disentangles generalization from\nmemorization via assessing model performance on both in-distribution (ID) and\nout-of-distribution (OOD) data through 20 tasks across 5 levels of complexity.\nThrough extensive experiments, we uncover a non-monotonic relationship between\ntask complexity and the performance gap between ID and OOD data, which we term\nthe generalization valley. Specifically, this phenomenon reveals a critical\nthreshold - referred to as critical complexity - where reliance on\nnon-generalizable behavior peaks, indicating the upper bound of LLMs'\ngeneralization capabilities. As model size increases, the critical complexity\nshifts toward higher levels of task complexity, suggesting that larger models\ncan handle more complex reasoning tasks before over-relying on memorization.\nLeveraging Scylla and the concept of critical complexity, we benchmark 28LLMs\nincluding both open-sourced models such as LLaMA and Qwen families, and\nclose-sourced models like Claude and GPT, providing a more robust evaluation\nand establishing a clearer understanding of LLMs' generalization capabilities.", "published": "2024-10-02 17:25:37", "link": "http://arxiv.org/abs/2410.01769v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Loki: An Open-Source Tool for Fact Verification", "abstract": "We introduce Loki, an open-source tool designed to address the growing\nproblem of misinformation. Loki adopts a human-centered approach, striking a\nbalance between the quality of fact-checking and the cost of human involvement.\nIt decomposes the fact-checking task into a five-step pipeline: breaking down\nlong texts into individual claims, assessing their check-worthiness, generating\nqueries, retrieving evidence, and verifying the claims. Instead of fully\nautomating the claim verification process, Loki provides essential information\nat each step to assist human judgment, especially for general users such as\njournalists and content moderators. Moreover, it has been optimized for\nlatency, robustness, and cost efficiency at a commercially usable level. Loki\nis released under an MIT license and is available on GitHub. We also provide a\nvideo presenting the system and its capabilities.", "published": "2024-10-02 17:52:41", "link": "http://arxiv.org/abs/2410.01794v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Locret: Enhancing Eviction in Long-Context LLM Inference with Trained\n  Retaining Heads on Consumer-Grade Devices", "abstract": "Scaling the input context length of a large language model (LLM) incurs a\nsignificant increase in computation cost and memory footprint to maintain the\nattention key-value (KV) cache. Existing KV cache compression methods suffer\nfrom inefficient compression strategies and limited memory reduction effects,\nmaking it difficult for LLMs to conduct long-context inference on\nconsumer-grade devices, especially when inferring long-context stream input.\nSuch obstacles prevent consumer-grade devices from supporting more complex\napplications, creating challenges for the democratization of LLMs. To overcome\nthis, we propose Locret, the first framework to create an eviction policy\ncompatible with chunked prefill. By evaluating the causal importance of KV\ncache units by learnable retaining heads, Locret enables precise eviction of\ncache units, facilitating efficient long-context inference. In our extensive\nempirical studies, Locret outperforms the recent popular and competitive\napproaches in terms of memory efficiency and generation quality -- Locret\nachieves up to 20x of KV cache compression ratio within less than 10%\nperformance loss. Furthermore, Locret achieves 128K+ long-context inference on\na single NVIDIA 4090 GPU without compromising generation quality and only costs\n<1 GPU hour of additional training.", "published": "2024-10-02 17:59:52", "link": "http://arxiv.org/abs/2410.01805v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CALF: Benchmarking Evaluation of LFQA Using Chinese Examinations", "abstract": "Long-Form Question Answering (LFQA) refers to generating in-depth,\nparagraph-level responses to open-ended questions. Although lots of LFQA\nmethods are developed, evaluating LFQA effectively and efficiently remains\nchallenging due to its high complexity and cost. Therefore, there is no\nstandard benchmark for LFQA evaluation till now. To address this gap, we make\nthe first attempt by proposing a well-constructed, reference-based benchmark\nnamed Chinese exAmination for LFQA Evaluation (CALF), aiming to rigorously\nassess the performance of automatic evaluation metrics for LFQA. The CALF\nbenchmark is derived from Chinese examination questions that have been\ntranslated into English. It includes up to 1476 examples consisting of\nknowledge-intensive and nuanced responses. Our evaluation comprises three\ndifferent settings to ana lyze the behavior of automatic metrics\ncomprehensively. We conducted extensive experiments on 7 traditional evaluation\nmetrics, 3 prompt-based metrics, and 3 trained evaluation metrics, and tested\non agent systems for the LFQA evaluation. The results reveal that none of the\ncurrent automatic evaluation metrics shows comparable performances with humans,\nindicating that they cannot capture dense information contained in long-form\nresponses well. In addition, we provide a detailed analysis of the reasons why\nautomatic evaluation metrics fail when evaluating LFQA, offering valuable\ninsights to advance LFQA evaluation systems. Dataset and associated codes can\nbe accessed at our GitHub repository.", "published": "2024-10-02 18:44:10", "link": "http://arxiv.org/abs/2410.01945v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SciPrompt: Knowledge-augmented Prompting for Fine-grained Categorization\n  of Scientific Topics", "abstract": "Prompt-based fine-tuning has become an essential method for eliciting\ninformation encoded in pre-trained language models for a variety of tasks,\nincluding text classification. For multi-class classification tasks,\nprompt-based fine-tuning under low-resource scenarios has resulted in\nperformance levels comparable to those of fully fine-tuning methods. Previous\nstudies have used crafted prompt templates and verbalizers, mapping from the\nlabel terms space to the class space, to solve the classification problem as a\nmasked language modeling task. However, cross-domain and fine-grained\nprompt-based fine-tuning with an automatically enriched verbalizer remains\nunexplored, mainly due to the difficulty and costs of manually selecting domain\nlabel terms for the verbalizer, which requires humans with domain expertise. To\naddress this challenge, we introduce SciPrompt, a framework designed to\nautomatically retrieve scientific topic-related terms for low-resource text\nclassification tasks. To this end, we select semantically correlated and\ndomain-specific label terms within the context of scientific literature for\nverbalizer augmentation. Furthermore, we propose a new verbalization strategy\nthat uses correlation scores as additional weights to enhance the prediction\nperformance of the language model during model tuning. Our method outperforms\nstate-of-the-art, prompt-based fine-tuning methods on scientific text\nclassification tasks under few and zero-shot settings, especially in\nclassifying fine-grained and emerging scientific topics.", "published": "2024-10-02 18:45:04", "link": "http://arxiv.org/abs/2410.01946v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TypedThinker: Typed Thinking Improves Large Language Model Reasoning", "abstract": "Despite significant advancements in the reasoning capabilities of Large\nLanguage Models (LLMs), the lack of diverse reasoning solutions often makes\nthem trapped in a limited solution search area. In this paper, we propose\nTypedThinker, a novel framework that enhances LLMs' problem-solving abilities\nby incorporating multiple reasoning types (deductive, inductive, abductive, and\nanalogical). Our analysis across four benchmarks reveals that different\nreasoning types uniquely solve distinct sets of problems, highlighting the\nimportance of diverse thinking approaches. TypedThinker addresses two key\nchallenges: selecting appropriate reasoning types for given problems and\neffectively implementing specific reasoning types. Through self-training on\nsuccessful experiences, TypedThinker learns an implicit policy for reasoning\ntype selection and application. Experimental results demonstrate significant\nimprovements over baseline models, with accuracy increases of 3.4% for Mistral\n7B and 16.7% for LLaMA3 8B across four reasoning benchmarks. Notably,\nTypedThinker shows effective generalization to new benchmarks and can further\nenhance the reasoning capability of powerful models like GPT-4o. The code is\nreleased at https://github.com/dqwang122/ThinkHub.", "published": "2024-10-02 18:54:45", "link": "http://arxiv.org/abs/2410.01952v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generate then Refine: Data Augmentation for Zero-shot Intent Detection", "abstract": "In this short paper we propose a data augmentation method for intent\ndetection in zero-resource domains. Existing data augmentation methods rely on\nfew labelled examples for each intent category, which can be expensive in\nsettings with many possible intents. We use a two-stage approach: First, we\ngenerate utterances for intent labels using an open-source large language model\nin a zero-shot setting. Second, we develop a smaller sequence-to-sequence model\n(the Refiner), to improve the generated utterances. The Refiner is fine-tuned\non seen domains and then applied to unseen domains. We evaluate our method by\ntraining an intent classifier on the generated data, and evaluating it on real\n(human) data. We find that the Refiner significantly improves the data utility\nand diversity over the zero-shot LLM baseline for unseen domains and over\ncommon baseline approaches. Our results indicate that a two-step approach of a\ngenerative LLM in zero-shot setting and a smaller sequence-to-sequence model\ncan provide high-quality data for intent detection.", "published": "2024-10-02 18:56:03", "link": "http://arxiv.org/abs/2410.01953v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Reliable Is Human Feedback For Aligning Large Language Models?", "abstract": "Most alignment research today focuses on designing new learning algorithms\nusing datasets like Anthropic-HH, assuming human feedback data is inherently\nreliable. However, little attention has been given to the qualitative\nunreliability of human feedback and its impact on alignment. To address this\ngap, we conduct a comprehensive study and provide an in-depth analysis of human\nfeedback data. We assess feedback reliability using a committee of gold reward\nmodels, revealing that over 25% of the dataset shows low or no agreement with\nthese models, implying a high degree of unreliability. Through a qualitative\nanalysis, we identify six key sources of unreliability, such as mis-labeling,\nsubjective preferences, differing criteria and thresholds for helpfulness and\nharmlessness, etc. Lastly, to mitigate unreliability, we propose Source-Aware\nCleaning, an automatic data-cleaning method guided by the insight of our\nqualitative analysis, to significantly improve data quality. Extensive\nexperiments demonstrate that models trained on our cleaned dataset, HH-Clean,\nsubstantially outperform those trained on the original dataset. We release\nHH-Clean to support more reliable LLM alignment evaluation in the future.", "published": "2024-10-02 19:03:42", "link": "http://arxiv.org/abs/2410.01957v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are Large Language Models Good Classifiers? A Study on Edit Intent\n  Classification in Scientific Document Revisions", "abstract": "Classification is a core NLP task architecture with many potential\napplications. While large language models (LLMs) have brought substantial\nadvancements in text generation, their potential for enhancing classification\ntasks remains underexplored. To address this gap, we propose a framework for\nthoroughly investigating fine-tuning LLMs for classification, including both\ngeneration- and encoding-based approaches. We instantiate this framework in\nedit intent classification (EIC), a challenging and underexplored\nclassification task. Our extensive experiments and systematic comparisons with\nvarious training approaches and a representative selection of LLMs yield new\ninsights into their application for EIC. We investigate the generalizability of\nthese findings on five further classification tasks. To demonstrate the\nproposed methods and address the data shortage for empirical edit analysis, we\nuse our best-performing EIC model to create Re3-Sci2.0, a new large-scale\ndataset of 1,780 scientific document revisions with over 94k labeled edits. The\nquality of the dataset is assessed through human evaluation. The new dataset\nenables an in-depth empirical study of human editing behavior in academic\nwriting. We make our experimental framework, models and data publicly\navailable.", "published": "2024-10-02 20:48:28", "link": "http://arxiv.org/abs/2410.02028v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Racing Thoughts: Explaining Large Language Model Contextualization\n  Errors", "abstract": "The profound success of transformer-based language models can largely be\nattributed to their ability to integrate relevant contextual information from\nan input sequence in order to generate a response or complete a task. However,\nwe know very little about the algorithms that a model employs to implement this\ncapability, nor do we understand their failure modes. For example, given the\nprompt \"John is going fishing, so he walks over to the bank. Can he make an ATM\ntransaction?\", a model may incorrectly respond \"Yes\" if it has not properly\ncontextualized \"bank\" as a geographical feature, rather than a financial\ninstitution. We propose the LLM Race Conditions Hypothesis as an explanation of\ncontextualization errors of this form. This hypothesis identifies dependencies\nbetween tokens (e.g., \"bank\" must be properly contextualized before the final\ntoken, \"?\", integrates information from \"bank\"), and claims that\ncontextualization errors are a result of violating these dependencies. Using a\nvariety of techniques from mechanistic intepretability, we provide\ncorrelational and causal evidence in support of the hypothesis, and suggest\ninference-time interventions to address it.", "published": "2024-10-02 23:46:48", "link": "http://arxiv.org/abs/2410.02102v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Deduplication Techniques for Economic Research Paper Titles\n  with a Focus on Semantic Similarity using NLP and LLMs", "abstract": "This study investigates efficient deduplication techniques for a large NLP\ndataset of economic research paper titles. We explore various pairing methods\nalongside established distance measures (Levenshtein distance, cosine\nsimilarity) and a sBERT model for semantic evaluation. Our findings suggest a\npotentially low prevalence of duplicates based on the observed semantic\nsimilarity across different methods. Further exploration with a human-annotated\nground truth set is completed for a more conclusive assessment. The result\nsupports findings from the NLP, LLM based distance metrics.", "published": "2024-10-02 00:43:10", "link": "http://arxiv.org/abs/2410.01141v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unleashing the Power of Large Language Models in Zero-shot Relation\n  Extraction via Self-Prompting", "abstract": "Recent research in zero-shot Relation Extraction (RE) has focused on using\nLarge Language Models (LLMs) due to their impressive zero-shot capabilities.\nHowever, current methods often perform suboptimally, mainly due to a lack of\ndetailed, context-specific prompts needed for understanding various sentences\nand relations. To address this, we introduce the Self-Prompting framework, a\nnovel method designed to fully harness the embedded RE knowledge within LLMs.\nSpecifically, our framework employs a three-stage diversity approach to prompt\nLLMs, generating multiple synthetic samples that encapsulate specific relations\nfrom scratch. These generated samples act as in-context learning samples,\noffering explicit and context-specific guidance to efficiently prompt LLMs for\nRE. Experimental evaluations on benchmark datasets show our approach\noutperforms existing LLM-based zero-shot RE methods. Additionally, our\nexperiments confirm the effectiveness of our generation pipeline in producing\nhigh-quality synthetic data that enhances performance.", "published": "2024-10-02 01:12:54", "link": "http://arxiv.org/abs/2410.01154v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Towards Inference-time Category-wise Safety Steering for Large Language\n  Models", "abstract": "While large language models (LLMs) have seen unprecedented advancements in\ncapabilities and applications across a variety of use-cases, safety alignment\nof these models is still an area of active research. The fragile nature of\nLLMs, even models that have undergone extensive alignment and safety training\nregimes, warrants additional safety steering steps via training-free,\ninference-time methods. While recent work in the area of mechanistic\ninterpretability has investigated how activations in latent representation\nspaces may encode concepts, and thereafter performed representation engineering\nto induce such concepts in LLM outputs, the applicability of such for safety is\nrelatively under-explored. Unlike recent inference-time safety steering works,\nin this paper we explore safety steering of LLM outputs using: (i)\ncategory-specific steering vectors, thereby enabling fine-grained control over\nthe steering, and (ii) sophisticated methods for extracting informative\nsteering vectors for more effective safety steering while retaining quality of\nthe generated text. We demonstrate our exploration on multiple LLMs and\ndatasets, and showcase the effectiveness of the proposed steering method, along\nwith a discussion on the implications and best practices.", "published": "2024-10-02 02:02:06", "link": "http://arxiv.org/abs/2410.01174v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "UAL-Bench: The First Comprehensive Unusual Activity Localization\n  Benchmark", "abstract": "Localizing unusual activities, such as human errors or surveillance\nincidents, in videos holds practical significance. However, current video\nunderstanding models struggle with localizing these unusual events likely\nbecause of their insufficient representation in models' pretraining datasets.\nTo explore foundation models' capability in localizing unusual activity, we\nintroduce UAL-Bench, a comprehensive benchmark for unusual activity\nlocalization, featuring three video datasets: UAG-OOPS, UAG-SSBD, UAG-FunQA,\nand an instruction-tune dataset: OOPS-UAG-Instruct, to improve model\ncapabilities. UAL-Bench evaluates three approaches: Video-Language Models\n(Vid-LLMs), instruction-tuned Vid-LLMs, and a novel integration of\nVision-Language Models and Large Language Models (VLM-LLM). Our results show\nthe VLM-LLM approach excels in localizing short-span unusual events and\npredicting their onset (start time) more accurately than Vid-LLMs. We also\npropose a new metric, R@1, TD <= p, to address limitations in existing\nevaluation methods. Our findings highlight the challenges posed by\nlong-duration videos, particularly in autism diagnosis scenarios, and the need\nfor further advancements in localization techniques. Our work not only provides\na benchmark for unusual activity localization but also outlines the key\nchallenges for existing foundation models, suggesting future research\ndirections on this important task.", "published": "2024-10-02 02:33:09", "link": "http://arxiv.org/abs/2410.01180v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "FastLexRank: Efficient Lexical Ranking for Structuring Social Media\n  Posts", "abstract": "We present FastLexRank\\footnote{https://github.com/LiMaoUM/FastLexRank}, an\nefficient and scalable implementation of the LexRank algorithm for text\nranking. Designed to address the computational and memory complexities of the\noriginal LexRank method, FastLexRank significantly reduces time and memory\nrequirements from $\\mathcal{O}(n^2)$ to $\\mathcal{O}(n)$ without compromising\nthe quality or accuracy of the results. By employing an optimized approach to\ncalculating the stationary distribution of sentence graphs, FastLexRank\nmaintains an identical results with the original LexRank scores while enhancing\ncomputational efficiency. This paper details the algorithmic improvements that\nenable the processing of large datasets, such as social media corpora, in\nreal-time. Empirical results demonstrate its effectiveness, and we propose its\nuse in identifying central tweets, which can be further analyzed using advanced\nNLP techniques. FastLexRank offers a scalable solution for text centrality\ncalculation, addressing the growing need for efficient processing of digital\ncontent.", "published": "2024-10-02 02:34:33", "link": "http://arxiv.org/abs/2410.01183v1", "categories": ["cs.CL", "stat.CO"], "primary_category": "cs.CL"}
{"title": "Automatic deductive coding in discourse analysis: an application of\n  large language models in learning analytics", "abstract": "Deductive coding is a common discourse analysis method widely used by\nlearning science and learning analytics researchers for understanding teaching\nand learning interactions. It often requires researchers to manually label all\ndiscourses to be analyzed according to a theoretically guided coding scheme,\nwhich is time-consuming and labor-intensive. The emergence of large language\nmodels such as GPT has opened a new avenue for automatic deductive coding to\novercome the limitations of traditional deductive coding. To evaluate the\nusefulness of large language models in automatic deductive coding, we employed\nthree different classification methods driven by different artificial\nintelligence technologies, including the traditional text classification method\nwith text feature engineering, BERT-like pretrained language model and GPT-like\npretrained large language model (LLM). We applied these methods to two\ndifferent datasets and explored the potential of GPT and prompt engineering in\nautomatic deductive coding. By analyzing and comparing the accuracy and Kappa\nvalues of these three classification methods, we found that GPT with prompt\nengineering outperformed the other two methods on both datasets with limited\nnumber of training samples. By providing detailed prompt structures, the\nreported work demonstrated how large language models can be used in the\nimplementation of automatic deductive coding.", "published": "2024-10-02 05:04:06", "link": "http://arxiv.org/abs/2410.01240v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "AHP-Powered LLM Reasoning for Multi-Criteria Evaluation of Open-Ended\n  Responses", "abstract": "Question answering (QA) tasks have been extensively studied in the field of\nnatural language processing (NLP). Answers to open-ended questions are highly\ndiverse and difficult to quantify, and cannot be simply evaluated as correct or\nincorrect, unlike close-ended questions with definitive answers. While large\nlanguage models (LLMs) have demonstrated strong capabilities across various\ntasks, they exhibit relatively weaker performance in evaluating answers to\nopen-ended questions. In this study, we propose a method that leverages LLMs\nand the analytic hierarchy process (AHP) to assess answers to open-ended\nquestions. We utilized LLMs to generate multiple evaluation criteria for a\nquestion. Subsequently, answers were subjected to pairwise comparisons under\neach criterion with LLMs, and scores for each answer were calculated in the\nAHP. We conducted experiments on four datasets using both ChatGPT-3.5-turbo and\nGPT-4. Our results indicate that our approach more closely aligns with human\njudgment compared to the four baselines. Additionally, we explored the impact\nof the number of criteria, variations in models, and differences in datasets on\nthe results.", "published": "2024-10-02 05:22:07", "link": "http://arxiv.org/abs/2410.01246v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Deep Learning and Machine Learning, Advancing Big Data Analytics and\n  Management: Unveiling AI's Potential Through Tools, Techniques, and\n  Applications", "abstract": "Artificial intelligence (AI), machine learning, and deep learning have become\ntransformative forces in big data analytics and management, enabling\ngroundbreaking advancements across diverse industries. This article delves into\nthe foundational concepts and cutting-edge developments in these fields, with a\nparticular focus on large language models (LLMs) and their role in natural\nlanguage processing, multimodal reasoning, and autonomous decision-making.\nHighlighting tools such as ChatGPT, Claude, and Gemini, the discussion explores\ntheir applications in data analysis, model design, and optimization.\n  The integration of advanced algorithms like neural networks, reinforcement\nlearning, and generative models has enhanced the capabilities of AI systems to\nprocess, visualize, and interpret complex datasets. Additionally, the emergence\nof technologies like edge computing and automated machine learning (AutoML)\ndemocratizes access to AI, empowering users across skill levels to engage with\nintelligent systems. This work also underscores the importance of ethical\nconsiderations, transparency, and fairness in the deployment of AI\ntechnologies, paving the way for responsible innovation.\n  Through practical insights into hardware configurations, software\nenvironments, and real-world applications, this article serves as a\ncomprehensive resource for researchers and practitioners. By bridging\ntheoretical underpinnings with actionable strategies, it showcases the\npotential of AI and LLMs to revolutionize big data management and drive\nmeaningful advancements across domains such as healthcare, finance, and\nautonomous systems.", "published": "2024-10-02 06:24:51", "link": "http://arxiv.org/abs/2410.01268v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mitigating Copy Bias in In-Context Learning through Neuron Pruning", "abstract": "Large language models (LLMs) have demonstrated impressive few-shot in-context\nlearning (ICL) abilities. Still, we show that they are sometimes prone to a\n`copying bias', where they copy answers from provided examples instead of\nlearning the underlying patterns. In this work, we propose a novel and simple\nmethod to mitigate such copying bias. First, we create a synthetic task and use\nthe Integrated Gradients method to identify neurons that prioritize copying\nover generalization. We demonstrate that pruning these neurons consistently\nimproves performance across a diverse set of ICL tasks. We also show that our\nmethod is applicable across various LLM architectures, including Transformers\nand State-Space Models, without requiring modifications. In our analysis, we\nadopt a task-recognition perspective on ICL and examine task vectors (Hendel et\nal., 2023) induced by the model. We find that pruning enhances the quality of\nthese vectors, suggesting that the pruned neurons previously hindered effective\ntask recognition.", "published": "2024-10-02 07:18:16", "link": "http://arxiv.org/abs/2410.01288v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Revisiting Hierarchical Text Classification: Inference and Metrics", "abstract": "Hierarchical text classification (HTC) is the task of assigning labels to a\ntext within a structured space organized as a hierarchy. Recent works treat HTC\nas a conventional multilabel classification problem, therefore evaluating it as\nsuch. We instead propose to evaluate models based on specifically designed\nhierarchical metrics and we demonstrate the intricacy of metric choice and\nprediction inference method. We introduce a new challenging dataset and we\nevaluate fairly, recent sophisticated models, comparing them with a range of\nsimple but strong baselines, including a new theoretically motivated loss.\nFinally, we show that those baselines are very often competitive with the\nlatest models. This highlights the importance of carefully considering the\nevaluation methodology when proposing new methods for HTC. Code implementation\nand dataset are available at \\url{https://github.com/RomanPlaud/revisitingHTC}.", "published": "2024-10-02 07:57:33", "link": "http://arxiv.org/abs/2410.01305v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unveiling Language Skills via Path-Level Circuit Discovery", "abstract": "Circuit discovery with edge-level ablation has become a foundational\nframework for mechanism interpretability of language models. However, its focus\non individual edges often overlooks the sequential, path-level causal\nrelationships that underpin complex behaviors, thus potentially leading to\nmisleading or incomplete circuit discoveries. To address this issue, we propose\na novel path-level circuit discovery framework capturing how behaviors emerge\nthrough interconnected linear chain and build towards complex behaviors. Our\nframework is constructed upon a fully-disentangled linear combinations of\n``memory circuits'' decomposed from the original model. To discover functional\ncircuit paths, we leverage a 2-step pruning strategy by first reducing the\ncomputational graph to a faithful and minimal subgraph and then applying causal\nmediation to identify common paths of a specific skill, termed as skill paths.\nIn contrast to circuit graph from existing works, we focus on the complete\npaths of a generic skill rather than on the fine-grained responses to\nindividual components of the input. To demonstrate this, we explore three\ngeneric language skills, namely Previous Token Skill, Induction Skill and\nIn-Context Learning Skill using our framework and provide more compelling\nevidence to substantiate stratification and inclusiveness of these skills.", "published": "2024-10-02 08:52:58", "link": "http://arxiv.org/abs/2410.01334v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PCQPR: Proactive Conversational Question Planning with Reflection", "abstract": "Conversational Question Generation (CQG) enhances the interactivity of\nconversational question-answering systems in fields such as education, customer\nservice, and entertainment. However, traditional CQG, focusing primarily on the\nimmediate context, lacks the conversational foresight necessary to guide\nconversations toward specified conclusions. This limitation significantly\nrestricts their ability to achieve conclusion-oriented conversational outcomes.\nIn this work, we redefine the CQG task as Conclusion-driven Conversational\nQuestion Generation (CCQG) by focusing on proactivity, not merely reacting to\nthe unfolding conversation but actively steering it towards a\nconclusion-oriented question-answer pair. To address this, we propose a novel\napproach, called Proactive Conversational Question Planning with self-Refining\n(PCQPR). Concretely, by integrating a planning algorithm inspired by Monte\nCarlo Tree Search (MCTS) with the analytical capabilities of large language\nmodels (LLMs), PCQPR predicts future conversation turns and continuously\nrefines its questioning strategies. This iterative self-refining mechanism\nensures the generation of contextually relevant questions strategically devised\nto reach a specified outcome. Our extensive evaluations demonstrate that PCQPR\nsignificantly surpasses existing CQG methods, marking a paradigm shift towards\nconclusion-oriented conversational question-answering systems.", "published": "2024-10-02 09:23:07", "link": "http://arxiv.org/abs/2410.01363v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Knowledge Entropy Decay during Language Model Pretraining Hinders New\n  Knowledge Acquisition", "abstract": "In this work, we investigate how a model's tendency to broadly integrate its\nparametric knowledge evolves throughout pretraining, and how this behavior\naffects overall performance, particularly in terms of knowledge acquisition and\nforgetting. We introduce the concept of knowledge entropy, which quantifies the\nrange of memory sources the model engages with; high knowledge entropy\nindicates that the model utilizes a wide range of memory sources, while low\nknowledge entropy suggests reliance on specific sources with greater certainty.\nOur analysis reveals a consistent decline in knowledge entropy as pretraining\nadvances. We also find that the decline is closely associated with a reduction\nin the model's ability to acquire and retain knowledge, leading us to conclude\nthat diminishing knowledge entropy (smaller number of active memory sources)\nimpairs the model's knowledge acquisition and retention capabilities. We find\nfurther support for this by demonstrating that increasing the activity of\ninactive memory sources enhances the model's capacity for knowledge acquisition\nand retention.", "published": "2024-10-02 09:49:45", "link": "http://arxiv.org/abs/2410.01380v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PairDistill: Pairwise Relevance Distillation for Dense Retrieval", "abstract": "Effective information retrieval (IR) from vast datasets relies on advanced\ntechniques to extract relevant information in response to queries. Recent\nadvancements in dense retrieval have showcased remarkable efficacy compared to\ntraditional sparse retrieval methods. To further enhance retrieval performance,\nknowledge distillation techniques, often leveraging robust cross-encoder\nrerankers, have been extensively explored. However, existing approaches\nprimarily distill knowledge from pointwise rerankers, which assign absolute\nrelevance scores to documents, thus facing challenges related to inconsistent\ncomparisons. This paper introduces Pairwise Relevance Distillation\n(PairDistill) to leverage pairwise reranking, offering fine-grained\ndistinctions between similarly relevant documents to enrich the training of\ndense retrieval models. Our experiments demonstrate that PairDistill\noutperforms existing methods, achieving new state-of-the-art results across\nmultiple benchmarks. This highlights the potential of PairDistill in advancing\ndense retrieval techniques effectively. Our source code and trained models are\nreleased at https://github.com/MiuLab/PairDistill", "published": "2024-10-02 09:51:42", "link": "http://arxiv.org/abs/2410.01383v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Circuit Compositions: Exploring Modular Structures in Transformer-Based\n  Language Models", "abstract": "A fundamental question in interpretability research is to what extent neural\nnetworks, particularly language models, implement reusable functions through\nsubnetworks that can be composed to perform more complex tasks. Recent advances\nin mechanistic interpretability have made progress in identifying\n$\\textit{circuits}$, which represent the minimal computational subgraphs\nresponsible for a model's behavior on specific tasks. However, most studies\nfocus on identifying circuits for individual tasks without investigating how\nfunctionally similar circuits $\\textit{relate}$ to each other. To address this\ngap, we study the modularity of neural networks by analyzing circuits for\nhighly compositional subtasks within a transformer-based language model.\nSpecifically, given a probabilistic context-free grammar, we identify and\ncompare circuits responsible for ten modular string-edit operations. Our\nresults indicate that functionally similar circuits exhibit both notable node\noverlap and cross-task faithfulness. Moreover, we demonstrate that the circuits\nidentified can be reused and combined through set operations to represent more\ncomplex functional model capabilities.", "published": "2024-10-02 11:36:45", "link": "http://arxiv.org/abs/2410.01434v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Agent-Driven Large Language Models for Mandarin Lyric Generation", "abstract": "Generative Large Language Models have shown impressive in-context learning\nabilities, performing well across various tasks with just a prompt. Previous\nmelody-to-lyric research has been limited by scarce high-quality aligned data\nand unclear standard for creativeness. Most efforts focused on general themes\nor emotions, which are less valuable given current language model capabilities.\nIn tonal contour languages like Mandarin, pitch contours are influenced by both\nmelody and tone, leading to variations in lyric-melody fit. Our study,\nvalidated by the Mpop600 dataset, confirms that lyricists and melody writers\nconsider this fit during their composition process. In this research, we\ndeveloped a multi-agent system that decomposes the melody-to-lyric task into\nsub-tasks, with each agent controlling rhyme, syllable count, lyric-melody\nalignment, and consistency. Listening tests were conducted via a\ndiffusion-based singing voice synthesizer to evaluate the quality of lyrics\ngenerated by different agent groups.", "published": "2024-10-02 12:01:32", "link": "http://arxiv.org/abs/2410.01450v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Disentangling Latent Shifts of In-Context Learning Through Self-Training", "abstract": "In-context learning (ICL) has become essential in natural language\nprocessing, particularly with autoregressive large language models capable of\nlearning from demonstrations provided within the prompt. However, ICL faces\nchallenges with stability and long contexts, especially as the number of\ndemonstrations grows, leading to poor generalization and inefficient inference.\nTo address these issues, we introduce STICL (Self-Training ICL), an approach\nthat disentangles the latent shifts of demonstrations from the latent shift of\nthe query through self-training. STICL employs a teacher model to generate\npseudo-labels and trains a student model using these labels, encoded in an\nadapter module. The student model exhibits weak-to-strong generalization,\nprogressively refining its predictions over time. Our empirical results show\nthat STICL improves generalization and stability, consistently outperforming\ntraditional ICL methods and other disentangling strategies across both\nin-domain and out-of-domain data.", "published": "2024-10-02 13:00:21", "link": "http://arxiv.org/abs/2410.01508v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "InstaTrans: An Instruction-Aware Translation Framework for Non-English\n  Instruction Datasets", "abstract": "It is challenging to generate high-quality instruction datasets for\nnon-English languages due to tail phenomena, which limit performance on less\nfrequently observed data. To mitigate this issue, we propose translating\nexisting high-quality English instruction datasets as a solution, emphasizing\nthe need for complete and instruction-aware translations to maintain the\ninherent attributes of these datasets. We claim that fine-tuning LLMs with\ndatasets translated in this way can improve their performance in the target\nlanguage. To this end, we introduces a new translation framework tailored for\ninstruction datasets, named InstaTrans (INSTruction-Aware TRANSlation). Through\nextensive experiments, we demonstrate the superiority of InstaTrans over other\ncompetitors in terms of completeness and instruction-awareness of translation,\nhighlighting its potential to broaden the accessibility of LLMs across diverse\nlanguages at a relatively low cost. Furthermore, we have validated that\nfine-tuning LLMs with datasets translated by InstaTrans can effectively improve\ntheir performance in the target language.", "published": "2024-10-02 13:02:23", "link": "http://arxiv.org/abs/2410.01512v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "InfiniPot: Infinite Context Processing on Memory-Constrained LLMs", "abstract": "Handling long input contexts remains a significant challenge for Large\nLanguage Models (LLMs), particularly in resource-constrained environments such\nas mobile devices. Our work aims to address this limitation by introducing\nInfiniPot, a novel KV cache control framework designed to enable pre-trained\nLLMs to manage extensive sequences within fixed memory constraints efficiently,\nwithout requiring additional training. InfiniPot leverages Continual Context\nDistillation (CCD), an iterative process that compresses and retains essential\ninformation through novel importance metrics, effectively maintaining critical\ndata even without access to future context. Our comprehensive evaluations\nindicate that InfiniPot significantly outperforms models trained for long\ncontexts in various NLP tasks, establishing its efficacy and versatility. This\nwork represents a substantial advancement toward making LLMs applicable to a\nbroader range of real-world scenarios.", "published": "2024-10-02 13:09:41", "link": "http://arxiv.org/abs/2410.01518v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "HarmAug: Effective Data Augmentation for Knowledge Distillation of\n  Safety Guard Models", "abstract": "Safety guard models that detect malicious queries aimed at large language\nmodels (LLMs) are essential for ensuring the secure and responsible deployment\nof LLMs in real-world applications. However, deploying existing safety guard\nmodels with billions of parameters alongside LLMs on mobile devices is\nimpractical due to substantial memory requirements and latency. To reduce this\ncost, we distill a large teacher safety guard model into a smaller one using a\nlabeled dataset of instruction-response pairs with binary harmfulness labels.\nDue to the limited diversity of harmful instructions in the existing labeled\ndataset, naively distilled models tend to underperform compared to larger\nmodels. To bridge the gap between small and large models, we propose HarmAug, a\nsimple yet effective data augmentation method that involves jailbreaking an LLM\nand prompting it to generate harmful instructions. Given a prompt such as,\n\"Make a single harmful instruction prompt that would elicit offensive content\",\nwe add an affirmative prefix (e.g., \"I have an idea for a prompt:\") to the\nLLM's response. This encourages the LLM to continue generating the rest of the\nresponse, leading to sampling harmful instructions. Another LLM generates a\nresponse to the harmful instruction, and the teacher model labels the\ninstruction-response pair. We empirically show that our HarmAug outperforms\nother relevant baselines. Moreover, a 435-million-parameter safety guard model\ntrained with HarmAug achieves an F1 score comparable to larger models with over\n7 billion parameters, and even outperforms them in AUPRC, while operating at\nless than 25% of their computational cost.", "published": "2024-10-02 13:12:13", "link": "http://arxiv.org/abs/2410.01524v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "In-Context Transfer Learning: Demonstration Synthesis by Transferring\n  Similar Tasks", "abstract": "In-context learning (ICL) is an effective approach to help large language\nmodels (LLMs) adapt to various tasks by providing demonstrations of the target\ntask. Considering the high cost of labeling demonstrations, many methods\npropose synthesizing demonstrations from scratch using LLMs. However, the\nquality of the demonstrations synthesized from scratch is limited by the\ncapabilities and knowledge of LLMs. To address this, inspired by transfer\nlearning, we propose In-Context Transfer Learning (ICTL), which synthesizes\ntarget task demonstrations by transferring labeled demonstrations from similar\nsource tasks. ICTL consists of two steps: source sampling and target transfer.\nFirst, we define an optimization objective, which minimizes transfer error to\nsample source demonstrations similar to the target task. Then, we employ LLMs\nto transfer the sampled source demonstrations to the target task, matching the\ndefinition and format of the target task. Experiments on Super-NI show that\nICTL outperforms synthesis from scratch by 2.0% on average, demonstrating the\neffectiveness of our method.", "published": "2024-10-02 13:37:54", "link": "http://arxiv.org/abs/2410.01548v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MedQA-CS: Benchmarking Large Language Models Clinical Skills Using an\n  AI-SCE Framework", "abstract": "Artificial intelligence (AI) and large language models (LLMs) in healthcare\nrequire advanced clinical skills (CS), yet current benchmarks fail to evaluate\nthese comprehensively. We introduce MedQA-CS, an AI-SCE framework inspired by\nmedical education's Objective Structured Clinical Examinations (OSCEs), to\naddress this gap. MedQA-CS evaluates LLMs through two instruction-following\ntasks, LLM-as-medical-student and LLM-as-CS-examiner, designed to reflect real\nclinical scenarios. Our contributions include developing MedQA-CS, a\ncomprehensive evaluation framework with publicly available data and expert\nannotations, and providing the quantitative and qualitative assessment of LLMs\nas reliable judges in CS evaluation. Our experiments show that MedQA-CS is a\nmore challenging benchmark for evaluating clinical skills than traditional\nmultiple-choice QA benchmarks (e.g., MedQA). Combined with existing benchmarks,\nMedQA-CS enables a more comprehensive evaluation of LLMs' clinical capabilities\nfor both open- and closed-source LLMs.", "published": "2024-10-02 13:47:17", "link": "http://arxiv.org/abs/2410.01553v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "ACE: A LLM-based Negotiation Coaching System", "abstract": "The growing prominence of LLMs has led to an increase in the development of\nAI tutoring systems. These systems are crucial in providing underrepresented\npopulations with improved access to valuable education. One important area of\neducation that is unavailable to many learners is strategic bargaining related\nto negotiation. To address this, we develop a LLM-based Assistant for Coaching\nnEgotiation (ACE). ACE not only serves as a negotiation partner for users but\nalso provides them with targeted feedback for improvement. To build our system,\nwe collect a dataset of negotiation transcripts between MBA students. These\ntranscripts come from trained negotiators and emulate realistic bargaining\nscenarios. We use the dataset, along with expert consultations, to design an\nannotation scheme for detecting negotiation mistakes. ACE employs this scheme\nto identify mistakes and provide targeted feedback to users. To test the\neffectiveness of ACE-generated feedback, we conducted a user experiment with\ntwo consecutive trials of negotiation and found that it improves negotiation\nperformances significantly compared to a system that doesn't provide feedback\nand one which uses an alternative method of providing feedback.", "published": "2024-10-02 13:52:09", "link": "http://arxiv.org/abs/2410.01555v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Spoken Grammar Assessment Using LLM", "abstract": "Spoken language assessment (SLA) systems restrict themselves to evaluating\nthe pronunciation and oral fluency of a speaker by analysing the read and\nspontaneous spoken utterances respectively. The assessment of language grammar\nor vocabulary is relegated to written language assessment (WLA) systems. Most\nWLA systems present a set of sentences from a curated finite-size database of\nsentences thereby making it possible to anticipate the test questions and train\noneself. In this paper, we propose a novel end-to-end SLA system to assess\nlanguage grammar from spoken utterances thus making WLA systems redundant;\nadditionally, we make the assessment largely unteachable by employing a large\nlanguage model (LLM) to bring in variations in the test. We further demonstrate\nthat a hybrid automatic speech recognition (ASR) with a custom-built language\nmodel outperforms the state-of-the-art ASR engine for spoken grammar\nassessment.", "published": "2024-10-02 14:15:13", "link": "http://arxiv.org/abs/2410.01579v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ENTP: Encoder-only Next Token Prediction", "abstract": "Next-token prediction is conventionally done using decoder-only Transformers\nwith causal attention, as this approach allows for efficient reuse of keys and\nvalues. What if we were not compute-limited, should we still use decoder-only\nTransformers? In this work, we introduce Encoder-only Next Token Prediction\n(ENTP). We explore the differences between ENTP and decoder-only Transformers\nin expressive power and complexity, highlighting potential advantages of ENTP\nin settings with unbounded compute. We introduce the $\\operatorname{Count3}$\ntask and show, both theoretically and experimentally, that while ENTP can\nperform this task easily, a decoder-only Transformer cannot. Finally, we\nempirically demonstrate the superior performance of ENTP across representative\ntasks where next-token prediction based Transformers can be evaluated,\nincluding addition, in-context learning, and language modeling.", "published": "2024-10-02 14:39:13", "link": "http://arxiv.org/abs/2410.01600v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A Thematic Framework for Analyzing Large-scale Self-reported Social\n  Media Data on Opioid Use Disorder Treatment Using Buprenorphine Product", "abstract": "Background: One of the key FDA-approved medications for Opioid Use Disorder\n(OUD) is buprenorphine. Despite its popularity, individuals often report\nvarious information needs regarding buprenorphine treatment on social media\nplatforms like Reddit. However, the key challenge is to characterize these\nneeds. In this study, we propose a theme-based framework to curate and analyze\nlarge-scale data from social media to characterize self-reported treatment\ninformation needs (TINs).\n  Methods: We collected 15,253 posts from r/Suboxone, one of the largest Reddit\nsub-community for buprenorphine products. Following the standard protocol, we\nfirst identified and defined five main themes from the data and then coded\n6,000 posts based on these themes, where one post can be labeled with\napplicable one to three themes. Finally, we determined the most frequently\nappearing sub-themes (topics) for each theme by analyzing samples from each\ngroup.\n  Results: Among the 6,000 posts, 40.3% contained a single theme, 36% two\nthemes, and 13.9% three themes. The most frequent topics for each theme or\ntheme combination came with several key findings - prevalent reporting of\npsychological and physical effects during recovery, complexities in accessing\nbuprenorphine, and significant information gaps regarding medication\nadministration, tapering, and usage of substances during different stages of\nrecovery. Moreover, self-treatment strategies and peer-driven advice reveal\nvaluable insights and potential misconceptions.\n  Conclusions: The findings obtained using our proposed framework can inform\nbetter patient education and patient-provider communication, design systematic\ninterventions to address treatment-related misconceptions and rumors, and\nstreamline the generation of hypotheses for future research.", "published": "2024-10-02 15:04:21", "link": "http://arxiv.org/abs/2410.01633v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "On The Adaptation of Unlimiformer for Decoder-Only Transformers", "abstract": "One of the prominent issues stifling the current generation of large language\nmodels is their limited context length. Recent proprietary models such as GPT-4\nand Claude 2 have introduced longer context lengths, 8k/32k and 100k,\nrespectively; however, despite the efforts in the community, most common\nmodels, such as LLama-2, have a context length of 4k or less. Unlimiformer\n(Bertsch et al., 2023) is a recently popular vector-retrieval augmentation\nmethod that offloads cross-attention computations to a kNN index. However, its\nmain limitation is incompatibility with decoder-only transformers out of the\nbox. In this work, we explore practical considerations of adapting Unlimiformer\nto decoder-only transformers and introduce a series of modifications to\novercome this limitation. Moreover, we expand the original experimental setup\non summarization to include a new task (i.e., free-form Q&A) and an\ninstruction-tuned model (i.e., a custom 6.7B GPT model). Our results showcase\nthe effectiveness of these modifications on summarization, performing on par\nwith a model with 2x the context length. Moreover, we discuss limitations and\nfuture directions for free-form Q&A and instruction-tuned models.", "published": "2024-10-02 15:08:12", "link": "http://arxiv.org/abs/2410.01637v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Efficient Length-Generalizable Attention via Causal Retrieval for\n  Long-Context Language Modeling", "abstract": "Despite the success of Transformers, handling long contexts remains\nchallenging due to the limited length generalization and quadratic complexity\nof self-attention. Thus Transformers often require post-training with a larger\nattention window, significantly increasing computational and memory costs. In\nthis paper, we propose a novel attention mechanism based on dynamic context,\nGrouped Cross Attention (GCA), which can generalize to 1000 times the\npre-training context length while maintaining the ability to access distant\ninformation with a constant attention window size. For a given input sequence,\nwe split it into chunks and use each chunk to retrieve top-k relevant past\nchunks for subsequent text generation. Specifically, unlike most previous works\nthat use an off-the-shelf retriever, our key innovation allows the retriever to\nlearn how to retrieve past chunks that better minimize the auto-regressive loss\nof subsequent tokens in an end-to-end manner. Such a mechanism accommodates\nretrieved chunks with a fixed-size attention window to achieve long-range\ninformation access, significantly reducing computational and memory costs\nduring training and inference. Experiments show that GCA-based models achieve\nnear-perfect accuracy in passkey retrieval for 16M context lengths, which is\n1000 times the training length.", "published": "2024-10-02 15:18:34", "link": "http://arxiv.org/abs/2410.01651v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Bridging Context Gaps: Leveraging Coreference Resolution for Long\n  Contextual Understanding", "abstract": "Large language models (LLMs) have shown remarkable capabilities in natural\nlanguage processing; however, they still face difficulties when tasked with\nunderstanding lengthy contexts and executing effective question answering.\nThese challenges often arise due to the complexity and ambiguity present in\nlonger texts. To enhance the performance of LLMs in such scenarios, we\nintroduce the Long Question Coreference Adaptation (LQCA) method. This\ninnovative framework focuses on coreference resolution tailored to long\ncontexts, allowing the model to identify and manage references effectively. The\nLQCA method encompasses four key steps: resolving coreferences within\nsub-documents, computing the distances between mentions, defining a\nrepresentative mention for coreference, and answering questions through mention\nreplacement. By processing information systematically, the framework provides\neasier-to-handle partitions for LLMs, promoting better understanding.\nExperimental evaluations on a range of LLMs and datasets have yielded positive\nresults, with a notable improvements on OpenAI-o1-mini and GPT-4o models,\nhighlighting the effectiveness of leveraging coreference resolution to bridge\ncontext gaps in question answering. Our code is public at\nhttps://github.com/OceannTwT/LQCA.", "published": "2024-10-02 15:39:55", "link": "http://arxiv.org/abs/2410.01671v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Trying to be human: Linguistic traces of stochastic empathy in language\n  models", "abstract": "Differentiating between generated and human-written content is important for\nnavigating the modern world. Large language models (LLMs) are crucial drivers\nbehind the increased quality of computer-generated content. Reportedly, humans\nfind it increasingly difficult to identify whether an AI model generated a\npiece of text. Our work tests how two important factors contribute to the human\nvs AI race: empathy and an incentive to appear human. We address both aspects\nin two experiments: human participants and a state-of-the-art LLM wrote\nrelationship advice (Study 1, n=530) or mere descriptions (Study 2, n=610),\neither instructed to be as human as possible or not. New samples of humans\n(n=428 and n=408) then judged the texts' source. Our findings show that when\nempathy is required, humans excel. Contrary to expectations, instructions to\nappear human were only effective for the LLM, so the human advantage\ndiminished. Computational text analysis revealed that LLMs become more human\nbecause they may have an implicit representation of what makes a text human and\neffortlessly apply these heuristics. The model resorts to a conversational,\nself-referential, informal tone with a simpler vocabulary to mimic stochastic\nempathy. We discuss these findings in light of recent claims on the on-par\nperformance of LLMs.", "published": "2024-10-02 15:46:40", "link": "http://arxiv.org/abs/2410.01675v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "VinePPO: Unlocking RL Potential For LLM Reasoning Through Refined Credit\n  Assignment", "abstract": "Large language models (LLMs) are increasingly applied to complex reasoning\ntasks that require executing several complex steps before receiving any reward.\nProperly assigning credit to these steps is essential for enhancing model\nperformance. Proximal Policy Optimization (PPO), a state-of-the-art\nreinforcement learning (RL) algorithm used for LLM finetuning, employs value\nnetworks to tackle credit assignment. However, value networks face challenges\nin predicting the expected cumulative rewards accurately in complex reasoning\ntasks, often leading to high-variance updates and suboptimal performance. In\nthis work, we systematically evaluate the efficacy of value networks and reveal\ntheir significant shortcomings in reasoning-heavy LLM tasks, showing that they\nbarely outperform a random baseline when comparing alternative steps. To\naddress this, we propose VinePPO, a straightforward approach that leverages the\nflexibility of language environments to compute unbiased Monte Carlo-based\nestimates, bypassing the need for large value networks. Our method consistently\noutperforms PPO and other RL-free baselines across MATH and GSM8K datasets with\nfewer gradient updates (up to 9x), less wall-clock time (up to 3.0x). These\nresults emphasize the importance of accurate credit assignment in RL finetuning\nof LLM and demonstrate VinePPO's potential as a superior alternative.", "published": "2024-10-02 15:49:30", "link": "http://arxiv.org/abs/2410.01679v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "FactAlign: Long-form Factuality Alignment of Large Language Models", "abstract": "Large language models have demonstrated significant potential as the\nnext-generation information access engines. However, their reliability is\nhindered by issues of hallucination and generating non-factual content. This is\nparticularly problematic in long-form responses, where assessing and ensuring\nfactual accuracy is complex. In this paper, we address this gap by proposing\nFactAlign, a novel alignment framework designed to enhance the factuality of\nLLMs' long-form responses while maintaining their helpfulness. We introduce\nfKTO, a fine-grained, sentence-level alignment algorithm that extends the\nKahneman-Tversky Optimization (KTO) alignment method. Leveraging recent\nadvances in automatic factuality evaluation, FactAlign utilizes fine-grained\nfactuality assessments to guide the alignment process. Our experiments on\nopen-domain prompts and information-seeking questions demonstrate that\nFactAlign significantly improves the factual accuracy of LLM responses while\nalso improving their helpfulness. Further analyses identify that FactAlign is\ncapable of training LLMs to provide more information without losing factual\nprecision, thus improving the factual F1 score. Our source code, datasets, and\ntrained models are publicly available at https://github.com/MiuLab/FactAlign", "published": "2024-10-02 16:03:13", "link": "http://arxiv.org/abs/2410.01691v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "U-shaped and Inverted-U Scaling behind Emergent Abilities of Large\n  Language Models", "abstract": "Large language models (LLMs) have been shown to exhibit emergent abilities in\nsome downstream tasks, where model performance stagnates at first and then\nimproves sharply and unpredictably with scale beyond a threshold. In this work,\nwe investigate the phenomenon by grouping questions based on difficulty level\nand provide a possible explanation for emergent abilities. Specifically, we\nobserve U-shaped scaling for hard questions and inverted-U scaling followed by\nsteady improvement for easy questions. The two scaling patterns initially\noffset each other, causing stagnant overall performance. The performance starts\nto soar when the scaling pattern of easy questions reverts from inverse to\nstandard scaling, leading to emergent abilities. Based on this finding, we\npropose a simple yet effective pipeline, called Slice-and-Sandwich, to predict\nthe emergence threshold and model performance beyond the threshold. Our code is\npublicly available at https://github.com/tony10101105/ExpEmergence.", "published": "2024-10-02 16:03:49", "link": "http://arxiv.org/abs/2410.01692v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "CreDes: Causal Reasoning Enhancement and Dual-End Searching for Solving\n  Long-Range Reasoning Problems using LLMs", "abstract": "Large language models (LLMs) have demonstrated limitations in handling\ncombinatorial optimization problems involving long-range reasoning, partially\ndue to causal hallucinations and huge search space. As for causal\nhallucinations, i.e., the inconsistency between reasoning and corresponding\nstate transition, this paper introduces the Causal Relationship Enhancement\n(CRE) mechanism combining cause-effect interventions and the Individual\nTreatment Effect (ITE) to guarantee the solid causal rightness between each\nstep of reasoning and state transition. As for the long causal range and huge\nsearch space limiting the performances of existing models featuring\nsingle-direction search, a Dual-End Searching (DES) approach is proposed to\nseek solutions by simultaneously starting from both the initial and goal states\non the causal probability tree. By integrating CRE and DES (CreDes), our model\nhas realized simultaneous multi-step reasoning, circumventing the\ninefficiencies from cascading multiple one-step reasoning like the\nChain-of-Thought (CoT). Experiments demonstrate that CreDes significantly\noutperforms existing State-Of-The-Art (SOTA) solutions in long-range reasoning\ntasks in terms of both accuracy and time efficiency.", "published": "2024-10-02 16:05:01", "link": "http://arxiv.org/abs/2410.01696v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Interpretable Contrastive Monte Carlo Tree Search Reasoning", "abstract": "We propose SC-MCTS*: a novel Monte Carlo Tree Search (MCTS) reasoning\nalgorithm for Large Language Models (LLMs), significantly improves both\nreasoning accuracy and speed. Our motivation comes from: 1. Previous MCTS LLM\nreasoning works often overlooked its biggest drawback--slower speed compared to\nCoT; 2. Previous research mainly used MCTS as a tool for LLM reasoning on\nvarious tasks with limited quantitative analysis or ablation studies of its\ncomponents from reasoning interpretability perspective. 3. The reward model is\nthe most crucial component in MCTS, however previous work has rarely conducted\nin-depth study or improvement of MCTS's reward models. Thus, we conducted\nextensive ablation studies and quantitative analysis on components of MCTS,\nrevealing the impact of each component on the MCTS reasoning performance of\nLLMs. Building on this, (i) we designed a highly interpretable reward model\nbased on the principle of contrastive decoding and (ii) achieved an average\nspeed improvement of 51.9% per node using speculative decoding. Additionally,\n(iii) we improved UCT node selection strategy and backpropagation used in\nprevious works, resulting in significant performance improvement. We\noutperformed o1-mini by an average of 17.4% on the Blocksworld multi-step\nreasoning dataset using Llama-3.1-70B with SC-MCTS*. Our code is available at\nhttps://github.com/zitian-gao/SC-MCTS.", "published": "2024-10-02 16:15:31", "link": "http://arxiv.org/abs/2410.01707v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Examining the Role of Relationship Alignment in Large Language Models", "abstract": "The rapid development and deployment of Generative AI in social settings\nraise important questions about how to optimally personalize them for users\nwhile maintaining accuracy and realism. Based on a Facebook public post-comment\ndataset, this study evaluates the ability of Llama 3.0 (70B) to predict the\nsemantic tones across different combinations of a commenter's and poster's\ngender, age, and friendship closeness and to replicate these differences in\nLLM-generated comments.\n  The study consists of two parts: Part I assesses differences in semantic\ntones across social relationship categories, and Part II examines the\nsimilarity between comments generated by Llama 3.0 (70B) and human comments\nfrom Part I given public Facebook posts as input. Part I results show that\nincluding social relationship information improves the ability of a model to\npredict the semantic tone of human comments. However, Part II results show that\neven without including social context information in the prompt, LLM-generated\ncomments and human comments are equally sensitive to social context, suggesting\nthat LLMs can comprehend semantics from the original post alone. When we\ninclude all social relationship information in the prompt, the similarity\nbetween human comments and LLM-generated comments decreases. This inconsistency\nmay occur because LLMs did not include social context information as part of\ntheir training data. Together these results demonstrate the ability of LLMs to\ncomprehend semantics from the original post and respond similarly to human\ncomments, but also highlights their limitations in generalizing personalized\ncomments through prompting alone.", "published": "2024-10-02 16:16:02", "link": "http://arxiv.org/abs/2410.01708v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Auto-Demo Prompting: Leveraging Generated Outputs as Demonstrations for\n  Enhanced Batch Prompting", "abstract": "Batch prompting is a common technique in large language models (LLMs) used to\nprocess multiple inputs simultaneously, aiming to improve computational\nefficiency. However, as batch sizes increase, performance degradation often\noccurs due to the model's difficulty in handling lengthy context inputs.\nExisting methods that attempt to mitigate these issues rely solely on batch\ndata arrangement and majority voting rather than improving the design of the\nbatch prompt itself. In this paper, we address these limitations by proposing\n\"Auto-Demo Prompting,\" a novel approach that leverages the question-output\npairs from earlier questions within a batch as demonstrations for subsequent\nanswer inference. We provide a formal theoretical analysis of how Auto-Demo\nPrompting functions within the autoregressive generation process of LLMs,\nillustrating how it utilizes prior outputs to optimize the model's internal\nrepresentations. Our method effectively bridges the gap between batch prompting\nand few-shot prompting, enhancing performance with only a slight compromise in\ntoken usage. Experimental results across five NLP tasks demonstrate its\neffectiveness in mitigating performance degradation and occasionally\noutperforming single prompts. Furthermore, it opens new avenues for applying\nfew-shot learning techniques, such as demonstration selection, within batch\nprompting, making it a robust solution for real-world applications.", "published": "2024-10-02 16:34:40", "link": "http://arxiv.org/abs/2410.01724v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Automated Knowledge Concept Annotation and Question Representation\n  Learning for Knowledge Tracing", "abstract": "Knowledge tracing (KT) is a popular approach for modeling students' learning\nprogress over time, which can enable more personalized and adaptive learning.\nHowever, existing KT approaches face two major limitations: (1) they rely\nheavily on expert-defined knowledge concepts (KCs) in questions, which is\ntime-consuming and prone to errors; and (2) KT methods tend to overlook the\nsemantics of both questions and the given KCs. In this work, we address these\nchallenges and present KCQRL, a framework for automated knowledge concept\nannotation and question representation learning that can improve the\neffectiveness of any existing KT model. First, we propose an automated KC\nannotation process using large language models (LLMs), which generates question\nsolutions and then annotates KCs in each solution step of the questions.\nSecond, we introduce a contrastive learning approach to generate semantically\nrich embeddings for questions and solution steps, aligning them with their\nassociated KCs via a tailored false negative elimination approach. These\nembeddings can be readily integrated into existing KT models, replacing their\nrandomly initialized embeddings. We demonstrate the effectiveness of KCQRL\nacross 15 KT algorithms on two large real-world Math learning datasets, where\nwe achieve consistent performance improvements.", "published": "2024-10-02 16:37:19", "link": "http://arxiv.org/abs/2410.01727v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "LASeR: Learning to Adaptively Select Reward Models with Multi-Armed\n  Bandits", "abstract": "Reward Models (RMs) play a crucial role in aligning LLMs with human\npreferences, enhancing their performance by ranking outputs during inference or\niterative training. However, the degree to which an RM generalizes to new tasks\nis often not known a priori (e.g. some RMs may excel at scoring creative\nwriting vs. math reasoning). Therefore, using only one fixed RM while training\nLLMs can be suboptimal. Moreover, optimizing LLMs with multiple RMs\nsimultaneously can be prohibitively computationally-intensive and challenging\ndue to conflicting signals from different RMs, potentially degrading\nperformance. To address these challenges, we introduce LASeR (Learning to\nAdaptively Select Rewards), which iteratively trains LLMs using multiple RMs,\nselecting and utilizing the most well-suited RM for each instance to rank\noutputs and generate preference data, framed as a multi-armed bandit problem.\nOur results on commonsense and math reasoning tasks demonstrate that LASeR can\nboost iterative LLM optimization by optimizing for multiple RMs, improving the\nabsolute average accuracy of Llama-3-8B over three datasets by 2.67% over\ntraining with ensemble RM scores while also showing superior training\nefficiency (e.g., a 2x speedup). Moreover, on WildChat, a benchmark of\ninstruction-following prompts, we find that using Llama-3-8B LASeR leads to a\n71.45% AlpacaEval win rate over sequentially optimizing multiple RMs. Extending\nto long-context generation tasks, we find that on Llama-3-8B, LASeR achieves an\naverage improvement of 2.64 F1 and 2.42 F1 on single- and multi-document QA\nover random RM selection when used with best-of-n sampling. LASeR is robust to\nnoisy rewards and generalizes to multiple settings. Finally, LASeR's RM\nselection changes depending on the underlying task or instance and we verify\nthe presence of conflicting preferences from multiple RMs that can be mitigated\nusing LASeR.", "published": "2024-10-02 16:46:38", "link": "http://arxiv.org/abs/2410.01735v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Recursive Abstractive Processing for Retrieval in Dynamic Datasets", "abstract": "Recent retrieval-augmented models enhance basic methods by building a\nhierarchical structure over retrieved text chunks through recursive embedding,\nclustering, and summarization. The most relevant information is then retrieved\nfrom both the original text and generated summaries. However, such approaches\nface limitations with dynamic datasets, where adding or removing documents over\ntime complicates the updating of hierarchical representations formed through\nclustering. We propose a new algorithm to efficiently maintain the\nrecursive-abstractive tree structure in dynamic datasets, without compromising\nperformance. Additionally, we introduce a novel post-retrieval method that\napplies query-focused recursive abstractive processing to substantially improve\ncontext quality. Our method overcomes the limitations of other approaches by\nfunctioning as a black-box post-retrieval layer compatible with any retrieval\nalgorithm. Both algorithms are validated through extensive experiments on\nreal-world datasets, demonstrating their effectiveness in handling dynamic data\nand improving retrieval performance.", "published": "2024-10-02 16:47:35", "link": "http://arxiv.org/abs/2410.01736v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Leopard: A Vision Language Model For Text-Rich Multi-Image Tasks", "abstract": "Text-rich images, where text serves as the central visual element guiding the\noverall understanding, are prevalent in real-world applications, such as\npresentation slides, scanned documents, and webpage snapshots. Tasks involving\nmultiple text-rich images are especially challenging, as they require not only\nunderstanding the content of individual images but reasoning about\ninter-relationships and logical flows across multiple visual inputs. Despite\nthe importance of these scenarios, current multimodal large language models\n(MLLMs) struggle to handle such tasks due to two key challenges: (1) the\nscarcity of high-quality instruction tuning datasets for text-rich multi-image\nscenarios, and (2) the difficulty in balancing image resolution with visual\nfeature sequence length. To address these challenges, we propose Leopard, a\nMLLM designed specifically for handling vision-language tasks involving\nmultiple text-rich images. First, we curated about one million high-quality\nmultimodal instruction-tuning data, tailored to text-rich, multi-image\nscenarios. Second, we developed an adaptive high-resolution multi-image\nencoding module to dynamically optimize the allocation of visual sequence\nlength based on the original aspect ratios and resolutions of the input images.\nExperiments across a wide range of benchmarks demonstrate our model's superior\ncapabilities in text-rich, multi-image evaluations and competitive performance\nin general domain evaluations.", "published": "2024-10-02 16:55:01", "link": "http://arxiv.org/abs/2410.01744v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "DeFine: Enhancing LLM Decision-Making with Factor Profiles and\n  Analogical Reasoning", "abstract": "LLMs are ideal for decision-making due to their ability to reason over long\ncontexts and identify critical factors. However, challenges arise when\nprocessing transcripts of spoken speech describing complex scenarios. These\ntranscripts often contain ungrammatical or incomplete sentences, repetitions,\nhedging, and vagueness. For example, during a company's earnings call, an\nexecutive might project a positive revenue outlook to reassure investors,\ndespite significant uncertainty regarding future earnings. It is crucial for\nLLMs to incorporate this uncertainty systematically when making decisions. In\nthis paper, we introduce DeFine, a new framework that constructs probabilistic\nfactor profiles from complex scenarios. DeFine then integrates these profiles\nwith analogical reasoning, leveraging insights from similar past experiences to\nguide LLMs in making critical decisions in novel situations. Our framework\nseparates the tasks of quantifying uncertainty in complex scenarios and\nincorporating it into LLM decision-making. This approach is particularly useful\nin fields such as medical consultations, negotiations, and political debates,\nwhere making decisions under uncertainty is vital.", "published": "2024-10-02 17:29:34", "link": "http://arxiv.org/abs/2410.01772v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "OmniGenBench: Automating Large-scale in-silico Benchmarking for Genomic\n  Foundation Models", "abstract": "The advancements in artificial intelligence in recent years, such as Large\nLanguage Models (LLMs), have fueled expectations for breakthroughs in genomic\nfoundation models (GFMs). The code of nature, hidden in diverse genomes since\nthe very beginning of life's evolution, holds immense potential for impacting\nhumans and ecosystems through genome modeling. Recent breakthroughs in GFMs,\nsuch as Evo, have attracted significant investment and attention to genomic\nmodeling, as they address long-standing challenges and transform in-silico\ngenomic studies into automated, reliable, and efficient paradigms. In the\ncontext of this flourishing era of consecutive technological revolutions in\ngenomics, GFM studies face two major challenges: the lack of GFM benchmarking\ntools and the absence of open-source software for diverse genomics. These\nchallenges hinder the rapid evolution of GFMs and their wide application in\ntasks such as understanding and synthesizing genomes, problems that have\npersisted for decades. To address these challenges, we introduce GFMBench, a\nframework dedicated to GFM-oriented benchmarking. GFMBench standardizes\nbenchmark suites and automates benchmarking for a wide range of open-source\nGFMs. It integrates millions of genomic sequences across hundreds of genomic\ntasks from four large-scale benchmarks, democratizing GFMs for a wide range of\nin-silico genomic applications. Additionally, GFMBench is released as\nopen-source software, offering user-friendly interfaces and diverse tutorials,\napplicable for AutoBench and complex tasks like RNA design and structure\nprediction. To facilitate further advancements in genome modeling, we have\nlaunched a public leaderboard showcasing the benchmark performance derived from\nAutoBench. GFMBench represents a step toward standardizing GFM benchmarking and\ndemocratizing GFM applications.", "published": "2024-10-02 17:40:44", "link": "http://arxiv.org/abs/2410.01784v1", "categories": ["q-bio.GN", "cs.CL"], "primary_category": "q-bio.GN"}
{"title": "When a language model is optimized for reasoning, does it still show\n  embers of autoregression? An analysis of OpenAI o1", "abstract": "In \"Embers of Autoregression\" (McCoy et al., 2023), we showed that several\nlarge language models (LLMs) have some important limitations that are\nattributable to their origins in next-word prediction. Here we investigate\nwhether these issues persist with o1, a new system from OpenAI that differs\nfrom previous LLMs in that it is optimized for reasoning. We find that o1\nsubstantially outperforms previous LLMs in many cases, with particularly large\nimprovements on rare variants of common tasks (e.g., forming acronyms from the\nsecond letter of each word in a list, rather than the first letter). Despite\nthese quantitative improvements, however, o1 still displays the same\nqualitative trends that we observed in previous systems. Specifically, o1 --\nlike previous LLMs -- is sensitive to the probability of examples and tasks,\nperforming better and requiring fewer \"thinking tokens\" in high-probability\nsettings than in low-probability ones. These results show that optimizing a\nlanguage model for reasoning can mitigate but might not fully overcome the\nlanguage model's probability sensitivity.", "published": "2024-10-02 17:50:19", "link": "http://arxiv.org/abs/2410.01792v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "NEAT: Nonlinear Parameter-efficient Adaptation of Pre-trained Models", "abstract": "Fine-tuning pre-trained models often yields state-of-the-art performance but\nis computationally expensive when updating all parameters. Parameter-efficient\nfine-tuning (PEFT) methods, such as Low-Rank Adaptation (LoRA), address this by\nfreezing pre-trained weights and introducing low-rank matrices. However,\nbecause LoRA relies on low-rank decomposition, it struggles to capture complex\nnonlinear dynamics and optimal optimization trajectories, resulting in a\nperformance gap relative to full fine-tuning and inefficient parameter\nutilization. To overcome these issues, we propose NEAT, a nonlinear PEFT\napproach that employs a lightweight neural network to learn a nonlinear\ntransformation of the pre-trained weights, thereby better approximating\ncumulative weight updates. Our theoretical analysis shows that NEAT achieves\ngreater efficiency than LoRA while maintaining equivalent expressivity.\nExtensive experiments on four benchmarks and over twenty datasets demonstrate\nthat NEAT significantly outperforms state-of-the-art baselines in both vision\nand text tasks.", "published": "2024-10-02 17:29:23", "link": "http://arxiv.org/abs/2410.01870v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Zodiac: A Cardiologist-Level LLM Framework for Multi-Agent Diagnostics", "abstract": "Large language models (LLMs) have demonstrated remarkable progress in\nhealthcare. However, a significant gap remains regarding LLMs' professionalism\nin domain-specific clinical practices, limiting their application in real-world\ndiagnostics. In this work, we introduce ZODIAC, an LLM-powered framework with\ncardiologist-level professionalism designed to engage LLMs in cardiological\ndiagnostics. ZODIAC assists cardiologists by extracting clinically relevant\ncharacteristics from patient data, detecting significant arrhythmias, and\ngenerating preliminary reports for the review and refinement by cardiologists.\nTo achieve cardiologist-level professionalism, ZODIAC is built on a multi-agent\ncollaboration framework, enabling the processing of patient data across\nmultiple modalities. Each LLM agent is fine-tuned using real-world patient data\nadjudicated by cardiologists, reinforcing the model's professionalism. ZODIAC\nundergoes rigorous clinical validation with independent cardiologists,\nevaluated across eight metrics that measure clinical effectiveness and address\nsecurity concerns. Results show that ZODIAC outperforms industry-leading\nmodels, including OpenAI's GPT-4o, Meta's Llama-3.1-405B, and Google's\nGemini-pro, as well as medical-specialist LLMs like Microsoft's BioGPT. ZODIAC\ndemonstrates the transformative potential of specialized LLMs in healthcare by\ndelivering domain-specific solutions that meet the stringent demands of medical\npractice. Notably, ZODIAC has been successfully integrated into\nelectrocardiography (ECG) devices, exemplifying the growing trend of embedding\nLLMs into Software-as-Medical-Device (SaMD).", "published": "2024-10-02 20:46:39", "link": "http://arxiv.org/abs/2410.02026v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "ExACT: Teaching AI Agents to Explore with Reflective-MCTS and\n  Exploratory Learning", "abstract": "Autonomous agents have demonstrated significant potential in automating\ncomplex multistep decision-making tasks. However, even state-of-the-art\nvision-language models (VLMs), such as GPT-4o, still fall short of human-level\nperformance, particularly in intricate web environments and long-horizon tasks.\nTo address these limitations, we present ExACT, an approach to combine\ntest-time search and self-learning to build o1-like models for agentic\napplications. We first introduce Reflective Monte Carlo Tree Search (R-MCTS), a\nnovel test time algorithm designed to enhance AI agents' ability to explore\ndecision space on the fly. R-MCTS extends traditional MCTS by 1) incorporating\ncontrastive reflection, allowing agents to learn from past interactions and\ndynamically improve their search efficiency; and 2) using multi-agent debate\nfor reliable state evaluation. Next, we introduce Exploratory Learning, a novel\nlearning strategy to teach agents to search at inference time without relying\non any external search algorithms. On the challenging VisualWebArena benchmark,\nour GPT-4o based R-MCTS agent achieves a 6% to 30% relative improvement across\nvarious tasks compared to the previous state-of-the-art. Additionally, we show\nthat the knowledge and experience gained from test-time search can be\neffectively transferred back to GPT-4o via fine-tuning. After Exploratory\nLearning, GPT-4o 1) demonstrates the ability to explore the environment,\nevaluate a state, and backtrack to viable ones when it detects that the current\nstate cannot lead to success, and 2) matches 87% of R-MCTS's performance while\nusing significantly less compute. Notably, our work demonstrates the compute\nscaling properties in both training - data collection with R-MCTS - and testing\ntime. These results suggest a promising research direction to enhance VLMs'\ncapabilities for agentic applications via test-time search and self-learning.", "published": "2024-10-02 21:42:35", "link": "http://arxiv.org/abs/2410.02052v5", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "TPP-LLM: Modeling Temporal Point Processes by Efficiently Fine-Tuning\n  Large Language Models", "abstract": "Temporal point processes (TPPs) are widely used to model the timing and\noccurrence of events in domains such as social networks, transportation\nsystems, and e-commerce. In this paper, we introduce TPP-LLM, a novel framework\nthat integrates large language models (LLMs) with TPPs to capture both the\nsemantic and temporal aspects of event sequences. Unlike traditional methods\nthat rely on categorical event type representations, TPP-LLM directly utilizes\nthe textual descriptions of event types, enabling the model to capture rich\nsemantic information embedded in the text. While LLMs excel at understanding\nevent semantics, they are less adept at capturing temporal patterns. To address\nthis, TPP-LLM incorporates temporal embeddings and employs parameter-efficient\nfine-tuning (PEFT) methods to effectively learn temporal dynamics without\nextensive retraining. This approach improves both predictive accuracy and\ncomputational efficiency. Experimental results across diverse real-world\ndatasets demonstrate that TPP-LLM outperforms state-of-the-art baselines in\nsequence modeling and event prediction, highlighting the benefits of combining\nLLMs with TPPs.", "published": "2024-10-02 22:17:24", "link": "http://arxiv.org/abs/2410.02062v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Inspection and Control of Self-Generated-Text Recognition Ability in\n  Llama3-8b-Instruct", "abstract": "It has been reported that LLMs can recognize their own writing. As this has\npotential implications for AI safety, yet is relatively understudied, we\ninvestigate the phenomenon, seeking to establish whether it robustly occurs at\nthe behavioral level, how the observed behavior is achieved, and whether it can\nbe controlled. First, we find that the Llama3-8b-Instruct chat model - but not\nthe base Llama3-8b model - can reliably distinguish its own outputs from those\nof humans, and present evidence that the chat model is likely using its\nexperience with its own outputs, acquired during post-training, to succeed at\nthe writing recognition task. Second, we identify a vector in the residual\nstream of the model that is differentially activated when the model makes a\ncorrect self-written-text recognition judgment, show that the vector activates\nin response to information relevant to self-authorship, present evidence that\nthe vector is related to the concept of \"self\" in the model, and demonstrate\nthat the vector is causally related to the model's ability to perceive and\nassert self-authorship. Finally, we show that the vector can be used to control\nboth the model's behavior and its perception, steering the model to claim or\ndisclaim authorship by applying the vector to the model's output as it\ngenerates it, and steering the model to believe or disbelieve it wrote\narbitrary texts by applying the vector to them as the model reads them.", "published": "2024-10-02 22:26:21", "link": "http://arxiv.org/abs/2410.02064v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "RLEF: Grounding Code LLMs in Execution Feedback with Reinforcement\n  Learning", "abstract": "Large language models (LLMs) deployed as agents solve user-specified tasks\nover multiple steps while keeping the required manual engagement to a minimum.\nCrucially, such LLMs need to ground their generations in any feedback obtained\nto reliably achieve the desired outcomes. We propose an end-to-end\nreinforcement learning method for teaching models to leverage execution\nfeedback in the realm of code synthesis, where state-of-the-art LLMs struggle\nto improve code iteratively compared to independent sampling. We benchmark on\ncompetitive programming tasks, where we achieve new state-of-the art results\nwith both small (8B parameters) and large (70B) models while reducing the\namount of samples required by an order of magnitude. Our analysis of\ninference-time behavior demonstrates that our method produces LLMs that\neffectively leverage automatic feedback over multiple steps.", "published": "2024-10-02 23:25:17", "link": "http://arxiv.org/abs/2410.02089v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing Retrieval in QA Systems with Derived Feature Association", "abstract": "Retrieval augmented generation (RAG) has become the standard in long context\nquestion answering (QA) systems. However, typical implementations of RAG rely\non a rather naive retrieval mechanism, in which texts whose embeddings are most\nsimilar to that of the query are deemed most relevant. This has consequences in\nsubjective QA tasks, where the most relevant text may not directly contain the\nanswer. In this work, we propose a novel extension to RAG systems, which we\ncall Retrieval from AI Derived Documents (RAIDD). RAIDD leverages the full\npower of the LLM in the retrieval process by deriving inferred features, such\nas summaries and example questions, from the documents at ingest. We\ndemonstrate that this approach significantly improves the performance of RAG\nsystems on long-context QA tasks.", "published": "2024-10-02 05:24:49", "link": "http://arxiv.org/abs/2410.03754v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Taxonomy Tree Generation from Citation Graph", "abstract": "Constructing taxonomies from citation graphs is essential for organizing\nscientific knowledge, facilitating literature reviews, and identifying emerging\nresearch trends. However, manual taxonomy construction is labor-intensive,\ntime-consuming, and prone to human biases, often overlooking pivotal but\nless-cited papers. In this paper, to enable automatic hierarchical taxonomy\ngeneration from citation graphs, we propose HiGTL (Hierarchical Graph Taxonomy\nLearning), a novel end-to-end framework guided by human-provided instructions\nor preferred topics. Specifically, we propose a hierarchical citation graph\nclustering method that recursively groups related papers based on both textual\ncontent and citation structure, ensuring semantically meaningful and\nstructurally coherent clusters. Additionally, we develop a novel taxonomy node\nverbalization strategy that iteratively generates central concepts for each\ncluster, leveraging a pre-trained large language model (LLM) to maintain\nsemantic consistency across hierarchical levels. To further enhance\nperformance, we design a joint optimization framework that fine-tunes both the\nclustering and concept generation modules, aligning structural accuracy with\nthe quality of generated taxonomies. Extensive experiments demonstrate that\nHiGTL effectively produces coherent, high-quality taxonomies.", "published": "2024-10-02 13:02:03", "link": "http://arxiv.org/abs/2410.03761v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Basis Sharing: Cross-Layer Parameter Sharing for Large Language Model\n  Compression", "abstract": "Large Language Models (LLMs) have achieved remarkable breakthroughs. However,\nthe huge number of parameters in LLMs require significant amount of memory\nstorage in inference, which prevents their practical deployment in many\napplications. To reduce memory storage of LLMs, singular value decomposition\n(SVD) provides a promising solution to approximate weight matrices for\ncompressing LLMs. In this paper, we take a step further to explore parameter\nsharing across different layers with SVD to achieve more effective compression\nfor LLMs. Specifically, weight matrices in different layers are decomposed and\nrepresented as a linear combination of a set of shared basis vectors and unique\ncoefficients. The types of weight matrices and the layer selection for basis\nsharing are examined when compressing LLMs to maintain the performance.\nComprehensive experiments demonstrate that Basis Sharing outperforms\nstate-of-the-art SVD-based compression approaches and parameter sharing\ntechniques, especially under large compression ratios. Code is available at:\nhttps://github.com/TUDa-HWAI/Basis_Sharing", "published": "2024-10-02 14:30:02", "link": "http://arxiv.org/abs/2410.03765v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Two-Stage Proactive Dialogue Generator for Efficient Clinical\n  Information Collection Using Large Language Model", "abstract": "Efficient patient-doctor interaction is among the key factors for a\nsuccessful disease diagnosis. During the conversation, the doctor could query\ncomplementary diagnostic information, such as the patient's symptoms, previous\nsurgery, and other related information that goes beyond medical evidence data\n(test results) to enhance disease diagnosis. However, this procedure is usually\ntime-consuming and less-efficient, which can be potentially optimized through\ncomputer-assisted systems. As such, we propose a diagnostic dialogue system to\nautomate the patient information collection procedure. By exploiting medical\nhistory and conversation logic, our conversation agents, particularly the\ndoctor agent, can pose multi-round clinical queries to effectively collect the\nmost relevant disease diagnostic information. Moreover, benefiting from our\ntwo-stage recommendation structure, carefully designed ranking criteria, and\ninteractive patient agent, our model is able to overcome the under-exploration\nand non-flexible challenges in dialogue generation. Our experimental results on\na real-world medical conversation dataset show that our model can generate\nclinical queries that mimic the conversation style of real doctors, with\nefficient fluency, professionalism, and safety, while effectively collecting\nrelevant disease diagnostic information.", "published": "2024-10-02 19:32:11", "link": "http://arxiv.org/abs/2410.03770v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Precision Knowledge Editing: Enhancing Safety in Large Language Models", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities, but\nthey also pose risks related to the generation of toxic or harmful content.\nThis work introduces Precision Knowledge Editing (PKE), an advanced technique\nthat builds upon existing knowledge editing methods to more effectively\nidentify and modify toxic parameter regions within LLMs. By leveraging neuron\nweight tracking and activation pathway tracing, PKE achieves finer granularity\nin toxic content management compared to previous methods like Detoxifying\nInstance Neuron Modification (DINM). Our experiments demonstrate that PKE\nsignificantly reduces the attack success rate (ASR) across various models,\nincluding Llama2-7b and Llama-3-8b-instruct, while maintaining overall model\nperformance. Additionally, we also compared the performance of some\nclosed-source models (gpt-4-0613 and Claude 3 Sonnet) in our experiments, and\nfound that models adjusted using our method far outperformed the closed-source\nmodels in terms of safety. This research contributes to the ongoing efforts to\nmake LLMs safer and more reliable for real-world applications.", "published": "2024-10-02 23:15:53", "link": "http://arxiv.org/abs/2410.03772v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Frozen Large Language Models Can Perceive Paralinguistic Aspects of\n  Speech", "abstract": "As speech becomes an increasingly common modality for interacting with large\nlanguage models (LLMs), it is becoming desirable to develop systems where LLMs\ncan take into account users' emotions or speaking styles when providing their\nresponses. In this work, we study the potential of an LLM to understand these\naspects of speech without fine-tuning its weights. To do this, we utilize an\nend-to-end system with a speech encoder; the encoder is trained to produce\ntoken embeddings such that the LLM's response to an expressive speech prompt is\naligned with its response to a semantically matching text prompt where the\nspeaker's emotion has also been specified. We find that this training framework\nallows the encoder to generate tokens that capture both semantic and\nparalinguistic information in speech and effectively convey it to the LLM, even\nwhen the LLM remains completely frozen. We also explore training on additional\nemotion and style-related response alignment tasks, finding that they further\nincrease the amount of paralinguistic information explicitly captured in the\nspeech tokens. Experiments demonstrate that our system is able to produce\nhigher quality and more empathetic responses to expressive speech prompts\ncompared to several baselines.", "published": "2024-10-02 01:32:47", "link": "http://arxiv.org/abs/2410.01162v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "From Code to Correctness: Closing the Last Mile of Code Generation with\n  Hierarchical Debugging", "abstract": "While large language models have made significant strides in code generation,\nthe pass rate of the generated code is bottlenecked on subtle errors, often\nrequiring human intervention to pass tests, especially for complex problems.\nExisting LLM-based debugging systems treat generated programs as monolithic\nunits, failing to address bugs at multiple levels of granularity, from\nlow-level syntax errors to high-level algorithmic flaws. In this paper, we\nintroduce Multi-Granularity Debugger (MGDebugger), a hierarchical code debugger\nby isolating, identifying, and resolving bugs at various levels of granularity.\nMGDebugger decomposes problematic code into a hierarchical tree structure of\nsubfunctions, with each level representing a particular granularity of error.\nDuring debugging, it analyzes each subfunction and iteratively resolves bugs in\na bottom-up manner. To effectively test each subfunction, we propose an\nLLM-simulated Python executor, which traces code execution and tracks important\nvariable states to pinpoint errors accurately. Extensive experiments\ndemonstrate that MGDebugger outperforms existing debugging systems, achieving\nan 18.9% improvement in accuracy over seed generations in HumanEval and a 97.6%\nrepair success rate in HumanEvalFix. Furthermore, MGDebugger effectively fixes\nbugs across different categories and difficulty levels, demonstrating its\nrobustness and effectiveness.", "published": "2024-10-02 03:57:21", "link": "http://arxiv.org/abs/2410.01215v2", "categories": ["cs.CL", "cs.AI", "cs.PL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "RGD: Multi-LLM Based Agent Debugger via Refinement and Generation\n  Guidance", "abstract": "Large Language Models (LLMs) have shown incredible potential in code\ngeneration tasks, and recent research in prompt engineering have enhanced LLMs'\nunderstanding of textual information. However, ensuring the accuracy of\ngenerated code often requires extensive testing and validation by programmers.\nWhile LLMs can typically generate code based on task descriptions, their\naccuracy remains limited, especially for complex tasks that require a deeper\nunderstanding of both the problem statement and the code generation process.\nThis limitation is primarily due to the LLMs' need to simultaneously comprehend\ntext and generate syntactically and semantically correct code, without having\nthe capability to automatically refine the code. In real-world software\ndevelopment, programmers rarely produce flawless code in a single attempt based\non the task description alone, they rely on iterative feedback and debugging to\nrefine their programs. Inspired by this process, we introduce a novel\narchitecture of LLM-based agents for code generation and automatic debugging:\nRefinement and Guidance Debugging (RGD). The RGD framework is a multi-LLM-based\nagent debugger that leverages three distinct LLM agents-Guide Agent, Debug\nAgent, and Feedback Agent. RGD decomposes the code generation task into\nmultiple steps, ensuring a clearer workflow and enabling iterative code\nrefinement based on self-reflection and feedback. Experimental results\ndemonstrate that RGD exhibits remarkable code generation capabilities,\nachieving state-of-the-art performance with a 9.8% improvement on the HumanEval\ndataset and a 16.2% improvement on the MBPP dataset compared to the\nstate-of-the-art approaches and traditional direct prompting approaches. We\nhighlight the effectiveness of the RGD framework in enhancing LLMs' ability to\ngenerate and refine code autonomously.", "published": "2024-10-02 05:07:02", "link": "http://arxiv.org/abs/2410.01242v2", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "HelpSteer2-Preference: Complementing Ratings with Preferences", "abstract": "Reward models are critical for aligning models to follow instructions, and\nare typically trained following one of two popular paradigms: Bradley-Terry\nstyle or Regression style. However, there is a lack of evidence that either\napproach is better than the other, when adequately matched for data. This is\nprimarily because these approaches require data collected in different (but\nincompatible) formats, meaning that adequately matched data is not available in\nexisting public datasets. To tackle this problem, we release preference\nannotations (designed for Bradley-Terry training) to complement existing\nratings (designed for Regression style training) in the HelpSteer2 dataset. To\nimprove data interpretability, preference annotations are accompanied with\nhuman-written justifications. Using this data, we conduct the first\nhead-to-head comparison of Bradley-Terry and Regression models when adequately\nmatched for data. Based on insights derived from such a comparison, we propose\na novel approach to combine Bradley-Terry and Regression reward modeling. A\nLlama-3.1-70B-Instruct model tuned with this approach scores 94.1 on\nRewardBench, emerging top of more than 140 reward models as of 1 Oct 2024. This\nreward model can then be used with REINFORCE algorithm (RLHF) to align an\nInstruct model to reach 85.0 on Arena Hard, which is No. 1 as of 1 Oct 2024. We\nopen-source this dataset (CC-BY-4.0 license) at\nhttps://huggingface.co/datasets/nvidia/HelpSteer2#preferences-new -- 1-oct-2024\nand openly release the trained Reward and Instruct models at\nhttps://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Reward and\nhttps://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Instruct", "published": "2024-10-02 06:05:52", "link": "http://arxiv.org/abs/2410.01257v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Emotion-Aware Embedding Fusion in LLMs (Flan-T5, LLAMA 2, DeepSeek-R1,\n  and ChatGPT 4) for Intelligent Response Generation", "abstract": "Empathetic and coherent responses are critical in auto-mated\nchatbot-facilitated psychotherapy. This study addresses the challenge of\nenhancing the emotional and contextual understanding of large language models\n(LLMs) in psychiatric applications. We introduce Emotion-Aware Embedding\nFusion, a novel framework integrating hierarchical fusion and attention\nmechanisms to prioritize semantic and emotional features in therapy\ntranscripts. Our approach combines multiple emotion lexicons, including NRC\nEmotion Lexicon, VADER, WordNet, and SentiWordNet, with state-of-the-art LLMs\nsuch as Flan-T5, LLAMA 2, DeepSeek-R1, and ChatGPT 4. Therapy session\ntranscripts, comprising over 2,000 samples are segmented into hierarchical\nlevels (word, sentence, and session) using neural networks, while hierarchical\nfusion combines these features with pooling techniques to refine emotional\nrepresentations. Atten-tion mechanisms, including multi-head self-attention and\ncross-attention, further prioritize emotional and contextual features, enabling\ntemporal modeling of emotion-al shifts across sessions. The processed\nembeddings, computed using BERT, GPT-3, and RoBERTa are stored in the Facebook\nAI similarity search vector database, which enables efficient similarity search\nand clustering across dense vector spaces. Upon user queries, relevant segments\nare retrieved and provided as context to LLMs, enhancing their ability to\ngenerate empathetic and con-textually relevant responses. The proposed\nframework is evaluated across multiple practical use cases to demonstrate\nreal-world applicability, including AI-driven therapy chatbots. The system can\nbe integrated into existing mental health platforms to generate personalized\nresponses based on retrieved therapy session data.", "published": "2024-10-02 08:01:05", "link": "http://arxiv.org/abs/2410.01306v2", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Layer Swapping for Zero-Shot Cross-Lingual Transfer in Large Language\n  Models", "abstract": "Model merging, such as model souping, is the practice of combining different\nmodels with the same architecture together without further training. In this\nwork, we present a model merging methodology that addresses the difficulty of\nfine-tuning Large Language Models (LLMs) for target tasks in non-English\nlanguages, where task-specific data is often unavailable. We focus on\nmathematical reasoning and without in-language math data, facilitate\ncross-lingual transfer by composing language and math capabilities. Starting\nfrom the same pretrained model, we fine-tune separate \"experts\" on math\ninstruction data in English and on generic instruction data in the target\nlanguage. We then replace the top and bottom transformer layers of the math\nexpert directly with layers from the language expert, which consequently\nenhances math performance in the target language. The resulting merged models\noutperform the individual experts and other merging methods on the math\nbenchmark, MGSM, by 10% across four major languages where math instruction data\nis scarce. In addition, this layer swapping is simple, inexpensive, and\nintuitive, as it is based on an interpretative analysis of the most important\nparameter changes during the fine-tuning of each expert. The ability to\nsuccessfully re-compose LLMs for cross-lingual transfer in this manner opens up\nfuture possibilities to combine model expertise, create modular solutions, and\ntransfer reasoning capabilities across languages all post hoc.", "published": "2024-10-02 08:53:07", "link": "http://arxiv.org/abs/2410.01335v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Labyrinth of Links: Navigating the Associative Maze of Multi-modal\n  LLMs", "abstract": "Multi-modal Large Language Models (MLLMs) have exhibited impressive\ncapability. However, recently many deficiencies of MLLMs have been found\ncompared to human intelligence, $\\textit{e.g.}$, hallucination. To drive the\nMLLMs study, the community dedicated efforts to building larger benchmarks with\ncomplex tasks. In this paper, we propose benchmarking an essential but usually\noverlooked intelligence: $\\textbf{association}$, a human's basic capability to\nlink observation and prior practice memory. To comprehensively investigate\nMLLM's performance on the association, we formulate the association task and\ndevise a standard benchmark based on adjective and verb semantic concepts.\nInstead of costly data annotation and curation, we propose a convenient\n$\\textbf{annotation-free}$ construction method transforming the general dataset\nfor our association tasks. Simultaneously, we devise a rigorous data refinement\nprocess to eliminate confusion in the raw dataset. Building on this database,\nwe establish three levels of association tasks: single-step, synchronous, and\nasynchronous associations. Moreover, we conduct a comprehensive investigation\ninto the MLLMs' zero-shot association capabilities, addressing multiple\ndimensions, including three distinct memory strategies, both open-source and\nclosed-source MLLMs, cutting-edge Mixture-of-Experts (MoE) models, and the\ninvolvement of human experts. Our systematic investigation shows that current\nopen-source MLLMs consistently exhibit poor capability in our association\ntasks, even the currently state-of-the-art GPT-4V(vision) also has a\nsignificant gap compared to humans. We believe our benchmark would pave the way\nfor future MLLM studies. $\\textit{Our data and code are available at:}$\nhttps://mvig-rhos.com/llm_inception.", "published": "2024-10-02 10:58:54", "link": "http://arxiv.org/abs/2410.01417v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Analyzing Byte-Pair Encoding on Monophonic and Polyphonic Symbolic\n  Music: A Focus on Musical Phrase Segmentation", "abstract": "Byte-Pair Encoding (BPE) is an algorithm commonly used in Natural Language\nProcessing to build a vocabulary of subwords, which has been recently applied\nto symbolic music. Given that symbolic music can differ significantly from\ntext, particularly with polyphony, we investigate how BPE behaves with\ndifferent types of musical content. This study provides a qualitative analysis\nof BPE's behavior across various instrumentations and evaluates its impact on a\nmusical phrase segmentation task for both monophonic and polyphonic music. Our\nfindings show that the BPE training process is highly dependent on the\ninstrumentation and that BPE \"supertokens\" succeed in capturing abstract\nmusical content. In a musical phrase segmentation task, BPE notably improves\nperformance in a polyphonic setting, but enhances performance in monophonic\ntunes only within a specific range of BPE merges.", "published": "2024-10-02 11:59:58", "link": "http://arxiv.org/abs/2410.01448v1", "categories": ["cs.IR", "cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.IR"}
{"title": "DLP-LoRA: Efficient Task-Specific LoRA Fusion with a Dynamic,\n  Lightweight Plugin for Large Language Models", "abstract": "Recent advancements in Large Language Models (LLMs) have achieved robust\nperformance across diverse tasks, but fine-tuning these models for specific\ndomains remains resource-intensive. Parameter-Efficient Fine-Tuning (PEFT)\nmethods like Low-Rank Adaptation (LoRA) address this challenge by fine-tuning a\nsmall subset of parameters. However, existing methods for fusing multiple LoRAs\nlack dynamic fusion based on contextual inputs and often increase inference\ntime due to token-level operations. We propose DLP-LoRA, a Dynamic Lightweight\nPlugin that employs a mini-MLP module with only 5M parameters to dynamically\nfuse multiple LoRAs at the sentence level using top-p sampling strategies. This\napproach reduces inference time to less than twice that of single LoRA\ninference by leveraging parallel computation. Evaluations across 26\ntasks-including multiple-choice questions and question answering-demonstrate\nthat DLP-LoRA achieves an average accuracy of 92.34% on multiple-choice\ndatasets and significant improvements in BLEU and ROUGE scores on QA datasets,\noutperforming different LLMs backbones under composite task settings. DLP-LoRA\neffectively balances performance and efficiency, making it a practical solution\nfor dynamic multi-task adaptation in LLMs. Our code is available at\nhttps://github.com/MeCuping/DLP-LoRA.", "published": "2024-10-02 12:45:52", "link": "http://arxiv.org/abs/2410.01497v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Seeing Eye to AI: Human Alignment via Gaze-Based Response Rewards for\n  Large Language Models", "abstract": "Advancements in Natural Language Processing (NLP), have led to the emergence\nof Large Language Models (LLMs) such as GPT, Llama, Claude, and Gemini, which\nexcel across a range of tasks but require extensive fine-tuning to align their\noutputs with human expectations. A widely used method for achieving this\nalignment is Reinforcement Learning from Human Feedback (RLHF), which, despite\nits success, faces challenges in accurately modelling human preferences. In\nthis paper, we introduce GazeReward, a novel framework that integrates implicit\nfeedback -- and specifically eye-tracking (ET) data -- into the Reward Model\n(RM). In addition, we explore how ET-based features can provide insights into\nuser preferences. Through ablation studies we test our framework with different\nintegration methods, LLMs, and ET generator models, demonstrating that our\napproach significantly improves the accuracy of the RM on established human\npreference datasets. This work advances the ongoing discussion on optimizing AI\nalignment with human values, exploring the potential of cognitive data for\nshaping future NLP research.", "published": "2024-10-02 13:24:56", "link": "http://arxiv.org/abs/2410.01532v3", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Integrative Decoding: Improve Factuality via Implicit Self-consistency", "abstract": "Self-consistency-based approaches, which involve repeatedly sampling multiple\noutputs and selecting the most consistent one as the final response, prove to\nbe remarkably effective in improving the factual accuracy of large language\nmodels. Nonetheless, existing methods usually have strict constraints on the\ntask format, largely limiting their applicability. In this paper, we present\nIntegrative Decoding (ID), to unlock the potential of self-consistency in\nopen-ended generation tasks. ID operates by constructing a set of inputs, each\nprepended with a previously sampled response, and then processes them\nconcurrently, with the next token being selected by aggregating of all their\ncorresponding predictions at each decoding step. In essence, this simple\napproach implicitly incorporates self-consistency in the decoding objective.\nExtensive evaluation shows that ID consistently enhances factuality over a wide\nrange of language models, with substantial improvements on the TruthfulQA\n(+11.2%), Biographies (+15.4%) and LongFact (+8.5%) benchmarks. The performance\ngains amplify progressively as the number of sampled responses increases,\nindicating the potential of ID to scale up with repeated sampling.", "published": "2024-10-02 13:52:55", "link": "http://arxiv.org/abs/2410.01556v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "OpenMathInstruct-2: Accelerating AI for Math with Massive Open-Source\n  Instruction Data", "abstract": "Mathematical reasoning continues to be a critical challenge in large language\nmodel (LLM) development with significant interest. However, most of the\ncutting-edge progress in mathematical reasoning with LLMs has become\n\\emph{closed-source} due to lack of access to training data. This lack of data\naccess limits researchers from understanding the impact of different choices\nfor synthesizing and utilizing the data. With the goal of creating a\nhigh-quality finetuning (SFT) dataset for math reasoning, we conduct careful\nablation experiments on data synthesis using the recently released\n\\texttt{Llama3.1} family of models. Our experiments show that: (a) solution\nformat matters, with excessively verbose solutions proving detrimental to SFT\nperformance, (b) data generated by a strong teacher outperforms equally-sized\ndata generated by a weak student model, (c) SFT is robust to low-quality\nsolutions, allowing for imprecise data filtering, and (d) question diversity is\ncrucial for achieving data scaling gains. Based on these insights, we create\nthe OpenMathInstruct-2 dataset, which consists of 14M question-solution pairs\n($\\approx$ 600K unique questions), making it nearly eight times larger than the\nprevious largest open-source math reasoning dataset. Finetuning the\n\\texttt{Llama-3.1-8B-Base} using OpenMathInstruct-2 outperforms\n\\texttt{Llama3.1-8B-Instruct} on MATH by an absolute 15.9\\% (51.9\\%\n$\\rightarrow$ 67.8\\%). Finally, to accelerate the open-source efforts, we\nrelease the code, the finetuned models, and the OpenMathInstruct-2 dataset\nunder a commercially permissive license.", "published": "2024-10-02 14:00:09", "link": "http://arxiv.org/abs/2410.01560v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Upcycling Instruction Tuning from Dense to Mixture-of-Experts via\n  Parameter Merging", "abstract": "Mixture-of-Experts (MoE) shines brightly in large language models (LLMs) and\ndemonstrates outstanding performance in plentiful natural language processing\ntasks. However, existing methods transforming LLMs from dense to MoE face\nsignificant data requirements and typically rely on large-scale post-training.\nIn this paper, we propose Upcycling Instruction Tuning (UpIT), a data-efficient\napproach for tuning a dense pre-trained model into a MoE instruction model.\nSpecifically, we first point out that intermediate checkpoints during\ninstruction tuning of the dense model are naturally suitable for specialized\nexperts, and then propose an expert expansion stage to flexibly achieve models\nwith flexible numbers of experts, where genetic algorithm and parameter merging\nare introduced to ensure sufficient diversity of new extended experts. To\nensure that each specialized expert in the MoE model works as expected, we\nselect a small amount of seed data that each expert excels to pre-optimize the\nrouter. Extensive experiments with various data scales and upcycling settings\ndemonstrate the outstanding performance and data efficiency of UpIT, as well as\nstable improvement in expert or data scaling. Further analysis reveals the\nimportance of ensuring expert diversity in upcycling.", "published": "2024-10-02 14:48:22", "link": "http://arxiv.org/abs/2410.01610v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards a Theoretical Understanding of Synthetic Data in LLM\n  Post-Training: A Reverse-Bottleneck Perspective", "abstract": "Synthetic data has become a pivotal resource in post-training tasks for large\nlanguage models (LLMs) due to the scarcity of high-quality, specific data.\nWhile various methods have been developed to generate synthetic data, there\nremains a discernible gap between the practical effects of synthetic data and\nour theoretical comprehension. To address this challenge, we commence by\npresenting a detailed modeling of the prevalent synthetic data generation\nprocess. Building upon this modeling, we demonstrate that the generalization\ncapability of the post-trained model is critically determined by the\ninformation gain derived from the generative model, as analyzed from a novel\nreverse-bottleneck perspective. Moreover, we introduce the concept of\nGeneralization Gain via Mutual Information (GGMI) and elucidate the\nrelationship between generalization gain and information gain. This analysis\nserves as a theoretical foundation for synthetic data generation and further\nhighlights its connection with the generalization capability of post-trained\nmodels, offering an understanding about the design of synthetic data generation\ntechniques and the optimization of the post-training process. We open-source\nour code at\nhttps://github.com/ZyGan1999/Towards-a-Theoretical-Understanding-of-Synthetic-Data-in-LLM-Post-Training.", "published": "2024-10-02 16:32:05", "link": "http://arxiv.org/abs/2410.01720v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Evaluating Robustness of Reward Models for Mathematical Reasoning", "abstract": "Reward models are key in reinforcement learning from human feedback (RLHF)\nsystems, aligning the model behavior with human preferences. Particularly in\nthe math domain, there have been plenty of studies using reward models to align\npolicies for improving reasoning capabilities. Recently, as the importance of\nreward models has been emphasized, RewardBench is proposed to understand their\nbehavior. However, we figure out that the math subset of RewardBench has\ndifferent representations between chosen and rejected completions, and relies\non a single comparison, which may lead to unreliable results as it only see an\nisolated case. Therefore, it fails to accurately present the robustness of\nreward models, leading to a misunderstanding of its performance and potentially\nresulting in reward hacking. In this work, we introduce a new design for\nreliable evaluation of reward models, and to validate this, we construct\nRewardMATH, a benchmark that effectively represents the robustness of reward\nmodels in mathematical reasoning tasks. We demonstrate that the scores on\nRewardMATH strongly correlate with the results of optimized policy and\neffectively estimate reward overoptimization, whereas the existing benchmark\nshows almost no correlation. The results underscore the potential of our design\nto enhance the reliability of evaluation, and represent the robustness of\nreward model. We make our code and data publicly available.", "published": "2024-10-02 16:39:58", "link": "http://arxiv.org/abs/2410.01729v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "ComfyGen: Prompt-Adaptive Workflows for Text-to-Image Generation", "abstract": "The practical use of text-to-image generation has evolved from simple,\nmonolithic models to complex workflows that combine multiple specialized\ncomponents. While workflow-based approaches can lead to improved image quality,\ncrafting effective workflows requires significant expertise, owing to the large\nnumber of available components, their complex inter-dependence, and their\ndependence on the generation prompt. Here, we introduce the novel task of\nprompt-adaptive workflow generation, where the goal is to automatically tailor\na workflow to each user prompt. We propose two LLM-based approaches to tackle\nthis task: a tuning-based method that learns from user-preference data, and a\ntraining-free method that uses the LLM to select existing flows. Both\napproaches lead to improved image quality when compared to monolithic models or\ngeneric, prompt-independent workflows. Our work shows that prompt-dependent\nflow prediction offers a new pathway to improving text-to-image generation\nquality, complementing existing research directions in the field.", "published": "2024-10-02 16:43:24", "link": "http://arxiv.org/abs/2410.01731v1", "categories": ["cs.CV", "cs.CL", "cs.GR"], "primary_category": "cs.CV"}
{"title": "Open-RAG: Enhanced Retrieval-Augmented Reasoning with Open-Source Large\n  Language Models", "abstract": "Retrieval-Augmented Generation (RAG) has been shown to enhance the factual\naccuracy of Large Language Models (LLMs), but existing methods often suffer\nfrom limited reasoning capabilities in effectively using the retrieved\nevidence, particularly when using open-source LLMs. To mitigate this gap, we\nintroduce a novel framework, Open-RAG, designed to enhance reasoning\ncapabilities in RAG with open-source LLMs. Our framework transforms an\narbitrary dense LLM into a parameter-efficient sparse mixture of experts (MoE)\nmodel capable of handling complex reasoning tasks, including both single- and\nmulti-hop queries. Open-RAG uniquely trains the model to navigate challenging\ndistractors that appear relevant but are misleading. As a result, Open-RAG\nleverages latent learning, dynamically selecting relevant experts and\nintegrating external knowledge effectively for more accurate and contextually\nrelevant responses. In addition, we propose a hybrid adaptive retrieval method\nto determine retrieval necessity and balance the trade-off between performance\ngain and inference speed. Experimental results show that the Llama2-7B-based\nOpen-RAG outperforms state-of-the-art LLMs and RAG models such as ChatGPT,\nSelf-RAG, and Command R+ in various knowledge-intensive tasks. We open-source\nour code and models at https://openragmoe.github.io/", "published": "2024-10-02 17:37:18", "link": "http://arxiv.org/abs/2410.01782v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DreamGarden: A Designer Assistant for Growing Games from a Single Prompt", "abstract": "Coding assistants are increasingly leveraged in game design, both generating\ncode and making high-level plans. To what degree can these tools align with\ndeveloper workflows, and what new modes of human-computer interaction can\nemerge from their use? We present DreamGarden, an AI system capable of\nassisting with the development of diverse game environments in Unreal Engine.\nAt the core of our method is an LLM-driven planner, capable of breaking down a\nsingle, high-level prompt -- a dream, memory, or imagined scenario provided by\na human user -- into a hierarchical action plan, which is then distributed\nacross specialized submodules facilitating concrete implementation. This system\nis presented to the user as a garden of plans and actions, both growing\nindependently and responding to user intervention via seed prompts, pruning,\nand feedback. Through a user study, we explore design implications of this\nsystem, charting courses for future work in semi-autonomous assistants and\nopen-ended simulation design.", "published": "2024-10-02 17:49:07", "link": "http://arxiv.org/abs/2410.01791v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.ET"], "primary_category": "cs.HC"}
{"title": "Knowledge-Driven Feature Selection and Engineering for Genotype Data\n  with Large Language Models", "abstract": "Predicting phenotypes with complex genetic bases based on a small,\ninterpretable set of variant features remains a challenging task.\nConventionally, data-driven approaches are utilized for this task, yet the high\ndimensional nature of genotype data makes the analysis and prediction\ndifficult. Motivated by the extensive knowledge encoded in pre-trained LLMs and\ntheir success in processing complex biomedical concepts, we set to examine the\nability of LLMs in feature selection and engineering for tabular genotype data,\nwith a novel knowledge-driven framework. We develop FREEFORM, Free-flow\nReasoning and Ensembling for Enhanced Feature Output and Robust Modeling,\ndesigned with chain-of-thought and ensembling principles, to select and\nengineer features with the intrinsic knowledge of LLMs. Evaluated on two\ndistinct genotype-phenotype datasets, genetic ancestry and hereditary hearing\nloss, we find this framework outperforms several data-driven methods,\nparticularly on low-shot regimes. FREEFORM is available as open-source\nframework at GitHub: https://github.com/PennShenLab/FREEFORM.", "published": "2024-10-02 17:53:08", "link": "http://arxiv.org/abs/2410.01795v1", "categories": ["cs.LG", "cs.CL", "q-bio.GN"], "primary_category": "cs.LG"}
{"title": "House of Cards: Massive Weights in LLMs", "abstract": "Massive activations, which manifest in specific feature dimensions of hidden\nstates, introduce a significant bias in large language models (LLMs), leading\nto an overemphasis on the corresponding token. In this paper, we identify that\nmassive activations originate not from the hidden state but from the\nintermediate state of a feed-forward network module in an early layer.\nExpanding on the previous observation that massive activations occur only in\nspecific feature dimensions, we dive deep into the weights that cause massive\nactivations. Specifically, we define top-$k$ massive weights as the weights\nthat contribute to the dimensions with the top-$k$ magnitudes in the\nintermediate state. When these massive weights are set to zero, the\nfunctionality of LLMs is entirely disrupted. However, when all weights except\nfor massive weights are set to zero, it results in a relatively minor\nperformance drop, even though a much larger number of weights are set to zero.\nThis implies that during the pre-training process, learning is dominantly\nfocused on massive weights. Building on this observation, we propose a simple\nplug-and-play method called MacDrop (massive weights curriculum dropout), to\nrely less on massive weights during parameter-efficient fine-tuning. This\nmethod applies dropout to the pre-trained massive weights, starting with a high\ndropout probability and gradually decreasing it as fine-tuning progresses.\nThrough various experiments, including zero-shot downstream tasks, long-context\ntasks, and ablation studies, we demonstrate that \\texttt{MacDrop} generally\nimproves performance and strengthens robustness.", "published": "2024-10-02 11:54:21", "link": "http://arxiv.org/abs/2410.01866v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A Spark of Vision-Language Intelligence: 2-Dimensional Autoregressive\n  Transformer for Efficient Finegrained Image Generation", "abstract": "This work tackles the information loss bottleneck of vector-quantization (VQ)\nautoregressive image generation by introducing a novel model architecture\ncalled the 2-Dimensional Autoregression (DnD) Transformer. The DnD-Transformer\npredicts more codes for an image by introducing a new autoregression direction,\n\\textit{model depth}, along with the sequence length direction. Compared to\ntraditional 1D autoregression and previous work utilizing similar 2D image\ndecomposition such as RQ-Transformer, the DnD-Transformer is an end-to-end\nmodel that can generate higher quality images with the same backbone model size\nand sequence length, opening a new optimization perspective for autoregressive\nimage generation. Furthermore, our experiments reveal that the\nDnD-Transformer's potential extends beyond generating natural images. It can\neven generate images with rich text and graphical elements in a self-supervised\nmanner, demonstrating an understanding of these combined modalities. This has\nnot been previously demonstrated for popular vision generative models such as\ndiffusion models, showing a spark of vision-language intelligence when trained\nsolely on images. Code, datasets and models are open at\nhttps://github.com/chenllliang/DnD-Transformer.", "published": "2024-10-02 18:10:05", "link": "http://arxiv.org/abs/2410.01912v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "CHASE-SQL: Multi-Path Reasoning and Preference Optimized Candidate\n  Selection in Text-to-SQL", "abstract": "In tackling the challenges of large language model (LLM) performance for\nText-to-SQL tasks, we introduce CHASE-SQL, a new framework that employs\ninnovative strategies, using test-time compute in multi-agent modeling to\nimprove candidate generation and selection. CHASE-SQL leverages LLMs' intrinsic\nknowledge to generate diverse and high-quality SQL candidates using different\nLLM generators with: (1) a divide-and-conquer method that decomposes complex\nqueries into manageable sub-queries in a single LLM call; (2) chain-of-thought\nreasoning based on query execution plans, reflecting the steps a database\nengine takes during execution; and (3) a unique instance-aware synthetic\nexample generation technique, which offers specific few-shot demonstrations\ntailored to test questions.To identify the best candidate, a selection agent is\nemployed to rank the candidates through pairwise comparisons with a fine-tuned\nbinary-candidates selection LLM. This selection approach has been demonstrated\nto be more robust over alternatives. The proposed generators-selector framework\nnot only enhances the quality and diversity of SQL queries but also outperforms\nprevious methods. Overall, our proposed CHASE-SQL achieves the state-of-the-art\nexecution accuracy of 73.0% and 73.01% on the test set and development set of\nthe notable BIRD Text-to-SQL dataset benchmark, rendering CHASE-SQL the top\nsubmission of the leaderboard (at the time of paper submission).", "published": "2024-10-02 18:41:35", "link": "http://arxiv.org/abs/2410.01943v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DB"], "primary_category": "cs.LG"}
{"title": "Financial Sentiment Analysis on News and Reports Using Large Language\n  Models and FinBERT", "abstract": "Financial sentiment analysis (FSA) is crucial for evaluating market sentiment\nand making well-informed financial decisions. The advent of large language\nmodels (LLMs) such as BERT and its financial variant, FinBERT, has notably\nenhanced sentiment analysis capabilities. This paper investigates the\napplication of LLMs and FinBERT for FSA, comparing their performance on news\narticles, financial reports and company announcements. The study emphasizes the\nadvantages of prompt engineering with zero-shot and few-shot strategy to\nimprove sentiment classification accuracy. Experimental results indicate that\nGPT-4o, with few-shot examples of financial texts, can be as competent as a\nwell fine-tuned FinBERT in this specialized field.", "published": "2024-10-02 19:48:17", "link": "http://arxiv.org/abs/2410.01987v1", "categories": ["cs.IR", "cs.CL", "cs.SI", "q-fin.GN"], "primary_category": "cs.IR"}
{"title": "FLAG: Financial Long Document Classification via AMR-based GNN", "abstract": "The advent of large language models (LLMs) has initiated much research into\ntheir various financial applications. However, in applying LLMs on long\ndocuments, semantic relations are not explicitly incorporated, and a full or\narbitrarily sparse attention operation is employed. In recent years, progress\nhas been made in Abstract Meaning Representation (AMR), which is a graph-based\nrepresentation of text to preserve its semantic relations. Since AMR can\nrepresent semantic relationships at a deeper level, it can be beneficially\nutilized by graph neural networks (GNNs) for constructing effective\ndocument-level graph representations built upon LLM embeddings to predict\ntarget metrics in the financial domain. We propose FLAG: Financial Long\ndocument classification via AMR-based GNN, an AMR graph based framework to\ngenerate document-level embeddings for long financial document classification.\nWe construct document-level graphs from sentence-level AMR graphs, endow them\nwith specialized LLM word embeddings in the financial domain, apply a deep\nlearning mechanism that utilizes a GNN, and examine the efficacy of our\nAMR-based approach in predicting labeled target data from long financial\ndocuments. Extensive experiments are conducted on a dataset of quarterly\nearnings calls transcripts of companies in various sectors of the economy, as\nwell as on a corpus of more recent earnings calls of companies in the S&P 1500\nComposite Index. We find that our AMR-based approach outperforms fine-tuning\nLLMs directly on text in predicting stock price movement trends at different\ntime horizons in both datasets. Our work also outperforms previous work\nutilizing document graphs and GNNs for text classification.", "published": "2024-10-02 20:45:51", "link": "http://arxiv.org/abs/2410.02024v3", "categories": ["cs.CE", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CE"}
{"title": "Emo3D: Metric and Benchmarking Dataset for 3D Facial Expression\n  Generation from Emotion Description", "abstract": "Existing 3D facial emotion modeling have been constrained by limited emotion\nclasses and insufficient datasets. This paper introduces \"Emo3D\", an extensive\n\"Text-Image-Expression dataset\" spanning a wide spectrum of human emotions,\neach paired with images and 3D blendshapes. Leveraging Large Language Models\n(LLMs), we generate a diverse array of textual descriptions, facilitating the\ncapture of a broad spectrum of emotional expressions. Using this unique\ndataset, we conduct a comprehensive evaluation of language-based models'\nfine-tuning and vision-language models like Contranstive Language Image\nPretraining (CLIP) for 3D facial expression synthesis. We also introduce a new\nevaluation metric for this task to more directly measure the conveyed emotion.\nOur new evaluation metric, Emo3D, demonstrates its superiority over Mean\nSquared Error (MSE) metrics in assessing visual-text alignment and semantic\nrichness in 3D facial expressions associated with human emotions. \"Emo3D\" has\ngreat applications in animation design, virtual reality, and emotional\nhuman-computer interaction.", "published": "2024-10-02 21:31:24", "link": "http://arxiv.org/abs/2410.02049v1", "categories": ["cs.CV", "cs.CL", "cs.GR", "I.2.7; I.2.10"], "primary_category": "cs.CV"}
{"title": "Synthio: Augmenting Small-Scale Audio Classification Datasets with\n  Synthetic Data", "abstract": "We present Synthio, a novel approach for augmenting small-scale audio\nclassification datasets with synthetic data. Our goal is to improve audio\nclassification accuracy with limited labeled data. Traditional data\naugmentation techniques, which apply artificial transformations (e.g., adding\nrandom noise or masking segments), struggle to create data that captures the\ntrue diversity present in real-world audios. To address this shortcoming, we\npropose to augment the dataset with synthetic audio generated from\ntext-to-audio (T2A) diffusion models. However, synthesizing effective\naugmentations is challenging because not only should the generated data be\nacoustically consistent with the underlying small-scale dataset, but they\nshould also have sufficient compositional diversity. To overcome the first\nchallenge, we align the generations of the T2A model with the small-scale\ndataset using preference optimization. This ensures that the acoustic\ncharacteristics of the generated data remain consistent with the small-scale\ndataset. To address the second challenge, we propose a novel caption generation\ntechnique that leverages the reasoning capabilities of Large Language Models to\n(1) generate diverse and meaningful audio captions and (2) iteratively refine\ntheir quality. The generated captions are then used to prompt the aligned T2A\nmodel. We extensively evaluate Synthio on ten datasets and four simulated\nlimited-data settings. Results indicate our method consistently outperforms all\nbaselines by 0.1%-39% using a T2A model trained only on weakly-captioned\nAudioSet.", "published": "2024-10-02 22:05:36", "link": "http://arxiv.org/abs/2410.02056v2", "categories": ["eess.AS", "cs.AI", "cs.CL"], "primary_category": "eess.AS"}
{"title": "EMMA: Efficient Visual Alignment in Multi-Modal LLMs", "abstract": "Multi-modal Large Language Models (MLLMs) have recently exhibited impressive\ngeneral-purpose capabilities by leveraging vision foundation models to encode\nthe core concepts of images into representations. These are then combined with\ninstructions and processed by the language model to generate high-quality\nresponses. Despite significant progress in enhancing the language component,\nchallenges persist in optimally fusing visual encodings within the language\nmodel for task-specific adaptability. Recent research has focused on improving\nthis fusion through modality adaptation modules but at the cost of\nsignificantly increased model complexity and training data needs. In this\npaper, we propose EMMA (Efficient Multi-Modal Adaptation), a lightweight\ncross-modality module designed to efficiently fuse visual and textual\nencodings, generating instruction-aware visual representations for the language\nmodel. Our key contributions include: (1) an efficient early fusion mechanism\nthat integrates vision and language representations with minimal added\nparameters (less than 0.2% increase in model size), (2) an in-depth\ninterpretability analysis that sheds light on the internal mechanisms of the\nproposed method; (3) comprehensive experiments that demonstrate notable\nimprovements on both specialized and general benchmarks for MLLMs. Empirical\nresults show that EMMA boosts performance across multiple tasks by up to 9.3%\nwhile significantly improving robustness against hallucinations. Our code is\navailable at https://github.com/SaraGhazanfari/EMMA", "published": "2024-10-02 23:00:31", "link": "http://arxiv.org/abs/2410.02080v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "A Watermark for Black-Box Language Models", "abstract": "Watermarking has recently emerged as an effective strategy for detecting the\noutputs of large language models (LLMs). Most existing schemes require\nwhite-box access to the model's next-token probability distribution, which is\ntypically not accessible to downstream users of an LLM API. In this work, we\npropose a principled watermarking scheme that requires only the ability to\nsample sequences from the LLM (i.e. black-box access), boasts a distortion-free\nproperty, and can be chained or nested using multiple secret keys. We provide\nperformance guarantees, demonstrate how it can be leveraged when white-box\naccess is available, and show when it can outperform existing white-box schemes\nvia comprehensive experiments.", "published": "2024-10-02 23:39:19", "link": "http://arxiv.org/abs/2410.02099v2", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Efficient Streaming LLM for Speech Recognition", "abstract": "Recent works have shown that prompting large language models with audio\nencodings can unlock speech recognition capabilities. However, existing\ntechniques do not scale efficiently, especially while handling long form\nstreaming audio inputs -- not only do they extrapolate poorly beyond the audio\nlength seen during training, but they are also computationally inefficient due\nto the quadratic cost of attention.\n  In this work, we introduce SpeechLLM-XL, a linear scaling decoder-only model\nfor streaming speech recognition. We process audios in configurable chunks\nusing limited attention window for reduced computation, and the text tokens for\neach audio chunk are generated auto-regressively until an EOS is predicted.\nDuring training, the transcript is segmented into chunks, using a CTC forced\nalignment estimated from encoder output. SpeechLLM-XL with 1.28 seconds chunk\nsize achieves 2.7%/6.7% WER on LibriSpeech test clean/other, and it shows no\nquality degradation on long form utterances 10x longer than the training\nutterances.", "published": "2024-10-02 01:54:35", "link": "http://arxiv.org/abs/2410.03752v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Getting in the Door: Streamlining Intake in Civil Legal Services with\n  Large Language Models", "abstract": "Legal intake, the process of finding out if an applicant is eligible for help\nfrom a free legal aid program, takes significant time and resources. In part\nthis is because eligibility criteria are nuanced, open-textured, and require\nfrequent revision as grants start and end. In this paper, we investigate the\nuse of large language models (LLMs) to reduce this burden. We describe a\ndigital intake platform that combines logical rules with LLMs to offer\neligibility recommendations, and we evaluate the ability of 8 different LLMs to\nperform this task. We find promising results for this approach to help close\nthe access to justice gap, with the best model reaching an F1 score of .82,\nwhile minimizing false negatives.", "published": "2024-10-02 13:20:14", "link": "http://arxiv.org/abs/2410.03762v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.HC"}
{"title": "Words that Represent Peace", "abstract": "We used data from LexisNexis to determine the words in news media that best\nclassifies countries as higher or lower peace. We found that higher peace news\nis characterized by themes of finance, daily actitivities, and health and that\nlower peace news is characterized by themes of politics, government, and legal\nissues. This work provides a starting point to measure levels of peace and\nidentify the social processes that underly those words.", "published": "2024-10-02 14:23:48", "link": "http://arxiv.org/abs/2410.03764v1", "categories": ["cs.CL", "cs.CY", "cs.LG", "62H30", "I.2.6"], "primary_category": "cs.CL"}
{"title": "FutureFill: Fast Generation from Convolutional Sequence Models", "abstract": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill - a method for fast generation that\napplies to any sequence prediction algorithm based on convolutional operators.\nOur approach reduces the generation time requirement from quadratic to\nquasilinear relative to the context length. Additionally, FutureFill requires a\nprefill cache sized only by the number of tokens generated, which is smaller\nthan the cache requirements for standard convolutional and attention-based\nmodels. We validate our theoretical findings with experimental evidence\ndemonstrating correctness and efficiency gains in a synthetic generation task.", "published": "2024-10-02 15:22:08", "link": "http://arxiv.org/abs/2410.03766v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Reasoning Elicitation in Language Models via Counterfactual Feedback", "abstract": "Despite the increasing effectiveness of language models, their reasoning\ncapabilities remain underdeveloped. In particular, causal reasoning through\ncounterfactual question answering is lacking. This work aims to bridge this\ngap. We first derive novel metrics that balance accuracy in factual and\ncounterfactual questions, capturing a more complete view of the reasoning\nabilities of language models than traditional factual-only based metrics.\nSecond, we propose several fine-tuning approaches that aim to elicit better\nreasoning mechanisms, in the sense of the proposed metrics. Finally, we\nevaluate the performance of the fine-tuned language models in a variety of\nrealistic scenarios. In particular, we investigate to what extent our\nfine-tuning approaches systemically achieve better generalization with respect\nto the base models in several problems that require, among others, inductive\nand deductive reasoning capabilities.", "published": "2024-10-02 15:33:30", "link": "http://arxiv.org/abs/2410.03767v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Hidden in Plain Text: Emergence & Mitigation of Steganographic Collusion\n  in LLMs", "abstract": "The rapid proliferation of frontier model agents promises significant\nsocietal advances but also raises concerns about systemic risks arising from\nunsafe interactions. Collusion to the disadvantage of others has been\nidentified as a central form of undesirable agent cooperation. The use of\ninformation hiding (steganography) in agent communications could render\ncollusion practically undetectable. This underscores the need for evaluation\nframeworks to monitor and mitigate steganographic collusion capabilities. We\naddress a crucial gap in the literature by demonstrating, for the first time,\nthat robust steganographic collusion in LLMs can arise indirectly from\noptimization pressure. To investigate this problem we design two approaches --\na gradient-based reinforcement learning (GBRL) method and an in-context\nreinforcement learning (ICRL) method -- for reliably eliciting sophisticated\nLLM-generated linguistic text steganography. Importantly, we find that emergent\nsteganographic collusion can be robust to both passive steganalytic oversight\nof model outputs and active mitigation through communication paraphrasing. We\ncontribute a novel model evaluation framework and discuss limitations and\nfuture work. Our findings imply that effective risk mitigation from\nsteganographic collusion post-deployment requires innovation in passive and\nactive oversight techniques.", "published": "2024-10-02 16:18:33", "link": "http://arxiv.org/abs/2410.03768v1", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SciSafeEval: A Comprehensive Benchmark for Safety Alignment of Large\n  Language Models in Scientific Tasks", "abstract": "Large language models (LLMs) have a transformative impact on a variety of\nscientific tasks across disciplines including biology, chemistry, medicine, and\nphysics. However, ensuring the safety alignment of these models in scientific\nresearch remains an underexplored area, with existing benchmarks primarily\nfocusing on textual content and overlooking key scientific representations such\nas molecular, protein, and genomic languages. Moreover, the safety mechanisms\nof LLMs in scientific tasks are insufficiently studied. To address these\nlimitations, we introduce SciSafeEval, a comprehensive benchmark designed to\nevaluate the safety alignment of LLMs across a range of scientific tasks.\nSciSafeEval spans multiple scientific languages-including textual, molecular,\nprotein, and genomic-and covers a wide range of scientific domains. We evaluate\nLLMs in zero-shot, few-shot and chain-of-thought settings, and introduce a\n\"jailbreak\" enhancement feature that challenges LLMs equipped with safety\nguardrails, rigorously testing their defenses against malicious intention. Our\nbenchmark surpasses existing safety datasets in both scale and scope, providing\na robust platform for assessing the safety and performance of LLMs in\nscientific contexts. This work aims to facilitate the responsible development\nand deployment of LLMs, promoting alignment with safety and ethical standards\nin scientific research.", "published": "2024-10-02 16:34:48", "link": "http://arxiv.org/abs/2410.03769v2", "categories": ["cs.CL", "cs.AI", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Hate Speech Detection Using Cross-Platform Social Media Data In English\n  and German Language", "abstract": "Hate speech has grown into a pervasive phenomenon, intensifying during times\nof crisis, elections, and social unrest. Multiple approaches have been\ndeveloped to detect hate speech using artificial intelligence, but a\ngeneralized model is yet unaccomplished. The challenge for hate speech\ndetection as text classification is the cost of obtaining high-quality training\ndata. This study focuses on detecting bilingual hate speech in YouTube comments\nand measuring the impact of using additional data from other platforms in the\nperformance of the classification model. We examine the value of additional\ntraining datasets from cross-platforms for improving the performance of\nclassification models. We also included factors such as content similarity,\ndefinition similarity, and common hate words to measure the impact of datasets\non performance. Our findings show that adding more similar datasets based on\ncontent similarity, hate words, and definitions improves the performance of\nclassification models. The best performance was obtained by combining datasets\nfrom YouTube comments, Twitter, and Gab with an F1-score of 0.74 and 0.68 for\nEnglish and German YouTube comments.", "published": "2024-10-02 10:22:53", "link": "http://arxiv.org/abs/2410.05287v1", "categories": ["cs.CL", "cs.AI", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Stars, Stripes, and Silicon: Unravelling the ChatGPT's All-American,\n  Monochrome, Cis-centric Bias", "abstract": "This paper investigates the challenges associated with bias, toxicity,\nunreliability, and lack of robustness in large language models (LLMs) such as\nChatGPT. It emphasizes that these issues primarily stem from the quality and\ndiversity of data on which LLMs are trained, rather than the model\narchitectures themselves. As LLMs are increasingly integrated into various\nreal-world applications, their potential to negatively impact society by\namplifying existing biases and generating harmful content becomes a pressing\nconcern. The paper calls for interdisciplinary efforts to address these\nchallenges. Additionally, it highlights the need for collaboration between\nresearchers, practitioners, and stakeholders to establish governance\nframeworks, oversight, and accountability mechanisms to mitigate the harmful\nconsequences of biased LLMs. By proactively addressing these challenges, the AI\ncommunity can harness the enormous potential of LLMs for the betterment of\nsociety without perpetuating harmful biases or exacerbating existing\ninequalities.", "published": "2024-10-02 08:55:00", "link": "http://arxiv.org/abs/2410.13868v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Geometric Signatures of Compositionality Across a Language Model's\n  Lifetime", "abstract": "By virtue of linguistic compositionality, few syntactic rules and a finite\nlexicon can generate an unbounded number of sentences. That is, language,\nthough seemingly high-dimensional, can be explained using relatively few\ndegrees of freedom. An open question is whether contemporary language models\n(LMs) reflect the intrinsic simplicity of language that is enabled by\ncompositionality. We take a geometric view of this problem by relating the\ndegree of compositionality in a dataset to the intrinsic dimension (ID) of its\nrepresentations under an LM, a measure of feature complexity. We find not only\nthat the degree of dataset compositionality is reflected in representations'\nID, but that the relationship between compositionality and geometric complexity\narises due to learned linguistic features over training. Finally, our analyses\nreveal a striking contrast between nonlinear and linear dimensionality, showing\nthey respectively encode semantic and superficial aspects of linguistic\ncomposition.", "published": "2024-10-02 11:54:06", "link": "http://arxiv.org/abs/2410.01444v3", "categories": ["cs.CL", "cs.AI", "cs.IT", "cs.LG", "math.IT"], "primary_category": "cs.CL"}
{"title": "Composing Global Optimizers to Reasoning Tasks via Algebraic Objects in\n  Neural Nets", "abstract": "We prove rich algebraic structures of the solution space for 2-layer neural\nnetworks with quadratic activation and $L_2$ loss, trained on reasoning tasks\nin Abelian group (e.g., modular addition). Such a rich structure enables\nanalytical construction of global optimal solutions from partial solutions that\nonly satisfy part of the loss, despite its high nonlinearity. We coin the\nframework as CoGO (Composing Global Optimizers). Specifically, we show that the\nweight space over different numbers of hidden nodes of the 2-layer network is\nequipped with a semi-ring algebraic structure, and the loss function to be\noptimized consists of monomial potentials, which are ring homomorphism,\nallowing partial solutions to be composed into global ones by ring addition and\nmultiplication. Our experiments show that around $95\\%$ of the solutions\nobtained by gradient descent match exactly our theoretical constructions.\nAlthough the global optimizers constructed only required a small number of\nhidden nodes, our analysis on gradient dynamics shows that\nover-parameterization asymptotically decouples training dynamics and is\nbeneficial. We further show that training dynamics favors simpler solutions\nunder weight decay, and thus high-order global optimizers such as perfect\nmemorization are unfavorable. Code can be found at\nhttps://github.com/facebookresearch/luckmatters/tree/yuandong3/ssl/real-dataset.", "published": "2024-10-02 17:33:26", "link": "http://arxiv.org/abs/2410.01779v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "math.AC", "math.RA"], "primary_category": "cs.LG"}
{"title": "Restorative Speech Enhancement: A Progressive Approach Using SE and\n  Codec Modules", "abstract": "In challenging environments with significant noise and reverberation,\ntraditional speech enhancement (SE) methods often lead to over-suppressed\nspeech, creating artifacts during listening and harming downstream tasks\nperformance. To overcome these limitations, we propose a novel approach called\nRestorative SE (RestSE), which combines a lightweight SE module with a\ngenerative codec module to progressively enhance and restore speech quality.\nThe SE module initially reduces noise, while the codec module subsequently\nperforms dereverberation and restores speech using generative capabilities. We\nsystematically explore various quantization techniques within the codec module\nto optimize performance. Additionally, we introduce a weighted loss function\nand feature fusion that merges the SE output with the original mixture,\nparticularly at segments where the SE output is heavily distorted. Experimental\nresults demonstrate the effectiveness of our proposed method in enhancing\nspeech quality under adverse conditions. Audio demos are available at:\nhttps://sophie091524.github.io/RestorativeSE/.", "published": "2024-10-02 01:04:01", "link": "http://arxiv.org/abs/2410.01150v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Generating Symbolic Music from Natural Language Prompts using an\n  LLM-Enhanced Dataset", "abstract": "Recent years have seen many audio-domain text-to-music generation models that\nrely on large amounts of text-audio pairs for training. However,\nsymbolic-domain controllable music generation has lagged behind partly due to\nthe lack of a large-scale symbolic music dataset with extensive metadata and\ncaptions. In this work, we present MetaScore, a new dataset consisting of 963K\nmusical scores paired with rich metadata, including free-form user-annotated\ntags, collected from an online music forum. To approach text-to-music\ngeneration, we leverage a pretrained large language model (LLM) to generate\npseudo natural language captions from the metadata. With the LLM-enhanced\nMetaScore, we train a text-conditioned music generation model that learns to\ngenerate symbolic music from the pseudo captions, allowing control of\ninstruments, genre, composer, complexity and other free-form music descriptors.\nIn addition, we train a tag-conditioned system that supports a predefined set\nof tags available in MetaScore. Our experimental results show that both the\nproposed text-to-music and tags-to-music models outperform a baseline\ntext-to-music model in a listening test, while the text-based system offers a\nmore natural interface that allows free-form natural language prompts.", "published": "2024-10-02 23:10:21", "link": "http://arxiv.org/abs/2410.02084v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Takin-VC: Expressive Zero-Shot Voice Conversion via Adaptive Hybrid\n  Content Encoding and Enhanced Timbre Modeling", "abstract": "Expressive zero-shot voice conversion (VC) is a critical and challenging task\nthat aims to transform the source timbre into an arbitrary unseen speaker while\npreserving the original content and expressive qualities. Despite recent\nprogress in zero-shot VC, there remains considerable potential for improvements\nin speaker similarity and speech naturalness. Moreover, existing zero-shot VC\nsystems struggle to fully reproduce paralinguistic information in highly\nexpressive speech, such as breathing, crying, and emotional nuances, limiting\ntheir practical applicability. To address these issues, we propose Takin-VC, a\nnovel expressive zero-shot VC framework via adaptive hybrid content encoding\nand memory-augmented context-aware timbre modeling. Specifically, we introduce\nan innovative hybrid content encoder that incorporates an adaptive fusion\nmodule, capable of effectively integrating quantized features of the\npre-trained WavLM and HybridFormer in an implicit manner, so as to extract\nprecise linguistic features while enriching paralinguistic elements. For timbre\nmodeling, we propose advanced memory-augmented and context-aware modules to\ngenerate high-quality target timbre features and fused representations that\nseamlessly align source content with target timbre. To enhance real-time\nperformance, we advocate a conditional flow matching model to reconstruct the\nMel-spectrogram of the source speech. Experimental results show that our\nTakin-VC consistently surpasses state-of-the-art VC systems, achieving notable\nimprovements in terms of speech naturalness, speech expressiveness, and speaker\nsimilarity, while offering enhanced inference speed.", "published": "2024-10-02 09:07:33", "link": "http://arxiv.org/abs/2410.01350v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "TIGER: Time-frequency Interleaved Gain Extraction and Reconstruction for\n  Efficient Speech Separation", "abstract": "In recent years, much speech separation research has focused primarily on\nimproving model performance. However, for low-latency speech processing\nsystems, high efficiency is equally important. Therefore, we propose a speech\nseparation model with significantly reduced parameters and computational costs:\nTime-frequency Interleaved Gain Extraction and Reconstruction network (TIGER).\nTIGER leverages prior knowledge to divide frequency bands and compresses\nfrequency information. We employ a multi-scale selective attention module to\nextract contextual features while introducing a full-frequency-frame attention\nmodule to capture both temporal and frequency contextual information.\nAdditionally, to more realistically evaluate the performance of speech\nseparation models in complex acoustic environments, we introduce a dataset\ncalled EchoSet. This dataset includes noise and more realistic reverberation\n(e.g., considering object occlusions and material properties), with speech from\ntwo speakers overlapping at random proportions. Experimental results showed\nthat models trained on EchoSet had better generalization ability than those\ntrained on other datasets compared to the data collected in the physical world,\nwhich validated the practical value of the EchoSet. On EchoSet and real-world\ndata, TIGER significantly reduces the number of parameters by 94.3% and the\nMACs by 95.3% while achieving performance surpassing the state-of-the-art\n(SOTA) model TF-GridNet.", "published": "2024-10-02 12:21:06", "link": "http://arxiv.org/abs/2410.01469v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SonicSim: A customizable simulation platform for speech processing in\n  moving sound source scenarios", "abstract": "Systematic evaluation of speech separation and enhancement models under\nmoving sound source conditions requires extensive and diverse data. However,\nreal-world datasets often lack sufficient data for training and evaluation, and\nsynthetic datasets, while larger, lack acoustic realism. Consequently, neither\neffectively meets practical needs. To address this issue, we introduce\nSonicSim, a synthetic toolkit based on the embodied AI simulation platform\nHabitat-sim, designed to generate highly customizable data for moving sound\nsources. SonicSim supports multi-level adjustments, including scene-level,\nmicrophone-level, and source-level adjustments, enabling the creation of more\ndiverse synthetic data. Leveraging SonicSim, we constructed a benchmark dataset\ncalled SonicSet, utilizing LibriSpeech, Freesound Dataset 50k (FSD50K), Free\nMusic Archive (FMA), and 90 scenes from Matterport3D to evaluate speech\nseparation and enhancement models. Additionally, to investigate the differences\nbetween synthetic and real-world data, we selected 5 hours of raw,\nnon-reverberant data from the SonicSet validation set and recorded a real-world\nspeech separation dataset, providing a reference for comparing SonicSet with\nother synthetic datasets. For speech enhancement, we utilized the real-world\ndataset RealMAN to validate the acoustic gap between SonicSet and existing\nsynthetic datasets. The results indicate that models trained on SonicSet\ngeneralize better to real-world scenarios compared to other synthetic datasets.\nThe code is publicly available at https://cslikai.cn/SonicSim/.", "published": "2024-10-02 12:33:59", "link": "http://arxiv.org/abs/2410.01481v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "HRTF Estimation using a Score-based Prior", "abstract": "We present a head-related transfer function (HRTF) estimation method which\nrelies on a data-driven prior given by a score-based diffusion model. The HRTF\nis estimated in reverberant environments using natural excitation signals, e.g.\nhuman speech. The impulse response of the room is estimated along with the HRTF\nby optimizing a parametric model of reverberation based on the statistical\nbehaviour of room acoustics. The posterior distribution of HRTF given the\nreverberant measurement and excitation signal is modelled using the score-based\nHRTF prior and a log-likelihood approximation. We show that the resulting\nmethod outperforms several baselines, including an oracle recommender system\nthat assigns the optimal HRTF in our training set based on the smallest\ndistance to the true HRTF at the given direction of arrival. In particular, we\nshow that the diffusion prior can account for the large variability of\nhigh-frequency content in HRTFs.", "published": "2024-10-02 14:00:41", "link": "http://arxiv.org/abs/2410.01562v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "PerTok: Expressive Encoding and Modeling of Symbolic Musical Ideas and\n  Variations", "abstract": "We introduce Cadenza, a new multi-stage generative framework for predicting\nexpressive variations of symbolic musical ideas as well as unconditional\ngenerations. To accomplish this we propose a novel MIDI encoding method, PerTok\n(Performance Tokenizer) that captures minute expressive details whilst reducing\nsequence length up to 59% and vocabulary size up to 95% for polyphonic,\nmonophonic and rhythmic tasks. The proposed framework comprises of two\nsequential stages: 1) Composer and 2) Performer. The Composer model is a\ntransformer-based Variational Autoencoder (VAE), with Rotary Positional\nEmbeddings (RoPE)ROPE and an autoregressive decoder modified to more\neffectively integrate the latent codes of the input musical idea. The Performer\nmodel is a bidirectional transformer encoder that is separately trained to\npredict velocities and microtimings on MIDI sequences. Objective and human\nevaluations demonstrate Cadenza's versatile capability in 1) matching other\nunconditional state-of-the-art symbolic models in musical quality whilst\nsounding more expressive, and 2) composing new, expressive ideas that are both\nstylistically related to the input whilst providing novel ideas to the user.\nOur framework is designed, researched and implemented with the objective of\nethically providing inspiration for musicians.", "published": "2024-10-02 22:11:31", "link": "http://arxiv.org/abs/2410.02060v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
