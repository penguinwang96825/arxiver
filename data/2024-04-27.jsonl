{"title": "Medical Vision-Language Pre-Training for Brain Abnormalities", "abstract": "Vision-language models have become increasingly powerful for tasks that\nrequire an understanding of both visual and linguistic elements, bridging the\ngap between these modalities. In the context of multimodal clinical AI, there\nis a growing need for models that possess domain-specific knowledge, as\nexisting models often lack the expertise required for medical applications. In\nthis paper, we take brain abnormalities as an example to demonstrate how to\nautomatically collect medical image-text aligned data for pretraining from\npublic resources such as PubMed. In particular, we present a pipeline that\nstreamlines the pre-training process by initially collecting a large brain\nimage-text dataset from case reports and published journals and subsequently\nconstructing a high-performance vision-language model tailored to specific\nmedical tasks. We also investigate the unique challenge of mapping subfigures\nto subcaptions in the medical domain. We evaluated the resulting model with\nquantitative and qualitative intrinsic evaluations. The resulting dataset and\nour code can be found here\nhttps://github.com/masoud-monajati/MedVL_pretraining_pipeline", "published": "2024-04-27 05:03:42", "link": "http://arxiv.org/abs/2404.17779v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Temporal Scaling Law for Large Language Models", "abstract": "Recently, Large Language Models (LLMs) have been widely adopted in a wide\nrange of tasks, leading to increasing attention towards the research on how\nscaling LLMs affects their performance. Existing works, termed Scaling Laws,\nhave discovered that the final test loss of LLMs scales as power-laws with\nmodel size, computational budget, and dataset size. However, the temporal\nchange of the test loss of an LLM throughout its pre-training process remains\nunexplored, though it is valuable in many aspects, such as selecting better\nhyperparameters \\textit{directly} on the target LLM. In this paper, we propose\nthe novel concept of Temporal Scaling Law, studying how the test loss of an LLM\nevolves as the training steps scale up. In contrast to modeling the test loss\nas a whole in a coarse-grained manner, we break it down and dive into the\nfine-grained test loss of each token position, and further develop a dynamic\nhyperbolic-law. Afterwards, we derive the much more precise temporal scaling\nlaw by studying the temporal patterns of the parameters in the dynamic\nhyperbolic-law. Results on both in-distribution (ID) and out-of-distribution\n(OOD) validation datasets demonstrate that our temporal scaling law accurately\npredicts the test loss of LLMs across training steps. Our temporal scaling law\nhas broad practical applications. First, it enables direct and efficient\nhyperparameter selection on the target LLM, such as data mixture proportions.\nSecondly, viewing the LLM pre-training dynamics from the token position\ngranularity provides some insights to enhance the understanding of LLM\npre-training.", "published": "2024-04-27 05:49:11", "link": "http://arxiv.org/abs/2404.17785v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Scaffold-BPE: Enhancing Byte Pair Encoding for Large Language Models\n  with Simple and Effective Scaffold Token Removal", "abstract": "Byte Pair Encoding (BPE) serves as a foundation method for text tokenization\nin the Natural Language Processing (NLP) field. Despite its wide adoption, the\noriginal BPE algorithm harbors an inherent flaw: it inadvertently introduces a\nfrequency imbalance for tokens in the text corpus. Since BPE iteratively merges\nthe most frequent token pair in the text corpus to generate a new token and\nkeeps all generated tokens in the vocabulary, it unavoidably holds tokens that\nprimarily act as components of a longer token and appear infrequently on their\nown. We term such tokens as Scaffold Tokens. Due to their infrequent\noccurrences in the text corpus, Scaffold Tokens pose a learning imbalance\nissue. To address that issue, we propose Scaffold-BPE, which incorporates a\ndynamic scaffold token removal mechanism by parameter-free, computation-light,\nand easy-to-implement modifications to the original BPE method. This novel\napproach ensures the exclusion of low-frequency Scaffold Tokens from the token\nrepresentations for given texts, thereby mitigating the issue of frequency\nimbalance and facilitating model training. On extensive experiments across\nlanguage modeling and even machine translation, Scaffold-BPE consistently\noutperforms the original BPE, well demonstrating its effectiveness.", "published": "2024-04-27 07:12:07", "link": "http://arxiv.org/abs/2404.17808v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluation of Few-Shot Learning for Classification Tasks in the Polish\n  Language", "abstract": "We introduce a few-shot benchmark consisting of 7 different classification\ntasks native to the Polish language. We conducted an empirical comparison with\n0 and 16 shots between fine-tuning, linear probing, SetFit, and in-context\nlearning (ICL) using various pre-trained commercial and open-source models. Our\nfindings reveal that ICL achieves the best performance, with commercial models\nlike GPT-3.5 and GPT-4 attaining the best performance. However, there remains a\nsignificant 14 percentage points gap between our best few-shot learning score\nand the performance of HerBERT-large fine-tuned on the entire training dataset.\nAmong the techniques, SetFit emerges as the second-best approach, closely\nfollowed by linear probing. We observed the worst and most unstable performance\nwith non-linear head fine-tuning. Results for ICL indicate that continual\npre-training of models like Mistral-7b or Llama-2-13b on Polish corpora is\nbeneficial. This is confirmed by the improved performances of Bielik-7b and\nTrurl-13b, respectively. To further support experiments in few-shot learning\nfor Polish, we are releasing handcrafted templates for the ICL.", "published": "2024-04-27 08:53:58", "link": "http://arxiv.org/abs/2404.17832v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "VANER: Leveraging Large Language Model for Versatile and Adaptive\n  Biomedical Named Entity Recognition", "abstract": "Prevalent solution for BioNER involves using representation learning\ntechniques coupled with sequence labeling. However, such methods are inherently\ntask-specific, demonstrate poor generalizability, and often require dedicated\nmodel for each dataset. To leverage the versatile capabilities of recently\nremarkable large language models (LLMs), several endeavors have explored\ngenerative approaches to entity extraction. Yet, these approaches often fall\nshort of the effectiveness of previouly sequence labeling approaches. In this\npaper, we utilize the open-sourced LLM LLaMA2 as the backbone model, and design\nspecific instructions to distinguish between different types of entities and\ndatasets. By combining the LLM's understanding of instructions with sequence\nlabeling techniques, we use mix of datasets to train a model capable of\nextracting various types of entities. Given that the backbone LLMs lacks\nspecialized medical knowledge, we also integrate external entity knowledge\nbases and employ instruction tuning to compel the model to densely recognize\ncarefully curated entities. Our model VANER, trained with a small partition of\nparameters, significantly outperforms previous LLMs-based models and, for the\nfirst time, as a model based on LLM, surpasses the majority of conventional\nstate-of-the-art BioNER systems, achieving the highest F1 scores across three\ndatasets.", "published": "2024-04-27 09:00:39", "link": "http://arxiv.org/abs/2404.17835v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Toxicity Classification in Ukrainian", "abstract": "The task of toxicity detection is still a relevant task, especially in the\ncontext of safe and fair LMs development. Nevertheless, labeled binary toxicity\nclassification corpora are not available for all languages, which is\nunderstandable given the resource-intensive nature of the annotation process.\nUkrainian, in particular, is among the languages lacking such resources. To our\nknowledge, there has been no existing toxicity classification corpus in\nUkrainian. In this study, we aim to fill this gap by investigating\ncross-lingual knowledge transfer techniques and creating labeled corpora by:\n(i)~translating from an English corpus, (ii)~filtering toxic samples using\nkeywords, and (iii)~annotating with crowdsourcing. We compare LLMs prompting\nand other cross-lingual transfer approaches with and without fine-tuning\noffering insights into the most robust and efficient baselines.", "published": "2024-04-27 09:20:13", "link": "http://arxiv.org/abs/2404.17841v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting Multi-modal Emotion Learning with Broad State Space Models\n  and Probability-guidance Fusion", "abstract": "Multi-modal Emotion Recognition in Conversation (MERC) has received\nconsiderable attention in various fields, e.g., human-computer interaction and\nrecommendation systems. Most existing works perform feature disentanglement and\nfusion to extract emotional contextual information from multi-modal features\nand emotion classification. After revisiting the characteristic of MERC, we\nargue that long-range contextual semantic information should be extracted in\nthe feature disentanglement stage and the inter-modal semantic information\nconsistency should be maximized in the feature fusion stage. Inspired by recent\nState Space Models (SSMs), Mamba can efficiently model long-distance\ndependencies. Therefore, in this work, we fully consider the above insights to\nfurther improve the performance of MERC. Specifically, on the one hand, in the\nfeature disentanglement stage, we propose a Broad Mamba, which does not rely on\na self-attention mechanism for sequence modeling, but uses state space models\nto compress emotional representation, and utilizes broad learning systems to\nexplore the potential data distribution in broad space. Different from previous\nSSMs, we design a bidirectional SSM convolution to extract global context\ninformation. On the other hand, we design a multi-modal fusion strategy based\non probability guidance to maximize the consistency of information between\nmodalities. Experimental results show that the proposed method can overcome the\ncomputational and memory limitations of Transformer when modeling long-distance\ncontexts, and has great potential to become a next-generation general\narchitecture in MERC.", "published": "2024-04-27 10:22:03", "link": "http://arxiv.org/abs/2404.17858v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting Multimodal Emotion Recognition in Conversation from the\n  Perspective of Graph Spectrum", "abstract": "Efficiently capturing consistent and complementary semantic features in a\nmultimodal conversation context is crucial for Multimodal Emotion Recognition\nin Conversation (MERC). Existing methods mainly use graph structures to model\ndialogue context semantic dependencies and employ Graph Neural Networks (GNN)\nto capture multimodal semantic features for emotion recognition. However, these\nmethods are limited by some inherent characteristics of GNN, such as\nover-smoothing and low-pass filtering, resulting in the inability to learn\nlong-distance consistency information and complementary information\nefficiently. Since consistency and complementarity information correspond to\nlow-frequency and high-frequency information, respectively, this paper revisits\nthe problem of multimodal emotion recognition in conversation from the\nperspective of the graph spectrum. Specifically, we propose a\nGraph-Spectrum-based Multimodal Consistency and Complementary collaborative\nlearning framework GS-MCC. First, GS-MCC uses a sliding window to construct a\nmultimodal interaction graph to model conversational relationships and uses\nefficient Fourier graph operators to extract long-distance high-frequency and\nlow-frequency information, respectively. Then, GS-MCC uses contrastive learning\nto construct self-supervised signals that reflect complementarity and\nconsistent semantic collaboration with high and low-frequency signals, thereby\nimproving the ability of high and low-frequency information to reflect real\nemotions. Finally, GS-MCC inputs the collaborative high and low-frequency\ninformation into the MLP network and softmax function for emotion prediction.\nExtensive experiments have proven the superiority of the GS-MCC architecture\nproposed in this paper on two benchmark data sets.", "published": "2024-04-27 10:47:07", "link": "http://arxiv.org/abs/2404.17862v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Languages to Geographies: Towards Evaluating Cultural Bias in Hate\n  Speech Datasets", "abstract": "Perceptions of hate can vary greatly across cultural contexts. Hate speech\n(HS) datasets, however, have traditionally been developed by language. This\nhides potential cultural biases, as one language may be spoken in different\ncountries home to different cultures. In this work, we evaluate cultural bias\nin HS datasets by leveraging two interrelated cultural proxies: language and\ngeography. We conduct a systematic survey of HS datasets in eight languages and\nconfirm past findings on their English-language bias, but also show that this\nbias has been steadily decreasing in the past few years. For three\ngeographically-widespread languages -- English, Arabic and Spanish -- we then\nleverage geographical metadata from tweets to approximate geo-cultural contexts\nby pairing language and country information. We find that HS datasets for these\nlanguages exhibit a strong geo-cultural bias, largely overrepresenting a\nhandful of countries (e.g., US and UK for English) relative to their prominence\nin both the broader social media population and the general population speaking\nthese languages. Based on these findings, we formulate recommendations for the\ncreation of future HS datasets.", "published": "2024-04-27 12:10:10", "link": "http://arxiv.org/abs/2404.17874v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PromptCL: Improving Event Representation via Prompt Template and\n  Contrastive Learning", "abstract": "The representation of events in text plays a significant role in various NLP\ntasks. Recent research demonstrates that contrastive learning has the ability\nto improve event comprehension capabilities of Pre-trained Language Models\n(PLMs) and enhance the performance of event representation learning. However,\nthe efficacy of event representation learning based on contrastive learning and\nPLMs is limited by the short length of event texts. The length of event texts\ndiffers significantly from the text length used in the pre-training of PLMs. As\na result, there is inconsistency in the distribution of text length between\npre-training and event representation learning, which may undermine the\nlearning process of event representation based on PLMs. In this study, we\npresent PromptCL, a novel framework for event representation learning that\neffectively elicits the capabilities of PLMs to comprehensively capture the\nsemantics of short event texts. PromptCL utilizes a Prompt template borrowed\nfrom prompt learning to expand the input text during Contrastive Learning. This\nhelps in enhancing the event representation learning by providing a structured\noutline of the event components. Moreover, we propose Subject-Predicate-Object\n(SPO) word order and Event-oriented Masked Language Modeling (EventMLM) to\ntrain PLMs to understand the relationships between event components. Our\nexperimental results demonstrate that PromptCL outperforms state-of-the-art\nbaselines on event related tasks. Additionally, we conduct a thorough analysis\nand demonstrate that using a prompt results in improved generalization\ncapabilities for event representations. Our code will be available at\nhttps://github.com/YuboFeng2023/PromptCL.", "published": "2024-04-27 12:22:43", "link": "http://arxiv.org/abs/2404.17877v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tool Calling: Enhancing Medication Consultation via Retrieval-Augmented\n  Large Language Models", "abstract": "Large-scale language models (LLMs) have achieved remarkable success across\nvarious language tasks but suffer from hallucinations and temporal\nmisalignment. To mitigate these shortcomings, Retrieval-augmented generation\n(RAG) has been utilized to provide external knowledge to facilitate the answer\ngeneration. However, applying such models to the medical domain faces several\nchallenges due to the lack of domain-specific knowledge and the intricacy of\nreal-world scenarios. In this study, we explore LLMs with RAG framework for\nknowledge-intensive tasks in the medical field. To evaluate the capabilities of\nLLMs, we introduce MedicineQA, a multi-round dialogue benchmark that simulates\nthe real-world medication consultation scenario and requires LLMs to answer\nwith retrieved evidence from the medicine database. MedicineQA contains 300\nmulti-round question-answering pairs, each embedded within a detailed dialogue\nhistory, highlighting the challenge posed by this knowledge-intensive task to\ncurrent LLMs. We further propose a new \\textit{Distill-Retrieve-Read} framework\ninstead of the previous \\textit{Retrieve-then-Read}. Specifically, the\ndistillation and retrieval process utilizes a tool calling mechanism to\nformulate search queries that emulate the keyword-based inquiries used by\nsearch engines. With experimental results, we show that our framework brings\nnotable performance improvements and surpasses the previous counterparts in the\nevidence retrieval process in terms of evidence retrieval accuracy. This\nadvancement sheds light on applying RAG to the medical domain.", "published": "2024-04-27 13:11:42", "link": "http://arxiv.org/abs/2404.17897v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "I Have an Attention Bridge to Sell You: Generalization Capabilities of\n  Modular Translation Architectures", "abstract": "Modularity is a paradigm of machine translation with the potential of\nbringing forth models that are large at training time and small during\ninference. Within this field of study, modular approaches, and in particular\nattention bridges, have been argued to improve the generalization capabilities\nof models by fostering language-independent representations. In the present\npaper, we study whether modularity affects translation quality; as well as how\nwell modular architectures generalize across different evaluation scenarios.\nFor a given computational budget, we find non-modular architectures to be\nalways comparable or preferable to all modular designs we study.", "published": "2024-04-27 14:10:51", "link": "http://arxiv.org/abs/2404.17918v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transfer Learning Enhanced Single-choice Decision for Multi-choice\n  Question Answering", "abstract": "Multi-choice Machine Reading Comprehension (MMRC) aims to select the correct\nanswer from a set of options based on a given passage and question. The\nexisting methods employ the pre-trained language model as the encoder, share\nand transfer knowledge through fine-tuning.These methods mainly focus on the\ndesign of exquisite mechanisms to effectively capture the relationships among\nthe triplet of passage, question and answers. It is non-trivial but ignored to\ntransfer knowledge from other MRC tasks such as SQuAD due to task specific of\nMMRC.In this paper, we reconstruct multi-choice to single-choice by training a\nbinary classification to distinguish whether a certain answer is correct. Then\nselect the option with the highest confidence score as the final answer. Our\nproposed method gets rid of the multi-choice framework and can leverage\nresources of other tasks. We construct our model based on the ALBERT-xxlarge\nmodel and evaluate it on the RACE and DREAM datasets. Experimental results show\nthat our model performs better than multi-choice methods. In addition, by\ntransferring knowledge from other kinds of MRC tasks, our model achieves\nstate-of-the-art results in both single and ensemble settings.", "published": "2024-04-27 16:02:55", "link": "http://arxiv.org/abs/2404.17949v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Pre-Trained Generative Language Models with Question Attended\n  Span Extraction on Machine Reading Comprehension", "abstract": "Machine Reading Comprehension (MRC) poses a significant challenge in the\nfield of Natural Language Processing (NLP). While mainstream MRC methods\npredominantly leverage extractive strategies using encoder-only models such as\nBERT, generative approaches face the issue of out-of-control generation -- a\ncritical problem where answers generated are often incorrect, irrelevant, or\nunfaithful to the source text. To address these limitations in generative\nmodels for MRC, we introduce the Question-Attended Span Extraction (QASE)\nmodule. Integrated during the fine-tuning phase of pre-trained generative\nlanguage models (PLMs), QASE significantly enhances their performance, allowing\nthem to surpass the extractive capabilities of advanced Large Language Models\n(LLMs) such as GPT-4 in few-shot settings. Notably, these gains in performance\ndo not come with an increase in computational demands. The efficacy of the QASE\nmodule has been rigorously tested across various datasets, consistently\nachieving or even surpassing state-of-the-art (SOTA) results, thereby bridging\nthe gap between generative and extractive models in extractive MRC tasks.", "published": "2024-04-27 19:42:51", "link": "http://arxiv.org/abs/2404.17991v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Quality Estimation with $k$-nearest Neighbors and Automatic Evaluation\n  for Model-specific Quality Estimation", "abstract": "Providing quality scores along with Machine Translation (MT) output,\nso-called reference-free Quality Estimation (QE), is crucial to inform users\nabout the reliability of the translation. We propose a model-specific,\nunsupervised QE approach, termed $k$NN-QE, that extracts information from the\nMT model's training data using $k$-nearest neighbors. Measuring the performance\nof model-specific QE is not straightforward, since they provide quality scores\non their own MT output, thus cannot be evaluated using benchmark QE test sets\ncontaining human quality scores on premade MT output. Therefore, we propose an\nautomatic evaluation method that uses quality scores from reference-based\nmetrics as gold standard instead of human-generated ones. We are the first to\nconduct detailed analyses and conclude that this automatic method is\nsufficient, and the reference-based MetricX-23 is best for the task.", "published": "2024-04-27 23:52:51", "link": "http://arxiv.org/abs/2404.18031v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Building a Large Japanese Web Corpus for Large Language Models", "abstract": "Open Japanese large language models (LLMs) have been trained on the Japanese\nportions of corpora such as CC-100, mC4, and OSCAR. However, these corpora were\nnot created for the quality of Japanese texts. This study builds a large\nJapanese web corpus by extracting and refining text from the Common Crawl\narchive (21 snapshots of approximately 63.4 billion pages crawled between 2020\nand 2023). This corpus consists of approximately 312.1 billion characters\n(approximately 173 million pages), which is the largest of all available\ntraining corpora for Japanese LLMs, surpassing CC-100 (approximately 25.8\nbillion characters), mC4 (approximately 239.7 billion characters) and OSCAR\n23.10 (approximately 74 billion characters). To confirm the quality of the\ncorpus, we performed continual pre-training on Llama 2 7B, 13B, 70B, Mistral 7B\nv0.1, and Mixtral 8x7B Instruct as base LLMs and gained consistent (6.6-8.1\npoints) improvements on Japanese benchmark datasets. We also demonstrate that\nthe improvement on Llama 2 13B brought from the presented corpus was the\nlargest among those from other existing corpora.", "published": "2024-04-27 00:02:45", "link": "http://arxiv.org/abs/2404.17733v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "UMass-BioNLP at MEDIQA-M3G 2024: DermPrompt -- A Systematic Exploration\n  of Prompt Engineering with GPT-4V for Dermatological Diagnosis", "abstract": "This paper presents our team's participation in the MEDIQA-ClinicalNLP2024\nshared task B. We present a novel approach to diagnosing clinical dermatology\ncases by integrating large multimodal models, specifically leveraging the\ncapabilities of GPT-4V under a retriever and a re-ranker framework. Our\ninvestigation reveals that GPT-4V, when used as a retrieval agent, can\naccurately retrieve the correct skin condition 85% of the time using\ndermatological images and brief patient histories. Additionally, we empirically\nshow that Naive Chain-of-Thought (CoT) works well for retrieval while Medical\nGuidelines Grounded CoT is required for accurate dermatological diagnosis.\nFurther, we introduce a Multi-Agent Conversation (MAC) framework and show its\nsuperior performance and potential over the best CoT strategy. The experiments\nsuggest that using naive CoT for retrieval and multi-agent conversation for\ncritique-based diagnosis, GPT-4V can lead to an early and accurate diagnosis of\ndermatological conditions. The implications of this work extend to improving\ndiagnostic workflows, supporting dermatological education, and enhancing\npatient care by providing a scalable, accessible, and accurate diagnostic tool.", "published": "2024-04-27 01:39:05", "link": "http://arxiv.org/abs/2404.17749v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "MRScore: Evaluating Radiology Report Generation with LLM-based Reward\n  System", "abstract": "In recent years, automated radiology report generation has experienced\nsignificant growth. This paper introduces MRScore, an automatic evaluation\nmetric tailored for radiology report generation by leveraging Large Language\nModels (LLMs). Conventional NLG (natural language generation) metrics like BLEU\nare inadequate for accurately assessing the generated radiology reports, as\nsystematically demonstrated by our observations within this paper. To address\nthis challenge, we collaborated with radiologists to develop a framework that\nguides LLMs for radiology report evaluation, ensuring alignment with human\nanalysis. Our framework includes two key components: i) utilizing GPT to\ngenerate large amounts of training data, i.e., reports with different\nqualities, and ii) pairing GPT-generated reports as accepted and rejected\nsamples and training LLMs to produce MRScore as the model reward. Our\nexperiments demonstrate MRScore's higher correlation with human judgments and\nsuperior performance in model selection compared to traditional metrics. Our\ncode and datasets will be available on GitHub.", "published": "2024-04-27 04:42:45", "link": "http://arxiv.org/abs/2404.17778v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Continual Pre-Training for Cross-Lingual LLM Adaptation: Enhancing\n  Japanese Language Capabilities", "abstract": "Cross-lingual continual pre-training of large language models (LLMs)\ninitially trained on English corpus allows us to leverage the vast amount of\nEnglish language resources and reduce the pre-training cost. In this study, we\nconstructed Swallow, an LLM with enhanced Japanese capability, by extending the\nvocabulary of Llama 2 to include Japanese characters and conducting continual\npre-training on a large Japanese web corpus. Experimental results confirmed\nthat the performance on Japanese tasks drastically improved through continual\npre-training, and the performance monotonically increased with the amount of\ntraining data up to 100B tokens. Consequently, Swallow achieved superior\nperformance compared to other LLMs that were trained from scratch in English\nand Japanese. An analysis of the effects of continual pre-training revealed\nthat it was particularly effective for Japanese question answering tasks.\nFurthermore, to elucidate effective methodologies for cross-lingual continual\npre-training from English to Japanese, we investigated the impact of vocabulary\nexpansion and the effectiveness of incorporating parallel corpora. The results\nshowed that the efficiency gained through vocabulary expansion had no negative\nimpact on performance, except for the summarization task, and that the combined\nuse of parallel corpora enhanced translation ability.", "published": "2024-04-27 06:07:55", "link": "http://arxiv.org/abs/2404.17790v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Empirical Analysis of Dialogue Relation Extraction with Large Language\n  Models", "abstract": "Dialogue relation extraction (DRE) aims to extract relations between two\narguments within a dialogue, which is more challenging than standard RE due to\nthe higher person pronoun frequency and lower information density in dialogues.\nHowever, existing DRE methods still suffer from two serious issues: (1) hard to\ncapture long and sparse multi-turn information, and (2) struggle to extract\ngolden relations based on partial dialogues, which motivates us to discover\nmore effective methods that can alleviate the above issues. We notice that the\nrise of large language models (LLMs) has sparked considerable interest in\nevaluating their performance across diverse tasks. To this end, we initially\ninvestigate the capabilities of different LLMs in DRE, considering both\nproprietary models and open-source models. Interestingly, we discover that LLMs\nsignificantly alleviate two issues in existing DRE methods. Generally, we have\nfollowing findings: (1) scaling up model size substantially boosts the overall\nDRE performance and achieves exceptional results, tackling the difficulty of\ncapturing long and sparse multi-turn information; (2) LLMs encounter with much\nsmaller performance drop from entire dialogue setting to partial dialogue\nsetting compared to existing methods; (3) LLMs deliver competitive or superior\nperformances under both full-shot and few-shot settings compared to current\nstate-of-the-art; (4) LLMs show modest performances on inverse relations but\nmuch stronger improvements on general relations, and they can handle dialogues\nof various lengths especially for longer sequences.", "published": "2024-04-27 06:55:41", "link": "http://arxiv.org/abs/2404.17802v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Meta In-Context Learning Makes Large Language Models Better Zero and\n  Few-Shot Relation Extractors", "abstract": "Relation extraction (RE) is an important task that aims to identify the\nrelationships between entities in texts. While large language models (LLMs)\nhave revealed remarkable in-context learning (ICL) capability for general zero\nand few-shot learning, recent studies indicate that current LLMs still struggle\nwith zero and few-shot RE. Previous studies are mainly dedicated to design\nprompt formats and select good examples for improving ICL-based RE. Although\nboth factors are vital for ICL, if one can fundamentally boost the ICL\ncapability of LLMs in RE, the zero and few-shot RE performance via ICL would be\nsignificantly improved. To this end, we introduce \\textsc{Micre} (\\textbf{M}eta\n\\textbf{I}n-\\textbf{C}ontext learning of LLMs for \\textbf{R}elation\n\\textbf{E}xtraction), a new meta-training framework for zero and few-shot RE\nwhere an LLM is tuned to do ICL on a diverse collection of RE datasets (i.e.,\nlearning to learn in context for RE). Through meta-training, the model becomes\nmore effectively to learn a new RE task in context by conditioning on a few\ntraining examples with no parameter updates or task-specific templates at\ninference time, enabling better zero and few-shot task generalization. We\nexperiment \\textsc{Micre} on various LLMs with different model scales and 12\npublic RE datasets, and then evaluate it on unseen RE benchmarks under zero and\nfew-shot settings. \\textsc{Micre} delivers comparable or superior performance\ncompared to a range of baselines including supervised fine-tuning and typical\nin-context learning methods. We find that the gains are particular significant\nfor larger model scales, and using a diverse set of the meta-training RE\ndatasets is key to improvements. Empirically, we show that \\textsc{Micre} can\ntransfer the relation semantic knowledge via relation label name during\ninference on target RE datasets.", "published": "2024-04-27 07:06:39", "link": "http://arxiv.org/abs/2404.17807v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Recall, Retrieve and Reason: Towards Better In-Context Relation\n  Extraction", "abstract": "Relation extraction (RE) aims to identify relations between entities\nmentioned in texts. Although large language models (LLMs) have demonstrated\nimpressive in-context learning (ICL) abilities in various tasks, they still\nsuffer from poor performances compared to most supervised fine-tuned RE\nmethods. Utilizing ICL for RE with LLMs encounters two challenges: (1)\nretrieving good demonstrations from training examples, and (2) enabling LLMs\nexhibit strong ICL abilities in RE. On the one hand, retrieving good\ndemonstrations is a non-trivial process in RE, which easily results in low\nrelevance regarding entities and relations. On the other hand, ICL with an LLM\nachieves poor performance in RE while RE is different from language modeling in\nnature or the LLM is not large enough. In this work, we propose a novel\nrecall-retrieve-reason RE framework that synergizes LLMs with retrieval corpora\n(training examples) to enable relevant retrieving and reliable in-context\nreasoning. Specifically, we distill the consistently ontological knowledge from\ntraining datasets to let LLMs generate relevant entity pairs grounded by\nretrieval corpora as valid queries. These entity pairs are then used to\nretrieve relevant training examples from the retrieval corpora as\ndemonstrations for LLMs to conduct better ICL via instruction tuning. Extensive\nexperiments on different LLMs and RE datasets demonstrate that our method\ngenerates relevant and valid entity pairs and boosts ICL abilities of LLMs,\nachieving competitive or new state-of-the-art performance on sentence-level RE\ncompared to previous supervised fine-tuning methods and ICL-based methods.", "published": "2024-04-27 07:12:52", "link": "http://arxiv.org/abs/2404.17809v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "T-CLAP: Temporal-Enhanced Contrastive Language-Audio Pretraining", "abstract": "Contrastive language-audio pretraining~(CLAP) has been developed to align the\nrepresentations of audio and language, achieving remarkable performance in\nretrieval and classification tasks. However, current CLAP struggles to capture\ntemporal information within audio and text features, presenting substantial\nlimitations for tasks such as audio retrieval and generation. To address this\ngap, we introduce T-CLAP, a temporal-enhanced CLAP model. We use Large Language\nModels~(LLMs) and mixed-up strategies to generate temporal-contrastive captions\nfor audio clips from extensive audio-text datasets. Subsequently, a new\ntemporal-focused contrastive loss is designed to fine-tune the CLAP model by\nincorporating these synthetic data. We conduct comprehensive experiments and\nanalysis in multiple downstream tasks. T-CLAP shows improved capability in\ncapturing the temporal relationship of sound events and outperforms\nstate-of-the-art models by a significant margin.", "published": "2024-04-27 07:05:48", "link": "http://arxiv.org/abs/2404.17806v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SERPENT-VLM : Self-Refining Radiology Report Generation Using Vision\n  Language Models", "abstract": "Radiology Report Generation (R2Gen) demonstrates how Multi-modal Large\nLanguage Models (MLLMs) can automate the creation of accurate and coherent\nradiological reports. Existing methods often hallucinate details in text-based\nreports that don't accurately reflect the image content. To mitigate this, we\nintroduce a novel strategy, SERPENT-VLM (SElf Refining Radiology RePort\nGENeraTion using Vision Language Models), which improves the R2Gen task by\nintegrating a self-refining mechanism into the MLLM framework. We employ a\nunique self-supervised loss that leverages similarity between pooled image\nrepresentations and the contextual representations of the generated\nradiological text, alongside the standard Causal Language Modeling objective,\nto refine image-text representations. This allows the model to scrutinize and\nalign the generated text through dynamic interaction between a given image and\nthe generated text, therefore reducing hallucination and continuously enhancing\nnuanced report generation. SERPENT-VLM outperforms existing baselines such as\nLLaVA-Med, BiomedGPT, etc., achieving SoTA performance on the IU X-ray and\nRadiology Objects in COntext (ROCO) datasets, and also proves to be robust\nagainst noisy images. A qualitative case study emphasizes the significant\nadvancements towards more sophisticated MLLM frameworks for R2Gen, opening\npaths for further research into self-supervised refinement in the medical\nimaging domain.", "published": "2024-04-27 13:46:23", "link": "http://arxiv.org/abs/2404.17912v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Spatio-Temporal Side Tuning Pre-trained Foundation Models for\n  Video-based Pedestrian Attribute Recognition", "abstract": "Existing pedestrian attribute recognition (PAR) algorithms are mainly\ndeveloped based on a static image, however, the performance is unreliable in\nchallenging scenarios, such as heavy occlusion, motion blur, etc. In this work,\nwe propose to understand human attributes using video frames that can fully use\ntemporal information by fine-tuning a pre-trained multi-modal foundation model\nefficiently. Specifically, we formulate the video-based PAR as a\nvision-language fusion problem and adopt a pre-trained foundation model CLIP to\nextract the visual features. More importantly, we propose a novel\nspatiotemporal side-tuning strategy to achieve parameter-efficient optimization\nof the pre-trained vision foundation model. To better utilize the semantic\ninformation, we take the full attribute list that needs to be recognized as\nanother input and transform the attribute words/phrases into the corresponding\nsentence via split, expand, and prompt operations. Then, the text encoder of\nCLIP is utilized for embedding processed attribute descriptions. The averaged\nvisual tokens and text tokens are concatenated and fed into a fusion\nTransformer for multi-modal interactive learning. The enhanced tokens will be\nfed into a classification head for pedestrian attribute prediction. Extensive\nexperiments on two large-scale video-based PAR datasets fully validated the\neffectiveness of our proposed framework. The source code of this paper is\navailable at https://github.com/Event-AHU/OpenPAR.", "published": "2024-04-27 14:43:32", "link": "http://arxiv.org/abs/2404.17929v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Usefulness of Emotional Prosody in Neural Machine Translation", "abstract": "Neural Machine Translation (NMT) is the task of translating a text from one\nlanguage to another with the use of a trained neural network. Several existing\nworks aim at incorporating external information into NMT models to improve or\ncontrol predicted translations (e.g. sentiment, politeness, gender). In this\nwork, we propose to improve translation quality by adding another external\nsource of information: the automatically recognized emotion in the voice. This\nwork is motivated by the assumption that each emotion is associated with a\nspecific lexicon that can overlap between emotions. Our proposed method follows\na two-stage procedure. At first, we select a state-of-the-art Speech Emotion\nRecognition (SER) model to predict dimensional emotion values from all input\naudio in the dataset. Then, we use these predicted emotions as source tokens\nadded at the beginning of input texts to train our NMT model. We show that\nintegrating emotion information, especially arousal, into NMT systems leads to\nbetter translations.", "published": "2024-04-27 18:04:28", "link": "http://arxiv.org/abs/2404.17968v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Automating Customer Needs Analysis: A Comparative Study of Large\n  Language Models in the Travel Industry", "abstract": "In the rapidly evolving landscape of Natural Language Processing (NLP), Large\nLanguage Models (LLMs) have emerged as powerful tools for many tasks, such as\nextracting valuable insights from vast amounts of textual data. In this study,\nwe conduct a comparative analysis of LLMs for the extraction of travel customer\nneeds from TripAdvisor and Reddit posts. Leveraging a diverse range of models,\nincluding both open-source and proprietary ones such as GPT-4 and Gemini, we\naim to elucidate their strengths and weaknesses in this specialized domain.\nThrough an evaluation process involving metrics such as BERTScore, ROUGE, and\nBLEU, we assess the performance of each model in accurately identifying and\nsummarizing customer needs. Our findings highlight the efficacy of opensource\nLLMs, particularly Mistral 7B, in achieving comparable performance to larger\nclosed models while offering affordability and customization benefits.\nAdditionally, we underscore the importance of considering factors such as model\nsize, resource requirements, and performance metrics when selecting the most\nsuitable LLM for customer needs analysis tasks. Overall, this study contributes\nvaluable insights for businesses seeking to leverage advanced NLP techniques to\nenhance customer experience and drive operational efficiency in the travel\nindustry.", "published": "2024-04-27 18:28:10", "link": "http://arxiv.org/abs/2404.17975v2", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TI-ASU: Toward Robust Automatic Speech Understanding through\n  Text-to-speech Imputation Against Missing Speech Modality", "abstract": "Automatic Speech Understanding (ASU) aims at human-like speech\ninterpretation, providing nuanced intent, emotion, sentiment, and content\nunderstanding from speech and language (text) content conveyed in speech.\nTypically, training a robust ASU model relies heavily on acquiring large-scale,\nhigh-quality speech and associated transcriptions. However, it is often\nchallenging to collect or use speech data for training ASU due to concerns such\nas privacy. To approach this setting of enabling ASU when speech (audio)\nmodality is missing, we propose TI-ASU, using a pre-trained text-to-speech\nmodel to impute the missing speech. We report extensive experiments evaluating\nTI-ASU on various missing scales, both multi- and single-modality settings, and\nthe use of LLMs. Our findings show that TI-ASU yields substantial benefits to\nimprove ASU in scenarios where even up to 95% of training speech is missing.\nMoreover, we show that TI-ASU is adaptive to dropout training, improving model\nrobustness in addressing missing speech during inference.", "published": "2024-04-27 19:13:05", "link": "http://arxiv.org/abs/2404.17983v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Detection of Conspiracy Theories Beyond Keyword Bias in German-Language\n  Telegram Using Large Language Models", "abstract": "The automated detection of conspiracy theories online typically relies on\nsupervised learning. However, creating respective training data requires\nexpertise, time and mental resilience, given the often harmful content.\nMoreover, available datasets are predominantly in English and often\nkeyword-based, introducing a token-level bias into the models. Our work\naddresses the task of detecting conspiracy theories in German Telegram\nmessages. We compare the performance of supervised fine-tuning approaches using\nBERT-like models with prompt-based approaches using Llama2, GPT-3.5, and GPT-4\nwhich require little or no additional training data. We use a dataset of\n$\\sim\\!\\! 4,000$ messages collected during the COVID-19 pandemic, without the\nuse of keyword filters.\n  Our findings demonstrate that both approaches can be leveraged effectively:\nFor supervised fine-tuning, we report an F1 score of $\\sim\\!\\! 0.8$ for the\npositive class, making our model comparable to recent models trained on\nkeyword-focused English corpora. We demonstrate our model's adaptability to\nintra-domain temporal shifts, achieving F1 scores of $\\sim\\!\\! 0.7$. Among\nprompting variants, the best model is GPT-4, achieving an F1 score of $\\sim\\!\\!\n0.8$ for the positive class in a zero-shot setting and equipped with a custom\nconspiracy theory definition.", "published": "2024-04-27 19:17:31", "link": "http://arxiv.org/abs/2404.17985v1", "categories": ["cs.CL", "cs.AI", "cs.SI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "MediFact at MEDIQA-CORR 2024: Why AI Needs a Human Touch", "abstract": "Accurate representation of medical information is crucial for patient safety,\nyet artificial intelligence (AI) systems, such as Large Language Models (LLMs),\nencounter challenges in error-free clinical text interpretation. This paper\npresents a novel approach submitted to the MEDIQA-CORR 2024 shared task (Ben\nAbacha et al., 2024a), focusing on the automatic correction of single-word\nerrors in clinical notes. Unlike LLMs that rely on extensive generic data, our\nmethod emphasizes extracting contextually relevant information from available\nclinical text data. Leveraging an ensemble of extractive and abstractive\nquestion-answering approaches, we construct a supervised learning framework\nwith domain-specific feature engineering. Our methodology incorporates domain\nexpertise to enhance error correction accuracy. By integrating domain expertise\nand prioritizing meaningful information extraction, our approach underscores\nthe significance of a human-centric strategy in adapting AI for healthcare.", "published": "2024-04-27 20:28:38", "link": "http://arxiv.org/abs/2404.17999v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CRISPR-GPT: An LLM Agent for Automated Design of Gene-Editing\n  Experiments", "abstract": "The introduction of genome engineering technology has transformed biomedical\nresearch, making it possible to make precise changes to genetic information.\nHowever, creating an efficient gene-editing system requires a deep\nunderstanding of CRISPR technology, and the complex experimental systems under\ninvestigation. While Large Language Models (LLMs) have shown promise in various\ntasks, they often lack specific knowledge and struggle to accurately solve\nbiological design problems. In this work, we introduce CRISPR-GPT, an LLM agent\naugmented with domain knowledge and external tools to automate and enhance the\ndesign process of CRISPR-based gene-editing experiments. CRISPR-GPT leverages\nthe reasoning ability of LLMs to facilitate the process of selecting CRISPR\nsystems, designing guide RNAs, recommending cellular delivery methods, drafting\nprotocols, and designing validation experiments to confirm editing outcomes. We\nshowcase the potential of CRISPR-GPT for assisting non-expert researchers with\ngene-editing experiments from scratch and validate the agent's effectiveness in\na real-world use case. Furthermore, we explore the ethical and regulatory\nconsiderations associated with automated gene-editing design, highlighting the\nneed for responsible and transparent use of these tools. Our work aims to\nbridge the gap between beginner biological researchers and CRISPR genome\nengineering techniques, and demonstrate the potential of LLM agents in\nfacilitating complex biological discovery tasks.", "published": "2024-04-27 22:59:17", "link": "http://arxiv.org/abs/2404.18021v1", "categories": ["cs.AI", "cs.CL", "cs.HC", "q-bio.QM"], "primary_category": "cs.AI"}
{"title": "Evaluating the Application of ChatGPT in Outpatient Triage Guidance: A\n  Comparative Study", "abstract": "The integration of Artificial Intelligence (AI) in healthcare presents a\ntransformative potential for enhancing operational efficiency and health\noutcomes. Large Language Models (LLMs), such as ChatGPT, have shown their\ncapabilities in supporting medical decision-making. Embedding LLMs in medical\nsystems is becoming a promising trend in healthcare development. The potential\nof ChatGPT to address the triage problem in emergency departments has been\nexamined, while few studies have explored its application in outpatient\ndepartments. With a focus on streamlining workflows and enhancing efficiency\nfor outpatient triage, this study specifically aims to evaluate the consistency\nof responses provided by ChatGPT in outpatient guidance, including both\nwithin-version response analysis and between-version comparisons. For\nwithin-version, the results indicate that the internal response consistency for\nChatGPT-4.0 is significantly higher than ChatGPT-3.5 (p=0.03) and both have a\nmoderate consistency (71.2% for 4.0 and 59.6% for 3.5) in their top\nrecommendation. However, the between-version consistency is relatively low\n(mean consistency score=1.43/3, median=1), indicating few recommendations match\nbetween the two versions. Also, only 50% top recommendations match perfectly in\nthe comparisons. Interestingly, ChatGPT-3.5 responses are more likely to be\ncomplete than those from ChatGPT-4.0 (p=0.02), suggesting possible differences\nin information processing and response generation between the two versions. The\nfindings offer insights into AI-assisted outpatient operations, while also\nfacilitating the exploration of potentials and limitations of LLMs in\nhealthcare utilization. Future research may focus on carefully optimizing LLMs\nand AI integration in healthcare systems based on ergonomic and human factors\nprinciples, precisely aligning with the specific needs of effective outpatient\ntriage.", "published": "2024-04-27 04:12:02", "link": "http://arxiv.org/abs/2405.00728v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "MediFact at MEDIQA-M3G 2024: Medical Question Answering in Dermatology\n  with Multimodal Learning", "abstract": "The MEDIQA-M3G 2024 challenge necessitates novel solutions for Multilingual &\nMultimodal Medical Answer Generation in dermatology (wai Yim et al., 2024a).\nThis paper addresses the limitations of traditional methods by proposing a\nweakly supervised learning approach for open-ended medical question-answering\n(QA). Our system leverages readily available MEDIQA-M3G images via a\nVGG16-CNN-SVM model, enabling multilingual (English, Chinese, Spanish) learning\nof informative skin condition representations. Using pre-trained QA models, we\nfurther bridge the gap between visual and textual information through\nmultimodal fusion. This approach tackles complex, open-ended questions even\nwithout predefined answer choices. We empower the generation of comprehensive\nanswers by feeding the ViT-CLIP model with multiple responses alongside images.\nThis work advances medical QA research, paving the way for clinical decision\nsupport systems and ultimately improving healthcare delivery.", "published": "2024-04-27 20:03:47", "link": "http://arxiv.org/abs/2405.01583v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Comparison of Differential Performance Metrics for the Evaluation of\n  Automatic Speaker Verification Fairness", "abstract": "When decisions are made and when personal data is treated by automated\nprocesses, there is an expectation of fairness -- that members of different\ndemographic groups receive equitable treatment. This expectation applies to\nbiometric systems such as automatic speaker verification (ASV). We present a\ncomparison of three candidate fairness metrics and extend previous work\nperformed for face recognition, by examining differential performance across a\nrange of different ASV operating points. Results show that the Gini Aggregation\nRate for Biometric Equitability (GARBE) is the only one which meets three\nfunctional fairness measure criteria. Furthermore, a comprehensive evaluation\nof the fairness and verification performance of five state-of-the-art ASV\nsystems is also presented. Our findings reveal a nuanced trade-off between\nfairness and verification accuracy underscoring the complex interplay between\nsystem design, demographic inclusiveness, and verification reliability.", "published": "2024-04-27 07:28:10", "link": "http://arxiv.org/abs/2404.17810v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Towards Privacy-Preserving Audio Classification Systems", "abstract": "Audio signals can reveal intimate details about a person's life, including\ntheir conversations, health status, emotions, location, and personal\npreferences. Unauthorized access or misuse of this information can have\nprofound personal and social implications. In an era increasingly populated by\ndevices capable of audio recording, safeguarding user privacy is a critical\nobligation. This work studies the ethical and privacy concerns in current audio\nclassification systems. We discuss the challenges and research directions in\ndesigning privacy-preserving audio sensing systems. We propose\nprivacy-preserving audio features that can be used to classify wide range of\naudio classes, while being privacy preserving.", "published": "2024-04-27 20:36:52", "link": "http://arxiv.org/abs/2404.18002v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "An automatic mixing speech enhancement system for multi-track audio", "abstract": "We propose a speech enhancement system for multitrack audio. The system will\nminimize auditory masking while allowing one to hear multiple simultaneous\nspeakers. The system can be used in multiple communication scenarios e.g.,\nteleconferencing, invoice gaming, and live streaming. The ITU-R BS.1387\nPerceptual Evaluation of Audio Quality (PEAQ) model is used to evaluate the\namount of masking in the audio signals. Different audio effects e.g., level\nbalance, equalization, dynamic range compression, and spatialization are\napplied via an iterative Harmony searching algorithm that aims to minimize the\nmasking. In the subjective listening test, the designed system can compete with\nmixes by professional sound engineers and outperforms mixes by existing\nauto-mixing systems.", "published": "2024-04-27 08:00:43", "link": "http://arxiv.org/abs/2404.17821v2", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
