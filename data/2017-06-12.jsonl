{"title": "Dialog Structure Through the Lens of Gender, Gender Environment, and\n  Power", "abstract": "Understanding how the social context of an interaction affects our dialog\nbehavior is of great interest to social scientists who study human behavior, as\nwell as to computer scientists who build automatic methods to infer those\nsocial contexts. In this paper, we study the interaction of power, gender, and\ndialog behavior in organizational interactions. In order to perform this study,\nwe first construct the Gender Identified Enron Corpus of emails, in which we\nsemi-automatically assign the gender of around 23,000 individuals who authored\naround 97,000 email messages in the Enron corpus. This corpus, which is made\nfreely available, is orders of magnitude larger than previously existing gender\nidentified corpora in the email domain. Next, we use this corpus to perform a\nlarge-scale data-oriented study of the interplay of gender and manifestations\nof power. We argue that, in addition to one's own gender, the \"gender\nenvironment\" of an interaction, i.e., the gender makeup of one's interlocutors,\nalso affects the way power is manifested in dialog. We focus especially on\nmanifestations of power in the dialog structure --- both, in a shallow sense\nthat disregards the textual content of messages (e.g., how often do the\nparticipants contribute, how often do they get replies etc.), as well as the\nstructure that is expressed within the textual content (e.g., who issues\nrequests and how are they made, whose requests get responses etc.). We find\nthat both gender and gender environment affect the ways power is manifested in\ndialog, resulting in patterns that reveal the underlying factors. Finally, we\nshow the utility of gender information in the problem of automatically\npredicting the direction of power between pairs of participants in email\ninteractions.", "published": "2017-06-12 02:24:33", "link": "http://arxiv.org/abs/1706.03441v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SU-RUG at the CoNLL-SIGMORPHON 2017 shared task: Morphological\n  Inflection with Attentional Sequence-to-Sequence Models", "abstract": "This paper describes the Stockholm University/University of Groningen\n(SU-RUG) system for the SIGMORPHON 2017 shared task on morphological\ninflection. Our system is based on an attentional sequence-to-sequence neural\nnetwork model using Long Short-Term Memory (LSTM) cells, with joint training of\nmorphological inflection and the inverse transformation, i.e. lemmatization and\nmorphological analysis. Our system outperforms the baseline with a large\nmargin, and our submission ranks as the 4th best team for the track we\nparticipate in (task 1, high-resource).", "published": "2017-06-12 08:08:00", "link": "http://arxiv.org/abs/1706.03499v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Candidate sentence selection for language learning exercises: from a\n  comprehensive framework to an empirical evaluation", "abstract": "We present a framework and its implementation relying on Natural Language\nProcessing methods, which aims at the identification of exercise item\ncandidates from corpora. The hybrid system combining heuristics and machine\nlearning methods includes a number of relevant selection criteria. We focus on\ntwo fundamental aspects: linguistic complexity and the dependence of the\nextracted sentences on their original context. Previous work on exercise\ngeneration addressed these two criteria only to a limited extent, and a refined\noverall candidate sentence selection framework appears also to be lacking. In\naddition to a detailed description of the system, we present the results of an\nempirical evaluation conducted with language teachers and learners which\nindicate the usefulness of the system for educational purposes. We have\nintegrated our system into a freely available online learning platform.", "published": "2017-06-12 09:21:45", "link": "http://arxiv.org/abs/1706.03530v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring the Syntactic Abilities of RNNs with Multi-task Learning", "abstract": "Recent work has explored the syntactic abilities of RNNs using the\nsubject-verb agreement task, which diagnoses sensitivity to sentence structure.\nRNNs performed this task well in common cases, but faltered in complex\nsentences (Linzen et al., 2016). We test whether these errors are due to\ninherent limitations of the architecture or to the relatively indirect\nsupervision provided by most agreement dependencies in a corpus. We trained a\nsingle RNN to perform both the agreement task and an additional task, either\nCCG supertagging or language modeling. Multi-task training led to significantly\nlower error rates, in particular on complex sentences, suggesting that RNNs\nhave the ability to evolve more sophisticated syntactic representations than\nshown before. We also show that easily available agreement training data can\nimprove performance on other syntactic tasks, in particular when only a limited\namount of training data is available for those tasks. The multi-task paradigm\ncan also be leveraged to inject grammatical knowledge into language models.", "published": "2017-06-12 10:00:47", "link": "http://arxiv.org/abs/1706.03542v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Acoustic data-driven lexicon learning based on a greedy pronunciation\n  selection framework", "abstract": "Speech recognition systems for irregularly-spelled languages like English\nnormally require hand-written pronunciations. In this paper, we describe a\nsystem for automatically obtaining pronunciations of words for which\npronunciations are not available, but for which transcribed data exists. Our\nmethod integrates information from the letter sequence and from the acoustic\nevidence. The novel aspect of the problem that we address is the problem of how\nto prune entries from such a lexicon (since, empirically, lexicons with too\nmany entries do not tend to be good for ASR performance). Experiments on\nvarious ASR tasks show that, with the proposed framework, starting with an\ninitial lexicon of several thousand words, we are able to learn a lexicon which\nperforms close to a full expert lexicon in terms of WER performance on test\ndata, and is better than lexicons built using G2P alone or with a pruning\ncriterion based on pronunciation probability.", "published": "2017-06-12 17:35:41", "link": "http://arxiv.org/abs/1706.03747v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Verb Physics: Relative Physical Knowledge of Actions and Objects", "abstract": "Learning commonsense knowledge from natural language text is nontrivial due\nto reporting bias: people rarely state the obvious, e.g., \"My house is bigger\nthan me.\" However, while rarely stated explicitly, this trivial everyday\nknowledge does influence the way people talk about the world, which provides\nindirect clues to reason about the world. For example, a statement like, \"Tyler\nentered his house\" implies that his house is bigger than Tyler.\n  In this paper, we present an approach to infer relative physical knowledge of\nactions and objects along five dimensions (e.g., size, weight, and strength)\nfrom unstructured natural language text. We frame knowledge acquisition as\njoint inference over two closely related problems: learning (1) relative\nphysical knowledge of object pairs and (2) physical implications of actions\nwhen applied to those object pairs. Empirical results demonstrate that it is\npossible to extract knowledge of actions and objects from language and that\njoint inference over different types of knowledge improves performance.", "published": "2017-06-12 18:24:25", "link": "http://arxiv.org/abs/1706.03799v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Query-by-Example Search with Discriminative Neural Acoustic Word\n  Embeddings", "abstract": "Query-by-example search often uses dynamic time warping (DTW) for comparing\nqueries and proposed matching segments. Recent work has shown that comparing\nspeech segments by representing them as fixed-dimensional vectors --- acoustic\nword embeddings --- and measuring their vector distance (e.g., cosine distance)\ncan discriminate between words more accurately than DTW-based approaches. We\nconsider an approach to query-by-example search that embeds both the query and\ndatabase segments according to a neural model, followed by nearest-neighbor\nsearch to find the matching segments. Earlier work on embedding-based\nquery-by-example, using template-based acoustic word embeddings, achieved\ncompetitive performance. We find that our embeddings, based on recurrent neural\nnetworks trained to optimize word discrimination, achieve substantial\nimprovements in performance and run-time efficiency over the previous\napproaches.", "published": "2017-06-12 19:30:57", "link": "http://arxiv.org/abs/1706.03818v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Attention-based Vocabulary Selection for NMT Decoding", "abstract": "Neural Machine Translation (NMT) models usually use large target vocabulary\nsizes to capture most of the words in the target language. The vocabulary size\nis a big factor when decoding new sentences as the final softmax layer\nnormalizes over all possible target words. To address this problem, it is\nwidely common to restrict the target vocabulary with candidate lists based on\nthe source sentence. Usually, the candidate lists are a combination of external\nword-to-word aligner, phrase table entries or most frequent words. In this\nwork, we propose a simple and yet novel approach to learn candidate lists\ndirectly from the attention layer during NMT training. The candidate lists are\nhighly optimized for the current NMT model and do not need any external\ncomputation of the candidate pool. We show significant decoding speedup\ncompared with using the entire vocabulary, without losing any translation\nquality for two language pairs.", "published": "2017-06-12 19:51:00", "link": "http://arxiv.org/abs/1706.03824v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Six Challenges for Neural Machine Translation", "abstract": "We explore six challenges for neural machine translation: domain mismatch,\namount of training data, rare words, long sentences, word alignment, and beam\nsearch. We show both deficiencies and improvements over the quality of\nphrase-based statistical machine translation.", "published": "2017-06-12 23:57:48", "link": "http://arxiv.org/abs/1706.03872v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extract with Order for Coherent Multi-Document Summarization", "abstract": "In this work, we aim at developing an extractive summarizer in the\nmulti-document setting. We implement a rank based sentence selection using\ncontinuous vector representations along with key-phrases. Furthermore, we\npropose a model to tackle summary coherence for increasing readability. We\nconduct experiments on the Document Understanding Conference (DUC) 2004\ndatasets using ROUGE toolkit. Our experiments demonstrate that the methods\nbring significant improvements over the state of the art methods in terms of\ninformativity and coherence.", "published": "2017-06-12 04:54:41", "link": "http://arxiv.org/abs/1706.06542v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Scientific document summarization via citation contextualization and\n  scientific discourse", "abstract": "The rapid growth of scientific literature has made it difficult for the\nresearchers to quickly learn about the developments in their respective fields.\nScientific document summarization addresses this challenge by providing\nsummaries of the important contributions of scientific papers. We present a\nframework for scientific summarization which takes advantage of the citations\nand the scientific discourse structure. Citation texts often lack the evidence\nand context to support the content of the cited paper and are even sometimes\ninaccurate. We first address the problem of inaccuracy of the citation texts by\nfinding the relevant context from the cited paper. We propose three approaches\nfor contextualizing citations which are based on query reformulation, word\nembeddings, and supervised learning. We then train a model to identify the\ndiscourse facets for each citation. We finally propose a method for summarizing\nscientific papers by leveraging the faceted citations and their corresponding\ncontexts. We evaluate our proposed method on two scientific summarization\ndatasets in the biomedical and computational linguistics domains. Extensive\nevaluation results show that our methods can improve over the state of the art\nby large margins.", "published": "2017-06-12 03:21:38", "link": "http://arxiv.org/abs/1706.03449v1", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "Attention Is All You Need", "abstract": "The dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks in an encoder-decoder configuration. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer, based\nsolely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to be\nsuperior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\nEnglish-to-German translation task, improving over the existing best results,\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\ntranslation task, our model establishes a new single-model state-of-the-art\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\nof the training costs of the best models from the literature. We show that the\nTransformer generalizes well to other tasks by applying it successfully to\nEnglish constituency parsing both with large and limited training data.", "published": "2017-06-12 17:57:34", "link": "http://arxiv.org/abs/1706.03762v7", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Neural Domain Adaptation for Biomedical Question Answering", "abstract": "Factoid question answering (QA) has recently benefited from the development\nof deep learning (DL) systems. Neural network models outperform traditional\napproaches in domains where large datasets exist, such as SQuAD (ca. 100,000\nquestions) for Wikipedia articles. However, these systems have not yet been\napplied to QA in more specific domains, such as biomedicine, because datasets\nare generally too small to train a DL system from scratch. For example, the\nBioASQ dataset for biomedical QA comprises less then 900 factoid (single\nanswer) and list (multiple answers) QA instances. In this work, we adapt a\nneural QA system trained on a large open-domain dataset (SQuAD, source) to a\nbiomedical dataset (BioASQ, target) by employing various transfer learning\ntechniques. Our network architecture is based on a state-of-the-art QA system,\nextended with biomedical word embeddings and a novel mechanism to answer list\nquestions. In contrast to existing biomedical QA systems, our system does not\nrely on domain-specific ontologies, parsers or entity taggers, which are\nexpensive to create. Despite this fact, our systems achieve state-of-the-art\nresults on factoid questions and competitive results on list questions.", "published": "2017-06-12 13:08:21", "link": "http://arxiv.org/abs/1706.03610v2", "categories": ["cs.CL", "cs.AI", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Semantic Entity Retrieval Toolkit", "abstract": "Unsupervised learning of low-dimensional, semantic representations of words\nand entities has recently gained attention. In this paper we describe the\nSemantic Entity Retrieval Toolkit (SERT) that provides implementations of our\npreviously published entity representation models. The toolkit provides a\nunified interface to different representation learning algorithms, fine-grained\nparsing configuration and can be used transparently with GPUs. In addition,\nusers can easily modify existing models or implement their own models in the\nframework. After model training, SERT can be used to rank entities according to\na textual query and extract the learned entity/word representation for use in\ndownstream algorithms, such as clustering or recommendation.", "published": "2017-06-12 17:51:05", "link": "http://arxiv.org/abs/1706.03757v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Encoding of phonology in a recurrent neural model of grounded speech", "abstract": "We study the representation and encoding of phonemes in a recurrent neural\nnetwork model of grounded speech. We use a model which processes images and\ntheir spoken descriptions, and projects the visual and auditory representations\ninto the same semantic space. We perform a number of analyses on how\ninformation about individual phonemes is encoded in the MFCC features extracted\nfrom the speech signal, and the activations of the layers of the model. Via\nexperiments with phoneme decoding and phoneme discrimination we show that\nphoneme representations are most salient in the lower layers of the model,\nwhere low-level signals are processed at a fine-grained level, although a large\namount of phonological information is retain at the top recurrent layer. We\nfurther find out that the attention mechanism following the top recurrent layer\nsignificantly attenuates encoding of phonology and makes the utterance\nembeddings much more invariant to synonymy. Moreover, a hierarchical clustering\nof phoneme representations learned by the network shows an organizational\nstructure of phonemes similar to those proposed in linguistics.", "published": "2017-06-12 19:07:02", "link": "http://arxiv.org/abs/1706.03815v2", "categories": ["cs.CL", "cs.LG", "cs.SD"], "primary_category": "cs.CL"}
{"title": "Adversarial Feature Matching for Text Generation", "abstract": "The Generative Adversarial Network (GAN) has achieved great success in\ngenerating realistic (real-valued) synthetic data. However, convergence issues\nand difficulties dealing with discrete data hinder the applicability of GAN to\ntext. We propose a framework for generating realistic text via adversarial\ntraining. We employ a long short-term memory network as generator, and a\nconvolutional network as discriminator. Instead of using the standard objective\nof GAN, we propose matching the high-dimensional latent feature distributions\nof real and synthetic sentences, via a kernelized discrepancy metric. This\neases adversarial training by alleviating the mode-collapsing problem. Our\nexperiments show superior performance in quantitative evaluation, and\ndemonstrate that our model can generate realistic-looking sentences.", "published": "2017-06-12 20:55:51", "link": "http://arxiv.org/abs/1706.03850v3", "categories": ["stat.ML", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Topic supervised non-negative matrix factorization", "abstract": "Topic models have been extensively used to organize and interpret the\ncontents of large, unstructured corpora of text documents. Although topic\nmodels often perform well on traditional training vs. test set evaluations, it\nis often the case that the results of a topic model do not align with human\ninterpretation. This interpretability fallacy is largely due to the\nunsupervised nature of topic models, which prohibits any user guidance on the\nresults of a model. In this paper, we introduce a semi-supervised method called\ntopic supervised non-negative matrix factorization (TS-NMF) that enables the\nuser to provide labeled example documents to promote the discovery of more\nmeaningful semantic structure of a corpus. In this way, the results of TS-NMF\nbetter match the intuition and desired labeling of the user. The core of TS-NMF\nrelies on solving a non-convex optimization problem for which we derive an\niterative algorithm that is shown to be monotonic and convergent to a local\noptimum. We demonstrate the practical utility of TS-NMF on the Reuters and\nPubMed corpora, and find that TS-NMF is especially useful for conceptual or\nbroad topics, where topic key terms are not well understood. Although\nidentifying an optimal latent structure for the data is not a primary objective\nof the proposed approach, we find that TS-NMF achieves higher weighted Jaccard\nsimilarity scores than the contemporary methods, (unsupervised) NMF and latent\nDirichlet allocation, at supervision rates as low as 10% to 20%.", "published": "2017-06-12 04:20:04", "link": "http://arxiv.org/abs/1706.05084v2", "categories": ["cs.CL", "cs.IR", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
