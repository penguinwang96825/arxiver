{"title": "Orders Are Unwanted: Dynamic Deep Graph Convolutional Network for\n  Personality Detection", "abstract": "Predicting personality traits based on online posts has emerged as an\nimportant task in many fields such as social network analysis. One of the\nchallenges of this task is assembling information from various posts into an\noverall profile for each user. While many previous solutions simply concatenate\nthe posts into a long document and then encode the document by sequential or\nhierarchical models, they introduce unwarranted orders for the posts, which may\nmislead the models. In this paper, we propose a dynamic deep graph\nconvolutional network (D-DGCN) to overcome the above limitation. Specifically,\nwe design a learn-to-connect approach that adopts a dynamic multi-hop structure\ninstead of a deterministic structure, and combine it with a DGCN module to\nautomatically learn the connections between posts. The modules of post encoder,\nlearn-to-connect, and DGCN are jointly trained in an end-to-end manner.\nExperimental results on the Kaggle and Pandora datasets show the superior\nperformance of D-DGCN to state-of-the-art baselines. Our code is available at\nhttps://github.com/djz233/D-DGCN.", "published": "2022-12-03 02:55:14", "link": "http://arxiv.org/abs/2212.01515v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The RoyalFlush System for the WMT 2022 Efficiency Task", "abstract": "This paper describes the submission of the RoyalFlush neural machine\ntranslation system for the WMT 2022 translation efficiency task. Unlike the\ncommonly used autoregressive translation system, we adopted a two-stage\ntranslation paradigm called Hybrid Regression Translation (HRT) to combine the\nadvantages of autoregressive and non-autoregressive translation. Specifically,\nHRT first autoregressively generates a discontinuous sequence (e.g., make a\nprediction every $k$ tokens, $k>1$) and then fills in all previously skipped\ntokens at once in a non-autoregressive manner. Thus, we can easily trade off\nthe translation quality and speed by adjusting $k$. In addition, by integrating\nother modeling techniques (e.g., sequence-level knowledge distillation and\ndeep-encoder-shallow-decoder layer allocation strategy) and a mass of\nengineering efforts, HRT improves 80\\% inference speed and achieves equivalent\ntranslation performance with the same-capacity AT counterpart. Our fastest\nsystem reaches 6k+ words/second on the GPU latency setting, estimated to be\nabout 3.1x faster than the last year's winner.", "published": "2022-12-03 05:36:10", "link": "http://arxiv.org/abs/2212.01543v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Meta Learning for Few-Shot Medical Text Classification", "abstract": "Medical professionals frequently work in a data constrained setting to\nprovide insights across a unique demographic. A few medical observations, for\ninstance, informs the diagnosis and treatment of a patient. This suggests a\nunique setting for meta-learning, a method to learn models quickly on new\ntasks, to provide insights unattainable by other methods. We investigate the\nuse of meta-learning and robustness techniques on a broad corpus of benchmark\ntext and medical data. To do this, we developed new data pipelines, combined\nlanguage models with meta-learning approaches, and extended existing\nmeta-learning algorithms to minimize worst case loss. We find that\nmeta-learning on text is a suitable framework for text-based data, providing\nbetter data efficiency and comparable performance to few-shot language models\nand can be successfully applied to medical note data. Furthermore,\nmeta-learning models coupled with DRO can improve worst case loss across\ndisease codes.", "published": "2022-12-03 06:46:52", "link": "http://arxiv.org/abs/2212.01552v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Label Correlations for Ultra-Fine Entity Typing with Neural\n  Pairwise Conditional Random Field", "abstract": "Ultra-fine entity typing (UFET) aims to predict a wide range of type phrases\nthat correctly describe the categories of a given entity mention in a sentence.\nMost recent works infer each entity type independently, ignoring the\ncorrelations between types, e.g., when an entity is inferred as a president, it\nshould also be a politician and a leader. To this end, we use an undirected\ngraphical model called pairwise conditional random field (PCRF) to formulate\nthe UFET problem, in which the type variables are not only unarily influenced\nby the input but also pairwisely relate to all the other type variables. We use\nvarious modern backbones for entity typing to compute unary potentials, and\nderive pairwise potentials from type phrase representations that both capture\nprior semantic information and facilitate accelerated inference. We use\nmean-field variational inference for efficient type inference on very large\ntype sets and unfold it as a neural network module to enable end-to-end\ntraining. Experiments on UFET show that the Neural-PCRF consistently\noutperforms its backbones with little cost and results in a competitive\nperformance against cross-encoder based SOTA while being thousands of times\nfaster. We also find Neural- PCRF effective on a widely used fine-grained\nentity typing dataset with a smaller type set. We pack Neural-PCRF as a network\nmodule that can be plugged onto multi-label type classifiers with ease and\nrelease it in https://github.com/modelscope/adaseq/tree/master/examples/NPCRF.", "published": "2022-12-03 09:49:15", "link": "http://arxiv.org/abs/2212.01581v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CoP: Factual Inconsistency Detection by Controlling the Preference", "abstract": "Abstractive summarization is the process of generating a summary given a\ndocument as input. Although significant progress has been made, the factual\ninconsistency between the document and the generated summary still limits its\npractical applications. Previous work found that the probabilities assigned by\nthe generation model reflect its preferences for the generated summary,\nincluding the preference for factual consistency, and the preference for the\nlanguage or knowledge prior as well. To separate the preference for factual\nconsistency, we propose an unsupervised framework named CoP by controlling the\npreference of the generation model with the help of prompt. More specifically,\nthe framework performs an extra inference step in which a text prompt is\nintroduced as an additional input. In this way, another preference is described\nby the generation probability of this extra inference process. The difference\nbetween the above two preferences, i.e. the difference between the\nprobabilities, could be used as measurements for detecting factual\ninconsistencies. Interestingly, we found that with the properly designed\nprompt, our framework could evaluate specific preferences and serve as\nmeasurements for fine-grained categories of inconsistency, such as\nentity-related inconsistency, coreference-related inconsistency, etc. Moreover,\nour framework could also be extended to the supervised setting to learn better\nprompt from the labeled data as well. Experiments show that our framework\nachieves new SOTA results on three factual inconsistency detection tasks.", "published": "2022-12-03 13:05:24", "link": "http://arxiv.org/abs/2212.01611v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Named Entity and Relation Extraction with Multi-Modal Retrieval", "abstract": "Multi-modal named entity recognition (NER) and relation extraction (RE) aim\nto leverage relevant image information to improve the performance of NER and\nRE. Most existing efforts largely focused on directly extracting potentially\nuseful information from images (such as pixel-level features, identified\nobjects, and associated captions). However, such extraction processes may not\nbe knowledge aware, resulting in information that may not be highly relevant.\nIn this paper, we propose a novel Multi-modal Retrieval based framework (MoRe).\nMoRe contains a text retrieval module and an image-based retrieval module,\nwhich retrieve related knowledge of the input text and image in the knowledge\ncorpus respectively. Next, the retrieval results are sent to the textual and\nvisual models respectively for predictions. Finally, a Mixture of Experts (MoE)\nmodule combines the predictions from the two models to make the final decision.\nOur experiments show that both our textual model and visual model can achieve\nstate-of-the-art performance on four multi-modal NER datasets and one\nmulti-modal RE dataset. With MoE, the model performance can be further improved\nand our analysis demonstrates the benefits of integrating both textual and\nvisual cues for such tasks.", "published": "2022-12-03 13:11:32", "link": "http://arxiv.org/abs/2212.01612v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "T-STAR: Truthful Style Transfer using AMR Graph as Intermediate\n  Representation", "abstract": "Unavailability of parallel corpora for training text style transfer (TST)\nmodels is a very challenging yet common scenario. Also, TST models implicitly\nneed to preserve the content while transforming a source sentence into the\ntarget style. To tackle these problems, an intermediate representation is often\nconstructed that is devoid of style while still preserving the meaning of the\nsource sentence. In this work, we study the usefulness of Abstract Meaning\nRepresentation (AMR) graph as the intermediate style agnostic representation.\nWe posit that semantic notations like AMR are a natural choice for an\nintermediate representation. Hence, we propose T-STAR: a model comprising of\ntwo components, text-to-AMR encoder and a AMR-to-text decoder. We propose\nseveral modeling improvements to enhance the style agnosticity of the generated\nAMR. To the best of our knowledge, T-STAR is the first work that uses AMR as an\nintermediate representation for TST. With thorough experimental evaluation we\nshow T-STAR significantly outperforms state of the art techniques by achieving\non an average 15.2% higher content preservation with negligible loss (3%\napprox.) in style accuracy. Through detailed human evaluation with 90,000\nratings, we also show that T-STAR has up to 50% lesser hallucinations compared\nto state of the art TST models.", "published": "2022-12-03 18:38:03", "link": "http://arxiv.org/abs/2212.01667v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey on Medical Document Summarization", "abstract": "The internet has had a dramatic effect on the healthcare industry, allowing\ndocuments to be saved, shared, and managed digitally. This has made it easier\nto locate and share important data, improving patient care and providing more\nopportunities for medical studies. As there is so much data accessible to\ndoctors and patients alike, summarizing it has become increasingly necessary -\nthis has been supported through the introduction of deep learning and\ntransformer-based networks, which have boosted the sector significantly in\nrecent years. This paper gives a comprehensive survey of the current techniques\nand trends in medical summarization", "published": "2022-12-03 18:46:44", "link": "http://arxiv.org/abs/2212.01669v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Robust NLG Bias Evaluation with Syntactically-diverse Prompts", "abstract": "We present a robust methodology for evaluating biases in natural language\ngeneration(NLG) systems. Previous works use fixed hand-crafted prefix templates\nwith mentions of various demographic groups to prompt models to generate\ncontinuations for bias analysis. These fixed prefix templates could themselves\nbe specific in terms of styles or linguistic structures, which may lead to\nunreliable fairness conclusions that are not representative of the general\ntrends from tone varying prompts. To study this problem, we paraphrase the\nprompts with different syntactic structures and use these to evaluate\ndemographic bias in NLG systems. Our results suggest similar overall bias\ntrends but some syntactic structures lead to contradictory conclusions compared\nto past works. We show that our methodology is more robust and that some\nsyntactic structures prompt more toxic content while others could prompt less\nbiased generation. This suggests the importance of not relying on a fixed\nsyntactic structure and using tone-invariant prompts. Introducing\nsyntactically-diverse prompts can achieve more robust NLG (bias) evaluation.", "published": "2022-12-03 22:11:17", "link": "http://arxiv.org/abs/2212.01700v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Linguistic Constructs as the Representation of the Domain Model in an\n  Intelligent Language Tutoring System", "abstract": "This paper presents the development of an AI-based language learning platform\nRevita. It is a freely available intelligent online tutor, developed to support\nlearners of multiple languages, from low-intermediate to advanced levels. It\nhas been in pilot use by hundreds of students at several universities, whose\nfeedback and needs are shaping the development. One of the main emerging\nfeatures of Revita is the introduction of a system of linguistic constructs as\nthe representation of domain knowledge. The system of constructs is developed\nin close collaboration with experts in language teaching. Constructs define the\ntypes of exercises, the content of the feedback, and enable the detailed\nmodeling and evaluation of learning progress.", "published": "2022-12-03 23:42:28", "link": "http://arxiv.org/abs/2212.01711v1", "categories": ["cs.CL", "K.3"], "primary_category": "cs.CL"}
{"title": "RHO ($\u03c1$): Reducing Hallucination in Open-domain Dialogues with\n  Knowledge Grounding", "abstract": "Dialogue systems can leverage large pre-trained language models and knowledge\nto generate fluent and informative responses. However, these models are still\nprone to produce hallucinated responses not supported by the input source,\nwhich greatly hinders their application. The heterogeneity between external\nknowledge and dialogue context challenges representation learning and source\nintegration, and further contributes to unfaithfulness. To handle this\nchallenge and generate more faithful responses, this paper presents RHO\n($\\rho$) utilizing the representations of linked entities and relation\npredicates from a knowledge graph (KG). We propose (1) local knowledge\ngrounding to combine textual embeddings with the corresponding KG embeddings;\nand (2) global knowledge grounding to equip RHO with multi-hop reasoning\nabilities via the attention mechanism. In addition, we devise a response\nre-ranking technique based on walks over KG sub-graphs for better\nconversational reasoning. Experimental results on OpenDialKG show that our\napproach significantly outperforms state-of-the-art methods on both automatic\nand human evaluation by a large margin, especially in hallucination reduction\n(17.54% in FeQA).", "published": "2022-12-03 10:36:34", "link": "http://arxiv.org/abs/2212.01588v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Intermediate Entity-based Sparse Interpretable Representation Learning", "abstract": "Interpretable entity representations (IERs) are sparse embeddings that are\n\"human-readable\" in that dimensions correspond to fine-grained entity types and\nvalues are predicted probabilities that a given entity is of the corresponding\ntype. These methods perform well in zero-shot and low supervision settings.\nCompared to standard dense neural embeddings, such interpretable\nrepresentations may permit analysis and debugging. However, while fine-tuning\nsparse, interpretable representations improves accuracy on downstream tasks, it\ndestroys the semantics of the dimensions which were enforced in pre-training.\nCan we maintain the interpretable semantics afforded by IERs while improving\npredictive performance on downstream tasks? Toward this end, we propose\nIntermediate enTity-based Sparse Interpretable Representation Learning\n(ItsIRL). ItsIRL realizes improved performance over prior IERs on biomedical\ntasks, while maintaining \"interpretability\" generally and their ability to\nsupport model debugging specifically. The latter is enabled in part by the\nability to perform \"counterfactual\" fine-grained entity type manipulation,\nwhich we explore in this work. Finally, we propose a method to construct entity\ntype based class prototypes for revealing global semantic properties of classes\nlearned by our model.", "published": "2022-12-03 16:16:11", "link": "http://arxiv.org/abs/2212.01641v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Global memory transformer for processing long documents", "abstract": "Transformer variants dominate the state-of-the-art in different natural\nlanguage processing tasks such as translation, reading comprehension and\nsummarization. Our paper is more directed to use general memory slots added to\nthe inputs and studying the results of adding these slots. This paper is a go\non study of general memory slots rule that were added to the input of the\nproposed model in previous work. We have two main tasks;1) pretraining task\nusing masked language modeling and b) fine tuning task using HotpotQA . This\nstudy aims to verify the ability of the proposed model to handle chunks as if\nthey were one chunk comparing with the base model. As baseline we used T5\ntransformer. We studied the rule of memory slots augmented to each input chunk\nand studied the model performance without selector. We found that adding memory\nto input chunks helped the proposed model to overcome the baseline on Masked\nlanguage modeling task with specific training parameters. Ablation study\nreveals the ability of using the compressed input chunks with a degradation in\nperformance.", "published": "2022-12-03 16:40:16", "link": "http://arxiv.org/abs/2212.01650v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Language Models as Agent Models", "abstract": "Language models (LMs) are trained on collections of documents, written by\nindividual human agents to achieve specific goals in an outside world. During\ntraining, LMs have access only to text of these documents, with no direct\nevidence of the internal states of the agents that produced them -- a fact\noften used to argue that LMs are incapable of modeling goal-directed aspects of\nhuman language production and comprehension. Can LMs trained on text learn\nanything at all about the relationship between language and use? I argue that\nLMs are models of intentional communication in a specific, narrow sense. When\nperforming next word prediction given a textual context, an LM can infer and\nrepresent properties of an agent likely to have produced that context. These\nrepresentations can in turn influence subsequent LM generation in the same way\nthat agents' communicative intentions influence their language. I survey\nfindings from the recent literature showing that -- even in today's non-robust\nand error-prone models -- LMs infer and use representations of fine-grained\ncommunicative intentions and more abstract beliefs and goals. Despite the\nlimited nature of their training data, they can thus serve as building blocks\nfor systems that communicate and act intentionally.", "published": "2022-12-03 20:18:16", "link": "http://arxiv.org/abs/2212.01681v1", "categories": ["cs.CL", "cs.MA"], "primary_category": "cs.CL"}
{"title": "Unsupervised Fine-Tuning Data Selection for ASR Using Self-Supervised\n  Speech Models", "abstract": "Self-supervised learning (SSL) has been able to leverage unlabeled data to\nboost the performance of automatic speech recognition (ASR) models when we have\naccess to only a small amount of transcribed speech data. However, this raises\nthe question of which subset of the available unlabeled data should be selected\nfor transcription. Our work investigates different unsupervised data selection\ntechniques for fine-tuning the HuBERT model under a limited transcription\nbudget. We investigate the impact of speaker diversity, gender bias, and topic\ndiversity on the downstream ASR performance. We also devise two novel\ntechniques for unsupervised data selection: pre-training loss based data\nselection and the perplexity of byte pair encoded clustered units (PBPE) and we\nshow how these techniques compare to pure random data selection. Finally, we\nanalyze the correlations between the inherent characteristics of the selected\nfine-tuning subsets as well as how these characteristics correlate with the\nresultant word error rate. We demonstrate the importance of token diversity,\nspeaker diversity, and topic diversity in achieving the best performance in\nterms of WER.", "published": "2022-12-03 18:05:08", "link": "http://arxiv.org/abs/2212.01661v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Can In-context Learners Learn a Reasoning Concept from Demonstrations?", "abstract": "Language models exhibit an emergent ability to learn a new task from a small\nnumber of input-output demonstrations. However, recent work shows that\nin-context learners largely rely on their pre-trained knowledge, such as the\nsentiment of the labels, instead of learning new associations from the input.\nWe argue that the commonly-used few-shot evaluation using a random selection of\nin-context demonstrations can not disentangle models' reliance on such biases,\nas most of the randomly-selected demonstrations do not present relations\ninformative for prediction beyond exposing the task's input-output\ndistribution.\n  Therefore, to evaluate models' in-context learning ability independent of\nmodels' memory, we introduce a Concept-sharing few-shot learning method\nchoosing the demonstrations that share an underlying concept with the predicted\nsample. We extract a set of such concepts from available human explanations and\nmeasure how much models can benefit from presenting these concepts in few-shot\ndemonstrations.\n  We find that most of the recent in-context learners can not consistently\nbenefit from the demonstrated concepts, irrespective of the model size.\nHowever, we note that T0 models are more sensitive to exhibited concepts,\nbenefiting from concept-sharing demonstrations in 7 out of 8 evaluation\nscenarios.", "published": "2022-12-03 21:14:32", "link": "http://arxiv.org/abs/2212.01692v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "UniSyn: An End-to-End Unified Model for Text-to-Speech and Singing Voice\n  Synthesis", "abstract": "Text-to-speech (TTS) and singing voice synthesis (SVS) aim at generating\nhigh-quality speaking and singing voice according to textual input and music\nscores, respectively. Unifying TTS and SVS into a single system is crucial to\nthe applications requiring both of them. Existing methods usually suffer from\nsome limitations, which rely on either both singing and speaking data from the\nsame person or cascaded models of multiple tasks. To address these problems, a\nsimplified elegant framework for TTS and SVS, named UniSyn, is proposed in this\npaper. It is an end-to-end unified model that can make a voice speak and sing\nwith only singing or speaking data from this person. To be specific, a\nmulti-conditional variational autoencoder (MC-VAE), which constructs two\nindependent latent sub-spaces with the speaker- and style-related (i.e. speak\nor sing) conditions for flexible control, is proposed in UniSyn. Moreover,\nsupervised guided-VAE and timbre perturbation with the Wasserstein distance\nconstraint are leveraged to further disentangle the speaker timbre and style.\nExperiments conducted on two speakers and two singers demonstrate that UniSyn\ncan generate natural speaking and singing voice without corresponding training\ndata. The proposed approach outperforms the state-of-the-art end-to-end voice\ngeneration work, which proves the effectiveness and advantages of UniSyn.", "published": "2022-12-03 05:58:10", "link": "http://arxiv.org/abs/2212.01546v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A dataset for audio-video based vehicle speed estimation", "abstract": "Accurate speed estimation of road vehicles is important for several reasons.\nOne is speed limit enforcement, which represents a crucial tool in decreasing\ntraffic accidents and fatalities. Compared with other research areas and\ndomains, the number of available datasets for vehicle speed estimation is still\nvery limited. We present a dataset of on-road audio-video recordings of single\nvehicles passing by a camera at known speeds, maintained stable by the on-board\ncruise control. The dataset contains thirteen vehicles, selected to be as\ndiverse as possible in terms of manufacturer, production year, engine type,\npower and transmission, resulting in a total of $ 400 $ annotated audio-video\nrecordings. The dataset is fully available and intended as a public benchmark\nto facilitate research in audio-video vehicle speed estimation. In addition to\nthe dataset, we propose a cross-validation strategy which can be used in a\nmachine learning model for vehicle speed estimation. Two approaches to\ntraining-validation split of the dataset are proposed.", "published": "2022-12-03 17:02:57", "link": "http://arxiv.org/abs/2212.01651v1", "categories": ["cs.LG", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "A subjective study of the perceptual acceptability of audio-video\n  desynchronization in sports videos", "abstract": "This paper presents the results of a study conducted on the perceptual\nacceptability of audio-video desynchronization for sports videos. The study was\nconducted with 45 videos generated by applying 8 audio-video offsets on 5\nsource contents. 20 subjects participated in the study. The results show that\nhumans are more sensitive to audio-video offset errors for speech stimuli, and\nthe complex events that occur in sports broadcasts have higher thresholds of\nacceptability. This suggests the tuning of audio-video synchronization\nrequirements in broadcasting to the content of the broadcast.", "published": "2022-12-03 20:35:16", "link": "http://arxiv.org/abs/2212.01686v1", "categories": ["eess.IV", "cs.SD", "eess.AS"], "primary_category": "eess.IV"}
