{"title": "Learning when to trust distant supervision: An application to\n  low-resource POS tagging using cross-lingual projection", "abstract": "Cross lingual projection of linguistic annotation suffers from many sources\nof bias and noise, leading to unreliable annotations that cannot be used\ndirectly. In this paper, we introduce a novel approach to sequence tagging that\nlearns to correct the errors from cross-lingual projection using an explicit\ndebiasing layer. This is framed as joint learning over two corpora, one tagged\nwith gold standard and the other with projected tags. We evaluated with only\n1,000 tokens tagged with gold standard tags, along with more plentiful parallel\ndata. Our system equals or exceeds the state-of-the-art on eight simulated\nlow-resource settings, as well as two real low-resource languages, Malagasy and\nKinyarwanda.", "published": "2016-07-05 07:31:22", "link": "http://arxiv.org/abs/1607.01133v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Target-Side Context for Discriminative Models in Statistical Machine\n  Translation", "abstract": "Discriminative translation models utilizing source context have been shown to\nhelp statistical machine translation performance. We propose a novel extension\nof this work using target context information. Surprisingly, we show that this\nmodel can be efficiently integrated directly in the decoding process. Our\napproach scales to large training data sizes and results in consistent\nimprovements in translation quality on four language pairs. We also provide an\nanalysis comparing the strengths of the baseline source-context model with our\nextended source-context and target-context model and we show that our extension\nallows us to better capture morphological coherence. Our work is freely\navailable as part of Moses.", "published": "2016-07-05 08:51:21", "link": "http://arxiv.org/abs/1607.01149v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Chains of Reasoning over Entities, Relations, and Text using Recurrent\n  Neural Networks", "abstract": "Our goal is to combine the rich multistep inference of symbolic logical\nreasoning with the generalization capabilities of neural networks. We are\nparticularly interested in complex reasoning about entities and relations in\ntext and large-scale knowledge bases (KBs). Neelakantan et al. (2015) use RNNs\nto compose the distributed semantics of multi-hop paths in KBs; however for\nmultiple reasons, the approach lacks accuracy and practicality. This paper\nproposes three significant modeling advances: (1) we learn to jointly reason\nabout relations, entities, and entity-types; (2) we use neural attention\nmodeling to incorporate multiple paths; (3) we learn to share strength in a\nsingle RNN that represents logical composition across all relations. On a\nlargescale Freebase+ClueWeb prediction task, we achieve 25% error reduction,\nand a 53% error reduction on sparse relations due to shared strength. On chains\nof reasoning in WordNet we reduce error in mean quantile by 84% versus previous\nstate-of-the-art. The code and data are available at\nhttps://rajarshd.github.io/ChainsofReasoning", "published": "2016-07-05 21:59:04", "link": "http://arxiv.org/abs/1607.01426v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Global Neural CCG Parsing with Optimality Guarantees", "abstract": "We introduce the first global recursive neural parsing model with optimality\nguarantees during decoding. To support global features, we give up dynamic\nprograms and instead search directly in the space of all possible subtrees.\nAlthough this space is exponentially large in the sentence length, we show it\nis possible to learn an efficient A* parser. We augment existing parsing\nmodels, which have informative bounds on the outside score, with a global model\nthat has loose bounds but only needs to model non-local phenomena. The global\nmodel is trained with a new objective that encourages the parser to explore a\ntiny fraction of the search space. The approach is applied to CCG parsing,\nimproving state-of-the-art accuracy by 0.4 F1. The parser finds the optimal\nparse for 99.9% of held-out sentences, exploring on average only 190 subtrees.", "published": "2016-07-05 22:25:10", "link": "http://arxiv.org/abs/1607.01432v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
