{"title": "Hierarchical Multi-Label Classification of Scientific Documents", "abstract": "Automatic topic classification has been studied extensively to assist\nmanaging and indexing scientific documents in a digital collection. With the\nlarge number of topics being available in recent years, it has become necessary\nto arrange them in a hierarchy. Therefore, the automatic classification systems\nneed to be able to classify the documents hierarchically. In addition, each\npaper is often assigned to more than one relevant topic. For example, a paper\ncan be assigned to several topics in a hierarchy tree. In this paper, we\nintroduce a new dataset for hierarchical multi-label text classification\n(HMLTC) of scientific papers called SciHTC, which contains 186,160 papers and\n1,233 categories from the ACM CCS tree. We establish strong baselines for HMLTC\nand propose a multi-task learning approach for topic classification with\nkeyword labeling as an auxiliary task. Our best model achieves a Macro-F1 score\nof 34.57% which shows that this dataset provides significant research\nopportunities on hierarchical scientific topic classification. We make our\ndataset and code available on Github.", "published": "2022-11-05 04:12:57", "link": "http://arxiv.org/abs/2211.02810v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluation of Automated Speech Recognition Systems for Conversational\n  Speech: A Linguistic Perspective", "abstract": "Automatic speech recognition (ASR) meets more informal and free-form input\ndata as voice user interfaces and conversational agents such as the voice\nassistants such as Alexa, Google Home, etc., gain popularity. Conversational\nspeech is both the most difficult and environmentally relevant sort of data for\nspeech recognition. In this paper, we take a linguistic perspective, and take\nthe French language as a case study toward disambiguation of the French\nhomophones. Our contribution aims to provide more insight into human speech\ntranscription accuracy in conditions to reproduce those of state-of-the-art ASR\nsystems, although in a much focused situation. We investigate a case study\ninvolving the most common errors encountered in the automatic transcription of\nFrench language.", "published": "2022-11-05 04:35:40", "link": "http://arxiv.org/abs/2211.02812v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Aligning Recommendation and Conversation via Dual Imitation", "abstract": "Human conversations of recommendation naturally involve the shift of\ninterests which can align the recommendation actions and conversation process\nto make accurate recommendations with rich explanations. However, existing\nconversational recommendation systems (CRS) ignore the advantage of user\ninterest shift in connecting recommendation and conversation, which leads to an\nineffective loose coupling structure of CRS. To address this issue, by modeling\nthe recommendation actions as recommendation paths in a knowledge graph (KG),\nwe propose DICR (Dual Imitation for Conversational Recommendation), which\ndesigns a dual imitation to explicitly align the recommendation paths and user\ninterest shift paths in a recommendation module and a conversation module,\nrespectively. By exchanging alignment signals, DICR achieves bidirectional\npromotion between recommendation and conversation modules and generates\nhigh-quality responses with accurate recommendations and coherent explanations.\nExperiments demonstrate that DICR outperforms the state-of-the-art models on\nrecommendation and conversation performance with automatic, human, and novel\nexplainability metrics.", "published": "2022-11-05 08:13:46", "link": "http://arxiv.org/abs/2211.02848v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HERB: Measuring Hierarchical Regional Bias in Pre-trained Language\n  Models", "abstract": "Fairness has become a trending topic in natural language processing (NLP),\nwhich addresses biases targeting certain social groups such as genders and\nreligions. However, regional bias in language models (LMs), a long-standing\nglobal discrimination problem, still remains unexplored. This paper bridges the\ngap by analysing the regional bias learned by the pre-trained language models\nthat are broadly used in NLP tasks. In addition to verifying the existence of\nregional bias in LMs, we find that the biases on regional groups can be\nstrongly influenced by the geographical clustering of the groups. We\naccordingly propose a HiErarchical Regional Bias evaluation method (HERB)\nutilising the information from the sub-region clusters to quantify the bias in\npre-trained LMs. Experiments show that our hierarchical metric can effectively\nevaluate the regional bias with respect to comprehensive topics and measure the\npotential regional bias that can be propagated to downstream tasks. Our codes\nare available at https://github.com/Bernard-Yang/HERB.", "published": "2022-11-05 11:30:57", "link": "http://arxiv.org/abs/2211.02882v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Legal Argument Reasoning Task in Civil Procedure", "abstract": "We present a new NLP task and dataset from the domain of the U.S. civil\nprocedure. Each instance of the dataset consists of a general introduction to\nthe case, a particular question, and a possible solution argument, accompanied\nby a detailed analysis of why the argument applies in that case. Since the\ndataset is based on a book aimed at law students, we believe that it represents\na truly complex task for benchmarking modern legal language models. Our\nbaseline evaluation shows that fine-tuning a legal transformer provides some\nadvantage over random baseline models, but our analysis reveals that the actual\nability to infer legal arguments remains a challenging open research question.", "published": "2022-11-05 17:41:00", "link": "http://arxiv.org/abs/2211.02950v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Privacy-Preserving Models for Legal Natural Language Processing", "abstract": "Pre-training large transformer models with in-domain data improves domain\nadaptation and helps gain performance on the domain-specific downstream tasks.\nHowever, sharing models pre-trained on potentially sensitive data is prone to\nadversarial privacy attacks. In this paper, we asked to which extent we can\nguarantee privacy of pre-training data and, at the same time, achieve better\ndownstream performance on legal tasks without the need of additional labeled\ndata. We extensively experiment with scalable self-supervised learning of\ntransformer models under the formal paradigm of differential privacy and show\nthat under specific training configurations we can improve downstream\nperformance without sacrifying privacy protection for the in-domain data. Our\nmain contribution is utilizing differential privacy for large-scale\npre-training of transformer language models in the legal NLP domain, which, to\nthe best of our knowledge, has not been addressed before.", "published": "2022-11-05 18:10:50", "link": "http://arxiv.org/abs/2211.02956v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Infer from Unlabeled Data: A Semi-supervised Learning\n  Approach for Robust Natural Language Inference", "abstract": "Natural Language Inference (NLI) or Recognizing Textual Entailment (RTE) aims\nat predicting the relation between a pair of sentences (premise and hypothesis)\nas entailment, contradiction or semantic independence. Although deep learning\nmodels have shown promising performance for NLI in recent years, they rely on\nlarge scale expensive human-annotated datasets. Semi-supervised learning (SSL)\nis a popular technique for reducing the reliance on human annotation by\nleveraging unlabeled data for training. However, despite its substantial\nsuccess on single sentence classification tasks where the challenge in making\nuse of unlabeled data is to assign \"good enough\" pseudo-labels, for NLI tasks,\nthe nature of unlabeled data is more complex: one of the sentences in the pair\n(usually the hypothesis) along with the class label are missing from the data\nand require human annotations, which makes SSL for NLI more challenging. In\nthis paper, we propose a novel way to incorporate unlabeled data in SSL for NLI\nwhere we use a conditional language model, BART to generate the hypotheses for\nthe unlabeled sentences (used as premises). Our experiments show that our SSL\nframework successfully exploits unlabeled data and substantially improves the\nperformance of four NLI datasets in low-resource settings. We release our code\nat: https://github.com/msadat3/SSL_for_NLI.", "published": "2022-11-05 20:34:08", "link": "http://arxiv.org/abs/2211.02971v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BEKG: A Built Environment Knowledge Graph", "abstract": "Practices in the built environment have become more digitalized with the\nrapid development of modern design and construction technologies. However, the\nrequirement of practitioners or scholars to gather complicated professional\nknowledge in the built environment has not been satisfied yet. In this paper,\nmore than 80,000 paper abstracts in the built environment field were obtained\nto build a knowledge graph, a knowledge base storing entities and their\nconnective relations in a graph-structured data model. To ensure the retrieval\naccuracy of the entities and relations in the knowledge graph, two\nwell-annotated datasets have been created, containing 2,000 instances and 1,450\ninstances each in 29 relations for the named entity recognition task and\nrelation extraction task respectively. These two tasks were solved by two\nBERT-based models trained on the proposed dataset. Both models attained an\naccuracy above 85% on these two tasks. More than 200,000 high-quality relations\nand entities were obtained using these models to extract all abstract data.\nFinally, this knowledge graph is presented as a self-developed visualization\nsystem to reveal relations between various entities in the domain. Both the\nsource code and the annotated dataset can be found here:\nhttps://github.com/HKUST-KnowComp/BEKG.", "published": "2022-11-05 09:52:45", "link": "http://arxiv.org/abs/2211.02864v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Tri-Attention: Explicit Context-Aware Attention Mechanism for Natural\n  Language Processing", "abstract": "In natural language processing (NLP), the context of a word or sentence plays\nan essential role. Contextual information such as the semantic representation\nof a passage or historical dialogue forms an essential part of a conversation\nand a precise understanding of the present phrase or sentence. However, the\nstandard attention mechanisms typically generate weights using query and key\nbut ignore context, forming a Bi-Attention framework, despite their great\nsuccess in modeling sequence alignment. This Bi-Attention mechanism does not\nexplicitly model the interactions between the contexts, queries and keys of\ntarget sequences, missing important contextual information and resulting in\npoor attention performance. Accordingly, a novel and general triple-attention\n(Tri-Attention) framework expands the standard Bi-Attention mechanism and\nexplicitly interacts query, key, and context by incorporating context as the\nthird dimension in calculating relevance scores. Four variants of Tri-Attention\nare generated by expanding the two-dimensional vector-based additive,\ndot-product, scaled dot-product, and bilinear operations in Bi-Attention to the\ntensor operations for Tri-Attention. Extensive experiments on three NLP tasks\ndemonstrate that Tri-Attention outperforms about 30 state-of-the-art\nnon-attention, standard Bi-Attention, contextual Bi-Attention approaches and\npretrained neural language models1.", "published": "2022-11-05 13:07:40", "link": "http://arxiv.org/abs/2211.02899v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Event and Entity Extraction from Generated Video Captions", "abstract": "Annotation of multimedia data by humans is time-consuming and costly, while\nreliable automatic generation of semantic metadata is a major challenge. We\npropose a framework to extract semantic metadata from automatically generated\nvideo captions. As metadata, we consider entities, the entities' properties,\nrelations between entities, and the video category. We employ two\nstate-of-the-art dense video captioning models with masked transformer (MT) and\nparallel decoding (PVDC) to generate captions for videos of the ActivityNet\nCaptions dataset. Our experiments show that it is possible to extract entities,\ntheir properties, relations between entities, and the video category from the\ngenerated captions. We observe that the quality of the extracted information is\nmainly influenced by the quality of the event localization in the video as well\nas the performance of the event caption generation.", "published": "2022-11-05 22:06:50", "link": "http://arxiv.org/abs/2211.02982v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "LAMASSU: Streaming Language-Agnostic Multilingual Speech Recognition and\n  Translation Using Neural Transducers", "abstract": "Automatic speech recognition (ASR) and speech translation (ST) can both use\nneural transducers as the model structure. It is thus possible to use a single\ntransducer model to perform both tasks. In real-world applications, such joint\nASR and ST models may need to be streaming and do not require source language\nidentification (i.e. language-agnostic). In this paper, we propose LAMASSU, a\nstreaming language-agnostic multilingual speech recognition and translation\nmodel using neural transducers. Based on the transducer model structure, we\npropose four methods, a unified joint and prediction network for multilingual\noutput, a clustered multilingual encoder, target language identification for\nencoder, and connectionist temporal classification regularization. Experimental\nresults show that LAMASSU not only drastically reduces the model size but also\nreaches the performances of monolingual ASR and bilingual ST models.", "published": "2022-11-05 04:03:55", "link": "http://arxiv.org/abs/2211.02809v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "PASTA: Table-Operations Aware Fact Verification via Sentence-Table Cloze\n  Pre-training", "abstract": "Fact verification has attracted a lot of research attention recently, e.g.,\nin journalism, marketing, and policymaking, as misinformation and\ndisinformation online can sway one's opinion and affect one's actions. While\nfact-checking is a hard task in general, in many cases, false statements can be\neasily debunked based on analytics over tables with reliable information.\nHence, table-based fact verification has recently emerged as an important and\ngrowing research area. Yet, progress has been limited due to the lack of\ndatasets that can be used to pre-train language models (LMs) to be aware of\ncommon table operations, such as aggregating a column or comparing tuples. To\nbridge this gap, in this paper we introduce PASTA, a novel state-of-the-art\nframework for table-based fact verification via pre-training with synthesized\nsentence-table cloze questions. In particular, we design six types of common\nsentence-table cloze tasks, including Filter, Aggregation, Superlative,\nComparative, Ordinal, and Unique, based on which we synthesize a large corpus\nconsisting of 1.2 million sentence-table pairs from WikiTables. PASTA uses a\nrecent pre-trained LM, DeBERTaV3, and further pretrains it on our corpus. Our\nexperimental results show that PASTA achieves new state-of-the-art performance\non two table-based fact verification benchmarks: TabFact and SEM-TAB-FACTS. In\nparticular, on the complex set of TabFact, which contains multiple operations,\nPASTA largely outperforms the previous state of the art by 4.7 points (85.6%\nvs. 80.9%), and the gap between PASTA and human performance on the small\nTabFact test set is narrowed to just 1.5 points (90.6% vs. 92.1%).", "published": "2022-11-05 05:17:38", "link": "http://arxiv.org/abs/2211.02816v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2.7; I.2.6"], "primary_category": "cs.CL"}
{"title": "EventEA: Benchmarking Entity Alignment for Event-centric Knowledge\n  Graphs", "abstract": "Entity alignment is to find identical entities in different knowledge graphs\n(KGs) that refer to the same real-world object. Embedding-based entity\nalignment techniques have been drawing a lot of attention recently because they\ncan help solve the issue of symbolic heterogeneity in different KGs. However,\nin this paper, we show that the progress made in the past was due to biased and\nunchallenging evaluation. We highlight two major flaws in existing datasets\nthat favor embedding-based entity alignment techniques, i.e., the isomorphic\ngraph structures in relation triples and the weak heterogeneity in attribute\ntriples. Towards a critical evaluation of embedding-based entity alignment\nmethods, we construct a new dataset with heterogeneous relations and attributes\nbased on event-centric KGs. We conduct extensive experiments to evaluate\nexisting popular methods, and find that they fail to achieve promising\nperformance. As a new approach to this difficult problem, we propose a\ntime-aware literal encoder for entity alignment. The dataset and source code\nare publicly available to foster future research. Our work calls for more\neffective and practical embedding-based solutions to entity alignment.", "published": "2022-11-05 05:34:21", "link": "http://arxiv.org/abs/2211.02817v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Textual Manifold-based Defense Against Natural Language Adversarial\n  Examples", "abstract": "Recent studies on adversarial images have shown that they tend to leave the\nunderlying low-dimensional data manifold, making them significantly more\nchallenging for current models to make correct predictions. This so-called\noff-manifold conjecture has inspired a novel line of defenses against\nadversarial attacks on images. In this study, we find a similar phenomenon\noccurs in the contextualized embedding space induced by pretrained language\nmodels, in which adversarial texts tend to have their embeddings diverge from\nthe manifold of natural ones. Based on this finding, we propose Textual\nManifold-based Defense (TMD), a defense mechanism that projects text embeddings\nonto an approximated embedding manifold before classification. It reduces the\ncomplexity of potential adversarial examples, which ultimately enhances the\nrobustness of the protected model. Through extensive experiments, our method\nconsistently and significantly outperforms previous defenses under various\nattack settings without trading off clean accuracy. To the best of our\nknowledge, this is the first NLP defense that leverages the manifold structure\nagainst adversarial attacks. Our code is available at\n\\url{https://github.com/dangne/tmd}.", "published": "2022-11-05 11:19:47", "link": "http://arxiv.org/abs/2211.02878v1", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Comparison of Automatic Labelling Approaches for Sentiment Analysis", "abstract": "Labelling a large quantity of social media data for the task of supervised\nmachine learning is not only time-consuming but also difficult and expensive.\nOn the other hand, the accuracy of supervised machine learning models is\nstrongly related to the quality of the labelled data on which they train, and\nautomatic sentiment labelling techniques could reduce the time and cost of\nhuman labelling. We have compared three automatic sentiment labelling\ntechniques: TextBlob, Vader, and Afinn to assign sentiments to tweets without\nany human assistance. We compare three scenarios: one uses training and testing\ndatasets with existing ground truth labels; the second experiment uses\nautomatic labels as training and testing datasets; and the third experiment\nuses three automatic labelling techniques to label the training dataset and\nuses the ground truth labels for testing. The experiments were evaluated on two\nTwitter datasets: SemEval-2013 (DS-1) and SemEval-2016 (DS-2). Results show\nthat the Afinn labelling technique obtains the highest accuracy of 80.17%\n(DS-1) and 80.05% (DS-2) using a BiLSTM deep learning model. These findings\nimply that automatic text labelling could provide significant benefits, and\nsuggest a feasible alternative to the time and cost of human labelling efforts.", "published": "2022-11-05 21:41:44", "link": "http://arxiv.org/abs/2211.02976v1", "categories": ["cs.CL", "cs.DL", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "VISinger 2: High-Fidelity End-to-End Singing Voice Synthesis Enhanced by\n  Digital Signal Processing Synthesizer", "abstract": "End-to-end singing voice synthesis (SVS) model VISinger can achieve better\nperformance than the typical two-stage model with fewer parameters. However,\nVISinger has several problems: text-to-phase problem, the end-to-end model\nlearns the meaningless mapping of text-to-phase; glitches problem, the harmonic\ncomponents corresponding to the periodic signal of the voiced segment occurs a\nsudden change with audible artefacts; low sampling rate, the sampling rate of\n24KHz does not meet the application needs of high-fidelity generation with the\nfull-band rate (44.1KHz or higher). In this paper, we propose VISinger 2 to\naddress these issues by integrating the digital signal processing (DSP) methods\nwith VISinger. Specifically, inspired by recent advances in differentiable\ndigital signal processing (DDSP), we incorporate a DSP synthesizer into the\ndecoder to solve the above issues. The DSP synthesizer consists of a harmonic\nsynthesizer and a noise synthesizer to generate periodic and aperiodic signals,\nrespectively, from the latent representation z in VISinger. It supervises the\nposterior encoder to extract the latent representation without phase\ninformation and avoid the prior encoder modelling text-to-phase mapping. To\navoid glitch artefacts, the HiFi-GAN is modified to accept the waveforms\ngenerated by the DSP synthesizer as a condition to produce the singing voice.\nMoreover, with the improved waveform decoder, VISinger 2 manages to generate\n44.1kHz singing audio with richer expression and better quality. Experiments on\nOpenCpop corpus show that VISinger 2 outperforms VISinger, CpopSing and\nRefineSinger in both subjective and objective metrics.", "published": "2022-11-05 13:35:00", "link": "http://arxiv.org/abs/2211.02903v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Breaking the trade-off in personalized speech enhancement with\n  cross-task knowledge distillation", "abstract": "Personalized speech enhancement (PSE) models achieve promising results\ncompared with unconditional speech enhancement models due to their ability to\nremove interfering speech in addition to background noise. Unlike unconditional\nspeech enhancement, causal PSE models may occasionally remove the target speech\nby mistake. The PSE models also tend to leak interfering speech when the target\nspeaker is silent for an extended period. We show that existing PSE methods\nsuffer from a trade-off between speech over-suppression and interference\nleakage by addressing one problem at the expense of the other. We propose a new\nPSE model training framework using cross-task knowledge distillation to\nmitigate this trade-off. Specifically, we utilize a personalized voice activity\ndetector (pVAD) during training to exclude the non-target speech frames that\nare wrongly identified as containing the target speaker with hard or soft\nclassification. This prevents the PSE model from being too aggressive while\nstill allowing the model to learn to suppress the input speech when it is\nlikely to be spoken by interfering speakers. Comprehensive evaluation results\nare presented, covering various PSE usage scenarios.", "published": "2022-11-05 17:02:55", "link": "http://arxiv.org/abs/2211.02944v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Effective Audio Classification Network Based on Paired Inverse Pyramid\n  Structure and Dense MLP Block", "abstract": "Recently, massive architectures based on Convolutional Neural Network (CNN)\nand self-attention mechanisms have become necessary for audio classification.\nWhile these techniques are state-of-the-art, these works' effectiveness can\nonly be guaranteed with huge computational costs and parameters, large amounts\nof data augmentation, transfer from large datasets and some other tricks. By\nutilizing the lightweight nature of audio, we propose an efficient network\nstructure called Paired Inverse Pyramid Structure (PIP) and a network called\nPaired Inverse Pyramid Structure MLP Network (PIPMN). The PIPMN reaches 96\\% of\nEnvironmental Sound Classification (ESC) accuracy on the UrbanSound8K dataset\nand 93.2\\% of Music Genre Classification (MGC) on the GTAZN dataset, with only\n1 million parameters. Both of the results are achieved without data\naugmentation or model transfer. Public code is available at:\nhttps://github.com/JNAIC/PIPMN", "published": "2022-11-05 16:47:17", "link": "http://arxiv.org/abs/2211.02940v4", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
