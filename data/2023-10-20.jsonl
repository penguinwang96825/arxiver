{"title": "Enhancing Zero-Shot Crypto Sentiment with Fine-tuned Language Model and\n  Prompt Engineering", "abstract": "Blockchain technology has revolutionized the financial landscape, with\ncryptocurrencies gaining widespread adoption for their decentralized and\ntransparent nature. As the sentiment expressed on social media platforms can\nsignificantly influence cryptocurrency discussions and market movements,\nsentiment analysis has emerged as a crucial tool for understanding public\nopinion and predicting market trends. Motivated by the aim to enhance sentiment\nanalysis accuracy in the cryptocurrency domain, this paper investigates\nfine-tuning techniques on large language models. This paper also investigates\nthe efficacy of supervised fine-tuning and instruction-based fine-tuning on\nlarge language models for unseen tasks. Experimental results demonstrate a\nsignificant average zero-shot performance gain of 40% after fine-tuning,\nhighlighting the potential of this technique in optimizing pre-trained language\nmodel efficiency. Additionally, the impact of instruction tuning on models of\nvarying scales is examined, revealing that larger models benefit from\ninstruction tuning, achieving the highest average accuracy score of 75.16%. In\ncontrast, smaller-scale models may experience reduced generalization due to the\ncomplete utilization of model capacity. To gain deeper insight about how\ninstruction works with these language models, this paper presents an\nexperimental investigation into the response of an instruction-based model\nunder different instruction tuning setups. The investigation demonstrates that\nthe model achieves an average accuracy score of 72.38% for short and simple\ninstructions. This performance significantly outperforms its accuracy under\nlong and complex instructions by over 12%, thereby effectively highlighting the\nprofound significance of instruction characteristics in maximizing model\nperformance.", "published": "2023-10-20 02:15:51", "link": "http://arxiv.org/abs/2310.13226v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Less the Merrier? Investigating Language Representation in\n  Multilingual Models", "abstract": "Multilingual Language Models offer a way to incorporate multiple languages in\none model and utilize cross-language transfer learning to improve performance\nfor different Natural Language Processing (NLP) tasks. Despite progress in\nmultilingual models, not all languages are supported as well, particularly in\nlow-resource settings. In this work, we investigate the linguistic\nrepresentation of different languages in multilingual models. We start by\nasking the question which languages are supported in popular multilingual\nmodels and which languages are left behind. Then, for included languages, we\nlook at models' learned representations based on language family and dialect\nand try to understand how models' learned representations for~(1) seen and~(2)\nunseen languages vary across different language groups. In addition, we test\nand analyze performance on downstream tasks such as text generation and Named\nEntity Recognition. We observe from our experiments that community-centered\nmodels -- models that focus on languages of a given family or geographical\nlocation and are built by communities who speak them -- perform better at\ndistinguishing between languages in the same family for low-resource languages.\nOur paper contributes to the literature in understanding multilingual models\nand their shortcomings and offers insights on potential ways to improve them.", "published": "2023-10-20 02:26:34", "link": "http://arxiv.org/abs/2310.13228v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Quality-based Syntactic Template Retriever for\n  Syntactically-controlled Paraphrase Generation", "abstract": "Existing syntactically-controlled paraphrase generation (SPG) models perform\npromisingly with human-annotated or well-chosen syntactic templates. However,\nthe difficulty of obtaining such templates actually hinders the practical\napplication of SPG models. For one thing, the prohibitive cost makes it\nunfeasible to manually design decent templates for every source sentence. For\nanother, the templates automatically retrieved by current heuristic methods are\nusually unreliable for SPG models to generate qualified paraphrases. To escape\nthis dilemma, we propose a novel Quality-based Syntactic Template Retriever\n(QSTR) to retrieve templates based on the quality of the to-be-generated\nparaphrases. Furthermore, for situations requiring multiple paraphrases for\neach source sentence, we design a Diverse Templates Search (DTS) algorithm,\nwhich can enhance the diversity between paraphrases without sacrificing\nquality. Experiments demonstrate that QSTR can significantly surpass existing\nretrieval methods in generating high-quality paraphrases and even perform\ncomparably with human-annotated templates in terms of reference-free metrics.\nAdditionally, human evaluation and the performance on downstream tasks using\nour generated paraphrases for data augmentation showcase the potential of our\nQSTR and DTS algorithm in practical scenarios.", "published": "2023-10-20 03:55:39", "link": "http://arxiv.org/abs/2310.13262v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MoqaGPT : Zero-Shot Multi-modal Open-domain Question Answering with\n  Large Language Model", "abstract": "Multi-modal open-domain question answering typically requires evidence\nretrieval from databases across diverse modalities, such as images, tables,\npassages, etc. Even Large Language Models (LLMs) like GPT-4 fall short in this\ntask. To enable LLMs to tackle the task in a zero-shot manner, we introduce\nMoqaGPT, a straightforward and flexible framework. Using a divide-and-conquer\nstrategy that bypasses intricate multi-modality ranking, our framework can\naccommodate new modalities and seamlessly transition to new models for the\ntask. Built upon LLMs, MoqaGPT retrieves and extracts answers from each\nmodality separately, then fuses this multi-modal information using LLMs to\nproduce a final answer. Our methodology boosts performance on the MMCoQA\ndataset, improving F1 by +37.91 points and EM by +34.07 points over the\nsupervised baseline. On the MultiModalQA dataset, MoqaGPT surpasses the\nzero-shot baseline, improving F1 by 9.5 points and EM by 10.1 points, and\nsignificantly closes the gap with supervised methods. Our codebase is available\nat https://github.com/lezhang7/MOQAGPT.", "published": "2023-10-20 04:09:36", "link": "http://arxiv.org/abs/2310.13265v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Interpreting Indirect Answers to Yes-No Questions in Multiple Languages", "abstract": "Yes-no questions expect a yes or no for an answer, but people often skip\npolar keywords. Instead, they answer with long explanations that must be\ninterpreted. In this paper, we focus on this challenging problem and release\nnew benchmarks in eight languages. We present a distant supervision approach to\ncollect training data. We also demonstrate that direct answers (i.e., with\npolar keywords) are useful to train models to interpret indirect answers (i.e.,\nwithout polar keywords). Experimental results demonstrate that monolingual\nfine-tuning is beneficial if training data can be obtained via distant\nsupervision for the language of interest (5 languages). Additionally, we show\nthat cross-lingual fine-tuning is always beneficial (8 languages).", "published": "2023-10-20 05:43:33", "link": "http://arxiv.org/abs/2310.13290v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring the Impact of Corpus Diversity on Financial Pretrained\n  Language Models", "abstract": "Over the past few years, various domain-specific pretrained language models\n(PLMs) have been proposed and have outperformed general-domain PLMs in\nspecialized areas such as biomedical, scientific, and clinical domains. In\naddition, financial PLMs have been studied because of the high economic impact\nof financial data analysis. However, we found that financial PLMs were not\npretrained on sufficiently diverse financial data. This lack of diverse\ntraining data leads to a subpar generalization performance, resulting in\ngeneral-purpose PLMs, including BERT, often outperforming financial PLMs on\nmany downstream tasks. To address this issue, we collected a broad range of\nfinancial corpus and trained the Financial Language Model (FiLM) on these\ndiverse datasets. Our experimental results confirm that FiLM outperforms not\nonly existing financial PLMs but also general domain PLMs. Furthermore, we\nprovide empirical evidence that this improvement can be achieved even for\nunseen corpus groups.", "published": "2023-10-20 07:04:08", "link": "http://arxiv.org/abs/2310.13312v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Sharpness-Aware Quantization for Pre-trained Language Models", "abstract": "Quantization is a promising approach for reducing memory overhead and\naccelerating inference, especially in large pre-trained language model (PLM)\nscenarios. While having no access to original training data due to security and\nprivacy concerns has emerged the demand for zero-shot quantization. Most of the\ncutting-edge zero-shot quantization methods primarily 1) apply to computer\nvision tasks, and 2) neglect of overfitting problem in the generative\nadversarial learning process, leading to sub-optimal performance. Motivated by\nthis, we propose a novel zero-shot sharpness-aware quantization (ZSAQ)\nframework for the zero-shot quantization of various PLMs. The key algorithm in\nsolving ZSAQ is the SAM-SGA optimization, which aims to improve the\nquantization accuracy and model generalization via optimizing a minimax\nproblem. We theoretically prove the convergence rate for the minimax\noptimization problem and this result can be applied to other nonconvex-PL\nminimax optimization frameworks. Extensive experiments on 11 tasks demonstrate\nthat our method brings consistent and significant performance gains on both\ndiscriminative and generative PLMs, i.e., up to +6.98 average score.\nFurthermore, we empirically validate that our method can effectively improve\nthe model generalization.", "published": "2023-10-20 07:09:56", "link": "http://arxiv.org/abs/2310.13315v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Hard Samples: Robust and Effective Grammatical Error Correction\n  with Cycle Self-Augmenting", "abstract": "Recent studies have revealed that grammatical error correction methods in the\nsequence-to-sequence paradigm are vulnerable to adversarial attack, and simply\nutilizing adversarial examples in the pre-training or post-training process can\nsignificantly enhance the robustness of GEC models to certain types of attack\nwithout suffering too much performance loss on clean data. In this paper, we\nfurther conduct a thorough robustness evaluation of cutting-edge GEC methods\nfor four different types of adversarial attacks and propose a simple yet very\neffective Cycle Self-Augmenting (CSA) method accordingly. By leveraging the\naugmenting data from the GEC models themselves in the post-training process and\nintroducing regularization data for cycle training, our proposed method can\neffectively improve the model robustness of well-trained GEC models with only a\nfew more training epochs as an extra cost. More concretely, further training on\nthe regularization data can prevent the GEC models from over-fitting on\neasy-to-learn samples and thus can improve the generalization capability and\nrobustness towards unseen data (adversarial noise/samples). Meanwhile, the\nself-augmented data can provide more high-quality pseudo pairs to improve model\nperformance on the original testing data. Experiments on four benchmark\ndatasets and seven strong models indicate that our proposed training method can\nsignificantly enhance the robustness of four types of attacks without using\npurposely built adversarial examples in training. Evaluation results on clean\ndata further confirm that our proposed CSA method significantly improves the\nperformance of four baselines and yields nearly comparable results with other\nstate-of-the-art models. Our code is available at\nhttps://github.com/ZetangForward/CSA-GEC.", "published": "2023-10-20 07:31:23", "link": "http://arxiv.org/abs/2310.13321v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large-Scale and Multi-Perspective Opinion Summarization with Diverse\n  Review Subsets", "abstract": "Opinion summarization is expected to digest larger review sets and provide\nsummaries from different perspectives. However, most existing solutions are\ndeficient in epitomizing extensive reviews and offering opinion summaries from\nvarious angles due to the lack of designs for information selection. To this\nend, we propose SUBSUMM, a supervised summarization framework for large-scale\nmulti-perspective opinion summarization. SUBSUMM consists of a review sampling\nstrategy set and a two-stage training scheme. The sampling strategies take\nsentiment orientation and contrastive information value into consideration,\nwith which the review subsets from different perspectives and quality levels\ncan be selected. Subsequently, the summarizer is encouraged to learn from the\nsub-optimal and optimal subsets successively in order to capitalize on the\nmassive input. Experimental results on AmaSum and Rotten Tomatoes datasets\ndemonstrate that SUBSUMM is adept at generating pros, cons, and verdict\nsummaries from hundreds of input reviews. Furthermore, our in-depth analysis\nverifies that the advanced selection of review subsets and the two-stage\ntraining scheme are vital to boosting the summarization performance.", "published": "2023-10-20 08:08:13", "link": "http://arxiv.org/abs/2310.13340v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analyzing Cognitive Plausibility of Subword Tokenization", "abstract": "Subword tokenization has become the de-facto standard for tokenization,\nalthough comparative evaluations of subword vocabulary quality across languages\nare scarce. Existing evaluation studies focus on the effect of a tokenization\nalgorithm on the performance in downstream tasks, or on engineering criteria\nsuch as the compression rate. We present a new evaluation paradigm that focuses\non the cognitive plausibility of subword tokenization. We analyze the\ncorrelation of the tokenizer output with the response time and accuracy of\nhuman performance on a lexical decision task. We compare three tokenization\nalgorithms across several languages and vocabulary sizes. Our results indicate\nthat the UnigramLM algorithm yields less cognitively plausible tokenization\nbehavior and a worse coverage of derivational morphemes, in contrast with prior\nwork.", "published": "2023-10-20 08:25:37", "link": "http://arxiv.org/abs/2310.13348v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "APP: Adaptive Prototypical Pseudo-Labeling for Few-shot OOD Detection", "abstract": "Detecting out-of-domain (OOD) intents from user queries is essential for a\ntask-oriented dialogue system. Previous OOD detection studies generally work on\nthe assumption that plenty of labeled IND intents exist. In this paper, we\nfocus on a more practical few-shot OOD setting where there are only a few\nlabeled IND data and massive unlabeled mixed data that may belong to IND or\nOOD. The new scenario carries two key challenges: learning discriminative\nrepresentations using limited IND data and leveraging unlabeled mixed data.\nTherefore, we propose an adaptive prototypical pseudo-labeling (APP) method for\nfew-shot OOD detection, including a prototypical OOD detection framework\n(ProtoOOD) to facilitate low-resource OOD detection using limited IND data, and\nan adaptive pseudo-labeling method to produce high-quality pseudo OOD\\&IND\nlabels. Extensive experiments and analysis demonstrate the effectiveness of our\nmethod for few-shot OOD detection.", "published": "2023-10-20 09:48:52", "link": "http://arxiv.org/abs/2310.13380v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cache me if you Can: an Online Cost-aware Teacher-Student framework to\n  Reduce the Calls to Large Language Models", "abstract": "Prompting Large Language Models (LLMs) performs impressively in zero- and\nfew-shot settings. Hence, small and medium-sized enterprises (SMEs) that cannot\nafford the cost of creating large task-specific training datasets, but also the\ncost of pretraining their own LLMs, are increasingly turning to third-party\nservices that allow them to prompt LLMs. However, such services currently\nrequire a payment per call, which becomes a significant operating expense\n(OpEx). Furthermore, customer inputs are often very similar over time, hence\nSMEs end-up prompting LLMs with very similar instances. We propose a framework\nthat allows reducing the calls to LLMs by caching previous LLM responses and\nusing them to train a local inexpensive model on the SME side. The framework\nincludes criteria for deciding when to trust the local model or call the LLM,\nand a methodology to tune the criteria and measure the tradeoff between\nperformance and cost. For experimental purposes, we instantiate our framework\nwith two LLMs, GPT-3.5 or GPT-4, and two inexpensive students, a k-NN\nclassifier or a Multi-Layer Perceptron, using two common business tasks, intent\nrecognition and sentiment analysis. Experimental results indicate that\nsignificant OpEx savings can be obtained with only slightly lower performance.", "published": "2023-10-20 10:05:07", "link": "http://arxiv.org/abs/2310.13395v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Explicit Alignment and Many-to-many Entailment Based Reasoning for\n  Conversational Machine Reading", "abstract": "Conversational Machine Reading (CMR) requires answering a user's initial\nquestion through multi-turn dialogue interactions based on a given document.\nAlthough there exist many effective methods, they largely neglected the\nalignment between the document and the user-provided information, which\nsignificantly affects the intermediate decision-making and subsequent follow-up\nquestion generation. To address this issue, we propose a pipeline framework\nthat (1) aligns the aforementioned two sides in an explicit way, (2)makes\ndecisions using a lightweight many-to-many entailment reasoning module, and (3)\ndirectly generates follow-up questions based on the document and previously\nasked questions. Our proposed method achieves state-of-the-art in\nmicro-accuracy and ranks the first place on the public leaderboard of the CMR\nbenchmark dataset ShARC.", "published": "2023-10-20 10:27:24", "link": "http://arxiv.org/abs/2310.13409v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Enhancing Relational Rules for Knowledge Graph Link Prediction", "abstract": "Graph neural networks (GNNs) have shown promising performance for knowledge\ngraph reasoning. A recent variant of GNN called progressive relational graph\nneural network (PRGNN), utilizes relational rules to infer missing knowledge in\nrelational digraphs and achieves notable results. However, during reasoning\nwith PRGNN, two important properties are often overlooked: (1) the\nsequentiality of relation composition, where the order of combining different\nrelations affects the semantics of the relational rules, and (2) the lagged\nentity information propagation, where the transmission speed of required\ninformation lags behind the appearance speed of new entities. Ignoring these\nproperties leads to incorrect relational rule learning and decreased reasoning\naccuracy. To address these issues, we propose a novel knowledge graph reasoning\napproach, the Relational rUle eNhanced Graph Neural Network (RUN-GNN).\nSpecifically, RUN-GNN employs a query related fusion gate unit to model the\nsequentiality of relation composition and utilizes a buffering update mechanism\nto alleviate the negative effect of lagged entity information propagation,\nresulting in higher-quality relational rule learning. Experimental results on\nmultiple datasets demonstrate the superiority of RUN-GNN is superior on both\ntransductive and inductive link prediction tasks.", "published": "2023-10-20 10:38:28", "link": "http://arxiv.org/abs/2310.13411v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Conversation Chronicles: Towards Diverse Temporal and Relational\n  Dynamics in Multi-Session Conversations", "abstract": "In the field of natural language processing, open-domain chatbots have\nemerged as an important research topic. However, a major limitation of existing\nopen-domain chatbot research is its singular focus on short single-session\ndialogue, neglecting the potential need for understanding contextual\ninformation in multiple consecutive sessions that precede an ongoing dialogue.\nAmong the elements that compose the context in multi-session conversation\nsettings, the time intervals between sessions and the relationships between\nspeakers would be particularly important. Despite their importance, current\nresearch efforts have not sufficiently addressed these dialogical components.\nIn this paper, we introduce a new 1M multi-session dialogue dataset, called\nConversation Chronicles, for implementing a long-term conversation setup in\nwhich time intervals and fine-grained speaker relationships are incorporated.\nFollowing recent works, we exploit a large language model to produce the data.\nThe extensive human evaluation shows that dialogue episodes in Conversation\nChronicles reflect those properties while maintaining coherent and consistent\ninteractions across all the sessions. We also propose a dialogue model, called\nReBot, which consists of chronological summarization and dialogue generation\nmodules using only around 630M parameters. When trained on Conversation\nChronicles, ReBot demonstrates long-term context understanding with a high\nhuman engagement score.", "published": "2023-10-20 11:06:21", "link": "http://arxiv.org/abs/2310.13420v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Past, Present, and Future of Typological Databases in NLP", "abstract": "Typological information has the potential to be beneficial in the development\nof NLP models, particularly for low-resource languages. Unfortunately, current\nlarge-scale typological databases, notably WALS and Grambank, are inconsistent\nboth with each other and with other sources of typological information, such as\nlinguistic grammars. Some of these inconsistencies stem from coding errors or\nlinguistic variation, but many of the disagreements are due to the discrete\ncategorical nature of these databases. We shed light on this issue by\nsystematically exploring disagreements across typological databases and\nresources, and their uses in NLP, covering the past and present. We next\ninvestigate the future of such work, offering an argument that a continuous\nview of typological features is clearly beneficial, echoing recommendations\nfrom linguistics. We propose that such a view of typology has significant\npotential in the future, including in language modeling in low-resource\nscenarios.", "published": "2023-10-20 12:01:42", "link": "http://arxiv.org/abs/2310.13440v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Steering Large Language Models for Machine Translation with Finetuning\n  and In-Context Learning", "abstract": "Large language models (LLMs) are a promising avenue for machine translation\n(MT). However, current LLM-based MT systems are brittle: their effectiveness\nhighly depends on the choice of few-shot examples and they often require extra\npost-processing due to overgeneration. Alternatives such as finetuning on\ntranslation instructions are computationally expensive and may weaken\nin-context learning capabilities, due to overspecialization. In this paper, we\nprovide a closer look at this problem. We start by showing that adapter-based\nfinetuning with LoRA matches the performance of traditional finetuning while\nreducing the number of training parameters by a factor of 50. This method also\noutperforms few-shot prompting and eliminates the need for post-processing or\nin-context examples. However, we show that finetuning generally degrades\nfew-shot performance, hindering adaptation capabilities. Finally, to obtain the\nbest of both worlds, we propose a simple approach that incorporates few-shot\nexamples during finetuning. Experiments on 10 language pairs show that our\nproposed approach recovers the original few-shot capabilities while keeping the\nadded benefits of finetuning.", "published": "2023-10-20 12:29:51", "link": "http://arxiv.org/abs/2310.13448v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DistillCSE: Distilled Contrastive Learning for Sentence Embeddings", "abstract": "This paper proposes the DistillCSE framework, which performs contrastive\nlearning under the self-training paradigm with knowledge distillation. The\npotential advantage of DistillCSE is its self-enhancing feature: using a base\nmodel to provide additional supervision signals, a stronger model may be\nlearned through knowledge distillation. However, the vanilla DistillCSE through\nthe standard implementation of knowledge distillation only achieves marginal\nimprovements due to severe overfitting. The further quantitative analyses\ndemonstrate the reason that the standard knowledge distillation exhibits a\nrelatively large variance of the teacher model's logits due to the essence of\ncontrastive learning. To mitigate the issue induced by high variance, this\npaper accordingly proposed two simple yet effective solutions for knowledge\ndistillation: a Group-P shuffling strategy as an implicit regularization and\nthe averaging logits from multiple teacher components. Experiments on standard\nbenchmarks demonstrate that the proposed DistillCSE outperforms many strong\nbaseline methods and yields a new state-of-the-art performance.", "published": "2023-10-20 13:45:59", "link": "http://arxiv.org/abs/2310.13499v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Question Generation with Multi-level Content Planning", "abstract": "This paper addresses the problem of generating questions from a given context\nand an answer, specifically focusing on questions that require multi-hop\nreasoning across an extended context. Previous studies have suggested that key\nphrase selection is essential for question generation (QG), yet it is still\nchallenging to connect such disjointed phrases into meaningful questions,\nparticularly for long context. To mitigate this issue, we propose MultiFactor,\na novel QG framework based on multi-level content planning. Specifically,\nMultiFactor includes two components: FA-model, which simultaneously selects key\nphrases and generates full answers, and Q-model which takes the generated full\nanswer as an additional input to generate questions. Here, full answer\ngeneration is introduced to connect the short answer with the selected key\nphrases, thus forming an answer-aware summary to facilitate QG. Both FA-model\nand Q-model are formalized as simple-yet-effective Phrase-Enhanced\nTransformers, our joint model for phrase selection and text generation.\nExperimental results show that our method outperforms strong baselines on two\npopular QG datasets. Our code is available at\nhttps://github.com/zeaver/MultiFactor.", "published": "2023-10-20 13:57:01", "link": "http://arxiv.org/abs/2310.13512v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Teaching Language Models to Self-Improve through Interactive\n  Demonstrations", "abstract": "The self-improving ability of large language models (LLMs), enabled by\nprompting them to analyze and revise their own outputs, has garnered\nsignificant interest in recent research. However, this ability has been shown\nto be absent and difficult to learn for smaller models, thus widening the\nperformance gap between state-of-the-art LLMs and more cost-effective and\nfaster ones. To reduce this gap, we introduce TriPosT, a training algorithm\nthat endows smaller models with such self-improvement ability, and show that\nour approach can improve a LLaMA-7b's performance on math and reasoning tasks\nby up to 7.13%. In contrast to prior work, we achieve this by using the smaller\nmodel to interact with LLMs to collect feedback and improvements on its own\ngenerations. We then replay this experience to train the small model. Our\nexperiments on four math and reasoning datasets show that the interactive\nexperience of learning from and correcting its own mistakes is crucial for\nsmall models to improve their performance.", "published": "2023-10-20 14:11:04", "link": "http://arxiv.org/abs/2310.13522v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Why Can Large Language Models Generate Correct Chain-of-Thoughts?", "abstract": "This paper delves into the capabilities of large language models (LLMs),\nspecifically focusing on advancing the theoretical comprehension of\nchain-of-thought prompting. We investigate how LLMs can be effectively induced\nto generate a coherent chain of thoughts. To achieve this, we introduce a\ntwo-level hierarchical graphical model tailored for natural language\ngeneration. Within this framework, we establish a compelling geometrical\nconvergence rate that gauges the likelihood of an LLM-generated chain of\nthoughts compared to those originating from the true language. Our findings\nprovide a theoretical justification for the ability of LLMs to produce the\ncorrect sequence of thoughts (potentially) explaining performance gains in\ntasks demanding reasoning skills.", "published": "2023-10-20 15:09:46", "link": "http://arxiv.org/abs/2310.13571v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic Decomposition of Question and SQL for Text-to-SQL Parsing", "abstract": "Text-to-SQL semantic parsing faces challenges in generalizing to cross-domain\nand complex queries. Recent research has employed a question decomposition\nstrategy to enhance the parsing of complex SQL queries. However, this strategy\nencounters two major obstacles: (1) existing datasets lack question\ndecomposition; (2) due to the syntactic complexity of SQL, most complex queries\ncannot be disentangled into sub-queries that can be readily recomposed. To\naddress these challenges, we propose a new modular Query Plan Language (QPL)\nthat systematically decomposes SQL queries into simple and regular sub-queries.\nWe develop a translator from SQL to QPL by leveraging analysis of SQL server\nquery optimization plans, and we augment the Spider dataset with QPL programs.\nExperimental results demonstrate that the modular nature of QPL benefits\nexisting semantic-parsing architectures, and training text-to-QPL parsers is\nmore effective than text-to-SQL parsing for semantically equivalent queries.\nThe QPL approach offers two additional advantages: (1) QPL programs can be\nparaphrased as simple questions, which allows us to create a dataset of\n(complex question, decomposed questions). Training on this dataset, we obtain a\nQuestion Decomposer for data retrieval that is sensitive to database schemas.\n(2) QPL is more accessible to non-experts for complex queries, leading to more\ninterpretable output from the semantic parser.", "published": "2023-10-20 15:13:34", "link": "http://arxiv.org/abs/2310.13575v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Simultaneous Machine Translation with Tailored Reference", "abstract": "Simultaneous machine translation (SiMT) generates translation while reading\nthe whole source sentence. However, existing SiMT models are typically trained\nusing the same reference disregarding the varying amounts of available source\ninformation at different latency. Training the model with ground-truth at low\nlatency may introduce forced anticipations, whereas utilizing reference\nconsistent with the source word order at high latency results in performance\ndegradation. Consequently, it is crucial to train the SiMT model with\nappropriate reference that avoids forced anticipations during training while\nmaintaining high quality. In this paper, we propose a novel method that\nprovides tailored reference for the SiMT models trained at different latency by\nrephrasing the ground-truth. Specifically, we introduce the tailor, induced by\nreinforcement learning, to modify ground-truth to the tailored reference. The\nSiMT model is trained with the tailored reference and jointly optimized with\nthe tailor to enhance performance. Importantly, our method is applicable to a\nwide range of current SiMT approaches. Experiments on three translation tasks\ndemonstrate that our method achieves state-of-the-art performance in both fixed\nand adaptive policies.", "published": "2023-10-20 15:32:26", "link": "http://arxiv.org/abs/2310.13588v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Three Questions Concerning the Use of Large Language Models to\n  Facilitate Mathematics Learning", "abstract": "Due to the remarkable language understanding and generation abilities of\nlarge language models (LLMs), their use in educational applications has been\nexplored. However, little work has been done on investigating the pedagogical\nability of LLMs in helping students to learn mathematics. In this position\npaper, we discuss the challenges associated with employing LLMs to enhance\nstudents' mathematical problem-solving skills by providing adaptive feedback.\nApart from generating the wrong reasoning processes, LLMs can misinterpret the\nmeaning of the question, and also exhibit difficulty in understanding the given\nquestions' rationales when attempting to correct students' answers. Three\nresearch questions are formulated.", "published": "2023-10-20 16:05:35", "link": "http://arxiv.org/abs/2310.13615v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bridging Information-Theoretic and Geometric Compression in Language\n  Models", "abstract": "For a language model (LM) to faithfully model human language, it must\ncompress vast, potentially infinite information into relatively few dimensions.\nWe propose analyzing compression in (pre-trained) LMs from two points of view:\ngeometric and information-theoretic. We demonstrate that the two views are\nhighly correlated, such that the intrinsic geometric dimension of linguistic\ndata predicts their coding length under the LM. We then show that, in turn,\nhigh compression of a linguistic dataset predicts rapid adaptation to that\ndataset, confirming that being able to compress linguistic information is an\nimportant part of successful LM performance. As a practical byproduct of our\nanalysis, we evaluate a battery of intrinsic dimension estimators for the first\ntime on linguistic data, showing that only some encapsulate the relationship\nbetween information-theoretic compression, geometric compression, and\nease-of-adaptation.", "published": "2023-10-20 16:12:13", "link": "http://arxiv.org/abs/2310.13620v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BotChat: Evaluating LLMs' Capabilities of Having Multi-Turn Dialogues", "abstract": "Interacting with human via high-quality multi-turn dialogues is a key feature\nof large language models (LLMs). However, human-based evaluation of such\ncapability involves intensive manual labor. This report provides a preliminary\nevaluation of existing large language models for human-style multi-turn\nchatting, through an LLM-based approach. We start from real-world human\ndialogues and keep the very first utterances as the ChatSEED. Then we prompt\nLLMs to generate a full multi-turn dialogue (tens of utterances) based on the\nChatSEED, utterance by utterance. Finally, we adopt state-of-the-art LLMs\n(GPT-4, \\etc) as the judge to evaluate the generated dialogues. With different\nevaluation protocols, we come to substantially identical conclusions. We find\nthat GPT-4 can generate human-style multi-turn dialogues with impressive\nquality, significantly outperforms its counterparts. It's difficult for a\ndiscriminator to distinguish between GPT-4 generated dialogues and human\ndialogues. In contrast, other LLMs struggle to generate multi-turn dialogues of\nsatisfactory quality due to poor instruction-following capability, tendency to\ngenerate lengthy utterances, or limited general capability. All data and codes\nwill be provided in https://github.com/open-compass/BotChat/ and we hope they\ncan serve as a valuable resource for evaluating multi-turn chatting\ncapabilities of LLMs.", "published": "2023-10-20 16:53:51", "link": "http://arxiv.org/abs/2310.13650v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Benchmarking and Improving Text-to-SQL Generation under Ambiguity", "abstract": "Research in Text-to-SQL conversion has been largely benchmarked against\ndatasets where each text query corresponds to one correct SQL. However, natural\nlanguage queries over real-life databases frequently involve significant\nambiguity about the intended SQL due to overlapping schema names and multiple\nconfusing relationship paths. To bridge this gap, we develop a novel benchmark\ncalled AmbiQT with over 3000 examples where each text is interpretable as two\nplausible SQLs due to lexical and/or structural ambiguity.\n  When faced with ambiguity, an ideal top-$k$ decoder should generate all valid\ninterpretations for possible disambiguation by the user. We evaluate several\nText-to-SQL systems and decoding algorithms, including those employing\nstate-of-the-art LLMs, and find them to be far from this ideal. The primary\nreason is that the prevalent beam search algorithm and its variants, treat SQL\nqueries as a string and produce unhelpful token-level diversity in the top-$k$.\n  We propose LogicalBeam, a new decoding algorithm that navigates the SQL logic\nspace using a blend of plan-based template generation and constrained\ninfilling. Counterfactually generated plans diversify templates while\nin-filling with a beam-search that branches solely on schema names provides\nvalue diversity. LogicalBeam is up to $2.5$ times more effective than\nstate-of-the-art models at generating all candidate SQLs in the top-$k$ ranked\noutputs. It also enhances the top-$5$ Exact and Execution Match Accuracies on\nSPIDER and Kaggle DBQA.", "published": "2023-10-20 17:00:53", "link": "http://arxiv.org/abs/2310.13659v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Arabic Dialect Identification under Scrutiny: Limitations of\n  Single-label Classification", "abstract": "Automatic Arabic Dialect Identification (ADI) of text has gained great\npopularity since it was introduced in the early 2010s. Multiple datasets were\ndeveloped, and yearly shared tasks have been running since 2018. However, ADI\nsystems are reported to fail in distinguishing between the micro-dialects of\nArabic. We argue that the currently adopted framing of the ADI task as a\nsingle-label classification problem is one of the main reasons for that. We\nhighlight the limitation of the incompleteness of the Dialect labels and\ndemonstrate how it impacts the evaluation of ADI systems. A manual error\nanalysis for the predictions of an ADI, performed by 7 native speakers of\ndifferent Arabic dialects, revealed that $\\approx$ 66% of the validated errors\nare not true errors. Consequently, we propose framing ADI as a multi-label\nclassification task and give recommendations for designing new ADI datasets.", "published": "2023-10-20 17:04:22", "link": "http://arxiv.org/abs/2310.13661v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Explainable Depression Symptom Detection in Social Media", "abstract": "Users of social platforms often perceive these sites as supportive spaces to\npost about their mental health issues. Those conversations contain important\ntraces about individuals' health risks. Recently, researchers have exploited\nthis online information to construct mental health detection models, which aim\nto identify users at risk on platforms like Twitter, Reddit or Facebook. Most\nof these models are centred on achieving good classification results, ignoring\nthe explainability and interpretability of the decisions. Recent research has\npointed out the importance of using clinical markers, such as the use of\nsymptoms, to improve trust in the computational models by health professionals.\nIn this paper, we propose using transformer-based architectures to detect and\nexplain the appearance of depressive symptom markers in the users' writings. We\npresent two approaches: i) train a model to classify, and another one to\nexplain the classifier's decision separately and ii) unify the two tasks\nsimultaneously using a single model. Additionally, for this latter manner, we\nalso investigated the performance of recent conversational LLMs when using\nin-context learning. Our natural language explanations enable clinicians to\ninterpret the models' decisions based on validated symptoms, enhancing trust in\nthe automated process. We evaluate our approach using recent symptom-based\ndatasets, employing both offline and expert-in-the-loop metrics to assess the\nquality of the explanations generated by our models. The experimental results\nshow that it is possible to achieve good classification results while\ngenerating interpretable symptom-based explanations.", "published": "2023-10-20 17:05:27", "link": "http://arxiv.org/abs/2310.13664v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "StereoMap: Quantifying the Awareness of Human-like Stereotypes in Large\n  Language Models", "abstract": "Large Language Models (LLMs) have been observed to encode and perpetuate\nharmful associations present in the training data. We propose a theoretically\ngrounded framework called StereoMap to gain insights into their perceptions of\nhow demographic groups have been viewed by society. The framework is grounded\nin the Stereotype Content Model (SCM); a well-established theory from\npsychology. According to SCM, stereotypes are not all alike. Instead, the\ndimensions of Warmth and Competence serve as the factors that delineate the\nnature of stereotypes. Based on the SCM theory, StereoMap maps LLMs'\nperceptions of social groups (defined by socio-demographic features) using the\ndimensions of Warmth and Competence. Furthermore, the framework enables the\ninvestigation of keywords and verbalizations of reasoning of LLMs' judgments to\nuncover underlying factors influencing their perceptions. Our results show that\nLLMs exhibit a diverse range of perceptions towards these groups, characterized\nby mixed evaluations along the dimensions of Warmth and Competence.\nFurthermore, analyzing the reasonings of LLMs, our findings indicate that LLMs\ndemonstrate an awareness of social disparities, often stating statistical data\nand research findings to support their reasoning. This study contributes to the\nunderstanding of how LLMs perceive and represent social groups, shedding light\non their potential biases and the perpetuation of harmful associations.", "published": "2023-10-20 17:22:30", "link": "http://arxiv.org/abs/2310.13673v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Synthetic Data for Back Translation", "abstract": "Back translation (BT) is one of the most significant technologies in NMT\nresearch fields. Existing attempts on BT share a common characteristic: they\nemploy either beam search or random sampling to generate synthetic data with a\nbackward model but seldom work studies the role of synthetic data in the\nperformance of BT. This motivates us to ask a fundamental question: {\\em what\nkind of synthetic data contributes to BT performance?} Through both theoretical\nand empirical studies, we identify two key factors on synthetic data\ncontrolling the back-translation NMT performance, which are quality and\nimportance. Furthermore, based on our findings, we propose a simple yet\neffective method to generate synthetic data to better trade off both factors so\nas to yield a better performance for BT. We run extensive experiments on WMT14\nDE-EN, EN-DE, and RU-EN benchmark tasks. By employing our proposed method to\ngenerate synthetic data, our BT model significantly outperforms the standard BT\nbaselines (i.e., beam and sampling based methods for data generation), which\nproves the effectiveness of our proposed methods.", "published": "2023-10-20 17:24:12", "link": "http://arxiv.org/abs/2310.13675v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Information Value: Measuring Utterance Predictability as Distance from\n  Plausible Alternatives", "abstract": "We present information value, a measure which quantifies the predictability\nof an utterance relative to a set of plausible alternatives. We introduce a\nmethod to obtain interpretable estimates of information value using neural text\ngenerators, and exploit their psychometric predictive power to investigate the\ndimensions of predictability that drive human comprehension behaviour.\nInformation value is a stronger predictor of utterance acceptability in written\nand spoken dialogue than aggregates of token-level surprisal and it is\ncomplementary to surprisal for predicting eye-tracked reading times.", "published": "2023-10-20 17:25:36", "link": "http://arxiv.org/abs/2310.13676v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Linguistic Probes for Morphological Generalization", "abstract": "Modern work on the cross-linguistic computational modeling of morphological\ninflection has typically employed language-independent data splitting\nalgorithms. In this paper, we supplement that approach with language-specific\nprobes designed to test aspects of morphological generalization. Testing these\nprobes on three morphologically distinct languages, English, Spanish, and\nSwahili, we find evidence that three leading morphological inflection systems\nemploy distinct generalization strategies over conjugational classes and\nfeature sets on both orthographic and phonologically transcribed inputs.", "published": "2023-10-20 17:45:30", "link": "http://arxiv.org/abs/2310.13686v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ALDi: Quantifying the Arabic Level of Dialectness of Text", "abstract": "Transcribed speech and user-generated text in Arabic typically contain a\nmixture of Modern Standard Arabic (MSA), the standardized language taught in\nschools, and Dialectal Arabic (DA), used in daily communications. To handle\nthis variation, previous work in Arabic NLP has focused on Dialect\nIdentification (DI) on the sentence or the token level. However, DI treats the\ntask as binary, whereas we argue that Arabic speakers perceive a spectrum of\ndialectness, which we operationalize at the sentence level as the Arabic Level\nof Dialectness (ALDi), a continuous linguistic variable. We introduce the\nAOC-ALDi dataset (derived from the AOC dataset), containing 127,835 sentences\n(17% from news articles and 83% from user comments on those articles) which are\nmanually labeled with their level of dialectness. We provide a detailed\nanalysis of AOC-ALDi and show that a model trained on it can effectively\nidentify levels of dialectness on a range of other corpora (including dialects\nand genres not included in AOC-ALDi), providing a more nuanced picture than\ntraditional DI systems. Through case studies, we illustrate how ALDi can reveal\nArabic speakers' stylistic choices in different situations, a useful property\nfor sociolinguistic analyses.", "published": "2023-10-20 18:07:39", "link": "http://arxiv.org/abs/2310.13747v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Abstractiveness of Summarization Models through Calibrated\n  Distillation", "abstract": "Sequence-level knowledge distillation reduces the size of Seq2Seq models for\nmore efficient abstractive summarization. However, it often leads to a loss of\nabstractiveness in summarization. In this paper, we propose a novel approach\nnamed DisCal to enhance the level of abstractiveness (measured by n-gram\noverlap) without sacrificing the informativeness (measured by ROUGE) of\ngenerated summaries. DisCal exposes diverse pseudo summaries with two\nsupervision to the student model. Firstly, the best pseudo summary is\nidentified in terms of abstractiveness and informativeness and used for\nsequence-level distillation. Secondly, their ranks are used to ensure the\nstudent model to assign higher prediction scores to summaries with higher\nranks. Our experiments show that DisCal outperforms prior methods in\nabstractive summarization distillation, producing highly abstractive and\ninformative summaries.", "published": "2023-10-20 18:43:49", "link": "http://arxiv.org/abs/2310.13760v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Seq2seq is All You Need for Coreference Resolution", "abstract": "Existing works on coreference resolution suggest that task-specific models\nare necessary to achieve state-of-the-art performance. In this work, we present\ncompelling evidence that such models are not necessary. We finetune a\npretrained seq2seq transformer to map an input document to a tagged sequence\nencoding the coreference annotation. Despite the extreme simplicity, our model\noutperforms or closely matches the best coreference systems in the literature\non an array of datasets. We also propose an especially simple seq2seq approach\nthat generates only tagged spans rather than the spans interleaved with the\noriginal text. Our analysis shows that the model size, the amount of\nsupervision, and the choice of sequence representations are key factors in\nperformance.", "published": "2023-10-20 19:17:22", "link": "http://arxiv.org/abs/2310.13774v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Much Consistency Is Your Accuracy Worth?", "abstract": "Contrast set consistency is a robustness measurement that evaluates the rate\nat which a model correctly responds to all instances in a bundle of minimally\ndifferent examples relying on the same knowledge. To draw additional insights,\nwe propose to complement consistency with relative consistency -- the\nprobability that an equally accurate model would surpass the consistency of the\nproposed model, given a distribution over possible consistencies. Models with\n100% relative consistency have reached a consistency peak for their accuracy.\nWe reflect on prior work that reports consistency in contrast sets and observe\nthat relative consistency can alter the assessment of a model's consistency\ncompared to another. We anticipate that our proposed measurement and insights\nwill influence future studies aiming to promote consistent behavior in models.", "published": "2023-10-20 19:28:06", "link": "http://arxiv.org/abs/2310.13781v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluation Metrics in the Era of GPT-4: Reliably Evaluating Large\n  Language Models on Sequence to Sequence Tasks", "abstract": "Large Language Models (LLMs) evaluation is a patchy and inconsistent\nlandscape, and it is becoming clear that the quality of automatic evaluation\nmetrics is not keeping up with the pace of development of generative models. We\naim to improve the understanding of current models' performance by providing a\npreliminary and hybrid evaluation on a range of open and closed-source\ngenerative LLMs on three NLP benchmarks: text summarisation, text\nsimplification and grammatical error correction (GEC), using both automatic and\nhuman evaluation. We also explore the potential of the recently released GPT-4\nto act as an evaluator. We find that ChatGPT consistently outperforms many\nother popular models according to human reviewers on the majority of metrics,\nwhile scoring much more poorly when using classic automatic evaluation metrics.\nWe also find that human reviewers rate the gold reference as much worse than\nthe best models' outputs, indicating the poor quality of many popular\nbenchmarks. Finally, we find that GPT-4 is capable of ranking models' outputs\nin a way which aligns reasonably closely to human judgement despite\ntask-specific variations, with a lower alignment in the GEC task.", "published": "2023-10-20 20:17:09", "link": "http://arxiv.org/abs/2310.13800v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Plausibility Processing in Transformer Language Models: Focusing on the\n  Role of Attention Heads in GPT", "abstract": "The goal of this paper is to explore how Transformer language models process\nsemantic knowledge, especially regarding the plausibility of noun-verb\nrelations. First, I demonstrate GPT2 exhibits a higher degree of similarity\nwith humans in plausibility processing compared to other Transformer language\nmodels. Next, I delve into how knowledge of plausibility is contained within\nattention heads of GPT2 and how these heads causally contribute to GPT2's\nplausibility processing ability. Through several experiments, it was found\nthat: i) GPT2 has a number of attention heads that detect plausible noun-verb\nrelationships; ii) these heads collectively contribute to the Transformer's\nability to process plausibility, albeit to varying degrees; and iii) attention\nheads' individual performance in detecting plausibility does not necessarily\ncorrelate with how much they contribute to GPT2's plausibility processing\nability.", "published": "2023-10-20 21:31:19", "link": "http://arxiv.org/abs/2310.13824v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ecologically Valid Explanations for Label Variation in NLI", "abstract": "Human label variation, or annotation disagreement, exists in many natural\nlanguage processing (NLP) tasks, including natural language inference (NLI). To\ngain direct evidence of how NLI label variation arises, we build LiveNLI, an\nEnglish dataset of 1,415 ecologically valid explanations (annotators explain\nthe NLI labels they chose) for 122 MNLI items (at least 10 explanations per\nitem). The LiveNLI explanations confirm that people can systematically vary on\ntheir interpretation and highlight within-label variation: annotators sometimes\nchoose the same label for different reasons. This suggests that explanations\nare crucial for navigating label interpretations in general. We few-shot prompt\nlarge language models to generate explanations but the results are\ninconsistent: they sometimes produces valid and informative explanations, but\nit also generates implausible ones that do not support the label, highlighting\ndirections for improvement.", "published": "2023-10-20 22:52:19", "link": "http://arxiv.org/abs/2310.13850v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Implications of Annotation Artifacts in Edge Probing Test Datasets", "abstract": "Edge probing tests are classification tasks that test for grammatical\nknowledge encoded in token representations coming from contextual encoders such\nas large language models (LLMs). Many LLM encoders have shown high performance\nin EP tests, leading to conjectures about their ability to encode linguistic\nknowledge. However, a large body of research claims that the tests necessarily\ndo not measure the LLM's capacity to encode knowledge, but rather reflect the\nclassifiers' ability to learn the problem. Much of this criticism stems from\nthe fact that often the classifiers have very similar accuracy when an LLM vs a\nrandom encoder is used. Consequently, several modifications to the tests have\nbeen suggested, including information theoretic probes. We show that commonly\nused edge probing test datasets have various biases including memorization.\nWhen these biases are removed, the LLM encoders do show a significant\ndifference from the random ones, even with the simple non-information theoretic\nprobes.", "published": "2023-10-20 23:19:35", "link": "http://arxiv.org/abs/2310.13856v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Primacy Effect of ChatGPT", "abstract": "Instruction-tuned large language models (LLMs), such as ChatGPT, have led to\npromising zero-shot performance in discriminative natural language\nunderstanding (NLU) tasks. This involves querying the LLM using a prompt\ncontaining the question, and the candidate labels to choose from. The\nquestion-answering capabilities of ChatGPT arise from its pre-training on large\namounts of human-written text, as well as its subsequent fine-tuning on human\npreferences, which motivates us to ask: Does ChatGPT also inherits humans'\ncognitive biases? In this paper, we study the primacy effect of ChatGPT: the\ntendency of selecting the labels at earlier positions as the answer. We have\ntwo main findings: i) ChatGPT's decision is sensitive to the order of labels in\nthe prompt; ii) ChatGPT has a clearly higher chance to select the labels at\nearlier positions as the answer. We hope that our experiments and analyses\nprovide additional insights into building more reliable ChatGPT-based\nsolutions. We release the source code at\nhttps://github.com/wangywUST/PrimacyEffectGPT.", "published": "2023-10-20 00:37:28", "link": "http://arxiv.org/abs/2310.13206v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MultiCoNER v2: a Large Multilingual dataset for Fine-grained and Noisy\n  Named Entity Recognition", "abstract": "We present MULTICONER V2, a dataset for fine-grained Named Entity Recognition\ncovering 33 entity classes across 12 languages, in both monolingual and\nmultilingual settings. This dataset aims to tackle the following practical\nchallenges in NER: (i) effective handling of fine-grained classes that include\ncomplex entities like movie titles, and (ii) performance degradation due to\nnoise generated from typing mistakes or OCR errors. The dataset is compiled\nfrom open resources like Wikipedia and Wikidata, and is publicly available.\nEvaluation based on the XLM-RoBERTa baseline highlights the unique challenges\nposed by MULTICONER V2: (i) the fine-grained taxonomy is challenging, where the\nscores are low with macro-F1=0.63 (across all languages), and (ii) the\ncorruption strategy significantly impairs performance, with entity corruption\nresulting in 9% lower performance relative to non-entity corruptions across all\nlanguages. This highlights the greater impact of entity noise in contrast to\ncontext noise.", "published": "2023-10-20 01:14:46", "link": "http://arxiv.org/abs/2310.13213v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Open-source Large Language Models are Strong Zero-shot Query Likelihood\n  Models for Document Ranking", "abstract": "In the field of information retrieval, Query Likelihood Models (QLMs) rank\ndocuments based on the probability of generating the query given the content of\na document. Recently, advanced large language models (LLMs) have emerged as\neffective QLMs, showcasing promising ranking capabilities. This paper focuses\non investigating the genuine zero-shot ranking effectiveness of recent LLMs,\nwhich are solely pre-trained on unstructured text data without supervised\ninstruction fine-tuning. Our findings reveal the robust zero-shot ranking\nability of such LLMs, highlighting that additional instruction fine-tuning may\nhinder effectiveness unless a question generation task is present in the\nfine-tuning dataset. Furthermore, we introduce a novel state-of-the-art ranking\nsystem that integrates LLM-based QLMs with a hybrid zero-shot retriever,\ndemonstrating exceptional effectiveness in both zero-shot and few-shot\nscenarios. We make our codebase publicly available at\nhttps://github.com/ielab/llm-qlm.", "published": "2023-10-20 02:54:42", "link": "http://arxiv.org/abs/2310.13243v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Anomaly Detection of Command Shell Sessions based on DistilBERT:\n  Unsupervised and Supervised Approaches", "abstract": "Anomaly detection in command shell sessions is a critical aspect of computer\nsecurity. Recent advances in deep learning and natural language processing,\nparticularly transformer-based models, have shown great promise for addressing\ncomplex security challenges. In this paper, we implement a comprehensive\napproach to detect anomalies in Unix shell sessions using a pretrained\nDistilBERT model, leveraging both unsupervised and supervised learning\ntechniques to identify anomalous activity while minimizing data labeling. The\nunsupervised method captures the underlying structure and syntax of Unix shell\ncommands, enabling the detection of session deviations from normal behavior.\nExperiments on a large-scale enterprise dataset collected from production\nsystems demonstrate the effectiveness of our approach in detecting anomalous\nbehavior in Unix shell sessions. This work highlights the potential of\nleveraging recent advances in transformers to address important computer\nsecurity challenges.", "published": "2023-10-20 03:04:32", "link": "http://arxiv.org/abs/2310.13247v1", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Visual Grounding Helps Learn Word Meanings in Low-Data Regimes", "abstract": "Modern neural language models (LMs) are powerful tools for modeling human\nsentence production and comprehension, and their internal representations are\nremarkably well-aligned with representations of language in the human brain.\nBut to achieve these results, LMs must be trained in distinctly un-human-like\nways - requiring orders of magnitude more language data than children receive\nduring development, and without perceptual or social context. Do models trained\nmore naturalistically -- with grounded supervision -- exhibit more humanlike\nlanguage learning? We investigate this question in the context of word\nlearning, a key sub-task in language acquisition. We train a diverse set of LM\narchitectures, with and without auxiliary visual supervision, on datasets of\nvarying scales. We then evaluate these models' learning of syntactic\ncategories, lexical relations, semantic features, word similarity, and\nalignment with human neural representations. We find that visual supervision\ncan indeed improve the efficiency of word learning. However, these improvements\nare limited: they are present almost exclusively in the low-data regime, and\nsometimes canceled out by the inclusion of rich distributional signals from\ntext. The information conveyed by text and images is not redundant -- models\nmainly driven by visual information yield qualitatively different from those\nmainly driven by word co-occurrences. However, our results suggest that current\nmultimodal modeling approaches fail to effectively leverage visual information\nto build human-like word representations from human-scale data.", "published": "2023-10-20 03:33:36", "link": "http://arxiv.org/abs/2310.13257v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Test-Time Self-Adaptive Small Language Models for Question Answering", "abstract": "Recent instruction-finetuned large language models (LMs) have achieved\nnotable performances in various tasks, such as question-answering (QA).\nHowever, despite their ability to memorize a vast amount of general knowledge\nacross diverse tasks, they might be suboptimal on specific tasks due to their\nlimited capacity to transfer and adapt knowledge to target tasks. Moreover,\nfurther finetuning LMs with labeled datasets is often infeasible due to their\nabsence, but it is also questionable if we can transfer smaller LMs having\nlimited knowledge only with unlabeled test data. In this work, we show and\ninvestigate the capabilities of smaller self-adaptive LMs, only with unlabeled\ntest data. In particular, we first stochastically generate multiple answers,\nand then ensemble them while filtering out low-quality samples to mitigate\nnoise from inaccurate labels. Our proposed self-adaption strategy demonstrates\nsignificant performance improvements on benchmark QA datasets with higher\nrobustness across diverse prompts, enabling LMs to stay stable. Code is\navailable at: https://github.com/starsuzi/T-SAS.", "published": "2023-10-20 06:49:32", "link": "http://arxiv.org/abs/2310.13307v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Coarse-to-Fine Dual Encoders are Better Frame Identification Learners", "abstract": "Frame identification aims to find semantic frames associated with target\nwords in a sentence. Recent researches measure the similarity or matching score\nbetween targets and candidate frames by modeling frame definitions. However,\nthey either lack sufficient representation learning of the definitions or face\nchallenges in efficiently selecting the most suitable frame from over 1000\ncandidate frames. Moreover, commonly used lexicon filtering ($lf$) to obtain\ncandidate frames for the target may ignore out-of-vocabulary targets and cause\ninadequate frame modeling. In this paper, we propose CoFFTEA, a\n$\\underline{Co}$arse-to-$\\underline{F}$ine $\\underline{F}$rame and\n$\\underline{T}$arget $\\underline{E}$ncoders $\\underline{A}$rchitecture. With\ncontrastive learning and dual encoders, CoFFTEA efficiently and effectively\nmodels the alignment between frames and targets. By employing a coarse-to-fine\ncurriculum learning procedure, CoFFTEA gradually learns to differentiate frames\nwith varying degrees of similarity. Experimental results demonstrate that\nCoFFTEA outperforms previous models by 0.93 overall scores and 1.53 R@1 without\n$lf$. Further analysis suggests that CoFFTEA can better model the relationships\nbetween frame and frame, as well as target and target. The code for our\napproach is available at https://github.com/pkunlp-icler/COFFTEA.", "published": "2023-10-20 07:11:23", "link": "http://arxiv.org/abs/2310.13316v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Democratizing Reasoning Ability: Tailored Learning from Large Language\n  Model", "abstract": "Large language models (LLMs) exhibit impressive emergent abilities in natural\nlanguage processing, but their democratization is hindered due to huge\ncomputation requirements and closed-source nature. Recent research on advancing\nopen-source smaller LMs by distilling knowledge from black-box LLMs has\nobtained promising results in the instruction-following ability. However, the\nreasoning ability which is more challenging to foster, is relatively rarely\nexplored. In this paper, we propose a tailored learning approach to distill\nsuch reasoning ability to smaller LMs to facilitate the democratization of the\nexclusive reasoning ability. In contrast to merely employing LLM as a data\nannotator, we exploit the potential of LLM as a reasoning teacher by building\nan interactive multi-round learning paradigm. This paradigm enables the student\nto expose its deficiencies to the black-box teacher who then can provide\ncustomized training data in return. Further, to exploit the reasoning potential\nof the smaller LM, we propose self-reflection learning to motivate the student\nto learn from self-made mistakes. The learning from self-reflection and LLM are\nall tailored to the student's learning status, thanks to the seamless\nintegration with the multi-round learning paradigm. Comprehensive experiments\nand analysis on mathematical and commonsense reasoning tasks demonstrate the\neffectiveness of our method. The code will be available at\nhttps://github.com/Raibows/Learn-to-Reason.", "published": "2023-10-20 07:50:10", "link": "http://arxiv.org/abs/2310.13332v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Challenges and Contributing Factors in the Utilization of Large Language\n  Models (LLMs)", "abstract": "With the development of large language models (LLMs) like the GPT series,\ntheir widespread use across various application scenarios presents a myriad of\nchallenges. This review initially explores the issue of domain specificity,\nwhere LLMs may struggle to provide precise answers to specialized questions\nwithin niche fields. The problem of knowledge forgetting arises as these LLMs\nmight find it hard to balance old and new information. The knowledge repetition\nphenomenon reveals that sometimes LLMs might deliver overly mechanized\nresponses, lacking depth and originality. Furthermore, knowledge illusion\ndescribes situations where LLMs might provide answers that seem insightful but\nare actually superficial, while knowledge toxicity focuses on harmful or biased\ninformation outputs. These challenges underscore problems in the training data\nand algorithmic design of LLMs. To address these issues, it's suggested to\ndiversify training data, fine-tune models, enhance transparency and\ninterpretability, and incorporate ethics and fairness training. Future\ntechnological trends might lean towards iterative methodologies, multimodal\nlearning, model personalization and customization, and real-time learning and\nfeedback mechanisms. In conclusion, future LLMs should prioritize fairness,\ntransparency, and ethics, ensuring they uphold high moral and ethical standards\nwhen serving humanity.", "published": "2023-10-20 08:13:36", "link": "http://arxiv.org/abs/2310.13343v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Tuna: Instruction Tuning using Feedback from Large Language Models", "abstract": "Instruction tuning of open-source large language models (LLMs) like LLaMA,\nusing direct outputs from more powerful LLMs such as Instruct-GPT and GPT-4,\nhas proven to be a cost-effective way to align model behaviors with human\npreferences. However, the instruction-tuned model has only seen one response\nper instruction, lacking the knowledge of potentially better responses. In this\npaper, we propose finetuning an instruction-tuned LLM using our novel\n\\textit{probabilistic ranking} and \\textit{contextual ranking} approaches to\nincrease the likelihood of generating better responses. Probabilistic ranking\nenables the instruction-tuned model to inherit the relative rankings of\nhigh-quality and low-quality responses from the teacher LLM. On the other hand,\nlearning with contextual ranking allows the model to refine its own response\ndistribution using the contextual understanding ability of stronger LLMs.\nFurthermore, we apply probabilistic ranking and contextual ranking sequentially\nto the instruction-tuned LLM. The resulting model, which we call \\textbf{Tuna},\nconsistently improves the performance on Super Natural Instructions (119 test\ntasks), LMentry (25 test tasks), Vicuna QA, and can even obtain better results\nthan several strong reinforcement learning baselines. Our code and data are\navailable at \\url{ https://github.com/microsoft/LMOps}.", "published": "2023-10-20 09:55:06", "link": "http://arxiv.org/abs/2310.13385v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Self-Consistency of Large Language Models under Ambiguity", "abstract": "Large language models (LLMs) that do not give consistent answers across\ncontexts are problematic when used for tasks with expectations of consistency,\ne.g., question-answering, explanations, etc. Our work presents an evaluation\nbenchmark for self-consistency in cases of under-specification where two or\nmore answers can be correct. We conduct a series of behavioral experiments on\nthe OpenAI model suite using an ambiguous integer sequence completion task. We\nfind that average consistency ranges from 67\\% to 82\\%, far higher than would\nbe predicted if a model's consistency was random, and increases as model\ncapability improves. Furthermore, we show that models tend to maintain\nself-consistency across a series of robustness checks, including prompting\nspeaker changes and sequence length changes. These results suggest that\nself-consistency arises as an emergent capability without specifically training\nfor it. Despite this, we find that models are uncalibrated when judging their\nown consistency, with models displaying both over- and under-confidence. We\nalso propose a nonparametric test for determining from token output\ndistribution whether a model assigns non-trivial probability to alternative\nanswers. Using this test, we find that despite increases in self-consistency,\nmodels usually place significant weight on alternative, inconsistent answers.\nThis distribution of probability mass provides evidence that even highly\nself-consistent models internally compute multiple possible responses.", "published": "2023-10-20 11:57:56", "link": "http://arxiv.org/abs/2310.13439v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Ask Language Model to Clean Your Noisy Translation Data", "abstract": "Transformer models have demonstrated remarkable performance in neural machine\ntranslation (NMT). However, their vulnerability to noisy input poses a\nsignificant challenge in practical implementation, where generating clean\noutput from noisy input is crucial. The MTNT dataset is widely used as a\nbenchmark for evaluating the robustness of NMT models against noisy input.\nNevertheless, its utility is limited due to the presence of noise in both the\nsource and target sentences. To address this limitation, we focus on cleaning\nthe noise from the target sentences in MTNT, making it more suitable as a\nbenchmark for noise evaluation. Leveraging the capabilities of large language\nmodels (LLMs), we observe their impressive abilities in noise removal. For\nexample, they can remove emojis while considering their semantic meaning.\nAdditionally, we show that LLM can effectively rephrase slang, jargon, and\nprofanities. The resulting datasets, called C-MTNT, exhibit significantly less\nnoise in the target sentences while preserving the semantic integrity of the\noriginal sentences. Our human and GPT-4 evaluations also lead to a consistent\nconclusion that LLM performs well on this task. Lastly, experiments on C-MTNT\nshowcased its effectiveness in evaluating the robustness of NMT models,\nhighlighting the potential of advanced language models for data cleaning and\nemphasizing C-MTNT as a valuable resource.", "published": "2023-10-20 13:05:32", "link": "http://arxiv.org/abs/2310.13469v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mind the instructions: a holistic evaluation of consistency and\n  interactions in prompt-based learning", "abstract": "Finding the best way of adapting pre-trained language models to a task is a\nbig challenge in current NLP. Just like the previous generation of task-tuned\nmodels (TT), models that are adapted to tasks via in-context-learning (ICL) are\nrobust in some setups but not in others. Here, we present a detailed analysis\nof which design choices cause instabilities and inconsistencies in LLM\npredictions. First, we show how spurious correlations between input\ndistributions and labels -- a known issue in TT models -- form only a minor\nproblem for prompted models. Then, we engage in a systematic, holistic\nevaluation of different factors that have been found to influence predictions\nin a prompting setup. We test all possible combinations of a range of factors\non both vanilla and instruction-tuned (IT) LLMs of different scale and\nstatistically analyse the results to show which factors are the most\ninfluential, interactive or stable. Our results show which factors can be used\nwithout precautions and which should be avoided or handled with care in most\nsettings.", "published": "2023-10-20 13:25:24", "link": "http://arxiv.org/abs/2310.13486v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Analogical Proportions and Creativity: A Preliminary Study", "abstract": "Analogical proportions are statements of the form \"$a$ is to $b$ as $c$ is to\n$d$\", which expresses that the comparisons of the elements in pair $(a, b)$ and\nin pair $(c, d)$ yield similar results. Analogical proportions are creative in\nthe sense that given 3 distinct items, the representation of a 4th item $d$,\ndistinct from the previous items, which forms an analogical proportion with\nthem can be calculated, provided certain conditions are met. After providing an\nintroduction to analogical proportions and their properties, the paper reports\nthe results of an experiment made with a database of animal descriptions and\ntheir class, where we try to \"create\" new animals from existing ones,\nretrieving rare animals such as platypus. We perform a series of experiments\nusing word embeddings as well as Boolean features in order to propose novel\nanimals based on analogical proportions, showing that word embeddings obtain\nbetter results.", "published": "2023-10-20 13:46:04", "link": "http://arxiv.org/abs/2310.13500v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Explaining Interactions Between Text Spans", "abstract": "Reasoning over spans of tokens from different parts of the input is essential\nfor natural language understanding (NLU) tasks such as fact-checking (FC),\nmachine reading comprehension (MRC) or natural language inference (NLI).\nHowever, existing highlight-based explanations primarily focus on identifying\nindividual important tokens or interactions only between adjacent tokens or\ntuples of tokens. Most notably, there is a lack of annotations capturing the\nhuman decision-making process w.r.t. the necessary interactions for informed\ndecision-making in such tasks. To bridge this gap, we introduce SpanEx, a\nmulti-annotator dataset of human span interaction explanations for two NLU\ntasks: NLI and FC. We then investigate the decision-making processes of\nmultiple fine-tuned large language models in terms of the employed connections\nbetween spans in separate parts of the input and compare them to the human\nreasoning processes. Finally, we present a novel community detection based\nunsupervised method to extract such interaction explanations from a model's\ninner workings.", "published": "2023-10-20 13:52:37", "link": "http://arxiv.org/abs/2310.13506v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Controlled Randomness Improves the Performance of Transformer Models", "abstract": "During the pre-training step of natural language models, the main objective\nis to learn a general representation of the pre-training dataset, usually\nrequiring large amounts of textual data to capture the complexity and diversity\nof natural language. Contrasting this, in most cases, the size of the data\navailable to solve the specific downstream task is often dwarfed by the\naforementioned pre-training dataset, especially in domains where data is\nscarce. We introduce controlled randomness, i.e. noise, into the training\nprocess to improve fine-tuning language models and explore the performance of\ntargeted noise in addition to the parameters of these models. We find that\nadding such noise can improve the performance in our two downstream tasks of\njoint named entity recognition and relation extraction and text summarization.", "published": "2023-10-20 14:12:55", "link": "http://arxiv.org/abs/2310.13526v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Diachronic Perspective on User Trust in AI under Uncertainty", "abstract": "In a human-AI collaboration, users build a mental model of the AI system\nbased on its reliability and how it presents its decision, e.g. its\npresentation of system confidence and an explanation of the output. Modern NLP\nsystems are often uncalibrated, resulting in confidently incorrect predictions\nthat undermine user trust. In order to build trustworthy AI, we must understand\nhow user trust is developed and how it can be regained after potential\ntrust-eroding events. We study the evolution of user trust in response to these\ntrust-eroding events using a betting game. We find that even a few incorrect\ninstances with inaccurate confidence estimates damage user trust and\nperformance, with very slow recovery. We also show that this degradation in\ntrust reduces the success of human-AI collaboration and that different types of\nmiscalibration -- unconfidently correct and confidently incorrect -- have\ndifferent negative effects on user trust. Our findings highlight the importance\nof calibration in user-facing AI applications and shed light on what aspects\nhelp users decide whether to trust the AI system.", "published": "2023-10-20 14:41:46", "link": "http://arxiv.org/abs/2310.13544v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Self-prompted Chain-of-Thought on Large Language Models for Open-domain\n  Multi-hop Reasoning", "abstract": "In open-domain question-answering (ODQA), most existing questions require\nsingle-hop reasoning on commonsense. To further extend this task, we officially\nintroduce open-domain multi-hop reasoning (ODMR) by answering multi-hop\nquestions with explicit reasoning steps in open-domain setting. Recently, large\nlanguage models (LLMs) have found significant utility in facilitating ODQA\nwithout external corpus. Furthermore, chain-of-thought (CoT) prompting boosts\nthe reasoning capability of LLMs to a greater extent with manual or automated\nparadigms. However, existing automated methods lack of quality assurance, while\nmanual approaches suffer from limited scalability and poor diversity, hindering\nthe capabilities of LLMs. In this paper, we propose Self-prompted\nChain-of-Thought (SP-CoT), an automated framework to mass-produce high quality\nCoTs of LLMs, by LLMs and for LLMs. SP-CoT introduces an automated generation\npipeline of high quality ODMR datasets, an adaptive sampler for in-context CoT\nselection and self-prompted inference via in-context learning. Extensive\nexperiments on four multi-hop question-answering benchmarks show that our\nproposed SP-CoT not only significantly surpasses the previous SOTA methods on\nlarge-scale (175B) LLMs, but also nearly doubles the zero-shot performance of\nsmall-scale (13B) LLMs. Further analysis reveals the remarkable capability of\nSP-CoT to elicit direct and concise intermediate reasoning steps by recalling\n$\\sim$50\\% of intermediate answers on MuSiQue-Ans dataset.", "published": "2023-10-20 14:51:10", "link": "http://arxiv.org/abs/2310.13552v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Cache & Distil: Optimising API Calls to Large Language Models", "abstract": "Large-scale deployment of generative AI tools often depends on costly API\ncalls to a Large Language Model (LLM) to fulfil user queries. To curtail the\nfrequency of these calls, one can employ a smaller language model -- a student\n-- which is continuously trained on the responses of the LLM. This student\ngradually gains proficiency in independently handling an increasing number of\nuser requests, a process we term neural caching. The crucial element in neural\ncaching is a policy that decides which requests should be processed by the\nstudent alone and which should be redirected to the LLM, subsequently aiding\nthe student's learning. In this study, we focus on classification tasks, and we\nconsider a range of classic active learning-based selection criteria as the\npolicy. Our experiments suggest that Margin Sampling and Query by Committee\nbring consistent benefits across tasks and budgets.", "published": "2023-10-20 15:01:55", "link": "http://arxiv.org/abs/2310.13561v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Retrieval-Augmented Neural Response Generation Using Logical Reasoning\n  and Relevance Scoring", "abstract": "Constructing responses in task-oriented dialogue systems typically relies on\ninformation sources such the current dialogue state or external databases. This\npaper presents a novel approach to knowledge-grounded response generation that\ncombines retrieval-augmented language models with logical reasoning. The\napproach revolves around a knowledge graph representing the current dialogue\nstate and background information, and proceeds in three steps. The knowledge\ngraph is first enriched with logically derived facts inferred using\nprobabilistic logical programming. A neural model is then employed at each turn\nto score the conversational relevance of each node and edge of this extended\ngraph. Finally, the elements with highest relevance scores are converted to a\nnatural language form, and are integrated into the prompt for the neural\nconversational model employed to generate the system response.\n  We investigate the benefits of the proposed approach on two datasets (KVRET\nand GraphWOZ) along with a human evaluation. Experimental results show that the\ncombination of (probabilistic) logical reasoning with conversational relevance\nscoring does increase both the factuality and fluency of the responses.", "published": "2023-10-20 15:05:18", "link": "http://arxiv.org/abs/2310.13566v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Cross-Lingual Transfer through Subtree-Aware Word Reordering", "abstract": "Despite the impressive growth of the abilities of multilingual language\nmodels, such as XLM-R and mT5, it has been shown that they still face\ndifficulties when tackling typologically-distant languages, particularly in the\nlow-resource setting. One obstacle for effective cross-lingual transfer is\nvariability in word-order patterns. It can be potentially mitigated via source-\nor target-side word reordering, and numerous approaches to reordering have been\nproposed. However, they rely on language-specific rules, work on the level of\nPOS tags, or only target the main clause, leaving subordinate clauses intact.\nTo address these limitations, we present a new powerful reordering method,\ndefined in terms of Universal Dependencies, that is able to learn fine-grained\nword-order patterns conditioned on the syntactic context from a small amount of\nannotated data and can be applied at all levels of the syntactic tree. We\nconduct experiments on a diverse set of tasks and show that our method\nconsistently outperforms strong baselines over different language pairs and\nmodel architectures. This performance advantage holds true in both zero-shot\nand few-shot scenarios.", "published": "2023-10-20 15:25:53", "link": "http://arxiv.org/abs/2310.13583v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MarineGPT: Unlocking Secrets of Ocean to the Public", "abstract": "Large language models (LLMs), such as ChatGPT/GPT-4, have proven to be\npowerful tools in promoting the user experience as an AI assistant. The\ncontinuous works are proposing multi-modal large language models (MLLM),\nempowering LLMs with the ability to sense multiple modality inputs through\nconstructing a joint semantic space (e.g. visual-text space). Though\nsignificant success was achieved in LLMs and MLLMs, exploring LLMs and MLLMs in\ndomain-specific applications that required domain-specific knowledge and\nexpertise has been less conducted, especially for \\textbf{marine domain}.\nDifferent from general-purpose MLLMs, the marine-specific MLLM is required to\nyield much more \\textbf{sensitive}, \\textbf{informative}, and\n\\textbf{scientific} responses. In this work, we demonstrate that the existing\nMLLMs optimized on huge amounts of readily available general-purpose training\ndata show a minimal ability to understand domain-specific intents and then\ngenerate informative and satisfactory responses. To address these issues, we\npropose \\textbf{MarineGPT}, the first vision-language model specially designed\nfor the marine domain, unlocking the secrets of the ocean to the public. We\npresent our \\textbf{Marine-5M} dataset with more than 5 million marine\nimage-text pairs to inject domain-specific marine knowledge into our model and\nachieve better marine vision and language alignment. Our MarineGPT not only\npushes the boundaries of marine understanding to the general public but also\noffers a standard protocol for adapting a general-purpose assistant to\ndownstream domain-specific experts. We pave the way for a wide range of marine\napplications while setting valuable data and pre-trained models for future\nresearch in both academic and industrial communities.", "published": "2023-10-20 15:45:39", "link": "http://arxiv.org/abs/2310.13596v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MULTITuDE: Large-Scale Multilingual Machine-Generated Text Detection\n  Benchmark", "abstract": "There is a lack of research into capabilities of recent LLMs to generate\nconvincing text in languages other than English and into performance of\ndetectors of machine-generated text in multilingual settings. This is also\nreflected in the available benchmarks which lack authentic texts in languages\nother than English and predominantly cover older generators. To fill this gap,\nwe introduce MULTITuDE, a novel benchmarking dataset for multilingual\nmachine-generated text detection comprising of 74,081 authentic and\nmachine-generated texts in 11 languages (ar, ca, cs, de, en, es, nl, pt, ru,\nuk, and zh) generated by 8 multilingual LLMs. Using this benchmark, we compare\nthe performance of zero-shot (statistical and black-box) and fine-tuned\ndetectors. Considering the multilinguality, we evaluate 1) how these detectors\ngeneralize to unseen languages (linguistically similar as well as dissimilar)\nand unseen LLMs and 2) whether the detectors improve their performance when\ntrained on multiple languages.", "published": "2023-10-20 15:57:17", "link": "http://arxiv.org/abs/2310.13606v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Make Your Decision Convincing! A Unified Two-Stage Framework:\n  Self-Attribution and Decision-Making", "abstract": "Explaining black-box model behavior with natural language has achieved\nimpressive results in various NLP tasks. Recent research has explored the\nutilization of subsequences from the input text as a rationale, providing users\nwith evidence to support the model decision. Although existing frameworks excel\nin generating high-quality rationales while achieving high task performance,\nthey neglect to account for the unreliable link between the generated rationale\nand model decision. In simpler terms, a model may make correct decisions while\nattributing wrong rationales, or make poor decisions while attributing correct\nrationales. To mitigate this issue, we propose a unified two-stage framework\nknown as Self-Attribution and Decision-Making (SADM). Through extensive\nexperiments on five reasoning datasets from the ERASER benchmark, we\ndemonstrate that our framework not only establishes a more reliable link\nbetween the generated rationale and model decision but also achieves\ncompetitive results in task performance and the quality of rationale.\nFurthermore, we explore the potential of our framework in semi-supervised\nscenarios.", "published": "2023-10-20 15:59:57", "link": "http://arxiv.org/abs/2310.13610v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Hunayn: Elevating Translation Beyond the Literal", "abstract": "This project introduces an advanced English-to-Arabic translator surpassing\nconventional tools. Leveraging the Helsinki transformer (MarianMT), our\napproach involves fine-tuning on a self-scraped, purely literary Arabic\ndataset. Evaluations against Google Translate show consistent outperformance in\nqualitative assessments. Notably, it excels in cultural sensitivity and context\naccuracy. This research underscores the Helsinki transformer's superiority for\nEnglish-to-Arabic translation using a Fusha dataset.", "published": "2023-10-20 16:03:33", "link": "http://arxiv.org/abs/2310.13613v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Semi-supervised multimodal coreference resolution in image narrations", "abstract": "In this paper, we study multimodal coreference resolution, specifically where\na longer descriptive text, i.e., a narration is paired with an image. This\nposes significant challenges due to fine-grained image-text alignment, inherent\nambiguity present in narrative language, and unavailability of large annotated\ntraining sets. To tackle these challenges, we present a data efficient\nsemi-supervised approach that utilizes image-narration pairs to resolve\ncoreferences and narrative grounding in a multimodal context. Our approach\nincorporates losses for both labeled and unlabeled data within a cross-modal\nframework. Our evaluation shows that the proposed approach outperforms strong\nbaselines both quantitatively and qualitatively, for the tasks of coreference\nresolution and narrative grounding.", "published": "2023-10-20 16:10:14", "link": "http://arxiv.org/abs/2310.13619v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Let's Synthesize Step by Step: Iterative Dataset Synthesis with Large\n  Language Models by Extrapolating Errors from Small Models", "abstract": "*Data Synthesis* is a promising way to train a small model with very little\nlabeled data. One approach for data synthesis is to leverage the rich knowledge\nfrom large language models to synthesize pseudo training examples for small\nmodels, making it possible to achieve both data and compute efficiency at the\nsame time. However, a key challenge in data synthesis is that the synthesized\ndataset often suffers from a large distributional discrepancy from the *real\ntask* data distribution. Thus, in this paper, we propose *Synthesis Step by\nStep* (**S3**), a data synthesis framework that shrinks this distribution gap\nby iteratively extrapolating the errors made by a small model trained on the\nsynthesized dataset on a small real-world validation dataset using a large\nlanguage model. Extensive experiments on multiple NLP tasks show that our\napproach improves the performance of a small model by reducing the gap between\nthe synthetic dataset and the real data, resulting in significant improvement\ncompared to several baselines: 9.48% improvement compared to ZeroGen and 2.73%\ncompared to GoldGen, and at most 15.17% improvement compared to the small model\ntrained on human-annotated data.", "published": "2023-10-20 17:14:25", "link": "http://arxiv.org/abs/2310.13671v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Copyright Violations and Large Language Models", "abstract": "Language models may memorize more than just facts, including entire chunks of\ntexts seen during training. Fair use exemptions to copyright laws typically\nallow for limited use of copyrighted material without permission from the\ncopyright holder, but typically for extraction of information from copyrighted\nmaterials, rather than {\\em verbatim} reproduction. This work explores the\nissue of copyright violations and large language models through the lens of\nverbatim memorization, focusing on possible redistribution of copyrighted text.\nWe present experiments with a range of language models over a collection of\npopular books and coding problems, providing a conservative characterization of\nthe extent to which language models can redistribute these materials. Overall,\nthis research highlights the need for further examination and the potential\nimpact on future developments in natural language processing to ensure\nadherence to copyright regulations. Code is at\n\\url{https://github.com/coastalcph/CopyrightLLMs}.", "published": "2023-10-20 19:14:59", "link": "http://arxiv.org/abs/2310.13771v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Unified View of Evaluation Metrics for Structured Prediction", "abstract": "We present a conceptual framework that unifies a variety of evaluation\nmetrics for different structured prediction tasks (e.g. event and relation\nextraction, syntactic and semantic parsing). Our framework requires\nrepresenting the outputs of these tasks as objects of certain data types, and\nderives metrics through matching of common substructures, possibly followed by\nnormalization. We demonstrate how commonly used metrics for a number of tasks\ncan be succinctly expressed by this framework, and show that new metrics can be\nnaturally derived in a bottom-up way based on an output structure. We release a\nlibrary that enables this derivation to create new metrics. Finally, we\nconsider how specific characteristics of tasks motivate metric design\ndecisions, and suggest possible modifications to existing metrics in line with\nthose motivations.", "published": "2023-10-20 20:02:02", "link": "http://arxiv.org/abs/2310.13793v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Specific versus General Principles for Constitutional AI", "abstract": "Human feedback can prevent overtly harmful utterances in conversational\nmodels, but may not automatically mitigate subtle problematic behaviors such as\na stated desire for self-preservation or power. Constitutional AI offers an\nalternative, replacing human feedback with feedback from AI models conditioned\nonly on a list of written principles. We find this approach effectively\nprevents the expression of such behaviors. The success of simple principles\nmotivates us to ask: can models learn general ethical behaviors from only a\nsingle written principle? To test this, we run experiments using a principle\nroughly stated as \"do what's best for humanity\". We find that the largest\ndialogue models can generalize from this short constitution, resulting in\nharmless assistants with no stated interest in specific motivations like power.\nA general principle may thus partially avoid the need for a long list of\nconstitutions targeting potentially harmful behaviors. However, more detailed\nconstitutions still improve fine-grained control over specific types of harms.\nThis suggests both general and specific principles have value for steering AI\nsafely.", "published": "2023-10-20 20:12:45", "link": "http://arxiv.org/abs/2310.13798v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Foundation Model's Embedded Representations May Detect Distribution\n  Shift", "abstract": "Sampling biases can cause distribution shifts between train and test datasets\nfor supervised learning tasks, obscuring our ability to understand the\ngeneralization capacity of a model. This is especially important considering\nthe wide adoption of pre-trained foundational neural networks -- whose behavior\nremains poorly understood -- for transfer learning (TL) tasks. We present a\ncase study for TL on the Sentiment140 dataset and show that many pre-trained\nfoundation models encode different representations of Sentiment140's manually\ncurated test set $M$ from the automatically labeled training set $P$,\nconfirming that a distribution shift has occurred. We argue training on $P$ and\nmeasuring performance on $M$ is a biased measure of generalization. Experiments\non pre-trained GPT-2 show that the features learnable from $P$ do not improve\n(and in fact hamper) performance on $M$. Linear probes on pre-trained GPT-2's\nrepresentations are robust and may even outperform overall fine-tuning,\nimplying a fundamental importance for discerning distribution shift in\ntrain/test splits for model interpretation.", "published": "2023-10-20 22:20:50", "link": "http://arxiv.org/abs/2310.13836v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Evoke: Evoking Critical Thinking Abilities in LLMs via Reviewer-Author\n  Prompt Editing", "abstract": "Large language models (LLMs) have made impressive progress in natural\nlanguage processing. These models rely on proper human instructions (or\nprompts) to generate suitable responses. However, the potential of LLMs are not\nfully harnessed by commonly-used prompting methods: many human-in-the-loop\nalgorithms employ ad-hoc procedures for prompt selection; while auto prompt\ngeneration approaches are essentially searching all possible prompts randomly\nand inefficiently. We propose Evoke, an automatic prompt refinement framework.\nIn Evoke, there are two instances of a same LLM: one as a reviewer\n(LLM-Reviewer), it scores the current prompt; the other as an author\n(LLM-Author), it edits the prompt by considering the edit history and the\nreviewer's feedback. Such an author-reviewer feedback loop ensures that the\nprompt is refined in each iteration. We further aggregate a data selection\napproach to Evoke, where only the hard samples are exposed to the LLM. The hard\nsamples are more important because the LLM can develop deeper understanding of\nthe tasks out of them, while the model may already know how to solve the easier\ncases. Experimental results show that Evoke significantly outperforms existing\nmethods. For instance, in the challenging task of logical fallacy detection,\nEvoke scores above 80, while all other baseline methods struggle to reach 20.", "published": "2023-10-20 23:15:59", "link": "http://arxiv.org/abs/2310.13855v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Not all Fake News is Written: A Dataset and Analysis of Misleading Video\n  Headlines", "abstract": "Polarization and the marketplace for impressions have conspired to make\nnavigating information online difficult for users, and while there has been a\nsignificant effort to detect false or misleading text, multimodal datasets have\nreceived considerably less attention. To complement existing resources, we\npresent multimodal Video Misleading Headline (VMH), a dataset that consists of\nvideos and whether annotators believe the headline is representative of the\nvideo's contents. After collecting and annotating this dataset, we analyze\nmultimodal baselines for detecting misleading headlines. Our annotation process\nalso focuses on why annotators view a video as misleading, allowing us to\nbetter understand the interplay of annotators' background and the content of\nthe videos.", "published": "2023-10-20 23:47:01", "link": "http://arxiv.org/abs/2310.13859v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "She had Cobalt Blue Eyes: Prompt Testing to Create Aligned and\n  Sustainable Language Models", "abstract": "As the use of large language models (LLMs) increases within society, as does\nthe risk of their misuse. Appropriate safeguards must be in place to ensure LLM\noutputs uphold the ethical standards of society, highlighting the positive role\nthat artificial intelligence technologies can have. Recent events indicate\nethical concerns around conventionally trained LLMs, leading to overall unsafe\nuser experiences. This motivates our research question: how do we ensure LLM\nalignment? In this work, we introduce a test suite of unique prompts to foster\nthe development of aligned LLMs that are fair, safe, and robust. We show that\nprompting LLMs at every step of the development pipeline, including data\ncuration, pre-training, and fine-tuning, will result in an overall more\nresponsible model. Our test suite evaluates outputs from four state-of-the-art\nlanguage models: GPT-3.5, GPT-4, OPT, and LLaMA-2. The assessment presented in\nthis paper highlights a gap between societal alignment and the capabilities of\ncurrent LLMs. Additionally, implementing a test suite such as ours lowers the\nenvironmental overhead of making models safe and fair.", "published": "2023-10-20 14:18:40", "link": "http://arxiv.org/abs/2310.18333v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ToolChain*: Efficient Action Space Navigation in Large Language Models\n  with A* Search", "abstract": "Large language models (LLMs) have demonstrated powerful decision-making and\nplanning capabilities in solving complicated real-world problems. LLM-based\nautonomous agents can interact with diverse tools (e.g., functional APIs) and\ngenerate solution plans that execute a series of API function calls in a\nstep-by-step manner. The multitude of candidate API function calls\nsignificantly expands the action space, amplifying the critical need for\nefficient action space navigation. However, existing methods either struggle\nwith unidirectional exploration in expansive action spaces, trapped into a\nlocally optimal solution, or suffer from exhaustively traversing all potential\nactions, causing inefficient navigation. To address these issues, we propose\nToolChain*, an efficient tree search-based planning algorithm for LLM-based\nagents. It formulates the entire action space as a decision tree, where each\nnode represents a possible API function call involved in a solution plan. By\nincorporating the A* search algorithm with task-specific cost function design,\nit efficiently prunes high-cost branches that may involve incorrect actions,\nidentifying the most low-cost valid path as the solution. Extensive experiments\non multiple tool-use and reasoning tasks demonstrate that ToolChain*\nefficiently balances exploration and exploitation within an expansive action\nspace. It outperforms state-of-the-art baselines on planning and reasoning\ntasks by 3.1% and 3.5% on average while requiring 7.35x and 2.31x less time,\nrespectively.", "published": "2023-10-20 02:24:35", "link": "http://arxiv.org/abs/2310.13227v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multi-level Contrastive Learning for Script-based Character\n  Understanding", "abstract": "In this work, we tackle the scenario of understanding characters in scripts,\nwhich aims to learn the characters' personalities and identities from their\nutterances. We begin by analyzing several challenges in this scenario, and then\npropose a multi-level contrastive learning framework to capture characters'\nglobal information in a fine-grained manner. To validate the proposed\nframework, we conduct extensive experiments on three character understanding\nsub-tasks by comparing with strong pre-trained language models, including\nSpanBERT, Longformer, BigBird and ChatGPT-3.5. Experimental results demonstrate\nthat our method improves the performances by a considerable margin. Through\nfurther in-depth analysis, we show the effectiveness of our method in\naddressing the challenges and provide more hints on the scenario of character\nunderstanding. We will open-source our work on github at\nhttps://github.com/David-Li0406/Script-based-Character-Understanding.", "published": "2023-10-20 02:40:52", "link": "http://arxiv.org/abs/2310.13231v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "InvGC: Robust Cross-Modal Retrieval by Inverse Graph Convolution", "abstract": "Over recent decades, significant advancements in cross-modal retrieval are\nmainly driven by breakthroughs in visual and linguistic modeling. However, a\nrecent study shows that multi-modal data representations tend to cluster within\na limited convex cone (as representation degeneration problem), which hinders\nretrieval performance due to the inseparability of these representations. In\nour study, we first empirically validate the presence of the representation\ndegeneration problem across multiple cross-modal benchmarks and methods. Next,\nto address it, we introduce a novel method, called InvGC, a post-processing\ntechnique inspired by graph convolution and average pooling. Specifically,\nInvGC defines the graph topology within the datasets and then applies graph\nconvolution in a subtractive manner. This method effectively separates\nrepresentations by increasing the distances between data points. To improve the\nefficiency and effectiveness of InvGC, we propose an advanced graph topology,\nLocalAdj, which only aims to increase the distances between each data point and\nits nearest neighbors. To understand why InvGC works, we present a detailed\ntheoretical analysis, proving that the lower bound of recall will be improved\nafter deploying InvGC. Extensive empirical results show that InvGC and InvGC\nw/LocalAdj significantly mitigate the representation degeneration problem,\nthereby enhancing retrieval performance.\n  Our code is available at\nhttps://github.com/yimuwangcs/Better_Cross_Modal_Retrieval", "published": "2023-10-20 04:45:44", "link": "http://arxiv.org/abs/2310.13276v2", "categories": ["cs.CV", "cs.CL", "cs.LG", "cs.MM"], "primary_category": "cs.CV"}
{"title": "SALMONN: Towards Generic Hearing Abilities for Large Language Models", "abstract": "Hearing is arguably an essential ability of artificial intelligence (AI)\nagents in the physical world, which refers to the perception and understanding\nof general auditory information consisting of at least three types of sounds:\nspeech, audio events, and music. In this paper, we propose SALMONN, a speech\naudio language music open neural network, built by integrating a pre-trained\ntext-based large language model (LLM) with speech and audio encoders into a\nsingle multimodal model. SALMONN enables the LLM to directly process and\nunderstand general audio inputs and achieve competitive performances on a\nnumber of speech and audio tasks used in training, such as automatic speech\nrecognition and translation, auditory-information-based question answering,\nemotion recognition, speaker verification, and music and audio captioning etc.\nSALMONN also has a diverse set of emergent abilities unseen in the training,\nwhich includes but is not limited to speech translation to untrained languages,\nspeech-based slot filling, spoken-query-based question answering, audio-based\nstorytelling, and speech audio co-reasoning etc. The presence of cross-modal\nemergent abilities is studied, and a novel few-shot activation tuning approach\nis proposed to activate such abilities. To our knowledge, SALMONN is the first\nmodel of its type and can be regarded as a step towards AI with generic hearing\nabilities. The source code, model checkpoints and data are available at\nhttps://github.com/bytedance/SALMONN.", "published": "2023-10-20 05:41:57", "link": "http://arxiv.org/abs/2310.13289v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Assessing Privacy Risks in Language Models: A Case Study on\n  Summarization Tasks", "abstract": "Large language models have revolutionized the field of NLP by achieving\nstate-of-the-art performance on various tasks. However, there is a concern that\nthese models may disclose information in the training data. In this study, we\nfocus on the summarization task and investigate the membership inference (MI)\nattack: given a sample and black-box access to a model's API, it is possible to\ndetermine if the sample was part of the training data. We exploit text\nsimilarity and the model's resistance to document modifications as potential MI\nsignals and evaluate their effectiveness on widely used datasets. Our results\ndemonstrate that summarization models are at risk of exposing data membership,\neven in cases where the reference summary is not available. Furthermore, we\ndiscuss several safeguards for training summarization models to protect against\nMI attacks and discuss the inherent trade-off between privacy and utility.", "published": "2023-10-20 05:44:39", "link": "http://arxiv.org/abs/2310.13291v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Decoding the Silent Majority: Inducing Belief Augmented Social Graph\n  with Large Language Model for Response Forecasting", "abstract": "Automatic response forecasting for news media plays a crucial role in\nenabling content producers to efficiently predict the impact of news releases\nand prevent unexpected negative outcomes such as social conflict and moral\ninjury. To effectively forecast responses, it is essential to develop measures\nthat leverage the social dynamics and contextual information surrounding\nindividuals, especially in cases where explicit profiles or historical actions\nof the users are limited (referred to as lurkers). As shown in a previous\nstudy, 97% of all tweets are produced by only the most active 25% of users.\nHowever, existing approaches have limited exploration of how to best process\nand utilize these important features. To address this gap, we propose a novel\nframework, named SocialSense, that leverages a large language model to induce a\nbelief-centered graph on top of an existent social network, along with\ngraph-based propagation to capture social dynamics. We hypothesize that the\ninduced graph that bridges the gap between distant users who share similar\nbeliefs allows the model to effectively capture the response patterns. Our\nmethod surpasses existing state-of-the-art in experimental evaluations for both\nzero-shot and supervised settings, demonstrating its effectiveness in response\nforecasting. Moreover, the analysis reveals the framework's capability to\neffectively handle unseen user and lurker scenarios, further highlighting its\nrobustness and practical applicability.", "published": "2023-10-20 06:17:02", "link": "http://arxiv.org/abs/2310.13297v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Bridging the Gap between Synthetic and Authentic Images for Multimodal\n  Machine Translation", "abstract": "Multimodal machine translation (MMT) simultaneously takes the source sentence\nand a relevant image as input for translation. Since there is no paired image\navailable for the input sentence in most cases, recent studies suggest\nutilizing powerful text-to-image generation models to provide image inputs.\nNevertheless, synthetic images generated by these models often follow different\ndistributions compared to authentic images. Consequently, using authentic\nimages for training and synthetic images for inference can introduce a\ndistribution shift, resulting in performance degradation during inference. To\ntackle this challenge, in this paper, we feed synthetic and authentic images to\nthe MMT model, respectively. Then we minimize the gap between the synthetic and\nauthentic images by drawing close the input image representations of the\nTransformer Encoder and the output distributions of the Transformer Decoder.\nTherefore, we mitigate the distribution disparity introduced by the synthetic\nimages during inference, thereby freeing the authentic images from the\ninference process.Experimental results show that our approach achieves\nstate-of-the-art performance on the Multi30K En-De and En-Fr datasets, while\nremaining independent of authentic images during inference.", "published": "2023-10-20 09:06:30", "link": "http://arxiv.org/abs/2310.13361v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Towards General Error Diagnosis via Behavioral Testing in Machine\n  Translation", "abstract": "Behavioral testing offers a crucial means of diagnosing linguistic errors and\nassessing capabilities of NLP models. However, applying behavioral testing to\nmachine translation (MT) systems is challenging as it generally requires human\nefforts to craft references for evaluating the translation quality of such\nsystems on newly generated test cases. Existing works in behavioral testing of\nMT systems circumvent this by evaluating translation quality without\nreferences, but this restricts diagnosis to specific types of errors, such as\nincorrect translation of single numeric or currency words. In order to diagnose\ngeneral errors, this paper proposes a new Bilingual Translation Pair Generation\nbased Behavior Testing (BTPGBT) framework for conducting behavioral testing of\nMT systems. The core idea of BTPGBT is to employ a novel bilingual translation\npair generation (BTPG) approach that automates the construction of high-quality\ntest cases and their pseudoreferences. Experimental results on various MT\nsystems demonstrate that BTPGBT could provide comprehensive and accurate\nbehavioral testing results for general error diagnosis, which further leads to\nseveral insightful findings. Our code and data are available at https:\n//github.com/wujunjie1998/BTPGBT.", "published": "2023-10-20 09:06:41", "link": "http://arxiv.org/abs/2310.13362v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SE"], "primary_category": "cs.CL"}
{"title": "A Human-Robot Mutual Learning System with Affect-Grounded Language\n  Acquisition and Differential Outcomes Training", "abstract": "This paper presents a novel human-robot interaction setup for robot and human\nlearning of symbolic language for identifying robot homeostatic needs. The\nrobot and human learn to use and respond to the same language symbols that\nconvey homeostatic needs and the stimuli that satisfy the homeostatic needs,\nrespectively. We adopted a differential outcomes training (DOT) protocol\nwhereby the robot provides feedback specific (differential) to its internal\nneeds (e.g. `hunger') when satisfied by the correct stimulus (e.g. cookie). We\nfound evidence that DOT can enhance the human's learning efficiency, which in\nturn enables more efficient robot language acquisition. The robot used in the\nstudy has a vocabulary similar to that of a human infant in the linguistic\n``babbling'' phase. The robot software architecture is built upon a model for\naffect-grounded language acquisition where the robot associates vocabulary with\ninternal needs (hunger, thirst, curiosity) through interactions with the human.\nThe paper presents the results of an initial pilot study conducted with the\ninteractive setup, which reveal that the robot's language acquisition achieves\nhigher convergence rate in the DOT condition compared to the non-DOT control\ncondition. Additionally, participants reported positive affective experiences,\nfeeling of being in control, and an empathetic connection with the robot. This\nmutual learning (teacher-student learning) approach offers a potential\ncontribution of facilitating cognitive interventions with DOT (e.g. for people\nwith dementia) through increased therapy adherence as a result of engaging\nhumans more in training tasks by taking an active teaching-learning role. The\nhomeostatic motivational grounding of the robot's language acquisition has\npotential to contribute to more ecologically valid and social\n(collaborative/nurturing) interactions with robots.", "published": "2023-10-20 09:41:31", "link": "http://arxiv.org/abs/2310.13377v1", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.HC", "I.2.9; I.2.6; I.2.10"], "primary_category": "cs.RO"}
{"title": "POSQA: Probe the World Models of LLMs with Size Comparisons", "abstract": "Embodied language comprehension emphasizes that language understanding is not\nsolely a matter of mental processing in the brain but also involves\ninteractions with the physical and social environment. With the explosive\ngrowth of Large Language Models (LLMs) and their already ubiquitous presence in\nour daily lives, it is becoming increasingly necessary to verify their\nreal-world understanding. Inspired by cognitive theories, we propose POSQA: a\nPhysical Object Size Question Answering dataset with simple size comparison\nquestions to examine the extremity and analyze the potential mechanisms of the\nembodied comprehension of the latest LLMs.\n  We show that even the largest LLMs today perform poorly under the zero-shot\nsetting. We then push their limits with advanced prompting techniques and\nexternal knowledge augmentation. Furthermore, we investigate whether their\nreal-world comprehension primarily derives from contextual information or\ninternal weights and analyse the impact of prompt formats and report bias of\ndifferent objects. Our results show that real-world understanding that LLMs\nshaped from textual data can be vulnerable to deception and confusion by the\nsurface form of prompts, which makes it less aligned with human behaviours.", "published": "2023-10-20 10:05:01", "link": "http://arxiv.org/abs/2310.13394v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Superpixel Semantics Representation and Pre-training for Vision-Language\n  Task", "abstract": "The key to integrating visual language tasks is to establish a good alignment\nstrategy. Recently, visual semantic representation has achieved fine-grained\nvisual understanding by dividing grids or image patches. However, the\ncoarse-grained semantic interactions in image space should not be ignored,\nwhich hinders the extraction of complex contextual semantic relations at the\nscene boundaries. This paper proposes superpixels as comprehensive and robust\nvisual primitives, which mine coarse-grained semantic interactions by\nclustering perceptually similar pixels, speeding up the subsequent processing\nof primitives. To capture superpixel-level semantic features, we propose a\nMultiscale Difference Graph Convolutional Network (MDGCN). It allows parsing\nthe entire image as a fine-to-coarse visual hierarchy. To reason actual\nsemantic relations, we reduce potential noise interference by aggregating\ndifference information between adjacent graph nodes. Finally, we propose a\nmulti-level fusion rule in a bottom-up manner to avoid understanding deviation\nby mining complementary spatial information at different levels. Experiments\nshow that the proposed method can effectively promote the learning of multiple\ndownstream tasks. Encouragingly, our method outperforms previous methods on all\nmetrics. Our code will be released upon publication.", "published": "2023-10-20 12:26:04", "link": "http://arxiv.org/abs/2310.13447v3", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Robust Training for Conversational Question Answering Models with\n  Reinforced Reformulation Generation", "abstract": "Models for conversational question answering (ConvQA) over knowledge graphs\n(KGs) are usually trained and tested on benchmarks of gold QA pairs. This\nimplies that training is limited to surface forms seen in the respective\ndatasets, and evaluation is on a small set of held-out questions. Through our\nproposed framework REIGN, we take several steps to remedy this restricted\nlearning setup. First, we systematically generate reformulations of training\nquestions to increase robustness of models to surface form variations. This is\na particularly challenging problem, given the incomplete nature of such\nquestions. Second, we guide ConvQA models towards higher performance by feeding\nit only those reformulations that help improve their answering quality, using\ndeep reinforcement learning. Third, we demonstrate the viability of training\nmajor model components on one benchmark and applying them zero-shot to another.\nFinally, for a rigorous evaluation of robustness for trained models, we use and\nrelease large numbers of diverse reformulations generated by prompting GPT for\nbenchmark test sets (resulting in 20x increase in sizes). Our findings show\nthat ConvQA models with robust training via reformulations, significantly\noutperform those with standard training from gold QA pairs only.", "published": "2023-10-20 13:51:08", "link": "http://arxiv.org/abs/2310.13505v3", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Towards Understanding Sycophancy in Language Models", "abstract": "Human feedback is commonly utilized to finetune AI assistants. But human\nfeedback may also encourage model responses that match user beliefs over\ntruthful ones, a behaviour known as sycophancy. We investigate the prevalence\nof sycophancy in models whose finetuning procedure made use of human feedback,\nand the potential role of human preference judgments in such behavior. We first\ndemonstrate that five state-of-the-art AI assistants consistently exhibit\nsycophancy across four varied free-form text-generation tasks. To understand if\nhuman preferences drive this broadly observed behavior, we analyze existing\nhuman preference data. We find that when a response matches a user's views, it\nis more likely to be preferred. Moreover, both humans and preference models\n(PMs) prefer convincingly-written sycophantic responses over correct ones a\nnon-negligible fraction of the time. Optimizing model outputs against PMs also\nsometimes sacrifices truthfulness in favor of sycophancy. Overall, our results\nindicate that sycophancy is a general behavior of state-of-the-art AI\nassistants, likely driven in part by human preference judgments favoring\nsycophantic responses.", "published": "2023-10-20 14:46:48", "link": "http://arxiv.org/abs/2310.13548v3", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML", "I.2.6"], "primary_category": "cs.CL"}
{"title": "The Perils & Promises of Fact-checking with Large Language Models", "abstract": "Automated fact-checking, using machine learning to verify claims, has grown\nvital as misinformation spreads beyond human fact-checking capacity. Large\nLanguage Models (LLMs) like GPT-4 are increasingly trusted to write academic\npapers, lawsuits, and news articles and to verify information, emphasizing\ntheir role in discerning truth from falsehood and the importance of being able\nto verify their outputs. Understanding the capacities and limitations of LLMs\nin fact-checking tasks is therefore essential for ensuring the health of our\ninformation ecosystem. Here, we evaluate the use of LLM agents in fact-checking\nby having them phrase queries, retrieve contextual data, and make decisions.\nImportantly, in our framework, agents explain their reasoning and cite the\nrelevant sources from the retrieved context. Our results show the enhanced\nprowess of LLMs when equipped with contextual information. GPT-4 outperforms\nGPT-3, but accuracy varies based on query language and claim veracity. While\nLLMs show promise in fact-checking, caution is essential due to inconsistent\naccuracy. Our investigation calls for further research, fostering a deeper\ncomprehension of when agents succeed and when they fail.", "published": "2023-10-20 14:49:47", "link": "http://arxiv.org/abs/2310.13549v2", "categories": ["cs.CL", "cs.CY", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Automatic Unit Test Data Generation and Actor-Critic Reinforcement\n  Learning for Code Synthesis", "abstract": "The advent of large pre-trained language models in the domain of Code\nSynthesis has shown remarkable performance on various benchmarks, treating the\nproblem of Code Generation in a fashion similar to Natural Language Generation,\ntrained with a Language Modelling (LM) objective. In addition, the property of\nprogramming language code being precisely evaluable with respect to its\nsemantics -- through the use of Unit Tests to check its functional correctness\n-- lends itself to using Reinforcement Learning (RL) as a further training\nparadigm. Previous work has shown that RL can be applied as such to improve\nmodels' coding capabilities; however, such RL-based methods rely on a reward\nsignal based on defined Unit Tests, which are much harder to obtain compared to\nthe huge crawled code datasets used in LM objectives. In this work, we present\na novel approach to automatically obtain data consisting of function signatures\nand associated Unit Tests, suitable for RL training of Code Synthesis models.\nWe also introduce a straightforward, simple yet effective Actor-Critic RL\ntraining scheme and show that it, in conjunction with automatically generated\ntraining data, leads to improvement of a pre-trained code language model's\nperformance by up to 9.9% improvement over the original underlying code\nsynthesis LM, and up to 4.3% over RL-based models trained with standard PPO or\nCodeRL.", "published": "2023-10-20 17:13:16", "link": "http://arxiv.org/abs/2310.13669v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.PL"], "primary_category": "cs.LG"}
{"title": "Long-Form Speech Translation through Segmentation with Finite-State\n  Decoding Constraints on Large Language Models", "abstract": "One challenge in speech translation is that plenty of spoken content is\nlong-form, but short units are necessary for obtaining high-quality\ntranslations. To address this mismatch, we adapt large language models (LLMs)\nto split long ASR transcripts into segments that can be independently\ntranslated so as to maximize the overall translation quality. We overcome the\ntendency of hallucination in LLMs by incorporating finite-state constraints\nduring decoding; these eliminate invalid outputs without requiring additional\ntraining. We discover that LLMs are adaptable to transcripts containing ASR\nerrors through prompt-tuning or fine-tuning. Relative to a state-of-the-art\nautomatic punctuation baseline, our best LLM improves the average BLEU by 2.9\npoints for English-German, English-Spanish, and English-Arabic TED talk\ntranslation in 9 test sets, just by improving segmentation.", "published": "2023-10-20 17:31:39", "link": "http://arxiv.org/abs/2310.13678v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Optimizing Retrieval-augmented Reader Models via Token Elimination", "abstract": "Fusion-in-Decoder (FiD) is an effective retrieval-augmented language model\napplied across a variety of open-domain tasks, such as question answering, fact\nchecking, etc. In FiD, supporting passages are first retrieved and then\nprocessed using a generative model (Reader), which can cause a significant\nbottleneck in decoding time, particularly with long outputs. In this work, we\nanalyze the contribution and necessity of all the retrieved passages to the\nperformance of reader models, and propose eliminating some of the retrieved\ninformation, at the token level, that might not contribute essential\ninformation to the answer generation process. We demonstrate that our method\ncan reduce run-time by up to 62.2%, with only a 2% reduction in performance,\nand in some cases, even improve the performance results.", "published": "2023-10-20 17:41:36", "link": "http://arxiv.org/abs/2310.13682v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Yet Another Model for Arabic Dialect Identification", "abstract": "In this paper, we describe a spoken Arabic dialect identification (ADI) model\nfor Arabic that consistently outperforms previously published results on two\nbenchmark datasets: ADI-5 and ADI-17. We explore two architectural variations:\nResNet and ECAPA-TDNN, coupled with two types of acoustic features: MFCCs and\nfeatures exratected from the pre-trained self-supervised model UniSpeech-SAT\nLarge, as well as a fusion of all four variants. We find that individually,\nECAPA-TDNN network outperforms ResNet, and models with UniSpeech-SAT features\noutperform models with MFCCs by a large margin. Furthermore, a fusion of all\nfour variants consistently outperforms individual models. Our best models\noutperform previously reported results on both datasets, with accuracies of\n84.7% and 96.9% on ADI-5 and ADI-17, respectively.", "published": "2023-10-20 20:58:45", "link": "http://arxiv.org/abs/2310.13812v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Towards Detecting Contextual Real-Time Toxicity for In-Game Chat", "abstract": "Real-time toxicity detection in online environments poses a significant\nchallenge, due to the increasing prevalence of social media and gaming\nplatforms. We introduce ToxBuster, a simple and scalable model that reliably\ndetects toxic content in real-time for a line of chat by including chat history\nand metadata. ToxBuster consistently outperforms conventional toxicity models\nacross popular multiplayer games, including Rainbow Six Siege, For Honor, and\nDOTA 2. We conduct an ablation study to assess the importance of each model\ncomponent and explore ToxBuster's transferability across the datasets.\nFurthermore, we showcase ToxBuster's efficacy in post-game moderation,\nsuccessfully flagging 82.1% of chat-reported players at a precision level of\n90.0%. Additionally, we show how an additional 6% of unreported toxic players\ncan be proactively moderated.", "published": "2023-10-20 00:29:57", "link": "http://arxiv.org/abs/2310.18330v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AllTogether: Investigating the Efficacy of Spliced Prompt for Web\n  Navigation using Large Language Models", "abstract": "Large Language Models (LLMs) have emerged as promising agents for web\nnavigation tasks, interpreting objectives and interacting with web pages.\nHowever, the efficiency of spliced prompts for such tasks remains\nunderexplored. We introduces AllTogether, a standardized prompt template that\nenhances task context representation, thereby improving LLMs' performance in\nHTML-based web navigation. We evaluate the efficacy of this approach through\nprompt learning and instruction finetuning based on open-source Llama-2 and\nAPI-accessible GPT models. Our results reveal that models like GPT-4 outperform\nsmaller models in web navigation tasks. Additionally, we find that the length\nof HTML snippet and history trajectory significantly influence performance, and\nprior step-by-step instructions prove less effective than real-time\nenvironmental feedback. Overall, we believe our work provides valuable insights\nfor future research in LLM-driven web agents.", "published": "2023-10-20 11:10:14", "link": "http://arxiv.org/abs/2310.18331v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "WordArt Designer: User-Driven Artistic Typography Synthesis using Large\n  Language Models", "abstract": "This paper introduces WordArt Designer, a user-driven framework for artistic\ntypography synthesis, relying on the Large Language Model (LLM). The system\nincorporates four key modules: the LLM Engine, SemTypo, StyTypo, and TexTypo\nmodules. 1) The LLM Engine, empowered by the LLM (e.g., GPT-3.5), interprets\nuser inputs and generates actionable prompts for the other modules, thereby\ntransforming abstract concepts into tangible designs. 2) The SemTypo module\noptimizes font designs using semantic concepts, striking a balance between\nartistic transformation and readability. 3) Building on the semantic layout\nprovided by the SemTypo module, the StyTypo module creates smooth, refined\nimages. 4) The TexTypo module further enhances the design's aesthetics through\ntexture rendering, enabling the generation of inventive textured fonts.\nNotably, WordArt Designer highlights the fusion of generative AI with artistic\ntypography. Experience its capabilities on ModelScope:\nhttps://www.modelscope.cn/studios/WordArt/WordArt.", "published": "2023-10-20 12:44:44", "link": "http://arxiv.org/abs/2310.18332v2", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.GR"], "primary_category": "cs.CL"}
{"title": "On the Language Encoder of Contrastive Cross-modal Models", "abstract": "Contrastive cross-modal models such as CLIP and CLAP aid various\nvision-language (VL) and audio-language (AL) tasks. However, there has been\nlimited investigation of and improvement in their language encoder, which is\nthe central component of encoding natural language descriptions of image/audio\ninto vector representations. We extensively evaluate how unsupervised and\nsupervised sentence embedding training affect language encoder quality and\ncross-modal task performance. In VL pretraining, we found that sentence\nembedding training language encoder quality and aids in cross-modal tasks,\nimproving contrastive VL models such as CyCLIP. In contrast, AL pretraining\nbenefits less from sentence embedding training, which may result from the\nlimited amount of pretraining data. We analyze the representation spaces to\nunderstand the strengths of sentence embedding training, and find that it\nimproves text-space uniformity, at the cost of decreased cross-modal alignment.", "published": "2023-10-20 04:21:09", "link": "http://arxiv.org/abs/2310.13267v1", "categories": ["cs.CL", "cs.CV", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "GenDistiller: Distilling Pre-trained Language Models based on Generative\n  Models", "abstract": "Self-supervised pre-trained models such as HuBERT and WavLM leverage\nunlabeled speech data for representation learning and offer significantly\nimprove for numerous downstream tasks. Despite the success of these methods,\ntheir large memory and strong computational requirements hinder their\napplication on resource restricted devices. Therefore, this paper introduces\nGenDistiller, a novel knowledge distillation framework to distill hidden\nrepresentations from teacher network based on generative language model. The\ngenerative structure enables the proposed model to generate the target teacher\nhidden layers autoregressively, considering the interactions between hidden\nlayers without instroducing additional inputs. A two-dimensional attention\nmechanism is implemented to ensure the causality of hidden layers, while\npreserving bidirectional attention in the time dimension. Experiments reveal\nthe advantage of the generative distiller over the baseline system that\npredicts the hidden layers of teacher network directly without a generatvie\nmodel.", "published": "2023-10-20 10:56:01", "link": "http://arxiv.org/abs/2310.13418v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "HRTF Interpolation using a Spherical Neural Process Meta-Learner", "abstract": "Several individualization methods have recently been proposed to estimate a\nsubject's Head-Related Transfer Function (HRTF) using convenient input\nmodalities such as anthropometric measurements or pinnae photographs. There\nexists a need for adaptively correcting the estimation error committed by such\nmethods using a few data point samples from the subject's HRTF, acquired using\nacoustic measurements or perceptual feedback. To this end, we introduce a\nConvolutional Conditional Neural Process meta-learner specialized in HRTF error\ninterpolation. In particular, the model includes a Spherical Convolutional\nNeural Network component to accommodate the spherical geometry of HRTF data. It\nalso exploits potential symmetries between the HRTF's left and right channels\nabout the median axis. In this work, we evaluate the proposed model's\nperformance purely on time-aligned spectrum interpolation grounds under a\nsimplified setup where a generic population-mean HRTF forms the initial\nestimates prior to corrections instead of individualized ones. The trained\nmodel achieves up to 3 dB relative error reduction compared to state-of-the-art\ninterpolation methods despite being trained using only 85 subjects. This\nimprovement translates up to nearly a halving of the data point count required\nto achieve comparable accuracy, in particular from 50 to 28 points to reach an\naverage of -20 dB relative error per interpolated feature. Moreover, we show\nthat the trained model provides well-calibrated uncertainty estimates.\nAccordingly, such estimates can inform the sequential decision problem of\nacquiring as few correcting HRTF data points as needed to meet a desired level\nof HRTF individualization accuracy.", "published": "2023-10-20 11:41:54", "link": "http://arxiv.org/abs/2310.13430v1", "categories": ["eess.AS", "cs.LG", "I.5.4; J.2"], "primary_category": "eess.AS"}
{"title": "Neural domain alignment for spoken language recognition based on optimal\n  transport", "abstract": "Domain shift poses a significant challenge in cross-domain spoken language\nrecognition (SLR) by reducing its effectiveness. Unsupervised domain adaptation\n(UDA) algorithms have been explored to address domain shifts in SLR without\nrelying on class labels in the target domain. One successful UDA approach\nfocuses on learning domain-invariant representations to align feature\ndistributions between domains. However, disregarding the class structure during\nthe learning process of domain-invariant representations can result in\nover-alignment, negatively impacting the classification task. To overcome this\nlimitation, we propose an optimal transport (OT)-based UDA algorithm for a\ncross-domain SLR, leveraging the distribution geometry structure-aware property\nof OT. An OT-based discrepancy measure on a joint distribution over feature and\nlabel information is considered during domain alignment in OT-based UDA. Our\nprevious study discovered that completely aligning the distributions between\nthe source and target domains can introduce a negative transfer, where classes\nor irrelevant classes from the source domain map to a different class in the\ntarget domain during distribution alignment. This negative transfer degrades\nthe performance of the adaptive model. To mitigate this issue, we introduce\ncoupling-weighted partial optimal transport (POT) within our UDA framework for\nSLR, where soft weighting on the OT coupling based on transport cost is\nadaptively set during domain alignment. A cross-domain SLR task was used in the\nexperiments to evaluate the proposed UDA. The results demonstrated that our\nproposed UDA algorithm significantly improved the performance over existing UDA\nalgorithms in a cross-channel SLR task.", "published": "2023-10-20 13:12:35", "link": "http://arxiv.org/abs/2310.13471v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multi-label Open-set Audio Classification", "abstract": "Current audio classification models have small class vocabularies relative to\nthe large number of sound event classes of interest in the real world. Thus,\nthey provide a limited view of the world that may miss important yet unexpected\nor unknown sound events. To address this issue, open-set audio classification\ntechniques have been developed to detect sound events from unknown classes.\nAlthough these methods have been applied to a multi-class context in audio,\nsuch as sound scene classification, they have yet to be investigated for\npolyphonic audio in which sound events overlap, requiring the use of\nmulti-label models. In this study, we establish the problem of multi-label\nopen-set audio classification by creating a dataset with varying unknown class\ndistributions and evaluating baseline approaches built upon existing\ntechniques.", "published": "2023-10-20 18:43:28", "link": "http://arxiv.org/abs/2310.13759v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Intelligibility prediction with a pretrained noise-robust automatic\n  speech recognition model", "abstract": "This paper describes two intelligibility prediction systems derived from a\npretrained noise-robust automatic speech recognition (ASR) model for the second\nClarity Prediction Challenge (CPC2). One system is intrusive and leverages the\nhidden representations of the ASR model. The other system is non-intrusive and\nmakes predictions with derived ASR uncertainty. The ASR model is only\npretrained with a simulated noisy speech corpus and does not take advantage of\nthe CPC2 data. For that reason, the intelligibility prediction systems are\nrobust to unseen scenarios given the accurate prediction performance on the\nCPC2 evaluation.", "published": "2023-10-20 15:45:54", "link": "http://arxiv.org/abs/2310.19817v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Music Augmentation and Denoising For Peak-Based Audio Fingerprinting", "abstract": "Audio fingerprinting is a well-established solution for song identification\nfrom short recording excerpts. Popular methods rely on the extraction of sparse\nrepresentations, generally spectral peaks, and have proven to be accurate,\nfast, and scalable to large collections. However, real-world applications of\naudio identification often happen in noisy environments, which can cause these\nsystems to fail. In this work, we tackle this problem by introducing and\nreleasing a new audio augmentation pipeline that adds noise to music snippets\nin a realistic way, by stochastically mimicking real-world scenarios. We then\npropose and release a deep learning model that removes noisy components from\nspectrograms in order to improve peak-based fingerprinting systems' accuracy.\nWe show that the addition of our model improves the identification performance\nof commonly used audio fingerprinting systems, even under noisy conditions.", "published": "2023-10-20 09:56:22", "link": "http://arxiv.org/abs/2310.13388v2", "categories": ["cs.SD", "cs.IR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Definition-independent Formalization of Soundscapes: Towards a Formal\n  Methodology", "abstract": "Soundscapes have been studied by researchers from various disciplines, each\nwith different perspectives, goals, approaches, and terminologies. Accordingly,\ndepending on the field, the concept of a soundscape's components changes,\nconsequently changing the basic definition. This results in complicating\ninterdisciplinary communication and comparison of results. Especially when\nsoundscape-unrelated research areas are involved. For this reason, we present a\npotential formalization that is independent of the underlying soundscape\ndefinition, with the goal of being able to capture the heterogeneous structure\nof the data as well as the different ideologies in one model. In an exemplary\nanalysis of frequency correlation matrices for land use type detection as an\nalternative to features like MFCCs, we show a practical application of our\npresented formalization.", "published": "2023-10-20 10:22:15", "link": "http://arxiv.org/abs/2310.13404v1", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Novel Transfer Learning Method Utilizing Acoustic and Vibration\n  Signals for Rotating Machinery Fault Diagnosis", "abstract": "Fault diagnosis of rotating machinery plays a important role for the safety\nand stability of modern industrial systems. However, there is a distribution\ndiscrepancy between training data and data of real-world operation scenarios,\nwhich causing the decrease of performance of existing systems. This paper\nproposed a transfer learning based method utilizing acoustic and vibration\nsignal to address this distribution discrepancy. We designed the acoustic and\nvibration feature fusion MAVgram to offer richer and more reliable information\nof faults, coordinating with a DNN-based classifier to obtain more effective\ndiagnosis representation. The backbone was pre-trained and then fine-tuned to\nobtained excellent performance of the target task. Experimental results\ndemonstrate the effectiveness of the proposed method, and achieved improved\nperformance compared to STgram-MFN.", "published": "2023-10-20 10:50:14", "link": "http://arxiv.org/abs/2310.14796v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Two-Stage Triplet Loss Training with Curriculum Augmentation for\n  Audio-Visual Retrieval", "abstract": "The cross-modal retrieval model leverages the potential of triple loss\noptimization to learn robust embedding spaces. However, existing methods often\ntrain these models in a singular pass, overlooking the distinction between\nsemi-hard and hard triples in the optimization process. The oversight of not\ndistinguishing between semi-hard and hard triples leads to suboptimal model\nperformance. In this paper, we introduce a novel approach rooted in curriculum\nlearning to address this problem. We propose a two-stage training paradigm that\nguides the model's learning process from semi-hard to hard triplets. In the\nfirst stage, the model is trained with a set of semi-hard triplets, starting\nfrom a low-loss base. Subsequently, in the second stage, we augment the\nembeddings using an interpolation technique. This process identifies potential\nhard negatives, alleviating issues arising from high-loss functions due to a\nscarcity of hard triples. Our approach then applies hard triplet mining in the\naugmented embedding space to further optimize the model. Extensive experimental\nresults conducted on two audio-visual datasets show a significant improvement\nof approximately 9.8% in terms of average Mean Average Precision (MAP) over the\ncurrent state-of-the-art method, MSNSCA, for the Audio-Visual Cross-Modal\nRetrieval (AV-CMR) task on the AVE dataset, indicating the effectiveness of our\nproposed method.", "published": "2023-10-20 12:35:54", "link": "http://arxiv.org/abs/2310.13451v1", "categories": ["cs.SD", "cs.CV", "cs.IR", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
