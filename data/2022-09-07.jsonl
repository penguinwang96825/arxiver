{"title": "Adam Mickiewicz University at WMT 2022: NER-Assisted and Quality-Aware\n  Neural Machine Translation", "abstract": "This paper presents Adam Mickiewicz University's (AMU) submissions to the\nconstrained track of the WMT 2022 General MT Task. We participated in the\nUkrainian $\\leftrightarrow$ Czech translation directions. The systems are a\nweighted ensemble of four models based on the Transformer (big) architecture.\nThe models use source factors to utilize the information about named entities\npresent in the input. Each of the models in the ensemble was trained using only\nthe data provided by the shared task organizers. A noisy back-translation\ntechnique was used to augment the training corpora. One of the models in the\nensemble is a document-level model, trained on parallel and synthetic longer\nsequences. During the sentence-level decoding process, the ensemble generated\nthe n-best list. The n-best list was merged with the n-best list generated by a\nsingle document-level model which translated multiple sentences at a time.\nFinally, existing quality estimation models and minimum Bayes risk decoding\nwere used to rerank the n-best list so that the best hypothesis was chosen\naccording to the COMET evaluation metric. According to the automatic evaluation\nresults, our systems rank first in both translation directions.", "published": "2022-09-07 07:14:07", "link": "http://arxiv.org/abs/2209.02962v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "That Slepen Al the Nyght with Open Ye! Cross-era Sequence Segmentation\n  with Switch-memory", "abstract": "The evolution of language follows the rule of gradual change. Grammar,\nvocabulary, and lexical semantic shifts take place over time, resulting in a\ndiachronic linguistic gap. As such, a considerable amount of texts are written\nin languages of different eras, which creates obstacles for natural language\nprocessing tasks, such as word segmentation and machine translation. Although\nthe Chinese language has a long history, previous Chinese natural language\nprocessing research has primarily focused on tasks within a specific era.\nTherefore, we propose a cross-era learning framework for Chinese word\nsegmentation (CWS), CROSSWISE, which uses the Switch-memory (SM) module to\nincorporate era-specific linguistic knowledge. Experiments on four corpora from\ndifferent eras show that the performance of each corpus significantly improves.\nFurther analyses also demonstrate that the SM can effectively integrate the\nknowledge of the eras into the neural network.", "published": "2022-09-07 07:21:23", "link": "http://arxiv.org/abs/2209.02967v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fengshenbang 1.0: Being the Foundation of Chinese Cognitive Intelligence", "abstract": "Nowadays, foundation models become one of fundamental infrastructures in\nartificial intelligence, paving ways to the general intelligence. However, the\nreality presents two urgent challenges: existing foundation models are\ndominated by the English-language community; users are often given limited\nresources and thus cannot always use foundation models. To support the\ndevelopment of the Chinese-language community, we introduce an open-source\nproject, called Fengshenbang, which leads by the research center for Cognitive\nComputing and Natural Language (CCNL). Our project has comprehensive\ncapabilities, including large pre-trained models, user-friendly APIs,\nbenchmarks, datasets, and others. We wrap all these in three sub-projects: the\nFengshenbang Model, the Fengshen Framework, and the Fengshen Benchmark. An\nopen-source roadmap, Fengshenbang, aims to re-evaluate the open-source\ncommunity of Chinese pre-trained large-scale models, prompting the development\nof the entire Chinese large-scale model community. We also want to build a\nuser-centered open-source ecosystem to allow individuals to access the desired\nmodels to match their computing resources. Furthermore, we invite companies,\ncolleges, and research institutions to collaborate with us to build the\nlarge-scale open-source model-based ecosystem. We hope that this project will\nbe the foundation of Chinese cognitive intelligence.", "published": "2022-09-07 07:32:37", "link": "http://arxiv.org/abs/2209.02970v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Non-Standard Vietnamese Word Detection and Normalization for\n  Text-to-Speech", "abstract": "Converting written texts into their spoken forms is an essential problem in\nany text-to-speech (TTS) systems. However, building an effective text\nnormalization solution for a real-world TTS system face two main challenges:\n(1) the semantic ambiguity of non-standard words (NSWs), e.g., numbers, dates,\nranges, scores, abbreviations, and (2) transforming NSWs into pronounceable\nsyllables, such as URL, email address, hashtag, and contact name. In this\npaper, we propose a new two-phase normalization approach to deal with these\nchallenges. First, a model-based tagger is designed to detect NSWs. Then,\ndepending on NSW types, a rule-based normalizer expands those NSWs into their\nfinal verbal forms. We conducted three empirical experiments for NSW detection\nusing Conditional Random Fields (CRFs), BiLSTM-CNN-CRF, and BERT-BiGRU-CRF\nmodels on a manually annotated dataset including 5819 sentences extracted from\nVietnamese news articles. In the second phase, we propose a forward\nlexicon-based maximum matching algorithm to split down the hashtag, email, URL,\nand contact name. The experimental results of the tagging phase show that the\naverage F1 scores of the BiLSTM-CNN-CRF and CRF models are above 90.00%,\nreaching the highest F1 of 95.00% with the BERT-BiGRU-CRF model. Overall, our\napproach has low sentence error rates, at 8.15% with CRF and 7.11% with\nBiLSTM-CNN-CRF taggers, and only 6.67% with BERT-BiGRU-CRF tagger.", "published": "2022-09-07 07:34:05", "link": "http://arxiv.org/abs/2209.02971v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving the Cross-Lingual Generalisation in Visual Question Answering", "abstract": "While several benefits were realized for multilingual vision-language\npretrained models, recent benchmarks across various tasks and languages showed\npoor cross-lingual generalisation when multilingually pre-trained\nvision-language models are applied to non-English data, with a large gap\nbetween (supervised) English performance and (zero-shot) cross-lingual\ntransfer. In this work, we explore the poor performance of these models on a\nzero-shot cross-lingual visual question answering (VQA) task, where models are\nfine-tuned on English visual-question data and evaluated on 7 typologically\ndiverse languages. We improve cross-lingual transfer with three strategies: (1)\nwe introduce a linguistic prior objective to augment the cross-entropy loss\nwith a similarity-based loss to guide the model during training, (2) we learn a\ntask-specific subnetwork that improves cross-lingual generalisation and reduces\nvariance without model modification, (3) we augment training examples using\nsynthetic code-mixing to promote alignment of embeddings between source and\ntarget languages. Our experiments on xGQA using the pretrained multilingual\nmultimodal transformers UC2 and M3P demonstrate the consistent effectiveness of\nthe proposed fine-tuning strategy for 7 languages, outperforming existing\ntransfer methods with sparse models. Code and data to reproduce our findings\nare publicly available.", "published": "2022-09-07 08:07:43", "link": "http://arxiv.org/abs/2209.02982v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge-enhanced Iterative Instruction Generation and Reasoning for\n  Knowledge Base Question Answering", "abstract": "Multi-hop Knowledge Base Question Answering(KBQA) aims to find the answer\nentity in a knowledge base which is several hops from the topic entity\nmentioned in the question. Existing Retrieval-based approaches first generate\ninstructions from the question and then use them to guide the multi-hop\nreasoning on the knowledge graph. As the instructions are fixed during the\nwhole reasoning procedure and the knowledge graph is not considered in\ninstruction generation, the model cannot revise its mistake once it predicts an\nintermediate entity incorrectly. To handle this, we propose KBIGER(Knowledge\nBase Iterative Instruction GEnerating and Reasoning), a novel and efficient\napproach to generate the instructions dynamically with the help of reasoning\ngraph. Instead of generating all the instructions before reasoning, we take the\n(k-1)-th reasoning graph into consideration to build the k-th instruction. In\nthis way, the model could check the prediction from the graph and generate new\ninstructions to revise the incorrect prediction of intermediate entities. We do\nexperiments on two multi-hop KBQA benchmarks and outperform the existing\napproaches, becoming the new-state-of-the-art. Further experiments show our\nmethod does detect the incorrect prediction of intermediate entities and has\nthe ability to revise such errors.", "published": "2022-09-07 09:02:45", "link": "http://arxiv.org/abs/2209.03005v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Complementarity between Pre-Training and Random-Initialization\n  for Resource-Rich Machine Translation", "abstract": "Pre-Training (PT) of text representations has been successfully applied to\nlow-resource Neural Machine Translation (NMT). However, it usually fails to\nachieve notable gains (sometimes, even worse) on resource-rich NMT on par with\nits Random-Initialization (RI) counterpart. We take the first step to\ninvestigate the complementarity between PT and RI in resource-rich scenarios\nvia two probing analyses, and find that: 1) PT improves NOT the accuracy, but\nthe generalization by achieving flatter loss landscapes than that of RI; 2) PT\nimproves NOT the confidence of lexical choice, but the negative diversity by\nassigning smoother lexical probability distributions than that of RI. Based on\nthese insights, we propose to combine their complementarities with a model\nfusion algorithm that utilizes optimal transport to align neurons between PT\nand RI. Experiments on two resource-rich translation benchmarks, WMT'17\nEnglish-Chinese (20M) and WMT'19 English-German (36M), show that PT and RI\ncould be nicely complementary to each other, achieving substantial improvements\nconsidering both translation accuracy, generalization, and negative diversity.\nProbing tools and code are released at: https://github.com/zanchangtong/PTvsRI.", "published": "2022-09-07 17:23:08", "link": "http://arxiv.org/abs/2209.03316v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating Reasons for Disagreement in Natural Language Inference", "abstract": "We investigate how disagreement in natural language inference (NLI)\nannotation arises. We developed a taxonomy of disagreement sources with 10\ncategories spanning 3 high-level classes. We found that some disagreements are\ndue to uncertainty in the sentence meaning, others to annotator biases and task\nartifacts, leading to different interpretations of the label distribution. We\nexplore two modeling approaches for detecting items with potential\ndisagreement: a 4-way classification with a \"Complicated\" label in addition to\nthe three standard NLI labels, and a multilabel classification approach. We\nfound that the multilabel classification is more expressive and gives better\nrecall of the possible interpretations in the data.", "published": "2022-09-07 18:01:39", "link": "http://arxiv.org/abs/2209.03392v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Entity-based SpanCopy for Abstractive Summarization to Improve the\n  Factual Consistency", "abstract": "Despite the success of recent abstractive summarizers on automatic evaluation\nmetrics, the generated summaries still present factual inconsistencies with the\nsource document. In this paper, we focus on entity-level factual inconsistency,\ni.e. reducing the mismatched entities between the generated summaries and the\nsource documents. We therefore propose a novel entity-based SpanCopy mechanism,\nand explore its extension with a Global Relevance component. Experiment results\non four summarization datasets show that SpanCopy can effectively improve the\nentity-level factual consistency with essentially no change in the word-level\nand entity-level saliency. The code is available at\nhttps://github.com/Wendy-Xiao/Entity-based-SpanCopy", "published": "2022-09-07 21:38:08", "link": "http://arxiv.org/abs/2209.03479v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Facilitating Global Team Meetings Between Language-Based Subgroups: When\n  and How Can Machine Translation Help?", "abstract": "Global teams frequently consist of language-based subgroups who put together\ncomplementary information to achieve common goals. Previous research outlines a\ntwo-step work communication flow in these teams. There are team meetings using\na required common language (i.e., English); in preparation for those meetings,\npeople have subgroup conversations in their native languages. Work\ncommunication at team meetings is often less effective than in subgroup\nconversations. In the current study, we investigate the idea of leveraging\nmachine translation (MT) to facilitate global team meetings. We hypothesize\nthat exchanging subgroup conversation logs before a team meeting offers\ncontextual information that benefits teamwork at the meeting. MT can translate\nthese logs, which enables comprehension at a low cost. To test our hypothesis,\nwe conducted a between-subjects experiment where twenty quartets of\nparticipants performed a personnel selection task. Each quartet included two\nEnglish native speakers (NS) and two non-native speakers (NNS) whose native\nlanguage was Mandarin. All participants began the task with subgroup\nconversations in their native languages, then proceeded to team meetings in\nEnglish. We manipulated the exchange of subgroup conversation logs prior to\nteam meetings: with MT-mediated exchanges versus without. Analysis of\nparticipants' subjective experience, task performance, and depth of discussions\nas reflected through their conversational moves jointly indicates that team\nmeeting quality improved when there were MT-mediated exchanges of subgroup\nconversation logs as opposed to no exchanges. We conclude with reflections on\nwhen and how MT could be applied to enhance global teamwork across a language\nbarrier.", "published": "2022-09-07 03:31:25", "link": "http://arxiv.org/abs/2209.02906v2", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "VGStore: A Multimodal Extension to SPARQL for Querying RDF Scene Graph", "abstract": "Semantic Web technology has successfully facilitated many RDF models with\nrich data representation methods. It also has the potential ability to\nrepresent and store multimodal knowledge bases such as multimodal scene graphs.\nHowever, most existing query languages, especially SPARQL, barely explore the\nimplicit multimodal relationships like semantic similarity, spatial relations,\netc. We first explored this issue by organizing a large-scale scene graph\ndataset, namely Visual Genome, in the RDF graph database. Based on the proposed\nRDF-stored multimodal scene graph, we extended SPARQL queries to answer\nquestions containing relational reasoning about color, spatial, etc. Further\ndemo (i.e., VGStore) shows the effectiveness of customized queries and\ndisplaying multimodal data.", "published": "2022-09-07 08:05:00", "link": "http://arxiv.org/abs/2209.02981v1", "categories": ["cs.DB", "cs.CL"], "primary_category": "cs.DB"}
{"title": "The Ethical Need for Watermarks in Machine-Generated Language", "abstract": "Watermarks should be introduced in the natural language outputs of AI systems\nin order to maintain the distinction between human and machine-generated text.\nThe ethical imperative to not blur this distinction arises from the asemantic\nnature of large language models and from human projections of emotional and\ncognitive states on machines, possibly leading to manipulation, spreading\nfalsehoods or emotional distress. Enforcing this distinction requires\nunintrusive, yet easily accessible marks of the machine origin. We propose to\nimplement a code based on equidistant letter sequences. While no such code\nexists in human-written texts, its appearance in machine-generated ones would\nprove helpful for ethical reasons.", "published": "2022-09-07 13:09:44", "link": "http://arxiv.org/abs/2209.03118v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "A multiclass Q-NLP sentiment analysis experiment using DisCoCat", "abstract": "Sentiment analysis is a branch of Natural Language Processing (NLP) which\ngoal is to assign sentiments or emotions to particular sentences or words.\nPerforming this task is particularly useful for companies wishing to take into\naccount customer feedback through chatbots or verbatim. This has been done\nextensively in the literature using various approaches, ranging from simple\nmodels to deep transformer neural networks. In this paper, we will tackle\nsentiment analysis in the Noisy Intermediate Scale Computing (NISQ) era, using\nthe DisCoCat model of language. We will first present the basics of quantum\ncomputing and the DisCoCat model. This will enable us to define a general\nframework to perform NLP tasks on a quantum computer. We will then extend the\ntwo-class classification that was performed by Lorenz et al. (2021) to a\nfour-class sentiment analysis experiment on a much larger dataset, showing the\nscalability of such a framework.", "published": "2022-09-07 13:47:35", "link": "http://arxiv.org/abs/2209.03152v1", "categories": ["cs.CL", "cs.ET"], "primary_category": "cs.CL"}
{"title": "On the Effectiveness of Compact Biomedical Transformers", "abstract": "Language models pre-trained on biomedical corpora, such as BioBERT, have\nrecently shown promising results on downstream biomedical tasks. Many existing\npre-trained models, on the other hand, are resource-intensive and\ncomputationally heavy owing to factors such as embedding size, hidden\ndimension, and number of layers. The natural language processing (NLP)\ncommunity has developed numerous strategies to compress these models utilising\ntechniques such as pruning, quantisation, and knowledge distillation, resulting\nin models that are considerably faster, smaller, and subsequently easier to use\nin practice. By the same token, in this paper we introduce six lightweight\nmodels, namely, BioDistilBERT, BioTinyBERT, BioMobileBERT, DistilBioBERT,\nTinyBioBERT, and CompactBioBERT which are obtained either by knowledge\ndistillation from a biomedical teacher or continual learning on the Pubmed\ndataset via the Masked Language Modelling (MLM) objective. We evaluate all of\nour models on three biomedical tasks and compare them with BioBERT-v1.1 to\ncreate efficient lightweight models that perform on par with their larger\ncounterparts. All the models will be publicly available on our Huggingface\nprofile at https://huggingface.co/nlpie and the codes used to run the\nexperiments will be available at\nhttps://github.com/nlpie-research/Compact-Biomedical-Transformers.", "published": "2022-09-07 14:24:04", "link": "http://arxiv.org/abs/2209.03182v1", "categories": ["cs.CL", "cs.LG", "68T50"], "primary_category": "cs.CL"}
{"title": "AILAB-Udine@SMM4H 22: Limits of Transformers and BERT Ensembles", "abstract": "This paper describes the models developed by the AILAB-Udine team for the\nSMM4H 22 Shared Task. We explored the limits of Transformer based models on\ntext classification, entity extraction and entity normalization, tackling Tasks\n1, 2, 5, 6 and 10. The main take-aways we got from participating in different\ntasks are: the overwhelming positive effects of combining different\narchitectures when using ensemble learning, and the great potential of\ngenerative models for term normalization.", "published": "2022-09-07 20:17:15", "link": "http://arxiv.org/abs/2209.03452v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SynSciPass: detecting appropriate uses of scientific text generation", "abstract": "Approaches to machine generated text detection tend to focus on binary\nclassification of human versus machine written text. In the scientific domain\nwhere publishers might use these models to examine manuscripts under\nsubmission, misclassification has the potential to cause harm to authors.\nAdditionally, authors may appropriately use text generation models such as with\nthe use of assistive technologies like translation tools. In this setting, a\nbinary classification scheme might be used to flag appropriate uses of\nassistive text generation technology as simply machine generated which is a\ncause of concern. In our work, we simulate this scenario by presenting a\nstate-of-the-art detector trained on the DAGPap22 with machine translated\npassages from Scielo and find that the model performs at random. Given this\nfinding, we develop a framework for dataset development that provides a nuanced\napproach to detecting machine generated text by having labels for the type of\ntechnology used such as for translation or paraphrase resulting in the\nconstruction of SynSciPass. By training the same model that performed well on\nDAGPap22 on SynSciPass, we show that not only is the model more robust to\ndomain shifts but also is able to uncover the type of technology used for\nmachine generated text. Despite this, we conclude that current datasets are\nneither comprehensive nor realistic enough to understand how these models would\nperform in the wild where manuscript submissions can come from many unknown or\nnovel distributions, how they would perform on scientific full-texts rather\nthan small passages, and what might happen when there is a mix of appropriate\nand inappropriate uses of natural language generation.", "published": "2022-09-07 13:16:40", "link": "http://arxiv.org/abs/2209.03742v2", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "Power of Explanations: Towards automatic debiasing in hate speech\n  detection", "abstract": "Hate speech detection is a common downstream application of natural language\nprocessing (NLP) in the real world. In spite of the increasing accuracy,\ncurrent data-driven approaches could easily learn biases from the imbalanced\ndata distributions originating from humans. The deployment of biased models\ncould further enhance the existing social biases. But unlike handling tabular\ndata, defining and mitigating biases in text classifiers, which deal with\nunstructured data, are more challenging. A popular solution for improving\nmachine learning fairness in NLP is to conduct the debiasing process with a\nlist of potentially discriminated words given by human annotators. In addition\nto suffering from the risks of overlooking the biased terms, exhaustively\nidentifying bias with human annotators are unsustainable since discrimination\nis variable among different datasets and may evolve over time. To this end, we\npropose an automatic misuse detector (MiD) relying on an explanation method for\ndetecting potential bias. And built upon that, an end-to-end debiasing\nframework with the proposed staged correction is designed for text classifiers\nwithout any external resources required.", "published": "2022-09-07 14:14:03", "link": "http://arxiv.org/abs/2209.09975v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Predicting the clinical citation count of biomedical papers using\n  multilayer perceptron neural network", "abstract": "The number of clinical citations received from clinical guidelines or\nclinical trials has been considered as one of the most appropriate indicators\nfor quantifying the clinical impact of biomedical papers. Therefore, the early\nprediction of the clinical citation count of biomedical papers is critical to\nscientific activities in biomedicine, such as research evaluation, resource\nallocation, and clinical translation. In this study, we designed a four-layer\nmultilayer perceptron neural network (MPNN) model to predict the clinical\ncitation count of biomedical papers in the future by using 9,822,620 biomedical\npapers published from 1985 to 2005. We extracted ninety-one paper features from\nthree dimensions as the input of the model, including twenty-one features in\nthe paper dimension, thirty-five in the reference dimension, and thirty-five in\nthe citing paper dimension. In each dimension, the features can be classified\ninto three categories, i.e., the citation-related features, the clinical\ntranslation-related features, and the topic-related features. Besides, in the\npaper dimension, we also considered the features that have previously been\ndemonstrated to be related to the citation counts of research papers. The\nresults showed that the proposed MPNN model outperformed the other five\nbaseline models, and the features in the reference dimension were the most\nimportant.", "published": "2022-09-07 12:08:24", "link": "http://arxiv.org/abs/2210.06346v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Taking a Language Detour: How International Migrants Speaking a Minority\n  Language Seek COVID-Related Information in Their Host Countries", "abstract": "Information seeking is crucial for people's self-care and wellbeing in times\nof public crises. Extensive research has investigated empirical understandings\nas well as technical solutions to facilitate information seeking by domestic\ncitizens of affected regions. However, limited knowledge is established to\nsupport international migrants who need to survive a crisis in their host\ncountries. The current paper presents an interview study with two cohorts of\nChinese migrants living in Japan (N=14) and the United States (N=14).\nParticipants reflected on their information seeking experiences during the\nCOVID pandemic. The reflection was supplemented by two weeks of self-tracking\nwhere participants maintained records of their COVIDrelated information seeking\npractice. Our data indicated that participants often took language detours, or\nvisits to Mandarin resources for information about the COVID outbreak in their\nhost countries. They also made strategic use of the Mandarin information to\nperform selective reading, cross-checking, and contextualized interpretation of\nCOVID-related information in Japanese or English. While such practices enhanced\nparticipants' perceived effectiveness of COVID-related information gathering\nand sensemaking, they disadvantaged people through sometimes incognizant ways.\nFurther, participants lacked the awareness or preference to review\nmigrant-oriented information that was issued by the host country's public\nauthorities despite its availability. Building upon these findings, we\ndiscussed solutions to improve international migrants' COVID-related\ninformation seeking in their non-native language and cultural environment. We\nadvocated inclusive crisis infrastructures that would engage people with\ndiverse levels of local language fluency, information literacy, and experience\nin leveraging public services.", "published": "2022-09-07 03:28:48", "link": "http://arxiv.org/abs/2209.02903v2", "categories": ["cs.CY", "cs.CL", "cs.HC"], "primary_category": "cs.CY"}
{"title": "INFACT: An Online Human Evaluation Framework for Conversational\n  Recommendation", "abstract": "Conversational recommender systems (CRS) are interactive agents that support\ntheir users in recommendation-related goals through multi-turn conversations.\nGenerally, a CRS can be evaluated in various dimensions. Today's CRS mainly\nrely on offline(computational) measures to assess the performance of their\nalgorithms in comparison to different baselines. However, offline measures can\nhave limitations, for example, when the metrics for comparing a newly generated\nresponse with a ground truth do not correlate with human perceptions, because\nvarious alternative generated responses might be suitable too in a given dialog\nsituation. Current research on machine learning-based CRS models therefore\nacknowledges the importance of humans in the evaluation process, knowing that\npure offline measures may not be sufficient in evaluating a highly interactive\nsystem like a CRS.", "published": "2022-09-07 15:16:59", "link": "http://arxiv.org/abs/2209.03213v1", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Fact-Saboteurs: A Taxonomy of Evidence Manipulation Attacks against\n  Fact-Verification Systems", "abstract": "Mis- and disinformation are a substantial global threat to our security and\nsafety. To cope with the scale of online misinformation, researchers have been\nworking on automating fact-checking by retrieving and verifying against\nrelevant evidence. However, despite many advances, a comprehensive evaluation\nof the possible attack vectors against such systems is still lacking.\nParticularly, the automated fact-verification process might be vulnerable to\nthe exact disinformation campaigns it is trying to combat. In this work, we\nassume an adversary that automatically tampers with the online evidence in\norder to disrupt the fact-checking model via camouflaging the relevant evidence\nor planting a misleading one. We first propose an exploratory taxonomy that\nspans these two targets and the different threat model dimensions. Guided by\nthis, we design and propose several potential attack methods. We show that it\nis possible to subtly modify claim-salient snippets in the evidence and\ngenerate diverse and claim-aligned evidence. Thus, we highly degrade the\nfact-checking performance under many different permutations of the taxonomy's\ndimensions. The attacks are also robust against post-hoc modifications of the\nclaim. Our analysis further hints at potential limitations in models' inference\nwhen faced with contradicting evidence. We emphasize that these attacks can\nhave harmful implications on the inspectable and human-in-the-loop usage\nscenarios of such models, and we conclude by discussing challenges and\ndirections for future defenses.", "published": "2022-09-07 13:39:24", "link": "http://arxiv.org/abs/2209.03755v4", "categories": ["cs.CR", "cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Decoding Demographic un-fairness from Indian Names", "abstract": "Demographic classification is essential in fairness assessment in recommender\nsystems or in measuring unintended bias in online networks and voting systems.\nImportant fields like education and politics, which often lay a foundation for\nthe future of equality in society, need scrutiny to design policies that can\nbetter foster equality in resource distribution constrained by the unbalanced\ndemographic distribution of people in the country.\n  We collect three publicly available datasets to train state-of-the-art\nclassifiers in the domain of gender and caste classification. We train the\nmodels in the Indian context, where the same name can have different styling\nconventions (Jolly Abraham/Kumar Abhishikta in one state may be written as\nAbraham Jolly/Abishikta Kumar in the other). Finally, we also perform\ncross-testing (training and testing on different datasets) to understand the\nefficacy of the above models.\n  We also perform an error analysis of the prediction models. Finally, we\nattempt to assess the bias in the existing Indian system as case studies and\nfind some intriguing patterns manifesting in the complex demographic layout of\nthe sub-continent across the dimensions of gender and caste.", "published": "2022-09-07 11:54:49", "link": "http://arxiv.org/abs/2209.03089v1", "categories": ["cs.CY", "cs.CL", "cs.DL", "cs.LG", "cs.SI", "J.4; K.4.1"], "primary_category": "cs.CY"}
{"title": "DM$^2$S$^2$: Deep Multi-Modal Sequence Sets with Hierarchical Modality\n  Attention", "abstract": "There is increasing interest in the use of multimodal data in various web\napplications, such as digital advertising and e-commerce. Typical methods for\nextracting important information from multimodal data rely on a mid-fusion\narchitecture that combines the feature representations from multiple encoders.\nHowever, as the number of modalities increases, several potential problems with\nthe mid-fusion model structure arise, such as an increase in the dimensionality\nof the concatenated multimodal features and missing modalities. To address\nthese problems, we propose a new concept that considers multimodal inputs as a\nset of sequences, namely, deep multimodal sequence sets (DM$^2$S$^2$). Our\nset-aware concept consists of three components that capture the relationships\namong multiple modalities: (a) a BERT-based encoder to handle the inter- and\nintra-order of elements in the sequences, (b) intra-modality residual attention\n(IntraMRA) to capture the importance of the elements in a modality, and (c)\ninter-modality residual attention (InterMRA) to enhance the importance of\nelements with modality-level granularity further. Our concept exhibits\nperformance that is comparable to or better than the previous set-aware models.\nFurthermore, we demonstrate that the visualization of the learned InterMRA and\nIntraMRA weights can provide an interpretation of the prediction results.", "published": "2022-09-07 13:25:09", "link": "http://arxiv.org/abs/2209.03126v2", "categories": ["cs.MM", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.MM"}
{"title": "Foundations and Trends in Multimodal Machine Learning: Principles,\n  Challenges, and Open Questions", "abstract": "Multimodal machine learning is a vibrant multi-disciplinary research field\nthat aims to design computer agents with intelligent capabilities such as\nunderstanding, reasoning, and learning through integrating multiple\ncommunicative modalities, including linguistic, acoustic, visual, tactile, and\nphysiological messages. With the recent interest in video understanding,\nembodied autonomous agents, text-to-image generation, and multisensor fusion in\napplication domains such as healthcare and robotics, multimodal machine\nlearning has brought unique computational and theoretical challenges to the\nmachine learning community given the heterogeneity of data sources and the\ninterconnections often found between modalities. However, the breadth of\nprogress in multimodal research has made it difficult to identify the common\nthemes and open questions in the field. By synthesizing a broad range of\napplication domains and theoretical frameworks from both historical and recent\nperspectives, this paper is designed to provide an overview of the\ncomputational and theoretical foundations of multimodal machine learning. We\nstart by defining three key principles of modality heterogeneity, connections,\nand interactions that have driven subsequent innovations, and propose a\ntaxonomy of six core technical challenges: representation, alignment,\nreasoning, generation, transference, and quantification covering historical and\nrecent trends. Recent technical achievements will be presented through the lens\nof this taxonomy, allowing researchers to understand the similarities and\ndifferences across new approaches. We end by motivating several open problems\nfor future research as identified by our taxonomy.", "published": "2022-09-07 19:21:19", "link": "http://arxiv.org/abs/2209.03430v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.MM"], "primary_category": "cs.LG"}
{"title": "Improving Choral Music Separation through Expressive Synthesized Data\n  from Sampled Instruments", "abstract": "Choral music separation refers to the task of extracting tracks of voice\nparts (e.g., soprano, alto, tenor, and bass) from mixed audio. The lack of\ndatasets has impeded research on this topic as previous work has only been able\nto train and evaluate models on a few minutes of choral music data due to\ncopyright issues and dataset collection difficulties. In this paper, we\ninvestigate the use of synthesized training data for the source separation task\non real choral music. We make three contributions: first, we provide an\nautomated pipeline for synthesizing choral music data from sampled instrument\nplugins within controllable options for instrument expressiveness. This\nproduces an 8.2-hour-long choral music dataset from the JSB Chorales Dataset\nand one can easily synthesize additional data. Second, we conduct an experiment\nto evaluate multiple separation models on available choral music separation\ndatasets from previous work. To the best of our knowledge, this is the first\nexperiment to comprehensively evaluate choral music separation. Third,\nexperiments demonstrate that the synthesized choral data is of sufficient\nquality to improve the model's performance on real choral music datasets. This\nprovides additional experimental statistics and data support for the choral\nmusic separation study.", "published": "2022-09-07 01:13:59", "link": "http://arxiv.org/abs/2209.02871v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AudioLM: a Language Modeling Approach to Audio Generation", "abstract": "We introduce AudioLM, a framework for high-quality audio generation with\nlong-term consistency. AudioLM maps the input audio to a sequence of discrete\ntokens and casts audio generation as a language modeling task in this\nrepresentation space. We show how existing audio tokenizers provide different\ntrade-offs between reconstruction quality and long-term structure, and we\npropose a hybrid tokenization scheme to achieve both objectives. Namely, we\nleverage the discretized activations of a masked language model pre-trained on\naudio to capture long-term structure and the discrete codes produced by a\nneural audio codec to achieve high-quality synthesis. By training on large\ncorpora of raw audio waveforms, AudioLM learns to generate natural and coherent\ncontinuations given short prompts. When trained on speech, and without any\ntranscript or annotation, AudioLM generates syntactically and semantically\nplausible speech continuations while also maintaining speaker identity and\nprosody for unseen speakers. Furthermore, we demonstrate how our approach\nextends beyond speech by generating coherent piano music continuations, despite\nbeing trained without any symbolic representation of music.", "published": "2022-09-07 13:40:08", "link": "http://arxiv.org/abs/2209.03143v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multimodal Speech Enhancement Using Burst Propagation", "abstract": "This paper proposes the MBURST, a novel multimodal solution for audio-visual\nspeech enhancements that consider the most recent neurological discoveries\nregarding pyramidal cells of the prefrontal cortex and other brain regions. The\nso-called burst propagation implements several criteria to address the credit\nassignment problem in a more biologically plausible manner: steering the sign\nand magnitude of plasticity through feedback, multiplexing the feedback and\nfeedforward information across layers through different weight connections,\napproximating feedback and feedforward connections, and linearizing the\nfeedback signals. MBURST benefits from such capabilities to learn correlations\nbetween the noisy signal and the visual stimuli, thus attributing meaning to\nthe speech by amplifying relevant information and suppressing noise.\nExperiments conducted over a Grid Corpus and CHiME3-based dataset show that\nMBURST can reproduce similar mask reconstructions to the multimodal\nbackpropagation-based baseline while demonstrating outstanding energy\nefficiency management, reducing the neuron firing rates to values up to\n\\textbf{$70\\%$} lower. Such a feature implies more sustainable implementations,\nsuitable and desirable for hearing aids or any other similar embedded systems.", "published": "2022-09-07 16:27:34", "link": "http://arxiv.org/abs/2209.03275v3", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ESSYS* Sharing #UC: An Emotion-driven Audiovisual Installation", "abstract": "We present ESSYS* Sharing #UC, an audiovisual installation artwork that\nreflects upon the emotional context related to the university and the city of\nCoimbra, based on the data shared about them on Twitter. The installation was\npresented in an urban art gallery of C\\'irculo de Artes Pl\\'asticas de Coimbra\nduring the summer and autumn of 2021. In the installation space, one may see a\ncollection of typographic posters displaying the tweets and listening to an\never-changing ambient sound. The present audiovisuals are created by an\nautonomous computational creative approach, which employs a neural classifier\nto recognize the emotional context of a tweet and uses this resulting data as\nfeedstock for the audiovisual generation. The installation's space is designed\nto promote an approach and blend between the online and physical perceptions of\nthe same location. We applied multiple experiments with the proposed approach\nto evaluate the capability and performance. Also, we conduct interview-based\nevaluation sessions to understand how the installation elements, especially\nposter designs, are experienced by people regarding diversity, expressiveness\nand possible employment in other commercial and social scenarios.", "published": "2022-09-07 17:51:21", "link": "http://arxiv.org/abs/2209.03338v1", "categories": ["cs.MM", "cs.HC", "cs.IR", "cs.SD", "eess.AS", "H.4.m; H.5.1; H.5.5"], "primary_category": "cs.MM"}
{"title": "Modeling Dependent Structure for Utterances in ASR Evaluation", "abstract": "The bootstrap resampling method has been popular for performing significance\nanalysis on word error rate (WER) in automatic speech recognition (ASR)\nevaluation. To deal with dependent speech data, the blockwise bootstrap\napproach is also introduced. By dividing utterances into uncorrelated blocks,\nthis approach resamples these blocks instead of original data. However, it is\ntypically nontrivial to uncover the dependent structure among utterances and\nidentify the blocks, which might lead to subjective conclusions in statistical\ntesting. In this paper, we present graphical lasso based methods to explicitly\nmodel such dependency and estimate uncorrelated blocks of utterances in a\nrigorous way, after which blockwise bootstrap is applied on top of the inferred\nblocks. We show the resulting variance estimator of WER in ASR evaluation is\nstatistically consistent under mild conditions. We also demonstrate the\nvalidity of proposed approach on LibriSpeech dataset.", "published": "2022-09-07 21:51:06", "link": "http://arxiv.org/abs/2209.05281v2", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
