{"title": "Local Contextual Attention with Hierarchical Structure for Dialogue Act\n  Recognition", "abstract": "Dialogue act recognition is a fundamental task for an intelligent dialogue\nsystem. Previous work models the whole dialog to predict dialog acts, which may\nbring the noise from unrelated sentences. In this work, we design a\nhierarchical model based on self-attention to capture intra-sentence and\ninter-sentence information. We revise the attention distribution to focus on\nthe local and contextual semantic information by incorporating the relative\nposition information between utterances. Based on the found that the length of\ndialog affects the performance, we introduce a new dialog segmentation\nmechanism to analyze the effect of dialog length and context padding length\nunder online and offline settings. The experiment shows that our method\nachieves promising performance on two datasets: Switchboard Dialogue Act and\nDailyDialog with the accuracy of 80.34\\% and 85.81\\% respectively.\nVisualization of the attention weights shows that our method can learn the\ncontext dependency between utterances explicitly.", "published": "2020-03-12 22:26:11", "link": "http://arxiv.org/abs/2003.06044v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Medical Scribe: Corpus Development and Model Performance Analyses", "abstract": "There is a growing interest in creating tools to assist in clinical note\ngeneration using the audio of provider-patient encounters. Motivated by this\ngoal and with the help of providers and medical scribes, we developed an\nannotation scheme to extract relevant clinical concepts. We used this\nannotation scheme to label a corpus of about 6k clinical encounters. This was\nused to train a state-of-the-art tagging model. We report ontologies, labeling\nresults, model performances, and detailed analyses of the results. Our results\nshow that the entities related to medications can be extracted with a\nrelatively high accuracy of 0.90 F-score, followed by symptoms at 0.72 F-score,\nand conditions at 0.57 F-score. In our task, we not only identify where the\nsymptoms are mentioned but also map them to canonical forms as they appear in\nthe clinical notes. Of the different types of errors, in about 19-38% of the\ncases, we find that the model output was correct, and about 17-32% of the\nerrors do not impact the clinical note. Taken together, the models developed in\nthis work are more useful than the F-scores reflect, making it a promising\napproach for practical applications.", "published": "2020-03-12 03:10:25", "link": "http://arxiv.org/abs/2003.11531v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning word-referent mappings and concepts from raw inputs", "abstract": "How do children learn correspondences between the language and the world from\nnoisy, ambiguous, naturalistic input? One hypothesis is via cross-situational\nlearning: tracking words and their possible referents across multiple\nsituations allows learners to disambiguate correct word-referent mappings (Yu &\nSmith, 2007). However, previous models of cross-situational word learning\noperate on highly simplified representations, side-stepping two important\naspects of the actual learning problem. First, how can word-referent mappings\nbe learned from raw inputs such as images? Second, how can these learned\nmappings generalize to novel instances of a known word? In this paper, we\npresent a neural network model trained from scratch via self-supervision that\ntakes in raw images and words as inputs, and show that it can learn\nword-referent mappings from fully ambiguous scenes and utterances through\ncross-situational learning. In addition, the model generalizes to novel word\ninstances, locates referents of words in a scene, and shows a preference for\nmutual exclusivity.", "published": "2020-03-12 02:18:19", "link": "http://arxiv.org/abs/2003.05573v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "It Means More if It Sounds Good: Yet Another Hypothesis Concerning the\n  Evolution of Polysemous Words", "abstract": "This position paper looks into the formation of language and shows ties\nbetween structural properties of the words in the English language and their\npolysemy. Using Ollivier-Ricci curvature over a large graph of synonyms to\nestimate polysemy it shows empirically that the words that arguably are easier\nto pronounce also tend to have multiple meanings.", "published": "2020-03-12 12:55:50", "link": "http://arxiv.org/abs/2003.05758v2", "categories": ["cs.CL", "math.MG", "68U15, 68R10", "G.2.2; J.5"], "primary_category": "cs.CL"}
{"title": "Regular Intersection Emptiness of Graph Problems: Finding a Needle in a\n  Haystack of Graphs with the Help of Automata", "abstract": "The Int_reg-problem of a combinatorial problem P asks, given a\nnondeterministic automaton M as input, whether the language L(M) accepted by M\ncontains any positive instance of the problem P. We consider the\nInt_reg-problem for a number of different graph problems and give general\ncriteria that give decision procedures for these Int_reg-problems. To achieve\nthis goal, we consider a natural graph encoding so that the language of all\ngraph encodings is regular. Then, we draw the connection between classical\npumping- and interchange-arguments from the field of formal language theory\nwith the graph operations induced on the encoded graph. Our techniques apply\namong others to the Int_reg-problem of well-known graph problems like Vertex\nCover and Independent Set, as well as to subgraph problems, graph-edit problems\nand graph-partitioning problems, including coloring problems.", "published": "2020-03-12 14:51:41", "link": "http://arxiv.org/abs/2003.05826v1", "categories": ["cs.FL", "cs.CL"], "primary_category": "cs.FL"}
{"title": "CRWIZ: A Framework for Crowdsourcing Real-Time Wizard-of-Oz Dialogues", "abstract": "Large corpora of task-based and open-domain conversational dialogues are\nhugely valuable in the field of data-driven dialogue systems. Crowdsourcing\nplatforms, such as Amazon Mechanical Turk, have been an effective method for\ncollecting such large amounts of data. However, difficulties arise when\ntask-based dialogues require expert domain knowledge or rapid access to\ndomain-relevant information, such as databases for tourism. This will become\neven more prevalent as dialogue systems become increasingly ambitious,\nexpanding into tasks with high levels of complexity that require collaboration\nand forward planning, such as in our domain of emergency response. In this\npaper, we propose CRWIZ: a framework for collecting real-time Wizard of Oz\ndialogues through crowdsourcing for collaborative, complex tasks. This\nframework uses semi-guided dialogue to avoid interactions that breach\nprocedures and processes only known to experts, while enabling the capture of a\nwide variety of interactions. The framework is available at\nhttps://github.com/JChiyah/crwiz", "published": "2020-03-12 19:47:29", "link": "http://arxiv.org/abs/2003.05995v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Sentiment Analysis with Contextual Embeddings and Self-Attention", "abstract": "In natural language the intended meaning of a word or phrase is often\nimplicit and depends on the context. In this work, we propose a simple yet\neffective method for sentiment analysis using contextual embeddings and a\nself-attention mechanism. The experimental results for three languages,\nincluding morphologically rich Polish and German, show that our model is\ncomparable to or even outperforms state-of-the-art models. In all cases the\nsuperiority of models leveraging contextual embeddings is demonstrated.\nFinally, this work is intended as a step towards introducing a universal,\nmultilingual sentiment classifier.", "published": "2020-03-12 02:19:51", "link": "http://arxiv.org/abs/2003.05574v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Heterogeneous Relational Reasoning in Knowledge Graphs with\n  Reinforcement Learning", "abstract": "Path-based relational reasoning over knowledge graphs has become increasingly\npopular due to a variety of downstream applications such as question answering\nin dialogue systems, fact prediction, and recommender systems. In recent years,\nreinforcement learning (RL) has provided solutions that are more interpretable\nand explainable than other deep learning models. However, these solutions still\nface several challenges, including large action space for the RL agent and\naccurate representation of entity neighborhood structure. We address these\nproblems by introducing a type-enhanced RL agent that uses the local\nneighborhood information for efficient path-based reasoning over knowledge\ngraphs. Our solution uses graph neural network (GNN) for encoding the\nneighborhood information and utilizes entity types to prune the action space.\nExperiments on real-world dataset show that our method outperforms\nstate-of-the-art RL methods and discovers more novel paths during the training\nprocedure.", "published": "2020-03-12 22:39:58", "link": "http://arxiv.org/abs/2003.06050v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Hybrid Autoregressive Transducer (hat)", "abstract": "This paper proposes and evaluates the hybrid autoregressive transducer (HAT)\nmodel, a time-synchronous encoderdecoder model that preserves the modularity of\nconventional automatic speech recognition systems. The HAT model provides a way\nto measure the quality of the internal language model that can be used to\ndecide whether inference with an external language model is beneficial or not.\nThis article also presents a finite context version of the HAT model that\naddresses the exposure bias problem and significantly simplifies the overall\ntraining and inference. We evaluate our proposed model on a large-scale voice\nsearch task. Our experiments show significant improvements in WER compared to\nthe state-of-the-art approaches.", "published": "2020-03-12 20:47:06", "link": "http://arxiv.org/abs/2003.07705v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Meta-CoTGAN: A Meta Cooperative Training Paradigm for Improving\n  Adversarial Text Generation", "abstract": "Training generative models that can generate high-quality text with\nsufficient diversity is an important open problem for Natural Language\nGeneration (NLG) community. Recently, generative adversarial models have been\napplied extensively on text generation tasks, where the adversarially trained\ngenerators alleviate the exposure bias experienced by conventional maximum\nlikelihood approaches and result in promising generation quality. However, due\nto the notorious defect of mode collapse for adversarial training, the\nadversarially trained generators face a quality-diversity trade-off, i.e., the\ngenerator models tend to sacrifice generation diversity severely for increasing\ngeneration quality. In this paper, we propose a novel approach which aims to\nimprove the performance of adversarial text generation via efficiently\ndecelerating mode collapse of the adversarial training. To this end, we\nintroduce a cooperative training paradigm, where a language model is\ncooperatively trained with the generator and we utilize the language model to\nefficiently shape the data distribution of the generator against mode collapse.\nMoreover, instead of engaging the cooperative update for the generator in a\nprincipled way, we formulate a meta learning mechanism, where the cooperative\nupdate to the generator serves as a high level meta task, with an intuition of\nensuring the parameters of the generator after the adversarial update would\nstay resistant against mode collapse. In the experiment, we demonstrate our\nproposed approach can efficiently slow down the pace of mode collapse for the\nadversarial text generators. Overall, our proposed method is able to outperform\nthe baseline approaches with significant margins in terms of both generation\nquality and diversity in the testified domains.", "published": "2020-03-12 04:47:52", "link": "http://arxiv.org/abs/2003.11530v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Bringing in the outliers: A sparse subspace clustering approach to learn\n  a dictionary of mouse ultrasonic vocalizations", "abstract": "Mice vocalize in the ultrasonic range during social interactions. These\nvocalizations are used in neuroscience and clinical studies to tap into complex\nbehaviors and states. The analysis of these ultrasonic vocalizations (USVs) has\nbeen traditionally a manual process, which is prone to errors and human bias,\nand is not scalable to large scale analysis. We propose a new method to\nautomatically create a dictionary of USVs based on a two-step spectral\nclustering approach, where we split the set of USVs into inlier and outlier\ndata sets. This approach is motivated by the known degrading performance of\nsparse subspace clustering with outliers. We apply spectral clustering to the\ninlier data set and later find the clusters for the outliers. We propose\nquantitative and qualitative performance measures to evaluate our method in\nthis setting, where there is no ground truth. Our approach outperforms two\nbaselines based on k-means and spectral clustering in all of the proposed\nperformance measures, showing greater distances between clusters and more\nvariability between clusters.", "published": "2020-03-12 16:55:37", "link": "http://arxiv.org/abs/2003.05897v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Efficient Content-Based Sparse Attention with Routing Transformers", "abstract": "Self-attention has recently been adopted for a wide range of sequence\nmodeling problems. Despite its effectiveness, self-attention suffers from\nquadratic compute and memory requirements with respect to sequence length.\nSuccessful approaches to reduce this complexity focused on attending to local\nsliding windows or a small set of locations independent of content. Our work\nproposes to learn dynamic sparse attention patterns that avoid allocating\ncomputation and memory to attend to content unrelated to the query of interest.\nThis work builds upon two lines of research: it combines the modeling\nflexibility of prior work on content-based sparse attention with the efficiency\ngains from approaches based on local, temporal sparse attention. Our model, the\nRouting Transformer, endows self-attention with a sparse routing module based\non online k-means while reducing the overall complexity of attention to\n$O\\left(n^{1.5}d\\right)$ from $O\\left(n^2d\\right)$ for sequence length $n$ and\nhidden dimension $d$. We show that our model outperforms comparable sparse\nattention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity)\nas well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while\nusing fewer self-attention layers. Additionally, we set a new state-of-the-art\non the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with\na 22 layer Routing Transformer model trained on sequences of length 8192.", "published": "2020-03-12 19:50:14", "link": "http://arxiv.org/abs/2003.05997v5", "categories": ["cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
