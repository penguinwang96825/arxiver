{"title": "Fast and Simple Mixture of Softmaxes with BPE and Hybrid-LightRNN for\n  Language Generation", "abstract": "Mixture of Softmaxes (MoS) has been shown to be effective at addressing the\nexpressiveness limitation of Softmax-based models. Despite the known advantage,\nMoS is practically sealed by its large consumption of memory and computational\ntime due to the need of computing multiple Softmaxes. In this work, we set out\nto unleash the power of MoS in practical applications by investigating improved\nword coding schemes, which could effectively reduce the vocabulary size and\nhence relieve the memory and computation burden. We show both BPE and our\nproposed Hybrid-LightRNN lead to improved encoding mechanisms that can halve\nthe time and memory consumption of MoS without performance losses. With MoS, we\nachieve an improvement of 1.5 BLEU scores on IWSLT 2014 German-to-English\ncorpus and an improvement of 0.76 CIDEr score on image captioning. Moreover, on\nthe larger WMT 2014 machine translation dataset, our MoS-boosted Transformer\nyields 29.5 BLEU score for English-to-German and 42.1 BLEU score for\nEnglish-to-French, outperforming the single-Softmax Transformer by 0.8 and 0.4\nBLEU scores respectively and achieving the state-of-the-art result on WMT 2014\nEnglish-to-German task.", "published": "2018-09-25 03:02:38", "link": "http://arxiv.org/abs/1809.09296v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ComQA: A Community-sourced Dataset for Complex Factoid Question\n  Answering with Paraphrase Clusters", "abstract": "To bridge the gap between the capabilities of the state-of-the-art in factoid\nquestion answering (QA) and what users ask, we need large datasets of real user\nquestions that capture the various question phenomena users are interested in,\nand the diverse ways in which these questions are formulated. We introduce\nComQA, a large dataset of real user questions that exhibit different\nchallenging aspects such as compositionality, temporal reasoning, and\ncomparisons. ComQA questions come from the WikiAnswers community QA platform,\nwhich typically contains questions that are not satisfactorily answerable by\nexisting search engine technology. Through a large crowdsourcing effort, we\nclean the question dataset, group questions into paraphrase clusters, and\nannotate clusters with their answers. ComQA contains 11,214 questions grouped\ninto 4,834 paraphrase clusters. We detail the process of constructing ComQA,\nincluding the measures taken to ensure its high quality while making effective\nuse of crowdsourcing. We also present an extensive analysis of the dataset and\nthe results achieved by state-of-the-art systems on ComQA, demonstrating that\nour dataset can be a driver of future research on QA.", "published": "2018-09-25 14:54:26", "link": "http://arxiv.org/abs/1809.09528v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question\n  Answering", "abstract": "Existing question answering (QA) datasets fail to train QA systems to perform\ncomplex reasoning and provide explanations for answers. We introduce HotpotQA,\na new dataset with 113k Wikipedia-based question-answer pairs with four key\nfeatures: (1) the questions require finding and reasoning over multiple\nsupporting documents to answer; (2) the questions are diverse and not\nconstrained to any pre-existing knowledge bases or knowledge schemas; (3) we\nprovide sentence-level supporting facts required for reasoning, allowing QA\nsystems to reason with strong supervision and explain the predictions; (4) we\noffer a new type of factoid comparison questions to test QA systems' ability to\nextract relevant facts and perform necessary comparison. We show that HotpotQA\nis challenging for the latest QA systems, and the supporting facts enable\nmodels to improve performance and make explainable predictions.", "published": "2018-09-25 17:28:20", "link": "http://arxiv.org/abs/1809.09600v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Re-ranker Scheme for Integrating Large Scale NLU models", "abstract": "Large scale Natural Language Understanding (NLU) systems are typically\ntrained on large quantities of data, requiring a fast and scalable training\nstrategy. A typical design for NLU systems consists of domain-level NLU modules\n(domain classifier, intent classifier and named entity recognizer). Hypotheses\n(NLU interpretations consisting of various intent+slot combinations) from these\ndomain specific modules are typically aggregated with another downstream\ncomponent. The re-ranker integrates outputs from domain-level recognizers,\nreturning a scored list of cross domain hypotheses. An ideal re-ranker will\nexhibit the following two properties: (a) it should prefer the most relevant\nhypothesis for the given input as the top hypothesis and, (b) the\ninterpretation scores corresponding to each hypothesis produced by the\nre-ranker should be calibrated. Calibration allows the final NLU interpretation\nscore to be comparable across domains. We propose a novel re-ranker strategy\nthat addresses these aspects, while also maintaining domain specific\nmodularity. We design optimization loss functions for such a modularized\nre-ranker and present results on decreasing the top hypothesis error rate as\nwell as maintaining the model calibration. We also experiment with an extension\ninvolving training the domain specific re-rankers on datasets curated\nindependently by each domain to allow further asynchronization. %The proposed\nre-ranker design showcases the following: (i) improved NLU performance over an\nunweighted aggregation strategy, (ii) cross-domain calibrated performance and,\n(iii) support for use cases involving training each re-ranker on datasets\ncurated by each domain independently.", "published": "2018-09-25 17:35:57", "link": "http://arxiv.org/abs/1809.09605v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Non-native children speech recognition through transfer learning", "abstract": "This work deals with non-native children's speech and investigates both\nmulti-task and transfer learning approaches to adapt a multi-language Deep\nNeural Network (DNN) to speakers, specifically children, learning a foreign\nlanguage. The application scenario is characterized by young students learning\nEnglish and German and reading sentences in these second-languages, as well as\nin their mother language. The paper analyzes and discusses techniques for\ntraining effective DNN-based acoustic models starting from children native\nspeech and performing adaptation with limited non-native audio material. A\nmulti-lingual model is adopted as baseline, where a common phonetic lexicon,\ndefined in terms of the units of the International Phonetic Alphabet (IPA), is\nshared across the three languages at hand (Italian, German and English); DNN\nadaptation methods based on transfer learning are evaluated on significant\nnon-native evaluation sets. Results show that the resulting non-native models\nallow a significant improvement with respect to a mono-lingual system adapted\nto speakers of the target language.", "published": "2018-09-25 18:50:39", "link": "http://arxiv.org/abs/1809.09658v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BanditSum: Extractive Summarization as a Contextual Bandit", "abstract": "In this work, we propose a novel method for training neural networks to\nperform single-document extractive summarization without\nheuristically-generated extractive labels. We call our approach BanditSum as it\ntreats extractive summarization as a contextual bandit (CB) problem, where the\nmodel receives a document to summarize (the context), and chooses a sequence of\nsentences to include in the summary (the action). A policy gradient\nreinforcement learning algorithm is used to train the model to select sequences\nof sentences that maximize ROUGE score. We perform a series of experiments\ndemonstrating that BanditSum is able to achieve ROUGE scores that are better\nthan or comparable to the state-of-the-art for extractive summarization, and\nconverges using significantly fewer update steps than competing approaches. In\naddition, we show empirically that BanditSum performs significantly better than\ncompeting approaches when good summary sentences appear late in the source\ndocument.", "published": "2018-09-25 19:18:52", "link": "http://arxiv.org/abs/1809.09672v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Chinese User Service Intention Classification Based on Hybrid Neural\n  Network", "abstract": "In order to satisfy the consumers' increasing personalized service demand,\nthe Intelligent service has arisen. User service intention recognition is an\nimportant challenge for intelligent service system to provide precise service.\nIt is difficult for the intelligent system to understand the semantics of user\ndemand which leads to poor recognition effect, because of the noise in user\nrequirement descriptions. Therefore, a hybrid neural network classification\nmodel based on BiLSTM and CNN is proposed to recognize users service\nintentions. The model can fuse the temporal semantics and spatial semantics of\nthe user descriptions. The experimental results show that our model achieves a\nbetter effect compared with other models, reaching 0.94 on the F1 score.", "published": "2018-09-25 10:56:20", "link": "http://arxiv.org/abs/1809.09408v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Coordinating and Integrating Faceted Classification with Rich Semantic\n  Modeling", "abstract": "Faceted classifications define dimensions for the types of entities included.\nIn effect, the facets provide an \"ontological commitment\". We compare a faceted\nthesaurus, the Art and Architecture Thesaurus (AAT), with ontologies derived\nfrom the Basic Formal Ontology (BFO2), which is an upper (or formal) ontology\nwidely used to describe entities in biomedicine. We consider how the AAT and\nBFO2-based ontologies could be coordinated and integrated into a Human Activity\nand Infrastructure Foundry (HAIF). To extend the AAT to enable this\ncoordination and integration, we describe how a wider range of relationships\namong its terms could be introduced. Using these extensions, we explore richer\nmodeling of topics from AAT that deal with Technology. Finally, we consider how\nontology-based frames and semantic role frames can be integrated to make rich\nsemantic statements about changes in the world.", "published": "2018-09-25 15:29:11", "link": "http://arxiv.org/abs/1809.09548v1", "categories": ["cs.DL", "cs.CL"], "primary_category": "cs.DL"}
{"title": "Attention Mechanism in Speaker Recognition: What Does It Learn in Deep\n  Speaker Embedding?", "abstract": "This paper presents an experimental study on deep speaker embedding with an\nattention mechanism that has been found to be a powerful representation\nlearning technique in speaker recognition. In this framework, an attention\nmodel works as a frame selector that computes an attention weight for each\nframe-level feature vector, in accord with which an utterancelevel\nrepresentation is produced at the pooling layer in a speaker embedding network.\nIn general, an attention model is trained together with the speaker embedding\nnetwork on a single objective function, and thus those two components are\ntightly bound to one another. In this paper, we consider the possibility that\nthe attention model might be decoupled from its parent network and assist other\nspeaker embedding networks and even conventional i-vector extractors. This\npossibility is demonstrated through a series of experiments on a NIST Speaker\nRecognition Evaluation (SRE) task, with 9.0% EER reduction and 3.8%\nmin_Cprimary reduction when the attention weights are applied to i-vector\nextraction. Another experiment shows that DNN-based soft voice activity\ndetection (VAD) can be effectively combined with the attention mechanism to\nyield further reduction of minCprimary by 6.6% and 1.6% in deep speaker\nembedding and i-vector systems, respectively.", "published": "2018-09-25 04:12:18", "link": "http://arxiv.org/abs/1809.09311v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "An Exploration of Mimic Architectures for Residual Network Based\n  Spectral Mapping", "abstract": "Spectral mapping uses a deep neural network (DNN) to map directly from noisy\nspeech to clean speech. Our previous study found that the performance of\nspectral mapping improves greatly when using helpful cues from an acoustic\nmodel trained on clean speech. The mapper network learns to mimic the input\nfavored by the spectral classifier and cleans the features accordingly. In this\nstudy, we explore two new innovations: we replace a DNN-based spectral mapper\nwith a residual network that is more attuned to the goal of predicting clean\nspeech. We also examine how integrating long term context in the mimic\ncriterion (via wide-residual biLSTM networks) affects the performance of\nspectral mapping compared to DNNs. Our goal is to derive a model that can be\nused as a preprocessor for any recognition system; the features derived from\nour model are passed through the standard Kaldi ASR pipeline and achieve a WER\nof 9.3%, which is the lowest recorded word error rate for CHiME-2 dataset using\nonly feature adaptation.", "published": "2018-09-25 23:20:32", "link": "http://arxiv.org/abs/1809.09756v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "WaveCycleGAN: Synthetic-to-natural speech waveform conversion using\n  cycle-consistent adversarial networks", "abstract": "We propose a learning-based filter that allows us to directly modify a\nsynthetic speech waveform into a natural speech waveform. Speech-processing\nsystems using a vocoder framework such as statistical parametric speech\nsynthesis and voice conversion are convenient especially for a limited number\nof data because it is possible to represent and process interpretable acoustic\nfeatures over a compact space, such as the fundamental frequency (F0) and\nmel-cepstrum. However, a well-known problem that leads to the quality\ndegradation of generated speech is an over-smoothing effect that eliminates\nsome detailed structure of generated/converted acoustic features. To address\nthis issue, we propose a synthetic-to-natural speech waveform conversion\ntechnique that uses cycle-consistent adversarial networks and which does not\nrequire any explicit assumption about speech waveform in adversarial learning.\nIn contrast to current techniques, since our modification is performed at the\nwaveform level, we expect that the proposed method will also make it possible\nto generate `vocoder-less' sounding speech even if the input speech is\nsynthesized using a vocoder framework. The experimental results demonstrate\nthat our proposed method can 1) alleviate the over-smoothing effect of the\nacoustic features despite the direct modification method used for the waveform\nand 2) greatly improve the naturalness of the generated speech sounds.", "published": "2018-09-25 13:03:43", "link": "http://arxiv.org/abs/1809.10288v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
