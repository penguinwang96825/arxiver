{"title": "Multi-task Learning for Low-resource Second Language Acquisition\n  Modeling", "abstract": "Second language acquisition (SLA) modeling is to predict whether second\nlanguage learners could correctly answer the questions according to what they\nhave learned. It is a fundamental building block of the personalized learning\nsystem and has attracted more and more attention recently. However, as far as\nwe know, almost all existing methods cannot work well in low-resource scenarios\ndue to lacking of training data. Fortunately, there are some latent common\npatterns among different language-learning tasks, which gives us an opportunity\nto solve the low-resource SLA modeling problem. Inspired by this idea, in this\npaper, we propose a novel SLA modeling method, which learns the latent common\npatterns among different language-learning datasets by multi-task learning and\nare further applied to improving the prediction performance in low-resource\nscenarios. Extensive experiments show that the proposed method performs much\nbetter than the state-of-the-art baselines in the low-resource scenario.\nMeanwhile, it also obtains improvement slightly in the non-low-resource\nscenario.", "published": "2019-08-25 09:05:06", "link": "http://arxiv.org/abs/1908.09283v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Domain Adaptive Text Style Transfer", "abstract": "Text style transfer without parallel data has achieved some practical\nsuccess. However, in the scenario where less data is available, these methods\nmay yield poor performance. In this paper, we examine domain adaptation for\ntext style transfer to leverage massively available data from other domains.\nThese data may demonstrate domain shift, which impedes the benefits of\nutilizing such data for training. To address this challenge, we propose simple\nyet effective domain adaptive text style transfer models, enabling\ndomain-adaptive information exchange. The proposed models presumably learn from\nthe source domain to: (i) distinguish stylized information and generic content\ninformation; (ii) maximally preserve content information; and (iii) adaptively\ntransfer the styles in a domain-aware manner. We evaluate the proposed models\non two style transfer tasks (sentiment and formality) over multiple target\ndomains where only limited non-parallel data is available. Extensive\nexperiments demonstrate the effectiveness of the proposed model compared to the\nbaselines.", "published": "2019-08-25 21:29:28", "link": "http://arxiv.org/abs/1908.09395v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Don't Just Scratch the Surface: Enhancing Word Representations for\n  Korean with Hanja", "abstract": "We propose a simple yet effective approach for improving Korean word\nrepresentations using additional linguistic annotation (i.e. Hanja). We employ\ncross-lingual transfer learning in training word representations by leveraging\nthe fact that Hanja is closely related to Chinese. We evaluate the intrinsic\nquality of representations learned through our approach using the word analogy\nand similarity tests. In addition, we demonstrate their effectiveness on\nseveral downstream tasks, including a novel Korean news headline generation\ntask.", "published": "2019-08-25 08:57:50", "link": "http://arxiv.org/abs/1908.09282v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multilingual Neural Machine Translation with Language Clustering", "abstract": "Multilingual neural machine translation (NMT), which translates multiple\nlanguages using a single model, is of great practical importance due to its\nadvantages in simplifying the training process, reducing online maintenance\ncosts, and enhancing low-resource and zero-shot translation. Given there are\nthousands of languages in the world and some of them are very different, it is\nextremely burdensome to handle them all in a single model or use a separate\nmodel for each language pair. Therefore, given a fixed resource budget, e.g.,\nthe number of models, how to determine which languages should be supported by\none model is critical to multilingual NMT, which, unfortunately, has been\nignored by previous work. In this work, we develop a framework that clusters\nlanguages into different groups and trains one multilingual model for each\ncluster. We study two methods for language clustering: (1) using prior\nknowledge, where we cluster languages according to language family, and (2)\nusing language embedding, in which we represent each language by an embedding\nvector and cluster them in the embedding space. In particular, we obtain the\nembedding vectors of all the languages by training a universal neural machine\ntranslation model. Our experiments on 23 languages show that the first\nclustering method is simple and easy to understand but leading to suboptimal\ntranslation accuracy, while the second method sufficiently captures the\nrelationship among languages well and improves the translation accuracy for\nalmost all the languages over baseline methods", "published": "2019-08-25 13:27:57", "link": "http://arxiv.org/abs/1908.09324v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Efficient Bidirectional Neural Machine Translation", "abstract": "The encoder-decoder based neural machine translation usually generates a\ntarget sequence token by token from left to right. Due to error propagation,\nthe tokens in the right side of the generated sequence are usually of poorer\nquality than those in the left side. In this paper, we propose an efficient\nmethod to generate a sequence in both left-to-right and right-to-left manners\nusing a single encoder and decoder, combining the advantages of both generation\ndirections. Experiments on three translation tasks show that our method\nachieves significant improvements over conventional unidirectional approach.\nCompared with ensemble methods that train and combine two models with different\ngeneration directions, our method saves 50% model parameters and about 40%\ntraining time, and also improve inference speed.", "published": "2019-08-25 13:59:03", "link": "http://arxiv.org/abs/1908.09329v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Patient Knowledge Distillation for BERT Model Compression", "abstract": "Pre-trained language models such as BERT have proven to be highly effective\nfor natural language processing (NLP) tasks. However, the high demand for\ncomputing resources in training such models hinders their application in\npractice. In order to alleviate this resource hunger in large-scale model\ntraining, we propose a Patient Knowledge Distillation approach to compress an\noriginal large model (teacher) into an equally-effective lightweight shallow\nnetwork (student). Different from previous knowledge distillation methods,\nwhich only use the output from the last layer of the teacher network for\ndistillation, our student model patiently learns from multiple intermediate\nlayers of the teacher model for incremental knowledge extraction, following two\nstrategies: ($i$) PKD-Last: learning from the last $k$ layers; and ($ii$)\nPKD-Skip: learning from every $k$ layers. These two patient distillation\nschemes enable the exploitation of rich information in the teacher's hidden\nlayers, and encourage the student model to patiently learn from and imitate the\nteacher through a multi-layer distillation process. Empirically, this\ntranslates into improved results on multiple NLP tasks with significant gain in\ntraining efficiency, without sacrificing model accuracy.", "published": "2019-08-25 16:13:24", "link": "http://arxiv.org/abs/1908.09355v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Transforming Delete, Retrieve, Generate Approach for Controlled Text\n  Style Transfer", "abstract": "Text style transfer is the task of transferring the style of text having\ncertain stylistic attributes, while preserving non-stylistic or content\ninformation. In this work we introduce the Generative Style Transformer (GST) -\na new approach to rewriting sentences to a target style in the absence of\nparallel style corpora. GST leverages the power of both, large unsupervised\npre-trained language models as well as the Transformer. GST is a part of a\nlarger `Delete Retrieve Generate' framework, in which we also propose a novel\nmethod of deleting style attributes from the source sentence by exploiting the\ninner workings of the Transformer. Our models outperform state-of-art systems\nacross 5 datasets on sentiment, gender and political slant transfer. We also\npropose the use of the GLEU metric as an automatic metric of evaluation of\nstyle transfer, which we found to compare better with human ratings than the\npredominantly used BLEU score.", "published": "2019-08-25 17:34:40", "link": "http://arxiv.org/abs/1908.09368v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On Measuring and Mitigating Biased Inferences of Word Embeddings", "abstract": "Word embeddings carry stereotypical connotations from the text they are\ntrained on, which can lead to invalid inferences in downstream models that rely\non them. We use this observation to design a mechanism for measuring\nstereotypes using the task of natural language inference. We demonstrate a\nreduction in invalid inferences via bias mitigation strategies on static word\nembeddings (GloVe). Further, we show that for gender bias, these techniques\nextend to contextualized embeddings when applied selectively only to the static\ncomponents of contextualized embeddings (ELMo, BERT).", "published": "2019-08-25 17:50:18", "link": "http://arxiv.org/abs/1908.09369v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Open Event Extraction from Online Text using a Generative Adversarial\n  Network", "abstract": "To extract the structured representations of open-domain events, Bayesian\ngraphical models have made some progress. However, these approaches typically\nassume that all words in a document are generated from a single event. While\nthis may be true for short text such as tweets, such an assumption does not\ngenerally hold for long text such as news articles. Moreover, Bayesian\ngraphical models often rely on Gibbs sampling for parameter inference which may\ntake long time to converge. To address these limitations, we propose an event\nextraction model based on Generative Adversarial Nets, called\nAdversarial-neural Event Model (AEM). AEM models an event with a Dirichlet\nprior and uses a generator network to capture the patterns underlying latent\nevents. A discriminator is used to distinguish documents reconstructed from the\nlatent events and the original documents. A byproduct of the discriminator is\nthat the features generated by the learned discriminator network allow the\nvisualization of the extracted events. Our model has been evaluated on two\nTwitter datasets and a news article dataset. Experimental results show that our\nmodel outperforms the baseline approaches on all the datasets, with more\nsignificant improvements observed on the news article dataset where an increase\nof 15\\% is observed in F-measure.", "published": "2019-08-25 03:17:38", "link": "http://arxiv.org/abs/1908.09246v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Method for Estimating the Proximity of Vector Representation Groups in\n  Multidimensional Space. On the Example of the Paraphrase Task", "abstract": "The following paper presents a method of comparing two sets of vectors. The\nmethod can be applied in all tasks, where it is necessary to measure the\ncloseness of two objects presented as sets of vectors. It may be applicable\nwhen we compare the meanings of two sentences as part of the problem of\nparaphrasing. This is the problem of measuring semantic similarity of two\nsentences (group of words). The existing methods are not sensible for the word\norder or syntactic connections in the considered sentences. The method appears\nto be advantageous because it neither presents a group of words as one scalar\nvalue, nor does it try to show the closeness through an aggregation vector,\nwhich is mean for the set of vectors. Instead of that we measure the cosine of\nthe angle as the mean for the first group vectors projections (the context) on\none side and each vector of the second group on the other side. The similarity\nof two sentences defined by these means does not lose any semantic\ncharacteristics and takes account of the words traits. The method was verified\non the comparison of sentence pairs in Russian.", "published": "2019-08-25 14:54:49", "link": "http://arxiv.org/abs/1908.09341v2", "categories": ["cs.LG", "cs.CL", "stat.ML", "I.2.6; I.2.7"], "primary_category": "cs.LG"}
{"title": "Empirical Study on Detecting Controversy in Social Media", "abstract": "Companies and financial investors are paying increasing attention to social\nconsciousness in developing their corporate strategies and making investment\ndecisions to support a sustainable economy for the future. Public discussion on\nincidents and events -- controversies -- of companies can provide valuable\ninsights on how well the company operates with regards to social consciousness\nand indicate the company's overall operational capability. However, there are\nchallenges in evaluating the degree of a company's social consciousness and\nenvironmental sustainability due to the lack of systematic data. We introduce a\nsystem that utilizes Twitter data to detect and monitor controversial events\nand show their impact on market volatility. In our study, controversial events\nare identified from clustered tweets that share the same 5W terms and sentiment\npolarities of these clusters. Credible news links inside the event tweets are\nused to validate the truth of the event. A case study on the Starbucks\nPhiladelphia arrests shows that this method can provide the desired\nfunctionality.", "published": "2019-08-25 18:36:55", "link": "http://arxiv.org/abs/1909.01093v1", "categories": ["cs.SI", "cs.CL", "cs.IR", "cs.LG", "stat.ML"], "primary_category": "cs.SI"}
