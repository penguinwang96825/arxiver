{"title": "Less is More for Long Document Summary Evaluation by LLMs", "abstract": "Large Language Models (LLMs) have shown promising performance in summary\nevaluation tasks, yet they face challenges such as high computational costs and\nthe Lost-in-the-Middle problem where important information in the middle of\nlong documents is often overlooked. To address these issues, this paper\nintroduces a novel approach, Extract-then-Evaluate, which involves extracting\nkey sentences from a long source document and then evaluating the summary by\nprompting LLMs. The results reveal that the proposed method not only\nsignificantly reduces evaluation costs but also exhibits a higher correlation\nwith human evaluations. Furthermore, we provide practical recommendations for\noptimal document length and sentence extraction methods, contributing to the\ndevelopment of cost-effective yet more accurate methods for LLM-based text\ngeneration evaluation.", "published": "2023-09-14 01:59:15", "link": "http://arxiv.org/abs/2309.07382v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Interactive Framework for Profiling News Media Sources", "abstract": "The recent rise of social media has led to the spread of large amounts of\nfake and biased news, content published with the intent to sway beliefs. While\ndetecting and profiling the sources that spread this news is important to\nmaintain a healthy society, it is challenging for automated systems.\n  In this paper, we propose an interactive framework for news media profiling.\nIt combines the strengths of graph based news media profiling models,\nPre-trained Large Language Models, and human insight to characterize the social\ncontext on social media. Experimental results show that with as little as 5\nhuman interactions, our framework can rapidly detect fake and biased news\nmedia, even in the most challenging settings of emerging news events, where\ntest data is unseen.", "published": "2023-09-14 02:03:45", "link": "http://arxiv.org/abs/2309.07384v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ChatGPT MT: Competitive for High- (but not Low-) Resource Languages", "abstract": "Large language models (LLMs) implicitly learn to perform a range of language\ntasks, including machine translation (MT). Previous studies explore aspects of\nLLMs' MT capabilities. However, there exist a wide variety of languages for\nwhich recent LLM MT performance has never before been evaluated. Without\npublished experimental evidence on the matter, it is difficult for speakers of\nthe world's diverse languages to know how and whether they can use LLMs for\ntheir languages. We present the first experimental evidence for an expansive\nset of 204 languages, along with MT cost analysis, using the FLORES-200\nbenchmark. Trends reveal that GPT models approach or exceed traditional MT\nmodel performance for some high-resource languages (HRLs) but consistently lag\nfor low-resource languages (LRLs), under-performing traditional MT for 84.1% of\nlanguages we covered. Our analysis reveals that a language's resource level is\nthe most important feature in determining ChatGPT's relative ability to\ntranslate it, and suggests that ChatGPT is especially disadvantaged for LRLs\nand African languages.", "published": "2023-09-14 04:36:00", "link": "http://arxiv.org/abs/2309.07423v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adapted Large Language Models Can Outperform Medical Experts in Clinical\n  Text Summarization", "abstract": "Analyzing vast textual data and summarizing key information from electronic\nhealth records imposes a substantial burden on how clinicians allocate their\ntime. Although large language models (LLMs) have shown promise in natural\nlanguage processing (NLP), their effectiveness on a diverse range of clinical\nsummarization tasks remains unproven. In this study, we apply adaptation\nmethods to eight LLMs, spanning four distinct clinical summarization tasks:\nradiology reports, patient questions, progress notes, and doctor-patient\ndialogue. Quantitative assessments with syntactic, semantic, and conceptual NLP\nmetrics reveal trade-offs between models and adaptation methods. A clinical\nreader study with ten physicians evaluates summary completeness, correctness,\nand conciseness; in a majority of cases, summaries from our best adapted LLMs\nare either equivalent (45%) or superior (36%) compared to summaries from\nmedical experts. The ensuing safety analysis highlights challenges faced by\nboth LLMs and medical experts, as we connect errors to potential medical harm\nand categorize types of fabricated information. Our research provides evidence\nof LLMs outperforming medical experts in clinical text summarization across\nmultiple tasks. This suggests that integrating LLMs into clinical workflows\ncould alleviate documentation burden, allowing clinicians to focus more on\npatient care.", "published": "2023-09-14 05:15:01", "link": "http://arxiv.org/abs/2309.07430v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SIB-200: A Simple, Inclusive, and Big Evaluation Dataset for Topic\n  Classification in 200+ Languages and Dialects", "abstract": "Despite the progress we have recorded in the last few years in multilingual\nnatural language processing, evaluation is typically limited to a small set of\nlanguages with available datasets which excludes a large number of low-resource\nlanguages. In this paper, we created SIB-200 -- a large-scale open-sourced\nbenchmark dataset for topic classification in 200 languages and dialects to\naddress the lack of evaluation dataset for Natural Language Understanding\n(NLU). For many of the languages covered in SIB-200, this is the first publicly\navailable evaluation dataset for NLU. The dataset is based on Flores-200\nmachine translation corpus. We annotated the English portion of the dataset and\nextended the sentence-level annotation to the remaining 203 languages covered\nin the corpus. Despite the simplicity of this task, our evaluation in\nfull-supervised setting, cross-lingual transfer setting and prompting of large\nlanguage model setting show that there is still a large gap between the\nperformance of high-resource and low-resource languages when multilingual\nevaluation is scaled to numerous world languages. We found that languages\nunseen during the pre-training of multilingual language models,\nunder-represented language families (like Nilotic and Altantic-Congo), and\nlanguages from the regions of Africa, Americas, Oceania and South East Asia,\noften have the lowest performance on our topic classification dataset. We hope\nour dataset will encourage a more inclusive evaluation of multilingual language\nmodels on a more diverse set of languages. https://github.com/dadelani/sib-200", "published": "2023-09-14 05:56:49", "link": "http://arxiv.org/abs/2309.07445v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are Large Language Model-based Evaluators the Solution to Scaling Up\n  Multilingual Evaluation?", "abstract": "Large Language Models (LLMs) excel in various Natural Language Processing\n(NLP) tasks, yet their evaluation, particularly in languages beyond the top\n$20$, remains inadequate due to existing benchmarks and metrics limitations.\nEmploying LLMs as evaluators to rank or score other models' outputs emerges as\na viable solution, addressing the constraints tied to human annotators and\nestablished benchmarks. In this study, we explore the potential of LLM-based\nevaluators, specifically GPT-4 in enhancing multilingual evaluation by\ncalibrating them against $20$K human judgments across three text-generation\ntasks, five metrics, and eight languages. Our analysis reveals a bias in\nGPT4-based evaluators towards higher scores, underscoring the necessity of\ncalibration with native speaker judgments, especially in low-resource and\nnon-Latin script languages, to ensure accurate evaluation of LLM performance\nacross diverse languages.", "published": "2023-09-14 06:41:58", "link": "http://arxiv.org/abs/2309.07462v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DBLPLink: An Entity Linker for the DBLP Scholarly Knowledge Graph", "abstract": "In this work, we present a web application named DBLPLink, which performs\nentity linking over the DBLP scholarly knowledge graph. DBLPLink uses\ntext-to-text pre-trained language models, such as T5, to produce entity label\nspans from an input text question. Entity candidates are fetched from a\ndatabase based on the labels, and an entity re-ranker sorts them based on\nentity embeddings, such as TransE, DistMult and ComplEx. The results are\ndisplayed so that users may compare and contrast the results between T5-small,\nT5-base and the different KG embeddings used. The demo can be accessed at\nhttps://ltdemos.informatik.uni-hamburg.de/dblplink/.", "published": "2023-09-14 09:15:36", "link": "http://arxiv.org/abs/2309.07545v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adaptive Prompt Learning with Distilled Connective Knowledge for\n  Implicit Discourse Relation Recognition", "abstract": "Implicit discourse relation recognition (IDRR) aims at recognizing the\ndiscourse relation between two text segments without an explicit connective.\nRecently, the prompt learning has just been applied to the IDRR task with great\nperformance improvements over various neural network-based approaches. However,\nthe discrete nature of the state-art-of-art prompting approach requires manual\ndesign of templates and answers, a big hurdle for its practical applications.\nIn this paper, we propose a continuous version of prompt learning together with\nconnective knowledge distillation, called AdaptPrompt, to reduce manual design\nefforts via continuous prompting while further improving performance via\nknowledge transfer. In particular, we design and train a few virtual tokens to\nform continuous templates and automatically select the most suitable one by\ngradient search in the embedding space. We also design an answer-relation\nmapping rule to generate a few virtual answers as the answer space.\nFurthermore, we notice the importance of annotated connectives in the training\ndataset and design a teacher-student architecture for knowledge transfer.\nExperiments on the up-to-date PDTB Corpus V3.0 validate our design objectives\nin terms of the better relation recognition performance over the\nstate-of-the-art competitors.", "published": "2023-09-14 09:44:46", "link": "http://arxiv.org/abs/2309.07561v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting Supertagging for Faster HPSG Pasing", "abstract": "We present new supertaggers trained on English grammar-based treebanks and\ntest the effects of the best tagger on parsing speed and accuracy. The\ntreebanks are produced automatically by large manually built grammars and\nfeature high-quality annotation based on a well-developed linguistic theory\n(HPSG). The English Resource Grammar treebanks include diverse and challenging\ntest datasets, beyond the usual WSJ section 23 and Wikipedia data. HPSG\nsupertagging has previously relied on MaxEnt-based models. We use SVM and\nneural CRF- and BERT-based methods and show that both SVM and neural\nsupertaggers achieve considerably higher accuracy compared to the baseline and\nlead to an increase not only in the parsing speed but also the parser accuracy\nwith respect to gold dependency structures. Our fine-tuned BERT-based tagger\nachieves 97.26\\% accuracy on 950 sentences from WSJ23 and 93.88% on the\nout-of-domain technical essay The Cathedral and the Bazaar (cb). We present\nexperiments with integrating the best supertagger into an HPSG parser and\nobserve a speedup of a factor of 3 with respect to the system which uses no\ntagging at all, as well as large recall gains and an overall precision gain. We\nalso compare our system to an existing integrated tagger and show that although\nthe well-integrated tagger remains the fastest, our experimental system can be\nmore accurate. Finally, we hope that the diverse and difficult datasets we used\nfor evaluation will gain more popularity in the field: we show that results can\ndiffer depending on the dataset, even if it is an in-domain one. We contribute\nthe complete datasets reformatted for Huggingface token classification.", "published": "2023-09-14 10:49:16", "link": "http://arxiv.org/abs/2309.07590v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dynamic MOdularized Reasoning for Compositional Structured Explanation\n  Generation", "abstract": "Despite the success of neural models in solving reasoning tasks, their\ncompositional generalization capabilities remain unclear. In this work, we\npropose a new setting of the structured explanation generation task to\nfacilitate compositional reasoning research. Previous works found that symbolic\nmethods achieve superior compositionality by using pre-defined inference rules\nfor iterative reasoning. But these approaches rely on brittle symbolic\ntransfers and are restricted to well-defined tasks. Hence, we propose a dynamic\nmodularized reasoning model, MORSE, to improve the compositional generalization\nof neural models. MORSE factorizes the inference process into a combination of\nmodules, where each module represents a functional unit. Specifically, we adopt\nmodularized self-attention to dynamically select and route inputs to dedicated\nheads, which specializes them to specific functions. We conduct experiments for\nincreasing lengths and shapes of reasoning trees on two benchmarks to test\nMORSE's compositional generalization abilities, and find it outperforms\ncompetitive baselines. Model ablation and deeper analyses show the\neffectiveness of dynamic reasoning modules and their generalization abilities.", "published": "2023-09-14 11:40:30", "link": "http://arxiv.org/abs/2309.07624v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Data Visualization Generation from Chinese Natural Language\n  Questions", "abstract": "Data visualization has emerged as an effective tool for getting insights from\nmassive datasets. Due to the hardness of manipulating the programming languages\nof data visualization, automatic data visualization generation from natural\nlanguages (Text-to-Vis) is becoming increasingly popular. Despite the plethora\nof research effort on the English Text-to-Vis, studies have yet to be conducted\non data visualization generation from questions in Chinese. Motivated by this,\nwe propose a Chinese Text-to-Vis dataset in the paper and demonstrate our first\nattempt to tackle this problem. Our model integrates multilingual BERT as the\nencoder, boosts the cross-lingual ability, and infuses the $n$-gram information\ninto our word representation learning. Our experimental results show that our\ndataset is challenging and deserves further research.", "published": "2023-09-14 12:16:21", "link": "http://arxiv.org/abs/2309.07650v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Aligning Speakers: Evaluating and Visualizing Text-based Diarization\n  Using Efficient Multiple Sequence Alignment (Extended Version)", "abstract": "This paper presents a novel evaluation approach to text-based speaker\ndiarization (SD), tackling the limitations of traditional metrics that do not\naccount for any contextual information in text. Two new metrics are proposed,\nText-based Diarization Error Rate and Diarization F1, which perform utterance-\nand word-level evaluations by aligning tokens in reference and hypothesis\ntranscripts. Our metrics encompass more types of errors compared to existing\nones, allowing us to make a more comprehensive analysis in SD. To align tokens,\na multiple sequence alignment algorithm is introduced that supports multiple\nsequences in the reference while handling high-dimensional alignment to the\nhypothesis using dynamic programming. Our work is packaged into two tools,\nalign4d providing an API for our alignment algorithm and TranscribeView for\nvisualizing and evaluating SD errors, which can greatly aid in the creation of\nhigh-quality data, fostering the advancement of dialogue systems.", "published": "2023-09-14 12:43:26", "link": "http://arxiv.org/abs/2309.07677v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PerPLM: Personalized Fine-tuning of Pretrained Language Models via\n  Writer-specific Intermediate Learning and Prompts", "abstract": "The meanings of words and phrases depend not only on where they are used\n(contexts) but also on who use them (writers). Pretrained language models\n(PLMs) are powerful tools for capturing context, but they are typically\npretrained and fine-tuned for universal use across different writers. This\nstudy aims to improve the accuracy of text understanding tasks by personalizing\nthe fine-tuning of PLMs for specific writers. We focus on a general setting\nwhere only the plain text from target writers are available for\npersonalization. To avoid the cost of fine-tuning and storing multiple copies\nof PLMs for different users, we exhaustively explore using writer-specific\nprompts to personalize a unified PLM. Since the design and evaluation of these\nprompts is an underdeveloped area, we introduce and compare different types of\nprompts that are possible in our setting. To maximize the potential of\nprompt-based personalized fine-tuning, we propose a personalized intermediate\nlearning based on masked language modeling to extract task-independent traits\nof writers' text. Our experiments, using multiple tasks, datasets, and PLMs,\nreveal the nature of different prompts and the effectiveness of our\nintermediate learning approach.", "published": "2023-09-14 14:03:48", "link": "http://arxiv.org/abs/2309.07727v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Dynamical Principles of Storytelling", "abstract": "When considering the opening part of 1800 short stories, we find that the\nfirst dozen paragraphs of the average narrative follow an action principle as\ndefined in arXiv:2309.06600. When the order of the paragraphs is shuffled, the\naverage no longer exhibits this property. The findings show that there is a\npreferential direction we take in semantic space when starting a story,\npossibly related to a common Western storytelling tradition as implied by\nAristotle in Poetics.", "published": "2023-09-14 15:36:10", "link": "http://arxiv.org/abs/2309.07797v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CATfOOD: Counterfactual Augmented Training for Improving Out-of-Domain\n  Performance and Calibration", "abstract": "In recent years, large language models (LLMs) have shown remarkable\ncapabilities at scale, particularly at generating text conditioned on a prompt.\nIn our work, we investigate the use of LLMs to augment training data of small\nlanguage models~(SLMs) with automatically generated counterfactual~(CF)\ninstances -- i.e. minimally altered inputs -- in order to improve\nout-of-domain~(OOD) performance of SLMs in the extractive question\nanswering~(QA) setup. We show that, across various LLM generators, such data\naugmentation consistently enhances OOD performance and improves model\ncalibration for both confidence-based and rationale-augmented calibrator\nmodels. Furthermore, these performance improvements correlate with higher\ndiversity of CF instances in terms of their surface form and semantic content.\nFinally, we show that CF augmented models which are easier to calibrate also\nexhibit much lower entropy when assigning importance, indicating that\nrationale-augmented calibrators prefer concise explanations.", "published": "2023-09-14 16:16:40", "link": "http://arxiv.org/abs/2309.07822v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Agents: An Open-source Framework for Autonomous Language Agents", "abstract": "Recent advances on large language models (LLMs) enable researchers and\ndevelopers to build autonomous language agents that can automatically solve\nvarious tasks and interact with environments, humans, and other agents using\nnatural language interfaces. We consider language agents as a promising\ndirection towards artificial general intelligence and release Agents, an\nopen-source library with the goal of opening up these advances to a wider\nnon-specialist audience. Agents is carefully engineered to support important\nfeatures including planning, memory, tool usage, multi-agent communication, and\nfine-grained symbolic control. Agents is user-friendly as it enables\nnon-specialists to build, customize, test, tune, and deploy state-of-the-art\nautonomous language agents without much coding. The library is also\nresearch-friendly as its modularized design makes it easily extensible for\nresearchers. Agents is available at https://github.com/aiwaves-cn/agents.", "published": "2023-09-14 17:18:25", "link": "http://arxiv.org/abs/2309.07870v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language\n  Models that Follow Instructions", "abstract": "Training large language models to follow instructions makes them perform\nbetter on a wide range of tasks and generally become more helpful. However, a\nperfectly helpful model will follow even the most malicious instructions and\nreadily generate harmful content. In this paper, we raise concerns over the\nsafety of models that only emphasize helpfulness, not harmlessness, in their\ninstruction-tuning. We show that several popular instruction-tuned models are\nhighly unsafe. Moreover, we show that adding just 3% safety examples (a few\nhundred demonstrations) when fine-tuning a model like LLaMA can substantially\nimprove its safety. Our safety-tuning does not make models significantly less\ncapable or helpful as measured by standard benchmarks. However, we do find\nexaggerated safety behaviours, where too much safety-tuning makes models refuse\nperfectly safe prompts if they superficially resemble unsafe ones. As a whole,\nour results illustrate trade-offs in training LLMs to be helpful and training\nthem to be safe.", "published": "2023-09-14 17:23:37", "link": "http://arxiv.org/abs/2309.07875v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Contextual Information for Effective Entity Salience\n  Detection", "abstract": "In text documents such as news articles, the content and key events usually\nrevolve around a subset of all the entities mentioned in a document. These\nentities, often deemed as salient entities, provide useful cues of the\naboutness of a document to a reader. Identifying the salience of entities was\nfound helpful in several downstream applications such as search, ranking, and\nentity-centric summarization, among others. Prior work on salient entity\ndetection mainly focused on machine learning models that require heavy feature\nengineering. We show that fine-tuning medium-sized language models with a\ncross-encoder style architecture yields substantial performance gains over\nfeature engineering approaches. To this end, we conduct a comprehensive\nbenchmarking of four publicly available datasets using models representative of\nthe medium-sized pre-trained language model family. Additionally, we show that\nzero-shot prompting of instruction-tuned language models yields inferior\nresults, indicating the task's uniqueness and complexity.", "published": "2023-09-14 19:04:40", "link": "http://arxiv.org/abs/2309.07990v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring the Impact of Human Evaluator Group on Chat-Oriented Dialogue\n  Evaluation", "abstract": "Human evaluation has been widely accepted as the standard for evaluating\nchat-oriented dialogue systems. However, there is a significant variation in\nprevious work regarding who gets recruited as evaluators. Evaluator groups such\nas domain experts, university students, and professional annotators have been\nused to assess and compare dialogue systems, although it is unclear to what\nextent the choice of an evaluator group can affect results. This paper analyzes\nthe evaluator group impact on dialogue system evaluation by testing 4\nstate-of-the-art dialogue systems using 4 distinct evaluator groups. Our\nanalysis reveals a robustness towards evaluator groups for Likert evaluations\nthat is not seen for Pairwise, with only minor differences observed when\nchanging evaluator groups. Furthermore, two notable limitations to this\nrobustness are observed, which reveal discrepancies between evaluators with\ndifferent levels of chatbot expertise and indicate that evaluator objectivity\nis beneficial for certain dialogue metrics.", "published": "2023-09-14 19:19:50", "link": "http://arxiv.org/abs/2309.07998v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bias in News Summarization: Measures, Pitfalls and Corpora", "abstract": "Summarization is an important application of large language models (LLMs).\nMost previous evaluation of summarization models has focused on their content\nselection, faithfulness, grammaticality and coherence. However, it is well\nknown that LLMs can reproduce and reinforce harmful social biases. This raises\nthe question: Do biases affect model outputs in a constrained setting like\nsummarization? To help answer this question, we first motivate and introduce a\nnumber of definitions for biased behaviours in summarization models, along with\npractical operationalizations. Since we find that biases inherent to input\ndocuments can confound bias analysis in summaries, we propose a method to\ngenerate input documents with carefully controlled demographic attributes. This\nallows us to study summarizer behavior in a controlled setting, while still\nworking with realistic input documents. We measure gender bias in English\nsummaries generated by both purpose-built summarization models and general\npurpose chat models as a case study. We find content selection in single\ndocument summarization to be largely unaffected by gender bias, while\nhallucinations exhibit evidence of bias. To demonstrate the generality of our\napproach, we additionally investigate racial bias, including intersectional\nsettings.", "published": "2023-09-14 22:20:27", "link": "http://arxiv.org/abs/2309.08047v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Connecting the Dots in News Analysis: Bridging the Cross-Disciplinary\n  Disparities in Media Bias and Framing", "abstract": "The manifestation and effect of bias in news reporting have been central\ntopics in the social sciences for decades, and have received increasing\nattention in the NLP community recently. While NLP can help to scale up\nanalyses or contribute automatic procedures to investigate the impact of biased\nnews in society, we argue that methodologies that are currently dominant fall\nshort of addressing the complex questions and effects addressed in theoretical\nmedia studies. In this survey paper, we review social science approaches and\ndraw a comparison with typical task formulations, methods, and evaluation\nmetrics used in the analysis of media bias in NLP. We discuss open questions\nand suggest possible directions to close identified gaps between theory and\npredictive models, and their evaluation. These include model transparency,\nconsidering document-external information, and cross-document reasoning rather\nthan single-label assignment.", "published": "2023-09-14 23:57:55", "link": "http://arxiv.org/abs/2309.08069v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Anchor Points: Benchmarking Models with Much Fewer Examples", "abstract": "Modern language models often exhibit powerful but brittle behavior, leading\nto the development of larger and more diverse benchmarks to reliably assess\ntheir behavior. Here, we suggest that model performance can be benchmarked and\nelucidated with much smaller evaluation sets. We first show that in six popular\nlanguage classification benchmarks, model confidence in the correct class on\nmany pairs of points is strongly correlated across models. We build upon this\nphenomenon to propose Anchor Point Selection, a technique to select small\nsubsets of datasets that capture model behavior across the entire dataset.\nAnchor points reliably rank models: across 87 diverse language model-prompt\npairs, evaluating models using 1-30 anchor points outperforms uniform sampling\nand other baselines at accurately ranking models. Moreover, just several anchor\npoints can be used to estimate model per-class predictions on all other points\nin a dataset with low mean absolute error, sufficient for gauging where the\nmodel is likely to fail. Lastly, we present Anchor Point Maps for visualizing\nthese insights and facilitating comparisons of the performance of different\nmodels on various regions within the dataset distribution.", "published": "2023-09-14 17:45:51", "link": "http://arxiv.org/abs/2309.08638v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "VDialogUE: A Unified Evaluation Benchmark for Visually-grounded Dialogue", "abstract": "Visually-grounded dialog systems, which integrate multiple modes of\ncommunication such as text and visual inputs, have become an increasingly\npopular area of investigation. However, the absence of a standardized\nevaluation framework poses a challenge in assessing the development of this\nfield. To this end, we propose \\textbf{VDialogUE}, a \\textbf{V}isually-grounded\n\\textbf{Dialog}ue benchmark for \\textbf{U}nified \\textbf{E}valuation. It\ndefines five core multi-modal dialogue tasks and covers six datasets.\nFurthermore, in order to provide a comprehensive assessment of the model's\nperformance across all tasks, we developed a novel evaluation metric called\nVDscore, which is based on the Analytic Hierarchy Process~(AHP) method.\nAdditionally, we present a straightforward yet efficient baseline model, named\n\\textbf{VISIT}~(\\textbf{VIS}ually-grounded d\\textbf{I}alog\n\\textbf{T}ransformer), to promote the advancement of general multi-modal\ndialogue systems. It progressively builds its multi-modal foundation and\ndialogue capability via a two-stage pre-training strategy.\n  We believe that the VDialogUE benchmark, along with the evaluation scripts\nand our baseline models, will accelerate the development of visually-grounded\ndialog systems and lead to the development of more sophisticated and effective\npre-trained models.", "published": "2023-09-14 02:09:20", "link": "http://arxiv.org/abs/2309.07387v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "DebCSE: Rethinking Unsupervised Contrastive Sentence Embedding Learning\n  in the Debiasing Perspective", "abstract": "Several prior studies have suggested that word frequency biases can cause the\nBert model to learn indistinguishable sentence embeddings. Contrastive learning\nschemes such as SimCSE and ConSERT have already been adopted successfully in\nunsupervised sentence embedding to improve the quality of embeddings by\nreducing this bias. However, these methods still introduce new biases such as\nsentence length bias and false negative sample bias, that hinders model's\nability to learn more fine-grained semantics. In this paper, we reexamine the\nchallenges of contrastive sentence embedding learning from a debiasing\nperspective and argue that effectively eliminating the influence of various\nbiases is crucial for learning high-quality sentence embeddings. We think all\nthose biases are introduced by simple rules for constructing training data in\ncontrastive learning and the key for contrastive learning sentence embedding is\nto mimic the distribution of training data in supervised machine learning in\nunsupervised way. We propose a novel contrastive framework for sentence\nembedding, termed DebCSE, which can eliminate the impact of these biases by an\ninverse propensity weighted sampling method to select high-quality positive and\nnegative pairs according to both the surface and semantic similarity between\nsentences. Extensive experiments on semantic textual similarity (STS)\nbenchmarks reveal that DebCSE significantly outperforms the latest\nstate-of-the-art models with an average Spearman's correlation coefficient of\n80.33% on BERTbase.", "published": "2023-09-14 02:43:34", "link": "http://arxiv.org/abs/2309.07396v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Advancing Regular Language Reasoning in Linear Recurrent Neural Networks", "abstract": "In recent studies, linear recurrent neural networks (LRNNs) have achieved\nTransformer-level performance in natural language and long-range modeling,\nwhile offering rapid parallel training and constant inference cost. With the\nresurgence of interest in LRNNs, we study whether they can learn the hidden\nrules in training sequences, such as the grammatical structures of regular\nlanguage. We theoretically analyze some existing LRNNs and discover their\nlimitations in modeling regular language. Motivated by this analysis, we\npropose a new LRNN equipped with a block-diagonal and input-dependent\ntransition matrix. Experiments suggest that the proposed model is the only LRNN\ncapable of performing length extrapolation on regular language tasks such as\nSum, Even Pair, and Modular Arithmetic. The code is released at\n\\url{https://github.com/tinghanf/RegluarLRNN}.", "published": "2023-09-14 03:36:01", "link": "http://arxiv.org/abs/2309.07412v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Semantic Parsing in Limited Resource Conditions", "abstract": "This thesis explores challenges in semantic parsing, specifically focusing on\nscenarios with limited data and computational resources. It offers solutions\nusing techniques like automatic data curation, knowledge transfer, active\nlearning, and continual learning.\n  For tasks with no parallel training data, the thesis proposes generating\nsynthetic training examples from structured database schemas. When there is\nabundant data in a source domain but limited parallel data in a target domain,\nknowledge from the source is leveraged to improve parsing in the target domain.\n  For multilingual situations with limited data in the target languages, the\nthesis introduces a method to adapt parsers using a limited human translation\nbudget. Active learning is applied to select source-language samples for manual\ntranslation, maximizing parser performance in the target language. In addition,\nan alternative method is also proposed to utilize machine translation services,\nsupplemented by human-translated data, to train a more effective parser.\n  When computational resources are limited, a continual learning approach is\nintroduced to minimize training time and computational memory. This maintains\nthe parser's efficiency in previously learned tasks while adapting it to new\ntasks, mitigating the problem of catastrophic forgetting.\n  Overall, the thesis provides a comprehensive set of methods to improve\nsemantic parsing in resource-constrained conditions.", "published": "2023-09-14 05:03:09", "link": "http://arxiv.org/abs/2309.07429v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Zero-shot Audio Topic Reranking using Large Language Models", "abstract": "Multimodal Video Search by Examples (MVSE) investigates using video clips as\nthe query term for information retrieval, rather than the more traditional text\nquery. This enables far richer search modalities such as images, speaker,\ncontent, topic, and emotion. A key element for this process is highly rapid and\nflexible search to support large archives, which in MVSE is facilitated by\nrepresenting video attributes with embeddings. This work aims to compensate for\nany performance loss from this rapid archive search by examining reranking\napproaches. In particular, zero-shot reranking methods using large language\nmodels (LLMs) are investigated as these are applicable to any video archive\naudio content. Performance is evaluated for topic-based retrieval on a publicly\navailable video archive, the BBC Rewind corpus. Results demonstrate that\nreranking significantly improves retrieval ranking without requiring any\ntask-specific in-domain training data. Furthermore, three sources of\ninformation (ASR transcriptions, automatic summaries and synopses) as input for\nLLM reranking were compared. To gain a deeper understanding and further\ninsights into the performance differences and limitations of these text\nsources, we employ a fact-checking approach to analyse the information\nconsistency among them.", "published": "2023-09-14 11:13:36", "link": "http://arxiv.org/abs/2309.07606v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "A Conversation is Worth A Thousand Recommendations: A Survey of Holistic\n  Conversational Recommender Systems", "abstract": "Conversational recommender systems (CRS) generate recommendations through an\ninteractive process. However, not all CRS approaches use human conversations as\ntheir source of interaction data; the majority of prior CRS work simulates\ninteractions by exchanging entity-level information. As a result, claims of\nprior CRS work do not generalise to real-world settings where conversations\ntake unexpected turns, or where conversational and intent understanding is not\nperfect. To tackle this challenge, the research community has started to\nexamine holistic CRS, which are trained using conversational data collected\nfrom real-world scenarios. Despite their emergence, such holistic approaches\nare under-explored.\n  We present a comprehensive survey of holistic CRS methods by summarizing the\nliterature in a structured manner. Our survey recognises holistic CRS\napproaches as having three components: 1) a backbone language model, the\noptional use of 2) external knowledge, and/or 3) external guidance. We also\ngive a detailed analysis of CRS datasets and evaluation methods in real\napplication scenarios. We offer our insight as to the current challenges of\nholistic CRS and possible future trends.", "published": "2023-09-14 12:55:23", "link": "http://arxiv.org/abs/2309.07682v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Detecting ChatGPT: A Survey of the State of Detecting ChatGPT-Generated\n  Text", "abstract": "While recent advancements in the capabilities and widespread accessibility of\ngenerative language models, such as ChatGPT (OpenAI, 2022), have brought about\nvarious benefits by generating fluent human-like text, the task of\ndistinguishing between human- and large language model (LLM) generated text has\nemerged as a crucial problem. These models can potentially deceive by\ngenerating artificial text that appears to be human-generated. This issue is\nparticularly significant in domains such as law, education, and science, where\nensuring the integrity of text is of the utmost importance. This survey\nprovides an overview of the current approaches employed to differentiate\nbetween texts generated by humans and ChatGPT. We present an account of the\ndifferent datasets constructed for detecting ChatGPT-generated text, the\nvarious methods utilized, what qualitative analyses into the characteristics of\nhuman versus ChatGPT-generated text have been performed, and finally, summarize\nour findings into general insights", "published": "2023-09-14 13:05:20", "link": "http://arxiv.org/abs/2309.07689v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Generative AI Text Classification using Ensemble LLM Approaches", "abstract": "Large Language Models (LLMs) have shown impressive performance across a\nvariety of Artificial Intelligence (AI) and natural language processing tasks,\nsuch as content creation, report generation, etc. However, unregulated malign\napplication of these models can create undesirable consequences such as\ngeneration of fake news, plagiarism, etc. As a result, accurate detection of\nAI-generated language can be crucial in responsible usage of LLMs. In this\nwork, we explore 1) whether a certain body of text is AI generated or written\nby human, and 2) attribution of a specific language model in generating a body\nof text. Texts in both English and Spanish are considered. The datasets used in\nthis study are provided as part of the Automated Text Identification\n(AuTexTification) shared task. For each of the research objectives stated\nabove, we propose an ensemble neural model that generates probabilities from\ndifferent pre-trained LLMs which are used as features to a Traditional Machine\nLearning (TML) classifier following it. For the first task of distinguishing\nbetween AI and human generated text, our model ranked in fifth and thirteenth\nplace (with macro $F1$ scores of 0.733 and 0.649) for English and Spanish\ntexts, respectively. For the second task on model attribution, our model ranked\nin first place with macro $F1$ scores of 0.625 and 0.653 for English and\nSpanish texts, respectively.", "published": "2023-09-14 14:41:46", "link": "http://arxiv.org/abs/2309.07755v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PROGrasp: Pragmatic Human-Robot Communication for Object Grasping", "abstract": "Interactive Object Grasping (IOG) is the task of identifying and grasping the\ndesired object via human-robot natural language interaction. Current IOG\nsystems assume that a human user initially specifies the target object's\ncategory (e.g., bottle). Inspired by pragmatics, where humans often convey\ntheir intentions by relying on context to achieve goals, we introduce a new IOG\ntask, Pragmatic-IOG, and the corresponding dataset, Intention-oriented\nMulti-modal Dialogue (IM-Dial). In our proposed task scenario, an\nintention-oriented utterance (e.g., \"I am thirsty\") is initially given to the\nrobot. The robot should then identify the target object by interacting with a\nhuman user. Based on the task setup, we propose a new robotic system that can\ninterpret the user's intention and pick up the target object, Pragmatic Object\nGrasping (PROGrasp). PROGrasp performs Pragmatic-IOG by incorporating modules\nfor visual grounding, question asking, object grasping, and most importantly,\nanswer interpretation for pragmatic inference. Experimental results show that\nPROGrasp is effective in offline (i.e., target object discovery) and online\n(i.e., IOG with a physical robot arm) settings. Code and data are available at\nhttps://github.com/gicheonkang/prograsp.", "published": "2023-09-14 14:45:47", "link": "http://arxiv.org/abs/2309.07759v4", "categories": ["cs.CL", "cs.RO"], "primary_category": "cs.CL"}
{"title": "Pop Quiz! Do Pre-trained Code Models Possess Knowledge of Correct API\n  Names?", "abstract": "Recent breakthroughs in pre-trained code models, such as CodeBERT and Codex,\nhave shown their superior performance in various downstream tasks. The\ncorrectness and unambiguity of API usage among these code models are crucial\nfor achieving desirable program functionalities, requiring them to learn\nvarious API fully qualified names structurally and semantically. Recent studies\nreveal that even state-of-the-art pre-trained code models struggle with\nsuggesting the correct APIs during code generation. However, the reasons for\nsuch poor API usage performance are barely investigated. To address this\nchallenge, we propose using knowledge probing as a means of interpreting code\nmodels, which uses cloze-style tests to measure the knowledge stored in models.\nOur comprehensive study examines a code model's capability of understanding API\nfully qualified names from two different perspectives: API call and API import.\nSpecifically, we reveal that current code models struggle with understanding\nAPI names, with pre-training strategies significantly affecting the quality of\nAPI name learning. We demonstrate that natural language context can assist code\nmodels in locating Python API names and generalize Python API name knowledge to\nunseen data. Our findings provide insights into the limitations and\ncapabilities of current pre-trained code models, and suggest that incorporating\nAPI structure into the pre-training process can improve automated API usage and\ncode representations. This work provides significance for advancing code\nintelligence practices and direction for future studies. All experiment\nresults, data and source code used in this work are available at\n\\url{https://doi.org/10.5281/zenodo.7902072}.", "published": "2023-09-14 15:46:41", "link": "http://arxiv.org/abs/2309.07804v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Text Classification of Cancer Clinical Trial Eligibility Criteria", "abstract": "Automatic identification of clinical trials for which a patient is eligible\nis complicated by the fact that trial eligibility is stated in natural\nlanguage. A potential solution to this problem is to employ text classification\nmethods for common types of eligibility criteria. In this study, we focus on\nseven common exclusion criteria in cancer trials: prior malignancy, human\nimmunodeficiency virus, hepatitis B, hepatitis C, psychiatric illness,\ndrug/substance abuse, and autoimmune illness. Our dataset consists of 764 phase\nIII cancer trials with these exclusions annotated at the trial level. We\nexperiment with common transformer models as well as a new pre-trained clinical\ntrial BERT model. Our results demonstrate the feasibility of automatically\nclassifying common exclusion criteria. Additionally, we demonstrate the value\nof a pre-trained language model specifically for clinical trials, which yields\nthe highest average performance across all criteria.", "published": "2023-09-14 15:59:16", "link": "http://arxiv.org/abs/2309.07812v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ExpertQA: Expert-Curated Questions and Attributed Answers", "abstract": "As language models are adopted by a more sophisticated and diverse set of\nusers, the importance of guaranteeing that they provide factually correct\ninformation supported by verifiable sources is critical across fields of study.\nThis is especially the case for high-stakes fields, such as medicine and law,\nwhere the risk of propagating false information is high and can lead to\nundesirable societal consequences. Previous work studying attribution and\nfactuality has not focused on analyzing these characteristics of language model\noutputs in domain-specific scenarios. In this work, we conduct human evaluation\nof responses from a few representative systems along various axes of\nattribution and factuality, by bringing domain experts in the loop.\nSpecifically, we collect expert-curated questions from 484 participants across\n32 fields of study, and then ask the same experts to evaluate generated\nresponses to their own questions. In addition, we ask experts to improve upon\nresponses from language models. The output of our analysis is ExpertQA, a\nhigh-quality long-form QA dataset with 2177 questions spanning 32 fields, along\nwith verified answers and attributions for claims in the answers.", "published": "2023-09-14 16:54:34", "link": "http://arxiv.org/abs/2309.07852v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Rise and Potential of Large Language Model Based Agents: A Survey", "abstract": "For a long time, humanity has pursued artificial intelligence (AI) equivalent\nto or surpassing the human level, with AI agents considered a promising vehicle\nfor this pursuit. AI agents are artificial entities that sense their\nenvironment, make decisions, and take actions. Many efforts have been made to\ndevelop intelligent agents, but they mainly focus on advancement in algorithms\nor training strategies to enhance specific capabilities or performance on\nparticular tasks. Actually, what the community lacks is a general and powerful\nmodel to serve as a starting point for designing AI agents that can adapt to\ndiverse scenarios. Due to the versatile capabilities they demonstrate, large\nlanguage models (LLMs) are regarded as potential sparks for Artificial General\nIntelligence (AGI), offering hope for building general AI agents. Many\nresearchers have leveraged LLMs as the foundation to build AI agents and have\nachieved significant progress. In this paper, we perform a comprehensive survey\non LLM-based agents. We start by tracing the concept of agents from its\nphilosophical origins to its development in AI, and explain why LLMs are\nsuitable foundations for agents. Building upon this, we present a general\nframework for LLM-based agents, comprising three main components: brain,\nperception, and action, and the framework can be tailored for different\napplications. Subsequently, we explore the extensive applications of LLM-based\nagents in three aspects: single-agent scenarios, multi-agent scenarios, and\nhuman-agent cooperation. Following this, we delve into agent societies,\nexploring the behavior and personality of LLM-based agents, the social\nphenomena that emerge from an agent society, and the insights they offer for\nhuman society. Finally, we discuss several key topics and open problems within\nthe field. A repository for the related papers at\nhttps://github.com/WooooDyy/LLM-Agent-Paper-List.", "published": "2023-09-14 17:12:03", "link": "http://arxiv.org/abs/2309.07864v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Ambiguity-Aware In-Context Learning with Large Language Models", "abstract": "In-context learning (ICL) i.e. showing LLMs only a few task-specific\ndemonstrations has led to downstream gains with no task-specific fine-tuning\nrequired. However, LLMs are sensitive to the choice of prompts, and therefore a\ncrucial research question is how to select good demonstrations for ICL. One\neffective strategy is leveraging semantic similarity between the ICL\ndemonstrations and test inputs by using a text retriever, which however is\nsub-optimal as that does not consider the LLM's existing knowledge about that\ntask. From prior work (Lyu et al., 2023), we already know that labels paired\nwith the demonstrations bias the model predictions. This leads us to our\nhypothesis whether considering LLM's existing knowledge about the task,\nespecially with respect to the output label space can help in a better\ndemonstration selection strategy. Through extensive experimentation on three\ntext classification tasks, we find that it is beneficial to not only choose\nsemantically similar ICL demonstrations but also to choose those demonstrations\nthat help resolve the inherent label ambiguity surrounding the test example.\nInterestingly, we find that including demonstrations that the LLM previously\nmis-classified and also fall on the test example's decision boundary, brings\nthe most performance gain.", "published": "2023-09-14 17:48:34", "link": "http://arxiv.org/abs/2309.07900v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "An Empirical Evaluation of Prompting Strategies for Large Language\n  Models in Zero-Shot Clinical Natural Language Processing", "abstract": "Large language models (LLMs) have shown remarkable capabilities in Natural\nLanguage Processing (NLP), especially in domains where labeled data is scarce\nor expensive, such as clinical domain. However, to unlock the clinical\nknowledge hidden in these LLMs, we need to design effective prompts that can\nguide them to perform specific clinical NLP tasks without any task-specific\ntraining data. This is known as in-context learning, which is an art and\nscience that requires understanding the strengths and weaknesses of different\nLLMs and prompt engineering approaches. In this paper, we present a\ncomprehensive and systematic experimental study on prompt engineering for five\nclinical NLP tasks: Clinical Sense Disambiguation, Biomedical Evidence\nExtraction, Coreference Resolution, Medication Status Extraction, and\nMedication Attribute Extraction. We assessed the prompts proposed in recent\nliterature, including simple prefix, simple cloze, chain of thought, and\nanticipatory prompts, and introduced two new types of prompts, namely heuristic\nprompting and ensemble prompting. We evaluated the performance of these prompts\non three state-of-the-art LLMs: GPT-3.5, BARD, and LLAMA2. We also contrasted\nzero-shot prompting with few-shot prompting, and provide novel insights and\nguidelines for prompt engineering for LLMs in clinical NLP. To the best of our\nknowledge, this is one of the first works on the empirical evaluation of\ndifferent prompt engineering approaches for clinical NLP in this era of\ngenerative AI, and we hope that it will inspire and inform future research in\nthis area.", "published": "2023-09-14 19:35:00", "link": "http://arxiv.org/abs/2309.08008v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TextBind: Multi-turn Interleaved Multimodal Instruction-following in the\n  Wild", "abstract": "Large language models with instruction-following abilities have\nrevolutionized the field of artificial intelligence. These models show\nexceptional generalizability to tackle various real-world tasks through their\nnatural language interfaces. However, their performance heavily relies on\nhigh-quality exemplar data, which is often difficult to obtain. This challenge\nis further exacerbated when it comes to multimodal instruction following. We\nintroduce TextBind, an almost annotation-free framework for empowering larger\nlanguage models with the multi-turn interleaved multimodal\ninstruction-following capabilities. Our approach requires only image-caption\npairs and generates multi-turn multimodal instruction-response conversations\nfrom a language model. To accommodate interleaved image-text inputs and\noutputs, we devise MIM, a language model-centric architecture that seamlessly\nintegrates image encoder and decoder models. We release our dataset, model, and\ndemo to foster future research in the area of multimodal instruction following.", "published": "2023-09-14 15:34:01", "link": "http://arxiv.org/abs/2309.08637v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Hybrid Attention-based Encoder-decoder Model for Efficient Language\n  Model Adaptation", "abstract": "The attention-based encoder-decoder (AED) speech recognition model has been\nwidely successful in recent years. However, the joint optimization of acoustic\nmodel and language model in end-to-end manner has created challenges for text\nadaptation. In particular, effective, quick and inexpensive adaptation with\ntext input has become a primary concern for deploying AED systems in the\nindustry. To address this issue, we propose a novel model, the hybrid\nattention-based encoder-decoder (HAED) speech recognition model that preserves\nthe modularity of conventional hybrid automatic speech recognition systems. Our\nHAED model separates the acoustic and language models, allowing for the use of\nconventional text-based language model adaptation techniques. We demonstrate\nthat the proposed HAED model yields 23% relative Word Error Rate (WER)\nimprovements when out-of-domain text data is used for language model\nadaptation, with only a minor degradation in WER on a general test set compared\nwith the conventional AED model.", "published": "2023-09-14 01:07:36", "link": "http://arxiv.org/abs/2309.07369v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "CPPF: A contextual and post-processing-free model for automatic speech\n  recognition", "abstract": "ASR systems have become increasingly widespread in recent years. However,\ntheir textual outputs often require post-processing tasks before they can be\npractically utilized. To address this issue, we draw inspiration from the\nmultifaceted capabilities of LLMs and Whisper, and focus on integrating\nmultiple ASR text processing tasks related to speech recognition into the ASR\nmodel. This integration not only shortens the multi-stage pipeline, but also\nprevents the propagation of cascading errors, resulting in direct generation of\npost-processed text. In this study, we focus on ASR-related processing tasks,\nincluding Contextual ASR and multiple ASR post processing tasks. To achieve\nthis objective, we introduce the CPPF model, which offers a versatile and\nhighly effective alternative to ASR processing. CPPF seamlessly integrates\nthese tasks without any significant loss in recognition performance.", "published": "2023-09-14 03:40:14", "link": "http://arxiv.org/abs/2309.07413v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "PromptASR for contextualized ASR with controllable style", "abstract": "Prompts are crucial to large language models as they provide context\ninformation such as topic or logical relationships. Inspired by this, we\npropose PromptASR, a framework that integrates prompts in end-to-end automatic\nspeech recognition (E2E ASR) systems to achieve contextualized ASR with\ncontrollable style of transcriptions. Specifically, a dedicated text encoder\nencodes the text prompts and the encodings are injected into the speech encoder\nby cross-attending the features from two modalities. When using the ground\ntruth text from preceding utterances as content prompt, the proposed system\nachieves 21.9% and 6.8% relative word error rate reductions on a book reading\ndataset and an in-house dataset compared to a baseline ASR system. The system\ncan also take word-level biasing lists as prompt to improve recognition\naccuracy on rare words. An additional style prompt can be given to the text\nencoder and guide the ASR system to output different styles of transcriptions.\nThe code is available at icefall.", "published": "2023-09-14 03:43:07", "link": "http://arxiv.org/abs/2309.07414v3", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Direct Text to Speech Translation System using Acoustic Units", "abstract": "This paper proposes a direct text to speech translation system using discrete\nacoustic units. This framework employs text in different source languages as\ninput to generate speech in the target language without the need for text\ntranscriptions in this language. Motivated by the success of acoustic units in\nprevious works for direct speech to speech translation systems, we use the same\npipeline to extract the acoustic units using a speech encoder combined with a\nclustering algorithm. Once units are obtained, an encoder-decoder architecture\nis trained to predict them. Then a vocoder generates speech from units. Our\napproach for direct text to speech translation was tested on the new CVSS\ncorpus with two different text mBART models employed as initialisation. The\nsystems presented report competitive performance for most of the language pairs\nevaluated. Besides, results show a remarkable improvement when initialising our\nproposed architecture with a model pre-trained with more languages.", "published": "2023-09-14 07:35:14", "link": "http://arxiv.org/abs/2309.07478v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "C-Pack: Packed Resources For General Chinese Embeddings", "abstract": "We introduce C-Pack, a package of resources that significantly advance the\nfield of general Chinese embeddings. C-Pack includes three critical resources.\n1) C-MTEB is a comprehensive benchmark for Chinese text embeddings covering 6\ntasks and 35 datasets. 2) C-MTP is a massive text embedding dataset curated\nfrom labeled and unlabeled Chinese corpora for training embedding models. 3)\nC-TEM is a family of embedding models covering multiple sizes. Our models\noutperform all prior Chinese text embeddings on C-MTEB by up to +10% upon the\ntime of the release. We also integrate and optimize the entire suite of\ntraining methods for C-TEM. Along with our resources on general Chinese\nembedding, we release our data and models for English text embeddings. The\nEnglish models achieve state-of-the-art performance on MTEB benchmark;\nmeanwhile, our released English data is 2 times larger than the Chinese data.\nAll these resources are made publicly available at\nhttps://github.com/FlagOpen/FlagEmbedding.", "published": "2023-09-14 10:57:50", "link": "http://arxiv.org/abs/2309.07597v5", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Weakly Supervised Veracity Classification with LLM-Predicted Credibility\n  Signals", "abstract": "Credibility signals represent a wide range of heuristics typically used by\njournalists and fact-checkers to assess the veracity of online content.\nAutomating the extraction of credibility signals presents significant\nchallenges due to the necessity of training high-accuracy, signal-specific\nextractors, coupled with the lack of sufficiently large annotated datasets.\nThis paper introduces Pastel (Prompted weAk Supervision wiTh crEdibility\nsignaLs), a weakly supervised approach that leverages large language models\n(LLMs) to extract credibility signals from web content, and subsequently\ncombines them to predict the veracity of content without relying on human\nsupervision. We validate our approach using four article-level misinformation\ndetection datasets, demonstrating that Pastel outperforms zero-shot veracity\ndetection by 38.3% and achieves 86.7% of the performance of the\nstate-of-the-art system trained with human supervision. Moreover, in\ncross-domain settings where training and testing datasets originate from\ndifferent domains, Pastel significantly outperforms the state-of-the-art\nsupervised model by 63%. We further study the association between credibility\nsignals and veracity, and perform an ablation study showing the impact of each\nsignal on model performance. Our findings reveal that 12 out of the 19 proposed\nsignals exhibit strong associations with veracity across all datasets, while\nsome signals show domain-specific strengths.", "published": "2023-09-14 11:06:51", "link": "http://arxiv.org/abs/2309.07601v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Incorporating Class-based Language Model for Named Entity Recognition in\n  Factorized Neural Transducer", "abstract": "Despite advancements of end-to-end (E2E) models in speech recognition, named\nentity recognition (NER) is still challenging but critical for semantic\nunderstanding. Previous studies mainly focus on various rule-based or\nattention-based contextual biasing algorithms. However, their performance might\nbe sensitive to the biasing weight or degraded by excessive attention to the\nnamed entity list, along with a risk of false triggering. Inspired by the\nsuccess of the class-based language model (LM) in NER in conventional hybrid\nsystems and the effective decoupling of acoustic and linguistic information in\nthe factorized neural Transducer (FNT), we propose C-FNT, a novel E2E model\nthat incorporates class-based LMs into FNT. In C-FNT, the LM score of named\nentities can be associated with the name class instead of its surface form. The\nexperimental results show that our proposed C-FNT significantly reduces error\nin named entities without hurting performance in general word recognition.", "published": "2023-09-14 12:14:49", "link": "http://arxiv.org/abs/2309.07648v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Assessing the nature of large language models: A caution against\n  anthropocentrism", "abstract": "Generative AI models garnered a large amount of public attention and\nspeculation with the release of OpenAIs chatbot, ChatGPT. At least two opinion\ncamps exist: one excited about possibilities these models offer for fundamental\nchanges to human tasks, and another highly concerned about power these models\nseem to have. To address these concerns, we assessed several LLMs, primarily\nGPT 3.5, using standard, normed, and validated cognitive and personality\nmeasures. For this seedling project, we developed a battery of tests that\nallowed us to estimate the boundaries of some of these models capabilities, how\nstable those capabilities are over a short period of time, and how they compare\nto humans. Our results indicate that LLMs are unlikely to have developed\nsentience, although its ability to respond to personality inventories is\ninteresting. GPT3.5 did display large variability in both cognitive and\npersonality measures over repeated observations, which is not expected if it\nhad a human-like personality. Variability notwithstanding, LLMs display what in\na human would be considered poor mental health, including low self-esteem,\nmarked dissociation from reality, and in some cases narcissism and psychopathy,\ndespite upbeat and helpful responses.", "published": "2023-09-14 12:58:30", "link": "http://arxiv.org/abs/2309.07683v3", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC"], "primary_category": "cs.AI"}
{"title": "Tree of Uncertain Thoughts Reasoning for Large Language Models", "abstract": "While the recently introduced Tree of Thoughts (ToT) has heralded\nadvancements in allowing Large Language Models (LLMs) to reason through\nforesight and backtracking for global decision-making, it has overlooked the\ninherent local uncertainties in intermediate decision points or \"thoughts\".\nThese local uncertainties, intrinsic to LLMs given their potential for diverse\nresponses, remain a significant concern in the reasoning process. Addressing\nthis pivotal gap, we introduce the Tree of Uncertain Thoughts (TouT) - a\nreasoning framework tailored for LLMs. Our TouT effectively leverages Monte\nCarlo Dropout to quantify uncertainty scores associated with LLMs' diverse\nlocal responses at these intermediate steps. By marrying this local uncertainty\nquantification with global search algorithms, TouT enhances the model's\nprecision in response generation. We substantiate our approach with rigorous\nexperiments on two demanding planning tasks: Game of 24 and Mini Crosswords.\nThe empirical evidence underscores TouT's superiority over both ToT and\nchain-of-thought prompting methods.", "published": "2023-09-14 13:14:51", "link": "http://arxiv.org/abs/2309.07694v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CoLLD: Contrastive Layer-to-layer Distillation for Compressing\n  Multilingual Pre-trained Speech Encoders", "abstract": "Large-scale self-supervised pre-trained speech encoders outperform\nconventional approaches in speech recognition and translation tasks. Due to the\nhigh cost of developing these large models, building new encoders for new tasks\nand deploying them to on-device applications are infeasible. Prior studies\npropose model compression methods to address this issue, but those works focus\non smaller models and less realistic tasks. Thus, we propose Contrastive\nLayer-to-layer Distillation (CoLLD), a novel knowledge distillation method to\ncompress pre-trained speech encoders by leveraging masked prediction and\ncontrastive learning to train student models to copy the behavior of a large\nteacher model. CoLLD outperforms prior methods and closes the gap between small\nand large models on multilingual speech-to-text translation and recognition\nbenchmarks.", "published": "2023-09-14 13:38:02", "link": "http://arxiv.org/abs/2309.07707v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "L1-aware Multilingual Mispronunciation Detection Framework", "abstract": "The phonological discrepancies between a speaker's native (L1) and the\nnon-native language (L2) serves as a major factor for mispronunciation. This\npaper introduces a novel multilingual MDD architecture, L1-MultiMDD, enriched\nwith L1-aware speech representation. An end-to-end speech encoder is trained on\nthe input signal and its corresponding reference phoneme sequence. First, an\nattention mechanism is deployed to align the input audio with the reference\nphoneme sequence. Afterwards, the L1-L2-speech embedding are extracted from an\nauxiliary model, pretrained in a multi-task setup identifying L1 and L2\nlanguage, and are infused with the primary network. Finally, the L1-MultiMDD is\nthen optimized for a unified multilingual phoneme recognition task using\nconnectionist temporal classification (CTC) loss for the target languages:\nEnglish, Arabic, and Mandarin. Our experiments demonstrate the effectiveness of\nthe proposed L1-MultiMDD framework on both seen -- L2-ARTIC, LATIC, and\nAraVoiceL2v2; and unseen -- EpaDB and Speechocean762 datasets. The consistent\ngains in PER, and false rejection rate (FRR) across all target languages\nconfirm our approach's robustness, efficacy, and generalizability.", "published": "2023-09-14 13:53:17", "link": "http://arxiv.org/abs/2309.07719v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Explaining Speech Classification Models via Word-Level Audio Segments\n  and Paralinguistic Features", "abstract": "Recent advances in eXplainable AI (XAI) have provided new insights into how\nmodels for vision, language, and tabular data operate. However, few approaches\nexist for understanding speech models. Existing work focuses on a few spoken\nlanguage understanding (SLU) tasks, and explanations are difficult to interpret\nfor most users. We introduce a new approach to explain speech classification\nmodels. We generate easy-to-interpret explanations via input perturbation on\ntwo information levels. 1) Word-level explanations reveal how each word-related\naudio segment impacts the outcome. 2) Paralinguistic features (e.g., prosody\nand background noise) answer the counterfactual: ``What would the model\nprediction be if we edited the audio signal in this way?'' We validate our\napproach by explaining two state-of-the-art SLU models on two speech\nclassification tasks in English and Italian. Our findings demonstrate that the\nexplanations are faithful to the model's inner workings and plausible to\nhumans. Our method and findings pave the way for future research on\ninterpreting speech models.", "published": "2023-09-14 14:12:34", "link": "http://arxiv.org/abs/2309.07733v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "The complementary roles of non-verbal cues for Robust Pronunciation\n  Assessment", "abstract": "Research on pronunciation assessment systems focuses on utilizing phonetic\nand phonological aspects of non-native (L2) speech, often neglecting the rich\nlayer of information hidden within the non-verbal cues. In this study, we\nproposed a novel pronunciation assessment framework, IntraVerbalPA. % The\nframework innovatively incorporates both fine-grained frame- and abstract\nutterance-level non-verbal cues, alongside the conventional speech and phoneme\nrepresentations. Additionally, we introduce ''Goodness of phonemic-duration''\nmetric to effectively model duration distribution within the framework. Our\nresults validate the effectiveness of the proposed IntraVerbalPA framework and\nits individual components, yielding performance that either matches or\noutperforms existing research works.", "published": "2023-09-14 14:18:07", "link": "http://arxiv.org/abs/2309.07739v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Echotune: A Modular Extractor Leveraging the Variable-Length Nature of\n  Speech in ASR Tasks", "abstract": "The Transformer architecture has proven to be highly effective for Automatic\nSpeech Recognition (ASR) tasks, becoming a foundational component for a\nplethora of research in the domain. Historically, many approaches have leaned\non fixed-length attention windows, which becomes problematic for varied speech\nsamples in duration and complexity, leading to data over-smoothing and neglect\nof essential long-term connectivity. Addressing this limitation, we introduce\nEcho-MSA, a nimble module equipped with a variable-length attention mechanism\nthat accommodates a range of speech sample complexities and durations. This\nmodule offers the flexibility to extract speech features across various\ngranularities, spanning from frames and phonemes to words and discourse. The\nproposed design captures the variable length feature of speech and addresses\nthe limitations of fixed-length attention. Our evaluation leverages a parallel\nattention architecture complemented by a dynamic gating mechanism that\namalgamates traditional attention with the Echo-MSA module output. Empirical\nevidence from our study reveals that integrating Echo-MSA into the primary\nmodel's training regime significantly enhances the word error rate (WER)\nperformance, all while preserving the intrinsic stability of the original\nmodel.", "published": "2023-09-14 14:51:51", "link": "http://arxiv.org/abs/2309.07765v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Spoken Humanoid Embodied Conversational Agents in Mobile Serious Games:\n  A Usability Assessment", "abstract": "This paper presents an empirical investigation of the extent to which spoken\nHumanoid Embodied Conversational Agents (HECAs) can foster usability in mobile\nserious game (MSG) applications. The aim of the research is to assess the\nimpact of multiple agents and illusion of humanness on the quality of the\ninteraction. The experiment investigates two styles of agent presentation: an\nagent of high human-likeness (HECA) and an agent of low human-likeness (text).\nThe purpose of the experiment is to assess whether and how agents of high\nhumanlikeness can evoke the illusion of humanness and affect usability. Agents\nof high human-likeness were designed by following the ECA design model that is\na proposed guide for ECA development. The results of the experiment with 90\nparticipants show that users prefer to interact with the HECAs. The difference\nbetween the two versions is statistically significant with a large effect size\n(d=1.01), with many of the participants justifying their choice by saying that\nthe human-like characteristics of the HECA made the version more appealing.\nThis research provides key information on the potential effect of HECAs on\nserious games, which can provide insight into the design of future mobile\nserious games.", "published": "2023-09-14 15:02:05", "link": "http://arxiv.org/abs/2309.07773v3", "categories": ["cs.HC", "cs.CL", "cs.MM", "H.5.2; H.5.1"], "primary_category": "cs.HC"}
{"title": "Improving Multimodal Classification of Social Media Posts by Leveraging\n  Image-Text Auxiliary Tasks", "abstract": "Effectively leveraging multimodal information from social media posts is\nessential to various downstream tasks such as sentiment analysis, sarcasm\ndetection or hate speech classification. Jointly modeling text and images is\nchallenging because cross-modal semantics might be hidden or the relation\nbetween image and text is weak. However, prior work on multimodal\nclassification of social media posts has not yet addressed these challenges. In\nthis work, we present an extensive study on the effectiveness of using two\nauxiliary losses jointly with the main task during fine-tuning multimodal\nmodels. First, Image-Text Contrastive (ITC) is designed to minimize the\ndistance between image-text representations within a post, thereby effectively\nbridging the gap between posts where the image plays an important role in\nconveying the post's meaning. Second, Image-Text Matching (ITM) enhances the\nmodel's ability to understand the semantic relationship between images and\ntext, thus improving its capacity to handle ambiguous or loosely related\nmodalities. We combine these objectives with five multimodal models across five\ndiverse social media datasets, demonstrating consistent improvements of up to\n2.6 points F1. Our comprehensive analysis shows the specific scenarios where\neach auxiliary task is most effective.", "published": "2023-09-14 15:30:59", "link": "http://arxiv.org/abs/2309.07794v2", "categories": ["cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "CiwaGAN: Articulatory information exchange", "abstract": "Humans encode information into sounds by controlling articulators and decode\ninformation from sounds using the auditory apparatus. This paper introduces\nCiwaGAN, a model of human spoken language acquisition that combines\nunsupervised articulatory modeling with an unsupervised model of information\nexchange through the auditory modality. While prior research includes\nunsupervised articulatory modeling and information exchange separately, our\nmodel is the first to combine the two components. The paper also proposes an\nimproved articulatory model with more interpretable internal representations.\nThe proposed CiwaGAN model is the most realistic approximation of human spoken\nlanguage acquisition using deep learning. As such, it is useful for cognitively\nplausible simulations of the human speech act.", "published": "2023-09-14 17:10:39", "link": "http://arxiv.org/abs/2309.07861v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MMICL: Empowering Vision-language Model with Multi-Modal In-Context\n  Learning", "abstract": "Since the resurgence of deep learning, vision-language models (VLMs) enhanced\nby large language models (LLMs) have grown exponentially in popularity.\nHowever, while LLMs can utilize extensive background knowledge and task\ninformation with in-context learning, most VLMs still struggle with\nunderstanding complex multi-modal prompts with multiple images, making VLMs\nless effective in downstream vision-language tasks. In this paper, we address\nthe limitation above by 1) introducing vision-language Model with Multi-Modal\nIn-Context Learning(MMICL), a new approach to allow the VLM to deal with\nmulti-modal inputs efficiently; 2) proposing a novel context scheme to augment\nthe in-context learning ability of the VLM; 3) constructing the Multi-modal\nIn-Context Learning (MIC) dataset, designed to enhance the VLM's ability to\nunderstand complex multi-modal prompts. Our experiments confirm that MMICL\nachieves new state-of-the-art zero-shot performance on a wide range of general\nvision-language tasks, especially for complex benchmarks, including MME and\nMMBench. Our analysis demonstrates that MMICL effectively tackles the challenge\nof complex multi-modal prompt understanding and emerges the impressive ICL\nability. Furthermore, we observe that MMICL successfully alleviates language\nbias in VLMs, a common issue for VLMs that often leads to hallucination when\nfaced with extensive textual context. Our code, dataset, dataset tool, and\nmodel are available at https://github.com/PKUnlp-icler/MIC", "published": "2023-09-14 17:59:17", "link": "http://arxiv.org/abs/2309.07915v3", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "DiariST: Streaming Speech Translation with Speaker Diarization", "abstract": "End-to-end speech translation (ST) for conversation recordings involves\nseveral under-explored challenges such as speaker diarization (SD) without\naccurate word time stamps and handling of overlapping speech in a streaming\nfashion. In this work, we propose DiariST, the first streaming ST and SD\nsolution. It is built upon a neural transducer-based streaming ST system and\nintegrates token-level serialized output training and t-vector, which were\noriginally developed for multi-talker speech recognition. Due to the absence of\nevaluation benchmarks in this area, we develop a new evaluation dataset,\nDiariST-AliMeeting, by translating the reference Chinese transcriptions of the\nAliMeeting corpus into English. We also propose new metrics, called\nspeaker-agnostic BLEU and speaker-attributed BLEU, to measure the ST quality\nwhile taking SD accuracy into account. Our system achieves a strong ST and SD\ncapability compared to offline systems based on Whisper, while performing\nstreaming inference for overlapping speech. To facilitate the research in this\nnew direction, we release the evaluation data, the offline baseline systems,\nand the evaluation code.", "published": "2023-09-14 19:33:27", "link": "http://arxiv.org/abs/2309.08007v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "AV2Wav: Diffusion-Based Re-synthesis from Continuous Self-supervised\n  Features for Audio-Visual Speech Enhancement", "abstract": "Speech enhancement systems are typically trained using pairs of clean and\nnoisy speech. In audio-visual speech enhancement (AVSE), there is not as much\nground-truth clean data available; most audio-visual datasets are collected in\nreal-world environments with background noise and reverberation, hampering the\ndevelopment of AVSE. In this work, we introduce AV2Wav, a resynthesis-based\naudio-visual speech enhancement approach that can generate clean speech despite\nthe challenges of real-world training data. We obtain a subset of nearly clean\nspeech from an audio-visual corpus using a neural quality estimator, and then\ntrain a diffusion model on this subset to generate waveforms conditioned on\ncontinuous speech representations from AV-HuBERT with noise-robust training. We\nuse continuous rather than discrete representations to retain prosody and\nspeaker information. With this vocoding task alone, the model can perform\nspeech enhancement better than a masking-based baseline. We further fine-tune\nthe diffusion model on clean/noisy utterance pairs to improve the performance.\nOur approach outperforms a masking-based baseline in terms of both automatic\nmetrics and a human listening test and is close in quality to the target speech\nin the listening test. Audio samples can be found at\nhttps://home.ttic.edu/~jcchou/demo/avse/avse_demo.html.", "published": "2023-09-14 21:07:53", "link": "http://arxiv.org/abs/2309.08030v5", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Using Large Language Models to Generate, Validate, and Apply User Intent\n  Taxonomies", "abstract": "Log data can reveal valuable information about how users interact with Web\nsearch services, what they want, and how satisfied they are. However, analyzing\nuser intents in log data is not easy, especially for emerging forms of Web\nsearch such as AI-driven chat. To understand user intents from log data, we\nneed a way to label them with meaningful categories that capture their\ndiversity and dynamics. Existing methods rely on manual or machine-learned\nlabeling, which are either expensive or inflexible for large and dynamic\ndatasets. We propose a novel solution using large language models (LLMs), which\ncan generate rich and relevant concepts, descriptions, and examples for user\nintents. However, using LLMs to generate a user intent taxonomy and apply it\nfor log analysis can be problematic for two main reasons: (1) such a taxonomy\nis not externally validated; and (2) there may be an undesirable feedback loop.\nTo address this, we propose a new methodology with human experts and assessors\nto verify the quality of the LLM-generated taxonomy. We also present an\nend-to-end pipeline that uses an LLM with human-in-the-loop to produce, refine,\nand apply labels for user intent analysis in log data. We demonstrate its\neffectiveness by uncovering new insights into user intents from search and chat\nlogs from the Microsoft Bing commercial search engine. The proposed work's\nnovelty stems from the method for generating purpose-driven user intent\ntaxonomies with strong validation. This method not only helps remove\nmethodological and practical bottlenecks from intent-focused research, but also\nprovides a new framework for generating, validating, and applying other kinds\nof taxonomies in a scalable and adaptable way with reasonable human effort.", "published": "2023-09-14 20:46:48", "link": "http://arxiv.org/abs/2309.13063v3", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "ChatGPT v Bard v Bing v Claude 2 v Aria v human-expert. How good are AI\n  chatbots at scientific writing?", "abstract": "Historical emphasis on writing mastery has shifted with advances in\ngenerative AI, especially in scientific writing. This study analysed six AI\nchatbots for scholarly writing in humanities and archaeology. Using methods\nthat assessed factual correctness and scientific contribution, ChatGPT-4 showed\nthe highest quantitative accuracy, closely followed by ChatGPT-3.5, Bing, and\nBard. However, Claude 2 and Aria scored considerably lower. Qualitatively, all\nAIs exhibited proficiency in merging existing knowledge, but none produced\noriginal scientific content. Inter-estingly, our findings suggest ChatGPT-4\nmight represent a plateau in large language model size. This research\nemphasizes the unique, intricate nature of human research, suggesting that AI's\nemulation of human originality in scientific writing is challenging. As of\n2023, while AI has transformed content generation, it struggles with original\ncontributions in humanities. This may change as AI chatbots continue to evolve\ninto LLM-powered software.", "published": "2023-09-14 14:04:03", "link": "http://arxiv.org/abs/2309.08636v3", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.ET", "cs.HC", "68T01", "I.2.0"], "primary_category": "cs.CL"}
{"title": "Training Audio Captioning Models without Audio", "abstract": "Automated Audio Captioning (AAC) is the task of generating natural language\ndescriptions given an audio stream. A typical AAC system requires manually\ncurated training data of audio segments and corresponding text caption\nannotations. The creation of these audio-caption pairs is costly, resulting in\ngeneral data scarcity for the task. In this work, we address this major\nlimitation and propose an approach to train AAC systems using only text. Our\napproach leverages the multimodal space of contrastively trained audio-text\nmodels, such as CLAP. During training, a decoder generates captions conditioned\non the pretrained CLAP text encoder. During inference, the text encoder is\nreplaced with the pretrained CLAP audio encoder. To bridge the modality gap\nbetween text and audio embeddings, we propose the use of noise injection or a\nlearnable adapter, during training. We find that the proposed text-only\nframework performs competitively with state-of-the-art models trained with\npaired audio, showing that efficient text-to-audio transfer is possible.\nFinally, we showcase both stylized audio captioning and caption enrichment\nwhile training without audio or human-created text captions.", "published": "2023-09-14 01:16:02", "link": "http://arxiv.org/abs/2309.07372v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Towards Universal Speech Discrete Tokens: A Case Study for ASR and TTS", "abstract": "Self-supervised learning (SSL) proficiency in speech-related tasks has driven\nresearch into utilizing discrete tokens for speech tasks like recognition and\ntranslation, which offer lower storage requirements and great potential to\nemploy natural language processing techniques. However, these studies, mainly\nsingle-task focused, faced challenges like overfitting and performance\ndegradation in speech recognition tasks, often at the cost of sacrificing\nperformance in multi-task scenarios. This study presents a comprehensive\ncomparison and optimization of discrete tokens generated by various leading SSL\nmodels in speech recognition and synthesis tasks. We aim to explore the\nuniversality of speech discrete tokens across multiple speech tasks.\nExperimental results demonstrate that discrete tokens achieve comparable\nresults against systems trained on FBank features in speech recognition tasks\nand outperform mel-spectrogram features in speech synthesis in subjective and\nobjective metrics. These findings suggest that universal discrete tokens have\nenormous potential in various speech-related tasks. Our work is open-source and\npublicly available at https://github.com/k2-fsa/icefall.", "published": "2023-09-14 01:39:43", "link": "http://arxiv.org/abs/2309.07377v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multi-dimensional Speech Quality Assessment in Crowdsourcing", "abstract": "Subjective speech quality assessment is the gold standard for evaluating\nspeech enhancement processing and telecommunication systems. The commonly used\nstandard ITU-T Rec. P.800 defines how to measure speech quality in lab\nenvironments, and ITU-T Rec.~P.808 extended it for crowdsourcing. ITU-T Rec.\nP.835 extends P.800 to measure the quality of speech in the presence of noise.\nITU-T Rec. P.804 targets the conversation test and introduces perceptual speech\nquality dimensions which are measured during the listening phase of the\nconversation. The perceptual dimensions are noisiness, coloration,\ndiscontinuity, and loudness. We create a crowdsourcing implementation of a\nmulti-dimensional subjective test following the scales from P.804 and extend it\nto include reverberation, the speech signal, and overall quality. We show the\ntool is both accurate and reproducible. The tool has been used in the ICASSP\n2023 Speech Signal Improvement challenge and we show the utility of these\nspeech quality dimensions in this challenge. The tool will be publicly\navailable as open-source at https://github.com/microsoft/P.808.", "published": "2023-09-14 02:04:02", "link": "http://arxiv.org/abs/2309.07385v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "BANC: Towards Efficient Binaural Audio Neural Codec for Overlapping\n  Speech", "abstract": "We introduce BANC, a neural binaural audio codec designed for efficient\nspeech compression in single and two-speaker scenarios while preserving the\nspatial location information of each speaker. Our key contributions are as\nfollows: 1) The ability of our proposed model to compress and decode\noverlapping speech. 2) A novel architecture that compresses speech content and\nspatial cues separately, ensuring the preservation of each speaker's spatial\ncontext after decoding. 3) BANC's proficiency in reducing the bandwidth\nrequired for compressing binaural speech by 48% compared to compressing\nindividual binaural channels. In our evaluation, we employed speech\nenhancement, room acoustics, and perceptual metrics to assess the accuracy of\nBANC's clean speech and spatial cue estimates.", "published": "2023-09-14 04:04:50", "link": "http://arxiv.org/abs/2309.07416v4", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Mandarin Lombard Flavor Classification", "abstract": "The Lombard effect refers to individuals' unconscious modulation of vocal\neffort in response to variations in the ambient noise levels, intending to\nenhance speech intelligibility. The impact of different decibel levels and\ntypes of background noise on Lombard effects remains unclear. Building upon the\ncharacteristic of Lombard speech that individuals adjust their speech to\nimprove intelligibility dynamically based on the self-feedback speech, we\npropose a flavor classification approach for the Lombard effect. We first\ncollected Mandarin Lombard speech under different noise conditions, then\nsimulated self-feedback speech, and ultimately conducted the statistical test\non the word correct rate. We found that both SSN and babble noise types result\nin four distinct categories of Mandarin Lombard speech in the range of 30 to 80\ndBA with different transition points.", "published": "2023-09-14 04:24:07", "link": "http://arxiv.org/abs/2309.07419v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SpatialCodec: Neural Spatial Speech Coding", "abstract": "In this work, we address the challenge of encoding speech captured by a\nmicrophone array using deep learning techniques with the aim of preserving and\naccurately reconstructing crucial spatial cues embedded in multi-channel\nrecordings. We propose a neural spatial audio coding framework that achieves a\nhigh compression ratio, leveraging single-channel neural sub-band codec and\nSpatialCodec. Our approach encompasses two phases: (i) a neural sub-band codec\nis designed to encode the reference channel with low bit rates, and (ii), a\nSpatialCodec captures relative spatial information for accurate multi-channel\nreconstruction at the decoder end. In addition, we also propose novel\nevaluation metrics to assess the spatial cue preservation: (i) spatial\nsimilarity, which calculates cosine similarity on a spatially intuitive\nbeamspace, and (ii), beamformed audio quality. Our system shows superior\nspatial performance compared with high bitrate baselines and black-box neural\narchitecture. Demos are available at https://xzwy.github.io/SpatialCodecDemo.\nCodes and models are available at https://github.com/XZWY/SpatialCodec.", "published": "2023-09-14 05:28:05", "link": "http://arxiv.org/abs/2309.07432v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Analysis of Speech Separation Performance Degradation on Emotional\n  Speech Mixtures", "abstract": "Despite recent strides made in Speech Separation, most models are trained on\ndatasets with neutral emotions. Emotional speech has been known to degrade\nperformance of models in a variety of speech tasks, which reduces the\neffectiveness of these models when deployed in real-world scenarios. In this\npaper we perform analysis to differentiate the performance degradation arising\nfrom the emotions in speech from the impact of out-of-domain inference. This is\nmeasured using a carefully designed test dataset, Emo2Mix, consisting of\nbalanced data across all emotional combinations. We show that even models with\nstrong out-of-domain performance such as Sepformer can still suffer significant\ndegradation of up to 5.1 dB SI-SDRi on mixtures with strong emotions. This\ndemonstrates the importance of accounting for emotions in real-world speech\nseparation applications.", "published": "2023-09-14 06:35:37", "link": "http://arxiv.org/abs/2309.07458v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Codec Data Augmentation for Time-domain Heart Sound Classification", "abstract": "Heart auscultations are a low-cost and effective way of detecting valvular\nheart diseases early, which can save lives. Nevertheless, it has been difficult\nto scale this screening method since the effectiveness of auscultations is\ndependent on the skill of doctors. As such, there has been increasing research\ninterest in the automatic classification of heart sounds using deep learning\nalgorithms. However, it is currently difficult to develop good heart sound\nclassification models due to the limited data available for training. In this\nwork, we propose a simple time domain approach, to the heart sound\nclassification problem with a base classification error rate of 0.8 and show\nthat augmentation of the data through codec simulation can improve the\nclassification error rate to 0.2. With data augmentation, our approach\noutperforms the existing time-domain CNN-BiLSTM baseline model. Critically, our\nexperiments show that codec data augmentation is effective in getting around\nthe data limitation.", "published": "2023-09-14 06:47:21", "link": "http://arxiv.org/abs/2309.07466v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Hierarchical Metadata Information Constrained Self-Supervised Learning\n  for Anomalous Sound Detection Under Domain Shift", "abstract": "Self-supervised learning methods have achieved promising performance for\nanomalous sound detection (ASD) under domain shift, where the type of domain\nshift is considered in feature learning by incorporating section IDs. However,\nthe attributes accompanying audio files under each section, such as machine\noperating conditions and noise types, have not been considered, although they\nare also crucial for characterizing domain shifts. In this paper, we present a\nhierarchical metadata information constrained self-supervised (HMIC) ASD\nmethod, where the hierarchical relation between section IDs and attributes is\nconstructed, and used as constraints to obtain finer feature representation. In\naddition, we propose an attribute-group-center (AGC)-based method for\ncalculating the anomaly score under the domain shift condition. Experiments are\nperformed to demonstrate its improved performance over the state-of-the-art\nself-supervised methods in DCASE 2022 challenge Task 2.", "published": "2023-09-14 08:05:10", "link": "http://arxiv.org/abs/2309.07498v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Outlier-aware Inlier Modeling and Multi-scale Scoring for Anomalous\n  Sound Detection via Multitask Learning", "abstract": "This paper proposes an approach for anomalous sound detection that\nincorporates outlier exposure and inlier modeling within a unified framework by\nmultitask learning. While outlier exposure-based methods can extract features\nefficiently, it is not robust. Inlier modeling is good at generating robust\nfeatures, but the features are not very effective. Recently, serial approaches\nare proposed to combine these two methods, but it still requires a separate\ntraining step for normal data modeling. To overcome these limitations, we use\nmultitask learning to train a conformer-based encoder for outlier-aware inlier\nmodeling. Moreover, our approach provides multi-scale scores for detecting\nanomalies. Experimental results on the MIMII and DCASE 2020 task 2 datasets\nshow that our approach outperforms state-of-the-art single-model systems and\nachieves comparable results with top-ranked multi-system ensembles.", "published": "2023-09-14 08:08:25", "link": "http://arxiv.org/abs/2309.07500v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Emo-StarGAN: A Semi-Supervised Any-to-Many Non-Parallel\n  Emotion-Preserving Voice Conversion", "abstract": "Speech anonymisation prevents misuse of spoken data by removing any personal\nidentifier while preserving at least linguistic content. However, emotion\npreservation is crucial for natural human-computer interaction. The well-known\nvoice conversion technique StarGANv2-VC achieves anonymisation but fails to\npreserve emotion. This work presents an any-to-many semi-supervised\nStarGANv2-VC variant trained on partially emotion-labelled non-parallel data.\nWe propose emotion-aware losses computed on the emotion embeddings and acoustic\nfeatures correlated to emotion. Additionally, we use an emotion classifier to\nprovide direct emotion supervision. Objective and subjective evaluations show\nthat the proposed approach significantly improves emotion preservation over the\nvanilla StarGANv2-VC. This considerable improvement is seen over diverse\ndatasets, emotions, target speakers, and inter-group conversions without\ncompromising intelligibility and anonymisation.", "published": "2023-09-14 10:40:43", "link": "http://arxiv.org/abs/2309.07586v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "StarGAN-VC++: Towards Emotion Preserving Voice Conversion Using Deep\n  Embeddings", "abstract": "Voice conversion (VC) transforms an utterance to sound like another person\nwithout changing the linguistic content. A recently proposed generative\nadversarial network-based VC method, StarGANv2-VC is very successful in\ngenerating natural-sounding conversions. However, the method fails to preserve\nthe emotion of the source speaker in the converted samples. Emotion\npreservation is necessary for natural human-computer interaction. In this\npaper, we show that StarGANv2-VC fails to disentangle the speaker and emotion\nrepresentations, pertinent to preserve emotion. Specifically, there is an\nemotion leakage from the reference audio used to capture the speaker embeddings\nwhile training. To counter the problem, we propose novel emotion-aware losses\nand an unsupervised method which exploits emotion supervision through latent\nemotion representations. The objective and subjective evaluations prove the\nefficacy of the proposed strategy over diverse datasets, emotions, gender, etc.", "published": "2023-09-14 10:52:32", "link": "http://arxiv.org/abs/2309.07592v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "AAS-VC: On the Generalization Ability of Automatic Alignment Search\n  based Non-autoregressive Sequence-to-sequence Voice Conversion", "abstract": "Non-autoregressive (non-AR) sequence-to-seqeunce (seq2seq) models for voice\nconversion (VC) is attractive in its ability to effectively model the temporal\nstructure while enjoying boosted intelligibility and fast inference thanks to\nnon-AR modeling. However, the dependency of current non-AR seq2seq VC models on\nground truth durations extracted from an external AR model greatly limits its\ngeneralization ability to smaller training datasets. In this paper, we first\ndemonstrate the above-mentioned problem by varying the training data size.\nThen, we present AAS-VC, a non-AR seq2seq VC model based on automatic alignment\nsearch (AAS), which removes the dependency on external durations and serves as\na proper inductive bias to provide the required generalization ability for\nsmall datasets. Experimental results show that AAS-VC can generalize better to\na training dataset of only 5 minutes. We also conducted ablation studies to\njustify several model design choices. The audio samples and implementation are\navailable online.", "published": "2023-09-14 10:59:29", "link": "http://arxiv.org/abs/2309.07598v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multilingual Audio Captioning using machine translated data", "abstract": "Automated Audio Captioning (AAC) systems attempt to generate a natural\nlanguage sentence, a caption, that describes the content of an audio recording,\nin terms of sound events. Existing datasets provide audio-caption pairs, with\ncaptions written in English only. In this work, we explore multilingual AAC,\nusing machine translated captions. We translated automatically two prominent\nAAC datasets, AudioCaps and Clotho, from English to French, German and Spanish.\nWe trained and evaluated monolingual systems in the four languages, on\nAudioCaps and Clotho. In all cases, the models achieved similar performance,\nabout 75% CIDEr on AudioCaps and 43% on Clotho. In French, we acquired manual\ncaptions of the AudioCaps eval subset. The French system, trained on the\nmachine translated version of AudioCaps, achieved significantly better results\non the manual eval subset, compared to the English system for which we\nautomatically translated the outputs to French. This advocates in favor of\nbuilding systems in a target language instead of simply translating to a target\nlanguage the English captions from the English system. Finally, we built a\nmultilingual model, which achieved results in each language comparable to each\nmonolingual system, while using much less parameters than using a collection of\nmonolingual systems.", "published": "2023-09-14 11:24:55", "link": "http://arxiv.org/abs/2309.07615v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DDSP-based Neural Waveform Synthesis of Polyphonic Guitar Performance\n  from String-wise MIDI Input", "abstract": "We explore the use of neural synthesis for acoustic guitar from string-wise\nMIDI input. We propose four different systems and compare them with both\nobjective metrics and subjective evaluation against natural audio and a\nsample-based baseline. We iteratively develop these four systems by making\nvarious considerations on the architecture and intermediate tasks, such as\npredicting pitch and loudness control features. We find that formulating the\ncontrol feature prediction task as a classification task rather than a\nregression task yields better results. Furthermore, we find that our simplest\nproposed system, which directly predicts synthesis parameters from MIDI input\nperforms the best out of the four proposed systems. Audio examples are\navailable at https://erl-j.github.io/neural-guitar-web-supplement.", "published": "2023-09-14 12:23:09", "link": "http://arxiv.org/abs/2309.07658v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Complexity Scaling for Speech Denoising", "abstract": "Computational complexity is critical when deploying deep learning-based\nspeech denoising models for on-device applications. Most prior research focused\non optimizing model architectures to meet specific computational cost\nconstraints, often creating distinct neural network architectures for different\ncomplexity limitations. This study conducts complexity scaling for speech\ndenoising tasks, aiming to consolidate models with various complexities into a\nunified architecture. We present a Multi-Path Transform-based (MPT)\narchitecture to handle both low- and high-complexity scenarios. A series of MPT\nnetworks present high performance covering a wide range of computational\ncomplexities on the DNS challenge dataset. Moreover, inspired by the scaling\nexperiments in natural language processing, we explore the empirical\nrelationship between model performance and computational cost on the denoising\ntask. As the complexity number of multiply-accumulate operations (MACs) is\nscaled from 50M/s to 15G/s on MPT networks, we observe a linear increase in the\nvalues of PESQ-WB and SI-SNR, proportional to the logarithm of MACs, which\nmight contribute to the understanding and application of complexity scaling in\nspeech denoising tasks.", "published": "2023-09-14 14:45:17", "link": "http://arxiv.org/abs/2309.07757v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SnakeGAN: A Universal Vocoder Leveraging DDSP Prior Knowledge and\n  Periodic Inductive Bias", "abstract": "Generative adversarial network (GAN)-based neural vocoders have been widely\nused in audio synthesis tasks due to their high generation quality, efficient\ninference, and small computation footprint. However, it is still challenging to\ntrain a universal vocoder which can generalize well to out-of-domain (OOD)\nscenarios, such as unseen speaking styles, non-speech vocalization, singing,\nand musical pieces. In this work, we propose SnakeGAN, a GAN-based universal\nvocoder, which can synthesize high-fidelity audio in various OOD scenarios.\nSnakeGAN takes a coarse-grained signal generated by a differentiable digital\nsignal processing (DDSP) model as prior knowledge, aiming at recovering\nhigh-fidelity waveform from a Mel-spectrogram. We introduce periodic\nnonlinearities through the Snake activation function and anti-aliased\nrepresentation into the generator, which further brings the desired inductive\nbias for audio synthesis and significantly improves the extrapolation capacity\nfor universal vocoding in unseen scenarios. To validate the effectiveness of\nour proposed method, we train SnakeGAN with only speech data and evaluate its\nperformance for various OOD distributions with both subjective and objective\nmetrics. Experimental results show that SnakeGAN significantly outperforms the\ncompared approaches and can generate high-fidelity audio samples including\nunseen speakers with unseen styles, singing voices, instrumental pieces, and\nnonverbal vocalization.", "published": "2023-09-14 15:46:39", "link": "http://arxiv.org/abs/2309.07803v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "VoicePAT: An Efficient Open-source Evaluation Toolkit for Voice Privacy\n  Research", "abstract": "Speaker anonymization is the task of modifying a speech recording such that\nthe original speaker cannot be identified anymore. Since the first Voice\nPrivacy Challenge in 2020, along with the release of a framework, the\npopularity of this research topic is continually increasing. However, the\ncomparison and combination of different anonymization approaches remains\nchallenging due to the complexity of evaluation and the absence of\nuser-friendly research frameworks. We therefore propose an efficient speaker\nanonymization and evaluation framework based on a modular and easily extendable\nstructure, almost fully in Python. The framework facilitates the orchestration\nof several anonymization approaches in parallel and allows for interfacing\nbetween different techniques. Furthermore, we propose modifications to common\nevaluation methods which improves the quality of the evaluation and reduces\ntheir computation time by 65 to 95%, depending on the metric. Our code is fully\nopen source.", "published": "2023-09-14 22:22:15", "link": "http://arxiv.org/abs/2309.08049v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DDSP-SFX: Acoustically-guided sound effects generation with\n  differentiable digital signal processing", "abstract": "Controlling the variations of sound effects using neural audio synthesis\nmodels has been a difficult task. Differentiable digital signal processing\n(DDSP) provides a lightweight solution that achieves high-quality sound\nsynthesis while enabling deterministic acoustic attribute control by\nincorporating pre-processed audio features and digital synthesizers. In this\nresearch, we introduce DDSP-SFX, a model based on the DDSP architecture capable\nof synthesizing high-quality sound effects while enabling users to control the\ntimbre variations easily. We propose a transient modelling technique with\nhigher objective evaluation scores and subjective ratings over impulsive\nsignals (footsteps, gunshots). We propose a simple method that achieves timbre\nvariation control while also allowing deterministic attribute control. We\nfurther qualitatively show the timbre transfer performance using voice as the\nguiding sound.", "published": "2023-09-14 23:13:48", "link": "http://arxiv.org/abs/2309.08060v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Diff-SV: A Unified Hierarchical Framework for Noise-Robust Speaker\n  Verification Using Score-Based Diffusion Probabilistic Models", "abstract": "Background noise considerably reduces the accuracy and reliability of speaker\nverification (SV) systems. These challenges can be addressed using a speech\nenhancement system as a front-end module. Recently, diffusion probabilistic\nmodels (DPMs) have exhibited remarkable noise-compensation capabilities in the\nspeech enhancement domain. Building on this success, we propose Diff-SV, a\nnoise-robust SV framework that leverages DPM. Diff-SV unifies a DPM-based\nspeech enhancement system with a speaker embedding extractor, and yields a\ndiscriminative and noise-tolerable speaker representation through a\nhierarchical structure. The proposed model was evaluated under both in-domain\nand out-of-domain noisy conditions using the VoxCeleb1 test set, an external\nnoise source, and the VOiCES corpus. The obtained experimental results\ndemonstrate that Diff-SV achieves state-of-the-art performance, outperforming\nrecently proposed noise-robust SV systems.", "published": "2023-09-14 10:43:19", "link": "http://arxiv.org/abs/2309.08320v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "EnCodecMAE: Leveraging neural codecs for universal audio representation\n  learning", "abstract": "The goal of universal audio representation learning is to obtain foundational\nmodels that can be used for a variety of downstream tasks involving speech,\nmusic and environmental sounds. To approach this problem, methods inspired by\nworks on self-supervised learning for NLP, like BERT, or computer vision, like\nmasked autoencoders (MAE), are often adapted to the audio domain. In this work,\nwe propose masking representations of the audio signal, and training a MAE to\nreconstruct the masked segments. The reconstruction is done by predicting the\ndiscrete units generated by EnCodec, a neural audio codec, from the unmasked\ninputs. We evaluate this approach, which we call EnCodecMAE, on a wide range of\ntasks involving speech, music and environmental sounds. Our best model\noutperforms various state-of-the-art audio representation models in terms of\nglobal performance. Additionally, we evaluate the resulting representations in\nthe challenging task of automatic speech recognition (ASR), obtaining decent\nresults and paving the way for a universal audio representation.", "published": "2023-09-14 02:21:53", "link": "http://arxiv.org/abs/2309.07391v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "FunCodec: A Fundamental, Reproducible and Integrable Open-source Toolkit\n  for Neural Speech Codec", "abstract": "This paper presents FunCodec, a fundamental neural speech codec toolkit,\nwhich is an extension of the open-source speech processing toolkit FunASR.\nFunCodec provides reproducible training recipes and inference scripts for the\nlatest neural speech codec models, such as SoundStream and Encodec. Thanks to\nthe unified design with FunASR, FunCodec can be easily integrated into\ndownstream tasks, such as speech recognition. Along with FunCodec, pre-trained\nmodels are also provided, which can be used for academic or generalized\npurposes. Based on the toolkit, we further propose the frequency-domain codec\nmodels, FreqCodec, which can achieve comparable speech quality with much lower\ncomputation and parameter complexity. Experimental results show that, under the\nsame compression ratio, FunCodec can achieve better reconstruction quality\ncompared with other toolkits and released models. We also demonstrate that the\npre-trained models are suitable for downstream tasks, including automatic\nspeech recognition and personalized text-to-speech synthesis. This toolkit is\npublicly available at https://github.com/alibaba-damo-academy/FunCodec.", "published": "2023-09-14 03:18:24", "link": "http://arxiv.org/abs/2309.07405v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SingFake: Singing Voice Deepfake Detection", "abstract": "The rise of singing voice synthesis presents critical challenges to artists\nand industry stakeholders over unauthorized voice usage. Unlike synthesized\nspeech, synthesized singing voices are typically released in songs containing\nstrong background music that may hide synthesis artifacts. Additionally,\nsinging voices present different acoustic and linguistic characteristics from\nspeech utterances. These unique properties make singing voice deepfake\ndetection a relevant but significantly different problem from synthetic speech\ndetection. In this work, we propose the singing voice deepfake detection task.\nWe first present SingFake, the first curated in-the-wild dataset consisting of\n28.93 hours of bonafide and 29.40 hours of deepfake song clips in five\nlanguages from 40 singers. We provide a train/validation/test split where the\ntest sets include various scenarios. We then use SingFake to evaluate four\nstate-of-the-art speech countermeasure systems trained on speech utterances. We\nfind these systems lag significantly behind their performance on speech test\ndata. When trained on SingFake, either using separated vocal tracks or song\nmixtures, these systems show substantial improvement. However, our evaluations\nalso identify challenges associated with unseen singers, communication codecs,\nlanguages, and musical contexts, calling for dedicated research into singing\nvoice deepfake detection. The SingFake dataset and related resources are\navailable at https://www.singfake.org/.", "published": "2023-09-14 08:49:05", "link": "http://arxiv.org/abs/2309.07525v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speech-to-Speech Translation with Discrete-Unit-Based Style Transfer", "abstract": "Direct speech-to-speech translation (S2ST) with discrete self-supervised\nrepresentations has achieved remarkable accuracy, but is unable to preserve the\nspeaker timbre of the source speech. Meanwhile, the scarcity of high-quality\nspeaker-parallel data poses a challenge for learning style transfer during\ntranslation. We design an S2ST pipeline with style-transfer capability on the\nbasis of discrete self-supervised speech representations and codec units. The\nacoustic language model we introduce for style transfer leverages\nself-supervised in-context learning, acquiring style transfer ability without\nrelying on any speaker-parallel data, thereby overcoming data scarcity. By\nusing extensive training data, our model achieves zero-shot cross-lingual style\ntransfer on previously unseen source languages. Experiments show that our model\ngenerates translated speeches with high fidelity and speaker similarity. Audio\nsamples are available at http://stylelm.github.io/ .", "published": "2023-09-14 09:52:08", "link": "http://arxiv.org/abs/2309.07566v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "EMOCONV-DIFF: Diffusion-based Speech Emotion Conversion for Non-parallel\n  and In-the-wild Data", "abstract": "Speech emotion conversion is the task of converting the expressed emotion of\na spoken utterance to a target emotion while preserving the lexical content and\nspeaker identity. While most existing works in speech emotion conversion rely\non acted-out datasets and parallel data samples, in this work we specifically\nfocus on more challenging in-the-wild scenarios and do not rely on parallel\ndata. To this end, we propose a diffusion-based generative model for speech\nemotion conversion, the EmoConv-Diff, that is trained to reconstruct an input\nutterance while also conditioning on its emotion. Subsequently, at inference, a\ntarget emotion embedding is employed to convert the emotion of the input\nutterance to the given target emotion. As opposed to performing emotion\nconversion on categorical representations, we use a continuous arousal\ndimension to represent emotions while also achieving intensity control. We\nvalidate the proposed methodology on a large in-the-wild dataset, the\nMSP-Podcast v1.10. Our results show that the proposed diffusion model is indeed\ncapable of synthesizing speech with a controllable target emotion. Crucially,\nthe proposed approach shows improved performance along the extreme values of\narousal and thereby addresses a common challenge in the speech emotion\nconversion literature.", "published": "2023-09-14 16:18:49", "link": "http://arxiv.org/abs/2309.07828v2", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Voxtlm: unified decoder-only models for consolidating speech\n  recognition/synthesis and speech/text continuation tasks", "abstract": "We propose a decoder-only language model, VoxtLM, that can perform four\ntasks: speech recognition, speech synthesis, text generation, and speech\ncontinuation. VoxtLM integrates text vocabulary with discrete speech tokens\nfrom self-supervised speech features and uses special tokens to enable\nmultitask learning. Compared to a single-task model, VoxtLM exhibits a\nsignificant improvement in speech synthesis, with improvements in both speech\nintelligibility from 28.9 to 5.6 and objective quality from 2.68 to 3.90.\nVoxtLM also improves speech generation and speech recognition performance over\nthe single-task counterpart. Further, VoxtLM is trained with publicly available\ndata and training recipes and model checkpoints are open-sourced to make fully\nreproducible work.", "published": "2023-09-14 03:13:18", "link": "http://arxiv.org/abs/2309.07937v3", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Folding Attention: Memory and Power Optimization for On-Device\n  Transformer-based Streaming Speech Recognition", "abstract": "Transformer-based models excel in speech recognition. Existing efforts to\noptimize Transformer inference, typically for long-context applications, center\non simplifying attention score calculations. However, streaming speech\nrecognition models usually process a limited number of tokens each time, making\nattention score calculation less of a bottleneck. Instead, the bottleneck lies\nin the linear projection layers of multi-head attention and feedforward\nnetworks, constituting a substantial portion of the model size and contributing\nsignificantly to computation, memory, and power usage.\n  To address this bottleneck, we propose folding attention, a technique\ntargeting these linear layers, significantly reducing model size and improving\nmemory and power efficiency. Experiments on on-device Transformer-based\nstreaming speech recognition models show that folding attention reduces model\nsize (and corresponding memory consumption) by up to 24% and power consumption\nby up to 23%, all without compromising model accuracy or computation overhead.", "published": "2023-09-14 19:01:08", "link": "http://arxiv.org/abs/2309.07988v3", "categories": ["cs.LG", "cs.AR", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Efficient Face Detection with Audio-Based Region Proposals for\n  Human-Robot Interactions", "abstract": "Efficient face detection is critical to provide natural human-robot\ninteractions. However, computer vision tends to involve a large computational\nload due to the amount of data (i.e. pixels) that needs to be processed in a\nshort amount of time. This is undesirable on robotics platforms where multiple\nprocesses need to run in parallel and where the processing power is limited by\nportability constraints. Existing solutions often involve reducing image\nquality which can negatively impact processing. The literature also reports\nmethods to generate regions of interest in images from pixel data. Although it\nis a promising idea, these methods often involve heavy vision algorithms. In\nthis paper, we evaluate how audio can be used to generate regions of interest\nin optical images to reduce the number of pixels to process with computer\nvision. Thereby, we propose a unique attention mechanism to localize a speech\nsource and evaluate its impact on an existing face detection algorithm. Our\nresults show that the attention mechanism reduces the computational load and\noffers an interesting trade-off between speed and accuracy. The proposed\npipeline is flexible and can be easily adapted to other applications such as\nrobot surveillance, video conferences or smart glasses.", "published": "2023-09-14 19:29:12", "link": "http://arxiv.org/abs/2309.08005v2", "categories": ["eess.AS", "cs.SD", "eess.IV"], "primary_category": "eess.AS"}
{"title": "USM-SCD: Multilingual Speaker Change Detection Based on Large Pretrained\n  Foundation Models", "abstract": "We introduce a multilingual speaker change detection model (USM-SCD) that can\nsimultaneously detect speaker turns and perform ASR for 96 languages. This\nmodel is adapted from a speech foundation model trained on a large quantity of\nsupervised and unsupervised data, demonstrating the utility of fine-tuning from\na large generic foundation model for a downstream task. We analyze the\nperformance of this multilingual speaker change detection model through a\nseries of ablation studies. We show that the USM-SCD model can achieve more\nthan 75% average speaker change detection F1 score across a test set that\nconsists of data from 96 languages. On American English, the USM-SCD model can\nachieve an 85.8% speaker change detection F1 score across various public and\ninternal test sets, beating the previous monolingual baseline model by 21%\nrelative. We also show that we only need to fine-tune one-quarter of the\ntrainable model parameters to achieve the best model performance. The USM-SCD\nmodel exhibits state-of-the-art ASR quality compared with a strong public ASR\nbaseline, making it suitable to handle both tasks with negligible additional\ncomputational cost.", "published": "2023-09-14 20:46:49", "link": "http://arxiv.org/abs/2309.08023v3", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Comparative Assessment of Markov Models and Recurrent Neural Networks\n  for Jazz Music Generation", "abstract": "As generative models have risen in popularity, a domain that has risen\nalongside is generative models for music. Our study aims to compare the\nperformance of a simple Markov chain model and a recurrent neural network (RNN)\nmodel, two popular models for sequence generating tasks, in jazz music\nimprovisation. While music, especially jazz, remains subjective in telling\nwhether a composition is \"good\" or \"bad\", we aim to quantify our results using\nmetrics of groove pattern similarity and pitch class histogram entropy. We\ntrained both models using transcriptions of jazz blues choruses from\nprofessional jazz players, and also fed musical jazz seeds to help give our\nmodel some context in beginning the generation. Our results show that the RNN\noutperforms the Markov model on both of our metrics, indicating better rhythmic\nconsistency and tonal stability in the generated music. Through the use of\nmusic21 library, we tokenized our jazz dataset into pitches and durations that\nour model could interpret and train on. Our findings contribute to the growing\nfield of AI-generated music, highlighting the important use of metrics to\nassess generation quality. Future work includes expanding the dataset of MIDI\nfiles to a larger scale, conducting human surveys for subjective evaluations,\nand incorporating additional metrics to address the challenge of subjectivity\nin music evaluation. Our study provides valuable insight into the use of\nrecurrent neural networks for sequential based tasks like generating music.", "published": "2023-09-14 20:57:45", "link": "http://arxiv.org/abs/2309.08027v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Retrieval-Augmented Text-to-Audio Generation", "abstract": "Despite recent progress in text-to-audio (TTA) generation, we show that the\nstate-of-the-art models, such as AudioLDM, trained on datasets with an\nimbalanced class distribution, such as AudioCaps, are biased in their\ngeneration performance. Specifically, they excel in generating common audio\nclasses while underperforming in the rare ones, thus degrading the overall\ngeneration performance. We refer to this problem as long-tailed text-to-audio\ngeneration. To address this issue, we propose a simple retrieval-augmented\napproach for TTA models. Specifically, given an input text prompt, we first\nleverage a Contrastive Language Audio Pretraining (CLAP) model to retrieve\nrelevant text-audio pairs. The features of the retrieved audio-text data are\nthen used as additional conditions to guide the learning of TTA models. We\nenhance AudioLDM with our proposed approach and denote the resulting augmented\nsystem as Re-AudioLDM. On the AudioCaps dataset, Re-AudioLDM achieves a\nstate-of-the-art Frechet Audio Distance (FAD) of 1.37, outperforming the\nexisting approaches by a large margin. Furthermore, we show that Re-AudioLDM\ncan generate realistic audio for complex scenes, rare audio classes, and even\nunseen audio types, indicating its potential in TTA tasks.", "published": "2023-09-14 22:35:39", "link": "http://arxiv.org/abs/2309.08051v2", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SLMIA-SR: Speaker-Level Membership Inference Attacks against Speaker\n  Recognition Systems", "abstract": "Membership inference attacks allow adversaries to determine whether a\nparticular example was contained in the model's training dataset. While\nprevious works have confirmed the feasibility of such attacks in various\napplications, none has focused on speaker recognition (SR), a promising\nvoice-based biometric recognition technique. In this work, we propose SLMIA-SR,\nthe first membership inference attack tailored to SR. In contrast to\nconventional example-level attack, our attack features speaker-level membership\ninference, i.e., determining if any voices of a given speaker, either the same\nas or different from the given inference voices, have been involved in the\ntraining of a model. It is particularly useful and practical since the training\nand inference voices are usually distinct, and it is also meaningful\nconsidering the open-set nature of SR, namely, the recognition speakers were\noften not present in the training data. We utilize intra-similarity and\ninter-dissimilarity, two training objectives of SR, to characterize the\ndifferences between training and non-training speakers and quantify them with\ntwo groups of features driven by carefully-established feature engineering to\nmount the attack. To improve the generalizability of our attack, we propose a\nnovel mixing ratio training strategy to train attack models. To enhance the\nattack performance, we introduce voice chunk splitting to cope with the limited\nnumber of inference voices and propose to train attack models dependent on the\nnumber of inference voices. Our attack is versatile and can work in both\nwhite-box and black-box scenarios. Additionally, we propose two novel\ntechniques to reduce the number of black-box queries while maintaining the\nattack performance. Extensive experiments demonstrate the effectiveness of\nSLMIA-SR.", "published": "2023-09-14 18:40:28", "link": "http://arxiv.org/abs/2309.07983v2", "categories": ["cs.CR", "cs.LG", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
