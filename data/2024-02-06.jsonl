{"title": "Professional Agents -- Evolving Large Language Models into Autonomous\n  Experts with Human-Level Competencies", "abstract": "The advent of large language models (LLMs) such as ChatGPT, PaLM, and GPT-4\nhas catalyzed remarkable advances in natural language processing, demonstrating\nhuman-like language fluency and reasoning capacities. This position paper\nintroduces the concept of Professional Agents (PAgents), an application\nframework harnessing LLM capabilities to create autonomous agents with\ncontrollable, specialized, interactive, and professional-level competencies. We\nposit that PAgents can reshape professional services through continuously\ndeveloped expertise. Our proposed PAgents framework entails a tri-layered\narchitecture for genesis, evolution, and synergy: a base tool layer, a middle\nagent layer, and a top synergy layer. This paper aims to spur discourse on\npromising real-world applications of LLMs. We argue the increasing\nsophistication and integration of PAgents could lead to AI systems exhibiting\nprofessional mastery over complex domains, serving critical needs, and\npotentially achieving artificial general intelligence.", "published": "2024-02-06 01:48:53", "link": "http://arxiv.org/abs/2402.03628v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "INSIDE: LLMs' Internal States Retain the Power of Hallucination\n  Detection", "abstract": "Knowledge hallucination have raised widespread concerns for the security and\nreliability of deployed LLMs. Previous efforts in detecting hallucinations have\nbeen employed at logit-level uncertainty estimation or language-level\nself-consistency evaluation, where the semantic information is inevitably lost\nduring the token-decoding procedure. Thus, we propose to explore the dense\nsemantic information retained within LLMs' \\textbf{IN}ternal \\textbf{S}tates\nfor halluc\\textbf{I}nation \\textbf{DE}tection (\\textbf{INSIDE}). In particular,\na simple yet effective \\textbf{EigenScore} metric is proposed to better\nevaluate responses' self-consistency, which exploits the eigenvalues of\nresponses' covariance matrix to measure the semantic consistency/diversity in\nthe dense embedding space. Furthermore, from the perspective of self-consistent\nhallucination detection, a test time feature clipping approach is explored to\ntruncate extreme activations in the internal states, which reduces\noverconfident generations and potentially benefits the detection of\noverconfident hallucinations. Extensive experiments and ablation studies are\nperformed on several popular LLMs and question-answering (QA) benchmarks,\nshowing the effectiveness of our proposal.", "published": "2024-02-06 06:23:12", "link": "http://arxiv.org/abs/2402.03744v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rethinking Skill Extraction in the Job Market Domain using Large\n  Language Models", "abstract": "Skill Extraction involves identifying skills and qualifications mentioned in\ndocuments such as job postings and resumes. The task is commonly tackled by\ntraining supervised models using a sequence labeling approach with BIO tags.\nHowever, the reliance on manually annotated data limits the generalizability of\nsuch approaches. Moreover, the common BIO setting limits the ability of the\nmodels to capture complex skill patterns and handle ambiguous mentions. In this\npaper, we explore the use of in-context learning to overcome these challenges,\non a benchmark of 6 uniformized skill extraction datasets. Our approach\nleverages the few-shot learning capabilities of large language models (LLMs) to\nidentify and extract skills from sentences. We show that LLMs, despite not\nbeing on par with traditional supervised models in terms of performance, can\nbetter handle syntactically complex skill mentions in skill extraction tasks.", "published": "2024-02-06 09:23:26", "link": "http://arxiv.org/abs/2402.03832v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Shifting social norms as a driving force for linguistic change:\n  Struggles about language and gender in the German Bundestag", "abstract": "This paper focuses on language change based on shifting social norms, in\nparticular with regard to the debate on language and gender. It is a recurring\nargument in this debate that language develops \"naturally\" and that \"severe\ninterventions\" - such as gender-inclusive language is often claimed to be - in\nthe allegedly \"organic\" language system are inappropriate and even \"dangerous\".\nSuch interventions are, however, not unprecedented. Socially motivated\nprocesses of language change are neither unusual nor new. We focus in our\ncontribution on one important political-social space in Germany, the German\nBundestag. Taking other struggles about language and gender in the plenaries of\nthe Bundestag as a starting point, our article illustrates that language and\ngender has been a recurring issue in the German Bundestag since the 1980s. We\ndemonstrate how this is reflected in linguistic practices of the Bundestag, by\nthe use of a) designations for gays and lesbians; b) pair forms such as\nB\\\"urgerinnen und B\\\"urger (female and male citizens); and c) female forms of\naddresses and personal nouns ('Pr\\\"asidentin' in addition to 'Pr\\\"asident').\nLastly, we will discuss implications of these earlier language battles for the\ncurrently very heated debate about gender-inclusive language, especially\nregarding new forms with gender symbols like the asterisk or the colon\n(Lehrer*innen, Lehrer:innen; male*female teachers) which are intended to\nencompass all gender identities.", "published": "2024-02-06 10:49:28", "link": "http://arxiv.org/abs/2402.03887v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pro-HAN: A Heterogeneous Graph Attention Network for Profile-Based\n  Spoken Language Understanding", "abstract": "Recently, Profile-based Spoken Language Understanding (SLU) has gained\nincreasing attention, which aims to incorporate various types of supplementary\nprofile information (i.e., Knowledge Graph, User Profile, Context Awareness) to\neliminate the prevalent ambiguities in user utterances. However, existing\napproaches can only separately model different profile information, without\nconsidering their interrelationships or excluding irrelevant and conflicting\ninformation within them. To address the above issues, we introduce a\nHeterogeneous Graph Attention Network to perform reasoning across multiple\nProfile information, called Pro-HAN. Specifically, we design three types of\nedges, denoted as intra-Pro, inter-Pro, and utterance-Pro, to capture\ninterrelationships among multiple Pros. We establish a new state-of-the-art on\nthe ProSLU dataset, with an improvement of approximately 8% across all three\nmetrics. Further analysis experiments also confirm the effectiveness of our\nmethod in modeling multi-source profile information.", "published": "2024-02-06 11:12:09", "link": "http://arxiv.org/abs/2402.03900v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sparse Graph Representations for Procedural Instructional Documents", "abstract": "Computation of document similarity is a critical task in various NLP domains\nthat has applications in deduplication, matching, and recommendation.\nTraditional approaches for document similarity computation include learning\nrepresentations of documents and employing a similarity or a distance function\nover the embeddings. However, pairwise similarities and differences are not\nefficiently captured by individual representations. Graph representations such\nas Joint Concept Interaction Graph (JCIG) represent a pair of documents as a\njoint undirected weighted graph. JCIGs facilitate an interpretable\nrepresentation of document pairs as a graph. However, JCIGs are undirected, and\ndon't consider the sequential flow of sentences in documents. We propose two\napproaches to model document similarity by representing document pairs as a\ndirected and sparse JCIG that incorporates sequential information. We propose\ntwo algorithms inspired by Supergenome Sorting and Hamiltonian Path that\nreplace the undirected edges with directed edges. Our approach also sparsifies\nthe graph to $O(n)$ edges from JCIG's worst case of $O(n^2)$. We show that our\nsparse directed graph model architecture consisting of a Siamese encoder and\nGCN achieves comparable results to the baseline on datasets not containing\nsequential information and beats the baseline by ten points on an instructional\ndocuments dataset containing sequential information.", "published": "2024-02-06 12:34:15", "link": "http://arxiv.org/abs/2402.03957v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Google Translate Error Analysis for Mental Healthcare Information:\n  Evaluating Accuracy, Comprehensibility, and Implications for Multilingual\n  Healthcare Communication", "abstract": "This study explores the use of Google Translate (GT) for translating mental\nhealthcare (MHealth) information and evaluates its accuracy, comprehensibility,\nand implications for multilingual healthcare communication through analysing GT\noutput in the MHealth domain from English to Persian, Arabic, Turkish,\nRomanian, and Spanish. Two datasets comprising MHealth information from the UK\nNational Health Service website and information leaflets from The Royal College\nof Psychiatrists were used. Native speakers of the target languages manually\nassessed the GT translations, focusing on medical terminology accuracy,\ncomprehensibility, and critical syntactic/semantic errors. GT output analysis\nrevealed challenges in accurately translating medical terminology, particularly\nin Arabic, Romanian, and Persian. Fluency issues were prevalent across various\nlanguages, affecting comprehension, mainly in Arabic and Spanish. Critical\nerrors arose in specific contexts, such as bullet-point formatting,\nspecifically in Persian, Turkish, and Romanian. Although improvements are seen\nin longer-text translations, there remains a need to enhance accuracy in\nmedical and mental health terminology and fluency, whilst also addressing\nformatting issues for a more seamless user experience. The findings highlight\nthe need to use customised translation engines for Mhealth translation and the\nchallenges when relying solely on machine-translated medical content,\nemphasising the crucial role of human reviewers in multilingual healthcare\ncommunication.", "published": "2024-02-06 14:16:32", "link": "http://arxiv.org/abs/2402.04023v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Iterative Prompt Refinement for Radiation Oncology Symptom Extraction\n  Using Teacher-Student Large Language Models", "abstract": "This study introduces a novel teacher-student architecture utilizing Large\nLanguage Models (LLMs) to improve prostate cancer radiotherapy symptom\nextraction from clinical notes. Mixtral, the student model, initially extracts\nsymptoms, followed by GPT-4, the teacher model, which refines prompts based on\nMixtral's performance. This iterative process involved 294 single symptom\nclinical notes across 12 symptoms, with up to 16 rounds of refinement per\nepoch. Results showed significant improvements in extracting symptoms from both\nsingle and multi-symptom notes. For 59 single symptom notes, accuracy increased\nfrom 0.51 to 0.71, precision from 0.52 to 0.82, recall from 0.52 to 0.72, and\nF1 score from 0.49 to 0.73. In 375 multi-symptom notes, accuracy rose from 0.24\nto 0.43, precision from 0.6 to 0.76, recall from 0.24 to 0.43, and F1 score\nfrom 0.20 to 0.44. These results demonstrate the effectiveness of advanced\nprompt engineering in LLMs for radiation oncology use.", "published": "2024-02-06 15:25:09", "link": "http://arxiv.org/abs/2402.04075v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Behind the Screen: Investigating ChatGPT's Dark Personality Traits and\n  Conspiracy Beliefs", "abstract": "ChatGPT is notorious for its intransparent behavior. This paper tries to shed\nlight on this, providing an in-depth analysis of the dark personality traits\nand conspiracy beliefs of GPT-3.5 and GPT-4. Different psychological tests and\nquestionnaires were employed, including the Dark Factor Test, the Mach-IV\nScale, the Generic Conspiracy Belief Scale, and the Conspiracy Mentality Scale.\nThe responses were analyzed computing average scores, standard deviations, and\nsignificance tests to investigate differences between GPT-3.5 and GPT-4. For\ntraits that have shown to be interdependent in human studies, correlations were\nconsidered. Additionally, system roles corresponding to groups that have shown\ndistinct answering behavior in the corresponding questionnaires were applied to\nexamine the models' ability to reflect characteristics associated with these\nroles in their responses. Dark personality traits and conspiracy beliefs were\nnot particularly pronounced in either model with little differences between\nGPT-3.5 and GPT-4. However, GPT-4 showed a pronounced tendency to believe in\ninformation withholding. This is particularly intriguing given that GPT-4 is\ntrained on a significantly larger dataset than GPT-3.5. Apparently, in this\ncase an increased data exposure correlates with a greater belief in the control\nof information. An assignment of extreme political affiliations increased the\nbelief in conspiracy theories. Test sequencing affected the models' responses\nand the observed correlations, indicating a form of contextual memory.", "published": "2024-02-06 16:03:57", "link": "http://arxiv.org/abs/2402.04110v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Harnessing the Plug-and-Play Controller by Prompting", "abstract": "Controllable text generation is a growing field within natural language\ngeneration (NLG) that focuses on producing text that meets specific constraints\nin real-world applications. Previous approaches, such as plug-and-play\ncontrollers (PPCs), aimed to steer the properties of generated text in a\nflexible manner. However, these methods often compromised the integrity of the\nlanguage model's decoding process, resulting in less smooth text generation.\nAlternatively, other techniques utilized multiple attribute prompts to align\nthe generated text with desired attributes, but this approach required prompt\ndesign for each attribute and was dependent on the size of the language model.\nThis paper introduces a novel method for flexible attribute control in text\ngeneration using pre-trained language models (PLMs). The proposed approach aims\nto enhance the fluency of generated text by guiding the generation process with\nPPCs. The key idea is to dynamically adjust the distribution of generated text\nby modifying prompts, effectively constraining the output space of the language\nmodel and influencing the desired attribute. To enable smooth cooperation\nbetween the PLM and the PPC, our work innovatively proposes a new model\nfine-tuning method: Reinforcement Learning with Dynamic Adjust Feedback\n(RLDAF).This fine-tuning process adapts a small subset of the language model's\nparameters based on the generating actions taken during the PPC control\nprocess. The resulting harmonious collaboration between the PLM and PPC leads\nto improved smoothness in text generation during inference. Extensive\nexperiments were conducted on the SST2 dataset, and the proposed method\noutperformed previous approaches in various evaluation metrics, including text\nfluency and attribute consistency.", "published": "2024-02-06 17:18:25", "link": "http://arxiv.org/abs/2402.04160v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What is \"Typological Diversity\" in NLP?", "abstract": "The NLP research community has devoted increased attention to languages\nbeyond English, resulting in considerable improvements for multilingual NLP.\nHowever, these improvements only apply to a small subset of the world's\nlanguages. Aiming to extend this, an increasing number of papers aspires to\nenhance generalizable multilingual performance across languages. To this end,\nlinguistic typology is commonly used to motivate language selection, on the\nbasis that a broad typological sample ought to imply generalization across a\nbroad range of languages. These selections are often described as being\n'typologically diverse'. In this work, we systematically investigate NLP\nresearch that includes claims regarding 'typological diversity'. We find there\nare no set definitions or criteria for such claims. We introduce metrics to\napproximate the diversity of language selection along several axes and find\nthat the results vary considerably across papers. Crucially, we show that\nskewed language selection can lead to overestimated multilingual performance.\nWe recommend future work to include an operationalization of 'typological\ndiversity' that empirically justifies the diversity of language samples.", "published": "2024-02-06 18:29:39", "link": "http://arxiv.org/abs/2402.04222v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Linear-time Minimum Bayes Risk Decoding with Reference Aggregation", "abstract": "Minimum Bayes Risk (MBR) decoding is a text generation technique that has\nbeen shown to improve the quality of machine translations, but is expensive,\neven if a sampling-based approximation is used. Besides requiring a large\nnumber of sampled sequences, it requires the pairwise calculation of a utility\nmetric, which has quadratic complexity. In this paper, we propose to\napproximate pairwise metric scores with scores calculated against aggregated\nreference representations. This changes the complexity of utility estimation\nfrom $O(n^2)$ to $O(n)$, while empirically preserving most of the quality gains\nof MBR decoding. We release our source code at https://github.com/ZurichNLP/mbr", "published": "2024-02-06 18:59:30", "link": "http://arxiv.org/abs/2402.04251v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls", "abstract": "We introduce AnyTool, a large language model agent designed to revolutionize\nthe utilization of a vast array of tools in addressing user queries. We utilize\nover 16,000 APIs from Rapid API, operating under the assumption that a subset\nof these APIs could potentially resolve the queries. AnyTool primarily\nincorporates three elements: an API retriever with a hierarchical structure, a\nsolver aimed at resolving user queries using a selected set of API candidates,\nand a self-reflection mechanism, which re-activates AnyTool if the initial\nsolution proves impracticable. AnyTool is powered by the function calling\nfeature of GPT-4, eliminating the need for training external modules. We also\nrevisit the evaluation protocol introduced by previous works and identify a\nlimitation in this protocol that leads to an artificially high pass rate. By\nrevising the evaluation protocol to better reflect practical application\nscenarios, we introduce an additional benchmark, termed AnyToolBench.\nExperiments across various datasets demonstrate the superiority of our AnyTool\nover strong baselines such as ToolLLM and a GPT-4 variant tailored for tool\nutilization. For instance, AnyTool outperforms ToolLLM by +35.4% in terms of\naverage pass rate on ToolBench. Code will be available at\nhttps://github.com/dyabel/AnyTool.", "published": "2024-02-06 18:59:57", "link": "http://arxiv.org/abs/2402.04253v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Training Language Models to Generate Text with Citations via\n  Fine-grained Rewards", "abstract": "While recent Large Language Models (LLMs) have proven useful in answering\nuser queries, they are prone to hallucination, and their responses often lack\ncredibility due to missing references to reliable sources. An intuitive\nsolution to these issues would be to include in-text citations referring to\nexternal documents as evidence. While previous works have directly prompted\nLLMs to generate in-text citations, their performances are far from\nsatisfactory, especially when it comes to smaller LLMs. In this work, we\npropose an effective training framework using fine-grained rewards to teach\nLLMs to generate highly supportive and relevant citations, while ensuring the\ncorrectness of their responses. We also conduct a systematic analysis of\napplying these fine-grained rewards to common LLM training strategies,\ndemonstrating its advantage over conventional practices. We conduct extensive\nexperiments on Question Answering (QA) datasets taken from the ALCE benchmark\nand validate the model's generalizability using EXPERTQA. On LLaMA-2-7B, the\nincorporation of fine-grained rewards achieves the best performance among the\nbaselines, even surpassing that of GPT-3.5-turbo.", "published": "2024-02-06 19:00:40", "link": "http://arxiv.org/abs/2402.04315v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Democratizing Large Language Models via Personalized Parameter-Efficient\n  Fine-tuning", "abstract": "Personalization in large language models (LLMs) is increasingly important,\naiming to align the LLMs' interactions, content, and recommendations with\nindividual user preferences. Recent advances have highlighted effective prompt\ndesign by enriching user queries with non-parametric knowledge through behavior\nhistory retrieval and textual profiles. However, these methods faced\nlimitations due to a lack of model ownership, resulting in constrained\ncustomization and privacy issues, and often failed to capture complex, dynamic\nuser behavior patterns. To address these shortcomings, we introduce One PEFT\nPer User (OPPU), employing personalized parameter-efficient fine-tuning (PEFT)\nmodules to store user-specific behavior patterns and preferences. By plugging\nin personal PEFT parameters, users can own and use their LLMs individually.\nOPPU integrates parametric user knowledge in the personal PEFT parameters with\nnon-parametric knowledge from retrieval and profiles, adapting LLMs to user\nbehavior shifts. Experimental results demonstrate that OPPU significantly\noutperforms existing prompt-based methods across seven diverse tasks in the\nLaMP benchmark. Further studies reveal OPPU's enhanced capabilities in handling\nuser behavior shifts, modeling users at different activity levels, maintaining\nrobustness across various user history formats, and displaying versatility with\ndifferent PEFT methods.", "published": "2024-02-06 21:03:52", "link": "http://arxiv.org/abs/2402.04401v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Embeddings for One-Shot Classification of Doctor-AI\n  Consultations", "abstract": "Effective communication between healthcare providers and patients is crucial\nto providing high-quality patient care. In this work, we investigate how\nDoctor-written and AI-generated texts in healthcare consultations can be\nclassified using state-of-the-art embeddings and one-shot classification\nsystems. By analyzing embeddings such as bag-of-words, character n-grams,\nWord2Vec, GloVe, fastText, and GPT2 embeddings, we examine how well our\none-shot classification systems capture semantic information within medical\nconsultations. Results show that the embeddings are capable of capturing\nsemantic features from text in a reliable and adaptable manner. Overall,\nWord2Vec, GloVe and Character n-grams embeddings performed well, indicating\ntheir suitability for modeling targeted to this task. GPT2 embedding also shows\nnotable performance, indicating its suitability for models tailored to this\ntask as well. Our machine learning architectures significantly improved the\nquality of health conversations when training data are scarce, improving\ncommunication between patients and healthcare providers.", "published": "2024-02-06 22:24:56", "link": "http://arxiv.org/abs/2402.04442v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LV-Eval: A Balanced Long-Context Benchmark with 5 Length Levels Up to\n  256K", "abstract": "State-of-the-art large language models (LLMs) are now claiming remarkable\nsupported context lengths of 256k or even more. In contrast, the average\ncontext lengths of mainstream benchmarks are insufficient (5k-21k), and they\nsuffer from potential knowledge leakage and inaccurate metrics, resulting in\nbiased evaluation. This paper introduces LV-Eval, a challenging long-context\nbenchmark with five length levels (16k, 32k, 64k, 128k, and 256k) reaching up\nto 256k words. LV-Eval features two main tasks, single-hop QA and multi-hop QA,\ncomprising 11 bilingual datasets. The design of LV-Eval has incorporated three\nkey techniques, namely confusing facts insertion, keyword and phrase\nreplacement, and keyword-recall-based metric design. The advantages of LV-Eval\ninclude controllable evaluation across different context lengths, challenging\ntest instances with confusing facts, mitigated knowledge leakage, and more\nobjective evaluations. We evaluate 15 LLMs on LV-Eval and conduct ablation\nstudies on the benchmarking techniques. The results reveal that: (i)\nMoonshot-v1 and recent large-scale open-source models, such as Qwen-2.5-72B and\nLlama-3.1-70B, achieve the highest performance on LV-Eval, particularly at\nlengths below 64k. (ii) Models exhibit distinct score trends. For example,\nGLM-4-9B-128k, Yi-6B-200k, and Llama3-8B-1M exhibit a relatively gentle\ndegradation of performance, but their absolute performances may not necessarily\nbe higher than those of LLMs with shorter context lengths. (iii) LLMs'\nperformances can significantly degrade in the presence of confusing\ninformation, especially in the pressure test of \"needle in a haystack\". (iv)\nIssues related to knowledge leakage and inaccurate metrics introduce bias in\nevaluation, and these concerns are alleviated in LV-Eval. All datasets and\nevaluation codes are released at: https://github.com/infinigence/LVEval.", "published": "2024-02-06 13:11:19", "link": "http://arxiv.org/abs/2402.05136v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Discover: Large Language Models Self-Compose Reasoning Structures", "abstract": "We introduce SELF-DISCOVER, a general framework for LLMs to self-discover the\ntask-intrinsic reasoning structures to tackle complex reasoning problems that\nare challenging for typical prompting methods. Core to the framework is a\nself-discovery process where LLMs select multiple atomic reasoning modules such\nas critical thinking and step-by-step thinking, and compose them into an\nexplicit reasoning structure for LLMs to follow during decoding. SELF-DISCOVER\nsubstantially improves GPT-4 and PaLM 2's performance on challenging reasoning\nbenchmarks such as BigBench-Hard, grounded agent reasoning, and MATH, by as\nmuch as 32% compared to Chain of Thought (CoT). Furthermore, SELF-DISCOVER\noutperforms inference-intensive methods such as CoT-Self-Consistency by more\nthan 20%, while requiring 10-40x fewer inference compute. Finally, we show that\nthe self-discovered reasoning structures are universally applicable across\nmodel families: from PaLM 2-L to GPT-4, and from GPT-4 to Llama2, and share\ncommonalities with human reasoning patterns.", "published": "2024-02-06 01:13:53", "link": "http://arxiv.org/abs/2402.03620v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Partially Recentralization Softmax Loss for Vision-Language Models\n  Robustness", "abstract": "As Large Language Models make a breakthrough in natural language processing\ntasks (NLP), multimodal technique becomes extremely popular. However, it has\nbeen shown that multimodal NLP are vulnerable to adversarial attacks, where the\noutputs of a model can be dramatically changed by a perturbation to the input.\nWhile several defense techniques have been proposed both in computer vision and\nNLP models, the multimodal robustness of models have not been fully explored.\nIn this paper, we study the adversarial robustness provided by modifying loss\nfunction of pre-trained multimodal models, by restricting top K softmax\noutputs. Based on the evaluation and scoring, our experiments show that after a\nfine-tuning, adversarial robustness of pre-trained models can be significantly\nimproved, against popular attacks. Further research should be studying, such as\noutput diversity, generalization and the robustness-performance trade-off of\nthis kind of loss functions. Our code will be available after this paper is\naccepted", "published": "2024-02-06 01:44:38", "link": "http://arxiv.org/abs/2402.03627v2", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Sentiment-enhanced Graph-based Sarcasm Explanation in Dialogue", "abstract": "Sarcasm Explanation in Dialogue (SED) is a new yet challenging task, which\naims to generate a natural language explanation for the given sarcastic\ndialogue that involves multiple modalities (\\ie utterance, video, and audio).\nAlthough existing studies have achieved great success based on the generative\npretrained language model BART, they overlook exploiting the sentiments\nresiding in the utterance, video and audio, which play important roles in\nreflecting sarcasm that essentially involves subtle sentiment contrasts.\nNevertheless, it is non-trivial to incorporate sentiments for boosting SED\nperformance, due to three main challenges: 1) diverse effects of utterance\ntokens on sentiments; 2) gap between video-audio sentiment signals and the\nembedding space of BART; and 3) various relations among utterances, utterance\nsentiments, and video-audio sentiments. To tackle these challenges, we propose\na novel sEntiment-enhanceD Graph-based multimodal sarcasm Explanation\nframework, named EDGE. In particular, we first propose a lexicon-guided\nutterance sentiment inference module, where a heuristic utterance sentiment\nrefinement strategy is devised. We then develop a module named Joint Cross\nAttention-based Sentiment Inference (JCA-SI) by extending the multimodal\nsentiment analysis model JCA to derive the joint sentiment label for each\nvideo-audio clip. Thereafter, we devise a context-sentiment graph to\ncomprehensively model the semantic relations among the utterances, utterance\nsentiments, and video-audio sentiments, to facilitate sarcasm explanation\ngeneration. Extensive experiments on the publicly released dataset WITS verify\nthe superiority of our model over cutting-edge methods.", "published": "2024-02-06 03:14:46", "link": "http://arxiv.org/abs/2402.03658v2", "categories": ["cs.CL", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Large Language Models as an Indirect Reasoner: Contrapositive and\n  Contradiction for Automated Reasoning", "abstract": "Recently, increasing attention has been focused on improving the ability of\nLarge Language Models (LLMs) to perform complex reasoning. Advanced methods,\nsuch as Chain-of-Thought (CoT) and its variants, are found to enhance their\nreasoning skills by designing suitable prompts or breaking down complex\nproblems into more manageable sub-problems. However, little concentration has\nbeen put on exploring the reasoning process, \\textit{i.e.}, we discovered that\nmost methods resort to Direct Reasoning (DR) and disregard Indirect Reasoning\n(IR). This can make LLMs difficult to solve IR tasks, which are often\nencountered in the real world. To address this issue, we propose a\nDirect-Indirect Reasoning (DIR) method, which considers DR and IR as multiple\nparallel reasoning paths that are merged to derive the final answer. We\nstimulate LLMs to implement IR by crafting prompt templates incorporating the\nprinciples of contrapositive and contradiction. These templates trigger LLMs to\nassume the negation of the conclusion as true, combine it with the premises to\ndeduce a conclusion, and utilize the logical equivalence of the contrapositive\nto enhance their comprehension of the rules used in the reasoning process. Our\nDIR method is simple yet effective and can be straightforwardly integrated with\nexisting variants of CoT methods. Experimental results on four datasets related\nto logical reasoning and mathematic proof demonstrate that our DIR method, when\ncombined with various baseline methods, significantly outperforms all the\noriginal methods.", "published": "2024-02-06 03:41:12", "link": "http://arxiv.org/abs/2402.03667v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Are Machines Better at Complex Reasoning? Unveiling Human-Machine\n  Inference Gaps in Entailment Verification", "abstract": "Making inferences in text comprehension to understand the meaning is\nessential in language processing. This work studies the entailment verification\n(EV) problem of multi-sentence premises that requires a system to make multiple\ninferences implicitly. Studying EV for such complex premises is important\nbecause modern NLP problems, such as detecting inconsistent model-generated\nrationales, require complex multi-hop reasoning. However, current textual\ninference datasets mostly contain short premises that only partially focus on\nthese challenges. To address this, we compile an EV benchmark that includes\ndatasets from three NLP domains (NLI, contextual QA, and rationales) containing\nmulti-sentence premises. On benchmarking humans and LLMs, we find that LLMs are\nbetter than humans in multi-hop reasoning across extended contexts, while\nhumans perform better in simple deductive reasoning tasks. We also finetune a\nFlan-T5 model for EV using two training objectives to obtain a strong\nopen-source model that outperforms GPT-3.5 and rivals GPT-4. Finally, we use\nthis model to filter out inconsistent model-generated rationales in\nself-consistency decoding, resulting in a 6% accuracy improvement on average\nacross three MCQ datasets.", "published": "2024-02-06 04:14:09", "link": "http://arxiv.org/abs/2402.03686v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Empowering Language Models with Active Inquiry for Deeper Understanding", "abstract": "The rise of large language models (LLMs) has revolutionized the way that we\ninteract with artificial intelligence systems through natural language.\nHowever, LLMs often misinterpret user queries because of their uncertain\nintention, leading to less helpful responses. In natural human interactions,\nclarification is sought through targeted questioning to uncover obscure\ninformation. Thus, in this paper, we introduce LaMAI (Language Model with\nActive Inquiry), designed to endow LLMs with this same level of interactive\nengagement. LaMAI leverages active learning techniques to raise the most\ninformative questions, fostering a dynamic bidirectional dialogue. This\napproach not only narrows the contextual gap but also refines the output of the\nLLMs, aligning it more closely with user expectations. Our empirical studies,\nacross a variety of complex datasets where LLMs have limited conversational\ncontext, demonstrate the effectiveness of LaMAI. The method improves answer\naccuracy from 31.9% to 50.9%, outperforming other leading question-answering\nframeworks. Moreover, in scenarios involving human participants, LaMAI\nconsistently generates responses that are superior or comparable to baseline\nmethods in more than 82% of the cases. The applicability of LaMAI is further\nevidenced by its successful integration with various LLMs, highlighting its\npotential for the future of interactive language models.", "published": "2024-02-06 05:24:16", "link": "http://arxiv.org/abs/2402.03719v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Language Models As MOOCs Graders", "abstract": "Massive open online courses (MOOCs) unlock the doors to free education for\nanyone around the globe with access to a computer and the internet. Despite\nthis democratization of learning, the massive enrollment in these courses means\nit is almost impossible for one instructor to assess every student's writing\nassignment. As a result, peer grading, often guided by a straightforward\nrubric, is the method of choice. While convenient, peer grading often falls\nshort in terms of reliability and validity. In this study, using 18 distinct\nsettings, we explore the feasibility of leveraging large language models (LLMs)\nto replace peer grading in MOOCs. Specifically, we focus on two\nstate-of-the-art LLMs: GPT-4 and GPT-3.5, across three distinct courses:\nIntroductory Astronomy, Astrobiology, and the History and Philosophy of\nAstronomy. To instruct LLMs, we use three different prompts based on a variant\nof the zero-shot chain-of-thought (Zero-shot-CoT) prompting technique:\nZero-shot-CoT combined with instructor-provided correct answers; Zero-shot-CoT\nin conjunction with both instructor-formulated answers and rubrics; and\nZero-shot-CoT with instructor-offered correct answers and LLM-generated\nrubrics. Our results show that Zero-shot-CoT, when integrated with\ninstructor-provided answers and rubrics, produces grades that are more aligned\nwith those assigned by instructors compared to peer grading. However, the\nHistory and Philosophy of Astronomy course proves to be more challenging in\nterms of grading as opposed to other courses. Finally, our study reveals a\npromising direction for automating grading systems for MOOCs, especially in\nsubjects with well-defined rubrics.", "published": "2024-02-06 07:43:07", "link": "http://arxiv.org/abs/2402.03776v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Soft Prompt Tuning for Cross-Lingual Transfer: When Less is More", "abstract": "Soft Prompt Tuning (SPT) is a parameter-efficient method for adapting\npre-trained language models (PLMs) to specific tasks by inserting learnable\nembeddings, or soft prompts, at the input layer of the PLM, without modifying\nits parameters. This paper investigates the potential of SPT for cross-lingual\ntransfer. Unlike previous studies on SPT for cross-lingual transfer that often\nfine-tune both the soft prompt and the model parameters, we adhere to the\noriginal intent of SPT by keeping the model parameters frozen and only training\nthe soft prompt. This does not only reduce the computational cost and storage\noverhead of full-model fine-tuning, but we also demonstrate that this very\nparameter efficiency intrinsic to SPT can enhance cross-lingual transfer\nperformance to linguistically distant languages. Moreover, we explore how\ndifferent factors related to the prompt, such as the length or its\nreparameterization, affect cross-lingual transfer performance.", "published": "2024-02-06 07:52:30", "link": "http://arxiv.org/abs/2402.03782v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ANLS* -- A Universal Document Processing Metric for Generative Large\n  Language Models", "abstract": "Traditionally, discriminative models have been the predominant choice for\ntasks like document classification and information extraction. These models\nmake predictions that fall into a limited number of predefined classes,\nfacilitating a binary true or false evaluation and enabling the direct\ncalculation of metrics such as the F1 score. However, recent advancements in\ngenerative large language models (GLLMs) have prompted a shift in the field due\nto their enhanced zero-shot capabilities, which eliminate the need for a\ndownstream dataset and computationally expensive fine-tuning. However,\nevaluating GLLMs presents a challenge as the binary true or false evaluation\nused for discriminative models is not applicable to the predictions made by\nGLLMs.\n  This paper introduces a new metric for generative models called ANLS* for\nevaluating a wide variety of tasks, including information extraction and\nclassification tasks. The ANLS* metric extends existing ANLS metrics as a\ndrop-in-replacement and is still compatible with previously reported ANLS\nscores. An evaluation of 7 different datasets, and more than 20 different GLLMs\ntogether with 3 different prompting methods using the ANLS* metric is also\nprovided, demonstrating the importance of the proposed metric.\n  We also benchmark a novel approach to generate prompts for documents, called\nSFT, against other prompting techniques such as LATIN. In almost all cases, SFT\noutperforms other techniques and improves the state-of-the-art, sometimes by as\nmuch as $10$ percentage points.\n  Sources are available at https://github.com/deepopinion/anls_star_metric", "published": "2024-02-06 09:50:08", "link": "http://arxiv.org/abs/2402.03848v9", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Less than one percent of words would be affected by gender-inclusive\n  language in German press texts", "abstract": "Research on gender and language is tightly knitted to social debates on\ngender equality and non-discriminatory language use. Psycholinguistic scholars\nhave made significant contributions in this field. However, corpus-based\nstudies that investigate these matters within the context of language use are\nstill rare. In our study, we address the question of how much textual material\nwould actually have to be changed if non-gender-inclusive texts were rewritten\nto be gender-inclusive. This quantitative measure is an important empirical\ninsight, as a recurring argument against the use of gender-inclusive German is\nthat it supposedly makes written texts too long and complicated. It is also\nargued that gender-inclusive language has negative effects on language\nlearners. However, such effects are only likely if gender-inclusive texts are\nvery different from those that are not gender-inclusive. In our\ncorpus-linguistic study, we manually annotated German press texts to identify\nthe parts that would have to be changed. Our results show that, on average,\nless than 1% of all tokens would be affected by gender-inclusive language. This\nsmall proportion calls into question whether gender-inclusive German presents a\nsubstantial barrier to understanding and learning the language, particularly\nwhen we take into account the potential complexities of interpreting masculine\ngenerics.", "published": "2024-02-06 10:32:34", "link": "http://arxiv.org/abs/2402.03870v1", "categories": ["cs.CL", "stat.AP"], "primary_category": "cs.CL"}
{"title": "Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large\n  Language Models", "abstract": "Large Language Models (LLMs) demonstrate ever-increasing abilities in\nmathematical and algorithmic tasks, yet their geometric reasoning skills are\nunderexplored. We investigate LLMs' abilities in constructive geometric\nproblem-solving one of the most fundamental steps in the development of human\nmathematical reasoning. Our work reveals notable challenges that the\nstate-of-the-art LLMs face in this domain despite many successes in similar\nareas. LLMs exhibit biases in target variable selection and struggle with 2D\nspatial relationships, often misrepresenting and hallucinating objects and\ntheir placements. To this end, we introduce a framework that formulates an\nLLMs-based multi-agents system that enhances their existing reasoning potential\nby conducting an internal dialogue. This work underscores LLMs' current\nlimitations in geometric reasoning and improves geometric reasoning\ncapabilities through self-correction, collaboration, and diverse role\nspecializations.", "published": "2024-02-06 10:37:21", "link": "http://arxiv.org/abs/2402.03877v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Can Large Language Models Detect Rumors on Social Media?", "abstract": "In this work, we investigate to use Large Language Models (LLMs) for rumor\ndetection on social media. However, it is challenging for LLMs to reason over\nthe entire propagation information on social media, which contains news\ncontents and numerous comments, due to LLMs may not concentrate on key clues in\nthe complex propagation information, and have trouble in reasoning when facing\nmassive and redundant information. Accordingly, we propose an LLM-empowered\nRumor Detection (LeRuD) approach, in which we design prompts to teach LLMs to\nreason over important clues in news and comments, and divide the entire\npropagation information into a Chain-of-Propagation for reducing LLMs' burden.\nWe conduct extensive experiments on the Twitter and Weibo datasets, and LeRuD\noutperforms several state-of-the-art rumor detection models by 3.2% to 7.7%.\nMeanwhile, by applying LLMs, LeRuD requires no data for training, and thus\nshows more promising rumor detection ability in few-shot or zero-shot\nscenarios.", "published": "2024-02-06 11:33:57", "link": "http://arxiv.org/abs/2402.03916v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in\n  Closed-Source LLMs", "abstract": "Natural Language Processing (NLP) research is increasingly focusing on the\nuse of Large Language Models (LLMs), with some of the most popular ones being\neither fully or partially closed-source. The lack of access to model details,\nespecially regarding training data, has repeatedly raised concerns about data\ncontamination among researchers. Several attempts have been made to address\nthis issue, but they are limited to anecdotal evidence and trial and error.\nAdditionally, they overlook the problem of \\emph{indirect} data leaking, where\nmodels are iteratively improved by using data coming from users. In this work,\nwe conduct the first systematic analysis of work using OpenAI's GPT-3.5 and\nGPT-4, the most prominently used LLMs today, in the context of data\ncontamination. By analysing 255 papers and considering OpenAI's data usage\npolicy, we extensively document the amount of data leaked to these models\nduring the first year after the model's release. We report that these models\nhave been globally exposed to $\\sim$4.7M samples from 263 benchmarks. At the\nsame time, we document a number of evaluation malpractices emerging in the\nreviewed papers, such as unfair or missing baseline comparisons and\nreproducibility issues. We release our results as a collaborative project on\nhttps://leak-llm.github.io/, where other researchers can contribute to our\nefforts.", "published": "2024-02-06 11:54:23", "link": "http://arxiv.org/abs/2402.03927v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Position: Stop Making Unscientific AGI Performance Claims", "abstract": "Developments in the field of Artificial Intelligence (AI), and particularly\nlarge language models (LLMs), have created a 'perfect storm' for observing\n'sparks' of Artificial General Intelligence (AGI) that are spurious. Like\nsimpler models, LLMs distill meaningful representations in their latent\nembeddings that have been shown to correlate with external variables.\nNonetheless, the correlation of such representations has often been linked to\nhuman-like intelligence in the latter but not the former. We probe models of\nvarying complexity including random projections, matrix decompositions, deep\nautoencoders and transformers: all of them successfully distill information\nthat can be used to predict latent or external variables and yet none of them\nhave previously been linked to AGI. We argue and empirically demonstrate that\nthe finding of meaningful patterns in latent spaces of models cannot be seen as\nevidence in favor of AGI. Additionally, we review literature from the social\nsciences that shows that humans are prone to seek such patterns and\nanthropomorphize. We conclude that both the methodological setup and common\npublic image of AI are ideal for the misinterpretation that correlations\nbetween model representations and some variables of interest are 'caused' by\nthe model's understanding of underlying 'ground truth' relationships. We,\ntherefore, call for the academic community to exercise extra caution, and to be\nkeenly aware of principles of academic integrity, in interpreting and\ncommunicating about AI research outcomes.", "published": "2024-02-06 12:42:21", "link": "http://arxiv.org/abs/2402.03962v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Systematic Biases in LLM Simulations of Debates", "abstract": "The emergence of Large Language Models (LLMs), has opened exciting\npossibilities for constructing computational simulations designed to replicate\nhuman behavior accurately. Current research suggests that LLM-based agents\nbecome increasingly human-like in their performance, sparking interest in using\nthese AI agents as substitutes for human participants in behavioral studies.\nHowever, LLMs are complex statistical learners without straightforward\ndeductive rules, making them prone to unexpected behaviors. Hence, it is\ncrucial to study and pinpoint the key behavioral distinctions between humans\nand LLM-based agents. In this study, we highlight the limitations of LLMs in\nsimulating human interactions, particularly focusing on LLMs' ability to\nsimulate political debates on topics that are important aspects of people's\nday-to-day lives and decision-making processes. Our findings indicate a\ntendency for LLM agents to conform to the model's inherent social biases\ndespite being directed to debate from certain political perspectives. This\ntendency results in behavioral patterns that seem to deviate from\nwell-established social dynamics among humans. We reinforce these observations\nusing an automatic self-fine-tuning method, which enables us to manipulate the\nbiases within the LLM and demonstrate that agents subsequently align with the\naltered biases. These results underscore the need for further research to\ndevelop methods that help agents overcome these biases, a critical step toward\ncreating more realistic simulations.", "published": "2024-02-06 14:51:55", "link": "http://arxiv.org/abs/2402.04049v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Retrieve to Explain: Evidence-driven Predictions with Language Models", "abstract": "Language models hold incredible promise for enabling scientific discovery by\nsynthesizing massive research corpora. Many complex scientific research\nquestions have multiple plausible answers, each supported by evidence of\nvarying strength. However, existing language models lack the capability to\nquantitatively and faithfully compare answer plausibility in terms of\nsupporting evidence. To address this issue, we introduce Retrieve to Explain\n(R2E), a retrieval-based language model. R2E scores and ranks all possible\nanswers to a research question based on evidence retrieved from a document\ncorpus. The architecture represents each answer only in terms of its supporting\nevidence, with the answer itself masked. This allows us to extend feature\nattribution methods, such as Shapley values, to transparently attribute each\nanswer's score back to its supporting evidence at inference time. The\narchitecture also allows R2E to incorporate new evidence without retraining,\nincluding non-textual data modalities templated into natural language. We\nassess on the challenging task of drug target identification from scientific\nliterature, a human-in-the-loop process where failures are extremely costly and\nexplainability is paramount. When predicting whether drug targets will\nsubsequently be confirmed as efficacious in clinical trials, R2E not only\nmatches non-explainable literature-based models but also surpasses a\ngenetics-based target identification approach used throughout the\npharmaceutical industry.", "published": "2024-02-06 15:13:17", "link": "http://arxiv.org/abs/2402.04068v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Measuring Implicit Bias in Explicitly Unbiased Large Language Models", "abstract": "Large language models (LLMs) can pass explicit social bias tests but still\nharbor implicit biases, similar to humans who endorse egalitarian beliefs yet\nexhibit subtle biases. Measuring such implicit biases can be a challenge: as\nLLMs become increasingly proprietary, it may not be possible to access their\nembeddings and apply existing bias measures; furthermore, implicit biases are\nprimarily a concern if they affect the actual decisions that these systems\nmake. We address both challenges by introducing two new measures of bias: LLM\nImplicit Bias, a prompt-based method for revealing implicit bias; and LLM\nDecision Bias, a strategy to detect subtle discrimination in decision-making\ntasks. Both measures are based on psychological research: LLM Implicit Bias\nadapts the Implicit Association Test, widely used to study the automatic\nassociations between concepts held in human minds; and LLM Decision Bias\noperationalizes psychological results indicating that relative evaluations\nbetween two candidates, not absolute evaluations assessing each independently,\nare more diagnostic of implicit biases. Using these measures, we found\npervasive stereotype biases mirroring those in society in 8 value-aligned\nmodels across 4 social categories (race, gender, religion, health) in 21\nstereotypes (such as race and criminality, race and weapons, gender and\nscience, age and negativity). Our prompt-based LLM Implicit Bias measure\ncorrelates with existing language model embedding-based bias methods, but\nbetter predicts downstream behaviors measured by LLM Decision Bias. These new\nprompt-based measures draw from psychology's long history of research into\nmeasuring stereotype biases based on purely observable behavior; they expose\nnuanced biases in proprietary value-aligned LLMs that appear unbiased according\nto standard benchmarks.", "published": "2024-02-06 15:59:23", "link": "http://arxiv.org/abs/2402.04105v2", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Can Generative Agents Predict Emotion?", "abstract": "Large Language Models (LLMs) have demonstrated a number of human-like\nabilities, however the empathic understanding and emotional state of LLMs is\nyet to be aligned to that of humans. In this work, we investigate how the\nemotional state of generative LLM agents evolves as they perceive new events,\nintroducing a novel architecture in which new experiences are compared to past\nmemories. Through this comparison, the agent gains the ability to understand\nnew experiences in context, which according to the appraisal theory of emotion\nis vital in emotion creation. First, the agent perceives new experiences as\ntime series text data. After perceiving each new input, the agent generates a\nsummary of past relevant memories, referred to as the norm, and compares the\nnew experience to this norm. Through this comparison we can analyse how the\nagent reacts to the new experience in context. The PANAS, a test of affect, is\nadministered to the agent, capturing the emotional state of the agent after the\nperception of the new event. Finally, the new experience is then added to the\nagents memory to be used in the creation of future norms. By creating multiple\nexperiences in natural language from emotionally charged situations, we test\nthe proposed architecture on a wide range of scenarios. The mixed results\nsuggests that introducing context can occasionally improve the emotional\nalignment of the agent, but further study and comparison with human evaluators\nis necessary. We hope that this paper is another step towards the alignment of\ngenerative agents.", "published": "2024-02-06 18:39:43", "link": "http://arxiv.org/abs/2402.04232v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "CogCoM: A Visual Language Model with Chain-of-Manipulations Reasoning", "abstract": "Vision-Language Models (VLMs) have demonstrated their broad effectiveness\nthanks to extensive training in aligning visual instructions to responses.\nHowever, such training of conclusive alignment leads models to ignore essential\nvisual reasoning, further resulting in failures in meticulous visual problems\nand unfaithful responses. Drawing inspiration from human cognition in solving\nvisual problems (e.g., marking, zoom in), this paper introduces Chain of\nManipulations, a mechanism that enables VLMs to solve problems step-by-step\nwith evidence. After training, models can solve various visual problems by\neliciting intrinsic manipulations (e.g., grounding, zoom in) with results\n(e.g., boxes, image) actively without involving external tools, while also\nallowing users to trace error causes. We study the roadmap to implement this\nmechanism, including (1) a flexible design of manipulations upon extensive\nanalysis, (2) an efficient automated data generation pipeline, (3) a compatible\nVLM architecture capable of multi-turn multi-image, and (4) a model training\nprocess for versatile capabilities. With the design, we also manually annotate\n6K high-quality samples for the challenging graphical mathematical problems.\nOur trained model, \\textbf{CogCoM}, equipped with this mechanism with 17B\nparameters achieves state-of-the-art performance across 9 benchmarks from 4\ncategories, demonstrating the effectiveness while preserving the\ninterpretability. Our code, model weights, and collected data are publicly\navailable at https://github.com/THUDM/CogCoM.", "published": "2024-02-06 18:43:48", "link": "http://arxiv.org/abs/2402.04236v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "The Hedgehog & the Porcupine: Expressive Linear Attentions with Softmax\n  Mimicry", "abstract": "Linear attentions have shown potential for improving Transformer efficiency,\nreducing attention's quadratic complexity to linear in sequence length. This\nholds exciting promise for (1) training linear Transformers from scratch, (2)\n\"finetuned-conversion\" of task-specific Transformers into linear versions that\nrecover task performance, and (3) \"pretrained-conversion\" of Transformers such\nas large language models into linear versions finetunable on downstream tasks.\nHowever, linear attentions often underperform standard softmax attention in\nquality. To close this performance gap, we find prior linear attentions lack\nkey properties of softmax attention tied to good performance: low-entropy (or\n\"spiky\") weights and dot-product monotonicity. We further observe surprisingly\nsimple feature maps that retain these properties and match softmax performance,\nbut are inefficient to compute in linear attention. We thus propose Hedgehog, a\nlearnable linear attention that retains the spiky and monotonic properties of\nsoftmax attention while maintaining linear complexity. Hedgehog uses simple\ntrainable MLPs to produce attention weights mimicking softmax attention.\nExperiments show Hedgehog recovers over 99% of standard Transformer quality in\ntrain-from-scratch and finetuned-conversion settings, outperforming prior\nlinear attentions up to 6 perplexity points on WikiText-103 with causal GPTs,\nand up to 8.7 GLUE score points on finetuned bidirectional BERTs. Hedgehog also\nenables pretrained-conversion. Converting a pretrained GPT-2 into a linear\nattention variant achieves state-of-the-art 16.7 perplexity on WikiText-103 for\n125M subquadratic decoder models. We finally turn a pretrained Llama-2 7B into\na viable linear attention Llama. With low-rank adaptation, Hedgehog-Llama2 7B\nachieves 28.1 higher ROUGE-1 points over the base standard attention model,\nwhere prior linear attentions lead to 16.5 point drops.", "published": "2024-02-06 19:31:26", "link": "http://arxiv.org/abs/2402.04347v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "The World of Generative AI: Deepfakes and Large Language Models", "abstract": "We live in the era of Generative Artificial Intelligence (GenAI). Deepfakes\nand Large Language Models (LLMs) are two examples of GenAI. Deepfakes, in\nparticular, pose an alarming threat to society as they are capable of spreading\nmisinformation and changing the truth. LLMs are powerful language models that\ngenerate general-purpose language. However due to its generative aspect, it can\nalso be a risk for people if used with ill intentions. The ethical use of these\ntechnologies is a big concern. This short article tries to find out the\ninterrelationship between them.", "published": "2024-02-06 20:18:32", "link": "http://arxiv.org/abs/2402.04373v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "DFA-RAG: Conversational Semantic Router for Large Language Model with\n  Definite Finite Automaton", "abstract": "This paper introduces the retrieval-augmented large language model with\nDefinite Finite Automaton (DFA-RAG), a novel framework designed to enhance the\ncapabilities of conversational agents using large language models (LLMs).\nTraditional LLMs face challenges in generating regulated and compliant\nresponses in special scenarios with predetermined response guidelines, like\nemotional support and customer service. Our framework addresses these\nchallenges by embedding a Definite Finite Automaton (DFA), learned from\ntraining dialogues, within the LLM. This structured approach acts as a semantic\nrouter which enables the LLM to adhere to a deterministic response pathway. The\nrouting is achieved by the retrieval-augmentation generation (RAG) strategy,\nwhich carefully selects dialogue examples aligned with the current\nconversational context. The advantages of DFA-RAG include an interpretable\nstructure through human-readable DFA, context-aware retrieval for responses in\nconversations, and plug-and-play compatibility with existing LLMs. Extensive\nbenchmarks validate DFA-RAG's effectiveness, indicating its potential as a\nvaluable contribution to the conversational agent.", "published": "2024-02-06 21:14:45", "link": "http://arxiv.org/abs/2402.04411v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning to Extract Structured Entities Using Language Models", "abstract": "Recent advances in machine learning have significantly impacted the field of\ninformation extraction, with Language Models (LMs) playing a pivotal role in\nextracting structured information from unstructured text. Prior works typically\nrepresent information extraction as triplet-centric and use classical metrics\nsuch as precision and recall for evaluation. We reformulate the task to be\nentity-centric, enabling the use of diverse metrics that can provide more\ninsights from various perspectives. We contribute to the field by introducing\nStructured Entity Extraction and proposing the Approximate Entity Set OverlaP\n(AESOP) metric, designed to appropriately assess model performance. Later, we\nintroduce a new Multistage Structured Entity Extraction (MuSEE) model that\nharnesses the power of LMs for enhanced effectiveness and efficiency by\ndecomposing the extraction task into multiple stages. Quantitative and human\nside-by-side evaluations confirm that our model outperforms baselines, offering\npromising directions for future advancements in structured entity extraction.\nOur source code and datasets are available at\nhttps://github.com/microsoft/Structured-Entity-Extraction.", "published": "2024-02-06 22:15:09", "link": "http://arxiv.org/abs/2402.04437v5", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Detecting Mode Collapse in Language Models via Narration", "abstract": "No two authors write alike. Personal flourishes invoked in written\nnarratives, from lexicon to rhetorical devices, imply a particular author--what\nliterary theorists label the implied or virtual author; distinct from the real\nauthor or narrator of a text. Early large language models trained on unfiltered\ntraining sets drawn from a variety of discordant sources yielded incoherent\npersonalities, problematic for conversational tasks but proving useful for\nsampling literature from multiple perspectives. Successes in alignment research\nin recent years have allowed researchers to impose subjectively consistent\npersonae on language models via instruction tuning and reinforcement learning\nfrom human feedback (RLHF), but whether aligned models retain the ability to\nmodel an arbitrary virtual author has received little scrutiny. By studying\n4,374 stories sampled from three OpenAI language models, we show successive\nversions of GPT-3 suffer from increasing degrees of \"mode collapse\" whereby\noverfitting the model during alignment constrains it from generalizing over\nauthorship: models suffering from mode collapse become unable to assume a\nmultiplicity of perspectives. Our method and results are significant for\nresearchers seeking to employ language models in sociological simulations.", "published": "2024-02-06 23:52:58", "link": "http://arxiv.org/abs/2402.04477v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SceMQA: A Scientific College Entrance Level Multimodal Question\n  Answering Benchmark", "abstract": "The paper introduces SceMQA, a novel benchmark for scientific multimodal\nquestion answering at the college entrance level. It addresses a critical\neducational phase often overlooked in existing benchmarks, spanning high school\nto pre-college levels. SceMQA focuses on core science subjects including\nMathematics, Physics, Chemistry, and Biology. It features a blend of\nmultiple-choice and free-response formats, ensuring a comprehensive evaluation\nof AI models' abilities. Additionally, our benchmark provides specific\nknowledge points for each problem and detailed explanations for each answer.\nSceMQA also uniquely presents problems with identical contexts but varied\nquestions to facilitate a more thorough and accurate assessment of reasoning\ncapabilities. In the experiment, we evaluate both open-source and close-source\nstate-of-the-art Multimodal Large Language Models (MLLMs), across various\nexperimental settings. The results show that further research and development\nare needed in developing more capable MLLM, as highlighted by only 50% to 60%\naccuracy achieved by the strongest models. Our benchmark and analysis will be\navailable at https://scemqa.github.io/", "published": "2024-02-06 19:16:55", "link": "http://arxiv.org/abs/2402.05138v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Breaking Symmetry When Training Transformers", "abstract": "As we show in this paper, the prediction for output token $n+1$ of\nTransformer architectures without one of the mechanisms of positional encodings\nand causal attention is invariant to permutations of input tokens $1, 2, ...,\nn-1$. Usually, both mechanisms are employed and the symmetry with respect to\nthe input tokens is broken. Recently, it has been shown that one can train\nTransformers without positional encodings. This must be enabled by the causal\nattention mechanism. In this paper, we elaborate on the argument that the\ncausal connection mechanism must be responsible for the fact that Transformers\nare able to model input sequences where the order is important. Vertical\n\"slices\" of Transformers are all encouraged to represent the same location $k$\nin the input sequence. We hypothesize that residual connections contribute to\nthis phenomenon, and demonstrate evidence for this.", "published": "2024-02-06 00:32:28", "link": "http://arxiv.org/abs/2402.05969v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Identifying Reasons for Contraceptive Switching from Real-World Data\n  Using Large Language Models", "abstract": "Prescription contraceptives play a critical role in supporting women's\nreproductive health. With nearly 50 million women in the United States using\ncontraceptives, understanding the factors that drive contraceptives selection\nand switching is of significant interest. However, many factors related to\nmedication switching are often only captured in unstructured clinical notes and\ncan be difficult to extract. Here, we evaluate the zero-shot abilities of a\nrecently developed large language model, GPT-4 (via HIPAA-compliant Microsoft\nAzure API), to identify reasons for switching between classes of contraceptives\nfrom the UCSF Information Commons clinical notes dataset. We demonstrate that\nGPT-4 can accurately extract reasons for contraceptive switching, outperforming\nbaseline BERT-based models with microF1 scores of 0.849 and 0.881 for\ncontraceptive start and stop extraction, respectively. Human evaluation of\nGPT-4-extracted reasons for switching showed 91.4% accuracy, with minimal\nhallucinations. Using extracted reasons, we identified patient preference,\nadverse events, and insurance as key reasons for switching using unsupervised\ntopic modeling approaches. Notably, we also showed using our approach that\n\"weight gain/mood change\" and \"insurance coverage\" are disproportionately found\nas reasons for contraceptive switching in specific demographic populations. Our\ncode and supplemental data are available at\nhttps://github.com/BMiao10/contraceptive-switching.", "published": "2024-02-06 00:14:53", "link": "http://arxiv.org/abs/2402.03597v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal\n  LLM Agents", "abstract": "Owing to recent advancements, Large Language Models (LLMs) can now be\ndeployed as agents for increasingly complex decision-making applications in\nareas including robotics, gaming, and API integration. However, reflecting past\nexperiences in current decision-making processes, an innate human behavior,\ncontinues to pose significant challenges. Addressing this, we propose\nRetrieval-Augmented Planning (RAP) framework, designed to dynamically leverage\npast experiences corresponding to the current situation and context, thereby\nenhancing agents' planning capabilities. RAP distinguishes itself by being\nversatile: it excels in both text-only and multimodal environments, making it\nsuitable for a wide range of tasks. Empirical evaluations demonstrate RAP's\neffectiveness, where it achieves SOTA performance in textual scenarios and\nnotably enhances multimodal LLM agents' performance for embodied tasks. These\nresults highlight RAP's potential in advancing the functionality and\napplicability of LLM agents in complex, real-world applications.", "published": "2024-02-06 00:53:27", "link": "http://arxiv.org/abs/2402.03610v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Leveraging Large Language Models for Hybrid Workplace Decision Support", "abstract": "Large Language Models (LLMs) hold the potential to perform a variety of text\nprocessing tasks and provide textual explanations for proposed actions or\ndecisions. In the era of hybrid work, LLMs can provide intelligent decision\nsupport for workers who are designing their hybrid work plans. In particular,\nthey can offer suggestions and explanations to workers balancing numerous\ndecision factors, thereby enhancing their work experience. In this paper, we\npresent a decision support model for workspaces in hybrid work environments,\nleveraging the reasoning skill of LLMs. We first examine LLM's capability of\nmaking suitable workspace suggestions. We find that its reasoning extends\nbeyond the guidelines in the prompt and the LLM can manage the trade-off among\nthe available resources in the workspaces. We conduct an extensive user study\nto understand workers' decision process for workspace choices and evaluate the\neffectiveness of the system. We observe that a worker's decision could be\ninfluenced by the LLM's suggestions and explanations. The participants in our\nstudy find the system to be convenient, regardless of whether reasons are\nprovided or not. Our results show that employees can benefit from the\nLLM-empowered system for their workspace selection in hybrid workplace.", "published": "2024-02-06 01:05:14", "link": "http://arxiv.org/abs/2402.03616v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Comparing Abstraction in Humans and Large Language Models Using\n  Multimodal Serial Reproduction", "abstract": "Humans extract useful abstractions of the world from noisy sensory data.\nSerial reproduction allows us to study how people construe the world through a\nparadigm similar to the game of telephone, where one person observes a stimulus\nand reproduces it for the next to form a chain of reproductions. Past serial\nreproduction experiments typically employ a single sensory modality, but humans\noften communicate abstractions of the world to each other through language. To\ninvestigate the effect language on the formation of abstractions, we implement\na novel multimodal serial reproduction framework by asking people who receive a\nvisual stimulus to reproduce it in a linguistic format, and vice versa. We ran\nunimodal and multimodal chains with both humans and GPT-4 and find that adding\nlanguage as a modality has a larger effect on human reproductions than GPT-4's.\nThis suggests human visual and linguistic representations are more dissociable\nthan those of GPT-4.", "published": "2024-02-06 01:07:56", "link": "http://arxiv.org/abs/2402.03618v1", "categories": ["cs.AI", "cs.CL", "q-bio.NC"], "primary_category": "cs.AI"}
{"title": "Stanceosaurus 2.0: Classifying Stance Towards Russian and Spanish\n  Misinformation", "abstract": "The Stanceosaurus corpus (Zheng et al., 2022) was designed to provide\nhigh-quality, annotated, 5-way stance data extracted from Twitter, suitable for\nanalyzing cross-cultural and cross-lingual misinformation. In the Stanceosaurus\n2.0 iteration, we extend this framework to encompass Russian and Spanish. The\nformer is of current significance due to prevalent misinformation amid\nescalating tensions with the West and the violent incursion into Ukraine. The\nlatter, meanwhile, represents an enormous community that has been largely\noverlooked on major social media platforms. By incorporating an additional\n3,874 Spanish and Russian tweets over 41 misinformation claims, our objective\nis to support research focused on these issues. To demonstrate the value of\nthis data, we employed zero-shot cross-lingual transfer on multilingual BERT,\nyielding results on par with the initial Stanceosaurus study with a macro F1\nscore of 43 for both languages. This underlines the viability of stance\nclassification as an effective tool for identifying multicultural\nmisinformation.", "published": "2024-02-06 02:39:59", "link": "http://arxiv.org/abs/2402.03642v1", "categories": ["cs.CL", "cs.CY", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Learning to Generate Explainable Stock Predictions using Self-Reflective\n  Large Language Models", "abstract": "Explaining stock predictions is generally a difficult task for traditional\nnon-generative deep learning models, where explanations are limited to\nvisualizing the attention weights on important texts. Today, Large Language\nModels (LLMs) present a solution to this problem, given their known\ncapabilities to generate human-readable explanations for their decision-making\nprocess. However, the task of stock prediction remains challenging for LLMs, as\nit requires the ability to weigh the varying impacts of chaotic social texts on\nstock prices. The problem gets progressively harder with the introduction of\nthe explanation component, which requires LLMs to explain verbally why certain\nfactors are more important than the others. On the other hand, to fine-tune\nLLMs for such a task, one would need expert-annotated samples of explanation\nfor every stock movement in the training set, which is expensive and\nimpractical to scale. To tackle these issues, we propose our\nSummarize-Explain-Predict (SEP) framework, which utilizes a self-reflective\nagent and Proximal Policy Optimization (PPO) to let a LLM teach itself how to\ngenerate explainable stock predictions in a fully autonomous manner. The\nreflective agent learns how to explain past stock movements through\nself-reasoning, while the PPO trainer trains the model to generate the most\nlikely explanations from input texts. The training samples for the PPO trainer\nare also the responses generated during the reflective process, which\neliminates the need for human annotators. Using our SEP framework, we fine-tune\na LLM that can outperform both traditional deep-learning and LLM methods in\nprediction accuracy and Matthews correlation coefficient for the stock\nclassification task. To justify the generalization capability of our framework,\nwe further test it on the portfolio construction task, and demonstrate its\neffectiveness through various portfolio metrics.", "published": "2024-02-06 03:18:58", "link": "http://arxiv.org/abs/2402.03659v3", "categories": ["cs.LG", "cs.CL", "q-fin.ST"], "primary_category": "cs.LG"}
{"title": "Listen, Chat, and Edit: Text-Guided Soundscape Modification for Enhanced\n  Auditory Experience", "abstract": "In daily life, we encounter a variety of sounds, both desirable and\nundesirable, with limited control over their presence and volume. Our work\nintroduces \"Listen, Chat, and Edit\" (LCE), a novel multimodal sound mixture\neditor that modifies each sound source in a mixture based on user-provided text\ninstructions. LCE distinguishes itself with a user-friendly chat interface and\nits unique ability to edit multiple sound sources simultaneously within a\nmixture, without needing to separate them. Users input open-vocabulary text\nprompts, which are interpreted by a large language model to create a semantic\nfilter for editing the sound mixture. The system then decomposes the mixture\ninto its components, applies the semantic filter, and reassembles it into the\ndesired output. We developed a 160-hour dataset with over 100k mixtures,\nincluding speech and various audio sources, along with text prompts for diverse\nediting tasks like extraction, removal, and volume control. Our experiments\ndemonstrate significant improvements in signal quality across all editing tasks\nand robust performance in zero-shot scenarios with varying numbers and types of\nsound sources.", "published": "2024-02-06 05:05:38", "link": "http://arxiv.org/abs/2402.03710v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Clarify: Improving Model Robustness With Natural Language Corrections", "abstract": "The standard way to teach models is by feeding them lots of data. However,\nthis approach often teaches models incorrect ideas because they pick up on\nmisleading signals in the data. To prevent such misconceptions, we must\nnecessarily provide additional information beyond the training data. Prior\nmethods incorporate additional instance-level supervision, such as labels for\nmisleading features or additional labels for debiased data. However, such\nstrategies require a large amount of labeler effort. We hypothesize that people\nare good at providing textual feedback at the concept level, a capability that\nexisting teaching frameworks do not leverage. We propose Clarify, a novel\ninterface and method for interactively correcting model misconceptions. Through\nClarify, users need only provide a short text description of a model's\nconsistent failure patterns. Then, in an entirely automated way, we use such\ndescriptions to improve the training process. Clarify is the first end-to-end\nsystem for user model correction. Our user studies show that non-expert users\ncan successfully describe model misconceptions via Clarify, leading to\nincreased worst-case performance in two datasets. We additionally conduct a\ncase study on a large-scale image dataset, ImageNet, using Clarify to find and\nrectify 31 novel hard subpopulations.", "published": "2024-02-06 05:11:38", "link": "http://arxiv.org/abs/2402.03715v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Similarity-based Neighbor Selection for Graph LLMs", "abstract": "Text-attributed graphs (TAGs) present unique challenges for direct processing\nby Language Learning Models (LLMs), yet their extensive commonsense knowledge\nand robust reasoning capabilities offer great promise for node classification\nin TAGs. Prior research in this field has grappled with issues such as\nover-squashing, heterophily, and ineffective graph information integration,\nfurther compounded by inconsistencies in dataset partitioning and\nunderutilization of advanced LLMs. To address these challenges, we introduce\nSimilarity-based Neighbor Selection (SNS). Using SimCSE and advanced neighbor\nselection techniques, SNS effectively improves the quality of selected\nneighbors, thereby improving graph representation and alleviating issues like\nover-squashing and heterophily. Besides, as an inductive and training-free\napproach, SNS demonstrates superior generalization and scalability over\ntraditional GNN methods. Our comprehensive experiments, adhering to standard\ndataset partitioning practices, demonstrate that SNS, through simple prompt\ninteractions with LLMs, consistently outperforms vanilla GNNs and achieves\nstate-of-the-art results on datasets like PubMed in node classification,\nshowcasing LLMs' potential in graph structure understanding. Our research\nfurther underscores the significance of graph structure integration in LLM\napplications and identifies key factors for their success in node\nclassification. Code is available at https://github.com/ruili33/SNS.", "published": "2024-02-06 05:29:05", "link": "http://arxiv.org/abs/2402.03720v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.SI"], "primary_category": "cs.LG"}
{"title": "Consistent Joint Decision-Making with Heterogeneous Learning Models", "abstract": "This paper introduces a novel decision-making framework that promotes\nconsistency among decisions made by diverse models while utilizing external\nknowledge. Leveraging the Integer Linear Programming (ILP) framework, we map\npredictions from various models into globally normalized and comparable values\nby incorporating information about decisions' prior probability, confidence\n(uncertainty), and the models' expected accuracy. Our empirical study\ndemonstrates the superiority of our approach over conventional baselines on\nmultiple datasets.", "published": "2024-02-06 05:50:04", "link": "http://arxiv.org/abs/2402.03728v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.LO"], "primary_category": "cs.AI"}
{"title": "Deep Outdated Fact Detection in Knowledge Graphs", "abstract": "Knowledge graphs (KGs) have garnered significant attention for their vast\npotential across diverse domains. However, the issue of outdated facts poses a\nchallenge to KGs, affecting their overall quality as real-world information\nevolves. Existing solutions for outdated fact detection often rely on manual\nrecognition. In response, this paper presents DEAN (Deep outdatEd fAct\ndetectioN), a novel deep learning-based framework designed to identify outdated\nfacts within KGs. DEAN distinguishes itself by capturing implicit structural\ninformation among facts through comprehensive modeling of both entities and\nrelations. To effectively uncover latent out-of-date information, DEAN employs\na contrastive approach based on a pre-defined Relations-to-Nodes (R2N) graph,\nweighted by the number of entities. Experimental results demonstrate the\neffectiveness and superiority of DEAN over state-of-the-art baseline methods.", "published": "2024-02-06 05:58:15", "link": "http://arxiv.org/abs/2402.03732v1", "categories": ["cs.AI", "cs.CL", "cs.DL", "cs.LG", "68T09, 68T30, 68P20", "I.2.6; I.2.4; H.3.7; H.3.3"], "primary_category": "cs.AI"}
{"title": "The Instinctive Bias: Spurious Images lead to Illusion in MLLMs", "abstract": "Large language models (LLMs) have recently experienced remarkable progress,\nwhere the advent of multi-modal large language models (MLLMs) has endowed LLMs\nwith visual capabilities, leading to impressive performances in various\nmulti-modal tasks. However, those powerful MLLMs such as GPT-4V still fail\nspectacularly when presented with certain image and text inputs. In this paper,\nwe identify a typical class of inputs that baffles MLLMs, which consist of\nimages that are highly relevant but inconsistent with answers, causing MLLMs to\nsuffer from visual illusion. To quantify the effect, we propose CorrelationQA,\nthe first benchmark that assesses the visual illusion level given spurious\nimages. This benchmark contains 7,308 text-image pairs across 13 categories.\nBased on the proposed CorrelationQA, we conduct a thorough analysis on 9\nmainstream MLLMs, illustrating that they universally suffer from this\ninstinctive bias to varying degrees. We hope that our curated benchmark and\nevaluation results aid in better assessments of the MLLMs' robustness in the\npresence of misleading images. The code and datasets are available at\nhttps://github.com/MasaiahHan/CorrelationQA.", "published": "2024-02-06 06:48:46", "link": "http://arxiv.org/abs/2402.03757v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Learning a Decision Tree Algorithm with Transformers", "abstract": "Decision trees are renowned for their ability to achieve high predictive\nperformance while remaining interpretable, especially on tabular data.\nTraditionally, they are constructed through recursive algorithms, where they\npartition the data at every node in a tree. However, identifying a good\npartition is challenging, as decision trees optimized for local segments may\nnot yield global generalization. To address this, we introduce MetaTree, a\ntransformer-based model trained via meta-learning to directly produce strong\ndecision trees. Specifically, we fit both greedy decision trees and globally\noptimized decision trees on a large number of datasets, and train MetaTree to\nproduce only the trees that achieve strong generalization performance. This\ntraining enables MetaTree to emulate these algorithms and intelligently adapt\nits strategy according to the context, thereby achieving superior\ngeneralization performance.", "published": "2024-02-06 07:40:53", "link": "http://arxiv.org/abs/2402.03774v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Exposing propaganda: an analysis of stylistic cues comparing human\n  annotations and machine classification", "abstract": "This paper investigates the language of propaganda and its stylistic\nfeatures. It presents the PPN dataset, standing for Propagandist Pseudo-News, a\nmultisource, multilingual, multimodal dataset composed of news articles\nextracted from websites identified as propaganda sources by expert agencies. A\nlimited sample from this set was randomly mixed with papers from the regular\nFrench press, and their URL masked, to conduct an annotation-experiment by\nhumans, using 11 distinct labels. The results show that human annotators were\nable to reliably discriminate between the two types of press across each of the\nlabels. We propose different NLP techniques to identify the cues used by the\nannotators, and to compare them with machine classification. They include the\nanalyzer VAGO to measure discourse vagueness and subjectivity, a TF-IDF to\nserve as a baseline, and four different classifiers: two RoBERTa-based models,\nCATS using syntax, and one XGBoost combining syntactic and semantic features.", "published": "2024-02-06 07:51:54", "link": "http://arxiv.org/abs/2402.03780v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "RevOrder: A Novel Method for Enhanced Arithmetic in Language Models", "abstract": "This paper presents RevOrder, a novel technique aimed at improving arithmetic\noperations in large language models (LLMs) by reversing the output digits in\naddition, subtraction, and n-digit by 1-digit (nD by 1D) multiplication tasks.\nOur method significantly reduces the Count of Sequential Intermediate Digits\n(CSID) to $\\mathcal{O}(1)$, a new metric we introduce to assess equation\ncomplexity. Through comprehensive testing, RevOrder not only achieves perfect\naccuracy in basic arithmetic operations but also substantially boosts LLM\nperformance in division tasks, particularly with large numbers where\ntraditional models struggle. Implementation of RevOrder is cost-effective for\nboth training and inference phases. Moreover, applying RevOrder to fine-tune\nthe LLaMA2-7B model on the GSM8K math task results in a considerable\nimprovement, reducing equation calculation errors by 46% and increasing overall\nscores from 41.6 to 44.4.", "published": "2024-02-06 09:10:35", "link": "http://arxiv.org/abs/2402.03822v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "DistiLLM: Towards Streamlined Distillation for Large Language Models", "abstract": "Knowledge distillation (KD) is widely used for compressing a teacher model to\na smaller student model, reducing its inference cost and memory footprint while\npreserving model capabilities. However, current KD methods for auto-regressive\nsequence models (e.g., large language models) suffer from missing a\nstandardized objective function. Moreover, the recent use of student-generated\noutputs to address training-inference mismatches has significantly escalated\ncomputational costs. To tackle these issues, we introduce DistiLLM, a more\neffective and efficient KD framework for auto-regressive language models.\nDistiLLM comprises two components: (1) a novel skew Kullback-Leibler divergence\nloss, where we unveil and leverage its theoretical properties, and (2) an\nadaptive off-policy approach designed to enhance the efficiency in utilizing\nstudent-generated outputs. Extensive experiments, including\ninstruction-following tasks, demonstrate the effectiveness of DistiLLM in\nbuilding high-performing student models while achieving up to 4.3$\\times$\nspeedup compared to recent KD methods.", "published": "2024-02-06 11:10:35", "link": "http://arxiv.org/abs/2402.03898v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "REBORN: Reinforcement-Learned Boundary Segmentation with Iterative\n  Training for Unsupervised ASR", "abstract": "Unsupervised automatic speech recognition (ASR) aims to learn the mapping\nbetween the speech signal and its corresponding textual transcription without\nthe supervision of paired speech-text data. A word/phoneme in the speech signal\nis represented by a segment of speech signal with variable length and unknown\nboundary, and this segmental structure makes learning the mapping between\nspeech and text challenging, especially without paired data. In this paper, we\npropose REBORN,Reinforcement-Learned Boundary Segmentation with Iterative\nTraining for Unsupervised ASR. REBORN alternates between (1) training a\nsegmentation model that predicts the boundaries of the segmental structures in\nspeech signals and (2) training the phoneme prediction model, whose input is\nthe speech feature segmented by the segmentation model, to predict a phoneme\ntranscription. Since supervised data for training the segmentation model is not\navailable, we use reinforcement learning to train the segmentation model to\nfavor segmentations that yield phoneme sequence predictions with a lower\nperplexity. We conduct extensive experiments and find that under the same\nsetting, REBORN outperforms all prior unsupervised ASR models on LibriSpeech,\nTIMIT, and five non-English languages in Multilingual LibriSpeech. We\ncomprehensively analyze why the boundaries learned by REBORN improve the\nunsupervised ASR performance.", "published": "2024-02-06 13:26:19", "link": "http://arxiv.org/abs/2402.03988v3", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "AlbNews: A Corpus of Headlines for Topic Modeling in Albanian", "abstract": "The scarcity of available text corpora for low-resource languages like\nAlbanian is a serious hurdle for research in natural language processing tasks.\nThis paper introduces AlbNews, a collection of 600 topically labeled news\nheadlines and 2600 unlabeled ones in Albanian. The data can be freely used for\nconducting topic modeling research. We report the initial classification scores\nof some traditional machine learning classifiers trained with the AlbNews\nsamples. These results show that basic models outrun the ensemble learning ones\nand can serve as a baseline for future experiments.", "published": "2024-02-06 14:24:28", "link": "http://arxiv.org/abs/2402.04028v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Use of a Large Language Model for Cyberbullying Detection", "abstract": "The dominance of social media has added to the channels of bullying for\nperpetrators. Unfortunately, cyberbullying (CB) is the most prevalent\nphenomenon in todays cyber world, and is a severe threat to the mental and\nphysical health of citizens. This opens the need to develop a robust system to\nprevent bullying content from online forums, blogs, and social media platforms\nto manage the impact in our society. Several machine learning (ML) algorithms\nhave been proposed for this purpose. However, their performances are not\nconsistent due to high class imbalance and generalisation issues. In recent\nyears, large language models (LLMs) like BERT and RoBERTa have achieved\nstate-of-the-art (SOTA) results in several natural language processing (NLP)\ntasks. Unfortunately, the LLMs have not been applied extensively for CB\ndetection. In our paper, we explored the use of these models for cyberbullying\n(CB) detection. We have prepared a new dataset (D2) from existing studies\n(Formspring and Twitter). Our experimental results for dataset D1 and D2 showed\nthat RoBERTa outperformed other models.", "published": "2024-02-06 15:46:31", "link": "http://arxiv.org/abs/2402.04088v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.AP", "H.3.3"], "primary_category": "cs.CL"}
{"title": "Scaling Laws for Downstream Task Performance in Machine Translation", "abstract": "Scaling laws provide important insights that can guide the design of large\nlanguage models (LLMs). Existing work has primarily focused on studying scaling\nlaws for pretraining (upstream) loss. However, in transfer learning settings,\nin which LLMs are pretrained on an unsupervised dataset and then finetuned on a\ndownstream task, we often also care about the downstream performance. In this\nwork, we study the scaling behavior in a transfer learning setting, where LLMs\nare finetuned for machine translation tasks. Specifically, we investigate how\nthe choice of the pretraining data and its size affect downstream performance\n(translation quality) as judged by: downstream cross-entropy and translation\nquality metrics such as BLEU and COMET scores. Our experiments indicate that\nthe size of the finetuning dataset and the distribution alignment between the\npretraining and downstream data significantly influence the scaling behavior.\nWith sufficient alignment, both downstream cross-entropy and translation\nquality scores improve monotonically with more pretraining data. In such cases,\nwe show that it is possible to predict the downstream translation quality\nmetrics with good accuracy using a log-law. However, there are cases where\nmoderate misalignment causes the downstream translation scores to fluctuate or\nget worse with more pretraining, whereas downstream cross-entropy monotonically\nimproves. By analyzing these, we provide new practical insights for choosing\nappropriate pretraining data.", "published": "2024-02-06 17:31:20", "link": "http://arxiv.org/abs/2402.04177v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science", "abstract": "Intelligent agents powered by large language models (LLMs) have demonstrated\nsubstantial promise in autonomously conducting experiments and facilitating\nscientific discoveries across various disciplines. While their capabilities are\npromising, these agents, called scientific LLM agents, also introduce novel\nvulnerabilities that demand careful consideration for safety. However, there\nexists a notable gap in the literature, as there has been no comprehensive\nexploration of these vulnerabilities. This perspective paper fills this gap by\nconducting a thorough examination of vulnerabilities in LLM-based agents within\nscientific domains, shedding light on potential risks associated with their\nmisuse and emphasizing the need for safety measures. We begin by providing a\ncomprehensive overview of the potential risks inherent to scientific LLM\nagents, taking into account user intent, the specific scientific domain, and\ntheir potential impact on the external environment. Then, we delve into the\norigins of these vulnerabilities and provide a scoping review of the limited\nexisting works. Based on our analysis, we propose a triadic framework involving\nhuman regulation, agent alignment, and an understanding of environmental\nfeedback (agent regulation) to mitigate these identified risks. Furthermore, we\nhighlight the limitations and challenges associated with safeguarding\nscientific agents and advocate for the development of improved models, robust\nbenchmarks, and comprehensive regulations to address these issues effectively.", "published": "2024-02-06 18:54:07", "link": "http://arxiv.org/abs/2402.04247v4", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming\n  and Robust Refusal", "abstract": "Automated red teaming holds substantial promise for uncovering and mitigating\nthe risks associated with the malicious use of large language models (LLMs),\nyet the field lacks a standardized evaluation framework to rigorously assess\nnew methods. To address this issue, we introduce HarmBench, a standardized\nevaluation framework for automated red teaming. We identify several desirable\nproperties previously unaccounted for in red teaming evaluations and\nsystematically design HarmBench to meet these criteria. Using HarmBench, we\nconduct a large-scale comparison of 18 red teaming methods and 33 target LLMs\nand defenses, yielding novel insights. We also introduce a highly efficient\nadversarial training method that greatly enhances LLM robustness across a wide\nrange of attacks, demonstrating how HarmBench enables codevelopment of attacks\nand defenses. We open source HarmBench at\nhttps://github.com/centerforaisafety/HarmBench.", "published": "2024-02-06 18:59:08", "link": "http://arxiv.org/abs/2402.04249v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "BiLLM: Pushing the Limit of Post-Training Quantization for LLMs", "abstract": "Pretrained large language models (LLMs) exhibit exceptional general language\nprocessing capabilities but come with significant demands on memory and\ncomputational resources. As a powerful compression technology, binarization can\nextremely reduce model weights to a mere 1 bit, lowering the expensive\ncomputation and memory requirements. However, existing quantization techniques\nfall short of maintaining LLM performance under ultra-low bit-widths. In\nresponse to this challenge, we present BiLLM, a groundbreaking 1-bit\npost-training quantization scheme tailored for pretrained LLMs. Based on the\nweight distribution of LLMs, BiLLM first identifies and structurally selects\nsalient weights, and minimizes the compression loss through an effective binary\nresidual approximation strategy. Moreover, considering the bell-shaped\ndistribution of the non-salient weights, we propose an optimal splitting search\nto group and binarize them accurately. BiLLM achieving for the first time\nhigh-accuracy inference (e.g. 8.41 perplexity on LLaMA2-70B) with only 1.08-bit\nweights across various LLMs families and evaluation metrics, outperforms SOTA\nquantization methods of LLM by significant margins. Moreover, BiLLM enables the\nbinarization process of the LLM with 7 billion weights within 0.5 hours on a\nsingle GPU, demonstrating satisfactory time efficiency. Our code is available\nat https://github.com/Aaronhuang-778/BiLLM.", "published": "2024-02-06 09:26:34", "link": "http://arxiv.org/abs/2402.04291v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "LESS: Selecting Influential Data for Targeted Instruction Tuning", "abstract": "Instruction tuning has unlocked powerful capabilities in large language\nmodels (LLMs), effectively using combined datasets to develop generalpurpose\nchatbots. However, real-world applications often require a specialized suite of\nskills (e.g., reasoning). The challenge lies in identifying the most relevant\ndata from these extensive datasets to effectively develop specific\ncapabilities, a setting we frame as targeted instruction tuning. We propose\nLESS, an optimizer-aware and practically efficient algorithm to effectively\nestimate data influences and perform Low-rank gradiEnt Similarity Search for\ninstruction data selection. Crucially, LESS adapts existing influence\nformulations to work with the Adam optimizer and variable-length instruction\ndata. LESS first constructs a highly reusable and transferable gradient\ndatastore with low-dimensional gradient features and then selects examples\nbased on their similarity to few-shot examples embodying a specific capability.\nExperiments show that training on a LESS-selected 5% of the data can often\noutperform training on the full dataset across diverse downstream tasks.\nFurthermore, the selected data is highly transferable: smaller models can be\nleveraged to select useful data for larger models and models from different\nfamilies. Our qualitative analysis shows that our method goes beyond surface\nform cues to identify data that exemplifies the necessary reasoning skills for\nthe intended downstream application.", "published": "2024-02-06 19:18:04", "link": "http://arxiv.org/abs/2402.04333v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LegalLens: Leveraging LLMs for Legal Violation Identification in\n  Unstructured Text", "abstract": "In this study, we focus on two main tasks, the first for detecting legal\nviolations within unstructured textual data, and the second for associating\nthese violations with potentially affected individuals. We constructed two\ndatasets using Large Language Models (LLMs) which were subsequently validated\nby domain expert annotators. Both tasks were designed specifically for the\ncontext of class-action cases. The experimental design incorporated fine-tuning\nmodels from the BERT family and open-source LLMs, and conducting few-shot\nexperiments using closed-source LLMs. Our results, with an F1-score of 62.69\\%\n(violation identification) and 81.02\\% (associating victims), show that our\ndatasets and setups can be used for both tasks. Finally, we publicly release\nthe datasets and the code used for the experiments in order to advance further\nresearch in the area of legal natural language processing (NLP).", "published": "2024-02-06 19:18:56", "link": "http://arxiv.org/abs/2402.04335v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "QuIP#: Even Better LLM Quantization with Hadamard Incoherence and\n  Lattice Codebooks", "abstract": "Post-training quantization (PTQ) reduces the memory footprint of LLMs by\nquantizing their weights to low-precision. In this work, we introduce QuIP#, a\nweight-only PTQ method that achieves state-of-the-art results in extreme\ncompression regimes ($\\le$ 4 bits per weight) using three novel techniques.\nFirst, QuIP# improves QuIP's (Chee et al., 2023) incoherence processing by\nusing the randomized Hadamard transform, which is faster and has better\ntheoretical properties. Second, QuIP# uses vector quantization to take\nadvantage of the ball-shaped sub-Gaussian distribution that incoherent weights\npossess: specifically, we introduce a set of hardware-efficient codebooks based\non the highly symmetric $E_8$ lattice, which achieves the optimal 8-dimension\nunit ball packing. Third, QuIP# uses fine-tuning to improve fidelity to the\noriginal model. Our experiments show that QuIP# outperforms existing PTQ\nmethods, enables new behaviors in PTQ scaling, and supports fast inference. Our\ncode can be found at https://github.com/Cornell-RelaxML/quip-sharp.", "published": "2024-02-06 20:52:12", "link": "http://arxiv.org/abs/2402.04396v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Dual-View Visual Contextualization for Web Navigation", "abstract": "Automatic web navigation aims to build a web agent that can follow language\ninstructions to execute complex and diverse tasks on real-world websites.\nExisting work primarily takes HTML documents as input, which define the\ncontents and action spaces (i.e., actionable elements and operations) of\nwebpages. Nevertheless, HTML documents may not provide a clear task-related\ncontext for each element, making it hard to select the right (sequence of)\nactions. In this paper, we propose to contextualize HTML elements through their\n\"dual views\" in webpage screenshots: each HTML element has its corresponding\nbounding box and visual content in the screenshot. We build upon the insight --\nweb developers tend to arrange task-related elements nearby on webpages to\nenhance user experiences -- and propose to contextualize each element with its\nneighbor elements, using both textual and visual features. The resulting\nrepresentations of HTML elements are more informative for the agent to take\naction. We validate our method on the recently released Mind2Web dataset, which\nfeatures diverse navigation domains and tasks on real-world websites. Our\nmethod consistently outperforms the baseline in all the scenarios, including\ncross-task, cross-website, and cross-domain ones.", "published": "2024-02-06 23:52:10", "link": "http://arxiv.org/abs/2402.04476v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Personalized Language Modeling from Personalized Human Feedback", "abstract": "Personalized large language models (LLMs) are designed to tailor responses to\nindividual user preferences. While Reinforcement Learning from Human Feedback\n(RLHF) is a commonly used framework for aligning LLMs with human preferences,\nvanilla RLHF assumes that all human preferences share the same distribution,\npreventing fine-tuned LLMs from generating personalized content when user\npreferences are diverse. In this work, we propose Personalized-RLHF (P-RLHF),\nan efficient framework that utilizes a lightweight user model to capture\nindividual user preferences and jointly learns the user model and the\npersonalized LLM from human feedback. P-RLHF exhibits the following three\ncharacteristics: (1) It enables an LLM to generate personalized content and\nscale efficiently with growing number of users. (2) It handles both explicit\nuser preferences described as textual input and implicit user preferences\nencoded in the feedback data. (3) It eliminates the need for users to fully\narticulate their preferences, which are normally needed for prompting LLMs to\ngenerate personalized content yet are often impractical to obtain in real-world\nscenarios. Our experimental results show that personalized LLMs trained using\nP-RLHF generate responses that are more closely aligned with individual user\npreferences, outperforming vanilla, non-personalized RLHF and prompting-based\npersonalization approaches across different tasks. We opensource our code at\nhttps://github.com/HumainLab/Personalized_RLHF.", "published": "2024-02-06 04:18:58", "link": "http://arxiv.org/abs/2402.05133v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CADReN: Contextual Anchor-Driven Relational Network for Controllable\n  Cross-Graphs Node Importance Estimation", "abstract": "Node Importance Estimation (NIE) is crucial for integrating external\ninformation into Large Language Models through Retriever-Augmented Generation.\nTraditional methods, focusing on static, single-graph characteristics, lack\nadaptability to new graphs and user-specific requirements. CADReN, our proposed\nmethod, addresses these limitations by introducing a Contextual Anchor (CA)\nmechanism. This approach enables the network to assess node importance relative\nto the CA, considering both structural and semantic features within Knowledge\nGraphs (KGs). Extensive experiments show that CADReN achieves better\nperformance in cross-graph NIE task, with zero-shot prediction ability. CADReN\nis also proven to match the performance of previous models on single-graph NIE\ntask. Additionally, we introduce and opensource two new datasets, RIC200 and\nWK1K, specifically designed for cross-graph NIE research, providing a valuable\nresource for future developments in this domain.", "published": "2024-02-06 11:29:44", "link": "http://arxiv.org/abs/2402.05135v1", "categories": ["cs.AI", "cs.CL", "cs.IR", "68T07"], "primary_category": "cs.AI"}
{"title": "Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains", "abstract": "Large Language Models (LLMs) have demonstrated remarkable proficiency in\nunderstanding and generating natural language. However, their capabilities wane\nin highly specialized domains underrepresented in the pretraining corpus, such\nas physical and biomedical sciences. This work explores how to repurpose\ngeneral LLMs into effective task solvers for specialized domains. We introduce\na novel, model-agnostic framework for learning custom input tags, which are\nparameterized as continuous vectors appended to the LLM's embedding layer, to\ncondition the LLM. We design two types of input tags: domain tags are used to\ndelimit specialized representations (e.g., chemical formulas) and provide\ndomain-relevant context; function tags are used to represent specific functions\n(e.g., predicting molecular properties) and compress function-solving\ninstructions. We develop a three-stage protocol to learn these tags using\nauxiliary data and domain knowledge. By explicitly disentangling task domains\nfrom task functions, our method enables zero-shot generalization to unseen\nproblems through diverse combinations of the input tags. It also boosts LLM's\nperformance in various specialized domains, such as predicting protein or\nchemical properties and modeling drug-target interactions, outperforming expert\nmodels tailored to these tasks.", "published": "2024-02-06 20:11:54", "link": "http://arxiv.org/abs/2402.05140v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Limits of Large Language Models in Debating Humans", "abstract": "Large Language Models (LLMs) have shown remarkable promise in communicating\nwith humans. Their potential use as artificial partners with humans in\nsociological experiments involving conversation is an exciting prospect. But\nhow viable is it? Here, we rigorously test the limits of agents that debate\nusing LLMs in a preregistered study that runs multiple debate-based opinion\nconsensus games. Each game starts with six humans, six agents, or three humans\nand three agents. We found that agents can blend in and concentrate on a\ndebate's topic better than humans, improving the productivity of all players.\nYet, humans perceive agents as less convincing and confident than other humans,\nand several behavioral metrics of humans and agents we collected deviate\nmeasurably from each other. We observed that agents are already decent\ndebaters, but their behavior generates a pattern distinctly different from the\nhuman-generated data.", "published": "2024-02-06 03:24:27", "link": "http://arxiv.org/abs/2402.06049v2", "categories": ["cs.AI", "cs.CL", "cs.HC", "stat.AP"], "primary_category": "cs.AI"}
{"title": "The Essential Role of Causality in Foundation World Models for Embodied\n  AI", "abstract": "Recent advances in foundation models, especially in large multi-modal models\nand conversational agents, have ignited interest in the potential of generally\ncapable embodied agents. Such agents will require the ability to perform new\ntasks in many different real-world environments. However, current foundation\nmodels fail to accurately model physical interactions and are therefore\ninsufficient for Embodied AI. The study of causality lends itself to the\nconstruction of veridical world models, which are crucial for accurately\npredicting the outcomes of possible interactions. This paper focuses on the\nprospects of building foundation world models for the upcoming generation of\nembodied agents and presents a novel viewpoint on the significance of causality\nwithin these. We posit that integrating causal considerations is vital to\nfacilitating meaningful physical interactions with the world. Finally, we\ndemystify misconceptions about causality in this context and present our\noutlook for future research.", "published": "2024-02-06 17:15:33", "link": "http://arxiv.org/abs/2402.06665v2", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.RO"], "primary_category": "cs.AI"}
{"title": "A Human-Machine Collaboration Framework for the Development of Schemas", "abstract": "The Winograd Schema Challenge (WSC), a seemingly well-thought-out test for\nmachine intelligence, has been proposed to shed light on developing systems\nthat exhibit human behavior. Since its introduction, it aimed to pivot the\nfocus of the AI community from the technology to the science of AI. While\ncommon and trivial for humans, studies show that it is still challenging for\nmachines, especially when they have to deal with novel schemas, that is,\nwell-designed sentences that require the resolving of definite pronouns. As\nresearchers have become increasingly interested in the challenge itself, this\npresumably necessitates the availability of an extensive collection of Winograd\nschemas, which goes beyond what human experts can reasonably develop\nthemselves, especially after proposed ways of utilizing them as novel forms of\nCAPTCHAs.\n  To address this necessity, we propose a novel framework that explicitly\nfocuses on how humans and machines can collaborate as teammates to design novel\nschemas from scratch. This is being accomplished by combining two recent\nstudies from the literature: i) Winventor, a machine-driven approach for the\ndevelopment of large amounts of Winograd schemas, albeit not of high quality,\nand ii) WinoFlexi, an online crowdsourcing system that allows crowd workers to\ndevelop a limited number of schemas often of similar quality to that of\nexperts. Our proposal crafts a new road map toward developing a novel\ncollaborative platform that amplifies human and machine intelligence by\ncombining their complementary strengths.", "published": "2024-02-06 15:41:49", "link": "http://arxiv.org/abs/2402.07932v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "I.2.0; I.2.3; I.2.4; I.2.7; I.5.1; I.5.4"], "primary_category": "cs.HC"}
{"title": "Enhancing Retrieval Processes for Language Generation with Augmented\n  Queries", "abstract": "In the rapidly changing world of smart technology, searching for documents\nhas become more challenging due to the rise of advanced language models. These\nmodels sometimes face difficulties, like providing inaccurate information,\ncommonly known as \"hallucination.\" This research focuses on addressing this\nissue through Retrieval-Augmented Generation (RAG), a technique that guides\nmodels to give accurate responses based on real facts. To overcome scalability\nissues, the study explores connecting user queries with sophisticated language\nmodels such as BERT and Orca2, using an innovative query optimization process.\nThe study unfolds in three scenarios: first, without RAG, second, without\nadditional assistance, and finally, with extra help. Choosing the compact yet\nefficient Orca2 7B model demonstrates a smart use of computing resources. The\nempirical results indicate a significant improvement in the initial language\nmodel's performance under RAG, particularly when assisted with prompts\naugmenters. Consistency in document retrieval across different encodings\nhighlights the effectiveness of using language model-generated queries. The\nintroduction of UMAP for BERT further simplifies document retrieval while\nmaintaining strong results.", "published": "2024-02-06 13:19:53", "link": "http://arxiv.org/abs/2402.16874v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Enhancing Cross-Modal Contextual Congruence for Crowdfunding Success\n  using Knowledge-infused Learning", "abstract": "The digital landscape continually evolves with multimodality, enriching the\nonline experience for users. Creators and marketers aim to weave subtle\ncontextual cues from various modalities into congruent content to engage users\nwith a harmonious message. This interplay of multimodal cues is often a crucial\nfactor in attracting users' attention. However, this richness of multimodality\npresents a challenge to computational modeling, as the semantic contextual cues\nspanning across modalities need to be unified to capture the true holistic\nmeaning of the multimodal content. This contextual meaning is critical in\nattracting user engagement as it conveys the intended message of the brand or\nthe organization. In this work, we incorporate external commonsense knowledge\nfrom knowledge graphs to enhance the representation of multimodal data using\ncompact Visual Language Models (VLMs) and predict the success of multi-modal\ncrowdfunding campaigns. Our results show that external knowledge commonsense\nbridges the semantic gap between text and image modalities, and the enhanced\nknowledge-infused representations improve the predictive performance of models\nfor campaign success upon the baselines without knowledge. Our findings\nhighlight the significance of contextual congruence in online multimodal\ncontent for engaging and successful crowdfunding campaigns.", "published": "2024-02-06 00:51:27", "link": "http://arxiv.org/abs/2402.03607v2", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.CY", "cs.HC", "I.2.7; I.2.10; I.2.4; I.2.1"], "primary_category": "cs.AI"}
{"title": "Attention with Markov: A Framework for Principled Analysis of\n  Transformers via Markov Chains", "abstract": "In recent years, attention-based transformers have achieved tremendous\nsuccess across a variety of disciplines including natural languages. A key\ningredient behind their success is the generative pretraining procedure, during\nwhich these models are trained on a large text corpus in an auto-regressive\nmanner. To shed light on this phenomenon, we propose a new framework that\nallows both theory and systematic experiments to study the sequential modeling\ncapabilities of transformers through the lens of Markov chains. Inspired by the\nMarkovianity of natural languages, we model the data as a Markovian source and\nutilize this framework to systematically study the interplay between the\ndata-distributional properties, the transformer architecture, the learnt\ndistribution, and the final model performance. In particular, we theoretically\ncharacterize the loss landscape of single-layer transformers and show the\nexistence of global minima and bad local minima contingent upon the specific\ndata characteristics and the transformer architecture. Backed by experiments,\nwe demonstrate that our theoretical findings are in congruence with the\nempirical results. We further investigate these findings in the broader context\nof higher order Markov chains and deeper architectures, and outline open\nproblems in this arena. Code is available at\n\\url{https://github.com/Bond1995/Markov}.", "published": "2024-02-06 17:18:59", "link": "http://arxiv.org/abs/2402.04161v1", "categories": ["cs.LG", "cs.CL", "cs.IT", "math.IT", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Monitoring the evolution of antisemitic discourse on extremist social\n  media using BERT", "abstract": "Racism and intolerance on social media contribute to a toxic online\nenvironment which may spill offline to foster hatred, and eventually lead to\nphysical violence. That is the case with online antisemitism, the specific\ncategory of hatred considered in this study. Tracking antisemitic themes and\ntheir associated terminology over time in online discussions could help monitor\nthe sentiments of their participants and their evolution, and possibly offer\navenues for intervention that may prevent the escalation of hatred. Due to the\nlarge volume and constant evolution of online traffic, monitoring conversations\nmanually is impractical. Instead, we propose an automated method that extracts\nantisemitic themes and terminology from extremist social media over time and\ncaptures their evolution. Since supervised learning would be too limited for\nsuch a task, we created an unsupervised online machine learning approach that\nuses large language models to assess the contextual similarity of posts. The\nmethod clusters similar posts together, dividing, and creating additional\nclusters over time when sub-themes emerge from existing ones or new themes\nappear. The antisemitic terminology used within each theme is extracted from\nthe posts in each cluster. Our experiments show that our methodology\noutperforms existing baselines and demonstrates the kind of themes and\nsub-themes it discovers within antisemitic discourse along with their\nassociated terminology. We believe that our approach will be useful for\nmonitoring the evolution of all kinds of hatred beyond antisemitism on social\nplatforms.", "published": "2024-02-06 20:34:49", "link": "http://arxiv.org/abs/2403.05548v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.IR", "cs.LG", "cs.SI"], "primary_category": "cs.CY"}
{"title": "Binaural sound source localization using a hybrid time and frequency\n  domain model", "abstract": "This paper introduces a new approach to sound source localization using\nhead-related transfer function (HRTF) characteristics, which enable precise\nfull-sphere localization from raw data. While previous research focused\nprimarily on using extensive microphone arrays in the frontal plane, this\narrangement often encountered limitations in accuracy and robustness when\ndealing with smaller microphone arrays. Our model proposes using both time and\nfrequency domain for sound source localization while utilizing Deep Learning\n(DL) approach. The performance of our proposed model, surpasses the current\nstate-of-the-art results. Specifically, it boasts an average angular error of\n$0.24 degrees and an average Euclidean distance of 0.01 meters, while the known\nstate-of-the-art gives average angular error of 19.07 degrees and average\nEuclidean distance of 1.08 meters. This level of accuracy is of paramount\nimportance for a wide range of applications, including robotics, virtual\nreality, and aiding individuals with cochlear implants (CI).", "published": "2024-02-06 10:28:07", "link": "http://arxiv.org/abs/2402.03867v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MusicRL: Aligning Music Generation to Human Preferences", "abstract": "We propose MusicRL, the first music generation system finetuned from human\nfeedback. Appreciation of text-to-music models is particularly subjective since\nthe concept of musicality as well as the specific intention behind a caption\nare user-dependent (e.g. a caption such as \"upbeat work-out music\" can map to a\nretro guitar solo or a techno pop beat). Not only this makes supervised\ntraining of such models challenging, but it also calls for integrating\ncontinuous human feedback in their post-deployment finetuning. MusicRL is a\npretrained autoregressive MusicLM (Agostinelli et al., 2023) model of discrete\naudio tokens finetuned with reinforcement learning to maximise sequence-level\nrewards. We design reward functions related specifically to text-adherence and\naudio quality with the help from selected raters, and use those to finetune\nMusicLM into MusicRL-R. We deploy MusicLM to users and collect a substantial\ndataset comprising 300,000 pairwise preferences. Using Reinforcement Learning\nfrom Human Feedback (RLHF), we train MusicRL-U, the first text-to-music model\nthat incorporates human feedback at scale. Human evaluations show that both\nMusicRL-R and MusicRL-U are preferred to the baseline. Ultimately, MusicRL-RU\ncombines the two approaches and results in the best model according to human\nraters. Ablation studies shed light on the musical attributes influencing human\npreferences, indicating that text adherence and quality only account for a part\nof it. This underscores the prevalence of subjectivity in musical appreciation\nand calls for further involvement of human listeners in the finetuning of music\ngeneration models.", "published": "2024-02-06 18:36:52", "link": "http://arxiv.org/abs/2402.04229v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Bidirectional Autoregressive Diffusion Model for Dance Generation", "abstract": "Dance serves as a powerful medium for expressing human emotions, but the\nlifelike generation of dance is still a considerable challenge. Recently,\ndiffusion models have showcased remarkable generative abilities across various\ndomains. They hold promise for human motion generation due to their adaptable\nmany-to-many nature. Nonetheless, current diffusion-based motion generation\nmodels often create entire motion sequences directly and unidirectionally,\nlacking focus on the motion with local and bidirectional enhancement. When\nchoreographing high-quality dance movements, people need to take into account\nnot only the musical context but also the nearby music-aligned dance motions.\nTo authentically capture human behavior, we propose a Bidirectional\nAutoregressive Diffusion Model (BADM) for music-to-dance generation, where a\nbidirectional encoder is built to enforce that the generated dance is\nharmonious in both the forward and backward directions. To make the generated\ndance motion smoother, a local information decoder is built for local motion\nenhancement. The proposed framework is able to generate new motions based on\nthe input conditions and nearby motions, which foresees individual motion\nslices iteratively and consolidates all predictions. To further refine the\nsynchronicity between the generated dance and the beat, the beat information is\nincorporated as an input to generate better music-aligned dance movements.\nExperimental results demonstrate that the proposed model achieves\nstate-of-the-art performance compared to existing unidirectional approaches on\nthe prominent benchmark for music-to-dance generation.", "published": "2024-02-06 19:42:18", "link": "http://arxiv.org/abs/2402.04356v4", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
