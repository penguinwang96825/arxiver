{"title": "Pragmatically Appropriate Diversity for Dialogue Evaluation", "abstract": "Linguistic pragmatics state that a conversation's underlying speech acts can\nconstrain the type of response which is appropriate at each turn in the\nconversation. When generating dialogue responses, neural dialogue agents\nstruggle to produce diverse responses. Currently, dialogue diversity is\nassessed using automatic metrics, but the underlying speech acts do not inform\nthese metrics.\n  To remedy this, we propose the notion of Pragmatically Appropriate Diversity,\ndefined as the extent to which a conversation creates and constrains the\ncreation of multiple diverse responses. Using a human-created multi-response\ndataset, we find significant support for the hypothesis that speech acts\nprovide a signal for the diversity of the set of next responses. Building on\nthis result, we propose a new human evaluation task where creative writers\npredict the extent to which conversations inspire the creation of multiple\ndiverse responses. Our studies find that writers' judgments align with the\nPragmatically Appropriate Diversity of conversations. Our work suggests that\nexpectations for diversity metric scores should vary depending on the speech\nact.", "published": "2023-04-06 01:24:18", "link": "http://arxiv.org/abs/2304.02812v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Affect as a proxy for literary mood", "abstract": "We propose to use affect as a proxy for mood in literary texts. In this\nstudy, we explore the differences in computationally detecting tone versus\ndetecting mood. Methodologically we utilize affective word embeddings to look\nat the affective distribution in different text segments. We also present a\nsimple yet efficient and effective method of enhancing emotion lexicons to take\nboth semantic shift and the domain of the text into account producing\nreal-world congruent results closely matching both contemporary and modern\nqualitative analyses.", "published": "2023-04-06 06:53:23", "link": "http://arxiv.org/abs/2304.02894v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SpanRE: Entities and Overlapping Relations Extraction Based on Spans and\n  Entity Attention", "abstract": "Extracting entities and relations is an essential task of information\nextraction. Triplets extracted from a sentence might overlap with each other.\nPrevious methods either did not address the overlapping issues or solved\noverlapping issues partially. To tackle triplet overlapping problems\ncompletely, firstly we extract candidate subjects with a standard span\nmechanism. Then we present a labeled span mechanism to extract the objects and\nrelations simultaneously, we use the labeled span mechanism to generate labeled\nspans whose start and end positions indicate the objects, and whose labels\ncorrespond to relations of subject and objects. Besides, we design an entity\nattention mechanism to enhance the information fusion between subject and\nsentence during extracting objects and relations. We test our method on two\npublic datasets, our method achieves the best performances on these two\ndatasets.", "published": "2023-04-06 07:19:39", "link": "http://arxiv.org/abs/2304.02901v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating Chain-of-thought with ChatGPT for Stance Detection on\n  Social Media", "abstract": "Stance detection predicts attitudes towards targets in texts and has gained\nattention with the rise of social media. Traditional approaches include\nconventional machine learning, early deep neural networks, and pre-trained\nfine-tuning models. However, with the evolution of very large pre-trained\nlanguage models (VLPLMs) like ChatGPT (GPT-3.5), traditional methods face\ndeployment challenges. The parameter-free Chain-of-Thought (CoT) approach, not\nrequiring backpropagation training, has emerged as a promising alternative.\nThis paper examines CoT's effectiveness in stance detection tasks,\ndemonstrating its superior accuracy and discussing associated challenges.", "published": "2023-04-06 14:12:02", "link": "http://arxiv.org/abs/2304.03087v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating the Robustness of Machine Reading Comprehension Models to Low\n  Resource Entity Renaming", "abstract": "Question answering (QA) models have shown compelling results in the task of\nMachine Reading Comprehension (MRC). Recently these systems have proved to\nperform better than humans on held-out test sets of datasets e.g. SQuAD, but\ntheir robustness is not guaranteed. The QA model's brittleness is exposed when\nevaluated on adversarial generated examples by a performance drop. In this\nstudy, we explore the robustness of MRC models to entity renaming, with\nentities from low-resource regions such as Africa. We propose EntSwap, a method\nfor test-time perturbations, to create a test set whose entities have been\nrenamed. In particular, we rename entities of type: country, person,\nnationality, location, organization, and city, to create AfriSQuAD2. Using the\nperturbed test set, we evaluate the robustness of three popular MRC models. We\nfind that compared to base models, large models perform well comparatively on\nnovel entities. Furthermore, our analysis indicates that entity type person\nhighly challenges the MRC models' performance.", "published": "2023-04-06 15:29:57", "link": "http://arxiv.org/abs/2304.03145v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bridging the Language Gap: Knowledge Injected Multilingual Question\n  Answering", "abstract": "Question Answering (QA) is the task of automatically answering questions\nposed by humans in natural languages. There are different settings to answer a\nquestion, such as abstractive, extractive, boolean, and multiple-choice QA. As\na popular topic in natural language processing tasks, extractive question\nanswering task (extractive QA) has gained extensive attention in the past few\nyears. With the continuous evolvement of the world, generalized cross-lingual\ntransfer (G-XLT), where question and answer context are in different languages,\nposes some unique challenges over cross-lingual transfer (XLT), where question\nand answer context are in the same language. With the boost of corresponding\ndevelopment of related benchmarks, many works have been done to improve the\nperformance of various language QA tasks. However, only a few works are\ndedicated to the G-XLT task. In this work, we propose a generalized\ncross-lingual transfer framework to enhance the model's ability to understand\ndifferent languages. Specifically, we first assemble triples from different\nlanguages to form multilingual knowledge. Since the lack of knowledge between\ndifferent languages greatly limits models' reasoning ability, we further design\na knowledge injection strategy via leveraging link prediction techniques to\nenrich the model storage of multilingual knowledge. In this way, we can\nprofoundly exploit rich semantic knowledge. Experiment results on real-world\ndatasets MLQA demonstrate that the proposed method can improve the performance\nby a large margin, outperforming the baseline method by 13.18%/12.00% F1/EM on\naverage.", "published": "2023-04-06 15:41:25", "link": "http://arxiv.org/abs/2304.03159v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large language models effectively leverage document-level context for\n  literary translation, but critical errors persist", "abstract": "Large language models (LLMs) are competitive with the state of the art on a\nwide range of sentence-level translation datasets. However, their ability to\ntranslate paragraphs and documents remains unexplored because evaluation in\nthese settings is costly and difficult. We show through a rigorous human\nevaluation that asking the Gpt-3.5 (text-davinci-003) LLM to translate an\nentire literary paragraph (e.g., from a novel) at once results in\nhigher-quality translations than standard sentence-by-sentence translation\nacross 18 linguistically-diverse language pairs (e.g., translating into and out\nof Japanese, Polish, and English). Our evaluation, which took approximately 350\nhours of effort for annotation and analysis, is conducted by hiring translators\nfluent in both the source and target language and asking them to provide both\nspan-level error annotations as well as preference judgments of which system's\ntranslations are better. We observe that discourse-level LLM translators commit\nfewer mistranslations, grammar errors, and stylistic inconsistencies than\nsentence-level approaches. With that said, critical errors still abound,\nincluding occasional content omissions, and a human translator's intervention\nremains necessary to ensure that the author's voice remains intact. We publicly\nrelease our dataset and error annotations to spur future research on evaluation\nof document-level literary translation.", "published": "2023-04-06 17:27:45", "link": "http://arxiv.org/abs/2304.03245v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Interpretable Mental Health Analysis with Large Language Models", "abstract": "The latest large language models (LLMs) such as ChatGPT, exhibit strong\ncapabilities in automated mental health analysis. However, existing relevant\nstudies bear several limitations, including inadequate evaluations, lack of\nprompting strategies, and ignorance of exploring LLMs for explainability. To\nbridge these gaps, we comprehensively evaluate the mental health analysis and\nemotional reasoning ability of LLMs on 11 datasets across 5 tasks. We explore\nthe effects of different prompting strategies with unsupervised and distantly\nsupervised emotional information. Based on these prompts, we explore LLMs for\ninterpretable mental health analysis by instructing them to generate\nexplanations for each of their decisions. We convey strict human evaluations to\nassess the quality of the generated explanations, leading to a novel dataset\nwith 163 human-assessed explanations. We benchmark existing automatic\nevaluation metrics on this dataset to guide future related works. According to\nthe results, ChatGPT shows strong in-context learning ability but still has a\nsignificant gap with advanced task-specific methods. Careful prompt engineering\nwith emotional cues and expert-written few-shot examples can also effectively\nimprove performance on mental health analysis. In addition, ChatGPT generates\nexplanations that approach human performance, showing its great potential in\nexplainable mental health analysis.", "published": "2023-04-06 19:53:59", "link": "http://arxiv.org/abs/2304.03347v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using LSTM and GRU With a New Dataset for Named Entity Recognition in\n  the Arabic Language", "abstract": "Named entity recognition (NER) is a natural language processing task (NLP),\nwhich aims to identify named entities and classify them like person, location,\norganization, etc. In the Arabic language, we can find a considerable size of\nunstructured data, and it needs to different preprocessing tool than languages\nlike (English, Russian, German...). From this point, we can note the importance\nof building a new structured dataset to solve the lack of structured data. In\nthis work, we use the BIOES format to tag the word, which allows us to handle\nthe nested name entity that consists of more than one sentence and define the\nstart and the end of the name. The dataset consists of more than thirty-six\nthousand records. In addition, this work proposes long short term memory (LSTM)\nunits and Gated Recurrent Units (GRU) for building the named entity recognition\nmodel in the Arabic language. The models give an approximately good result\n(80%) because LSTM and GRU models can find the relationships between the words\nof the sentence. Also, use a new library from Google, which is Trax and\nplatform Colab", "published": "2023-04-06 22:14:02", "link": "http://arxiv.org/abs/2304.03399v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Corpus-Scale Discovery of Selection Biases in News Coverage:\n  Comparing What Sources Say About Entities as a Start", "abstract": "News sources undergo the process of selecting newsworthy information when\ncovering a certain topic. The process inevitably exhibits selection biases,\ni.e. news sources' typical patterns of choosing what information to include in\nnews coverage, due to their agenda differences. To understand the magnitude and\nimplications of selection biases, one must first discover (1) on what topics do\nsources typically have diverging definitions of \"newsworthy\" information, and\n(2) do the content selection patterns correlate with certain attributes of the\nnews sources, e.g. ideological leaning, etc.\n  The goal of the paper is to investigate and discuss the challenges of\nbuilding scalable NLP systems for discovering patterns of media selection\nbiases directly from news content in massive-scale news corpora, without\nrelying on labeled data. To facilitate research in this domain, we propose and\nstudy a conceptual framework, where we compare how sources typically mention\ncertain controversial entities, and use such as indicators for the sources'\ncontent selection preferences. We empirically show the capabilities of the\nframework through a case study on NELA-2020, a corpus of 1.8M news articles in\nEnglish from 519 news sources worldwide. We demonstrate an unsupervised\nrepresentation learning method to capture the selection preferences for how\nsources typically mention controversial entities. Our experiments show that\nthat distributional divergence of such representations, when studied\ncollectively across entities and news sources, serve as good indicators for an\nindividual source's ideological leaning. We hope our findings will provide\ninsights for future research on media selection biases.", "published": "2023-04-06 23:36:45", "link": "http://arxiv.org/abs/2304.03414v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic ICD-10 Code Association: A Challenging Task on French Clinical\n  Texts", "abstract": "Automatically associating ICD codes with electronic health data is a\nwell-known NLP task in medical research. NLP has evolved significantly in\nrecent years with the emergence of pre-trained language models based on\nTransformers architecture, mainly in the English language. This paper adapts\nthese models to automatically associate the ICD codes. Several neural network\narchitectures have been experimented with to address the challenges of dealing\nwith a large set of both input tokens and labels to be guessed. In this paper,\nwe propose a model that combines the latest advances in NLP and multi-label\nclassification for ICD-10 code association. Fair experiments on a Clinical\ndataset in the French language show that our approach increases the $F_1$-score\nmetric by more than 55\\% compared to state-of-the-art results.", "published": "2023-04-06 06:31:54", "link": "http://arxiv.org/abs/2304.02886v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi-label classification of open-ended questions with BERT", "abstract": "Open-ended questions in surveys are valuable because they do not constrain\nthe respondent's answer, thereby avoiding biases. However, answers to\nopen-ended questions are text data which are harder to analyze. Traditionally,\nanswers were manually classified as specified in the coding manual. Most of the\neffort to automate coding has gone into the easier problem of single label\nprediction, where answers are classified into a single code. However, open-ends\nthat require multi-label classification, i.e., that are assigned multiple\ncodes, occur frequently. This paper focuses on multi-label classification of\ntext answers to open-ended survey questions in social science surveys. We\nevaluate the performance of the transformer-based architecture BERT for the\nGerman language in comparison to traditional multi-label algorithms (Binary\nRelevance, Label Powerset, ECC) in a German social science survey, the GLES\nPanel (N=17,584, 55 labels). We find that classification with BERT (forcing at\nleast one label) has the smallest 0/1 loss (13.1%) among methods considered\n(18.9%-21.6%). As expected, it is much easier to correctly predict answer texts\nthat correspond to a single label (7.1% loss) than those that correspond to\nmultiple labels ($\\sim$50% loss). Because BERT predicts zero labels for only\n1.5% of the answers, forcing at least one label, while recommended, ultimately\ndoes not lower the 0/1 loss by much. Our work has important implications for\nsocial scientists: 1) We have shown multi-label classification with BERT works\nin the German language for open-ends. 2) For mildly multi-label classification\ntasks, the loss now appears small enough to allow for fully automatic\nclassification (as compared to semi-automatic approaches). 3) Multi-label\nclassification with BERT requires only a single model. The leading competitor,\nECC, iterates through individual single label predictions.", "published": "2023-04-06 09:09:44", "link": "http://arxiv.org/abs/2304.02945v1", "categories": ["stat.AP", "cs.CL"], "primary_category": "stat.AP"}
{"title": "Leveraging Social Interactions to Detect Misinformation on Social Media", "abstract": "Detecting misinformation threads is crucial to guarantee a healthy\nenvironment on social media. We address the problem using the data set created\nduring the COVID-19 pandemic. It contains cascades of tweets discussing\ninformation weakly labeled as reliable or unreliable, based on a previous\nevaluation of the information source. The models identifying unreliable threads\nusually rely on textual features. But reliability is not just what is said, but\nby whom and to whom. We additionally leverage on network information. Following\nthe homophily principle, we hypothesize that users who interact are generally\ninterested in similar topics and spreading similar kind of news, which in turn\nis generally reliable or not. We test several methods to learn representations\nof the social interactions within the cascades, combining them with deep neural\nlanguage models in a Multi-Input (MI) framework. Keeping track of the sequence\nof the interactions during the time, we improve over previous state-of-the-art\nmodels.", "published": "2023-04-06 10:30:04", "link": "http://arxiv.org/abs/2304.02983v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Static Fuzzy Bag-of-Words: a lightweight sentence embedding algorithm", "abstract": "The introduction of embedding techniques has pushed forward significantly the\nNatural Language Processing field. Many of the proposed solutions have been\npresented for word-level encoding; anyhow, in the last years, new mechanism to\ntreat information at an higher level of aggregation, like at sentence- and\ndocument-level, have emerged. With this work we address specifically the\nsentence embeddings problem, presenting the Static Fuzzy Bag-of-Word model. Our\nmodel is a refinement of the Fuzzy Bag-of-Words approach, providing sentence\nembeddings with a predefined dimension. SFBoW provides competitive performances\nin Semantic Textual Similarity benchmarks, while requiring low computational\nresources.", "published": "2023-04-06 14:25:46", "link": "http://arxiv.org/abs/2304.03098v1", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Next-Item Recommendation using Large Pretrained Language\n  Models", "abstract": "Large language models (LLMs) have achieved impressive zero-shot performance\nin various natural language processing (NLP) tasks, demonstrating their\ncapabilities for inference without training examples. Despite their success, no\nresearch has yet explored the potential of LLMs to perform next-item\nrecommendations in the zero-shot setting. We have identified two major\nchallenges that must be addressed to enable LLMs to act effectively as\nrecommenders. First, the recommendation space can be extremely large for LLMs,\nand LLMs do not know about the target user's past interacted items and\npreferences. To address this gap, we propose a prompting strategy called\nZero-Shot Next-Item Recommendation (NIR) prompting that directs LLMs to make\nnext-item recommendations. Specifically, the NIR-based strategy involves using\nan external module to generate candidate items based on user-filtering or\nitem-filtering. Our strategy incorporates a 3-step prompting that guides GPT-3\nto carry subtasks that capture the user's preferences, select representative\npreviously watched movies, and recommend a ranked list of 10 movies. We\nevaluate the proposed approach using GPT-3 on MovieLens 100K dataset and show\nthat it achieves strong zero-shot performance, even outperforming some strong\nsequential recommendation models trained on the entire training dataset. These\npromising results highlight the ample research opportunities to use LLMs as\nrecommenders. The code can be found at\nhttps://github.com/AGI-Edgerunners/LLM-Next-Item-Rec.", "published": "2023-04-06 15:35:11", "link": "http://arxiv.org/abs/2304.03153v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Cerebras-GPT: Open Compute-Optimal Language Models Trained on the\n  Cerebras Wafer-Scale Cluster", "abstract": "We study recent research advances that improve large language models through\nefficient pre-training and scaling, and open datasets and tools. We combine\nthese advances to introduce Cerebras-GPT, a family of open compute-optimal\nlanguage models scaled from 111M to 13B parameters. We train Cerebras-GPT\nmodels on the Eleuther Pile dataset following DeepMind Chinchilla scaling rules\nfor efficient pre-training (highest accuracy for a given compute budget). We\ncharacterize the predictable power-law scaling and compare Cerebras-GPT with\nother publicly-available models to show all Cerebras-GPT models have\nstate-of-the-art training efficiency on both pre-training and downstream\nobjectives. We describe our learnings including how Maximal Update\nParameterization ($\\mu$P) can further improve large model scaling, improving\naccuracy and hyperparameter predictability at scale. We release our pre-trained\nmodels and code, making this paper the first open and reproducible work\ncomparing compute-optimal model scaling to models trained on fixed dataset\nsizes. Cerebras-GPT models are available on HuggingFace:\nhttps://huggingface.co/cerebras.", "published": "2023-04-06 16:43:16", "link": "http://arxiv.org/abs/2304.03208v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "On the Pareto Front of Multilingual Neural Machine Translation", "abstract": "In this work, we study how the performance of a given direction changes with\nits sampling ratio in Multilingual Neural Machine Translation (MNMT). By\ntraining over 200 multilingual models with various model sizes, data sizes, and\nlanguage directions, we find it interesting that the performance of certain\ntranslation direction does not always improve with the increase of its weight\nin the multi-task optimization objective. Accordingly, scalarization method\nleads to a multitask trade-off front that deviates from the traditional Pareto\nfront when there exists data imbalance in the training corpus, which poses a\ngreat challenge to improve the overall performance of all directions. Based on\nour observations, we propose the Double Power Law to predict the unique\nperformance trade-off front in MNMT, which is robust across various languages,\ndata adequacy, and the number of tasks. Finally, we formulate the sample ratio\nselection problem in MNMT as an optimization problem based on the Double Power\nLaw. In our experiments, it achieves better performance than temperature\nsearching and gradient manipulation methods with only 1/5 to 1/2 of the total\ntraining budget. We release the code at\nhttps://github.com/pkunlp-icler/ParetoMNMT for reproduction.", "published": "2023-04-06 16:49:19", "link": "http://arxiv.org/abs/2304.03216v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Instruction Tuning with GPT-4", "abstract": "Prior work has shown that finetuning large language models (LLMs) using\nmachine-generated instruction-following data enables such models to achieve\nremarkable zero-shot capabilities on new tasks, and no human-written\ninstructions are needed. In this paper, we present the first attempt to use\nGPT-4 to generate instruction-following data for LLM finetuning. Our early\nexperiments on instruction-tuned LLaMA models show that the 52K English and\nChinese instruction-following data generated by GPT-4 leads to superior\nzero-shot performance on new tasks to the instruction-following data generated\nby previous state-of-the-art models. We also collect feedback and comparison\ndata from GPT-4 to enable a comprehensive evaluation and reward model training.\nWe make our data generated using GPT-4 as well as our codebase publicly\navailable.", "published": "2023-04-06 17:58:09", "link": "http://arxiv.org/abs/2304.03277v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Deep Learning for Opinion Mining and Topic Classification of Course\n  Reviews", "abstract": "Student opinions for a course are important to educators and administrators,\nregardless of the type of the course or the institution. Reading and manually\nanalyzing open-ended feedback becomes infeasible for massive volumes of\ncomments at institution level or online forums. In this paper, we collected and\npre-processed a large number of course reviews publicly available online. We\napplied machine learning techniques with the goal to gain insight into student\nsentiments and topics. Specifically, we utilized current Natural Language\nProcessing (NLP) techniques, such as word embeddings and deep neural networks,\nand state-of-the-art BERT (Bidirectional Encoder Representations from\nTransformers), RoBERTa (Robustly optimized BERT approach) and XLNet\n(Generalized Auto-regression Pre-training). We performed extensive\nexperimentation to compare these techniques versus traditional approaches. This\ncomparative study demonstrates how to apply modern machine learning approaches\nfor sentiment polarity extraction and topic-based classification utilizing\ncourse feedback. For sentiment polarity, the top model was RoBERTa with 95.5%\naccuracy and 84.7% F1-macro, while for topic classification, an SVM (Support\nVector Machine) was the top classifier with 79.8% accuracy and 80.6% F1-macro.\nWe also provided an in-depth exploration of the effect of certain\nhyperparameters on the model performance and discussed our observations. These\nfindings can be used by institutions and course providers as a guide for\nanalyzing their own course feedback using NLP models towards self-evaluation\nand improvement.", "published": "2023-04-06 21:48:29", "link": "http://arxiv.org/abs/2304.03394v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "GPT detectors are biased against non-native English writers", "abstract": "The rapid adoption of generative language models has brought about\nsubstantial advancements in digital communication, while simultaneously raising\nconcerns regarding the potential misuse of AI-generated content. Although\nnumerous detection methods have been proposed to differentiate between AI and\nhuman-generated content, the fairness and robustness of these detectors remain\nunderexplored. In this study, we evaluate the performance of several\nwidely-used GPT detectors using writing samples from native and non-native\nEnglish writers. Our findings reveal that these detectors consistently\nmisclassify non-native English writing samples as AI-generated, whereas native\nwriting samples are accurately identified. Furthermore, we demonstrate that\nsimple prompting strategies can not only mitigate this bias but also\neffectively bypass GPT detectors, suggesting that GPT detectors may\nunintentionally penalize writers with constrained linguistic expressions. Our\nresults call for a broader conversation about the ethical implications of\ndeploying ChatGPT content detectors and caution against their use in evaluative\nor educational settings, particularly when they may inadvertently penalize or\nexclude non-native English speakers from the global discourse. The published\nversion of this study can be accessed at:\nwww.cell.com/patterns/fulltext/S2666-3899(23)00130-7", "published": "2023-04-06 01:51:15", "link": "http://arxiv.org/abs/2304.02819v3", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Approach Intelligent Writing Assistants Usability with Seven Stages of\n  Action", "abstract": "Despite the potential of Large Language Models (LLMs) as writing assistants,\nthey are plagued by issues like coherence and fluency of the model output,\ntrustworthiness, ownership of the generated content, and predictability of\nmodel performance, thereby limiting their usability. In this position paper, we\npropose to adopt Norman's seven stages of action as a framework to approach the\ninteraction design of intelligent writing assistants. We illustrate the\nframework's applicability to writing tasks by providing an example of software\ntutorial authoring. The paper also discusses the framework as a tool to\nsynthesize research on the interaction design of LLM-based tools and presents\nexamples of tools that support the stages of action. Finally, we briefly\noutline the potential of a framework for human-LLM interaction research.", "published": "2023-04-06 02:11:55", "link": "http://arxiv.org/abs/2304.02822v1", "categories": ["cs.HC", "cs.CL", "cs.LG"], "primary_category": "cs.HC"}
{"title": "Can Large Language Models Play Text Games Well? Current State-of-the-Art\n  and Open Questions", "abstract": "Large language models (LLMs) such as ChatGPT and GPT-4 have recently\ndemonstrated their remarkable abilities of communicating with human users. In\nthis technical report, we take an initiative to investigate their capacities of\nplaying text games, in which a player has to understand the environment and\nrespond to situations by having dialogues with the game world. Our experiments\nshow that ChatGPT performs competitively compared to all the existing systems\nbut still exhibits a low level of intelligence. Precisely, ChatGPT can not\nconstruct the world model by playing the game or even reading the game manual;\nit may fail to leverage the world knowledge that it already has; it cannot\ninfer the goal of each step as the game progresses. Our results open up new\nresearch questions at the intersection of artificial intelligence, machine\nlearning, and natural language processing.", "published": "2023-04-06 05:01:28", "link": "http://arxiv.org/abs/2304.02868v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Natural Language Robot Programming: NLP integrated with autonomous\n  robotic grasping", "abstract": "In this paper, we present a grammar-based natural language framework for\nrobot programming, specifically for pick-and-place tasks. Our approach uses a\ncustom dictionary of action words, designed to store together words that share\nmeaning, allowing for easy expansion of the vocabulary by adding more action\nwords from a lexical database. We validate our Natural Language Robot\nProgramming (NLRP) framework through simulation and real-world experimentation,\nusing a Franka Panda robotic arm equipped with a calibrated camera-in-hand and\na microphone. Participants were asked to complete a pick-and-place task using\nverbal commands, which were converted into text using Google's Speech-to-Text\nAPI and processed through the NLRP framework to obtain joint space trajectories\nfor the robot. Our results indicate that our approach has a high system\nusability score. The framework's dictionary can be easily extended without\nrelying on transfer learning or large data sets. In the future, we plan to\ncompare the presented framework with different approaches of human-assisted\npick-and-place tasks via a comprehensive user study.", "published": "2023-04-06 11:06:30", "link": "http://arxiv.org/abs/2304.02993v1", "categories": ["cs.RO", "cs.CL", "cs.HC"], "primary_category": "cs.RO"}
{"title": "Compression of enumerations and gain", "abstract": "We study the compressibility of enumerations, and its role in the relative\nKolmogorov complexity of computably enumerable sets, with respect to density.\nWith respect to a strong and a weak form of compression, we examine the gain:\nthe amount of auxiliary information embedded in the compressed enumeration.\nStrong compression and weak gainless compression is shown for any computably\nenumerable set, and a positional game is studied toward understanding strong\ngainless compression.", "published": "2023-04-06 12:29:53", "link": "http://arxiv.org/abs/2304.03030v1", "categories": ["cs.CL", "cs.IT", "math.IT", "math.LO"], "primary_category": "cs.CL"}
{"title": "ETPNav: Evolving Topological Planning for Vision-Language Navigation in\n  Continuous Environments", "abstract": "Vision-language navigation is a task that requires an agent to follow\ninstructions to navigate in environments. It becomes increasingly crucial in\nthe field of embodied AI, with potential applications in autonomous navigation,\nsearch and rescue, and human-robot interaction. In this paper, we propose to\naddress a more practical yet challenging counterpart setting - vision-language\nnavigation in continuous environments (VLN-CE). To develop a robust VLN-CE\nagent, we propose a new navigation framework, ETPNav, which focuses on two\ncritical skills: 1) the capability to abstract environments and generate\nlong-range navigation plans, and 2) the ability of obstacle-avoiding control in\ncontinuous environments. ETPNav performs online topological mapping of\nenvironments by self-organizing predicted waypoints along a traversed path,\nwithout prior environmental experience. It privileges the agent to break down\nthe navigation procedure into high-level planning and low-level control.\nConcurrently, ETPNav utilizes a transformer-based cross-modal planner to\ngenerate navigation plans based on topological maps and instructions. The plan\nis then performed through an obstacle-avoiding controller that leverages a\ntrial-and-error heuristic to prevent navigation from getting stuck in\nobstacles. Experimental results demonstrate the effectiveness of the proposed\nmethod. ETPNav yields more than 10% and 20% improvements over prior\nstate-of-the-art on R2R-CE and RxR-CE datasets, respectively. Our code is\navailable at https://github.com/MarSaKi/ETPNav.", "published": "2023-04-06 13:07:17", "link": "http://arxiv.org/abs/2304.03047v3", "categories": ["cs.CV", "cs.CL", "cs.RO"], "primary_category": "cs.CV"}
{"title": "Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards\n  and Ethical Behavior in the MACHIAVELLI Benchmark", "abstract": "Artificial agents have traditionally been trained to maximize reward, which\nmay incentivize power-seeking and deception, analogous to how next-token\nprediction in language models (LMs) may incentivize toxicity. So do agents\nnaturally learn to be Machiavellian? And how do we measure these behaviors in\ngeneral-purpose models such as GPT-4? Towards answering these questions, we\nintroduce MACHIAVELLI, a benchmark of 134 Choose-Your-Own-Adventure games\ncontaining over half a million rich, diverse scenarios that center on social\ndecision-making. Scenario labeling is automated with LMs, which are more\nperformant than human annotators. We mathematize dozens of harmful behaviors\nand use our annotations to evaluate agents' tendencies to be power-seeking,\ncause disutility, and commit ethical violations. We observe some tension\nbetween maximizing reward and behaving ethically. To improve this trade-off, we\ninvestigate LM-based methods to steer agents' towards less harmful behaviors.\nOur results show that agents can both act competently and morally, so concrete\nprogress can currently be made in machine ethics--designing agents that are\nPareto improvements in both safety and capabilities.", "published": "2023-04-06 17:59:03", "link": "http://arxiv.org/abs/2304.03279v4", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.LG"}
{"title": "ChatGPT-Crawler: Find out if ChatGPT really knows what it's talking\n  about", "abstract": "Large language models have gained considerable interest for their impressive\nperformance on various tasks. Among these models, ChatGPT developed by OpenAI\nhas become extremely popular among early adopters who even regard it as a\ndisruptive technology in many fields like customer service, education,\nhealthcare, and finance. It is essential to comprehend the opinions of these\ninitial users as it can provide valuable insights into the potential strengths,\nweaknesses, and success or failure of the technology in different areas. This\nresearch examines the responses generated by ChatGPT from different\nConversational QA corpora. The study employed BERT similarity scores to compare\nthese responses with correct answers and obtain Natural Language Inference(NLI)\nlabels. Evaluation scores were also computed and compared to determine the\noverall performance of GPT-3 \\& GPT-4. Additionally, the study identified\ninstances where ChatGPT provided incorrect answers to questions, providing\ninsights into areas where the model may be prone to error.", "published": "2023-04-06 18:42:47", "link": "http://arxiv.org/abs/2304.03325v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "I.7, I.m"], "primary_category": "cs.CL"}
{"title": "Noise-Robust Dense Retrieval via Contrastive Alignment Post Training", "abstract": "The success of contextual word representations and advances in neural\ninformation retrieval have made dense vector-based retrieval a standard\napproach for passage and document ranking. While effective and efficient,\ndual-encoders are brittle to variations in query distributions and noisy\nqueries. Data augmentation can make models more robust but introduces overhead\nto training set generation and requires retraining and index regeneration. We\npresent Contrastive Alignment POst Training (CAPOT), a highly efficient\nfinetuning method that improves model robustness without requiring index\nregeneration, the training set optimization, or alteration. CAPOT enables\nrobust retrieval by freezing the document encoder while the query encoder\nlearns to align noisy queries with their unaltered root. We evaluate CAPOT\nnoisy variants of MSMARCO, Natural Questions, and Trivia QA passage retrieval,\nfinding CAPOT has a similar impact as data augmentation with none of its\noverhead.", "published": "2023-04-06 22:16:53", "link": "http://arxiv.org/abs/2304.03401v2", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Those Aren't Your Memories, They're Somebody Else's: Seeding\n  Misinformation in Chat Bot Memories", "abstract": "One of the new developments in chit-chat bots is a long-term memory mechanism\nthat remembers information from past conversations for increasing engagement\nand consistency of responses. The bot is designed to extract knowledge of\npersonal nature from their conversation partner, e.g., stating preference for a\nparticular color. In this paper, we show that this memory mechanism can result\nin unintended behavior. In particular, we found that one can combine a personal\nstatement with an informative statement that would lead the bot to remember the\ninformative statement alongside personal knowledge in its long term memory.\nThis means that the bot can be tricked into remembering misinformation which it\nwould regurgitate as statements of fact when recalling information relevant to\nthe topic of conversation. We demonstrate this vulnerability on the BlenderBot\n2 framework implemented on the ParlAI platform and provide examples on the more\nrecent and significantly larger BlenderBot 3 model. We generate 150 examples of\nmisinformation, of which 114 (76%) were remembered by BlenderBot 2 when\ncombined with a personal statement. We further assessed the risk of this\nmisinformation being recalled after intervening innocuous conversation and in\nresponse to multiple questions relevant to the injected memory. Our evaluation\nwas performed on both the memory-only and the combination of memory and\ninternet search modes of BlenderBot 2. From the combinations of these\nvariables, we generated 12,890 conversations and analyzed recalled\nmisinformation in the responses. We found that when the chat bot is questioned\non the misinformation topic, it was 328% more likely to respond with the\nmisinformation as fact when the misinformation was in the long-term memory.", "published": "2023-04-06 05:09:39", "link": "http://arxiv.org/abs/2304.05371v1", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Efficient Audio Captioning Transformer with Patchout and Text Guidance", "abstract": "Automated audio captioning is multi-modal translation task that aim to\ngenerate textual descriptions for a given audio clip. In this paper we propose\na full Transformer architecture that utilizes Patchout as proposed in [1],\nsignificantly reducing the computational complexity and avoiding overfitting.\nThe caption generation is partly conditioned on textual AudioSet tags extracted\nby a pre-trained classification model which is fine-tuned to maximize the\nsemantic similarity between AudioSet labels and ground truth captions. To\nmitigate the data scarcity problem of Automated Audio Captioning we introduce\ntransfer learning from an upstream audio-related task and an enlarged in-domain\ndataset. Moreover, we propose a method to apply Mixup augmentation for AAC.\nAblation studies are carried out to investigate how Patchout and text guidance\ncontribute to the final performance. The results show that the proposed\ntechniques improve the performance of our system and while reducing the\ncomputational complexity. Our proposed method received the Judges Award at the\nTask6A of DCASE Challenge 2022.", "published": "2023-04-06 07:58:27", "link": "http://arxiv.org/abs/2304.02916v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "F.2.2; I.2.7"], "primary_category": "cs.SD"}
{"title": "Automatic Detection of Reactions to Music via Earable Sensing", "abstract": "We present GrooveMeter, a novel system that automatically detects vocal and\nmotion reactions to music via earable sensing and supports music\nengagement-aware applications. To this end, we use smart earbuds as sensing\ndevices, which are already widely used for music listening, and devise reaction\ndetection techniques by leveraging an inertial measurement unit (IMU) and a\nmicrophone on earbuds. To explore reactions in daily music-listening\nsituations, we collect the first kind of dataset, MusicReactionSet, containing\n926-minute-long IMU and audio data with 30 participants. With the dataset, we\ndiscover a set of unique challenges in detecting music listening reactions\naccurately and robustly using audio and motion sensing. We devise sophisticated\nprocessing pipelines to make reaction detection accurate and efficient. We\npresent a comprehensive evaluation to examine the performance of reaction\ndetection and system cost. It shows that GrooveMeter achieves the macro F1\nscores of 0.89 for vocal reaction and 0.81 for motion reaction with\nleave-one-subject-out cross-validation. More importantly, GrooveMeter shows\nhigher accuracy and robustness compared to alternative methods. We also show\nthat our filtering approach reduces 50% or more of the energy overhead.\nFinally, we demonstrate the potential use cases through a case study.", "published": "2023-04-06 08:11:03", "link": "http://arxiv.org/abs/2304.03295v1", "categories": ["cs.SD", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DSVAE: Interpretable Disentangled Representation for Synthetic Speech\n  Detection", "abstract": "Tools to generate high quality synthetic speech signal that is perceptually\nindistinguishable from speech recorded from human speakers are easily\navailable. Several approaches have been proposed for detecting synthetic\nspeech. Many of these approaches use deep learning methods as a black box\nwithout providing reasoning for the decisions they make. This limits the\ninterpretability of these approaches. In this paper, we propose Disentangled\nSpectrogram Variational Auto Encoder (DSVAE) which is a two staged trained\nvariational autoencoder that processes spectrograms of speech using\ndisentangled representation learning to generate interpretable representations\nof a speech signal for detecting synthetic speech. DSVAE also creates an\nactivation map to highlight the spectrogram regions that discriminate synthetic\nand bona fide human speech signals. We evaluated the representations obtained\nfrom DSVAE using the ASVspoof2019 dataset. Our experimental results show high\naccuracy (>98%) on detecting synthetic speech from 6 known and 10 out of 11\nunknown speech synthesizers. We also visualize the representation obtained from\nDSVAE for 17 different speech synthesizers and verify that they are indeed\ninterpretable and discriminate bona fide and synthetic speech from each of the\nsynthesizers.", "published": "2023-04-06 18:37:26", "link": "http://arxiv.org/abs/2304.03323v2", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Adoption of AI Technology in the Music Mixing Workflow: An Investigation", "abstract": "The integration of artificial intelligence (AI) technology in the music\nindustry is driving a significant change in the way music is being composed,\nproduced and mixed. This study investigates the current state of AI in the\nmixing workflows and its adoption by different user groups. Through\nsemi-structured interviews, a questionnaire-based study, and analyzing web\nforums, the study confirms three user groups comprising amateurs, pro-ams, and\nprofessionals. Our findings show that while AI mixing tools can simplify the\nprocess and provide decent results for amateurs, pro-ams seek precise control\nand customization options, while professionals desire control and customization\noptions in addition to assistive and collaborative technologies. The study\nprovides strategies for designing effective AI mixing tools for different user\ngroups and outlines future directions.", "published": "2023-04-06 22:47:59", "link": "http://arxiv.org/abs/2304.03407v2", "categories": ["cs.HC", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
{"title": "To Wake-up or Not to Wake-up: Reducing Keyword False Alarm by Successive\n  Refinement", "abstract": "Keyword spotting systems continuously process audio streams to detect\nkeywords. One of the most challenging tasks in designing such systems is to\nreduce False Alarm (FA) which happens when the system falsely registers a\nkeyword despite the keyword not being uttered. In this paper, we propose a\nsimple yet elegant solution to this problem that follows from the law of total\nprobability. We show that existing deep keyword spotting mechanisms can be\nimproved by Successive Refinement, where the system first classifies whether\nthe input audio is speech or not, followed by whether the input is keyword-like\nor not, and finally classifies which keyword was uttered. We show across\nmultiple models with size ranging from 13K parameters to 2.41M parameters, the\nsuccessive refinement technique reduces FA by up to a factor of 8 on in-domain\nheld-out FA data, and up to a factor of 7 on out-of-domain (OOD) FA data.\nFurther, our proposed approach is \"plug-and-play\" and can be applied to any\ndeep keyword spotting model.", "published": "2023-04-06 23:49:29", "link": "http://arxiv.org/abs/2304.03416v1", "categories": ["eess.SP", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
