{"title": "Improving Interpretability of Word Embeddings by Generating Definition\n  and Usage", "abstract": "Word embeddings are substantially successful in capturing semantic relations\namong words. However, these lexical semantics are difficult to be interpreted.\nDefinition modeling provides a more intuitive way to evaluate embeddings by\nutilizing them to generate natural language definitions of corresponding words.\nThis task is of great significance for practical application and in-depth\nunderstanding of word representations. We propose a novel framework for\ndefinition modeling, which can generate reasonable and understandable\ncontext-dependent definitions. Moreover, we introduce usage modeling and study\nwhether it is possible to utilize embeddings to generate example sentences of\nwords. These ways are a more direct and explicit expression of embedding's\nsemantics for better interpretability. We extend the single task model to\nmulti-task setting and investigate several joint multi-task models to combine\nusage modeling and definition modeling together. Experimental results on\nexisting Oxford dataset and a new collected Oxford-2019 dataset show that our\nsingle-task model achieves the state-of-the-art result in definition modeling\nand the multi-task learning methods are helpful for two tasks to improve the\nperformance.", "published": "2019-12-12 12:45:34", "link": "http://arxiv.org/abs/1912.05898v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Encoding Knowledge Graph Entity Aliases in Attentive Neural Network for\n  Wikidata Entity Linking", "abstract": "The collaborative knowledge graphs such as Wikidata excessively rely on the\ncrowd to author the information. Since the crowd is not bound to a standard\nprotocol for assigning entity titles, the knowledge graph is populated by\nnon-standard, noisy, long or even sometimes awkward titles. The issue of long,\nimplicit, and nonstandard entity representations is a challenge in Entity\nLinking (EL) approaches for gaining high precision and recall. Underlying KG,\nin general, is the source of target entities for EL approaches, however, it\noften contains other relevant information, such as aliases of entities (e.g.,\nObama and Barack Hussein Obama are aliases for the entity Barack Obama). EL\nmodels usually ignore such readily available entity attributes. In this paper,\nwe examine the role of knowledge graph context on an attentive neural network\napproach for entity linking on Wikidata. Our approach contributes by exploiting\nthe sufficient context from a KG as a source of background knowledge, which is\nthen fed into the neural network. This approach demonstrates merit to address\nchallenges associated with entity titles (multi-word, long, implicit,\ncase-sensitive). Our experimental study shows approx 8% improvements over the\nbaseline approach, and significantly outperform an end to end approach for\nWikidata entity linking.", "published": "2019-12-12 21:11:56", "link": "http://arxiv.org/abs/1912.06214v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AliMe KBQA: Question Answering over Structured Knowledge for E-commerce\n  Customer Service", "abstract": "With the rise of knowledge graph (KG), question answering over knowledge base\n(KBQA) has attracted increasing attention in recent years. Despite much\nresearch has been conducted on this topic, it is still challenging to apply\nKBQA technology in industry because business knowledge and real-world questions\ncan be rather complicated. In this paper, we present AliMe-KBQA, a bold attempt\nto apply KBQA in the E-commerce customer service field. To handle real\nknowledge and questions, we extend the classic \"subject-predicate-object (SPO)\"\nstructure with property hierarchy, key-value structure and compound value type\n(CVT), and enhance traditional KBQA with constraints recognition and reasoning\nability. We launch AliMe-KBQA in the Marketing Promotion scenario for merchants\nduring the \"Double 11\" period in 2018 and other such promotional events\nafterwards. Online results suggest that AliMe-KBQA is not only able to gain\nbetter resolution and improve customer satisfaction, but also becomes the\npreferred knowledge management method by business knowledge staffs since it\noffers a more convenient and efficient management experience.", "published": "2019-12-12 02:04:44", "link": "http://arxiv.org/abs/1912.05728v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Extending Machine Language Models toward Human-Level Language\n  Understanding", "abstract": "Language is crucial for human intelligence, but what exactly is its role? We\ntake language to be a part of a system for understanding and communicating\nabout situations. The human ability to understand and communicate about\nsituations emerges gradually from experience and depends on domain-general\nprinciples of biological neural networks: connection-based learning,\ndistributed representation, and context-sensitive, mutual constraint\nsatisfaction-based processing. Current artificial language processing systems\nrely on the same domain general principles, embodied in artificial neural\nnetworks. Indeed, recent progress in this field depends on \\emph{query-based\nattention}, which extends the ability of these systems to exploit context and\nhas contributed to remarkable breakthroughs. Nevertheless, most current models\nfocus exclusively on language-internal tasks, limiting their ability to perform\ntasks that depend on understanding situations. These systems also lack memory\nfor the contents of prior situations outside of a fixed contextual span. We\ndescribe the organization of the brain's distributed understanding system,\nwhich includes a fast learning system that addresses the memory problem. We\nsketch a framework for future models of understanding drawing equally on\ncognitive neuroscience and artificial intelligence and exploiting query-based\nattention. We highlight relevant current directions and consider further\ndevelopments needed to fully capture human-level language understanding in a\ncomputational system.", "published": "2019-12-12 11:02:30", "link": "http://arxiv.org/abs/1912.05877v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Text as Environment: A Deep Reinforcement Learning Text Readability\n  Assessment Model", "abstract": "Evaluating the readability of a text can significantly facilitate the precise\nexpression of information in written form. The formulation of text readability\nassessment involves the identification of meaningful properties of the text\nregardless of its length. Sophisticated features and models are used to\nevaluate the comprehensibility of texts accurately. Despite this, the problem\nof assessing texts' readability efficiently remains relatively untouched. The\nefficiency of state-of-the-art text readability assessment models can be\nfurther improved using deep reinforcement learning models. Using a hard\nattention-based active inference technique, the proposed approach makes\nefficient use of input text and computational resources. Through the use of\nsemi-supervised signals, the reinforcement learning model uses the minimum\namount of text in order to determine text's readability. A comparison of the\nmodel on Weebit and Cambridge Exams with state-of-the-art models, such as the\nBERT text readability model, shows that it is capable of achieving\nstate-of-the-art accuracy with a significantly smaller amount of input text\nthan other models.", "published": "2019-12-12 13:54:09", "link": "http://arxiv.org/abs/1912.05957v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Shaping representations through communication: community size effect in\n  artificial learning systems", "abstract": "Motivated by theories of language and communication that explain why\ncommunities with large numbers of speakers have, on average, simpler languages\nwith more regularity, we cast the representation learning problem in terms of\nlearning to communicate. Our starting point sees the traditional autoencoder\nsetup as a single encoder with a fixed decoder partner that must learn to\ncommunicate. Generalizing from there, we introduce community-based autoencoders\nin which multiple encoders and decoders collectively learn representations by\nbeing randomly paired up on successive training iterations. We find that\nincreasing community sizes reduce idiosyncrasies in the learned codes,\nresulting in representations that better encode concept categories and\ncorrelate with human feature norms.", "published": "2019-12-12 20:56:59", "link": "http://arxiv.org/abs/1912.06208v1", "categories": ["cs.CL", "cs.NE"], "primary_category": "cs.CL"}
{"title": "The Benefits of Close-Domain Fine-Tuning for Table Detection in Document\n  Images", "abstract": "A correct localisation of tables in a document is instrumental for\ndetermining their structure and extracting their contents; therefore, table\ndetection is a key step in table understanding. Nowadays, the most successful\nmethods for table detection in document images employ deep learning algorithms;\nand, particularly, a technique known as fine-tuning. In this context, such a\ntechnique exports the knowledge acquired to detect objects in natural images to\ndetect tables in document images. However, there is only a vague relation\nbetween natural and document images, and fine-tuning works better when there is\na close relation between the source and target task. In this paper, we show\nthat it is more beneficial to employ fine-tuning from a closer domain. To this\naim, we train different object detection algorithms (namely, Mask R-CNN,\nRetinaNet, SSD and YOLO) using the TableBank dataset (a dataset of images of\nacademic documents designed for table detection and recognition), and fine-tune\nthem for several heterogeneous table detection datasets. Using this approach,\nwe considerably improve the accuracy of the detection models fine-tuned from\nnatural images (in mean a 17%, and, in the best case, up to a 60%).", "published": "2019-12-12 09:30:02", "link": "http://arxiv.org/abs/1912.05846v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Singing Synthesis: with a little help from my attention", "abstract": "We present UTACO, a singing synthesis model based on an attention-based\nsequence-to-sequence mechanism and a vocoder based on dilated causal\nconvolutions. These two classes of models have significantly affected the field\nof text-to-speech, but have never been thoroughly applied to the task of\nsinging synthesis. UTACO demonstrates that attention can be successfully\napplied to the singing synthesis field and improves naturalness over the state\nof the art. The system requires considerably less explicit modelling of voice\nfeatures such as F0 patterns, vibratos, and note and phoneme durations, than\nprevious models in the literature. Despite this, it shows a strong improvement\nin naturalness with respect to previous neural singing synthesis models. The\nmodel does not require any durations or pitch patterns as inputs, and learns to\ninsert vibrato autonomously according to the musical context. However, we\nobserve that, by completely dispensing with any explicit duration modelling it\nbecomes harder to obtain the fine control of timing needed to exactly match the\ntempo of a song.", "published": "2019-12-12 11:17:30", "link": "http://arxiv.org/abs/1912.05881v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Training without training data: Improving the generalizability of\n  automated medical abbreviation disambiguation", "abstract": "Abbreviation disambiguation is important for automated clinical note\nprocessing due to the frequent use of abbreviations in clinical settings.\nCurrent models for automated abbreviation disambiguation are restricted by the\nscarcity and imbalance of labeled training data, decreasing their\ngeneralizability to orthogonal sources. In this work we propose a novel data\naugmentation technique that utilizes information from related medical concepts,\nwhich improves our model's ability to generalize. Furthermore, we show that\nincorporating the global context information within the whole medical note (in\naddition to the traditional local context window), can significantly improve\nthe model's representation for abbreviations. We train our model on a public\ndataset (MIMIC III) and test its performance on datasets from different sources\n(CASI, i2b2). Together, these two techniques boost the accuracy of abbreviation\ndisambiguation by almost 14% on the CASI dataset and 4% on i2b2.", "published": "2019-12-12 19:32:41", "link": "http://arxiv.org/abs/1912.06174v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "ManiGAN: Text-Guided Image Manipulation", "abstract": "The goal of our paper is to semantically edit parts of an image matching a\ngiven text that describes desired attributes (e.g., texture, colour, and\nbackground), while preserving other contents that are irrelevant to the text.\nTo achieve this, we propose a novel generative adversarial network (ManiGAN),\nwhich contains two key components: text-image affine combination module (ACM)\nand detail correction module (DCM). The ACM selects image regions relevant to\nthe given text and then correlates the regions with corresponding semantic\nwords for effective manipulation. Meanwhile, it encodes original image features\nto help reconstruct text-irrelevant contents. The DCM rectifies mismatched\nattributes and completes missing contents of the synthetic image. Finally, we\nsuggest a new metric for evaluating image manipulation results, in terms of\nboth the generation of new attributes and the reconstruction of text-irrelevant\ncontents. Extensive experiments on the CUB and COCO datasets demonstrate the\nsuperior performance of the proposed method. Code is available at\nhttps://github.com/mrlibw/ManiGAN.", "published": "2019-12-12 20:48:52", "link": "http://arxiv.org/abs/1912.06203v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Extracting clinical concepts from user queries", "abstract": "Clinical concept extraction often begins with clinical Named Entity\nRecognition (NER). Often trained on annotated clinical notes, clinical NER\nmodels tend to struggle with tagging clinical entities in user queries because\nof the structural differences between clinical notes and user queries. User\nqueries, unlike clinical notes, are often ungrammatical and incoherent. In many\ncases, user queries are compounded of multiple clinical entities, without comma\nor conjunction words separating them. By using as dataset a mixture of\nannotated clinical notes and synthesized user queries, we adapt a clinical NER\nmodel based on the BiLSTM-CRF architecture for tagging clinical entities in\nuser queries. Our contribution are the following: 1) We found that when trained\non a mixture of synthesized user queries and clinical notes, the NER model\nperforms better on both user queries and clinical notes. 2) We provide an\nend-to-end and easy-to-implement framework for clinical concept extraction from\nuser queries.", "published": "2019-12-12 23:18:16", "link": "http://arxiv.org/abs/1912.06262v2", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Two Way Adversarial Unsupervised Word Translation", "abstract": "Word translation is a problem in machine translation that seeks to build\nmodels that recover word level correspondence between languages. Recent\napproaches to this problem have shown that word translation models can learned\nwith very small seeding dictionaries, and even without any starting\nsupervision. In this paper we propose a method to jointly find translations\nbetween a pair of languages. Not only does our method learn translations in\nboth directions but it improves accuracy of those translations over past\nmethods.", "published": "2019-12-12 21:21:45", "link": "http://arxiv.org/abs/1912.10168v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Speech-driven facial animation using polynomial fusion of features", "abstract": "Speech-driven facial animation involves using a speech signal to generate\nrealistic videos of talking faces. Recent deep learning approaches to facial\nsynthesis rely on extracting low-dimensional representations and concatenating\nthem, followed by a decoding step of the concatenated vector. This accounts for\nonly first-order interactions of the features and ignores higher-order\ninteractions. In this paper we propose a polynomial fusion layer that models\nthe joint representation of the encodings by a higher-order polynomial, with\nthe parameters modelled by a tensor decomposition. We demonstrate the\nsuitability of this approach through experiments on generated videos evaluated\non a range of metrics on video quality, audiovisual synchronisation and\ngeneration of blinks.", "published": "2019-12-12 08:46:57", "link": "http://arxiv.org/abs/1912.05833v2", "categories": ["cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "On Neural Phone Recognition of Mixed-Source ECoG Signals", "abstract": "The emerging field of neural speech recognition (NSR) using\nelectrocorticography has recently attracted remarkable research interest for\nstudying how human brains recognize speech in quiet and noisy surroundings. In\nthis study, we demonstrate the utility of NSR systems to objectively prove the\nability of human beings to attend to a single speech source while suppressing\nthe interfering signals in a simulated cocktail party scenario. The\nexperimental results show that the relative degradation of the NSR system\nperformance when tested in a mixed-source scenario is significantly lower than\nthat of automatic speech recognition (ASR). In this paper, we have\nsignificantly enhanced the performance of our recently published framework by\nusing manual alignments for initialization instead of the flat start technique.\nWe have also improved the NSR system performance by accounting for the possible\ntranscription mismatch between the acoustic and neural signals.", "published": "2019-12-12 10:37:22", "link": "http://arxiv.org/abs/1912.05869v1", "categories": ["eess.AS", "cs.NE", "cs.SD", "q-bio.NC"], "primary_category": "eess.AS"}
