{"title": "Mittens: An Extension of GloVe for Learning Domain-Specialized\n  Representations", "abstract": "We present a simple extension of the GloVe representation learning model that\nbegins with general-purpose representations and updates them based on data from\na specialized domain. We show that the resulting representations can lead to\nfaster learning and better results on a variety of tasks.", "published": "2018-03-27 05:23:01", "link": "http://arxiv.org/abs/1803.09901v1", "categories": ["cs.CL", "68T50"], "primary_category": "cs.CL"}
{"title": "Deep Communicating Agents for Abstractive Summarization", "abstract": "We present deep communicating agents in an encoder-decoder architecture to\naddress the challenges of representing a long document for abstractive\nsummarization. With deep communicating agents, the task of encoding a long text\nis divided across multiple collaborating agents, each in charge of a subsection\nof the input text. These encoders are connected to a single decoder, trained\nend-to-end using reinforcement learning to generate a focused and coherent\nsummary. Empirical results demonstrate that multiple communicating encoders\nlead to a higher quality summary compared to several strong baselines,\nincluding those based on a single encoder or multiple non-communicating\nencoders.", "published": "2018-03-27 23:29:23", "link": "http://arxiv.org/abs/1803.10357v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Multi-Discriminator CycleGAN for Unsupervised Non-Parallel Speech\n  Domain Adaptation", "abstract": "Domain adaptation plays an important role for speech recognition models, in\nparticular, for domains that have low resources. We propose a novel generative\nmodel based on cyclic-consistent generative adversarial network (CycleGAN) for\nunsupervised non-parallel speech domain adaptation. The proposed model employs\nmultiple independent discriminators on the power spectrogram, each in charge of\ndifferent frequency bands. As a result we have 1) better discriminators that\nfocus on fine-grained details of the frequency features, and 2) a generator\nthat is capable of generating more realistic domain-adapted spectrogram. We\ndemonstrate the effectiveness of our method on speech recognition with gender\nadaptation, where the model only has access to supervised data from one gender\nduring training, but is evaluated on the other at test time. Our model is able\nto achieve an average of $7.41\\%$ on phoneme error rate, and $11.10\\%$ word\nerror rate relative performance improvement as compared to the baseline, on\nTIMIT and WSJ dataset, respectively. Qualitatively, our model also generates\nmore natural sounding speech, when conditioned on data from the other domain.", "published": "2018-03-27 05:04:39", "link": "http://arxiv.org/abs/1804.00522v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Neural Baby Talk", "abstract": "We introduce a novel framework for image captioning that can produce natural\nlanguage explicitly grounded in entities that object detectors find in the\nimage. Our approach reconciles classical slot filling approaches (that are\ngenerally better grounded in images) with modern neural captioning approaches\n(that are generally more natural sounding and accurate). Our approach first\ngenerates a sentence `template' with slot locations explicitly tied to specific\nimage regions. These slots are then filled in by visual concepts identified in\nthe regions by object detectors. The entire architecture (sentence template\ngeneration and slot filling with object detectors) is end-to-end\ndifferentiable. We verify the effectiveness of our proposed model on different\nimage captioning tasks. On standard image captioning and novel object\ncaptioning, our model reaches state-of-the-art on both COCO and Flickr30k\ndatasets. We also demonstrate that our model has unique advantages when the\ntrain and test distributions of scene compositions -- and hence language priors\nof associated captions -- are different. Code has been made available at:\nhttps://github.com/jiasenlu/NeuralBabyTalk", "published": "2018-03-27 01:59:56", "link": "http://arxiv.org/abs/1803.09845v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Investigating Generative Adversarial Networks based Speech\n  Dereverberation for Robust Speech Recognition", "abstract": "We investigate the use of generative adversarial networks (GANs) in speech\ndereverberation for robust speech recognition. GANs have been recently studied\nfor speech enhancement to remove additive noises, but there still lacks of a\nwork to examine their ability in speech dereverberation and the advantages of\nusing GANs have not been fully established. In this paper, we provide deep\ninvestigations in the use of GAN-based dereverberation front-end in ASR. First,\nwe study the effectiveness of different dereverberation networks (the generator\nin GAN) and find that LSTM leads a significant improvement as compared with\nfeed-forward DNN and CNN in our dataset. Second, further adding residual\nconnections in the deep LSTMs can boost the performance as well. Finally, we\nfind that, for the success of GAN, it is important to update the generator and\nthe discriminator using the same mini-batch data during training. Moreover,\nusing reverberant spectrogram as a condition to discriminator, as suggested in\nprevious studies, may degrade the performance. In summary, our GAN-based\ndereverberation front-end achieves 14%-19% relative CER reduction as compared\nto the baseline DNN dereverberation network when tested on a strong\nmulti-condition training acoustic model.", "published": "2018-03-27 15:23:12", "link": "http://arxiv.org/abs/1803.10132v3", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Empirical Evaluation of Speaker Adaptation on DNN based Acoustic Model", "abstract": "Speaker adaptation aims to estimate a speaker specific acoustic model from a\nspeaker independent one to minimize the mismatch between the training and\ntesting conditions arisen from speaker variabilities. A variety of neural\nnetwork adaptation methods have been proposed since deep learning models have\nbecome the main stream. But there still lacks an experimental comparison\nbetween different methods, especially when DNN-based acoustic models have been\nadvanced greatly. In this paper, we aim to close this gap by providing an\nempirical evaluation of three typical speaker adaptation methods: LIN, LHUC and\nKLD. Adaptation experiments, with different size of adaptation data, are\nconducted on a strong TDNN-LSTM acoustic model. More challengingly, here, the\nsource and target we are concerned with are standard Mandarin speaker model and\naccented Mandarin speaker model. We compare the performances of different\nmethods and their combinations. Speaker adaptation performance is also examined\nby speaker's accent degree.", "published": "2018-03-27 15:39:46", "link": "http://arxiv.org/abs/1803.10146v3", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multi-Modal Data Augmentation for End-to-End ASR", "abstract": "We present a new end-to-end architecture for automatic speech recognition\n(ASR) that can be trained using \\emph{symbolic} input in addition to the\ntraditional acoustic input. This architecture utilizes two separate encoders:\none for acoustic input and another for symbolic input, both sharing the\nattention and decoder parameters. We call this architecture a multi-modal data\naugmentation network (MMDA), as it can support multi-modal (acoustic and\nsymbolic) input and enables seamless mixing of large text datasets with\nsignificantly smaller transcribed speech corpora during training. We study\ndifferent ways of transforming large text corpora into a symbolic form suitable\nfor training our MMDA network. Our best MMDA setup obtains small improvements\non character error rate (CER), and as much as 7-10\\% relative word error rate\n(WER) improvement over a baseline both with and without an external language\nmodel.", "published": "2018-03-27 20:12:39", "link": "http://arxiv.org/abs/1803.10299v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Automatic Minimisation of Masking in Multitrack Audio using Subgroups", "abstract": "The iterative process of masking minimisation when mixing multitrack audio is\na challenging optimisation problem, in part due to the complexity and\nnon-linearity of auditory perception. In this article, we first propose a\nmultitrack masking metric inspired by the MPEG psychoacoustic model. We\ninvestigate different audio processing techniques to manipulate the frequency\nand dynamic characteristics of the signal in order to reduce masking based on\nthe proposed metric. We also investigate whether or not automatically mixing\nusing subgrouping is beneficial or not to perceived quality and clarity of a\nmix. Evaluation results suggest that our proposed masking metric when used in\nan automatic mixing framework can be used to reduce inter-channel auditory\nmasking as well as improve the perceived quality and perceived clarity of a\nmix. Furthermore, our results suggest that using subgrouping in an automatic\nmixing framework can be used to improve the perceived quality and perceived\nclarity of a mix.", "published": "2018-03-27 08:43:46", "link": "http://arxiv.org/abs/1803.09960v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Student-Teacher Learning for BLSTM Mask-based Speech Enhancement", "abstract": "Spectral mask estimation using bidirectional long short-term memory (BLSTM)\nneural networks has been widely used in various speech enhancement\napplications, and it has achieved great success when it is applied to\nmultichannel enhancement techniques with a mask-based beamformer. However, when\nthese masks are used for single channel speech enhancement they severely\ndistort the speech signal and make them unsuitable for speech recognition. This\npaper proposes a student-teacher learning paradigm for single channel speech\nenhancement. The beamformed signal from multichannel enhancement is given as\ninput to the teacher network to obtain soft masks. An additional cross-entropy\nloss term with the soft mask target is combined with the original loss, so that\nthe student network with single-channel input is trained to mimic the soft mask\nobtained with multichannel input through beamforming. Experiments with the\nCHiME-4 challenge single channel track data shows improvement in ASR\nperformance.", "published": "2018-03-27 10:55:42", "link": "http://arxiv.org/abs/1803.10013v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Complex-Valued Restricted Boltzmann Machine for Direct Speech\n  Parameterization from Complex Spectra", "abstract": "This paper describes a novel energy-based probabilistic distribution that\nrepresents complex-valued data and explains how to apply it to direct feature\nextraction from complex-valued spectra. The proposed model, the complex-valued\nrestricted Boltzmann machine (CRBM), is designed to deal with complex-valued\nvisible units as an extension of the well-known restricted Boltzmann machine\n(RBM). Like the RBM, the CRBM learns the relationships between visible and\nhidden units without having connections between units in the same layer, which\ndramatically improves training efficiency by using Gibbs sampling or\ncontrastive divergence (CD). Another important characteristic is that the CRBM\nalso has connections between real and imaginary parts of each of the\ncomplex-valued visible units that help represent the data distribution in the\ncomplex domain. In speech signal processing, classification and generation\nfeatures are often based on amplitude spectra (e.g., MFCC, cepstra, and\nmel-cepstra) even if they are calculated from complex spectra, and they ignore\nphase information. In contrast, the proposed feature extractor using the CRBM\ndirectly encodes the complex spectra (or another complex-valued representation\nof the complex spectra) into binary-valued latent features (hidden units).\nSince the visible-hidden connections are undirected, we can also recover\n(decode) the complex spectra from the latent features directly. Our speech\ncoding experiments demonstrated that the CRBM outperformed other speech coding\nmethods, such as methods using the conventional RBM, the mel-log spectrum\napproximate (MLSA) decoder, etc.", "published": "2018-03-27 08:07:20", "link": "http://arxiv.org/abs/1803.09946v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Building state-of-the-art distant speech recognition using the CHiME-4\n  challenge with a setup of speech enhancement baseline", "abstract": "This paper describes a new baseline system for automatic speech recognition\n(ASR) in the CHiME-4 challenge to promote the development of noisy ASR in\nspeech processing communities by providing 1) state-of-the-art system with a\nsimplified single system comparable to the complicated top systems in the\nchallenge, 2) publicly available and reproducible recipe through the main\nrepository in the Kaldi speech recognition toolkit. The proposed system adopts\ngeneralized eigenvalue beamforming with bidirectional long short-term memory\n(LSTM) mask estimation. We also propose to use a time delay neural network\n(TDNN) based on the lattice-free version of the maximum mutual information\n(LF-MMI) trained with augmented all six microphones plus the enhanced data\nafter beamforming. Finally, we use a LSTM language model for lattice and n-best\nre-scoring. The final system achieved 2.74\\% WER for the real test set in the\n6-channel track, which corresponds to the 2nd place in the challenge. In\naddition, the proposed baseline recipe includes four different speech\nenhancement measures, short-time objective intelligibility measure (STOI),\nextended STOI (eSTOI), perceptual evaluation of speech quality (PESQ) and\nspeech distortion ratio (SDR) for the simulation test set. Thus, the recipe\nalso provides an experimental platform for speech enhancement studies with\nthese performance measures.", "published": "2018-03-27 14:33:53", "link": "http://arxiv.org/abs/1803.10109v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Comprehending Real Numbers: Development of Bengali Real Number Speech\n  Corpus", "abstract": "Speech recognition has received a less attention in Bengali literature due to\nthe lack of a comprehensive dataset. In this paper, we describe the development\nprocess of the first comprehensive Bengali speech dataset on real numbers. It\ncomprehends all the possible words that may arise in uttering any Bengali real\nnumber. The corpus has ten speakers from the different regions of Bengali\nnative people. It comprises of more than two thousands of speech samples in a\ntotal duration of closed to four hours. We also provide a deep analysis of our\ncorpus, highlight some of the notable features of it, and finally evaluate the\nperformances of two of the notable Bengali speech recognizers on it.", "published": "2018-03-27 15:27:07", "link": "http://arxiv.org/abs/1803.10136v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
