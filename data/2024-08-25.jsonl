{"title": "StockTime: A Time Series Specialized Large Language Model Architecture for Stock Price Prediction", "abstract": "The stock price prediction task holds a significant role in the financial\ndomain and has been studied for a long time. Recently, large language models\n(LLMs) have brought new ways to improve these predictions. While recent\nfinancial large language models (FinLLMs) have shown considerable progress in\nfinancial NLP tasks compared to smaller pre-trained language models (PLMs),\nchallenges persist in stock price forecasting. Firstly, effectively integrating\nthe modalities of time series data and natural language to fully leverage these\ncapabilities remains complex. Secondly, FinLLMs focus more on analysis and\ninterpretability, which can overlook the essential features of time series\ndata. Moreover, due to the abundance of false and redundant information in\nfinancial markets, models often produce less accurate predictions when faced\nwith such input data. In this paper, we introduce StockTime, a novel LLM-based\narchitecture designed specifically for stock price data. Unlike recent FinLLMs,\nStockTime is specifically designed for stock price time series data. It\nleverages the natural ability of LLMs to predict the next token by treating\nstock prices as consecutive tokens, extracting textual information such as\nstock correlations, statistical trends and timestamps directly from these stock\nprices. StockTime then integrates both textual and time series data into the\nembedding space. By fusing this multimodal data, StockTime effectively predicts\nstock prices across arbitrary look-back periods. Our experiments demonstrate\nthat StockTime outperforms recent LLMs, as it gives more accurate predictions\nwhile reducing memory usage and runtime costs.", "published": "2024-08-25 00:50:33", "link": "http://arxiv.org/abs/2409.08281v1", "categories": ["q-fin.ST", "cs.AI", "cs.CE", "cs.LG"], "primary_category": "q-fin.ST"}
{"title": "Poor-Supervised Evaluation for SuperLLM via Mutual Consistency", "abstract": "The guidance from capability evaluations has greatly propelled the progress\nof both human society and Artificial Intelligence. However, as LLMs evolve, it\nbecomes challenging to construct evaluation benchmarks for them with accurate\nlabels on hard tasks that approach the boundaries of human capabilities. To\ncredibly conduct evaluation without accurate labels (denoted as poor-supervised\nevaluation), we propose the PoEM framework. We first prove that the capability\nof a model can be equivalently assessed by the consistency between it and\ncertain reference model, when their prediction distributions are independent\nand the sample size is infinite. To alleviate the insufficiencies of the\nconditions in reality, we further introduce an algorithm that treats humans\n(when available) and the models under evaluation as reference models,\nalternately conducting model weights calibration and filtering during E-step\nand M-step. Comprehensive experiments across 3 types of tasks with 16\nmainstream LLMs have shown that PoEM under poor supervision can achieve an\naverage of 0.98 Pearson correlation coefficient with supervised evaluation\nresults, demonstrating good effectiveness, efficiency and generalizability.\nMore generally, PoEM has advanced the evaluation paradigm evolution from\nhuman-centric to human&model-centric by treating both of them as reference\nmodels, mitigating the limitations of human evaluation in the era of LLMs.", "published": "2024-08-25 06:49:03", "link": "http://arxiv.org/abs/2408.13738v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Reliable Medical Question Answering: Techniques and Challenges\n  in Mitigating Hallucinations in Language Models", "abstract": "The rapid advancement of large language models (LLMs) has significantly\nimpacted various domains, including healthcare and biomedicine. However, the\nphenomenon of hallucination, where LLMs generate outputs that deviate from\nfactual accuracy or context, poses a critical challenge, especially in\nhigh-stakes domains. This paper conducts a scoping study of existing techniques\nfor mitigating hallucinations in knowledge-based task in general and especially\nfor medical domains. Key methods covered in the paper include\nRetrieval-Augmented Generation (RAG)-based techniques, iterative feedback\nloops, supervised fine-tuning, and prompt engineering. These techniques, while\npromising in general contexts, require further adaptation and optimization for\nthe medical domain due to its unique demands for up-to-date, specialized\nknowledge and strict adherence to medical guidelines. Addressing these\nchallenges is crucial for developing trustworthy AI systems that enhance\nclinical decision-making and patient safety as well as accuracy of biomedical\nscientific research.", "published": "2024-08-25 11:09:15", "link": "http://arxiv.org/abs/2408.13808v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting the Exit from Nuclear Energy in Germany with NLP", "abstract": "Annotation of political discourse is resource-intensive, but recent\ndevelopments in NLP promise to automate complex annotation tasks. Fine-tuned\ntransformer-based models outperform human annotators in some annotation tasks,\nbut they require large manually annotated training datasets. In our\ncontribution, we explore to which degree a manually annotated dataset can be\nautomatically replicated with today's NLP methods, using unsupervised machine\nlearning and zero- and few-shot learning.", "published": "2024-08-25 11:13:29", "link": "http://arxiv.org/abs/2408.13810v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Biomedical Large Languages Models Seem not to be Superior to Generalist\n  Models on Unseen Medical Data", "abstract": "Large language models (LLMs) have shown potential in biomedical applications,\nleading to efforts to fine-tune them on domain-specific data. However, the\neffectiveness of this approach remains unclear. This study evaluates the\nperformance of biomedically fine-tuned LLMs against their general-purpose\ncounterparts on a variety of clinical tasks. We evaluated their performance on\nclinical case challenges from the New England Journal of Medicine (NEJM) and\nthe Journal of the American Medical Association (JAMA) and on several clinical\ntasks (e.g., information extraction, document summarization, and clinical\ncoding). Using benchmarks specifically chosen to be likely outside the\nfine-tuning datasets of biomedical models, we found that biomedical LLMs mostly\nperform inferior to their general-purpose counterparts, especially on tasks not\nfocused on medical knowledge. While larger models showed similar performance on\ncase tasks (e.g., OpenBioLLM-70B: 66.4% vs. Llama-3-70B-Instruct: 65% on JAMA\ncases), smaller biomedical models showed more pronounced underperformance\n(e.g., OpenBioLLM-8B: 30% vs. Llama-3-8B-Instruct: 64.3% on NEJM cases).\nSimilar trends were observed across the CLUE (Clinical Language Understanding\nEvaluation) benchmark tasks, with general-purpose models often performing\nbetter on text generation, question answering, and coding tasks. Our results\nsuggest that fine-tuning LLMs to biomedical data may not provide the expected\nbenefits and may potentially lead to reduced performance, challenging\nprevailing assumptions about domain-specific adaptation of LLMs and\nhighlighting the need for more rigorous evaluation frameworks in healthcare AI.\nAlternative approaches, such as retrieval-augmented generation, may be more\neffective in enhancing the biomedical capabilities of LLMs without compromising\ntheir general knowledge.", "published": "2024-08-25 13:36:22", "link": "http://arxiv.org/abs/2408.13833v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLM with Relation Classifier for Document-Level Relation Extraction", "abstract": "Large language models (LLMs) have created a new paradigm for natural language\nprocessing. Despite their advancement, LLM-based methods still lag behind\ntraditional approaches in document-level relation extraction (DocRE), a\ncritical task for understanding complex entity relations within long context.\nThis paper investigates the causes of this performance gap, identifying the\ndispersion of attention by LLMs due to entity pairs without relations as a key\nfactor. We then introduce a novel classifier-LLM approach to DocRE.\nParticularly, the proposed approach begins with a classifier designed to select\nentity pair candidates that exhibit potential relations and then feed them to\nLLM for final relation classification. This method ensures that the LLM's\nattention is directed at relation-expressing entity pairs instead of those\nwithout relations during inference. Experiments on DocRE benchmarks reveal that\nour method significantly outperforms recent LLM-based DocRE models and narrows\nthe performance gap with state-of-the-art BERT-based models.", "published": "2024-08-25 16:43:19", "link": "http://arxiv.org/abs/2408.13889v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MobileQuant: Mobile-friendly Quantization for On-device Language Models", "abstract": "Large language models (LLMs) have revolutionized language processing,\ndelivering outstanding results across multiple applications. However, deploying\nLLMs on edge devices poses several challenges with respect to memory, energy,\nand compute costs, limiting their widespread use in devices such as mobile\nphones. A promising solution is to reduce the number of bits used to represent\nweights and activations. While existing works have found partial success at\nquantizing LLMs to lower bitwidths, e.g. 4-bit weights, quantizing activations\nbeyond 16 bits often leads to large computational overheads due to poor\non-device quantization support, or a considerable accuracy drop. Yet, 8-bit\nactivations are very attractive for on-device deployment as they would enable\nLLMs to fully exploit mobile-friendly hardware, e.g. Neural Processing Units\n(NPUs). In this work, we make a first attempt to facilitate the on-device\ndeployment of LLMs using integer-only quantization. We first investigate the\nlimitations of existing quantization methods for on-device deployment, with a\nspecial focus on activation quantization. We then address these limitations by\nintroducing a simple post-training quantization method, named MobileQuant, that\nextends previous weight equivalent transformation works by jointly optimizing\nthe weight transformation and activation range parameters in an end-to-end\nmanner. MobileQuant demonstrates superior capabilities over existing methods by\n1) achieving near-lossless quantization on a wide range of LLM benchmarks, 2)\nreducing latency and energy consumption by 20\\%-50\\% compared to current\non-device quantization strategies, 3) requiring limited compute budget, 4)\nbeing compatible with mobile-friendly compute units, e.g. NPU.", "published": "2024-08-25 20:41:22", "link": "http://arxiv.org/abs/2408.13933v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Derailer-Rerailer: Adaptive Verification for Efficient and Reliable\n  Language Model Reasoning", "abstract": "Large Language Models (LLMs) have shown impressive reasoning capabilities,\nyet existing prompting methods face a critical trade-off: simple approaches\noften struggle with complex tasks and reasoning stability, while more\nsophisticated methods require multiple inferences and substantial computational\nresources, limiting their practical deployment. To address this challenge, we\npropose Derailer-Rerailer, a novel framework that adaptively balances reasoning\naccuracy and computational efficiency. At its core, our framework employs a\nlightweight Derailer mechanism to assess reasoning stability and selectively\ntriggers an advanced Rerailer verification process only when necessary, thereby\noptimizing computational resource usage. Extensive evaluation across both open\nand closed-source models on more than 20 categories of mathematical, symbolic,\nand commonsense reasoning tasks demonstrates our framework's effectiveness:\nDerailer-Rerailer achieves significant accuracy improvements (8-11\\% across\nvarious reasoning tasks) while maintaining 2-3 times better efficiency than\nexisting verification methods, with particularly strong performance in\nmathematical and symbolic reasoning, offering a practical solution for\nenhancing LLM reasoning reliability while significantly reducing computational\noverhead.", "published": "2024-08-25 21:20:17", "link": "http://arxiv.org/abs/2408.13940v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bidirectional Awareness Induction in Autoregressive Seq2Seq Models", "abstract": "Autoregressive Sequence-To-Sequence models are the foundation of many Deep\nLearning achievements in major research fields such as Vision and Natural\nLanguage Processing. Despite that, they still present significant limitations.\nFor instance, when errors occur in the early steps of the prediction, the whole\noutput is severely affected. Such reliance on previously predicted tokens and\nthe inherent computational unfriendliness of sequential algorithms, motivated\nresearchers to explore different architectures and methods in the search for\nbidirectional approaches. In this work, we introduce the Bidirectional\nAwareness Induction (BAI), a training method that leverages a subset of\nelements in the network, the Pivots, to perform bidirectional learning without\nbreaking the autoregressive constraints. To showcase its flexibility, we apply\nthe method to three architectures, the Transformer, ExpansionNet v2 and GPT,\nthen perform experiments over three tasks. Experimental results showcase BAI's\neffectiveness on all selected tasks and architectures. In particular, we\nobserved an increase of up to 2.4 CIDEr in Image-Captioning, 4.96 BLEU in\nNeural Machine Translation, and 1.16 ROUGE in Text Summarization compared to\nthe respective baselines. Notably, BAI not only has a positive impact on models\ntrained from scratch but on pre-trained models as well. Such an aspect,\ncombined with the absence of architectural requirements synergizes well with\nthe current trend of LLMs.", "published": "2024-08-25 23:46:35", "link": "http://arxiv.org/abs/2408.13959v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DHP Benchmark: Are LLMs Good NLG Evaluators?", "abstract": "Large Language Models (LLMs) are increasingly serving as evaluators in\nNatural Language Generation (NLG) tasks; this is often referred to as\n``LLM-as-a-judge'' paradigm. However, the capabilities of LLMs in evaluating\nNLG quality remain underexplored. Current studies depend on human assessments\nand simple metrics that fail to capture the discernment of LLMs across diverse\nNLG tasks. To address this gap, we propose the Discernment of Hierarchical\nPerturbation (DHP) benchmarking framework, which provides quantitative\ndiscernment scores for LLMs. This framework leverages hierarchically perturbed\ntext data and statistical tests to systematically measure the NLG evaluation\ncapabilities of LLMs. We re-established six evaluation datasets for this\nbenchmark, covering four NLG tasks: Summarization, Story Completion, Question\nAnswering, and Translation. Our comprehensive benchmarking of five major LLM\nfamilies provides critical insight into their strengths and limitations as NLG\nevaluators. Our dataset is available at\nhttps://huggingface.co/datasets/YCWANGVINCE/DHP_Benchmark.", "published": "2024-08-25 02:01:38", "link": "http://arxiv.org/abs/2408.13704v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Guardians of the Machine Translation Meta-Evaluation: Sentinel Metrics\n  Fall In!", "abstract": "Annually, at the Conference of Machine Translation (WMT), the Metrics Shared\nTask organizers conduct the meta-evaluation of Machine Translation (MT)\nmetrics, ranking them according to their correlation with human judgments.\nTheir results guide researchers toward enhancing the next generation of metrics\nand MT systems. With the recent introduction of neural metrics, the field has\nwitnessed notable advancements. Nevertheless, the inherent opacity of these\nmetrics has posed substantial challenges to the meta-evaluation process. This\nwork highlights two issues with the meta-evaluation framework currently\nemployed in WMT, and assesses their impact on the metrics rankings. To do this,\nwe introduce the concept of sentinel metrics, which are designed explicitly to\nscrutinize the meta-evaluation process's accuracy, robustness, and fairness. By\nemploying sentinel metrics, we aim to validate our findings, and shed light on\nand monitor the potential biases or inconsistencies in the rankings. We\ndiscover that the present meta-evaluation framework favors two categories of\nmetrics: i) those explicitly trained to mimic human quality assessments, and\nii) continuous metrics. Finally, we raise concerns regarding the evaluation\ncapabilities of state-of-the-art metrics, emphasizing that they might be basing\ntheir assessments on spurious correlations found in their training data.", "published": "2024-08-25 13:29:34", "link": "http://arxiv.org/abs/2408.13831v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Knowledge-Aware Reasoning over Multimodal Semi-structured Tables", "abstract": "Existing datasets for tabular question answering typically focus exclusively\non text within cells. However, real-world data is inherently multimodal, often\nblending images such as symbols, faces, icons, patterns, and charts with\ntextual content in tables. With the evolution of AI models capable of\nmultimodal reasoning, it is pertinent to assess their efficacy in handling such\nstructured data. This study investigates whether current AI models can perform\nknowledge-aware reasoning on multimodal structured data. We explore their\nability to reason on tables that integrate both images and text, introducing\nMMTabQA, a new dataset designed for this purpose. Our experiments highlight\nsubstantial challenges for current AI models in effectively integrating and\ninterpreting multiple text and image inputs, understanding visual context, and\ncomparing visual content across images. These findings establish our dataset as\na robust benchmark for advancing AI's comprehension and capabilities in\nanalyzing multimodal structured data.", "published": "2024-08-25 15:17:43", "link": "http://arxiv.org/abs/2408.13860v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "CodeGraph: Enhancing Graph Reasoning of LLMs with Code", "abstract": "With the increasing popularity of large language models (LLMs), reasoning on\nbasic graph algorithm problems is an essential intermediate step in assessing\ntheir abilities to process and infer complex graph reasoning tasks. Existing\nmethods usually convert graph-structured data to textual descriptions and then\nuse LLMs for reasoning and computation. However, LLMs often produce computation\nerrors on arithmetic parts in basic graph algorithm problems, such as counting\nnumber of edges. In addition, they struggle to control or understand the output\nof the reasoning process, raising concerns about whether LLMs are simply\nguessing. In this paper, we introduce CodeGraph, a method that encodes graph\nproblem solutions as code. The methods solve new graph problems by learning\nfrom exemplars, generating programs, and executing them via a program\ninterpreter. Using the few-shot setting, we evaluate CodeGraph with the base\nLLM being GPT-3.5 Turbo, Llama3-70B Instruct, Mixtral-8x22B Instruct, and\nMixtral-8x7B Instruct. Experimental results on six tasks with six graph\nencoding methods in the GraphQA dataset demonstrate that CodeGraph can boost\nperformance on graph reasoning tasks inside LLMs by 1.3% to 58.6%, depending on\nthe task. Compared to the existing methods, CodeGraph demonstrates strong\nperformance on arithmetic problems in graph tasks and offers a more\ncontrollable and interpretable approach to the reasoning process.", "published": "2024-08-25 15:27:21", "link": "http://arxiv.org/abs/2408.13863v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SpeechCaps: Advancing Instruction-Based Universal Speech Models with\n  Multi-Talker Speaking Style Captioning", "abstract": "Instruction-based speech processing is becoming popular. Studies show that\ntraining with multiple tasks boosts performance, but collecting diverse,\nlarge-scale tasks and datasets is expensive. Thus, it is highly desirable to\ndesign a fundamental task that benefits other downstream tasks. This paper\nintroduces a multi-talker speaking style captioning task to enhance the\nunderstanding of speaker and prosodic information. We used large language\nmodels to generate descriptions for multi-talker speech. Then, we trained our\nmodel with pre-training on this captioning task followed by instruction tuning.\nEvaluation on Dynamic-SUPERB shows our model outperforming the baseline\npre-trained only on single-talker tasks, particularly in speaker and emotion\nrecognition. Additionally, tests on a multi-talker QA task reveal that current\nmodels struggle with attributes such as gender, pitch, and speaking rate. The\ncode and dataset are available at https://github.com/cyhuang-tw/speechcaps.", "published": "2024-08-25 17:05:26", "link": "http://arxiv.org/abs/2408.13891v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "LowCLIP: Adapting the CLIP Model Architecture for Low-Resource Languages\n  in Multimodal Image Retrieval Task", "abstract": "This research explores the development of multimodal vision-language models\nfor image retrieval in low-resource languages, specifically Azerbaijani.\nExisting vision-language models primarily support high-resource languages, and\nfine-tuning them remains computationally demanding. To address challenges in\nvision-language retrieval for low-resource languages, we integrated the CLIP\nmodel architecture and employed several techniques to balance computational\nefficiency with performance. These techniques include synthetic data generation\nthrough machine translation, image augmentation, and further training the\nattention mechanisms of transformer-based models with domain-specific data. We\nintegrated Multilingual BERT as a text encoder with image encoders like\nResNet50, EfficientNet0, Vision Transformer (ViT), and Tiny Swin Transformer.\nOur study found that models like EfficientNet0 and Tiny Swin Transformer\nperform best on the datasets they were trained on, such as COCO, Flickr30k, and\nFlickr8k. Augmentation techniques boosted EfficientNet0 MAP on Flickr30k from\n0.84 to 0.87 and ResNet50 MAP on MSCOCO from 0.70 to 0.80, contributing to a\nnew state of the art in vision-language retrieval. We share our configurations\nand results to support further research. Code and pre-trained models are\navailable at https://github.com/aliasgerovs/azclip.", "published": "2024-08-25 18:10:16", "link": "http://arxiv.org/abs/2408.13909v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "LLMs are Superior Feedback Providers: Bootstrapping Reasoning for Lie\n  Detection with Self-Generated Feedback", "abstract": "Large Language Models (LLMs) excel at generating human-like dialogues and\ncomprehending text. However, understanding the subtleties of complex exchanges\nin language remains a challenge. We propose a bootstrapping framework that\nleverages self-generated feedback to enhance LLM reasoning capabilities for lie\ndetection. The framework consists of three stages: suggestion, feedback\ncollection, and modification. In the suggestion stage, a cost-effective\nlanguage model generates initial predictions based on game state and dialogue.\nThe feedback-collection stage involves a language model providing feedback on\nthese predictions. In the modification stage, a more advanced language model\nrefines the initial predictions using the auto-generated feedback. We\ninvestigate the application of the proposed framework for detecting betrayal\nand deception in Diplomacy games, and compare it with feedback from\nprofessional human players. The LLM-generated feedback exhibits superior\nquality and significantly enhances the performance of the model. Our approach\nachieves a 39% improvement over the zero-shot baseline in lying-F1 without the\nneed for any training data, rivaling state-of-the-art supervised learning\nresults.", "published": "2024-08-25 18:47:55", "link": "http://arxiv.org/abs/2408.13915v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Prediction of COPD Using Machine Learning, Clinical Summary Notes, and\n  Vital Signs", "abstract": "Chronic obstructive pulmonary disease (COPD) is a chronic inflammatory lung\ndisease that causes obstructed airflow from the lungs. In the United States,\nmore than 15.7 million Americans have been diagnosed with COPD, with 96% of\nindividuals living with at least one other chronic health condition. It is the\n4th leading cause of death in the country. Over 2.2 million patients are\nadmitted to hospitals annually due to COPD exacerbations. Monitoring and\npredicting patient exacerbations on-time could save their life. This paper\npresents two different predictive models to predict COPD exacerbation using AI\nand natural language processing (NLP) approaches. These models use respiration\nsummary notes, symptoms, and vital signs. To train and test these models, data\nrecords containing physiologic signals and vital signs time series were used.\nThese records were captured from patient monitors and comprehensive clinical\ndata obtained from hospital medical information systems for tens of thousands\nof Intensive Care Unit (ICU) patients. We achieved an area under the Receiver\noperating characteristic (ROC) curve of 0.82 in detection and prediction of\nCOPD exacerbation.", "published": "2024-08-25 23:41:39", "link": "http://arxiv.org/abs/2408.13958v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Vision-Language and Large Language Model Performance in\n  Gastroenterology: GPT, Claude, Llama, Phi, Mistral, Gemma, and Quantized\n  Models", "abstract": "Background and Aims: This study evaluates the medical reasoning performance\nof large language models (LLMs) and vision language models (VLMs) in\ngastroenterology.\n  Methods: We used 300 gastroenterology board exam-style multiple-choice\nquestions, 138 of which contain images to systematically assess the impact of\nmodel configurations and parameters and prompt engineering strategies utilizing\nGPT-3.5. Next, we assessed the performance of proprietary and open-source LLMs\n(versions), including GPT (3.5, 4, 4o, 4omini), Claude (3, 3.5), Gemini (1.0),\nMistral, Llama (2, 3, 3.1), Mixtral, and Phi (3), across different interfaces\n(web and API), computing environments (cloud and local), and model precisions\n(with and without quantization). Finally, we assessed accuracy using a\nsemiautomated pipeline.\n  Results: Among the proprietary models, GPT-4o (73.7%) and Claude3.5-Sonnet\n(74.0%) achieved the highest accuracy, outperforming the top open-source\nmodels: Llama3.1-405b (64%), Llama3.1-70b (58.3%), and Mixtral-8x7b (54.3%).\nAmong the quantized open-source models, the 6-bit quantized Phi3-14b (48.7%)\nperformed best. The scores of the quantized models were comparable to those of\nthe full-precision models Llama2-7b, Llama2--13b, and Gemma2-9b. Notably, VLM\nperformance on image-containing questions did not improve when the images were\nprovided and worsened when LLM-generated captions were provided. In contrast, a\n10% increase in accuracy was observed when images were accompanied by\nhuman-crafted image descriptions.\n  Conclusion: In conclusion, while LLMs exhibit robust zero-shot performance in\nmedical reasoning, the integration of visual data remains a challenge for VLMs.\nEffective deployment involves carefully determining optimal model\nconfigurations, encouraging users to consider either the high performance of\nproprietary models or the flexible adaptability of open-source models.", "published": "2024-08-25 14:50:47", "link": "http://arxiv.org/abs/2409.00084v2", "categories": ["cs.CL", "cs.AI", "92C50, 68T50", "J.3"], "primary_category": "cs.CL"}
{"title": "Genetic Approach to Mitigate Hallucination in Generative IR", "abstract": "Generative language models hallucinate. That is, at times, they generate\nfactually flawed responses. These inaccuracies are particularly insidious\nbecause the responses are fluent and well-articulated. We focus on the task of\nGrounded Answer Generation (part of Generative IR), which aims to produce\ndirect answers to a user's question based on results retrieved from a search\nengine. We address hallucination by adapting an existing genetic generation\napproach with a new 'balanced fitness function' consisting of a cross-encoder\nmodel for relevance and an n-gram overlap metric to promote grounding. Our\nbalanced fitness function approach quadruples the grounded answer generation\naccuracy while maintaining high relevance.", "published": "2024-08-25 20:03:08", "link": "http://arxiv.org/abs/2409.00085v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Path-Consistency: Prefix Enhancement for Efficient Inference in LLM", "abstract": "To enhance the reasoning capabilities of large language models (LLMs),\nself-consistency has gained significant popularity by combining multiple\nsampling with majority voting. However, the state-of-the-art self-consistency\napproaches consume substantial computational resources and lead to significant\nadditional time costs due to the multiple sampling. This prevents its full\npotential from being realized in scenarios where computational resources are\ncritical. To improve the inference efficiency, this paper introduces\n\\textit{path-consistency}, a method that leverages the confidence of answers\ngenerated in earlier branches to identify the prefix of the most promising\npath. By dynamically guiding the generation of subsequent branches based on\nthis prefix, the \\textit{path-consistency} mitigates both the errors and\nredundancies from random or less useful sampling in self-consistency. As a\nresult, it can significantly accelerate the inference process by reducing the\nnumber of tokens generated. Our extensive empirical evaluation shows that the\n\\textit{path-consistency} achieves significant acceleration in inference\nlatency ranging from $7.8\\%$ to $40.5\\%$, while maintaining or even improving\ntask accuracy across different datasets, including mathematical reasoning,\ncommon sense reasoning, symbolic reasoning, and code generation.", "published": "2024-08-25 01:45:53", "link": "http://arxiv.org/abs/2409.01281v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DOCE: Finding the Sweet Spot for Execution-Based Code Generation", "abstract": "Recently, a diverse set of decoding and reranking procedures have been shown\neffective for LLM-based code generation. However, a comprehensive framework\nthat links and experimentally compares these methods is missing. We address\nthis by proposing Decoding Objectives for Code Execution, a comprehensive\nframework that includes candidate generation, $n$-best reranking, minimum Bayes\nrisk (MBR) decoding, and self-debugging as the core components. We then study\nthe contributions of these components through execution-based evaluation\nmetrics. Our findings highlight the importance of execution-based methods and\nthe difference gap between execution-based and execution-free methods.\nFurthermore, we assess the impact of filtering based on trial unit tests, a\nsimple and effective strategy that has been often overlooked in prior works. We\nalso propose self-debugging on multiple candidates, obtaining state-of-the-art\nperformance on reranking for code generation. We expect our framework to\nprovide a solid guideline for future research on code generation.", "published": "2024-08-25 07:10:36", "link": "http://arxiv.org/abs/2408.13745v4", "categories": ["cs.CL", "cs.AI", "cs.PL"], "primary_category": "cs.CL"}
{"title": "SimpleSpeech 2: Towards Simple and Efficient Text-to-Speech with\n  Flow-based Scalar Latent Transformer Diffusion Models", "abstract": "Scaling Text-to-speech (TTS) to large-scale datasets has been demonstrated as\nan effective method for improving the diversity and naturalness of synthesized\nspeech. At the high level, previous large-scale TTS models can be categorized\ninto either Auto-regressive (AR) based (\\textit{e.g.}, VALL-E) or\nNon-auto-regressive (NAR) based models (\\textit{e.g.}, NaturalSpeech 2/3).\nAlthough these works demonstrate good performance, they still have potential\nweaknesses. For instance, AR-based models are plagued by unstable generation\nquality and slow generation speed; meanwhile, some NAR-based models need\nphoneme-level duration alignment information, thereby increasing the complexity\nof data pre-processing, model design, and loss design. In this work, we build\nupon our previous publication by implementing a simple and efficient\nnon-autoregressive (NAR) TTS framework, termed SimpleSpeech 2. SimpleSpeech 2\neffectively combines the strengths of both autoregressive (AR) and\nnon-autoregressive (NAR) methods, offering the following key advantages: (1)\nsimplified data preparation; (2) straightforward model and loss design; and (3)\nstable, high-quality generation performance with fast inference speed. Compared\nto our previous publication, we present ({\\romannumeral1}) a detailed analysis\nof the influence of speech tokenizer and noisy label for TTS performance;\n({\\romannumeral2}) four distinct types of sentence duration predictors;\n({\\romannumeral3}) a novel flow-based scalar latent transformer diffusion\nmodel. With these improvement, we show a significant improvement in generation\nperformance and generation speed compared to our previous work and other\nstate-of-the-art (SOTA) large-scale TTS models. Furthermore, we show that\nSimpleSpeech 2 can be seamlessly extended to multilingual TTS by training it on\nmultilingual speech datasets. Demos are available on:\n{https://dongchaoyang.top/SimpleSpeech2\\_demo/}.", "published": "2024-08-25 17:07:39", "link": "http://arxiv.org/abs/2408.13893v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "LLMs as Zero-shot Graph Learners: Alignment of GNN Representations with\n  LLM Token Embeddings", "abstract": "Zero-shot graph machine learning, especially with graph neural networks\n(GNNs), has garnered significant interest due to the challenge of scarce\nlabeled data. While methods like self-supervised learning and graph prompt\nlearning have been extensively explored, they often rely on fine-tuning with\ntask-specific labels, limiting their effectiveness in zero-shot scenarios.\nInspired by the zero-shot capabilities of instruction-fine-tuned large language\nmodels (LLMs), we introduce a novel framework named Token Embedding-Aligned\nGraph Language Model (TEA-GLM) that leverages LLMs as cross-dataset and\ncross-task zero-shot learners for graph machine learning. Concretely, we\npretrain a GNN, aligning its representations with token embeddings of an LLM.\nWe then train a linear projector that transforms the GNN's representations into\na fixed number of graph token embeddings without tuning the LLM. A unified\ninstruction is designed for various graph tasks at different levels, such as\nnode classification (node-level) and link prediction (edge-level). These design\nchoices collectively enhance our method's effectiveness in zero-shot learning,\nsetting it apart from existing methods. Experiments show that our graph token\nembeddings help the LLM predictor achieve state-of-the-art performance on\nunseen datasets and tasks compared to other methods using LLMs as predictors.", "published": "2024-08-25 04:32:45", "link": "http://arxiv.org/abs/2408.14512v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Literary and Colloquial Tamil Dialect Identification", "abstract": "Culture and language evolve together. The old literary form of Tamil is used\ncommonly for writing and the contemporary colloquial Tamil is used for\nspeaking. Human-computer interaction applications require Colloquial Tamil (CT)\nto make it more accessible and easy for the everyday user and, it requires\nLiterary Tamil (LT) when information is needed in a formal written format.\nContinuing the use of LT alongside CT in computer aided language learning\napplications will both preserve LT, and provide ease of use via CT, at the same\ntime. Hence there is a need for the conversion between LT and CT dialects,\nwhich demands as a first step, dialect identification. Dialect Identification\n(DID) of LT and CT is an unexplored area of research. In the current work,\nkeeping the nuances of both these dialects in mind, five methods are explored\nwhich include two implicit methods - Gaussian Mixture Model (GMM) and\nConvolutional Neural Network (CNN); two explicit methods - Parallel Phone\nRecognition (PPR) and Parallel Large Vocabulary Continuous Speech Recognition\n(P-LVCSR); two versions of the proposed explicit Unified Phone Recognition\nmethod (UPR-1 and UPR-2). These methods vary based on: the need for annotated\ndata, the size of the unit, the way in which modelling is carried out, and the\nway in which the final decision is made. Even though the average duration of\nthe test utterances is less - 4.9s for LT and 2.5s for CT - the systems\nperformed well, offering the following identification accuracies: 87.72% (GMM),\n93.97% (CNN), 89.24% (PPR), 94.21% (P-LVCSR), 88.57% (UPR-1), 93.53% (UPR-1\nwith P-LVCSR), 94.55% (UPR-2), and 95.61% (UPR-2 with P-LVCSR).", "published": "2024-08-25 06:52:48", "link": "http://arxiv.org/abs/2408.13739v1", "categories": ["eess.AS", "cs.CL", "cs.HC", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Unveiling the Statistical Foundations of Chain-of-Thought Prompting\n  Methods", "abstract": "Chain-of-Thought (CoT) prompting and its variants have gained popularity as\neffective methods for solving multi-step reasoning problems using pretrained\nlarge language models (LLMs). In this work, we analyze CoT prompting from a\nstatistical estimation perspective, providing a comprehensive characterization\nof its sample complexity. To this end, we introduce a multi-step latent\nvariable model that encapsulates the reasoning process, where the latent\nvariable encodes the task information. Under this framework, we demonstrate\nthat when the pretraining dataset is sufficiently large, the estimator formed\nby CoT prompting is equivalent to a Bayesian estimator. This estimator\neffectively solves the multi-step reasoning problem by aggregating a posterior\ndistribution inferred from the demonstration examples in the prompt. Moreover,\nwe prove that the statistical error of the CoT estimator can be decomposed into\ntwo main components: (i) a prompting error, which arises from inferring the\ntrue task using CoT prompts, and (ii) the statistical error of the pretrained\nLLM. We establish that, under appropriate assumptions, the prompting error\ndecays exponentially to zero as the number of demonstrations increases.\nAdditionally, we explicitly characterize the approximation and generalization\nerrors of the pretrained LLM. Notably, we construct a transformer model that\napproximates the target distribution of the multi-step reasoning problem with\nan error that decreases exponentially in the number of transformer blocks. Our\nanalysis extends to other variants of CoT, including Self-Consistent CoT,\nTree-of-Thought, and Selection-Inference, offering a broad perspective on the\nefficacy of these methods. We also provide numerical experiments to validate\nthe theoretical findings.", "published": "2024-08-25 04:07:18", "link": "http://arxiv.org/abs/2408.14511v2", "categories": ["cs.AI", "cs.CL", "cs.LG", "math.ST", "stat.ML", "stat.TH"], "primary_category": "cs.AI"}
{"title": "Chirp Group Delay based Onset Detection in Instruments with Fast Attack", "abstract": "The onset of a musical note is the earliest time at which a note can be\nreliably detected. Detection of these musical onsets pose challenges in the\npresence of ornamentation such as vibrato, bending, and if the attack of the\nnote transient is slower. The legacy systems such as spectral difference or\nflux and complex domain functions suffer from the addition of false positives\ndue to ornamentation posing as viable onsets. We propose that this can be\nsolved by appropriately improving the resolution of the onset strength signal\n(OSS) and smoothening it to increase true positives and decrease false\npositives, respectively. An appropriate peak picking algorithm that works well\nin unison with the OSS generated is also desired. Since onset detection is a\nlow-level process upon which many other tasks are built, computational\ncomplexity must also be reduced. We propose an onset detection alogrithm that\nis a combination of short-time spectral average-based OSS estimation, chirp\ngroup delay-based smoothening, and valley-peak distance-based peak picking.\nThis algorithm performs on par with the state-of-the-art, superflux and\nconvolutional neural networks-based onset detection, with an average F1 score\nof 0.88, across three datasets. Subsets from the IDMT-SMT-Guitar, Guitarset,\nand Musicnet datasets that fit the scope of the work, are used for evaluation.\nIt is also found that the proposed algorithm is computationally 300\\% more\nefficient than superflux. The positive effects of smoothening an OSS, in\ndetermining the onset locations, is established by refining the OSS produced by\nlegacy algorithms, where consistent improvement in onset detection performance\nis observed. To provide insights into the performance of the proposed\nalgorithms when different ornamentation styles are present in the recording,\nthree levels of results are computed, by selecting different subsets of the\nIDMT dataset.", "published": "2024-08-25 06:20:19", "link": "http://arxiv.org/abs/2408.13734v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Quartered Spectral Envelope and 1D-CNN-based Classification of Normally\n  Phonated and Whispered Speech", "abstract": "Whisper, as a form of speech, is not sufficiently addressed by mainstream\nspeech applications. This is due to the fact that systems built for normal\nspeech do not work as expected for whispered speech. A first step to building a\nspeech application that is inclusive of whispered speech, is the successful\nclassification of whispered speech and normal speech. Such a front-end\nclassification system is expected to have high accuracy and low computational\noverhead, which is the scope of this paper. One of the characteristics of\nwhispered speech is the absence of the fundamental frequency (or pitch), and\nhence the pitch harmonics as well. The presence of the pitch and pitch\nharmonics in normal speech, and its absence in whispered speech, is evident in\nthe spectral envelope of the Fourier transform. We observe that this\ncharacteristic is predominant in the first quarter of the spectrum, and exploit\nthe same as a feature. We propose the use of one dimensional convolutional\nneural networks (1D-CNN) to capture these features from the quartered spectral\nenvelope (QSE). The system yields an accuracy of 99.31% when trained and tested\non the wTIMIT dataset, and 100% on the CHAINS dataset. The proposed feature is\ncompared with Mel frequency cepstral coefficients (MFCC), a staple in the\nspeech domain. The proposed classification system is also compared with the\nstate-of-the-art system based on log-filterbank energy (LFBE) features trained\non long short-term memory (LSTM) network. The proposed system based on 1D-CNN\nperforms better than, or as good as, the state-of-the-art across multiple\nexperiments. It also converges sooner, with lesser computational overhead.\nFinally, the proposed system is evaluated under the presence of white noise at\nvarious signal-to-noise ratios and found to be robust.", "published": "2024-08-25 07:17:11", "link": "http://arxiv.org/abs/2408.13746v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Wav2Small: Distilling Wav2Vec2 to 72K parameters for Low-Resource Speech\n  emotion recognition", "abstract": "Speech Emotion Recognition (SER) needs high computational resources to\novercome the challenge of substantial annotator disagreement. Today SER is\nshifting towards dimensional annotations of arousal, dominance, and valence\n(A/D/V). Universal metrics as the L2 distance prove unsuitable for evaluating\nA/D/V accuracy due to non converging consensus of annotator opinions. However,\nConcordance Correlation Coefficient (CCC) arose as an alternative metric for\nA/D/V where a model's output is evaluated to match a whole dataset's CCC rather\nthan L2 distances of individual audios. Recent studies have shown that wav2vec2\n/ wavLM architectures outputing a float value for each A/D/V dimension achieve\ntoday's State-of-the-art (Sota) CCC on A/D/V. The Wav2Vec2.0 / WavLM family has\na high computational footprint, but training small models using human\nannotations has been unsuccessful. In this paper we use a large Transformer\nSota A/D/V model as Teacher/Annotator to train 5 student models: 4 MobileNets\nand our proposed Wav2Small, using only the Teacher's A/D/V outputs instead of\nhuman annotations. The Teacher model we propose also sets a new Sota on the MSP\nPodcast dataset of valence CCC=0.676. We choose MobileNetV4 / MobileNet-V3 as\nstudents, as MobileNet has been designed for fast execution times. We also\npropose Wav2Small - an architecture designed for minimal parameters and RAM\nconsumption. Wav2Small with an .onnx (quantised) of only 120KB is a potential\nsolution for A/D/V on hardware with low resources, having only 72K parameters\nvs 3.12M parameters for MobileNet-V4-Small.", "published": "2024-08-25 19:13:56", "link": "http://arxiv.org/abs/2408.13920v4", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Analyzing the Impact of Splicing Artifacts in Partially Fake Speech\n  Signals", "abstract": "Speech deepfake detection has recently gained significant attention within\nthe multimedia forensics community. Related issues have also been explored,\nsuch as the identification of partially fake signals, i.e., tracks that include\nboth real and fake speech segments. However, generating high-quality spliced\naudio is not as straightforward as it may appear. Spliced signals are typically\ncreated through basic signal concatenation. This process could introduce\nnoticeable artifacts that can make the generated data easier to detect. We\nanalyze spliced audio tracks resulting from signal concatenation, investigate\ntheir artifacts and assess whether such artifacts introduce any bias in\nexisting datasets. Our findings reveal that by analyzing splicing artifacts, we\ncan achieve a detection EER of 6.16% and 7.36% on PartialSpoof and HAD\ndatasets, respectively, without needing to train any detector. These results\nunderscore the complexities of generating reliable spliced audio data and lead\nto discussions that can help improve future research in this area.", "published": "2024-08-25 09:28:04", "link": "http://arxiv.org/abs/2408.13784v1", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The effect of self-motion and room familiarity on sound source\n  localization in virtual environments", "abstract": "This paper investigates the influence of lateral horizontal self-motion of\nparticipants during signal presentation on distance and azimuth perception for\nfrontal sound sources in a rectangular room. Additionally, the effect of\ndeviating room acoustics for a single sound presentation embedded in a sequence\nof presentations using a baseline room acoustics for familiarization is\nanalyzed. For this purpose, two experiments were conducted using audiovisual\nvirtual reality technology with dynamic head-tracking and real-time\nauralization over headphones combined with visual rendering of the room using a\nhead-mounted display. Results show an improved distance perception accuracy\nwhen participants moved laterally during signal presentation instead of staying\nat a fixed position, with only head movements allowed. Adaptation to the room\nacoustics also improves distance perception accuracy. Azimuth perception seems\nto be independent of lateral movements during signal presentation and could\neven be negatively influenced by the familiarity of the used room acoustics.", "published": "2024-08-25 18:01:49", "link": "http://arxiv.org/abs/2408.13904v1", "categories": ["cs.SD", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Unveiling Visual Biases in Audio-Visual Localization Benchmarks", "abstract": "Audio-Visual Source Localization (AVSL) aims to localize the source of sound\nwithin a video. In this paper, we identify a significant issue in existing\nbenchmarks: the sounding objects are often easily recognized based solely on\nvisual cues, which we refer to as visual bias. Such biases hinder these\nbenchmarks from effectively evaluating AVSL models. To further validate our\nhypothesis regarding visual biases, we examine two representative AVSL\nbenchmarks, VGG-SS and EpicSounding-Object, where the vision-only models\noutperform all audiovisual baselines. Our findings suggest that existing AVSL\nbenchmarks need further refinement to facilitate audio-visual learning.", "published": "2024-08-25 04:56:08", "link": "http://arxiv.org/abs/2409.06709v1", "categories": ["cs.MM", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
