{"title": "Automatic Story Generation: Challenges and Attempts", "abstract": "The scope of this survey paper is to explore the challenges in automatic\nstory generation. We hope to contribute in the following ways: 1. Explore how\nprevious research in story generation addressed those challenges. 2. Discuss\nfuture research directions and new technologies that may aid more advancements.\n3. Shed light on emerging and often overlooked challenges such as creativity\nand discourse.", "published": "2021-02-25 02:03:35", "link": "http://arxiv.org/abs/2102.12634v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentiment Analysis of Persian-English Code-mixed Texts", "abstract": "The rapid production of data on the internet and the need to understand how\nusers are feeling from a business and research perspective has prompted the\ncreation of numerous automatic monolingual sentiment detection systems. More\nrecently however, due to the unstructured nature of data on social media, we\nare observing more instances of multilingual and code-mixed texts. This\ndevelopment in content type has created a new demand for code-mixed sentiment\nanalysis systems. In this study we collect, label and thus create a dataset of\nPersian-English code-mixed tweets. We then proceed to introduce a model which\nuses BERT pretrained embeddings as well as translation models to automatically\nlearn the polarity scores of these Tweets. Our model outperforms the baseline\nmodels that use Na\\\"ive Bayes and Random Forest methods.", "published": "2021-02-25 06:05:59", "link": "http://arxiv.org/abs/2102.12700v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IIE-NLP-Eyas at SemEval-2021 Task 4: Enhancing PLM for ReCAM with\n  Special Tokens, Re-Ranking, Siamese Encoders and Back Translation", "abstract": "This paper introduces our systems for all three subtasks of SemEval-2021 Task\n4: Reading Comprehension of Abstract Meaning. To help our model better\nrepresent and understand abstract concepts in natural language, we well-design\nmany simple and effective approaches adapted to the backbone model (RoBERTa).\nSpecifically, we formalize the subtasks into the multiple-choice question\nanswering format and add special tokens to abstract concepts, then, the final\nprediction of question answering is considered as the result of subtasks.\nAdditionally, we employ many finetuning tricks to improve the performance.\nExperimental results show that our approaches achieve significant performance\ncompared with the baseline systems. Our approaches achieve eighth rank on\nsubtask-1 and tenth rank on subtask-2.", "published": "2021-02-25 10:51:48", "link": "http://arxiv.org/abs/2102.12777v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Emotion-Aware, Emotion-Agnostic, or Automatic: Corpus Creation\n  Strategies to Obtain Cognitive Event Appraisal Annotations", "abstract": "Appraisal theories explain how the cognitive evaluation of an event leads to\na particular emotion. In contrast to theories of basic emotions or affect\n(valence/arousal), this theory has not received a lot of attention in natural\nlanguage processing. Yet, in psychology it has been proven powerful: Smith and\nEllsworth (1985) showed that the appraisal dimensions attention, certainty,\nanticipated effort, pleasantness, responsibility/control and situational\ncontrol discriminate between (at least) 15 emotion classes. We study different\nannotation strategies for these dimensions, based on the event-focused enISEAR\ncorpus (Troiano et al., 2019). We analyze two manual annotation settings: (1)\nshowing the text to annotate while masking the experienced emotion label; (2)\nrevealing the emotion associated with the text. Setting 2 enables the\nannotators to develop a more realistic intuition of the described event, while\nSetting 1 is a more standard annotation procedure, purely relying on text. We\nevaluate these strategies in two ways: by measuring inter-annotator agreement\nand by fine-tuning RoBERTa to predict appraisal variables. Our results show\nthat knowledge of the emotion increases annotators' reliability. Further, we\nevaluate a purely automatic rule-based labeling strategy (inferring appraisal\nfrom annotated emotion classes). Training on automatically assigned labels\nleads to a competitive performance of our classifier, even when tested on\nmanual annotations. This is an indicator that it might be possible to\nautomatically create appraisal corpora for every domain for which emotion\ncorpora already exist.", "published": "2021-02-25 13:55:44", "link": "http://arxiv.org/abs/2102.12858v1", "categories": ["cs.CL", "68T01", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Are pre-trained text representations useful for multilingual and\n  multi-dimensional language proficiency modeling?", "abstract": "Development of language proficiency models for non-native learners has been\nan active area of interest in NLP research for the past few years. Although\nlanguage proficiency is multidimensional in nature, existing research typically\nconsiders a single \"overall proficiency\" while building models. Further,\nexisting approaches also considers only one language at a time. This paper\ndescribes our experiments and observations about the role of pre-trained and\nfine-tuned multilingual embeddings in performing multi-dimensional,\nmultilingual language proficiency classification. We report experiments with\nthree languages -- German, Italian, and Czech -- and model seven dimensions of\nproficiency ranging from vocabulary control to sociolinguistic appropriateness.\nOur results indicate that while fine-tuned embeddings are useful for\nmultilingual proficiency modeling, none of the features achieve consistently\nbest performance for all dimensions of language proficiency. All code, data and\nrelated supplementary material can be found at:\nhttps://github.com/nishkalavallabhi/MultidimCEFRScoring.", "published": "2021-02-25 16:23:52", "link": "http://arxiv.org/abs/2102.12971v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ANEA: Distant Supervision for Low-Resource Named Entity Recognition", "abstract": "Distant supervision allows obtaining labeled training corpora for\nlow-resource settings where only limited hand-annotated data exists. However,\nto be used effectively, the distant supervision must be easy to gather. In this\nwork, we present ANEA, a tool to automatically annotate named entities in texts\nbased on entity lists. It spans the whole pipeline from obtaining the lists to\nanalyzing the errors of the distant supervision. A tuning step allows the user\nto improve the automatic annotation with their linguistic insights without\nlabelling or checking all tokens manually. In six low-resource scenarios, we\nshow that the F1-score can be increased by on average 18 points through\ndistantly supervised data obtained by ANEA.", "published": "2021-02-25 19:07:45", "link": "http://arxiv.org/abs/2102.13129v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BERT-based Acronym Disambiguation with Multiple Training Strategies", "abstract": "Acronym disambiguation (AD) task aims to find the correct expansions of an\nambiguous ancronym in a given sentence. Although it is convenient to use\nacronyms, sometimes they could be difficult to understand. Identifying the\nappropriate expansions of an acronym is a practical task in natural language\nprocessing. Since few works have been done for AD in scientific field, we\npropose a binary classification model incorporating BERT and several training\nstrategies including dynamic negative sample selection, task adaptive\npretraining, adversarial training and pseudo labeling in this paper.\nExperiments on SciAD show the effectiveness of our proposed model and our score\nranks 1st in SDU@AAAI-21 shared task 2: Acronym Disambiguation.", "published": "2021-02-25 05:40:21", "link": "http://arxiv.org/abs/2103.00488v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LET: Linguistic Knowledge Enhanced Graph Transformer for Chinese Short\n  Text Matching", "abstract": "Chinese short text matching is a fundamental task in natural language\nprocessing. Existing approaches usually take Chinese characters or words as\ninput tokens. They have two limitations: 1) Some Chinese words are polysemous,\nand semantic information is not fully utilized. 2) Some models suffer potential\nissues caused by word segmentation. Here we introduce HowNet as an external\nknowledge base and propose a Linguistic knowledge Enhanced graph Transformer\n(LET) to deal with word ambiguity. Additionally, we adopt the word lattice\ngraph as input to maintain multi-granularity information. Our model is also\ncomplementary to pre-trained language models. Experimental results on two\nChinese datasets show that our models outperform various typical text matching\napproaches. Ablation study also indicates that both semantic information and\nmulti-granularity information are important for text matching modeling.", "published": "2021-02-25 04:01:51", "link": "http://arxiv.org/abs/2102.12671v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LazyFormer: Self Attention with Lazy Update", "abstract": "Improving the efficiency of Transformer-based language pre-training is an\nimportant task in NLP, especially for the self-attention module, which is\ncomputationally expensive. In this paper, we propose a simple but effective\nsolution, called \\emph{LazyFormer}, which computes the self-attention\ndistribution infrequently. LazyFormer composes of multiple lazy blocks, each of\nwhich contains multiple Transformer layers. In each lazy block, the\nself-attention distribution is only computed once in the first layer and then\nis reused in all upper layers. In this way, the cost of computation could be\nlargely saved. We also provide several training tricks for LazyFormer.\nExtensive experiments demonstrate the effectiveness of the proposed method.", "published": "2021-02-25 06:18:20", "link": "http://arxiv.org/abs/2102.12702v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Spanish Biomedical and Clinical Language Embeddings", "abstract": "We computed both Word and Sub-word Embeddings using FastText. For Sub-word\nembeddings we selected Byte Pair Encoding (BPE) algorithm to represent the\nsub-words. We evaluated the Biomedical Word Embeddings obtaining better results\nthan previous versions showing the implication that with more data, we obtain\nbetter representations.", "published": "2021-02-25 13:30:04", "link": "http://arxiv.org/abs/2102.12843v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Automated essay scoring using efficient transformer-based language\n  models", "abstract": "Automated Essay Scoring (AES) is a cross-disciplinary effort involving\nEducation, Linguistics, and Natural Language Processing (NLP). The efficacy of\nan NLP model in AES tests it ability to evaluate long-term dependencies and\nextrapolate meaning even when text is poorly written. Large pretrained\ntransformer-based language models have dominated the current state-of-the-art\nin many NLP tasks, however, the computational requirements of these models make\nthem expensive to deploy in practice. The goal of this paper is to challenge\nthe paradigm in NLP that bigger is better when it comes to AES. To do this, we\nevaluate the performance of several fine-tuned pretrained NLP models with a\nmodest number of parameters on an AES dataset. By ensembling our models, we\nachieve excellent results with fewer parameters than most pretrained\ntransformer-based models.", "published": "2021-02-25 19:28:39", "link": "http://arxiv.org/abs/2102.13136v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PharmKE: Knowledge Extraction Platform for Pharmaceutical Texts using\n  Transfer Learning", "abstract": "The challenge of recognizing named entities in a given text has been a very\ndynamic field in recent years. This is due to the advances in neural network\narchitectures, increase of computing power and the availability of diverse\nlabeled datasets, which deliver pre-trained, highly accurate models. These\ntasks are generally focused on tagging common entities, but domain-specific\nuse-cases require tagging custom entities which are not part of the pre-trained\nmodels. This can be solved by either fine-tuning the pre-trained models, or by\ntraining custom models. The main challenge lies in obtaining reliable labeled\ntraining and test datasets, and manual labeling would be a highly tedious task.\n  In this paper we present PharmKE, a text analysis platform focused on the\npharmaceutical domain, which applies deep learning through several stages for\nthorough semantic analysis of pharmaceutical articles. It performs text\nclassification using state-of-the-art transfer learning models, and thoroughly\nintegrates the results obtained through a proposed methodology. The methodology\nis used to create accurately labeled training and test datasets, which are then\nused to train models for custom entity labeling tasks, centered on the\npharmaceutical domain. The obtained results are compared to the fine-tuned BERT\nand BioBERT models trained on the same dataset. Additionally, the PharmKE\nplatform integrates the results obtained from named entity recognition tasks to\nresolve co-references of entities and analyze the semantic relations in every\nsentence, thus setting up a baseline for additional text analysis tasks, such\nas question answering and fact extraction. The recognized entities are also\nused to expand the knowledge graph generated by DBpedia Spotlight for a given\npharmaceutical text.", "published": "2021-02-25 19:36:35", "link": "http://arxiv.org/abs/2102.13139v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Named Tensor Notation", "abstract": "We propose a notation for tensors with named axes, which relieves the author,\nreader, and future implementers of machine learning models from the burden of\nkeeping track of the order of axes and the purpose of each. The notation makes\nit easy to lift operations on low-order tensors to higher order ones, for\nexample, from images to minibatches of images, or from an attention mechanism\nto multiple attention heads.\n  After a brief overview and formal definition of the notation, we illustrate\nit through several examples from modern machine learning, from building blocks\nlike attention and convolution to full models like Transformers and LeNet. We\nthen discuss differential calculus in our notation and compare with some\nalternative notations. Our proposals build on ideas from many previous papers\nand software libraries. We hope that our notation will encourage more authors\nto use named tensors, resulting in clearer papers and more precise\nimplementations.", "published": "2021-02-25 22:21:30", "link": "http://arxiv.org/abs/2102.13196v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "MixSpeech: Data Augmentation for Low-resource Automatic Speech\n  Recognition", "abstract": "In this paper, we propose MixSpeech, a simple yet effective data augmentation\nmethod based on mixup for automatic speech recognition (ASR). MixSpeech trains\nan ASR model by taking a weighted combination of two different speech features\n(e.g., mel-spectrograms or MFCC) as the input, and recognizing both text\nsequences, where the two recognition losses use the same combination weight. We\napply MixSpeech on two popular end-to-end speech recognition models including\nLAS (Listen, Attend and Spell) and Transformer, and conduct experiments on\nseveral low-resource datasets including TIMIT, WSJ, and HKUST. Experimental\nresults show that MixSpeech achieves better accuracy than the baseline models\nwithout data augmentation, and outperforms a strong data augmentation method\nSpecAugment on these recognition tasks. Specifically, MixSpeech outperforms\nSpecAugment with a relative PER improvement of 10.6$\\%$ on TIMIT dataset, and\nachieves a strong WER of 4.7$\\%$ on WSJ dataset.", "published": "2021-02-25 03:40:43", "link": "http://arxiv.org/abs/2102.12664v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Cognitive network science for understanding online social cognitions: A\n  brief review", "abstract": "Social media are digitalising massive amounts of users' cognitions in terms\nof timelines and emotional content. Such Big Data opens unprecedented\nopportunities for investigating cognitive phenomena like perception,\npersonality and information diffusion but requires suitable interpretable\nframeworks. Since social media data come from users' minds, worthy candidates\nfor this challenge are cognitive networks, models of cognition giving structure\nto mental conceptual associations. This work outlines how cognitive network\nscience can open new, quantitative ways for understanding cognition through\nonline media, like: (i) reconstructing how users semantically and emotionally\nframe events with contextual knowledge unavailable to machine learning, (ii)\ninvestigating conceptual salience/prominence through knowledge structure in\nsocial discourse; (iii) studying users' personality traits like\nopenness-to-experience, curiosity, and creativity through language in posts;\n(iv) bridging cognitive/emotional content and social dynamics via multilayer\nnetworks comparing the mindsets of influencers and followers. These\nadvancements combine cognitive-, network- and computer science to understand\ncognitive mechanisms in both digital and real-world settings but come with\nlimitations concerning representativeness, individual variability and data\nintegration. Such aspects are discussed along the ethical implications of\nmanipulating socio-cognitive data. In the future, reading cognitions through\nnetworks and social media can expose cognitive biases amplified by online\nplatforms and relevantly inform policy making, education and markets about\nmassive, complex cognitive trends.", "published": "2021-02-25 11:53:28", "link": "http://arxiv.org/abs/2102.12799v1", "categories": ["cs.CY", "cs.CL", "cs.SI", "physics.soc-ph"], "primary_category": "cs.CY"}
{"title": "ZJUKLAB at SemEval-2021 Task 4: Negative Augmentation with Language\n  Model for Reading Comprehension of Abstract Meaning", "abstract": "This paper presents our systems for the three Subtasks of SemEval Task4:\nReading Comprehension of Abstract Meaning (ReCAM). We explain the algorithms\nused to learn our models and the process of tuning the algorithms and selecting\nthe best model. Inspired by the similarity of the ReCAM task and the language\npre-training, we propose a simple yet effective technology, namely, negative\naugmentation with language model. Evaluation results demonstrate the\neffectiveness of our proposed approach. Our models achieve the 4th rank on both\nofficial test sets of Subtask 1 and Subtask 2 with an accuracy of 87.9% and an\naccuracy of 92.8%, respectively. We further conduct comprehensive model\nanalysis and observe interesting error cases, which may promote future\nresearches.", "published": "2021-02-25 13:03:05", "link": "http://arxiv.org/abs/2102.12828v3", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "QNLP in Practice: Running Compositional Models of Meaning on a Quantum\n  Computer", "abstract": "Quantum Natural Language Processing (QNLP) deals with the design and\nimplementation of NLP models intended to be run on quantum hardware. In this\npaper, we present results on the first NLP experiments conducted on Noisy\nIntermediate-Scale Quantum (NISQ) computers for datasets of size greater than\n100 sentences. Exploiting the formal similarity of the compositional model of\nmeaning by Coecke, Sadrzadeh and Clark (2010) with quantum theory, we create\nrepresentations for sentences that have a natural mapping to quantum circuits.\nWe use these representations to implement and successfully train NLP models\nthat solve simple sentence classification tasks on quantum hardware. We conduct\nquantum simulations that compare the syntax-sensitive model of Coecke et al.\nwith two baselines that use less or no syntax; specifically, we implement the\nquantum analogues of a \"bag-of-words\" model, where syntax is not taken into\naccount at all, and of a word-sequence model, where only word order is\nrespected. We demonstrate that all models converge smoothly both in simulations\nand when run on quantum hardware, and that the results are the expected ones\nbased on the nature of the tasks and the datasets used. Another important goal\nof this paper is to describe in a way accessible to AI and NLP researchers the\nmain principles, process and challenges of experiments on quantum hardware. Our\naim in doing this is to take the first small steps in this unexplored research\nterritory and pave the way for practical Quantum Natural Language Processing.", "published": "2021-02-25 13:37:33", "link": "http://arxiv.org/abs/2102.12846v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "quant-ph"], "primary_category": "cs.CL"}
{"title": "A Primer on Contrastive Pretraining in Language Processing: Methods,\n  Lessons Learned and Perspectives", "abstract": "Modern natural language processing (NLP) methods employ self-supervised\npretraining objectives such as masked language modeling to boost the\nperformance of various application tasks. These pretraining methods are\nfrequently extended with recurrence, adversarial or linguistic property\nmasking, and more recently with contrastive learning objectives. Contrastive\nself-supervised training objectives enabled recent successes in image\nrepresentation pretraining by learning to contrast input-input pairs of\naugmented images as either similar or dissimilar. However, in NLP, automated\ncreation of text input augmentations is still very challenging because a single\ntoken can invert the meaning of a sentence. For this reason, some contrastive\nNLP pretraining methods contrast over input-label pairs, rather than over\ninput-input pairs, using methods from Metric Learning and Energy Based Models.\nIn this survey, we summarize recent self-supervised and supervised contrastive\nNLP pretraining methods and describe where they are used to improve language\nmodeling, few or zero-shot learning, pretraining data-efficiency and specific\nNLP end-tasks. We introduce key contrastive learning concepts with lessons\nlearned from prior research and structure works by applications and cross-field\nrelations. Finally, we point to open challenges and future directions for\ncontrastive NLP to encourage bringing contrastive NLP pretraining closer to\nrecent successes in image representation pretraining.", "published": "2021-02-25 16:35:07", "link": "http://arxiv.org/abs/2102.12982v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Investigating the Limitations of Transformers with Simple Arithmetic\n  Tasks", "abstract": "The ability to perform arithmetic tasks is a remarkable trait of human\nintelligence and might form a critical component of more complex reasoning\ntasks. In this work, we investigate if the surface form of a number has any\ninfluence on how sequence-to-sequence language models learn simple arithmetic\ntasks such as addition and subtraction across a wide range of values. We find\nthat how a number is represented in its surface form has a strong influence on\nthe model's accuracy. In particular, the model fails to learn addition of\nfive-digit numbers when using subwords (e.g., \"32\"), and it struggles to learn\nwith character-level representations (e.g., \"3 2\"). By introducing position\ntokens (e.g., \"3 10e1 2\"), the model learns to accurately add and subtract\nnumbers up to 60 digits. We conclude that modern pretrained language models can\neasily learn arithmetic from very few examples, as long as we use the proper\nsurface representation. This result bolsters evidence that subword tokenizers\nand positional encodings are components in current transformer designs that\nmight need improvement. Moreover, we show that regardless of the number of\nparameters and training examples, models cannot learn addition rules that are\nindependent of the length of the numbers seen during training. Code to\nreproduce our experiments is available at\nhttps://github.com/castorini/transformers-arithmetic", "published": "2021-02-25 17:22:53", "link": "http://arxiv.org/abs/2102.13019v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Retrieval Augmentation for Deep Neural Networks", "abstract": "Deep neural networks have achieved state-of-the-art results in various vision\nand/or language tasks. Despite the use of large training datasets, most models\nare trained by iterating over single input-output pairs, discarding the\nremaining examples for the current prediction. In this work, we actively\nexploit the training data, using the information from nearest training examples\nto aid the prediction both during training and testing. Specifically, our\napproach uses the target of the most similar training example to initialize the\nmemory state of an LSTM model, or to guide attention mechanisms. We apply this\napproach to image captioning and sentiment analysis, respectively through image\nand text retrieval. Results confirm the effectiveness of the proposed approach\nfor the two tasks, on the widely used Flickr8 and IMDB datasets. Our code is\npublicly available at http://github.com/RitaRamo/retrieval-augmentation-nn.", "published": "2021-02-25 17:38:31", "link": "http://arxiv.org/abs/2102.13030v2", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Inductive biases, pretraining and fine-tuning jointly account for brain\n  responses to speech", "abstract": "Our ability to comprehend speech remains, to date, unrivaled by deep learning\nmodels. This feat could result from the brain's ability to fine-tune generic\nsound representations for speech-specific processes. To test this hypothesis,\nwe compare i) five types of deep neural networks to ii) human brain responses\nelicited by spoken sentences and recorded in 102 Dutch subjects using\nfunctional Magnetic Resonance Imaging (fMRI). Each network was either trained\non an acoustics scene classification, a speech-to-text task (based on Bengali,\nEnglish, or Dutch), or not trained. The similarity between each model and the\nbrain is assessed by correlating their respective activations after an optimal\nlinear projection. The differences in brain-similarity across networks revealed\nthree main results. First, speech representations in the brain can be accounted\nfor by random deep networks. Second, learning to classify acoustic scenes leads\ndeep nets to increase their brain similarity. Third, learning to process\nphonetically-related speech inputs (i.e., Dutch vs English) leads deep nets to\nreach higher levels of brain-similarity than learning to process\nphonetically-distant speech inputs (i.e. Dutch vs Bengali). Together, these\nresults suggest that the human brain fine-tunes its heavily-trained auditory\nhierarchy to learn to process speech.", "published": "2021-02-25 19:11:55", "link": "http://arxiv.org/abs/2103.01032v1", "categories": ["cs.CL", "cs.SD", "eess.AS", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "Meta-Learning for improving rare word recognition in end-to-end ASR", "abstract": "We propose a new method of generating meaningful embeddings for speech,\nchanges to four commonly used meta learning approaches to enable them to\nperform keyword spotting in continuous signals and an approach of combining\ntheir outcomes into an end-to-end automatic speech recognition system to\nimprove rare word recognition. We verify the functionality of each of our three\ncontributions in two experiments exploring their performance for different\namounts of classes (N-way) and examples per class (k-shot) in a few-shot\nsetting. We find that the speech embeddings work well and the changes to the\nmeta learning approaches also clearly enable them to perform continuous signal\nspotting. Despite the interface between keyword spotting and speech recognition\nbeing very simple, we are able to consistently improve word error rate by up to\n5%.", "published": "2021-02-25 01:40:28", "link": "http://arxiv.org/abs/2102.12624v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Automatic Classification of OSA related Snoring Signals from Nocturnal\n  Audio Recordings", "abstract": "In this study, the development of an automatic algorithm is presented to\nclassify the nocturnal audio recording of an obstructive sleep apnoea (OSA)\npatient as OSA related snore, simple snore and other sounds. Recent studies has\nbeen shown that knowledge regarding the OSA related snore could assist in\nidentifying the site of airway collapse. Audio signal was recorded\nsimultaneously with full-night polysomnography during sleep with a ceiling\nmicrophone. Time and frequency features of the nocturnal audio signal were\nextracted to classify the audio signal into OSA related snore, simple snore and\nother sounds. Two algorithms were developed to extract OSA related snore using\nan linear discriminant analysis (LDA) classifier based on the hypothesis that\nOSA related snoring can assist in identifying the site-of-upper airway\ncollapse. An unbiased nested leave-one patient-out cross-validation process was\nused to select a high performing feature set from the full set of features.\nResults indicated that the algorithm achieved an accuracy of 87% for\nidentifying snore events from the audio recordings and an accuracy of 72% for\nidentifying OSA related snore events from the snore events. The direct method\nto extract OSA-related snore events using a multi-class LDA classifier achieved\nan accuracy of 64% using the feature selection algorithm. Our results gives a\nclear indication that OSA-related snore events can be extracted from nocturnal\nsound recordings, and therefore could potentially be used as a new tool for\nidentifying the site of airway collapse from the nocturnal audio recordings.", "published": "2021-02-25 13:04:30", "link": "http://arxiv.org/abs/2102.12829v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MaskCycleGAN-VC: Learning Non-parallel Voice Conversion with Filling in\n  Frames", "abstract": "Non-parallel voice conversion (VC) is a technique for training voice\nconverters without a parallel corpus. Cycle-consistent adversarial\nnetwork-based VCs (CycleGAN-VC and CycleGAN-VC2) are widely accepted as\nbenchmark methods. However, owing to their insufficient ability to grasp\ntime-frequency structures, their application is limited to mel-cepstrum\nconversion and not mel-spectrogram conversion despite recent advances in\nmel-spectrogram vocoders. To overcome this, CycleGAN-VC3, an improved variant\nof CycleGAN-VC2 that incorporates an additional module called time-frequency\nadaptive normalization (TFAN), has been proposed. However, an increase in the\nnumber of learned parameters is imposed. As an alternative, we propose\nMaskCycleGAN-VC, which is another extension of CycleGAN-VC2 and is trained\nusing a novel auxiliary task called filling in frames (FIF). With FIF, we apply\na temporal mask to the input mel-spectrogram and encourage the converter to\nfill in missing frames based on surrounding frames. This task allows the\nconverter to learn time-frequency structures in a self-supervised manner and\neliminates the need for an additional module such as TFAN. A subjective\nevaluation of the naturalness and speaker similarity showed that\nMaskCycleGAN-VC outperformed both CycleGAN-VC2 and CycleGAN-VC3 with a model\nsize similar to that of CycleGAN-VC2. Audio samples are available at\nhttp://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/maskcyclegan-vc/index.html.", "published": "2021-02-25 13:26:58", "link": "http://arxiv.org/abs/2102.12841v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Deepfakes Generation and Detection: State-of-the-art, open challenges,\n  countermeasures, and way forward", "abstract": "Easy access to audio-visual content on social media, combined with the\navailability of modern tools such as Tensorflow or Keras, open-source trained\nmodels, and economical computing infrastructure, and the rapid evolution of\ndeep-learning (DL) methods, especially Generative Adversarial Networks (GAN),\nhave made it possible to generate deepfakes to disseminate disinformation,\nrevenge porn, financial frauds, hoaxes, and to disrupt government functioning.\nThe existing surveys have mainly focused on the detection of deepfake images\nand videos. This paper provides a comprehensive review and detailed analysis of\nexisting tools and machine learning (ML) based approaches for deepfake\ngeneration and the methodologies used to detect such manipulations for both\naudio and visual deepfakes. For each category of deepfake, we discuss\ninformation related to manipulation approaches, current public datasets, and\nkey standards for the performance evaluation of deepfake detection techniques\nalong with their results. Additionally, we also discuss open challenges and\nenumerate future directions to guide future researchers on issues that need to\nbe considered to improve the domains of both deepfake generation and detection.\nThis work is expected to assist the readers in understanding the creation and\ndetection mechanisms of deepfakes, along with their current limitations and\nfuture direction.", "published": "2021-02-25 18:26:50", "link": "http://arxiv.org/abs/2103.00484v2", "categories": ["cs.CR", "cs.LG", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.CR"}
