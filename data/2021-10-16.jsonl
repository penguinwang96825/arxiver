{"title": "Multilingual unsupervised sequence segmentation transfers to extremely\n  low-resource languages", "abstract": "We show that unsupervised sequence-segmentation performance can be\ntransferred to extremely low-resource languages by pre-training a Masked\nSegmental Language Model (Downey et al., 2021) multilingually. Further, we show\nthat this transfer can be achieved by training over a collection of\nlow-resource languages that are typologically similar (but phylogenetically\nunrelated) to the target language. In our experiments, we transfer from a\ncollection of 10 Indigenous American languages (AmericasNLP, Mager et al.,\n2021) to K'iche', a Mayan language. We compare our multilingual model to a\nmonolingual (from-scratch) baseline, as well as a model pre-trained on Quechua\nonly. We show that the multilingual pre-trained approach yields consistent\nsegmentation quality across target dataset sizes, exceeding the monolingual\nbaseline in 6/10 experimental settings. Our model yields especially strong\nresults at small target sizes, including a zero-shot performance of 20.6 F1.\nThese results have promising implications for low-resource NLP pipelines\ninvolving human-like linguistic units, such as the sparse transcription\nframework proposed by Bird (2020).", "published": "2021-10-16 00:08:28", "link": "http://arxiv.org/abs/2110.08415v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EncT5: A Framework for Fine-tuning T5 as Non-autoregressive Models", "abstract": "Pre-trained encoder-decoder transformer architectures have become\nincreasingly popular recently with the advent of T5 models. T5 has also become\nmore favorable over other architectures like BERT due to the amount of data\nthat it is pre-trained on, increased scale of model parameter sizes and easy\napplicability to a diverse set of tasks due to the generative nature of the\nmodel. While being able to generalize to a wide variety of tasks, it is not\nclear that encoder-decoder architectures are the most efficient for fine-tuning\ntasks that don't require auto-regressive decoding. In this work, we study\nfine-tuning pre-trained encoder-decoder models for tasks such as\nclassification, multi-label classification, and structured prediction. We\npropose \\textbf{EncT5}, a framework for these problems, and illustrate\ninstantiations for these tasks. Our experiment results show that EncT5 has\nadvantages over T5 such as efficiency and usability out performs BERT when\nevaluated on publicly available pre-trained checkpoints.", "published": "2021-10-16 00:50:08", "link": "http://arxiv.org/abs/2110.08426v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prix-LM: Pretraining for Multilingual Knowledge Base Construction", "abstract": "Knowledge bases (KBs) contain plenty of structured world and commonsense\nknowledge. As such, they often complement distributional text-based information\nand facilitate various downstream tasks. Since their manual construction is\nresource- and time-intensive, recent efforts have tried leveraging large\npretrained language models (PLMs) to generate additional monolingual knowledge\nfacts for KBs. However, such methods have not been attempted for building and\nenriching multilingual KBs. Besides wider application, such multilingual KBs\ncan provide richer combined knowledge than monolingual (e.g., English) KBs.\nKnowledge expressed in different languages may be complementary and unequally\ndistributed: this implies that the knowledge available in high-resource\nlanguages can be transferred to low-resource ones. To achieve this, it is\ncrucial to represent multilingual knowledge in a shared/unified space. To this\nend, we propose a unified representation model, Prix-LM, for multilingual KB\nconstruction and completion. We leverage two types of knowledge, monolingual\ntriples and cross-lingual links, extracted from existing multilingual KBs, and\ntune a multilingual language encoder XLM-R via a causal language modeling\nobjective. Prix-LM integrates useful multilingual and KB-based factual\nknowledge into a single model. Experiments on standard entity-related tasks,\nsuch as link prediction in multiple languages, cross-lingual entity linking and\nbilingual lexicon induction, demonstrate its effectiveness, with gains reported\nover strong task-specialised baselines.", "published": "2021-10-16 02:08:46", "link": "http://arxiv.org/abs/2110.08443v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Well Do You Know Your Audience? Toward Socially-aware Question\n  Generation", "abstract": "When writing, a person may need to anticipate questions from their audience,\nbut different social groups may ask very different types of questions. If\nsomeone is writing about a problem they want to resolve, what kind of follow-up\nquestion will a domain expert ask, and could the writer better address the\nexpert's information needs by rewriting their original post? In this paper, we\nexplore the task of socially-aware question generation. We collect a data set\nof questions and posts from social media, including background information\nabout the question-askers' social groups. We find that different social groups,\nsuch as experts and novices, consistently ask different types of questions. We\ntrain several text-generation models that incorporate social information, and\nwe find that a discrete social-representation model outperforms the text-only\nmodel when different social groups ask highly different questions from one\nanother. Our work provides a framework for developing text generation models\nthat can help writers anticipate the information expectations of highly\ndifferent social groups.", "published": "2021-10-16 02:10:16", "link": "http://arxiv.org/abs/2110.08445v2", "categories": ["cs.CL", "I.7"], "primary_category": "cs.CL"}
{"title": "Good Examples Make A Faster Learner: Simple Demonstration-based Learning\n  for Low-resource NER", "abstract": "Recent advances in prompt-based learning have shown strong results on\nfew-shot text classification by using cloze-style templates. Similar attempts\nhave been made on named entity recognition (NER) which manually design\ntemplates to predict entity types for every text span in a sentence. However,\nsuch methods may suffer from error propagation induced by entity span\ndetection, high cost due to enumeration of all possible text spans, and\nomission of inter-dependencies among token labels in a sentence. Here we\npresent a simple demonstration-based learning method for NER, which lets the\ninput be prefaced by task demonstrations for in-context learning. We perform a\nsystematic study on demonstration strategy regarding what to include (entity\nexamples, with or without surrounding context), how to select the examples, and\nwhat templates to use. Results on in-domain learning and domain adaptation show\nthat the model's performance in low-resource settings can be largely improved\nwith a suitable demonstration strategy (e.g., a 4-17% improvement on 25 train\ninstances). We also find that good demonstration can save many labeled examples\nand consistency in demonstration contributes to better performance.", "published": "2021-10-16 03:24:44", "link": "http://arxiv.org/abs/2110.08454v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge Enhanced Pretrained Language Models: A Compreshensive Survey", "abstract": "Pretrained Language Models (PLM) have established a new paradigm through\nlearning informative contextualized representations on large-scale text corpus.\nThis new paradigm has revolutionized the entire field of natural language\nprocessing, and set the new state-of-the-art performance for a wide variety of\nNLP tasks. However, though PLMs could store certain knowledge/facts from\ntraining corpus, their knowledge awareness is still far from satisfactory. To\naddress this issue, integrating knowledge into PLMs have recently become a very\nactive research area and a variety of approaches have been developed. In this\npaper, we provide a comprehensive survey of the literature on this emerging and\nfast-growing field - Knowledge Enhanced Pretrained Language Models (KE-PLMs).\nWe introduce three taxonomies to categorize existing work. Besides, we also\nsurvey the various NLU and NLG applications on which KE-PLM has demonstrated\nsuperior performance over vanilla PLMs. Finally, we discuss challenges that\nface KE-PLMs and also promising directions for future research.", "published": "2021-10-16 03:27:56", "link": "http://arxiv.org/abs/2110.08455v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Controllable Semantic Parsing via Retrieval Augmentation", "abstract": "In practical applications of semantic parsing, we often want to rapidly\nchange the behavior of the parser, such as enabling it to handle queries in a\nnew domain, or changing its predictions on certain targeted queries. While we\ncan introduce new training examples exhibiting the target behavior, a mechanism\nfor enacting such behavior changes without expensive model re-training would be\npreferable. To this end, we propose ControllAble Semantic Parser via Exemplar\nRetrieval (CASPER). Given an input query, the parser retrieves related\nexemplars from a retrieval index, augments them to the query, and then applies\na generative seq2seq model to produce an output parse. The exemplars act as a\ncontrol mechanism over the generic generative model: by manipulating the\nretrieval index or how the augmented query is constructed, we can manipulate\nthe behavior of the parser. On the MTOP dataset, in addition to achieving\nstate-of-the-art on the standard setup, we show that CASPER can parse queries\nin a new domain, adapt the prediction toward the specified patterns, or adapt\nto new semantic schemas without having to further re-train the model.", "published": "2021-10-16 03:34:49", "link": "http://arxiv.org/abs/2110.08458v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Short Study on Compressing Decoder-Based Language Models", "abstract": "Pre-trained Language Models (PLMs) have been successful for a wide range of\nnatural language processing (NLP) tasks. The state-of-the-art of PLMs, however,\nare extremely large to be used on edge devices. As a result, the topic of model\ncompression has attracted increasing attention in the NLP community. Most of\nthe existing works focus on compressing encoder-based models (tiny-BERT,\ndistilBERT, distilRoBERTa, etc), however, to the best of our knowledge, the\ncompression of decoder-based models (such as GPT-2) has not been investigated\nmuch. Our paper aims to fill this gap. Specifically, we explore two directions:\n1) we employ current state-of-the-art knowledge distillation techniques to\nimprove fine-tuning of DistilGPT-2. 2) we pre-train a compressed GPT-2 model\nusing layer truncation and compare it against the distillation-based method\n(DistilGPT2). The training time of our compressed model is significantly less\nthan DistilGPT-2, but it can achieve better performance when fine-tuned on\ndownstream tasks. We also demonstrate the impact of data cleaning on model\nperformance.", "published": "2021-10-16 03:37:08", "link": "http://arxiv.org/abs/2110.08460v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Knowledge in Multilingual Commonsense Reasoning", "abstract": "Commonsense reasoning (CSR) requires the model to be equipped with general\nworld knowledge. While CSR is a language-agnostic process, most comprehensive\nknowledge sources are in few popular languages, especially English. Thus, it\nremains unclear how to effectively conduct multilingual commonsense reasoning\n(XCSR) for various languages. In this work, we propose to utilize English\nknowledge sources via a translate-retrieve-translate (TRT) strategy. For\nmultilingual commonsense questions and choices, we collect related knowledge\nvia translation and retrieval from the knowledge sources. The retrieved\nknowledge is then translated into the target language and integrated into a\npre-trained multilingual language model via visible knowledge attention. Then\nwe utilize a diverse of 4 English knowledge sources to provide more\ncomprehensive coverage of knowledge in different formats. Extensive results on\nthe XCSR benchmark demonstrate that TRT with external knowledge can\nsignificantly improve multilingual commonsense reasoning in both zero-shot and\ntranslate-train settings, outperforming 3.3 and 3.6 points over the previous\nstate-of-the-art on XCSR benchmark datasets (X-CSQA and X-CODAH).", "published": "2021-10-16 03:51:53", "link": "http://arxiv.org/abs/2110.08462v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Seeking Patterns, Not just Memorizing Procedures: Contrastive Learning\n  for Solving Math Word Problems", "abstract": "Math Word Problem (MWP) solving needs to discover the quantitative\nrelationships over natural language narratives. Recent work shows that existing\nmodels memorize procedures from context and rely on shallow heuristics to solve\nMWPs. In this paper, we look at this issue and argue that the cause is a lack\nof overall understanding of MWP patterns. We first investigate how a neural\nnetwork understands patterns only from semantics, and observe that, if the\nprototype equations are the same, most problems get closer representations and\nthose representations apart from them or close to other prototypes tend to\nproduce wrong solutions. Inspired by it, we propose a contrastive learning\napproach, where the neural network perceives the divergence of patterns. We\ncollect contrastive examples by converting the prototype equation into a tree\nand seeking similar tree structures. The solving model is trained with an\nauxiliary objective on the collected examples, resulting in the representations\nof problems with similar prototypes being pulled closer. We conduct experiments\non the Chinese dataset Math23k and the English dataset MathQA. Our method\ngreatly improves the performance in monolingual and multilingual settings.", "published": "2021-10-16 04:03:47", "link": "http://arxiv.org/abs/2110.08464v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Safety of Conversational Models: Taxonomy, Dataset, and Benchmark", "abstract": "Dialogue safety problems severely limit the real-world deployment of neural\nconversational models and have attracted great research interests recently.\nHowever, dialogue safety problems remain under-defined and the corresponding\ndataset is scarce. We propose a taxonomy for dialogue safety specifically\ndesigned to capture unsafe behaviors in human-bot dialogue settings, with\nfocuses on context-sensitive unsafety, which is under-explored in prior works.\nTo spur research in this direction, we compile DiaSafety, a dataset with rich\ncontext-sensitive unsafe examples. Experiments show that existing safety\nguarding tools fail severely on our dataset. As a remedy, we train a dialogue\nsafety classifier to provide a strong baseline for context-sensitive dialogue\nunsafety detection. With our classifier, we perform safety evaluations on\npopular conversational models and show that existing dialogue systems still\nexhibit concerning context-sensitive safety problems.", "published": "2021-10-16 04:17:12", "link": "http://arxiv.org/abs/2110.08466v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PRIMERA: Pyramid-based Masked Sentence Pre-training for Multi-document\n  Summarization", "abstract": "We introduce PRIMERA, a pre-trained model for multi-document representation\nwith a focus on summarization that reduces the need for dataset-specific\narchitectures and large amounts of fine-tuning labeled data. PRIMERA uses our\nnewly proposed pre-training objective designed to teach the model to connect\nand aggregate information across documents. It also uses efficient\nencoder-decoder transformers to simplify the processing of concatenated input\ndocuments. With extensive experiments on 6 multi-document summarization\ndatasets from 3 different domains on zero-shot, few-shot and full-supervised\nsettings, PRIMERA outperforms current state-of-the-art dataset-specific and\npre-trained models on most of these settings with large margins. The code and\npre-trained models can be found at \\url{https://github.com/allenai/PRIMER}.", "published": "2021-10-16 07:22:24", "link": "http://arxiv.org/abs/2110.08499v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Think Before You Speak: Explicitly Generating Implicit Commonsense\n  Knowledge for Response Generation", "abstract": "Implicit knowledge, such as common sense, is key to fluid human\nconversations. Current neural response generation (RG) models are trained to\ngenerate responses directly, omitting unstated implicit knowledge. In this\npaper, we present Think-Before-Speaking (TBS), a generative approach to first\nexternalize implicit commonsense knowledge (think) and use this knowledge to\ngenerate responses (speak). We expect that externalizing implicit knowledge\nallows more efficient learning, produces more informative responses, and\nenables more explainable models. We analyze different choices to collect\nknowledge-aligned dialogues, represent implicit knowledge, and transition\nbetween knowledge and dialogues. Empirical results show TBS models outperform\nend-to-end and knowledge-augmented RG baselines on most automatic metrics and\ngenerate more informative, specific, and commonsense-following responses, as\nevaluated by human annotators. TBS also generates knowledge that makes sense\nand is relevant to the dialogue around 85\\% of the time.", "published": "2021-10-16 07:27:12", "link": "http://arxiv.org/abs/2110.08501v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MarkupLM: Pre-training of Text and Markup Language for Visually-rich\n  Document Understanding", "abstract": "Multimodal pre-training with text, layout, and image has made significant\nprogress for Visually Rich Document Understanding (VRDU), especially the\nfixed-layout documents such as scanned document images. While, there are still\na large number of digital documents where the layout information is not fixed\nand needs to be interactively and dynamically rendered for visualization,\nmaking existing layout-based pre-training approaches not easy to apply. In this\npaper, we propose MarkupLM for document understanding tasks with markup\nlanguages as the backbone, such as HTML/XML-based documents, where text and\nmarkup information is jointly pre-trained. Experiment results show that the\npre-trained MarkupLM significantly outperforms the existing strong baseline\nmodels on several document understanding tasks. The pre-trained model and code\nwill be publicly available at https://aka.ms/markuplm.", "published": "2021-10-16 09:17:28", "link": "http://arxiv.org/abs/2110.08518v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DISAPERE: A Dataset for Discourse Structure in Peer Review Discussions", "abstract": "At the foundation of scientific evaluation is the labor-intensive process of\npeer review. This critical task requires participants to consume vast amounts\nof highly technical text. Prior work has annotated different aspects of review\nargumentation, but discourse relations between reviews and rebuttals have yet\nto be examined. We present DISAPERE, a labeled dataset of 20k sentences\ncontained in 506 review-rebuttal pairs in English, annotated by experts.\nDISAPERE synthesizes label sets from prior work and extends them to include\nfine-grained annotation of the rebuttal sentences, characterizing their context\nin the review and the authors' stance towards review arguments. Further, we\nannotate every review and rebuttal sentence. We show that discourse cues from\nrebuttals can shed light on the quality and interpretation of reviews. Further,\nan understanding of the argumentative strategies employed by the reviewers and\nauthors provides useful signal for area chairs and other decision makers.", "published": "2021-10-16 09:18:12", "link": "http://arxiv.org/abs/2110.08520v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Power of Prompt Tuning for Low-Resource Semantic Parsing", "abstract": "Prompt tuning has recently emerged as an effective method for adapting\npre-trained language models to a number of language understanding and\ngeneration tasks. In this paper, we investigate prompt tuning for semantic\nparsing -- the task of mapping natural language utterances onto formal meaning\nrepresentations. On the low-resource splits of Overnight and TOPv2, we find\nthat a prompt tuned T5-xl significantly outperforms its fine-tuned counterpart,\nas well as strong GPT-3 and BART baselines. We also conduct ablation studies\nacross different model scales and target representations, finding that, with\nincreasing model scale, prompt tuned T5 models improve at generating target\nrepresentations that are far from the pre-training distribution.", "published": "2021-10-16 09:33:09", "link": "http://arxiv.org/abs/2110.08525v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pro-KD: Progressive Distillation by Following the Footsteps of the\n  Teacher", "abstract": "With ever growing scale of neural models, knowledge distillation (KD)\nattracts more attention as a prominent tool for neural model compression.\nHowever, there are counter intuitive observations in the literature showing\nsome challenging limitations of KD. A case in point is that the best performing\ncheckpoint of the teacher might not necessarily be the best teacher for\ntraining the student in KD. Therefore, one important question would be how to\nfind the best checkpoint of the teacher for distillation? Searching through the\ncheckpoints of the teacher would be a very tedious and computationally\nexpensive process, which we refer to as the \\textit{checkpoint-search problem}.\nMoreover, another observation is that larger teachers might not necessarily be\nbetter teachers in KD which is referred to as the \\textit{capacity-gap}\nproblem. To address these challenging problems, in this work, we introduce our\nprogressive knowledge distillation (Pro-KD) technique which defines a smoother\ntraining path for the student by following the training footprints of the\nteacher instead of solely relying on distilling from a single mature\nfully-trained teacher. We demonstrate that our technique is quite effective in\nmitigating the capacity-gap problem and the checkpoint search problem. We\nevaluate our technique using a comprehensive set of experiments on different\ntasks such as image classification (CIFAR-10 and CIFAR-100), natural language\nunderstanding tasks of the GLUE benchmark, and question answering (SQuAD 1.1\nand 2.0) using BERT-based models and consistently got superior results over\nstate-of-the-art techniques.", "published": "2021-10-16 09:49:43", "link": "http://arxiv.org/abs/2110.08532v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lifelong Pretraining: Continually Adapting Language Models to Emerging\n  Corpora", "abstract": "Pretrained language models (PTLMs) are typically learned over a large, static\ncorpus and further fine-tuned for various downstream tasks. However, when\ndeployed in the real world, a PTLM-based model must deal with data\ndistributions that deviate from what the PTLM was initially trained on. In this\npaper, we study a lifelong language model pretraining challenge where a PTLM is\ncontinually updated so as to adapt to emerging data. Over a domain-incremental\nresearch paper stream and a chronologically-ordered tweet stream, we\nincrementally pretrain a PTLM with different continual learning algorithms, and\nkeep track of the downstream task performance (after fine-tuning). We evaluate\nPTLM's ability to adapt to new corpora while retaining learned knowledge in\nearlier corpora. Our experiments show distillation-based approaches to be most\neffective in retaining downstream performance in earlier domains. The\nalgorithms also improve knowledge transfer, allowing models to achieve better\ndownstream performance over the latest data, and improve temporal\ngeneralization when distribution gaps exist between training and evaluation\nbecause of time. We believe our problem formulation, methods, and analysis will\ninspire future studies towards continual pretraining of language models.", "published": "2021-10-16 09:59:33", "link": "http://arxiv.org/abs/2110.08534v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Substructure Distribution Projection for Zero-Shot Cross-Lingual\n  Dependency Parsing", "abstract": "We present substructure distribution projection (SubDP), a technique that\nprojects a distribution over structures in one domain to another, by projecting\nsubstructure distributions separately. Models for the target domains can be\nthen trained, using the projected distributions as soft silver labels. We\nevaluate SubDP on zero-shot cross-lingual dependency parsing, taking dependency\narcs as substructures: we project the predicted dependency arc distributions in\nthe source language(s) to target language(s), and train a target language\nparser to fit the resulting distributions. When an English treebank is the only\nannotation that involves human effort, SubDP achieves better unlabeled\nattachment score than all prior work on the Universal Dependencies v2.2 (Nivre\net al., 2020) test set across eight diverse target languages, as well as the\nbest labeled attachment score on six out of eight languages. In addition, SubDP\nimproves zero-shot cross-lingual dependency parsing with very few (e.g., 50)\nsupervised bitext pairs, across a broader range of target languages.", "published": "2021-10-16 10:12:28", "link": "http://arxiv.org/abs/2110.08538v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hey AI, Can You Solve Complex Tasks by Talking to Agents?", "abstract": "Training giant models from scratch for each complex task is resource- and\ndata-inefficient. To help develop models that can leverage existing systems, we\npropose a new challenge: Learning to solve complex tasks by communicating with\nexisting agents (or models) in natural language. We design a synthetic\nbenchmark, CommaQA, with three complex reasoning tasks (explicit, implicit,\nnumeric) designed to be solved by communicating with existing QA agents. For\ninstance, using text and table QA agents to answer questions such as \"Who had\nthe longest javelin throw from USA?\". We show that black-box models struggle to\nlearn this task from scratch (accuracy under 50\\%) even with access to each\nagent's knowledge and gold facts supervision. In contrast, models that learn to\ncommunicate with agents outperform black-box models, reaching scores of 100\\%\nwhen given gold decomposition supervision. However, we show that the challenge\nof learning to solve complex tasks by communicating with existing agents\n\\emph{without relying on any auxiliary supervision or data} still remains\nhighly elusive. We release CommaQA, along with a compositional generalization\ntest split, to advance research in this direction. Dataset and Code available\nat https://github.com/allenai/commaqa.", "published": "2021-10-16 10:37:34", "link": "http://arxiv.org/abs/2110.08542v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Answering Open-Domain Multi-Answer Questions via a Recall-then-Verify\n  Framework", "abstract": "Open-domain questions are likely to be open-ended and ambiguous, leading to\nmultiple valid answers. Existing approaches typically adopt the\nrerank-then-read framework, where a reader reads top-ranking evidence to\npredict answers. According to our empirical analysis, this framework faces\nthree problems: first, to leverage a large reader under a memory constraint,\nthe reranker should select only a few relevant passages to cover diverse\nanswers, while balancing relevance and diversity is non-trivial; second, the\nsmall reading budget prevents the reader from accessing valuable retrieved\nevidence filtered out by the reranker; third, when using a generative reader to\npredict answers all at once based on all selected evidence, whether a valid\nanswer will be predicted also pathologically depends on the evidence of some\nother valid answer(s). To address these issues, we propose to answer\nopen-domain multi-answer questions with a recall-then-verify framework, which\nseparates the reasoning process of each answer so that we can make better use\nof retrieved evidence while also leveraging large models under the same memory\nconstraint. Our framework achieves state-of-the-art results on two multi-answer\ndatasets, and predicts significantly more gold answers than a rerank-then-read\nsystem that uses an oracle reranker.", "published": "2021-10-16 10:48:10", "link": "http://arxiv.org/abs/2110.08544v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Making the Most of Multilingual Pretraining for Zero-Shot Neural\n  Machine Translation", "abstract": "This paper demonstrates that multilingual pretraining and multilingual\nfine-tuning are both critical for facilitating cross-lingual transfer in\nzero-shot translation, where the neural machine translation (NMT) model is\ntested on source languages unseen during supervised training. Following this\nidea, we present SixT+, a strong many-to-English NMT model that supports 100\nsource languages but is trained with a parallel dataset in only six source\nlanguages. SixT+ initializes the decoder embedding and the full encoder with\nXLM-R large and then trains the encoder and decoder layers with a simple\ntwo-stage training strategy. SixT+ achieves impressive performance on\nmany-to-English translation. It significantly outperforms CRISS and m2m-100,\ntwo strong multilingual NMT systems, with an average gain of 7.2 and 5.0 BLEU\nrespectively. Additionally, SixT+ offers a set of model parameters that can be\nfurther fine-tuned to other unsupervised tasks. We demonstrate that adding\nSixT+ initialization outperforms state-of-the-art explicitly designed\nunsupervised NMT models on Si<->En and Ne<->En by over 1.2 average BLEU. When\napplied to zero-shot cross-lingual abstractive summarization, it produces an\naverage performance gain of 12.3 ROUGE-L over mBART-ft. We conduct detailed\nanalyses to understand the key ingredients of SixT+, including multilinguality\nof the auxiliary parallel data, positional disentangled encoder, and the\ncross-lingual transferability of its encoder.", "published": "2021-10-16 10:59:39", "link": "http://arxiv.org/abs/2110.08547v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PAGnol: An Extra-Large French Generative Model", "abstract": "Access to large pre-trained models of varied architectures, in many different\nlanguages, is central to the democratization of NLP. We introduce PAGnol, a\ncollection of French GPT models. Using scaling laws, we efficiently train\nPAGnol-XL (1.5B parameters) with the same computational budget as CamemBERT, a\nmodel 13 times smaller. PAGnol-XL is the largest model trained to date for the\nFrench language. We plan to train increasingly large and performing versions of\nPAGnol, exploring the capabilities of French extreme-scale models.\n  For this first release, we focus on the pre-training and scaling calculations\nunderlining PAGnol. We fit a scaling law for compute for the French language,\nand compare it with its English counterpart. We find the pre-training dataset\nsignificantly conditions the quality of the outputs, with common datasets such\nas OSCAR leading to low-quality offensive text. We evaluate our models on\ndiscriminative and generative tasks in French, comparing to other\nstate-of-the-art French and multilingual models, and reaching the state of the\nart in the abstract summarization task. Our research was conducted on the\npublic GENCI Jean Zay supercomputer, and our models up to the Large are made\npublicly available.", "published": "2021-10-16 11:44:23", "link": "http://arxiv.org/abs/2110.08554v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Robustness of Reading Comprehension Models to Entity Renaming", "abstract": "We study the robustness of machine reading comprehension (MRC) models to\nentity renaming -- do models make more wrong predictions when the same\nquestions are asked about an entity whose name has been changed? Such failures\nimply that models overly rely on entity information to answer questions, and\nthus may generalize poorly when facts about the world change or questions are\nasked about novel entities. To systematically audit this issue, we present a\npipeline to automatically generate test examples at scale, by replacing entity\nnames in the original test sample with names from a variety of sources, ranging\nfrom names in the same test set, to common names in life, to arbitrary strings.\nAcross five datasets and three pretrained model architectures, MRC models\nconsistently perform worse when entities are renamed, with particularly large\naccuracy drops on datasets constructed via distant supervision. We also find\nlarge differences between models: SpanBERT, which is pretrained with span-level\nmasking, is more robust than RoBERTa, despite having similar accuracy on\nunperturbed test data. We further experiment with different masking strategies\nas the continual pretraining objective and find that entity-based masking can\nimprove the robustness of MRC models.", "published": "2021-10-16 11:46:32", "link": "http://arxiv.org/abs/2110.08555v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FrugalScore: Learning Cheaper, Lighter and Faster Evaluation Metricsfor\n  Automatic Text Generation", "abstract": "Fast and reliable evaluation metrics are key to R&D progress. While\ntraditional natural language generation metrics are fast, they are not very\nreliable. Conversely, new metrics based on large pretrained language models are\nmuch more reliable, but require significant computational resources. In this\npaper, we propose FrugalScore, an approach to learn a fixed, low cost version\nof any expensive NLG metric, while retaining most of its original performance.\nExperiments with BERTScore and MoverScore on summarization and translation show\nthat FrugalScore is on par with the original metrics (and sometimes better),\nwhile having several orders of magnitude less parameters and running several\ntimes faster. On average over all learned metrics, tasks, and variants,\nFrugalScore retains 96.8% of the performance, runs 24 times faster, and has 35\ntimes less parameters than the original metrics. We make our trained metrics\npublicly available, to benefit the entire NLP community and in particular\nresearchers and practitioners with limited resources.", "published": "2021-10-16 11:59:48", "link": "http://arxiv.org/abs/2110.08559v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LSA: Modeling Aspect Sentiment Coherency via Local Sentiment Aggregation", "abstract": "Aspect sentiment coherency is an intriguing yet underexplored topic in the\nfield of aspect-based sentiment classification. This concept reflects the\ncommon pattern where adjacent aspects often share similar sentiments. Despite\nits prevalence, current studies have not fully recognized the potential of\nmodeling aspect sentiment coherency, including its implications in adversarial\ndefense. To model aspect sentiment coherency, we propose a novel local\nsentiment aggregation (LSA) paradigm based on constructing a\ndifferential-weighted sentiment aggregation window. We have rigorously\nevaluated our model through experiments, and the results affirm the proficiency\nof LSA in terms of aspect coherency prediction and aspect sentiment\nclassification. For instance, it outperforms existing models and achieves\nstate-of-the-art sentiment classification performance across five public\ndatasets. Furthermore, we demonstrate the promising ability of LSA in ABSC\nadversarial defense, thanks to its sentiment coherency modeling. To encourage\nfurther exploration and application of this concept, we have made our code\npublicly accessible. This will provide researchers with a valuable tool to\ndelve into sentiment coherency modeling in future research.", "published": "2021-10-16 16:22:43", "link": "http://arxiv.org/abs/2110.08604v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Summ^N: A Multi-Stage Summarization Framework for Long Input Dialogues\n  and Documents", "abstract": "Text summarization helps readers capture salient information from documents,\nnews, interviews, and meetings. However, most state-of-the-art pretrained\nlanguage models (LM) are unable to efficiently process long text for many\nsummarization tasks. In this paper, we propose Summ$^N$, a simple, flexible,\nand effective multi-stage framework for input texts that are longer than the\nmaximum context length of typical pretrained LMs. Summ$^N$ first splits the\ndata samples and generates a coarse summary in multiple stages and then\nproduces the final fine-grained summary based on it. Our framework can process\ninput text of arbitrary length by adjusting the number of stages while keeping\nthe LM input size fixed. Moreover, it can deal with both single-source\ndocuments and dialogues, and it can be used on top of different backbone\nabstractive summarization models. To the best of our knowledge, Summ$^N$ is the\nfirst multi-stage split-then-summarize framework for long input summarization.\nOur experiments demonstrate that Summ$^N$ outperforms previous state-of-the-art\nmethods by improving ROUGE scores on three long meeting summarization datasets\nAMI, ICSI, and QMSum, two long TV series datasets from SummScreen, and a long\ndocument summarization dataset GovReport. Our data and code are available at\nhttps://github.com/psunlpgroup/Summ-N.", "published": "2021-10-16 06:19:54", "link": "http://arxiv.org/abs/2110.10150v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Invariant Language Modeling", "abstract": "Large pretrained language models are critical components of modern NLP\npipelines. Yet, they suffer from spurious correlations, poor out-of-domain\ngeneralization, and biases. Inspired by recent progress in causal machine\nlearning, in particular the invariant risk minimization (IRM) paradigm, we\npropose invariant language modeling, a framework for learning invariant\nrepresentations that generalize better across multiple environments. In\nparticular, we adapt a game-theoretic formulation of IRM (IRM-games) to\nlanguage models, where the invariance emerges from a specific training schedule\nin which all the environments compete to optimize their own\nenvironment-specific loss by updating subsets of the model in a round-robin\nfashion. We focus on controlled experiments to precisely demonstrate the\nability of our method to (i) remove structured noise, (ii) ignore specific\nspurious correlations without affecting global performance, and (iii) achieve\nbetter out-of-domain generalization. These benefits come with a negligible\ncomputational overhead compared to standard training, do not require changing\nthe local loss, and can be applied to any language model. We believe this\nframework is promising to help mitigate spurious correlations and biases in\nlanguage models.", "published": "2021-10-16 00:03:19", "link": "http://arxiv.org/abs/2110.08413v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Open Domain Question Answering with A Unified Knowledge Interface", "abstract": "The retriever-reader framework is popular for open-domain question answering\n(ODQA) due to its ability to use explicit knowledge. Although prior work has\nsought to increase the knowledge coverage by incorporating structured knowledge\nbeyond text, accessing heterogeneous knowledge sources through a unified\ninterface remains an open question. While data-to-text generation has the\npotential to serve as a universal interface for data and text, its feasibility\nfor downstream tasks remains largely unknown. In this work, we bridge this gap\nand use the data-to-text method as a means for encoding structured knowledge\nfor ODQA. Specifically, we propose a verbalizer-retriever-reader framework for\nODQA over data and text where verbalized tables from Wikipedia and graphs from\nWikidata are used as augmented knowledge sources. We show that our Unified Data\nand Text QA, UDT-QA, can effectively benefit from the expanded knowledge index,\nleading to large gains over text-only baselines. Notably, our approach sets the\nsingle-model state-of-the-art on Natural Questions. Furthermore, our analyses\nindicate that verbalized knowledge is preferred for answer reasoning for both\nadapted and hot-swap settings.", "published": "2021-10-16 00:11:21", "link": "http://arxiv.org/abs/2110.08417v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Robustness Challenges in Model Distillation and Pruning for Natural\n  Language Understanding", "abstract": "Recent work has focused on compressing pre-trained language models (PLMs)\nlike BERT where the major focus has been to improve the in-distribution\nperformance for downstream tasks. However, very few of these studies have\nanalyzed the impact of compression on the generalizability and robustness of\ncompressed models for out-of-distribution (OOD) data. Towards this end, we\nstudy two popular model compression techniques including knowledge distillation\nand pruning and show that the compressed models are significantly less robust\nthan their PLM counterparts on OOD test sets although they obtain similar\nperformance on in-distribution development sets for a task. Further analysis\nindicates that the compressed models overfit on the shortcut samples and\ngeneralize poorly on the hard ones. We further leverage this observation to\ndevelop a regularization strategy for robust model compression based on sample\nuncertainty. Experimental results on several natural language understanding\ntasks demonstrate that our bias mitigation framework improves the OOD\ngeneralization of the compressed models, while not sacrificing the\nin-distribution task performance.", "published": "2021-10-16 00:20:04", "link": "http://arxiv.org/abs/2110.08419v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Metadata Shaping: Natural Language Annotations for the Tail", "abstract": "Language models (LMs) have made remarkable progress, but still struggle to\ngeneralize beyond the training data to rare linguistic patterns. Since rare\nentities and facts are prevalent in the queries users submit to popular\napplications such as search and personal assistant systems, improving the\nability of LMs to reliably capture knowledge over rare entities is a pressing\nchallenge studied in significant prior work. Noticing that existing approaches\nprimarily modify the LM architecture or introduce auxiliary objectives to\ninject useful entity knowledge, we ask to what extent we could match the\nquality of these architectures using a base LM architecture, and only changing\nthe data? We propose metadata shaping, a method in which readily available\nmetadata, such as entity descriptions and categorical tags, are appended to\nexamples based on information theoretic metrics. Intuitively, if metadata\ncorresponding to popular entities overlap with metadata for rare entities, the\nLM may be able to better reason about the rare entities using patterns learned\nfrom similar popular entities. On standard entity-rich tasks (TACRED, FewRel,\nOpenEntity), with no changes to the LM whatsoever, metadata shaping exceeds the\nBERT-baseline by up to 5.3 F1 points, and achieves or competes with\nstate-of-the-art results. We further show the improvements are up to 10x larger\non examples containing tail versus popular entities.", "published": "2021-10-16 01:00:47", "link": "http://arxiv.org/abs/2110.08430v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Compositional Generalization with Self-Training for\n  Data-to-Text Generation", "abstract": "Data-to-text generation focuses on generating fluent natural language\nresponses from structured meaning representations (MRs). Such representations\nare compositional and it is costly to collect responses for all possible\ncombinations of atomic meaning schemata, thereby necessitating few-shot\ngeneralization to novel MRs. In this work, we systematically study the\ncompositional generalization of the state-of-the-art T5 models in few-shot\ndata-to-text tasks. We show that T5 models fail to generalize to unseen MRs,\nand we propose a template-based input representation that considerably improves\nthe model's generalization capability. To further improve the model's\nperformance, we propose an approach based on self-training using fine-tuned\nBLEURT for pseudo response selection. On the commonly-used SGD and Weather\nbenchmarks, the proposed self-training approach improves tree accuracy by 46%+\nand reduces the slot error rates by 73%+ over the strong T5 baselines in\nfew-shot settings.", "published": "2021-10-16 04:26:56", "link": "http://arxiv.org/abs/2110.08467v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Case-based Reasoning for Better Generalization in Textual Reinforcement\n  Learning", "abstract": "Text-based games (TBG) have emerged as promising environments for driving\nresearch in grounded language understanding and studying problems like\ngeneralization and sample efficiency. Several deep reinforcement learning (RL)\nmethods with varying architectures and learning schemes have been proposed for\nTBGs. However, these methods fail to generalize efficiently, especially under\ndistributional shifts. In a departure from deep RL approaches, in this paper,\nwe propose a general method inspired by case-based reasoning to train agents\nand generalize out of the training distribution. The case-based reasoner\ncollects instances of positive experiences from the agent's interaction with\nthe world in the past and later reuses the collected experiences to act\nefficiently. The method can be applied in conjunction with any existing\non-policy neural agent in the literature for TBGs. Our experiments show that\nthe proposed approach consistently improves existing methods, obtains good\nout-of-distribution generalization, and achieves new state-of-the-art results\non widely used environments.", "published": "2021-10-16 04:51:34", "link": "http://arxiv.org/abs/2110.08470v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Good Prompt Is Worth Millions of Parameters: Low-resource Prompt-based\n  Learning for Vision-Language Models", "abstract": "Large pre-trained vision-language (VL) models can learn a new task with a\nhandful of examples and generalize to a new task without fine-tuning. However,\nthese VL models are hard to deploy for real-world applications due to their\nimpractically huge sizes and slow inference speed. To solve this limitation, we\nstudy prompt-based low-resource learning of VL tasks with our proposed method,\nFewVLM, relatively smaller than recent few-shot learners. For FewVLM, we\npre-train a sequence-to-sequence transformer model with prefix language\nmodeling (PrefixLM) and masked language modeling (MaskedLM). Furthermore, we\nanalyze the effect of diverse prompts for few-shot tasks. Experimental results\non VQA show that FewVLM with prompt-based learning outperforms Frozen which is\n31x larger than FewVLM by 18.2% point and achieves comparable results to a 246x\nlarger model, PICa. In our analysis, we observe that (1) prompts significantly\naffect zero-shot performance but marginally affect few-shot performance, (2)\nmodels with noisy prompts learn as quickly as hand-crafted prompts given larger\ntraining data, and (3) MaskedLM helps VQA tasks while PrefixLM boosts\ncaptioning performance. Our code is publicly available at\n\\url{https://github.com/woojeongjin/FewVLM}", "published": "2021-10-16 06:07:59", "link": "http://arxiv.org/abs/2110.08484v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Understanding Multimodal Procedural Knowledge by Sequencing Multimodal\n  Instructional Manuals", "abstract": "The ability to sequence unordered events is an essential skill to comprehend\nand reason about real world task procedures, which often requires thorough\nunderstanding of temporal common sense and multimodal information, as these\nprocedures are often communicated through a combination of texts and images.\nSuch capability is essential for applications such as sequential task planning\nand multi-source instruction summarization. While humans are capable of\nreasoning about and sequencing unordered multimodal procedural instructions,\nwhether current machine learning models have such essential capability is still\nan open question. In this work, we benchmark models' capability of reasoning\nover and sequencing unordered multimodal instructions by curating datasets from\npopular online instructional manuals and collecting comprehensive human\nannotations. We find models not only perform significantly worse than humans\nbut also seem incapable of efficiently utilizing the multimodal information. To\nimprove machines' performance on multimodal event sequencing, we propose\nsequentiality-aware pretraining techniques that exploit the sequential\nalignment properties of both texts and images, resulting in > 5% significant\nimprovements.", "published": "2021-10-16 06:12:15", "link": "http://arxiv.org/abs/2110.08486v4", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Analyzing Dynamic Adversarial Training Data in the Limit", "abstract": "To create models that are robust across a wide range of test inputs, training\ndatasets should include diverse examples that span numerous phenomena. Dynamic\nadversarial data collection (DADC), where annotators craft examples that\nchallenge continually improving models, holds promise as an approach for\ngenerating such diverse training sets. Prior work has shown that running DADC\nover 1-3 rounds can help models fix some error types, but it does not\nnecessarily lead to better generalization beyond adversarial test data. We\nargue that running DADC over many rounds maximizes its training-time benefits,\nas the different rounds can together cover many of the task-relevant phenomena.\nWe present the first study of longer-term DADC, where we collect 20 rounds of\nNLI examples for a small set of premise paragraphs, with both adversarial and\nnon-adversarial approaches. Models trained on DADC examples make 26% fewer\nerrors on our expert-curated test set compared to models trained on\nnon-adversarial data. Our analysis shows that DADC yields examples that are\nmore difficult, more lexically and syntactically diverse, and contain fewer\nannotation artifacts compared to non-adversarial examples.", "published": "2021-10-16 08:48:52", "link": "http://arxiv.org/abs/2110.08514v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Empirical Survey of the Effectiveness of Debiasing Techniques for\n  Pre-trained Language Models", "abstract": "Recent work has shown pre-trained language models capture social biases from\nthe large amounts of text they are trained on. This has attracted attention to\ndeveloping techniques that mitigate such biases. In this work, we perform an\nempirical survey of five recently proposed bias mitigation techniques:\nCounterfactual Data Augmentation (CDA), Dropout, Iterative Nullspace\nProjection, Self-Debias, and SentenceDebias. We quantify the effectiveness of\neach technique using three intrinsic bias benchmarks while also measuring the\nimpact of these techniques on a model's language modeling ability, as well as\nits performance on downstream NLU tasks. We experimentally find that: (1)\nSelf-Debias is the strongest debiasing technique, obtaining improved scores on\nall bias benchmarks; (2) Current debiasing techniques perform less consistently\nwhen mitigating non-gender biases; And (3) improvements on bias benchmarks such\nas StereoSet and CrowS-Pairs by using debiasing strategies are often\naccompanied by a decrease in language modeling ability, making it difficult to\ndetermine whether the bias mitigation was effective.", "published": "2021-10-16 09:40:30", "link": "http://arxiv.org/abs/2110.08527v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sharpness-Aware Minimization Improves Language Model Generalization", "abstract": "The allure of superhuman-level capabilities has led to considerable interest\nin language models like GPT-3 and T5, wherein the research has, by and large,\nrevolved around new model architectures, training tasks, and loss objectives,\nalong with substantial engineering efforts to scale up model capacity and\ndataset size. Comparatively little work has been done to improve the\ngeneralization of these models through better optimization. In this work, we\nshow that Sharpness-Aware Minimization (SAM), a recently proposed optimization\nprocedure that encourages convergence to flatter minima, can substantially\nimprove the generalization of language models without much computational\noverhead. We show that SAM is able to boost performance on SuperGLUE, GLUE, Web\nQuestions, Natural Questions, Trivia QA, and TyDiQA, with particularly large\ngains when training data for these tasks is limited.", "published": "2021-10-16 09:44:06", "link": "http://arxiv.org/abs/2110.08529v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sparse Distillation: Speeding Up Text Classification by Using Bigger\n  Student Models", "abstract": "Distilling state-of-the-art transformer models into lightweight student\nmodels is an effective way to reduce computation cost at inference time. The\nstudent models are typically compact transformers with fewer parameters, while\nexpensive operations such as self-attention persist. Therefore, the improved\ninference speed may still be unsatisfactory for real-time or high-volume use\ncases. In this paper, we aim to further push the limit of inference speed by\ndistilling teacher models into bigger, sparser student models -- bigger in that\nthey scale up to billions of parameters; sparser in that most of the model\nparameters are n-gram embeddings. Our experiments on six single-sentence text\nclassification tasks show that these student models retain 97% of the\nRoBERTa-Large teacher performance on average, and meanwhile achieve up to 600x\nspeed-up on both GPUs and CPUs at inference time. Further investigation reveals\nthat our pipeline is also helpful for sentence-pair classification tasks, and\nin domain generalization settings.", "published": "2021-10-16 10:04:14", "link": "http://arxiv.org/abs/2110.08536v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Virtual Augmentation Supported Contrastive Learning of Sentence\n  Representations", "abstract": "Despite profound successes, contrastive representation learning relies on\ncarefully designed data augmentations using domain specific knowledge. This\nchallenge is magnified in natural language processing where no general rules\nexist for data augmentation due to the discrete nature of natural language. We\ntackle this challenge by presenting a Virtual augmentation Supported\nContrastive Learning of sentence representations (VaSCL). Originating from the\ninterpretation that data augmentation essentially constructs the neighborhoods\nof each training instance, we in turn utilize the neighborhood to generate\neffective data augmentations. Leveraging the large training batch size of\ncontrastive learning, we approximate the neighborhood of an instance via its\nK-nearest in-batch neighbors in the representation space. We then define an\ninstance discrimination task regarding this neighborhood and generate the\nvirtual augmentation in an adversarial training manner. We access the\nperformance of VaSCL on a wide range of downstream tasks, and set a new\nstate-of-the-art for unsupervised sentence representation learning.", "published": "2021-10-16 11:29:03", "link": "http://arxiv.org/abs/2110.08552v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "n-stage Latent Dirichlet Allocation: A Novel Approach for LDA", "abstract": "Nowadays, data analysis has become a problem as the amount of data is\nconstantly increasing. In order to overcome this problem in textual data, many\nmodels and methods are used in natural language processing. The topic modeling\nfield is one of these methods. Topic modeling allows determining the semantic\nstructure of a text document. Latent Dirichlet Allocation (LDA) is the most\ncommon method among topic modeling methods. In this article, the proposed\nn-stage LDA method, which can enable the LDA method to be used more\neffectively, is explained in detail. The positive effect of the method has been\ndemonstrated by the applied English and Turkish studies. Since the method\nfocuses on reducing the word count in the dictionary, it can be used\nlanguage-independently. You can access the open-source code of the method and\nthe example: https://github.com/anil1055/n-stage_LDA", "published": "2021-10-16 15:26:53", "link": "http://arxiv.org/abs/2110.08591v2", "categories": ["cs.CL", "cs.IR", "H.3.3; I.2.7; I.7.0"], "primary_category": "cs.CL"}
{"title": "Understanding Dataset Difficulty with $\\mathcal{V}$-Usable Information", "abstract": "Estimating the difficulty of a dataset typically involves comparing\nstate-of-the-art models to humans; the bigger the performance gap, the harder\nthe dataset is said to be. However, this comparison provides little\nunderstanding of how difficult each instance in a given distribution is, or\nwhat attributes make the dataset difficult for a given model. To address these\nquestions, we frame dataset difficulty -- w.r.t. a model $\\mathcal{V}$ -- as\nthe lack of $\\mathcal{V}$-$\\textit{usable information}$ (Xu et al., 2019),\nwhere a lower value indicates a more difficult dataset for $\\mathcal{V}$. We\nfurther introduce $\\textit{pointwise $\\mathcal{V}$-information}$ (PVI) for\nmeasuring the difficulty of individual instances w.r.t. a given distribution.\nWhile standard evaluation metrics typically only compare different models for\nthe same dataset, $\\mathcal{V}$-$\\textit{usable information}$ and PVI also\npermit the converse: for a given model $\\mathcal{V}$, we can compare different\ndatasets, as well as different instances/slices of the same dataset.\nFurthermore, our framework allows for the interpretability of different input\nattributes via transformations of the input, which we use to discover\nannotation artefacts in widely-used NLP benchmarks.", "published": "2021-10-16 00:21:42", "link": "http://arxiv.org/abs/2110.08420v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unsupervised Natural Language Inference Using PHL Triplet Generation", "abstract": "Transformer-based models achieve impressive performance on numerous Natural\nLanguage Inference (NLI) benchmarks when trained on respective training\ndatasets. However, in certain cases, training samples may not be available or\ncollecting them could be time-consuming and resource-intensive. In this work,\nwe address the above challenge and present an explorative study on unsupervised\nNLI, a paradigm in which no human-annotated training samples are available. We\ninvestigate it under three settings: PH, P, and NPH that differ in the extent\nof unlabeled data available for learning. As a solution, we propose a\nprocedural data generation approach that leverages a set of sentence\ntransformations to collect PHL (Premise, Hypothesis, Label) triplets for\ntraining NLI models, bypassing the need for human-annotated training data.\nComprehensive experiments with several NLI datasets show that the proposed\napproach results in accuracies of up to 66.75%, 65.9%, 65.39% in PH, P, and NPH\nsettings respectively, outperforming all existing unsupervised baselines.\nFurthermore, fine-tuning our model with as little as ~0.1% of the\nhuman-annotated training dataset (500 instances) leads to 12.2% higher accuracy\nthan the model trained from scratch on the same 500 instances. Supported by\nthis superior performance, we conclude with a recommendation for collecting\nhigh-quality task-specific data.", "published": "2021-10-16 01:40:34", "link": "http://arxiv.org/abs/2110.08438v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Unified Speaker Adaptation Approach for ASR", "abstract": "Transformer models have been used in automatic speech recognition (ASR)\nsuccessfully and yields state-of-the-art results. However, its performance is\nstill affected by speaker mismatch between training and test data. Further\nfinetuning a trained model with target speaker data is the most natural\napproach for adaptation, but it takes a lot of compute and may cause\ncatastrophic forgetting to the existing speakers. In this work, we propose a\nunified speaker adaptation approach consisting of feature adaptation and model\nadaptation. For feature adaptation, we employ a speaker-aware persistent memory\nmodel which generalizes better to unseen test speakers by making use of speaker\ni-vectors to form a persistent memory. For model adaptation, we use a novel\ngradual pruning method to adapt to target speakers without changing the model\narchitecture, which to the best of our knowledge, has never been explored in\nASR. Specifically, we gradually prune less contributing parameters on model\nencoder to a certain sparsity level, and use the pruned parameters for\nadaptation, while freezing the unpruned parameters to keep the original model\nperformance. We conduct experiments on the Librispeech dataset. Our proposed\napproach brings relative 2.74-6.52% word error rate (WER) reduction on general\nspeaker adaptation. On target speaker adaptation, our method outperforms the\nbaseline with up to 20.58% relative WER reduction, and surpasses the finetuning\nmethod by up to relative 2.54%. Besides, with extremely low-resource adaptation\ndata (e.g., 1 utterance), our method could improve the WER by relative 6.53%\nwith only a few epochs of training.", "published": "2021-10-16 10:48:52", "link": "http://arxiv.org/abs/2110.08545v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "HRKD: Hierarchical Relational Knowledge Distillation for Cross-domain\n  Language Model Compression", "abstract": "On many natural language processing tasks, large pre-trained language models\n(PLMs) have shown overwhelming performances compared with traditional neural\nnetwork methods. Nevertheless, their huge model size and low inference speed\nhave hindered the deployment on resource-limited devices in practice. In this\npaper, we target to compress PLMs with knowledge distillation, and propose a\nhierarchical relational knowledge distillation (HRKD) method to capture both\nhierarchical and domain relational information. Specifically, to enhance the\nmodel capability and transferability, we leverage the idea of meta-learning and\nset up domain-relational graphs to capture the relational information across\ndifferent domains. And to dynamically select the most representative prototypes\nfor each domain, we propose a hierarchical compare-aggregate mechanism to\ncapture hierarchical relationships. Extensive experiments on public\nmulti-domain datasets demonstrate the superior performance of our HRKD method\nas well as its strong few-shot learning ability. For reproducibility, we\nrelease the code at https://github.com/cheneydon/hrkd.", "published": "2021-10-16 11:23:02", "link": "http://arxiv.org/abs/2110.08551v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Transformers with Probabilistic Attention Keys", "abstract": "Multi-head attention is a driving force behind state-of-the-art transformers,\nwhich achieve remarkable performance across a variety of natural language\nprocessing (NLP) and computer vision tasks. It has been observed that for many\napplications, those attention heads learn redundant embedding, and most of them\ncan be removed without degrading the performance of the model. Inspired by this\nobservation, we propose Transformer with a Mixture of Gaussian Keys\n(Transformer-MGK), a novel transformer architecture that replaces redundant\nheads in transformers with a mixture of keys at each head. These mixtures of\nkeys follow a Gaussian mixture model and allow each attention head to focus on\ndifferent parts of the input sequence efficiently. Compared to its conventional\ntransformer counterpart, Transformer-MGK accelerates training and inference,\nhas fewer parameters, and requires fewer FLOPs to compute while achieving\ncomparable or better accuracy across tasks. Transformer-MGK can also be easily\nextended to use with linear attention. We empirically demonstrate the advantage\nof Transformer-MGK in a range of practical applications, including language\nmodeling and tasks that involve very long sequences. On the Wikitext-103 and\nLong Range Arena benchmark, Transformer-MGKs with 4 heads attain comparable or\nbetter performance to the baseline transformers with 8 heads.", "published": "2021-10-16 23:43:24", "link": "http://arxiv.org/abs/2110.08678v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Multimodal Dialogue Response Generation", "abstract": "Responsing with image has been recognized as an important capability for an\nintelligent conversational agent. Yet existing works only focus on exploring\nthe multimodal dialogue models which depend on retrieval-based methods, but\nneglecting generation methods. To fill in the gaps, we first present a\nmultimodal dialogue generation model, which takes the dialogue history as\ninput, then generates a textual sequence or an image as response. Learning such\na model often requires multimodal dialogues containing both texts and images\nwhich are difficult to obtain. Motivated by the challenge in practice, we\nconsider multimodal dialogue generation under a natural assumption that only\nlimited training examples are available. In such a low-resource setting, we\ndevise a novel conversational agent, Divter, in order to isolate parameters\nthat depend on multimodal dialogues from the entire generation model. By this\nmeans, the major part of the model can be learned from a large number of\ntext-only dialogues and text-image pairs respectively, then the whole\nparameters can be well fitted using the limited training examples. Extensive\nexperiments demonstrate our method achieves state-of-the-art results in both\nautomatic and human evaluation, and can generate informative text and\nhigh-resolution image responses.", "published": "2021-10-16 08:52:26", "link": "http://arxiv.org/abs/2110.08515v2", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "primary_category": "cs.CL"}
{"title": "ASR4REAL: An extended benchmark for speech models", "abstract": "Popular ASR benchmarks such as Librispeech and Switchboard are limited in the\ndiversity of settings and speakers they represent. We introduce a set of\nbenchmarks matching real-life conditions, aimed at spotting possible biases and\nweaknesses in models. We have found out that even though recent models do not\nseem to exhibit a gender bias, they usually show important performance\ndiscrepancies by accent, and even more important ones depending on the\nsocio-economic status of the speakers. Finally, all tested models show a strong\nperformance drop when tested on conversational speech, and in this precise\ncontext even a language model trained on a dataset as big as Common Crawl does\nnot seem to have significant positive effect which reiterates the importance of\ndeveloping conversational language models", "published": "2021-10-16 14:34:25", "link": "http://arxiv.org/abs/2110.08583v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "NN3A: Neural Network supported Acoustic Echo Cancellation, Noise\n  Suppression and Automatic Gain Control for Real-Time Communications", "abstract": "Acoustic echo cancellation (AEC), noise suppression (NS) and automatic gain\ncontrol (AGC) are three often required modules for real-time communications\n(RTC). This paper proposes a neural network supported algorithm for RTC, namely\nNN3A, which incorporates an adaptive filter and a multi-task model for residual\necho suppression, noise reduction and near-end speech activity detection. The\nproposed algorithm is shown to outperform both a method using separate models\nand an end-to-end alternative. It is further shown that there exists a\ntrade-off in the model between residual suppression and near-end speech\ndistortion, which could be balanced by a novel loss weighting function. Several\npractical aspects of training the joint model are also investigated to push its\nperformance to limit.", "published": "2021-10-16 01:38:33", "link": "http://arxiv.org/abs/2110.08437v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Controllable Multichannel Speech Dereverberation based on Deep Neural\n  Networks", "abstract": "Neural network based speech dereverberation has achieved promising results in\nrecent studies. Nevertheless, many are focused on recovery of only the direct\npath sound and early reflections, which could be beneficial to speech\nperception, are discarded. The performance of a model trained to recover clean\nspeech degrades when evaluated on early reverberation targets, and vice versa.\nThis paper proposes a novel deep neural network based multichannel speech\ndereverberation algorithm, in which the dereverberation level is controllable.\nThis is realized by adding a simple floating-point number as target controller\nof the model. Experiments are conducted using spatially distributed\nmicrophones, and the efficacy of the proposed algorithm is confirmed in various\nsimulated conditions.", "published": "2021-10-16 01:41:25", "link": "http://arxiv.org/abs/2110.08439v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Learning velocity model for complex media with deep convolutional neural\n  networks", "abstract": "The paper considers the problem of velocity model acquisition for a complex\nmedia based on boundary measurements. The acoustic model is used to describe\nthe media. We used an open-source dataset of velocity distributions to compare\nthe presented results with the previous works directly. Forward modeling is\nperformed using the grid-characteristic numerical method. The inverse problem\nis solved using deep convolutional neural networks. Modifications for a\nbaseline UNet architecture are proposed to improve both structural similarity\nindex measure quantitative correspondence of the velocity profiles with the\nground truth. We evaluate our enhancements and demonstrate the statistical\nsignificance of the results.", "published": "2021-10-16 17:52:08", "link": "http://arxiv.org/abs/2110.08626v1", "categories": ["cs.LG", "cs.SD", "eess.AS", "86-10, 86A22", "I.2.6"], "primary_category": "cs.LG"}
{"title": "Towards Robust Waveform-Based Acoustic Models", "abstract": "We study the problem of learning robust acoustic models in adverse\nenvironments, characterized by a significant mismatch between training and test\nconditions. This problem is of paramount importance for the deployment of\nspeech recognition systems that need to perform well in unseen environments.\nFirst, we characterize data augmentation theoretically as an instance of\nvicinal risk minimization, which aims at improving risk estimates during\ntraining by replacing the delta functions that define the empirical density\nover the input space with an approximation of the marginal population density\nin the vicinity of the training samples. More specifically, we assume that\nlocal neighborhoods centered at training samples can be approximated using a\nmixture of Gaussians, and demonstrate theoretically that this can incorporate\nrobust inductive bias into the learning process. We then specify the individual\nmixture components implicitly via data augmentation schemes, designed to\naddress common sources of spurious correlations in acoustic models. To avoid\npotential confounding effects on robustness due to information loss, which has\nbeen associated with standard feature extraction techniques (e.g., FBANK and\nMFCC features), we focus on the waveform-based setting. Our empirical results\nshow that the approach can generalize to unseen noise conditions, with 150%\nrelative improvement in out-of-distribution generalization compared to training\nusing the standard risk minimization principle. Moreover, the results\ndemonstrate competitive performance relative to models learned using a training\nsample designed to match the acoustic conditions characteristic of test\nutterances.", "published": "2021-10-16 18:21:34", "link": "http://arxiv.org/abs/2110.08634v2", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "A Variational Bayesian Approach to Learning Latent Variables for\n  Acoustic Knowledge Transfer", "abstract": "We propose a variational Bayesian (VB) approach to learning distributions of\nlatent variables in deep neural network (DNN) models for cross-domain knowledge\ntransfer, to address acoustic mismatches between training and testing\nconditions. Instead of carrying out point estimation in conventional maximum a\nposteriori estimation with a risk of having a curse of dimensionality in\nestimating a huge number of model parameters, we focus our attention on\nestimating a manageable number of latent variables of DNNs via a VB inference\nframework. To accomplish model transfer, knowledge learnt from a source domain\nis encoded in prior distributions of latent variables and optimally combined,\nin a Bayesian sense, with a small set of adaptation data from a target domain\nto approximate the corresponding posterior distributions. Experimental results\non device adaptation in acoustic scene classification show that our proposed VB\napproach can obtain good improvements on target devices, and consistently\noutperforms 13 state-of-the-art knowledge transfer algorithms.", "published": "2021-10-16 15:54:01", "link": "http://arxiv.org/abs/2110.08598v2", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.NE", "cs.SD"], "primary_category": "eess.AS"}
