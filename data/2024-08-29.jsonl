{"title": "Brief Synopsis of the Scientific Career of T. R. Hurd", "abstract": "As an introduction to a Special Issue of International Journal of Theoretical\nand Applied Finance in Honour of the Memory of Thomas Robert Hurd we present a\nbrief synopsis of Tom Hurd's scientific career and a bibliography of his\nscientific publications.", "published": "2024-08-29 20:25:26", "link": "http://arxiv.org/abs/2408.16891v2", "categories": ["q-fin.MF", "math-ph", "math.MP", "quant-ph"], "primary_category": "q-fin.MF"}
{"title": "From cart to truck: meaning shift through words in English in the last\n  two centuries", "abstract": "This onomasiological study uses diachronic word embeddings to explore how\ndifferent words represented the same concepts over time, using historical word\ndata from 1800 to 2000. We identify shifts in energy, transport, entertainment,\nand computing domains, revealing connections between language and societal\nchanges.\n  Our approach consisted in using diachronic word embeddings trained using\nword2vec with skipgram and aligning them using orthogonal Procrustes. We\ndiscuss possible difficulties linked to the relationships the method\nidentifies. Moreover, we look at the ethical aspects of interpreting results,\nhighlighting the need for expert insights to understand the method's\nsignificance.", "published": "2024-08-29 02:05:39", "link": "http://arxiv.org/abs/2408.16209v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Making the Most of your Model: Methods for Finetuning and Applying\n  Pretrained Transformers", "abstract": "This thesis provides methods and analysis of models which make progress on\nthis goal. The techniques outlined are task agnostic, and should provide\nbenefit when used with nearly any transformer LM. We introduce two new\nfinetuning methods which add new capabilities to the models they are used on.\nThe first adds a recurrence mechanism, which removes the fixed-window sized\nconstraint and improves the efficiency of a transformer decoder. The second\nallows masked language models (MLMs) to be used for initialization of both the\nencoder and decoder of a non-autoregressive sequence-to-sequence transformer,\nopening up generative applications of models which were previously only used\nfor natural language understanding tasks.\n  We also introduce two new techniques for improving the quality of predictions\nof any transformer decoder without additional finetuning. One, hidden state\noptimization, can be applied to any transformer decoder to improve the quality\nof predictions at inference time, especially for few-shot classification. The\nother, conditional beam search, allows practitioners to search for natural\nlanguage generation (NLG) model outputs with high likelihood while conditioning\non the event that the output is not degenerate (e.g. empty, repetitive, etc.).\n  Finally, we provide theoretical and empirical insights on the divergence of\nmodel-likelihood and output quality which has widely been observed in prior\nwork. These insights apply to any model which represents a distribution over\ntext, and apply to language models which are not transformers or even\nautoregressive. We argue that the NLP community has, to some extent,\nmisunderstood the implications of these findings, and encourage a point of view\nwhich has more nuance.", "published": "2024-08-29 03:50:24", "link": "http://arxiv.org/abs/2408.16241v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing AI-Driven Psychological Consultation: Layered Prompts with\n  Large Language Models", "abstract": "Psychological consultation is essential for improving mental health and\nwell-being, yet challenges such as the shortage of qualified professionals and\nscalability issues limit its accessibility. To address these challenges, we\nexplore the use of large language models (LLMs) like GPT-4 to augment\npsychological consultation services. Our approach introduces a novel layered\nprompting system that dynamically adapts to user input, enabling comprehensive\nand relevant information gathering. We also develop empathy-driven and\nscenario-based prompts to enhance the LLM's emotional intelligence and\ncontextual understanding in therapeutic settings. We validated our approach\nthrough experiments using a newly collected dataset of psychological\nconsultation dialogues, demonstrating significant improvements in response\nquality. The results highlight the potential of our prompt engineering\ntechniques to enhance AI-driven psychological consultation, offering a scalable\nand accessible solution to meet the growing demand for mental health support.", "published": "2024-08-29 05:47:14", "link": "http://arxiv.org/abs/2408.16276v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Critic-CoT: Boosting the reasoning abilities of large language model via\n  Chain-of-thoughts Critic", "abstract": "Self-critic has become a crucial mechanism for enhancing the reasoning\nperformance of LLMs. However, current approaches mainly involve basic prompts\nfor intuitive instance-level feedback, which resembles System-1 processes and\nlimits the reasoning capabilities. Moreover, there is a lack of in-depth\ninvestigations into the relationship between LLM's ability to criticize and its\ntask-solving performance. To address these issues, we propose Critic-CoT, a\nnovel framework that pushes LLMs toward System-2-like critic capability.\nThrough a step-wise CoT reasoning paradigm and the automatic construction of\ndistant-supervision data without human annotation, Critic-CoT enables LLMs to\nengage in slow, analytic self-critique and refinement, thereby improving their\nreasoning abilities. Experiments on GSM8K and MATH demonstrate that our\nenhanced model significantly boosts task-solving performance by filtering out\ninvalid solutions or iterative refinement. Furthermore, we investigate the\nintrinsic correlation between critique and task-solving abilities within LLMs,\ndiscovering that these abilities can mutually reinforce each other rather than\nconflict.", "published": "2024-08-29 08:02:09", "link": "http://arxiv.org/abs/2408.16326v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Unreasonable Ineffectiveness of Nucleus Sampling on Mitigating Text\n  Memorization", "abstract": "This work analyses the text memorization behavior of large language models\n(LLMs) when subjected to nucleus sampling. Stochastic decoding methods like\nnucleus sampling are typically applied to overcome issues such as monotonous\nand repetitive text generation, which are often observed with\nmaximization-based decoding techniques. We hypothesize that nucleus sampling\nmight also reduce the occurrence of memorization patterns, because it could\nlead to the selection of tokens outside the memorized sequence. To test this\nhypothesis we create a diagnostic dataset with a known distribution of\nduplicates that gives us some control over the likelihood of memorization of\ncertain parts of the training data. Our analysis of two GPT-Neo models\nfine-tuned on this dataset interestingly shows that (i) an increase of the\nnucleus size reduces memorization only modestly, and (ii) even when models do\nnot engage in \"hard\" memorization -- a verbatim reproduction of training\nsamples -- they may still display \"soft\" memorization whereby they generate\noutputs that echo the training data but without a complete one-by-one\nresemblance.", "published": "2024-08-29 08:30:33", "link": "http://arxiv.org/abs/2408.16345v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MQM-Chat: Multidimensional Quality Metrics for Chat Translation", "abstract": "The complexities of chats pose significant challenges for machine translation\nmodels. Recognizing the need for a precise evaluation metric to address the\nissues of chat translation, this study introduces Multidimensional Quality\nMetrics for Chat Translation (MQM-Chat). Through the experiments of five models\nusing MQM-Chat, we observed that all models generated certain fundamental\nerrors, while each of them has different shortcomings, such as omission, overly\ncorrecting ambiguous source content, and buzzword issues, resulting in the loss\nof stylized information. Our findings underscore the effectiveness of MQM-Chat\nin evaluating chat translation, emphasizing the importance of stylized content\nand dialogue consistency for future studies.", "published": "2024-08-29 09:52:01", "link": "http://arxiv.org/abs/2408.16390v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Instruction-tuned Large Language Models for Machine Translation in the\n  Medical Domain", "abstract": "Large Language Models (LLMs) have shown promising results on machine\ntranslation for high resource language pairs and domains. However, in\nspecialised domains (e.g. medical) LLMs have shown lower performance compared\nto standard neural machine translation models. The consistency in the machine\ntranslation of terminology is crucial for users, researchers, and translators\nin specialised domains. In this study, we compare the performance between\nbaseline LLMs and instruction-tuned LLMs in the medical domain. In addition, we\nintroduce terminology from specialised medical dictionaries into the\ninstruction formatted datasets for fine-tuning LLMs. The instruction-tuned LLMs\nsignificantly outperform the baseline models with automatic metrics.", "published": "2024-08-29 11:05:54", "link": "http://arxiv.org/abs/2408.16440v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SurveySum: A Dataset for Summarizing Multiple Scientific Articles into a\n  Survey Section", "abstract": "Document summarization is a task to shorten texts into concise and\ninformative summaries. This paper introduces a novel dataset designed for\nsummarizing multiple scientific articles into a section of a survey. Our\ncontributions are: (1) SurveySum, a new dataset addressing the gap in\ndomain-specific summarization tools; (2) two specific pipelines to summarize\nscientific articles into a section of a survey; and (3) the evaluation of these\npipelines using multiple metrics to compare their performance. Our results\nhighlight the importance of high-quality retrieval stages and the impact of\ndifferent configurations on the quality of generated summaries.", "published": "2024-08-29 11:13:23", "link": "http://arxiv.org/abs/2408.16444v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Alignment: Improving Alignment of Cultural Values in LLMs via\n  In-Context Learning", "abstract": "Improving the alignment of Large Language Models (LLMs) with respect to the\ncultural values that they encode has become an increasingly important topic. In\nthis work, we study whether we can exploit existing knowledge about cultural\nvalues at inference time to adjust model responses to cultural value probes. We\npresent a simple and inexpensive method that uses a combination of in-context\nlearning (ICL) and human survey data, and show that we can improve the\nalignment to cultural values across 5 models that include both English-centric\nand multilingual LLMs. Importantly, we show that our method could prove useful\nin test languages other than English and can improve alignment to the cultural\nvalues that correspond to a range of culturally diverse countries.", "published": "2024-08-29 12:18:04", "link": "http://arxiv.org/abs/2408.16482v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning from Negative Samples in Generative Biomedical Entity Linking", "abstract": "Generative models have become widely used in biomedical entity linking\n(BioEL) due to their excellent performance and efficient memory usage. However,\nthese models are usually trained only with positive samples--entities that\nmatch the input mention's identifier--and do not explicitly learn from hard\nnegative samples, which are entities that look similar but have different\nmeanings. To address this limitation, we introduce ANGEL (Learning from\nNegative Samples in Generative Biomedical Entity Linking), the first framework\nthat trains generative BioEL models using negative samples. Specifically, a\ngenerative model is initially trained to generate positive samples from the\nknowledge base for given input entities. Subsequently, both correct and\nincorrect outputs are gathered from the model's top-k predictions. The model is\nthen updated to prioritize the correct predictions through direct preference\noptimization. Our models fine-tuned with ANGEL outperform the previous best\nbaseline models by up to an average top-1 accuracy of 1.4% on five benchmarks.\nWhen incorporating our framework into pre-training, the performance improvement\nfurther increases to 1.7%, demonstrating its effectiveness in both the\npre-training and fine-tuning stages. Our code is available at\nhttps://github.com/dmis-lab/ANGEL.", "published": "2024-08-29 12:44:01", "link": "http://arxiv.org/abs/2408.16493v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLMs vs Established Text Augmentation Techniques for Classification:\n  When do the Benefits Outweight the Costs?", "abstract": "The generative large language models (LLMs) are increasingly being used for\ndata augmentation tasks, where text samples are LLM-paraphrased and then used\nfor classifier fine-tuning. However, a research that would confirm a clear\ncost-benefit advantage of LLMs over more established augmentation methods is\nlargely missing. To study if (and when) is the LLM-based augmentation\nadvantageous, we compared the effects of recent LLM augmentation methods with\nestablished ones on 6 datasets, 3 classifiers and 2 fine-tuning methods. We\nalso varied the number of seeds and collected samples to better explore the\ndownstream model accuracy space. Finally, we performed a cost-benefit analysis\nand show that LLM-based methods are worthy of deployment only when very small\nnumber of seeds is used. Moreover, in many cases, established methods lead to\nsimilar or better model accuracies.", "published": "2024-08-29 13:01:42", "link": "http://arxiv.org/abs/2408.16502v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Interpretable and Crosslingual Method for Evaluating Second-Language\n  Dialogues", "abstract": "We analyse the cross-lingual transferability of a dialogue evaluation\nframework that assesses the relationships between micro-level linguistic\nfeatures (e.g. backchannels) and macro-level interactivity labels (e.g. topic\nmanagement), originally designed for English-as-a-second-language dialogues. To\nthis end, we develop CNIMA (Chinese Non-Native Interactivity Measurement and\nAutomation), a Chinese-as-a-second-language labelled dataset with 10K\ndialogues. We found the evaluation framework to be robust across distinct\nlanguages: English and Chinese, revealing language-specific and\nlanguage-universal relationships between micro-level and macro-level features.\nNext, we propose an automated, interpretable approach with low data requirement\nthat scores the overall quality of a second-language dialogue based on the\nframework. Our approach is interpretable in that it reveals the key linguistic\nand interactivity features that contributed to the overall quality score. As\nour approach does not require labelled data, it can also be adapted to other\nlanguages for second-language dialogue evaluation.", "published": "2024-08-29 13:28:52", "link": "http://arxiv.org/abs/2408.16518v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Theoretical and Methodological Framework for Studying Texts Produced by\n  Large Language Models", "abstract": "This paper addresses the conceptual, methodological and technical challenges\nin studying large language models (LLMs) and the texts they produce from a\nquantitative linguistics perspective. It builds on a theoretical framework that\ndistinguishes between the LLM as a substrate and the entities the model\nsimulates. The paper advocates for a strictly non-anthropomorphic approach to\nmodels while cautiously applying methodologies used in studying human\nlinguistic behavior to the simulated entities. While natural language\nprocessing researchers focus on the models themselves, their architecture,\nevaluation, and methods for improving performance, we as quantitative linguists\nshould strive to build a robust theory concerning the characteristics of texts\nproduced by LLMs, how they differ from human-produced texts, and the properties\nof simulated entities. Additionally, we should explore the potential of LLMs as\nan instrument for studying human culture, of which language is an integral\npart.", "published": "2024-08-29 17:34:10", "link": "http://arxiv.org/abs/2408.16740v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Well Do LLMs Handle Cantonese? Benchmarking Cantonese Capabilities\n  of Large Language Models", "abstract": "The rapid evolution of large language models (LLMs) has transformed the\ncompetitive landscape in natural language processing (NLP), particularly for\nEnglish and other data-rich languages. However, underrepresented languages like\nCantonese, spoken by over 85 million people, face significant development gaps,\nwhich is particularly concerning given the economic significance of the\nGuangdong-Hong Kong-Macau Greater Bay Area, and in substantial\nCantonese-speaking populations in places like Singapore and North America.\nDespite its wide use, Cantonese has scant representation in NLP research,\nespecially compared to other languages from similarly developed regions. To\nbridge these gaps, we outline current Cantonese NLP methods and introduce new\nbenchmarks designed to evaluate LLM performance in factual generation,\nmathematical logic, complex reasoning, and general knowledge in Cantonese,\nwhich aim to advance open-source Cantonese LLM technology. We also propose\nfuture research directions and recommended models to enhance Cantonese LLM\ndevelopment.", "published": "2024-08-29 17:54:14", "link": "http://arxiv.org/abs/2408.16756v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling offensive content detection for TikTok", "abstract": "The advent of social media transformed interpersonal communication and\ninformation consumption processes. This digital landscape accommodates user\nintentions, also resulting in an increase of offensive language and harmful\nbehavior. Concurrently, social media platforms collect vast datasets comprising\nuser-generated content and behavioral information. These datasets are\ninstrumental for platforms deploying machine learning and data-driven\nstrategies, facilitating customer insights and countermeasures against social\nmanipulation mechanisms like disinformation and offensive content.\nNevertheless, the availability of such datasets, along with the application of\nvarious machine learning techniques, to researchers and practitioners, for\nspecific social media platforms regarding particular events, is limited. In\nparticular for TikTok, which offers unique tools for personalized content\ncreation and sharing, the existing body of knowledge would benefit from having\ndiverse comprehensive datasets and associated data analytics solutions on\noffensive content. While efforts from social media platforms, research, and\npractitioner communities are seen on this behalf, such content continues to\nproliferate. This translates to an essential need to make datasets publicly\navailable and build corresponding intelligent solutions. On this behalf, this\nresearch undertakes the collection and analysis of TikTok data containing\noffensive content, building a series of machine learning and deep learning\nmodels for offensive content detection. This is done aiming at answering the\nfollowing research question: \"How to develop a series of computational models\nto detect offensive content on TikTok?\". To this end, a Data Science\nmethodological approach is considered, 120.423 TikTok comments are collected,\nand on a balanced, binary classification approach, F1 score performance results\nof 0.863 is obtained.", "published": "2024-08-29 18:47:41", "link": "http://arxiv.org/abs/2408.16857v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Plausible-Parrots @ MSP2023: Enhancing Semantic Plausibility Modeling\n  using Entity and Event Knowledge", "abstract": "In this work, we investigate the effectiveness of injecting external\nknowledge to a large language model (LLM) to identify semantic plausibility of\nsimple events. Specifically, we enhance the LLM with fine-grained entity types,\nevent types and their definitions extracted from an external knowledge base.\nThese knowledge are injected into our system via designed templates. We also\naugment the data to balance the label distribution and adapt the task setting\nto real world scenarios in which event mentions are expressed as natural\nlanguage sentences. The experimental results show the effectiveness of the\ninjected knowledge on modeling semantic plausibility of events. An error\nanalysis further emphasizes the importance of identifying non-trivial entity\nand event types.", "published": "2024-08-29 23:13:45", "link": "http://arxiv.org/abs/2408.16937v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ReXamine-Global: A Framework for Uncovering Inconsistencies in Radiology\n  Report Generation Metrics", "abstract": "Given the rapidly expanding capabilities of generative AI models for\nradiology, there is a need for robust metrics that can accurately measure the\nquality of AI-generated radiology reports across diverse hospitals. We develop\nReXamine-Global, a LLM-powered, multi-site framework that tests metrics across\ndifferent writing styles and patient populations, exposing gaps in their\ngeneralization. First, our method tests whether a metric is undesirably\nsensitive to reporting style, providing different scores depending on whether\nAI-generated reports are stylistically similar to ground-truth reports or not.\nSecond, our method measures whether a metric reliably agrees with experts, or\nwhether metric and expert scores of AI-generated report quality diverge for\nsome sites. Using 240 reports from 6 hospitals around the world, we apply\nReXamine-Global to 7 established report evaluation metrics and uncover serious\ngaps in their generalizability. Developers can apply ReXamine-Global when\ndesigning new report evaluation metrics, ensuring their robustness across\nsites. Additionally, our analysis of existing metrics can guide users of those\nmetrics towards evaluation procedures that work reliably at their sites of\ninterest.", "published": "2024-08-29 02:03:05", "link": "http://arxiv.org/abs/2408.16208v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "LoraMap: Harnessing the Power of LoRA Connections", "abstract": "Fact-checking techniques can mitigate hallucinations in Large Language Models\n(LLMs), a prominent issue in specialized domains. As parameter-efficient\ntechniques such as Low-Rank Adaptation (LoRA) can overcome substantial\ncomputational overhead, some studies have explored the integration of multiple\nLoRAs. While previous studies focus on parallel integration, this paper\ninvestigates methods to establish connections among multiple LoRAs. We create\nthree reasoning datasets tailored to fact-checking and fine-tune individual\nLoRAs, allowing them to view and reason from diverse perspectives. Then, we\nexplore strategies for allocating these reasoning LoRAs and introduce LoraMap,\nan approach to map connections between them. The results of the fact-checking\ntask demonstrate that the performance of LoraMap is superior to LoraHub, an\nexisting method for integrating LoRAs. LoraMap also outperforms with\nsignificantly fewer trainable parameters than LoraConcat, which concatenates\nLoRAs and further fine-tunes them.", "published": "2024-08-29 05:02:52", "link": "http://arxiv.org/abs/2408.16264v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Is text normalization relevant for classifying medieval charters?", "abstract": "This study examines the impact of historical text normalization on the\nclassification of medieval charters, specifically focusing on document dating\nand locating. Using a data set of Middle High German charters from a digital\narchive, we evaluate various classifiers, including traditional and\ntransformer-based models, with and without normalization. Our results indicate\nthat the given normalization minimally improves locating tasks but reduces\naccuracy for dating, implying that original texts contain crucial features that\nnormalization may obscure. We find that support vector machines and gradient\nboosting outperform other models, questioning the efficiency of transformers\nfor this use case. Results suggest a selective approach to historical text\nnormalization, emphasizing the significance of preserving some textual\ncharacteristics that are critical for classification tasks in document\nanalysis.", "published": "2024-08-29 11:19:57", "link": "http://arxiv.org/abs/2408.16446v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Enhancing Dialogue Generation in Werewolf Game Through Situation\n  Analysis and Persuasion Strategies", "abstract": "Recent advancements in natural language processing, particularly with large\nlanguage models (LLMs) like GPT-4, have significantly enhanced dialogue\nsystems, enabling them to generate more natural and fluent conversations.\nDespite these improvements, challenges persist, such as managing continuous\ndialogues, memory retention, and minimizing hallucinations. The AIWolfDial2024\naddresses these challenges by employing the Werewolf Game, an incomplete\ninformation game, to test the capabilities of LLMs in complex interactive\nenvironments. This paper introduces a LLM-based Werewolf Game AI, where each\nrole is supported by situation analysis to aid response generation.\nAdditionally, for the werewolf role, various persuasion strategies, including\nlogical appeal, credibility appeal, and emotional appeal, are employed to\neffectively persuade other players to align with its actions.", "published": "2024-08-29 14:49:13", "link": "http://arxiv.org/abs/2408.16586v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal\n  Sampling", "abstract": "Training on high-quality synthetic data from strong language models (LMs) is\na common strategy to improve the reasoning performance of LMs. In this work, we\nrevisit whether this strategy is compute-optimal under a fixed inference budget\n(e.g., FLOPs). To do so, we investigate the trade-offs between generating\nsynthetic data using a stronger but more expensive (SE) model versus a weaker\nbut cheaper (WC) model. We evaluate the generated data across three key\nmetrics: coverage, diversity, and false positive rate, and show that the data\nfrom WC models may have higher coverage and diversity, but also exhibit higher\nfalse positive rates. We then finetune LMs on data from SE and WC models in\ndifferent settings: knowledge distillation, self-improvement, and a novel\nweak-to-strong improvement setup where a weaker LM teaches reasoning to a\nstronger LM. Our findings reveal that models finetuned on WC-generated data\nconsistently outperform those trained on SE-generated data across multiple\nbenchmarks and multiple choices of WC and SE models. These results challenge\nthe prevailing practice of relying on SE models for synthetic data generation,\nsuggesting that WC may be the compute-optimal approach for training advanced LM\nreasoners.", "published": "2024-08-29 17:32:35", "link": "http://arxiv.org/abs/2408.16737v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Assessing Large Language Models for Online Extremism Research:\n  Identification, Explanation, and New Knowledge", "abstract": "The United States has experienced a significant increase in violent\nextremism, prompting the need for automated tools to detect and limit the\nspread of extremist ideology online. This study evaluates the performance of\nBidirectional Encoder Representations from Transformers (BERT) and Generative\nPre-Trained Transformers (GPT) in detecting and classifying online domestic\nextremist posts. We collected social media posts containing \"far-right\" and\n\"far-left\" ideological keywords and manually labeled them as extremist or\nnon-extremist. Extremist posts were further classified into one or more of five\ncontributing elements of extremism based on a working definitional framework.\nThe BERT model's performance was evaluated based on training data size and\nknowledge transfer between categories. We also compared the performance of GPT\n3.5 and GPT 4 models using different prompts: na\\\"ive, layperson-definition,\nrole-playing, and professional-definition. Results showed that the best\nperforming GPT models outperformed the best performing BERT models, with more\ndetailed prompts generally yielding better results. However, overly complex\nprompts may impair performance. Different versions of GPT have unique\nsensitives to what they consider extremist. GPT 3.5 performed better at\nclassifying far-left extremist posts, while GPT 4 performed better at\nclassifying far-right extremist posts. Large language models, represented by\nGPT models, hold significant potential for online extremism classification\ntasks, surpassing traditional BERT models in a zero-shot setting. Future\nresearch should explore human-computer interactions in optimizing GPT models\nfor extremist detection and classification tasks to develop more efficient\n(e.g., quicker, less effort) and effective (e.g., fewer errors or mistakes)\nmethods for identifying extremist content.", "published": "2024-08-29 17:43:03", "link": "http://arxiv.org/abs/2408.16749v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Reinforcement Learning without Human Feedback for Last Mile Fine-Tuning\n  of Large Language Models", "abstract": "Reinforcement learning is used to align language models with human preference\nsignals after first pre-training the model to predict the next token of text\nwithin a large corpus using likelihood maximization. Before being deployed in a\nspecific domain, models are often further fine-tuned on task specific data.\nSince human preferences are often unavailable for the last step, it is\nperformed using likelihood maximization as that is the typical default method.\nHowever, reinforcement learning has other advantages besides facilitating\nalignment to a human derived reward function. For one, whereas likelihood\nmaximization is a form of imitation learning in which the model is trained on\nwhat to do under ideal conditions, reinforcement learning is not limited to\ndemonstrating actions just for optimally reached states and trains a model what\nto do under a range of scenarios as it explores the policy space. In addition,\nit also trains a model what not to do, suppressing competitive but poor\nactions. This work develops a framework for last-mile fine-tuning using\nreinforcement learning and tests whether it garners performance gains. The\nexperiments center on abstractive summarization, but the framework is general\nand broadly applicable. Use of the procedure produced significantly better\nresults than likelihood maximization when comparing raw predictions. For the\nspecific data tested, the gap could be bridged by employing post-processing of\nthe maximum likelihood outputs. Nonetheless, the framework offers a new avenue\nfor model optimization in situations where post-processing may be less\nstraightforward or effective, and it can be extended to include more complex\nclasses of undesirable outputs to penalize and train against, such as\nhallucinations.", "published": "2024-08-29 17:49:18", "link": "http://arxiv.org/abs/2408.16753v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LLaVA-Chef: A Multi-modal Generative Model for Food Recipes", "abstract": "In the rapidly evolving landscape of online recipe sharing within a\nglobalized context, there has been a notable surge in research towards\ncomprehending and generating food recipes. Recent advancements in large\nlanguage models (LLMs) like GPT-2 and LLaVA have paved the way for Natural\nLanguage Processing (NLP) approaches to delve deeper into various facets of\nfood-related tasks, encompassing ingredient recognition and comprehensive\nrecipe generation. Despite impressive performance and multi-modal adaptability\nof LLMs, domain-specific training remains paramount for their effective\napplication. This work evaluates existing LLMs for recipe generation and\nproposes LLaVA-Chef, a novel model trained on a curated dataset of diverse\nrecipe prompts in a multi-stage approach. First, we refine the mapping of\nvisual food image embeddings to the language space. Second, we adapt LLaVA to\nthe food domain by fine-tuning it on relevant recipe data. Third, we utilize\ndiverse prompts to enhance the model's recipe comprehension. Finally, we\nimprove the linguistic quality of generated recipes by penalizing the model\nwith a custom loss function. LLaVA-Chef demonstrates impressive improvements\nover pretrained LLMs and prior works. A detailed qualitative analysis reveals\nthat LLaVA-Chef generates more detailed recipes with precise ingredient\nmentions, compared to existing approaches.", "published": "2024-08-29 20:20:49", "link": "http://arxiv.org/abs/2408.16889v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploring Multiple Strategies to Improve Multilingual Coreference\n  Resolution in CorefUD", "abstract": "Coreference resolution, the task of identifying expressions in text that\nrefer to the same entity, is a critical component in various natural language\nprocessing applications. This paper presents a novel end-to-end neural\ncoreference resolution system utilizing the CorefUD 1.1 dataset, which spans 17\ndatasets across 12 languages. The proposed model is based on the standard\nend-to-end neural coreference resolution system. We first establish baseline\nmodels, including monolingual and cross-lingual variations, and then propose\nseveral extensions to enhance performance across diverse linguistic contexts.\nThese extensions include cross-lingual training, incorporation of syntactic\ninformation, a Span2Head model for optimized headword prediction, and advanced\nsingleton modeling. We also experiment with headword span representation and\nlong-documents modeling through overlapping segments. The proposed extensions,\nparticularly the heads-only approach, singleton modeling, and long document\nprediction, significantly improve performance across most datasets. We also\nperform zero-shot cross-lingual experiments, highlighting the potential and\nlimitations of cross-lingual transfer in coreference resolution. Our findings\ncontribute to the development of robust and scalable coreference systems for\nmultilingual coreference resolution. Finally, we evaluate our model on the\nCorefUD 1.1 test set and surpass the best model from the CRAC 2023 shared task\nof comparable size by a large margin.", "published": "2024-08-29 20:27:05", "link": "http://arxiv.org/abs/2408.16893v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ACE-2005-PT: Corpus for Event Extraction in Portuguese", "abstract": "Event extraction is an NLP task that commonly involves identifying the\ncentral word (trigger) for an event and its associated arguments in text.\nACE-2005 is widely recognised as the standard corpus in this field. While other\ncorpora, like PropBank, primarily focus on annotating predicate-argument\nstructure, ACE-2005 provides comprehensive information about the overall event\nstructure and semantics. However, its limited language coverage restricts its\nusability. This paper introduces ACE-2005-PT, a corpus created by translating\nACE-2005 into Portuguese, with European and Brazilian variants. To speed up the\nprocess of obtaining ACE-2005-PT, we rely on automatic translators. This,\nhowever, poses some challenges related to automatically identifying the correct\nalignments between multi-word annotations in the original text and in the\ncorresponding translated sentence. To achieve this, we developed an alignment\npipeline that incorporates several alignment techniques: lemmatization, fuzzy\nmatching, synonym matching, multiple translations and a BERT-based word\naligner. To measure the alignment effectiveness, a subset of annotations from\nthe ACE-2005-PT corpus was manually aligned by a linguist expert. This subset\nwas then compared against our pipeline results which achieved exact and relaxed\nmatch scores of 70.55\\% and 87.55\\% respectively. As a result, we successfully\ngenerated a Portuguese version of the ACE-2005 corpus, which has been accepted\nfor publication by LDC.", "published": "2024-08-29 22:05:08", "link": "http://arxiv.org/abs/2408.16928v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Event Extraction for Portuguese: A QA-driven Approach using ACE-2005", "abstract": "Event extraction is an Information Retrieval task that commonly consists of\nidentifying the central word for the event (trigger) and the event's arguments.\nThis task has been extensively studied for English but lags behind for\nPortuguese, partly due to the lack of task-specific annotated corpora. This\npaper proposes a framework in which two separated BERT-based models were\nfine-tuned to identify and classify events in Portuguese documents. We\ndecompose this task into two sub-tasks. Firstly, we use a token classification\nmodel to detect event triggers. To extract event arguments, we train a Question\nAnswering model that queries the triggers about their corresponding event\nargument roles. Given the lack of event annotated corpora in Portuguese, we\ntranslated the original version of the ACE-2005 dataset (a reference in the\nfield) into Portuguese, producing a new corpus for Portuguese event extraction.\nTo accomplish this, we developed an automatic translation pipeline. Our\nframework obtains F1 marks of 64.4 for trigger classification and 46.7 for\nargument classification setting, thus a new state-of-the-art reference for\nthese tasks in Portuguese.", "published": "2024-08-29 22:14:21", "link": "http://arxiv.org/abs/2408.16932v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A longitudinal sentiment analysis of Sinophobia during COVID-19 using\n  large language models", "abstract": "The COVID-19 pandemic has exacerbated xenophobia, particularly Sinophobia,\nleading to widespread discrimination against individuals of Chinese descent.\nLarge language models (LLMs) are pre-trained deep learning models used for\nnatural language processing (NLP) tasks. The ability of LLMs to understand and\ngenerate human-like text makes them particularly useful for analysing social\nmedia data to detect and evaluate sentiments. We present a sentiment analysis\nframework utilising LLMs for longitudinal sentiment analysis of the Sinophobic\nsentiments expressed in X (Twitter) during the COVID-19 pandemic. The results\nshow a significant correlation between the spikes in Sinophobic tweets,\nSinophobic sentiments and surges in COVID-19 cases, revealing that the\nevolution of the pandemic influenced public sentiment and the prevalence of\nSinophobic discourse. Furthermore, the sentiment analysis revealed a\npredominant presence of negative sentiments, such as annoyance and denial,\nwhich underscores the impact of political narratives and misinformation shaping\npublic opinion. The lack of empathetic sentiment which was present in previous\nstudies related to COVID-19 highlights the way the political narratives in\nmedia viewed the pandemic and how it blamed the Chinese community. Our study\nhighlights the importance of transparent communication in mitigating xenophobic\nsentiments during global crises.", "published": "2024-08-29 23:39:11", "link": "http://arxiv.org/abs/2408.16942v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Logic Contrastive Reasoning with Lightweight Large Language Model for\n  Math Word Problems", "abstract": "This study focuses on improving the performance of lightweight Large Language\nModels (LLMs) in mathematical reasoning tasks. We introduce a novel method for\nmeasuring mathematical logic similarity and design an automatic screening\nmechanism to construct a set of reference problems that integrate both semantic\nand logical similarity. By employing carefully crafted positive and negative\nexample prompts, we guide the model towards adopting sound reasoning logic. To\nthe best of our knowledge, this is the first attempt to utilize\nretrieval-enhanced generation for mathematical problem-solving. Experimental\nresults demonstrate that our method achieves a 15.8% improvement over the Chain\nof Thought approach on the SVAMP dataset and a 21.5 % improvement on the GSM8K\ndataset. Further application of this method to a large-scale model with 175\nbillion parameters yields performance comparable to the best results on both\naforementioned datasets. Finally, we conduct an analysis of errors during the\nreasoning process, providing valuable insights and directions for future\nresearch on reasoning tasks using large language models.", "published": "2024-08-29 08:26:42", "link": "http://arxiv.org/abs/2409.00131v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Survey for Large Language Models in Biomedicine", "abstract": "Recent breakthroughs in large language models (LLMs) offer unprecedented\nnatural language understanding and generation capabilities. However, existing\nsurveys on LLMs in biomedicine often focus on specific applications or model\narchitectures, lacking a comprehensive analysis that integrates the latest\nadvancements across various biomedical domains. This review, based on an\nanalysis of 484 publications sourced from databases including PubMed, Web of\nScience, and arXiv, provides an in-depth examination of the current landscape,\napplications, challenges, and prospects of LLMs in biomedicine, distinguishing\nitself by focusing on the practical implications of these models in real-world\nbiomedical contexts. Firstly, we explore the capabilities of LLMs in zero-shot\nlearning across a broad spectrum of biomedical tasks, including diagnostic\nassistance, drug discovery, and personalized medicine, among others, with\ninsights drawn from 137 key studies. Then, we discuss adaptation strategies of\nLLMs, including fine-tuning methods for both uni-modal and multi-modal LLMs to\nenhance their performance in specialized biomedical contexts where zero-shot\nfails to achieve, such as medical question answering and efficient processing\nof biomedical literature. Finally, we discuss the challenges that LLMs face in\nthe biomedicine domain including data privacy concerns, limited model\ninterpretability, issues with dataset quality, and ethics due to the sensitive\nnature of biomedical data, the need for highly reliable model outputs, and the\nethical implications of deploying AI in healthcare. To address these\nchallenges, we also identify future research directions of LLM in biomedicine\nincluding federated learning methods to preserve data privacy and integrating\nexplainable AI methodologies to enhance the transparency of LLMs.", "published": "2024-08-29 12:39:16", "link": "http://arxiv.org/abs/2409.00133v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "HoneyComb: A Flexible LLM-Based Agent System for Materials Science", "abstract": "The emergence of specialized large language models (LLMs) has shown promise\nin addressing complex tasks for materials science. Many LLMs, however, often\nstruggle with distinct complexities of material science tasks, such as\nmaterials science computational tasks, and often rely heavily on outdated\nimplicit knowledge, leading to inaccuracies and hallucinations. To address\nthese challenges, we introduce HoneyComb, the first LLM-based agent system\nspecifically designed for materials science. HoneyComb leverages a novel,\nhigh-quality materials science knowledge base (MatSciKB) and a sophisticated\ntool hub (ToolHub) to enhance its reasoning and computational capabilities\ntailored to materials science. MatSciKB is a curated, structured knowledge\ncollection based on reliable literature, while ToolHub employs an Inductive\nTool Construction method to generate, decompose, and refine API tools for\nmaterials science. Additionally, HoneyComb leverages a retriever module that\nadaptively selects the appropriate knowledge source or tools for specific\ntasks, thereby ensuring accuracy and relevance. Our results demonstrate that\nHoneyComb significantly outperforms baseline models across various tasks in\nmaterials science, effectively bridging the gap between current LLM\ncapabilities and the specialized needs of this domain. Furthermore, our\nadaptable framework can be easily extended to other scientific domains,\nhighlighting its potential for broad applicability in advancing scientific\nresearch and applications.", "published": "2024-08-29 15:38:40", "link": "http://arxiv.org/abs/2409.00135v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Benchmarking Japanese Speech Recognition on ASR-LLM Setups with\n  Multi-Pass Augmented Generative Error Correction", "abstract": "With the strong representational power of large language models (LLMs),\ngenerative error correction (GER) for automatic speech recognition (ASR) aims\nto provide semantic and phonetic refinements to address ASR errors. This work\nexplores how LLM-based GER can enhance and expand the capabilities of Japanese\nlanguage processing, presenting the first GER benchmark for Japanese ASR with\n0.9-2.6k text utterances. We also introduce a new multi-pass augmented\ngenerative error correction (MPA GER) by integrating multiple system hypotheses\non the input side with corrections from multiple LLMs on the output side and\nthen merging them. To the best of our knowledge, this is the first\ninvestigation of the use of LLMs for Japanese GER, which involves second-pass\nlanguage modeling on the output transcriptions generated by the ASR system\n(e.g., N-best hypotheses). Our experiments demonstrated performance improvement\nin the proposed methods of ASR quality and generalization both in SPREDS-U1-ja\nand CSJ data.", "published": "2024-08-29 00:18:12", "link": "http://arxiv.org/abs/2408.16180v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "M4CXR: Exploring Multi-task Potentials of Multi-modal Large Language\n  Models for Chest X-ray Interpretation", "abstract": "The rapid evolution of artificial intelligence, especially in large language\nmodels (LLMs), has significantly impacted various domains, including\nhealthcare. In chest X-ray (CXR) analysis, previous studies have employed LLMs,\nbut with limitations: either underutilizing the multi-tasking capabilities of\nLLMs or lacking clinical accuracy. This paper presents M4CXR, a multi-modal LLM\ndesigned to enhance CXR interpretation. The model is trained on a visual\ninstruction-following dataset that integrates various task-specific datasets in\na conversational format. As a result, the model supports multiple tasks such as\nmedical report generation (MRG), visual grounding, and visual question\nanswering (VQA). M4CXR achieves state-of-the-art clinical accuracy in MRG by\nemploying a chain-of-thought prompting strategy, in which it identifies\nfindings in CXR images and subsequently generates corresponding reports. The\nmodel is adaptable to various MRG scenarios depending on the available inputs,\nsuch as single-image, multi-image, and multi-study contexts. In addition to\nMRG, M4CXR performs visual grounding at a level comparable to specialized\nmodels and also demonstrates outstanding performance in VQA. Both quantitative\nand qualitative assessments reveal M4CXR's versatility in MRG, visual\ngrounding, and VQA, while consistently maintaining clinical accuracy.", "published": "2024-08-29 02:12:58", "link": "http://arxiv.org/abs/2408.16213v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "SSDM: Scalable Speech Dysfluency Modeling", "abstract": "Speech dysfluency modeling is the core module for spoken language learning,\nand speech therapy. However, there are three challenges. First, current\nstate-of-the-art solutions\\cite{lian2023unconstrained-udm,\nlian-anumanchipalli-2024-towards-hudm} suffer from poor scalability. Second,\nthere is a lack of a large-scale dysfluency corpus. Third, there is not an\neffective learning framework. In this paper, we propose \\textit{SSDM: Scalable\nSpeech Dysfluency Modeling}, which (1) adopts articulatory gestures as scalable\nforced alignment; (2) introduces connectionist subsequence aligner (CSA) to\nachieve dysfluency alignment; (3) introduces a large-scale simulated dysfluency\ncorpus called Libri-Dys; and (4) develops an end-to-end system by leveraging\nthe power of large language models (LLMs). We expect SSDM to serve as a\nstandard in the area of dysfluency modeling. Demo is available at\n\\url{https://berkeley-speech-group.github.io/SSDM/}.", "published": "2024-08-29 02:35:53", "link": "http://arxiv.org/abs/2408.16221v3", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Measuring the Accuracy of Automatic Speech Recognition Solutions", "abstract": "For d/Deaf and hard of hearing (DHH) people, captioning is an essential\naccessibility tool. Significant developments in artificial intelligence (AI)\nmean that Automatic Speech Recognition (ASR) is now a part of many popular\napplications. This makes creating captions easy and broadly available - but\ntranscription needs high levels of accuracy to be accessible. Scientific\npublications and industry report very low error rates, claiming AI has reached\nhuman parity or even outperforms manual transcription. At the same time the DHH\ncommunity reports serious issues with the accuracy and reliability of ASR.\nThere seems to be a mismatch between technical innovations and the real-life\nexperience for people who depend on transcription. Independent and\ncomprehensive data is needed to capture the state of ASR. We measured the\nperformance of eleven common ASR services with recordings of Higher Education\nlectures. We evaluated the influence of technical conditions like streaming,\nthe use of vocabularies, and differences between languages. Our results show\nthat accuracy ranges widely between vendors and for the individual audio\nsamples. We also measured a significant lower quality for streaming ASR, which\nis used for live events. Our study shows that despite the recent improvements\nof ASR, common services lack reliability in accuracy.", "published": "2024-08-29 06:38:55", "link": "http://arxiv.org/abs/2408.16287v1", "categories": ["cs.CL", "cs.SD", "eess.AS", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Physics of Language Models: Part 2.2, How to Learn From Mistakes on\n  Grade-School Math Problems", "abstract": "Language models have demonstrated remarkable performance in solving reasoning\ntasks; however, even the strongest models still occasionally make reasoning\nmistakes. Recently, there has been active research aimed at improving reasoning\naccuracy, particularly by using pretrained language models to \"self-correct\"\ntheir mistakes via multi-round prompting. In this paper, we follow this line of\nwork but focus on understanding the usefulness of incorporating\n\"error-correction\" data directly into the pretraining stage. This data consists\nof erroneous solution steps immediately followed by their corrections. Using a\nsynthetic math dataset, we show promising results: this type of pretrain data\ncan help language models achieve higher reasoning accuracy directly (i.e.,\nthrough simple auto-regression, without multi-round prompting) compared to\npretraining on the same amount of error-free data. We also delve into many\ndetails, such as (1) how this approach differs from beam search, (2) how such\ndata can be prepared, (3) whether masking is needed on the erroneous tokens,\n(4) the amount of error required, (5) whether such data can be deferred to the\nfine-tuning stage, and many others.", "published": "2024-08-29 06:49:20", "link": "http://arxiv.org/abs/2408.16293v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SALSA: Speedy ASR-LLM Synchronous Aggregation", "abstract": "Harnessing pre-trained LLMs to improve ASR systems, particularly for\nlow-resource languages, is now an emerging area of research. Existing methods\nrange from using LLMs for ASR error correction to tightly coupled systems that\nreplace the ASR decoder with the LLM. These approaches either increase decoding\ntime or require expensive training of the cross-attention layers. We propose\nSALSA, which couples the decoder layers of the ASR to the LLM decoder, while\nsynchronously advancing both decoders. Such coupling is performed with a simple\nprojection of the last decoder state, and is thus significantly more training\nefficient than earlier approaches. A challenge of our proposed coupling is\nhandling the mismatch between the tokenizers of the LLM and ASR systems. We\nhandle this mismatch using cascading tokenization with respect to the LLM and\nASR vocabularies. We evaluate SALSA on 8 low-resource languages in the FLEURS\nbenchmark, yielding substantial WER reductions of up to 38%.", "published": "2024-08-29 14:00:57", "link": "http://arxiv.org/abs/2408.16542v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Predictability maximization and the origins of word order harmony", "abstract": "We address the linguistic problem of the sequential arrangement of a head and\nits dependents from an information theoretic perspective. In particular, we\nconsider the optimal placement of a head that maximizes the predictability of\nthe sequence. We assume that dependents are statistically independent given a\nhead, in line with the open-choice principle and the core assumptions of\ndependency grammar. We demonstrate the optimality of harmonic order, i.e.,\nplacing the head last maximizes the predictability of the head whereas placing\nthe head first maximizes the predictability of dependents. We also show that\npostponing the head is the optimal strategy to maximize its predictability\nwhile bringing it forward is the optimal strategy to maximize the\npredictability of dependents. We unravel the advantages of the strategy of\nmaximizing the predictability of the head over maximizing the predictability of\ndependents. Our findings shed light on the placements of the head adopted by\nreal languages or emerging in different kinds of experiments.", "published": "2024-08-29 14:37:05", "link": "http://arxiv.org/abs/2408.16570v5", "categories": ["cs.CL", "physics.soc-ph", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "Iterative Graph Alignment", "abstract": "By compressing diverse narratives, LLMs go beyond memorization, achieving\nintelligence by capturing generalizable causal relationships. However, they\nsuffer from local 'representation gaps' due to insufficient training data\ndiversity, limiting their real-world utility, especially in tasks requiring\nstrict alignment to rules. Traditional alignment methods relying on heavy human\nannotations are inefficient and unscalable. Recent self-alignment techniques\nalso fall short, as they often depend on self-selection based prompting and\nmemorization-based learning. To address these issues, we introduce Iterative\nGraph Alignment (IGA), an annotation-free rule-based alignment algorithm. A\nteacher model (VLM) employs Iterative Graph Prompting (IGP) to create logical\ngraphs and reference answers. The student model (LLM) identifies local\nknowledge gaps by attempting to align its responses with these references,\ncollaborating with helper models to generate diverse answers. These aligned\nresponses are then used for iterative supervised fine-tuning (SFT). Our\nevaluations across five rule-based scenarios demonstrate IGP's effectiveness,\nwith a 73.12\\% alignment improvement in Claude Sonnet 3.5, and\nLlama3-8B-Instruct achieving an 86.20\\% improvement, outperforming Claude\nSonnet 3.5 in rule-based alignment.", "published": "2024-08-29 16:15:01", "link": "http://arxiv.org/abs/2408.16667v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.LG"}
{"title": "Jina-ColBERT-v2: A General-Purpose Multilingual Late Interaction\n  Retriever", "abstract": "Multi-vector dense models, such as ColBERT, have proven highly effective in\ninformation retrieval. ColBERT's late interaction scoring approximates the\njoint query-document attention seen in cross-encoders while maintaining\ninference efficiency closer to traditional dense retrieval models, thanks to\nits bi-encoder architecture and recent optimizations in indexing and search. In\nthis work we propose a number of incremental improvements to the ColBERT model\narchitecture and training pipeline, using methods shown to work in the more\nmature single-vector embedding model training paradigm, particularly those that\napply to heterogeneous multilingual data or boost efficiency with little\ntradeoff. Our new model, Jina-ColBERT-v2, demonstrates strong performance\nacross a range of English and multilingual retrieval tasks.", "published": "2024-08-29 16:21:00", "link": "http://arxiv.org/abs/2408.16672v4", "categories": ["cs.IR", "cs.AI", "cs.CL", "68T50", "I.2.7"], "primary_category": "cs.IR"}
{"title": "A Gradient Analysis Framework for Rewarding Good and Penalizing Bad\n  Examples in Language Models", "abstract": "Beyond maximum likelihood estimation (MLE), the standard objective of a\nlanguage model (LM) that optimizes good examples probabilities, many studies\nhave explored ways that also penalize bad examples for enhancing the quality of\noutput distribution, including unlikelihood training, exponential maximizing\naverage treatment effect (ExMATE), and direct preference optimization (DPO). To\nsystematically compare these methods and further provide a unified recipe for\nLM optimization, in this paper, we present a unique angle of gradient analysis\nof loss functions that simultaneously reward good examples and penalize bad\nones in LMs. Through both mathematical results and experiments on\nCausalDialogue and Anthropic HH-RLHF datasets, we identify distinct functional\ncharacteristics among these methods. We find that ExMATE serves as a superior\nsurrogate for MLE, and that combining DPO with ExMATE instead of MLE further\nenhances both the statistical (5-7%) and generative (+18% win rate)\nperformance.", "published": "2024-08-29 17:46:18", "link": "http://arxiv.org/abs/2408.16751v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "SAM2Point: Segment Any 3D as Videos in Zero-shot and Promptable Manners", "abstract": "We introduce SAM2Point, a preliminary exploration adapting Segment Anything\nModel 2 (SAM 2) for zero-shot and promptable 3D segmentation. SAM2Point\ninterprets any 3D data as a series of multi-directional videos, and leverages\nSAM 2 for 3D-space segmentation, without further training or 2D-3D projection.\nOur framework supports various prompt types, including 3D points, boxes, and\nmasks, and can generalize across diverse scenarios, such as 3D objects, indoor\nscenes, outdoor environments, and raw sparse LiDAR. Demonstrations on multiple\n3D datasets, e.g., Objaverse, S3DIS, ScanNet, Semantic3D, and KITTI, highlight\nthe robust generalization capabilities of SAM2Point. To our best knowledge, we\npresent the most faithful implementation of SAM in 3D, which may serve as a\nstarting point for future research in promptable 3D segmentation. Online Demo:\nhttps://huggingface.co/spaces/ZiyuG/SAM2Point . Code:\nhttps://github.com/ZiyuGuo99/SAM2Point .", "published": "2024-08-29 17:59:45", "link": "http://arxiv.org/abs/2408.16768v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "See or Guess: Counterfactually Regularized Image Captioning", "abstract": "Image captioning, which generates natural language descriptions of the visual\ninformation in an image, is a crucial task in vision-language research.\nPrevious models have typically addressed this task by aligning the generative\ncapabilities of machines with human intelligence through statistical fitting of\nexisting datasets. While effective for normal images, they may struggle to\naccurately describe those where certain parts of the image are obscured or\nedited, unlike humans who excel in such cases. These weaknesses they exhibit,\nincluding hallucinations and limited interpretability, often hinder performance\nin scenarios with shifted association patterns. In this paper, we present a\ngeneric image captioning framework that employs causal inference to make\nexisting models more capable of interventional tasks, and counterfactually\nexplainable. Our approach includes two variants leveraging either total effect\nor natural direct effect. Integrating them into the training process enables\nmodels to handle counterfactual scenarios, increasing their generalizability.\nExtensive experiments on various datasets show that our method effectively\nreduces hallucinations and improves the model's faithfulness to images,\ndemonstrating high portability across both small-scale and large-scale\nimage-to-text models. The code is available at\nhttps://github.com/Aman-4-Real/See-or-Guess.", "published": "2024-08-29 17:59:57", "link": "http://arxiv.org/abs/2408.16809v1", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Can AI Replace Human Subjects? A Large-Scale Replication of\n  Psychological Experiments with LLMs", "abstract": "Artificial Intelligence (AI) is increasingly being integrated into scientific\nresearch, particularly in the social sciences, where understanding human\nbehavior is critical. Large Language Models (LLMs) like GPT-4 have shown\npromise in replicating human-like responses in various psychological\nexperiments. However, the extent to which LLMs can effectively replace human\nsubjects across diverse experimental contexts remains unclear. Here, we conduct\na large-scale study replicating 154 psychological experiments from top social\nscience journals with 618 main effects and 138 interaction effects using GPT-4\nas a simulated participant. We find that GPT-4 successfully replicates 76.0\npercent of main effects and 47.0 percent of interaction effects observed in the\noriginal studies, closely mirroring human responses in both direction and\nsignificance. However, only 19.44 percent of GPT-4's replicated confidence\nintervals contain the original effect sizes, with the majority of replicated\neffect sizes exceeding the 95 percent confidence interval of the original\nstudies. Additionally, there is a 71.6 percent rate of unexpected significant\nresults where the original studies reported null findings, suggesting potential\noverestimation or false positives. Our results demonstrate the potential of\nLLMs as powerful tools in psychological research but also emphasize the need\nfor caution in interpreting AI-driven findings. While LLMs can complement human\nstudies, they cannot yet fully replace the nuanced insights provided by human\nsubjects.", "published": "2024-08-29 05:18:50", "link": "http://arxiv.org/abs/2409.00128v2", "categories": ["cs.CL", "cs.AI", "econ.GN", "q-fin.EC"], "primary_category": "cs.CL"}
{"title": "Emerging Vulnerabilities in Frontier Models: Multi-Turn Jailbreak\n  Attacks", "abstract": "Large language models (LLMs) are improving at an exceptional rate. However,\nthese models are still susceptible to jailbreak attacks, which are becoming\nincreasingly dangerous as models become increasingly powerful. In this work, we\nintroduce a dataset of jailbreaks where each example can be input in both a\nsingle or a multi-turn format. We show that while equivalent in content, they\nare not equivalent in jailbreak success: defending against one structure does\nnot guarantee defense against the other. Similarly, LLM-based filter guardrails\nalso perform differently depending on not just the input content but the input\nstructure. Thus, vulnerabilities of frontier models should be studied in both\nsingle and multi-turn settings; this dataset provides a tool to do so.", "published": "2024-08-29 17:30:05", "link": "http://arxiv.org/abs/2409.00137v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "PrivacyLens: Evaluating Privacy Norm Awareness of Language Models in\n  Action", "abstract": "As language models (LMs) are widely utilized in personalized communication\nscenarios (e.g., sending emails, writing social media posts) and endowed with a\ncertain level of agency, ensuring they act in accordance with the contextual\nprivacy norms becomes increasingly critical. However, quantifying the privacy\nnorm awareness of LMs and the emerging privacy risk in LM-mediated\ncommunication is challenging due to (1) the contextual and long-tailed nature\nof privacy-sensitive cases, and (2) the lack of evaluation approaches that\ncapture realistic application scenarios. To address these challenges, we\npropose PrivacyLens, a novel framework designed to extend privacy-sensitive\nseeds into expressive vignettes and further into agent trajectories, enabling\nmulti-level evaluation of privacy leakage in LM agents' actions. We instantiate\nPrivacyLens with a collection of privacy norms grounded in privacy literature\nand crowdsourced seeds. Using this dataset, we reveal a discrepancy between LM\nperformance in answering probing questions and their actual behavior when\nexecuting user instructions in an agent setup. State-of-the-art LMs, like GPT-4\nand Llama-3-70B, leak sensitive information in 25.68% and 38.69% of cases, even\nwhen prompted with privacy-enhancing instructions. We also demonstrate the\ndynamic nature of PrivacyLens by extending each seed into multiple trajectories\nto red-team LM privacy leakage risk. Dataset and code are available at\nhttps://github.com/SALT-NLP/PrivacyLens.", "published": "2024-08-29 17:58:38", "link": "http://arxiv.org/abs/2409.00138v3", "categories": ["cs.CL", "cs.AI", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Tiny-Toxic-Detector: A compact transformer-based model for toxic content\n  detection", "abstract": "This paper presents Tiny-toxic-detector, a compact transformer-based model\ndesigned for toxic content detection. Despite having only 2.1 million\nparameters, Tiny-toxic-detector achieves competitive performance on benchmark\ndatasets, with 90.97% accuracy on ToxiGen and 86.98% accuracy on the Jigsaw\ndataset, rivaling models over 50 times its size. This efficiency enables\ndeployment in resource-constrained environments, addressing the need for\neffective content moderation tools that balance performance with computational\nefficiency. The model architecture features 4 transformer encoder layers, each\nwith 2 attention heads, an embedding dimension of 64, and a feedforward\ndimension of 128. Trained on both public and private datasets,\nTiny-toxic-detector demonstrates the potential of efficient, task-specific\nmodels for addressing online toxicity. The paper covers the model architecture,\ntraining process, performance benchmarks, and limitations, underscoring its\nsuitability for applications such as social media monitoring and content\nmoderation. By achieving results comparable to much larger models while\nsignificantly reducing computational demands, Tiny-toxic-detector represents\nprogress toward more sustainable and scalable AI-driven content moderation\nsolutions.", "published": "2024-08-29 22:31:38", "link": "http://arxiv.org/abs/2409.02114v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "WET: Overcoming Paraphrasing Vulnerabilities in Embeddings-as-a-Service\n  with Linear Transformation Watermarks", "abstract": "Embeddings-as-a-Service (EaaS) is a service offered by large language model\n(LLM) developers to supply embeddings generated by LLMs. Previous research\nsuggests that EaaS is prone to imitation attacks -- attacks that clone the\nunderlying EaaS model by training another model on the queried embeddings. As a\nresult, EaaS watermarks are introduced to protect the intellectual property of\nEaaS providers. In this paper, we first show that existing EaaS watermarks can\nbe removed by paraphrasing when attackers clone the model. Subsequently, we\npropose a novel watermarking technique that involves linearly transforming the\nembeddings, and show that it is empirically and theoretically robust against\nparaphrasing.", "published": "2024-08-29 18:59:56", "link": "http://arxiv.org/abs/2409.04459v1", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "ChatSUMO: Large Language Model for Automating Traffic Scenario\n  Generation in Simulation of Urban MObility", "abstract": "Large Language Models (LLMs), capable of handling multi-modal input and\noutputs such as text, voice, images, and video, are transforming the way we\nprocess information. Beyond just generating textual responses to prompts, they\ncan integrate with different software platforms to offer comprehensive\nsolutions across diverse applications. In this paper, we present ChatSUMO, a\nLLM-based agent that integrates language processing skills to generate abstract\nand real-world simulation scenarios in the widely-used traffic simulator -\nSimulation of Urban MObility (SUMO). Our methodology begins by leveraging the\nLLM for user input which converts to relevant keywords needed to run python\nscripts. These scripts are designed to convert specified regions into\ncoordinates, fetch data from OpenStreetMap, transform it into a road network,\nand subsequently run SUMO simulations with the designated traffic conditions.\nThe outputs of the simulations are then interpreted by the LLM resulting in\ninformative comparisons and summaries. Users can continue the interaction and\ngenerate a variety of customized scenarios without prior traffic simulation\nexpertise. For simulation generation, we created a real-world simulation for\nthe city of Albany with an accuracy of 96\\%. ChatSUMO also realizes the\ncustomizing of edge edit, traffic light optimization, and vehicle edit by users\neffectively.", "published": "2024-08-29 03:59:11", "link": "http://arxiv.org/abs/2409.09040v1", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Acceptable Use Policies for Foundation Models", "abstract": "As foundation models have accumulated hundreds of millions of users,\ndevelopers have begun to take steps to prevent harmful types of uses. One\nsalient intervention that foundation model developers adopt is acceptable use\npolicies: legally binding policies that prohibit users from using a model for\nspecific purposes. This paper identifies acceptable use policies from 30\nfoundation model developers, analyzes the use restrictions they contain, and\nargues that acceptable use policies are an important lens for understanding the\nregulation of foundation models. Taken together, developers' acceptable use\npolicies include 127 distinct use restrictions; the wide variety in the number\nand type of use restrictions may create fragmentation across the AI supply\nchain. Developers also employ acceptable use policies to prevent competitors or\nspecific industries from making use of their models. Developers alone decide\nwhat constitutes acceptable use, and rarely provide transparency about how they\nenforce their policies. In practice, acceptable use policies are difficult to\nenforce, and scrupulous enforcement can act as a barrier to researcher access\nand limit beneficial uses of foundation models. Nevertheless, acceptable use\npolicies for foundation models are an early example of self-regulation that\nhave a significant impact on the market for foundation models and the overall\nAI ecosystem.", "published": "2024-08-29 06:04:16", "link": "http://arxiv.org/abs/2409.09041v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "68T01", "K.5.0"], "primary_category": "cs.CY"}
{"title": "United in Diversity? Contextual Biases in LLM-Based Predictions of the\n  2024 European Parliament Elections", "abstract": "Large language models (LLMs) are perceived by some as having the potential to\nrevolutionize social science research, considering their training data includes\ninformation on human attitudes and behavior. If these attitudes are reflected\nin LLM output, LLM-generated \"synthetic samples\" could be used as a viable and\nefficient alternative to surveys of real humans. However, LLM-synthetic samples\nmight exhibit coverage bias due to training data and fine-tuning processes\nbeing unrepresentative of diverse linguistic, social, political, and digital\ncontexts. In this study, we examine to what extent LLM-based predictions of\npublic opinion exhibit context-dependent biases by predicting voting behavior\nin the 2024 European Parliament elections using a state-of-the-art LLM. We\nprompt GPT-4-Turbo with anonymized individual-level background information,\nvarying prompt content and language, ask the LLM to predict each person's\nvoting behavior, and compare the weighted aggregates to the real election\nresults. Our findings emphasize the limited applicability of LLM-synthetic\nsamples to public opinion prediction. We show that (1) the LLM-based prediction\nof future voting behavior largely fails, (2) prediction accuracy is unequally\ndistributed across national and linguistic contexts, and (3) improving LLM\npredictions requires detailed attitudinal information about individuals for\nprompting. In investigating the contextual differences of LLM-based predictions\nof public opinion, our research contributes to the understanding and mitigation\nof biases and inequalities in the development of LLMs and their applications in\ncomputational social science.", "published": "2024-08-29 16:01:06", "link": "http://arxiv.org/abs/2409.09045v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "stat.AP"], "primary_category": "cs.CY"}
{"title": "Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming", "abstract": "Recent advances in language models have achieved significant progress.\nGPT-4o, as a new milestone, has enabled real-time conversations with humans,\ndemonstrating near-human natural fluency. Such human-computer interaction\nnecessitates models with the capability to perform reasoning directly with the\naudio modality and generate output in streaming. However, this remains beyond\nthe reach of current academic models, as they typically depend on extra TTS\nsystems for speech synthesis, resulting in undesirable latency. This paper\nintroduces the Mini-Omni, an audio-based end-to-end conversational model,\ncapable of real-time speech interaction. To achieve this capability, we propose\na text-instructed speech generation method, along with batch-parallel\nstrategies during inference to further boost the performance. Our method also\nhelps to retain the original model's language capabilities with minimal\ndegradation, enabling other works to establish real-time interaction\ncapabilities. We call this training method \"Any Model Can Talk\". We also\nintroduce the VoiceAssistant-400K dataset to fine-tune models optimized for\nspeech output. To our best knowledge, Mini-Omni is the first fully end-to-end,\nopen-source model for real-time speech interaction, offering valuable potential\nfor future research.", "published": "2024-08-29 17:18:53", "link": "http://arxiv.org/abs/2408.16725v3", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.AI"}
{"title": "Enabling Beam Search for Language Model-Based Text-to-Speech Synthesis", "abstract": "Tokenising continuous speech into sequences of discrete tokens and modelling\nthem with language models (LMs) has led to significant success in\ntext-to-speech (TTS) synthesis. Although these models can generate speech with\nhigh quality and naturalness, their synthesised samples can still suffer from\nartefacts, mispronunciation, word repeating, etc. In this paper, we argue these\nundesirable properties could partly be caused by the randomness of\nsampling-based strategies during the autoregressive decoding of LMs. Therefore,\nwe look at maximisation-based decoding approaches and propose Temporal\nRepetition Aware Diverse Beam Search (TRAD-BS) to find the most probable\nsequences of the generated speech tokens. Experiments with two state-of-the-art\nLM-based TTS models demonstrate that our proposed maximisation-based decoding\nstrategy generates speech with fewer mispronunciations and improved speaker\nconsistency.", "published": "2024-08-29 09:31:06", "link": "http://arxiv.org/abs/2408.16373v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Denoising of photogrammetric dummy head ear point clouds for individual\n  Head-Related Transfer Functions computation", "abstract": "Individual Head-Related Transfer Functions (HRTFs), crucial for realistic\nvirtual audio rendering, can be efficiently numerically computed from precise\nthree-dimensional head and ear scans. While photogrammetry scanning is\npromising, it generally lacks accuracy, leading to HRTFs showing significant\nperceptual deviation from reference data, mainly due to scanning errors\naffecting the most occluded pinna structures. This paper examines the\napplication of Deep Neural Networks (DNNs) for denoising photogrammetric ear\nscans. Several DNNs, fine-tuned on pinna samples corrupted with synthetic error\nmodelled to mimic that observed in photogrammetric dummy head scans, are tested\nand benchmarked against a classical denoising method. One DNN is further\nmodified and retrained to enhance its denoising performance. The comparison of\nHRTFs derived from original and denoised scans against reference data shows\nthat the best-performing DNN marginally reduces the deviation of\nphotogrammetric dummy head HRTFs to levels closer to accurately measured ones.\nAdditionally, correlation analysis between geometric and HRTF metrics, computed\non the scanned point clouds and their corresponding HRTFs, is used to identify\nkey measures for evaluating the deviation between target and reference scans.\nThese findings are expected to guide the selection of relevant loss functions\nand foster improvements in this and similar DNN models.", "published": "2024-08-29 10:15:19", "link": "http://arxiv.org/abs/2408.16410v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "WHISMA: A Speech-LLM to Perform Zero-shot Spoken Language Understanding", "abstract": "Speech large language models (speech-LLMs) integrate speech and text-based\nfoundation models to provide a unified framework for handling a wide range of\ndownstream tasks. In this paper, we introduce WHISMA, a speech-LLM tailored for\nspoken language understanding (SLU) that demonstrates robust performance in\nvarious zero-shot settings. WHISMA combines the speech encoder from Whisper\nwith the Llama-3 LLM, and is fine-tuned in a parameter-efficient manner on a\ncomprehensive collection of SLU-related datasets. Our experiments show that\nWHISMA significantly improves the zero-shot slot filling performance on the\nSLURP benchmark, achieving a relative gain of 26.6% compared to the current\nstate-of-the-art model. Furthermore, to evaluate WHISMA's generalisation\ncapabilities to unseen domains, we develop a new task-agnostic benchmark named\nSLU-GLUE. The evaluation results indicate that WHISMA outperforms an existing\nspeech-LLM (Qwen-Audio) with a relative gain of 33.0%.", "published": "2024-08-29 10:31:52", "link": "http://arxiv.org/abs/2408.16423v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "RAVE for Speech: Efficient Voice Conversion at High Sampling Rates", "abstract": "Voice conversion has gained increasing popularity within the field of audio\nmanipulation and speech synthesis. Often, the main objective is to transfer the\ninput identity to that of a target speaker without changing its linguistic\ncontent. While current work provides high-fidelity solutions they rarely focus\non model simplicity, high-sampling rate environments or stream-ability. By\nincorporating speech representation learning into a generative timbre transfer\nmodel, traditionally created for musical purposes, we investigate the realm of\nvoice conversion generated directly in the time domain at high sampling rates.\nMore specifically, we guide the latent space of a baseline model towards\nlinguistically relevant representations and condition it on external speaker\ninformation. Through objective and subjective assessments, we demonstrate that\nthe proposed solution can attain levels of naturalness, quality, and\nintelligibility comparable to those of a state-of-the-art solution for seen\nspeakers, while significantly decreasing inference time. However, despite the\npresence of target speaker characteristics in the converted output, the actual\nsimilarity to unseen speakers remains a challenge.", "published": "2024-08-29 14:09:37", "link": "http://arxiv.org/abs/2408.16546v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio xLSTMs: Learning Self-Supervised Audio Representations with xLSTMs", "abstract": "While the transformer has emerged as the eminent neural architecture, several\nindependent lines of research have emerged to address its limitations.\nRecurrent neural approaches have also observed a lot of renewed interest,\nincluding the extended long short-term memory (xLSTM) architecture, which\nreinvigorates the original LSTM architecture. However, while xLSTMs have shown\ncompetitive performance compared to the transformer, their viability for\nlearning self-supervised general-purpose audio representations has not yet been\nevaluated. This work proposes Audio xLSTM (AxLSTM), an approach to learn audio\nrepresentations from masked spectrogram patches in a self-supervised setting.\nPretrained on the AudioSet dataset, the proposed AxLSTM models outperform\ncomparable self-supervised audio spectrogram transformer (SSAST) baselines by\nup to 20% in relative performance across a set of ten diverse downstream tasks\nwhile having up to 45% fewer parameters.", "published": "2024-08-29 14:35:56", "link": "http://arxiv.org/abs/2408.16568v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Revisit Micro-batch Clipping: Adaptive Data Pruning via Gradient\n  Manipulation", "abstract": "Micro-batch clipping, a gradient clipping method, has recently shown\npotential in enhancing auto-speech recognition (ASR) model performance.\nHowever, the underlying mechanism behind this improvement remains mysterious,\nparticularly the observation that only certain micro-batch sizes are\nbeneficial. In this paper, we make the first attempt to explain this\nphenomenon. Inspired by recent data pruning research, we assume that specific\ntraining samples may impede model convergence during certain training phases.\nUnder this assumption, the convergence analysis shows that micro-batch clipping\ncan improve the convergence rate asymptotically at the cost of an additional\nconstant bias that does not diminish with more training iterations. The bias is\ndependent on a few factors and can be minimized at specific micro-batch size,\nthereby elucidating the existence of the sweet-spot micro-batch size observed\npreviously. We also verify the effectiveness of micro-batch clipping beyond\nspeech models on vision and language models, and show promising performance\ngains in these domains. An exploration of potential limitations shows that\nmicro-batch clipping is less effective when training data originates from\nmultiple distinct domains.", "published": "2024-08-29 01:50:13", "link": "http://arxiv.org/abs/2408.16204v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Human-Inspired Audio-Visual Speech Recognition: Spike Activity, Cueing\n  Interaction and Causal Processing", "abstract": "Humans naturally perform audiovisual speech recognition (AVSR), enhancing the\naccuracy and robustness by integrating auditory and visual information. Spiking\nneural networks (SNNs), which mimic the brain's information-processing\nmechanisms, are well-suited for emulating the human capability of AVSR. Despite\ntheir potential, research on SNNs for AVSR is scarce, with most existing\naudio-visual multimodal methods focused on object or digit recognition. These\nmodels simply integrate features from both modalities, neglecting their unique\ncharacteristics and interactions. Additionally, they often rely on future\ninformation for current processing, which increases recognition latency and\nlimits real-time applicability. Inspired by human speech perception, this paper\nproposes a novel human-inspired SNN named HI-AVSNN for AVSR, incorporating\nthree key characteristics: cueing interaction, causal processing and spike\nactivity. For cueing interaction, we propose a visual-cued auditory attention\nmodule (VCA2M) that leverages visual cues to guide attention to auditory\nfeatures. We achieve causal processing by aligning the SNN's temporal dimension\nwith that of visual and auditory features and applying temporal masking to\nutilize only past and current information. To implement spike activity, in\naddition to using SNNs, we leverage the event camera to capture lip movement as\nspikes, mimicking the human retina and providing efficient visual data. We\nevaluate HI-AVSNN on an audiovisual speech recognition dataset combining the\nDVS-Lip dataset with its corresponding audio samples. Experimental results\ndemonstrate the superiority of our proposed fusion method, outperforming\nexisting audio-visual SNN fusion methods and achieving a 2.27% improvement in\naccuracy over the only existing SNN-based AVSR method.", "published": "2024-08-29 14:30:56", "link": "http://arxiv.org/abs/2408.16564v1", "categories": ["cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "Towards Efficient Modelling of String Dynamics: A Comparison of State\n  Space and Koopman based Deep Learning Methods", "abstract": "This paper presents an examination of State Space Models (SSM) and\nKoopman-based deep learning methods for modelling the dynamics of both linear\nand non-linear stiff strings. Through experiments with datasets generated under\ndifferent initial conditions and sample rates, we assess the capacity of these\nmodels to accurately model the complex behaviours observed in string dynamics.\nOur findings indicate that our proposed Koopman-based model performs as well as\nor better than other existing approaches in non-linear cases for long-sequence\nmodelling.\n  We inform the design of these architectures with the structure of the\nproblems at hand. Although challenges remain in extending model predictions\nbeyond the training horizon (i.e., extrapolation), the focus of our\ninvestigation lies in the models' ability to generalise across different\ninitial conditions within the training time interval. This research contributes\ninsights into the physical modelling of dynamical systems (in particular those\naddressing musical acoustics) by offering a comparative overview of these and\nprevious methods and introducing innovative strategies for model improvement.\nOur results highlight the efficacy of these models in simulating non-linear\ndynamics and emphasise their wide-ranging applicability in accurately modelling\ndynamical systems over extended sequences.", "published": "2024-08-29 15:55:27", "link": "http://arxiv.org/abs/2408.16650v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "physics.comp-ph"], "primary_category": "cs.SD"}
{"title": "Automatic detection of Mild Cognitive Impairment using high-dimensional\n  acoustic features in spontaneous speech", "abstract": "This study addresses the TAUKADIAL challenge, focusing on the classification\nof speech from people with Mild Cognitive Impairment (MCI) and neurotypical\ncontrols. We conducted three experiments comparing five machine-learning\nmethods: Random Forests, Sparse Logistic Regression, k-Nearest Neighbors,\nSparse Support Vector Machine, and Decision Tree, utilizing 1076 acoustic\nfeatures automatically extracted using openSMILE. In Experiment 1, the entire\ndataset was used to train a language-agnostic model. Experiment 2 introduced a\nlanguage detection step, leading to separate model training for each language.\nExperiment 3 further enhanced the language-agnostic model from Experiment 1,\nwith a specific focus on evaluating the robustness of the models using\nout-of-sample test data. Across all three experiments, results consistently\nfavored models capable of handling high-dimensional data, such as Random Forest\nand Sparse Logistic Regression, in classifying speech from MCI and controls.", "published": "2024-08-29 17:23:43", "link": "http://arxiv.org/abs/2408.16732v1", "categories": ["q-bio.NC", "cs.SD", "eess.AS", "q-bio.QM"], "primary_category": "q-bio.NC"}
{"title": "WavTokenizer: an Efficient Acoustic Discrete Codec Tokenizer for Audio\n  Language Modeling", "abstract": "Language models have been effectively applied to modeling natural signals,\nsuch as images, video, speech, and audio. A crucial component of these models\nis the codec tokenizer, which compresses high-dimensional natural signals into\nlower-dimensional discrete tokens. In this paper, we introduce WavTokenizer,\nwhich offers several advantages over previous SOTA acoustic codec models in the\naudio domain: 1)extreme compression. By compressing the layers of quantizers\nand the temporal dimension of the discrete codec, one-second audio of 24kHz\nsampling rate requires only a single quantizer with 40 or 75 tokens. 2)improved\nsubjective quality. Despite the reduced number of tokens, WavTokenizer achieves\nstate-of-the-art reconstruction quality with outstanding UTMOS scores and\ninherently contains richer semantic information. Specifically, we achieve these\nresults by designing a broader VQ space, extended contextual windows, and\nimproved attention networks, as well as introducing a powerful multi-scale\ndiscriminator and an inverse Fourier transform structure. We conducted\nextensive reconstruction experiments in the domains of speech, audio, and\nmusic. WavTokenizer exhibited strong performance across various objective and\nsubjective metrics compared to state-of-the-art models. We also tested semantic\ninformation, VQ utilization, and adaptability to generative models.\nComprehensive ablation studies confirm the necessity of each module in\nWavTokenizer. The related code, demos, and pre-trained models are available at\nhttps://github.com/jishengpeng/WavTokenizer.", "published": "2024-08-29 13:43:36", "link": "http://arxiv.org/abs/2408.16532v3", "categories": ["eess.AS", "cs.LG", "cs.MM", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
