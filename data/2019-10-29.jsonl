{"title": "A Richly Annotated Corpus for Different Tasks in Automated Fact-Checking", "abstract": "Automated fact-checking based on machine learning is a promising approach to\nidentify false information distributed on the web. In order to achieve\nsatisfactory performance, machine learning methods require a large corpus with\nreliable annotations for the different tasks in the fact-checking process.\nHaving analyzed existing fact-checking corpora, we found that none of them\nmeets these criteria in full. They are either too small in size, do not provide\ndetailed annotations, or are limited to a single domain. Motivated by this gap,\nwe present a new substantially sized mixed-domain corpus with annotations of\ngood quality for the core fact-checking tasks: document retrieval, evidence\nextraction, stance detection, and claim validation. To aid future corpus\nconstruction, we describe our methodology for corpus creation and annotation,\nand demonstrate that it results in substantial inter-annotator agreement. As\nbaselines for future research, we perform experiments on our corpus with a\nnumber of model architectures that reach high performance in similar problem\nsettings. Finally, to support the development of future models, we provide a\ndetailed error analysis for each of the tasks. Our results show that the\nrealistic, multi-domain setting defined by our data poses new challenges for\nthe existing models, providing opportunities for considerable improvement by\nfuture systems.", "published": "2019-10-29 16:07:12", "link": "http://arxiv.org/abs/1911.01214v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Incorporating Interlocutor-Aware Context into Response Generation on\n  Multi-Party Chatbots", "abstract": "Conventional chatbots focus on two-party response generation, which\nsimplifies the real dialogue scene. In this paper, we strive toward a novel\ntask of Response Generation on Multi-Party Chatbot (RGMPC), where the generated\nresponses heavily rely on the interlocutors' roles (e.g., speaker and\naddressee) and their utterances. Unfortunately, complex interactions among the\ninterlocutors' roles make it challenging to precisely capture conversational\ncontexts and interlocutors' information. Facing this challenge, we present a\nresponse generation model which incorporates Interlocutor-aware Contexts into\nRecurrent Encoder-Decoder frameworks (ICRED) for RGMPC. Specifically, we employ\ninteractive representations to capture dialogue contexts for different\ninterlocutors. Moreover, we leverage an addressee memory to enhance contextual\ninterlocutor information for the target addressee. Finally, we construct a\ncorpus for RGMPC based on an existing open-access dataset. Automatic and manual\nevaluations demonstrate that the ICRED remarkably outperforms strong baselines.", "published": "2019-10-29 06:43:51", "link": "http://arxiv.org/abs/1910.13106v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transformer-based Cascaded Multimodal Speech Translation", "abstract": "This paper describes the cascaded multimodal speech translation systems\ndeveloped by Imperial College London for the IWSLT 2019 evaluation campaign.\nThe architecture consists of an automatic speech recognition (ASR) system\nfollowed by a Transformer-based multimodal machine translation (MMT) system.\nWhile the ASR component is identical across the experiments, the MMT model\nvaries in terms of the way of integrating the visual context (simple\nconditioning vs. attention), the type of visual features exploited (pooled,\nconvolutional, action categories) and the underlying architecture. For the\nlatter, we explore both the canonical transformer and its deliberation version\nwith additive and cascade variants which differ in how they integrate the\ntextual attention. Upon conducting extensive experiments, we found that (i) the\nexplored visual integration schemes often harm the translation performance for\nthe transformer and additive deliberation, but considerably improve the cascade\ndeliberation; (ii) the transformer and cascade deliberation integrate the\nvisual modality better than the additive deliberation, as shown by the\nincongruence analysis.", "published": "2019-10-29 11:56:12", "link": "http://arxiv.org/abs/1910.13215v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BPE-Dropout: Simple and Effective Subword Regularization", "abstract": "Subword segmentation is widely used to address the open vocabulary problem in\nmachine translation. The dominant approach to subword segmentation is Byte Pair\nEncoding (BPE), which keeps the most frequent words intact while splitting the\nrare ones into multiple tokens. While multiple segmentations are possible even\nwith the same vocabulary, BPE splits words into unique sequences; this may\nprevent a model from better learning the compositionality of words and being\nrobust to segmentation errors. So far, the only way to overcome this BPE\nimperfection, its deterministic nature, was to create another subword\nsegmentation algorithm (Kudo, 2018). In contrast, we show that BPE itself\nincorporates the ability to produce multiple segmentations of the same word. We\nintroduce BPE-dropout - simple and effective subword regularization method\nbased on and compatible with conventional BPE. It stochastically corrupts the\nsegmentation procedure of BPE, which leads to producing multiple segmentations\nwithin the same fixed BPE framework. Using BPE-dropout during training and the\nstandard BPE during inference improves translation quality up to 3 BLEU\ncompared to BPE and up to 0.9 BLEU compared to the previous subword\nregularization.", "published": "2019-10-29 13:42:56", "link": "http://arxiv.org/abs/1910.13267v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Findings of the Third Workshop on Neural Generation and Translation", "abstract": "This document describes the findings of the Third Workshop on Neural\nGeneration and Translation, held in concert with the annual conference of the\nEmpirical Methods in Natural Language Processing (EMNLP 2019). First, we\nsummarize the research trends of papers presented in the proceedings. Second,\nwe describe the results of the two shared tasks 1) efficient neural machine\ntranslation (NMT) where participants were tasked with creating NMT systems that\nare both accurate and efficient, and 2) document-level generation and\ntranslation (DGT) where participants were tasked with developing systems that\ngenerate summaries from structured data, potentially with assistance from text\nin another language.", "published": "2019-10-29 14:41:19", "link": "http://arxiv.org/abs/1910.13299v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Quantifying the Semantic Core of Gender Systems", "abstract": "Many of the world's languages employ grammatical gender on the lexeme. For\nexample, in Spanish, the word for 'house' (casa) is feminine, whereas the word\nfor 'paper' (papel) is masculine. To a speaker of a genderless language, this\nassignment seems to exist with neither rhyme nor reason. But is the assignment\nof inanimate nouns to grammatical genders truly arbitrary? We present the first\nlarge-scale investigation of the arbitrariness of noun-gender assignments. To\nthat end, we use canonical correlation analysis to correlate the grammatical\ngender of inanimate nouns with an externally grounded definition of their\nlexical semantics. We find that 18 languages exhibit a significant correlation\nbetween grammatical gender and lexical semantics.", "published": "2019-10-29 19:44:17", "link": "http://arxiv.org/abs/1910.13497v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Big Bidirectional Insertion Representations for Documents", "abstract": "The Insertion Transformer is well suited for long form text generation due to\nits parallel generation capabilities, requiring $O(\\log_2 n)$ generation steps\nto generate $n$ tokens. However, modeling long sequences is difficult, as there\nis more ambiguity captured in the attention mechanism. This work proposes the\nBig Bidirectional Insertion Representations for Documents (Big BIRD), an\ninsertion-based model for document-level translation tasks. We scale up the\ninsertion-based models to long form documents. Our key contribution is\nintroducing sentence alignment via sentence-positional embeddings between the\nsource and target document. We show an improvement of +4.3 BLEU on the WMT'19\nEnglish$\\rightarrow$German document-level translation task compared with the\nInsertion Transformer baseline.", "published": "2019-10-29 01:38:24", "link": "http://arxiv.org/abs/1910.13034v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "JarKA: Modeling Attribute Interactions for Cross-lingual Knowledge\n  Alignment", "abstract": "Abstract. Cross-lingual knowledge alignment is the cornerstone in building a\ncomprehensive knowledge graph (KG), which can benefit various knowledge-driven\napplications. As the structures of KGs are usually sparse, attributes of\nentities may play an important role in aligning the entities. However, the\nheterogeneity of the attributes across KGs prevents from accurately embedding\nand comparing entities. To deal with the issue, we propose to model the\ninteractions between attributes, instead of globally embedding an entity with\nall the attributes. We further propose a joint framework to merge the\nalignments inferred from the attributes and the structures. Experimental\nresults show that the proposed model outperforms the state-of-art baselines by\nup to 38.48% HitRatio@1. The results also demonstrate that our model can infer\nthe alignments between attributes, relationships and values, in addition to\nentities.", "published": "2019-10-29 06:41:30", "link": "http://arxiv.org/abs/1910.13105v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Generating Questions for Knowledge Bases via Incorporating Diversified\n  Contexts and Answer-Aware Loss", "abstract": "We tackle the task of question generation over knowledge bases. Conventional\nmethods for this task neglect two crucial research issues: 1) the given\npredicate needs to be expressed; 2) the answer to the generated question needs\nto be definitive. In this paper, we strive toward the above two issues via\nincorporating diversified contexts and answer-aware loss. Specifically, we\npropose a neural encoder-decoder model with multi-level copy mechanisms to\ngenerate such questions. Furthermore, the answer aware loss is introduced to\nmake generated questions corresponding to more definitive answers. Experiments\ndemonstrate that our model achieves state-of-the-art performance. Meanwhile,\nsuch generated question can express the given predicate and correspond to a\ndefinitive answer.", "published": "2019-10-29 06:45:24", "link": "http://arxiv.org/abs/1910.13108v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Contrastive Attention Mechanism for Abstractive Sentence Summarization", "abstract": "We propose a contrastive attention mechanism to extend the\nsequence-to-sequence framework for abstractive sentence summarization task,\nwhich aims to generate a brief summary of a given source sentence. The proposed\ncontrastive attention mechanism accommodates two categories of attention: one\nis the conventional attention that attends to relevant parts of the source\nsentence, the other is the opponent attention that attends to irrelevant or\nless relevant parts of the source sentence. Both attentions are trained in an\nopposite way so that the contribution from the conventional attention is\nencouraged and the contribution from the opponent attention is discouraged\nthrough a novel softmax and softmin functionality. Experiments on benchmark\ndatasets show that, the proposed contrastive attention mechanism is more\nfocused on the relevant parts for the summary than the conventional attention\nmechanism, and greatly advances the state-of-the-art performance on the\nabstractive sentence summarization task. We release the code at\nhttps://github.com/travel-go/Abstractive-Text-Summarization", "published": "2019-10-29 06:56:46", "link": "http://arxiv.org/abs/1910.13114v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sentence Embeddings for Russian NLU", "abstract": "We investigate the performance of sentence embeddings models on several tasks\nfor the Russian language. In our comparison, we include such tasks as multiple\nchoice question answering, next sentence prediction, and paraphrase\nidentification. We employ FastText embeddings as a baseline and compare it to\nELMo and BERT embeddings. We conduct two series of experiments, using both\nunsupervised (i.e., based on similarity measure only) and supervised approaches\nfor the tasks. Finally, we present datasets for multiple choice question\nanswering and next sentence prediction in Russian.", "published": "2019-10-29 14:28:34", "link": "http://arxiv.org/abs/1910.13291v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Rethinking Cooperative Rationalization: Introspective Extraction and\n  Complement Control", "abstract": "Selective rationalization has become a common mechanism to ensure that\npredictive models reveal how they use any available features. The selection may\nbe soft or hard, and identifies a subset of input features relevant for\nprediction. The setup can be viewed as a co-operate game between the selector\n(aka rationale generator) and the predictor making use of only the selected\nfeatures. The co-operative setting may, however, be compromised for two\nreasons. First, the generator typically has no direct access to the outcome it\naims to justify, resulting in poor performance. Second, there's typically no\ncontrol exerted on the information left outside the selection. We revise the\noverall co-operative framework to address these challenges. We introduce an\nintrospective model which explicitly predicts and incorporates the outcome into\nthe selection process. Moreover, we explicitly control the rationale complement\nvia an adversary so as not to leave any useful information out of the\nselection. We show that the two complementary mechanisms maintain both high\npredictive accuracy and lead to comprehensive rationales.", "published": "2019-10-29 14:32:54", "link": "http://arxiv.org/abs/1910.13294v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Scalable Evaluation and Improvement of Document Set Expansion via Neural\n  Positive-Unlabeled Learning", "abstract": "We consider the situation in which a user has collected a small set of\ndocuments on a cohesive topic, and they want to retrieve additional documents\non this topic from a large collection. Information Retrieval (IR) solutions\ntreat the document set as a query, and look for similar documents in the\ncollection. We propose to extend the IR approach by treating the problem as an\ninstance of positive-unlabeled (PU) learning -- i.e., learning binary\nclassifiers from only positive and unlabeled data, where the positive data\ncorresponds to the query documents, and the unlabeled data is the results\nreturned by the IR engine. Utilizing PU learning for text with big neural\nnetworks is a largely unexplored field. We discuss various challenges in\napplying PU learning to the setting, including an unknown class prior,\nextremely imbalanced data and large-scale accurate evaluation of models, and we\npropose solutions and empirically validate them. We demonstrate the\neffectiveness of the method using a series of experiments of retrieving PubMed\nabstracts adhering to fine-grained topics. We demonstrate improvements over the\nbase IR solution and other baselines.", "published": "2019-10-29 15:56:33", "link": "http://arxiv.org/abs/1910.13339v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Empirical Study of Generation Order for Machine Translation", "abstract": "In this work, we present an empirical study of generation order for machine\ntranslation. Building on recent advances in insertion-based modeling, we first\nintroduce a soft order-reward framework that enables us to train models to\nfollow arbitrary oracle generation policies. We then make use of this framework\nto explore a large variety of generation orders, including uninformed orders,\nlocation-based orders, frequency-based orders, content-based orders, and\nmodel-based orders. Curiously, we find that for the WMT'14 English $\\to$ German\ntranslation task, order does not have a substantial impact on output quality,\nwith unintuitive orderings such as alphabetical and shortest-first matching the\nperformance of a standard Transformer. This demonstrates that traditional\nleft-to-right generation is not strictly necessary to achieve high performance.\nOn the other hand, results on the WMT'18 English $\\to$ Chinese task tend to\nvary more widely, suggesting that translation for less well-aligned language\npairs may be more sensitive to generation order.", "published": "2019-10-29 17:54:36", "link": "http://arxiv.org/abs/1910.13437v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Ordered Memory", "abstract": "Stack-augmented recurrent neural networks (RNNs) have been of interest to the\ndeep learning community for some time. However, the difficulty of training\nmemory models remains a problem obstructing the widespread use of such models.\nIn this paper, we propose the Ordered Memory architecture. Inspired by Ordered\nNeurons (Shen et al., 2018), we introduce a new attention-based mechanism and\nuse its cumulative probability to control the writing and erasing operation of\nthe memory. We also introduce a new Gated Recursive Cell to compose lower-level\nrepresentations into higher-level representation. We demonstrate that our model\nachieves strong performance on the logical inference task (Bowman et al.,\n2015)and the ListOps (Nangia and Bowman, 2018) task. We can also interpret the\nmodel to retrieve the induced tree structure, and find that these induced\nstructures align with the ground truth. Finally, we evaluate our model on the\nStanford SentimentTreebank tasks (Socher et al., 2013), and find that it\nperforms comparatively with the state-of-the-art methods in the literature.", "published": "2019-10-29 18:14:14", "link": "http://arxiv.org/abs/1910.13466v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Understand customer reviews with less data and in short time: pretrained\n  language representation and active learning", "abstract": "In this paper, we address customer review understanding problems by using\nsupervised machine learning approaches, in order to achieve a fully automatic\nreview aspects categorisation and sentiment analysis. In general, such\nsupervised learning algorithms require domain-specific expert knowledge for\ngenerating high quality labeled training data, and the cost of labeling can be\nvery high. To achieve an in-production customer review machine learning enabled\nanalysis tool with only a limited amount of data and within a reasonable\ntraining data collection time, we propose to use pre-trained language\nrepresentation to boost model performance and active learning framework for\naccelerating the iterative training process. The results show that with\nintegration of both components, the fully automatic review analysis can be\nachieved at a much faster pace.", "published": "2019-10-29 12:39:37", "link": "http://arxiv.org/abs/1911.01198v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Detect Toxic Content to Improve Online Conversations", "abstract": "Social media is filled with toxic content. The aim of this paper is to build\na model that can detect insincere questions. We use the 'Quora Insincere\nQuestions Classification' dataset for our analysis. The dataset is composed of\nsincere and insincere questions, with the majority of sincere questions. The\ndataset is processed and analyzed using Python and its libraries such as\nsklearn, numpy, pandas, keras etc. The dataset is converted to vector form\nusing word embeddings such as GloVe, Wiki-news and TF-IDF. The imbalance in the\ndataset is handled by resampling techniques. We train and compare various\nmachine learning and deep learning models to come up with the best results.\nModels discussed include SVM, Naive Bayes, GRU and LSTM.", "published": "2019-10-29 01:42:22", "link": "http://arxiv.org/abs/1911.01217v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Inducing brain-relevant bias in natural language processing models", "abstract": "Progress in natural language processing (NLP) models that estimate\nrepresentations of word sequences has recently been leveraged to improve the\nunderstanding of language processing in the brain. However, these models have\nnot been specifically designed to capture the way the brain represents language\nmeaning. We hypothesize that fine-tuning these models to predict recordings of\nbrain activity of people reading text will lead to representations that encode\nmore brain-activity-relevant language information. We demonstrate that a\nversion of BERT, a recently introduced and powerful language model, can improve\nthe prediction of brain activity after fine-tuning. We show that the\nrelationship between language and brain activity learned by BERT during this\nfine-tuning transfers across multiple participants. We also show that, for some\nparticipants, the fine-tuned representations learned from both\nmagnetoencephalography (MEG) and functional magnetic resonance imaging (fMRI)\nare better for predicting fMRI than the representations learned from fMRI\nalone, indicating that the learned representations capture\nbrain-activity-relevant information that is not simply an artifact of the\nmodality. While changes to language representations help the model predict\nbrain activity, they also do not harm the model's ability to perform downstream\nNLP tasks. Our findings are notable for research on language understanding in\nthe brain.", "published": "2019-10-29 23:28:16", "link": "http://arxiv.org/abs/1911.03268v1", "categories": ["q-bio.NC", "cs.CL", "cs.LG"], "primary_category": "q-bio.NC"}
{"title": "An Efficient Model for Sentiment Analysis of Electronic Product Reviews\n  in Vietnamese", "abstract": "In the past few years, the growth of e-commerce and digital marketing in\nVietnam has generated a huge volume of opinionated data. Analyzing those data\nwould provide enterprises with insight for better business decisions. In this\nwork, as part of the Advosights project, we study sentiment analysis of product\nreviews in Vietnamese. The final solution is based on Self-attention neural\nnetworks, a flexible architecture for text classification task with about\n90.16% of accuracy in 0.0124 second, a very fast inference time.", "published": "2019-10-29 10:06:56", "link": "http://arxiv.org/abs/1910.13162v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "a novel cross-lingual voice cloning approach with a few text-free\n  samples", "abstract": "In this paper, we present a cross-lingual voice cloning approach. BN features\nobtained by SI-ASR model are used as a bridge across speakers and language\nboundaries. The relationships between text and BN features are modeled by the\nlatent prosody model. The acoustic model learns the translation from BN\nfeatures to acoustic features. The acoustic model is fine-tuned with a few\nsamples of the target speaker to realize voice cloning. This system can\ngenerate speech of arbitrary utterance of target language in cross-lingual\nspeakers' voice. We verify that with small amount of audio data, our proposed\napproach can well handle cross-lingual tasks. And in intra-lingual tasks, our\nproposed approach also performs better than baseline approach in naturalness\nand similarity.", "published": "2019-10-29 14:06:49", "link": "http://arxiv.org/abs/1910.13276v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Weakly-Supervised Deep Learning for Domain Invariant Sentiment\n  Classification", "abstract": "The task of learning a sentiment classification model that adapts well to any\ntarget domain, different from the source domain, is a challenging problem.\nMajority of the existing approaches focus on learning a common representation\nby leveraging both source and target data during training. In this paper, we\nintroduce a two-stage training procedure that leverages weakly supervised\ndatasets for developing simple lift-and-shift-based predictive models without\nbeing exposed to the target domain during the training phase. Experimental\nresults show that transfer with weak supervision from a source domain to\nvarious target domains provides performance very close to that obtained via\nsupervised training on the target domain itself.", "published": "2019-10-29 17:43:37", "link": "http://arxiv.org/abs/1910.13425v2", "categories": ["cs.LG", "cs.CL", "cs.IR", "stat.ML"], "primary_category": "cs.LG"}
{"title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language\n  Generation, Translation, and Comprehension", "abstract": "We present BART, a denoising autoencoder for pretraining sequence-to-sequence\nmodels. BART is trained by (1) corrupting text with an arbitrary noising\nfunction, and (2) learning a model to reconstruct the original text. It uses a\nstandard Tranformer-based neural machine translation architecture which,\ndespite its simplicity, can be seen as generalizing BERT (due to the\nbidirectional encoder), GPT (with the left-to-right decoder), and many other\nmore recent pretraining schemes. We evaluate a number of noising approaches,\nfinding the best performance by both randomly shuffling the order of the\noriginal sentences and using a novel in-filling scheme, where spans of text are\nreplaced with a single mask token. BART is particularly effective when fine\ntuned for text generation but also works well for comprehension tasks. It\nmatches the performance of RoBERTa with comparable training resources on GLUE\nand SQuAD, achieves new state-of-the-art results on a range of abstractive\ndialogue, question answering, and summarization tasks, with gains of up to 6\nROUGE. BART also provides a 1.1 BLEU increase over a back-translation system\nfor machine translation, with only target language pretraining. We also report\nablation experiments that replicate other pretraining schemes within the BART\nframework, to better measure which factors most influence end-task performance.", "published": "2019-10-29 18:01:00", "link": "http://arxiv.org/abs/1910.13461v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Does Speech enhancement of publicly available data help build robust\n  Speech Recognition Systems?", "abstract": "Automatic speech recognition (ASR) systems play a key role in many commercial\nproducts including voice assistants. Typically, they require large amounts of\nclean speech data for training which gives an undue advantage to large\norganizations which have tons of private data. In this paper, we have first\ncurated a fairly big dataset using publicly available data sources. Thereafter,\nwe tried to investigate if we can use publicly available noisy data to train\nrobust ASR systems. We have used speech enhancement to clean the noisy data\nfirst and then used it together with its cleaned version to train ASR systems.\nWe have found that using speech enhancement gives 9.5\\% better word error rate\nthan training on just noisy data and 9\\% better than training on just clean\ndata. It's performance is also comparable to the ideal case scenario when\ntrained on noisy and its clean version.", "published": "2019-10-29 19:23:16", "link": "http://arxiv.org/abs/1910.13488v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Semi-Supervised Natural Language Approach for Fine-Grained\n  Classification of Medical Reports", "abstract": "Although machine learning has become a powerful tool to augment doctors in\nclinical analysis, the immense amount of labeled data that is necessary to\ntrain supervised learning approaches burdens each development task as time and\nresource intensive. The vast majority of dense clinical information is stored\nin written reports, detailing pertinent patient information. The challenge with\nutilizing natural language data for standard model development is due to the\ncomplex nature of the modality. In this research, a model pipeline was\ndeveloped to utilize an unsupervised approach to train an encoder-language\nmodel, a recurrent network, to generate document encodings; which then can be\nused as features passed into a decoder-classifier model that requires\nmagnitudes less labeled data than previous approaches to differentiate between\nfine-grained disease classes accurately. The language model was trained on\nunlabeled radiology reports from the Massachusetts General Hospital Radiology\nDepartment (n=218,159) and terminated with a loss of 1.62. The classification\nmodels were trained on three labeled datasets of head CT studies of reported\npatients, presenting large vessel occlusion (n=1403), acute ischemic strokes\n(n=331), and intracranial hemorrhage (n=4350), to identify a variety of\ndifferent findings directly from the radiology report data; resulting in AUCs\nof 0.98, 0.95, and 0.99, respectively, for the large vessel occlusion, acute\nischemic stroke, and intracranial hemorrhage datasets. The output encodings are\nable to be used in conjunction with imaging data, to create models that can\nprocess a multitude of different modalities. The ability to automatically\nextract relevant features from textual data allows for faster model development\nand integration of textual modality, overall, allowing clinical reports to\nbecome a more viable input for more encompassing and accurate deep learning\nmodels.", "published": "2019-10-29 23:25:59", "link": "http://arxiv.org/abs/1910.13573v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Privacy Enhanced Multimodal Neural Representations for Emotion\n  Recognition", "abstract": "Many mobile applications and virtual conversational agents now aim to\nrecognize and adapt to emotions. To enable this, data are transmitted from\nusers' devices and stored on central servers. Yet, these data contain sensitive\ninformation that could be used by mobile applications without user's consent\nor, maliciously, by an eavesdropping adversary. In this work, we show how\nmultimodal representations trained for a primary task, here emotion\nrecognition, can unintentionally leak demographic information, which could\noverride a selected opt-out option by the user. We analyze how this leakage\ndiffers in representations obtained from textual, acoustic, and multimodal\ndata. We use an adversarial learning paradigm to unlearn the private\ninformation present in a representation and investigate the effect of varying\nthe strength of the adversarial component on the primary task and on the\nprivacy metric, defined here as the inability of an attacker to predict\nspecific demographic information. We evaluate this paradigm on multiple\ndatasets and show that we can improve the privacy metric while not\nsignificantly impacting the performance on the primary task. To the best of our\nknowledge, this is the first work to analyze how the privacy metric differs\nacross modalities and how multiple privacy concerns can be tackled while still\nmaintaining performance on emotion recognition.", "published": "2019-10-29 11:49:30", "link": "http://arxiv.org/abs/1910.13212v1", "categories": ["cs.LG", "cs.CL", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Spoofing Speaker Verification Systems with Deep Multi-speaker\n  Text-to-speech Synthesis", "abstract": "This paper proposes a deep multi-speaker text-to-speech (TTS) model for\nspoofing speaker verification (SV) systems. The proposed model employs one\nnetwork to synthesize time-downsampled mel-spectrograms from text input and\nanother network to convert them to linear-frequency spectrograms, which are\nfurther converted to the time domain using the Griffin-Lim algorithm. Both\nnetworks are trained separately under the generative adversarial networks (GAN)\nframework. Spoofing experiments on two state-of-the-art SV systems (i-vectors\nand Google's GE2E) show that the proposed system can successfully spoof these\nsystems with a high success rate. Spoofing experiments on anti-spoofing systems\n(i.e., binary classifiers for discriminating real and synthetic speech) also\nshow a high spoof success rate when such anti-spoofing systems' structures are\nexposed to the proposed TTS system.", "published": "2019-10-29 02:50:11", "link": "http://arxiv.org/abs/1910.13054v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Disentangling Timbre and Singing Style with Multi-singer Singing\n  Synthesis System", "abstract": "In this study, we define the identity of the singer with two independent\nconcepts - timbre and singing style - and propose a multi-singer singing\nsynthesis system that can model them separately. To this end, we extend our\nsingle-singer model into a multi-singer model in the following ways: first, we\ndesign a singer identity encoder that can adequately reflect the identity of a\nsinger. Second, we use encoded singer identity to condition the two independent\ndecoders that model timbre and singing style, respectively. Through a user\nstudy with the listening tests, we experimentally verify that the proposed\nframework is capable of generating a natural singing voice of high quality\nwhile independently controlling the timbre and singing style. Also, by using\nthe method of changing singing styles while fixing the timbre, we suggest that\nour proposed network can produce a more expressive singing voice.", "published": "2019-10-29 03:40:07", "link": "http://arxiv.org/abs/1910.13069v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Replay Spoofing Countermeasure Using Autoencoder and Siamese Network on\n  ASVspoof 2019 Challenge", "abstract": "Automatic Speaker Verification (ASV) is the process of identifying a person\nbased on the voice presented to a system. Different synthetic approaches allow\nspoofing to deceive ASV systems (ASVs), whether using techniques to imitate a\nvoice or recunstruct the features. Attackers try to beat up the ASVs using four\ngeneral techniques; impersonation, speech synthesis, voice conversion, and\nreplay. The last technique is considered as a common and high potential tool\nfor spoofing purposes since replay attacks are more accessible and require no\ntechnical knowledge from adversaries. In this study, we introduce a novel\nreplay spoofing countermeasure for ASVs. Accordingly, we used the Constant Q\nCepstral Coefficient (CQCC) features fed into an autoencoder to attain more\ninformative features and to consider the noise information of spoofed\nutterances for discrimination purpose. Finally, different configurations of the\nSiamese network were used for the first time in this context for\nclassification. The experiments performed on ASVspoof challenge 2019 dataset\nusing Equal Error Rate (EER) and Tandem Detection Cost Function (t-DCF) as\nevaluation metrics show that the proposed system improved the results over the\nbaseline by 10.73% and 0.2344 in terms of EER and t-DCF, respectively.", "published": "2019-10-29 16:03:04", "link": "http://arxiv.org/abs/1910.13345v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DEPA: Self-Supervised Audio Embedding for Depression Detection", "abstract": "Depression detection research has increased over the last few decades, one\nmajor bottleneck of which is the limited data availability and representation\nlearning. Recently, self-supervised learning has seen success in pretraining\ntext embeddings and has been applied broadly on related tasks with sparse data,\nwhile pretrained audio embeddings based on self-supervised learning are rarely\ninvestigated. This paper proposes DEPA, a self-supervised, pretrained\ndepression audio embedding method for depression detection. An encoder-decoder\nnetwork is used to extract DEPA on in-domain depressed datasets (DAIC and MDD)\nand out-domain (Switchboard, Alzheimer's) datasets. With DEPA as the audio\nembedding extracted at response-level, a significant performance gain is\nachieved on downstream tasks, evaluated on both sparse datasets like DAIC and\nlarge major depression disorder dataset (MDD). This paper not only exhibits\nitself as a novel embedding extracting method capturing response-level\nrepresentation for depression detection but more significantly, is an\nexploration of self-supervised learning in a specific task within audio\nprocessing.", "published": "2019-10-29 01:11:58", "link": "http://arxiv.org/abs/1910.13028v3", "categories": ["cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
{"title": "On Investigation of Unsupervised Speech Factorization Based on\n  Normalization Flow", "abstract": "Speech signals are complex composites of various information, including\nphonetic content, speaker traits, channel effect, etc. Decomposing this\ncomplicated mixture into independent factors, i.e., speech factorization, is\nfundamentally important and plays the central role in many important algorithms\nof modern speech processing tasks. In this paper, we present a preliminary\ninvestigation on unsupervised speech factorization based on the normalization\nflow model. This model constructs a complex invertible transform, by which we\ncan project speech segments into a latent code space where the distribution is\na simple diagonal Gaussian. Our preliminary investigation on the TIMIT database\nshows that this code space exhibits favorable properties such as denseness and\npseudo linearity, and perceptually important factors such as phonetic content\nand speaker trait can be represented as particular directions within the code\nspace.", "published": "2019-10-29 14:26:22", "link": "http://arxiv.org/abs/1910.13288v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improving sequence-to-sequence speech recognition training with\n  on-the-fly data augmentation", "abstract": "Sequence-to-Sequence (S2S) models recently started to show state-of-the-art\nperformance for automatic speech recognition (ASR). With these large and deep\nmodels overfitting remains the largest problem, outweighing performance\nimprovements that can be obtained from better architectures. One solution to\nthe overfitting problem is increasing the amount of available training data and\nthe variety exhibited by the training data with the help of data augmentation.\nIn this paper we examine the influence of three data augmentation methods on\nthe performance of two S2S model architectures. One of the data augmentation\nmethod comes from literature, while two other methods are our own development -\na time perturbation in the frequency domain and sub-sequence sampling. Our\nexperiments on Switchboard and Fisher data show state-of-the-art performance\nfor S2S models that are trained solely on the speech training data and do not\nuse additional text data.", "published": "2019-10-29 14:38:22", "link": "http://arxiv.org/abs/1910.13296v2", "categories": ["eess.AS", "cs.CV", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A novel fuzzy logic-based metric for audio quality assessment: Objective\n  audio quality assessment", "abstract": "ITU-R BS.1387 states a method for objective assessment of perceived audio\nquality. This Recommendation, known also as PEAQ (Perceptual Evaluation of\nAudio Quality) is based on a psychoacoustic model of the human ear and was\nstandardized by the International Telecommunications Union as an alternative to\nsubjective tests, which are expensive and time-consuming processes. PEAQ\ncombines various physiological and psycho-acoustical properties of the human\near to give a measure of the quality difference between a reference audio and a\ntest audio. The reference audio signal could be considered as a distortion-free\nsource, whereas the test signal is a distorted version of the reference, which\nmay have audible artifacts because of compression. The algorithm computes the\nModel Output Variables (MOVs) which are mapped to a single quality measure,\nObjective Difference Grade (ODG), using a three-layer perceptron artificial\nneural network. The ODG estimates the perceived distortion between both audio\nsignals. In this paper we propose a new metric of low computational complexity\ncalled FQI (Fuzzy Quality Index) which is based on Fuzzy Logic reasoning and\nhas been incorporated into the existing PEAQ model to improve its overall\nperformance. Results show that the modified version slightly outperforms PEAQ.", "published": "2019-10-29 23:10:01", "link": "http://arxiv.org/abs/1910.13571v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
