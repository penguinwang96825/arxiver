{"title": "A Generative Approach for Script Event Prediction via Contrastive\n  Fine-tuning", "abstract": "Script event prediction aims to predict the subsequent event given the\ncontext. This requires the capability to infer the correlations between events.\nRecent works have attempted to improve event correlation reasoning by using\npretrained language models and incorporating external knowledge~(e.g.,\ndiscourse relations). Though promising results have been achieved, some\nchallenges still remain. First, the pretrained language models adopted by\ncurrent works ignore event-level knowledge, resulting in an inability to\ncapture the correlations between events well. Second, modeling correlations\nbetween events with discourse relations is limited because it can only capture\nexplicit correlations between events with discourse markers, and cannot capture\nmany implicit correlations. To this end, we propose a novel generative approach\nfor this task, in which a pretrained language model is fine-tuned with an\nevent-centric pretraining objective and predicts the next event within a\ngenerative paradigm. Specifically, we first introduce a novel event-level blank\ninfilling strategy as the learning objective to inject event-level knowledge\ninto the pretrained language model, and then design a likelihood-based\ncontrastive loss for fine-tuning the generative model. Instead of using an\nadditional prediction layer, we perform prediction by using sequence\nlikelihoods generated by the generative model. Our approach models correlations\nbetween events in a soft way without any external knowledge. The\nlikelihood-based prediction eliminates the need to use additional networks to\nmake predictions and is somewhat interpretable since it scores each word in the\nevent. Experimental results on the multi-choice narrative cloze~(MCNC) task\ndemonstrate that our approach achieves better results than other\nstate-of-the-art baselines. Our code will be available at\nhttps://github.com/zhufq00/mcnc.", "published": "2022-12-07 07:32:47", "link": "http://arxiv.org/abs/2212.03496v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WIDER & CLOSER: Mixture of Short-channel Distillers for Zero-shot\n  Cross-lingual Named Entity Recognition", "abstract": "Zero-shot cross-lingual named entity recognition (NER) aims at transferring\nknowledge from annotated and rich-resource data in source languages to\nunlabeled and lean-resource data in target languages. Existing mainstream\nmethods based on the teacher-student distillation framework ignore the rich and\ncomplementary information lying in the intermediate layers of pre-trained\nlanguage models, and domain-invariant information is easily lost during\ntransfer. In this study, a mixture of short-channel distillers (MSD) method is\nproposed to fully interact the rich hierarchical information in the teacher\nmodel and to transfer knowledge to the student model sufficiently and\nefficiently. Concretely, a multi-channel distillation framework is designed for\nsufficient information transfer by aggregating multiple distillers as a\nmixture. Besides, an unsupervised method adopting parallel domain adaptation is\nproposed to shorten the channels between the teacher and student models to\npreserve domain-invariant features. Experiments on four datasets across nine\nlanguages demonstrate that the proposed method achieves new state-of-the-art\nperformance on zero-shot cross-lingual NER and shows great generalization and\ncompatibility across languages and fields.", "published": "2022-12-07 08:13:22", "link": "http://arxiv.org/abs/2212.03506v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tag Embedding and Well-defined Intermediate Representation improve\n  Auto-Formulation of Problem Description", "abstract": "In this report, I address auto-formulation of problem description, the task\nof converting an optimization problem into a canonical representation. I first\nsimplify the auto-formulation task by defining an intermediate representation,\nthen introduce entity tag embedding to utilize a given entity tag information.\nThe ablation study demonstrate the effectiveness of the proposed method, which\nfinally took second place in NeurIPS 2022 NL4Opt competition subtask 2.", "published": "2022-12-07 11:23:43", "link": "http://arxiv.org/abs/2212.03575v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "G-MAP: General Memory-Augmented Pre-trained Language Model for Domain\n  Tasks", "abstract": "Recently, domain-specific PLMs have been proposed to boost the task\nperformance of specific domains (e.g., biomedical and computer science) by\ncontinuing to pre-train general PLMs with domain-specific corpora. However,\nthis Domain-Adaptive Pre-Training (DAPT; Gururangan et al. (2020)) tends to\nforget the previous general knowledge acquired by general PLMs, which leads to\na catastrophic forgetting phenomenon and sub-optimal performance. To alleviate\nthis problem, we propose a new framework of General Memory Augmented\nPre-trained Language Model (G-MAP), which augments the domain-specific PLM by a\nmemory representation built from the frozen general PLM without losing any\ngeneral knowledge. Specifically, we propose a new memory-augmented layer, and\nbased on it, different augmented strategies are explored to build the memory\nrepresentation and then adaptively fuse it into the domain-specific PLM. We\ndemonstrate the effectiveness of G-MAP on various domains (biomedical and\ncomputer science publications, news, and reviews) and different kinds (text\nclassification, QA, NER) of tasks, and the extensive results show that the\nproposed G-MAP can achieve SOTA results on all tasks.", "published": "2022-12-07 13:07:24", "link": "http://arxiv.org/abs/2212.03613v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Harnessing Knowledge and Reasoning for Human-Like Natural Language\n  Generation: A Brief Review", "abstract": "The rapid development and application of natural language generation (NLG)\ntechniques has revolutionized the field of automatic text production. However,\nthese techniques are still limited in their ability to produce human-like text\nthat is truly reasonable and informative. In this paper, we explore the\nimportance of NLG being guided by knowledge, in order to convey human-like\nreasoning through language generation. We propose ten goals for intelligent NLG\nsystems to pursue, and briefly review the achievement of NLG techniques guided\nby knowledge and reasoning. We also conclude by envisioning future directions\nand challenges in the pursuit of these goals.", "published": "2022-12-07 16:18:19", "link": "http://arxiv.org/abs/2212.03747v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Memorization of Named Entities in Fine-tuned BERT Models", "abstract": "Privacy preserving deep learning is an emerging field in machine learning\nthat aims to mitigate the privacy risks in the use of deep neural networks. One\nsuch risk is training data extraction from language models that have been\ntrained on datasets, which contain personal and privacy sensitive information.\nIn our study, we investigate the extent of named entity memorization in\nfine-tuned BERT models. We use single-label text classification as\nrepresentative downstream task and employ three different fine-tuning setups in\nour experiments, including one with Differential Privacy (DP). We create a\nlarge number of text samples from the fine-tuned BERT models utilizing a custom\nsequential sampling strategy with two prompting strategies. We search in these\nsamples for named entities and check if they are also present in the\nfine-tuning datasets. We experiment with two benchmark datasets in the domains\nof emails and blogs. We show that the application of DP has a detrimental\neffect on the text generation capabilities of BERT. Furthermore, we show that a\nfine-tuned BERT does not generate more named entities specific to the\nfine-tuning dataset than a BERT model that is pre-trained only. This suggests\nthat BERT is unlikely to emit personal or privacy sensitive named entities.\nOverall, our results are important to understand to what extent BERT-based\nservices are prone to training data extraction attacks.", "published": "2022-12-07 16:20:50", "link": "http://arxiv.org/abs/2212.03749v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Robustness of Learning from Task Instructions", "abstract": "Traditional supervised learning mostly works on individual tasks and requires\ntraining on a large set of task-specific examples. This paradigm seriously\nhinders the development of task generalization since preparing a task-specific\nexample set is costly. To build a system that can quickly and easily generalize\nto new tasks, task instructions have been adopted as an emerging trend of\nsupervision recently. These instructions give the model the definition of the\ntask and allow the model to output the appropriate answer based on the\ninstructions and inputs. However, task instructions are often expressed in\ndifferent forms, which can be interpreted from two threads: first, some\ninstructions are short sentences and are pretrained language model (PLM)\noriented, such as prompts, while other instructions are paragraphs and are\nhuman-oriented, such as those in Amazon MTurk; second, different end-users very\nlikely explain the same task with instructions of different textual\nexpressions. A robust system for task generalization should be able to handle\nany new tasks regardless of the variability of instructions.\n  However, the system robustness in dealing with instruction-driven task\ngeneralization is still unexplored. This work investigates the system\nrobustness when the instructions of new tasks are (i) manipulated, (ii)\nparaphrased, or (iii) from different levels of conciseness. To our knowledge,\nthis is the first work that systematically studies how robust a PLM is when it\nis supervised by instructions with different factors of variability.", "published": "2022-12-07 17:54:59", "link": "http://arxiv.org/abs/2212.03813v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards using Few-Shot Prompt Learning for Automating Model Completion", "abstract": "We propose a simple yet a novel approach to improve completion in domain\nmodeling activities. Our approach exploits the power of large language models\nby using few-shot prompt learning without the need to train or fine-tune those\nmodels with large datasets that are scarce in this field. We implemented our\napproach and tested it on the completion of static and dynamic domain diagrams.\nOur initial evaluation shows that such an approach is effective and can be\nintegrated in different ways during the modeling activities.", "published": "2022-12-07 02:11:26", "link": "http://arxiv.org/abs/2212.03404v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "JamPatoisNLI: A Jamaican Patois Natural Language Inference Dataset", "abstract": "JamPatoisNLI provides the first dataset for natural language inference in a\ncreole language, Jamaican Patois. Many of the most-spoken low-resource\nlanguages are creoles. These languages commonly have a lexicon derived from a\nmajor world language and a distinctive grammar reflecting the languages of the\noriginal speakers and the process of language birth by creolization. This gives\nthem a distinctive place in exploring the effectiveness of transfer from large\nmonolingual or multilingual pretrained models. While our work, along with\nprevious work, shows that transfer from these models to low-resource languages\nthat are unrelated to languages in their training set is not very effective, we\nwould expect stronger results from transfer to creoles. Indeed, our experiments\nshow considerably better results from few-shot learning of JamPatoisNLI than\nfor such unrelated languages, and help us begin to understand how the unique\nrelationship between creoles and their high-resource base languages affect\ncross-lingual transfer. JamPatoisNLI, which consists of naturally-occurring\npremises and expert-written hypotheses, is a step towards steering research\ninto a traditionally underserved language and a useful benchmark for\nunderstanding cross-lingual NLP.", "published": "2022-12-07 03:07:02", "link": "http://arxiv.org/abs/2212.03419v1", "categories": ["cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training", "abstract": "This paper presents E5, a family of state-of-the-art text embeddings that\ntransfer well to a wide range of tasks. The model is trained in a contrastive\nmanner with weak supervision signals from our curated large-scale text pair\ndataset (called CCPairs). E5 can be readily used as a general-purpose embedding\nmodel for any tasks requiring a single-vector representation of texts such as\nretrieval, clustering, and classification, achieving strong performance in both\nzero-shot and fine-tuned settings. We conduct extensive evaluations on 56\ndatasets from the BEIR and MTEB benchmarks. For zero-shot settings, E5 is the\nfirst model that outperforms the strong BM25 baseline on the BEIR retrieval\nbenchmark without using any labeled data. When fine-tuned, E5 obtains the best\nresults on the MTEB benchmark, beating existing embedding models with 40x more\nparameters.", "published": "2022-12-07 09:25:54", "link": "http://arxiv.org/abs/2212.03533v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Talking About Large Language Models", "abstract": "Thanks to rapid progress in artificial intelligence, we have entered an era\nwhen technology and philosophy intersect in interesting ways. Sitting squarely\nat the centre of this intersection are large language models (LLMs). The more\nadept LLMs become at mimicking human language, the more vulnerable we become to\nanthropomorphism, to seeing the systems in which they are embedded as more\nhuman-like than they really are. This trend is amplified by the natural\ntendency to use philosophically loaded terms, such as \"knows\", \"believes\", and\n\"thinks\", when describing these systems. To mitigate this trend, this paper\nadvocates the practice of repeatedly stepping back to remind ourselves of how\nLLMs, and the systems of which they form a part, actually work. The hope is\nthat increased scientific precision will encourage more philosophical nuance in\nthe discourse around artificial intelligence, both within the field and in the\npublic sphere.", "published": "2022-12-07 10:01:44", "link": "http://arxiv.org/abs/2212.03551v5", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Pivotal Role of Language Modeling in Recommender Systems: Enriching\n  Task-specific and Task-agnostic Representation Learning", "abstract": "Recent studies have proposed unified user modeling frameworks that leverage\nuser behavior data from various applications. Many of them benefit from\nutilizing users' behavior sequences as plain texts, representing rich\ninformation in any domain or system without losing generality. Hence, a\nquestion arises: Can language modeling for user history corpus help improve\nrecommender systems? While its versatile usability has been widely investigated\nin many domains, its applications to recommender systems still remain\nunderexplored. We show that language modeling applied directly to task-specific\nuser histories achieves excellent results on diverse recommendation tasks.\nAlso, leveraging additional task-agnostic user histories delivers significant\nperformance benefits. We further demonstrate that our approach can provide\npromising transfer learning capabilities for a broad spectrum of real-world\nrecommender systems, even on unseen domains and services.", "published": "2022-12-07 16:31:14", "link": "http://arxiv.org/abs/2212.03760v5", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "TweetDrought: A Deep-Learning Drought Impacts Recognizer based on\n  Twitter Data", "abstract": "Acquiring a better understanding of drought impacts becomes increasingly\nvital under a warming climate. Traditional drought indices describe mainly\nbiophysical variables and not impacts on social, economic, and environmental\nsystems. We utilized natural language processing and bidirectional encoder\nrepresentation from Transformers (BERT) based transfer learning to fine-tune\nthe model on the data from the news-based Drought Impact Report (DIR) and then\napply it to recognize seven types of drought impacts based on the filtered\nTwitter data from the United States. Our model achieved a satisfying macro-F1\nscore of 0.89 on the DIR test set. The model was then applied to California\ntweets and validated with keyword-based labels. The macro-F1 score was 0.58.\nHowever, due to the limitation of keywords, we also spot-checked tweets with\ncontroversial labels. 83.5% of BERT labels were correct compared to the keyword\nlabels. Overall, the fine-tuned BERT-based recognizer provided proper\npredictions and valuable information on drought impacts. The interpretation and\nanalysis of the model were consistent with experiential domain expertise.", "published": "2022-12-07 23:21:36", "link": "http://arxiv.org/abs/2212.04001v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Analysis and Utilization of Entrainment on Acoustic and Emotion Features\n  in User-agent Dialogue", "abstract": "Entrainment is the phenomenon by which an interlocutor adapts their speaking\nstyle to align with their partner in conversations. It has been found in\ndifferent dimensions as acoustic, prosodic, lexical or syntactic. In this work,\nwe explore and utilize the entrainment phenomenon to improve spoken dialogue\nsystems for voice assistants. We first examine the existence of the entrainment\nphenomenon in human-to-human dialogues in respect to acoustic feature and then\nextend the analysis to emotion features. The analysis results show strong\nevidence of entrainment in terms of both acoustic and emotion features. Based\non this findings, we implement two entrainment policies and assess if the\nintegration of entrainment principle into a Text-to-Speech (TTS) system\nimproves the synthesis performance and the user experience. It is found that\nthe integration of the entrainment principle into a TTS system brings\nperformance improvement when considering acoustic features, while no obvious\nimprovement is observed when considering emotion features.", "published": "2022-12-07 01:45:15", "link": "http://arxiv.org/abs/2212.03398v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improve Bilingual TTS Using Dynamic Language and Phonology Embedding", "abstract": "In most cases, bilingual TTS needs to handle three types of input scripts:\nfirst language only, second language only, and second language embedded in the\nfirst language. In the latter two situations, the pronunciation and intonation\nof the second language are usually quite different due to the influence of the\nfirst language. Therefore, it is a big challenge to accurately model the\npronunciation and intonation of the second language in different contexts\nwithout mutual interference. This paper builds a Mandarin-English TTS system to\nacquire more standard spoken English speech from a monolingual Chinese speaker.\nWe introduce phonology embedding to capture the English differences between\ndifferent phonology. Embedding mask is applied to language embedding for\ndistinguishing information between different languages and to phonology\nembedding for focusing on English expression. We specially design an embedding\nstrength modulator to capture the dynamic strength of language and phonology.\nExperiments show that our approach can produce significantly more natural and\nstandard spoken English speech of the monolingual Chinese speaker. From\nanalysis, we find that suitable phonology control contributes to better\nperformance in different scenarios.", "published": "2022-12-07 03:46:18", "link": "http://arxiv.org/abs/2212.03435v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improved Self-Supervised Multilingual Speech Representation Learning\n  Combined with Auxiliary Language Information", "abstract": "Multilingual end-to-end models have shown great improvement over monolingual\nsystems. With the development of pre-training methods on speech,\nself-supervised multilingual speech representation learning like XLSR has shown\nsuccess in improving the performance of multilingual automatic speech\nrecognition (ASR). However, similar to the supervised learning, multilingual\npre-training may also suffer from language interference and further affect the\napplication of multilingual system. In this paper, we introduce several\ntechniques for improving self-supervised multilingual pre-training by\nleveraging auxiliary language information, including the language adversarial\ntraining, language embedding and language adaptive training during the\npre-training stage. We conduct experiments on a multilingual ASR task\nconsisting of 16 languages. Our experimental results demonstrate 14.3% relative\ngain over the standard XLSR model, and 19.8% relative gain over the no\npre-training multilingual model.", "published": "2022-12-07 06:18:59", "link": "http://arxiv.org/abs/2212.03476v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Low-Resource End-to-end Sanskrit TTS using Tacotron2, WaveGlow and\n  Transfer Learning", "abstract": "End-to-end text-to-speech (TTS) systems have been developed for European\nlanguages like English and Spanish with state-of-the-art speech quality,\nprosody, and naturalness. However, development of end-to-end TTS for Indian\nlanguages is lagging behind in terms of quality. The challenges involved in\nsuch a task are: 1) scarcity of quality training data; 2) low efficiency during\ntraining and inference; 3) slow convergence in the case of large vocabulary\nsize. In our work reported in this paper, we have investigated the use of\nfine-tuning the English-pretrained Tacotron2 model with limited Sanskrit data\nto synthesize natural sounding speech in Sanskrit in low resource settings. Our\nexperiments show encouraging results, achieving an overall MOS of 3.38 from 37\nevaluators with good Sanskrit spoken knowledge. This is really a very good\nresult, considering the fact that the speech data we have used is of duration\n2.5 hours only.", "published": "2022-12-07 10:15:34", "link": "http://arxiv.org/abs/2212.03558v1", "categories": ["cs.CL", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.CL"}
{"title": "M3ST: Mix at Three Levels for Speech Translation", "abstract": "How to solve the data scarcity problem for end-to-end speech-to-text\ntranslation (ST)? It's well known that data augmentation is an efficient method\nto improve performance for many tasks by enlarging the dataset. In this paper,\nwe propose Mix at three levels for Speech Translation (M^3ST) method to\nincrease the diversity of the augmented training corpus. Specifically, we\nconduct two phases of fine-tuning based on a pre-trained model using external\nmachine translation (MT) data. In the first stage of fine-tuning, we mix the\ntraining corpus at three levels, including word level, sentence level and frame\nlevel, and fine-tune the entire model with mixed data. At the second stage of\nfine-tuning, we take both original speech sequences and original text sequences\nin parallel into the model to fine-tune the network, and use Jensen-Shannon\ndivergence to regularize their outputs. Experiments on MuST-C speech\ntranslation benchmark and analysis show that M^3ST outperforms current strong\nbaselines and achieves state-of-the-art results on eight directions with an\naverage BLEU of 29.9.", "published": "2022-12-07 14:22:00", "link": "http://arxiv.org/abs/2212.03657v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Discovering Latent Knowledge in Language Models Without Supervision", "abstract": "Existing techniques for training language models can be misaligned with the\ntruth: if we train models with imitation learning, they may reproduce errors\nthat humans make; if we train them to generate text that humans rate highly,\nthey may output errors that human evaluators can't detect. We propose\ncircumventing this issue by directly finding latent knowledge inside the\ninternal activations of a language model in a purely unsupervised way.\nSpecifically, we introduce a method for accurately answering yes-no questions\ngiven only unlabeled model activations. It works by finding a direction in\nactivation space that satisfies logical consistency properties, such as that a\nstatement and its negation have opposite truth values. We show that despite\nusing no supervision and no model outputs, our method can recover diverse\nknowledge represented in large language models: across 6 models and 10\nquestion-answering datasets, it outperforms zero-shot accuracy by 4\\% on\naverage. We also find that it cuts prompt sensitivity in half and continues to\nmaintain high accuracy even when models are prompted to generate incorrect\nanswers. Our results provide an initial step toward discovering what language\nmodels know, distinct from what they say, even when we don't have access to\nexplicit ground truth labels.", "published": "2022-12-07 18:17:56", "link": "http://arxiv.org/abs/2212.03827v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Hierarchical multimodal transformers for Multi-Page DocVQA", "abstract": "Document Visual Question Answering (DocVQA) refers to the task of answering\nquestions from document images. Existing work on DocVQA only considers\nsingle-page documents. However, in real scenarios documents are mostly composed\nof multiple pages that should be processed altogether. In this work we extend\nDocVQA to the multi-page scenario. For that, we first create a new dataset,\nMP-DocVQA, where questions are posed over multi-page documents instead of\nsingle pages. Second, we propose a new hierarchical method, Hi-VT5, based on\nthe T5 architecture, that overcomes the limitations of current methods to\nprocess long multi-page documents. The proposed method is based on a\nhierarchical transformer architecture where the encoder summarizes the most\nrelevant information of every page and then, the decoder takes this summarized\ninformation to generate the final answer. Through extensive experimentation, we\ndemonstrate that our method is able, in a single stage, to answer the questions\nand provide the page that contains the relevant information to find the answer,\nwhich can be used as a kind of explainability measure.", "published": "2022-12-07 10:09:49", "link": "http://arxiv.org/abs/2212.05935v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Lattice-Free Sequence Discriminative Training for Phoneme-Based Neural\n  Transducers", "abstract": "Recently, RNN-Transducers have achieved remarkable results on various\nautomatic speech recognition tasks. However, lattice-free sequence\ndiscriminative training methods, which obtain superior performance in hybrid\nmodels, are rarely investigated in RNN-Transducers. In this work, we propose\nthree lattice-free training objectives, namely lattice-free maximum mutual\ninformation, lattice-free segment-level minimum Bayes risk, and lattice-free\nminimum Bayes risk, which are used for the final posterior output of the\nphoneme-based neural transducer with a limited context dependency. Compared to\ncriteria using N-best lists, lattice-free methods eliminate the decoding step\nfor hypotheses generation during training, which leads to more efficient\ntraining. Experimental results show that lattice-free methods gain up to 6.5%\nrelative improvement in word error rate compared to a sequence-level\ncross-entropy trained model. Compared to the N-best-list based minimum Bayes\nrisk objectives, lattice-free methods gain 40% - 70% relative training time\nspeedup with a small degradation in performance.", "published": "2022-12-07 12:49:10", "link": "http://arxiv.org/abs/2212.04325v3", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Pre-Training With Scientific Text Improves Educational Question\n  Generation", "abstract": "With the boom of digital educational materials and scalable e-learning\nsystems, the potential for realising AI-assisted personalised learning has\nskyrocketed. In this landscape, the automatic generation of educational\nquestions will play a key role, enabling scalable self-assessment when a global\npopulation is manoeuvring their personalised learning journeys. We develop\nEduQG, a novel educational question generation model built by adapting a large\nlanguage model. Our initial experiments demonstrate that EduQG can produce\nsuperior educational questions by pre-training on scientific text.", "published": "2022-12-07 17:17:58", "link": "http://arxiv.org/abs/2212.03869v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.IR", "cs.LG", "stat.ML", "H.3.3; J.1; I.2.0"], "primary_category": "cs.CL"}
{"title": "Selector-Enhancer: Learning Dynamic Selection of Local and Non-local\n  Attention Operation for Speech Enhancement", "abstract": "Attention mechanisms, such as local and non-local attention, play a\nfundamental role in recent deep learning based speech enhancement (SE) systems.\nHowever, natural speech contains many fast-changing and relatively brief\nacoustic events, therefore, capturing the most informative speech features by\nindiscriminately using local and non-local attention is challenged. We observe\nthat the noise type and speech feature vary within a sequence of speech and the\nlocal and non-local operations can respectively extract different features from\ncorrupted speech. To leverage this, we propose Selector-Enhancer, a\ndual-attention based convolution neural network (CNN) with a feature-filter\nthat can dynamically select regions from low-resolution speech features and\nfeed them to local or non-local attention operations. In particular, the\nproposed feature-filter is trained by using reinforcement learning (RL) with a\ndeveloped difficulty-regulated reward that is related to network performance,\nmodel complexity, and \"the difficulty of the SE task\". The results show that\nour method achieves comparable or superior performance to existing approaches.\nIn particular, Selector-Enhancer is potentially effective for real-world\ndenoising, where the number and types of noise are varies on a single noisy\nmixture.", "published": "2022-12-07 02:34:03", "link": "http://arxiv.org/abs/2212.03408v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improving trajectory localization accuracy via direction-of-arrival\n  derivative estimation", "abstract": "Sound source localization is crucial in acoustic sensing and\nmonitoring-related applications. In this paper, we do a comprehensive analysis\nof improvement in sound source localization by combining the direction of\narrivals (DOAs) with their derivatives which quantify the changes in the\npositions of sources over time. This study uses the SALSA-Lite feature with a\nconvolutional recurrent neural network (CRNN) model for predicting DOAs and\ntheir first-order derivatives. An update rule is introduced to combine the\npredicted DOAs with the estimated derivatives to obtain the final DOAs. The\nexperimental validation is done using TAU-NIGENS Spatial Sound Events (TNSSE)\n2021 dataset. We compare the performance of the networks predicting DOAs with\nderivative vs. the one predicting only the DOAs at low SNR levels. The results\nshow that combining the derivatives with the DOAs improves the localization\naccuracy of moving sources.", "published": "2022-12-07 05:33:45", "link": "http://arxiv.org/abs/2212.03470v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Progressive Multi-Scale Self-Supervised Learning for Speech Recognition", "abstract": "Self-supervised learning (SSL) models have achieved considerable improvements\nin automatic speech recognition (ASR). In addition, ASR performance could be\nfurther improved if the model is dedicated to audio content information\nlearning theoretically. To this end, we propose a progressive multi-scale\nself-supervised learning (PMS-SSL) method, which uses fine-grained target sets\nto compute SSL loss at top layer while uses coarse-grained target sets at\nintermediate layers. Furthermore, PMS-SSL introduces multi-scale structure into\nmulti-head self-attention for better speech representation, which restricts the\nattention area into a large scope at higher layers while restricts the\nattention area into a small scope at lower layers. Experiments on Librispeech\ndataset indicate the effectiveness of our proposed method. Compared with\nHuBERT, PMS-SSL achieves 13.7% / 12.7% relative WER reduction on test other\nevaluation subsets respectively when fine-tuned on 10hours / 100hours subsets.", "published": "2022-12-07 06:29:00", "link": "http://arxiv.org/abs/2212.03480v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improved Speech Pre-Training with Supervision-Enhanced Acoustic Unit", "abstract": "Speech pre-training has shown great success in learning useful and general\nlatent representations from large-scale unlabeled data. Based on a\nwell-designed self-supervised learning pattern, pre-trained models can be used\nto serve lots of downstream speech tasks such as automatic speech recognition.\nIn order to take full advantage of the labed data in low resource task, we\npresent an improved pre-training method by introducing a supervision-enhanced\nacoustic unit (SEAU) pattern to intensify the expression of comtext information\nand ruduce the training cost. Encoder representations extracted from the SEAU\npattern are used to generate more representative target units for HuBERT\npre-training process. The proposed method, named SeHuBERT, achieves a relative\nword error rate reductions of 10.5% and 4.9% comared with the standard HuBERT\non Turkmen speech recognition task with 500 hours and 100 hours fine-tuning\ndata respectively. Extended to more languages and more data, SeHuBERT can aslo\nachieve a relative word error rate reductions of approximately 10% at half of\nthe training cost compared with HuBERT.", "published": "2022-12-07 06:31:31", "link": "http://arxiv.org/abs/2212.03482v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MIMO-DBnet: Multi-channel Input and Multiple Outputs DOA-aware\n  Beamforming Network for Speech Separation", "abstract": "Recently, many deep learning based beamformers have been proposed for\nmulti-channel speech separation. Nevertheless, most of them rely on extra cues\nknown in advance, such as speaker feature, face image or directional\ninformation. In this paper, we propose an end-to-end beamforming network for\ndirection guided speech separation given merely the mixture signal, namely\nMIMO-DBnet. Specifically, we design a multi-channel input and multiple outputs\narchitecture to predict the direction-of-arrival based embeddings and\nbeamforming weights for each source. The precisely estimated directional\nembedding provides quite effective spatial discrimination guidance for the\nneural beamformer to offset the effect of phase wrapping, thus allowing more\naccurate reconstruction of two sources' speech signals. Experiments show that\nour proposed MIMO-DBnet not only achieves a comprehensive decent improvement\ncompared to baseline systems, but also maintain the performance on high\nfrequency bands when phase wrapping occurs.", "published": "2022-12-07 01:52:40", "link": "http://arxiv.org/abs/2212.03401v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "FineDance: A Fine-grained Choreography Dataset for 3D Full Body Dance\n  Generation", "abstract": "Generating full-body and multi-genre dance sequences from given music is a\nchallenging task, due to the limitations of existing datasets and the inherent\ncomplexity of the fine-grained hand motion and dance genres. To address these\nproblems, we propose FineDance, which contains 14.6 hours of music-dance paired\ndata, with fine-grained hand motions, fine-grained genres (22 dance genres),\nand accurate posture. To the best of our knowledge, FineDance is the largest\nmusic-dance paired dataset with the most dance genres. Additionally, to address\nmonotonous and unnatural hand movements existing in previous methods, we\npropose a full-body dance generation network, which utilizes the diverse\ngeneration capabilities of the diffusion model to solve monotonous problems,\nand use expert nets to solve unreal problems. To further enhance the\ngenre-matching and long-term stability of generated dances, we propose a\nGenre&Coherent aware Retrieval Module. Besides, we propose a novel metric named\nGenre Matching Score to evaluate the genre-matching degree between dance and\nmusic. Quantitative and qualitative experiments demonstrate the quality of\nFineDance, and the state-of-the-art performance of FineNet. The FineDance\nDataset and more qualitative samples can be found at our website.", "published": "2022-12-07 16:10:08", "link": "http://arxiv.org/abs/2212.03741v4", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "iQuery: Instruments as Queries for Audio-Visual Sound Separation", "abstract": "Current audio-visual separation methods share a standard architecture design\nwhere an audio encoder-decoder network is fused with visual encoding features\nat the encoder bottleneck. This design confounds the learning of multi-modal\nfeature encoding with robust sound decoding for audio separation. To generalize\nto a new instrument: one must finetune the entire visual and audio network for\nall musical instruments. We re-formulate visual-sound separation task and\npropose Instrument as Query (iQuery) with a flexible query expansion mechanism.\nOur approach ensures cross-modal consistency and cross-instrument\ndisentanglement. We utilize \"visually named\" queries to initiate the learning\nof audio queries and use cross-modal attention to remove potential sound source\ninterference at the estimated waveforms. To generalize to a new instrument or\nevent class, drawing inspiration from the text-prompt design, we insert an\nadditional query as an audio prompt while freezing the attention mechanism.\nExperimental results on three benchmarks demonstrate that our iQuery improves\naudio-visual sound source separation performance.", "published": "2022-12-07 17:55:06", "link": "http://arxiv.org/abs/2212.03814v2", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Talking Head Generation with Probabilistic Audio-to-Visual Diffusion\n  Priors", "abstract": "In this paper, we introduce a simple and novel framework for one-shot\naudio-driven talking head generation. Unlike prior works that require\nadditional driving sources for controlled synthesis in a deterministic manner,\nwe instead probabilistically sample all the holistic lip-irrelevant facial\nmotions (i.e. pose, expression, blink, gaze, etc.) to semantically match the\ninput audio while still maintaining both the photo-realism of audio-lip\nsynchronization and the overall naturalness. This is achieved by our newly\nproposed audio-to-visual diffusion prior trained on top of the mapping between\naudio and disentangled non-lip facial representations. Thanks to the\nprobabilistic nature of the diffusion prior, one big advantage of our framework\nis it can synthesize diverse facial motion sequences given the same audio clip,\nwhich is quite user-friendly for many real applications. Through comprehensive\nevaluations on public benchmarks, we conclude that (1) our diffusion prior\noutperforms auto-regressive prior significantly on almost all the concerned\nmetrics; (2) our overall system is competitive with prior works in terms of\naudio-lip synchronization but can effectively sample rich and natural-looking\nlip-irrelevant facial motions while still semantically harmonized with the\naudio input.", "published": "2022-12-07 17:55:41", "link": "http://arxiv.org/abs/2212.04248v1", "categories": ["cs.GR", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.GR"}
