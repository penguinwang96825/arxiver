{"title": "Procedural Dilemma Generation for Evaluating Moral Reasoning in Humans\n  and Language Models", "abstract": "As AI systems like language models are increasingly integrated into\ndecision-making processes affecting people's lives, it's critical to ensure\nthat these systems have sound moral reasoning. To test whether they do, we need\nto develop systematic evaluations. We provide a framework that uses a language\nmodel to translate causal graphs that capture key aspects of moral dilemmas\ninto prompt templates. With this framework, we procedurally generated a large\nand diverse set of moral dilemmas -- the OffTheRails benchmark -- consisting of\n50 scenarios and 400 unique test items. We collected moral permissibility and\nintention judgments from human participants for a subset of our items and\ncompared these judgments to those from two language models (GPT-4 and Claude-2)\nacross eight conditions. We find that moral dilemmas in which the harm is a\nnecessary means (as compared to a side effect) resulted in lower permissibility\nand higher intention ratings for both participants and language models. The\nsame pattern was observed for evitable versus inevitable harmful outcomes.\nHowever, there was no clear effect of whether the harm resulted from an agent's\naction versus from having omitted to act. We discuss limitations of our prompt\ngeneration pipeline and opportunities for improving scenarios to increase the\nstrength of experimental effects.", "published": "2024-04-17 01:13:04", "link": "http://arxiv.org/abs/2404.10975v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Offset Unlearning for Large Language Models", "abstract": "Despite the strong capabilities of Large Language Models (LLMs) to acquire\nknowledge from their training corpora, the memorization of sensitive\ninformation in the corpora such as copyrighted, harmful, and private content\nhas led to ethical and legal concerns. In response to these challenges,\nunlearning has emerged as a potential remedy for LLMs affected by problematic\ntraining data. However, previous unlearning techniques are either not\napplicable to black-box LLMs due to required access to model internal weights,\nor violate data protection principles by retaining sensitive data for\ninference-time correction. We propose $\\delta$-unlearning, an offset unlearning\nframework for black-box LLMs. Instead of tuning the black-box LLM itself,\n$\\delta$-unlearning learns the logit offset needed for unlearning by\ncontrasting the logits from a pair of smaller models. Experiments demonstrate\nthat $\\delta$-unlearning can effectively unlearn target data while maintaining\nsimilar or even stronger performance on general out-of-forget-scope tasks.\n$\\delta$-unlearning also effectively incorporates different unlearning\nalgorithms, making our approach a versatile solution to adapting various\nexisting unlearning algorithms to black-box LLMs.", "published": "2024-04-17 03:39:51", "link": "http://arxiv.org/abs/2404.11045v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do LLMs Think Fast and Slow? A Causal Study on Sentiment Analysis", "abstract": "Sentiment analysis (SA) aims to identify the sentiment expressed in a text,\nsuch as a product review. Given a review and the sentiment associated with it,\nthis work formulates SA as a combination of two tasks: (1) a causal discovery\ntask that distinguishes whether a review \"primes\" the sentiment (Causal\nHypothesis C1), or the sentiment \"primes\" the review (Causal Hypothesis C2);\nand (2) the traditional prediction task to model the sentiment using the review\nas input. Using the peak-end rule in psychology, we classify a sample as C1 if\nits overall sentiment score approximates an average of all the sentence-level\nsentiments in the review, and C2 if the overall sentiment score approximates an\naverage of the peak and end sentiments. For the prediction task, we use the\ndiscovered causal mechanisms behind the samples to improve LLM performance by\nproposing causal prompts that give the models an inductive bias of the\nunderlying causal graph, leading to substantial improvements by up to 32.13 F1\npoints on zero-shot five-class SA. Our code is at\nhttps://github.com/cogito233/causal-sa", "published": "2024-04-17 04:04:34", "link": "http://arxiv.org/abs/2404.11055v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unified Examination of Entity Linking in Absence of Candidate Sets", "abstract": "Despite remarkable strides made in the development of entity linking systems\nin recent years, a comprehensive comparative analysis of these systems using a\nunified framework is notably absent. This paper addresses this oversight by\nintroducing a new black-box benchmark and conducting a comprehensive evaluation\nof all state-of-the-art entity linking methods. We use an ablation study to\ninvestigate the impact of candidate sets on the performance of entity linking.\nOur findings uncover exactly how much such entity linking systems depend on\ncandidate sets, and how much this limits the general applicability of each\nsystem. We present an alternative approach to candidate sets, demonstrating\nthat leveraging the entire in-domain candidate set can serve as a viable\nsubstitute for certain models. We show the trade-off between less restrictive\ncandidate sets, increased inference time and memory footprint for some models.", "published": "2024-04-17 04:37:58", "link": "http://arxiv.org/abs/2404.11061v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Consistency Training by Synthetic Question Generation for Conversational\n  Question Answering", "abstract": "Efficiently modeling historical information is a critical component in\naddressing user queries within a conversational question-answering (QA)\ncontext, as historical context plays a vital role in clarifying the user's\nquestions. However, irrelevant history induces noise in the reasoning process,\nespecially for those questions with a considerable historical context. In our\nnovel model-agnostic approach, referred to as CoTaH (Consistency-Trained\naugmented History), we augment the historical information with synthetic\nquestions and subsequently employ consistency training to train a model that\nutilizes both real and augmented historical data to implicitly make the\nreasoning robust to irrelevant history. To the best of our knowledge, this is\nthe first instance of research using question generation as a form of data\naugmentation to model conversational QA settings. By citing a common modeling\nerror prevalent in previous research, we introduce a new baseline model and\ncompare our model's performance against it, demonstrating an improvement in\nresults, particularly when dealing with questions that include a substantial\namount of historical context. The source code can be found on our GitHub page.", "published": "2024-04-17 06:49:14", "link": "http://arxiv.org/abs/2404.11109v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Novel ICD Coding Method Based on Associated and Hierarchical Code\n  Description Distillation", "abstract": "ICD(International Classification of Diseases) coding involves assigning ICD\ncodes to patients visit based on their medical notes. ICD coding is a\nchallenging multilabel text classification problem due to noisy medical\ndocument inputs. Recent advancements in automated ICD coding have enhanced\nperformance by integrating additional data and knowledge bases with the\nencoding of medical notes and codes. However, most of them ignore the code\nhierarchy, leading to improper code assignments. To address these problems, we\npropose a novel framework based on associated and hierarchical code description\ndistillation (AHDD) for better code representation learning and avoidance of\nimproper code assignment.we utilize the code description and the hierarchical\nstructure inherent to the ICD codes. Therefore, in this paper, we leverage the\ncode description and the hierarchical structure inherent to the ICD codes. The\ncode description is also applied to aware the attention layer and output layer.\nExperimental results on the benchmark dataset show the superiority of the\nproposed framework over several state-of-the-art baselines.", "published": "2024-04-17 07:26:23", "link": "http://arxiv.org/abs/2404.11132v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Context-Aware Siamese Networks for Efficient Emotion Recognition in\n  Conversation", "abstract": "The advent of deep learning models has made a considerable contribution to\nthe achievement of Emotion Recognition in Conversation (ERC). However, this\ntask still remains an important challenge due to the plurality and subjectivity\nof human emotions. Previous work on ERC provides predictive models using mostly\ngraph-based conversation representations. In this work, we propose a way to\nmodel the conversational context that we incorporate into a metric learning\ntraining strategy, with a two-step process. This allows us to perform ERC in a\nflexible classification scenario and to end up with a lightweight yet efficient\nmodel. Using metric learning through a Siamese Network architecture, we achieve\n57.71 in macro F1 score for emotion classification in conversation on\nDailyDialog dataset, which outperforms the related work. This state-of-the-art\nresult is promising regarding the use of metric learning for emotion\nrecognition, yet perfectible compared to the microF1 score obtained.", "published": "2024-04-17 07:36:40", "link": "http://arxiv.org/abs/2404.11141v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FIZZ: Factual Inconsistency Detection by Zoom-in Summary and Zoom-out\n  Document", "abstract": "Through the advent of pre-trained language models, there have been notable\nadvancements in abstractive summarization systems. Simultaneously, a\nconsiderable number of novel methods for evaluating factual consistency in\nabstractive summarization systems has been developed. But these evaluation\napproaches incorporate substantial limitations, especially on refinement and\ninterpretability. In this work, we propose highly effective and interpretable\nfactual inconsistency detection method metric Factual Inconsistency Detection\nby Zoom-in Summary and Zoom-out Document for abstractive summarization systems\nthat is based on fine-grained atomic facts decomposition. Moreover, we align\natomic facts decomposed from the summary with the source document through\nadaptive granularity expansion. These atomic facts represent a more\nfine-grained unit of information, facilitating detailed understanding and\ninterpretability of the summary's factual inconsistency. Experimental results\ndemonstrate that our proposed factual consistency checking system significantly\noutperforms existing systems.", "published": "2024-04-17 09:01:02", "link": "http://arxiv.org/abs/2404.11184v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neuron Specialization: Leveraging intrinsic task modularity for\n  multilingual machine translation", "abstract": "Training a unified multilingual model promotes knowledge transfer but\ninevitably introduces negative interference. Language-specific modeling methods\nshow promise in reducing interference. However, they often rely on heuristics\nto distribute capacity and struggle to foster cross-lingual transfer via\nisolated modules. In this paper, we explore intrinsic task modularity within\nmultilingual networks and leverage these observations to circumvent\ninterference under multilingual translation. We show that neurons in the\nfeed-forward layers tend to be activated in a language-specific manner.\nMeanwhile, these specialized neurons exhibit structural overlaps that reflect\nlanguage proximity, which progress across layers. Based on these findings, we\npropose Neuron Specialization, an approach that identifies specialized neurons\nto modularize feed-forward layers and then continuously updates them through\nsparse networks. Extensive experiments show that our approach achieves\nconsistent performance gains over strong baselines with additional analyses\ndemonstrating reduced interference and increased knowledge transfer.", "published": "2024-04-17 09:33:19", "link": "http://arxiv.org/abs/2404.11201v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prompt-tuning for Clickbait Detection via Text Summarization", "abstract": "Clickbaits are surprising social posts or deceptive news headlines that\nattempt to lure users for more clicks, which have posted at unprecedented rates\nfor more profit or commercial revenue. The spread of clickbait has significant\nnegative impacts on the users, which brings users misleading or even\nclick-jacking attacks. Different from fake news, the crucial problem in\nclickbait detection is determining whether the headline matches the\ncorresponding content. Most existing methods compute the semantic similarity\nbetween the headlines and contents for detecting clickbait. However, due to\nsignificant differences in length and semantic features between headlines and\ncontents, directly calculating semantic similarity is often difficult to\nsummarize the relationship between them. To address this problem, we propose a\nprompt-tuning method for clickbait detection via text summarization in this\npaper, text summarization is introduced to summarize the contents, and\nclickbait detection is performed based on the similarity between the generated\nsummary and the contents. Specifically, we first introduce a two-stage text\nsummarization model to produce high-quality news summaries based on pre-trained\nlanguage models, and then both the headlines and new generated summaries are\nincorporated as the inputs for prompt-tuning. Additionally, a variety of\nstrategies are conducted to incorporate external knowledge for improving the\nperformance of clickbait detection. The extensive experiments on well-known\nclickbait detection datasets demonstrate that our method achieved\nstate-of-the-art performance.", "published": "2024-04-17 09:39:02", "link": "http://arxiv.org/abs/2404.11206v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sampling-based Pseudo-Likelihood for Membership Inference Attacks", "abstract": "Large Language Models (LLMs) are trained on large-scale web data, which makes\nit difficult to grasp the contribution of each text. This poses the risk of\nleaking inappropriate data such as benchmarks, personal information, and\ncopyrighted texts in the training data. Membership Inference Attacks (MIA),\nwhich determine whether a given text is included in the model's training data,\nhave been attracting attention. Previous studies of MIAs revealed that\nlikelihood-based classification is effective for detecting leaks in LLMs.\nHowever, the existing methods cannot be applied to some proprietary models like\nChatGPT or Claude 3 because the likelihood is unavailable to the user. In this\nstudy, we propose a Sampling-based Pseudo-Likelihood (\\textbf{SPL}) method for\nMIA (\\textbf{SaMIA}) that calculates SPL using only the text generated by an\nLLM to detect leaks. The SaMIA treats the target text as the reference text and\nmultiple outputs from the LLM as text samples, calculates the degree of\n$n$-gram match as SPL, and determines the membership of the text in the\ntraining data. Even without likelihoods, SaMIA performed on par with existing\nlikelihood-based methods.", "published": "2024-04-17 11:12:59", "link": "http://arxiv.org/abs/2404.11262v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Preference-driven Paradigm for Enhanced Translation with Large\n  Language Models", "abstract": "Recent research has shown that large language models (LLMs) can achieve\nremarkable translation performance through supervised fine-tuning (SFT) using\nonly a small amount of parallel data. However, SFT simply instructs the model\nto imitate the reference translations at the token level, making it vulnerable\nto the noise present in the references. Hence, the assistance from SFT often\nreaches a plateau once the LLMs have achieved a certain level of translation\ncapability, and further increasing the size of parallel data does not provide\nadditional benefits. To overcome this plateau associated with imitation-based\nSFT, we propose a preference-based approach built upon the Plackett-Luce model.\nThe objective is to steer LLMs towards a more nuanced understanding of\ntranslation preferences from a holistic view, while also being more resilient\nin the absence of gold translations. We further build a dataset named MAPLE to\nverify the effectiveness of our approach, which includes multiple translations\nof varying quality for each source sentence. Extensive experiments demonstrate\nthe superiority of our approach in \"breaking the plateau\" across diverse LLMs\nand test settings. Our in-depth analysis underscores the pivotal role of\ndiverse translations and accurate preference scores in the success of our\napproach.", "published": "2024-04-17 11:52:47", "link": "http://arxiv.org/abs/2404.11288v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "To Drop or Not to Drop? Predicting Argument Ellipsis Judgments: A Case\n  Study in Japanese", "abstract": "Speakers sometimes omit certain arguments of a predicate in a sentence; such\nomission is especially frequent in pro-drop languages. This study addresses a\nquestion about ellipsis -- what can explain the native speakers' ellipsis\ndecisions? -- motivated by the interest in human discourse processing and\nwriting assistance for this choice. To this end, we first collect large-scale\nhuman annotations of whether and why a particular argument should be omitted\nacross over 2,000 data points in the balanced corpus of Japanese, a\nprototypical pro-drop language. The data indicate that native speakers overall\nshare common criteria for such judgments and further clarify their quantitative\ncharacteristics, e.g., the distribution of related linguistic factors in the\nbalanced corpus. Furthermore, the performance of the language model-based\nargument ellipsis judgment model is examined, and the gap between the systems'\nprediction and human judgments in specific linguistic aspects is revealed. We\nhope our fundamental resource encourages further studies on natural human\nellipsis judgment.", "published": "2024-04-17 12:26:52", "link": "http://arxiv.org/abs/2404.11315v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TeClass: A Human-Annotated Relevance-based Headline Classification and\n  Generation Dataset for Telugu", "abstract": "News headline generation is a crucial task in increasing productivity for\nboth the readers and producers of news. This task can easily be aided by\nautomated News headline-generation models. However, the presence of irrelevant\nheadlines in scraped news articles results in sub-optimal performance of\ngeneration models. We propose that relevance-based headline classification can\ngreatly aid the task of generating relevant headlines. Relevance-based headline\nclassification involves categorizing news headlines based on their relevance to\nthe corresponding news articles. While this task is well-established in\nEnglish, it remains under-explored in low-resource languages like Telugu due to\na lack of annotated data. To address this gap, we present TeClass, the\nfirst-ever human-annotated Telugu news headline classification dataset,\ncontaining 78,534 annotations across 26,178 article-headline pairs. We\nexperiment with various baseline models and provide a comprehensive analysis of\ntheir results. We further demonstrate the impact of this work by fine-tuning\nvarious headline generation models using TeClass dataset. The headlines\ngenerated by the models fine-tuned on highly relevant article-headline pairs,\nshowed about a 5 point increment in the ROUGE-L scores. To encourage future\nresearch, the annotated dataset as well as the annotation guidelines will be\nmade publicly available.", "published": "2024-04-17 13:07:56", "link": "http://arxiv.org/abs/2404.11349v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pack of LLMs: Model Fusion at Test-Time via Perplexity Optimization", "abstract": "Fusing knowledge from multiple Large Language Models (LLMs) can combine their\ndiverse strengths to achieve improved performance on a given task. However,\ncurrent fusion approaches either rely on learning-based fusers that do not\ngeneralize to new LLMs, or do not take into account how well each LLM\nunderstands the input. In this work, we study LLM fusion at test-time, which\nenables leveraging knowledge from arbitrary user-specified LLMs during\ninference. We introduce Pack of LLMs (PackLLM), an effective method for\ntest-time fusion that leverages each LLM's expertise, given an input prompt.\nPackLLM performs model fusion by solving an optimization problem for\ndetermining each LLM's importance, so that perplexity over the input prompt is\nminimized. First, our simple PackLLM-sim variant validates that perplexity is a\ngood indicator for measuring each LLM's expertise. Second, our PackLLM-opt\nvariant approximately solves the perplexity minimization problem via a greedy\nalgorithm. The derived importance weights are used to combine the LLMs during\ninference. We conduct experiments with over 100 total LLMs on a diverse set of\ntasks. Experimental results show that (i) perplexity is a reliable measure for\nLLM fusion, (ii) PackLLM outperforms test-time fusion baselines by 1.89%\naccuracy points, and (iii) PackLLM can leverage new LLMs to improve performance\nover learning-based fusion approaches by 3.92-11.94% accuracy points.", "published": "2024-04-17 16:24:07", "link": "http://arxiv.org/abs/2404.11531v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Select and Reorder: A Novel Approach for Neural Sign Language Production", "abstract": "Sign languages, often categorised as low-resource languages, face significant\nchallenges in achieving accurate translation due to the scarcity of parallel\nannotated datasets. This paper introduces Select and Reorder (S&R), a novel\napproach that addresses data scarcity by breaking down the translation process\ninto two distinct steps: Gloss Selection (GS) and Gloss Reordering (GR). Our\nmethod leverages large spoken language models and the substantial lexical\noverlap between source spoken languages and target sign languages to establish\nan initial alignment. Both steps make use of Non-AutoRegressive (NAR) decoding\nfor reduced computation and faster inference speeds. Through this\ndisentanglement of tasks, we achieve state-of-the-art BLEU and Rouge scores on\nthe Meine DGS Annotated (mDGS) dataset, demonstrating a substantial BLUE-1\nimprovement of 37.88% in Text to Gloss (T2G) Translation. This innovative\napproach paves the way for more effective translation models for sign\nlanguages, even in resource-constrained settings.", "published": "2024-04-17 16:25:19", "link": "http://arxiv.org/abs/2404.11532v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Span Extraction in Generative Paradigm: A Reflection on\n  Aspect-Based Sentiment Analysis", "abstract": "In the era of rapid evolution of generative language models within the realm\nof natural language processing, there is an imperative call to revisit and\nreformulate evaluation methodologies, especially in the domain of aspect-based\nsentiment analysis (ABSA). This paper addresses the emerging challenges\nintroduced by the generative paradigm, which has moderately blurred traditional\nboundaries between understanding and generation tasks. Building upon prevailing\npractices in the field, we analyze the advantages and shortcomings associated\nwith the prevalent ABSA evaluation paradigms. Through an in-depth examination,\nsupplemented by illustrative examples, we highlight the intricacies involved in\naligning generative outputs with other evaluative metrics, specifically those\nderived from other tasks, including question answering. While we steer clear of\nadvocating for a singular and definitive metric, our contribution lies in\npaving the path for a comprehensive guideline tailored for ABSA evaluations in\nthis generative paradigm. In this position paper, we aim to provide\npractitioners with profound reflections, offering insights and directions that\ncan aid in navigating this evolving landscape, ensuring evaluations that are\nboth accurate and reflective of generative capabilities.", "published": "2024-04-17 16:33:22", "link": "http://arxiv.org/abs/2404.11539v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Related Work and Citation Text Generation: A Survey", "abstract": "To convince readers of the novelty of their research paper, authors must\nperform a literature review and compose a coherent story that connects and\nrelates prior works to the current work. This challenging nature of literature\nreview writing makes automatic related work generation (RWG) academically and\ncomputationally interesting, and also makes it an excellent test bed for\nexamining the capability of SOTA natural language processing (NLP) models.\nSince the initial proposal of the RWG task, its popularity has waxed and waned,\nfollowing the capabilities of mainstream NLP approaches. In this work, we\nsurvey the zoo of RWG historical works, summarizing the key approaches and task\ndefinitions and discussing the ongoing challenges of RWG.", "published": "2024-04-17 17:37:30", "link": "http://arxiv.org/abs/2404.11588v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MemLLM: Finetuning LLMs to Use An Explicit Read-Write Memory", "abstract": "While current large language models (LLMs) perform well on many\nknowledge-related tasks, they are limited by relying on their parameters as an\nimplicit storage mechanism. As a result, they struggle with memorizing rare\nevents and with updating their memory as facts change over time. In addition,\nthe uninterpretable nature of parametric memory makes it challenging to prevent\nhallucination. Model editing and augmenting LLMs with parameters specialized\nfor memory are only partial solutions. In this paper, we introduce MemLLM, a\nnovel method of enhancing LLMs by integrating a structured and explicit\nread-and-write memory module. MemLLM tackles the aforementioned challenges by\nenabling dynamic interaction with the memory and improving the LLM's\ncapabilities in using stored knowledge. Our experiments indicate that MemLLM\nenhances the LLM's performance and interpretability, in language modeling in\ngeneral and knowledge-intensive tasks in particular. We see MemLLM as an\nimportant step towards making LLMs more grounded and factual through memory\naugmentation.", "published": "2024-04-17 18:13:16", "link": "http://arxiv.org/abs/2404.11672v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Well Can You Articulate that Idea? Insights from Automated Formative\n  Assessment", "abstract": "Automated methods are becoming increasingly integrated into studies of\nformative feedback on students' science explanation writing. Most of this work,\nhowever, addresses students' responses to short answer questions. We\ninvestigate automated feedback on students' science explanation essays, where\nstudents must articulate multiple ideas. Feedback is based on a rubric that\nidentifies the main ideas students are prompted to include in explanatory\nessays about the physics of energy and mass, given their experiments with a\nsimulated roller coaster. We have found that students generally improve on\nrevised versions of their essays. Here, however, we focus on two factors that\naffect the accuracy of the automated feedback. First, we find that the main\nideas in the rubric differ with respect to how much freedom they afford in\nexplanations of the idea, thus explanation of a natural law is relatively\nconstrained. Students have more freedom in how they explain complex relations\nthey observe in their roller coasters, such as transfer of different forms of\nenergy. Second, by tracing the automated decision process, we can diagnose when\na student's statement lacks sufficient clarity for the automated tool to\nassociate it more strongly with one of the main ideas above all others. This in\nturn provides an opportunity for teachers and peers to help students reflect on\nhow to state their ideas more clearly.", "published": "2024-04-17 18:27:59", "link": "http://arxiv.org/abs/2404.11682v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improvement in Semantic Address Matching using Natural Language\n  Processing", "abstract": "Address matching is an important task for many businesses especially delivery\nand take out companies which help them to take out a certain address from their\ndata warehouse. Existing solution uses similarity of strings, and edit distance\nalgorithms to find out the similar addresses from the address database, but\nthese algorithms could not work effectively with redundant, unstructured, or\nincomplete address data. This paper discuss semantic Address matching\ntechnique, by which we can find out a particular address from a list of\npossible addresses. We have also reviewed existing practices and their\nshortcoming. Semantic address matching is an essentially NLP task in the field\nof deep learning. Through this technique We have the ability to triumph the\ndrawbacks of existing methods like redundant or abbreviated data problems. The\nsolution uses the OCR on invoices to extract the address and create the data\npool of addresses. Then this data is fed to the algorithm BM-25 for scoring the\nbest matching entries. Then to observe the best result, this will pass through\nBERT for giving the best possible result from the similar queries. Our\ninvestigation exhibits that our methodology enormously improves both accuracy\nand review of cutting-edge technology existing techniques.", "published": "2024-04-17 18:42:36", "link": "http://arxiv.org/abs/2404.11691v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How often are errors in natural language reasoning due to paraphrastic\n  variability?", "abstract": "Large language models have been shown to behave inconsistently in response to\nmeaning-preserving paraphrastic inputs. At the same time, researchers evaluate\nthe knowledge and reasoning abilities of these models with test evaluations\nthat do not disaggregate the effect of paraphrastic variability on performance.\nWe propose a metric for evaluating the paraphrastic consistency of natural\nlanguage reasoning models based on the probability of a model achieving the\nsame correctness on two paraphrases of the same problem. We mathematically\nconnect this metric to the proportion of a model's variance in correctness\nattributable to paraphrasing. To estimate paraphrastic consistency, we collect\nParaNLU, a dataset of 7,782 human-written and validated paraphrased reasoning\nproblems constructed on top of existing benchmark datasets for defeasible and\nabductive natural language inference. Using ParaNLU, we measure the\nparaphrastic consistency of several model classes and show that consistency\ndramatically increases with pretraining but not finetuning. All models tested\nexhibited room for improvement in paraphrastic consistency.", "published": "2024-04-17 20:11:32", "link": "http://arxiv.org/abs/2404.11717v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating Gender Bias in Turkish Language Models", "abstract": "Language models are trained mostly on Web data, which often contains social\nstereotypes and biases that the models can inherit. This has potentially\nnegative consequences, as models can amplify these biases in downstream tasks\nor applications. However, prior research has primarily focused on the English\nlanguage, especially in the context of gender bias. In particular,\ngrammatically gender-neutral languages such as Turkish are underexplored\ndespite representing different linguistic properties to language models with\npossibly different effects on biases. In this paper, we fill this research gap\nand investigate the significance of gender bias in Turkish language models. We\nbuild upon existing bias evaluation frameworks and extend them to the Turkish\nlanguage by translating existing English tests and creating new ones designed\nto measure gender bias in the context of T\\\"urkiye. Specifically, we also\nevaluate Turkish language models for their embedded ethnic bias toward Kurdish\npeople. Based on the experimental results, we attribute possible biases to\ndifferent model characteristics such as the model size, their multilingualism,\nand the training corpora. We make the Turkish gender bias dataset publicly\navailable.", "published": "2024-04-17 20:24:41", "link": "http://arxiv.org/abs/2404.11726v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Models Still Struggle to Zero-shot Reason about Time Series", "abstract": "Time series are critical for decision-making in fields like finance and\nhealthcare. Their importance has driven a recent influx of works passing time\nseries into language models, leading to non-trivial forecasting on some\ndatasets. But it remains unknown whether non-trivial forecasting implies that\nlanguage models can reason about time series. To address this gap, we generate\na first-of-its-kind evaluation framework for time series reasoning, including\nformal tasks and a corresponding dataset of multi-scale time series paired with\ntext captions across ten domains. Using these data, we probe whether language\nmodels achieve three forms of reasoning: (1) Etiological Reasoning - given an\ninput time series, can the language model identify the scenario that most\nlikely created it? (2) Question Answering - can a language model answer factual\nquestions about time series? (3) Context-Aided Forecasting - does highly\nrelevant textual context improve a language model's time series forecasts?\n  We find that otherwise highly-capable language models demonstrate\nsurprisingly limited time series reasoning: they score marginally above random\non etiological and question answering tasks (up to 30 percentage points worse\nthan humans) and show modest success in using context to improve forecasting.\nThese weakness showcase that time series reasoning is an impactful, yet deeply\nunderdeveloped direction for language model research. We also make our datasets\nand code public at to support further research in this direction at\nhttps://github.com/behavioral-data/TSandLanguage", "published": "2024-04-17 21:27:33", "link": "http://arxiv.org/abs/2404.11757v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Argument Summarization: Prioritizing Exhaustiveness in Key\n  Point Generation and Introducing an Automatic Coverage Evaluation Metric", "abstract": "The proliferation of social media platforms has given rise to the amount of\nonline debates and arguments. Consequently, the need for automatic\nsummarization methods for such debates is imperative, however this area of\nsummarization is rather understudied. The Key Point Analysis (KPA) task\nformulates argument summarization as representing the summary of a large\ncollection of arguments in the form of concise sentences in bullet-style\nformat, called key points. A sub-task of KPA, called Key Point Generation\n(KPG), focuses on generating these key points given the arguments. This paper\nintroduces a novel extractive approach for key point generation, that\noutperforms previous state-of-the-art methods for the task. Our method utilizes\nan extractive clustering based approach that offers concise, high quality\ngenerated key points with higher coverage of reference summaries, and less\nredundant outputs. In addition, we show that the existing evaluation metrics\nfor summarization such as ROUGE are incapable of differentiating between\ngenerated key points of different qualities. To this end, we propose a new\nevaluation metric for assessing the generated key points by their coverage. Our\ncode can be accessed online.", "published": "2024-04-17 23:00:29", "link": "http://arxiv.org/abs/2404.11793v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-Platform Hate Speech Detection with Weakly Supervised Causal\n  Disentanglement", "abstract": "Content moderation faces a challenging task as social media's ability to\nspread hate speech contrasts with its role in promoting global connectivity.\nWith rapidly evolving slang and hate speech, the adaptability of conventional\ndeep learning to the fluid landscape of online dialogue remains limited. In\nresponse, causality inspired disentanglement has shown promise by segregating\nplatform specific peculiarities from universal hate indicators. However, its\ndependency on available ground truth target labels for discerning these nuances\nfaces practical hurdles with the incessant evolution of platforms and the\nmutable nature of hate speech. Using confidence based reweighting and\ncontrastive regularization, this study presents HATE WATCH, a novel framework\nof weakly supervised causal disentanglement that circumvents the need for\nexplicit target labeling and effectively disentangles input features into\ninvariant representations of hate. Empirical validation across platforms two\nwith target labels and two without positions HATE WATCH as a novel method in\ncross platform hate speech detection with superior performance. HATE WATCH\nadvances scalable content moderation techniques towards developing safer online\ncommunities.", "published": "2024-04-17 03:25:54", "link": "http://arxiv.org/abs/2404.11036v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "ViLLM-Eval: A Comprehensive Evaluation Suite for Vietnamese Large\n  Language Models", "abstract": "The rapid advancement of large language models (LLMs) necessitates the\ndevelopment of new benchmarks to accurately assess their capabilities. To\naddress this need for Vietnamese, this work aims to introduce ViLLM-Eval, the\ncomprehensive evaluation suite designed to measure the advanced knowledge and\nreasoning abilities of foundation models within a Vietnamese context.\nViLLM-Eval consists of multiple-choice questions and predict next word tasks\nspanning various difficulty levels and diverse disciplines, ranging from\nhumanities to science and engineering. A thorough evaluation of the most\nadvanced LLMs on ViLLM-Eval revealed that even the best performing models have\nsignificant room for improvement in understanding and responding to Vietnamese\nlanguage tasks. ViLLM-Eval is believed to be instrumental in identifying key\nstrengths and weaknesses of foundation models, ultimately promoting their\ndevelopment and enhancing their performance for Vietnamese users. This paper\nprovides a thorough overview of ViLLM-Eval as part of the Vietnamese Large\nLanguage Model shared task, held within the 10th International Workshop on\nVietnamese Language and Speech Processing (VLSP 2023).", "published": "2024-04-17 05:57:17", "link": "http://arxiv.org/abs/2404.11086v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Inductive-Deductive Strategy Reuse for Multi-Turn Instructional\n  Dialogues", "abstract": "Aligning large language models (LLMs) with human expectations requires\nhigh-quality instructional dialogues, which usually require instructions that\nare diverse and in-depth. Existing methods leverage two LLMs to interact for\nautomatic collection: one simulating a user to pose instructions, and the other\nacting as a system agent to respond. However, these user simulators struggle to\nmodel the rules behind how dialogues can pose different instructions without\nexplicit guidance, resulting in general instructions. In this paper, we propose\nto explicitly capture the complex rules to help the user simulator pose diverse\nand in-depth instruction. Specifically, we first induce high-level instruction\nstrategies from various real instruction dialogues serving as rules. Afterward,\ndifferent possible strategies are applied to the newly given dialogue scenario\ndeductively to pose various instructions. Experimental results show that our\nmethod can generate diverse and in-depth instructions. The constructed\nmulti-turn instructional dialogues can outperform competitive baselines on the\ndownstream chat model.", "published": "2024-04-17 06:26:32", "link": "http://arxiv.org/abs/2404.11095v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "What's under the hood: Investigating Automatic Metrics on Meeting\n  Summarization", "abstract": "Meeting summarization has become a critical task considering the increase in\nonline interactions. While new techniques are introduced regularly, their\nevaluation uses metrics not designed to capture meeting-specific errors,\nundermining effective evaluation. This paper investigates what the frequently\nused automatic metrics capture and which errors they mask by correlating\nautomatic metric scores with human evaluations across a broad error taxonomy.\nWe commence with a comprehensive literature review on English meeting\nsummarization to define key challenges like speaker dynamics and contextual\nturn-taking and error types such as missing information and linguistic\ninaccuracy, concepts previously loosely defined in the field. We examine the\nrelationship between characteristic challenges and errors by using annotated\ntranscripts and summaries from Transformer-based sequence-to-sequence and\nautoregressive models from the general summary QMSum dataset. Through\nexperimental validation, we find that different model architectures respond\nvariably to challenges in meeting transcripts, resulting in different\npronounced links between challenges and errors. Current default-used metrics\nstruggle to capture observable errors, showing weak to mid-correlations, while\na third of the correlations show trends of error masking. Only a subset reacts\naccurately to specific errors, while most correlations show either\nunresponsiveness or failure to reflect the error's impact on summary quality.", "published": "2024-04-17 07:15:07", "link": "http://arxiv.org/abs/2404.11124v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Pose2Gest: A Few-Shot Model-Free Approach Applied In South Indian\n  Classical Dance Gesture Recognition", "abstract": "The classical dances from India utilize a set of hand gestures known as\nMudras, serving as the foundational elements of its posture vocabulary.\nIdentifying these mudras represents a primary task in digitizing the dance\nperformances. With Kathakali, a dance-drama, as the focus, this work addresses\nmudra recognition by framing it as a 24-class classification problem and\nproposes a novel vector-similarity-based approach leveraging pose estimation\ntechniques. This method obviates the need for extensive training or\nfine-tuning, thus mitigating the issue of limited data availability common in\nsimilar AI applications. Achieving an accuracy rate of 92%, our approach\ndemonstrates comparable or superior performance to existing\nmodel-training-based methodologies in this domain. Notably, it remains\neffective even with small datasets comprising just 1 or 5 samples, albeit with\na slightly diminished performance. Furthermore, our system supports processing\nimages, videos, and real-time streams, accommodating both hand-cropped and\nfull-body images. As part of this research, we have curated and released a\npublicly accessible Hasta Mudra dataset, which applies to multiple South Indian\nart forms including Kathakali. The implementation of the proposed method is\nalso made available as a web application.", "published": "2024-04-17 09:37:25", "link": "http://arxiv.org/abs/2404.11205v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "In-Context Learning State Vector with Inner and Momentum Optimization", "abstract": "Large Language Models (LLMs) have exhibited an impressive ability to perform\nIn-Context Learning (ICL) from only a few examples. Recent works have indicated\nthat the functions learned by ICL can be represented through compressed vectors\nderived from the transformer. However, the working mechanisms and optimization\nof these vectors are yet to be thoroughly explored. In this paper, we address\nthis gap by presenting a comprehensive analysis of these compressed vectors,\ndrawing parallels to the parameters trained with gradient descent, and\nintroduce the concept of state vector. Inspired by the works on model soup and\nmomentum-based gradient descent, we propose inner and momentum optimization\nmethods that are applied to refine the state vector progressively as test-time\nadaptation. Moreover, we simulate state vector aggregation in the multiple\nexample setting, where demonstrations comprising numerous examples are usually\ntoo lengthy for regular ICL, and further propose a divide-and-conquer\naggregation method to address this challenge. We conduct extensive experiments\nusing Llama-2 and GPT-J in both zero-shot setting and few-shot setting. The\nexperimental results show that our optimization method effectively enhances the\nstate vector and achieves the state-of-the-art performance on diverse tasks.\nCode is available at https://github.com/HITsz-TMG/ICL-State-Vector", "published": "2024-04-17 10:19:15", "link": "http://arxiv.org/abs/2404.11225v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring Key Point Analysis with Pairwise Generation and Graph\n  Partitioning", "abstract": "Key Point Analysis (KPA), the summarization of multiple arguments into a\nconcise collection of key points, continues to be a significant and unresolved\nissue within the field of argument mining. Existing models adapt a two-stage\npipeline of clustering arguments or generating key points for argument\nclusters. This approach rely on semantic similarity instead of measuring the\nexistence of shared key points among arguments. Additionally, it only models\nthe intra-cluster relationship among arguments, disregarding the inter-cluster\nrelationship between arguments that do not share key points. To address these\nlimitations, we propose a novel approach for KPA with pairwise generation and\ngraph partitioning. Our objective is to train a generative model that can\nsimultaneously provide a score indicating the presence of shared key point\nbetween a pair of arguments and generate the shared key point. Subsequently, to\nmap generated redundant key points to a concise set of key points, we proceed\nto construct an arguments graph by considering the arguments as vertices, the\ngenerated key points as edges, and the scores as edge weights. We then propose\na graph partitioning algorithm to partition all arguments sharing the same key\npoints to the same subgraph. Notably, our experimental findings demonstrate\nthat our proposed model surpasses previous models when evaluated on both the\nArgKP and QAM datasets.", "published": "2024-04-17 13:44:29", "link": "http://arxiv.org/abs/2404.11384v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Research on emotionally intelligent dialogue generation based on\n  automatic dialogue system", "abstract": "Automated dialogue systems are important applications of artificial\nintelligence, and traditional systems struggle to understand user emotions and\nprovide empathetic feedback. This study integrates emotional intelligence\ntechnology into automated dialogue systems and creates a dialogue generation\nmodel with emotional intelligence through deep learning and natural language\nprocessing techniques. The model can detect and understand a wide range of\nemotions and specific pain signals in real time, enabling the system to provide\nempathetic interaction. By integrating the results of the study \"Can artificial\nintelligence detect pain and express pain empathy?\", the model's ability to\nunderstand the subtle elements of pain empathy has been enhanced, setting\nhigher standards for emotional intelligence dialogue systems. The project aims\nto provide theoretical understanding and practical suggestions to integrate\nadvanced emotional intelligence capabilities into dialogue systems, thereby\nimproving user experience and interaction quality.", "published": "2024-04-17 14:55:03", "link": "http://arxiv.org/abs/2404.11447v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "AI-Enhanced Cognitive Behavioral Therapy: Deep Learning and Large\n  Language Models for Extracting Cognitive Pathways from Social Media Texts", "abstract": "Cognitive Behavioral Therapy (CBT) is an effective technique for addressing\nthe irrational thoughts stemming from mental illnesses, but it necessitates\nprecise identification of cognitive pathways to be successfully implemented in\npatient care. In current society, individuals frequently express negative\nemotions on social media on specific topics, often exhibiting cognitive\ndistortions, including suicidal behaviors in extreme cases. Yet, there is a\nnotable absence of methodologies for analyzing cognitive pathways that could\naid psychotherapists in conducting effective interventions online. In this\nstudy, we gathered data from social media and established the task of\nextracting cognitive pathways, annotating the data based on a cognitive\ntheoretical framework. We initially categorized the task of extracting\ncognitive pathways as a hierarchical text classification with four main\ncategories and nineteen subcategories. Following this, we structured a text\nsummarization task to help psychotherapists quickly grasp the essential\ninformation. Our experiments evaluate the performance of deep learning and\nlarge language models (LLMs) on these tasks. The results demonstrate that our\ndeep learning method achieved a micro-F1 score of 62.34% in the hierarchical\ntext classification task. Meanwhile, in the text summarization task, GPT-4\nattained a Rouge-1 score of 54.92 and a Rouge-2 score of 30.86, surpassing the\nexperimental deep learning model's performance. However, it may suffer from an\nissue of hallucination. We have made all models and codes publicly available to\nsupport further research in this field.", "published": "2024-04-17 14:55:27", "link": "http://arxiv.org/abs/2404.11449v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Octopus v3: Technical Report for On-device Sub-billion Multimodal AI\n  Agent", "abstract": "A multimodal AI agent is characterized by its ability to process and learn\nfrom various types of data, including natural language, visual, and audio\ninputs, to inform its actions. Despite advancements in large language models\nthat incorporate visual data, such as GPT-4V, effectively translating\nimage-based data into actionable outcomes for AI agents continues to be\nchallenging. In this paper, we introduce a multimodal model that incorporates\nthe concept of functional token specifically designed for AI agent\napplications. To ensure compatibility with edge devices, our model is optimized\nto a compact size of less than 1B parameters. Like GPT-4, our model can process\nboth English and Chinese. We demonstrate that this model is capable of\noperating efficiently on a wide range of edge devices, including as constrained\nas a Raspberry Pi.", "published": "2024-04-17 15:07:06", "link": "http://arxiv.org/abs/2404.11459v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "A Federated Learning Approach to Privacy Preserving Offensive Language\n  Identification", "abstract": "The spread of various forms of offensive speech online is an important\nconcern in social media. While platforms have been investing heavily in ways of\ncoping with this problem, the question of privacy remains largely unaddressed.\nModels trained to detect offensive language on social media are trained and/or\nfine-tuned using large amounts of data often stored in centralized servers.\nSince most social media data originates from end users, we propose a privacy\npreserving decentralized architecture for identifying offensive language online\nby introducing Federated Learning (FL) in the context of offensive language\nidentification. FL is a decentralized architecture that allows multiple models\nto be trained locally without the need for data sharing hence preserving users'\nprivacy. We propose a model fusion approach to perform FL. We trained multiple\ndeep learning models on four publicly available English benchmark datasets\n(AHSD, HASOC, HateXplain, OLID) and evaluated their performance in detail. We\nalso present initial cross-lingual experiments in English and Spanish. We show\nthat the proposed model fusion approach outperforms baselines in all the\ndatasets while preserving privacy.", "published": "2024-04-17 15:23:12", "link": "http://arxiv.org/abs/2404.11470v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Data-Driven Representation for Sign Language Production", "abstract": "Phonetic representations are used when recording spoken languages, but no\nequivalent exists for recording signed languages. As a result, linguists have\nproposed several annotation systems that operate on the gloss or sub-unit\nlevel; however, these resources are notably irregular and scarce.\n  Sign Language Production (SLP) aims to automatically translate spoken\nlanguage sentences into continuous sequences of sign language. However, current\nstate-of-the-art approaches rely on scarce linguistic resources to work. This\nhas limited progress in the field. This paper introduces an innovative solution\nby transforming the continuous pose generation problem into a discrete sequence\ngeneration problem. Thus, overcoming the need for costly annotation. Although,\nif available, we leverage the additional information to enhance our approach.\n  By applying Vector Quantisation (VQ) to sign language data, we first learn a\ncodebook of short motions that can be combined to create a natural sequence of\nsign. Where each token in the codebook can be thought of as the lexicon of our\nrepresentation. Then using a transformer we perform a translation from spoken\nlanguage text to a sequence of codebook tokens. Each token can be directly\nmapped to a sequence of poses allowing the translation to be performed by a\nsingle network. Furthermore, we present a sign stitching method to effectively\njoin tokens together. We evaluate on the RWTH-PHOENIX-Weather-2014T\n(PHOENIX14T) and the more challenging Meine DGS Annotated (mDGS) datasets. An\nextensive evaluation shows our approach outperforms previous methods,\nincreasing the BLEU-1 back translation score by up to 72%.", "published": "2024-04-17 15:52:38", "link": "http://arxiv.org/abs/2404.11499v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Paraphrase and Solve: Exploring and Exploiting the Impact of Surface\n  Form on Mathematical Reasoning in Large Language Models", "abstract": "This paper studies the relationship between the surface form of a\nmathematical problem and its solvability by large language models. We find that\nsubtle alterations in the surface form can significantly impact the answer\ndistribution and the solve rate, exposing the language model's lack of\nrobustness and sensitivity to the surface form in reasoning through complex\nproblems. To improve mathematical reasoning performance, we propose\nSelf-Consistency-over-Paraphrases (SCoP), which diversifies reasoning paths\nfrom specific surface forms of the problem. We evaluate our approach on four\nmathematics reasoning benchmarks over three large language models and show that\nSCoP improves mathematical reasoning performance over vanilla self-consistency,\nparticularly for problems initially deemed unsolvable. Finally, we provide\nadditional experiments and discussion regarding problem difficulty and surface\nforms, including cross-model difficulty agreement and paraphrasing\ntransferability, and Variance of Variations (VOV) for language model\nevaluation.", "published": "2024-04-17 15:53:49", "link": "http://arxiv.org/abs/2404.11500v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Coarse-to-Fine Evaluation of Inference Efficiency for Large\n  Language Models", "abstract": "In real world, large language models (LLMs) can serve as the assistant to\nhelp users accomplish their jobs, and also support the development of advanced\napplications. For the wide application of LLMs, the inference efficiency is an\nessential concern, which has been widely studied in existing work, and numerous\noptimization algorithms and code libraries have been proposed to improve it.\nNonetheless, users still find it challenging to compare the effectiveness of\nall the above methods and understand the underlying mechanisms. In this work,\nwe perform a detailed coarse-to-fine analysis of the inference performance of\nvarious code libraries. To evaluate the overall effectiveness, we examine four\nusage scenarios within two practical applications. We further provide both\ntheoretical and empirical fine-grained analyses of each module in the\nTransformer architecture. Our experiments yield comprehensive results that are\ninvaluable for researchers to evaluate code libraries and improve inference\nstrategies.", "published": "2024-04-17 15:57:50", "link": "http://arxiv.org/abs/2404.11502v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GenFighter: A Generative and Evolutive Textual Attack Removal", "abstract": "Adversarial attacks pose significant challenges to deep neural networks\n(DNNs) such as Transformer models in natural language processing (NLP). This\npaper introduces a novel defense strategy, called GenFighter, which enhances\nadversarial robustness by learning and reasoning on the training classification\ndistribution. GenFighter identifies potentially malicious instances deviating\nfrom the distribution, transforms them into semantically equivalent instances\naligned with the training data, and employs ensemble techniques for a unified\nand robust response. By conducting extensive experiments, we show that\nGenFighter outperforms state-of-the-art defenses in accuracy under attack and\nattack success rate metrics. Additionally, it requires a high number of queries\nper attack, making the attack more challenging in real scenarios. The ablation\nstudy shows that our approach integrates transfer learning, a\ngenerative/evolutive procedure, and an ensemble method, providing an effective\ndefense against NLP adversarial attacks.", "published": "2024-04-17 16:32:13", "link": "http://arxiv.org/abs/2404.11538v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "The Landscape of Emerging AI Agent Architectures for Reasoning,\n  Planning, and Tool Calling: A Survey", "abstract": "This survey paper examines the recent advancements in AI agent\nimplementations, with a focus on their ability to achieve complex goals that\nrequire enhanced reasoning, planning, and tool execution capabilities. The\nprimary objectives of this work are to a) communicate the current capabilities\nand limitations of existing AI agent implementations, b) share insights gained\nfrom our observations of these systems in action, and c) suggest important\nconsiderations for future developments in AI agent design. We achieve this by\nproviding overviews of single-agent and multi-agent architectures, identifying\nkey patterns and divergences in design choices, and evaluating their overall\nimpact on accomplishing a provided goal. Our contribution outlines key themes\nwhen selecting an agentic architecture, the impact of leadership on agent\nsystems, agent communication styles, and key phases for planning, execution,\nand reflection that enable robust AI agent systems.", "published": "2024-04-17 17:32:41", "link": "http://arxiv.org/abs/2404.11584v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Missed Connections: Lateral Thinking Puzzles for Large Language Models", "abstract": "The Connections puzzle published each day by the New York Times tasks players\nwith dividing a bank of sixteen words into four groups of four words that each\nrelate to a common theme. Solving the puzzle requires both common linguistic\nknowledge (i.e. definitions and typical usage) as well as, in many cases,\nlateral or abstract thinking. This is because the four categories ascend in\ncomplexity, with the most challenging category often requiring thinking about\nwords in uncommon ways or as parts of larger phrases. We investigate the\ncapacity for automated AI systems to play Connections and explore the game's\npotential as an automated benchmark for abstract reasoning and a way to measure\nthe semantic information encoded by data-driven linguistic systems. In\nparticular, we study both a sentence-embedding baseline and modern large\nlanguage models (LLMs). We report their accuracy on the task, measure the\nimpacts of chain-of-thought prompting, and discuss their failure modes.\nOverall, we find that the Connections task is challenging yet feasible, and a\nstrong test-bed for future work.", "published": "2024-04-17 20:31:05", "link": "http://arxiv.org/abs/2404.11730v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mapping Violence: Developing an Extensive Framework to Build a Bangla\n  Sectarian Expression Dataset from Social Media Interactions", "abstract": "Communal violence in online forums has become extremely prevalent in South\nAsia, where many communities of different cultures coexist and share resources.\nThese societies exhibit a phenomenon characterized by strong bonds within their\nown groups and animosity towards others, leading to conflicts that frequently\nescalate into violent confrontations. To address this issue, we have developed\nthe first comprehensive framework for the automatic detection of communal\nviolence markers in online Bangla content accompanying the largest collection\n(13K raw sentences) of social media interactions that fall under the definition\nof four major violence class and their 16 coarse expressions. Our workflow\nintroduces a 7-step expert annotation process incorporating insights from\nsocial scientists, linguists, and psychologists. By presenting data statistics\nand benchmarking performance using this dataset, we have determined that, aside\nfrom the category of Non-communal violence, Religio-communal violence is\nparticularly pervasive in Bangla text. Moreover, we have substantiated the\neffectiveness of fine-tuning language models in identifying violent comments by\nconducting preliminary benchmarking on the state-of-the-art Bangla deep\nlearning model.", "published": "2024-04-17 21:09:13", "link": "http://arxiv.org/abs/2404.11752v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Demystifying Legalese: An Automated Approach for Summarizing and\n  Analyzing Overlaps in Privacy Policies and Terms of Service", "abstract": "The complexities of legalese in terms and policy documents can bind\nindividuals to contracts they do not fully comprehend, potentially leading to\nuninformed data sharing. Our work seeks to alleviate this issue by developing\nlanguage models that provide automated, accessible summaries and scores for\nsuch documents, aiming to enhance user understanding and facilitate informed\ndecisions. We compared transformer-based and conventional models during\ntraining on our dataset, and RoBERTa performed better overall with a remarkable\n0.74 F1-score. Leveraging our best-performing model, RoBERTa, we highlighted\nredundancies and potential guideline violations by identifying overlaps in\nGDPR-required documents, underscoring the necessity for stricter GDPR\ncompliance.", "published": "2024-04-17 19:53:59", "link": "http://arxiv.org/abs/2404.13087v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Survey on Retrieval-Augmented Text Generation for Large Language\n  Models", "abstract": "Retrieval-Augmented Generation (RAG) merges retrieval methods with deep\nlearning advancements to address the static limitations of large language\nmodels (LLMs) by enabling the dynamic integration of up-to-date external\ninformation. This methodology, focusing primarily on the text domain, provides\na cost-effective solution to the generation of plausible but possibly incorrect\nresponses by LLMs, thereby enhancing the accuracy and reliability of their\noutputs through the use of real-world data. As RAG grows in complexity and\nincorporates multiple concepts that can influence its performance, this paper\norganizes the RAG paradigm into four categories: pre-retrieval, retrieval,\npost-retrieval, and generation, offering a detailed perspective from the\nretrieval viewpoint. It outlines RAG's evolution and discusses the field's\nprogression through the analysis of significant studies. Additionally, the\npaper introduces evaluation methods for RAG, addressing the challenges faced\nand proposing future research directions. By offering an organized framework\nand categorization, the study aims to consolidate existing research on RAG,\nclarify its technological underpinnings, and highlight its potential to broaden\nthe adaptability and applications of LLMs.", "published": "2024-04-17 01:27:42", "link": "http://arxiv.org/abs/2404.10981v2", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Many-Shot In-Context Learning", "abstract": "Large language models (LLMs) excel at few-shot in-context learning (ICL) --\nlearning from a few examples provided in context at inference, without any\nweight updates. Newly expanded context windows allow us to investigate ICL with\nhundreds or thousands of examples -- the many-shot regime. Going from few-shot\nto many-shot, we observe significant performance gains across a wide variety of\ngenerative and discriminative tasks. While promising, many-shot ICL can be\nbottlenecked by the available amount of human-generated examples. To mitigate\nthis limitation, we explore two new settings: Reinforced and Unsupervised ICL.\nReinforced ICL uses model-generated chain-of-thought rationales in place of\nhuman examples. Unsupervised ICL removes rationales from the prompt altogether,\nand prompts the model only with domain-specific questions. We find that both\nReinforced and Unsupervised ICL can be quite effective in the many-shot regime,\nparticularly on complex reasoning tasks. Finally, we demonstrate that, unlike\nfew-shot learning, many-shot learning is effective at overriding pretraining\nbiases, can learn high-dimensional functions with numerical inputs, and\nperforms comparably to fine-tuning. We also find that inference cost increases\nlinearly in the many-shot regime, and frontier LLMs benefit from many-shot ICL\nto varying degrees. Our analysis also reveals the limitations of next-token\nprediction loss as an indicator of downstream ICL performance.", "published": "2024-04-17 02:49:26", "link": "http://arxiv.org/abs/2404.11018v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Advancing Social Intelligence in AI Agents: Technical Challenges and\n  Open Questions", "abstract": "Building socially-intelligent AI agents (Social-AI) is a multidisciplinary,\nmultimodal research goal that involves creating agents that can sense,\nperceive, reason about, learn from, and respond to affect, behavior, and\ncognition of other agents (human or artificial). Progress towards Social-AI has\naccelerated in the past decade across several computing communities, including\nnatural language processing, machine learning, robotics, human-machine\ninteraction, computer vision, and speech. Natural language processing, in\nparticular, has been prominent in Social-AI research, as language plays a key\nrole in constructing the social world. In this position paper, we identify a\nset of underlying technical challenges and open questions for researchers\nacross computing communities to advance Social-AI. We anchor our discussion in\nthe context of social intelligence concepts and prior progress in Social-AI\nresearch.", "published": "2024-04-17 02:57:42", "link": "http://arxiv.org/abs/2404.11023v2", "categories": ["cs.HC", "cs.CL", "cs.LG"], "primary_category": "cs.HC"}
{"title": "Stepwise Alignment for Constrained Language Model Policy Optimization", "abstract": "Safety and trustworthiness are indispensable requirements for real-world\napplications of AI systems using large language models (LLMs). This paper\nformulates human value alignment as an optimization problem of the language\nmodel policy to maximize reward under a safety constraint, and then proposes an\nalgorithm, Stepwise Alignment for Constrained Policy Optimization (SACPO). One\nkey idea behind SACPO, supported by theory, is that the optimal policy\nincorporating reward and safety can be directly obtained from a reward-aligned\npolicy. Building on this key idea, SACPO aligns LLMs step-wise with each metric\nwhile leveraging simple yet powerful alignment algorithms such as direct\npreference optimization (DPO). SACPO offers several advantages, including\nsimplicity, stability, computational efficiency, and flexibility of algorithms\nand datasets. Under mild assumptions, our theoretical analysis provides the\nupper bounds on optimality and safety constraint violation. Our experimental\nresults show that SACPO can fine-tune Alpaca-7B better than the\nstate-of-the-art method in terms of both helpfulness and harmlessness.", "published": "2024-04-17 03:44:58", "link": "http://arxiv.org/abs/2404.11049v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Position Engineering: Boosting Large Language Models through Positional\n  Information Manipulation", "abstract": "The performance of large language models (LLMs) is significantly influenced\nby the quality of the prompts provided. In response, researchers have developed\nenormous prompt engineering strategies aimed at modifying the prompt text to\nenhance task performance. In this paper, we introduce a novel technique termed\nposition engineering, which offers a more efficient way to guide large language\nmodels. Unlike prompt engineering, which requires substantial effort to modify\nthe text provided to LLMs, position engineering merely involves altering the\npositional information in the prompt without modifying the text itself. We have\nevaluated position engineering in two widely-used LLM scenarios:\nretrieval-augmented generation (RAG) and in-context learning (ICL). Our\nfindings show that position engineering substantially improves upon the\nbaseline in both cases. Position engineering thus represents a promising new\nstrategy for exploiting the capabilities of large language models.", "published": "2024-04-17 10:00:56", "link": "http://arxiv.org/abs/2404.11216v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Open-Ended Wargames with Large Language Models", "abstract": "Wargames are a powerful tool for understanding and rehearsing real-world\ndecision making. Automated play of wargames using artificial intelligence (AI)\nenables possibilities beyond those of human-conducted games, such as playing\nthe game many times over to see a range of possible outcomes. There are two\ncategories of wargames: quantitative games, with discrete types of moves, and\nqualitative games, which revolve around open-ended responses. Historically,\nautomation efforts have focused on quantitative games, but large language\nmodels (LLMs) make it possible to automate qualitative wargames. We introduce\n\"Snow Globe,\" an LLM-powered multi-agent system for playing qualitative\nwargames. With Snow Globe, every stage of a text-based qualitative wargame from\nscenario preparation to post-game analysis can be optionally carried out by AI,\nhumans, or a combination thereof. We describe its software architecture\nconceptually and release an open-source implementation alongside this\npublication. As case studies, we simulate a tabletop exercise about an AI\nincident response and a political wargame about a geopolitical crisis. We\ndiscuss potential applications of the approach and how it fits into the broader\nwargaming ecosystem.", "published": "2024-04-17 14:54:58", "link": "http://arxiv.org/abs/2404.11446v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Bias and Unfairness in Information Retrieval Systems: New Challenges in\n  the LLM Era", "abstract": "With the rapid advancements of large language models (LLMs), information\nretrieval (IR) systems, such as search engines and recommender systems, have\nundergone a significant paradigm shift. This evolution, while heralding new\nopportunities, introduces emerging challenges, particularly in terms of biases\nand unfairness, which may threaten the information ecosystem. In this paper, we\npresent a comprehensive survey of existing works on emerging and pressing bias\nand unfairness issues in IR systems when the integration of LLMs. We first\nunify bias and unfairness issues as distribution mismatch problems, providing a\ngroundwork for categorizing various mitigation strategies through distribution\nalignment. Subsequently, we systematically delve into the specific bias and\nunfairness issues arising from three critical stages of LLMs integration into\nIR systems: data collection, model development, and result evaluation. In doing\nso, we meticulously review and analyze recent literature, focusing on the\ndefinitions, characteristics, and corresponding mitigation strategies\nassociated with these issues. Finally, we identify and highlight some open\nproblems and challenges for future work, aiming to inspire researchers and\nstakeholders in the IR field and beyond to better understand and mitigate bias\nand unfairness issues of IR in this LLM era. We also consistently maintain a\nGitHub repository for the relevant papers and resources in this rising\ndirection at https://github.com/KID-22/LLM-IR-Bias-Fairness-Survey.", "published": "2024-04-17 15:05:03", "link": "http://arxiv.org/abs/2404.11457v2", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Language Ranker: A Metric for Quantifying LLM Performance Across High\n  and Low-Resource Languages", "abstract": "The development of Large Language Models (LLMs) relies on extensive text\ncorpora, which are often unevenly distributed across languages. This imbalance\nresults in LLMs performing significantly better on high-resource languages like\nEnglish, German, and French, while their capabilities in low-resource languages\nremain inadequate. Currently, there is a lack of quantitative methods to\nevaluate the performance of LLMs in these low-resource languages. To address\nthis gap, we propose the Language Ranker, an intrinsic metric designed to\nbenchmark and rank languages based on LLM performance using internal\nrepresentations. By comparing the LLM's internal representation of various\nlanguages against a baseline derived from English, we can assess the model's\nmultilingual capabilities in a robust and language-agnostic manner. Our\nanalysis reveals that high-resource languages exhibit higher similarity scores\nwith English, demonstrating superior performance, while low-resource languages\nshow lower similarity scores, underscoring the effectiveness of our metric in\nassessing language-specific capabilities. Besides, the experiments show that\nthere is a strong correlation between the LLM's performance in different\nlanguages and the proportion of those languages in its pre-training corpus.\nThese insights underscore the efficacy of the Language Ranker as a tool for\nevaluating LLM performance across different languages, particularly those with\nlimited resources.", "published": "2024-04-17 16:53:16", "link": "http://arxiv.org/abs/2404.11553v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "REQUAL-LM: Reliability and Equity through Aggregation in Large Language\n  Models", "abstract": "The extensive scope of large language models (LLMs) across various domains\nunderscores the critical importance of responsibility in their application,\nbeyond natural language processing. In particular, the randomized nature of\nLLMs, coupled with inherent biases and historical stereotypes in data, raises\ncritical concerns regarding reliability and equity. Addressing these challenges\nare necessary before using LLMs for applications with societal impact. Towards\naddressing this gap, we introduce REQUAL-LM, a novel method for finding\nreliable and equitable LLM outputs through aggregation. Specifically, we\ndevelop a Monte Carlo method based on repeated sampling to find a reliable\noutput close to the mean of the underlying distribution of possible outputs. We\nformally define the terms such as reliability and bias, and design an\nequity-aware aggregation to minimize harmful bias while finding a highly\nreliable output. REQUAL-LM does not require specialized hardware, does not\nimpose a significant computing load, and uses LLMs as a blackbox. This design\nchoice enables seamless scalability alongside the rapid advancement of LLM\ntechnologies. Our system does not require retraining the LLMs, which makes it\ndeployment ready and easy to adapt. Our comprehensive experiments using various\ntasks and datasets demonstrate that REQUAL- LM effectively mitigates bias and\nselects a more equitable response, specifically the outputs that properly\nrepresents minority groups.", "published": "2024-04-17 22:12:41", "link": "http://arxiv.org/abs/2404.11782v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SuRe: Summarizing Retrievals using Answer Candidates for Open-domain QA\n  of LLMs", "abstract": "Large language models (LLMs) have made significant advancements in various\nnatural language processing tasks, including question answering (QA) tasks.\nWhile incorporating new information with the retrieval of relevant passages is\na promising way to improve QA with LLMs, the existing methods often require\nadditional fine-tuning which becomes infeasible with recent LLMs. Augmenting\nretrieved passages via prompting has the potential to address this limitation,\nbut this direction has been limitedly explored. To this end, we design a simple\nyet effective framework to enhance open-domain QA (ODQA) with LLMs, based on\nthe summarized retrieval (SuRe). SuRe helps LLMs predict more accurate answers\nfor a given question, which are well-supported by the summarized retrieval that\ncould be viewed as an explicit rationale extracted from the retrieved passages.\nSpecifically, SuRe first constructs summaries of the retrieved passages for\neach of the multiple answer candidates. Then, SuRe confirms the most plausible\nanswer from the candidate set by evaluating the validity and ranking of the\ngenerated summaries. Experimental results on diverse ODQA benchmarks\ndemonstrate the superiority of SuRe, with improvements of up to 4.6% in exact\nmatch (EM) and 4.0% in F1 score over standard prompting approaches. SuRe also\ncan be integrated with a broad range of retrieval methods and LLMs. Finally,\nthe generated summaries from SuRe show additional advantages to measure the\nimportance of retrieved passages and serve as more preferred rationales by\nmodels and humans.", "published": "2024-04-17 01:15:54", "link": "http://arxiv.org/abs/2404.13081v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Efficient Contextual LLM Cascades through Budget-Constrained Policy\n  Learning", "abstract": "Recent successes in natural language processing have led to the proliferation\nof large language models (LLMs) by multiple providers. Each LLM offering has\ndifferent inference accuracy, monetary cost, and latency, and their accuracy\nfurther depends on the exact wording of the question (i.e., the specific\nprompt). At the same time, users often have a limit on monetary budget and\nlatency to answer all their questions, and they do not know which LLMs to\nchoose for each question to meet their accuracy and long term budget\nrequirements. To navigate this rich design space, we propose TREACLE\n($\\underline{T}$hrifty $\\underline{Rea}$soning via $\\underline{C}$ontext-Aware\n$\\underline{L}$LM and Prompt S$\\underline{e}$lection), a reinforcement learning\npolicy that jointly selects the model and prompting scheme while respecting the\nuser's monetary cost and latency constraints. TREACLE uses the problem context,\nincluding question text embeddings (reflecting the type or difficulty of a\nquery) and the response history (reflecting the consistency of previous\nresponses) to make smart decisions. Our evaluations on standard reasoning\ndatasets (GSM8K, CSQA, and LLC) with various LLMs and prompts show that TREACLE\nenables cost savings of up to 85% compared to baselines, while maintaining high\naccuracy. Importantly, it provides the user with the ability to gracefully\ntrade off accuracy for cost.", "published": "2024-04-17 05:56:49", "link": "http://arxiv.org/abs/2404.13082v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Jointly Recognizing Speech and Singing Voices Based on Multi-Task Audio\n  Source Separation", "abstract": "In short video and live broadcasts, speech, singing voice, and background\nmusic often overlap and obscure each other. This complexity creates\ndifficulties in structuring and recognizing the audio content, which may impair\nsubsequent ASR and music understanding applications. This paper proposes a\nmulti-task audio source separation (MTASS) based ASR model called JRSV, which\nJointly Recognizes Speech and singing Voices. Specifically, the MTASS module\nseparates the mixed audio into distinct speech and singing voice tracks while\nremoving background music. The CTC/attention hybrid recognition module\nrecognizes both tracks. Online distillation is proposed to improve the\nrobustness of recognition further. To evaluate the proposed methods, a\nbenchmark dataset is constructed and released. Experimental results demonstrate\nthat JRSV can significantly improve recognition accuracy on each track of the\nmixed audio.", "published": "2024-04-17 11:31:16", "link": "http://arxiv.org/abs/2404.11275v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "In situ sound absorption estimation with the discrete complex image\n  source method", "abstract": "Estimating the sound absorption in situ relies on accurately describing the\nmeasured sound field. Evidence suggests that modeling the reflection of\nimpinging spherical waves is important, especially for compact measurement\nsystems. This article proposes a method for estimating the sound absorption\ncoefficient of a material sample by mapping the sound pressure, measured by a\nmicrophone array, to a distribution of monopoles along a line in the complex\nplane. The proposed method is compared to modeling the sound field as a\nsuperposition of two sources (a monopole and an image source). The obtained\ninverse problems are solved with Tikhonov regularization, with automatic choice\nof the regularization parameter by the L-curve criterion. The sound absorption\nmeasurement is tested with simulations of the sound field above infinite and\nfinite porous absorbers. The approaches are compared to the plane-wave\nabsorption coefficient and the one obtained by spherical wave incidence.\nExperimental analysis of two porous samples and one resonant absorber is also\ncarried out in situ. Four arrays were tested with an increasing aperture and\nnumber of sensors. It was demonstrated that measurements are feasible even with\nan array with only a few microphones. The discretization of the integral\nequation led to a more accurate reconstruction of the sound pressure and\nparticle velocity at the sample's surface. The resulting absorption coefficient\nagrees with the one obtained for spherical wave incidence, indicating that\nincluding more monopoles along the complex line is an essential feature of the\nsound field.", "published": "2024-04-17 14:03:42", "link": "http://arxiv.org/abs/2404.11399v1", "categories": ["eess.AS", "cs.SD", "physics.class-ph"], "primary_category": "eess.AS"}
{"title": "FairSSD: Understanding Bias in Synthetic Speech Detectors", "abstract": "Methods that can generate synthetic speech which is perceptually\nindistinguishable from speech recorded by a human speaker, are easily\navailable. Several incidents report misuse of synthetic speech generated from\nthese methods to commit fraud. To counter such misuse, many methods have been\nproposed to detect synthetic speech. Some of these detectors are more\ninterpretable, can generalize to detect synthetic speech in the wild and are\nrobust to noise. However, limited work has been done on understanding bias in\nthese detectors. In this work, we examine bias in existing synthetic speech\ndetectors to determine if they will unfairly target a particular gender, age\nand accent group. We also inspect whether these detectors will have a higher\nmisclassification rate for bona fide speech from speech-impaired speakers w.r.t\nfluent speakers. Extensive experiments on 6 existing synthetic speech detectors\nusing more than 0.9 million speech signals demonstrate that most detectors are\ngender, age and accent biased, and future work is needed to ensure fairness. To\nsupport future research, we release our evaluation dataset, models used in our\nstudy and source code at https://gitlab.com/viper-purdue/fairssd.", "published": "2024-04-17 01:53:03", "link": "http://arxiv.org/abs/2404.10989v1", "categories": ["cs.CV", "cs.LG", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Music Enhancement with Deep Filters: A Technical Report for The ICASSP\n  2024 Cadenza Challenge", "abstract": "In this challenge, we disentangle the deep filters from the original\nDeepfilterNet and incorporate them into our Spec-UNet-based network to further\nimprove a hybrid Demucs (hdemucs) based remixing pipeline. The motivation\nbehind the use of the deep filter component lies at its potential in better\nhandling temporal fine structures. We demonstrate an incremental improvement in\nboth the Signal-to-Distortion Ratio (SDR) and the Hearing Aid Audio Quality\nIndex (HAAQI) metrics when comparing the performance of hdemucs against\ndifferent versions of our model.", "published": "2024-04-17 07:01:29", "link": "http://arxiv.org/abs/2404.11116v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
