{"title": "Sequential Recurrent Neural Networks for Language Modeling", "abstract": "Feedforward Neural Network (FNN)-based language models estimate the\nprobability of the next word based on the history of the last N words, whereas\nRecurrent Neural Networks (RNN) perform the same task based only on the last\nword and some context information that cycles in the network. This paper\npresents a novel approach, which bridges the gap between these two categories\nof networks. In particular, we propose an architecture which takes advantage of\nthe explicit, sequential enumeration of the word history in FNN structure while\nenhancing each word representation at the projection layer through recurrent\ncontext information that evolves in the network. The context integration is\nperformed using an additional word-dependent weight matrix that is also learned\nduring the training. Extensive experiments conducted on the Penn Treebank (PTB)\nand the Large Text Compression Benchmark (LTCB) corpus showed a significant\nreduction of the perplexity when compared to state-of-the-art feedforward as\nwell as recurrent neural network architectures.", "published": "2017-03-23 13:48:45", "link": "http://arxiv.org/abs/1703.08068v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multimodal Compact Bilinear Pooling for Multimodal Neural Machine\n  Translation", "abstract": "In state-of-the-art Neural Machine Translation, an attention mechanism is\nused during decoding to enhance the translation. At every step, the decoder\nuses this mechanism to focus on different parts of the source sentence to\ngather the most useful information before outputting its target word. Recently,\nthe effectiveness of the attention mechanism has also been explored for\nmultimodal tasks, where it becomes possible to focus both on sentence parts and\nimage regions. Approaches to pool two modalities usually include element-wise\nproduct, sum or concatenation. In this paper, we evaluate the more advanced\nMultimodal Compact Bilinear pooling method, which takes the outer product of\ntwo vectors to combine the attention features for the two modalities. This has\nbeen previously investigated for visual question answering. We try out this\napproach for multimodal image caption translation and show improvements\ncompared to basic combination methods.", "published": "2017-03-23 14:20:52", "link": "http://arxiv.org/abs/1703.08084v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rapid-Rate: A Framework for Semi-supervised Real-time Sentiment Trend\n  Detection in Unstructured Big Data", "abstract": "Commercial establishments like restaurants, service centres and retailers\nhave several sources of customer feedback about products and services, most of\nwhich need not be as structured as rated reviews provided by services like\nYelp, or Amazon, in terms of sentiment conveyed. For instance, Amazon provides\na fine-grained score on a numeric scale for product reviews. Some sources,\nhowever, like social media (Twitter, Facebook), mailing lists (Google Groups)\nand forums (Quora) contain text data that is much more voluminous, but\nunstructured and unlabelled. It might be in the best interests of a business\nestablishment to assess the general sentiment towards their brand on these\nplatforms as well. This text could be pipelined into a system with a built-in\nprediction model, with the objective of generating real-time graphs on opinion\nand sentiment trends. Although such tasks like the one described about have\nbeen explored with respect to document classification problems in the past, the\nimplementation described in this paper, by virtue of learning a continuous\nfunction rather than a discrete one, offers a lot more depth of insight as\ncompared to document classification approaches. This study aims to explore the\nvalidity of such a continuous function predicting model to quantify sentiment\nabout an entity, without the additional overhead of manual labelling, and\ncomputational preprocessing & feature extraction. This research project also\naims to design and implement a re-usable document regression pipeline as a\nframework, Rapid-Rate, that can be used to predict document scores in\nreal-time.", "published": "2017-03-23 14:37:15", "link": "http://arxiv.org/abs/1703.08088v2", "categories": ["cs.CL", "68T50"], "primary_category": "cs.CL"}
{"title": "TokTrack: A Complete Token Provenance and Change Tracking Dataset for\n  the English Wikipedia", "abstract": "We present a dataset that contains every instance of all tokens (~ words)\never written in undeleted, non-redirect English Wikipedia articles until\nOctober 2016, in total 13,545,349,787 instances. Each token is annotated with\n(i) the article revision it was originally created in, and (ii) lists with all\nthe revisions in which the token was ever deleted and (potentially) re-added\nand re-deleted from its article, enabling a complete and straightforward\ntracking of its history. This data would be exceedingly hard to create by an\naverage potential user as it is (i) very expensive to compute and as (ii)\naccurately tracking the history of each token in revisioned documents is a\nnon-trivial task. Adapting a state-of-the-art algorithm, we have produced a\ndataset that allows for a range of analyses and metrics, already popular in\nresearch and going beyond, to be generated on complete-Wikipedia scale;\nensuring quality and allowing researchers to forego expensive text-comparison\ncomputation, which so far has hindered scalable usage. We show how this data\nenables, on token-level, computation of provenance, measuring survival of\ncontent over time, very detailed conflict metrics, and fine-grained\ninteractions of editors like partial reverts, re-additions and other metrics,\nin the process gaining several novel insights.", "published": "2017-03-23 22:20:45", "link": "http://arxiv.org/abs/1703.08244v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A network of deep neural networks for distant speech recognition", "abstract": "Despite the remarkable progress recently made in distant speech recognition,\nstate-of-the-art technology still suffers from a lack of robustness, especially\nwhen adverse acoustic conditions characterized by non-stationary noises and\nreverberation are met. A prominent limitation of current systems lies in the\nlack of matching and communication between the various technologies involved in\nthe distant speech recognition process. The speech enhancement and speech\nrecognition modules are, for instance, often trained independently. Moreover,\nthe speech enhancement normally helps the speech recognizer, but the output of\nthe latter is not commonly used, in turn, to improve the speech enhancement. To\naddress both concerns, we propose a novel architecture based on a network of\ndeep neural networks, where all the components are jointly trained and better\ncooperate with each other thanks to a full communication scheme between them.\nExperiments, conducted using different datasets, tasks and acoustic conditions,\nrevealed that the proposed framework can overtake other competitive solutions,\nincluding recent joint training approaches.", "published": "2017-03-23 11:02:47", "link": "http://arxiv.org/abs/1703.08002v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Dynamic Bernoulli Embeddings for Language Evolution", "abstract": "Word embeddings are a powerful approach for unsupervised analysis of\nlanguage. Recently, Rudolph et al. (2016) developed exponential family\nembeddings, which cast word embeddings in a probabilistic framework. Here, we\ndevelop dynamic embeddings, building on exponential family embeddings to\ncapture how the meanings of words change over time. We use dynamic embeddings\nto analyze three large collections of historical texts: the U.S. Senate\nspeeches from 1858 to 2009, the history of computer science ACM abstracts from\n1951 to 2014, and machine learning papers on the Arxiv from 2007 to 2015. We\nfind dynamic embeddings provide better fits than classical embeddings and\ncapture interesting patterns about how language changes.", "published": "2017-03-23 13:00:14", "link": "http://arxiv.org/abs/1703.08052v1", "categories": ["stat.ML", "cs.CL"], "primary_category": "stat.ML"}
{"title": "Recurrent and Contextual Models for Visual Question Answering", "abstract": "We propose a series of recurrent and contextual neural network models for\nmultiple choice visual question answering on the Visual7W dataset. Motivated by\ndivergent trends in model complexities in the literature, we explore the\nbalance between model expressiveness and simplicity by studying incrementally\nmore complex architectures. We start with LSTM-encoding of input questions and\nanswers; build on this with context generation by LSTM-encodings of neural\nimage and question representations and attention over images; and evaluate the\ndiversity and predictive power of our models and the ensemble thereof. All\nmodels are evaluated against a simple baseline inspired by the current\nstate-of-the-art, consisting of involving simple concatenation of bag-of-words\nand CNN representations for the text and images, respectively. Generally, we\nobserve marked variation in image-reasoning performance between our models not\nobvious from their overall performance, as well as evidence of dataset bias.\nOur standalone models achieve accuracies up to $64.6\\%$, while the ensemble of\nall models achieves the best accuracy of $66.67\\%$, within $0.5\\%$ of the\ncurrent state-of-the-art for Visual7W.", "published": "2017-03-23 15:57:23", "link": "http://arxiv.org/abs/1703.08120v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "An embedded segmental K-means model for unsupervised segmentation and\n  clustering of speech", "abstract": "Unsupervised segmentation and clustering of unlabelled speech are core\nproblems in zero-resource speech processing. Most approaches lie at\nmethodological extremes: some use probabilistic Bayesian models with\nconvergence guarantees, while others opt for more efficient heuristic\ntechniques. Despite competitive performance in previous work, the full Bayesian\napproach is difficult to scale to large speech corpora. We introduce an\napproximation to a recent Bayesian model that still has a clear objective\nfunction but improves efficiency by using hard clustering and segmentation\nrather than full Bayesian inference. Like its Bayesian counterpart, this\nembedded segmental K-means model (ES-KMeans) represents arbitrary-length word\nsegments as fixed-dimensional acoustic word embeddings. We first compare\nES-KMeans to previous approaches on common English and Xitsonga data sets (5\nand 2.5 hours of speech): ES-KMeans outperforms a leading heuristic method in\nword segmentation, giving similar scores to the Bayesian model while being 5\ntimes faster with fewer hyperparameters. However, its clusters are less pure\nthan those of the other models. We then show that ES-KMeans scales to larger\ncorpora by applying it to the 5 languages of the Zero Resource Speech Challenge\n2017 (up to 45 hours), where it performs competitively compared to the\nchallenge baseline.", "published": "2017-03-23 16:45:22", "link": "http://arxiv.org/abs/1703.08135v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Visually grounded learning of keyword prediction from untranscribed\n  speech", "abstract": "During language acquisition, infants have the benefit of visual cues to\nground spoken language. Robots similarly have access to audio and visual\nsensors. Recent work has shown that images and spoken captions can be mapped\ninto a meaningful common space, allowing images to be retrieved using speech\nand vice versa. In this setting of images paired with untranscribed spoken\ncaptions, we consider whether computer vision systems can be used to obtain\ntextual labels for the speech. Concretely, we use an image-to-words multi-label\nvisual classifier to tag images with soft textual labels, and then train a\nneural network to map from the speech to these soft targets. We show that the\nresulting speech system is able to predict which words occur in an\nutterance---acting as a spoken bag-of-words classifier---without seeing any\nparallel speech and text. We find that the model often confuses semantically\nrelated words, e.g. \"man\" and \"person\", making it even more effective as a\nsemantic keyword spotter.", "published": "2017-03-23 16:46:00", "link": "http://arxiv.org/abs/1703.08136v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "A survey of embedding models of entities and relationships for knowledge\n  graph completion", "abstract": "Knowledge graphs (KGs) of real-world facts about entities and their\nrelationships are useful resources for a variety of natural language processing\ntasks. However, because knowledge graphs are typically incomplete, it is useful\nto perform knowledge graph completion or link prediction, i.e. predict whether\na relationship not in the knowledge graph is likely to be true. This paper\nserves as a comprehensive survey of embedding models of entities and\nrelationships for knowledge graph completion, summarizing up-to-date\nexperimental results on standard benchmark datasets and pointing out potential\nfuture research directions.", "published": "2017-03-23 15:15:26", "link": "http://arxiv.org/abs/1703.08098v9", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
