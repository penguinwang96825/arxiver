{"title": "Two Kinds of Recall", "abstract": "It is an established assumption that pattern-based models are good at\nprecision, while learning based models are better at recall. But is that really\nthe case? I argue that there are two kinds of recall: d-recall, reflecting\ndiversity, and e-recall, reflecting exhaustiveness. I demonstrate through\nexperiments that while neural methods are indeed significantly better at\nd-recall, it is sometimes the case that pattern-based methods are still\nsubstantially better at e-recall. Ideal methods should aim for both kinds, and\nthis ideal should in turn be reflected in our evaluations.", "published": "2023-03-19 01:40:31", "link": "http://arxiv.org/abs/2303.10527v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How People Respond to the COVID-19 Pandemic on Twitter: A Comparative\n  Analysis of Emotional Expressions from US and India", "abstract": "The COVID-19 pandemic has claimed millions of lives worldwide and elicited\nheightened emotions. This study examines the expression of various emotions\npertaining to COVID-19 in the United States and India as manifested in over 54\nmillion tweets, covering the fifteen-month period from February 2020 through\nApril 2021, a period which includes the beginnings of the huge and disastrous\nincrease in COVID-19 cases that started to ravage India in March 2021.\nEmploying pre-trained emotion analysis and topic modeling algorithms, four\ndistinct types of emotions (fear, anger, happiness, and sadness) and their\ntime- and location-associated variations were examined. Results revealed\nsignificant country differences and temporal changes in the relative\nproportions of fear, anger, and happiness, with fear declining and anger and\nhappiness fluctuating in 2020 until new situations over the first four months\nof 2021 reversed the trends. Detected differences are discussed briefly in\nterms of the latent topics revealed and through the lens of appraisal theories\nof emotions, and the implications of the findings are discussed.", "published": "2023-03-19 04:05:10", "link": "http://arxiv.org/abs/2303.10560v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extracting Incidents, Effects, and Requested Advice from MeToo Posts", "abstract": "Survivors of sexual harassment frequently share their experiences on social\nmedia, revealing their feelings and emotions and seeking advice. We observed\nthat on Reddit, survivors regularly share long posts that describe a\ncombination of (i) a sexual harassment incident, (ii) its effect on the\nsurvivor, including their feelings and emotions, and (iii) the advice being\nsought. We term such posts MeToo posts, even though they may not be so tagged\nand may appear in diverse subreddits. A prospective helper (such as a counselor\nor even a casual reader) must understand a survivor's needs from such posts.\nBut long posts can be time-consuming to read and respond to.\n  Accordingly, we address the problem of extracting key information from a long\nMeToo post. We develop a natural language-based model to identify sentences\nfrom a post that describe any of the above three categories.\n  On ten-fold cross-validation of a dataset, our model achieves a macro F1\nscore of 0.82.\n  In addition, we contribute MeThree, a dataset comprising 8,947 labeled\nsentences extracted from Reddit posts. We apply the LIWC-22 toolkit on MeThree\nto understand how different language patterns in sentences of the three\ncategories can reveal differences in emotional tone, authenticity, and other\naspects.", "published": "2023-03-19 05:22:12", "link": "http://arxiv.org/abs/2303.10573v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CTRAN: CNN-Transformer-based Network for Natural Language Understanding", "abstract": "Intent-detection and slot-filling are the two main tasks in natural language\nunderstanding. In this study, we propose CTRAN, a novel encoder-decoder\nCNN-Transformer-based architecture for intent-detection and slot-filling. In\nthe encoder, we use BERT, followed by several convolutional layers, and\nrearrange the output using window feature sequence. We use stacked Transformer\nencoders after the window feature sequence. For the intent-detection decoder,\nwe utilize self-attention followed by a linear layer. In the slot-filling\ndecoder, we introduce the aligned Transformer decoder, which utilizes a zero\ndiagonal mask, aligning output tags with input tokens. We apply our network on\nATIS and SNIPS, and surpass the current state-of-the-art in slot-filling on\nboth datasets. Furthermore, we incorporate the language model as word\nembeddings, and show that this strategy yields a better result when compared to\nthe language model as an encoder.", "published": "2023-03-19 08:57:39", "link": "http://arxiv.org/abs/2303.10606v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "COVID-19 event extraction from Twitter via extractive question answering\n  with continuous prompts", "abstract": "As COVID-19 ravages the world, social media analytics could augment\ntraditional surveys in assessing how the pandemic evolves and capturing\nconsumer chatter that could help healthcare agencies in addressing it. This\ntypically involves mining disclosure events that mention testing positive for\nthe disease or discussions surrounding perceptions and beliefs in preventative\nor treatment options. The 2020 shared task on COVID-19 event extraction\n(conducted as part of the W-NUT workshop during the EMNLP conference)\nintroduced a new Twitter dataset for benchmarking event extraction from\nCOVID-19 tweets. In this paper, we cast the problem of event extraction as\nextractive question answering using recent advances in continuous prompting in\nlanguage models. On the shared task test dataset, our approach leads to over 5%\nabsolute micro-averaged F1-score improvement over prior best results, across\nall COVID-19 event slots. Our ablation study shows that continuous prompts have\na major impact on the eventual performance.", "published": "2023-03-19 13:47:56", "link": "http://arxiv.org/abs/2303.10659v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Toward Artificial Empathy for Human-Centered Design: A Framework", "abstract": "In the early stages of the design process, designers explore opportunities by\ndiscovering unmet needs and developing innovative concepts as potential\nsolutions. From a human-centered design perspective, designers must develop\nempathy with people to truly understand their needs. However, developing\nempathy is a complex and subjective process that relies heavily on the\ndesigner's empathic capability. Therefore, the development of empathic\nunderstanding is intuitive, and the discovery of underlying needs is often\nserendipitous. This paper aims to provide insights from artificial intelligence\nresearch to indicate the future direction of AI-driven human-centered design,\ntaking into account the essential role of empathy. Specifically, we conduct an\ninterdisciplinary investigation of research areas such as data-driven user\nstudies, empathic understanding development, and artificial empathy. Based on\nthis foundation, we discuss the role that artificial empathy can play in\nhuman-centered design and propose an artificial empathy framework for\nhuman-centered design. Building on the mechanisms behind empathy and insights\nfrom empathic design research, the framework aims to break down the rather\ncomplex and subjective concept of empathy into components and modules that can\npotentially be modeled computationally. Furthermore, we discuss the expected\nbenefits of developing such systems and identify current research gaps to\nencourage future research efforts.", "published": "2023-03-19 06:09:52", "link": "http://arxiv.org/abs/2303.10583v2", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Bangla Grammatical Error Detection Using T5 Transformer Model", "abstract": "This paper presents a method for detecting grammatical errors in Bangla using\na Text-to-Text Transfer Transformer (T5) Language Model, using the small\nvariant of BanglaT5, fine-tuned on a corpus of 9385 sentences where errors were\nbracketed by the dedicated demarcation symbol. The T5 model was primarily\ndesigned for translation and is not specifically designed for this task, so\nextensive post-processing was necessary to adapt it to the task of error\ndetection. Our experiments show that the T5 model can achieve low Levenshtein\nDistance in detecting grammatical errors in Bangla, but post-processing is\nessential to achieve optimal performance. The final average Levenshtein\nDistance after post-processing the output of the fine-tuned model was 1.0394 on\na test set of 5000 sentences. This paper also presents a detailed analysis of\nthe errors detected by the model and discusses the challenges of adapting a\ntranslation model for grammar. Our approach can be extended to other languages,\ndemonstrating the potential of T5 models for detecting grammatical errors in a\nwide range of languages.", "published": "2023-03-19 09:24:48", "link": "http://arxiv.org/abs/2303.10612v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FVQA 2.0: Introducing Adversarial Samples into Fact-based Visual\n  Question Answering", "abstract": "The widely used Fact-based Visual Question Answering (FVQA) dataset contains\nvisually-grounded questions that require information retrieval using common\nsense knowledge graphs to answer. It has been observed that the original\ndataset is highly imbalanced and concentrated on a small portion of its\nassociated knowledge graph. We introduce FVQA 2.0 which contains adversarial\nvariants of test questions to address this imbalance. We show that systems\ntrained with the original FVQA train sets can be vulnerable to adversarial\nsamples and we demonstrate an augmentation scheme to reduce this vulnerability\nwithout human annotations.", "published": "2023-03-19 16:07:42", "link": "http://arxiv.org/abs/2303.10699v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "On the Importance of Signer Overlap for Sign Language Detection", "abstract": "Sign language detection, identifying if someone is signing or not, is\nbecoming crucially important for its applications in remote conferencing\nsoftware and for selecting useful sign data for training sign language\nrecognition or translation tasks. We argue that the current benchmark data sets\nfor sign language detection estimate overly positive results that do not\ngeneralize well due to signer overlap between train and test partitions. We\nquantify this with a detailed analysis of the effect of signer overlap on\ncurrent sign detection benchmark data sets. Comparing accuracy with and without\noverlap on the DGS corpus and Signing in the Wild, we observed a relative\ndecrease in accuracy of 4.17% and 6.27%, respectively. Furthermore, we propose\nnew data set partitions that are free of overlap and allow for more realistic\nperformance assessment. We hope this work will contribute to improving the\naccuracy and generalization of sign language detection systems.", "published": "2023-03-19 22:15:05", "link": "http://arxiv.org/abs/2303.10782v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "PACO: Provocation Involving Action, Culture, and Oppression", "abstract": "In India, people identify with a particular group based on certain attributes\nsuch as religion. The same religious groups are often provoked against each\nother. Previous studies show the role of provocation in increasing tensions\nbetween India's two prominent religious groups: Hindus and Muslims. With the\nadvent of the Internet, such provocation also surfaced on social media\nplatforms such as WhatsApp.\n  By leveraging an existing dataset of Indian WhatsApp posts, we identified\nthree categories of provoking sentences against Indian Muslims. Further, we\nlabeled 7,000 sentences for three provocation categories and called this\ndataset PACO. We leveraged PACO to train a model that can identify provoking\nsentences from a WhatsApp post. Our best model is fine-tuned RoBERTa and\nachieved a 0.851 average AUC score over five-fold cross-validation.\nAutomatically identifying provoking sentences could stop provoking text from\nreaching out to the masses, and can prevent possible discrimination or violence\nagainst the target religious group.\n  Further, we studied the provocative speech through a pragmatic lens, by\nidentifying the dialog acts and impoliteness super-strategies used against the\nreligious group.", "published": "2023-03-19 04:39:36", "link": "http://arxiv.org/abs/2303.12808v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PheME: A deep ensemble framework for improving phenotype prediction from\n  multi-modal data", "abstract": "Detailed phenotype information is fundamental to accurate diagnosis and risk\nestimation of diseases. As a rich source of phenotype information, electronic\nhealth records (EHRs) promise to empower diagnostic variant interpretation.\nHowever, how to accurately and efficiently extract phenotypes from the\nheterogeneous EHR data remains a challenge. In this work, we present PheME, an\nEnsemble framework using Multi-modality data of structured EHRs and\nunstructured clinical notes for accurate Phenotype prediction. Firstly, we\nemploy multiple deep neural networks to learn reliable representations from the\nsparse structured EHR data and redundant clinical notes. A multi-modal model\nthen aligns multi-modal features onto the same latent space to predict\nphenotypes. Secondly, we leverage ensemble learning to combine outputs from\nsingle-modal models and multi-modal models to improve phenotype predictions. We\nchoose seven diseases to evaluate the phenotyping performance of the proposed\nframework. Experimental results show that using multi-modal data significantly\nimproves phenotype prediction in all diseases, the proposed ensemble learning\nframework can further boost the performance.", "published": "2023-03-19 23:41:04", "link": "http://arxiv.org/abs/2303.10794v2", "categories": ["cs.LG", "cs.CL", "cs.MM", "q-bio.QM"], "primary_category": "cs.LG"}
{"title": "The Graph feature fusion technique for speaker recognition based on\n  wav2vec2.0 framework", "abstract": "Pre-trained wav2vec2.0 model has been proved its effectiveness for speaker\nrecognition. However, current feature processing methods are focusing on\nclassical pooling on the output features of the pre-trained wav2vec2.0 model,\nsuch as mean pooling, max pooling etc. That methods take the features as the\nindependent and irrelevant units, ignoring the inter-relationship among all the\nfeatures, and do not take the features as an overall representation of a\nspeaker. Gated Recurrent Unit (GRU), as a feature fusion method, can also be\nconsidered as a complicated pooling technique, mainly focuses on the temporal\ninformation, which may show poor performance in some situations that the main\ninformation is not on the temporal dimension. In this paper, we investigate the\ngraph neural network (GNN) as a backend processing module based on wav2vec2.0\nframework to provide a solution for the mentioned matters. The GNN takes all\nthe output features as the graph signal data and extracts the related graph\nstructure information of features for speaker recognition. Specifically, we\nfirst give a simple proof that the GNN feature fusion method can outperform\nthan the mean, max, random pooling methods and so on theoretically. Then, we\nmodel the output features of wav2vec2.0 as the vertices of a graph, and\nconstruct the graph adjacency matrix by graph attention network (GAT). Finally,\nwe follow the message passing neural network (MPNN) to design our message\nfunction, vertex update function and readout function to transform the speaker\nfeatures into the graph features. The experiments show our performance can\nprovide a relative improvement compared to the baseline methods. Code is\navailable at xxx.", "published": "2023-03-19 03:50:16", "link": "http://arxiv.org/abs/2303.10556v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Audio-Text Models Do Not Yet Leverage Natural Language", "abstract": "Multi-modal contrastive learning techniques in the audio-text domain have\nquickly become a highly active area of research. Most works are evaluated with\nstandard audio retrieval and classification benchmarks assuming that (i) these\nmodels are capable of leveraging the rich information contained in natural\nlanguage, and (ii) current benchmarks are able to capture the nuances of such\ninformation. In this work, we show that state-of-the-art audio-text models do\nnot yet really understand natural language, especially contextual concepts such\nas sequential or concurrent ordering of sound events. Our results suggest that\nexisting benchmarks are not sufficient to assess these models' capabilities to\nmatch complex contexts from the audio and text modalities. We propose a\nTransformer-based architecture and show that, unlike prior work, it is capable\nof modeling the sequential relationship between sound events in the text and\naudio, given appropriate benchmark data. We advocate for the collection or\ngeneration of additional, diverse, data to allow future research to fully\nleverage natural language for audio-text modeling.", "published": "2023-03-19 14:14:46", "link": "http://arxiv.org/abs/2303.10667v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Textless Speech-to-Music Retrieval Using Emotion Similarity", "abstract": "We introduce a framework that recommends music based on the emotions of\nspeech. In content creation and daily life, speech contains information about\nhuman emotions, which can be enhanced by music. Our framework focuses on a\ncross-domain retrieval system to bridge the gap between speech and music via\nemotion labels. We explore different speech representations and report their\nimpact on different speech types, including acting voice and wake-up words. We\nalso propose an emotion similarity regularization term in cross-domain\nretrieval tasks. By incorporating the regularization term into training,\nsimilar speech-and-music pairs in the emotion space are closer in the joint\nembedding space. Our comprehensive experimental results show that the proposed\nmodel is effective in textless speech-to-music retrieval.", "published": "2023-03-19 02:33:19", "link": "http://arxiv.org/abs/2303.10539v1", "categories": ["cs.SD", "cs.IR", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Right the docs: Characterising voice dataset documentation practices\n  used in machine learning", "abstract": "Voice-enabled technology is quickly becoming ubiquitous, and is constituted\nfrom machine learning (ML)-enabled components such as speech recognition and\nvoice activity detection. However, these systems don't yet work well for\neveryone. They exhibit bias - the systematic and unfair discrimination against\nindividuals or cohorts of individuals in favour of others (Friedman &\nNissembaum, 1996) - across axes such as age, gender and accent.\n  ML is reliant on large datasets for training. Dataset documentation is\ndesigned to give ML Practitioners (MLPs) a better understanding of a dataset's\ncharacteristics. However, there is a lack of empirical research on voice\ndataset documentation specifically. Additionally, while MLPs are frequent\nparticipants in fairness research, little work focuses on those who work with\nvoice data. Our work makes an empirical contribution to this gap.\n  Here, we combine two methods to form an exploratory study. First, we\nundertake 13 semi-structured interviews, exploring multiple perspectives of\nvoice dataset documentation practice. Using open and axial coding methods, we\nexplore MLPs' practices through the lenses of roles and tradeoffs. Drawing from\nthis work, we then purposively sample voice dataset documents (VDDs) for 9\nvoice datasets. Our findings then triangulate these two methods, using the\nlenses of MLP roles and trade-offs. We find that current VDD practices are\ninchoate, inadequate and incommensurate. The characteristics of voice datasets\nare codified in fragmented, disjoint ways that often do not meet the needs of\nMLPs. Moreover, they cannot be readily compared, presenting a barrier to\npractitioners' bias reduction efforts.\n  We then discuss the implications of these findings for bias practices in\nvoice data and speech technologies. We conclude by setting out a program of\nfuture work to address these findings -- that is, how we may \"right the docs\".", "published": "2023-03-19 17:32:07", "link": "http://arxiv.org/abs/2303.10721v1", "categories": ["cs.HC", "cs.CY", "cs.SD", "eess.AS", "K.4"], "primary_category": "cs.HC"}
{"title": "ERSAM: Neural Architecture Search For Energy-Efficient and Real-Time\n  Social Ambiance Measurement", "abstract": "Social ambiance describes the context in which social interactions happen,\nand can be measured using speech audio by counting the number of concurrent\nspeakers. This measurement has enabled various mental health tracking and\nhuman-centric IoT applications. While on-device Socal Ambiance Measure (SAM) is\nhighly desirable to ensure user privacy and thus facilitate wide adoption of\nthe aforementioned applications, the required computational complexity of\nstate-of-the-art deep neural networks (DNNs) powered SAM solutions stands at\nodds with the often constrained resources on mobile devices. Furthermore, only\nlimited labeled data is available or practical when it comes to SAM under\nclinical settings due to various privacy constraints and the required human\neffort, further challenging the achievable accuracy of on-device SAM solutions.\nTo this end, we propose a dedicated neural architecture search framework for\nEnergy-efficient and Real-time SAM (ERSAM). Specifically, our ERSAM framework\ncan automatically search for DNNs that push forward the achievable accuracy vs.\nhardware efficiency frontier of mobile SAM solutions. For example,\nERSAM-delivered DNNs only consume 40 mW x 12 h energy and 0.05 seconds\nprocessing latency for a 5 seconds audio segment on a Pixel 3 phone, while only\nachieving an error rate of 14.3% on a social ambiance dataset generated by\nLibriSpeech. We can expect that our ERSAM framework can pave the way for\nubiquitous on-device SAM solutions which are in growing demand.", "published": "2023-03-19 18:08:18", "link": "http://arxiv.org/abs/2303.10727v3", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Multiscale Audio Spectrogram Transformer for Efficient Audio\n  Classification", "abstract": "Audio event has a hierarchical architecture in both time and frequency and\ncan be grouped together to construct more abstract semantic audio classes. In\nthis work, we develop a multiscale audio spectrogram Transformer (MAST) that\nemploys hierarchical representation learning for efficient audio\nclassification. Specifically, MAST employs one-dimensional (and\ntwo-dimensional) pooling operators along the time (and frequency domains) in\ndifferent stages, and progressively reduces the number of tokens and increases\nthe feature dimensions. MAST significantly outperforms AST~\\cite{gong2021ast}\nby 22.2\\%, 4.4\\% and 4.7\\% on Kinetics-Sounds, Epic-Kitchens-100 and VGGSound\nin terms of the top-1 accuracy without external training data. On the\ndownloaded AudioSet dataset, which has over 20\\% missing audios, MAST also\nachieves slightly better accuracy than AST. In addition, MAST is 5x more\nefficient in terms of multiply-accumulates (MACs) with 42\\% reduction in the\nnumber of parameters compared to AST. Through clustering metrics and\nvisualizations, we demonstrate that the proposed MAST can learn semantically\nmore separable feature representations from audio signals.", "published": "2023-03-19 20:21:29", "link": "http://arxiv.org/abs/2303.10757v1", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
