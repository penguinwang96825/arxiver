{"title": "Pricing Weather Derivatives: A Time Series Neural Network Approach", "abstract": "The objective of the paper is to price weather derivative contracts based on\ntemperature and precipitation as underlying climate variables. We use a neural\nnetwork approach combined with time series forecast to value Pacific Rim index\nin Toronto and Chicago", "published": "2024-11-18 19:54:28", "link": "http://arxiv.org/abs/2411.12013v1", "categories": ["q-fin.MF", "cs.LG", "q-fin.ST", "stat.ML", "27M10, 68T09", "G.3"], "primary_category": "q-fin.MF"}
{"title": "Robust Bernoulli mixture models for credit portfolio risk", "abstract": "This paper presents comparison results and establishes risk bounds for credit\nportfolios within classes of Bernoulli mixture models, assuming conditionally\nindependent defaults that are stochastically increasing with a common risk\nfactor. We provide simple and interpretable conditions for conditional default\nprobabilities that imply a comparison of credit portfolio losses in convex\norder. In the case of threshold models, the ranking of portfolio losses is\nbased on a pointwise comparison of the underlying copulas. Our setting includes\nas special case the well-known Gaussian copula model but allows for general\ntail dependencies, which are crucial for modeling credit portfolio risks.\nMoreover, our results extend the classical parameterized models, such as the\nindustry models CreditMetrics and KMV Portfolio Manager, to a robust setting\nwhere individual parameters or the copula modeling the dependence structure can\nbe ambiguous. A simulation study and a real data example under model\nuncertainty offer evidence supporting the effectiveness of our approach.", "published": "2024-11-18 12:31:39", "link": "http://arxiv.org/abs/2411.11522v1", "categories": ["q-fin.RM", "q-fin.MF", "62P05, 91Gxx"], "primary_category": "q-fin.RM"}
{"title": "Multidimensional specific relative entropy between continuous martingales", "abstract": "In continuous time, the laws of martingales tend to be singular to each\nother. Notably, N. Gantert introduced the concept of specific relative entropy\nbetween real-valued continuous martingales, defined as a scaling limit of\nfinite-dimensional relative entropies, and showed that this quantity is\nnon-trivial despite the aforementioned mutual singularity of martingale laws.\n  Our main mathematical contribution is to extend this object, originally\nrestricted to one-dimensional martingales, to multiple dimensions. Among other\nresults, we establish that Gantert's inequality, bounding the specific relative\nentropy with respect to Wiener measure from below by an explicit functional of\nthe quadratic variation, essentially carries over to higher dimensions. We also\nprove that this lower bound is tight, in the sense that it is the convex lower\nsemicontinuous envelope of the specific relative entropy. This is a novel\nresult even in dimension one. Finally we establish closed-form expressions for\nthe specific relative entropy in simple multidimensional examples.", "published": "2024-11-18 09:30:02", "link": "http://arxiv.org/abs/2411.11408v1", "categories": ["math.PR", "q-fin.MF"], "primary_category": "math.PR"}
{"title": "High resolution microprice estimates from limit orderbook data using hyperdimensional vector Tsetlin Machines", "abstract": "We propose an error-correcting model for the microprice, a high-frequency\nestimator of future prices given higher order information of imbalances in the\norderbook. The model takes into account a current microprice estimate given the\nspread and best bid to ask imbalance, and adjusts the microprice based on\nrecent dynamics of higher price rank imbalances. We introduce a computationally\nfast estimator using a recently proposed hyperdimensional vector Tsetlin\nmachine framework and demonstrate empirically that this estimator can provide a\nrobust estimate of future prices in the orderbook.", "published": "2024-11-18 13:18:30", "link": "http://arxiv.org/abs/2411.13594v1", "categories": ["q-fin.TR", "cs.LG", "q-fin.ST"], "primary_category": "q-fin.TR"}
{"title": "Advance Detection Of Bull And Bear Phases In Cryptocurrency Markets", "abstract": "Cryptocurrencies are highly volatile financial instruments with more and more\nnew retail investors joining the scene with each passing day. Bitcoin has\nalways proved to determine in which way the rest of the cryptocurrency market\nis headed towards. As of today Bitcoin has a market dominance of close to 50\npercent. Bull and bear phases in cryptocurrencies are determined based on the\nperformance of Bitcoin over the 50 Day and 200 Day Moving Averages. The aim of\nthis paper is to foretell the performance of bitcoin in the near future by\nemploying predictive algorithms. This predicted data will then be used to\ncalculate the 50 Day and 200 Day Moving Averages and subsequently plotted to\nestablish the potential bull and bear phases.", "published": "2024-11-18 01:48:16", "link": "http://arxiv.org/abs/2411.13586v1", "categories": ["q-fin.ST", "cs.AI"], "primary_category": "q-fin.ST"}
{"title": "Large corpora and large language models: a replicable method for\n  automating grammatical annotation", "abstract": "Much linguistic research relies on annotated datasets of features extracted\nfrom text corpora, but the rapid quantitative growth of these corpora has\ncreated practical difficulties for linguists to manually annotate large data\nsamples. In this paper, we present a replicable, supervised method that\nleverages large language models for assisting the linguist in grammatical\nannotation through prompt engineering, training, and evaluation. We introduce a\nmethodological pipeline applied to the case study of formal variation in the\nEnglish evaluative verb construction 'consider X (as) (to be) Y', based on the\nlarge language model Claude 3.5 Sonnet and corpus data from Davies' NOW and\nEnTenTen21 (SketchEngine). Overall, we reach a model accuracy of over 90% on\nour held-out test samples with only a small amount of training data, validating\nthe method for the annotation of very large quantities of tokens of the\nconstruction in the future. We discuss the generalisability of our results for\na wider range of case studies of grammatical constructions and grammatical\nvariation and change, underlining the value of AI copilots as tools for future\nlinguistic research, notwithstanding some important caveats.", "published": "2024-11-18 03:29:48", "link": "http://arxiv.org/abs/2411.11260v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "VersaTune: An Efficient Data Composition Framework for Training\n  Multi-Capability LLMs", "abstract": "Large-scale pretrained models, particularly Large Language Models (LLMs),\nhave exhibited remarkable capabilities in handling multiple tasks across\ndomains due to their emergent properties. These capabilities are further\naugmented during the Supervised Fine-Tuning (SFT) phase. Despite their\npotential, existing work mainly focuses on domain-specific enhancements during\nfine-tuning, the challenge of which lies in catastrophic forgetting of\nknowledge across other domains. In this study, we introduce VersaTune, a novel\ndata composition framework designed for enhancing LLMs' overall multi-ability\nperformances during training. We categorize knowledge into distinct domains\nincluding law, medicine, finance, science, code, etc. We begin with detecting\nthe distribution of domain-specific knowledge within the base model, followed\nby the training data composition that aligns with the model's existing\nknowledge distribution. During the training process, domain weights are\ndynamically adjusted based on their learnable potential and forgetting degree.\nExperimental results demonstrate that VersaTune achieves significant\nimprovements in multi-domain performance, with an 35.21% enhancement in\ncomprehensive multi-domain tasks. Additionally, in scenarios where specific\ndomain optimization is required, VersaTune reduces the degradation of\nperformance in other domains by 38.77%, without compromising the target\ndomain's training efficacy.", "published": "2024-11-18 03:45:34", "link": "http://arxiv.org/abs/2411.11266v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Membership Inference Attack against Long-Context Large Language Models", "abstract": "Recent advances in Large Language Models (LLMs) have enabled them to overcome\ntheir context window limitations, and demonstrate exceptional retrieval and\nreasoning capacities on longer context. Quesion-answering systems augmented\nwith Long-Context Language Models (LCLMs) can automatically search massive\nexternal data and incorporate it into their contexts, enabling faithful\npredictions and reducing issues such as hallucinations and knowledge staleness.\nExisting studies targeting LCLMs mainly concentrate on addressing the so-called\nlost-in-the-middle problem or improving the inference effiencicy, leaving their\nprivacy risks largely unexplored. In this paper, we aim to bridge this gap and\nargue that integrating all information into the long context makes it a\nrepository of sensitive information, which often contains private data such as\nmedical records or personal identities. We further investigate the membership\nprivacy within LCLMs external context, with the aim of determining whether a\ngiven document or sequence is included in the LCLMs context. Our basic idea is\nthat if a document lies in the context, it will exhibit a low generation loss\nor a high degree of semantic similarity to the contents generated by LCLMs. We\nfor the first time propose six membership inference attack (MIA) strategies\ntailored for LCLMs and conduct extensive experiments on various popular models.\nEmpirical results demonstrate that our attacks can accurately infer membership\nstatus in most cases, e.g., 90.66% attack F1-score on Multi-document QA\ndatasets with LongChat-7b-v1.5-32k, highlighting significant risks of\nmembership leakage within LCLMs input contexts. Furthermore, we examine the\nunderlying reasons why LCLMs are susceptible to revealing such membership\ninformation.", "published": "2024-11-18 09:50:54", "link": "http://arxiv.org/abs/2411.11424v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Value-Spectrum: Quantifying Preferences of Vision-Language Models via\n  Value Decomposition in Social Media Contexts", "abstract": "The recent progress in Vision-Language Models (VLMs) has broadened the scope\nof multimodal applications. However, evaluations often remain limited to\nfunctional tasks, neglecting abstract dimensions such as personality traits and\nhuman values. To address this gap, we introduce Value-Spectrum, a novel Visual\nQuestion Answering (VQA) benchmark aimed at assessing VLMs based on Schwartz's\nvalue dimensions that capture core values guiding people's preferences and\nactions. We designed a VLM agent pipeline to simulate video browsing and\nconstructed a vector database comprising over 50,000 short videos from TikTok,\nYouTube Shorts, and Instagram Reels. These videos span multiple months and\ncover diverse topics, including family, health, hobbies, society, technology,\netc. Benchmarking on Value-Spectrum highlights notable variations in how VLMs\nhandle value-oriented content. Beyond identifying VLMs' intrinsic preferences,\nwe also explored the ability of VLM agents to adopt specific personas when\nexplicitly prompted, revealing insights into the adaptability of the model in\nrole-playing scenarios. These findings highlight the potential of\nValue-Spectrum as a comprehensive evaluation set for tracking VLM alignments in\nvalue-based tasks and abilities to simulate diverse personas.", "published": "2024-11-18 11:31:10", "link": "http://arxiv.org/abs/2411.11479v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Safe + Safe = Unsafe? Exploring How Safe Images Can Be Exploited to\n  Jailbreak Large Vision-Language Models", "abstract": "Recent advances in Large Vision-Language Models (LVLMs) have showcased strong\nreasoning abilities across multiple modalities, achieving significant\nbreakthroughs in various real-world applications. Despite this great success,\nthe safety guardrail of LVLMs may not cover the unforeseen domains introduced\nby the visual modality. Existing studies primarily focus on eliciting LVLMs to\ngenerate harmful responses via carefully crafted image-based jailbreaks\ndesigned to bypass alignment defenses. In this study, we reveal that a safe\nimage can be exploited to achieve the same jailbreak consequence when combined\nwith additional safe images and prompts. This stems from two fundamental\nproperties of LVLMs: universal reasoning capabilities and safety snowball\neffect. Building on these insights, we propose Safety Snowball Agent (SSA), a\nnovel agent-based framework leveraging agents' autonomous and tool-using\nabilities to jailbreak LVLMs. SSA operates through two principal stages: (1)\ninitial response generation, where tools generate or retrieve jailbreak images\nbased on potential harmful intents, and (2) harmful snowballing, where refined\nsubsequent prompts induce progressively harmful outputs. Our experiments\ndemonstrate that \\ours can use nearly any image to induce LVLMs to produce\nunsafe content, achieving high success jailbreaking rates against the latest\nLVLMs. Unlike prior works that exploit alignment flaws, \\ours leverages the\ninherent properties of LVLMs, presenting a profound challenge for enforcing\nsafety in generative multimodal systems. Our code is avaliable at\n\\url{https://github.com/gzcch/Safety_Snowball_Agent}.", "published": "2024-11-18 11:58:07", "link": "http://arxiv.org/abs/2411.11496v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OASIS: Open Agent Social Interaction Simulations with One Million Agents", "abstract": "There has been a growing interest in enhancing rule-based agent-based models\n(ABMs) for social media platforms (i.e., X, Reddit) with more realistic large\nlanguage model (LLM) agents, thereby allowing for a more nuanced study of\ncomplex systems. As a result, several LLM-based ABMs have been proposed in the\npast year. While they hold promise, each simulator is specifically designed to\nstudy a particular scenario, making it time-consuming and resource-intensive to\nexplore other phenomena using the same ABM. Additionally, these models simulate\nonly a limited number of agents, whereas real-world social media platforms\ninvolve millions of users. To this end, we propose OASIS, a generalizable and\nscalable social media simulator. OASIS is designed based on real-world social\nmedia platforms, incorporating dynamically updated environments (i.e., dynamic\nsocial networks and post information), diverse action spaces (i.e., following,\ncommenting), and recommendation systems (i.e., interest-based and\nhot-score-based). Additionally, OASIS supports large-scale user simulations,\ncapable of modeling up to one million users. With these features, OASIS can be\neasily extended to different social media platforms to study large-scale group\nphenomena and behaviors. We replicate various social phenomena, including\ninformation spreading, group polarization, and herd effects across X and Reddit\nplatforms. Moreover, we provide observations of social phenomena at different\nagent group scales. We observe that the larger agent group scale leads to more\nenhanced group dynamics and more diverse and helpful agents' opinions. These\nfindings demonstrate OASIS's potential as a powerful tool for studying complex\nsystems in digital environments.", "published": "2024-11-18 13:57:35", "link": "http://arxiv.org/abs/2411.11581v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Federated Incremental Named Entity Recognition", "abstract": "Federated Named Entity Recognition (FNER) boosts model training within each\nlocal client by aggregating the model updates of decentralized local clients,\nwithout sharing their private data. However, existing FNER methods assume fixed\nentity types and local clients in advance, leading to their ineffectiveness in\npractical applications. In a more realistic scenario, local clients receive new\nentity types continuously, while new local clients collecting novel data may\nirregularly join the global FNER training. This challenging setup, referred to\nhere as Federated Incremental NER, renders the global model suffering from\nheterogeneous forgetting of old entity types from both intra-client and\ninter-client perspectives. To overcome these challenges, we propose a\nLocal-Global Forgetting Defense (LGFD) model. Specifically, to address\nintra-client forgetting, we develop a structural knowledge distillation loss to\nretain the latent space's feature structure and a pseudo-label-guided\ninter-type contrastive loss to enhance discriminative capability over different\nentity types, effectively preserving previously learned knowledge within local\nclients. To tackle inter-client forgetting, we propose a task switching monitor\nthat can automatically identify new entity types under privacy protection and\nstore the latest old global model for knowledge distillation and\npseudo-labeling. Experiments demonstrate significant improvement of our LGFD\nmodel over comparison methods.", "published": "2024-11-18 14:53:53", "link": "http://arxiv.org/abs/2411.11623v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Advacheck at GenAI Detection Task 1: AI Detection Powered by\n  Domain-Aware Multi-Tasking", "abstract": "The paper describes a system designed by Advacheck team to recognise\nmachine-generated and human-written texts in the monolingual subtask of GenAI\nDetection Task 1 competition. Our developed system is a multi-task architecture\nwith shared Transformer Encoder between several classification heads. One head\nis responsible for binary classification between human-written and\nmachine-generated texts, while the other heads are auxiliary multiclass\nclassifiers for texts of different domains from particular datasets. As\nmulticlass heads were trained to distinguish the domains presented in the data,\nthey provide a better understanding of the samples. This approach led us to\nachieve the first place in the official ranking with 83.07% macro F1-score on\nthe test set and bypass the baseline by 10%. We further study obtained system\nthrough ablation, error and representation analyses, finding that multi-task\nlearning outperforms single-task mode and simultaneous tasks form a cluster\nstructure in embeddings space.", "published": "2024-11-18 17:03:30", "link": "http://arxiv.org/abs/2411.11736v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MEMO-Bench: A Multiple Benchmark for Text-to-Image and Multimodal Large\n  Language Models on Human Emotion Analysis", "abstract": "Artificial Intelligence (AI) has demonstrated significant capabilities in\nvarious fields, and in areas such as human-computer interaction (HCI), embodied\nintelligence, and the design and animation of virtual digital humans, both\npractitioners and users are increasingly concerned with AI's ability to\nunderstand and express emotion. Consequently, the question of whether AI can\naccurately interpret human emotions remains a critical challenge. To date, two\nprimary classes of AI models have been involved in human emotion analysis:\ngenerative models and Multimodal Large Language Models (MLLMs). To assess the\nemotional capabilities of these two classes of models, this study introduces\nMEMO-Bench, a comprehensive benchmark consisting of 7,145 portraits, each\ndepicting one of six different emotions, generated by 12 Text-to-Image (T2I)\nmodels. Unlike previous works, MEMO-Bench provides a framework for evaluating\nboth T2I models and MLLMs in the context of sentiment analysis. Additionally, a\nprogressive evaluation approach is employed, moving from coarse-grained to\nfine-grained metrics, to offer a more detailed and comprehensive assessment of\nthe sentiment analysis capabilities of MLLMs. The experimental results\ndemonstrate that existing T2I models are more effective at generating positive\nemotions than negative ones. Meanwhile, although MLLMs show a certain degree of\neffectiveness in distinguishing and recognizing human emotions, they fall short\nof human-level accuracy, particularly in fine-grained emotion analysis. The\nMEMO-Bench will be made publicly available to support further research in this\narea.", "published": "2024-11-18 02:09:48", "link": "http://arxiv.org/abs/2411.11235v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ZeFaV: Boosting Large Language Models for Zero-shot Fact Verification", "abstract": "In this paper, we propose ZeFaV - a zero-shot based fact-checking\nverification framework to enhance the performance on fact verification task of\nlarge language models by leveraging the in-context learning ability of large\nlanguage models to extract the relations among the entities within a claim,\nre-organized the information from the evidence in a relationally logical form,\nand combine the above information with the original evidence to generate the\ncontext from which our fact-checking model provide verdicts for the input\nclaims. We conducted empirical experiments to evaluate our approach on two\nmulti-hop fact-checking datasets including HoVer and FEVEROUS, and achieved\npotential results results comparable to other state-of-the-art fact\nverification task methods.", "published": "2024-11-18 02:35:15", "link": "http://arxiv.org/abs/2411.11247v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LP Data Pipeline: Lightweight, Purpose-driven Data Pipeline for Large\n  Language Models", "abstract": "Creating high-quality, large-scale datasets for large language models (LLMs)\noften relies on resource-intensive, GPU-accelerated models for quality\nfiltering, making the process time-consuming and costly. This dependence on\nGPUs limits accessibility for organizations lacking significant computational\ninfrastructure. To address this issue, we introduce the Lightweight,\nPurpose-driven (LP) Data Pipeline, a framework that operates entirely on CPUs\nto streamline the processes of dataset extraction, filtering, and curation.\nBased on our four core principles, the LP Data Pipeline significantly reduces\npreparation time and cost while maintaining high data quality. Importantly, our\npipeline enables the creation of purpose-driven datasets tailored to specific\ndomains and languages, enhancing the applicability of LLMs in specialized\ncontexts. We anticipate that our pipeline will lower the barriers to LLM\ndevelopment, enabling a wide range of organizations to access LLMs more easily.", "published": "2024-11-18 05:17:27", "link": "http://arxiv.org/abs/2411.11289v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Transcending Language Boundaries: Harnessing LLMs for Low-Resource\n  Language Translation", "abstract": "Large Language Models (LLMs) have demonstrated remarkable success across a\nwide range of tasks and domains. However, their performance in low-resource\nlanguage translation, particularly when translating into these languages,\nremains underexplored. This gap poses significant challenges, as linguistic\nbarriers hinder the cultural preservation and development of minority\ncommunities. To address this issue, this paper introduces a novel\nretrieval-based method that enhances translation quality for low-resource\nlanguages by focusing on key terms, which involves translating keywords and\nretrieving corresponding examples from existing data. To evaluate the\neffectiveness of this method, we conducted experiments translating from English\ninto three low-resource languages: Cherokee, a critically endangered indigenous\nlanguage of North America; Tibetan, a historically and culturally significant\nlanguage in Asia; and Manchu, a language with few remaining speakers. Our\ncomparison with the zero-shot performance of GPT-4o and LLaMA 3.1 405B,\nhighlights the significant challenges these models face when translating into\nlow-resource languages. In contrast, our retrieval-based method shows promise\nin improving both word-level accuracy and overall semantic understanding by\nleveraging existing resources more effectively.", "published": "2024-11-18 05:41:27", "link": "http://arxiv.org/abs/2411.11295v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mitigating Knowledge Conflicts in Language Model-Driven Question\n  Answering", "abstract": "In the context of knowledge-driven seq-to-seq generation tasks, such as\ndocument-based question answering and document summarization systems, two\nfundamental knowledge sources play crucial roles: the inherent knowledge\nembedded within model parameters and the external knowledge obtained through\ncontext. Recent studies revealed a significant challenge: when there exists a\nmisalignment between the model's inherent knowledge and the ground truth\nanswers in training data, the system may exhibit problematic behaviors during\ninference, such as ignoring input context, or generating unfaithful content.\nOur investigation proposes a strategy to minimize hallucination by building\nexplicit connection between source inputs and generated outputs. We\nspecifically target a common hallucination pattern in question answering,\nexamining how the correspondence between entities and their contexts during\nmodel training influences the system's performance at inference time.", "published": "2024-11-18 07:33:10", "link": "http://arxiv.org/abs/2411.11344v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MAIRA-Seg: Enhancing Radiology Report Generation with Segmentation-Aware\n  Multimodal Large Language Models", "abstract": "There is growing interest in applying AI to radiology report generation,\nparticularly for chest X-rays (CXRs). This paper investigates whether\nincorporating pixel-level information through segmentation masks can improve\nfine-grained image interpretation of multimodal large language models (MLLMs)\nfor radiology report generation. We introduce MAIRA-Seg, a segmentation-aware\nMLLM framework designed to utilize semantic segmentation masks alongside CXRs\nfor generating radiology reports. We train expert segmentation models to obtain\nmask pseudolabels for radiology-specific structures in CXRs. Subsequently,\nbuilding on the architectures of MAIRA, a CXR-specialised model for report\ngeneration, we integrate a trainable segmentation tokens extractor that\nleverages these mask pseudolabels, and employ mask-aware prompting to generate\ndraft radiology reports. Our experiments on the publicly available MIMIC-CXR\ndataset show that MAIRA-Seg outperforms non-segmentation baselines. We also\ninvestigate set-of-marks prompting with MAIRA and find that MAIRA-Seg\nconsistently demonstrates comparable or superior performance. The results\nconfirm that using segmentation masks enhances the nuanced reasoning of MLLMs,\npotentially contributing to better clinical outcomes.", "published": "2024-11-18 08:13:22", "link": "http://arxiv.org/abs/2411.11362v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Rethinking Thinking Tokens: Understanding Why They Underperform in\n  Practice", "abstract": "Thinking Tokens (TT) have been proposed as an unsupervised method to\nfacilitate reasoning in language models. However, despite their conceptual\nappeal, our findings show that TTs marginally improves performance and\nconsistently underperforms compared to Chain-of-Thought (CoT) reasoning across\nmultiple benchmarks. We hypothesize that this underperformance stems from the\nreliance on a single embedding for TTs, which results in inconsistent learning\nsignals and introduces noisy gradients. This paper provides a comprehensive\nempirical analysis to validate this hypothesis and discusses the implications\nfor future research on unsupervised reasoning in LLMs.", "published": "2024-11-18 08:34:38", "link": "http://arxiv.org/abs/2411.11371v1", "categories": ["cs.CL", "cs.LG", "I.2.6"], "primary_category": "cs.CL"}
{"title": "Re-examining learning linear functions in context", "abstract": "In-context learning (ICL) has emerged as a powerful paradigm for easily\nadapting Large Language Models (LLMs) to various tasks. However, our\nunderstanding of how ICL works remains limited. We explore a simple model of\nICL in a controlled setup with synthetic training data to investigate ICL of\nunivariate linear functions. We experiment with a range of GPT-2-like\ntransformer models trained from scratch. Our findings challenge the prevailing\nnarrative that transformers adopt algorithmic approaches like linear regression\nto learn a linear function in-context. These models fail to generalize beyond\ntheir training distribution, highlighting fundamental limitations in their\ncapacity to infer abstract task structures. Our experiments lead us to propose\na mathematically precise hypothesis of what the model might be learning.", "published": "2024-11-18 10:58:46", "link": "http://arxiv.org/abs/2411.11465v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Addressing Hallucinations in Language Models with Knowledge Graph\n  Embeddings as an Additional Modality", "abstract": "In this paper we present an approach to reduce hallucinations in Large\nLanguage Models (LLMs) by incorporating Knowledge Graphs (KGs) as an additional\nmodality. Our method involves transforming input text into a set of KG\nembeddings and using an adapter to integrate these embeddings into the language\nmodel space, without relying on external retrieval processes.\n  To facilitate this, we created WikiEntities, a dataset containing over 3\nmillion Wikipedia texts annotated with entities from Wikidata and their\ncorresponding embeddings from PyTorch-BigGraph. This dataset serves as a\nvaluable resource for training Entity Linking models and adapting the described\nmethod to various LLMs using specialized adapters.\n  Our method does not require fine-tuning of the language models themselves;\ninstead, we only train the adapter. This ensures that the model's performance\non other tasks is not affected. We trained an adapter for the Mistral 7B, LLaMA\n2-7B (chat), and LLaMA 3-8B (instruct) models using this dataset and\ndemonstrated that our approach improves performance on the HaluEval, True-False\nbenchmarks and FEVER dataset. The results indicate that incorporating KGs as a\nnew modality can effectively reduce hallucinations and improve the factual\naccuracy of language models, all without the need for external retrieval.", "published": "2024-11-18 12:40:51", "link": "http://arxiv.org/abs/2411.11531v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Chapter 7 Review of Data-Driven Generative AI Models for Knowledge\n  Extraction from Scientific Literature in Healthcare", "abstract": "This review examines the development of abstractive NLP-based text\nsummarization approaches and compares them to existing techniques for\nextractive summarization. A brief history of text summarization from the 1950s\nto the introduction of pre-trained language models such as Bidirectional\nEncoder Representations from Transformer (BERT) and Generative Pre-training\nTransformers (GPT) are presented. In total, 60 studies were identified in\nPubMed and Web of Science, of which 29 were excluded and 24 were read and\nevaluated for eligibility, resulting in the use of seven studies for further\nanalysis. This chapter also includes a section with examples including an\nexample of a comparison between GPT-3 and state-of-the-art GPT-4 solutions in\nscientific text summarisation. Natural language processing has not yet reached\nits full potential in the generation of brief textual summaries. As there are\nacknowledged concerns that must be addressed, we can expect gradual\nintroduction of such models in practise.", "published": "2024-11-18 15:13:47", "link": "http://arxiv.org/abs/2411.11635v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing LLM Reasoning with Reward-guided Tree Search", "abstract": "Recently, test-time scaling has garnered significant attention from the\nresearch community, largely due to the substantial advancements of the o1 model\nreleased by OpenAI. By allocating more computational resources during the\ninference phase, large language models~(LLMs) can extensively explore the\nsolution space by generating more thought tokens or diverse solutions, thereby\nproducing more accurate responses. However, developing an o1-like reasoning\napproach is challenging, and researchers have been making various attempts to\nadvance this open area of research. In this paper, we present a preliminary\nexploration into enhancing the reasoning abilities of LLMs through\nreward-guided tree search algorithms. This framework is implemented by\nintegrating the policy model, reward model, and search algorithm. It is\nprimarily constructed around a tree search algorithm, where the policy model\nnavigates a dynamically expanding tree guided by a specially trained reward\nmodel. The implemented framework is denoted as \\textbf{STILL-1}. We thoroughly\nexplore various design considerations necessary for implementing this framework\nand provide a detailed report of the technical aspects. To assess the\neffectiveness of our approach, we focus on mathematical reasoning tasks and\nconduct extensive evaluations on four challenging datasets, significantly\nenhancing the reasoning abilities of LLMs.", "published": "2024-11-18 16:15:17", "link": "http://arxiv.org/abs/2411.11694v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FedCoLLM: A Parameter-Efficient Federated Co-tuning Framework for Large\n  and Small Language Models", "abstract": "By adapting Large Language Models (LLMs) to domain-specific tasks or\nenriching them with domain-specific knowledge, we can fully harness the\ncapabilities of LLMs. Nonetheless, a gap persists in achieving simultaneous\nmutual enhancement between the server's LLM and the downstream clients' Small\nLanguage Models (SLMs). To address this, we propose FedCoLLM, a novel and\nparameter-efficient federated framework designed for co-tuning LLMs and SLMs.\nThis approach is aimed at adaptively transferring server-side LLMs knowledge to\nclients' SLMs while simultaneously enriching the LLMs with domain insights from\nthe clients. To accomplish this, FedCoLLM utilizes lightweight adapters in\nconjunction with SLMs, facilitating knowledge exchange between server and\nclients in a manner that respects data privacy while also minimizing\ncomputational and communication overhead. Our evaluation of FedCoLLM, utilizing\nvarious public LLMs and SLMs across a range of NLP text generation tasks,\nreveals that the performance of clients' SLMs experiences notable improvements\nwith the assistance of the LLMs. Simultaneously, the LLMs enhanced via FedCoLLM\nachieves comparable performance to that obtained through direct fine-tuning on\nclients' data.", "published": "2024-11-18 16:34:58", "link": "http://arxiv.org/abs/2411.11707v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Moral Persuasion in Large Language Models: Evaluating Susceptibility and\n  Ethical Alignment", "abstract": "We explore how large language models (LLMs) can be influenced by prompting\nthem to alter their initial decisions and align them with established ethical\nframeworks. Our study is based on two experiments designed to assess the\nsusceptibility of LLMs to moral persuasion. In the first experiment, we examine\nthe susceptibility to moral ambiguity by evaluating a Base Agent LLM on morally\nambiguous scenarios and observing how a Persuader Agent attempts to modify the\nBase Agent's initial decisions. The second experiment evaluates the\nsusceptibility of LLMs to align with predefined ethical frameworks by prompting\nthem to adopt specific value alignments rooted in established philosophical\ntheories. The results demonstrate that LLMs can indeed be persuaded in morally\ncharged scenarios, with the success of persuasion depending on factors such as\nthe model used, the complexity of the scenario, and the conversation length.\nNotably, LLMs of distinct sizes but from the same company produced markedly\ndifferent outcomes, highlighting the variability in their susceptibility to\nethical persuasion.", "published": "2024-11-18 16:59:59", "link": "http://arxiv.org/abs/2411.11731v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CNMBERT: A Model for Converting Hanyu Pinyin Abbreviations to Chinese\n  Characters", "abstract": "The task of converting Hanyu Pinyin abbreviations to Chinese characters is a\nsignificant branch within the domain of Chinese Spelling Correction (CSC). It\nplays an important role in many downstream applications such as named entity\nrecognition and sentiment analysis. This task typically involves text-length\nalignment and seems easy to solve; however, due to the limited information\ncontent in pinyin abbreviations, achieving accurate conversion is challenging.\nIn this paper, we treat this as a fill-mask task and propose CNMBERT, which\nstands for zh-CN Pinyin Multi-mask BERT Model, as a solution to this issue. By\nintroducing a multi-mask strategy and Mixture of Experts (MoE) layers, CNMBERT\noutperforms fine-tuned GPT models and ChatGPT-4o with a 61.53% MRR score and\n51.86% accuracy on a 10,373-sample test dataset.", "published": "2024-11-18 17:50:34", "link": "http://arxiv.org/abs/2411.11770v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Bi-Mamba: Towards Accurate 1-Bit State Space Models", "abstract": "The typical selective state-space model (SSM) of Mamba addresses several\nlimitations of Transformers, such as quadratic computational complexity with\nsequence length and significant inference-time memory requirements due to the\nkey-value cache. However, the growing size of Mamba models continues to pose\ntraining and deployment challenges and raises environmental concerns due to\nconsiderable energy consumption. In this work, we introduce Bi-Mamba, a\nscalable and powerful 1-bit Mamba architecture designed for more efficient\nlarge language models with multiple sizes across 780M, 1.3B, and 2.7B. Bi-Mamba\nmodels are trained from scratch on data volume as regular LLM pertaining using\nan autoregressive distillation loss. Extensive experimental results on language\nmodeling demonstrate that Bi-Mamba achieves performance comparable to its\nfull-precision counterparts (e.g., FP16 or BF16) and much better accuracy than\npost-training-binarization (PTB) Mamba baselines, while significantly reducing\nmemory footprint and energy consumption compared to the original Mamba model.\nOur study pioneers a new linear computational complexity LLM framework under\nlow-bit representation and facilitates the future design of specialized\nhardware tailored for efficient 1-bit Mamba-based LLMs.", "published": "2024-11-18 18:59:15", "link": "http://arxiv.org/abs/2411.11843v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ByteScience: Bridging Unstructured Scientific Literature and Structured\n  Data with Auto Fine-tuned Large Language Model in Token Granularity", "abstract": "Natural Language Processing (NLP) is widely used to supply summarization\nability from long context to structured information. However, extracting\nstructured knowledge from scientific text by NLP models remains a challenge\nbecause of its domain-specific nature to complex data preprocessing and the\ngranularity of multi-layered device-level information. To address this, we\nintroduce ByteScience, a non-profit cloud-based auto fine-tuned Large Language\nModel (LLM) platform, which is designed to extract structured scientific data\nand synthesize new scientific knowledge from vast scientific corpora. The\nplatform capitalizes on DARWIN, an open-source, fine-tuned LLM dedicated to\nnatural science. The platform was built on Amazon Web Services (AWS) and\nprovides an automated, user-friendly workflow for custom model development and\ndata extraction. The platform achieves remarkable accuracy with only a small\namount of well-annotated articles. This innovative tool streamlines the\ntransition from the science literature to structured knowledge and data and\nbenefits the advancements in natural informatics.", "published": "2024-11-18 19:36:26", "link": "http://arxiv.org/abs/2411.12000v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mitigating Gender Bias in Contextual Word Embeddings", "abstract": "Word embeddings have been shown to produce remarkable results in tackling a\nvast majority of NLP related tasks. Unfortunately, word embeddings also capture\nthe stereotypical biases that are prevalent in society, affecting the\npredictive performance of the embeddings when used in downstream tasks. While\nvarious techniques have been proposed \\cite{bolukbasi2016man, zhao2018learning}\nand criticized\\cite{gonen2019lipstick} for static embeddings, very little work\nhas focused on mitigating bias in contextual embeddings. In this paper, we\npropose a novel objective function for MLM(Masked-Language Modeling) which\nlargely mitigates the gender bias in contextual embeddings and also preserves\nthe performance for downstream tasks. Since previous works on measuring bias in\ncontextual embeddings lack in normative reasoning, we also propose novel\nevaluation metrics that are straight-forward and aligned with our motivations\nin debiasing. We also propose new methods for debiasing static embeddings and\nprovide empirical proof via extensive analysis and experiments, as to why the\nmain source of bias in static embeddings stems from the presence of\nstereotypical names rather than gendered words themselves. All experiments and\nembeddings studied are in English, unless otherwise\nspecified.\\citep{bender2011achieving}.", "published": "2024-11-18 21:36:44", "link": "http://arxiv.org/abs/2411.12074v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Does Unlearning Truly Unlearn? A Black Box Evaluation of LLM Unlearning\n  Methods", "abstract": "Large language model unlearning aims to remove harmful information that LLMs\nhave learnt to prevent their use for malicious purposes. LLMU and RMU have been\nproposed as two methods for LLM unlearning, achieving impressive results on\nunlearning benchmarks. We study in detail the impact of unlearning on LLM\nperformance metrics using the WMDP dataset as well as a new biology dataset we\ncreate. We show that unlearning has a notable impact on general model\ncapabilities, with the performance degradation being more significant in\ngeneral for LLMU. We further test the robustness of the two methods and find\nthat doing 5-shot prompting or rephrasing the question in simple ways can lead\nto an over ten-fold increase in accuracy on unlearning benchmarks. Finally, we\nshow that training on unrelated data can almost completely recover\npre-unlearning performance, demonstrating that these methods fail at truly\nunlearning. Our methodology serves as an evaluation framework for LLM\nunlearning methods. The code is available at:\nhttps://github.com/JaiDoshi/Knowledge-Erasure.", "published": "2024-11-18 22:31:17", "link": "http://arxiv.org/abs/2411.12103v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mechanism and Emergence of Stacked Attention Heads in Multi-Layer\n  Transformers", "abstract": "In this paper, I introduce the retrieval problem, a simple yet common\nreasoning task that can be solved only by transformers with a minimum number of\nlayers, which grows logarithmically with the input size. I empirically show\nthat large language models can solve the task under different prompting\nformulations without any fine-tuning. To understand how transformers solve the\nretrieval problem, I train several transformers on a minimal formulation.\nSuccessful learning occurs only under the presence of an implicit curriculum. I\nuncover the learned mechanisms by studying the attention maps in the trained\ntransformers. I also study the training process, uncovering that attention\nheads always emerge in a specific sequence guided by the implicit curriculum.", "published": "2024-11-18 23:12:13", "link": "http://arxiv.org/abs/2411.12118v4", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Testing Uncertainty of Large Language Models for Physics Knowledge and\n  Reasoning", "abstract": "Large Language Models (LLMs) have gained significant popularity in recent\nyears for their ability to answer questions in various fields. However, these\nmodels have a tendency to \"hallucinate\" their responses, making it challenging\nto evaluate their performance. A major challenge is determining how to assess\nthe certainty of a model's predictions and how it correlates with accuracy. In\nthis work, we introduce an analysis for evaluating the performance of popular\nopen-source LLMs, as well as gpt-3.5 Turbo, on multiple choice physics\nquestionnaires. We focus on the relationship between answer accuracy and\nvariability in topics related to physics. Our findings suggest that most models\nprovide accurate replies in cases where they are certain, but this is by far\nnot a general behavior. The relationship between accuracy and uncertainty\nexposes a broad horizontal bell-shaped distribution. We report how the\nasymmetry between accuracy and uncertainty intensifies as the questions demand\nmore logical reasoning of the LLM agent, while the same relationship remains\nsharp for knowledge retrieval tasks.", "published": "2024-11-18 13:42:13", "link": "http://arxiv.org/abs/2411.14465v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Popular LLMs Amplify Race and Gender Disparities in Human Mobility", "abstract": "As large language models (LLMs) are increasingly applied in areas influencing\nsocietal outcomes, it is critical to understand their tendency to perpetuate\nand amplify biases. This study investigates whether LLMs exhibit biases in\npredicting human mobility -- a fundamental human behavior -- based on race and\ngender. Using three prominent LLMs -- GPT-4, Gemini, and Claude -- we analyzed\ntheir predictions of visitations to points of interest (POIs) for individuals,\nrelying on prompts that included names with and without explicit demographic\ndetails. We find that LLMs frequently reflect and amplify existing societal\nbiases. Specifically, predictions for minority groups were disproportionately\nskewed, with these individuals being significantly less likely to be associated\nwith wealth-related points of interest (POIs). Gender biases were also evident,\nas female individuals were consistently linked to fewer career-related POIs\ncompared to their male counterparts. These biased associations suggest that\nLLMs not only mirror but also exacerbate societal stereotypes, particularly in\ncontexts involving race and gender.", "published": "2024-11-18 19:41:20", "link": "http://arxiv.org/abs/2411.14469v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Language Model for Qualitative Research -- A Systematic Mapping\n  Study", "abstract": "The exponential growth of text-based data in domains such as healthcare,\neducation, and social sciences has outpaced the capacity of traditional\nqualitative analysis methods, which are time-intensive and prone to\nsubjectivity. Large Language Models (LLMs), powered by advanced generative AI,\nhave emerged as transformative tools capable of automating and enhancing\nqualitative analysis. This study systematically maps the literature on the use\nof LLMs for qualitative research, exploring their application contexts,\nconfigurations, methodologies, and evaluation metrics. Findings reveal that\nLLMs are utilized across diverse fields, demonstrating the potential to\nautomate processes traditionally requiring extensive human input. However,\nchallenges such as reliance on prompt engineering, occasional inaccuracies, and\ncontextual limitations remain significant barriers. This research highlights\nopportunities for integrating LLMs with human expertise, improving model\nrobustness, and refining evaluation methodologies. By synthesizing trends and\nidentifying research gaps, this study aims to guide future innovations in the\napplication of LLMs for qualitative analysis.", "published": "2024-11-18 21:28:00", "link": "http://arxiv.org/abs/2411.14473v4", "categories": ["cs.CL", "cs.AI", "I.2.7; I.2.10; H.3.3"], "primary_category": "cs.CL"}
{"title": "ToxiLab: How Well Do Open-Source LLMs Generate Synthetic Toxicity Data?", "abstract": "Effective toxic content detection relies heavily on high-quality and diverse\ndata, which serve as the foundation for robust content moderation models.\nSynthetic data has become a common approach for training models across various\nNLP tasks. However, its effectiveness remains uncertain for highly subjective\ntasks like hate speech detection, with previous research yielding mixed\nresults. This study explores the potential of open-source LLMs for harmful data\nsynthesis, utilizing controlled prompting and supervised fine-tuning techniques\nto enhance data quality and diversity. We systematically evaluated 6 open\nsource LLMs on 5 datasets, assessing their ability to generate diverse,\nhigh-quality harmful data while minimizing hallucination and duplication. Our\nresults show that Mistral consistently outperforms other open models, and\nsupervised fine-tuning significantly enhances data reliability and diversity.\nWe further analyze the trade-offs between prompt-based vs. fine-tuned toxic\ndata synthesis, discuss real-world deployment challenges, and highlight ethical\nconsiderations. Our findings demonstrate that fine-tuned open source LLMs\nprovide scalable and cost-effective solutions to augment toxic content\ndetection datasets, paving the way for more accessible and transparent content\nmoderation tools.", "published": "2024-11-18 00:21:14", "link": "http://arxiv.org/abs/2411.15175v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Causal Effect of Group Diversity on Redundancy and Coverage in\n  Peer-Reviewing", "abstract": "A large host of scientific journals and conferences solicit peer reviews from\nmultiple reviewers for the same submission, aiming to gather a broader range of\nperspectives and mitigate individual biases. In this work, we reflect on the\nrole of diversity in the slate of reviewers assigned to evaluate a submitted\npaper as a factor in diversifying perspectives and improving the utility of the\npeer-review process. We propose two measures for assessing review utility:\nreview coverage -- reviews should cover most contents of the paper -- and\nreview redundancy -- reviews should add information not already present in\nother reviews. We hypothesize that reviews from diverse reviewers will exhibit\nhigh coverage and low redundancy. We conduct a causal study of different\nmeasures of reviewer diversity on review coverage and redundancy using\nobservational data from a peer-reviewed conference with approximately 5,000\nsubmitted papers. Our study reveals disparate effects of different diversity\nmeasures on review coverage and redundancy. Our study finds that assigning a\ngroup of reviewers that are topically diverse, have different seniority levels,\nor have distinct publication networks leads to broader coverage of the paper or\nreview criteria, but we find no evidence of an increase in coverage for\nreviewer slates with reviewers from diverse organizations or geographical\nlocations. Reviewers from different organizations, seniority levels, topics, or\npublications networks (all except geographical diversity) lead to a decrease in\nredundancy in reviews. Furthermore, publication network-based diversity alone\nalso helps bring in varying perspectives (that is, low redundancy), even within\nspecific review criteria. Our study adopts a group decision-making perspective\nfor reviewer assignments in peer review and suggests dimensions of diversity\nthat can help guide the reviewer assignment process.", "published": "2024-11-18 10:08:10", "link": "http://arxiv.org/abs/2411.11437v1", "categories": ["cs.DL", "cs.CL", "stat.AP"], "primary_category": "cs.DL"}
{"title": "Search, Verify and Feedback: Towards Next Generation Post-training\n  Paradigm of Foundation Models via Verifier Engineering", "abstract": "The evolution of machine learning has increasingly prioritized the\ndevelopment of powerful models and more scalable supervision signals. However,\nthe emergence of foundation models presents significant challenges in providing\neffective supervision signals necessary for further enhancing their\ncapabilities. Consequently, there is an urgent need to explore novel\nsupervision signals and technical approaches. In this paper, we propose\nverifier engineering, a novel post-training paradigm specifically designed for\nthe era of foundation models. The core of verifier engineering involves\nleveraging a suite of automated verifiers to perform verification tasks and\ndeliver meaningful feedback to foundation models. We systematically categorize\nthe verifier engineering process into three essential stages: search, verify,\nand feedback, and provide a comprehensive review of state-of-the-art research\ndevelopments within each stage. We believe that verifier engineering\nconstitutes a fundamental pathway toward achieving Artificial General\nIntelligence.", "published": "2024-11-18 12:04:52", "link": "http://arxiv.org/abs/2411.11504v1", "categories": ["cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.AI"}
{"title": "The Power of Many: Multi-Agent Multimodal Models for Cultural Image\n  Captioning", "abstract": "Large Multimodal Models (LMMs) exhibit impressive performance across various\nmultimodal tasks. However, their effectiveness in cross-cultural contexts\nremains limited due to the predominantly Western-centric nature of most data\nand models. Conversely, multi-agent models have shown significant capability in\nsolving complex tasks. Our study evaluates the collective performance of LMMs\nin a multi-agent interaction setting for the novel task of cultural image\ncaptioning. Our contributions are as follows: (1) We introduce MosAIC, a\nMulti-Agent framework to enhance cross-cultural Image Captioning using LMMs\nwith distinct cultural personas; (2) We provide a dataset of culturally\nenriched image captions in English for images from China, India, and Romania\nacross three datasets: GeoDE, GD-VCR, CVQA; (3) We propose a culture-adaptable\nmetric for evaluating cultural information within image captions; and (4) We\nshow that the multi-agent interaction outperforms single-agent models across\ndifferent metrics, and offer valuable insights for future research. Our dataset\nand models can be accessed at https://github.com/MichiganNLP/MosAIC.", "published": "2024-11-18 17:37:10", "link": "http://arxiv.org/abs/2411.11758v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Drowning in Documents: Consequences of Scaling Reranker Inference", "abstract": "Rerankers, typically cross-encoders, are often used to re-score the documents\nretrieved by cheaper initial IR systems. This is because, though expensive,\nrerankers are assumed to be more effective. We challenge this assumption by\nmeasuring reranker performance for full retrieval, not just re-scoring\nfirst-stage retrieval. Our experiments reveal a surprising trend: the best\nexisting rerankers provide diminishing returns when scoring progressively more\ndocuments and actually degrade quality beyond a certain limit. In fact, in this\nsetting, rerankers can frequently assign high scores to documents with no\nlexical or semantic overlap with the query. We hope that our findings will spur\nfuture research to improve reranking.", "published": "2024-11-18 17:46:32", "link": "http://arxiv.org/abs/2411.11767v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Tackling prediction tasks in relational databases with LLMs", "abstract": "Though large language models (LLMs) have demonstrated exceptional performance\nacross numerous problems, their application to predictive tasks in relational\ndatabases remains largely unexplored. In this work, we address the notion that\nLLMs cannot yield satisfactory results on relational databases due to their\ninterconnected tables, complex relationships, and heterogeneous data types.\nUsing the recently introduced RelBench benchmark, we demonstrate that even a\nstraightforward application of LLMs achieves competitive performance on these\ntasks. These findings establish LLMs as a promising new baseline for ML on\nrelational databases and encourage further research in this direction.", "published": "2024-11-18 18:48:13", "link": "http://arxiv.org/abs/2411.11829v1", "categories": ["cs.LG", "cs.CL", "cs.DB"], "primary_category": "cs.LG"}
{"title": "Reviving Dormant Memories: Investigating Catastrophic Forgetting in\n  Language Models through Rationale-Guidance Difficulty", "abstract": "Although substantial efforts have been made to mitigate catastrophic\nforgetting in continual learning, the intrinsic mechanisms are not well\nunderstood. In this paper, we discover that when a forgetting model passively\nreceives an externally provided partial appropriate rationale, its performance\non the forgotten task can be restored. Furthermore, by simply adding a\ntask-agnostic prefix to the original instruction, the forgetting model can\nactively generate an appropriate rationale to reach the correct answer. These\nfindings suggest that the model does not actually ``forget'' the task\nknowledge; instead, the degraded performance can be attributed to the failure\nof the original instructions in guiding the model to generate the appropriate\nrationales. Based on this insight, we propose the Rationale-Guidance Difficulty\nmetric to evaluate how effectively a given instruction guides the model in\ngenerating appropriate rationales. We apply this metric to optimize the\nallocation of replay data in replay-based continual learning algorithm.\nExperimental results demonstrate that our data allocation method effectively\nmitigates catastrophic forgetting and maintains better model plasticity\nsimultaneously across models.", "published": "2024-11-18 14:28:04", "link": "http://arxiv.org/abs/2411.11932v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "METEOR: Evolutionary Journey of Large Language Models from Guidance to\n  Self-Growth", "abstract": "Model evolution enables learning from feedback to refine experiences and\nupdate skills, transforming models from having no domain knowledge to becoming\ndomain experts. However, there is currently no unified and effective method for\nguiding this evolutionary process. To address this gap, we propose the Meteor\nmethod, which includes three training phases: weak-to-strong data distillation,\niterative training, and self-evolution strategies. Each phase maximizes the\nmodel's inherent domain capabilities, allowing it to autonomously refine its\ndomain knowledge and enhance performance. Experiments demonstrate that our\napproach significantly improves accuracy, completeness, relevance, coherence,\nand reliability across domain-specific tasks.", "published": "2024-11-18 15:09:50", "link": "http://arxiv.org/abs/2411.11933v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Understanding Chain-of-Thought in LLMs through Information Theory", "abstract": "Large Language Models (LLMs) have shown impressive performance in complex\nreasoning tasks through Chain-of-Thought (CoT) reasoning, allowing models to\nbreak down problems into manageable sub-tasks. However, existing CoT evaluation\ntechniques either require annotated CoT data or fall short in accurately\nassessing intermediate reasoning steps, leading to high rates of false\npositives. In this paper, we formalize CoT reasoning in LLMs through an\ninformation-theoretic lens. Specifically, our framework quantifies the\n`information gain' at each reasoning step, enabling the identification of\nfailure modes in LLMs without the need for expensive annotated datasets. We\ndemonstrate the efficacy of our approach through extensive experiments on toy\nand GSM-8K data, where it significantly outperforms existing outcome-based\nmethods by providing more accurate insights into model performance on\nindividual tasks.", "published": "2024-11-18 19:14:36", "link": "http://arxiv.org/abs/2411.11984v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Benchmarking pre-trained text embedding models in aligning built asset\n  information", "abstract": "Accurate mapping of the built asset information to established data\nclassification systems and taxonomies is crucial for effective asset\nmanagement, whether for compliance at project handover or ad-hoc data\nintegration scenarios. Due to the complex nature of built asset data, which\npredominantly comprises technical text elements, this process remains largely\nmanual and reliant on domain expert input. Recent breakthroughs in contextual\ntext representation learning (text embedding), particularly through pre-trained\nlarge language models, offer promising approaches that can facilitate the\nautomation of cross-mapping of the built asset data. However, no comprehensive\nevaluation has yet been conducted to assess these models' ability to\neffectively represent the complex semantics specific to built asset technical\nterminology. This study presents a comparative benchmark of state-of-the-art\ntext embedding models to evaluate their effectiveness in aligning built asset\ninformation with domain-specific technical concepts. Our proposed datasets are\nderived from two renowned built asset data classification dictionaries. The\nresults of our benchmarking across six proposed datasets, covering three tasks\nof clustering, retrieval, and reranking, highlight the need for future research\non domain adaptation techniques. The benchmarking resources are published as an\nopen-source library, which will be maintained and extended to support future\nevaluations in this field.", "published": "2024-11-18 20:54:17", "link": "http://arxiv.org/abs/2411.12056v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Suicide Risk Assessment on Social Media with Semi-Supervised Learning", "abstract": "With social media communities increasingly becoming places where suicidal\nindividuals post and congregate, natural language processing presents an\nexciting avenue for the development of automated suicide risk assessment\nsystems. However, past efforts suffer from a lack of labeled data and class\nimbalances within the available labeled data. To accommodate this task's\nimperfect data landscape, we propose a semi-supervised framework that leverages\nlabeled (n=500) and unlabeled (n=1,500) data and expands upon the self-training\nalgorithm with a novel pseudo-label acquisition process designed to handle\nimbalanced datasets. To further ensure pseudo-label quality, we manually verify\na subset of the pseudo-labeled data that was not predicted unanimously across\nmultiple trials of pseudo-label generation. We test various models to serve as\nthe backbone for this framework, ultimately deciding that RoBERTa performs the\nbest. Ultimately, by leveraging partially validated pseudo-labeled data in\naddition to ground-truth labeled data, we substantially improve our model's\nability to assess suicide risk from social media posts.", "published": "2024-11-18 02:43:05", "link": "http://arxiv.org/abs/2411.12767v2", "categories": ["cs.CL", "cs.AI", "cs.SI"], "primary_category": "cs.CL"}
{"title": "CROW: Eliminating Backdoors from Large Language Models via Internal\n  Consistency Regularization", "abstract": "Recent studies reveal that Large Language Models (LLMs) are susceptible to\nbackdoor attacks, where adversaries embed hidden triggers that manipulate model\nresponses. Existing backdoor defense methods are primarily designed for vision\nor classification tasks, and are thus ineffective for text generation tasks,\nleaving LLMs vulnerable. We introduce Internal Consistency Regularization\n(CROW), a novel defense using consistency regularization finetuning to address\nlayer-wise inconsistencies caused by backdoor triggers. CROW leverages the\nintuition that clean models exhibit smooth, consistent transitions in hidden\nrepresentations across layers, whereas backdoored models show noticeable\nfluctuation when triggered. By enforcing internal consistency through\nadversarial perturbations and regularization, CROW neutralizes backdoor effects\nwithout requiring clean reference models or prior trigger knowledge, relying\nonly on a small set of clean data. This makes it practical for deployment\nacross various LLM architectures. Experimental results demonstrate that CROW\nconsistently achieves a significant reductions in attack success rates across\ndiverse backdoor strategies and tasks, including negative sentiment, targeted\nrefusal, and code injection, on models such as Llama-2 (7B, 13B), CodeLlama\n(7B, 13B) and Mistral-7B, while preserving the model's generative capabilities.", "published": "2024-11-18 07:52:12", "link": "http://arxiv.org/abs/2411.12768v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improved GUI Grounding via Iterative Narrowing", "abstract": "Graphical User Interface (GUI) grounding plays a crucial role in enhancing\nthe capabilities of Vision-Language Model (VLM) agents. While general VLMs,\nsuch as GPT-4V, demonstrate strong performance across various tasks, their\nproficiency in GUI grounding remains suboptimal. Recent studies have focused on\nfine-tuning these models specifically for zero-shot GUI grounding, yielding\nsignificant improvements over baseline performance. We introduce a visual\nprompting framework that employs an iterative narrowing mechanism to further\nimprove the performance of both general and fine-tuned models in GUI grounding.\nFor evaluation, we tested our method on a comprehensive benchmark comprising\nvarious UI platforms and provided the code to reproduce our results.", "published": "2024-11-18 05:47:12", "link": "http://arxiv.org/abs/2411.13591v5", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Learning to Ask: Conversational Product Search via Representation\n  Learning", "abstract": "Online shopping platforms, such as Amazon and AliExpress, are increasingly\nprevalent in society, helping customers purchase products conveniently. With\nrecent progress in natural language processing, researchers and practitioners\nshift their focus from traditional product search to conversational product\nsearch. Conversational product search enables user-machine conversations and\nthrough them collects explicit user feedback that allows to actively clarify\nthe users' product preferences. Therefore, prospective research on an\nintelligent shopping assistant via conversations is indispensable. Existing\npublications on conversational product search either model conversations\nindependently from users, queries, and products or lead to a vocabulary\nmismatch. In this work, we propose a new conversational product search model,\nConvPS, to assist users in locating desirable items. The model is first trained\nto jointly learn the semantic representations of user, query, item, and\nconversation via a unified generative framework. After learning these\nrepresentations, they are integrated to retrieve the target items in the latent\nsemantic space. Meanwhile, we propose a set of greedy and explore-exploit\nstrategies to learn to ask the user a sequence of high-performance questions\nfor conversations. Our proposed ConvPS model can naturally integrate the\nrepresentation learning of the user, query, item, and conversation into a\nunified generative framework, which provides a promising avenue for\nconstructing accurate and robust conversational product search systems that are\nflexible and adaptive. Experimental results demonstrate that our ConvPS model\nsignificantly outperforms state-of-the-art baselines.", "published": "2024-11-18 14:05:43", "link": "http://arxiv.org/abs/2411.14466v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Exploring the Potential Role of Generative AI in the TRAPD Procedure for\n  Survey Translation", "abstract": "This paper explores and assesses in what ways generative AI can assist in\ntranslating survey instruments. Writing effective survey questions is a\nchallenging and complex task, made even more difficult for surveys that will be\ntranslated and deployed in multiple linguistic and cultural settings.\nTranslation errors can be detrimental, with known errors rendering data\nunusable for its intended purpose and undetected errors leading to incorrect\nconclusions. A growing number of institutions face this problem as surveys\ndeployed by private and academic organizations globalize, and the success of\ntheir current efforts depends heavily on researchers' and translators'\nexpertise and the amount of time each party has to contribute to the task.\nThus, multilinguistic and multicultural surveys produced by teams with limited\nexpertise, budgets, or time are at significant risk for translation-based\nerrors in their data. We implement a zero-shot prompt experiment using ChatGPT\nto explore generative AI's ability to identify features of questions that might\nbe difficult to translate to a linguistic audience other than the source\nlanguage. We find that ChatGPT can provide meaningful feedback on translation\nissues, including common source survey language, inconsistent\nconceptualization, sensitivity and formality issues, and nonexistent concepts.\nIn addition, we provide detailed information on the practicality of the\napproach, including accessing the necessary software, associated costs, and\ncomputational run times. Lastly, based on our findings, we propose avenues for\nfuture research that integrate AI into survey translation practices.", "published": "2024-11-18 20:53:58", "link": "http://arxiv.org/abs/2411.14472v2", "categories": ["cs.CL", "stat.AP", "stat.ME"], "primary_category": "cs.CL"}
{"title": "PerfCodeGen: Improving Performance of LLM Generated Code with Execution\n  Feedback", "abstract": "Large Language Models (LLMs) are widely adopted for assisting in software\ndevelopment tasks, yet their performance evaluations have narrowly focused on\nthe functional correctness of generated code. Human programmers, however,\nrequire LLM-generated code to be not only correct but also optimally efficient.\nWe propose PerfCodeGen, a training-free framework that enhances the performance\nof LLM-generated code by incorporating feedback based on runtime during test\ncase execution into the self-refinement iterations. With PerfCodeGen, we\nachieve speedups for a significantly higher proportion of problems compared to\nusing the base LLM with sophisticated prompting techniques. Applied to open\nlanguage models like Phi-3-mini, PerfCodeGen achieves runtime efficiency\ncomparable to prompting powerful closed models like GPT-4. We achieve\nstate-of-the-art runtime efficiency on benchmarks such as HumanEval, MBPP, and\nAPPS, frequently surpassing the ground truth reference solutions with\nPerfCodeGen using GPT-3.5 and GPT-4. Additionally, we demonstrate the\neffectiveness of our approach in enhancing code quality across a range of open\nLLMs of varying sizes including Phi-3-mini, Llama 3 8B, Mixtral 8x7B, Command\nR, and Llama 3 70B.", "published": "2024-11-18 06:22:38", "link": "http://arxiv.org/abs/2412.03578v1", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.PL"], "primary_category": "cs.SE"}
{"title": "Understanding Student Sentiment on Mental Health Support in Colleges\n  Using Large Language Models", "abstract": "Mental health support in colleges is vital in educating students by offering\ncounseling services and organizing supportive events. However, evaluating its\neffectiveness faces challenges like data collection difficulties and lack of\nstandardized metrics, limiting research scope. Student feedback is crucial for\nevaluation but often relies on qualitative analysis without systematic\ninvestigation using advanced machine learning methods. This paper uses public\nStudent Voice Survey data to analyze student sentiments on mental health\nsupport with large language models (LLMs). We created a sentiment analysis\ndataset, SMILE-College, with human-machine collaboration. The investigation of\nboth traditional machine learning methods and state-of-the-art LLMs showed the\nbest performance of GPT-3.5 and BERT on this new dataset. The analysis\nhighlights challenges in accurately predicting response sentiments and offers\npractical insights on how LLMs can enhance mental health-related research and\nimprove college mental health services. This data-driven approach will\nfacilitate efficient and informed mental health support evaluation, management,\nand decision-making.", "published": "2024-11-18 02:53:15", "link": "http://arxiv.org/abs/2412.04326v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "SAMOS: A Neural MOS Prediction Model Leveraging Semantic Representations\n  and Acoustic Features", "abstract": "Assessing the naturalness of speech using mean opinion score (MOS) prediction\nmodels has positive implications for the automatic evaluation of speech\nsynthesis systems. Early MOS prediction models took the raw waveform or\namplitude spectrum of speech as input, whereas more advanced methods employed\nself-supervised-learning (SSL) based models to extract semantic representations\nfrom speech for MOS prediction. These methods utilized limited aspects of\nspeech information for MOS prediction, resulting in restricted prediction\naccuracy. Therefore, in this paper, we propose SAMOS, a MOS prediction model\nthat leverages both Semantic and Acoustic information of speech to be assessed.\nSpecifically, the proposed SAMOS leverages a pretrained wav2vec2 to extract\nsemantic representations and uses the feature extractor of a pretrained\nBiVocoder to extract acoustic features. These two types of features are then\nfed into the prediction network, which includes multi-task heads and an\naggregation layer, to obtain the final MOS score. Experimental results\ndemonstrate that the proposed SAMOS outperforms current state-of-the-art MOS\nprediction models on the BVCC dataset and performs comparable performance on\nthe BC2019 dataset, according to the results of system-level evaluation\nmetrics.", "published": "2024-11-18 01:54:51", "link": "http://arxiv.org/abs/2411.11232v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ESTVocoder: An Excitation-Spectral-Transformed Neural Vocoder\n  Conditioned on Mel Spectrogram", "abstract": "This paper proposes ESTVocoder, a novel excitation-spectral-transformed\nneural vocoder within the framework of source-filter theory. The ESTVocoder\ntransforms the amplitude and phase spectra of the excitation into the\ncorresponding speech amplitude and phase spectra using a neural filter whose\nbackbone is ConvNeXt v2 blocks. Finally, the speech waveform is reconstructed\nthrough the inverse short-time Fourier transform (ISTFT). The excitation is\nconstructed based on the F0: for voiced segments, it contains full harmonic\ninformation, while for unvoiced segments, it is represented by noise. The\nexcitation provides the filter with prior knowledge of the amplitude and phase\npatterns, expecting to reduce the modeling difficulty compared to conventional\nneural vocoders. To ensure the fidelity of the synthesized speech, an\nadversarial training strategy is applied to ESTVocoder with multi-scale and\nmulti-resolution discriminators. Analysis-synthesis and text-to-speech\nexperiments both confirm that our proposed ESTVocoder outperforms or is\ncomparable to other baseline neural vocoders, e.g., HiFi-GAN, SiFi-GAN, and\nVocos, in terms of synthesized speech quality, with a reasonable model\ncomplexity and generation speed. Additional analysis experiments also\ndemonstrate that the introduced excitation effectively accelerates the model's\nconvergence process, thanks to the speech spectral prior information contained\nin the excitation.", "published": "2024-11-18 03:22:34", "link": "http://arxiv.org/abs/2411.11258v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Uncovering the role of semantic and acoustic cues in normal and dichotic\n  listening", "abstract": "Despite extensive research, the precise role of acoustic and semantic cues in\ncomplex speech perception tasks remains unclear. In this study, we propose a\nparadigm to understand the encoding of these cues in electroencephalogram (EEG)\ndata, using match-mismatch (MM) classification task. The MM task involves\ndetermining whether the stimulus and response correspond to each other or not.\nWe design a multi-modal sequence model, based on long short term memory (LSTM)\narchitecture, to perform the MM task. The model is input with acoustic stimulus\n(derived from the speech envelope), semantic stimulus (derived from textual\nrepresentations of the speech content), and neural response (derived from the\nEEG data). Our experiments are performed on two separate conditions, i) natural\npassive listening condition and, ii) an auditory attention based dichotic\nlistening condition. Using the MM task as the analysis framework, we observe\nthat - a) speech perception is fragmented based on word boundaries, b) acoustic\nand semantic cues offer similar levels of MM task performance in natural\nlistening conditions, and c) semantic cues offer significantly improved MM\nclassification over acoustic cues in dichotic listening task. Further, the\nstudy provides evidence of right ear advantage in dichotic listening\nconditions.", "published": "2024-11-18 06:03:39", "link": "http://arxiv.org/abs/2411.11308v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "An Investigation of Reprogramming for Cross-Language Adaptation in\n  Speaker Verification Systems", "abstract": "Language mismatch is among the most common and challenging domain mismatches\nin deploying speaker verification (SV) systems. Adversarial reprogramming has\nshown promising results in cross-language adaptation for SV. The reprogramming\nis implemented by padding learnable parameters on the two sides of input speech\nsignals. In this paper, we investigate the relationship between the number of\npadded parameters and the performance of the reprogrammed models. Sufficient\nexperiments are conducted with different scales of SV models and datasets. The\nresults demonstrate that reprogramming consistently improves the performance of\ncross-language SV, while the improvement is saturated or even degraded when\nusing larger padding lengths. The performance is mainly determined by the\ncapacity of the original SV models instead of the number of padded parameters.\nThe SV models with larger scales have higher upper bounds in performance and\ncan endure longer padding without performance degradation.", "published": "2024-11-18 07:47:21", "link": "http://arxiv.org/abs/2411.11353v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Bandpass Twin-T Active Filter Used in the Buchla 200 Electric Music\n  Box Synthesizer", "abstract": "This paper analyzes an unusual active bandpass filter employed in the Buchla\nModel 295 10 Channel Comb Filter, a synthesizer module developed as part of the\nBuchla 200 Electric Music Box by Donald Buchla. The filter consists of a\npeculiar rearrangement of elements in a classic Twin-T configuration; to our\nknowledge, it has not been previously addressed in the literature. As an\nexample, we explore its specific application in the Model 295.", "published": "2024-11-18 08:08:03", "link": "http://arxiv.org/abs/2411.11358v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Using voice analysis as an early indicator of risk for depression in\n  young adults", "abstract": "Increasingly frequent publications in the literature report voice quality\ndifferences between depressed patients and controls. Here, we examine the\npossibility of using voice analysis as an early warning signal for the\ndevelopment of emotion disturbances in young adults. As part of a major\ninterdisciplinary European research project in four countries (ECoWeB),\nexamining the effects of web-based prevention programs to reduce the risk for\ndepression in young adults, we analyzed a large number of acoustic voice\ncharacteristics in vocal reports of emotions experienced by the participants on\na specific day. We were able to identify a number of significant differences in\nacoustic cues, particularly with respect to the energy distribution in the\nvoice spectrum, encouraging further research efforts to develop promising\nnon-obtrusive risk indicators in the normal speaking voice. This is\nparticularly important in the case of young adults who are less likely to\nexhibit standard risk factors for depression such as negative life experiences.", "published": "2024-11-18 12:57:43", "link": "http://arxiv.org/abs/2411.11541v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Vision Language Models Are Few-Shot Audio Spectrogram Classifiers", "abstract": "We demonstrate that vision language models (VLMs) are capable of recognizing\nthe content in audio recordings when given corresponding spectrogram images.\nSpecifically, we instruct VLMs to perform audio classification tasks in a\nfew-shot setting by prompting them to classify a spectrogram image given\nexample spectrogram images of each class. By carefully designing the\nspectrogram image representation and selecting good few-shot examples, we show\nthat GPT-4o can achieve 59.00% cross-validated accuracy on the ESC-10\nenvironmental sound classification dataset. Moreover, we demonstrate that VLMs\ncurrently outperform the only available commercial audio language model with\naudio understanding capabilities (Gemini-1.5) on the equivalent audio\nclassification task (59.00% vs. 49.62%), and even perform slightly better than\nhuman experts on visual spectrogram classification (73.75% vs. 72.50% on first\nfold). We envision two potential use cases for these findings: (1) combining\nthe spectrogram and language understanding capabilities of VLMs for audio\ncaption augmentation, and (2) posing visual spectrogram classification as a\nchallenge task for VLMs.", "published": "2024-11-18 20:56:44", "link": "http://arxiv.org/abs/2411.12058v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The Sound of Water: Inferring Physical Properties from Pouring Liquids", "abstract": "We study the connection between audio-visual observations and the underlying\nphysics of a mundane yet intriguing everyday activity: pouring liquids. Given\nonly the sound of liquid pouring into a container, our objective is to\nautomatically infer physical properties such as the liquid level, the shape and\nsize of the container, the pouring rate and the time to fill. To this end, we:\n(i) show in theory that these properties can be determined from the fundamental\nfrequency (pitch); (ii) train a pitch detection model with supervision from\nsimulated data and visual data with a physics-inspired objective; (iii)\nintroduce a new large dataset of real pouring videos for a systematic study;\n(iv) show that the trained model can indeed infer these physical properties for\nreal data; and finally, (v) we demonstrate strong generalization to various\ncontainer shapes, other datasets, and in-the-wild YouTube videos. Our work\npresents a keen understanding of a narrow yet rich problem at the intersection\nof acoustics, physics, and learning. It opens up applications to enhance\nmultisensory perception in robotic pouring.", "published": "2024-11-18 01:19:37", "link": "http://arxiv.org/abs/2411.11222v2", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Study of the Performance of CEEMDAN in Underdetermined Speech Separation", "abstract": "The CEEMDAN algorithm is one of the modern methods used in the analysis of\nnon-stationary signals. This research presents a study of the effectiveness of\nthis method in audio source separation to know the limits of its work. It\nconcluded two conditions related to frequencies and amplitudes of mixed signals\nto be separated by CEEMDAN. The performance of the algorithm in separating\nnoise from speech and separating speech signals from each other is studied. The\nresearch reached a conclusion that CEEMDAN can remove some types of noise from\nspeech (speech improvement), and it cannot separate speech signals from each\nother (cocktail party). Simulation is done using Matlab environment and Noizeus\ndatabase.", "published": "2024-11-18 06:13:51", "link": "http://arxiv.org/abs/2411.11312v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Do Captioning Metrics Reflect Music Semantic Alignment?", "abstract": "Music captioning has emerged as a promising task, fueled by the advent of\nadvanced language generation models. However, the evaluation of music\ncaptioning relies heavily on traditional metrics such as BLEU, METEOR, and\nROUGE which were developed for other domains, without proper justification for\ntheir use in this new field. We present cases where traditional metrics are\nvulnerable to syntactic changes, and show they do not correlate well with human\njudgments. By addressing these issues, we aim to emphasize the need for a\ncritical reevaluation of how music captions are assessed.", "published": "2024-11-18 16:13:49", "link": "http://arxiv.org/abs/2411.11692v1", "categories": ["cs.SD", "cs.IR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Compression of Higher Order Ambisonics with Multichannel RVQGAN", "abstract": "A multichannel extension to the RVQGAN neural coding method is proposed, and\nrealized for data-driven compression of third-order Ambisonics audio. The\ninput- and output layers of the generator and discriminator models are modified\nto accept multiple (16) channels without increasing the model bitrate. We also\npropose a loss function for accounting for spatial perception in immersive\nreproduction, and transfer learning from single-channel models. Listening test\nresults with 7.1.4 immersive playback show that the proposed extension is\nsuitable for coding scene-based, 16-channel Ambisonics content with good\nquality at 16 kbps when trained and tested on the EigenScape database. The\nmodel has potential applications for learning other types of content and\nmultichannel formats.", "published": "2024-11-18 19:48:18", "link": "http://arxiv.org/abs/2411.12008v3", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Attention-guided Spectrogram Sequence Modeling with CNNs for Music Genre\n  Classification", "abstract": "Music genre classification is a critical component of music recommendation\nsystems, generation algorithms, and cultural analytics. In this work, we\npresent an innovative model for classifying music genres using attention-based\ntemporal signature modeling. By processing spectrogram sequences through\nConvolutional Neural Networks (CNNs) and multi-head attention layers, our\napproach captures the most temporally significant moments within each piece,\ncrafting a unique \"signature\" for genre identification. This temporal focus not\nonly enhances classification accuracy but also reveals insights into\ngenre-specific characteristics that can be intuitively mapped to listener\nperceptions. Our findings offer potential applications in personalized music\nrecommendation systems by highlighting cross-genre similarities and\ndistinctiveness, aligning closely with human musical intuition. This work\nbridges the gap between technical classification tasks and the nuanced, human\nexperience of genre.", "published": "2024-11-18 21:57:03", "link": "http://arxiv.org/abs/2411.14474v1", "categories": ["cs.SD", "cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
