{"title": "The Voynich Manuscript is Written in Natural Language: The Pahlavi\n  Hypothesis", "abstract": "The late medieval Voynich Manuscript (VM) has resisted decryption and was\nconsidered a meaningless hoax or an unsolvable cipher. Here, we provide\nevidence that the VM is written in natural language by establishing a relation\nof the Voynich alphabet and the Iranian Pahlavi script. Many of the Voynich\ncharacters are upside-down versions of their Pahlavi counterparts, which may be\nan effect of different writing directions. Other Voynich letters can be\nexplained as ligatures or departures from Pahlavi with the intent to cope with\nknown problems due to the stupendous ambiguity of Pahlavi text. While a\ntranslation of the VM text is not attempted here, we can confirm the\nVoynich-Pahlavi relation at the character level by the transcription of many\nwords from the VM illustrations and from parts of the main text. Many of the\ntranscribed words can be identified as terms from Zoroastrian cosmology which\nis in line with the use of Pahlavi script in Zoroastrian communities from\nmedieval times.", "published": "2017-09-06 00:15:37", "link": "http://arxiv.org/abs/1709.01634v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Neural Language Model for Dynamically Representing the Meanings of\n  Unknown Words and Entities in a Discourse", "abstract": "This study addresses the problem of identifying the meaning of unknown words\nor entities in a discourse with respect to the word embedding approaches used\nin neural language models. We proposed a method for on-the-fly construction and\nexploitation of word embeddings in both the input and output layers of a neural\nmodel by tracking contexts. This extends the dynamic entity representation used\nin Kobayashi et al. (2016) and incorporates a copy mechanism proposed\nindependently by Gu et al. (2016) and Gulcehre et al. (2016). In addition, we\nconstruct a new task and dataset called Anonymized Language Modeling for\nevaluating the ability to capture word meanings while reading. Experiments\nconducted using our novel dataset show that the proposed variant of RNN\nlanguage model outperformed the baseline model. Furthermore, the experiments\nalso demonstrate that dynamic updates of an output layer help a model predict\nreappearing entities, whereas those of an input layer are effective to predict\nwords following reappearing entities.", "published": "2017-09-06 05:23:37", "link": "http://arxiv.org/abs/1709.01679v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Information-Propogation-Enhanced Neural Machine Translation by Relation\n  Model", "abstract": "Even though sequence-to-sequence neural machine translation (NMT) model have\nachieved state-of-art performance in the recent fewer years, but it is widely\nconcerned that the recurrent neural network (RNN) units are very hard to\ncapture the long-distance state information, which means RNN can hardly find\nthe feature with long term dependency as the sequence becomes longer.\nSimilarly, convolutional neural network (CNN) is introduced into NMT for\nspeeding recently, however, CNN focus on capturing the local feature of the\nsequence; To relieve this issue, we incorporate a relation network into the\nstandard encoder-decoder framework to enhance information-propogation in neural\nnetwork, ensuring that the information of the source sentence can flow into the\ndecoder adequately. Experiments show that proposed framework outperforms the\nstatistical MT model and the state-of-art NMT model significantly on two data\nsets with different scales.", "published": "2017-09-06 11:13:50", "link": "http://arxiv.org/abs/1709.01766v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Depression and Self-Harm Risk Assessment in Online Forums", "abstract": "Users suffering from mental health conditions often turn to online resources\nfor support, including specialized online support communities or general\ncommunities such as Twitter and Reddit. In this work, we present a neural\nframework for supporting and studying users in both types of communities. We\npropose methods for identifying posts in support communities that may indicate\na risk of self-harm, and demonstrate that our approach outperforms strong\npreviously proposed methods for identifying such posts. Self-harm is closely\nrelated to depression, which makes identifying depressed users on general\nforums a crucial related task. We introduce a large-scale general forum dataset\n(\"RSDD\") consisting of users with self-reported depression diagnoses matched\nwith control users. We show how our method can be applied to effectively\nidentify depressed users from their use of language alone. We demonstrate that\nour method outperforms strong baselines on this general forum dataset.", "published": "2017-09-06 14:50:42", "link": "http://arxiv.org/abs/1709.01848v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"Having 2 hours to write a paper is fun!\": Detecting Sarcasm in\n  Numerical Portions of Text", "abstract": "Sarcasm occurring due to the presence of numerical portions in text has been\nquoted as an error made by automatic sarcasm detection approaches in the past.\nWe present a first study in detecting sarcasm in numbers, as in the case of the\nsentence 'Love waking up at 4 am'. We analyze the challenges of the problem,\nand present Rule-based, Machine Learning and Deep Learning approaches to detect\nsarcasm in numerical portions of text. Our Deep Learning approach outperforms\nfour past works for sarcasm detection and Rule-based and Machine learning\napproaches on a dataset of tweets, obtaining an F1-score of 0.93. This shows\nthat special attention to text containing numbers may be useful to improve\nstate-of-the-art in sarcasm detection.", "published": "2017-09-06 18:09:15", "link": "http://arxiv.org/abs/1709.01950v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semi-Supervised Recurrent Neural Network for Adverse Drug Reaction\n  Mention Extraction", "abstract": "Social media is an useful platform to share health-related information due to\nits vast reach. This makes it a good candidate for public-health monitoring\ntasks, specifically for pharmacovigilance. We study the problem of extraction\nof Adverse-Drug-Reaction (ADR) mentions from social media, particularly from\ntwitter. Medical information extraction from social media is challenging,\nmainly due to short and highly information nature of text, as compared to more\ntechnical and formal medical reports.\n  Current methods in ADR mention extraction relies on supervised learning\nmethods, which suffers from labeled data scarcity problem. The State-of-the-art\nmethod uses deep neural networks, specifically a class of Recurrent Neural\nNetwork (RNN) which are Long-Short-Term-Memory networks (LSTMs)\n\\cite{hochreiter1997long}. Deep neural networks, due to their large number of\nfree parameters relies heavily on large annotated corpora for learning the end\ntask. But in real-world, it is hard to get large labeled data, mainly due to\nheavy cost associated with manual annotation. Towards this end, we propose a\nnovel semi-supervised learning based RNN model, which can leverage unlabeled\ndata also present in abundance on social media. Through experiments we\ndemonstrate the effectiveness of our method, achieving state-of-the-art\nperformance in ADR mention extraction.", "published": "2017-09-06 06:42:22", "link": "http://arxiv.org/abs/1709.01687v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Spoken English Intelligibility Remediation with PocketSphinx Alignment\n  and Feature Extraction Improves Substantially over the State of the Art", "abstract": "We use automatic speech recognition to assess spoken English learner\npronunciation based on the authentic intelligibility of the learners' spoken\nresponses determined from support vector machine (SVM) classifier or deep\nlearning neural network model predictions of transcription correctness. Using\nnumeric features produced by PocketSphinx alignment mode and many recognition\npasses searching for the substitution and deletion of each expected phoneme and\ninsertion of unexpected phonemes in sequence, the SVM models achieve 82 percent\nagreement with the accuracy of Amazon Mechanical Turk crowdworker\ntranscriptions, up from 75 percent reported by multiple independent\nresearchers. Using such features with SVM classifier probability prediction\nmodels can help computer-aided pronunciation teaching (CAPT) systems provide\nintelligibility remediation.", "published": "2017-09-06 08:27:59", "link": "http://arxiv.org/abs/1709.01713v3", "categories": ["cs.CL", "stat.ML", "68T10, 97U50", "I.2.7; K.3.1"], "primary_category": "cs.CL"}
{"title": "Measuring the Similarity of Sentential Arguments in Dialog", "abstract": "When people converse about social or political topics, similar arguments are\noften paraphrased by different speakers, across many different conversations.\nDebate websites produce curated summaries of arguments on such topics; these\nsummaries typically consist of lists of sentences that represent frequently\nparaphrased propositions, or labels capturing the essence of one particular\naspect of an argument, e.g. Morality or Second Amendment. We call these\nfrequently paraphrased propositions ARGUMENT FACETS. Like these curated sites,\nour goal is to induce and identify argument facets across multiple\nconversations, and produce summaries. However, we aim to do this automatically.\nWe frame the problem as consisting of two steps: we first extract sentences\nthat express an argument from raw social media dialogs, and then rank the\nextracted arguments in terms of their similarity to one another. Sets of\nsimilar arguments are used to represent argument facets. We show here that we\ncan predict ARGUMENT FACET SIMILARITY with a correlation averaging 0.63\ncompared to a human topline averaging 0.68 over three debate topics, easily\nbeating several reasonable baselines.", "published": "2017-09-06 17:15:49", "link": "http://arxiv.org/abs/1709.01887v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Neural Machine Translation with Latent Tree Attention", "abstract": "Building models that take advantage of the hierarchical structure of language\nwithout a priori annotation is a longstanding goal in natural language\nprocessing. We introduce such a model for the task of machine translation,\npairing a recurrent neural network grammar encoder with a novel attentional\nRNNG decoder and applying policy gradient reinforcement learning to induce\nunsupervised tree structures on both the source and target. When trained on\ncharacter-level datasets with no explicit segmentation or parse annotation, the\nmodel learns a plausible segmentation and shallow parse, obtaining performance\nclose to an attentional baseline.", "published": "2017-09-06 17:44:53", "link": "http://arxiv.org/abs/1709.01915v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Conditional Generative Adversarial Networks for Speech Enhancement and\n  Noise-Robust Speaker Verification", "abstract": "Improving speech system performance in noisy environments remains a\nchallenging task, and speech enhancement (SE) is one of the effective\ntechniques to solve the problem. Motivated by the promising results of\ngenerative adversarial networks (GANs) in a variety of image processing tasks,\nwe explore the potential of conditional GANs (cGANs) for SE, and in particular,\nwe make use of the image processing framework proposed by Isola et al. [1] to\nlearn a mapping from the spectrogram of noisy speech to an enhanced\ncounterpart. The SE cGAN consists of two networks, trained in an adversarial\nmanner: a generator that tries to enhance the input noisy spectrogram, and a\ndiscriminator that tries to distinguish between enhanced spectrograms provided\nby the generator and clean ones from the database using the noisy spectrogram\nas a condition. We evaluate the performance of the cGAN method in terms of\nperceptual evaluation of speech quality (PESQ), short-time objective\nintelligibility (STOI), and equal error rate (EER) of speaker verification (an\nexample application). Experimental results show that the cGAN method overall\noutperforms the classical short-time spectral amplitude minimum mean square\nerror (STSA-MMSE) SE algorithm, and is comparable to a deep neural\nnetwork-based SE approach (DNN-SE).", "published": "2017-09-06 07:51:18", "link": "http://arxiv.org/abs/1709.01703v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP", "stat.ML"], "primary_category": "eess.AS"}
