{"title": "Latent Dirichlet Allocation Based Organisation of Broadcast Media\n  Archives for Deep Neural Network Adaptation", "abstract": "This paper presents a new method for the discovery of latent domains in\ndiverse speech data, for the use of adaptation of Deep Neural Networks (DNNs)\nfor Automatic Speech Recognition. Our work focuses on transcription of\nmulti-genre broadcast media, which is often only categorised broadly in terms\nof high level genres such as sports, news, documentary, etc. However, in terms\nof acoustic modelling these categories are coarse. Instead, it is expected that\na mixture of latent domains can better represent the complex and diverse\nbehaviours within a TV show, and therefore lead to better and more robust\nperformance. We propose a new method, whereby these latent domains are\ndiscovered with Latent Dirichlet Allocation, in an unsupervised manner. These\nare used to adapt DNNs using the Unique Binary Code (UBIC) representation for\nthe LDA domains. Experiments conducted on a set of BBC TV broadcasts, with more\nthan 2,000 shows for training and 47 shows for testing, show that the use of\nLDA-UBIC DNNs reduces the error up to 13% relative compared to the baseline\nhybrid DNN models.", "published": "2015-11-16 18:25:33", "link": "http://arxiv.org/abs/1511.05076v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Programmer: Inducing Latent Programs with Gradient Descent", "abstract": "Deep neural networks have achieved impressive supervised classification\nperformance in many tasks including image recognition, speech recognition, and\nsequence to sequence learning. However, this success has not been translated to\napplications like question answering that may involve complex arithmetic and\nlogic reasoning. A major limitation of these models is in their inability to\nlearn even simple arithmetic and logic operations. For example, it has been\nshown that neural networks fail to learn to add two binary numbers reliably. In\nthis work, we propose Neural Programmer, an end-to-end differentiable neural\nnetwork augmented with a small set of basic arithmetic and logic operations.\nNeural Programmer can call these augmented operations over several steps,\nthereby inducing compositional programs that are more complex than the built-in\noperations. The model learns from a weak supervision signal which is the result\nof execution of the correct program, hence it does not require expensive\nannotation of the correct program itself. The decisions of what operations to\ncall, and what data segments to apply to are inferred by Neural Programmer.\nSuch decisions, during training, are done in a differentiable fashion so that\nthe entire network can be trained jointly by gradient descent. We find that\ntraining the model is difficult, but it can be greatly improved by adding\nrandom noise to the gradient. On a fairly complex synthetic table-comprehension\ndataset, traditional recurrent networks and attentional models perform poorly\nwhile Neural Programmer typically obtains nearly perfect accuracy.", "published": "2015-11-16 06:03:58", "link": "http://arxiv.org/abs/1511.04834v3", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "A Neural Transducer", "abstract": "Sequence-to-sequence models have achieved impressive results on various\ntasks. However, they are unsuitable for tasks that require incremental\npredictions to be made as more data arrives or tasks that have long input\nsequences and output sequences. This is because they generate an output\nsequence conditioned on an entire input sequence. In this paper, we present a\nNeural Transducer that can make incremental predictions as more input arrives,\nwithout redoing the entire computation. Unlike sequence-to-sequence models, the\nNeural Transducer computes the next-step distribution conditioned on the\npartially observed input sequence and the partially generated sequence. At each\ntime step, the transducer can decide to emit zero to many output symbols. The\ndata can be processed using an encoder and presented as input to the\ntransducer. The discrete decision to emit a symbol at every time step makes it\ndifficult to learn with conventional backpropagation. It is however possible to\ntrain the transducer by using a dynamic programming algorithm to generate\ntarget discrete decisions. Our experiments show that the Neural Transducer\nworks well in settings where it is required to produce output predictions as\ndata come in. We also find that the Neural Transducer performs well for long\nsequences even when attention mechanisms are not used.", "published": "2015-11-16 08:53:44", "link": "http://arxiv.org/abs/1511.04868v4", "categories": ["cs.LG", "cs.CL", "cs.NE"], "primary_category": "cs.LG"}
{"title": "Sherlock: Scalable Fact Learning in Images", "abstract": "We study scalable and uniform understanding of facts in images. Existing\nvisual recognition systems are typically modeled differently for each fact type\nsuch as objects, actions, and interactions. We propose a setting where all\nthese facts can be modeled simultaneously with a capacity to understand\nunbounded number of facts in a structured way. The training data comes as\nstructured facts in images, including (1) objects (e.g., $<$boy$>$), (2)\nattributes (e.g., $<$boy, tall$>$), (3) actions (e.g., $<$boy, playing$>$), and\n(4) interactions (e.g., $<$boy, riding, a horse $>$). Each fact has a semantic\nlanguage view (e.g., $<$ boy, playing$>$) and a visual view (an image with this\nfact). We show that learning visual facts in a structured way enables not only\na uniform but also generalizable visual understanding. We propose and\ninvestigate recent and strong approaches from the multiview learning literature\nand also introduce two learning representation models as potential baselines.\nWe applied the investigated methods on several datasets that we augmented with\nstructured facts and a large scale dataset of more than 202,000 facts and\n814,000 images. Our experiments show the advantage of relating facts by the\nstructure by the proposed models compared to the designed baselines on\nbidirectional fact retrieval.", "published": "2015-11-16 09:56:04", "link": "http://arxiv.org/abs/1511.04891v4", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Yin and Yang: Balancing and Answering Binary Visual Questions", "abstract": "The complex compositional structure of language makes problems at the\nintersection of vision and language challenging. But language also provides a\nstrong prior that can result in good superficial performance, without the\nunderlying models truly understanding the visual content. This can hinder\nprogress in pushing state of art in the computer vision aspects of multi-modal\nAI. In this paper, we address binary Visual Question Answering (VQA) on\nabstract scenes. We formulate this problem as visual verification of concepts\ninquired in the questions. Specifically, we convert the question to a tuple\nthat concisely summarizes the visual concept to be detected in the image. If\nthe concept can be found in the image, the answer to the question is \"yes\", and\notherwise \"no\". Abstract scenes play two roles (1) They allow us to focus on\nthe high-level semantics of the VQA task as opposed to the low-level\nrecognition problems, and perhaps more importantly, (2) They provide us the\nmodality to balance the dataset such that language priors are controlled, and\nthe role of vision is essential. In particular, we collect fine-grained pairs\nof scenes for every question, such that the answer to the question is \"yes\" for\none scene, and \"no\" for the other for the exact same question. Indeed, language\npriors alone do not perform better than chance on our balanced dataset.\nMoreover, our proposed approach matches the performance of a state-of-the-art\nVQA approach on the unbalanced dataset, and outperforms it on the balanced\ndataset.", "published": "2015-11-16 19:38:14", "link": "http://arxiv.org/abs/1511.05099v5", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning about Spanish dialects through Twitter", "abstract": "This paper maps the large-scale variation of the Spanish language by\nemploying a corpus based on geographically tagged Twitter messages. Lexical\ndialects are extracted from an analysis of variants of tens of concepts. The\nresulting maps show linguistic variation on an unprecedented scale across the\nglobe. We discuss the properties of the main dialects within a machine learning\napproach and find that varieties spoken in urban areas have an international\ncharacter in contrast to country areas where dialects show a more regional\nuniformity.", "published": "2015-11-16 14:29:38", "link": "http://arxiv.org/abs/1511.04970v2", "categories": ["stat.ML", "cs.CL", "cs.CY", "physics.soc-ph", "stat.AP"], "primary_category": "stat.ML"}
