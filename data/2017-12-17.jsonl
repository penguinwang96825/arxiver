{"title": "Query-Based Abstractive Summarization Using Neural Networks", "abstract": "In this paper, we present a model for generating summaries of text documents\nwith respect to a query. This is known as query-based summarization. We adapt\nan existing dataset of news article summaries for the task and train a\npointer-generator model using this dataset. The generated summaries are\nevaluated by measuring similarity to reference summaries. Our results show that\na neural network summarization model, similar to existing neural network models\nfor abstractive summarization, can be constructed to make use of queries to\nproduce targeted summaries.", "published": "2017-12-17 12:42:51", "link": "http://arxiv.org/abs/1712.06100v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards a science of human stories: using sentiment analysis and\n  emotional arcs to understand the building blocks of complex social systems", "abstract": "Given the growing assortment of sentiment measuring instruments, it is\nimperative to understand which aspects of sentiment dictionaries contribute to\nboth their classification accuracy and their ability to provide richer\nunderstanding of texts. Here, we perform detailed, quantitative tests and\nqualitative assessments of 6 dictionary-based methods applied, and briefly\nexamine a further 20 methods. We show that while inappropriate for sentences,\ndictionary-based methods are generally robust in their classification accuracy\nfor longer texts.\n  Stories often following distinct emotional trajectories, forming patterns\nthat are meaningful to us. By classifying the emotional arcs for a filtered\nsubset of 4,803 stories from Project Gutenberg's fiction collection, we find a\nset of six core trajectories which form the building blocks of complex\nnarratives. Of profound scientific interest will be the degree to which we can\neventually understand the full landscape of human stories, and data driven\napproaches will play a crucial role.\n  Finally, we utilize web-scale data from Twitter to study the limits of what\nsocial data can tell us about public health, mental illness, discourse around\nthe protest movement of #BlackLivesMatter, discourse around climate change, and\nhidden networks. We conclude with a review of published works in complex\nsystems that separately analyze charitable donations, the happiness of words in\n10 languages, 100 years of daily temperature data across the United States, and\nAustralian Rules Football games.", "published": "2017-12-17 19:33:36", "link": "http://arxiv.org/abs/1712.06163v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DeepNorm-A Deep Learning Approach to Text Normalization", "abstract": "This paper presents an simple yet sophisticated approach to the challenge by\nSproat and Jaitly (2016)- given a large corpus of written text aligned to its\nnormalized spoken form, train an RNN to learn the correct normalization\nfunction. Text normalization for a token seems very straightforward without\nit's context. But given the context of the used token and then normalizing\nbecomes tricky for some classes. We present a novel approach in which the\nprediction of our classification algorithm is used by our sequence to sequence\nmodel to predict the normalized text of the input token. Our approach takes\nvery less time to learn and perform well unlike what has been reported by\nGoogle (5 days on their GPU cluster). We have achieved an accuracy of 97.62\nwhich is impressive given the resources we use. Our approach is using the best\nof both worlds, gradient boosting - state of the art in most classification\ntasks and sequence to sequence learning - state of the art in machine\ntranslation. We present our experiments and report results with various\nparameter settings.", "published": "2017-12-17 18:31:26", "link": "http://arxiv.org/abs/1712.06994v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Benford's Law and First Letter of Word", "abstract": "A universal First-Letter Law (FLL) is derived and described. It predicts the\npercentages of first letters for words in novels. The FLL is akin to Benford's\nlaw (BL) of first digits, which predicts the percentages of first digits in a\ndata collection of numbers. Both are universal in the sense that FLL only\ndepends on the numbers of letters in the alphabet, whereas BL only depends on\nthe number of digits in the base of the number system. The existence of these\ntypes of universal laws appears counter-intuitive. Nonetheless both describe\ndata very well. Relations to some earlier works are given. FLL predicts that an\nEnglish author on the average starts about 16 out of 100 words with the English\nletter `t'. This is corroborated by data, yet an author can freely write\nanything. Fuller implications and the applicability of FLL remain for the\nfuture.", "published": "2017-12-17 08:53:13", "link": "http://arxiv.org/abs/1712.06074v1", "categories": ["cs.CL", "physics.soc-ph"], "primary_category": "cs.CL"}
{"title": "Probabilistic Semantic Retrieval for Surveillance Videos with Activity\n  Graphs", "abstract": "We present a novel framework for finding complex activities matching\nuser-described queries in cluttered surveillance videos. The wide diversity of\nqueries coupled with unavailability of annotated activity data limits our\nability to train activity models. To bridge the semantic gap we propose to let\nusers describe an activity as a semantic graph with object attributes and\ninter-object relationships associated with nodes and edges, respectively. We\nlearn node/edge-level visual predictors during training and, at test-time,\npropose to retrieve activity by identifying likely locations that match the\nsemantic graph. We formulate a novel CRF based probabilistic activity\nlocalization objective that accounts for mis-detections, mis-classifications\nand track-losses, and outputs a likelihood score for a candidate grounded\nlocation of the query in the video. We seek groundings that maximize overall\nprecision and recall. To handle the combinatorial search over all\nhigh-probability groundings, we propose a highest precision subgraph matching\nalgorithm. Our method outperforms existing retrieval methods on benchmarked\ndatasets.", "published": "2017-12-17 23:11:28", "link": "http://arxiv.org/abs/1712.06204v2", "categories": ["cs.MM", "cs.CL"], "primary_category": "cs.MM"}
{"title": "Deep Learning for Distant Speech Recognition", "abstract": "Deep learning is an emerging technology that is considered one of the most\npromising directions for reaching higher levels of artificial intelligence.\nAmong the other achievements, building computers that understand speech\nrepresents a crucial leap towards intelligent machines. Despite the great\nefforts of the past decades, however, a natural and robust human-machine speech\ninteraction still appears to be out of reach, especially when users interact\nwith a distant microphone in noisy and reverberant environments. The latter\ndisturbances severely hamper the intelligibility of a speech signal, making\nDistant Speech Recognition (DSR) one of the major open challenges in the field.\n  This thesis addresses the latter scenario and proposes some novel techniques,\narchitectures, and algorithms to improve the robustness of distant-talking\nacoustic models. We first elaborate on methodologies for realistic data\ncontamination, with a particular emphasis on DNN training with simulated data.\nWe then investigate on approaches for better exploiting speech contexts,\nproposing some original methodologies for both feed-forward and recurrent\nneural networks. Lastly, inspired by the idea that cooperation across different\nDNNs could be the key for counteracting the harmful effects of noise and\nreverberation, we propose a novel deep learning paradigm called network of deep\nneural networks. The analysis of the original concepts were based on extensive\nexperimental validations conducted on both real and simulated data, considering\ndifferent corpora, microphone configurations, environments, noisy conditions,\nand ASR tasks.", "published": "2017-12-17 10:29:15", "link": "http://arxiv.org/abs/1712.06086v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
