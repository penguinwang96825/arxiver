{"title": "Two-Stage Fine-Tuning: A Novel Strategy for Learning Class-Imbalanced\n  Data", "abstract": "Classification on long-tailed distributed data is a challenging problem,\nwhich suffers from serious class-imbalance and hence poor performance on tail\nclasses with only a few samples. Owing to this paucity of samples, learning on\nthe tail classes is especially challenging for the fine-tuning when\ntransferring a pretrained model to a downstream task. In this work, we present\na simple modification of standard fine-tuning to cope with these challenges.\nSpecifically, we propose a two-stage fine-tuning: we first fine-tune the final\nlayer of the pretrained model with class-balanced reweighting loss, and then we\nperform the standard fine-tuning. Our modification has several benefits: (1) it\nleverages pretrained representations by only fine-tuning a small portion of the\nmodel parameters while keeping the rest untouched; (2) it allows the model to\nlearn an initial representation of the specific task; and importantly (3) it\nprotects the learning of tail classes from being at a disadvantage during the\nmodel updating. We conduct extensive experiments on synthetic datasets of both\ntwo-class and multi-class tasks of text classification as well as a real-world\napplication to ADME (i.e., absorption, distribution, metabolism, and excretion)\nsemantic labeling. The experimental results show that the proposed two-stage\nfine-tuning outperforms both fine-tuning with conventional loss and fine-tuning\nwith a reweighting loss on the above datasets.", "published": "2022-07-22 03:39:51", "link": "http://arxiv.org/abs/2207.10858v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge-Grounded Conversational Data Augmentation with Generative\n  Conversational Networks", "abstract": "While rich, open-domain textual data are generally available and may include\ninteresting phenomena (humor, sarcasm, empathy, etc.) most are designed for\nlanguage processing tasks, and are usually in a non-conversational format. In\nthis work, we take a step towards automatically generating conversational data\nusing Generative Conversational Networks, aiming to benefit from the breadth of\navailable language and knowledge data, and train open domain social\nconversational agents. We evaluate our approach on conversations with and\nwithout knowledge on the Topical Chat dataset using automatic metrics and human\nevaluators. Our results show that for conversations without knowledge\ngrounding, GCN can generalize from the seed data, producing novel conversations\nthat are less relevant but more engaging and for knowledge-grounded\nconversations, it can produce more knowledge-focused, fluent, and engaging\nconversations. Specifically, we show that for open-domain conversations with\n10\\% of seed data, our approach performs close to the baseline that uses 100%\nof the data, while for knowledge-grounded conversations, it achieves the same\nusing only 1% of the data, on human ratings of engagingness, fluency, and\nrelevance.", "published": "2022-07-22 22:37:14", "link": "http://arxiv.org/abs/2207.11363v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Level Fine-Tuning, Data Augmentation, and Few-Shot Learning for\n  Specialized Cyber Threat Intelligence", "abstract": "Gathering cyber threat intelligence from open sources is becoming\nincreasingly important for maintaining and achieving a high level of security\nas systems become larger and more complex. However, these open sources are\noften subject to information overload. It is therefore useful to apply machine\nlearning models that condense the amount of information to what is necessary.\nYet, previous studies and applications have shown that existing classifiers are\nnot able to extract specific information about emerging cybersecurity events\ndue to their low generalization ability. Therefore, we propose a system to\novercome this problem by training a new classifier for each new incident. Since\nthis requires a lot of labelled data using standard training methods, we\ncombine three different low-data regime techniques - transfer learning, data\naugmentation, and few-shot learning - to train a high-quality classifier from\nvery few labelled instances. We evaluated our approach using a novel dataset\nderived from the Microsoft Exchange Server data breach of 2021 which was\nlabelled by three experts. Our findings reveal an increase in F1 score of more\nthan 21 points compared to standard training methods and more than 18 points\ncompared to a state-of-the-art method in few-shot learning. Furthermore, the\nclassifier trained with this method and 32 instances is only less than 5 F1\nscore points worse than a classifier trained with 1800 instances.", "published": "2022-07-22 13:34:28", "link": "http://arxiv.org/abs/2207.11076v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "ASR Error Detection via Audio-Transcript entailment", "abstract": "Despite improved performances of the latest Automatic Speech Recognition\n(ASR) systems, transcription errors are still unavoidable. These errors can\nhave a considerable impact in critical domains such as healthcare, when used to\nhelp with clinical documentation. Therefore, detecting ASR errors is a critical\nfirst step in preventing further error propagation to downstream applications.\nTo this end, we propose a novel end-to-end approach for ASR error detection\nusing audio-transcript entailment. To the best of our knowledge, we are the\nfirst to frame this problem as an end-to-end entailment task between the audio\nsegment and its corresponding transcript segment. Our intuition is that there\nshould be a bidirectional entailment between audio and transcript when there is\nno recognition error and vice versa. The proposed model utilizes an acoustic\nencoder and a linguistic encoder to model the speech and transcript\nrespectively. The encoded representations of both modalities are fused to\npredict the entailment. Since doctor-patient conversations are used in our\nexperiments, a particular emphasis is placed on medical terms. Our proposed\nmodel achieves classification error rates (CER) of 26.2% on all transcription\nerrors and 23% on medical errors specifically, leading to improvements upon a\nstrong baseline by 12% and 15.4%, respectively.", "published": "2022-07-22 02:47:15", "link": "http://arxiv.org/abs/2207.10849v1", "categories": ["cs.CL", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Assessing mortality prediction through different representation models\n  based on concepts extracted from clinical notes", "abstract": "Recent years have seen particular interest in using electronic medical\nrecords (EMRs) for secondary purposes to enhance the quality and safety of\nhealthcare delivery. EMRs tend to contain large amounts of valuable clinical\nnotes. Learning of embedding is a method for converting notes into a format\nthat makes them comparable. Transformer-based representation models have\nrecently made a great leap forward. These models are pre-trained on large\nonline datasets to understand natural language texts effectively. The quality\nof a learning embedding is influenced by how clinical notes are used as input\nto representation models. A clinical note has several sections with different\nlevels of information value. It is also common for healthcare providers to use\ndifferent expressions for the same concept. Existing methods use clinical notes\ndirectly or with an initial preprocessing as input to representation models.\nHowever, to learn a good embedding, we identified the most essential clinical\nnotes section. We then mapped the extracted concepts from selected sections to\nthe standard names in the Unified Medical Language System (UMLS). We used the\nstandard phrases corresponding to the unique concepts as input for clinical\nmodels. We performed experiments to measure the usefulness of the learned\nembedding vectors in the task of hospital mortality prediction on a subset of\nthe publicly available Medical Information Mart for Intensive Care (MIMIC-III)\ndataset. According to the experiments, clinical transformer-based\nrepresentation models produced better results with getting input generated by\nstandard names of extracted unique concepts compared to other input formats.\nThe best-performing models were BioBERT, PubMedBERT, and UmlsBERT,\nrespectively.", "published": "2022-07-22 04:34:33", "link": "http://arxiv.org/abs/2207.10872v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "68P20", "H.1; J.3"], "primary_category": "cs.CL"}
{"title": "Lagrangian Method for Q-Function Learning (with Applications to Machine\n  Translation)", "abstract": "This paper discusses a new approach to the fundamental problem of learning\noptimal Q-functions. In this approach, optimal Q-functions are formulated as\nsaddle points of a nonlinear Lagrangian function derived from the classic\nBellman optimality equation. The paper shows that the Lagrangian enjoys strong\nduality, in spite of its nonlinearity, which paves the way to a general\nLagrangian method to Q-function learning. As a demonstration, the paper\ndevelops an imitation learning algorithm based on the duality theory, and\napplies the algorithm to a state-of-the-art machine translation benchmark. The\npaper then turns to demonstrate a symmetry breaking phenomenon regarding the\noptimality of the Lagrangian saddle points, which justifies a largely\noverlooked direction in developing the Lagrangian method.", "published": "2022-07-22 15:57:52", "link": "http://arxiv.org/abs/2207.11161v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Toward Fairness in Speech Recognition: Discovery and mitigation of\n  performance disparities", "abstract": "As for other forms of AI, speech recognition has recently been examined with\nrespect to performance disparities across different user cohorts. One approach\nto achieve fairness in speech recognition is to (1) identify speaker cohorts\nthat suffer from subpar performance and (2) apply fairness mitigation measures\ntargeting the cohorts discovered. In this paper, we report on initial findings\nwith both discovery and mitigation of performance disparities using data from a\nproduct-scale AI assistant speech recognition system. We compare cohort\ndiscovery based on geographic and demographic information to a more scalable\nmethod that groups speakers without human labels, using speaker embedding\ntechnology. For fairness mitigation, we find that oversampling of\nunderrepresented cohorts, as well as modeling speaker cohort membership by\nadditional input variables, reduces the gap between top- and bottom-performing\ncohorts, without deteriorating overall recognition accuracy.", "published": "2022-07-22 21:33:29", "link": "http://arxiv.org/abs/2207.11345v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Panoptic Scene Graph Generation", "abstract": "Existing research addresses scene graph generation (SGG) -- a critical\ntechnology for scene understanding in images -- from a detection perspective,\ni.e., objects are detected using bounding boxes followed by prediction of their\npairwise relationships. We argue that such a paradigm causes several problems\nthat impede the progress of the field. For instance, bounding box-based labels\nin current datasets usually contain redundant classes like hairs, and leave out\nbackground information that is crucial to the understanding of context. In this\nwork, we introduce panoptic scene graph generation (PSG), a new problem task\nthat requires the model to generate a more comprehensive scene graph\nrepresentation based on panoptic segmentations rather than rigid bounding\nboxes. A high-quality PSG dataset, which contains 49k well-annotated\noverlapping images from COCO and Visual Genome, is created for the community to\nkeep track of its progress. For benchmarking, we build four two-stage\nbaselines, which are modified from classic methods in SGG, and two one-stage\nbaselines called PSGTR and PSGFormer, which are based on the efficient\nTransformer-based detector, i.e., DETR. While PSGTR uses a set of queries to\ndirectly learn triplets, PSGFormer separately models the objects and relations\nin the form of queries from two Transformer decoders, followed by a\nprompting-like relation-object matching mechanism. In the end, we share\ninsights on open challenges and future directions.", "published": "2022-07-22 17:59:53", "link": "http://arxiv.org/abs/2207.11247v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "primary_category": "cs.CV"}
{"title": "PanGu-Coder: Program Synthesis with Function-Level Language Modeling", "abstract": "We present PanGu-Coder, a pretrained decoder-only language model adopting the\nPanGu-Alpha architecture for text-to-code generation, i.e. the synthesis of\nprogramming language solutions given a natural language problem description. We\ntrain PanGu-Coder using a two-stage strategy: the first stage employs Causal\nLanguage Modelling (CLM) to pre-train on raw programming language data, while\nthe second stage uses a combination of Causal Language Modelling and Masked\nLanguage Modelling (MLM) training objectives that focus on the downstream task\nof text-to-code generation and train on loosely curated pairs of natural\nlanguage program definitions and code functions. Finally, we discuss\nPanGu-Coder-FT, which is fine-tuned on a combination of competitive programming\nproblems and code with continuous integration tests. We evaluate PanGu-Coder\nwith a focus on whether it generates functionally correct programs and\ndemonstrate that it achieves equivalent or better performance than similarly\nsized models, such as CodeX, while attending a smaller context window and\ntraining on less data.", "published": "2022-07-22 18:08:16", "link": "http://arxiv.org/abs/2207.11280v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.PL", "cs.SE"], "primary_category": "cs.LG"}
{"title": "DNN-Free Low-Latency Adaptive Speech Enhancement Based on Frame-Online\n  Beamforming Powered by Block-Online FastMNMF", "abstract": "This paper describes a practical dual-process speech enhancement system that\nadapts environment-sensitive frame-online beamforming (front-end) with help\nfrom environment-free block-online source separation (back-end). To use minimum\nvariance distortionless response (MVDR) beamforming, one may train a deep\nneural network (DNN) that estimates time-frequency masks used for computing the\ncovariance matrices of sources (speech and noise). Backpropagation-based\nrun-time adaptation of the DNN was proposed for dealing with the mismatched\ntraining-test conditions. Instead, one may try to directly estimate the source\ncovariance matrices with a state-of-the-art blind source separation method\ncalled fast multichannel non-negative matrix factorization (FastMNMF). In\npractice, however, neither the DNN nor the FastMNMF can be updated in a\nframe-online manner due to its computationally-expensive iterative nature. Our\nDNN-free system leverages the posteriors of the latest source spectrograms\ngiven by block-online FastMNMF to derive the current source covariance matrices\nfor frame-online beamforming. The evaluation shows that our frame-online system\ncan quickly respond to scene changes caused by interfering speaker movements\nand outperformed an existing block-online system with DNN-based beamforming by\n5.0 points in terms of the word error rate.", "published": "2022-07-22 08:17:00", "link": "http://arxiv.org/abs/2207.10934v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Physics-informed convolutional neural network with bicubic spline\n  interpolation for sound field estimation", "abstract": "A sound field estimation method based on a physics-informed convolutional\nneural network (PICNN) using spline interpolation is proposed. Most of the\nsound field estimation methods are based on wavefunction expansion, making the\nestimated function satisfy the Helmholtz equation. However, these methods rely\nonly on physical properties; thus, they suffer from a significant deterioration\nof accuracy when the number of measurements is small. Recent learning-based\nmethods based on neural networks have advantages in estimating from sparse\nmeasurements when training data are available. However, since physical\nproperties are not taken into consideration, the estimated function can be a\nphysically infeasible solution. We propose the application of PICNN to the\nsound field estimation problem by using a loss function that penalizes\ndeviation from the Helmholtz equation. Since the output of CNN is a spatially\ndiscretized pressure distribution, it is difficult to directly evaluate the\nHelmholtz-equation loss function. Therefore, we incorporate bicubic spline\ninterpolation in the PICNN framework. Experimental results indicated that\naccurate and physically feasible estimation from sparse measurements can be\nachieved with the proposed method.", "published": "2022-07-22 08:27:44", "link": "http://arxiv.org/abs/2207.10937v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Head-Related Transfer Function Interpolation from Spatially Sparse\n  Measurements Using Autoencoder with Source Position Conditioning", "abstract": "We propose a method of head-related transfer function (HRTF) interpolation\nfrom sparsely measured HRTFs using an autoencoder with source position\nconditioning. The proposed method is drawn from an analogy between an HRTF\ninterpolation method based on regularized linear regression (RLR) and an\nautoencoder. Through this analogy, we found the key feature of the RLR-based\nmethod that HRTFs are decomposed into source-position-dependent and\nsource-position-independent factors. On the basis of this finding, we design\nthe encoder and decoder so that their weights and biases are generated from\nsource positions. Furthermore, we introduce an aggregation module that reduces\nthe dependence of latent variables on source position for obtaining a\nsource-position-independent representation of each subject. Numerical\nexperiments show that the proposed method can work well for unseen subjects and\nachieve an interpolation performance with only one-eighth measurements\ncomparable to that of the RLR-based method.", "published": "2022-07-22 09:24:58", "link": "http://arxiv.org/abs/2207.10967v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Inference skipping for more efficient real-time speech enhancement with\n  parallel RNNs", "abstract": "Deep neural network (DNN) based speech enhancement models have attracted\nextensive attention due to their promising performance. However, it is\ndifficult to deploy a powerful DNN in real-time applications because of its\nhigh computational cost. Typical compression methods such as pruning and\nquantization do not make good use of the data characteristics. In this paper,\nwe introduce the Skip-RNN strategy into speech enhancement models with parallel\nRNNs. The states of the RNNs update intermittently without interrupting the\nupdate of the output mask, which leads to significant reduction of\ncomputational load without evident audio artifacts. To better leverage the\ndifference between the voice and the noise, we further regularize the skipping\nstrategy with voice activity detection (VAD) guidance, saving more\ncomputational load. Experiments on a high-performance speech enhancement model,\ndual-path convolutional recurrent network (DPCRN), show the superiority of our\nstrategy over strategies like network pruning or directly training a smaller\nmodel. We also validate the generalization of the proposed strategy on two\nother competitive speech enhancement models.", "published": "2022-07-22 14:34:11", "link": "http://arxiv.org/abs/2207.11108v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
