{"title": "Attention Boosted Sequential Inference Model", "abstract": "Attention mechanism has been proven effective on natural language processing.\nThis paper proposes an attention boosted natural language inference model named\naESIM by adding word attention and adaptive direction-oriented attention\nmechanisms to the traditional Bi-LSTM layer of natural language inference\nmodels, e.g. ESIM. This makes the inference model aESIM has the ability to\neffectively learn the representation of words and model the local subsentential\ninference between pairs of premise and hypothesis. The empirical studies on the\nSNLI, MultiNLI and Quora benchmarks manifest that aESIM is superior to the\noriginal ESIM model.", "published": "2018-12-05 07:24:51", "link": "http://arxiv.org/abs/1812.01840v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Medical Short Text Classification with Semantic Expansion\n  Using Word-Cluster Embedding", "abstract": "Automatic text classification (TC) research can be used for real-world\nproblems such as the classification of in-patient discharge summaries and\nmedical text reports, which is beneficial to make medical documents more\nunderstandable to doctors. However, in electronic medical records (EMR), the\ntexts containing sentences are shorter than that in general domain, which leads\nto the lack of semantic features and the ambiguity of semantic. To tackle this\nchallenge, we propose to add word-cluster embedding to deep neural network for\nimproving short text classification. Concretely, we first use hierarchical\nagglomerative clustering to cluster the word vectors in the semantic space.\nThen we calculate the cluster center vector which represents the implicit topic\ninformation of words in the cluster. Finally, we expand word vector with\ncluster center vector, and implement classifiers using CNN and LSTM\nrespectively. To evaluate the performance of our proposed method, we conduct\nexperiments on public data sets TREC and the medical short sentences data sets\nwhich is constructed and released by us. The experimental results demonstrate\nthat our proposed method outperforms state-of-the-art baselines in short\nsentence classification on both medical domain and general domain.", "published": "2018-12-05 10:02:59", "link": "http://arxiv.org/abs/1812.01885v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Inflection-Tolerant Ontology-Based Named Entity Recognition for\n  Real-Time Applications", "abstract": "A growing number of applications users daily interact with have to operate in\n(near) real-time: chatbots, digital companions, knowledge work support systems\n-- just to name a few. To perform the services desired by the user, these\nsystems have to analyze user activity logs or explicit user input extremely\nfast. In particular, text content (e.g. in form of text snippets) needs to be\nprocessed in an information extraction task. Regarding the aforementioned\ntemporal requirements, this has to be accomplished in just a few milliseconds,\nwhich limits the number of methods that can be applied. Practically, only very\nfast methods remain, which on the other hand deliver worse results than slower\nbut more sophisticated Natural Language Processing (NLP) pipelines. In this\npaper, we investigate and propose methods for real-time capable Named Entity\nRecognition (NER). As a first improvement step we address are word variations\ninduced by inflection, for example present in the German language. Our approach\nis ontology-based and makes use of several language information sources like\nWiktionary. We evaluated it using the German Wikipedia (about 9.4B characters),\nfor which the whole NER process took considerably less than an hour. Since\nprecision and recall are higher than with comparably fast methods, we conclude\nthat the quality gap between high speed methods and sophisticated NLP pipelines\ncan be narrowed a bit more without losing too much runtime performance.", "published": "2018-12-05 17:17:30", "link": "http://arxiv.org/abs/1812.02119v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are you tough enough? Framework for Robustness Validation of Machine\n  Comprehension Systems", "abstract": "Deep Learning NLP domain lacks procedures for the analysis of model\nrobustness. In this paper we propose a framework which validates robustness of\nany Question Answering model through model explainers. We propose that a robust\nmodel should transgress the initial notion of semantic similarity induced by\nword embeddings to learn a more human-like understanding of meaning. We test\nthis property by manipulating questions in two ways: swapping important\nquestion word for 1) its semantically correct synonym and 2) for word vector\nthat is close in embedding space. We estimate importance of words in asked\nquestions with Locally Interpretable Model Agnostic Explanations method (LIME).\nWith these two steps we compare state-of-the-art Q&A models. We show that\nalthough accuracy of state-of-the-art models is high, they are very fragile to\nchanges in the input. Moreover, we propose 2 adversarial training scenarios\nwhich raise model sensitivity to true synonyms by up to 7% accuracy measure.\nOur findings help to understand which models are more stable and how they can\nbe improved. In addition, we have created and published a new dataset that may\nbe used for validation of robustness of a Q&A model.", "published": "2018-12-05 19:55:24", "link": "http://arxiv.org/abs/1812.02205v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Weighted Global Normalization for Multiple Choice Reading Comprehension\n  over Long Documents", "abstract": "Motivated by recent evidence pointing out the fragility of high-performing\nspan prediction models, we direct our attention to multiple choice reading\ncomprehension. In particular, this work introduces a novel method for improving\nanswer selection on long documents through weighted global normalization of\npredictions over portions of the documents. We show that applying our method to\na span prediction model adapted for answer selection helps model performance on\nlong summaries from NarrativeQA, a challenging reading comprehension dataset\nwith an answer selection task, and we strongly improve on the task baseline\nperformance by +36.2 Mean Reciprocal Rank.", "published": "2018-12-05 22:27:20", "link": "http://arxiv.org/abs/1812.02253v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Graph based Question Answering System", "abstract": "In today's digital age in the dawning era of big data analytics it is not the\ninformation but the linking of information through entities and actions which\ndefines the discourse. Any textual data either available on the Internet off\noff-line (like newspaper data, Wikipedia dump, etc) is basically connect\ninformation which cannot be treated isolated for its wholesome semantics. There\nis a need for an automated retrieval process with proper information extraction\nto structure the data for relevant and fast text analytics. The first big\nchallenge is the conversion of unstructured textual data to structured data.\nUnlike other databases, graph databases handle relationships and connections\nelegantly. Our project aims at developing a graph-based information extraction\nand retrieval system.", "published": "2018-12-05 06:31:48", "link": "http://arxiv.org/abs/1812.01828v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "An enhanced computational feature selection method for medical synonym\n  identification via bilingualism and multi-corpus training", "abstract": "Medical synonym identification has been an important part of medical natural\nlanguage processing (NLP). However, in the field of Chinese medical synonym\nidentification, there are problems like low precision and low recall rate. To\nsolve the problem, in this paper, we propose a method for identifying Chinese\nmedical synonyms. We first selected 13 features including Chinese and English\nfeatures. Then we studied the synonym identification results of each feature\nalone and different combinations of the features. Through the comparison among\nidentification results, we present an optimal combination of features for\nChinese medical synonym identification. Experiments show that our selected\nfeatures have achieved 97.37% precision rate, 96.00% recall rate and 97.33% F1\nscore.", "published": "2018-12-05 09:45:52", "link": "http://arxiv.org/abs/1812.01879v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "MedSim: A Novel Semantic Similarity Measure in Bio-medical Knowledge\n  Graphs", "abstract": "We present MedSim, a novel semantic SIMilarity method based on public\nwell-established bio-MEDical knowledge graphs (KGs) and large-scale corpus, to\nstudy the therapeutic substitution of antibiotics. Besides hierarchy and corpus\nof KGs, MedSim further interprets medicine characteristics by constructing\nmulti-dimensional medicine-specific feature vectors. Dataset of 528 antibiotic\npairs scored by doctors is applied for evaluation and MedSim has produced\nstatistically significant improvement over other semantic similarity methods.\nFurthermore, some promising applications of MedSim in drug substitution and\ndrug abuse prevention are presented in case study.", "published": "2018-12-05 09:58:54", "link": "http://arxiv.org/abs/1812.01884v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Approach for Semi-automatic Construction of Anti-infective Drug Ontology\n  Based on Entity Linking", "abstract": "Ontology can be used for the interpretation of natural language. To construct\nan anti-infective drug ontology, one needs to design and deploy a\nmethodological step to carry out the entity discovery and linking. Medical\nsynonym resources have been an important part of medical natural language\nprocessing (NLP). However, there are problems such as low precision and low\nrecall rate. In this study, an NLP approach is adopted to generate candidate\nentities. Open ontology is analyzed to extract semantic relations. Six-word\nvector features and word-level features are selected to perform the entity\nlinking. The extraction results of synonyms with a single feature and different\ncombinations of features are studied. Experiments show that our selected\nfeatures have achieved a precision rate of 86.77%, a recall rate of 89.03% and\nan F1 score of 87.89%. This paper finally presents the structure of the\nproposed ontology and its relevant statistical data.", "published": "2018-12-05 10:08:29", "link": "http://arxiv.org/abs/1812.01887v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Knowledge Graph Based Solution for Entity Discovery and Linking in\n  Open-Domain Questions", "abstract": "Named entity discovery and linking is the fundamental and core component of\nquestion answering. In Question Entity Discovery and Linking (QEDL) problem,\ntraditional methods are challenged because multiple entities in one short\nquestion are difficult to be discovered entirely and the incomplete information\nin short text makes entity linking hard to implement. To overcome these\ndifficulties, we proposed a knowledge graph based solution for QEDL and\ndeveloped a system consists of Question Entity Discovery (QED) module and\nEntity Linking (EL) module. The method of QED module is a tradeoff and ensemble\nof two methods. One is the method based on knowledge graph retrieval, which\ncould extract more entities in questions and guarantee the recall rate, the\nother is the method based on Conditional Random Field (CRF), which improves the\nprecision rate. The EL module is treated as a ranking problem and Learning to\nRank (LTR) method with features such as semantic similarity, text similarity\nand entity popularity is utilized to extract and make full use of the\ninformation in short texts. On the official dataset of a shared QEDL evaluation\ntask, our approach could obtain 64.44% F1 score of QED and 64.86% accuracy of\nEL, which ranks the 2nd place and indicates its practical use for QEDL problem.", "published": "2018-12-05 10:10:56", "link": "http://arxiv.org/abs/1812.01889v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "End-to-end contextual speech recognition using class language models and\n  a token passing decoder", "abstract": "End-to-end modeling (E2E) of automatic speech recognition (ASR) blends all\nthe components of a traditional speech recognition system into a unified model.\nAlthough it simplifies training and decoding pipelines, the unified model is\nhard to adapt when mismatch exists between training and test data. In this\nwork, we focus on contextual speech recognition, which is particularly\nchallenging for E2E models because it introduces significant mismatch between\ntraining and test data. To improve the performance in the presence of complex\ncontextual information, we propose to use class-based language models(CLM) that\ncan populate the classes with contextdependent information in real-time. To\nenable this approach to scale to a large number of class members and minimize\nsearch errors, we propose a token passing decoder with efficient token\nrecombination for E2E systems for the first time. We evaluate the proposed\nsystem on general and contextual ASR, and achieve relative 62% Word Error\nRate(WER) reduction for contextual ASR without hurting performance for general\nASR. We show that the proposed method performs well without modification of the\ndecoding hyper-parameters across tasks, making it a general solution for E2E\nASR.", "published": "2018-12-05 18:00:36", "link": "http://arxiv.org/abs/1812.02142v1", "categories": ["cs.CL", "cs.LG", "68T10", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Attending to Mathematical Language with Transformers", "abstract": "Mathematical expressions were generated, evaluated and used to train neural\nnetwork models based on the transformer architecture. The expressions and their\ntargets were analyzed as a character-level sequence transduction task in which\nthe encoder and decoder are built on attention mechanisms. Three models were\ntrained to understand and evaluate symbolic variables and expressions in\nmathematics: (1) the self-attentive and feed-forward transformer without\nrecurrence or convolution, (2) the universal transformer with recurrence, and\n(3) the adaptive universal transformer with recurrence and adaptive computation\ntime. The models respectively achieved test accuracies as high as 76.1%, 78.8%\nand 84.9% in evaluating the expressions to match the target values. For the\ncases inferred incorrectly, the results differed from the targets by only one\nor two characters. The models notably learned to add, subtract and multiply\nboth positive and negative decimal numbers of variable digits assigned to\nsymbolic variables.", "published": "2018-12-05 03:05:08", "link": "http://arxiv.org/abs/1812.02825v5", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Text Data Augmentation Made Simple By Leveraging NLP Cloud APIs", "abstract": "In practice, it is common to find oneself with far too little text data to\ntrain a deep neural network. This \"Big Data Wall\" represents a challenge for\nminority language communities on the Internet, organizations, laboratories and\ncompanies that compete the GAFAM (Google, Amazon, Facebook, Apple, Microsoft).\nWhile most of the research effort in text data augmentation aims on the\nlong-term goal of finding end-to-end learning solutions, which is equivalent to\n\"using neural networks to feed neural networks\", this engineering work focuses\non the use of practical, robust, scalable and easy-to-implement data\naugmentation pre-processing techniques similar to those that are successful in\ncomputer vision. Several text augmentation techniques have been experimented.\nSome existing ones have been tested for comparison purposes such as noise\ninjection or the use of regular expressions. Others are modified or improved\ntechniques like lexical replacement. Finally more innovative ones, such as the\ngeneration of paraphrases using back-translation or by the transformation of\nsyntactic trees, are based on robust, scalable, and easy-to-use NLP Cloud APIs.\nAll the text augmentation techniques studied, with an amplification factor of\nonly 5, increased the accuracy of the results in a range of 4.3% to 21.6%, with\nsignificant statistical fluctuations, on a standardized task of text polarity\nprediction. Some standard deep neural network architectures were tested: the\nmultilayer perceptron (MLP), the long short-term memory recurrent network\n(LSTM) and the bidirectional LSTM (biLSTM). Classical XGBoost algorithm has\nbeen tested with up to 2.5% improvements.", "published": "2018-12-05 03:59:41", "link": "http://arxiv.org/abs/1812.04718v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Summarizing Videos with Attention", "abstract": "In this work we propose a novel method for supervised, keyshots based video\nsummarization by applying a conceptually simple and computationally efficient\nsoft, self-attention mechanism. Current state of the art methods leverage\nbi-directional recurrent networks such as BiLSTM combined with attention. These\nnetworks are complex to implement and computationally demanding compared to\nfully connected networks. To that end we propose a simple, self-attention based\nnetwork for video summarization which performs the entire sequence to sequence\ntransformation in a single feed forward pass and single backward pass during\ntraining. Our method sets a new state of the art results on two benchmarks\nTvSum and SumMe, commonly used in this domain.", "published": "2018-12-05 13:00:04", "link": "http://arxiv.org/abs/1812.01969v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Neural Abstractive Text Summarization with Sequence-to-Sequence Models", "abstract": "In the past few years, neural abstractive text summarization with\nsequence-to-sequence (seq2seq) models have gained a lot of popularity. Many\ninteresting techniques have been proposed to improve seq2seq models, making\nthem capable of handling different challenges, such as saliency, fluency and\nhuman readability, and generate high-quality summaries. Generally speaking,\nmost of these techniques differ in one of these three categories: network\nstructure, parameter inference, and decoding/generation. There are also other\nconcerns, such as efficiency and parallelism for training a model. In this\npaper, we provide a comprehensive literature survey on different seq2seq models\nfor abstractive text summarization from the viewpoint of network structures,\ntraining strategies, and summary generation algorithms. Several models were\nfirst proposed for language modeling and generation tasks, such as machine\ntranslation, and later applied to abstractive text summarization. Hence, we\nalso provide a brief review of these models. As part of this survey, we also\ndevelop an open source library, namely, Neural Abstractive Text Summarizer\n(NATS) toolkit, for the abstractive text summarization. An extensive set of\nexperiments have been conducted on the widely used CNN/Daily Mail dataset to\nexamine the effectiveness of several different neural network components.\nFinally, we benchmark two models implemented in NATS on the two recently\nreleased datasets, namely, Newsroom and Bytecup.", "published": "2018-12-05 04:06:27", "link": "http://arxiv.org/abs/1812.02303v4", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
