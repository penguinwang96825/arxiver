{"title": "CA-EHN: Commonsense Analogy from E-HowNet", "abstract": "Embedding commonsense knowledge is crucial for end-to-end models to\ngeneralize inference beyond training corpora. However, existing word analogy\ndatasets have tended to be handcrafted, involving permutations of hundreds of\nwords with only dozens of pre-defined relations, mostly morphological relations\nand named entities. In this work, we model commonsense knowledge down to\nword-level analogical reasoning by leveraging E-HowNet, an ontology that\nannotates 88K Chinese words with their structured sense definitions and English\ntranslations. We present CA-EHN, the first commonsense word analogy dataset\ncontaining 90,505 analogies covering 5,656 words and 763 relations. Experiments\nshow that CA-EHN stands out as a great indicator of how well word\nrepresentations embed commonsense knowledge. The dataset is publicly available\nat https://github.com/ckiplab/CA-EHN.", "published": "2019-08-20 08:33:58", "link": "http://arxiv.org/abs/1908.07218v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GlossBERT: BERT for Word Sense Disambiguation with Gloss Knowledge", "abstract": "Word Sense Disambiguation (WSD) aims to find the exact sense of an ambiguous\nword in a particular context. Traditional supervised methods rarely take into\nconsideration the lexical resources like WordNet, which are widely utilized in\nknowledge-based methods. Recent studies have shown the effectiveness of\nincorporating gloss (sense definition) into neural networks for WSD. However,\ncompared with traditional word expert supervised methods, they have not\nachieved much improvement. In this paper, we focus on how to better leverage\ngloss knowledge in a supervised neural WSD system. We construct context-gloss\npairs and propose three BERT-based models for WSD. We fine-tune the pre-trained\nBERT model on SemCor3.0 training corpus and the experimental results on several\nEnglish all-words WSD benchmark datasets show that our approach outperforms the\nstate-of-the-art systems.", "published": "2019-08-20 09:37:42", "link": "http://arxiv.org/abs/1908.07245v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep Contextualized Word Embeddings in Transition-Based and Graph-Based\n  Dependency Parsing -- A Tale of Two Parsers Revisited", "abstract": "Transition-based and graph-based dependency parsers have previously been\nshown to have complementary strengths and weaknesses: transition-based parsers\nexploit rich structural features but suffer from error propagation, while\ngraph-based parsers benefit from global optimization but have restricted\nfeature scope. In this paper, we show that, even though some details of the\npicture have changed after the switch to neural networks and continuous\nrepresentations, the basic trade-off between rich features and global\noptimization remains essentially the same. Moreover, we show that deep\ncontextualized word embeddings, which allow parsers to pack information about\nglobal sentence structure into local feature representations, benefit\ntransition-based parsers more than graph-based parsers, making the two\napproaches virtually equivalent in terms of both accuracy and error profile. We\nargue that the reason is that these representations help prevent search errors\nand thereby allow transition-based parsers to better exploit their inherent\nstrength of making accurate local decisions. We support this explanation by an\nerror analysis of parsing experiments on 13 languages.", "published": "2019-08-20 14:36:57", "link": "http://arxiv.org/abs/1908.07397v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Contextualized Embeddings on 54 Languages in POS Tagging,\n  Lemmatization and Dependency Parsing", "abstract": "We present an extensive evaluation of three recently proposed methods for\ncontextualized embeddings on 89 corpora in 54 languages of the Universal\nDependencies 2.3 in three tasks: POS tagging, lemmatization, and dependency\nparsing. Employing the BERT, Flair and ELMo as pretrained embedding inputs in a\nstrong baseline of UDPipe 2.0, one of the best-performing systems of the CoNLL\n2018 Shared Task and an overall winner of the EPE 2018, we present a one-to-one\ncomparison of the three contextualized word embedding methods, as well as a\ncomparison with word2vec-like pretrained embeddings and with end-to-end\ncharacter-level word embeddings. We report state-of-the-art results in all\nthree tasks as compared to results on UD 2.2 in the CoNLL 2018 Shared Task.", "published": "2019-08-20 15:52:59", "link": "http://arxiv.org/abs/1908.07448v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Controversy in Context", "abstract": "With the growing interest in social applications of Natural Language\nProcessing and Computational Argumentation, a natural question is how\ncontroversial a given concept is. Prior works relied on Wikipedia's metadata\nand on content analysis of the articles pertaining to a concept in question.\nHere we show that the immediate textual context of a concept is strongly\nindicative of this property, and, using simple and language-independent\nmachine-learning tools, we leverage this observation to achieve\nstate-of-the-art results in controversiality prediction. In addition, we\nanalyze and make available a new dataset of concepts labeled for\ncontroversiality. It is significantly larger than existing datasets, and grades\nconcepts on a 0-10 scale, rather than treating controversiality as a binary\nlabel.", "published": "2019-08-20 17:07:52", "link": "http://arxiv.org/abs/1908.07491v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GeoSQA: A Benchmark for Scenario-based Question Answering in the\n  Geography Domain at High School Level", "abstract": "Scenario-based question answering (SQA) has attracted increasing research\nattention. It typically requires retrieving and integrating knowledge from\nmultiple sources, and applying general knowledge to a specific case described\nby a scenario. SQA widely exists in the medical, geography, and legal\ndomains---both in practice and in the exams. In this paper, we introduce the\nGeoSQA dataset. It consists of 1,981 scenarios and 4,110 multiple-choice\nquestions in the geography domain at high school level, where diagrams (e.g.,\nmaps, charts) have been manually annotated with natural language descriptions\nto benefit NLP research. Benchmark results on a variety of state-of-the-art\nmethods for question answering, textual entailment, and reading comprehension\ndemonstrate the unique challenges presented by SQA for future research.", "published": "2019-08-20 15:11:06", "link": "http://arxiv.org/abs/1908.07855v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Universal Adversarial Triggers for Attacking and Analyzing NLP", "abstract": "Adversarial examples highlight model vulnerabilities and are useful for\nevaluation and interpretation. We define universal adversarial triggers:\ninput-agnostic sequences of tokens that trigger a model to produce a specific\nprediction when concatenated to any input from a dataset. We propose a\ngradient-guided search over tokens which finds short trigger sequences (e.g.,\none word for classification and four words for language modeling) that\nsuccessfully trigger the target prediction. For example, triggers cause SNLI\nentailment accuracy to drop from 89.94% to 0.55%, 72% of \"why\" questions in\nSQuAD to be answered \"to kill american people\", and the GPT-2 language model to\nspew racist output even when conditioned on non-racial contexts. Furthermore,\nalthough the triggers are optimized using white-box access to a specific model,\nthey transfer to other models for all tasks we consider. Finally, since\ntriggers are input-agnostic, they provide an analysis of global model behavior.\nFor instance, they confirm that SNLI models exploit dataset biases and help to\ndiagnose heuristics learned by reading comprehension models.", "published": "2019-08-20 01:51:40", "link": "http://arxiv.org/abs/1908.07125v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Grounding of Objects from Natural Language Queries", "abstract": "A phrase grounding system localizes a particular object in an image referred\nto by a natural language query. In previous work, the phrases were restricted\nto have nouns that were encountered in training, we extend the task to\nZero-Shot Grounding(ZSG) which can include novel, \"unseen\" nouns. Current\nphrase grounding systems use an explicit object detection network in a 2-stage\nframework where one stage generates sparse proposals and the other stage\nevaluates them. In the ZSG setting, generating appropriate proposals itself\nbecomes an obstacle as the proposal generator is trained on the entities common\nin the detection and grounding datasets. We propose a new single-stage model\ncalled ZSGNet which combines the detector network and the grounding system and\npredicts classification scores and regression parameters. Evaluation of ZSG\nsystem brings additional subtleties due to the influence of the relationship\nbetween the query and learned categories; we define four distinct conditions\nthat incorporate different levels of difficulty. We also introduce new\ndatasets, sub-sampled from Flickr30k Entities and Visual Genome, that enable\nevaluations for the four conditions. Our experiments show that ZSGNet achieves\nstate-of-the-art performance on Flickr30k and ReferIt under the usual \"seen\"\nsettings and performs significantly better than baseline in the zero-shot\nsetting.", "published": "2019-08-20 02:07:14", "link": "http://arxiv.org/abs/1908.07129v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Teacher-Student Framework Enhanced Multi-domain Dialogue Generation", "abstract": "Dialogue systems dealing with multi-domain tasks are highly required. How to\nrecord the state remains a key problem in a task-oriented dialogue system.\nNormally we use human-defined features as dialogue states and apply a state\ntracker to extract these features. However, the performance of such a system is\nlimited by the error propagation of a state tracker. In this paper, we propose\na dialogue generation model that needs no external state trackers and still\nbenefits from human-labeled semantic data. By using a teacher-student\nframework, several teacher models are firstly trained in their individual\ndomains, learn dialogue policies from labeled states. And then the learned\nknowledge and experience are merged and transferred to a universal student\nmodel, which takes raw utterance as its input. Experiments show that the\ndialogue system trained under our framework outperforms the one uses a belief\ntracker.", "published": "2019-08-20 02:59:37", "link": "http://arxiv.org/abs/1908.07137v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LogicENN: A Neural Based Knowledge Graphs Embedding Model with Logical\n  Rules", "abstract": "Knowledge graph embedding models have gained significant attention in AI\nresearch. Recent works have shown that the inclusion of background knowledge,\nsuch as logical rules, can improve the performance of embeddings in downstream\nmachine learning tasks. However, so far, most existing models do not allow the\ninclusion of rules. We address the challenge of including rules and present a\nnew neural based embedding model (LogicENN). We prove that LogicENN can learn\nevery ground truth of encoded rules in a knowledge graph. To the best of our\nknowledge, this has not been proved so far for the neural based family of\nembedding models. Moreover, we derive formulae for the inclusion of various\nrules, including (anti-)symmetric, inverse, irreflexive and transitive,\nimplication, composition, equivalence and negation. Our formulation allows to\navoid grounding for implication and equivalence relations. Our experiments show\nthat LogicENN outperforms the state-of-the-art models in link prediction.", "published": "2019-08-20 03:12:13", "link": "http://arxiv.org/abs/1908.07141v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "CBOWRA: A Representation Learning Approach for Medication Anomaly\n  Detection", "abstract": "Electronic health record is an important source for clinical researches and\napplications, and errors inevitably occur in the data, which could lead to\nsevere damages to both patients and hospital services. One of such error is the\nmismatches between diagnoses and prescriptions, which we address as 'medication\nanomaly' in the paper, and clinicians used to manually identify and correct\nthem. With the development of machine learning techniques, researchers are able\nto train specific model for the task, but the process still requires expert\nknowledge to construct proper features, and few semantic relations are\nconsidered. In this paper, we propose a simple, yet effective detection method\nthat tackles the problem by detecting the semantic inconsistency between\ndiagnoses and prescriptions. Unlike traditional outlier or anomaly detection,\nthe scheme uses continuous bag of words to construct the semantic connection\nbetween specific central words and their surrounding context. The detection of\nmedication anomaly is transformed into identifying the least possible central\nword based on given context. To help distinguish the anomaly from normal\ncontext, we also incorporate a ranking accumulation strategy. The experiments\nwere conducted on two real hospital electronic medical records, and the topN\naccuracy of the proposed method increased by 3.91 to 10.91% and 0.68 to 2.13%\non the datasets, respectively, which is highly competitive to other traditional\nmachine learning-based approaches.", "published": "2019-08-20 03:40:39", "link": "http://arxiv.org/abs/1908.07147v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Discriminative Topic Mining via Category-Name Guided Text Embedding", "abstract": "Mining a set of meaningful and distinctive topics automatically from massive\ntext corpora has broad applications. Existing topic models, however, typically\nwork in a purely unsupervised way, which often generate topics that do not fit\nusers' particular needs and yield suboptimal performance on downstream tasks.\nWe propose a new task, discriminative topic mining, which leverages a set of\nuser-provided category names to mine discriminative topics from text corpora.\nThis new task not only helps a user understand clearly and distinctively the\ntopics he/she is most interested in, but also benefits directly keyword-driven\nclassification tasks. We develop CatE, a novel category-name guided text\nembedding method for discriminative topic mining, which effectively leverages\nminimal user guidance to learn a discriminative embedding space and discover\ncategory representative terms in an iterative manner. We conduct a\ncomprehensive set of experiments to show that CatE mines high-quality set of\ntopics guided by category names only, and benefits a variety of downstream\napplications including weakly-supervised classification and lexical entailment\ndirection identification.", "published": "2019-08-20 04:32:30", "link": "http://arxiv.org/abs/1908.07162v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Latent-Variable Non-Autoregressive Neural Machine Translation with\n  Deterministic Inference Using a Delta Posterior", "abstract": "Although neural machine translation models reached high translation quality,\nthe autoregressive nature makes inference difficult to parallelize and leads to\nhigh translation latency. Inspired by recent refinement-based approaches, we\npropose LaNMT, a latent-variable non-autoregressive model with continuous\nlatent variables and deterministic inference procedure. In contrast to existing\napproaches, we use a deterministic inference algorithm to find the target\nsequence that maximizes the lowerbound to the log-probability. During\ninference, the length of translation automatically adapts itself. Our\nexperiments show that the lowerbound can be greatly increased by running the\ninference algorithm, resulting in significantly improved translation quality.\nOur proposed model closes the performance gap between non-autoregressive and\nautoregressive approaches on ASPEC Ja-En dataset with 8.6x faster decoding. On\nWMT'14 En-De dataset, our model narrows the gap with autoregressive baseline to\n2.0 BLEU points with 12.5x speedup. By decoding multiple initial latent\nvariables in parallel and rescore using a teacher model, the proposed model\nfurther brings the gap down to 1.0 BLEU point on WMT'14 En-De task with 6.8x\nspeedup.", "published": "2019-08-20 06:14:18", "link": "http://arxiv.org/abs/1908.07181v5", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ARAML: A Stable Adversarial Training Framework for Text Generation", "abstract": "Most of the existing generative adversarial networks (GAN) for text\ngeneration suffer from the instability of reinforcement learning training\nalgorithms such as policy gradient, leading to unstable performance. To tackle\nthis problem, we propose a novel framework called Adversarial Reward Augmented\nMaximum Likelihood (ARAML). During adversarial training, the discriminator\nassigns rewards to samples which are acquired from a stationary distribution\nnear the data rather than the generator's distribution. The generator is\noptimized with maximum likelihood estimation augmented by the discriminator's\nrewards instead of policy gradient. Experiments show that our model can\noutperform state-of-the-art text GANs with a more stable training process.", "published": "2019-08-20 07:25:14", "link": "http://arxiv.org/abs/1908.07195v1", "categories": ["cs.CL", "cs.LG", "68T50"], "primary_category": "cs.CL"}
{"title": "Density Matrices with Metric for Derivational Ambiguity", "abstract": "Recent work on vector-based compositional natural language semantics has\nproposed the use of density matrices to model lexical ambiguity and (graded)\nentailment (e.g. Piedeleu et al 2015, Bankova et al 2019, Sadrzadeh et al\n2018). Ambiguous word meanings, in this work, are represented as mixed states,\nand the compositional interpretation of phrases out of their constituent parts\ntakes the form of a strongly monoidal functor sending the derivational\nmorphisms of a pregroup syntax to linear maps in FdHilb. Our aims in this paper\nare threefold. Firstly, we replace the pregroup front end by a Lambek\ncategorial grammar with directional implications expressing a word's\nselectional requirements. By the Curry-Howard correspondence, the derivations\nof the grammar's type logic are associated with terms of the (ordered) linear\nlambda calculus; these terms can be read as programs for compositional meaning\nassembly with density matrices as the target semantic spaces. Secondly, we\nextend on the existing literature and introduce a symmetric, nondegenerate\nbilinear form called a \"metric\" that defines a canonical isomorphism between a\nvector space and its dual, allowing us to keep a distinction between left and\nright implication. Thirdly, we use this metric to define density matrix spaces\nin a directional form, modeling the ubiquitous derivational ambiguity of\nnatural language syntax, and show how this alows an integrated treatment of\nlexical and derivational forms of ambiguity controlled at the level of the\ninterpretation.", "published": "2019-08-20 13:49:30", "link": "http://arxiv.org/abs/1908.07347v3", "categories": ["cs.CL", "quant-ph"], "primary_category": "cs.CL"}
{"title": "Learning document embeddings along with their uncertainties", "abstract": "Majority of the text modelling techniques yield only point-estimates of\ndocument embeddings and lack in capturing the uncertainty of the estimates.\nThese uncertainties give a notion of how well the embeddings represent a\ndocument. We present Bayesian subspace multinomial model (Bayesian SMM), a\ngenerative log-linear model that learns to represent documents in the form of\nGaussian distributions, thereby encoding the uncertainty in its co-variance.\nAdditionally, in the proposed Bayesian SMM, we address a commonly encountered\nproblem of intractability that appears during variational inference in\nmixed-logit models. We also present a generative Gaussian linear classifier for\ntopic identification that exploits the uncertainty in document embeddings. Our\nintrinsic evaluation using perplexity measure shows that the proposed Bayesian\nSMM fits the data better as compared to the state-of-the-art neural variational\ndocument model on Fisher speech and 20Newsgroups text corpora. Our topic\nidentification experiments show that the proposed systems are robust to\nover-fitting on unseen test data. The topic ID results show that the proposed\nmodel is outperforms state-of-the-art unsupervised topic models and achieve\ncomparable results to the state-of-the-art fully supervised discriminative\nmodels.", "published": "2019-08-20 20:31:51", "link": "http://arxiv.org/abs/1908.07599v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Lost Croatian Cybernetic Machine Translation Program", "abstract": "We are exploring the historical significance of research in the field of\nmachine translation conducted by Bulcsu Laszlo, Croatian linguist, who was a\npioneer in machine translation in Yugoslavia during the 1950s. We are focused\non two important seminal papers written by members of his research group from\n1959 and 1962, as well as their legacy in establishing a Croatian machine\ntranslation program based around the Faculty of Humanities and Social Sciences\nof the University of Zagreb in the late 1950s and early 1960s. We are exploring\ntheir work in connection with the beginnings of machine translation in the USA\nand USSR, motivated by the Cold War and the intelligence needs of the period.\nWe also present the approach to machine translation advocated by the Croatian\ngroup in Yugoslavia, which is different from the usual logical approaches of\nthe period, and his advocacy of cybernetic methods, which would be adopted as a\ncanon by the mainstream AI community only decades later.", "published": "2019-08-20 05:27:34", "link": "http://arxiv.org/abs/1908.08917v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Prosodic Phrase Alignment for Machine Dubbing", "abstract": "Dubbing is a type of audiovisual translation where dialogues are translated\nand enacted so that they give the impression that the media is in the target\nlanguage. It requires a careful alignment of dubbed recordings with the lip\nmovements of performers in order to achieve visual coherence. In this paper, we\ndeal with the specific problem of prosodic phrase synchronization within the\nframework of machine dubbing. Our methodology exploits the attention mechanism\noutput in neural machine translation to find plausible phrasing for the\ntranslated dialogue lines and then uses them to condition their synthesis. Our\ninitial work in this field records comparable speech rate ratio to professional\ndubbing translation, and improvement in terms of lip-syncing of long dialogue\nlines.", "published": "2019-08-20 08:52:52", "link": "http://arxiv.org/abs/1908.07226v1", "categories": ["cs.CL", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Unsupervised Hierarchical Grouping of Knowledge Graph Entities", "abstract": "Knowledge graphs have attracted lots of attention in academic and industrial\nenvironments. Despite their usefulness, popular knowledge graphs suffer from\nincompleteness of information, especially in their type assertions. This has\nencouraged research in the automatic discovery of entity types. In this\ncontext, multiple works were developed to utilize logical inference on\nontologies and statistical machine learning methods to learn type assertion in\nknowledge graphs. However, these approaches suffer from limited performance on\nnoisy data, limited scalability and the dependence on labeled training samples.\nIn this work, we propose a new unsupervised approach that learns to categorize\nentities into a hierarchy of named groups. We show that our approach is able to\neffectively learn entity groups using a scalable procedure in noisy and sparse\ndatasets. We experiment our approach on a set of popular knowledge graph\nbenchmarking datasets, and we publish a collection of the outcome group\nhierarchies.", "published": "2019-08-20 11:40:16", "link": "http://arxiv.org/abs/1908.07281v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "LXMERT: Learning Cross-Modality Encoder Representations from\n  Transformers", "abstract": "Vision-and-language reasoning requires an understanding of visual concepts,\nlanguage semantics, and, most importantly, the alignment and relationships\nbetween these two modalities. We thus propose the LXMERT (Learning\nCross-Modality Encoder Representations from Transformers) framework to learn\nthese vision-and-language connections. In LXMERT, we build a large-scale\nTransformer model that consists of three encoders: an object relationship\nencoder, a language encoder, and a cross-modality encoder. Next, to endow our\nmodel with the capability of connecting vision and language semantics, we\npre-train the model with large amounts of image-and-sentence pairs, via five\ndiverse representative pre-training tasks: masked language modeling, masked\nobject prediction (feature regression and label classification), cross-modality\nmatching, and image question answering. These tasks help in learning both\nintra-modality and cross-modality relationships. After fine-tuning from our\npre-trained parameters, our model achieves the state-of-the-art results on two\nvisual question answering datasets (i.e., VQA and GQA). We also show the\ngeneralizability of our pre-trained cross-modality model by adapting it to a\nchallenging visual-reasoning task, NLVR2, and improve the previous best result\nby 22% absolute (54% to 76%). Lastly, we demonstrate detailed ablation studies\nto prove that both our novel model components and pre-training strategies\nsignificantly contribute to our strong results; and also present several\nattention visualizations for the different encoders. Code and pre-trained\nmodels publicly available at: https://github.com/airsplay/lxmert", "published": "2019-08-20 17:05:18", "link": "http://arxiv.org/abs/1908.07490v3", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Phrase Localization Without Paired Training Examples", "abstract": "Localizing phrases in images is an important part of image understanding and\ncan be useful in many applications that require mappings between textual and\nvisual information. Existing work attempts to learn these mappings from\nexamples of phrase-image region correspondences (strong supervision) or from\nphrase-image pairs (weak supervision). We postulate that such paired\nannotations are unnecessary, and propose the first method for the phrase\nlocalization problem where neither training procedure nor paired, task-specific\ndata is required. Our method is simple but effective: we use off-the-shelf\napproaches to detect objects, scenes and colours in images, and explore\ndifferent approaches to measure semantic similarity between the categories of\ndetected visual elements and words in phrases. Experiments on two well-known\nphrase localization datasets show that this approach surpasses all weakly\nsupervised methods by a large margin and performs very competitively to\nstrongly supervised methods, and can thus be considered a strong baseline to\nthe task. The non-paired nature of our method makes it applicable to any domain\nand where no paired phrase localization annotation is available.", "published": "2019-08-20 18:07:37", "link": "http://arxiv.org/abs/1908.07553v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "From Text to Sound: A Preliminary Study on Retrieving Sound Effects to\n  Radio Stories", "abstract": "Sound effects play an essential role in producing high-quality radio stories\nbut require enormous labor cost to add. In this paper, we address the problem\nof automatically adding sound effects to radio stories with a retrieval-based\nmodel. However, directly implementing a tag-based retrieval model leads to high\nfalse positives due to the ambiguity of story contents. To solve this problem,\nwe introduce a retrieval-based framework hybridized with a semantic inference\nmodel which helps to achieve robust retrieval results. Our model relies on\nfine-designed features extracted from the context of candidate triggers. We\ncollect two story dubbing datasets through crowdsourcing to analyze the setting\nof adding sound effects and to train and test our proposed methods. We further\ndiscuss the importance of each feature and introduce several heuristic rules\nfor the trade-off between precision and recall. Together with the\ntext-to-speech technology, our results reveal a promising automatic pipeline on\nproducing high-quality radio stories.", "published": "2019-08-20 20:10:05", "link": "http://arxiv.org/abs/1908.07590v1", "categories": ["cs.IR", "cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.IR"}
{"title": "Similarity Learning for Authorship Verification in Social Media", "abstract": "Authorship verification tries to answer the question if two documents with\nunknown authors were written by the same author or not. A range of successful\ntechnical approaches has been proposed for this task, many of which are based\non traditional linguistic features such as n-grams. These algorithms achieve\ngood results for certain types of written documents like books and novels.\nForensic authorship verification for social media, however, is a much more\nchallenging task since messages tend to be relatively short, with a large\nvariety of different genres and topics. At this point, traditional methods\nbased on features like n-grams have had limited success. In this work, we\npropose a new neural network topology for similarity learning that\nsignificantly improves the performance on the author verification task with\nsuch challenging data sets.", "published": "2019-08-20 04:08:58", "link": "http://arxiv.org/abs/1908.07844v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "A Microphone Array and Voice Algorithm based Smart Hearing Aid", "abstract": "Approximately 6.2% of the world's population (466 million people) suffer from\ndisabling hearing impairment [1]. Hearing impairment impacts negatively on\none's education, financial success [2][3], cognitive development in childhood\n[4], including increased risk of dementia in older adulthood [5]. Lack of or\nreduced social interaction due to hearing impairment affects creating or\nmaintaining healthy relationships at home, school and work [5]. Hence, hearing\nimpairment genuinely affects the overall quality of life and wellbeing.\n  The cocktail party effect, which is a healthy hearing individual's ability to\nunderstand one voice in a cacophony of other voices or sounds, is an important\nability lacking in people with hearing impairment. This inability results in\ndifficulties with simple daily activities such as partaking in group\ndiscussions or conversing in noisy restaurants [6]. This smart hearing aid aims\nto provide much-needed assistance with understanding speech in noisy\nenvironments. For example, if a person wants to partake in a group discussion,\nhe/she needs to place the microphone array based unit on a flat surface in\nfront of him/her, such as a table. When conversations take place, the\nmicrophone array will capture and process sound from all directions,\nintelligently prioritise and provide the lead speaker's voice by suppressing\nunwanted noises, including speeches of other people. This device selects and\nalternates voices between speakers automatically using voice algorithms.\nAdditionally, the user has the option of further fine-tuning the acoustic\nparameters as needed through a smartphone interface. This paper describes the\ndevelopment and functions of this new Smart Hearing Aid.", "published": "2019-08-20 13:17:09", "link": "http://arxiv.org/abs/1908.07324v4", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AI for Earth: Rainforest Conservation by Acoustic Surveillance", "abstract": "Saving rainforests is a key to halting adverse climate changes. In this\npaper, we introduce an innovative solution built on acoustic surveillance and\nmachine learning technologies to help rainforest conservation. In particular,\nWe propose new convolutional neural network (CNN) models for environmental\nsound classification and achieved promising preliminary results on two\ndatasets, including a public audio dataset and our real rainforest sound\ndataset. The proposed audio classification models can be easily extended in an\nautomated machine learning paradigm and integrated in cloud-based services for\nreal world deployment.", "published": "2019-08-20 03:50:57", "link": "http://arxiv.org/abs/1908.07517v1", "categories": ["cs.SD", "cs.DB", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
