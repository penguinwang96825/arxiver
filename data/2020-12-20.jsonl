{"title": "Transductive Visual Verb Sense Disambiguation", "abstract": "Verb Sense Disambiguation is a well-known task in NLP, the aim is to find the\ncorrect sense of a verb in a sentence. Recently, this problem has been extended\nin a multimodal scenario, by exploiting both textual and visual features of\nambiguous verbs leading to a new problem, the Visual Verb Sense Disambiguation\n(VVSD). Here, the sense of a verb is assigned considering the content of an\nimage paired with it rather than a sentence in which the verb appears.\nAnnotating a dataset for this task is more complex than textual disambiguation,\nbecause assigning the correct sense to a pair of $<$image, verb$>$ requires\nboth non-trivial linguistic and visual skills. In this work, differently from\nthe literature, the VVSD task will be performed in a transductive\nsemi-supervised learning (SSL) setting, in which only a small amount of labeled\ninformation is required, reducing tremendously the need for annotated data. The\ndisambiguation process is based on a graph-based label propagation method which\ntakes into account mono or multimodal representations for $<$image, verb$>$\npairs. Experiments have been carried out on the recently published dataset\nVerSe, the only available dataset for this task. The achieved results\noutperform the current state-of-the-art by a large margin while using only a\nsmall fraction of labeled samples per sense. Code available:\nhttps://github.com/GiBg1aN/TVVSD.", "published": "2020-12-20 01:07:30", "link": "http://arxiv.org/abs/2012.10821v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "A hybrid deep-learning approach for complex biochemical named entity\n  recognition", "abstract": "Named entity recognition (NER) of chemicals and drugs is a critical domain of\ninformation extraction in biochemical research. NER provides support for text\nmining in biochemical reactions, including entity relation extraction,\nattribute extraction, and metabolic response relationship extraction. However,\nthe existence of complex naming characteristics in the biomedical field, such\nas polysemy and special characters, make the NER task very challenging. Here,\nwe propose a hybrid deep learning approach to improve the recognition accuracy\nof NER. Specifically, our approach applies the Bidirectional Encoder\nRepresentations from Transformers (BERT) model to extract the underlying\nfeatures of the text, learns a representation of the context of the text\nthrough Bi-directional Long Short-Term Memory (BILSTM), and incorporates the\nmulti-head attention (MHATT) mechanism to extract chapter-level features. In\nthis approach, the MHATT mechanism aims to improve the recognition accuracy of\nabbreviations to efficiently deal with the problem of inconsistency in\nfull-text labels. Moreover, conditional random field (CRF) is used to label\nsequence tags because this probabilistic method does not need strict\nindependence assumptions and can accommodate arbitrary context information. The\nexperimental evaluation on a publicly-available dataset shows that the proposed\nhybrid approach achieves the best recognition performance; in particular, it\nsubstantially improves performance in recognizing abbreviations, polysemes, and\nlow-frequency entities, compared with the state-of-the-art approaches. For\ninstance, compared with the recognition accuracies for low-frequency entities\nproduced by the BILSTM-CRF algorithm, those produced by the hybrid approach on\ntwo entity datasets (MULTIPLE and IDENTIFIER) have been increased by 80% and\n21.69%, respectively.", "published": "2020-12-20 01:30:07", "link": "http://arxiv.org/abs/2012.10824v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Adaptive Bi-directional Attention: Exploring Multi-Granularity\n  Representations for Machine Reading Comprehension", "abstract": "Recently, the attention-enhanced multi-layer encoder, such as Transformer,\nhas been extensively studied in Machine Reading Comprehension (MRC). To predict\nthe answer, it is common practice to employ a predictor to draw information\nonly from the final encoder layer which generates the \\textit{coarse-grained}\nrepresentations of the source sequences, i.e., passage and question. Previous\nstudies have shown that the representation of source sequence becomes more\n\\textit{coarse-grained} from \\textit{fine-grained} as the encoding layer\nincreases. It is generally believed that with the growing number of layers in\ndeep neural networks, the encoding process will gather relevant information for\neach location increasingly, resulting in more \\textit{coarse-grained}\nrepresentations, which adds the likelihood of similarity to other locations\n(referring to homogeneity). Such a phenomenon will mislead the model to make\nwrong judgments so as to degrade the performance. To this end, we propose a\nnovel approach called Adaptive Bidirectional Attention, which adaptively\nexploits the source representations of different levels to the predictor.\nExperimental results on the benchmark dataset, SQuAD 2.0 demonstrate the\neffectiveness of our approach, and the results are better than the previous\nstate-of-the-art model by 2.5$\\%$ EM and 2.3$\\%$ F1 scores.", "published": "2020-12-20 09:31:35", "link": "http://arxiv.org/abs/2012.10877v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "KRISP: Integrating Implicit and Symbolic Knowledge for Open-Domain\n  Knowledge-Based VQA", "abstract": "One of the most challenging question types in VQA is when answering the\nquestion requires outside knowledge not present in the image. In this work we\nstudy open-domain knowledge, the setting when the knowledge required to answer\na question is not given/annotated, neither at training nor test time. We tap\ninto two types of knowledge representations and reasoning. First, implicit\nknowledge which can be learned effectively from unsupervised language\npre-training and supervised training data with transformer-based models.\nSecond, explicit, symbolic knowledge encoded in knowledge bases. Our approach\ncombines both - exploiting the powerful implicit reasoning of transformer\nmodels for answer prediction, and integrating symbolic representations from a\nknowledge graph, while never losing their explicit semantics to an implicit\nembedding. We combine diverse sources of knowledge to cover the wide variety of\nknowledge needed to solve knowledge-based questions. We show our approach,\nKRISP (Knowledge Reasoning with Implicit and Symbolic rePresentations),\nsignificantly outperforms state-of-the-art on OK-VQA, the largest available\ndataset for open-domain knowledge-based VQA. We show with extensive ablations\nthat while our model successfully exploits implicit knowledge reasoning, the\nsymbolic answer module which explicitly connects the knowledge graph to the\nanswer vocabulary is critical to the performance of our method and generalizes\nto rare answers.", "published": "2020-12-20 20:13:02", "link": "http://arxiv.org/abs/2012.11014v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Visual Speech Enhancement Without A Real Visual Stream", "abstract": "In this work, we re-think the task of speech enhancement in unconstrained\nreal-world environments. Current state-of-the-art methods use only the audio\nstream and are limited in their performance in a wide range of real-world\nnoises. Recent works using lip movements as additional cues improve the quality\nof generated speech over \"audio-only\" methods. But, these methods cannot be\nused for several applications where the visual stream is unreliable or\ncompletely absent. We propose a new paradigm for speech enhancement by\nexploiting recent breakthroughs in speech-driven lip synthesis. Using one such\nmodel as a teacher network, we train a robust student network to produce\naccurate lip movements that mask away the noise, thus acting as a \"visual noise\nfilter\". The intelligibility of the speech enhanced by our pseudo-lip approach\nis comparable (< 3% difference) to the case of using real lips. This implies\nthat we can exploit the advantages of using lip movements even in the absence\nof a real video stream. We rigorously evaluate our model using quantitative\nmetrics as well as human evaluations. Additional ablation studies and a demo\nvideo on our website containing qualitative comparisons and results clearly\nillustrate the effectiveness of our approach. We provide a demo video which\nclearly illustrates the effectiveness of our proposed approach on our website:\n\\url{http://cvit.iiit.ac.in/research/projects/cvit-projects/visual-speech-enhancement-without-a-real-visual-stream}.\nThe code and models are also released for future research:\n\\url{https://github.com/Sindhu-Hegde/pseudo-visual-speech-denoising}.", "published": "2020-12-20 06:02:12", "link": "http://arxiv.org/abs/2012.10852v1", "categories": ["cs.CV", "cs.LG", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
