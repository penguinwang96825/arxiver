{"title": "Not All Neural Embeddings are Born Equal", "abstract": "Neural language models learn word representations that capture rich\nlinguistic and conceptual information. Here we investigate the embeddings\nlearned by neural machine translation models. We show that translation-based\nembeddings outperform those learned by cutting-edge monolingual models at\nsingle-language tasks requiring knowledge of conceptual similarity and/or\nsyntactic role. The findings suggest that, while monolingual models learn\ninformation about how concepts are related, neural-translation models better\ncapture their true ontological status.", "published": "2014-10-02 21:35:35", "link": "http://arxiv.org/abs/1410.0718v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
