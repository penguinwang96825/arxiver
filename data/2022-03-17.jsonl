{"title": "Triangular Transfer: Freezing the Pivot for Triangular Machine\n  Translation", "abstract": "Triangular machine translation is a special case of low-resource machine\ntranslation where the language pair of interest has limited parallel data, but\nboth languages have abundant parallel data with a pivot language. Naturally,\nthe key to triangular machine translation is the successful exploitation of\nsuch auxiliary data. In this work, we propose a transfer-learning-based\napproach that utilizes all types of auxiliary data. As we train auxiliary\nsource-pivot and pivot-target translation models, we initialize some parameters\nof the pivot side with a pre-trained language model and freeze them to\nencourage both translation models to work in the same pivot language space, so\nthat they can be smoothly transferred to the source-target translation model.\nExperiments show that our approach can outperform previous ones.", "published": "2022-03-17 02:00:40", "link": "http://arxiv.org/abs/2203.09027v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine- and Coarse-Granularity Hybrid Self-Attention for Efficient BERT", "abstract": "Transformer-based pre-trained models, such as BERT, have shown extraordinary\nsuccess in achieving state-of-the-art results in many natural language\nprocessing applications. However, deploying these models can be prohibitively\ncostly, as the standard self-attention mechanism of the Transformer suffers\nfrom quadratic computational cost in the input sequence length. To confront\nthis, we propose FCA, a fine- and coarse-granularity hybrid self-attention that\nreduces the computation cost through progressively shortening the computational\nsequence length in self-attention. Specifically, FCA conducts an\nattention-based scoring strategy to determine the informativeness of tokens at\neach layer. Then, the informative tokens serve as the fine-granularity\ncomputing units in self-attention and the uninformative tokens are replaced\nwith one or several clusters as the coarse-granularity computing units in\nself-attention. Experiments on GLUE and RACE datasets show that BERT with FCA\nachieves 2x reduction in FLOPs over original BERT with <1% loss in accuracy. We\nshow that FCA offers a significantly better trade-off between accuracy and\nFLOPs compared to prior methods.", "published": "2022-03-17 03:33:47", "link": "http://arxiv.org/abs/2203.09055v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ask to Understand: Question Generation for Multi-hop Question Answering", "abstract": "Multi-hop Question Answering (QA) requires the machine to answer complex\nquestions by finding scattering clues and reasoning from multiple documents.\nGraph Network (GN) and Question Decomposition (QD) are two common approaches at\npresent. The former uses the \"black-box\" reasoning process to capture the\npotential relationship between entities and sentences, thus achieving good\nperformance. At the same time, the latter provides a clear reasoning logical\nroute by decomposing multi-hop questions into simple single-hop sub-questions.\nIn this paper, we propose a novel method to complete multi-hop QA from the\nperspective of Question Generation (QG). Specifically, we carefully design an\nend-to-end QG module on the basis of a classical QA module, which could help\nthe model understand the context by asking inherently logical sub-questions,\nthus inheriting interpretability from the QD-based method and showing superior\nperformance. Experiments on the HotpotQA dataset demonstrate that the\neffectiveness of our proposed QG module, human evaluation further clarifies its\ninterpretability quantitatively, and thorough analysis shows that the QG module\ncould generate better sub-questions than QD methods in terms of fluency,\nconsistency, and diversity.", "published": "2022-03-17 04:02:29", "link": "http://arxiv.org/abs/2203.09073v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PLANET: Dynamic Content Planning in Autoregressive Transformers for\n  Long-form Text Generation", "abstract": "Despite recent progress of pre-trained language models on generating fluent\ntext, existing methods still suffer from incoherence problems in long-form text\ngeneration tasks that require proper content control and planning to form a\ncoherent high-level logical flow. In this work, we propose PLANET, a novel\ngeneration framework leveraging autoregressive self-attention mechanism to\nconduct content planning and surface realization dynamically. To guide the\ngeneration of output sentences, our framework enriches the Transformer decoder\nwith latent representations to maintain sentence-level semantic plans grounded\nby bag-of-words. Moreover, we introduce a new coherence-based contrastive\nlearning objective to further improve the coherence of output. Extensive\nexperiments are conducted on two challenging long-form text generation tasks\nincluding counterargument generation and opinion article generation. Both\nautomatic and human evaluations show that our method significantly outperforms\nstrong baselines and generates more coherent texts with richer contents.", "published": "2022-03-17 05:52:35", "link": "http://arxiv.org/abs/2203.09100v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RelationPrompt: Leveraging Prompts to Generate Synthetic Data for\n  Zero-Shot Relation Triplet Extraction", "abstract": "Despite the importance of relation extraction in building and representing\nknowledge, less research is focused on generalizing to unseen relations types.\nWe introduce the task setting of Zero-Shot Relation Triplet Extraction\n(ZeroRTE) to encourage further research in low-resource relation extraction\nmethods. Given an input sentence, each extracted triplet consists of the head\nentity, relation label, and tail entity where the relation label is not seen at\nthe training stage. To solve ZeroRTE, we propose to synthesize relation\nexamples by prompting language models to generate structured texts. Concretely,\nwe unify language model prompts and structured text approaches to design a\nstructured prompt template for generating synthetic relation samples when\nconditioning on relation label prompts (RelationPrompt). To overcome the\nlimitation for extracting multiple relation triplets in a sentence, we design a\nnovel Triplet Search Decoding method. Experiments on FewRel and Wiki-ZSL\ndatasets show the efficacy of RelationPrompt for the ZeroRTE task and zero-shot\nrelation classification. Our code and data are available at\ngithub.com/declare-lab/RelationPrompt.", "published": "2022-03-17 05:55:14", "link": "http://arxiv.org/abs/2203.09101v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Type-Driven Multi-Turn Corrections for Grammatical Error Correction", "abstract": "Grammatical Error Correction (GEC) aims to automatically detect and correct\ngrammatical errors. In this aspect, dominant models are trained by\none-iteration learning while performing multiple iterations of corrections\nduring inference. Previous studies mainly focus on the data augmentation\napproach to combat the exposure bias, which suffers from two drawbacks. First,\nthey simply mix additionally-constructed training instances and original ones\nto train models, which fails to help models be explicitly aware of the\nprocedure of gradual corrections. Second, they ignore the interdependence\nbetween different types of corrections. In this paper, we propose a Type-Driven\nMulti-Turn Corrections approach for GEC. Using this approach, from each\ntraining instance, we additionally construct multiple training instances, each\nof which involves the correction of a specific type of errors. Then, we use\nthese additionally-constructed training instances and the original one to train\nthe model in turn. Experimental results and in-depth analysis show that our\napproach significantly benefits the model training. Particularly, our enhanced\nmodel achieves state-of-the-art single-model performance on English GEC\nbenchmarks. We release our code at Github.", "published": "2022-03-17 07:30:05", "link": "http://arxiv.org/abs/2203.09136v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Vision Features in Multimodal Machine Translation", "abstract": "Previous work on multimodal machine translation (MMT) has focused on the way\nof incorporating vision features into translation but little attention is on\nthe quality of vision models. In this work, we investigate the impact of vision\nmodels on MMT. Given the fact that Transformer is becoming popular in computer\nvision, we experiment with various strong models (such as Vision Transformer)\nand enhanced features (such as object-detection and image captioning). We\ndevelop a selective attention model to study the patch-level contribution of an\nimage in MMT. On detailed probing tasks, we find that stronger vision models\nare helpful for learning translation from the visual modality. Our results also\nsuggest the need of carefully examining MMT models, especially when current\nbenchmarks are small-scale and biased. Our code could be found at\n\\url{https://github.com/libeineu/fairseq_mmt}.", "published": "2022-03-17 08:51:09", "link": "http://arxiv.org/abs/2203.09173v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ODE Transformer: An Ordinary Differential Equation-Inspired Model for\n  Sequence Generation", "abstract": "Residual networks are an Euler discretization of solutions to Ordinary\nDifferential Equations (ODE). This paper explores a deeper relationship between\nTransformer and numerical ODE methods. We first show that a residual block of\nlayers in Transformer can be described as a higher-order solution to ODE.\nInspired by this, we design a new architecture, {\\it ODE Transformer}, which is\nanalogous to the Runge-Kutta method that is well motivated in ODE. As a natural\nextension to Transformer, ODE Transformer is easy to implement and efficient to\nuse. Experimental results on the large-scale machine translation, abstractive\nsummarization, and grammar error correction tasks demonstrate the high\ngenericity of ODE Transformer. It can gain large improvements in model\nperformance over strong baselines (e.g., 30.77 and 44.11 BLEU scores on the\nWMT'14 English-German and English-French benchmarks) at a slight cost in\ninference efficiency.", "published": "2022-03-17 08:54:31", "link": "http://arxiv.org/abs/2203.09176v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Detection of Personal Employment Status on Twitter", "abstract": "Detecting disclosures of individuals' employment status on social media can\nprovide valuable information to match job seekers with suitable vacancies,\noffer social protection, or measure labor market flows. However, identifying\nsuch personal disclosures is a challenging task due to their rarity in a sea of\nsocial media content and the variety of linguistic forms used to describe them.\nHere, we examine three Active Learning (AL) strategies in real-world settings\nof extreme class imbalance, and identify five types of disclosures about\nindividuals' employment status (e.g. job loss) in three languages using\nBERT-based classification models. Our findings show that, even under extreme\nimbalance settings, a small number of AL iterations is sufficient to obtain\nlarge and significant gains in precision, recall, and diversity of results\ncompared to a supervised baseline with the same number of labels. We also find\nthat no AL strategy consistently outperforms the rest. Qualitative analysis\nsuggests that AL helps focus the attention mechanism of BERT on core terms and\nadjust the boundaries of semantic expansion, highlighting the importance of\ninterpretable models to provide greater control and visibility into this\ndynamic learning process.", "published": "2022-03-17 08:55:18", "link": "http://arxiv.org/abs/2203.09178v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Entropy-based Attention Regularization Frees Unintended Bias Mitigation\n  from Lists", "abstract": "Natural Language Processing (NLP) models risk overfitting to specific terms\nin the training data, thereby reducing their performance, fairness, and\ngeneralizability. E.g., neural hate speech detection models are strongly\ninfluenced by identity terms like gay, or women, resulting in false positives,\nsevere unintended bias, and lower performance. Most mitigation techniques use\nlists of identity terms or samples from the target domain during training.\nHowever, this approach requires a-priori knowledge and introduces further bias\nif important terms are neglected. Instead, we propose a knowledge-free\nEntropy-based Attention Regularization (EAR) to discourage overfitting to\ntraining-specific terms. An additional objective function penalizes tokens with\nlow self-attention entropy. We fine-tune BERT via EAR: the resulting model\nmatches or exceeds state-of-the-art performance for hate speech classification\nand bias metrics on three benchmark corpora in English and Italian. EAR also\nreveals overfitting terms, i.e., terms most likely to induce bias, to help\nidentify their effect on the model, task, and predictions.", "published": "2022-03-17 09:29:50", "link": "http://arxiv.org/abs/2203.09192v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Universal Conditional Masked Language Pre-training for Neural Machine\n  Translation", "abstract": "Pre-trained sequence-to-sequence models have significantly improved Neural\nMachine Translation (NMT). Different from prior works where pre-trained models\nusually adopt an unidirectional decoder, this paper demonstrates that\npre-training a sequence-to-sequence model but with a bidirectional decoder can\nproduce notable performance gains for both Autoregressive and\nNon-autoregressive NMT. Specifically, we propose CeMAT, a conditional masked\nlanguage model pre-trained on large-scale bilingual and monolingual corpora in\nmany languages. We also introduce two simple but effective methods to enhance\nthe CeMAT, aligned code-switching & masking and dynamic dual-masking. We\nconduct extensive experiments and show that our CeMAT can achieve significant\nperformance improvement for all scenarios from low- to extremely high-resource\nlanguages, i.e., up to +14.4 BLEU on low resource and +7.9 BLEU improvements on\naverage for Autoregressive NMT. For Non-autoregressive NMT, we demonstrate it\ncan also produce consistent performance gains, i.e., up to +5.3 BLEU. To the\nbest of our knowledge, this is the first work to pre-train a unified model for\nfine-tuning on both NMT tasks. Code, data, and pre-trained models are available\nat https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/CeMAT.", "published": "2022-03-17 10:00:33", "link": "http://arxiv.org/abs/2203.09210v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Confidence Calibration for Intent Detection via Hyperspherical Space and\n  Rebalanced Accuracy-Uncertainty Loss", "abstract": "Data-driven methods have achieved notable performance on intent detection,\nwhich is a task to comprehend user queries. Nonetheless, they are controversial\nfor over-confident predictions. In some scenarios, users do not only care about\nthe accuracy but also the confidence of model. Unfortunately, mainstream neural\nnetworks are poorly calibrated, with a large gap between accuracy and\nconfidence. To handle this problem defined as confidence calibration, we\npropose a model using the hyperspherical space and rebalanced\naccuracy-uncertainty loss. Specifically, we project the label vector onto\nhyperspherical space uniformly to generate a dense label representation matrix,\nwhich mitigates over-confident predictions due to overfitting sparce one-hot\nlabel matrix. Besides, we rebalance samples of different accuracy and\nuncertainty to better guide model training. Experiments on the open datasets\nverify that our model outperforms the existing calibration methods and achieves\na significant improvement on the calibration metric.", "published": "2022-03-17 12:01:33", "link": "http://arxiv.org/abs/2203.09278v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Combining Static and Contextualised Multilingual Embeddings", "abstract": "Static and contextual multilingual embeddings have complementary strengths.\nStatic embeddings, while less expressive than contextual language models, can\nbe more straightforwardly aligned across multiple languages. We combine the\nstrengths of static and contextual models to improve multilingual\nrepresentations. We extract static embeddings for 40 languages from XLM-R,\nvalidate those embeddings with cross-lingual word retrieval, and then align\nthem using VecMap. This results in high-quality, highly multilingual static\nembeddings. Then we apply a novel continued pre-training approach to XLM-R,\nleveraging the high quality alignment of our static embeddings to better align\nthe representation space of XLM-R. We show positive results for multiple\ncomplex semantic tasks. We release the static embeddings and the continued\npre-training code. Unlike most previous work, our continued pre-training\napproach does not require parallel text.", "published": "2022-03-17 13:53:46", "link": "http://arxiv.org/abs/2203.09326v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Coloring the Blank Slate: Pre-training Imparts a Hierarchical Inductive\n  Bias to Sequence-to-sequence Models", "abstract": "Relations between words are governed by hierarchical structure rather than\nlinear ordering. Sequence-to-sequence (seq2seq) models, despite their success\nin downstream NLP applications, often fail to generalize in a\nhierarchy-sensitive manner when performing syntactic transformations - for\nexample, transforming declarative sentences into questions. However, syntactic\nevaluations of seq2seq models have only observed models that were not\npre-trained on natural language data before being trained to perform syntactic\ntransformations, in spite of the fact that pre-training has been found to\ninduce hierarchical linguistic generalizations in language models; in other\nwords, the syntactic capabilities of seq2seq models may have been greatly\nunderstated. We address this gap using the pre-trained seq2seq models T5 and\nBART, as well as their multilingual variants mT5 and mBART. We evaluate whether\nthey generalize hierarchically on two transformations in two languages:\nquestion formation and passivization in English and German. We find that\npre-trained seq2seq models generalize hierarchically when performing syntactic\ntransformations, whereas models trained from scratch on syntactic\ntransformations do not. This result presents evidence for the learnability of\nhierarchical syntactic information from non-annotated natural language text\nwhile also demonstrating that seq2seq models are capable of syntactic\ngeneralization, though only after exposure to much more language data than\nhuman learners receive.", "published": "2022-03-17 15:46:53", "link": "http://arxiv.org/abs/2203.09397v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "elBERto: Self-supervised Commonsense Learning for Question Answering", "abstract": "Commonsense question answering requires reasoning about everyday situations\nand causes and effects implicit in context. Typically, existing approaches\nfirst retrieve external evidence and then perform commonsense reasoning using\nthese evidence. In this paper, we propose a Self-supervised Bidirectional\nEncoder Representation Learning of Commonsense (elBERto) framework, which is\ncompatible with off-the-shelf QA model architectures. The framework comprises\nfive self-supervised tasks to force the model to fully exploit the additional\ntraining signals from contexts containing rich commonsense. The tasks include a\nnovel Contrastive Relation Learning task to encourage the model to distinguish\nbetween logically contrastive contexts, a new Jigsaw Puzzle task that requires\nthe model to infer logical chains in long contexts, and three classic SSL tasks\nto maintain pre-trained models language encoding ability. On the representative\nWIQA, CosmosQA, and ReClor datasets, elBERto outperforms all other methods,\nincluding those utilizing explicit graph reasoning and external knowledge\nretrieval. Moreover, elBERto achieves substantial improvements on\nout-of-paragraph and no-effect questions where simple lexical similarity\ncomparison does not help, indicating that it successfully learns commonsense\nand is able to leverage it when given dynamic context.", "published": "2022-03-17 16:23:45", "link": "http://arxiv.org/abs/2203.09424v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Expanding Pretrained Models to Thousands More Languages via\n  Lexicon-based Adaptation", "abstract": "The performance of multilingual pretrained models is highly dependent on the\navailability of monolingual or parallel text present in a target language.\nThus, the majority of the world's languages cannot benefit from recent progress\nin NLP as they have no or limited textual data. To expand possibilities of\nusing NLP technology in these under-represented languages, we systematically\nstudy strategies that relax the reliance on conventional language resources\nthrough the use of bilingual lexicons, an alternative resource with much better\nlanguage coverage. We analyze different strategies to synthesize textual or\nlabeled data using lexicons, and how this data can be combined with monolingual\nor parallel text when available. For 19 under-represented languages across 3\ntasks, our methods lead to consistent improvements of up to 5 and 15 points\nwith and without extra monolingual text respectively. Overall, our study\nhighlights how NLP methods can be adapted to thousands more languages that are\nunder-served by current technology", "published": "2022-03-17 16:48:22", "link": "http://arxiv.org/abs/2203.09435v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Imitation Learning Curriculum for Text Editing with\n  Non-Autoregressive Models", "abstract": "We propose a framework for training non-autoregressive sequence-to-sequence\nmodels for editing tasks, where the original input sequence is iteratively\nedited to produce the output. We show that the imitation learning algorithms\ndesigned to train such models for machine translation introduces mismatches\nbetween training and inference that lead to undertraining and poor\ngeneralization in editing scenarios. We address this issue with two\ncomplementary strategies: 1) a roll-in policy that exposes the model to\nintermediate training sequences that it is more likely to encounter during\ninference, 2) a curriculum that presents easy-to-learn edit operations first,\ngradually increasing the difficulty of training samples as the model becomes\ncompetent. We show the efficacy of these strategies on two challenging English\nediting tasks: controllable text simplification and abstractive summarization.\nOur approach significantly improves output quality on both tasks and controls\noutput complexity better on the simplification task.", "published": "2022-03-17 17:36:23", "link": "http://arxiv.org/abs/2203.09486v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and\n  Implicit Hate Speech Detection", "abstract": "Toxic language detection systems often falsely flag text that contains\nminority group mentions as toxic, as those groups are often the targets of\nonline hate. Such over-reliance on spurious correlations also causes systems to\nstruggle with detecting implicitly toxic language. To help mitigate these\nissues, we create ToxiGen, a new large-scale and machine-generated dataset of\n274k toxic and benign statements about 13 minority groups. We develop a\ndemonstration-based prompting framework and an adversarial\nclassifier-in-the-loop decoding method to generate subtly toxic and benign text\nwith a massive pretrained language model. Controlling machine generation in\nthis way allows ToxiGen to cover implicitly toxic text at a larger scale, and\nabout more demographic groups, than previous resources of human-written text.\nWe conduct a human evaluation on a challenging subset of ToxiGen and find that\nannotators struggle to distinguish machine-generated text from human-written\nlanguage. We also find that 94.5% of toxic examples are labeled as hate speech\nby human annotators. Using three publicly-available datasets, we show that\nfinetuning a toxicity classifier on our data improves its performance on\nhuman-written data substantially. We also demonstrate that ToxiGen can be used\nto fight machine-generated toxicity as finetuning improves the classifier\nsignificantly on our evaluation subset. Our code and data can be found at\nhttps://github.com/microsoft/ToxiGen.", "published": "2022-03-17 17:57:56", "link": "http://arxiv.org/abs/2203.09509v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HiStruct+: Improving Extractive Text Summarization with Hierarchical\n  Structure Information", "abstract": "Transformer-based language models usually treat texts as linear sequences.\nHowever, most texts also have an inherent hierarchical structure, i.e., parts\nof a text can be identified using their position in this hierarchy. In\naddition, section titles usually indicate the common topic of their respective\nsentences. We propose a novel approach to formulate, extract, encode and inject\nhierarchical structure information explicitly into an extractive summarization\nmodel based on a pre-trained, encoder-only Transformer language model\n(HiStruct+ model), which improves SOTA ROUGEs for extractive summarization on\nPubMed and arXiv substantially. Using various experimental settings on three\ndatasets (i.e., CNN/DailyMail, PubMed and arXiv), our HiStruct+ model\noutperforms a strong baseline collectively, which differs from our model only\nin that the hierarchical structure information is not injected. It is also\nobserved that the more conspicuous hierarchical structure the dataset has, the\nlarger improvements our method gains. The ablation study demonstrates that the\nhierarchical position information is the main contributor to our model's SOTA\nperformance.", "published": "2022-03-17 21:49:26", "link": "http://arxiv.org/abs/2203.09629v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dim Wihl Gat Tun: The Case for Linguistic Expertise in NLP for\n  Underdocumented Languages", "abstract": "Recent progress in NLP is driven by pretrained models leveraging massive\ndatasets and has predominantly benefited the world's political and economic\nsuperpowers. Technologically underserved languages are left behind because they\nlack such resources. Hundreds of underserved languages, nevertheless, have\navailable data sources in the form of interlinear glossed text (IGT) from\nlanguage documentation efforts. IGT remains underutilized in NLP work, perhaps\nbecause its annotations are only semi-structured and often language-specific.\nWith this paper, we make the case that IGT data can be leveraged successfully\nprovided that target language expertise is available. We specifically advocate\nfor collaboration with documentary linguists. Our paper provides a roadmap for\nsuccessful projects utilizing IGT data: (1) It is essential to define which NLP\ntasks can be accomplished with the given IGT data and how these will benefit\nthe speech community. (2) Great care and target language expertise is required\nwhen converting the data into structured formats commonly employed in NLP. (3)\nTask-specific and user-specific evaluation can help to ascertain that the tools\nwhich are created benefit the target language speech community. We illustrate\neach step through a case study on developing a morphological reinflection\nsystem for the Tsimchianic language Gitksan.", "published": "2022-03-17 22:02:25", "link": "http://arxiv.org/abs/2203.09632v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Natural Language Communication with a Teachable Agent", "abstract": "Conversational teachable agents offer a promising platform to support\nlearning, both in the classroom and in remote settings. In this context, the\nagent takes the role of the novice, while the student takes on the role of\nteacher. This framing is significant for its ability to elicit the Prot\\'eg\\'e\neffect in the student-teacher, a pedagogical phenomenon known to increase\nengagement in the teaching task, and also improve cognitive outcomes. In prior\nwork, teachable agents often take a passive role in the learning interaction,\nand there are few studies in which the agent and student engage in natural\nlanguage dialogue during the teaching task. This work investigates the effect\nof teaching modality when interacting with a virtual agent, via the web-based\nteaching platform, the Curiosity Notebook. A method of teaching the agent by\nselecting sentences from source material is compared to a method paraphrasing\nthe source material and typing text input to teach. A user study has been\nconducted to measure the effect teaching modality on the learning outcomes and\nengagement of the participants. The results indicate that teaching via\nparaphrasing and text input has a positive effect on learning outcomes for the\nmaterial covered, and also on aspects of affective engagement. Furthermore,\nincreased paraphrasing effort, as measured by the similarity between the source\nmaterial and the material the teacher conveyed to the robot, improves learning\noutcomes for participants.", "published": "2022-03-17 01:31:23", "link": "http://arxiv.org/abs/2203.09016v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Reducing Position Bias in Simultaneous Machine Translation with\n  Length-Aware Framework", "abstract": "Simultaneous machine translation (SiMT) starts translating while receiving\nthe streaming source inputs, and hence the source sentence is always incomplete\nduring translating. Different from the full-sentence MT using the conventional\nseq-to-seq architecture, SiMT often applies prefix-to-prefix architecture,\nwhich forces each target word to only align with a partial source prefix to\nadapt to the incomplete source in streaming inputs. However, the source words\nin the front positions are always illusoryly considered more important since\nthey appear in more prefixes, resulting in position bias, which makes the model\npay more attention on the front source positions in testing. In this paper, we\nfirst analyze the phenomenon of position bias in SiMT, and develop a\nLength-Aware Framework to reduce the position bias by bridging the structural\ngap between SiMT and full-sentence MT. Specifically, given the streaming\ninputs, we first predict the full-sentence length and then fill the future\nsource position with positional encoding, thereby turning the streaming inputs\ninto a pseudo full-sentence. The proposed framework can be integrated into most\nexisting SiMT methods to further improve performance. Experiments on two\nrepresentative SiMT methods, including the state-of-the-art adaptive policy,\nshow that our method successfully reduces the position bias and thereby\nachieves better SiMT performance.", "published": "2022-03-17 03:18:46", "link": "http://arxiv.org/abs/2203.09053v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "UNIMO-2: End-to-End Unified Vision-Language Grounded Learning", "abstract": "Vision-Language Pre-training (VLP) has achieved impressive performance on\nvarious cross-modal downstream tasks. However, most existing methods can only\nlearn from aligned image-caption data and rely heavily on expensive regional\nfeatures, which greatly limits their scalability and performance. In this\npaper, we propose an end-to-end unified-modal pre-training framework, namely\nUNIMO-2, for joint learning on both aligned image-caption data and unaligned\nimage-only and text-only corpus. We build a unified Transformer model to\njointly learn visual representations, textual representations and semantic\nalignment between images and texts. In particular, we propose to conduct\ngrounded learning on both images and texts via a sharing grounded space, which\nhelps bridge unaligned images and texts, and align the visual and textual\nsemantic spaces on different types of corpora. The experiments show that our\ngrounded learning method can improve textual and visual semantic alignment for\nimproving performance on various cross-modal tasks. Moreover, benefiting from\neffective joint modeling of different types of corpora, our model also achieves\nimpressive performance on single-modal visual and textual tasks. Our code and\nmodels are public at the UNIMO project page https://unimo-ptm.github.io/.", "published": "2022-03-17 03:53:11", "link": "http://arxiv.org/abs/2203.09067v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Gaussian Multi-head Attention for Simultaneous Machine Translation", "abstract": "Simultaneous machine translation (SiMT) outputs translation while receiving\nthe streaming source inputs, and hence needs a policy to determine where to\nstart translating. The alignment between target and source words often implies\nthe most informative source word for each target word, and hence provides the\nunified control over translation quality and latency, but unfortunately the\nexisting SiMT methods do not explicitly model the alignment to perform the\ncontrol. In this paper, we propose Gaussian Multi-head Attention (GMA) to\ndevelop a new SiMT policy by modeling alignment and translation in a unified\nmanner. For SiMT policy, GMA models the aligned source position of each target\nword, and accordingly waits until its aligned position to start translating. To\nintegrate the learning of alignment into the translation model, a Gaussian\ndistribution centered on predicted aligned position is introduced as an\nalignment-related prior, which cooperates with translation-related soft\nattention to determine the final attention. Experiments on En-Vi and De-En\ntasks show that our method outperforms strong baselines on the trade-off\nbetween translation and latency.", "published": "2022-03-17 04:01:25", "link": "http://arxiv.org/abs/2203.09072v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Knowledge Graph-Enabled Text-Based Automatic Personality Prediction", "abstract": "How people think, feel, and behave, primarily is a representation of their\npersonality characteristics. By being conscious of personality characteristics\nof individuals whom we are dealing with or decided to deal with, one can\ncompetently ameliorate the relationship, regardless of its type. With the rise\nof Internet-based communication infrastructures (social networks, forums,\netc.), a considerable amount of human communications take place there. The most\nprominent tool in such communications, is the language in written and spoken\nform that adroitly encodes all those essential personality characteristics of\nindividuals. Text-based Automatic Personality Prediction (APP) is the automated\nforecasting of the personality of individuals based on the generated/exchanged\ntext contents. This paper presents a novel knowledge graph-enabled approach to\ntext-based APP that relies on the Big Five personality traits. To this end,\ngiven a text a knowledge graph which is a set of interlinked descriptions of\nconcepts, was built through matching the input text's concepts with DBpedia\nknowledge base entries. Then, due to achieving more powerful representation the\ngraph was enriched with the DBpedia ontology, NRC Emotion Intensity Lexicon,\nand MRC psycholinguistic database information. Afterwards, the knowledge graph\nwhich is now a knowledgeable alternative for the input text was embedded to\nyield an embedding matrix. Finally, to perform personality predictions the\nresulting embedding matrix was fed to four suggested deep learning models\nindependently, which are based on convolutional neural network (CNN), simple\nrecurrent neural network (RNN), long short term memory (LSTM) and bidirectional\nlong short term memory (BiLSTM). The results indicated a considerable\nimprovements in prediction accuracies in all of the suggested classifiers.", "published": "2022-03-17 06:01:45", "link": "http://arxiv.org/abs/2203.09103v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Community-Driven Comprehensive Scientific Paper Summarization: Insight\n  from cvpaper.challenge", "abstract": "The present paper introduces a group activity involving writing summaries of\nconference proceedings by volunteer participants. The rapid increase in\nscientific papers is a heavy burden for researchers, especially non-native\nspeakers, who need to survey scientific literature. To alleviate this problem,\nwe organized a group of non-native English speakers to write summaries of\npapers presented at a computer vision conference to share the knowledge of the\npapers read by the group. We summarized a total of 2,000 papers presented at\nthe Conference on Computer Vision and Pattern Recognition, a top-tier\nconference on computer vision, in 2019 and 2020. We quantitatively analyzed\nparticipants' selection regarding which papers they read among the many\navailable papers. The experimental results suggest that we can summarize a wide\nrange of papers without asking participants to read papers unrelated to their\ninterests.", "published": "2022-03-17 06:31:17", "link": "http://arxiv.org/abs/2203.09109v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "ERNIE-GeoL: A Geography-and-Language Pre-trained Model and its\n  Applications in Baidu Maps", "abstract": "Pre-trained models (PTMs) have become a fundamental backbone for downstream\ntasks in natural language processing and computer vision. Despite initial gains\nthat were obtained by applying generic PTMs to geo-related tasks at Baidu Maps,\na clear performance plateau over time was observed. One of the main reasons for\nthis plateau is the lack of readily available geographic knowledge in generic\nPTMs. To address this problem, in this paper, we present ERNIE-GeoL, which is a\ngeography-and-language pre-trained model designed and developed for improving\nthe geo-related tasks at Baidu Maps. ERNIE-GeoL is elaborately designed to\nlearn a universal representation of geography-language by pre-training on\nlarge-scale data generated from a heterogeneous graph that contains abundant\ngeographic knowledge. Extensive quantitative and qualitative experiments\nconducted on large-scale real-world datasets demonstrate the superiority and\neffectiveness of ERNIE-GeoL. ERNIE-GeoL has already been deployed in production\nat Baidu Maps since April 2021, which significantly benefits the performance of\nvarious downstream tasks. This demonstrates that ERNIE-GeoL can serve as a\nfundamental backbone for a wide range of geo-related tasks.", "published": "2022-03-17 07:07:33", "link": "http://arxiv.org/abs/2203.09127v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Modeling Dual Read/Write Paths for Simultaneous Machine Translation", "abstract": "Simultaneous machine translation (SiMT) outputs translation while reading\nsource sentence and hence requires a policy to decide whether to wait for the\nnext source word (READ) or generate a target word (WRITE), the actions of which\nform a read/write path. Although the read/write path is essential to SiMT\nperformance, no direct supervision is given to the path in the existing\nmethods. In this paper, we propose a method of dual-path SiMT which introduces\nduality constraints to direct the read/write path. According to duality\nconstraints, the read/write path in source-to-target and target-to-source SiMT\nmodels can be mapped to each other. As a result, the two SiMT models can be\noptimized jointly by forcing their read/write paths to satisfy the mapping.\nExperiments on En-Vi and De-En tasks show that our method can outperform strong\nbaselines under all latency.", "published": "2022-03-17 08:35:36", "link": "http://arxiv.org/abs/2203.09163v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RoMe: A Robust Metric for Evaluating Natural Language Generation", "abstract": "Evaluating Natural Language Generation (NLG) systems is a challenging task.\nFirstly, the metric should ensure that the generated hypothesis reflects the\nreference's semantics. Secondly, it should consider the grammatical quality of\nthe generated sentence. Thirdly, it should be robust enough to handle various\nsurface forms of the generated sentence. Thus, an effective evaluation metric\nhas to be multifaceted. In this paper, we propose an automatic evaluation\nmetric incorporating several core aspects of natural language understanding\n(language competence, syntactic and semantic variation). Our proposed metric,\nRoMe, is trained on language features such as semantic similarity combined with\ntree edit distance and grammatical acceptability, using a self-supervised\nneural network to assess the overall quality of the generated sentence.\nMoreover, we perform an extensive robustness analysis of the state-of-the-art\nmethods and RoMe. Empirical results suggest that RoMe has a stronger\ncorrelation to human judgment over state-of-the-art metrics in evaluating\nsystem-generated sentences across several NLG tasks.", "published": "2022-03-17 09:07:39", "link": "http://arxiv.org/abs/2203.09183v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Abstract Interpretation on E-Graphs", "abstract": "Recent e-graph applications have typically considered concrete semantics of\nexpressions, where the notion of equivalence stems from concrete interpretation\nof expressions. However, equivalences that hold over one interpretation may not\nhold in an alternative interpretation. Such an observation can be exploited. We\nconsider the application of abstract interpretation to e-graphs, and show that\nwithin an e-graph, the lattice meet operation associated with the abstract\ndomain has a natural interpretation for an e-class, leading to improved\nprecision in over-approximation. In this extended abstract, we use Interval\nArithmetic (IA) to illustrate this point.", "published": "2022-03-17 09:29:44", "link": "http://arxiv.org/abs/2203.09191v1", "categories": ["cs.LO", "cs.CL"], "primary_category": "cs.LO"}
{"title": "Finding Structural Knowledge in Multimodal-BERT", "abstract": "In this work, we investigate the knowledge learned in the embeddings of\nmultimodal-BERT models. More specifically, we probe their capabilities of\nstoring the grammatical structure of linguistic data and the structure learned\nover objects in visual data. To reach that goal, we first make the inherent\nstructure of language and visuals explicit by a dependency parse of the\nsentences that describe the image and by the dependencies between the object\nregions in the image, respectively. We call this explicit visual structure the\n\\textit{scene tree}, that is based on the dependency tree of the language\ndescription. Extensive probing experiments show that the multimodal-BERT models\ndo not encode these scene trees.Code available at\n\\url{https://github.com/VSJMilewski/multimodal-probes}.", "published": "2022-03-17 13:20:01", "link": "http://arxiv.org/abs/2203.09306v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "EVA2.0: Investigating Open-Domain Chinese Dialogue Systems with\n  Large-Scale Pre-Training", "abstract": "Large-scale pre-training has shown remarkable performance in building\nopen-domain dialogue systems. However, previous works mainly focus on showing\nand evaluating the conversational performance of the released dialogue model,\nignoring the discussion of some key factors towards a powerful human-like\nchatbot, especially in Chinese scenarios. In this paper, we conduct extensive\nexperiments to investigate these under-explored factors, including data quality\ncontrol, model architecture designs, training approaches, and decoding\nstrategies. We propose EVA2.0, a large-scale pre-trained open-domain Chinese\ndialogue model with 2.8 billion parameters, and will make our models and codes\npublicly available. Automatic and human evaluations show that EVA2.0\nsignificantly outperforms other open-source counterparts. We also discuss the\nlimitations of this work by presenting some failure cases and pose some future\nresearch directions on large-scale Chinese open-domain dialogue systems.", "published": "2022-03-17 13:33:17", "link": "http://arxiv.org/abs/2203.09313v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "When Chosen Wisely, More Data Is What You Need: A Universal\n  Sample-Efficient Strategy For Data Augmentation", "abstract": "Data Augmentation (DA) is known to improve the generalizability of deep\nneural networks. Most existing DA techniques naively add a certain number of\naugmented samples without considering the quality and the added computational\ncost of these samples. To tackle this problem, a common strategy, adopted by\nseveral state-of-the-art DA methods, is to adaptively generate or re-weight\naugmented samples with respect to the task objective during training. However,\nthese adaptive DA methods: (1) are computationally expensive and not\nsample-efficient, and (2) are designed merely for a specific setting. In this\nwork, we present a universal DA technique, called Glitter, to overcome both\nissues. Glitter can be plugged into any DA method, making training\nsample-efficient without sacrificing performance. From a pre-generated pool of\naugmented samples, Glitter adaptively selects a subset of worst-case samples\nwith maximal loss, analogous to adversarial DA. Without altering the training\nstrategy, the task objective can be optimized on the selected subset. Our\nthorough experiments on the GLUE benchmark, SQuAD, and HellaSwag in three\nwidely used training setups including consistency training, self-distillation\nand knowledge distillation reveal that Glitter is substantially faster to train\nand achieves a competitive performance, compared to strong baselines.", "published": "2022-03-17 15:33:52", "link": "http://arxiv.org/abs/2203.09391v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "ECOLA: Enhanced Temporal Knowledge Embeddings with Contextualized\n  Language Representations", "abstract": "Since conventional knowledge embedding models cannot take full advantage of\nthe abundant textual information, there have been extensive research efforts in\nenhancing knowledge embedding using texts. However, existing enhancement\napproaches cannot apply to temporal knowledge graphs (tKGs), which contain\ntime-dependent event knowledge with complex temporal dynamics. Specifically,\nexisting enhancement approaches often assume knowledge embedding is\ntime-independent. In contrast, the entity embedding in tKG models usually\nevolves, which poses the challenge of aligning temporally relevant texts with\nentities. To this end, we propose to study enhancing temporal knowledge\nembedding with textual data in this paper. As an approach to this task, we\npropose Enhanced Temporal Knowledge Embeddings with Contextualized Language\nRepresentations (ECOLA), which takes the temporal aspect into account and\ninjects textual information into temporal knowledge embedding. To evaluate\nECOLA, we introduce three new datasets for training and evaluating ECOLA.\nExtensive experiments show that ECOLA significantly enhances temporal KG\nembedding models with up to 287% relative improvements regarding Hits@1 on the\nlink prediction task. The code and models are publicly available on\nhttps://anonymous.4open.science/r/ECOLA.", "published": "2022-03-17 20:08:25", "link": "http://arxiv.org/abs/2203.09590v5", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Responsible Natural Language Annotation for the Varieties of\n  Arabic", "abstract": "When building NLP models, there is a tendency to aim for broader coverage,\noften overlooking cultural and (socio)linguistic nuance. In this position\npaper, we make the case for care and attention to such nuances, particularly in\ndataset annotation, as well as the inclusion of cultural and linguistic\nexpertise in the process. We present a playbook for responsible dataset\ncreation for polyglossic, multidialectal languages. This work is informed by a\nstudy on Arabic annotation of social media content.", "published": "2022-03-17 20:23:27", "link": "http://arxiv.org/abs/2203.09597v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "DP-KB: Data Programming with Knowledge Bases Improves Transformer Fine\n  Tuning for Answer Sentence Selection", "abstract": "While transformers demonstrate impressive performance on many knowledge\nintensive (KI) tasks, their ability to serve as implicit knowledge bases (KBs)\nremains limited, as shown on several slot-filling, question-answering (QA),\nfact verification, and entity-linking tasks. In this paper, we implement an\nefficient, data-programming technique that enriches training data with\nKB-derived context and improves transformer utilization of encoded knowledge\nwhen fine-tuning for a particular QA task, namely answer sentence selection\n(AS2). Our method outperforms state of the art transformer approach on WikiQA\nand TrecQA, two widely studied AS2 benchmarks, increasing by 2.0% p@1, 1.3%\nMAP, 1.1% MRR, and 4.4% p@1, 0.9% MAP, 2.4% MRR, respectively. To demonstrate\nour improvements in an industry setting, we additionally evaluate our approach\non a proprietary dataset of Alexa QA pairs, and show increase of 2.3% F1 and\n2.0% MAP. We additionally find that these improvements remain even when KB\ncontext is omitted at inference time, allowing for the use of our models within\nexisting transformer workflows without additional latency or deployment costs.", "published": "2022-03-17 20:23:52", "link": "http://arxiv.org/abs/2203.09598v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On the Importance of Data Size in Probing Fine-tuned Models", "abstract": "Several studies have investigated the reasons behind the effectiveness of\nfine-tuning, usually through the lens of probing. However, these studies often\nneglect the role of the size of the dataset on which the model is fine-tuned.\nIn this paper, we highlight the importance of this factor and its undeniable\nrole in probing performance. We show that the extent of encoded linguistic\nknowledge depends on the number of fine-tuning samples. The analysis also\nreveals that larger training data mainly affects higher layers, and that the\nextent of this change is a factor of the number of iterations updating the\nmodel during fine-tuning rather than the diversity of the training samples.\nFinally, we show through a set of experiments that fine-tuning data size\naffects the recoverability of the changes made to the model's linguistic\nknowledge.", "published": "2022-03-17 21:45:17", "link": "http://arxiv.org/abs/2203.09627v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Deep Reinforcement Agent for Efficient Instant Search", "abstract": "Instant Search is a paradigm where a search system retrieves answers on the\nfly while typing. The na\\\"ive implementation of an Instant Search system would\nhit the search back-end for results each time a user types a key, imposing a\nvery high load on the underlying search system. In this paper, we propose to\naddress the load issue by identifying tokens that are semantically more salient\ntowards retrieving relevant documents and utilize this knowledge to trigger an\ninstant search selectively. We train a reinforcement agent that interacts\ndirectly with the search engine and learns to predict the word's importance.\nOur proposed method treats the underlying search system as a black box and is\nmore universally applicable to a diverse set of architectures. Furthermore, a\nnovel evaluation framework is presented to study the trade-off between the\nnumber of triggered searches and the system's performance. We utilize the\nframework to evaluate and compare the proposed reinforcement method with other\nintuitive baselines. Experimental results demonstrate the efficacy of the\nproposed method towards achieving a superior trade-off.", "published": "2022-03-17 22:47:15", "link": "http://arxiv.org/abs/2203.09644v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "AI Autonomy : Self-Initiated Open-World Continual Learning and\n  Adaptation", "abstract": "As more and more AI agents are used in practice, it is time to think about\nhow to make these agents fully autonomous so that they can (1) learn by\nthemselves continually in a self-motivated and self-initiated manner rather\nthan being retrained offline periodically on the initiation of human engineers\nand (2) accommodate or adapt to unexpected or novel circumstances. As the\nreal-world is an open environment that is full of unknowns or novelties, the\ncapabilities of detecting novelties, characterizing them,\naccommodating/adapting to them, gathering ground-truth training data and\nincrementally learning the unknowns/novelties become critical in making the AI\nagent more and more knowledgeable, powerful and self-sustainable over time. The\nkey challenge here is how to automate the process so that it is carried out\ncontinually on the agent's own initiative and through its own interactions with\nhumans, other agents and the environment just like human on-the-job learning.\nThis paper proposes a framework (called SOLA) for this learning paradigm to\npromote the research of building autonomous and continual learning enabled AI\nagents. To show feasibility, an implemented agent is also described.", "published": "2022-03-17 00:07:02", "link": "http://arxiv.org/abs/2203.08994v3", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.AI"}
{"title": "DU-VLG: Unifying Vision-and-Language Generation via Dual\n  Sequence-to-Sequence Pre-training", "abstract": "Due to the limitations of the model structure and pre-training objectives,\nexisting vision-and-language generation models cannot utilize pair-wise images\nand text through bi-directional generation. In this paper, we propose DU-VLG, a\nframework which unifies vision-and-language generation as sequence generation\nproblems. DU-VLG is trained with novel dual pre-training tasks: multi-modal\ndenoising autoencoder tasks and modality translation tasks. To bridge the gap\nbetween image understanding and generation, we further design a novel\ncommitment loss. We compare pre-training objectives on image captioning and\ntext-to-image generation datasets. Results show that DU-VLG yields better\nperformance than variants trained with uni-directional generation objectives or\nthe variant without the commitment loss. We also obtain higher scores compared\nto previous state-of-the-art systems on three vision-and-language generation\ntasks. In addition, human judges further confirm that our model generates real\nand relevant images as well as faithful and informative captions.", "published": "2022-03-17 03:18:22", "link": "http://arxiv.org/abs/2203.09052v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Time and the Value of Data", "abstract": "Managers often believe that collecting more data will continually improve the\naccuracy of their machine learning models. However, we argue in this paper that\nwhen data lose relevance over time, it may be optimal to collect a limited\namount of recent data instead of keeping around an infinite supply of older\n(less relevant) data. In addition, we argue that increasing the stock of data\nby including older datasets may, in fact, damage the model's accuracy.\nExpectedly, the model's accuracy improves by increasing the flow of data\n(defined as data collection rate); however, it requires other tradeoffs in\nterms of refreshing or retraining machine learning models more frequently.\n  Using these results, we investigate how the business value created by machine\nlearning models scales with data and when the stock of data establishes a\nsustainable competitive advantage. We argue that data's time-dependency weakens\nthe barrier to entry that the stock of data creates. As a result, a competing\nfirm equipped with a limited (yet sufficient) amount of recent data can develop\nmore accurate models. This result, coupled with the fact that older datasets\nmay deteriorate models' accuracy, suggests that created business value doesn't\nscale with the stock of available data unless the firm offloads less relevant\ndata from its data repository. Consequently, a firm's growth policy should\nincorporate a balance between the stock of historical data and the flow of new\ndata.\n  We complement our theoretical results with an experiment. In the experiment,\nwe empirically measure the loss in the accuracy of a next word prediction model\ntrained on datasets from various time periods. Our empirical measurements\nconfirm the economic significance of the value decline over time. For example,\n100MB of text data, after seven years, becomes as valuable as 50MB of current\ndata for the next word prediction task.", "published": "2022-03-17 06:53:46", "link": "http://arxiv.org/abs/2203.09118v1", "categories": ["cs.LG", "cs.CL", "econ.GN", "q-fin.EC"], "primary_category": "cs.LG"}
{"title": "Time Dependency, Data Flow, and Competitive Advantage", "abstract": "Data is fundamental to machine learning-based products and services and is\nconsidered strategic due to its externalities for businesses, governments,\nnon-profits, and more generally for society. It is renowned that the value of\norganizations (businesses, government agencies and programs, and even\nindustries) scales with the volume of available data. What is often less\nappreciated is that the data value in making useful organizational predictions\nwill range widely and is prominently a function of data characteristics and\nunderlying algorithms.\n  In this research, our goal is to study how the value of data changes over\ntime and how this change varies across contexts and business areas (e.g. next\nword prediction in the context of history, sports, politics). We focus on data\nfrom Reddit.com and compare the value's time-dependency across various Reddit\ntopics (Subreddits). We make this comparison by measuring the rate at which\nuser-generated text data loses its relevance to the algorithmic prediction of\nconversations. We show that different subreddits have different rates of\nrelevance decline over time.\n  Relating the text topics to various business areas of interest, we argue that\ncompeting in a business area in which data value decays rapidly alters\nstrategies to acquire competitive advantage. When data value decays rapidly,\naccess to a continuous flow of data will be more valuable than access to a\nfixed stock of data. In this kind of setting, improving user engagement and\nincreasing user-base help creating and maintaining a competitive advantage.", "published": "2022-03-17 07:09:30", "link": "http://arxiv.org/abs/2203.09128v1", "categories": ["cs.LG", "cs.CL", "econ.GN", "q-fin.EC"], "primary_category": "cs.LG"}
{"title": "How Many Data Samples is an Additional Instruction Worth?", "abstract": "Recently introduced instruction-paradigm empowers non-expert users to\nleverage NLP resources by defining a new task in natural language.\nInstruction-tuned models have significantly outperformed multitask learning\nmodels (without instruction); however they are far from state-of-the-art\ntask-specific models. Conventional approaches to improve model performance via\ncreating datasets with large number of task instances or architectural changes\nin the model may not be feasible for non-expert users. However, they can write\nalternate instructions to represent an instruction task. Is\nInstruction-augmentation helpful? We augment a subset of tasks in the expanded\nversion of NATURAL INSTRUCTIONS with additional instructions and find that it\nsignificantly improves model performance (up to 35%), especially in the\nlow-data regime. Our results indicate that an additional instruction can be\nequivalent to ~200 data samples on average across tasks.", "published": "2022-03-17 08:30:30", "link": "http://arxiv.org/abs/2203.09161v3", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Frost Hollow Experiments: Pavlovian Signalling as a Path to\n  Coordination and Communication Between Agents", "abstract": "Learned communication between agents is a powerful tool when approaching\ndecision-making problems that are hard to overcome by any single agent in\nisolation. However, continual coordination and communication learning between\nmachine agents or human-machine partnerships remains a challenging open\nproblem. As a stepping stone toward solving the continual communication\nlearning problem, in this paper we contribute a multi-faceted study into what\nwe term Pavlovian signalling -- a process by which learned, temporally extended\npredictions made by one agent inform decision-making by another agent with\ndifferent perceptual access to their shared environment. We seek to establish\nhow different temporal processes and representational choices impact Pavlovian\nsignalling between learning agents. To do so, we introduce a partially\nobservable decision-making domain we call the Frost Hollow. In this domain a\nprediction learning agent and a reinforcement learning agent are coupled into a\ntwo-part decision-making system that seeks to acquire sparse reward while\navoiding time-conditional hazards. We evaluate two domain variations: 1)\nmachine prediction and control learning in a linear walk, and 2) a prediction\nlearning machine interacting with a human participant in a virtual reality\nenvironment. Our results showcase the speed of learning for Pavlovian\nsignalling, the impact that different temporal representations do (and do not)\nhave on agent-agent coordination, and how temporal aliasing impacts agent-agent\nand human-agent interactions differently. As a main contribution, we establish\nPavlovian signalling as a natural bridge between fixed signalling paradigms and\nfully adaptive communication learning. Our results therefore point to an\nactionable, constructivist path towards continual communication learning\nbetween reinforcement learning agents, with potential impact in a range of\nreal-world settings.", "published": "2022-03-17 17:49:45", "link": "http://arxiv.org/abs/2203.09498v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "primary_category": "cs.AI"}
{"title": "Efficient Federated Learning on Knowledge Graphs via Privacy-preserving\n  Relation Embedding Aggregation", "abstract": "Federated learning (FL) can be essential in knowledge representation,\nreasoning, and data mining applications over multi-source knowledge graphs\n(KGs). A recent study FedE first proposes an FL framework that shares entity\nembeddings of KGs across all clients. However, entity embedding sharing from\nFedE would incur a severe privacy leakage. Specifically, the known entity\nembedding can be used to infer whether a specific relation between two entities\nexists in a private client. In this paper, we introduce a novel attack method\nthat aims to recover the original data based on the embedding information,\nwhich is further used to evaluate the vulnerabilities of FedE. Furthermore, we\npropose a Federated learning paradigm with privacy-preserving Relation\nembedding aggregation (FedR) to tackle the privacy issue in FedE. Besides,\nrelation embedding sharing can significantly reduce the communication cost due\nto its smaller size of queries. We conduct extensive experiments to evaluate\nFedR with five different KG embedding models and three datasets. Compared to\nFedE, FedR achieves similar utility and significant improvements regarding\nprivacy-preserving effect and communication efficiency on the link prediction\ntask.", "published": "2022-03-17 18:32:19", "link": "http://arxiv.org/abs/2203.09553v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Robotic Speech Synthesis: Perspectives on Interactions, Scenarios, and\n  Ethics", "abstract": "In recent years, many works have investigated the feasibility of\nconversational robots for performing specific tasks, such as healthcare and\ninterview. Along with this development comes a practical issue: how should we\nsynthesize robotic voices to meet the needs of different situations? In this\npaper, we discuss this issue from three perspectives: 1) the difficulties of\nsynthesizing non-verbal and interaction-oriented speech signals, particularly\nbackchannels; 2) the scenario classification for robotic voice synthesis; 3)\nthe ethical issues regarding the design of robot voice for its emotion and\nidentity. We present the findings of relevant literature and our prior work,\ntrying to bring the attention of human-robot interaction researchers to design\nbetter conversational robots in the future.", "published": "2022-03-17 20:24:17", "link": "http://arxiv.org/abs/2203.09599v1", "categories": ["cs.RO", "cs.CL", "cs.HC", "eess.AS"], "primary_category": "cs.RO"}
{"title": "Prediction of speech intelligibility with DNN-based performance measures", "abstract": "This paper presents a speech intelligibility model based on automatic speech\nrecognition (ASR), combining phoneme probabilities from deep neural networks\n(DNN) and a performance measure that estimates the word error rate from these\nprobabilities. This model does not require the clean speech reference nor the\nword labels during testing as the ASR decoding step, which finds the most\nlikely sequence of words given phoneme posterior probabilities, is omitted. The\nmodel is evaluated via the root-mean-squared error between the predicted and\nobserved speech reception thresholds from eight normal-hearing listeners. The\nrecognition task consists of identifying noisy words from a German matrix\nsentence test. The speech material was mixed with eight noise maskers covering\ndifferent modulation types, from speech-shaped stationary noise to a\nsingle-talker masker. The prediction performance is compared to five\nestablished models and an ASR-model using word labels. Two combinations of\nfeatures and networks were tested. Both include temporal information either at\nthe feature level (amplitude modulation filterbanks and a feed-forward network)\nor captured by the architecture (mel-spectrograms and a time-delay deep neural\nnetwork, TDNN). The TDNN model is on par with the DNN while reducing the number\nof parameters by a factor of 37; this optimization allows parallel streams on\ndedicated hearing aid hardware as a forward-pass can be computed within the\n10ms of each frame. The proposed model performs almost as well as the\nlabel-based model and produces more accurate predictions than the baseline\nmodels.", "published": "2022-03-17 08:05:38", "link": "http://arxiv.org/abs/2203.09148v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "To train or not to train adversarially: A study of bias mitigation\n  strategies for speaker recognition", "abstract": "Speaker recognition is increasingly used in several everyday applications\nincluding smart speakers, customer care centers and other speech-driven\nanalytics. It is crucial to accurately evaluate and mitigate biases present in\nmachine learning (ML) based speech technologies, such as speaker recognition,\nto ensure their inclusive adoption. ML fairness studies with respect to various\ndemographic factors in modern speaker recognition systems are lagging compared\nto other human-centered applications such as face recognition. Existing studies\non fairness in speaker recognition systems are largely limited to evaluating\nbiases at specific operating points of the systems, which can lead to false\nexpectations of fairness. Moreover, there are only a handful of bias mitigation\nstrategies developed for speaker recognition systems. In this paper, we\nsystematically evaluate the biases present in speaker recognition systems with\nrespect to gender across a range of system operating points. We also propose\nadversarial and multi-task learning techniques to improve the fairness of these\nsystems. We show through quantitative and qualitative evaluations that the\nproposed methods improve the fairness of ASV systems over baseline methods\ntrained using data balancing techniques. We also present a fairness-utility\ntrade-off analysis to jointly examine fairness and the overall system\nperformance. We show that although systems trained using adversarial techniques\nimprove fairness, they are prone to reduced utility. On the other hand,\nmulti-task methods can improve the fairness while retaining the utility. These\nfindings can inform the choice of bias mitigation strategies in the field of\nspeaker recognition.", "published": "2022-03-17 06:56:48", "link": "http://arxiv.org/abs/2203.09122v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Feature-informed Latent Space Regularization for Music Source Separation", "abstract": "The integration of additional side information to improve music source\nseparation has been investigated numerous times, e.g., by adding features to\nthe input or by adding learning targets in a multi-task learning scenario.\nThese approaches, however, require additional annotations such as musical\nscores, instrument labels, etc. in training and possibly during inference. The\navailable datasets for source separation do not usually provide these\nadditional annotations. In this work, we explore transfer learning strategies\nto incorporate VGGish features with a state-of-the-art source separation model;\nVGGish features are known to be a very condensed representation of audio\ncontent and have been successfully used in many MIR tasks. We introduce three\napproaches to incorporate the features, including two latent space\nregularization methods and one naive concatenation method. Experimental results\nshow that our proposed approaches improve several evaluation metrics for music\nsource separation.", "published": "2022-03-17 07:18:09", "link": "http://arxiv.org/abs/2203.09132v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Speaker recognition using residual signal of linear and nonlinear\n  prediction models", "abstract": "This Paper discusses the usefulness of the residual signal for speaker\nrecognition. It is shown that the combination of both a measure defined over\nLPCC coefficients and a measure defined over the energy of the residual signal\ngives rise to an improvement over the classical method which considers only the\nLPCC coefficients. If the residual signal is obtained from a linear prediction\nanalysis, the improvement is 2.63% (error rate drops from 6.31% to 3.68%) and\nif it is computed through a nonlinear predictive neural nets based model, the\nimprovement is 3.68%.", "published": "2022-03-17 10:36:58", "link": "http://arxiv.org/abs/2203.09231v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Assessing Progress of Parkinson s Disease Using Acoustic Analysis of\n  Phonation", "abstract": "This paper deals with a complex acoustic analysis of phonation in patients\nwith Parkinson's disease (PD) with a special focus on estimation of disease\nprogress that is described by 7 different clinical scales ,e. g. Unified\nParkinson's disease rating scale or Beck depression inventory. The analysis is\nbased on parametrization of 5 Czech vowels pronounced by 84 PD patients. Using\nclassification and regression trees we estimated all clinical scores with\nmaximal error lower or equal to 13 %. Best estimation was observed in the case\nof Mini-mental state examination (MAE = 0.77, estimation error 5.50 %. Finally,\nwe proposed a binary classification based on random forests that is able to\nidentify Parkinson's disease with sensitivity SEN = 92.86 % (SPE = 85.71 %).\nThe parametrization process was based on extraction of 107 speech features\nquantifying different clinical signs of hypokinetic dysarthria present in PD.", "published": "2022-03-17 12:57:40", "link": "http://arxiv.org/abs/2203.09295v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Robust and Complex Approach of Pathological Speech Signal Analysis", "abstract": "This paper presents a study of the approaches in the state-of-the-art in the\nfield of pathological speech signal analysis with a special focus on\nparametrization techniques. It provides a description of 92 speech features\nwhere some of them are already widely used in this field of science and some of\nthem have not been tried yet (they come from different areas of speech signal\nprocessing like speech recognition or coding). As an original contribution,\nthis work introduces 36 completely new pathological voice measures based on\nmodulation spectra, inferior colliculus coefficients, bicepstrum, sample and\napproximate entropy and empirical mode decomposition. The significance of these\nfeatures was tested on 3 (English, Spanish and Czech) pathological voice\ndatabases with respect to classification accuracy, sensitivity and specificity.", "published": "2022-03-17 15:54:44", "link": "http://arxiv.org/abs/2203.09402v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "TMS: A Temporal Multi-scale Backbone Design for Speaker Embedding", "abstract": "Speaker embedding is an important front-end module to explore discriminative\nspeaker features for many speech applications where speaker information is\nneeded. Current SOTA backbone networks for speaker embedding are designed to\naggregate multi-scale features from an utterance with multi-branch network\narchitectures for speaker representation. However, naively adding many branches\nof multi-scale features with the simple fully convolutional operation could not\nefficiently improve the performance due to the rapid increase of model\nparameters and computational complexity. Therefore, in the most current\nstate-of-the-art network architectures, only a few branches corresponding to a\nlimited number of temporal scales could be designed for speaker embeddings. To\naddress this problem, in this paper, we propose an effective temporal\nmulti-scale (TMS) model where multi-scale branches could be efficiently\ndesigned in a speaker embedding network almost without increasing computational\ncosts. The new model is based on the conventional TDNN, where the network\narchitecture is smartly separated into two modeling operators: a\nchannel-modeling operator and a temporal multi-branch modeling operator. Adding\ntemporal multi-scale in the temporal multi-branch operator needs only a little\nbit increase of the number of parameters, and thus save more computational\nbudget for adding more branches with large temporal scales. Moreover, in the\ninference stage, we further developed a systemic re-parameterization method to\nconvert the TMS-based model into a single-path-based topology in order to\nincrease inference speed. We investigated the performance of the new TMS method\nfor automatic speaker verification (ASV) on in-domain and out-of-domain\nconditions. Results show that the TMS-based model obtained a significant\nincrease in the performance over the SOTA ASV models, meanwhile, had a faster\ninference speed.", "published": "2022-03-17 05:49:35", "link": "http://arxiv.org/abs/2203.09098v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Contrastive Learning with Positive-Negative Frame Mask for Music\n  Representation", "abstract": "Self-supervised learning, especially contrastive learning, has made an\noutstanding contribution to the development of many deep learning research\nfields. Recently, researchers in the acoustic signal processing field noticed\nits success and leveraged contrastive learning for better music representation.\nTypically, existing approaches maximize the similarity between two distorted\naudio segments sampled from the same music. In other words, they ensure a\nsemantic agreement at the music level. However, those coarse-grained methods\nneglect some inessential or noisy elements at the frame level, which may be\ndetrimental to the model to learn the effective representation of music.\nTowards this end, this paper proposes a novel Positive-nEgative frame mask for\nMusic Representation based on the contrastive learning framework, abbreviated\nas PEMR. Concretely, PEMR incorporates a Positive-Negative Mask Generation\nmodule, which leverages transformer blocks to generate frame masks on the\nLog-Mel spectrogram. We can generate self-augmented negative and positive\nsamples by masking important components or inessential components,\nrespectively. We devise a novel contrastive learning objective to accommodate\nboth self-augmented positives/negatives sampled from the same music. We conduct\nexperiments on four public datasets. The experimental results of two\nmusic-related downstream tasks, music classification, and cover song\nidentification, demonstrate the generalization ability and transferability of\nmusic representation learned by PEMR.", "published": "2022-03-17 07:11:42", "link": "http://arxiv.org/abs/2203.09129v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Localizing Visual Sounds the Easy Way", "abstract": "Unsupervised audio-visual source localization aims at localizing visible\nsound sources in a video without relying on ground-truth localization for\ntraining. Previous works often seek high audio-visual similarities for likely\npositive (sounding) regions and low similarities for likely negative regions.\nHowever, accurately distinguishing between sounding and non-sounding regions is\nchallenging without manual annotations. In this work, we propose a simple yet\neffective approach for Easy Visual Sound Localization, namely EZ-VSL, without\nrelying on the construction of positive and/or negative regions during\ntraining. Instead, we align audio and visual spaces by seeking audio-visual\nrepresentations that are aligned in, at least, one location of the associated\nimage, while not matching other images, at any location. We also introduce a\nnovel object guided localization scheme at inference time for improved\nprecision. Our simple and effective framework achieves state-of-the-art\nperformance on two popular benchmarks, Flickr SoundNet and VGG-Sound Source. In\nparticular, we improve the CIoU of the Flickr SoundNet test set from 76.80% to\n83.94%, and on the VGG-Sound Source dataset from 34.60% to 38.85%. The code is\navailable at https://github.com/stoneMo/EZ-VSL.", "published": "2022-03-17 13:52:58", "link": "http://arxiv.org/abs/2203.09324v2", "categories": ["cs.CV", "cs.LG", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
