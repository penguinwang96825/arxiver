{"title": "Incorporating Bilingual Dictionaries for Low Resource Semi-Supervised\n  Neural Machine Translation", "abstract": "We explore ways of incorporating bilingual dictionaries to enable\nsemi-supervised neural machine translation. Conventional back-translation\nmethods have shown success in leveraging target side monolingual data. However,\nsince the quality of back-translation models is tied to the size of the\navailable parallel corpora, this could adversely impact the synthetically\ngenerated sentences in a low resource setting. We propose a simple data\naugmentation technique to address both this shortcoming. We incorporate widely\navailable bilingual dictionaries that yield word-by-word translations to\ngenerate synthetic sentences. This automatically expands the vocabulary of the\nmodel while maintaining high quality content. Our method shows an appreciable\nimprovement in performance over strong baselines.", "published": "2020-04-05 02:14:14", "link": "http://arxiv.org/abs/2004.02071v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Machine Translation Pre-training for Data-to-Text Generation -- A Case\n  Study in Czech", "abstract": "While there is a large body of research studying deep learning methods for\ntext generation from structured data, almost all of it focuses purely on\nEnglish. In this paper, we study the effectiveness of machine translation based\npre-training for data-to-text generation in non-English languages. Since the\nstructured data is generally expressed in English, text generation into other\nlanguages involves elements of translation, transliteration and copying -\nelements already encoded in neural machine translation systems. Moreover, since\ndata-to-text corpora are typically small, this task can benefit greatly from\npre-training. Based on our experiments on Czech, a morphologically complex\nlanguage, we find that pre-training lets us train end-to-end models with\nsignificantly improved performance, as judged by automatic metrics and human\nevaluation. We also show that this approach enjoys several desirable\nproperties, including improved performance in low data scenarios and robustness\nto unseen slot values.", "published": "2020-04-05 02:47:16", "link": "http://arxiv.org/abs/2004.02077v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Resource for Studying Chatino Verbal Morphology", "abstract": "We present the first resource focusing on the verbal inflectional morphology\nof San Juan Quiahije Chatino, a tonal mesoamerican language spoken in Mexico.\nWe provide a collection of complete inflection tables of 198 lemmata, with\nmorphological tags based on the UniMorph schema. We also provide baseline\nresults on three core NLP tasks: morphological analysis, lemmatization, and\nmorphological inflection.", "published": "2020-04-05 03:30:01", "link": "http://arxiv.org/abs/2004.02083v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Domain Clusters in Pretrained Language Models", "abstract": "The notion of \"in-domain data\" in NLP is often over-simplistic and vague, as\ntextual data varies in many nuanced linguistic aspects such as topic, style or\nlevel of formality. In addition, domain labels are many times unavailable,\nmaking it challenging to build domain-specific systems. We show that massive\npre-trained language models implicitly learn sentence representations that\ncluster by domains without supervision -- suggesting a simple data-driven\ndefinition of domains in textual data. We harness this property and propose\ndomain data selection methods based on such models, which require only a small\nset of in-domain monolingual data. We evaluate our data selection methods for\nneural machine translation across five diverse domains, where they outperform\nan established approach as measured by both BLEU and by precision and recall of\nsentence selection with respect to an oracle.", "published": "2020-04-05 06:22:16", "link": "http://arxiv.org/abs/2004.02105v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GIANT: Scalable Creation of a Web-scale Ontology", "abstract": "Understanding what online users may pay attention to is key to content\nrecommendation and search services. These services will benefit from a highly\nstructured and web-scale ontology of entities, concepts, events, topics and\ncategories. While existing knowledge bases and taxonomies embody a large volume\nof entities and categories, we argue that they fail to discover properly\ngrained concepts, events and topics in the language style of online population.\nNeither is a logically structured ontology maintained among these notions. In\nthis paper, we present GIANT, a mechanism to construct a user-centered,\nweb-scale, structured ontology, containing a large number of natural language\nphrases conforming to user attentions at various granularities, mined from a\nvast volume of web documents and search click graphs. Various types of edges\nare also constructed to maintain a hierarchy in the ontology. We present our\ngraph-neural-network-based techniques used in GIANT, and evaluate the proposed\nmethods as compared to a variety of baselines. GIANT has produced the Attention\nOntology, which has been deployed in various Tencent applications involving\nover a billion users. Online A/B testing performed on Tencent QQ Browser shows\nthat Attention Ontology can significantly improve click-through rates in news\nrecommendation.", "published": "2020-04-05 07:51:23", "link": "http://arxiv.org/abs/2004.02118v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reference Language based Unsupervised Neural Machine Translation", "abstract": "Exploiting a common language as an auxiliary for better translation has a\nlong tradition in machine translation and lets supervised learning-based\nmachine translation enjoy the enhancement delivered by the well-used pivot\nlanguage in the absence of a source language to target language parallel\ncorpus. The rise of unsupervised neural machine translation (UNMT) almost\ncompletely relieves the parallel corpus curse, though UNMT is still subject to\nunsatisfactory performance due to the vagueness of the clues available for its\ncore back-translation training. Further enriching the idea of pivot translation\nby extending the use of parallel corpora beyond the source-target paradigm, we\npropose a new reference language-based framework for UNMT, RUNMT, in which the\nreference language only shares a parallel corpus with the source, but this\ncorpus still indicates a signal clear enough to help the reconstruction\ntraining of UNMT through a proposed reference agreement mechanism. Experimental\nresults show that our methods improve the quality of UNMT over that of a strong\nbaseline that uses only one auxiliary language, demonstrating the usefulness of\nthe proposed reference language-based UNMT and establishing a good start for\nthe community.", "published": "2020-04-05 08:28:08", "link": "http://arxiv.org/abs/2004.02127v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reinforced Multi-task Approach for Multi-hop Question Generation", "abstract": "Question generation (QG) attempts to solve the inverse of question answering\n(QA) problem by generating a natural language question given a document and an\nanswer. While sequence to sequence neural models surpass rule-based systems for\nQG, they are limited in their capacity to focus on more than one supporting\nfact. For QG, we often require multiple supporting facts to generate\nhigh-quality questions. Inspired by recent works on multi-hop reasoning in QA,\nwe take up Multi-hop question generation, which aims at generating relevant\nquestions based on supporting facts in the context. We employ multitask\nlearning with the auxiliary task of answer-aware supporting fact prediction to\nguide the question generator. In addition, we also proposed a question-aware\nreward function in a Reinforcement Learning (RL) framework to maximize the\nutilization of the supporting facts. We demonstrate the effectiveness of our\napproach through experiments on the multi-hop question answering dataset,\nHotPotQA. Empirical evaluation shows our model to outperform the single-hop\nneural question generation models on both automatic evaluation metrics such as\nBLEU, METEOR, and ROUGE, and human evaluation metrics for quality and coverage\nof the generated questions.", "published": "2020-04-05 10:16:59", "link": "http://arxiv.org/abs/2004.02143v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FastBERT: a Self-distilling BERT with Adaptive Inference Time", "abstract": "Pre-trained language models like BERT have proven to be highly performant.\nHowever, they are often computationally expensive in many practical scenarios,\nfor such heavy models can hardly be readily implemented with limited resources.\nTo improve their efficiency with an assured model performance, we propose a\nnovel speed-tunable FastBERT with adaptive inference time. The speed at\ninference can be flexibly adjusted under varying demands, while redundant\ncalculation of samples is avoided. Moreover, this model adopts a unique\nself-distillation mechanism at fine-tuning, further enabling a greater\ncomputational efficacy with minimal loss in performance. Our model achieves\npromising results in twelve English and Chinese datasets. It is able to speed\nup by a wide range from 1 to 12 times than BERT if given different speedup\nthresholds to make a speed-performance tradeoff.", "published": "2020-04-05 12:29:20", "link": "http://arxiv.org/abs/2004.02178v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting and Understanding Generalization Barriers for Neural Machine\n  Translation", "abstract": "Generalization to unseen instances is our eternal pursuit for all data-driven\nmodels. However, for realistic task like machine translation, the traditional\napproach measuring generalization in an average sense provides poor\nunderstanding for the fine-grained generalization ability. As a remedy, this\npaper attempts to identify and understand generalization barrier words within\nan unseen input sentence that \\textit{cause} the degradation of fine-grained\ngeneralization. We propose a principled definition of generalization barrier\nwords and a modified version which is tractable in computation. Based on the\nmodified one, we propose three simple methods for barrier detection by the\nsearch-aware risk estimation through counterfactual generation. We then conduct\nextensive analyses on those detected generalization barrier words on both\nZh$\\Leftrightarrow$En NIST benchmarks from various perspectives. Potential\nusage of the detected barrier words is also discussed.", "published": "2020-04-05 12:33:51", "link": "http://arxiv.org/abs/2004.02181v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Arabic Offensive Language on Twitter: Analysis and Experiments", "abstract": "Detecting offensive language on Twitter has many applications ranging from\ndetecting/predicting bullying to measuring polarization. In this paper, we\nfocus on building a large Arabic offensive tweet dataset. We introduce a method\nfor building a dataset that is not biased by topic, dialect, or target. We\nproduce the largest Arabic dataset to date with special tags for vulgarity and\nhate speech. We thoroughly analyze the dataset to determine which topics,\ndialects, and gender are most associated with offensive tweets and how Arabic\nspeakers use offensive language. Lastly, we conduct many experiments to produce\nstrong results (F1 = 83.2) on the dataset using SOTA techniques.", "published": "2020-04-05 13:05:11", "link": "http://arxiv.org/abs/2004.02192v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AR: Auto-Repair the Synthetic Data for Neural Machine Translation", "abstract": "Compared with only using limited authentic parallel data as training corpus,\nmany studies have proved that incorporating synthetic parallel data, which\ngenerated by back translation (BT) or forward translation (FT, or\nselftraining), into the NMT training process can significantly improve\ntranslation quality. However, as a well-known shortcoming, synthetic parallel\ndata is noisy because they are generated by an imperfect NMT system. As a\nresult, the improvements in translation quality bring by the synthetic parallel\ndata are greatly diminished. In this paper, we propose a novel Auto- Repair\n(AR) framework to improve the quality of synthetic data. Our proposed AR model\ncan learn the transformation from low quality (noisy) input sentence to high\nquality sentence based on large scale monolingual data with BT and FT\ntechniques. The noise in synthetic parallel data will be sufficiently\neliminated by the proposed AR model and then the repaired synthetic parallel\ndata can help the NMT models to achieve larger improvements. Experimental\nresults show that our approach can effective improve the quality of synthetic\nparallel data and the NMT model with the repaired synthetic data achieves\nconsistent improvements on both WMT14 EN!DE and IWSLT14 DE!EN translation\ntasks.", "published": "2020-04-05 13:18:18", "link": "http://arxiv.org/abs/2004.02196v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Stylistic Dialogue Generation via Information-Guided Reinforcement\n  Learning Strategy", "abstract": "Stylistic response generation is crucial for building an engaging dialogue\nsystem for industrial use. While it has attracted much research interest,\nexisting methods often generate stylistic responses at the cost of the content\nquality (relevance and fluency). To enable better balance between the content\nquality and the style, we introduce a new training strategy, know as\nInformation-Guided Reinforcement Learning (IG-RL). In IG-RL, a training model\nis encouraged to explore stylistic expressions while being constrained to\nmaintain its content quality. This is achieved by adopting reinforcement\nlearning strategy with statistical style information guidance for\nquality-preserving explorations. Experiments on two datasets show that the\nproposed approach outperforms several strong baselines in terms of the overall\nresponse performance.", "published": "2020-04-05 13:58:14", "link": "http://arxiv.org/abs/2004.02202v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Syntax-driven Iterative Expansion Language Models for Controllable Text\n  Generation", "abstract": "The dominant language modeling paradigm handles text as a sequence of\ndiscrete tokens. While that approach can capture the latent structure of the\ntext, it is inherently constrained to sequential dynamics for text generation.\nWe propose a new paradigm for introducing a syntactic inductive bias into\nneural text generation, where the dependency parse tree is used to drive the\nTransformer model to generate sentences iteratively.\n  Our experiments show that this paradigm is effective at text generation, with\nquality between LSTMs and Transformers, and comparable diversity, requiring\nless than half their decoding steps, and its generation process allows direct\ncontrol over the syntactic constructions of the generated text, enabling the\ninduction of stylistic variations.", "published": "2020-04-05 14:29:40", "link": "http://arxiv.org/abs/2004.02211v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prototype-to-Style: Dialogue Generation with Style-Aware Editing on\n  Retrieval Memory", "abstract": "The ability of a dialog system to express prespecified language style during\nconversations has a direct, positive impact on its usability and on user\nsatisfaction. We introduce a new prototype-to-style (PS) framework to tackle\nthe challenge of stylistic dialogue generation. The framework uses an\nInformation Retrieval (IR) system and extracts a response prototype from the\nretrieved response. A stylistic response generator then takes the prototype and\nthe desired language style as model input to obtain a high-quality and\nstylistic response. To effectively train the proposed model, we propose a new\nstyle-aware learning objective as well as a de-noising learning strategy.\nResults on three benchmark datasets from two languages demonstrate that the\nproposed approach significantly outperforms existing baselines in both\nin-domain and cross-domain evaluations", "published": "2020-04-05 14:36:15", "link": "http://arxiv.org/abs/2004.02214v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantics of the Unwritten: The Effect of End of Paragraph and Sequence\n  Tokens on Text Generation with GPT2", "abstract": "The semantics of a text is manifested not only by what is read, but also by\nwhat is not read. In this article, we will study how the implicit \"not read\"\ninformation such as end-of-paragraph (\\eop) and end-of-sequence (\\eos) affect\nthe quality of text generation. Specifically, we find that the pre-trained\nlanguage model GPT2 can generate better continuations by learning to generate\nthe \\eop in the fine-tuning stage. Experimental results on English story\ngeneration show that \\eop can lead to higher BLEU score and lower \\eos\nperplexity. We also conduct experiments on a self-collected Chinese essay\ndataset with Chinese-GPT2, a character level LM without \\eop or \\eos during\npre-training. Experimental results show that the Chinese GPT2 can generate\nbetter essay endings with \\eop.", "published": "2020-04-05 16:55:09", "link": "http://arxiv.org/abs/2004.02251v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hierarchical Entity Typing via Multi-level Learning to Rank", "abstract": "We propose a novel method for hierarchical entity classification that\nembraces ontological structure at both training and during prediction. At\ntraining, our novel multi-level learning-to-rank loss compares positive types\nagainst negative siblings according to the type tree. During prediction, we\ndefine a coarse-to-fine decoder that restricts viable candidates at each level\nof the ontology based on already predicted parent type(s). We achieve\nstate-of-the-art across multiple datasets, particularly with respect to strict\naccuracy.", "published": "2020-04-05 19:27:18", "link": "http://arxiv.org/abs/2004.02286v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Domain-based Latent Personal Analysis and its use for impersonation\n  detection in social media", "abstract": "Zipf's law defines an inverse proportion between a word's ranking in a given\ncorpus and its frequency in it, roughly dividing the vocabulary into frequent\nwords and infrequent ones. Here, we stipulate that within a domain an author's\nsignature can be derived from, in loose terms, the author's missing popular\nwords and frequently used infrequent-words. We devise a method, termed Latent\nPersonal Analysis (LPA), for finding domain-based attributes for entities in a\ndomain: their distance from the domain and their signature, which determines\nhow they most differ from a domain. We identify the most suitable distance\nmetric for the method among several and construct the distances and personal\nsignatures for authors, the domain's entities. The signature consists of both\nover-used terms (compared to the average), and missing popular terms. We\nvalidate the correctness and power of the signatures in identifying users and\nset existence conditions. We then show uses for the method in explainable\nauthorship attribution: we define algorithms that utilize LPA to identify two\ntypes of impersonation in social media: (1) authors with sockpuppets (multiple)\naccounts; (2) front users accounts, operated by several authors. We validate\nthe algorithms and employ them over a large scale dataset obtained from a\nsocial media site with over 4000 users. We corroborate these results using\ntemporal rate analysis. LPA can further be used to devise personal attributes\nin a wide range of scientific domains in which the constituents have a\nlong-tail distribution of elements.", "published": "2020-04-05 23:00:09", "link": "http://arxiv.org/abs/2004.02346v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adding A Filter Based on The Discriminator to Improve Unconditional Text\n  Generation", "abstract": "The autoregressive language model (ALM) trained with maximum likelihood\nestimation (MLE) is widely used in unconditional text generation. Due to\nexposure bias, the generated texts still suffer from low quality and diversity.\nThis presents statistically as a discrepancy between the real text and\ngenerated text. Some research shows a discriminator can detect this\ndiscrepancy. Because the discriminator can encode more information than the\ngenerator, discriminator has the potentiality to improve generator. To\nalleviate the exposure bias, generative adversarial networks (GAN) use the\ndiscriminator to update the generator's parameters directly, but they fail by\nbeing evaluated precisely. A critical reason for the failure is the difference\nbetween the discriminator input and the ALM input. We propose a novel mechanism\nby adding a filter which has the same input as the discriminator. First,\ndiscriminator detects the discrepancy signals and passes to filter directly (or\nby learning). Then, we use the filter to reject some generated samples with a\nsampling-based method. Thus, the original generative distribution is revised to\nreduce the discrepancy. Two ALMs, RNN-based and Transformer-based, are\nexperimented. Evaluated precisely by three metrics, our mechanism consistently\noutperforms the ALMs and all kinds of GANs across two benchmark data sets.", "published": "2020-04-05 09:34:52", "link": "http://arxiv.org/abs/2004.02135v5", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Understanding Learning Dynamics for Neural Machine Translation", "abstract": "Despite the great success of NMT, there still remains a severe challenge: it\nis hard to interpret the internal dynamics during its training process. In this\npaper we propose to understand learning dynamics of NMT by using a recent\nproposed technique named Loss Change Allocation\n(LCA)~\\citep{lan-2019-loss-change-allocation}. As LCA requires calculating the\ngradient on an entire dataset for each update, we instead present an\napproximate to put it into practice in NMT scenario. %motivated by the lesson\nfrom sgd. Our simulated experiment shows that such approximate calculation is\nefficient and is empirically proved to deliver consistent results to the\nbrute-force implementation. In particular, extensive experiments on two\nstandard translation benchmark datasets reveal some valuable findings.", "published": "2020-04-05 13:32:58", "link": "http://arxiv.org/abs/2004.02199v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Continual Domain-Tuning for Pretrained Language Models", "abstract": "Pre-trained language models (LM) such as BERT, DistilBERT, and RoBERTa can be\ntuned for different domains (domain-tuning) by continuing the pre-training\nphase on a new target domain corpus. This simple domain tuning (SDT) technique\nhas been widely used to create domain-tuned models such as BioBERT, SciBERT and\nClinicalBERT. However, during the pretraining phase on the target domain, the\nLM models may catastrophically forget the patterns learned from their source\ndomain. In this work, we study the effects of catastrophic forgetting on\ndomain-tuned LM models and investigate methods that mitigate its negative\neffects. We propose continual learning (CL) based alternatives for SDT, that\naim to reduce catastrophic forgetting. We show that these methods may increase\nthe performance of LM models on downstream target domain tasks. Additionally,\nwe also show that constraining the LM model from forgetting the source domain\nleads to downstream task models that are more robust to domain shifts. We\nanalyze the computational cost of using our proposed CL methods and provide\nrecommendations for computationally lightweight and effective CL domain-tuning\nprocedures.", "published": "2020-04-05 19:31:44", "link": "http://arxiv.org/abs/2004.02288v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Speaker Recognition using SincNet and X-Vector Fusion", "abstract": "In this paper, we propose an innovative approach to perform speaker\nrecognition by fusing two recently introduced deep neural networks (DNNs)\nnamely - SincNet and X-Vector. The idea behind using SincNet filters on the raw\nspeech waveform is to extract more distinguishing frequency-related features in\nthe initial convolution layers of the CNN architecture. X-Vectors are used to\ntake advantage of the fact that this embedding is an efficient method to churn\nout fixed dimension features from variable length speech utterances, something\nwhich is challenging in plain CNN techniques, making it efficient both in terms\nof speed and accuracy. Our approach uses the best of both worlds by combining\nX-vector in the later layers while using SincNet filters in the initial layers\nof our deep model. This approach allows the network to learn better embedding\nand converge quicker. Previous works use either X-Vector or SincNet Filters or\nsome modifications, however we introduce a novel fusion architecture wherein we\nhave combined both the techniques to gather more information about the speech\nsignal hence, giving us better results. Our method focuses on the VoxCeleb1\ndataset for speaker recognition, and we have used it for both training and\ntesting purposes.", "published": "2020-04-05 14:44:14", "link": "http://arxiv.org/abs/2004.02219v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Natural language processing for word sense disambiguation and\n  information extraction", "abstract": "This research work deals with Natural Language Processing (NLP) and\nextraction of essential information in an explicit form. The most common among\nthe information management strategies is Document Retrieval (DR) and\nInformation Filtering. DR systems may work as combine harvesters, which bring\nback useful material from the vast fields of raw material. With large amount of\npotentially useful information in hand, an Information Extraction (IE) system\ncan then transform the raw material by refining and reducing it to a germ of\noriginal text. A Document Retrieval system collects the relevant documents\ncarrying the required information, from the repository of texts. An IE system\nthen transforms them into information that is more readily digested and\nanalyzed. It isolates relevant text fragments, extracts relevant information\nfrom the fragments, and then arranges together the targeted information in a\ncoherent framework. The thesis presents a new approach for Word Sense\nDisambiguation using thesaurus. The illustrative examples supports the\neffectiveness of this approach for speedy and effective disambiguation. A\nDocument Retrieval method, based on Fuzzy Logic has been described and its\napplication is illustrated. A question-answering system describes the operation\nof information extraction from the retrieved text documents. The process of\ninformation extraction for answering a query is considerably simplified by\nusing a Structured Description Language (SDL) which is based on cardinals of\nqueries in the form of who, what, when, where and why. The thesis concludes\nwith the presentation of a novel strategy based on Dempster-Shafer theory of\nevidential reasoning, for document retrieval and information extraction. This\nstrategy permits relaxation of many limitations, which are inherent in Bayesian\nprobabilistic approach.", "published": "2020-04-05 17:13:43", "link": "http://arxiv.org/abs/2004.02256v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Finding the Optimal Vocabulary Size for Neural Machine Translation", "abstract": "We cast neural machine translation (NMT) as a classification task in an\nautoregressive setting and analyze the limitations of both classification and\nautoregression components. Classifiers are known to perform better with\nbalanced class distributions during training. Since the Zipfian nature of\nlanguages causes imbalanced classes, we explore its effect on NMT. We analyze\nthe effect of various vocabulary sizes on NMT performance on multiple languages\nwith many data sizes, and reveal an explanation for why certain vocabulary\nsizes are better than others.", "published": "2020-04-05 22:17:34", "link": "http://arxiv.org/abs/2004.02334v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "TAPAS: Weakly Supervised Table Parsing via Pre-training", "abstract": "Answering natural language questions over tables is usually seen as a\nsemantic parsing task. To alleviate the collection cost of full logical forms,\none popular approach focuses on weak supervision consisting of denotations\ninstead of logical forms. However, training semantic parsers from weak\nsupervision poses difficulties, and in addition, the generated logical forms\nare only used as an intermediate step prior to retrieving the denotation. In\nthis paper, we present TAPAS, an approach to question answering over tables\nwithout generating logical forms. TAPAS trains from weak supervision, and\npredicts the denotation by selecting table cells and optionally applying a\ncorresponding aggregation operator to such selection. TAPAS extends BERT's\narchitecture to encode tables as input, initializes from an effective joint\npre-training of text segments and tables crawled from Wikipedia, and is trained\nend-to-end. We experiment with three different semantic parsing datasets, and\nfind that TAPAS outperforms or rivals semantic parsing models by improving\nstate-of-the-art accuracy on SQA from 55.1 to 67.2 and performing on par with\nthe state-of-the-art on WIKISQL and WIKITQ, but with a simpler model\narchitecture. We additionally find that transfer learning, which is trivial in\nour setting, from WIKISQL to WIKITQ, yields 48.7 accuracy, 4.2 points above the\nstate-of-the-art.", "published": "2020-04-05 23:18:37", "link": "http://arxiv.org/abs/2004.02349v2", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Optimus: Organizing Sentences via Pre-trained Modeling of a Latent Space", "abstract": "When trained effectively, the Variational Autoencoder (VAE) can be both a\npowerful generative model and an effective representation learning framework\nfor natural language. In this paper, we propose the first large-scale language\nVAE model, Optimus. A universal latent embedding space for sentences is first\npre-trained on large text corpus, and then fine-tuned for various language\ngeneration and understanding tasks. Compared with GPT-2, Optimus enables guided\nlanguage generation from an abstract level using the latent vectors. Compared\nwith BERT, Optimus can generalize better on low-resource language understanding\ntasks due to the smooth latent space structure. Extensive experimental results\non a wide range of language tasks demonstrate the effectiveness of Optimus. It\nachieves new state-of-the-art on VAE language modeling benchmarks. We hope that\nour first pre-trained big VAE language model itself and results can help the\nNLP community renew the interests of deep generative models in the era of\nlarge-scale pre-training, and make these principled methods more practical.", "published": "2020-04-05 06:20:18", "link": "http://arxiv.org/abs/2004.04092v4", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Using Cyclic Noise as the Source Signal for Neural Source-Filter-based\n  Speech Waveform Model", "abstract": "Neural source-filter (NSF) waveform models generate speech waveforms by\nmorphing sine-based source signals through dilated convolution in the time\ndomain. Although the sine-based source signals help the NSF models to produce\nvoiced sounds with specified pitch, the sine shape may constrain the generated\nwaveform when the target voiced sounds are less periodic. In this paper, we\npropose a more flexible source signal called cyclic noise, a quasi-periodic\nnoise sequence given by the convolution of a pulse train and a static random\nnoise with a trainable decaying rate that controls the signal shape. We further\npropose a masked spectral loss to guide the NSF models to produce periodic\nvoiced sounds from the cyclic noise-based source signal. Results from a\nlarge-scale listening test demonstrated the effectiveness of the cyclic noise\nand the masked spectral loss on speaker-independent NSF models in\ncopy-synthesis experiments on the CMU ARCTIC database.", "published": "2020-04-05 13:00:56", "link": "http://arxiv.org/abs/2004.02191v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Semi-supervised acoustic and language model training for English-isiZulu\n  code-switched speech recognition", "abstract": "We present an analysis of semi-supervised acoustic and language model\ntraining for English-isiZulu code-switched ASR using soap opera speech.\nApproximately 11 hours of untranscribed multilingual speech was transcribed\nautomatically using four bilingual code-switching transcription systems\noperating in English-isiZulu, English-isiXhosa, English-Setswana and\nEnglish-Sesotho. These transcriptions were incorporated into the acoustic and\nlanguage model training sets. Results showed that the TDNN-F acoustic models\nbenefit from the additional semi-supervised data and that even better\nperformance could be achieved by including additional CNN layers. Using these\nCNN-TDNN-F acoustic models, a first iteration of semi-supervised training\nachieved an absolute mixed-language WER reduction of 3.4%, and a further 2.2%\nafter a second iteration. Although the languages in the untranscribed data were\nunknown, the best results were obtained when all automatically transcribed data\nwas used for training and not just the utterances classified as\nEnglish-isiZulu. Despite reducing perplexity, the semi-supervised language\nmodel was not able to improve the ASR performance.", "published": "2020-04-05 06:27:29", "link": "http://arxiv.org/abs/2004.04054v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
