{"title": "Sequence Discriminative Training for Deep Learning based Acoustic\n  Keyword Spotting", "abstract": "Speech recognition is a sequence prediction problem. Besides employing\nvarious deep learning approaches for framelevel classification, sequence-level\ndiscriminative training has been proved to be indispensable to achieve the\nstate-of-the-art performance in large vocabulary continuous speech recognition\n(LVCSR). However, keyword spotting (KWS), as one of the most common speech\nrecognition tasks, almost only benefits from frame-level deep learning due to\nthe difficulty of getting competing sequence hypotheses. The few studies on\nsequence discriminative training for KWS are limited for fixed vocabulary or\nLVCSR based methods and have not been compared to the state-of-the-art deep\nlearning based KWS approaches. In this paper, a sequence discriminative\ntraining framework is proposed for both fixed vocabulary and unrestricted\nacoustic KWS. Sequence discriminative training for both sequence-level\ngenerative and discriminative models are systematically investigated. By\nintroducing word-independent phone lattices or non-keyword blank symbols to\nconstruct competing hypotheses, feasible and efficient sequence discriminative\ntraining approaches are proposed for acoustic KWS. Experiments showed that the\nproposed approaches obtained consistent and significant improvement in both\nfixed vocabulary and unrestricted KWS tasks, compared to previous frame-level\ndeep learning based acoustic KWS methods.", "published": "2018-08-02 02:26:53", "link": "http://arxiv.org/abs/1808.00639v1", "categories": ["cs.CL", "68T10", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Linguistic Search Optimization for Deep Learning Based LVCSR", "abstract": "Recent advances in deep learning based large vocabulary con- tinuous speech\nrecognition (LVCSR) invoke growing demands in large scale speech transcription.\nThe inference process of a speech recognizer is to find a sequence of labels\nwhose corresponding acoustic and language models best match the input feature\n[1]. The main computation includes two stages: acoustic model (AM) inference\nand linguistic search (weighted finite-state transducer, WFST). Large\ncomputational overheads of both stages hamper the wide application of LVCSR.\nBenefit from stronger classifiers, deep learning, and more powerful computing\ndevices, we propose general ideas and some initial trials to solve these\nfundamental problems.", "published": "2018-08-02 06:47:23", "link": "http://arxiv.org/abs/1808.00687v1", "categories": ["cs.CL", "68T10", "I.2.7"], "primary_category": "cs.CL"}
{"title": "OntoSenseNet: A Verb-Centric Ontological Resource for Indian Languages", "abstract": "Following approaches for understanding lexical meaning developed by Yaska,\nPatanjali and Bhartrihari from Indian linguistic traditions and extending\napproaches developed by Leibniz and Brentano in the modern times, a framework\nof formal ontology of language was developed. This framework proposes that\nmeaning of words are in-formed by intrinsic and extrinsic ontological\nstructures. The paper aims to capture such intrinsic and extrinsic meanings of\nwords for two major Indian languages, namely, Hindi and Telugu. Parts-of-speech\nhave been rendered into sense-types and sense-classes. Using them we have\ndeveloped a gold- standard annotated lexical resource to support semantic\nunderstanding of a language. The resource has collection of Hindi and Telugu\nlexicons, which has been manually annotated by native speakers of the languages\nfollowing our annotation guidelines. Further, the resource was utilised to\nderive adverbial sense-class distribution of verbs and karaka-verb sense- type\ndistribution. Different corpora (news, novels) were compared using verb\nsense-types distribution. Word Embedding was used as an aid for the enrichment\nof the resource. This is a work in progress that aims at lexical coverage of\nlanguage extensively.", "published": "2018-08-02 07:18:55", "link": "http://arxiv.org/abs/1808.00694v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cyberbullying Detection -- Technical Report 2/2018, Department of\n  Computer Science AGH, University of Science and Technology", "abstract": "The research described in this paper concerns automatic cyberbullying\ndetection in social media. There are two goals to achieve: building a gold\nstandard cyberbullying detection dataset and measuring the performance of the\nSamurai cyberbullying detection system. The Formspring dataset provided in a\nKaggle competition was re-annotated as a part of the research. The annotation\nprocedure is described in detail and, unlike many other recent data annotation\ninitiatives, does not use Mechanical Turk for finding people willing to perform\nthe annotation. The new annotation compared to the old one seems to be more\ncoherent since all tested cyberbullying detection system performed better on\nthe former. The performance of the Samurai system is compared with 5 commercial\nsystems and one well-known machine learning algorithm, used for classifying\ntextual content, namely Fasttext. It turns out that Samurai scores the best in\nall measures (accuracy, precision and recall), while Fasttext is the\nsecond-best performing algorithm.", "published": "2018-08-02 17:22:06", "link": "http://arxiv.org/abs/1808.00926v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SWDE : A Sub-Word And Document Embedding Based Engine for Clickbait\n  Detection", "abstract": "In order to expand their reach and increase website ad revenue, media outlets\nhave started using clickbait techniques to lure readers to click on articles on\ntheir digital platform. Having successfully enticed the user to open the\narticle, the article fails to satiate his curiosity serving only to boost\nclick-through rates. Initial methods for this task were dependent on feature\nengineering, which varies with each dataset. Industry systems have relied on an\nexhaustive set of rules to get the job done. Neural networks have barely been\nexplored to perform this task. We propose a novel approach considering\ndifferent textual embeddings of a news headline and the related article. We\ngenerate sub-word level embeddings of the title using Convolutional Neural\nNetworks and use them to train a bidirectional LSTM architecture. An attention\nlayer allows for calculation of significance of each term towards the nature of\nthe post. We also generate Doc2Vec embeddings of the title and article text and\nmodel how they interact, following which it is concatenated with the output of\nthe previous component. Finally, this representation is passed through a neural\nnetwork to obtain a score for the headline. We test our model over 2538 posts\n(having trained it on 17000 records) and achieve an accuracy of 83.49%\noutscoring previous state-of-the-art approaches.", "published": "2018-08-02 09:02:00", "link": "http://arxiv.org/abs/1808.00957v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Investigating accuracy of pitch-accent annotations in neural\n  network-based speech synthesis and denoising effects", "abstract": "We investigated the impact of noisy linguistic features on the performance of\na Japanese speech synthesis system based on neural network that uses WaveNet\nvocoder. We compared an ideal system that uses manually corrected linguistic\nfeatures including phoneme and prosodic information in training and test sets\nagainst a few other systems that use corrupted linguistic features. Both\nsubjective and objective results demonstrate that corrupted linguistic\nfeatures, especially those in the test set, affected the ideal system's\nperformance significantly in a statistical sense due to a mismatched condition\nbetween the training and test sets. Interestingly, while an utterance-level\nTuring test showed that listeners had a difficult time differentiating\nsynthetic speech from natural speech, it further indicated that adding noise to\nthe linguistic features in the training set can partially reduce the effect of\nthe mismatch, regularize the model, and help the system perform better when\nlinguistic features of the test set are noisy.", "published": "2018-08-02 04:25:30", "link": "http://arxiv.org/abs/1808.00665v1", "categories": ["eess.AS", "cs.CL", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "AVA-Speech: A Densely Labeled Dataset of Speech Activity in Movies", "abstract": "Speech activity detection (or endpointing) is an important processing step\nfor applications such as speech recognition, language identification and\nspeaker diarization. Both audio- and vision-based approaches have been used for\nthis task in various settings, often tailored toward end applications. However,\nmuch of the prior work reports results in synthetic settings, on task-specific\ndatasets, or on datasets that are not openly available. This makes it difficult\nto compare approaches and understand their strengths and weaknesses. In this\npaper, we describe a new dataset which we will release publicly containing\ndensely labeled speech activity in YouTube videos, with the goal of creating a\nshared, available dataset for this task. The labels in the dataset annotate\nthree different speech activity conditions: clean speech, speech co-occurring\nwith music, and speech co-occurring with noise, which enable analysis of model\nperformance in more challenging conditions based on the presence of overlapping\nnoise. We report benchmark performance numbers on AVA-Speech using\noff-the-shelf, state-of-the-art audio and vision models that serve as a\nbaseline to facilitate future research.", "published": "2018-08-02 00:13:11", "link": "http://arxiv.org/abs/1808.00606v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DCASE 2018 Challenge Surrey Cross-Task convolutional neural network\n  baseline", "abstract": "The Detection and Classification of Acoustic Scenes and Events (DCASE)\nconsists of five audio classification and sound event detection tasks: 1)\nAcoustic scene classification, 2) General-purpose audio tagging of Freesound,\n3) Bird audio detection, 4) Weakly-labeled semi-supervised sound event\ndetection and 5) Multi-channel audio classification. In this paper, we create a\ncross-task baseline system for all five tasks based on a convlutional neural\nnetwork (CNN): a \"CNN Baseline\" system. We implemented CNNs with 4 layers and 8\nlayers originating from AlexNet and VGG from computer vision. We investigated\nhow the performance varies from task to task with the same configuration of\nneural networks. Experiments show that deeper CNN with 8 layers performs better\nthan CNN with 4 layers on all tasks except Task 1. Using CNN with 8 layers, we\nachieve an accuracy of 0.680 on Task 1, an accuracy of 0.895 and a mean average\nprecision (MAP) of 0.928 on Task 2, an accuracy of 0.751 and an area under the\ncurve (AUC) of 0.854 on Task 3, a sound event detection F1 score of 20.8% on\nTask 4, and an F1 score of 87.75% on Task 5. We released the Python source code\nof the baseline systems under the MIT license for further research.", "published": "2018-08-02 12:03:09", "link": "http://arxiv.org/abs/1808.00773v4", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Statistical Speech Model Description with VMF Mixture Model", "abstract": "In this paper, we present the LSF parameters by a unit vector form, which has\ndirectional characteristics. The underlying distribution of this unit vector\nvariable is modeled by a von Mises-Fisher mixture model (VMM). With the high\nrate theory, the optimal inter-component bit allocation strategy is proposed\nand the distortion-rate (D-R) relation is derived for the VMM based-VQ (VVQ).\nExperimental results show that the VVQ outperforms our recently introduced DVQ\nand the conventional GVQ.", "published": "2018-08-02 13:47:27", "link": "http://arxiv.org/abs/1808.00960v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Dirichlet Mixture Model based VQ Performance Prediction for Line\n  Spectral Frequency", "abstract": "In this paper, we continue our previous work on the Dirichlet mixture model\n(DMM)-based VQ to derive the performance bound of the LSF VQ. The LSF\nparameters are transformed into the $\\Delta$LSF domain and the underlying\ndistribution of the $\\Delta$LSF parameters are modelled by a DMM with finite\nnumber of mixture components. The quantization distortion, in terms of the mean\nsquared error (MSE), is calculated with the high rate theory. The mapping\nrelation between the perceptually motivated log spectral distortion (LSD) and\nthe MSE is empirically approximated by a polynomial. With this mapping\nfunction, the minimum required bit rate for transparent coding of the LSF is\nestimated.", "published": "2018-08-02 14:02:50", "link": "http://arxiv.org/abs/1808.00818v2", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Histogram Transform-based Speaker Identification", "abstract": "A novel text-independent speaker identification (SI) method is proposed. This\nmethod uses the Mel-frequency Cepstral coefficients (MFCCs) and the dynamic\ninformation among adjacent frames as feature sets to capture speaker's\ncharacteristics. In order to utilize dynamic information, we design super-MFCCs\nfeatures by cascading three neighboring MFCCs frames together. The probability\ndensity function (PDF) of these super-MFCCs features is estimated by the\nrecently proposed histogram transform~(HT) method, which generates more\ntraining data by random transforms to realize the histogram PDF estimation and\nrecedes the commonly occurred discontinuity problem in multivariate histograms\ncomputing. Compared to the conventional PDF estimation methods, such as\nGaussian mixture models, the HT model shows promising improvement in the SI\nperformance.", "published": "2018-08-02 13:45:37", "link": "http://arxiv.org/abs/1808.00959v2", "categories": ["cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Normalization Before Shaking Toward Learning Symmetrically Distributed\n  Representation Without Margin in Speech Emotion Recognition", "abstract": "Regularization is crucial to the success of many practical deep learning\nmodels, in particular in a more often than not scenario where there are only a\nfew to a moderate number of accessible training samples. In addition to weight\ndecay, data augmentation and dropout, regularization based on multi-branch\narchitectures, such as Shake-Shake regularization, has been proven successful\nin many applications and attracted more and more attention. However, beyond\nmodel-based representation augmentation, it is unclear how Shake-Shake\nregularization helps to provide further improvement on classification tasks,\nlet alone the baffling interaction between batch normalization and shaking. In\nthis work, we present our investigation on Shake-Shake regularization, drawing\nconnections to the vicinal risk minimization principle and discriminative\nfeature learning in verification tasks. Furthermore, we identify a strong\nresemblance between batch normalized residual blocks and batch normalized\nrecurrent neural networks, where both of them share a similar convergence\nbehavior, which could be mitigated by a proper initialization of batch\nnormalization. Based on the findings, our experiments on speech emotion\nrecognition demonstrate simultaneously an improvement on the classification\naccuracy and a reduction on the generalization gap both with statistical\nsignificance.", "published": "2018-08-02 15:57:57", "link": "http://arxiv.org/abs/1808.00876v2", "categories": ["cs.LG", "cs.HC", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Acoustic Scene Classification: A Competition Review", "abstract": "In this paper we study the problem of acoustic scene classification, i.e.,\ncategorization of audio sequences into mutually exclusive classes based on\ntheir spectral content. We describe the methods and results discovered during a\ncompetition organized in the context of a graduate machine learning course;\nboth by the students and external participants. We identify the most suitable\nmethods and study the impact of each by performing an ablation study of the\nmixture of approaches. We also compare the results with a neural network\nbaseline, and show the improvement over that. Finally, we discuss the impact of\nusing a competition as a part of a university course, and justify its\nimportance in the curriculum based on student feedback.", "published": "2018-08-02 07:40:17", "link": "http://arxiv.org/abs/1808.02357v1", "categories": ["eess.AS", "cs.CV", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
