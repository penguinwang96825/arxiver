{"title": "The interplay between language similarity and script on a novel\n  multi-layer Algerian dialect corpus", "abstract": "Recent years have seen a rise in interest for cross-lingual transfer between\nlanguages with similar typology, and between languages of various scripts.\nHowever, the interplay between language similarity and difference in script on\ncross-lingual transfer is a less studied problem. We explore this interplay on\ncross-lingual transfer for two supervised tasks, namely part-of-speech tagging\nand sentiment analysis. We introduce a newly annotated corpus of Algerian\nuser-generated comments comprising parallel annotations of Algerian written in\nLatin, Arabic, and code-switched scripts, as well as annotations for sentiment\nand topic categories. We perform baseline experiments by fine-tuning\nmulti-lingual language models. We further explore the effect of script vs.\nlanguage similarity in cross-lingual transfer by fine-tuning multi-lingual\nmodels on languages which are a) typologically distinct, but use the same\nscript, b) typologically similar, but use a distinct script, or c) are\ntypologically similar and use the same script. We find there is a delicate\nrelationship between script and typology for part-of-speech, while sentiment\nanalysis is less sensitive.", "published": "2021-05-16 10:22:21", "link": "http://arxiv.org/abs/2105.07400v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How is BERT surprised? Layerwise detection of linguistic anomalies", "abstract": "Transformer language models have shown remarkable ability in detecting when a\nword is anomalous in context, but likelihood scores offer no information about\nthe cause of the anomaly. In this work, we use Gaussian models for density\nestimation at intermediate layers of three language models (BERT, RoBERTa, and\nXLNet), and evaluate our method on BLiMP, a grammaticality judgement benchmark.\nIn lower layers, surprisal is highly correlated to low token frequency, but\nthis correlation diminishes in upper layers. Next, we gather datasets of\nmorphosyntactic, semantic, and commonsense anomalies from psycholinguistic\nstudies; we find that the best performing model RoBERTa exhibits surprisal in\nearlier layers when the anomaly is morphosyntactic than when it is semantic,\nwhile commonsense anomalies do not exhibit surprisal at any intermediate layer.\nThese results suggest that language models employ separate mechanisms to detect\ndifferent types of linguistic anomalies.", "published": "2021-05-16 15:20:36", "link": "http://arxiv.org/abs/2105.07452v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data Augmentation for Sign Language Gloss Translation", "abstract": "Sign language translation (SLT) is often decomposed into video-to-gloss\nrecognition and gloss-to-text translation, where a gloss is a sequence of\ntranscribed spoken-language words in the order in which they are signed. We\nfocus here on gloss-to-text translation, which we treat as a low-resource\nneural machine translation (NMT) problem. However, unlike traditional\nlow-resource NMT, gloss-to-text translation differs because gloss-text pairs\noften have a higher lexical overlap and lower syntactic overlap than pairs of\nspoken languages. We exploit this lexical overlap and handle syntactic\ndivergence by proposing two rule-based heuristics that generate pseudo-parallel\ngloss-text pairs from monolingual spoken language text. By pre-training on the\nthus obtained synthetic data, we improve translation from American Sign\nLanguage (ASL) to English and German Sign Language (DGS) to German by up to\n3.14 and 2.20 BLEU, respectively.", "published": "2021-05-16 16:37:36", "link": "http://arxiv.org/abs/2105.07476v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Volctrans Neural Speech Translation System for IWSLT 2021", "abstract": "This paper describes the systems submitted to IWSLT 2021 by the Volctrans\nteam. We participate in the offline speech translation and text-to-text\nsimultaneous translation tracks. For offline speech translation, our best\nend-to-end model achieves 8.1 BLEU improvements over the benchmark on the\nMuST-C test set and is even approaching the results of a strong cascade\nsolution. For text-to-text simultaneous translation, we explore the best\npractice to optimize the wait-k model. As a result, our final submitted systems\nexceed the benchmark at around 7 BLEU on the same latency regime. We will\npublish our code and model to facilitate both future research works and\nindustrial applications.\n  This paper describes the systems submitted to IWSLT 2021 by the Volctrans\nteam. We participate in the offline speech translation and text-to-text\nsimultaneous translation tracks. For offline speech translation, our best\nend-to-end model achieves 7.9 BLEU improvements over the benchmark on the\nMuST-C test set and is even approaching the results of a strong cascade\nsolution. For text-to-text simultaneous translation, we explore the best\npractice to optimize the wait-k model. As a result, our final submitted systems\nexceed the benchmark at around 7 BLEU on the same latency regime. We release\nour code and model at\n\\url{https://github.com/bytedance/neurst/tree/master/examples/iwslt21} to\nfacilitate both future research works and industrial applications.", "published": "2021-05-16 00:11:59", "link": "http://arxiv.org/abs/2105.07319v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Few-NERD: A Few-Shot Named Entity Recognition Dataset", "abstract": "Recently, considerable literature has grown up around the theme of few-shot\nnamed entity recognition (NER), but little published benchmark data\nspecifically focused on the practical and challenging task. Current approaches\ncollect existing supervised NER datasets and re-organize them to the few-shot\nsetting for empirical study. These strategies conventionally aim to recognize\ncoarse-grained entity types with few examples, while in practice, most unseen\nentity types are fine-grained. In this paper, we present Few-NERD, a\nlarge-scale human-annotated few-shot NER dataset with a hierarchy of 8\ncoarse-grained and 66 fine-grained entity types. Few-NERD consists of 188,238\nsentences from Wikipedia, 4,601,160 words are included and each is annotated as\ncontext or a part of a two-level entity type. To the best of our knowledge,\nthis is the first few-shot NER dataset and the largest human-crafted NER\ndataset. We construct benchmark tasks with different emphases to\ncomprehensively assess the generalization capability of models. Extensive\nempirical results and analysis show that Few-NERD is challenging and the\nproblem requires further research. We make Few-NERD public at\nhttps://ningding97.github.io/fewnerd/.", "published": "2021-05-16 15:53:17", "link": "http://arxiv.org/abs/2105.07464v6", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Doc2Dict: Information Extraction as Text Generation", "abstract": "Typically, information extraction (IE) requires a pipeline approach: first, a\nsequence labeling model is trained on manually annotated documents to extract\nrelevant spans; then, when a new document arrives, a model predicts spans which\nare then post-processed and standardized to convert the information into a\ndatabase entry. We replace this labor-intensive workflow with a transformer\nlanguage model trained on existing database records to directly generate\nstructured JSON. Our solution removes the workload associated with producing\ntoken-level annotations and takes advantage of a data source which is generally\nquite plentiful (e.g. database records). As long documents are common in\ninformation extraction tasks, we use gradient checkpointing and chunked\nencoding to apply our method to sequences of up to 32,000 tokens on a single\nGPU. Our Doc2Dict approach is competitive with more complex, hand-engineered\npipelines and offers a simple but effective baseline for document-level\ninformation extraction. We release our Doc2Dict model and code to reproduce our\nexperiments and facilitate future work.", "published": "2021-05-16 20:46:29", "link": "http://arxiv.org/abs/2105.07510v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "X-Vectors with Multi-Scale Aggregation for Speaker Diarization", "abstract": "Speaker diarization is the process of labeling different speakers in a speech\nsignal. Deep speaker embeddings are generally extracted from short speech\nsegments and clustered to determine the segments belong to same speaker\nidentity. The x-vector, which embeds segment-level speaker characteristics by\nstatistically pooling frame-level representations, is one of the most widely\nused deep speaker embeddings in speaker diarization. Multi-scale aggregation,\nwhich employs multi-scale representations from different layers, has recently\nsuccessfully been used in short duration speaker verification. In this paper,\nwe investigate a multi-scale aggregation approach in an x-vector embedding\nframework for speaker diarization by exploiting multiple statistics pooling\nlayers from different frame-level layers. Thus, it is expected that x-vectors\nwith multi-scale aggregation have the potential to capture meaningful speaker\ncharacteristics from short segments, effectively taking advantage of different\ninformation at multiple layers. Experimental evaluation on the CALLHOME dataset\nshowed that our approach provides substantial improvement over the baseline\nx-vectors.", "published": "2021-05-16 06:23:36", "link": "http://arxiv.org/abs/2105.07367v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
