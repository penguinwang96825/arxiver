{"title": "QADiver: Interactive Framework for Diagnosing QA Models", "abstract": "Question answering (QA) extracting answers from text to the given question in\nnatural language, has been actively studied and existing models have shown a\npromise of outperforming human performance when trained and evaluated with\nSQuAD dataset. However, such performance may not be replicated in the actual\nsetting, for which we need to diagnose the cause, which is non-trivial due to\nthe complexity of model. We thus propose a web-based UI that provides how each\nmodel contributes to QA performances, by integrating visualization and analysis\ntools for model explanation. We expect this framework can help QA model\nresearchers to refine and improve their models.", "published": "2018-12-01 06:43:55", "link": "http://arxiv.org/abs/1812.00161v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "One for All: Neural Joint Modeling of Entities and Events", "abstract": "The previous work for event extraction has mainly focused on the predictions\nfor event triggers and argument roles, treating entity mentions as being\nprovided by human annotators. This is unrealistic as entity mentions are\nusually predicted by some existing toolkits whose errors might be propagated to\nthe event trigger and argument role recognition. Few of the recent work has\naddressed this problem by jointly predicting entity mentions, event triggers\nand arguments. However, such work is limited to using discrete engineering\nfeatures to represent contextual information for the individual tasks and their\ninteractions. In this work, we propose a novel model to jointly perform\npredictions for entity mentions, event triggers and arguments based on the\nshared hidden representations from deep learning. The experiments demonstrate\nthe benefits of the proposed method, leading to the state-of-the-art\nperformance for event extraction.", "published": "2018-12-01 12:13:55", "link": "http://arxiv.org/abs/1812.00195v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Deep Sequential Model for Discourse Parsing on Multi-Party Dialogues", "abstract": "Discourse structures are beneficial for various NLP tasks such as dialogue\nunderstanding, question answering, sentiment analysis, and so on. This paper\npresents a deep sequential model for parsing discourse dependency structures of\nmulti-party dialogues. The proposed model aims to construct a discourse\ndependency tree by predicting dependency relations and constructing the\ndiscourse structure jointly and alternately. It makes a sequential scan of the\nElementary Discourse Units (EDUs) in a dialogue. For each EDU, the model\ndecides to which previous EDU the current one should link and what the\ncorresponding relation type is. The predicted link and relation type are then\nused to build the discourse structure incrementally with a structured encoder.\nDuring link prediction and relation classification, the model utilizes not only\nlocal information that represents the concerned EDUs, but also global\ninformation that encodes the EDU sequence and the discourse structure that is\nalready built at the current step. Experiments show that the proposed model\noutperforms all the state-of-the-art baselines.", "published": "2018-12-01 08:13:48", "link": "http://arxiv.org/abs/1812.00176v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning to Caption Images through a Lifetime by Asking Questions", "abstract": "In order to bring artificial agents into our lives, we will need to go beyond\nsupervised learning on closed datasets to having the ability to continuously\nexpand knowledge. Inspired by a student learning in a classroom, we present an\nagent that can continuously learn by posing natural language questions to\nhumans. Our agent is composed of three interacting modules, one that performs\ncaptioning, another that generates questions and a decision maker that learns\nwhen to ask questions by implicitly reasoning about the uncertainty of the\nagent and expertise of the teacher. As compared to current active learning\nmethods which query images for full captions, our agent is able to ask pointed\nquestions to improve the generated captions. The agent trains on the improved\ncaptions, expanding its knowledge. We show that our approach achieves better\nperformance using less human supervision than the baselines on the challenging\nMSCOCO dataset.", "published": "2018-12-01 18:12:35", "link": "http://arxiv.org/abs/1812.00235v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Learning Speaker Representations with Mutual Information", "abstract": "Learning good representations is of crucial importance in deep learning.\nMutual Information (MI) or similar measures of statistical dependence are\npromising tools for learning these representations in an unsupervised way. Even\nthough the mutual information between two random variables is hard to measure\ndirectly in high dimensional spaces, some recent studies have shown that an\nimplicit optimization of MI can be achieved with an encoder-discriminator\narchitecture similar to that of Generative Adversarial Networks (GANs). In this\nwork, we learn representations that capture speaker identities by maximizing\nthe mutual information between the encoded representations of chunks of speech\nrandomly sampled from the same sentence. The proposed encoder relies on the\nSincNet architecture and transforms raw speech waveform into a compact feature\nvector. The discriminator is fed by either positive samples (of the joint\ndistribution of encoded chunks) or negative samples (from the product of the\nmarginals) and is trained to separate them. We report experiments showing that\nthis approach effectively learns useful speaker representations, leading to\npromising results on speaker identification and verification tasks. Our\nexperiments consider both unsupervised and semi-supervised settings and compare\nthe performance achieved with different objective functions.", "published": "2018-12-01 21:48:28", "link": "http://arxiv.org/abs/1812.00271v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.NE", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Lightweight and Optimized Sound Source Localization and Tracking Methods\n  for Open and Closed Microphone Array Configurations", "abstract": "Human-robot interaction in natural settings requires filtering out the\ndifferent sources of sounds from the environment. Such ability usually involves\nthe use of microphone arrays to localize, track and separate sound sources\nonline. Multi-microphone signal processing techniques can improve robustness to\nnoise but the processing cost increases with the number of microphones used,\nlimiting response time and widespread use on different types of mobile robots.\nSince sound source localization methods are the most expensive in terms of\ncomputing resources as they involve scanning a large 3D space, minimizing the\namount of computations required would facilitate their implementation and use\non robots. The robot's shape also brings constraints on the microphone array\ngeometry and configurations. In addition, sound source localization methods\nusually return noisy features that need to be smoothed and filtered by tracking\nthe sound sources. This paper presents a novel sound source localization\nmethod, called SRP-PHAT-HSDA, that scans space with coarse and fine resolution\ngrids to reduce the number of memory lookups. A microphone directivity model is\nused to reduce the number of directions to scan and ignore non significant\npairs of microphones. A configuration method is also introduced to\nautomatically set parameters that are normally empirically tuned according to\nthe shape of the microphone array. For sound source tracking, this paper\npresents a modified 3D Kalman (M3K) method capable of simultaneously tracking\nin 3D the directions of sound sources. Using a 16-microphone array and low cost\nhardware, results show that SRP-PHAT-HSDA and M3K perform at least as well as\nother sound source localization and tracking methods while using up to 4 and 30\ntimes less computing resources respectively.", "published": "2018-12-01 01:16:06", "link": "http://arxiv.org/abs/1812.00115v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SwishNet: A Fast Convolutional Neural Network for Speech, Music and\n  Noise Classification and Segmentation", "abstract": "Speech, Music and Noise classification/segmentation is an important\npreprocessing step for audio processing/indexing. To this end, we propose a\nnovel 1D Convolutional Neural Network (CNN) - SwishNet. It is a fast and\nlightweight architecture that operates on MFCC features which is suitable to be\nadded to the front-end of an audio processing pipeline. We showed that the\nperformance of our network can be improved by distilling knowledge from a 2D\nCNN, pretrained on ImageNet. We investigated the performance of our network on\nthe MUSAN corpus - an openly available comprehensive collection of noise, music\nand speech samples, suitable for deep learning. The proposed network achieved\nhigh overall accuracy in clip (length of 0.5-2s) classification (>97% accuracy)\nand frame-wise segmentation (>93% accuracy) tasks with even higher accuracy\n(>99%) in speech/non-speech discrimination task. To verify the robustness of\nour model, we trained it on MUSAN and evaluated it on a different corpus -\nGTZAN and found good accuracy with very little fine-tuning. We also\ndemonstrated that our model is fast on both CPU and GPU, consumes a low amount\nof memory and is suitable for implementation in embedded systems.", "published": "2018-12-01 05:39:07", "link": "http://arxiv.org/abs/1812.00149v1", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
