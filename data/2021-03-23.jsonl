{"title": "Fabula Entropy Indexing: Objective Measures of Story Coherence", "abstract": "Automated story generation remains a difficult area of research because it\nlacks strong objective measures. Generated stories may be linguistically sound,\nbut in many cases suffer poor narrative coherence required for a compelling,\nlogically-sound story. To address this, we present Fabula Entropy Indexing\n(FEI), an evaluation method to assess story coherence by measuring the degree\nto which human participants agree with each other when answering true/false\nquestions about stories. We devise two theoretically grounded measures of\nreader question-answering entropy, the entropy of world coherence (EWC), and\nthe entropy of transitional coherence (ETC), focusing on global and local\ncoherence, respectively. We evaluate these metrics by testing them on\nhuman-written stories and comparing against the same stories that have been\ncorrupted to introduce incoherencies. We show that in these controlled studies,\nour entropy indices provide a reliable objective measure of story coherence.", "published": "2021-03-23 02:29:37", "link": "http://arxiv.org/abs/2104.07472v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SelfExplain: A Self-Explaining Architecture for Neural Text Classifiers", "abstract": "We introduce SelfExplain, a novel self-explaining model that explains a text\nclassifier's predictions using phrase-based concepts. SelfExplain augments\nexisting neural classifiers by adding (1) a globally interpretable layer that\nidentifies the most influential concepts in the training set for a given sample\nand (2) a locally interpretable layer that quantifies the contribution of each\nlocal input concept by computing a relevance score relative to the predicted\nlabel. Experiments across five text-classification datasets show that\nSelfExplain facilitates interpretability without sacrificing performance. Most\nimportantly, explanations from SelfExplain show sufficiency for model\npredictions and are perceived as adequate, trustworthy and understandable by\nhuman judges compared to existing widely-used baselines.", "published": "2021-03-23 03:07:21", "link": "http://arxiv.org/abs/2103.12279v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Annotation of Chinese Predicate Heads and Relevant Elements", "abstract": "A predicate head is a verbal expression that plays a role as the structural\ncenter of a sentence. Identifying predicate heads is critical to understanding\na sentence. It plays the leading role in organizing the relevant syntactic\nelements in a sentence, including subject elements, adverbial elements, etc.\nFor some languages, such as English, word morphologies are valuable for\nidentifying predicate heads. However, Chinese offers no morphological\ninformation to indicate words` grammatical roles. A Chinese sentence often\ncontains several verbal expressions; identifying the expression that plays the\nrole of the predicate head is not an easy task. Furthermore, Chinese sentences\nare inattentive to structure and provide no delimitation between words.\nTherefore, identifying Chinese predicate heads involves significant challenges.\nIn Chinese information extraction, little work has been performed in predicate\nhead recognition. No generally accepted evaluation dataset supports work in\nthis important area. This paper presents the first attempt to develop an\nannotation guideline for Chinese predicate heads and their relevant syntactic\nelements. This annotation guideline emphasizes the role of the predicate as the\nstructural center of a sentence. The design of relevant syntactic element\nannotation also follows this principle. Many considerations are proposed to\nachieve this goal, e.g., patterns of predicate heads, a flattened annotation\nstructure, and a simpler syntactic unit type. Based on the proposed annotation\nguideline, more than 1,500 documents were manually annotated. The corpus will\nbe available online for public access. With this guideline and annotated\ncorpus, our goal is to broadly impact and advance the research in the area of\nChinese information extraction and to provide the research community with a\ncritical resource that has been lacking for a long time.", "published": "2021-03-23 03:11:59", "link": "http://arxiv.org/abs/2103.12280v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Discovering Emotion and Reasoning its Flip in Multi-Party Conversations\n  using Masked Memory Network and Transformer", "abstract": "Efficient discovery of a speaker's emotional states in a multi-party\nconversation is significant to design human-like conversational agents. During\na conversation, the cognitive state of a speaker often alters due to certain\npast utterances, which may lead to a flip in their emotional state. Therefore,\ndiscovering the reasons (triggers) behind the speaker's emotion-flip during a\nconversation is essential to explain the emotion labels of individual\nutterances. In this paper, along with addressing the task of emotion\nrecognition in conversations (ERC), we introduce a novel task - Emotion-Flip\nReasoning (EFR), that aims to identify past utterances which have triggered\none's emotional state to flip at a certain time. We propose a masked memory\nnetwork to address the former and a Transformer-based network for the latter\ntask. To this end, we consider MELD, a benchmark emotion recognition dataset in\nmulti-party conversations for the task of ERC, and augment it with new\nground-truth labels for EFR. An extensive comparison with five state-of-the-art\nmodels suggests improved performances of our models for both tasks. We further\npresent anecdotal evidence and both qualitative and quantitative error analyses\nto support the superiority of our models compared to the baselines.", "published": "2021-03-23 07:42:09", "link": "http://arxiv.org/abs/2103.12360v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exercise? I thought you said 'Extra Fries': Leveraging Sentence\n  Demarcations and Multi-hop Attention for Meme Affect Analysis", "abstract": "Today's Internet is awash in memes as they are humorous, satirical, or ironic\nwhich make people laugh. According to a survey, 33% of social media users in\nage bracket [13-35] send memes every day, whereas more than 50% send every\nweek. Some of these memes spread rapidly within a very short time-frame, and\ntheir virality depends on the novelty of their (textual and visual) content. A\nfew of them convey positive messages, such as funny or motivational quotes;\nwhile others are meant to mock/hurt someone's feelings through sarcastic or\noffensive messages. Despite the appealing nature of memes and their rapid\nemergence on social media, effective analysis of memes has not been adequately\nattempted to the extent it deserves.\n  In this paper, we attempt to solve the same set of tasks suggested in the\nSemEval'20-Memotion Analysis competition. We propose a multi-hop\nattention-based deep neural network framework, called MHA-MEME, whose prime\nobjective is to leverage the spatial-domain correspondence between the visual\nmodality (an image) and various textual segments to extract fine-grained\nfeature representations for classification. We evaluate MHA-MEME on the\n'Memotion Analysis' dataset for all three sub-tasks - sentiment classification,\naffect classification, and affect class quantification. Our comparative study\nshows sota performances of MHA-MEME for all three tasks compared to the top\nsystems that participated in the competition. Unlike all the baselines which\nperform inconsistently across all three tasks, MHA-MEME outperforms baselines\nin all the tasks on average. Moreover, we validate the generalization of\nMHA-MEME on another set of manually annotated test samples and observe it to be\nconsistent. Finally, we establish the interpretability of MHA-MEME.", "published": "2021-03-23 08:21:37", "link": "http://arxiv.org/abs/2103.12377v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Hate Speech with GPT-3", "abstract": "Sophisticated language models such as OpenAI's GPT-3 can generate hateful\ntext that targets marginalized groups. Given this capacity, we are interested\nin whether large language models can be used to identify hate speech and\nclassify text as sexist or racist. We use GPT-3 to identify sexist and racist\ntext passages with zero-, one-, and few-shot learning. We find that with zero-\nand one-shot learning, GPT-3 can identify sexist or racist text with an average\naccuracy between 55 per cent and 67 per cent, depending on the category of text\nand type of learning. With few-shot learning, the model's accuracy can be as\nhigh as 85 per cent. Large language models have a role to play in hate speech\ndetection, and with further development they could eventually be used to\ncounter hate speech.", "published": "2021-03-23 09:17:22", "link": "http://arxiv.org/abs/2103.12407v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Multi-domain, Heterogeneous Data using Deep Multitask\n  Learning for Hate Speech Detection", "abstract": "With the exponential rise in user-generated web content on social media, the\nproliferation of abusive languages towards an individual or a group across the\ndifferent sections of the internet is also rapidly increasing. It is very\nchallenging for human moderators to identify the offensive contents and filter\nthose out. Deep neural networks have shown promise with reasonable accuracy for\nhate speech detection and allied applications. However, the classifiers are\nheavily dependent on the size and quality of the training data. Such a\nhigh-quality large data set is not easy to obtain. Moreover, the existing data\nsets that have emerged in recent times are not created following the same\nannotation guidelines and are often concerned with different types and\nsub-types related to hate. To solve this data sparsity problem, and to obtain\nmore global representative features, we propose a Convolution Neural Network\n(CNN) based multi-task learning models (MTLs)\\footnote{code is available at\nhttps://github.com/imprasshant/STL-MTL} to leverage information from multiple\nsources. Empirical analysis performed on three benchmark datasets shows the\nefficacy of the proposed approach with the significant improvement in accuracy\nand F-score to obtain state-of-the-art performance with respect to the existing\nsystems.", "published": "2021-03-23 09:31:01", "link": "http://arxiv.org/abs/2103.12412v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling the Severity of Complaints in Social Media", "abstract": "The speech act of complaining is used by humans to communicate a negative\nmismatch between reality and expectations as a reaction to an unfavorable\nsituation. Linguistic theory of pragmatics categorizes complaints into various\nseverity levels based on the face-threat that the complainer is willing to\nundertake. This is particularly useful for understanding the intent of\ncomplainers and how humans develop suitable apology strategies. In this paper,\nwe study the severity level of complaints for the first time in computational\nlinguistics. To facilitate this, we enrich a publicly available data set of\ncomplaints with four severity categories and train different transformer-based\nnetworks combined with linguistic information achieving 55.7 macro F1. We also\njointly model binary complaint classification and complaint severity in a\nmulti-task setting achieving new state-of-the-art results on binary complaint\ndetection reaching up to 88.2 macro F1. Finally, we present a qualitative\nanalysis of the behavior of our models in predicting complaint severity levels.", "published": "2021-03-23 10:13:11", "link": "http://arxiv.org/abs/2103.12428v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A General Framework for Learning Prosodic-Enhanced Representation of Rap\n  Lyrics", "abstract": "Learning and analyzing rap lyrics is a significant basis for many web\napplications, such as music recommendation, automatic music categorization, and\nmusic information retrieval, due to the abundant source of digital music in the\nWorld Wide Web. Although numerous studies have explored the topic, knowledge in\nthis field is far from satisfactory, because critical issues, such as prosodic\ninformation and its effective representation, as well as appropriate\nintegration of various features, are usually ignored. In this paper, we propose\na hierarchical attention variational autoencoder framework (HAVAE), which\nsimultaneously consider semantic and prosodic features for rap lyrics\nrepresentation learning. Specifically, the representation of the prosodic\nfeatures is encoded by phonetic transcriptions with a novel and effective\nstrategy~(i.e., rhyme2vec). Moreover, a feature aggregation strategy is\nproposed to appropriately integrate various features and generate\nprosodic-enhanced representation. A comprehensive empirical evaluation\ndemonstrates that the proposed framework outperforms the state-of-the-art\napproaches under various metrics in different rap lyrics learning tasks.", "published": "2021-03-23 15:13:21", "link": "http://arxiv.org/abs/2103.12615v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "QuestEval: Summarization Asks for Fact-based Evaluation", "abstract": "Summarization evaluation remains an open research problem: current metrics\nsuch as ROUGE are known to be limited and to correlate poorly with human\njudgments. To alleviate this issue, recent work has proposed evaluation metrics\nwhich rely on question answering models to assess whether a summary contains\nall the relevant information in its source document. Though promising, the\nproposed approaches have so far failed to correlate better than ROUGE with\nhuman judgments.\n  In this paper, we extend previous approaches and propose a unified framework,\nnamed QuestEval. In contrast to established metrics such as ROUGE or BERTScore,\nQuestEval does not require any ground-truth reference. Nonetheless, QuestEval\nsubstantially improves the correlation with human judgments over four\nevaluation dimensions (consistency, coherence, fluency, and relevance), as\nshown in the extensive experiments we report.", "published": "2021-03-23 17:16:09", "link": "http://arxiv.org/abs/2103.12693v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Repairing Pronouns in Translation with BERT-Based Post-Editing", "abstract": "Pronouns are important determinants of a text's meaning but difficult to\ntranslate. This is because pronoun choice can depend on entities described in\nprevious sentences, and in some languages pronouns may be dropped when the\nreferent is inferrable from the context. These issues can lead Neural Machine\nTranslation (NMT) systems to make critical errors on pronouns that impair\nintelligibility and even reinforce gender bias. We investigate the severity of\nthis pronoun issue, showing that (1) in some domains, pronoun choice can\naccount for more than half of a NMT systems' errors, and (2) pronouns have a\ndisproportionately large impact on perceived translation quality. We then\ninvestigate a possible solution: fine-tuning BERT on a pronoun prediction task\nusing chunks of source-side sentences, then using the resulting classifier to\nrepair the translations of an existing NMT model. We offer an initial case\nstudy of this approach for the Japanese-English language pair, observing that a\nsmall number of translations are significantly improved according to human\nevaluators.", "published": "2021-03-23 21:01:03", "link": "http://arxiv.org/abs/2103.12838v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TeCoMiner: Topic Discovery Through Term Community Detection", "abstract": "This note is a short description of TeCoMiner, an interactive tool for\nexploring the topic content of text collections. Unlike other topic modeling\ntools, TeCoMiner is not based on some generative probabilistic model but on\ntopological considerations about co-occurrence networks of terms. We outline\nthe methods used for identifying topics, describe the features of the tool, and\nsketch an application, using a corpus of policy related scientific news on\nenvironmental issues published by the European Commission over the last decade.", "published": "2021-03-23 23:08:46", "link": "http://arxiv.org/abs/2103.12882v1", "categories": ["cs.CL", "I.2.7; I.5.3; H.5.2"], "primary_category": "cs.CL"}
{"title": "The NLP Cookbook: Modern Recipes for Transformer based Deep Learning\n  Architectures", "abstract": "In recent years, Natural Language Processing (NLP) models have achieved\nphenomenal success in linguistic and semantic tasks like text classification,\nmachine translation, cognitive dialogue systems, information retrieval via\nNatural Language Understanding (NLU), and Natural Language Generation (NLG).\nThis feat is primarily attributed due to the seminal Transformer architecture,\nleading to designs such as BERT, GPT (I, II, III), etc. Although these\nlarge-size models have achieved unprecedented performances, they come at high\ncomputational costs. Consequently, some of the recent NLP architectures have\nutilized concepts of transfer learning, pruning, quantization, and knowledge\ndistillation to achieve moderate model sizes while keeping nearly similar\nperformances as achieved by their predecessors. Additionally, to mitigate the\ndata size challenge raised by language models from a knowledge extraction\nperspective, Knowledge Retrievers have been built to extricate explicit data\ndocuments from a large corpus of databases with greater efficiency and\naccuracy. Recent research has also focused on superior inference by providing\nefficient attention to longer input sequences. In this paper, we summarize and\nexamine the current state-of-the-art (SOTA) NLP models that have been employed\nfor numerous NLP tasks for optimal performance and efficiency. We provide a\ndetailed understanding and functioning of the different architectures, a\ntaxonomy of NLP designs, comparative evaluations, and future directions in NLP.", "published": "2021-03-23 22:38:20", "link": "http://arxiv.org/abs/2104.10640v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multi-Modal Answer Validation for Knowledge-Based VQA", "abstract": "The problem of knowledge-based visual question answering involves answering\nquestions that require external knowledge in addition to the content of the\nimage. Such knowledge typically comes in various forms, including visual,\ntextual, and commonsense knowledge. Using more knowledge sources increases the\nchance of retrieving more irrelevant or noisy facts, making it challenging to\ncomprehend the facts and find the answer. To address this challenge, we propose\nMulti-modal Answer Validation using External knowledge (MAVEx), where the idea\nis to validate a set of promising answer candidates based on answer-specific\nknowledge retrieval. Instead of searching for the answer in a vast collection\nof often irrelevant facts as most existing approaches do, MAVEx aims to learn\nhow to extract relevant knowledge from noisy sources, which knowledge source to\ntrust for each answer candidate, and how to validate the candidate using that\nsource. Our multi-modal setting is the first to leverage external visual\nknowledge (images searched using Google), in addition to textual knowledge in\nthe form of Wikipedia sentences and ConceptNet concepts. Our experiments with\nOK-VQA, a challenging knowledge-based VQA dataset, demonstrate that MAVEx\nachieves new state-of-the-art results. Our code is available at\nhttps://github.com/jialinwu17/MAVEX", "published": "2021-03-23 00:49:36", "link": "http://arxiv.org/abs/2103.12248v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Hallucination of speech recognition errors with sequence to sequence\n  learning", "abstract": "Automatic Speech Recognition (ASR) is an imperfect process that results in\ncertain mismatches in ASR output text when compared to plain written text or\ntranscriptions. When plain text data is to be used to train systems for spoken\nlanguage understanding or ASR, a proven strategy to reduce said mismatch and\nprevent degradations, is to hallucinate what the ASR outputs would be given a\ngold transcription. Prior work in this domain has focused on modeling errors at\nthe phonetic level, while using a lexicon to convert the phones to words,\nusually accompanied by an FST Language model. We present novel end-to-end\nmodels to directly predict hallucinated ASR word sequence outputs, conditioning\non an input word sequence as well as a corresponding phoneme sequence. This\nimproves prior published results for recall of errors from an in-domain ASR\nsystem's transcription of unseen data, as well as an out-of-domain ASR system's\ntranscriptions of audio from an unrelated task, while additionally exploring an\nin-between scenario when limited characterization data from the test ASR system\nis obtainable. To verify the extrinsic validity of the method, we also use our\nhallucinated ASR errors to augment training for a spoken question classifier,\nfinding that they enable robustness to real ASR errors in a downstream task,\nwhen scarce or even zero task-specific audio was available at train-time.", "published": "2021-03-23 02:09:39", "link": "http://arxiv.org/abs/2103.12258v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TMR: Evaluating NER Recall on Tough Mentions", "abstract": "We propose the Tough Mentions Recall (TMR) metrics to supplement traditional\nnamed entity recognition (NER) evaluation by examining recall on specific\nsubsets of \"tough\" mentions: unseen mentions, those whose tokens or token/type\ncombination were not observed in training, and type-confusable mentions, token\nsequences with multiple entity types in the test data. We demonstrate the\nusefulness of these metrics by evaluating corpora of English, Spanish, and\nDutch using five recent neural architectures. We identify subtle differences\nbetween the performance of BERT and Flair on two English NER corpora and\nidentify a weak spot in the performance of current models in Spanish. We\nconclude that the TMR metrics enable differentiation between otherwise\nsimilar-scoring systems and identification of patterns in performance that\nwould go unnoticed from overall precision, recall, and F1.", "published": "2021-03-23 05:04:14", "link": "http://arxiv.org/abs/2103.12312v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Redefining Absent Keyphrases and their Effect on Retrieval Effectiveness", "abstract": "Neural keyphrase generation models have recently attracted much interest due\nto their ability to output absent keyphrases, that is, keyphrases that do not\nappear in the source text. In this paper, we discuss the usefulness of absent\nkeyphrases from an Information Retrieval (IR) perspective, and show that the\ncommonly drawn distinction between present and absent keyphrases is not made\nexplicit enough. We introduce a finer-grained categorization scheme that sheds\nmore light on the impact of absent keyphrases on scientific document retrieval.\nUnder this scheme, we find that only a fraction (around 20%) of the words that\nmake up keyphrases actually serves as document expansion, but that this small\nfraction of words is behind much of the gains observed in retrieval\neffectiveness. We also discuss how the proposed scheme can offer a new angle to\nevaluate the output of neural keyphrase generation models.", "published": "2021-03-23 10:42:18", "link": "http://arxiv.org/abs/2103.12440v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Ground Truths for the Humanities", "abstract": "Ensuring a faithful interaction with data and its representation for\nhumanities can and should depend on expert-constructed ground truths.", "published": "2021-03-23 21:05:42", "link": "http://arxiv.org/abs/2103.12841v1", "categories": ["cs.DL", "cs.CL"], "primary_category": "cs.DL"}
{"title": "Complex Factoid Question Answering with a Free-Text Knowledge Graph", "abstract": "We introduce DELFT, a factoid question answering system which combines the\nnuance and depth of knowledge graph question answering approaches with the\nbroader coverage of free-text. DELFT builds a free-text knowledge graph from\nWikipedia, with entities as nodes and sentences in which entities co-occur as\nedges. For each question, DELFT finds the subgraph linking question entity\nnodes to candidates using text sentences as edges, creating a dense and high\ncoverage semantic graph. A novel graph neural network reasons over the\nfree-text graph-combining evidence on the nodes via information along edge\nsentences-to select a final answer. Experiments on three question answering\ndatasets show DELFT can answer entity-rich questions better than machine\nreading based models, bert-based answer ranking and memory networks. DELFT's\nadvantage comes from both the high coverage of its free-text knowledge\ngraph-more than double that of dbpedia relations-and the novel graph neural\nnetwork which reasons on the rich but noisy free-text evidence.", "published": "2021-03-23 22:53:09", "link": "http://arxiv.org/abs/2103.12876v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Plug-and-Blend: A Framework for Controllable Story Generation with\n  Blended Control Codes", "abstract": "Large pre-trained neural language models (LM) have very powerful text\ngeneration capabilities. However, in practice, they are hard to control for\ncreative purposes. We describe a Plug-and-Play controllable language generation\nframework, Plug-and-Blend, that allows a human user to input multiple control\ncodes (topics). In the context of automated story generation, this allows a\nhuman user loose or fine-grained control of the topics and transitions between\nthem that will appear in the generated story, and can even allow for\noverlapping, blended topics. Automated evaluations show our framework, working\nwith different generative LMs, controls the generation towards given\ncontinuous-weighted control codes while keeping the generated sentences fluent,\ndemonstrating strong blending capability. A human participant evaluation shows\nthat the generated stories are observably transitioning between two topics.", "published": "2021-03-23 03:15:14", "link": "http://arxiv.org/abs/2104.04039v2", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Detecting cognitive decline using speech only: The ADReSSo Challenge", "abstract": "Building on the success of the ADReSS Challenge at Interspeech 2020, which\nattracted the participation of 34 teams from across the world, the ADReSSo\nChallenge targets three difficult automatic prediction problems of societal and\nmedical relevance, namely: detection of Alzheimer's Dementia, inference of\ncognitive testing scores, and prediction of cognitive decline. This paper\npresents these prediction tasks in detail, describes the datasets used, and\nreports the results of the baseline classification and regression models we\ndeveloped for each task. A combination of acoustic and linguistic features\nextracted directly from audio recordings, without human intervention, yielded a\nbaseline accuracy of 78.87% for the AD classification task, an MMSE prediction\nroot mean squared (RMSE) error of 5.28, and 68.75% accuracy for the cognitive\ndecline prediction task.", "published": "2021-03-23 01:09:38", "link": "http://arxiv.org/abs/2104.09356v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Are Neural Language Models Good Plagiarists? A Benchmark for Neural\n  Paraphrase Detection", "abstract": "The rise of language models such as BERT allows for high-quality text\nparaphrasing. This is a problem to academic integrity, as it is difficult to\ndifferentiate between original and machine-generated content. We propose a\nbenchmark consisting of paraphrased articles using recent language models\nrelying on the Transformer architecture. Our contribution fosters future\nresearch of paraphrase detection systems as it offers a large collection of\naligned original and paraphrased documents, a study regarding its structure,\nclassification experiments with state-of-the-art systems, and we make our\nfindings publicly available.", "published": "2021-03-23 11:01:35", "link": "http://arxiv.org/abs/2103.12450v5", "categories": ["cs.CL", "cs.AI", "cs.DL"], "primary_category": "cs.CL"}
{"title": "Multilingual Autoregressive Entity Linking", "abstract": "We present mGENRE, a sequence-to-sequence system for the Multilingual Entity\nLinking (MEL) problem -- the task of resolving language-specific mentions to a\nmultilingual Knowledge Base (KB). For a mention in a given language, mGENRE\npredicts the name of the target entity left-to-right, token-by-token in an\nautoregressive fashion. The autoregressive formulation allows us to effectively\ncross-encode mention string and entity names to capture more interactions than\nthe standard dot product between mention and entity vectors. It also enables\nfast search within a large KB even for mentions that do not appear in mention\ntables and with no need for large-scale vector indices. While prior MEL works\nuse a single representation for each entity, we match against entity names of\nas many languages as possible, which allows exploiting language connections\nbetween source input and target name. Moreover, in a zero-shot setting on\nlanguages with no training data at all, mGENRE treats the target language as a\nlatent variable that is marginalized at prediction time. This leads to over 50%\nimprovements in average accuracy. We show the efficacy of our approach through\nextensive evaluation including experiments on three popular MEL benchmarks\nwhere mGENRE establishes new state-of-the-art results. Code and pre-trained\nmodels at https://github.com/facebookresearch/GENRE.", "published": "2021-03-23 13:25:55", "link": "http://arxiv.org/abs/2103.12528v1", "categories": ["cs.CL", "cs.AI", "stat.ML"], "primary_category": "cs.CL"}
{"title": "PanGEA: The Panoramic Graph Environment Annotation Toolkit", "abstract": "PanGEA, the Panoramic Graph Environment Annotation toolkit, is a lightweight\ntoolkit for collecting speech and text annotations in photo-realistic 3D\nenvironments. PanGEA immerses annotators in a web-based simulation and allows\nthem to move around easily as they speak and/or listen. It includes database\nand cloud storage integration, plus utilities for automatically aligning\nrecorded speech with manual transcriptions and the virtual pose of the\nannotators. Out of the box, PanGEA supports two tasks -- collecting navigation\ninstructions and navigation instruction following -- and it could be easily\nadapted for annotating walking tours, finding and labeling landmarks or\nobjects, and similar tasks. We share best practices learned from using PanGEA\nin a 20,000 hour annotation effort to collect the Room-Across-Room dataset. We\nhope that our open-source annotation toolkit and insights will both expedite\nfuture data collection efforts and spur innovation on the kinds of grounded\nlanguage tasks such environments can support.", "published": "2021-03-23 17:24:12", "link": "http://arxiv.org/abs/2103.12703v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Unsupervised Contextual Paraphrase Generation using Lexical Control and\n  Reinforcement Learning", "abstract": "Customer support via chat requires agents to resolve customer queries with\nminimum wait time and maximum customer satisfaction. Given that the agents as\nwell as the customers can have varying levels of literacy, the overall quality\nof responses provided by the agents tend to be poor if they are not predefined.\nBut using only static responses can lead to customer detraction as the\ncustomers tend to feel that they are no longer interacting with a human. Hence,\nit is vital to have variations of the static responses to reduce monotonicity\nof the responses. However, maintaining a list of such variations can be\nexpensive. Given the conversation context and the agent response, we propose an\nunsupervised frame-work to generate contextual paraphrases using autoregressive\nmodels. We also propose an automated metric based on Semantic Similarity,\nTextual Entailment, Expression Diversity and Fluency to evaluate the quality of\ncontextual paraphrases and demonstrate performance improvement with\nReinforcement Learning (RL) fine-tuning using the automated metric as the\nreward function.", "published": "2021-03-23 18:22:03", "link": "http://arxiv.org/abs/2103.12777v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Variable Name Recovery in Decompiled Binary Code using Constrained\n  Masked Language Modeling", "abstract": "Decompilation is the procedure of transforming binary programs into a\nhigh-level representation, such as source code, for human analysts to examine.\nWhile modern decompilers can reconstruct and recover much information that is\ndiscarded during compilation, inferring variable names is still extremely\ndifficult. Inspired by recent advances in natural language processing, we\npropose a novel solution to infer variable names in decompiled code based on\nMasked Language Modeling, Byte-Pair Encoding, and neural architectures such as\nTransformers and BERT. Our solution takes \\textit{raw} decompiler output, the\nless semantically meaningful code, as input, and enriches it using our proposed\n\\textit{finetuning} technique, Constrained Masked Language Modeling. Using\nConstrained Masked Language Modeling introduces the challenge of predicting the\nnumber of masked tokens for the original variable name. We address this\n\\textit{count of token prediction} challenge with our post-processing\nalgorithm. Compared to the state-of-the-art approaches, our trained VarBERT\nmodel is simpler and of much better performance. We evaluated our model on an\nexisting large-scale data set with 164,632 binaries and showed that it can\npredict variable names identical to the ones present in the original source\ncode up to 84.15\\% of the time.", "published": "2021-03-23 19:09:22", "link": "http://arxiv.org/abs/2103.12801v1", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Towards a Formal Model of Narratives", "abstract": "In this paper, we propose the beginnings of a formal framework for modeling\nnarrative \\textit{qua} narrative. Our framework affords the ability to discuss\nkey qualities of stories and their communication, including the flow of\ninformation from a Narrator to a Reader, the evolution of a Reader's story\nmodel over time, and Reader uncertainty. We demonstrate its applicability to\ncomputational narratology by giving explicit algorithms for measuring the\naccuracy with which information was conveyed to the Reader and two novel\nmeasurements of story coherence.", "published": "2021-03-23 22:33:23", "link": "http://arxiv.org/abs/2103.12872v1", "categories": ["cs.CL", "cs.AI", "cs.LO"], "primary_category": "cs.CL"}
{"title": "How emoji and word embedding helps to unveil emotional transitions\n  during online messaging", "abstract": "During online chats, body-language and vocal characteristics are not part of\nthe communication mechanism making it challenging to facilitate an accurate\ninterpretation of feelings, emotions, and attitudes. The use of emojis to\nexpress emotional feeling is an alternative approach in these types of\ncommunication. In this project, we focus on modeling a customer's emotion in an\nonline messaging session with a chatbot. We use Affect Control Theory (ACT) to\npredict emotional change during the interaction. To let the customer use\nemojis, we also extend the affective dictionaries used by ACT. For this\npurpose, we mapped Emoji2vec embedding to the affective space. Our framework\ncan find emotional change during messaging and how a customer's reaction is\nchanged accordingly.", "published": "2021-03-23 12:45:17", "link": "http://arxiv.org/abs/2104.11032v1", "categories": ["cs.HC", "cs.CL", "cs.SI", "cs.SY", "eess.SY"], "primary_category": "cs.HC"}
{"title": "GISE-51: A scalable isolated sound events dataset", "abstract": "Most of the existing isolated sound event datasets comprise a small number of\nsound event classes, usually 10 to 15, restricted to a small domain, such as\ndomestic and urban sound events. In this work, we introduce GISE-51, a dataset\nspanning 51 isolated sound events belonging to a broad domain of event types.\nWe also release GISE-51-Mixtures, a dataset of 5-second soundscapes with\nhard-labelled event boundaries synthesized from GISE-51 isolated sound events.\nWe conduct baseline sound event recognition (SER) experiments on the\nGISE-51-Mixtures dataset, benchmarking prominent convolutional neural networks,\nand models trained with the dataset demonstrate strong transfer learning\nperformance on existing audio recognition benchmarks. Together, GISE-51 and\nGISE-51-Mixtures attempt to address some of the shortcomings of recent sound\nevent datasets, providing an open, reproducible benchmark for future research\nalong with the freedom to adapt the included isolated sound events for\ndomain-specific applications.", "published": "2021-03-23 04:59:06", "link": "http://arxiv.org/abs/2103.12306v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Joint framework with deep feature distillation and adaptive focal loss\n  for weakly supervised audio tagging and acoustic event detection", "abstract": "A good joint training framework is very helpful to improve the performances\nof weakly supervised audio tagging (AT) and acoustic event detection (AED)\nsimultaneously. In this study, we propose three methods to improve the best\nteacher-student framework in the IEEE AASP Challenge on Detection and\nClassification of Acoustic Scenes and Events (DCASE) 2019 Task 4 for both audio\ntagging and acoustic events detection tasks. A frame-level target-events based\ndeep feature distillation is first proposed, which aims to leverage the\npotential of limited strong-labeled data in weakly supervised framework to\nlearn better intermediate feature maps. Then, we propose an adaptive focal loss\nand two-stage training strategy to enable an effective and more accurate model\ntraining, where the contribution of hard and easy acoustic events to the total\ncost function can be automatically adjusted. Furthermore, an event-specific\npost processing is designed to improve the prediction of target event\ntime-stamps. Our experiments are performed on the public DCASE 2019 Task 4\ndataset, results show that our approach achieves competitive performances in\nboth AT (81.2\\% F1-score) and AED (49.8\\% F1-score) tasks.", "published": "2021-03-23 08:44:07", "link": "http://arxiv.org/abs/2103.12388v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Learned complex masks for multi-instrument source separation", "abstract": "Music source separation in the time-frequency domain is commonly achieved by\napplying a soft or binary mask to the magnitude component of (complex)\nspectrograms. The phase component is usually not estimated, but instead copied\nfrom the mixture and applied to the magnitudes of the estimated isolated\nsources. While this method has several practical advantages, it imposes an\nupper bound on the performance of the system, where the estimated isolated\nsources inherently exhibit audible \"phase artifacts\". In this paper we address\nthese shortcomings by directly estimating masks in the complex domain,\nextending recent work from the speech enhancement literature. The method is\nparticularly well suited for multi-instrument musical source separation since\nresidual phase artifacts are more pronounced for spectrally overlapping\ninstrument sources, a common scenario in music. We show that complex masks\nresult in better separation than masks that operate solely on the magnitude\ncomponent.", "published": "2021-03-23 21:56:28", "link": "http://arxiv.org/abs/2103.12864v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Automatic Cough Classification for Tuberculosis Screening in a\n  Real-World Environment", "abstract": "Objective: The automatic discrimination between the coughing sounds produced\nby patients with tuberculosis (TB) and those produced by patients with other\nlung ailments.\n  Approach: We present experiments based on a dataset of 1358 forced cough\nrecordings obtained in a developing-world clinic from 16 patients with\nconfirmed active pulmonary TB and 35 patients suffering from respiratory\nconditions suggestive of TB but confirmed to be TB negative. Using nested\ncross-validation, we have trained and evaluated five machine learning\nclassifiers: logistic regression (LR), support vector machines (SVM), k-nearest\nneighbour (KNN), multilayer perceptrons (MLP) and convolutional neural networks\n(CNN).\n  Main Results: Although classification is possible in all cases, the best\nperformance is achieved using LR. In combination with feature selection by\nsequential forward selection (SFS), our best LR system achieves an area under\nthe ROC curve (AUC) of 0.94 using 23 features selected from a set of 78\nhigh-resolution mel-frequency cepstral coefficients (MFCCs). This system\nachieves a sensitivity of 93\\% at a specificity of 95\\% and thus exceeds the\n90\\% sensitivity at 70\\% specificity specification considered by the World\nHealth Organisation (WHO) as a minimal requirement for a community-based TB\ntriage test.\n  Significance: The automatic classification of cough audio sounds, when\napplied to symptomatic patients requiring investigation for TB, can meet the\nWHO triage specifications for the identification of patients who should undergo\nexpensive molecular downstream testing. This makes it a promising and viable\nmeans of low cost, easily deployable frontline screening for TB, which can\nbenefit especially developing countries with a heavy TB burden.", "published": "2021-03-23 15:03:52", "link": "http://arxiv.org/abs/2103.13300v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
