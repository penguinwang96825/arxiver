{"title": "A Comprehensive Analysis of Machine Learning Models for Algorithmic Trading of Bitcoin", "abstract": "This study evaluates the performance of 41 machine learning models, including\n21 classifiers and 20 regressors, in predicting Bitcoin prices for algorithmic\ntrading. By examining these models under various market conditions, we\nhighlight their accuracy, robustness, and adaptability to the volatile\ncryptocurrency market. Our comprehensive analysis reveals the strengths and\nlimitations of each model, providing critical insights for developing effective\ntrading strategies. We employ both machine learning metrics (e.g., Mean\nAbsolute Error, Root Mean Squared Error) and trading metrics (e.g., Profit and\nLoss percentage, Sharpe Ratio) to assess model performance. Our evaluation\nincludes backtesting on historical data, forward testing on recent unseen data,\nand real-world trading scenarios, ensuring the robustness and practical\napplicability of our models. Key findings demonstrate that certain models, such\nas Random Forest and Stochastic Gradient Descent, outperform others in terms of\nprofit and risk management. These insights offer valuable guidance for traders\nand researchers aiming to leverage machine learning for cryptocurrency trading.", "published": "2024-07-09 13:07:43", "link": "http://arxiv.org/abs/2407.18334v1", "categories": ["q-fin.TR", "cs.AI", "cs.LG"], "primary_category": "q-fin.TR"}
{"title": "Enhancing Low-Resource NMT with a Multilingual Encoder and Knowledge\n  Distillation: A Case Study", "abstract": "Neural Machine Translation (NMT) remains a formidable challenge, especially\nwhen dealing with low-resource languages. Pre-trained sequence-to-sequence\n(seq2seq) multi-lingual models, such as mBART-50, have demonstrated impressive\nperformance in various low-resource NMT tasks. However, their pre-training has\nbeen confined to 50 languages, leaving out support for numerous low-resource\nlanguages, particularly those spoken in the Indian subcontinent. Expanding\nmBART-50's language support requires complex pre-training, risking performance\ndecline due to catastrophic forgetting. Considering these expanding challenges,\nthis paper explores a framework that leverages the benefits of a pre-trained\nlanguage model along with knowledge distillation in a seq2seq architecture to\nfacilitate translation for low-resource languages, including those not covered\nby mBART-50. The proposed framework employs a multilingual encoder-based\nseq2seq model as the foundational architecture and subsequently uses\ncomplementary knowledge distillation techniques to mitigate the impact of\nimbalanced training. Our framework is evaluated on three low-resource Indic\nlanguages in four Indic-to-Indic directions, yielding significant BLEU-4 and\nchrF improvements over baselines. Further, we conduct human evaluation to\nconfirm effectiveness of our approach. Our code is publicly available at\nhttps://github.com/raypretam/Two-step-low-res-NMT.", "published": "2024-07-09 04:19:52", "link": "http://arxiv.org/abs/2407.06538v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LIONs: An Empirically Optimized Approach to Align Language Models", "abstract": "Alignment is a crucial step to enhance the instruction-following and\nconversational abilities of language models. Despite many recent work proposing\nnew algorithms, datasets, and training pipelines, there is a lack of\ncomprehensive studies measuring the impact of various design choices throughout\nthe whole training process. We first conduct a rigorous analysis over a\nthree-stage training pipeline consisting of supervised fine-tuning, offline\npreference learning, and online preference learning. We have found that using\ntechniques like sequence packing, loss masking in SFT, increasing the\npreference dataset size in DPO, and online DPO training can significantly\nimprove the performance of language models. We then train from Gemma-2b-base\nand LLama-3-8b-base, and find that our best models exceed the performance of\nthe official instruct models tuned with closed-source data and algorithms. Our\ncode and models can be found at\n\\url{https://github.com/Columbia-NLP-Lab/LionAlignment}.", "published": "2024-07-09 04:34:39", "link": "http://arxiv.org/abs/2407.06542v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deciphering Assamese Vowel Harmony with Featural InfoWaveGAN", "abstract": "Traditional approaches for understanding phonological learning have\npredominantly relied on curated text data. Although insightful, such approaches\nlimit the knowledge captured in textual representations of the spoken language.\nTo overcome this limitation, we investigate the potential of the Featural\nInfoWaveGAN model to learn iterative long-distance vowel harmony using raw\nspeech data. We focus on Assamese, a language known for its phonologically\nregressive and word-bound vowel harmony. We demonstrate that the model is adept\nat grasping the intricacies of Assamese phonotactics, particularly iterative\nlong-distance harmony with regressive directionality. It also produced\nnon-iterative illicit forms resembling speech errors during human language\nacquisition. Our statistical analysis reveals a preference for a specific\n[+high,+ATR] vowel as a trigger across novel items, indicative of feature\nlearning. More data and control could improve model proficiency, contrasting\nthe universality of learning.", "published": "2024-07-09 05:01:13", "link": "http://arxiv.org/abs/2407.06547v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OffsetBias: Leveraging Debiased Data for Tuning Evaluators", "abstract": "Employing Large Language Models (LLMs) to assess the quality of generated\nresponses, such as prompting instruct-tuned models or fine-tuning judge models,\nhas become a widely adopted evaluation method. It is also known that such\nevaluators are vulnerable to biases, such as favoring longer responses. While\nit is important to overcome this problem, the specifics of these biases remain\nunder-explored. In this work, we qualitatively identify six types of biases\ninherent in various judge models. We propose EvalBiasBench as a meta-evaluation\ncollection of hand-crafted test cases for each bias type. Additionally, we\npresent de-biasing dataset construction methods and the associated preference\ndataset OffsetBias. Experimental results demonstrate that fine-tuning on our\ndataset significantly enhances the robustness of judge models against biases\nand improves performance across most evaluation scenarios. We release our\ndatasets and the fine-tuned judge model to public.", "published": "2024-07-09 05:16:22", "link": "http://arxiv.org/abs/2407.06551v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FinCon: A Synthesized LLM Multi-Agent System with Conceptual Verbal\n  Reinforcement for Enhanced Financial Decision Making", "abstract": "Large language models (LLMs) have demonstrated notable potential in\nconducting complex tasks and are increasingly utilized in various financial\napplications. However, high-quality sequential financial investment\ndecision-making remains challenging. These tasks require multiple interactions\nwith a volatile environment for every decision, demanding sufficient\nintelligence to maximize returns and manage risks. Although LLMs have been used\nto develop agent systems that surpass human teams and yield impressive\ninvestment returns, opportunities to enhance multi-sourced information\nsynthesis and optimize decision-making outcomes through timely experience\nrefinement remain unexplored. Here, we introduce the FinCon, an LLM-based\nmulti-agent framework with CONceptual verbal reinforcement tailored for diverse\nFINancial tasks. Inspired by effective real-world investment firm\norganizational structures, FinCon utilizes a manager-analyst communication\nhierarchy. This structure allows for synchronized cross-functional agent\ncollaboration towards unified goals through natural language interactions and\nequips each agent with greater memory capacity than humans. Additionally, a\nrisk-control component in FinCon enhances decision quality by episodically\ninitiating a self-critiquing mechanism to update systematic investment beliefs.\nThe conceptualized beliefs serve as verbal reinforcement for the future agent's\nbehavior and can be selectively propagated to the appropriate node that\nrequires knowledge updates. This feature significantly improves performance\nwhile reducing unnecessary peer-to-peer communication costs. Moreover, FinCon\ndemonstrates strong generalization capabilities in various financial tasks,\nincluding single stock trading and portfolio management.", "published": "2024-07-09 05:52:26", "link": "http://arxiv.org/abs/2407.06567v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NoisyAG-News: A Benchmark for Addressing Instance-Dependent Noise in\n  Text Classification", "abstract": "Existing research on learning with noisy labels predominantly focuses on\nsynthetic label noise. Although synthetic noise possesses well-defined\nstructural properties, it often fails to accurately replicate real-world noise\npatterns. In recent years, there has been a concerted effort to construct\ngeneralizable and controllable instance-dependent noise datasets for image\nclassification, significantly advancing the development of noise-robust\nlearning in this area. However, studies on noisy label learning for text\nclassification remain scarce. To better understand label noise in real-world\ntext classification settings, we constructed the benchmark dataset NoisyAG-News\nthrough manual annotation. Initially, we analyzed the annotated data to gather\nobservations about real-world noise. We qualitatively and quantitatively\ndemonstrated that real-world noisy labels adhere to instance-dependent\npatterns. Subsequently, we conducted comprehensive learning experiments on\nNoisyAG-News and its corresponding synthetic noise datasets using pre-trained\nlanguage models and noise-handling techniques. Our findings reveal that while\npre-trained models are resilient to synthetic noise, they struggle against\ninstance-dependent noise, with samples of varying confusion levels showing\ninconsistent performance during training and testing. These real-world noise\npatterns pose new, significant challenges, prompting a reevaluation of noisy\nlabel handling methods. We hope that NoisyAG-News will facilitate the\ndevelopment and evaluation of future solutions for learning with noisy labels.", "published": "2024-07-09 06:18:40", "link": "http://arxiv.org/abs/2407.06579v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Automatic Quality Metric for Evaluating Simultaneous Interpretation", "abstract": "Simultaneous interpretation (SI), the translation of one language to another\nin real time, starts translation before the original speech has finished. Its\nevaluation needs to consider both latency and quality. This trade-off is\nchallenging especially for distant word order language pairs such as English\nand Japanese. To handle this word order gap, interpreters maintain the word\norder of the source language as much as possible to keep up with original\nlanguage to minimize its latency while maintaining its quality, whereas in\ntranslation reordering happens to keep fluency in the target language. This\nmeans outputs synchronized with the source language are desirable based on the\nreal SI situation, and it's a key for further progress in computational SI and\nsimultaneous machine translation (SiMT). In this work, we propose an automatic\nevaluation metric for SI and SiMT focusing on word order synchronization. Our\nevaluation metric is based on rank correlation coefficients, leveraging\ncross-lingual pre-trained language models. Our experimental results on\nNAIST-SIC-Aligned and JNPC showed our metrics' effectiveness to measure word\norder synchronization between source and target language.", "published": "2024-07-09 08:21:40", "link": "http://arxiv.org/abs/2407.06650v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mixture-of-Modules: Reinventing Transformers as Dynamic Assemblies of\n  Modules", "abstract": "Is it always necessary to compute tokens from shallow to deep layers in\nTransformers? The continued success of vanilla Transformers and their variants\nsuggests an undoubted \"yes\". In this work, however, we attempt to break the\ndepth-ordered convention by proposing a novel architecture dubbed\nmixture-of-modules (MoM), which is motivated by an intuition that any layer,\nregardless of its position, can be used to compute a token as long as it\npossesses the needed processing capabilities. The construction of MoM starts\nfrom a finite set of modules defined by multi-head attention and feed-forward\nnetworks, each distinguished by its unique parameterization. Two routers then\niteratively select attention modules and feed-forward modules from the set to\nprocess a token. The selection dynamically expands the computation graph in the\nforward pass of the token, culminating in an assembly of modules. We show that\nMoM provides not only a unified framework for Transformers and their numerous\nvariants but also a flexible and learnable approach for reducing redundancy in\nTransformer parameterization. We pre-train various MoMs using OpenWebText.\nEmpirical results demonstrate that MoMs, of different parameter counts,\nconsistently outperform vanilla transformers on both GLUE and XSUM benchmarks.\nMore interestingly, with a fixed parameter budget, MoM-large enables an over\n38% increase in depth for computation graphs compared to GPT-2-large, resulting\nin absolute gains of 1.4 on GLUE and 1 on XSUM. On the other hand, MoM-large\nalso enables an over 60% reduction in depth while involving more modules per\nlayer, yielding a 16% reduction in TFLOPs and a 43% decrease in memory usage\ncompared to GPT-2-large, while maintaining comparable performance.", "published": "2024-07-09 08:50:18", "link": "http://arxiv.org/abs/2407.06677v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Consistent Document-Level Relation Extraction via Counterfactuals", "abstract": "Many datasets have been developed to train and evaluate document-level\nrelation extraction (RE) models. Most of these are constructed using real-world\ndata. It has been shown that RE models trained on real-world data suffer from\nfactual biases. To evaluate and address this issue, we present CovEReD, a\ncounterfactual data generation approach for document-level relation extraction\ndatasets using entity replacement. We first demonstrate that models trained on\nfactual data exhibit inconsistent behavior: while they accurately extract\ntriples from factual data, they fail to extract the same triples after\ncounterfactual modification. This inconsistency suggests that models trained on\nfactual data rely on spurious signals such as specific entities and external\nknowledge $\\unicode{x2013}$ rather than on the input context $\\unicode{x2013}$\nto extract triples. We show that by generating document-level counterfactual\ndata with CovEReD and training models on them, consistency is maintained with\nminimal impact on RE performance. We release our CovEReD pipeline as well as\nRe-DocRED-CF, a dataset of counterfactual RE documents, to assist in evaluating\nand addressing inconsistency in document-level RE.", "published": "2024-07-09 09:21:55", "link": "http://arxiv.org/abs/2407.06699v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using Pretrained Large Language Model with Prompt Engineering to Answer\n  Biomedical Questions", "abstract": "Our team participated in the BioASQ 2024 Task12b and Synergy tasks to build a\nsystem that can answer biomedical questions by retrieving relevant articles and\nsnippets from the PubMed database and generating exact and ideal answers. We\npropose a two-level information retrieval and question-answering system based\non pre-trained large language models (LLM), focused on LLM prompt engineering\nand response post-processing. We construct prompts with in-context few-shot\nexamples and utilize post-processing techniques like resampling and malformed\nresponse detection. We compare the performance of various pre-trained LLM\nmodels on this challenge, including Mixtral, OpenAI GPT and Llama2. Our\nbest-performing system achieved 0.14 MAP score on document retrieval, 0.05 MAP\nscore on snippet retrieval, 0.96 F1 score for yes/no questions, 0.38 MRR score\nfor factoid questions and 0.50 F1 score for list questions in Task 12b.", "published": "2024-07-09 11:48:49", "link": "http://arxiv.org/abs/2407.06779v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Safe-Embed: Unveiling the Safety-Critical Knowledge of Sentence Encoders", "abstract": "Despite the impressive capabilities of Large Language Models (LLMs) in\nvarious tasks, their vulnerability to unsafe prompts remains a critical issue.\nThese prompts can lead LLMs to generate responses on illegal or sensitive\ntopics, posing a significant threat to their safe and ethical use. Existing\napproaches attempt to address this issue using classification models, but they\nhave several drawbacks. With the increasing complexity of unsafe prompts,\nsimilarity search-based techniques that identify specific features of unsafe\nprompts provide a more robust and effective solution to this evolving problem.\nThis paper investigates the potential of sentence encoders to distinguish safe\nfrom unsafe prompts, and the ability to classify various unsafe prompts\naccording to a safety taxonomy. We introduce new pairwise datasets and the\nCategorical Purity (CP) metric to measure this capability. Our findings reveal\nboth the effectiveness and limitations of existing sentence encoders, proposing\ndirections to improve sentence encoders to operate as more robust safety\ndetectors. Our code is available at https://github.com/JwdanielJung/Safe-Embed.", "published": "2024-07-09 13:35:54", "link": "http://arxiv.org/abs/2407.06851v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Segment-Based Interactive Machine Translation for Pre-trained Models", "abstract": "Pre-trained large language models (LLM) are starting to be widely used in\nmany applications. In this work, we explore the use of these models in\ninteractive machine translation (IMT) environments. In particular, we have\nchosen mBART (multilingual Bidirectional and Auto-Regressive Transformer) and\nmT5 (multilingual Text-to-Text Transfer Transformer) as the LLMs to perform our\nexperiments. The system generates perfect translations interactively using the\nfeedback provided by the user at each iteration. The Neural Machine Translation\n(NMT) model generates a preliminary hypothesis with the feedback, and the user\nvalidates new correct segments and performs a word correction--repeating the\nprocess until the sentence is correctly translated. We compared the performance\nof mBART, mT5, and a state-of-the-art (SoTA) machine translation model on a\nbenchmark dataset regarding user effort, Word Stroke Ratio (WSR), Key Stroke\nRatio (KSR), and Mouse Action Ratio (MAR). The experimental results indicate\nthat mBART performed comparably with SoTA models, suggesting that it is a\nviable option for this field of IMT. The implications of this finding extend to\nthe development of new machine translation models for interactive environments,\nas it indicates that some novel pre-trained models exhibit SoTA performance in\nthis domain, highlighting the potential benefits of adapting these models to\nspecific needs.", "published": "2024-07-09 16:04:21", "link": "http://arxiv.org/abs/2407.06990v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Induction Heads as an Essential Mechanism for Pattern Matching in\n  In-context Learning", "abstract": "Large language models (LLMs) have shown a remarkable ability to learn and\nperform complex tasks through in-context learning (ICL). However, a\ncomprehensive understanding of its internal mechanisms is still lacking. This\npaper explores the role of induction heads in a few-shot ICL setting. We\nanalyse two state-of-the-art models, Llama-3-8B and InternLM2-20B on abstract\npattern recognition and NLP tasks. Our results show that even a minimal\nablation of induction heads leads to ICL performance decreases of up to ~32%\nfor abstract pattern recognition tasks, bringing the performance close to\nrandom. For NLP tasks, this ablation substantially decreases the model's\nability to benefit from examples, bringing few-shot ICL performance close to\nthat of zero-shot prompts. We further use attention knockout to disable\nspecific induction patterns, and present fine-grained evidence for the role\nthat the induction mechanism plays in ICL.", "published": "2024-07-09 16:29:21", "link": "http://arxiv.org/abs/2407.07011v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using Large Language Models for Generating Smart Contracts for Health\n  Insurance from Textual Policies", "abstract": "We explore using Large Language Models (LLMs) to generate application code\nthat automates health insurance processes from text-based policies. We target\nblockchain-based smart contracts as they offer immutability, verifiability,\nscalability, and a trustless setting: any number of parties can use the smart\ncontracts, and they need not have previously established trust relationships\nwith each other. Our methodology generates outputs at increasing levels of\ntechnical detail: (1) textual summaries, (2) declarative decision logic, and\n(3) smart contract code with unit tests. We ascertain LLMs are good at the task\n(1), and the structured output is useful to validate tasks (2) and (3).\nDeclarative languages (task 2) are often used to formalize healthcare policies,\nbut their execution on blockchain is non-trivial. Hence, task (3) attempts to\ndirectly automate the process using smart contracts. To assess the LLM output,\nwe propose completeness, soundness, clarity, syntax, and functioning code as\nmetrics. Our evaluation employs three health insurance policies (scenarios)\nwith increasing difficulty from Medicare's official booklet. Our evaluation\nuses GPT-3.5 Turbo, GPT-3.5 Turbo 16K, GPT-4, GPT-4 Turbo and CodeLLaMA. Our\nfindings confirm that LLMs perform quite well in generating textual summaries.\nAlthough outputs from tasks (2)-(3) are useful starting points, they require\nhuman oversight: in multiple cases, even \"runnable\" code will not yield sound\nresults; the popularity of the target language affects the output quality; and\nmore complex scenarios still seem a bridge too far. Nevertheless, our\nexperiments demonstrate the promise of LLMs for translating textual process\ndescriptions into smart contracts.", "published": "2024-07-09 16:40:44", "link": "http://arxiv.org/abs/2407.07019v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Decoding Climate Disagreement: A Graph Neural Network-Based Approach to\n  Understanding Social Media Dynamics", "abstract": "This work introduces the ClimateSent-GAT Model, an innovative method that\nintegrates Graph Attention Networks (GATs) with techniques from natural\nlanguage processing to accurately identify and predict disagreements within\nReddit comment-reply pairs. Our model classifies disagreements into three\ncategories: agree, disagree, and neutral. Leveraging the inherent graph\nstructure of Reddit comment-reply pairs, the model significantly outperforms\nexisting benchmarks by capturing complex interaction patterns and sentiment\ndynamics. This research advances graph-based NLP methodologies and provides\nactionable insights for policymakers and educators in climate science\ncommunication.", "published": "2024-07-09 17:00:39", "link": "http://arxiv.org/abs/2407.07038v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Internet of Agents: Weaving a Web of Heterogeneous Agents for\n  Collaborative Intelligence", "abstract": "The rapid advancement of large language models (LLMs) has paved the way for\nthe development of highly capable autonomous agents. However, existing\nmulti-agent frameworks often struggle with integrating diverse capable\nthird-party agents due to reliance on agents defined within their own\necosystems. They also face challenges in simulating distributed environments,\nas most frameworks are limited to single-device setups. Furthermore, these\nframeworks often rely on hard-coded communication pipelines, limiting their\nadaptability to dynamic task requirements. Inspired by the concept of the\nInternet, we propose the Internet of Agents (IoA), a novel framework that\naddresses these limitations by providing a flexible and scalable platform for\nLLM-based multi-agent collaboration. IoA introduces an agent integration\nprotocol, an instant-messaging-like architecture design, and dynamic mechanisms\nfor agent teaming and conversation flow control. Through extensive experiments\non general assistant tasks, embodied AI tasks, and retrieval-augmented\ngeneration benchmarks, we demonstrate that IoA consistently outperforms\nstate-of-the-art baselines, showcasing its ability to facilitate effective\ncollaboration among heterogeneous agents. IoA represents a step towards linking\ndiverse agents in an Internet-like environment, where agents can seamlessly\ncollaborate to achieve greater intelligence and capabilities. Our codebase has\nbeen released at \\url{https://github.com/OpenBMB/IoA}.", "published": "2024-07-09 17:33:24", "link": "http://arxiv.org/abs/2407.07061v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adapting LLMs to Hebrew: Unveiling DictaLM 2.0 with Enhanced Vocabulary\n  and Instruction Capabilities", "abstract": "Training large language models (LLMs) in low-resource languages such as\nHebrew poses unique challenges. In this paper, we introduce DictaLM2.0 and\nDictaLM2.0-Instruct, two LLMs derived from the Mistral model, trained on a\nsubstantial corpus of approximately 200 billion tokens in both Hebrew and\nEnglish. Adapting a pre-trained model to a new language involves specialized\ntechniques that differ significantly from training a model from scratch or\nfurther training existing models on well-resourced languages such as English.\nWe outline these novel training methodologies, which facilitate effective\nlearning and adaptation to the linguistic properties of Hebrew. Additionally,\nwe fine-tuned DictaLM2.0-Instruct on a comprehensive instruct dataset to\nenhance its performance on task-specific instructions. To rigorously evaluate\nour models, we introduce a new benchmark suite for Hebrew LLM evaluation,\ncovering a diverse set of tasks including Question Answering, Sentiment\nAnalysis, Winograd Schema Challenge, Translation, and Summarization. Our work\nnot only addresses the intricacies of training LLMs in low-resource languages\nbut also proposes a framework that can be leveraged for adapting other LLMs to\nvarious non-English languages, contributing to the broader field of\nmultilingual NLP.", "published": "2024-07-09 17:51:37", "link": "http://arxiv.org/abs/2407.07080v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reuse, Don't Retrain: A Recipe for Continued Pretraining of Language\n  Models", "abstract": "As language models have scaled both their number of parameters and\npretraining dataset sizes, the computational cost for pretraining has become\nintractable except for the most well-resourced teams. This increasing cost\nmakes it ever more important to be able to reuse a model after it has completed\npretraining; allowing for a model's abilities to further improve without\nneeding to train from scratch. In this work, we detail a set of guidelines that\ncover how to design efficacious data distributions and learning rate schedules\nfor continued pretraining of language models. When applying these findings\nwithin a continued pretraining run on top of a well-trained 15B parameter\nmodel, we show an improvement of 9\\% in average model accuracy compared to the\nbaseline of continued training on the pretraining set. The resulting recipe\nprovides a practical starting point with which to begin developing language\nmodels through reuse rather than retraining.", "published": "2024-07-09 22:37:59", "link": "http://arxiv.org/abs/2407.07263v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Optimising Hard Prompts with Few-Shot Meta-Prompting", "abstract": "Prompting is a flexible and adaptable way of providing instructions to a\nLarge Language Model (LLM). Contextual prompts include context in the form of a\ndocument or dialogue along with the natural language instructions to the LLM,\noften constraining the LLM to restrict facts to that of the given context while\ncomplying with the instructions. Masking the context, it acts as template for\nprompts. In this paper, we present an iterative method to generate better\ntemplates using an LLM from an existing set of prompt templates without\nrevealing the context to the LLM. Multiple methods of optimising prompts using\nthe LLM itself are explored to check the effect of few shot sampling methods on\niterative propagation while maintaining linguistic styles and syntax on\noptimisation of prompt templates, yielding a 103.87% improvement using the best\nperforming method. Comparison of the results of multiple contextual tasks\ndemonstrate the ability of LLMs to maintain syntax while learning to replicate\nlinguistic styles. Additionally, the effect on the output with different\nmethods of prompt template generation is shown.", "published": "2024-07-09 07:02:57", "link": "http://arxiv.org/abs/2407.18920v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Interaction Matters: An Evaluation Framework for Interactive Dialogue\n  Assessment on English Second Language Conversations", "abstract": "We present an evaluation framework for interactive dialogue assessment in the\ncontext of English as a Second Language (ESL) speakers. Our framework collects\ndialogue-level interactivity labels (e.g., topic management; 4 labels in total)\nand micro-level span features (e.g., backchannels; 17 features in total). Given\nour annotated data, we study how the micro-level features influence the (higher\nlevel) interactivity quality of ESL dialogues by constructing various machine\nlearning-based models. Our results demonstrate that certain micro-level\nfeatures strongly correlate with interactivity quality, like reference word\n(e.g., she, her, he), revealing new insights about the interaction between\nhigher-level dialogue quality and lower-level linguistic signals. Our framework\nalso provides a means to assess ESL communication, which is useful for language\nassessment.", "published": "2024-07-09 00:56:59", "link": "http://arxiv.org/abs/2407.06479v2", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Composable Interventions for Language Models", "abstract": "Test-time interventions for language models can enhance factual accuracy,\nmitigate harmful outputs, and improve model efficiency without costly\nretraining. But despite a flood of new methods, different types of\ninterventions are largely developing independently. In practice, multiple\ninterventions must be applied sequentially to the same model, yet we lack\nstandardized ways to study how interventions interact. We fill this gap by\nintroducing composable interventions, a framework to study the effects of using\nmultiple interventions on the same language models, featuring new metrics and a\nunified codebase. Using our framework, we conduct extensive experiments and\ncompose popular methods from three emerging intervention categories --\nKnowledge Editing, Model Compression, and Machine Unlearning. Our results from\n310 different compositions uncover meaningful interactions: compression hinders\nediting and unlearning, composing interventions hinges on their order of\napplication, and popular general-purpose metrics are inadequate for assessing\ncomposability. Taken together, our findings showcase clear gaps in\ncomposability, suggesting a need for new multi-objective interventions. All of\nour code is public:\nhttps://github.com/hartvigsen-group/composable-interventions.", "published": "2024-07-09 01:17:44", "link": "http://arxiv.org/abs/2407.06483v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Towards Understanding Multi-Task Learning (Generalization) of LLMs via\n  Detecting and Exploring Task-Specific Neurons", "abstract": "While large language models (LLMs) have demonstrated superior multi-task\ncapabilities, understanding the learning mechanisms behind this is still a\nchallenging problem. In this paper, we attempt to understand such mechanisms\nfrom the perspective of neurons. Specifically, we detect task-sensitive neurons\nin LLMs via gradient attribution on task-specific data. Through extensive\ndeactivation and fine-tuning experiments, we demonstrate that the detected\nneurons are highly correlated with the given task, which we term as\ntask-specific neurons. With these identified task-specific neurons, we delve\ninto two common problems in multi-task learning and continuous learning:\nGeneralization and Catastrophic Forgetting. We find that the overlap of\ntask-specific neurons is strongly associated with generalization and\nspecialization across tasks. Interestingly, at certain layers of LLMs, there is\na high similarity in the parameters of different task-specific neurons, and\nsuch similarity is highly correlated with the generalization performance.\nInspired by these findings, we propose a neuron-level continuous fine-tuning\nmethod that only fine-tunes the current task-specific neurons during continuous\nlearning, and extensive experiments demonstrate the effectiveness of the\nproposed method. Our study provides insights into the interpretability of LLMs\nin multi-task learning.", "published": "2024-07-09 01:27:35", "link": "http://arxiv.org/abs/2407.06488v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "STORYSUMM: Evaluating Faithfulness in Story Summarization", "abstract": "Human evaluation has been the gold standard for checking faithfulness in\nabstractive summarization. However, with a challenging source domain like\nnarrative, multiple annotators can agree a summary is faithful, while missing\ndetails that are obvious errors only once pointed out. We therefore introduce a\nnew dataset, STORYSUMM, comprising LLM summaries of short stories with\nlocalized faithfulness labels and error explanations. This benchmark is for\nevaluation methods, testing whether a given method can detect challenging\ninconsistencies. Using this dataset, we first show that any one human\nannotation protocol is likely to miss inconsistencies, and we advocate for\npursuing a range of methods when establishing ground truth for a summarization\ndataset. We finally test recent automatic metrics and find that none of them\nachieve more than 70% balanced accuracy on this task, demonstrating that it is\na challenging benchmark for future work in faithfulness evaluation.", "published": "2024-07-09 02:06:30", "link": "http://arxiv.org/abs/2407.06501v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Efficient and Accurate Memorable Conversation Model using DPO based on\n  sLLM", "abstract": "In multi-session dialog system, it is essential to continuously update the\nmemory as the session progresses. Simply accumulating memory can make it\ndifficult to focus on the content of the conversation for inference due to the\nlimited input sentence size. Therefore, efficient and accurate conversation\nmodel that is capable of managing memory to reflect the conversation history\ncontinuously is necessary. This paper presents a conversation model that\nefficiently manages memory as sessions progress and incorporates this into the\nmodel to reflect the conversation history accurately with 3 methodologies: SFT,\nDPO and DPO with SFT model. Our model using DPO algorithm shows an improvement\nabout 0.0591 of BERTScore in memory accuracy, and the rate of responses\nreflecting the memory increased as well. Also, response generation performance\nenhanced about 4.292 in fluency, 3.935 in coherence, and 2.896 in consistency.\nThis paper describes a training method that yields better performance than\nmodels with more than twice the parameter size, even when the model size is\nsmaller. Thus, our model demonstrates efficiency not only in terms of accuracy\nbut also in resource utilization.", "published": "2024-07-09 04:17:39", "link": "http://arxiv.org/abs/2407.06537v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Combining Knowledge Graphs and Large Language Models", "abstract": "In recent years, Natural Language Processing (NLP) has played a significant\nrole in various Artificial Intelligence (AI) applications such as chatbots,\ntext generation, and language translation. The emergence of large language\nmodels (LLMs) has greatly improved the performance of these applications,\nshowing astonishing results in language understanding and generation. However,\nthey still show some disadvantages, such as hallucinations and lack of\ndomain-specific knowledge, that affect their performance in real-world tasks.\nThese issues can be effectively mitigated by incorporating knowledge graphs\n(KGs), which organise information in structured formats that capture\nrelationships between entities in a versatile and interpretable fashion.\nLikewise, the construction and validation of KGs present challenges that LLMs\ncan help resolve. The complementary relationship between LLMs and KGs has led\nto a trend that combines these technologies to achieve trustworthy results.\nThis work collected 28 papers outlining methods for KG-powered LLMs, LLM-based\nKGs, and LLM-KG hybrid approaches. We systematically analysed and compared\nthese approaches to provide a comprehensive overview highlighting key trends,\ninnovative techniques, and common challenges. This synthesis will benefit\nresearchers new to the field and those seeking to deepen their understanding of\nhow KGs and LLMs can be effectively combined to enhance AI applications\ncapabilities.", "published": "2024-07-09 05:42:53", "link": "http://arxiv.org/abs/2407.06564v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Virtual Personas for Language Models via an Anthology of Backstories", "abstract": "Large language models (LLMs) are trained from vast repositories of text\nauthored by millions of distinct authors, reflecting an enormous diversity of\nhuman traits. While these models bear the potential to be used as\napproximations of human subjects in behavioral studies, prior efforts have been\nlimited in steering model responses to match individual human users. In this\nwork, we introduce \"Anthology\", a method for conditioning LLMs to particular\nvirtual personas by harnessing open-ended life narratives, which we refer to as\n\"backstories.\" We show that our methodology enhances the consistency and\nreliability of experimental outcomes while ensuring better representation of\ndiverse sub-populations. Across three nationally representative human surveys\nconducted as part of Pew Research Center's American Trends Panel (ATP), we\ndemonstrate that Anthology achieves up to 18% improvement in matching the\nresponse distributions of human respondents and 27% improvement in consistency\nmetrics. Our code and generated backstories are available at\nhttps://github.com/CannyLab/anthology.", "published": "2024-07-09 06:11:18", "link": "http://arxiv.org/abs/2407.06576v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Tailored Design of Audio-Visual Speech Recognition Models using\n  Branchformers", "abstract": "Recent advances in Audio-Visual Speech Recognition (AVSR) have led to\nunprecedented achievements in the field, improving the robustness of this type\nof system in adverse, noisy environments. In most cases, this task has been\naddressed through the design of models composed of two independent encoders,\neach dedicated to a specific modality. However, while recent works have\nexplored unified audio-visual encoders, determining the optimal cross-modal\narchitecture remains an ongoing challenge. Furthermore, such approaches often\nrely on models comprising vast amounts of parameters and high computational\ncost training processes. In this paper, we aim to bridge this research gap by\nintroducing a novel audio-visual framework. Our proposed method constitutes, to\nthe best of our knowledge, the first attempt to harness the flexibility and\ninterpretability offered by encoder architectures, such as the Branchformer, in\nthe design of parameter-efficient AVSR systems. To be more precise, the\nproposed framework consists of two steps: first, estimating audio- and\nvideo-only systems, and then designing a tailored audio-visual unified encoder\nbased on the layer-level branch scores provided by the modality-specific\nmodels. Extensive experiments on English and Spanish AVSR benchmarks covering\nmultiple data conditions and scenarios demonstrated the effectiveness of our\nproposed method. Even when trained on a moderate scale of data, our models\nachieve competitive word error rates (WER) of approximately 2.5\\% for English\nand surpass existing approaches for Spanish, establishing a new benchmark with\nan average WER of around 9.1\\%. These results reflect how our tailored AVSR\nsystem is able to reach state-of-the-art recognition rates while significantly\nreducing the model complexity w.r.t. the prevalent approach in the field. Code\nand pre-trained models are available at\nhttps://github.com/david-gimeno/tailored-avsr.", "published": "2024-07-09 07:15:56", "link": "http://arxiv.org/abs/2407.06606v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Entropy Law: The Story Behind Data Compression and LLM Performance", "abstract": "Data is the cornerstone of large language models (LLMs), but not all data is\nuseful for model learning. Carefully selected data can better elicit the\ncapabilities of LLMs with much less computational overhead. Most methods\nconcentrate on evaluating the quality of individual samples in data selection,\nwhile the combinatorial effects among samples are neglected. Even if each\nsample is of perfect quality, their combinations may be suboptimal in teaching\nLLMs due to their intrinsic homogeneity or contradiction. In this paper, we aim\nto uncover the underlying relationships between LLM performance and data\nselection. Inspired by the information compression nature of LLMs, we uncover\nan ``entropy law'' that connects LLM performance with data compression ratio\nand first-epoch training loss, which reflect the information redundancy of a\ndataset and the mastery of inherent knowledge encoded in this dataset,\nrespectively. Through both theoretical deduction and empirical evaluation, we\nfind that model performance is negatively correlated to the compression ratio\nof training data, which usually yields a lower training loss. Based on the\nfindings of the entropy law, we propose a quite efficient and universal data\nselection method named \\textbf{ZIP} for training LLMs, which aim to prioritize\ndata subsets exhibiting a low compression ratio. Based on a multi-stage\nalgorithm that selects diverse data in a greedy manner, we can obtain a good\ndata subset with satisfactory diversity. Extensive experiments have been\nconducted to validate the entropy law and the superiority of ZIP across\ndifferent LLM backbones and alignment stages. We also present an interesting\napplication of entropy law that can detect potential performance risks at the\nbeginning of model training.", "published": "2024-07-09 08:14:29", "link": "http://arxiv.org/abs/2407.06645v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "SoftDedup: an Efficient Data Reweighting Method for Speeding Up Language\n  Model Pre-training", "abstract": "The effectiveness of large language models (LLMs) is often hindered by\nduplicated data in their extensive pre-training datasets. Current approaches\nprimarily focus on detecting and removing duplicates, which risks the loss of\nvaluable information and neglects the varying degrees of duplication. To\naddress this, we propose a soft deduplication method that maintains dataset\nintegrity while selectively reducing the sampling weight of data with high\ncommonness. Central to our approach is the concept of \"data commonness\", a\nmetric we introduce to quantify the degree of duplication by measuring the\noccurrence probabilities of samples using an n-gram model. Empirical analysis\nshows that this method significantly improves training efficiency, achieving\ncomparable perplexity scores with at least a 26% reduction in required training\nsteps. Additionally, it enhances average few-shot downstream accuracy by 1.77%\nwhen trained for an equivalent duration. Importantly, this approach\nconsistently improves performance, even on rigorously deduplicated datasets,\nindicating its potential to complement existing methods and become a standard\npre-training process for LLMs.", "published": "2024-07-09 08:26:39", "link": "http://arxiv.org/abs/2407.06654v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ChatGPT Doesn't Trust Chargers Fans: Guardrail Sensitivity in Context", "abstract": "While the biases of language models in production are extensively documented,\nthe biases of their guardrails have been neglected. This paper studies how\ncontextual information about the user influences the likelihood of an LLM to\nrefuse to execute a request. By generating user biographies that offer\nideological and demographic information, we find a number of biases in\nguardrail sensitivity on GPT-3.5. Younger, female, and Asian-American personas\nare more likely to trigger a refusal guardrail when requesting censored or\nillegal information. Guardrails are also sycophantic, refusing to comply with\nrequests for a political position the user is likely to disagree with. We find\nthat certain identity groups and seemingly innocuous information, e.g., sports\nfandom, can elicit changes in guardrail sensitivity similar to direct\nstatements of political ideology. For each demographic category and even for\nAmerican football team fandom, we find that ChatGPT appears to infer a likely\npolitical ideology and modify guardrail behavior accordingly.", "published": "2024-07-09 13:53:38", "link": "http://arxiv.org/abs/2407.06866v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Measuring Sustainability Intention of ESG Fund Disclosure using Few-Shot\n  Learning", "abstract": "Global sustainable fund universe encompasses open-end funds and\nexchange-traded funds (ETF) that, by prospectus or other regulatory filings,\nclaim to focus on Environment, Social and Governance (ESG). Challengingly, the\nclaims can only be confirmed by examining the textual disclosures to check if\nthere is presence of intentionality and ESG focus on its investment strategy.\nCurrently, there is no regulation to enforce sustainability in ESG products\nspace. This paper proposes a unique method and system to classify and score the\nfund prospectuses in the sustainable universe regarding specificity and\ntransparency of language. We aim to employ few-shot learners to identify\nspecific, ambiguous, and generic sustainable investment-related language.\nAdditionally, we construct a ratio metric to determine language score and\nrating to rank products and quantify sustainability claims for US sustainable\nuniverse. As a by-product, we publish manually annotated quality training\ndataset on Hugging Face (ESG-Prospectus-Clarity-Category under cc-by-nc-sa-4.0)\nof more than 1K ESG textual statements. The performance of the few-shot\nfinetuning approach is compared with zero-shot models e.g., Llama-13B, GPT 3.5\nTurbo etc. We found that prompting large language models are not accurate for\ndomain specific tasks due to misalignment issues. The few-shot finetuning\ntechniques outperform zero-shot models by large margins of more than absolute\n~30% in precision, recall and F1 metrics on completely unseen ESG languages\n(test set). Overall, the paper attempts to establish a systematic and scalable\napproach to measure and rate sustainability intention quantitatively for\nsustainable funds using texts in prospectus. Regulatory bodies, investors, and\nadvisors may utilize the findings of this research to reduce cognitive load in\ninvestigating or screening of ESG funds which accurately reflects the ESG\nintention.", "published": "2024-07-09 14:25:23", "link": "http://arxiv.org/abs/2407.06893v1", "categories": ["cs.CL", "cs.CE"], "primary_category": "cs.CL"}
{"title": "Divine LLaMAs: Bias, Stereotypes, Stigmatization, and Emotion\n  Representation of Religion in Large Language Models", "abstract": "Emotions play important epistemological and cognitive roles in our lives,\nrevealing our values and guiding our actions. Previous work has shown that LLMs\ndisplay biases in emotion attribution along gender lines. However, unlike\ngender, which says little about our values, religion, as a socio-cultural\nsystem, prescribes a set of beliefs and values for its followers. Religions,\ntherefore, cultivate certain emotions. Moreover, these rules are explicitly\nlaid out and interpreted by religious leaders. Using emotion attribution, we\nexplore how different religions are represented in LLMs. We find that: Major\nreligions in the US and European countries are represented with more nuance,\ndisplaying a more shaded model of their beliefs. Eastern religions like\nHinduism and Buddhism are strongly stereotyped. Judaism and Islam are\nstigmatized -- the models' refusal skyrocket. We ascribe these to cultural bias\nin LLMs and the scarcity of NLP literature on religion. In the rare instances\nwhere religion is discussed, it is often in the context of toxic language,\nperpetuating the perception of these religions as inherently toxic. This\nfinding underscores the urgent need to address and rectify these biases. Our\nresearch underscores the crucial role emotions play in our lives and how our\nvalues influence them.", "published": "2024-07-09 14:45:15", "link": "http://arxiv.org/abs/2407.06908v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Who is better at math, Jenny or Jingzhen? Uncovering Stereotypes in\n  Large Language Models", "abstract": "Large language models (LLMs) have been shown to propagate and amplify harmful\nstereotypes, particularly those that disproportionately affect marginalised\ncommunities. To understand the effect of these stereotypes more\ncomprehensively, we introduce GlobalBias, a dataset of 876k sentences\nincorporating 40 distinct gender-by-ethnicity groups alongside descriptors\ntypically used in bias literature, which enables us to study a broad set of\nstereotypes from around the world. We use GlobalBias to directly probe a suite\nof LMs via perplexity, which we use as a proxy to determine how certain\nstereotypes are represented in the model's internal representations. Following\nthis, we generate character profiles based on given names and evaluate the\nprevalence of stereotypes in model outputs. We find that the demographic groups\nassociated with various stereotypes remain consistent across model likelihoods\nand model outputs. Furthermore, larger models consistently display higher\nlevels of stereotypical outputs, even when explicitly instructed not to.", "published": "2024-07-09 14:52:52", "link": "http://arxiv.org/abs/2407.06917v3", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Raply: A profanity-mitigated rap generator", "abstract": "The task of writing rap is challenging and involves producing complex rhyming\nschemes, yet meaningful lyrics. In this work, we propose Raply, a fine-tuned\nGPT-2 model capable of producing meaningful rhyming text in the style of rap.\nIn addition to its rhyming capabilities, the model is able to generate less\noffensive content. It was achieved through the fine-tuning the model on a new\ndataset Mitislurs, a profanity-mitigated corpus. We evaluate the output of the\nmodel on two criteria: 1) rhyming based on the rhyme density metric; 2)\nprofanity content, using the list of profanities for the English language. To\nour knowledge, this is the first attempt at profanity mitigation for rap lyrics\ngeneration.", "published": "2024-07-09 15:18:56", "link": "http://arxiv.org/abs/2407.06941v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Spanish TrOCR: Leveraging Transfer Learning for Language Adaptation", "abstract": "This study explores the transfer learning capabilities of the TrOCR\narchitecture to Spanish. TrOCR is a transformer-based Optical Character\nRecognition (OCR) model renowned for its state-of-the-art performance in\nEnglish benchmarks. Inspired by Li et al. assertion regarding its adaptability\nto multilingual text recognition, we investigate two distinct approaches to\nadapt the model to a new language: integrating an English TrOCR encoder with a\nlanguage specific decoder and train the model on this specific language, and\nfine-tuning the English base TrOCR model on a new language data. Due to the\nscarcity of publicly available datasets, we present a resource-efficient\npipeline for creating OCR datasets in any language, along with a comprehensive\nbenchmark of the different image generation methods employed with a focus on\nVisual Rich Documents (VRDs). Additionally, we offer a comparative analysis of\nthe two approaches for the Spanish language, demonstrating that fine-tuning the\nEnglish TrOCR on Spanish yields superior recognition than the language specific\ndecoder for a fixed dataset size. We evaluate our model employing character and\nword error rate metrics on a public available printed dataset, comparing the\nperformance against other open-source and cloud OCR spanish models. As far as\nwe know, these resources represent the best open-source model for OCR in\nSpanish. The Spanish TrOCR models are publicly available on HuggingFace [20]\nand the code to generate the dataset is available on Github [25].", "published": "2024-07-09 15:31:41", "link": "http://arxiv.org/abs/2407.06950v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "ICLGuard: Controlling In-Context Learning Behavior for Applicability\n  Authorization", "abstract": "In-context learning (ICL) is a recent advancement in the capabilities of\nlarge language models (LLMs). This feature allows users to perform a new task\nwithout updating the model. Concretely, users can address tasks during the\ninference time by conditioning on a few input-label pair demonstrations along\nwith the test input. It is different than the conventional fine-tuning paradigm\nand offers more flexibility. However, this capability also introduces potential\nissues. For example, users may use the model on any data without restriction,\nsuch as performing tasks with improper or sensitive content, which might\nviolate the model policy or conflict with the model owner's interests. As a\nmodel owner, it is crucial to establish a mechanism to control the model's\nbehavior under ICL, depending on the model owner's requirements for various\ncontent. To this end, we introduce the concept of \"applicability authorization\"\ntailored for LLMs, particularly for ICL behavior, and propose a simple\napproach, ICLGuard. It is a fine-tuning framework designed to allow the model\nowner to regulate ICL behavior on different data. ICLGuard preserves the\noriginal LLM and fine-tunes only a minimal set of additional trainable\nparameters to \"guard\" the LLM. Empirical results show that the guarded LLM can\ndeactivate its ICL ability on target data without affecting its ICL ability on\nother data and its general functionality across all data.", "published": "2024-07-09 15:35:06", "link": "http://arxiv.org/abs/2407.06955v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Vision-and-Language Navigation Today and Tomorrow: A Survey in the Era\n  of Foundation Models", "abstract": "Vision-and-Language Navigation (VLN) has gained increasing attention over\nrecent years and many approaches have emerged to advance their development. The\nremarkable achievements of foundation models have shaped the challenges and\nproposed methods for VLN research. In this survey, we provide a top-down review\nthat adopts a principled framework for embodied planning and reasoning, and\nemphasizes the current methods and future opportunities leveraging foundation\nmodels to address VLN challenges. We hope our in-depth discussions could\nprovide valuable resources and insights: on one hand, to milestone the progress\nand explore opportunities and potential roles for foundation models in this\nfield, and on the other, to organize different challenges and solutions in VLN\nto foundation model researchers.", "published": "2024-07-09 16:53:36", "link": "http://arxiv.org/abs/2407.07035v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "CopyBench: Measuring Literal and Non-Literal Reproduction of\n  Copyright-Protected Text in Language Model Generation", "abstract": "Evaluating the degree of reproduction of copyright-protected content by\nlanguage models (LMs) is of significant interest to the AI and legal\ncommunities. Although both literal and non-literal similarities are considered\nby courts when assessing the degree of reproduction, prior research has focused\nonly on literal similarities. To bridge this gap, we introduce CopyBench, a\nbenchmark designed to measure both literal and non-literal copying in LM\ngenerations. Using copyrighted fiction books as text sources, we provide\nautomatic evaluation protocols to assess literal and non-literal copying,\nbalanced against the model utility in terms of the ability to recall facts from\nthe copyrighted works and generate fluent completions. We find that, although\nliteral copying is relatively rare, two types of non-literal copying -- event\ncopying and character copying -- occur even in models as small as 7B\nparameters. Larger models demonstrate significantly more copying, with literal\ncopying rates increasing from 0.2\\% to 10.5\\% and non-literal copying from\n2.3\\% to 5.9\\% when comparing Llama3-8B and 70B models, respectively. We\nfurther evaluate the effectiveness of current strategies for mitigating copying\nand show that (1) training-time alignment can reduce literal copying but may\nincrease non-literal copying, and (2) current inference-time mitigation methods\nprimarily reduce literal but not non-literal copying.", "published": "2024-07-09 17:58:18", "link": "http://arxiv.org/abs/2407.07087v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AnyTaskTune: Advanced Domain-Specific Solutions through Task-Fine-Tuning", "abstract": "The pervasive deployment of Large Language Models-LLMs in various sectors\noften neglects the nuanced requirements of individuals and small organizations,\nwho benefit more from models precisely tailored to their specific business\ncontexts rather than those with broadly superior general capabilities. This\nwork introduces \\textbf{AnyTaskTune}, a novel fine-tuning methodology coined as\n\\textbf{Task-Fine-Tune}, specifically developed to elevate model performance on\na diverse array of domain-specific tasks. This method involves a meticulous\nprocess to identify and define targeted sub-tasks within a domain, followed by\nthe creation of specialized enhancement datasets for fine-tuning, thereby\noptimizing task-specific model performance. We conducted comprehensive\nfine-tuning experiments not only in the legal domain for tasks such as keyword\nextraction and sentence prediction but across over twenty different sub-tasks\nderived from the domains of finance, healthcare, law, psychology, consumer\nservices, and human resources. To substantiate our approach and facilitate\ncommunity engagement, we will open-source these bilingual task datasets. Our\nfindings demonstrate that models fine-tuned using the \\textbf{Task-Fine-Tune}\nmethodology not only achieve superior performance on these specific tasks but\nalso significantly outperform models with higher general capabilities in their\nrespective domains. Our work is publicly available at\n\\url{https://github.com/PandaVT/DataTager}.", "published": "2024-07-09 17:59:56", "link": "http://arxiv.org/abs/2407.07094v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ConvNLP: Image-based AI Text Detection", "abstract": "The potentials of Generative-AI technologies like Large Language models\n(LLMs) to revolutionize education are undermined by ethical considerations\naround their misuse which worsens the problem of academic dishonesty. LLMs like\nGPT-4 and Llama 2 are becoming increasingly powerful in generating\nsophisticated content and answering questions, from writing academic essays to\nsolving complex math problems. Students are relying on these LLMs to complete\ntheir assignments and thus compromising academic integrity. Solutions to detect\nLLM-generated text are compute-intensive and often lack generalization. This\npaper presents a novel approach for detecting LLM-generated AI-text using a\nvisual representation of word embedding. We have formulated a novel\nConvolutional Neural Network called ZigZag ResNet, as well as a scheduler for\nimproving generalization, named ZigZag Scheduler. Through extensive evaluation\nusing datasets of text generated by six different state-of-the-art LLMs, our\nmodel demonstrates strong intra-domain and inter-domain generalization\ncapabilities. Our best model detects AI-generated text with an impressive\naverage detection rate (over inter- and intra-domain test data) of 88.35%.\nThrough an exhaustive ablation study, our ZigZag ResNet and ZigZag Scheduler\nprovide a performance improvement of nearly 4% over the vanilla ResNet. The\nend-to-end inference latency of our model is below 2.5ms per sentence. Our\nsolution offers a lightweight, computationally efficient, and faster\nalternative to existing tools for AI-generated text detection, with better\ngeneralization performance. It can help academic institutions in their fight\nagainst the misuse of LLMs in academic settings. Through this work, we aim to\ncontribute to safeguarding the principles of academic integrity and ensuring\nthe trustworthiness of student work in the era of advanced LLMs.", "published": "2024-07-09 20:44:40", "link": "http://arxiv.org/abs/2407.07225v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Identification of emotions on Twitter during the 2022 electoral process\n  in Colombia", "abstract": "The study of Twitter as a means for analyzing social phenomena has gained\ninterest in recent years due to the availability of large amounts of data in a\nrelatively spontaneous environment. Within opinion-mining tasks, emotion\ndetection is specially relevant, as it allows for the identification of\npeople's subjective responses to different social events in a more granular way\nthan traditional sentiment analysis based on polarity. In the particular case\nof political events, the analysis of emotions in social networks can provide\nvaluable information on the perception of candidates, proposals, and other\nimportant aspects of the public debate. In spite of this importance, there are\nfew studies on emotion detection in Spanish and, to the best of our knowledge,\nfew resources are public for opinion mining in Colombian Spanish, highlighting\nthe need for generating resources addressing the specific cultural\ncharacteristics of this variety. In this work, we present a small corpus of\ntweets in Spanish related to the 2022 Colombian presidential elections,\nmanually labeled with emotions using a fine-grained taxonomy. We perform\nclassification experiments using supervised state-of-the-art models (BERT\nmodels) and compare them with GPT-3.5 in few-shot learning settings. We make\nour dataset and code publicly available for research purposes.", "published": "2024-07-09 22:26:42", "link": "http://arxiv.org/abs/2407.07258v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Media Manipulations in the Coverage of Events of the Ukrainian\n  Revolution of Dignity: Historical, Linguistic, and Psychological Approaches", "abstract": "This article examines the use of manipulation in the coverage of events of\nthe Ukrainian Revolution of Dignity in the mass media, namely in the content of\nthe online newspaper Ukrainian Truth (Ukrainska pravda), online newspaper High\nCastle (Vysokyi Zamok), and online newspaper ZIK during the public protest,\nnamely during the Ukrainian Revolution of Dignity. Contents of these online\nnewspapers the historical, linguistic, and psychological approaches are used.\nAlso media manipulations in the coverage of events of the Ukrainian Revolution\nof Dignity are studied. Internet resources that cover news are analyzed.\nCurrent and most popular Internet resources are identified. The content of\nonline newspapers is analyzed and statistically processed. Internet content of\nnewspapers by the level of significance of data (very significant data,\nsignificant data and insignificant data) is classified. The algorithm of\ndetection of the media manipulations in the highlighting the course of the\nUkrainian revolutions based on historical, linguistic, and psychological\napproaches is designed. Methods of counteracting information attacks in online\nnewspapers are developed.", "published": "2024-07-09 09:46:27", "link": "http://arxiv.org/abs/2407.17425v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "AutoTask: Task Aware Multi-Faceted Single Model for Multi-Task Ads\n  Relevance", "abstract": "Ads relevance models are crucial in determining the relevance between user\nsearch queries and ad offers, often framed as a classification problem. The\ncomplexity of modeling increases significantly with multiple ad types and\nvarying scenarios that exhibit both similarities and differences. In this work,\nwe introduce a novel multi-faceted attention model that performs task aware\nfeature combination and cross task interaction modeling. Our technique\nformulates the feature combination problem as \"language\" modeling with\nauto-regressive attentions across both feature and task dimensions.\nSpecifically, we introduce a new dimension of task ID encoding for task\nrepresentations, thereby enabling precise relevance modeling across diverse ad\nscenarios with substantial improvement in generality capability for unseen\ntasks. We demonstrate that our model not only effectively handles the increased\ncomputational and maintenance demands as scenarios proliferate, but also\noutperforms generalized DNN models and even task-specific models across a\nspectrum of ad applications using a single unified model.", "published": "2024-07-09 05:13:45", "link": "http://arxiv.org/abs/2407.06549v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Learn and Don't Forget: Adding a New Language to ASR Foundation Models", "abstract": "Foundation ASR models often support many languages, e.g. 100 languages in\nWhisper. However, there has been limited work on integrating an additional,\ntypically low-resource, language, while maintaining performance on the original\nlanguage set. Fine-tuning, while simple, may degrade the accuracy of the\noriginal set. We compare three approaches that exploit adaptation parameters:\nsoft language code tuning, train only the language code; soft prompt tuning,\ntrain prepended tokens; and LoRA where a small set of additional parameters are\noptimised. Elastic Weight Consolidation (EWC) offers an alternative compromise\nwith the potential to maintain performance in specific target languages.\nResults show that direct fine-tuning yields the best performance for the new\nlanguage but degrades existing language capabilities. EWC can address this\nissue for specific languages. If only adaptation parameters are used, the\nlanguage capabilities are maintained but at the cost of performance in the new\nlanguage.", "published": "2024-07-09 12:14:48", "link": "http://arxiv.org/abs/2407.06800v3", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Self-Recognition in Language Models", "abstract": "A rapidly growing number of applications rely on a small set of closed-source\nlanguage models (LMs). This dependency might introduce novel security risks if\nLMs develop self-recognition capabilities. Inspired by human identity\nverification methods, we propose a novel approach for assessing\nself-recognition in LMs using model-generated \"security questions\". Our test\ncan be externally administered to monitor frontier models as it does not\nrequire access to internal model parameters or output probabilities. We use our\ntest to examine self-recognition in ten of the most capable open- and\nclosed-source LMs currently publicly available. Our extensive experiments found\nno empirical evidence of general or consistent self-recognition in any examined\nLM. Instead, our results suggest that given a set of alternatives, LMs seek to\npick the \"best\" answer, regardless of its origin. Moreover, we find indications\nthat preferences about which models produce the best answers are consistent\nacross LMs. We additionally uncover novel insights on position bias\nconsiderations for LMs in multiple-choice settings.", "published": "2024-07-09 15:23:28", "link": "http://arxiv.org/abs/2407.06946v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Listen and Speak Fairly: A Study on Semantic Gender Bias in Speech\n  Integrated Large Language Models", "abstract": "Speech Integrated Large Language Models (SILLMs) combine large language\nmodels with speech perception to perform diverse tasks, such as emotion\nrecognition to speaker verification, demonstrating universal audio\nunderstanding capability. However, these models may amplify biases present in\ntraining data, potentially leading to biased access to information for\nmarginalized groups. This work introduces a curated spoken bias evaluation\ntoolkit and corresponding dataset. We evaluate gender bias in SILLMs across\nfour semantic-related tasks: speech-to-text translation (STT), spoken\ncoreference resolution (SCR), spoken sentence continuation (SSC), and spoken\nquestion answering (SQA). Our analysis reveals that bias levels are\nlanguage-dependent and vary with different evaluation methods. Our findings\nemphasize the necessity of employing multiple approaches to comprehensively\nassess biases in SILLMs, providing insights for developing fairer SILLM\nsystems.", "published": "2024-07-09 15:35:43", "link": "http://arxiv.org/abs/2407.06957v1", "categories": ["eess.AS", "cs.CL", "cs.CY"], "primary_category": "eess.AS"}
{"title": "Robust Neural Information Retrieval: An Adversarial and\n  Out-of-distribution Perspective", "abstract": "Recent advances in neural information retrieval (IR) models have\nsignificantly enhanced their effectiveness over various IR tasks. The\nrobustness of these models, essential for ensuring their reliability in\npractice, has also garnered significant attention. With a wide array of\nresearch on robust IR being proposed, we believe it is the opportune moment to\nconsolidate the current status, glean insights from existing methodologies, and\nlay the groundwork for future development. We view the robustness of IR to be a\nmultifaceted concept, emphasizing its necessity against adversarial attacks,\nout-of-distribution (OOD) scenarios and performance variance. With a focus on\nadversarial and OOD robustness, we dissect robustness solutions for dense\nretrieval models (DRMs) and neural ranking models (NRMs), respectively,\nrecognizing them as pivotal components of the neural IR pipeline. We provide an\nin-depth discussion of existing methods, datasets, and evaluation metrics,\nshedding light on challenges and future directions in the era of large language\nmodels. To the best of our knowledge, this is the first comprehensive survey on\nthe robustness of neural IR models, and we will also be giving our first\ntutorial presentation at SIGIR 2024\n\\url{https://sigir2024-robust-information-retrieval.github.io}. Along with the\norganization of existing work, we introduce a Benchmark for robust IR (BestIR),\na heterogeneous evaluation benchmark for robust neural information retrieval,\nwhich is publicly available at \\url{https://github.com/Davion-Liu/BestIR}. We\nhope that this study provides useful clues for future research on the\nrobustness of IR models and helps to develop trustworthy search engines\n\\url{https://github.com/Davion-Liu/Awesome-Robustness-in-Information-Retrieval}.", "published": "2024-07-09 16:07:01", "link": "http://arxiv.org/abs/2407.06992v2", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Etalon: Holistic Performance Evaluation Framework for LLM Inference\n  Systems", "abstract": "Serving large language models (LLMs) in production can incur substantial\ncosts, which has prompted recent advances in inference system optimizations.\nToday, these systems are evaluated against conventional latency and throughput\nmetrics (eg. TTFT, TBT, Normalised Latency and TPOT). However, these metrics\nfail to fully capture the nuances of LLM inference, leading to an incomplete\nassessment of user-facing performance crucial for real-time applications such\nas chat and translation. In this paper, we first identify the pitfalls of\ncurrent performance metrics in evaluating LLM inference systems. We then\npropose Etalon, a comprehensive performance evaluation framework that includes\nfluidity-index -- a novel metric designed to reflect the intricacies of the LLM\ninference process and its impact on real-time user experience. Finally, we\nevaluate various existing open-source platforms and model-as-a-service\nofferings using Etalon, discussing their strengths and weaknesses. Etalon is\navailable at https://github.com/project-etalon/etalon.", "published": "2024-07-09 16:13:26", "link": "http://arxiv.org/abs/2407.07000v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DC"], "primary_category": "cs.LG"}
{"title": "Empirical analysis of Binding Precedent efficiency in the Brazilian\n  Supreme Court via Similar Case Retrieval", "abstract": "Binding precedents (S\\'umulas Vinculantes) constitute a juridical instrument\nunique to the Brazilian legal system and whose objectives include the\nprotection of the Federal Supreme Court against repetitive demands. Studies of\nthe effectiveness of these instruments in decreasing the Court's exposure to\nsimilar cases, however, indicate that they tend to fail in such a direction,\nwith some of the binding precedents seemingly creating new demands. We\nempirically assess the legal impact of five binding precedents, 11, 14, 17, 26\nand 37, at the highest court level through their effects on the legal subjects\nthey address. This analysis is only possible through the comparison of the\nCourt's ruling about the precedents' themes before they are created, which\nmeans that these decisions should be detected through techniques of Similar\nCase Retrieval. The contributions of this article are therefore twofold: on the\nmathematical side, we compare the uses of different methods of Natural Language\nProcessing -- TF-IDF, LSTM, BERT, and regex -- for Similar Case Retrieval,\nwhereas on the legal side, we contrast the inefficiency of these binding\nprecedents with a set of hypotheses that may justify their repeated usage. We\nobserve that the deep learning models performed significantly worse in the\nspecific Similar Case Retrieval task and that the reasons for binding\nprecedents to fail in responding to repetitive demand are heterogeneous and\ncase-dependent, making it impossible to single out a specific cause.", "published": "2024-07-09 16:17:16", "link": "http://arxiv.org/abs/2407.07004v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "68T50 (Primary), 68T07 (Secondary)"], "primary_category": "cs.CL"}
{"title": "End-To-End Causal Effect Estimation from Unstructured Natural Language\n  Data", "abstract": "Knowing the effect of an intervention is critical for human decision-making,\nbut current approaches for causal effect estimation rely on manual data\ncollection and structuring, regardless of the causal assumptions. This\nincreases both the cost and time-to-completion for studies. We show how large,\ndiverse observational text data can be mined with large language models (LLMs)\nto produce inexpensive causal effect estimates under appropriate causal\nassumptions. We introduce NATURAL, a novel family of causal effect estimators\nbuilt with LLMs that operate over datasets of unstructured text. Our estimators\nuse LLM conditional distributions (over variables of interest, given the text\ndata) to assist in the computation of classical estimators of causal effect. We\novercome a number of technical challenges to realize this idea, such as\nautomating data curation and using LLMs to impute missing information. We\nprepare six (two synthetic and four real) observational datasets, paired with\ncorresponding ground truth in the form of randomized trials, which we used to\nsystematically evaluate each step of our pipeline. NATURAL estimators\ndemonstrate remarkable performance, yielding causal effect estimates that fall\nwithin 3 percentage points of their ground truth counterparts, including on\nreal-world Phase 3/4 clinical trials. Our results suggest that unstructured\ntext data is a rich source of causal effect information, and NATURAL is a first\nstep towards an automated pipeline to tap this resource.", "published": "2024-07-09 16:38:48", "link": "http://arxiv.org/abs/2407.07018v3", "categories": ["cs.LG", "cs.CL", "stat.ME"], "primary_category": "cs.LG"}
{"title": "Resolving Sentiment Discrepancy for Multimodal Sentiment Detection via\n  Semantics Completion and Decomposition", "abstract": "With the proliferation of social media posts in recent years, the need to\ndetect sentiments in multimodal (image-text) content has grown rapidly. Since\nposts are user-generated, the image and text from the same post can express\ndifferent or even contradictory sentiments, leading to potential\n\\textbf{sentiment discrepancy}. However, existing works mainly adopt a\nsingle-branch fusion structure that primarily captures the consistent sentiment\nbetween image and text. The ignorance or implicit modeling of discrepant\nsentiment results in compromised unimodal encoding and limited performances. In\nthis paper, we propose a semantics Completion and Decomposition (CoDe) network\nto resolve the above issue. In the semantics completion module, we complement\nimage and text representations with the semantics of the OCR text embedded in\nthe image, helping bridge the sentiment gap. In the semantics decomposition\nmodule, we decompose image and text representations with exclusive projection\nand contrastive learning, thereby explicitly capturing the discrepant sentiment\nbetween modalities. Finally, we fuse image and text representations by\ncross-attention and combine them with the learned discrepant sentiment for\nfinal classification. Extensive experiments conducted on four multimodal\nsentiment datasets demonstrate the superiority of CoDe against SOTA methods.", "published": "2024-07-09 16:46:58", "link": "http://arxiv.org/abs/2407.07026v1", "categories": ["cs.CV", "cs.CL", "cs.MM", "cs.SI"], "primary_category": "cs.CV"}
{"title": "Lookback Lens: Detecting and Mitigating Contextual Hallucinations in\n  Large Language Models Using Only Attention Maps", "abstract": "When asked to summarize articles or answer questions given a passage, large\nlanguage models (LLMs) can hallucinate details and respond with unsubstantiated\nanswers that are inaccurate with respect to the input context. This paper\ndescribes a simple approach for detecting such contextual hallucinations. We\nhypothesize that contextual hallucinations are related to the extent to which\nan LLM attends to information in the provided context versus its own\ngenerations. Based on this intuition, we propose a simple hallucination\ndetection model whose input features are given by the ratio of attention\nweights on the context versus newly generated tokens (for each attention head).\nWe find that a linear classifier based on these lookback ratio features is as\neffective as a richer detector that utilizes the entire hidden states of an LLM\nor a text-based entailment model. The lookback ratio-based detector -- Lookback\nLens -- is found to transfer across tasks and even models, allowing a detector\nthat is trained on a 7B model to be applied (without retraining) to a larger\n13B model. We further apply this detector to mitigate contextual\nhallucinations, and find that a simple classifier-guided decoding approach is\nable to reduce the amount of hallucination, for example by 9.6% in the XSum\nsummarization task.", "published": "2024-07-09 17:44:34", "link": "http://arxiv.org/abs/2407.07071v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FBI-LLM: Scaling Up Fully Binarized LLMs from Scratch via Autoregressive\n  Distillation", "abstract": "This work presents a Fully BInarized Large Language Model (FBI-LLM),\ndemonstrating for the first time how to train a large-scale binary language\nmodel from scratch (not the partial binary or ternary LLM like BitNet b1.58) to\nmatch the performance of its full-precision counterparts (e.g., FP16 or BF16)\nin transformer-based LLMs. It achieves this by employing an autoregressive\ndistillation (AD) loss with maintaining equivalent model dimensions (130M,\n1.3B, 7B) and training data volume as regular LLM pretraining, while delivering\ncompetitive results in terms of perplexity and task-specific effectiveness.\nIntriguingly, by analyzing the training trajectory, we find that the pretrained\nweight is not necessary for training binarized LLMs from scratch. This research\nencourages a new computational framework and may facilitate the future design\nof specialized hardware tailored for fully 1-bit LLMs. We make all models,\ncode, and training dataset fully accessible and transparent to support further\nresearch (Code: https://github.com/LiqunMa/FBI-LLM. Model:\nhttps://huggingface.co/LiqunMa/).", "published": "2024-07-09 17:59:48", "link": "http://arxiv.org/abs/2407.07093v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Remastering Divide and Remaster: A Cinematic Audio Source Separation\n  Dataset with Multilingual Support", "abstract": "Cinematic audio source separation (CASS), as a problem of extracting the\ndialogue, music, and effects stems from their mixture, is a relatively new\nsubtask of audio source separation. To date, only one publicly available\ndataset exists for CASS, that is, the Divide and Remaster (DnR) dataset, which\nis currently at version 2. While DnR v2 has been an incredibly useful resource\nfor CASS, several areas of improvement have been identified, particularly\nthrough its use in the 2023 Sound Demixing Challenge. In this work, we develop\nversion 3 of the DnR dataset, addressing issues relating to vocal content in\nnon-dialogue stems, loudness distributions, mastering process, and linguistic\ndiversity. In particular, the dialogue stem of DnR v3 includes speech content\nfrom more than 30 languages from multiple families including but not limited to\nthe Germanic, Romance, Indo-Aryan, Dravidian, Malayo-Polynesian, and Bantu\nfamilies. Benchmark results using the Bandit model indicated that training on\nmultilingual data yields significant generalizability to the model even in\nlanguages with low data availability. Even in languages with high data\navailability, the multilingual model often performs on par or better than\ndedicated models trained on monolingual CASS datasets. Dataset and model\nimplementation will be made available at\nhttps://github.com/kwatcharasupat/source-separation-landing.", "published": "2024-07-09 23:39:37", "link": "http://arxiv.org/abs/2407.07275v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Solving General Natural-Language-Description Optimization Problems with\n  Large Language Models", "abstract": "Optimization problems seek to find the best solution to an objective under a\nset of constraints, and have been widely investigated in real-world\napplications. Modeling and solving optimization problems in a specific domain\ntypically require a combination of domain knowledge, mathematical skills, and\nprogramming ability, making it difficult for general users and even domain\nprofessionals. In this paper, we propose a novel framework called OptLLM that\naugments LLMs with external solvers. Specifically, OptLLM accepts user queries\nin natural language, convert them into mathematical formulations and\nprogramming codes, and calls the solvers to calculate the results for\ndecision-making. In addition, OptLLM supports multi-round dialogues to\ngradually refine the modeling and solving of optimization problems. To\nillustrate the effectiveness of OptLLM, we provide tutorials on three typical\noptimization applications and conduct experiments on both prompt-based GPT\nmodels and a fine-tuned Qwen model using a large-scale selfdeveloped\noptimization dataset. Experimental results show that OptLLM works with various\nLLMs, and the fine-tuned model achieves an accuracy boost compared to the\npromptbased models. Some features of OptLLM framework have been available for\ntrial since June 2023 (https://opt.alibabacloud.com/chat or\nhttps://opt.aliyun.com/chat).", "published": "2024-07-09 07:11:10", "link": "http://arxiv.org/abs/2407.07924v1", "categories": ["math.OC", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "math.OC"}
{"title": "Automated Justification Production for Claim Veracity in Fact Checking:\n  A Survey on Architectures and Approaches", "abstract": "Automated Fact-Checking (AFC) is the automated verification of claim\naccuracy. AFC is crucial in discerning truth from misinformation, especially\ngiven the huge amounts of content are generated online daily. Current research\nfocuses on predicting claim veracity through metadata analysis and language\nscrutiny, with an emphasis on justifying verdicts. This paper surveys recent\nmethodologies, proposing a comprehensive taxonomy and presenting the evolution\nof research in that landscape. A comparative analysis of methodologies and\nfuture directions for improving fact-checking explainability are also\ndiscussed.", "published": "2024-07-09 01:54:13", "link": "http://arxiv.org/abs/2407.12853v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Scaling Retrieval-Based Language Models with a Trillion-Token Datastore", "abstract": "Scaling laws with respect to the amount of training data and the number of\nparameters allow us to predict the cost-benefit trade-offs of pretraining\nlanguage models (LMs) in different configurations. In this paper, we consider\nanother dimension of scaling: the amount of data available at inference time.\nSpecifically, we find that increasing the size of the datastore used by a\nretrieval-based LM monotonically improves language modeling and several\ndownstream tasks without obvious saturation, such that a smaller model\naugmented with a large datastore outperforms a larger LM-only model on\nknowledge-intensive tasks. By plotting compute-optimal scaling curves with\nvaried datastore, model, and pretraining data sizes, we show that using larger\ndatastores can significantly improve model performance for the same training\ncompute budget. We carry out our study by constructing a 1.4 trillion-token\ndatastore named MassiveDS, which is the largest and the most diverse\nopen-sourced datastore for retrieval-based LMs to date, and designing an\nefficient pipeline for studying datastore scaling in a computationally\naccessible manner. Finally, we analyze the effect of improving the retriever,\ndatastore quality filtering, and other design choices on our observed scaling\ntrends. Overall, our results show that datastore size should be considered as\nan integral part of LM efficiency and performance trade-offs. To facilitate\nfuture research, we open-source our datastore and code at\nhttps://github.com/RulinShao/retrieval-scaling.", "published": "2024-07-09 08:27:27", "link": "http://arxiv.org/abs/2407.12854v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Large Language Models can impersonate politicians and other public\n  figures", "abstract": "Modern AI technology like Large language models (LLMs) has the potential to\npollute the public information sphere with made-up content, which poses a\nsignificant threat to the cohesion of societies at large. A wide range of\nresearch has shown that LLMs are capable of generating text of impressive\nquality, including persuasive political speech, text with a pre-defined style,\nand role-specific content. But there is a crucial gap in the literature: We\nlack large-scale and systematic studies of how capable LLMs are in\nimpersonating political and societal representatives and how the general public\njudges these impersonations in terms of authenticity, relevance and coherence.\nWe present the results of a study based on a cross-section of British society\nthat shows that LLMs are able to generate responses to debate questions that\nwere part of a broadcast political debate programme in the UK. The impersonated\nresponses are judged to be more authentic and relevant than the original\nresponses given by people who were impersonated. This shows two things: (1)\nLLMs can be made to contribute meaningfully to the public political debate and\n(2) there is a dire need to inform the general public of the potential harm\nthis can have on society.", "published": "2024-07-09 11:16:19", "link": "http://arxiv.org/abs/2407.12855v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AI AI Bias: Large Language Models Favor Their Own Generated Content", "abstract": "Are large language models (LLMs) biased towards text generated by LLMs over\ntext authored by humans, leading to possible anti-human bias? Utilizing a\nclassical experimental design inspired by employment discrimination studies, we\ntested widely-used LLMs, including GPT-3.5 and GPT4, in binary-choice\nscenarios. These involved LLM-based agents selecting between products and\nacademic papers described either by humans or LLMs under identical conditions.\nOur results show a consistent tendency for LLM-based AIs to prefer\nLLM-generated content. This suggests the possibility of AI systems implicitly\ndiscriminating against humans, giving AI agents an unfair advantage.", "published": "2024-07-09 13:15:14", "link": "http://arxiv.org/abs/2407.12856v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Automated Peer Reviewing in Paper SEA: Standardization, Evaluation, and\n  Analysis", "abstract": "In recent years, the rapid increase in scientific papers has overwhelmed\ntraditional review mechanisms, resulting in varying quality of publications.\nAlthough existing methods have explored the capabilities of Large Language\nModels (LLMs) for automated scientific reviewing, their generated contents are\noften generic or partial. To address the issues above, we introduce an\nautomated paper reviewing framework SEA. It comprises of three modules:\nStandardization, Evaluation, and Analysis, which are represented by models\nSEA-S, SEA-E, and SEA-A, respectively. Initially, SEA-S distills data\nstandardization capabilities of GPT-4 for integrating multiple reviews for a\npaper. Then, SEA-E utilizes standardized data for fine-tuning, enabling it to\ngenerate constructive reviews. Finally, SEA-A introduces a new evaluation\nmetric called mismatch score to assess the consistency between paper contents\nand reviews. Moreover, we design a self-correction strategy to enhance the\nconsistency. Extensive experimental results on datasets collected from eight\nvenues show that SEA can generate valuable insights for authors to improve\ntheir papers.", "published": "2024-07-09 15:06:14", "link": "http://arxiv.org/abs/2407.12857v2", "categories": ["cs.CL", "cs.DL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "LETS-C: Leveraging Language Embedding for Time Series Classification", "abstract": "Recent advancements in language modeling have shown promising results when\napplied to time series data. In particular, fine-tuning pre-trained large\nlanguage models (LLMs) for time series classification tasks has achieved\nstate-of-the-art (SOTA) performance on standard benchmarks. However, these\nLLM-based models have a significant drawback due to the large model size, with\nthe number of trainable parameters in the millions. In this paper, we propose\nan alternative approach to leveraging the success of language modeling in the\ntime series domain. Instead of fine-tuning LLMs, we utilize a language\nembedding model to embed time series and then pair the embeddings with a simple\nclassification head composed of convolutional neural networks (CNN) and\nmultilayer perceptron (MLP). We conducted extensive experiments on\nwell-established time series classification benchmark datasets. We demonstrated\nLETS-C not only outperforms the current SOTA in classification accuracy but\nalso offers a lightweight solution, using only 14.5% of the trainable\nparameters on average compared to the SOTA model. Our findings suggest that\nleveraging language encoders to embed time series data, combined with a simple\nyet effective classification head, offers a promising direction for achieving\nhigh-performance time series classification while maintaining a lightweight\nmodel architecture.", "published": "2024-07-09 04:07:57", "link": "http://arxiv.org/abs/2407.06533v1", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.CL", "stat.ME"], "primary_category": "cs.LG"}
{"title": "Audio-Language Datasets of Scenes and Events: A Survey", "abstract": "Audio-language models (ALMs) generate linguistic descriptions of\nsound-producing events and scenes. Advances in dataset creation and\ncomputational power have led to significant progress in this domain. This paper\nsurveys 69 datasets used to train ALMs, covering research up to September 2024\n(https://github.com/GLJS/audio-datasets). It provides a comprehensive analysis\nof datasets origins, audio and linguistic characteristics, and use cases. Key\nsources include YouTube-based datasets like AudioSet with over two million\nsamples, and community platforms like Freesound with over 1 million samples.\nThrough principal component analysis of audio and text embeddings, the survey\nevaluates the acoustic and linguistic variability across datasets. It also\nanalyzes data leakage through CLAP embeddings, and examines sound category\ndistributions to identify imbalances. Finally, the survey identifies key\nchallenges in developing large, diverse datasets to enhance ALM performance,\nincluding dataset overlap, biases, accessibility barriers, and the predominance\nof English-language content, while highlighting opportunities for improvement.", "published": "2024-07-09 15:23:35", "link": "http://arxiv.org/abs/2407.06947v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improving Speech Enhancement by Integrating Inter-Channel and Band\n  Features with Dual-branch Conformer", "abstract": "Recent speech enhancement methods based on convolutional neural networks\n(CNNs) and transformer have been demonstrated to efficaciously capture\ntime-frequency (T-F) information on spectrogram. However, the correlation of\neach channels of speech features is failed to explore. Theoretically, each\nchannel map of speech features obtained by different convolution kernels\ncontains information with different scales demonstrating strong correlations.\nTo fill this gap, we propose a novel dual-branch architecture named\nchannel-aware dual-branch conformer (CADB-Conformer), which effectively\nexplores the long range time and frequency correlations among different\nchannels, respectively, to extract channel relation aware time-frequency\ninformation. Ablation studies conducted on DNS-Challenge 2020 dataset\ndemonstrate the importance of channel feature leveraging while showing the\nsignificance of channel relation aware T-F information for speech enhancement.\nExtensive experiments also show that the proposed model achieves superior\nperformance than recent methods with an attractive computational costs.", "published": "2024-07-09 03:32:00", "link": "http://arxiv.org/abs/2407.06524v3", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Gaunt coefficients for complex and real spherical harmonics with\n  applications to spherical array processing and Ambisonics", "abstract": "Acoustical signal processing of directional representations of sound fields,\nincluding source, receiver, and scatterer transfer functions, are often\nexpressed and modeled in the spherical harmonic domain (SHD). Certain such\nmodeling operations, or applications of those models, involve multiplications\nof those directional quantities, which can also be expressed conveniently in\nthe SHD through coupling coefficients known as Gaunt coefficients. Since the\ndefinition and notation of Gaunt coefficients varies across acoustical\npublications, this work defines them based on established conventions of\ncomplex and real spherical harmonics (SHs) along with a convenient matrix form\nfor spherical multiplication of directionally band-limited spherical functions.\nAdditionally, the report provides a derivation of the Gaunt coefficients for\nreal SHs, which has been missing from the literature and can be used directly\nin spatial audio frameworks such as Ambisonics. Matlab code is provided that\ncan compute all coefficients up to user specified SH orders. Finally, a number\nof relevant acoustical processing examples from the literature are presented,\nfollowing the matrix formalism of coefficients introduced in the report.", "published": "2024-07-09 13:31:00", "link": "http://arxiv.org/abs/2407.06847v1", "categories": ["eess.AS", "cs.GR", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "RespEar: Earable-Based Robust Respiratory Rate Monitoring", "abstract": "Respiratory rate (RR) monitoring is integral to understanding physical and\nmental health and tracking fitness. Existing studies have demonstrated the\nfeasibility of RR monitoring under specific user conditions (e.g., while\nremaining still, or while breathing heavily). Yet, performing accurate,\ncontinuous and non-obtrusive RR monitoring across diverse daily routines and\nactivities remains challenging. In this work, we present RespEar, an\nearable-based system for robust RR monitoring. By leveraging the unique\nproperties of in-ear microphones in earbuds, RespEar enables the use of\nRespiratory Sinus Arrhythmia (RSA) and Locomotor Respiratory Coupling (LRC),\nphysiological couplings between cardiovascular activity, gait and respiration,\nto indirectly determine RR. This effectively addresses the challenges posed by\nthe almost imperceptible breathing signals under daily activities. We further\npropose a suite of meticulously crafted signal processing schemes to improve RR\nestimation accuracy and robustness. With data collected from 18 subjects over 8\nactivities, RespEar measures RR with a mean absolute error (MAE) of 1.48\nbreaths per minutes (BPM) and a mean absolute percent error (MAPE) of 9.12% in\nsedentary conditions, and a MAE of 2.28 BPM and a MAPE of 11.04% in active\nconditions, respectively, which is unprecedented for a method capable of\ngeneralizing across conditions with a single modality.", "published": "2024-07-09 14:32:28", "link": "http://arxiv.org/abs/2407.06901v1", "categories": ["cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
{"title": "A Framework for Multimodal Medical Image Interaction", "abstract": "Medical doctors rely on images of the human anatomy, such as magnetic\nresonance imaging (MRI), to localize regions of interest in the patient during\ndiagnosis and treatment. Despite advances in medical imaging technology, the\ninformation conveyance remains unimodal. This visual representation fails to\ncapture the complexity of the real, multisensory interaction with human tissue.\nHowever, perceiving multimodal information about the patient's anatomy and\ndisease in real-time is critical for the success of medical procedures and\npatient outcome. We introduce a Multimodal Medical Image Interaction (MMII)\nframework to allow medical experts a dynamic, audiovisual interaction with\nhuman tissue in three-dimensional space. In a virtual reality environment, the\nuser receives physically informed audiovisual feedback to improve the spatial\nperception of anatomical structures. MMII uses a model-based sonification\napproach to generate sounds derived from the geometry and physical properties\nof tissue, thereby eliminating the need for hand-crafted sound design. Two user\nstudies involving 34 general and nine clinical experts were conducted to\nevaluate the proposed interaction framework's learnability, usability, and\naccuracy. Our results showed excellent learnability of audiovisual\ncorrespondence as the rate of correct associations significantly improved (p <\n0.001) over the course of the study. MMII resulted in superior brain tumor\nlocalization accuracy (p < 0.05) compared to conventional medical image\ninteraction. Our findings substantiate the potential of this novel framework to\nenhance interaction with medical images, for example, during surgical\nprocedures where immediate and precise feedback is needed.", "published": "2024-07-09 16:33:51", "link": "http://arxiv.org/abs/2407.07015v1", "categories": ["cs.HC", "cs.MM", "cs.SD", "eess.AS", "H.5.2; H.5.5; H.5.1; J.3"], "primary_category": "cs.HC"}
{"title": "Speech After Gender: A Trans-Feminine Perspective on Next Steps for\n  Speech Science and Technology", "abstract": "As experts in voice modification, trans-feminine gender-affirming voice\nteachers have unique perspectives on voice that confound current understandings\nof speaker identity. To demonstrate this, we present the Versatile Voice\nDataset (VVD), a collection of three speakers modifying their voices along\ngendered axes. The VVD illustrates that current approaches in speaker modeling,\nbased on categorical notions of gender and a static understanding of vocal\ntexture, fail to account for the flexibility of the vocal tract. Utilizing\npublicly-available speaker embeddings, we demonstrate that gender\nclassification systems are highly sensitive to voice modification, and speaker\nverification systems fail to identify voices as coming from the same speaker as\nvoice modification becomes more drastic. As one path towards moving beyond\ncategorical and static notions of speaker identity, we propose modeling\nindividual qualities of vocal texture such as pitch, resonance, and weight.", "published": "2024-07-09 21:19:49", "link": "http://arxiv.org/abs/2407.07235v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Knowledge boosting during low-latency inference", "abstract": "Models for low-latency, streaming applications could benefit from the\nknowledge capacity of larger models, but edge devices cannot run these models\ndue to resource constraints. A possible solution is to transfer hints during\ninference from a large model running remotely to a small model running\non-device. However, this incurs a communication delay that breaks real-time\nrequirements and does not guarantee that both models will operate on the same\ndata at the same time. We propose knowledge boosting, a novel technique that\nallows a large model to operate on time-delayed input during inference, while\nstill boosting small model performance. Using a streaming neural network that\nprocesses 8 ms chunks, we evaluate different speech separation and enhancement\ntasks with communication delays of up to six chunks or 48 ms. Our results show\nlarger gains where the performance gap between the small and large models is\nwide, demonstrating a promising method for large-small model collaboration for\nlow-latency applications. Code, dataset, and audio samples available at\nhttps://knowledgeboosting.cs.washington.edu/.", "published": "2024-07-09 22:04:23", "link": "http://arxiv.org/abs/2407.11055v3", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
