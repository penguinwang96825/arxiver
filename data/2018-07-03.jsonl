{"title": "COTA: Improving the Speed and Accuracy of Customer Support through Ranking and Deep Networks", "abstract": "For a company looking to provide delightful user experiences, it is of paramount importance to take care of any customer issues. This paper proposes COTA, a system to improve speed and reliability of customer support for end users through automated ticket classification and answers selection for support representatives. Two machine learning and natural language processing techniques are demonstrated: one relying on feature engineering (COTA v1) and the other exploiting raw signals through deep learning architectures (COTA v2). COTA v1 employs a new approach that converts the multi-classification task into a ranking problem, demonstrating significantly better performance in the case of thousands of classes. For COTA v2, we propose an Encoder-Combiner-Decoder, a novel deep learning architecture that allows for heterogeneous input and output feature types and injection of prior knowledge through network architecture choices. This paper compares these models and their variants on the task of ticket classification and answer selection, showing model COTA v2 outperforms COTA v1, and analyzes their inner workings and shortcomings. Finally, an A/B test is conducted in a production setting validating the real-world impact of COTA in reducing issue resolution time by 10 percent without reducing customer satisfaction.", "published": "2018-07-03 18:25:44", "link": "http://arxiv.org/abs/1807.01337v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Providing Explanations for Recommendations in Reciprocal Environments", "abstract": "Automated platforms which support users in finding a mutually beneficial match, such as online dating and job recruitment sites, are becoming increasingly popular. These platforms often include recommender systems that assist users in finding a suitable match. While recommender systems which provide explanations for their recommendations have shown many benefits, explanation methods have yet to be adapted and tested in recommending suitable matches. In this paper, we introduce and extensively evaluate the use of \"reciprocal explanations\" -- explanations which provide reasoning as to why both parties are expected to benefit from the match. Through an extensive empirical evaluation, in both simulated and real-world dating platforms with 287 human participants, we find that when the acceptance of a recommendation involves a significant cost (e.g., monetary or emotional), reciprocal explanations outperform standard explanation methods which consider the recommendation receiver alone. However, contrary to what one may expect, when the cost of accepting a recommendation is negligible, reciprocal explanations are shown to be less effective than the traditional explanation methods.", "published": "2018-07-03 15:10:01", "link": "http://arxiv.org/abs/1807.01227v1", "categories": ["cs.AI", "cs.HC", "cs.IR"], "primary_category": "cs.AI"}
{"title": "Stochastic Layer-Wise Precision in Deep Neural Networks", "abstract": "Low precision weights, activations, and gradients have been proposed as a way to improve the computational efficiency and memory footprint of deep neural networks. Recently, low precision networks have even shown to be more robust to adversarial attacks. However, typical implementations of low precision DNNs use uniform precision across all layers of the network. In this work, we explore whether a heterogeneous allocation of precision across a network leads to improved performance, and introduce a learning scheme where a DNN stochastically explores multiple precision configurations through learning. This permits a network to learn an optimal precision configuration. We show on convolutional neural networks trained on MNIST and ILSVRC12 that even though these nets learn a uniform or near-uniform allocation strategy respectively, stochastic precision leads to a favourable regularization effect improving generalization.", "published": "2018-07-03 01:11:14", "link": "http://arxiv.org/abs/1807.00942v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "One-Class Kernel Spectral Regression", "abstract": "The paper introduces a new efficient nonlinear one-class classifier formulated as the Rayleigh quotient criterion optimisation. The method, operating in a reproducing kernel Hilbert space, minimises the scatter of target distribution along an optimal projection direction while at the same time keeping projections of positive observations distant from the mean of the negative class. We provide a graph embedding view of the problem which can then be solved efficiently using the spectral regression approach. In this sense, unlike previous similar methods which often require costly eigen-computations of dense matrices, the proposed approach casts the problem under consideration into a regression framework which is computationally more efficient. In particular, it is shown that the dominant complexity of the proposed method is the complexity of computing the kernel matrix. Additional appealing characteristics of the proposed one-class classifier are: 1-the ability to be trained in an incremental fashion (allowing for application in streaming data scenarios while also reducing the computational complexity in a non-streaming operation mode); 2-being unsupervised, but providing the option for refining the solution using negative training examples, when available; And last but not the least, 3-the use of the kernel trick which facilitates a nonlinear mapping of the data into a high-dimensional feature space to seek better solutions.", "published": "2018-07-03 11:19:17", "link": "http://arxiv.org/abs/1807.01085v6", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "When Gaussian Process Meets Big Data: A Review of Scalable GPs", "abstract": "The vast quantity of information brought by big data as well as the evolving computer hardware encourages success stories in the machine learning community. In the meanwhile, it poses challenges for the Gaussian process (GP) regression, a well-known non-parametric and interpretable Bayesian model, which suffers from cubic complexity to data size. To improve the scalability while retaining desirable prediction quality, a variety of scalable GPs have been presented. But they have not yet been comprehensively reviewed and analyzed in order to be well understood by both academia and industry. The review of scalable GPs in the GP community is timely and important due to the explosion of data size. To this end, this paper is devoted to the review on state-of-the-art scalable GPs involving two main categories: global approximations which distillate the entire data and local approximations which divide the data for subspace learning. Particularly, for global approximations, we mainly focus on sparse approximations comprising prior approximations which modify the prior but perform exact inference, posterior approximations which retain exact prior but perform approximate inference, and structured sparse approximations which exploit specific structures in kernel matrix; for local approximations, we highlight the mixture/product of experts that conducts model averaging from multiple local experts to boost predictions. To present a complete review, recent advances for improving the scalability and capability of scalable GPs are reviewed. Finally, the extensions and open issues regarding the implementation of scalable GPs in various scenarios are reviewed and discussed to inspire novel ideas for future research avenues.", "published": "2018-07-03 10:19:25", "link": "http://arxiv.org/abs/1807.01065v2", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Proceedings of the 2018 ICML Workshop on Human Interpretability in Machine Learning (WHI 2018)", "abstract": "This is the Proceedings of the 2018 ICML Workshop on Human Interpretability in Machine Learning (WHI 2018), which was held in Stockholm, Sweden, July 14, 2018. Invited speakers were Barbara Engelhardt, Cynthia Rudin, Fernanda Vi\u00e9gas, and Martin Wattenberg.", "published": "2018-07-03 17:49:14", "link": "http://arxiv.org/abs/1807.01308v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Breast Cancer Diagnosis via Classification Algorithms", "abstract": "In this paper, we analyze the Wisconsin Diagnostic Breast Cancer Data using Machine Learning classification techniques, such as the SVM, Bayesian Logistic Regression (Variational Approximation), and K-Nearest-Neighbors. We describe each model, and compare their performance through different measures. We conclude that SVM has the best performance among all other classifiers, while it competes closely with the Bayesian Logistic Regression that is ranked second best method for this dataset.", "published": "2018-07-03 18:13:55", "link": "http://arxiv.org/abs/1807.01334v1", "categories": ["stat.ML", "cs.LG", "stat.AP", "stat.CO"], "primary_category": "stat.ML"}
{"title": "Does Massive MIMO Fail in Ricean Channels?", "abstract": "Massive multiple-input multiple-output (MIMO) is now making its way to the standardization exercise of future 5G networks. Yet, there are still fundamental questions pertaining to the robustness of massive MIMO against physically detrimental propagation conditions. On these grounds, we identify scenarios under which massive MIMO can potentially fail in Ricean channels, and characterize them physically, as well as, mathematically. Our analysis extends and generalizes a stream of recent papers on this topic and articulates emphatically that such harmful scenarios in Ricean fading conditions are unlikely and can be compensated using any standard scheduling scheme. This implies that massive MIMO is intrinsically effective at combating interuser interference and, if needed, can avail of the base-station scheduler for further robustness.", "published": "2018-07-03 10:35:41", "link": "http://arxiv.org/abs/1807.01071v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
