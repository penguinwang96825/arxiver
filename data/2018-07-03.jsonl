{"title": "Topic Discovery in Massive Text Corpora Based on Min-Hashing", "abstract": "The task of discovering topics in text corpora has been dominated by Latent\nDirichlet Allocation and other Topic Models for over a decade. In order to\napply these approaches to massive text corpora, the vocabulary needs to be\nreduced considerably and large computer clusters and/or GPUs are typically\nrequired. Moreover, the number of topics must be provided beforehand but this\ndepends on the corpus characteristics and it is often difficult to estimate,\nespecially for massive text corpora. Unfortunately, both topic quality and time\ncomplexity are sensitive to this choice. This paper describes an alternative\napproach to discover topics based on Min-Hashing, which can handle massive text\ncorpora and large vocabularies using modest computer hardware and does not\nrequire to fix the number of topics in advance. The basic idea is to generate\nmultiple random partitions of the corpus vocabulary to find sets of highly\nco-occurring words, which are then clustered to produce the final topics. In\ncontrast to probabilistic topic models where topics are distributions over the\ncomplete vocabulary, the topics discovered by the proposed approach are sets of\nhighly co-occurring words. Interestingly, these topics underlie various\nthematics with different levels of granularity. An extensive qualitative and\nquantitative evaluation using the 20 Newsgroups (18K), Reuters (800K), Spanish\nWikipedia (1M), and English Wikipedia (5M) corpora shows that the proposed\napproach is able to consistently discover meaningful and coherent topics.\nRemarkably, the time complexity of the proposed approach is linear with respect\nto corpus and vocabulary size; a non-parallel implementation was able to\ndiscover topics from the entire English edition of Wikipedia with over 5\nmillion documents and 1 million words in less than 7 hours.", "published": "2018-07-03 00:52:50", "link": "http://arxiv.org/abs/1807.00938v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Intent Generation for Goal-Oriented Dialogue Systems based on Schema.org\n  Annotations", "abstract": "Goal-oriented dialogue systems typically communicate with a backend (e.g.\ndatabase, Web API) to complete certain tasks to reach a goal. The intents that\na dialogue system can recognize are mostly included to the system by the\ndeveloper statically. For an open dialogue system that can work on more than a\nsmall set of well curated data and APIs, this manual intent creation will not\nscalable. In this paper, we introduce a straightforward methodology for intent\ncreation based on semantic annotation of data and services on the web. With\nthis method, the Natural Language Understanding (NLU) module of a goal-oriented\ndialogue system can adapt to newly introduced APIs without requiring heavy\ndeveloper involvement. We were able to extract intents and necessary slots to\nbe filled from schema.org annotations. We were also able to create a set of\ninitial training sentences for classifying user utterances into the generated\nintents. We demonstrate our approach on the NLU module of a state-of-the art\ndialogue system development framework.", "published": "2018-07-03 17:15:49", "link": "http://arxiv.org/abs/1807.01292v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Simpler but More Accurate Semantic Dependency Parsing", "abstract": "While syntactic dependency annotations concentrate on the surface or\nfunctional structure of a sentence, semantic dependency annotations aim to\ncapture between-word relationships that are more closely related to the meaning\nof a sentence, using graph-structured representations. We extend the LSTM-based\nsyntactic parser of Dozat and Manning (2017) to train on and generate these\ngraph structures. The resulting system on its own achieves state-of-the-art\nperformance, beating the previous, substantially more complex state-of-the-art\nsystem by 0.6% labeled F1. Adding linguistically richer input representations\npushes the margin even higher, allowing us to beat it by 1.9% labeled F1.", "published": "2018-07-03 23:29:49", "link": "http://arxiv.org/abs/1807.01396v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improved training of neural trans-dimensional random field language\n  models with dynamic noise-contrastive estimation", "abstract": "A new whole-sentence language model - neural trans-dimensional random field\nlanguage model (neural TRF LM), where sentences are modeled as a collection of\nrandom fields, and the potential function is defined by a neural network, has\nbeen introduced and successfully trained by noise-contrastive estimation (NCE).\nIn this paper, we extend NCE and propose dynamic noise-contrastive estimation\n(DNCE) to solve the two problems observed in NCE training. First, a dynamic\nnoise distribution is introduced and trained simultaneously to converge to the\ndata distribution. This helps to significantly cut down the noise sample number\nused in NCE and reduce the training cost. Second, DNCE discriminates between\nsentences generated from the noise distribution and sentences generated from\nthe interpolation of the data distribution and the noise distribution. This\nalleviates the overfitting problem caused by the sparseness of the training\nset. With DNCE, we can successfully and efficiently train neural TRF LMs on\nlarge corpus (about 0.8 billion words) with large vocabulary (about 568 K\nwords). Neural TRF LMs perform as good as LSTM LMs with less parameters and\nbeing 5x~114x faster in rescoring sentences. Interpolating neural TRF LMs with\nLSTM LMs and n-gram LMs can further reduce the error rates.", "published": "2018-07-03 06:36:48", "link": "http://arxiv.org/abs/1807.00993v1", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Getting the subtext without the text: Scalable multimodal sentiment\n  classification from visual and acoustic modalities", "abstract": "In the last decade, video blogs (vlogs) have become an extremely popular\nmethod through which people express sentiment. The ubiquitousness of these\nvideos has increased the importance of multimodal fusion models, which\nincorporate video and audio features with traditional text features for\nautomatic sentiment detection. Multimodal fusion offers a unique opportunity to\nbuild models that learn from the full depth of expression available to human\nviewers. In the detection of sentiment in these videos, acoustic and video\nfeatures provide clarity to otherwise ambiguous transcripts. In this paper, we\npresent a multimodal fusion model that exclusively uses high-level video and\naudio features to analyze spoken sentences for sentiment. We discard\ntraditional transcription features in order to minimize human intervention and\nto maximize the deployability of our model on at-scale real-world data. We\nselect high-level features for our model that have been successful in nonaffect\ndomains in order to test their generalizability in the sentiment detection\ndomain. We train and test our model on the newly released CMU Multimodal\nOpinion Sentiment and Emotion Intensity (CMUMOSEI) dataset, obtaining an F1\nscore of 0.8049 on the validation set and an F1 score of 0.6325 on the held-out\nchallenge test set.", "published": "2018-07-03 12:38:11", "link": "http://arxiv.org/abs/1807.01122v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Reaching Human-level Performance in Automatic Grammatical Error\n  Correction: An Empirical Study", "abstract": "Neural sequence-to-sequence (seq2seq) approaches have proven to be successful\nin grammatical error correction (GEC). Based on the seq2seq framework, we\npropose a novel fluency boost learning and inference mechanism. Fluency\nboosting learning generates diverse error-corrected sentence pairs during\ntraining, enabling the error correction model to learn how to improve a\nsentence's fluency from more instances, while fluency boosting inference allows\nthe model to correct a sentence incrementally with multiple inference steps.\nCombining fluency boost learning and inference with convolutional seq2seq\nmodels, our approach achieves the state-of-the-art performance: 75.72 (F_{0.5})\non CoNLL-2014 10 annotation dataset and 62.42 (GLEU) on JFLEG test set\nrespectively, becoming the first GEC system that reaches human-level\nperformance (72.58 for CoNLL and 62.37 for JFLEG) on both of the benchmarks.", "published": "2018-07-03 16:37:05", "link": "http://arxiv.org/abs/1807.01270v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Patient representation learning and interpretable evaluation using\n  clinical notes", "abstract": "We have three contributions in this work: 1. We explore the utility of a\nstacked denoising autoencoder and a paragraph vector model to learn\ntask-independent dense patient representations directly from clinical notes. To\nanalyze if these representations are transferable across tasks, we evaluate\nthem in multiple supervised setups to predict patient mortality, primary\ndiagnostic and procedural category, and gender. We compare their performance\nwith sparse representations obtained from a bag-of-words model. We observe that\nthe learned generalized representations significantly outperform the sparse\nrepresentations when we have few positive instances to learn from, and there is\nan absence of strong lexical features. 2. We compare the model performance of\nthe feature set constructed from a bag of words to that obtained from medical\nconcepts. In the latter case, concepts represent problems, treatments, and\ntests. We find that concept identification does not improve the classification\nperformance. 3. We propose novel techniques to facilitate model\ninterpretability. To understand and interpret the representations, we explore\nthe best encoded features within the patient representations obtained from the\nautoencoder model. Further, we calculate feature sensitivity across two\nnetworks to identify the most significant input features for different\nclassification tasks when we use these pretrained representations as the\nsupervised input. We successfully extract the most influential features for the\npipeline using this technique.", "published": "2018-07-03 23:20:49", "link": "http://arxiv.org/abs/1807.01395v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "COTA: Improving the Speed and Accuracy of Customer Support through\n  Ranking and Deep Networks", "abstract": "For a company looking to provide delightful user experiences, it is of\nparamount importance to take care of any customer issues. This paper proposes\nCOTA, a system to improve speed and reliability of customer support for end\nusers through automated ticket classification and answers selection for support\nrepresentatives. Two machine learning and natural language processing\ntechniques are demonstrated: one relying on feature engineering (COTA v1) and\nthe other exploiting raw signals through deep learning architectures (COTA v2).\nCOTA v1 employs a new approach that converts the multi-classification task into\na ranking problem, demonstrating significantly better performance in the case\nof thousands of classes. For COTA v2, we propose an Encoder-Combiner-Decoder, a\nnovel deep learning architecture that allows for heterogeneous input and output\nfeature types and injection of prior knowledge through network architecture\nchoices. This paper compares these models and their variants on the task of\nticket classification and answer selection, showing model COTA v2 outperforms\nCOTA v1, and analyzes their inner workings and shortcomings. Finally, an A/B\ntest is conducted in a production setting validating the real-world impact of\nCOTA in reducing issue resolution time by 10 percent without reducing customer\nsatisfaction.", "published": "2018-07-03 18:25:44", "link": "http://arxiv.org/abs/1807.01337v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "A Computational Study of the Role of Tonal Tension in Expressive Piano\n  Performance", "abstract": "Expressive variations of tempo and dynamics are an important aspect of music\nperformances, involving a variety of underlying factors. Previous work has\nshowed a relation between such expressive variations (in particular expressive\ntempo) and perceptual characteristics derived from the musical score, such as\nmusical expectations, and perceived tension. In this work we use a\ncomputational approach to study the role of three measures of tonal tension\nproposed by Herremans and Chew (2016) in the prediction of expressive\nperformances of classical piano music. These features capture tonal\nrelationships of the music represented in Chew's spiral array model, a three\ndimensional representation of pitch classes, chords and keys constructed in\nsuch a way that spatial proximity represents close tonal relationships. We use\nnon-linear sequential models (recurrent neural networks) to assess the\ncontribution of these features to the prediction of expressive dynamics and\nexpressive tempo using a dataset of Mozart piano sonatas performed by a\nprofessional concert pianist. Experiments of models trained with and without\ntonal tension features show that tonal tension helps predict change of tempo\nand dynamics more than absolute tempo and dynamics values. Furthermore, the\nimprovement is stronger for dynamics than for tempo.", "published": "2018-07-03 10:59:23", "link": "http://arxiv.org/abs/1807.01080v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Study of Material Sonification in Touchscreen Devices", "abstract": "Even in the digital age, designers largely rely on physical material samples\nto illustrate their products, as existing visual representations fail to\nsufficiently reproduce the look and feel of real world materials. Here, we\ninvestigate the use of interactive material sonification as an additional\nsensory modality for communicating well-established material qualities like\nsoftness, pleasantness or value. We developed a custom application for\ntouchscreen devices that receives tactile input and translate it into material\nrubbing sound using granular synthesis. We used this system to perform a\npsychophysical study, in which the ability of the user to rate subjective\nmaterial qualities is evaluated, with the actual material samples serving as\nreference stimulus. Our experimental results indicate that the considered audio\ncues do not significantly contribute to the perception of material qualities\nbut are able to increase the level of immersion when interacting with digital\nsamples.", "published": "2018-07-03 12:05:31", "link": "http://arxiv.org/abs/1807.01106v3", "categories": ["cs.HC", "cs.MM", "cs.SD", "eess.AS", "H.5.2"], "primary_category": "cs.HC"}
{"title": "Weakly Supervised Deep Recurrent Neural Networks for Basic Dance Step\n  Generation", "abstract": "Synthesizing human's movements such as dancing is a flourishing research\nfield which has several applications in computer graphics. Recent studies have\ndemonstrated the advantages of deep neural networks (DNNs) for achieving\nremarkable performance in motion and music tasks with little effort for feature\npre-processing. However, applying DNNs for generating dance to a piece of music\nis nevertheless challenging, because of 1) DNNs need to generate large\nsequences while mapping the music input, 2) the DNN needs to constraint the\nmotion beat to the music, and 3) DNNs require a considerable amount of\nhand-crafted data. In this study, we propose a weakly supervised deep recurrent\nmethod for real-time basic dance generation with audio power spectrum as input.\nThe proposed model employs convolutional layers and a multilayered Long\nShort-Term memory (LSTM) to process the audio input. Then, another deep LSTM\nlayer decodes the target dance sequence. Notably, this end-to-end approach has\n1) an auto-conditioned decode configuration that reduces accumulation of\nfeedback error of large dance sequence, 2) uses a contrastive cost function to\nregulate the mapping between the music and motion beat, and 3) trains with weak\nlabels generated from the motion beat, reducing the amount of hand-crafted\ndata. We evaluate the proposed network based on i) the similarities between\ngenerated and the baseline dancer motion with a cross entropy measure for large\ndance sequences, and ii) accurate timing between the music and motion beat with\nan F-measure. Experimental results revealed that, after training using a small\ndataset, the model generates basic dance steps with low cross entropy and\nmaintains an F-measure score similar to that of a baseline dancer.", "published": "2018-07-03 12:47:15", "link": "http://arxiv.org/abs/1807.01126v3", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
