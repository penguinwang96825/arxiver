{"title": "Document Flattening: Beyond Concatenating Context for Document-Level\n  Neural Machine Translation", "abstract": "Existing work in document-level neural machine translation commonly\nconcatenates several consecutive sentences as a pseudo-document, and then\nlearns inter-sentential dependencies. This strategy limits the model's ability\nto leverage information from distant context. We overcome this limitation with\na novel Document Flattening (DocFlat) technique that integrates Flat-Batch\nAttention (FBA) and Neural Context Gate (NCG) into Transformer model to utilize\ninformation beyond the pseudo-document boundaries. FBA allows the model to\nattend to all the positions in the batch and learns the relationships between\npositions explicitly and NCG identifies the useful information from the distant\ncontext. We conduct comprehensive experiments and analyses on three benchmark\ndatasets for English-German translation, and validate the effectiveness of two\nvariants of DocFlat. Empirical results show that our approach outperforms\nstrong baselines with statistical significance on BLEU, COMET and accuracy on\nthe contrastive test set. The analyses highlight that DocFlat is highly\neffective in capturing the long-range information.", "published": "2023-02-16 04:38:34", "link": "http://arxiv.org/abs/2302.08079v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do We Still Need Clinical Language Models?", "abstract": "Although recent advances in scaling large language models (LLMs) have\nresulted in improvements on many NLP tasks, it remains unclear whether these\nmodels trained primarily with general web text are the right tool in highly\nspecialized, safety critical domains such as clinical text. Recent results have\nsuggested that LLMs encode a surprising amount of medical knowledge. This\nraises an important question regarding the utility of smaller domain-specific\nlanguage models. With the success of general-domain LLMs, is there still a need\nfor specialized clinical models? To investigate this question, we conduct an\nextensive empirical analysis of 12 language models, ranging from 220M to 175B\nparameters, measuring their performance on 3 different clinical tasks that test\ntheir ability to parse and reason over electronic health records. As part of\nour experiments, we train T5-Base and T5-Large models from scratch on clinical\nnotes from MIMIC III and IV to directly investigate the efficiency of clinical\ntokens. We show that relatively small specialized clinical models substantially\noutperform all in-context learning approaches, even when finetuned on limited\nannotated data. Further, we find that pretraining on clinical tokens allows for\nsmaller, more parameter-efficient models that either match or outperform much\nlarger language models trained on general text. We release the code and the\nmodels used under the PhysioNet Credentialed Health Data license and data use\nagreement.", "published": "2023-02-16 05:08:34", "link": "http://arxiv.org/abs/2302.08091v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CluCDD:Contrastive Dialogue Disentanglement via Clustering", "abstract": "A huge number of multi-participant dialogues happen online every day, which\nleads to difficulty in understanding the nature of dialogue dynamics for both\nhumans and machines. Dialogue disentanglement aims at separating an entangled\ndialogue into detached sessions, thus increasing the readability of long\ndisordered dialogue. Previous studies mainly focus on message-pair\nclassification and clustering in two-step methods, which cannot guarantee the\nwhole clustering performance in a dialogue. To address this challenge, we\npropose a simple yet effective model named CluCDD, which aggregates utterances\nby contrastive learning. More specifically, our model pulls utterances in the\nsame session together and pushes away utterances in different ones. Then a\nclustering method is adopted to generate predicted clustering labels.\nComprehensive experiments conducted on the Movie Dialogue dataset and IRC\ndataset demonstrate that our model achieves a new state-of-the-art result.", "published": "2023-02-16 08:47:51", "link": "http://arxiv.org/abs/2302.08146v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reanalyzing L2 Preposition Learning with Bayesian Mixed Effects and a\n  Pretrained Language Model", "abstract": "We use both Bayesian and neural models to dissect a data set of Chinese\nlearners' pre- and post-interventional responses to two tests measuring their\nunderstanding of English prepositions. The results mostly replicate previous\nfindings from frequentist analyses and newly reveal crucial interactions\nbetween student ability, task type, and stimulus sentence. Given the sparsity\nof the data as well as high diversity among learners, the Bayesian method\nproves most useful; but we also see potential in using language model\nprobabilities as predictors of grammaticality and learnability.", "published": "2023-02-16 08:54:05", "link": "http://arxiv.org/abs/2302.08150v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dialogue State Distillation Network with Inter-slot Contrastive Learning\n  for Dialogue State Tracking", "abstract": "In task-oriented dialogue systems, Dialogue State Tracking (DST) aims to\nextract users' intentions from the dialogue history. Currently, most existing\napproaches suffer from error propagation and are unable to dynamically select\nrelevant information when utilizing previous dialogue states. Moreover, the\nrelations between the updates of different slots provide vital clues for DST.\nHowever, the existing approaches rely only on predefined graphs to indirectly\ncapture the relations. In this paper, we propose a Dialogue State Distillation\nNetwork (DSDN) to utilize relevant information of previous dialogue states and\nmigrate the gap of utilization between training and testing. Thus, it can\ndynamically exploit previous dialogue states and avoid introducing error\npropagation simultaneously. Further, we propose an inter-slot contrastive\nlearning loss to effectively capture the slot co-update relations from dialogue\ncontext. Experiments are conducted on the widely used MultiWOZ 2.0 and MultiWOZ\n2.1 datasets. The experimental results show that our proposed model achieves\nthe state-of-the-art performance for DST.", "published": "2023-02-16 11:05:24", "link": "http://arxiv.org/abs/2302.08220v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tragic and Comical Networks. Clustering Dramatic Genres According to\n  Structural Properties", "abstract": "There is a growing tradition in the joint field of network studies and drama\nhistory that produces interpretations from the character networks of the\nplays.The potential of such an interpretation is that the diagrams provide a\ndifferent representation of the relationships between characters as compared to\nreading the text or watching the performance. Our aim is to create a method\nthat is able to cluster texts with similar structures on the basis of the\nplay's well-interpretable and simple properties, independent from the number of\ncharacters in the drama, or in other words, the size of the network. Finding\nthese features is the most important part of our research, as well as\nestablishing the appropriate statistical procedure to calculate the\nsimilarities between the texts. Our data was downloaded from the DraCor\ndatabase and analyzed in R (we use the GerDracor and the ShakeDraCor\nsub-collection). We want to propose a robust method based on the distribution\nof words among characters; distribution of characters in scenes, average length\nof speech acts, or character-specific and macro-level network properties such\nas clusterization coefficient and network density. Based on these metrics a\nsupervised classification procedure is applied to the sub-collections to\nclassify comedies and tragedies using the Support Vector Machine (SVM) method.\nOur research shows that this approach can also produce reliable results on a\nsmall sample size.", "published": "2023-02-16 12:36:16", "link": "http://arxiv.org/abs/2302.08258v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NUAA-QMUL-AIIT at Memotion 3: Multi-modal Fusion with\n  Squeeze-and-Excitation for Internet Meme Emotion Analysis", "abstract": "This paper describes the participation of our NUAA-QMUL-AIIT team in the\nMemotion 3 shared task on meme emotion analysis. We propose a novel multi-modal\nfusion method, Squeeze-and-Excitation Fusion (SEFusion), and embed it into our\nsystem for emotion classification in memes. SEFusion is a simple fusion method\nthat employs fully connected layers, reshaping, and matrix multiplication.\nSEFusion learns a weight for each modality and then applies it to its own\nmodality feature. We evaluate the performance of our system on the three\nMemotion 3 sub-tasks. Among all participating systems in this Memotion 3 shared\ntask, our system ranked first on task A, fifth on task B, and second on task C.\nOur proposed SEFusion provides the flexibility to fuse any features from\ndifferent modalities. The source code for our method is published on\nhttps://github.com/xxxxxxxxy/memotion3-SEFusion.", "published": "2023-02-16 14:29:58", "link": "http://arxiv.org/abs/2302.08326v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Conversation Style Transfer using Few-Shot Learning", "abstract": "Conventional text style transfer approaches focus on sentence-level style\ntransfer without considering contextual information, and the style is described\nwith attributes (e.g., formality). When applying style transfer in\nconversations such as task-oriented dialogues, existing approaches suffer from\nthese limitations as context can play an important role and the style\nattributes are often difficult to define in conversations. In this paper, we\nintroduce conversation style transfer as a few-shot learning problem, where the\nmodel learns to perform style transfer by observing only a few example\ndialogues in the target style. We propose a novel in-context learning approach\nto solve the task with style-free dialogues as a pivot. Human evaluation shows\nthat by incorporating multi-turn context, the model is able to match the target\nstyle while having better appropriateness and semantic correctness compared to\nutterance/sentence-level style transfer. Additionally, we show that\nconversation style transfer can also benefit downstream tasks. For example, in\nmulti-domain intent classification tasks, the F1 scores improve after\ntransferring the style of training data to match the style of the test data.", "published": "2023-02-16 15:27:00", "link": "http://arxiv.org/abs/2302.08362v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LEALLA: Learning Lightweight Language-agnostic Sentence Embeddings with\n  Knowledge Distillation", "abstract": "Large-scale language-agnostic sentence embedding models such as LaBSE (Feng\net al., 2022) obtain state-of-the-art performance for parallel sentence\nalignment. However, these large-scale models can suffer from inference speed\nand computation overhead. This study systematically explores learning\nlanguage-agnostic sentence embeddings with lightweight models. We demonstrate\nthat a thin-deep encoder can construct robust low-dimensional sentence\nembeddings for 109 languages. With our proposed distillation methods, we\nachieve further improvements by incorporating knowledge from a teacher model.\nEmpirical results on Tatoeba, United Nations, and BUCC show the effectiveness\nof our lightweight models. We release our lightweight language-agnostic\nsentence embedding models LEALLA on TensorFlow Hub.", "published": "2023-02-16 16:05:34", "link": "http://arxiv.org/abs/2302.08387v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning with Rejection for Abstractive Text Summarization", "abstract": "State-of-the-art abstractive summarization systems frequently hallucinate\ncontent that is not supported by the source document, mainly due to noise in\nthe training dataset. Existing methods opt to drop the noisy samples or tokens\nfrom the training set entirely, reducing the effective training set size and\ncreating an artificial propensity to copy words from the source. In this work,\nwe propose a training objective for abstractive summarization based on\nrejection learning, in which the model learns whether or not to reject\npotentially noisy tokens. We further propose a regularized decoding objective\nthat penalizes non-factual candidate summaries during inference by using the\nrejection probability learned during training. We show that our method\nconsiderably improves the factuality of generated summaries in automatic and\nhuman evaluations when compared to five baseline models and that it does so\nwhile increasing the abstractiveness of the generated summaries.", "published": "2023-02-16 19:07:08", "link": "http://arxiv.org/abs/2302.08531v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "For Generated Text, Is NLI-Neutral Text the Best Text?", "abstract": "We explore incorporating natural language inference (NLI) into the text\ngenerative pipeline by using a pre-trained NLI model to assess whether a\ngenerated sentence entails, contradicts, or is neutral to the prompt and\npreceding text. First, we show that the NLI task is predictive of generation\nerrors made by GPT-3. We use these results to develop an NLI-informed\ngeneration procedure for GPT-J. Then, we evaluate these generations by\nobtaining human annotations on error types and overall quality. We find that an\nNLI strategy of maximizing entailment improves text generation when the nucleus\nsampling randomness parameter value is high, while one which maximizes\ncontradiction is in fact productive when the parameter value is low. Overall,\nthough, we demonstrate that an NLI strategy of maximizing the neutral class\nprovides the highest quality of generated text (significantly better than the\nvanilla generations), regardless of parameter value.", "published": "2023-02-16 20:46:36", "link": "http://arxiv.org/abs/2302.08577v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What A Situated Language-Using Agent Must be Able to Do: A Top-Down\n  Analysis", "abstract": "Even in our increasingly text-intensive times, the primary site of language\nuse is situated, co-present interaction. It is primary ontogenetically and\nphylogenetically, and it is arguably also still primary in negotiating everyday\nsocial situations. Situated interaction is also the final frontier of Natural\nLanguage Processing, where, compared to the area of text processing, very\nlittle progress has been made in the past decade, and where a myriad of\npractical applications is waiting to be unlocked. While the usual approach in\nthe field is to reach, bottom-up, for the ever next \"adjacent possible\", in\nthis paper I attempt a top-down analysis of what the demands are that\nunrestricted situated interaction makes on the participating agent, and suggest\nways in which this analysis can structure computational models and research on\nthem. Specifically, I discuss representational demands (the building up and\napplication of world model, language model, situation model, discourse model,\nand agent model) and what I call anchoring processes (incremental processing,\nincremental learning, conversational grounding, multimodal grounding) that bind\nthe agent to the here, now, and us.", "published": "2023-02-16 21:30:26", "link": "http://arxiv.org/abs/2302.08590v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bridge the Gap between Language models and Tabular Understanding", "abstract": "Table pretrain-then-finetune paradigm has been proposed and employed at a\nrapid pace after the success of pre-training in the natural language domain.\nDespite the promising findings in tabular pre-trained language models (TPLMs),\nthere is an input gap between pre-training and fine-tuning phases. For\ninstance, TPLMs jointly pre-trained with table and text input could be\neffective for tasks also with table-text joint input like table question\nanswering, but it may fail for tasks with only tables or text as input such as\ntable retrieval. To this end, we propose UTP, an approach that dynamically\nsupports three types of multi-modal inputs: table-text, table, and text.\nSpecifically, UTP is pre-trained with two strategies: (1) We first utilize a\nuniversal mask language modeling objective on each kind of input, enforcing the\nmodel to adapt various inputs. (2) We then present Cross-Modal Contrastive\nRegularization (CMCR), which utilizes contrastive learning to encourage the\nconsistency between table-text cross-modality representations via unsupervised\ninstance-wise training signals during pre-training. By these means, the\nresulting model not only bridges the input gap between pre-training and\nfine-tuning but also advances in the alignment of table and text. Extensive\nresults show UTP achieves superior results on uni-modal input tasks (e.g.,\ntable retrieval) and cross-modal input tasks (e.g., table question answering).", "published": "2023-02-16 15:16:55", "link": "http://arxiv.org/abs/2302.09302v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural\n  Networks", "abstract": "Graphs can model complex relationships between objects, enabling a myriad of\nWeb applications such as online page/article classification and social\nrecommendation. While graph neural networks(GNNs) have emerged as a powerful\ntool for graph representation learning, in an end-to-end supervised setting,\ntheir performance heavily rely on a large amount of task-specific supervision.\nTo reduce labeling requirement, the \"pre-train, fine-tune\" and \"pre-train,\nprompt\" paradigms have become increasingly common. In particular, prompting is\na popular alternative to fine-tuning in natural language processing, which is\ndesigned to narrow the gap between pre-training and downstream objectives in a\ntask-specific manner. However, existing study of prompting on graphs is still\nlimited, lacking a universal treatment to appeal to different downstream tasks.\nIn this paper, we propose GraphPrompt, a novel pre-training and prompting\nframework on graphs. GraphPrompt not only unifies pre-training and downstream\ntasks into a common task template, but also employs a learnable prompt to\nassist a downstream task in locating the most relevant knowledge from the\npre-train model in a task-specific manner. Finally, we conduct extensive\nexperiments on five public datasets to evaluate and analyze GraphPrompt.", "published": "2023-02-16 02:51:38", "link": "http://arxiv.org/abs/2302.08043v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Exploring the Limits of ChatGPT for Query or Aspect-based Text\n  Summarization", "abstract": "Text summarization has been a crucial problem in natural language processing\n(NLP) for several decades. It aims to condense lengthy documents into shorter\nversions while retaining the most critical information. Various methods have\nbeen proposed for text summarization, including extractive and abstractive\nsummarization. The emergence of large language models (LLMs) like GPT3 and\nChatGPT has recently created significant interest in using these models for\ntext summarization tasks. Recent studies \\cite{goyal2022news,\nzhang2023benchmarking} have shown that LLMs-generated news summaries are\nalready on par with humans. However, the performance of LLMs for more practical\napplications like aspect or query-based summaries is underexplored. To fill\nthis gap, we conducted an evaluation of ChatGPT's performance on four widely\nused benchmark datasets, encompassing diverse summaries from Reddit posts, news\narticles, dialogue meetings, and stories. Our experiments reveal that ChatGPT's\nperformance is comparable to traditional fine-tuning methods in terms of Rouge\nscores. Moreover, we highlight some unique differences between\nChatGPT-generated summaries and human references, providing valuable insights\ninto the superpower of ChatGPT for diverse text summarization tasks. Our\nfindings call for new directions in this area, and we plan to conduct further\nresearch to systematically examine the characteristics of ChatGPT-generated\nsummaries through extensive human evaluation.", "published": "2023-02-16 04:41:30", "link": "http://arxiv.org/abs/2302.08081v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Product Question Answering in E-Commerce: A Survey", "abstract": "Product question answering (PQA), aiming to automatically provide instant\nresponses to customer's questions in E-Commerce platforms, has drawn increasing\nattention in recent years. Compared with typical QA problems, PQA exhibits\nunique challenges such as the subjectivity and reliability of user-generated\ncontents in E-commerce platforms. Therefore, various problem settings and novel\nmethods have been proposed to capture these special characteristics. In this\npaper, we aim to systematically review existing research efforts on PQA.\nSpecifically, we categorize PQA studies into four problem settings in terms of\nthe form of provided answers. We analyze the pros and cons, as well as present\nexisting datasets and evaluation protocols for each setting. We further\nsummarize the most significant challenges that characterize PQA from general QA\napplications and discuss their corresponding solutions. Finally, we conclude\nthis paper by providing the prospect on several future directions.", "published": "2023-02-16 05:09:41", "link": "http://arxiv.org/abs/2302.08092v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Learning to Initialize: Can Meta Learning Improve Cross-task\n  Generalization in Prompt Tuning?", "abstract": "Prompt tuning (PT) which only tunes the embeddings of an additional sequence\nof tokens per task, keeping the pre-trained language model (PLM) frozen, has\nshown remarkable performance in few-shot learning. Despite this, PT has been\nshown to rely heavily on good initialization of the prompt embeddings. In this\nwork, we study meta prompt tuning (MPT) to systematically explore how\nmeta-learning can help improve (if it can) cross-task generalization in PT\nthrough learning to initialize the prompt embeddings from other relevant tasks.\nWe empirically analyze a representative set of meta learning algorithms in a\nwide range of adaptation settings with different source/target task\nconfigurations on a large set of few-shot tasks. With extensive experiments and\nanalysis, we demonstrate the effectiveness of MPT. We find the improvement to\nbe significant particularly on classification tasks. For other kinds of tasks\nsuch as question answering, we observe that while MPT can outperform PT in most\ncases, it does not always outperform multi-task learning. We further provide an\nin-depth analysis from the perspective of task similarity.", "published": "2023-02-16 08:37:22", "link": "http://arxiv.org/abs/2302.08143v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Empirical Investigation of Neural Symbolic Reasoning Strategies", "abstract": "Neural reasoning accuracy improves when generating intermediate reasoning\nsteps. However, the source of this improvement is yet unclear. Here, we\ninvestigate and factorize the benefit of generating intermediate steps for\nsymbolic reasoning. Specifically, we decompose the reasoning strategy w.r.t.\nstep granularity and chaining strategy. With a purely symbolic numerical\nreasoning dataset (e.g., A=1, B=3, C=A+3, C?), we found that the choice of\nreasoning strategies significantly affects the performance, with the gap\nbecoming even larger as the extrapolation length becomes longer. Surprisingly,\nwe also found that certain configurations lead to nearly perfect performance,\neven in the case of length extrapolation. Our results indicate the importance\nof further exploring effective strategies for neural reasoning models.", "published": "2023-02-16 08:49:47", "link": "http://arxiv.org/abs/2302.08148v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Un mod{\u00e8}le de base de connaissances terminologiques", "abstract": "In the present paper, we argue that Terminological Knowledge Bases (TKB) are\nall the more useful for addressing various needs as they do not fulfill formal\ncriteria. Moreover, they intend to clarify the terminology of a given domain by\nillustrating term uses in various contexts. Thus we designed a TKB structure\nincluding 3 linked features: terms, concepts and texts, that present the\npeculiar use of each term in the domain. Note that concepts are represented\ninto frames whose non-formal description is standardized. Associated with this\nstructure, we defined modeling criteria at the conceptual level. Finaly, we\ndiscuss the situation of TKB with regard to ontologies, and the use of TKB for\nthe development of AI systems.", "published": "2023-02-16 10:28:23", "link": "http://arxiv.org/abs/2302.08198v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Improving Spoken Language Identification with Map-Mix", "abstract": "The pre-trained multi-lingual XLSR model generalizes well for language\nidentification after fine-tuning on unseen languages. However, the performance\nsignificantly degrades when the languages are not very distinct from each\nother, for example, in the case of dialects. Low resource dialect\nclassification remains a challenging problem to solve. We present a new data\naugmentation method that leverages model training dynamics of individual data\npoints to improve sampling for latent mixup. The method works well in\nlow-resource settings where generalization is paramount. Our datamaps-based\nmixup technique, which we call Map-Mix improves weighted F1 scores by 2%\ncompared to the random mixup baseline and results in a significantly\nwell-calibrated model. The code for our method is open sourced on\nhttps://github.com/skit-ai/Map-Mix.", "published": "2023-02-16 11:27:46", "link": "http://arxiv.org/abs/2302.08229v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Retrieval-augmented Image Captioning", "abstract": "Inspired by retrieval-augmented language generation and pretrained Vision and\nLanguage (V&L) encoders, we present a new approach to image captioning that\ngenerates sentences given the input image and a set of captions retrieved from\na datastore, as opposed to the image alone. The encoder in our model jointly\nprocesses the image and retrieved captions using a pretrained V&L BERT, while\nthe decoder attends to the multimodal encoder representations, benefiting from\nthe extra textual evidence from the retrieved captions. Experimental results on\nthe COCO dataset show that image captioning can be effectively formulated from\nthis new perspective. Our model, named EXTRA, benefits from using captions\nretrieved from the training dataset, and it can also benefit from using an\nexternal dataset without the need for retraining. Ablation studies show that\nretrieving a sufficient number of captions (e.g., k=5) can improve captioning\nquality. Our work contributes towards using pretrained V&L encoders for\ngenerative tasks, instead of standard classification tasks.", "published": "2023-02-16 12:54:13", "link": "http://arxiv.org/abs/2302.08268v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Cluster-based Deep Ensemble Learning for Emotion Classification in\n  Internet Memes", "abstract": "Memes have gained popularity as a means to share visual ideas through the\nInternet and social media by mixing text, images and videos, often for humorous\npurposes. Research enabling automated analysis of memes has gained attention in\nrecent years, including among others the task of classifying the emotion\nexpressed in memes. In this paper, we propose a novel model, cluster-based deep\nensemble learning (CDEL), for emotion classification in memes. CDEL is a hybrid\nmodel that leverages the benefits of a deep learning model in combination with\na clustering algorithm, which enhances the model with additional information\nafter clustering memes with similar facial features. We evaluate the\nperformance of CDEL on a benchmark dataset for emotion classification, proving\nits effectiveness by outperforming a wide range of baseline models and\nachieving state-of-the-art performance. Further evaluation through ablated\nmodels demonstrates the effectiveness of the different components of CDEL.", "published": "2023-02-16 15:01:07", "link": "http://arxiv.org/abs/2302.08343v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "A Survey on Event-based News Narrative Extraction", "abstract": "Narratives are fundamental to our understanding of the world, providing us\nwith a natural structure for knowledge representation over time. Computational\nnarrative extraction is a subfield of artificial intelligence that makes heavy\nuse of information retrieval and natural language processing techniques.\nDespite the importance of computational narrative extraction, relatively little\nscholarly work exists on synthesizing previous research and strategizing future\nresearch in the area. In particular, this article focuses on extracting news\nnarratives from an event-centric perspective. Extracting narratives from news\ndata has multiple applications in understanding the evolving information\nlandscape. This survey presents an extensive study of research in the area of\nevent-based news narrative extraction. In particular, we screened over 900\narticles that yielded 54 relevant articles. These articles are synthesized and\norganized by representation model, extraction criteria, and evaluation\napproaches. Based on the reviewed studies, we identify recent trends, open\nchallenges, and potential research lines.", "published": "2023-02-16 15:11:53", "link": "http://arxiv.org/abs/2302.08351v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Large Language Models Fail on Trivial Alterations to Theory-of-Mind\n  Tasks", "abstract": "Intuitive psychology is a pillar of common-sense reasoning. The replication\nof this reasoning in machine intelligence is an important stepping-stone on the\nway to human-like artificial intelligence. Several recent tasks and benchmarks\nfor examining this reasoning in Large-Large Models have focused in particular\non belief attribution in Theory-of-Mind tasks. These tasks have shown both\nsuccesses and failures. We consider in particular a recent purported success\ncase, and show that small variations that maintain the principles of ToM turn\nthe results on their head. We argue that in general, the zero-hypothesis for\nmodel evaluation in intuitive psychology should be skeptical, and that outlying\nfailure cases should outweigh average success rates. We also consider what\npossible future successes on Theory-of-Mind tasks by more powerful LLMs would\nmean for ToM tasks with people.", "published": "2023-02-16 16:18:03", "link": "http://arxiv.org/abs/2302.08399v5", "categories": ["cs.AI", "cs.CL", "I.2.7"], "primary_category": "cs.AI"}
{"title": "Pretraining Language Models with Human Preferences", "abstract": "Language models (LMs) are pretrained to imitate internet text, including\ncontent that would violate human preferences if generated by an LM: falsehoods,\noffensive comments, personally identifiable information, low-quality or buggy\ncode, and more. Here, we explore alternative objectives for pretraining LMs in\na way that also guides them to generate text aligned with human preferences. We\nbenchmark five objectives for pretraining with human feedback across three\ntasks and study how they affect the trade-off between alignment and\ncapabilities of pretrained LMs. We find a Pareto-optimal and simple approach\namong those we explored: conditional training, or learning distribution over\ntokens conditional on their human preference scores given by a reward model.\nConditional training reduces the rate of undesirable content by up to an order\nof magnitude, both when generating without a prompt and with an\nadversarially-chosen prompt. Moreover, conditional training maintains the\ndownstream task performance of standard LM pretraining, both before and after\ntask-specific finetuning. Pretraining with human feedback results in much\nbetter preference satisfaction than standard LM pretraining followed by\nfinetuning with feedback, i.e., learning and then unlearning undesirable\nbehavior. Our results suggest that we should move beyond imitation learning\nwhen pretraining LMs and incorporate human preferences from the start of\ntraining.", "published": "2023-02-16 21:03:33", "link": "http://arxiv.org/abs/2302.08582v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Syntactic Structure Processing in the Brain while Listening", "abstract": "Syntactic parsing is the task of assigning a syntactic structure to a\nsentence. There are two popular syntactic parsing methods: constituency and\ndependency parsing. Recent works have used syntactic embeddings based on\nconstituency trees, incremental top-down parsing, and other word syntactic\nfeatures for brain activity prediction given the text stimuli to study how the\nsyntax structure is represented in the brain's language network. However, the\neffectiveness of dependency parse trees or the relative predictive power of the\nvarious syntax parsers across brain areas, especially for the listening task,\nis yet unexplored. In this study, we investigate the predictive power of the\nbrain encoding models in three settings: (i) individual performance of the\nconstituency and dependency syntactic parsing based embedding methods, (ii)\nefficacy of these syntactic parsing based embedding methods when controlling\nfor basic syntactic signals, (iii) relative effectiveness of each of the\nsyntactic embedding methods when controlling for the other. Further, we explore\nthe relative importance of syntactic information (from these syntactic\nembedding methods) versus semantic information using BERT embeddings. We find\nthat constituency parsers help explain activations in the temporal lobe and\nmiddle-frontal gyrus, while dependency parsers better encode syntactic\nstructure in the angular gyrus and posterior cingulate cortex. Although\nsemantic signals from BERT are more effective compared to any of the syntactic\nfeatures or embedding methods, syntactic embedding methods explain additional\nvariance for a few brain regions.", "published": "2023-02-16 21:28:11", "link": "http://arxiv.org/abs/2302.08589v1", "categories": ["cs.CL", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "InstructABSA: Instruction Learning for Aspect Based Sentiment Analysis", "abstract": "We introduce InstructABSA, an instruction learning paradigm for Aspect-Based\nSentiment Analysis (ABSA) subtasks. Our method introduces positive, negative,\nand neutral examples to each training sample, and instruction tune the model\n(Tk-Instruct) for ABSA subtasks, yielding significant performance improvements.\nExperimental results on the Sem Eval 2014, 15, and 16 datasets demonstrate that\nInstructABSA outperforms the previous state-of-the-art (SOTA) approaches on\nTerm Extraction (ATE), Sentiment Classification(ATSC) and Sentiment Pair\nExtraction (ASPE) subtasks. In particular, InstructABSA outperforms the\nprevious state-of-the-art (SOTA) on the Rest14 ATE subtask by 5.69% points, the\nRest15 ATSC subtask by 9.59% points, and the Lapt14 AOPE subtask by 3.37%\npoints, surpassing 7x larger models. We also get competitive results on AOOE,\nAOPE, and AOSTE subtasks indicating strong generalization ability to all\nsubtasks. Exploring sample efficiency reveals that just 50% train data is\nrequired to get competitive results with other instruction tuning approaches.\nLastly, we assess the quality of instructions and observe that InstructABSA's\nperformance experiences a decline of ~10% when adding misleading examples.", "published": "2023-02-16 23:29:22", "link": "http://arxiv.org/abs/2302.08624v6", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Search-Engine-augmented Dialogue Response Generation with Cheaply\n  Supervised Query Production", "abstract": "Knowledge-aided dialogue response generation aims at augmenting chatbots with\nrelevant external knowledge in the hope of generating more informative\nresponses. The majority of previous work assumes that the relevant knowledge is\ngiven as input or retrieved from a static pool of knowledge. However, this\nassumption violates the real-world situation, where knowledge is continually\nupdated and a chatbot has to dynamically retrieve useful knowledge. We propose\na dialogue model that can access the vast and dynamic information from any\nsearch engine for response generation. As the core module, a query producer is\nused to generate queries from a dialogue context to interact with a search\nengine. We design a training algorithm using cheap noisy supervision for the\nquery producer, where the signals are obtained by comparing retrieved articles\nwith the next dialogue response. As the result, the query producer is adjusted\nwithout any human annotation of gold queries, making it easily transferable to\nother domains and search engines. Experiments show that our query producer can\nachieve R@1 and R@5 rates of 62.4% and 74.8% for retrieving gold knowledge, and\nthe overall model generates better responses over strong knowledge-aided\nbaselines using BART and other typical systems.", "published": "2023-02-16 01:58:10", "link": "http://arxiv.org/abs/2302.09300v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dynamic Named Entity Recognition", "abstract": "Named Entity Recognition (NER) is a challenging and widely studied task that\ninvolves detecting and typing entities in text. So far,NER still approaches\nentity typing as a task of classification into universal classes (e.g. date,\nperson, or location). Recent advances innatural language processing focus on\narchitectures of increasing complexity that may lead to overfitting and\nmemorization, and thus, underuse of context. Our work targets situations where\nthe type of entities depends on the context and cannot be solved solely by\nmemorization. We hence introduce a new task: Dynamic Named Entity Recognition\n(DNER), providing a framework to better evaluate the ability of algorithms to\nextract entities by exploiting the context. The DNER benchmark is based on two\ndatasets, DNER-RotoWire and DNER-IMDb. We evaluate baseline models and present\nexperiments reflecting issues and research axes related to this novel task.", "published": "2023-02-16 15:50:02", "link": "http://arxiv.org/abs/2302.10314v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Generalization algorithm of multimodal pre-training model based on\n  graph-text self-supervised training", "abstract": "Recently, a large number of studies have shown that the introduction of\nvisual information can effectively improve the effect of neural machine\ntranslation (NMT). Its effectiveness largely depends on the availability of a\nlarge number of bilingual parallel sentence pairs and manual image annotation.\nThe lack of images and the effectiveness of images have been difficult to\nsolve. In this paper, a multimodal pre-training generalization algorithm for\nself-supervised training is proposed, which overcomes the lack of visual\ninformation and inaccuracy, and thus extends the applicability of images on\nNMT. Specifically, we will search for many pictures from the existing sentences\nthrough the search engine, and then through the relationship between visual\ninformation and text, do the self-supervised training task of graphics and text\nto obtain more effective visual information for text. We show that when the\nfiltered information is used as multimodal machine translation for fine-tuning,\nthe effect of translation in the global voice dataset is 0.5 BLEU higher than\nthe baseline.", "published": "2023-02-16 03:34:08", "link": "http://arxiv.org/abs/2302.10315v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LabelPrompt: Effective Prompt-based Learning for Relation Classification", "abstract": "Recently, prompt-based learning has gained popularity across many natural\nlanguage processing (NLP) tasks by reformulating them into a cloze-style format\nto better align pre-trained language models (PLMs) with downstream tasks.\nHowever, applying this approach to relation classification poses unique\nchallenges. Specifically, associating natural language words that fill the\nmasked token with semantic relation labels (\\textit{e.g.}\n\\textit{``org:founded\\_by}'') is difficult. To address this challenge, this\npaper presents a novel prompt-based learning method, namely LabelPrompt, for\nthe relation classification task. Motivated by the intuition to ``GIVE MODEL\nCHOICES!'', we first define additional tokens to represent relation labels,\nwhich regard these tokens as the verbaliser with semantic initialisation and\nexplicitly construct them with a prompt template method. Then, to mitigate\ninconsistency between predicted relations and given entities, we implement an\nentity-aware module with contrastive learning. Last, we conduct an attention\nquery strategy within the self-attention layer to differentiates prompt tokens\nand sequence tokens. Together, these strategies enhance the adaptability of\nprompt-based learning, especially when only small labelled datasets is\navailable. Comprehensive experiments on benchmark datasets demonstrate the\nsuperiority of our method, particularly in the few-shot scenario.", "published": "2023-02-16 04:06:25", "link": "http://arxiv.org/abs/2302.08068v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning Multi-Object Positional Relationships via Emergent\n  Communication", "abstract": "The study of emergent communication has been dedicated to interactive\nartificial intelligence. While existing work focuses on communication about\nsingle objects or complex image scenes, we argue that communicating\nrelationships between multiple objects is important in more realistic tasks,\nbut understudied. In this paper, we try to fill this gap and focus on emergent\ncommunication about positional relationships between two objects. We train\nagents in the referential game where observations contain two objects, and find\nthat generalization is the major problem when the positional relationship is\ninvolved. The key factor affecting the generalization ability of the emergent\nlanguage is the input variation between Speaker and Listener, which is realized\nby a random image generator in our work. Further, we find that the learned\nlanguage can generalize well in a new multi-step MDP task where the positional\nrelationship describes the goal, and performs better than raw-pixel images as\nwell as pre-trained image features, verifying the strong generalization ability\nof discrete sequences. We also show that language transfer from the referential\ngame performs better in the new task than learning language directly in this\ntask, implying the potential benefits of pre-training in referential games. All\nin all, our experiments demonstrate the viability and merit of having agents\nlearn to communicate positional relationships between multiple objects through\nemergent communication.", "published": "2023-02-16 04:44:53", "link": "http://arxiv.org/abs/2302.08084v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "TAPLoss: A Temporal Acoustic Parameter Loss for Speech Enhancement", "abstract": "Speech enhancement models have greatly progressed in recent years, but still\nshow limits in perceptual quality of their speech outputs. We propose an\nobjective for perceptual quality based on temporal acoustic parameters. These\nare fundamental speech features that play an essential role in various\napplications, including speaker recognition and paralinguistic analysis. We\nprovide a differentiable estimator for four categories of low-level acoustic\ndescriptors involving: frequency-related parameters, energy or\namplitude-related parameters, spectral balance parameters, and temporal\nfeatures. Unlike prior work that looks at aggregated acoustic parameters or a\nfew categories of acoustic parameters, our temporal acoustic parameter (TAP)\nloss enables auxiliary optimization and improvement of many fine-grain speech\ncharacteristics in enhancement workflows. We show that adding TAPLoss as an\nauxiliary objective in speech enhancement produces speech with improved\nperceptual quality and intelligibility. We use data from the Deep Noise\nSuppression 2020 Challenge to demonstrate that both time-domain models and\ntime-frequency domain models can benefit from our method.", "published": "2023-02-16 04:57:11", "link": "http://arxiv.org/abs/2302.08088v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "PAAPLoss: A Phonetic-Aligned Acoustic Parameter Loss for Speech\n  Enhancement", "abstract": "Despite rapid advancement in recent years, current speech enhancement models\noften produce speech that differs in perceptual quality from real clean speech.\nWe propose a learning objective that formalizes differences in perceptual\nquality, by using domain knowledge of acoustic-phonetics. We identify temporal\nacoustic parameters -- such as spectral tilt, spectral flux, shimmer, etc. --\nthat are non-differentiable, and we develop a neural network estimator that can\naccurately predict their time-series values across an utterance. We also model\nphoneme-specific weights for each feature, as the acoustic parameters are known\nto show different behavior in different phonemes. We can add this criterion as\nan auxiliary loss to any model that produces speech, to optimize speech outputs\nto match the values of clean speech in these features. Experimentally we show\nthat it improves speech enhancement workflows in both time-domain and\ntime-frequency domain, as measured by standard evaluation metrics. We also\nprovide an analysis of phoneme-dependent improvement on acoustic parameters,\ndemonstrating the additional interpretability that our method provides. This\nanalysis can suggest which features are currently the bottleneck for\nimprovement.", "published": "2023-02-16 05:17:06", "link": "http://arxiv.org/abs/2302.08095v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Aligning Language Models with Preferences through f-divergence\n  Minimization", "abstract": "Aligning language models with preferences can be posed as approximating a\ntarget distribution representing some desired behavior. Existing approaches\ndiffer both in the functional form of the target distribution and the algorithm\nused to approximate it. For instance, Reinforcement Learning from Human\nFeedback (RLHF) corresponds to minimizing a reverse KL from an implicit target\ndistribution arising from a KL penalty in the objective. On the other hand,\nGenerative Distributional Control (GDC) has an explicit target distribution and\nminimizes a forward KL from it using the Distributional Policy Gradient (DPG)\nalgorithm. In this paper, we propose a new approach, f-DPG, which allows the\nuse of any f-divergence to approximate any target distribution that can be\nevaluated. f-DPG unifies both frameworks (RLHF, GDC) and the approximation\nmethods (DPG, RL with KL penalties). We show the practical benefits of various\nchoices of divergence objectives and demonstrate that there is no universally\noptimal objective but that different divergences present different alignment\nand diversity trade-offs. We show that Jensen-Shannon divergence strikes a good\nbalance between these objectives, and frequently outperforms forward KL\ndivergence by a wide margin, leading to significant improvements over prior\nwork. These distinguishing characteristics between divergences persist as the\nmodel size increases, highlighting the importance of selecting appropriate\ndivergence objectives.", "published": "2023-02-16 10:59:39", "link": "http://arxiv.org/abs/2302.08215v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Efficiency 360: Efficient Vision Transformers", "abstract": "Transformers are widely used for solving tasks in natural language\nprocessing, computer vision, speech, and music domains. In this paper, we talk\nabout the efficiency of transformers in terms of memory (the number of\nparameters), computation cost (number of floating points operations), and\nperformance of models, including accuracy, the robustness of the model, and\nfair \\& bias-free features. We mainly discuss the vision transformer for the\nimage classification task. Our contribution is to introduce an efficient 360\nframework, which includes various aspects of the vision transformer, to make it\nmore efficient for industrial applications. By considering those applications,\nwe categorize them into multiple dimensions such as privacy, robustness,\ntransparency, fairness, inclusiveness, continual learning, probabilistic\nmodels, approximation, computational complexity, and spectral complexity. We\ncompare various vision transformer models based on their performance, the\nnumber of parameters, and the number of floating point operations (FLOPs) on\nmultiple datasets.", "published": "2023-02-16 15:43:32", "link": "http://arxiv.org/abs/2302.08374v3", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Evaluating and Improving the Coreference Capabilities of Machine\n  Translation Models", "abstract": "Machine translation (MT) requires a wide range of linguistic capabilities,\nwhich current end-to-end models are expected to learn implicitly by observing\naligned sentences in bilingual corpora. In this work, we ask: \\emph{How well do\nMT models learn coreference resolution from implicit signal?} To answer this\nquestion, we develop an evaluation methodology that derives coreference\nclusters from MT output and evaluates them without requiring annotations in the\ntarget language. We further evaluate several prominent open-source and\ncommercial MT systems, translating from English to six target languages, and\ncompare them to state-of-the-art coreference resolvers on three challenging\nbenchmarks. Our results show that the monolingual resolvers greatly outperform\nMT models. Motivated by this result, we experiment with different methods for\nincorporating the output of coreference resolution models in MT, showing\nimprovement over strong baselines.", "published": "2023-02-16 18:16:09", "link": "http://arxiv.org/abs/2302.08464v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LEVER: Learning to Verify Language-to-Code Generation with Execution", "abstract": "The advent of large language models trained on code (code LLMs) has led to\nsignificant progress in language-to-code generation. State-of-the-art\napproaches in this area combine LLM decoding with sample pruning and reranking\nusing test cases or heuristics based on the execution results. However, it is\nchallenging to obtain test cases for many real-world language-to-code\napplications, and heuristics cannot well capture the semantic features of the\nexecution results, such as data type and value range, which often indicates the\ncorrectness of the program. In this work, we propose LEVER, a simple approach\nto improve language-to-code generation by learning to verify the generated\nprograms with their execution results. Specifically, we train verifiers to\ndetermine whether a program sampled from the LLMs is correct or not based on\nthe natural language input, the program itself and its execution results. The\nsampled programs are reranked by combining the verification score with the LLM\ngeneration probability, and marginalizing over programs with the same execution\nresults. On four datasets across the domains of table QA, math QA and basic\nPython programming, LEVER consistently improves over the base code LLMs(4.6% to\n10.9% with code-davinci-002) and achieves new state-of-the-art results on all\nof them.", "published": "2023-02-16 18:23:22", "link": "http://arxiv.org/abs/2302.08468v3", "categories": ["cs.LG", "cs.CL", "cs.PL", "cs.SE"], "primary_category": "cs.LG"}
{"title": "Auditing large language models: a three-layered approach", "abstract": "Large language models (LLMs) represent a major advance in artificial\nintelligence (AI) research. However, the widespread use of LLMs is also coupled\nwith significant ethical and social challenges. Previous research has pointed\ntowards auditing as a promising governance mechanism to help ensure that AI\nsystems are designed and deployed in ways that are ethical, legal, and\ntechnically robust. However, existing auditing procedures fail to address the\ngovernance challenges posed by LLMs, which display emergent capabilities and\nare adaptable to a wide range of downstream tasks. In this article, we address\nthat gap by outlining a novel blueprint for how to audit LLMs. Specifically, we\npropose a three-layered approach, whereby governance audits (of technology\nproviders that design and disseminate LLMs), model audits (of LLMs after\npre-training but prior to their release), and application audits (of\napplications based on LLMs) complement and inform each other. We show how\naudits, when conducted in a structured and coordinated manner on all three\nlevels, can be a feasible and effective mechanism for identifying and managing\nsome of the ethical and social risks posed by LLMs. However, it is important to\nremain realistic about what auditing can reasonably be expected to achieve.\nTherefore, we discuss the limitations not only of our three-layered approach\nbut also of the prospect of auditing LLMs at all. Ultimately, this article\nseeks to expand the methodological toolkit available to technology providers\nand policymakers who wish to analyse and evaluate LLMs from technical, ethical,\nand legal perspectives.", "published": "2023-02-16 18:55:21", "link": "http://arxiv.org/abs/2302.08500v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "K.4; K.6"], "primary_category": "cs.CL"}
{"title": "Foundation Models for Natural Language Processing -- Pre-trained\n  Language Models Integrating Media", "abstract": "This open access book provides a comprehensive overview of the state of the\nart in research and applications of Foundation Models and is intended for\nreaders familiar with basic Natural Language Processing (NLP) concepts. Over\nthe recent years, a revolutionary new paradigm has been developed for training\nmodels for NLP. These models are first pre-trained on large collections of text\ndocuments to acquire general syntactic knowledge and semantic information.\nThen, they are fine-tuned for specific tasks, which they can often solve with\nsuperhuman accuracy. When the models are large enough, they can be instructed\nby prompts to solve new tasks without any fine-tuning. Moreover, they can be\napplied to a wide range of different media and problem domains, ranging from\nimage and video processing to robot control learning. Because they provide a\nblueprint for solving many tasks in artificial intelligence, they have been\ncalled Foundation Models. After a brief introduction to basic NLP models the\nmain pre-trained language models BERT, GPT and sequence-to-sequence transformer\nare described, as well as the concepts of self-attention and context-sensitive\nembedding. Then, different approaches to improving these models are discussed,\nsuch as expanding the pre-training criteria, increasing the length of input\ntexts, or including extra knowledge. An overview of the best-performing models\nfor about twenty application areas is then presented, e.g., question answering,\ntranslation, story generation, dialog systems, generating images from text,\netc. For each application area, the strengths and weaknesses of current models\nare discussed, and an outlook on further developments is given. In addition,\nlinks are provided to freely available program code. A concluding chapter\nsummarizes the economic opportunities, mitigation of risks, and potential\ndevelopments of AI.", "published": "2023-02-16 20:42:04", "link": "http://arxiv.org/abs/2302.08575v1", "categories": ["cs.CL", "cs.CV", "cs.LG", "cs.MM", "68W20, 68W25", "I.2.6; I.2.7; I.2.8; I.2.10; I.4.8; I.4.10; I.5.2; I.5.4; I.7.0;\n  J.1; J.3; K.4.1; K.4.2; K.5.0"], "primary_category": "cs.CL"}
{"title": "Exploring the Representation Manifolds of Stable Diffusion Through the\n  Lens of Intrinsic Dimension", "abstract": "Prompting has become an important mechanism by which users can more\neffectively interact with many flavors of foundation model. Indeed, the last\nseveral years have shown that well-honed prompts can sometimes unlock emergent\ncapabilities within such models. While there has been a substantial amount of\nempirical exploration of prompting within the community, relatively few works\nhave studied prompting at a mathematical level. In this work we aim to take a\nfirst step towards understanding basic geometric properties induced by prompts\nin Stable Diffusion, focusing on the intrinsic dimension of internal\nrepresentations within the model. We find that choice of prompt has a\nsubstantial impact on the intrinsic dimension of representations at both layers\nof the model which we explored, but that the nature of this impact depends on\nthe layer being considered. For example, in certain bottleneck layers of the\nmodel, intrinsic dimension of representations is correlated with prompt\nperplexity (measured using a surrogate model), while this correlation is not\napparent in the latent layers. Our evidence suggests that intrinsic dimension\ncould be a useful tool for future studies of the impact of different prompts on\ntext-to-image models.", "published": "2023-02-16 16:22:30", "link": "http://arxiv.org/abs/2302.09301v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Stabilising and accelerating light gated recurrent units for automatic\n  speech recognition", "abstract": "The light gated recurrent units (Li-GRU) is well-known for achieving\nimpressive results in automatic speech recognition (ASR) tasks while being\nlighter and faster to train than a standard gated recurrent units (GRU).\nHowever, the unbounded nature of its rectified linear unit on the candidate\nrecurrent gate induces an important gradient exploding phenomenon disrupting\nthe training process and preventing it from being applied to famous datasets.\nIn this paper, we theoretically and empirically derive the necessary conditions\nfor its stability as well as engineering mechanisms to speed up by a factor of\nfive its training time, hence introducing a novel version of this architecture\nnamed SLi-GRU. Then, we evaluate its performance both on a toy task\nillustrating its newly acquired capabilities and a set of three different ASR\ndatasets demonstrating lower word error rates compared to more complex\nrecurrent neural networks.", "published": "2023-02-16 16:18:58", "link": "http://arxiv.org/abs/2302.10144v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "E2E Spoken Entity Extraction for Virtual Agents", "abstract": "In human-computer conversations, extracting entities such as names, street\naddresses and email addresses from speech is a challenging task. In this paper,\nwe study the impact of fine-tuning pre-trained speech encoders on extracting\nspoken entities in human-readable form directly from speech without the need\nfor text transcription. We illustrate that such a direct approach optimizes the\nencoder to transcribe only the entity relevant portions of speech ignoring the\nsuperfluous portions such as carrier phrases, or spell name entities. In the\ncontext of dialog from an enterprise virtual agent, we demonstrate that the\n1-step approach outperforms the typical 2-step approach which first generates\nlexical transcriptions followed by text-based entity extraction for identifying\nspoken entities.", "published": "2023-02-16 21:45:35", "link": "http://arxiv.org/abs/2302.10186v7", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "GLUECons: A Generic Benchmark for Learning Under Constraints", "abstract": "Recent research has shown that integrating domain knowledge into deep\nlearning architectures is effective -- it helps reduce the amount of required\ndata, improves the accuracy of the models' decisions, and improves the\ninterpretability of models. However, the research community is missing a\nconvened benchmark for systematically evaluating knowledge integration methods.\nIn this work, we create a benchmark that is a collection of nine tasks in the\ndomains of natural language processing and computer vision. In all cases, we\nmodel external knowledge as constraints, specify the sources of the constraints\nfor each task, and implement various models that use these constraints. We\nreport the results of these models using a new set of extended evaluation\ncriteria in addition to the task performances for a more in-depth analysis.\nThis effort provides a framework for a more comprehensive and systematic\ncomparison of constraint integration techniques and for identifying related\nresearch challenges. It will facilitate further research for alleviating some\nproblems of state-of-the-art neural models.", "published": "2023-02-16 16:45:36", "link": "http://arxiv.org/abs/2302.10914v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "JEIT: Joint End-to-End Model and Internal Language Model Training for\n  Speech Recognition", "abstract": "We propose JEIT, a joint end-to-end (E2E) model and internal language model\n(ILM) training method to inject large-scale unpaired text into ILM during E2E\ntraining which improves rare-word speech recognition. With JEIT, the E2E model\ncomputes an E2E loss on audio-transcript pairs while its ILM estimates a\ncross-entropy loss on unpaired text. The E2E model is trained to minimize a\nweighted sum of E2E and ILM losses. During JEIT, ILM absorbs knowledge from\nunpaired text while the E2E training serves as regularization. Unlike ILM\nadaptation methods, JEIT does not require a separate adaptation step and avoids\nthe need for Kullback-Leibler divergence regularization of ILM. We also show\nthat modular hybrid autoregressive transducer (MHAT) performs better than HAT\nin the JEIT framework, and is much more robust than HAT during ILM adaptation.\nTo push the limit of unpaired text injection, we further propose a combined\nJEIT and JOIST training (CJJT) that benefits from modality matching, encoder\ntext injection and ILM training. Both JEIT and CJJT can foster a more effective\nLM fusion. With 100B unpaired sentences, JEIT/CJJT improves rare-word\nrecognition accuracy by up to 16.4% over a model trained without unpaired text.", "published": "2023-02-16 21:07:38", "link": "http://arxiv.org/abs/2302.08583v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Prompt Tuning of Deep Neural Networks for Speaker-adaptive Visual Speech\n  Recognition", "abstract": "Visual Speech Recognition (VSR) aims to infer speech into text depending on\nlip movements alone. As it focuses on visual information to model the speech,\nits performance is inherently sensitive to personal lip appearances and\nmovements, and this makes the VSR models show degraded performance when they\nare applied to unseen speakers. In this paper, to remedy the performance\ndegradation of the VSR model on unseen speakers, we propose prompt tuning\nmethods of Deep Neural Networks (DNNs) for speaker-adaptive VSR. Specifically,\nmotivated by recent advances in Natural Language Processing (NLP), we finetune\nprompts on adaptation data of target speakers instead of modifying the\npre-trained model parameters. Different from the previous prompt tuning methods\nmainly limited to Transformer variant architecture, we explore different types\nof prompts, the addition, the padding, and the concatenation form prompts that\ncan be applied to the VSR model which is composed of CNN and Transformer in\ngeneral. With the proposed prompt tuning, we show that the performance of the\npre-trained VSR model on unseen speakers can be largely improved by using a\nsmall amount of adaptation data (e.g., less than 5 minutes), even if the\npre-trained model is already developed with large speaker variations. Moreover,\nby analyzing the performance and parameters of different types of prompts, we\ninvestigate when the prompt tuning is preferred over the finetuning methods.\nThe effectiveness of the proposed method is evaluated on both word- and\nsentence-level VSR databases, LRW-ID and GRID.", "published": "2023-02-16 06:01:31", "link": "http://arxiv.org/abs/2302.08102v2", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.CL"}
{"title": "An Attention-based Approach to Hierarchical Multi-label Music Instrument\n  Classification", "abstract": "Although music is typically multi-label, many works have studied hierarchical\nmusic tagging with simplified settings such as single-label data. Moreover,\nthere lacks a framework to describe various joint training methods under the\nmulti-label setting. In order to discuss the above topics, we introduce\nhierarchical multi-label music instrument classification task. The task\nprovides a realistic setting where multi-instrument real music data is assumed.\nVarious hierarchical methods that jointly train a DNN are summarized and\nexplored in the context of the fusion of deep learning and conventional\ntechniques. For the effective joint training in the multi-label setting, we\npropose two methods to model the connection between fine- and coarse-level\ntags, where one uses rule-based grouped max-pooling, the other one uses the\nattention mechanism obtained in a data-driven manner. Our evaluation reveals\nthat the proposed methods have advantages over the method without joint\ntraining. In addition, the decision procedure within the proposed methods can\nbe interpreted by visualizing attention maps or referring to fixed rules.", "published": "2023-02-16 08:07:06", "link": "http://arxiv.org/abs/2302.08136v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DeepSpace: Dynamic Spatial and Source Cue Based Source Separation for\n  Dialog Enhancement", "abstract": "Dialog Enhancement (DE) is a feature which allows a user to increase the\nlevel of dialog in TV or movie content relative to non-dialog sounds. When only\nthe original mix is available, DE is \"unguided,\" and requires source\nseparation. In this paper, we describe the DeepSpace system, which performs\nsource separation using both dynamic spatial cues and source cues to support\nunguided DE. Its technologies include spatio-level filtering (SLF) and\ndeep-learning based dialog classification and denoising. Using subjective\nlistening tests, we show that DeepSpace demonstrates significantly improved\noverall performance relative to state-of-the-art systems available for testing.\nWe explore the feasibility of using existing automated metrics to evaluate\nunguided DE systems.", "published": "2023-02-16 10:35:42", "link": "http://arxiv.org/abs/2302.08202v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "QuickVC: Any-to-many Voice Conversion Using Inverse Short-time Fourier\n  Transform for Faster Conversion", "abstract": "With the development of automatic speech recognition (ASR) and text-to-speech\n(TTS) technology, high-quality voice conversion (VC) can be achieved by\nextracting source content information and target speaker information to\nreconstruct waveforms. However, current methods still require improvement in\nterms of inference speed. In this study, we propose a lightweight VITS-based VC\nmodel that uses the HuBERT-Soft model to extract content information features\nwithout speaker information. Through subjective and objective experiments on\nsynthesized speech, the proposed model demonstrates competitive results in\nterms of naturalness and similarity. Importantly, unlike the original VITS\nmodel, we use the inverse short-time Fourier transform (iSTFT) to replace the\nmost computationally expensive part. Experimental results show that our model\ncan generate samples at over 5000 kHz on the 3090 GPU and over 250 kHz on the\ni9-10900K CPU, achieving competitive speed for the same hardware configuration.", "published": "2023-02-16 13:49:09", "link": "http://arxiv.org/abs/2302.08296v4", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speech Enhancement with Multi-granularity Vector Quantization", "abstract": "With advances in deep learning, neural network based speech enhancement (SE)\nhas developed rapidly in the last decade. Meanwhile, the self-supervised\npre-trained model and vector quantization (VQ) have achieved excellent\nperformance on many speech-related tasks, while they are less explored on SE.\nAs it was shown in our previous work that utilizing a VQ module to discretize\nnoisy speech representations is beneficial for speech denoising, in this work\nwe therefore study the impact of using VQ at different layers with different\nnumber of codebooks. Different VQ modules indeed enable to extract\nmultiple-granularity speech features. Following an attention mechanism, the\ncontextual features extracted by a pre-trained model are fused with the local\nfeatures extracted by the encoder, such that both global and local information\nare preserved to reconstruct the enhanced speech. Experimental results on the\nValentini dataset show that the proposed model can improve the SE performance,\nwhere the impact of choosing pre-trained models is also revealed.", "published": "2023-02-16 14:53:41", "link": "http://arxiv.org/abs/2302.08342v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speaker Change Detection for Transformer Transducer ASR", "abstract": "Speaker change detection (SCD) is an important feature that improves the\nreadability of the recognized words from an automatic speech recognition (ASR)\nsystem by breaking the word sequence into paragraphs at speaker change points.\nExisting SCD solutions either require additional ensemble for the time based\ndecisions and recognized word sequences, or implement a tight integration\nbetween ASR and SCD, limiting the potential optimum performance for both tasks.\nTo address these issues, we propose a novel framework for the SCD task, where\nan additional SCD module is built on top of an existing Transformer Transducer\nASR (TT-ASR) network. Two variants of the SCD network are explored in this\nframework that naturally estimate speaker change probability for each word,\nwhile allowing the ASR and SCD to have independent optimization scheme for the\nbest performance. Experiments show that our methods can significantly improve\nthe F1 score on LibriCSS and Microsoft call center data sets without ASR\ndegradation, compared with a joint SCD and ASR baseline.", "published": "2023-02-16 19:55:53", "link": "http://arxiv.org/abs/2302.08549v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Adaptable End-to-End ASR Models using Replaceable Internal LMs and\n  Residual Softmax", "abstract": "End-to-end (E2E) automatic speech recognition (ASR) implicitly learns the\ntoken sequence distribution of paired audio-transcript training data. However,\nit still suffers from domain shifts from training to testing, and domain\nadaptation is still challenging. To alleviate this problem, this paper designs\na replaceable internal language model (RILM) method, which makes it feasible to\ndirectly replace the internal language model (LM) of E2E ASR models with a\ntarget-domain LM in the decoding stage when a domain shift is encountered.\nFurthermore, this paper proposes a residual softmax (R-softmax) that is\ndesigned for CTC-based E2E ASR models to adapt to the target domain without\nre-training during inference. For E2E ASR models trained on the LibriSpeech\ncorpus, experiments showed that the proposed methods gave a 2.6% absolute WER\nreduction on the Switchboard data and a 1.0% WER reduction on the AESRC2020\ncorpus while maintaining intra-domain ASR results.", "published": "2023-02-16 20:49:39", "link": "http://arxiv.org/abs/2302.08579v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Personalized Audio Quality Preference Prediction", "abstract": "This paper proposes to use both audio input and subject information to\npredict the personalized preference of two audio segments with the same content\nin different qualities. A siamese network is used to compare the inputs and\npredict the preference. Several different structures for each side of the\nsiamese network are investigated, and an LDNet with PANNs' CNN6 as the encoder\nand a multi-layer perceptron block as the decoder outperforms a baseline model\nusing only audio input the most, where the overall accuracy grows from 77.56%\nto 78.04%. Experimental results also show that using all the subject\ninformation, including age, gender, and the specifications of headphones or\nearphones, is more effective than using only a part of them.", "published": "2023-02-16 07:49:06", "link": "http://arxiv.org/abs/2302.08130v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ACE-VC: Adaptive and Controllable Voice Conversion using Explicitly\n  Disentangled Self-supervised Speech Representations", "abstract": "In this work, we propose a zero-shot voice conversion method using speech\nrepresentations trained with self-supervised learning. First, we develop a\nmulti-task model to decompose a speech utterance into features such as\nlinguistic content, speaker characteristics, and speaking style. To disentangle\ncontent and speaker representations, we propose a training strategy based on\nSiamese networks that encourages similarity between the content representations\nof the original and pitch-shifted audio. Next, we develop a synthesis model\nwith pitch and duration predictors that can effectively reconstruct the speech\nsignal from its decomposed representation. Our framework allows controllable\nand speaker-adaptive synthesis to perform zero-shot any-to-any voice conversion\nachieving state-of-the-art results on metrics evaluating speaker similarity,\nintelligibility, and naturalness. Using just 10 seconds of data for a target\nspeaker, our framework can perform voice swapping and achieves a speaker\nverification EER of 5.5% for seen speakers and 8.4% for unseen speakers.", "published": "2023-02-16 08:10:41", "link": "http://arxiv.org/abs/2302.08137v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Adaptive Axonal Delays in feedforward spiking neural networks for\n  accurate spoken word recognition", "abstract": "Spiking neural networks (SNN) are a promising research avenue for building\naccurate and efficient automatic speech recognition systems. Recent advances in\naudio-to-spike encoding and training algorithms enable SNN to be applied in\npractical tasks. Biologically-inspired SNN communicates using sparse\nasynchronous events. Therefore, spike-timing is critical to SNN performance. In\nthis aspect, most works focus on training synaptic weights and few have\nconsidered delays in event transmission, namely axonal delay. In this work, we\nconsider a learnable axonal delay capped at a maximum value, which can be\nadapted according to the axonal delay distribution in each network layer. We\nshow that our proposed method achieves the best classification results reported\non the SHD dataset (92.45%) and NTIDIGITS dataset (95.09%). Our work\nillustrates the potential of training axonal delays for tasks with complex\ntemporal structures.", "published": "2023-02-16 22:19:04", "link": "http://arxiv.org/abs/2302.08607v1", "categories": ["cs.NE", "cs.SD", "eess.AS"], "primary_category": "cs.NE"}
