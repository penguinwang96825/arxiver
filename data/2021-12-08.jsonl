{"title": "Learning to Select the Next Reasonable Mention for Entity Linking", "abstract": "Entity linking aims to establish a link between entity mentions in a document\nand the corresponding entities in knowledge graphs (KGs). Previous work has\nshown the effectiveness of global coherence for entity linking. However, most\nof the existing global linking methods based on sequential decisions focus on\nhow to utilize previously linked entities to enhance the later decisions. In\nthose methods, the order of mention is fixed, making the model unable to adjust\nthe subsequent linking targets according to the previously linked results,\nwhich will cause the previous information to be unreasonably utilized. To\naddress the problem, we propose a novel model, called DyMen, to dynamically\nadjust the subsequent linking target based on the previously linked entities\nvia reinforcement learning, enabling the model to select a link target that can\nfully use previously linked information. We sample mention by sliding window to\nreduce the action sampling space of reinforcement learning and maintain the\nsemantic coherence of mention. Experiments conducted on several benchmark\ndatasets have shown the effectiveness of the proposed model.", "published": "2021-12-08 04:12:50", "link": "http://arxiv.org/abs/2112.04104v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bidimensional Leaderboards: Generate and Evaluate Language Hand in Hand", "abstract": "Natural language processing researchers have identified limitations of\nevaluation methodology for generation tasks, with new questions raised about\nthe validity of automatic metrics and of crowdworker judgments. Meanwhile,\nefforts to improve generation models tend to depend on simple n-gram overlap\nmetrics (e.g., BLEU, ROUGE). We argue that new advances on models and metrics\nshould each more directly benefit and inform the other. We therefore propose a\ngeneralization of leaderboards, bidimensional leaderboards (Billboards), that\nsimultaneously tracks progress in language generation models and metrics for\ntheir evaluation. Unlike conventional unidimensional leaderboards that sort\nsubmitted systems by predetermined metrics, a Billboard accepts both generators\nand evaluation metrics as competing entries. A Billboard automatically creates\nan ensemble metric that selects and linearly combines a few metrics based on a\nglobal analysis across generators. Further, metrics are ranked based on their\ncorrelation with human judgments. We release four Billboards for machine\ntranslation, summarization, and image captioning. We demonstrate that a linear\nensemble of a few diverse metrics sometimes substantially outperforms existing\nmetrics in isolation. Our mixed-effects model analysis shows that most\nautomatic metrics, especially the reference-based ones, overrate machine over\nhuman generation, demonstrating the importance of updating metrics as\ngeneration models become stronger (and perhaps more similar to humans) in the\nfuture. Our project website is available at\nhttps://nlp.cs.washington.edu/billboard/.", "published": "2021-12-08 06:34:58", "link": "http://arxiv.org/abs/2112.04139v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "VIRT: Improving Representation-based Models for Text Matching through\n  Virtual Interaction", "abstract": "With the booming of pre-trained transformers, representation-based models\nbased on Siamese transformer encoders have become mainstream techniques for\nefficient text matching. However, these models suffer from severe performance\ndegradation due to the lack of interaction between the text pair, compared with\ninteraction-based models. Prior arts attempt to address this through performing\nextra interaction for Siamese encoded representations, while the interaction\nduring encoding is still ignored. To remedy this, we propose a \\textit{Virtual}\nInteRacTion mechanism (VIRT) to transfer interactive knowledge from\ninteraction-based models into Siamese encoders through attention map\ndistillation. As a train-time-only component, VIRT could completely maintain\nthe high efficiency of the Siamese structure and brings no extra computation\ncost during inference. To fully utilize the learned interactive knowledge, we\nfurther design a VIRT-adapted interaction strategy. Experimental results on\nmultiple text matching datasets demonstrate that our method outperforms\nstate-of-the-art representation-based models. What's more, VIRT can be easily\nintegrated into existing representation-based methods to achieve further\nimprovements.", "published": "2021-12-08 09:49:28", "link": "http://arxiv.org/abs/2112.04195v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "JABER and SABER: Junior and Senior Arabic BERt", "abstract": "Language-specific pre-trained models have proven to be more accurate than\nmultilingual ones in a monolingual evaluation setting, Arabic is no exception.\nHowever, we found that previously released Arabic BERT models were\nsignificantly under-trained. In this technical report, we present JABER and\nSABER, Junior and Senior Arabic BERt respectively, our pre-trained language\nmodel prototypes dedicated for Arabic. We conduct an empirical study to\nsystematically evaluate the performance of models across a diverse set of\nexisting Arabic NLU tasks. Experimental results show that JABER and SABER\nachieve state-of-the-art performances on ALUE, a new benchmark for Arabic\nLanguage Understanding Evaluation, as well as on a well-established NER\nbenchmark.", "published": "2021-12-08 15:19:24", "link": "http://arxiv.org/abs/2112.04329v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prompt-based Zero-shot Relation Extraction with Semantic Knowledge\n  Augmentation", "abstract": "In relation triplet extraction (RTE), recognizing unseen relations for which\nthere are no training instances is a challenging task. Efforts have been made\nto recognize unseen relations based on question-answering models or relation\ndescriptions. However, these approaches miss the semantic information about\nconnections between seen and unseen relations. In this paper, We propose a\nprompt-based model with semantic knowledge augmentation (ZS-SKA) to recognize\nunseen relations under the zero-shot setting. We present a new word-level\nanalogy-based sentence translation rule and generate augmented instances with\nunseen relations from instances with seen relations using that new rule. We\ndesign prompts with weighted virtual label construction based on an external\nknowledge graph to integrate semantic knowledge information learned from seen\nrelations. Instead of using the actual label sets in the prompt template, we\nconstruct weighted virtual label words. We learn the representations of both\nseen and unseen relations with augmented instances and prompts. We then\ncalculate the distance between the generated representations using prototypical\nnetworks to predict unseen relations. Extensive experiments conducted on three\npublic datasets FewRel, Wiki-ZSL, and NYT, show that ZS-SKA outperforms other\nmethods under zero-shot setting. Results also demonstrate the effectiveness and\nrobustness of ZS-SKA.", "published": "2021-12-08 19:34:27", "link": "http://arxiv.org/abs/2112.04539v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FreeTalky: Don't Be Afraid! Conversations Made Easier by a Humanoid\n  Robot using Persona-based Dialogue", "abstract": "We propose a deep learning-based foreign language learning platform, named\nFreeTalky, for people who experience anxiety dealing with foreign languages, by\nemploying a humanoid robot NAO and various deep learning models. A\npersona-based dialogue system that is embedded in NAO provides an interesting\nand consistent multi-turn dialogue for users. Also, an grammar error correction\nsystem promotes improvement in grammar skills of the users. Thus, our system\nenables personalized learning based on persona dialogue and facilitates grammar\nlearning of a user using grammar error feedback. Furthermore, we verified\nwhether FreeTalky provides practical help in alleviating xenoglossophobia by\nreplacing the real human in the conversation with a NAO robot, through human\nevaluation.", "published": "2021-12-08 05:48:11", "link": "http://arxiv.org/abs/2112.04126v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Recommendation as Language Modeling", "abstract": "Recommendation is the task of ranking items (e.g. movies or products)\naccording to individual user needs. Current systems rely on collaborative\nfiltering and content-based techniques, which both require structured training\ndata. We propose a framework for recommendation with off-the-shelf pretrained\nlanguage models (LM) that only used unstructured text corpora as training data.\nIf a user $u$ liked \\textit{Matrix} and \\textit{Inception}, we construct a\ntextual prompt, e.g. \\textit{\"Movies like Matrix, Inception, ${<}m{>}$\"} to\nestimate the affinity between $u$ and $m$ with LM likelihood. We motivate our\nidea with a corpus analysis, evaluate several prompt structures, and we compare\nLM-based recommendation with standard matrix factorization trained on different\ndata regimes. The code for our experiments is publicly available\n(https://colab.research.google.com/drive/1f1mlZ-FGaLGdo5rPzxf3vemKllbh2esT?usp=sharing).", "published": "2021-12-08 09:16:03", "link": "http://arxiv.org/abs/2112.04184v1", "categories": ["cs.CL", "cs.IR", "I.2.7; H.3.3; I.2.6"], "primary_category": "cs.CL"}
{"title": "Transformer-Based Approach for Joint Handwriting and Named Entity\n  Recognition in Historical documents", "abstract": "The extraction of relevant information carried out by named entities in\nhandwriting documents is still a challenging task. Unlike traditional\ninformation extraction approaches that usually face text transcription and\nnamed entity recognition as separate subsequent tasks, we propose in this paper\nan end-to-end transformer-based approach to jointly perform these two tasks.\nThe proposed approach operates at the paragraph level, which brings two main\nbenefits. First, it allows the model to avoid unrecoverable early errors due to\nline segmentation. Second, it allows the model to exploit larger bi-dimensional\ncontext information to identify the semantic categories, reaching a higher\nfinal prediction accuracy. We also explore different training scenarios to show\ntheir effect on the performance and we demonstrate that a two-stage learning\nstrategy can make the model reach a higher final prediction accuracy. As far as\nwe know, this work presents the first approach that adopts the transformer\nnetworks for named entity recognition in handwritten documents. We achieve the\nnew state-of-the-art performance in the ICDAR 2017 Information Extraction\ncompetition using the Esposalles database, for the complete task, even though\nthe proposed technique does not use any dictionaries, language modeling, or\npost-processing.", "published": "2021-12-08 09:26:21", "link": "http://arxiv.org/abs/2112.04189v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Improving language models by retrieving from trillions of tokens", "abstract": "We enhance auto-regressive language models by conditioning on document chunks\nretrieved from a large corpus, based on local similarity with preceding tokens.\nWith a $2$ trillion token database, our Retrieval-Enhanced Transformer (RETRO)\nobtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite\nusing 25$\\times$ fewer parameters. After fine-tuning, RETRO performance\ntranslates to downstream knowledge-intensive tasks such as question answering.\nRETRO combines a frozen Bert retriever, a differentiable encoder and a chunked\ncross-attention mechanism to predict tokens based on an order of magnitude more\ndata than what is typically consumed during training. We typically train RETRO\nfrom scratch, yet can also rapidly RETROfit pre-trained transformers with\nretrieval and still achieve good performance. Our work opens up new avenues for\nimproving language models through explicit memory at unprecedented scale.", "published": "2021-12-08 17:32:34", "link": "http://arxiv.org/abs/2112.04426v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Prompting Visual-Language Models for Efficient Video Understanding", "abstract": "Image-based visual-language (I-VL) pre-training has shown great success for\nlearning joint visual-textual representations from large-scale web data,\nrevealing remarkable ability for zero-shot generalisation. This paper presents\na simple but strong baseline to efficiently adapt the pre-trained I-VL model,\nand exploit its powerful ability for resource-hungry video understanding tasks,\nwith minimal training. Specifically, we propose to optimise a few random\nvectors, termed as continuous prompt vectors, that convert video-related tasks\ninto the same format as the pre-training objectives. In addition, to bridge the\ngap between static images and videos, temporal information is encoded with\nlightweight Transformers stacking on top of frame-wise visual features.\nExperimentally, we conduct extensive ablation studies to analyse the critical\ncomponents. On 10 public benchmarks of action recognition, action localisation,\nand text-video retrieval, across closed-set, few-shot, and zero-shot scenarios,\nwe achieve competitive or state-of-the-art performance to existing methods,\ndespite optimising significantly fewer parameters.", "published": "2021-12-08 18:58:16", "link": "http://arxiv.org/abs/2112.04478v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "FLAVA: A Foundational Language And Vision Alignment Model", "abstract": "State-of-the-art vision and vision-and-language models rely on large-scale\nvisio-linguistic pretraining for obtaining good performance on a variety of\ndownstream tasks. Generally, such models are often either cross-modal\n(contrastive) or multi-modal (with earlier fusion) but not both; and they often\nonly target specific modalities or tasks. A promising direction would be to use\na single holistic universal model, as a \"foundation\", that targets all\nmodalities at once -- a true vision and language foundation model should be\ngood at vision tasks, language tasks, and cross- and multi-modal vision and\nlanguage tasks. We introduce FLAVA as such a model and demonstrate impressive\nperformance on a wide range of 35 tasks spanning these target modalities.", "published": "2021-12-08 18:59:16", "link": "http://arxiv.org/abs/2112.04482v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "ADBCMM : Acronym Disambiguation by Building Counterfactuals and\n  Multilingual Mixing", "abstract": "Scientific documents often contain a large number of acronyms. Disambiguation\nof these acronyms will help researchers better understand the meaning of\nvocabulary in the documents. In the past, thanks to large amounts of data from\nEnglish literature, acronym task was mainly applied in English literature.\nHowever, for other low-resource languages, this task is difficult to obtain\ngood performance and receives less attention due to the lack of large amount of\nannotation data. To address the above issue, this paper proposes an new method\nfor acronym disambiguation, named as ADBCMM, which can significantly improve\nthe performance of low-resource languages by building counterfactuals and\nmultilingual mixing. Specifically, by balancing data bias in low-resource\nlangauge, ADBCMM will able to improve the test performance outside the data\nset. In SDU@AAAI-22 - Shared Task 2: Acronym Disambiguation, the proposed\nmethod won first place in French and Spanish. You can repeat our results here\nhttps://github.com/WENGSYX/ADBCMM.", "published": "2021-12-08 15:08:27", "link": "http://arxiv.org/abs/2112.08991v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Scaling Language Models: Methods, Analysis & Insights from Training\n  Gopher", "abstract": "Language modelling provides a step towards intelligent communication systems\nby harnessing large repositories of written human knowledge to better predict\nand understand the world. In this paper, we present an analysis of\nTransformer-based language model performance across a wide range of model\nscales -- from models with tens of millions of parameters up to a 280 billion\nparameter model called Gopher. These models are evaluated on 152 diverse tasks,\nachieving state-of-the-art performance across the majority. Gains from scale\nare largest in areas such as reading comprehension, fact-checking, and the\nidentification of toxic language, but logical and mathematical reasoning see\nless benefit. We provide a holistic analysis of the training dataset and\nmodel's behaviour, covering the intersection of model scale with bias and\ntoxicity. Finally we discuss the application of language models to AI safety\nand the mitigation of downstream harms.", "published": "2021-12-08 19:41:47", "link": "http://arxiv.org/abs/2112.11446v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Contrastive Instruction-Trajectory Learning for Vision-Language\n  Navigation", "abstract": "The vision-language navigation (VLN) task requires an agent to reach a target\nwith the guidance of natural language instruction. Previous works learn to\nnavigate step-by-step following an instruction. However, these works may fail\nto discriminate the similarities and discrepancies across\ninstruction-trajectory pairs and ignore the temporal continuity of\nsub-instructions. These problems hinder agents from learning distinctive\nvision-and-language representations, harming the robustness and\ngeneralizability of the navigation policy. In this paper, we propose a\nContrastive Instruction-Trajectory Learning (CITL) framework that explores\ninvariance across similar data samples and variance across different ones to\nlearn distinctive representations for robust navigation. Specifically, we\npropose: (1) a coarse-grained contrastive learning objective to enhance\nvision-and-language representations by contrasting semantics of full trajectory\nobservations and instructions, respectively; (2) a fine-grained contrastive\nlearning objective to perceive instructions by leveraging the temporal\ninformation of the sub-instructions; (3) a pairwise sample-reweighting\nmechanism for contrastive learning to mine hard samples and hence mitigate the\ninfluence of data sampling bias in contrastive learning. Our CITL can be easily\nintegrated with VLN backbones to form a new learning paradigm and achieve\nbetter generalizability in unseen environments. Extensive experiments show that\nthe model with CITL surpasses the previous state-of-the-art methods on R2R,\nR4R, and RxR.", "published": "2021-12-08 06:32:52", "link": "http://arxiv.org/abs/2112.04138v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "A study on native American English speech recognition by Indian\n  listeners with varying word familiarity level", "abstract": "In this study, listeners of varied Indian nativities are asked to listen and\nrecognize TIMIT utterances spoken by American speakers. We have three kinds of\nresponses from each listener while they recognize an utterance: 1. Sentence\ndifficulty ratings, 2. Speaker difficulty ratings, and 3. Transcription of the\nutterance. From these transcriptions, word error rate (WER) is calculated and\nused as a metric to evaluate the similarity between the recognized and the\noriginal sentences.The sentences selected in this study are categorized into\nthree groups: Easy, Medium and Hard, based on the frequency ofoccurrence of the\nwords in them. We observe that the sentence, speaker difficulty ratings and the\nWERs increase from easy to hard categories of sentences. We also compare the\nhuman speech recognition performance with that using three automatic speech\nrecognition (ASR) under following three combinations of acoustic model (AM) and\nlanguage model(LM): ASR1) AM trained with recordings from speakers of Indian\norigin and LM built on TIMIT text, ASR2) AM using recordings from native\nAmerican speakers and LM built ontext from LIBRI speech corpus, and ASR3) AM\nusing recordings from native American speakers and LM build on LIBRI speech and\nTIMIT text. We observe that HSR performance is similar to that of ASR1 whereas\nASR3 achieves the best performance. Speaker nativity wise analysis shows that\nutterances from speakers of some nativity are more difficult to recognize by\nIndian listeners compared to few other nativities", "published": "2021-12-08 07:43:38", "link": "http://arxiv.org/abs/2112.04151v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SNEAK: Synonymous Sentences-Aware Adversarial Attack on Natural Language\n  Video Localization", "abstract": "Natural language video localization (NLVL) is an important task in the\nvision-language understanding area, which calls for an in-depth understanding\nof not only computer vision and natural language side alone, but more\nimportantly the interplay between both sides. Adversarial vulnerability has\nbeen well-recognized as a critical security issue of deep neural network\nmodels, which requires prudent investigation. Despite its extensive yet\nseparated studies in video and language tasks, current understanding of the\nadversarial robustness in vision-language joint tasks like NLVL is less\ndeveloped. This paper therefore aims to comprehensively investigate the\nadversarial robustness of NLVL models by examining three facets of\nvulnerabilities from both attack and defense aspects. To achieve the attack\ngoal, we propose a new adversarial attack paradigm called synonymous\nsentences-aware adversarial attack on NLVL (SNEAK), which captures the\ncross-modality interplay between the vision and language sides.", "published": "2021-12-08 07:54:03", "link": "http://arxiv.org/abs/2112.04154v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Does Structure Matter? Leveraging Data-to-Text Generation for Answering\n  Complex Information Needs", "abstract": "In this work, our aim is to provide a structured answer in natural language\nto a complex information need. Particularly, we envision using generative\nmodels from the perspective of data-to-text generation. We propose the use of a\ncontent selection and planning pipeline which aims at structuring the answer by\ngenerating intermediate plans. The experimental evaluation is performed using\nthe TREC Complex Answer Retrieval (CAR) dataset. We evaluate both the generated\nanswer and its corresponding structure and show the effectiveness of\nplanning-based models in comparison to a text-to-text model.", "published": "2021-12-08 15:51:27", "link": "http://arxiv.org/abs/2112.04344v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Ethical and social risks of harm from Language Models", "abstract": "This paper aims to help structure the risk landscape associated with\nlarge-scale Language Models (LMs). In order to foster advances in responsible\ninnovation, an in-depth understanding of the potential risks posed by these\nmodels is needed. A wide range of established and anticipated risks are\nanalysed in detail, drawing on multidisciplinary expertise and literature from\ncomputer science, linguistics, and social sciences.\n  We outline six specific risk areas: I. Discrimination, Exclusion and\nToxicity, II. Information Hazards, III. Misinformation Harms, V. Malicious\nUses, V. Human-Computer Interaction Harms, VI. Automation, Access, and\nEnvironmental Harms. The first area concerns the perpetuation of stereotypes,\nunfair discrimination, exclusionary norms, toxic language, and lower\nperformance by social group for LMs. The second focuses on risks from private\ndata leaks or LMs correctly inferring sensitive information. The third\naddresses risks arising from poor, false or misleading information including in\nsensitive domains, and knock-on risks such as the erosion of trust in shared\ninformation. The fourth considers risks from actors who try to use LMs to cause\nharm. The fifth focuses on risks specific to LLMs used to underpin\nconversational agents that interact with human users, including unsafe use,\nmanipulation or deception. The sixth discusses the risk of environmental harm,\njob automation, and other challenges that may have a disparate effect on\ndifferent social groups or communities.\n  In total, we review 21 risks in-depth. We discuss the points of origin of\ndifferent risks and point to potential mitigation approaches. Lastly, we\ndiscuss organisational responsibilities in implementing mitigations, and the\nrole of collaboration and participation. We highlight directions for further\nresearch, particularly on expanding the toolkit for assessing and evaluating\nthe outlined risks in LMs.", "published": "2021-12-08 16:09:48", "link": "http://arxiv.org/abs/2112.04359v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Everything at Once -- Multi-modal Fusion Transformer for Video Retrieval", "abstract": "Multi-modal learning from video data has seen increased attention recently as\nit allows to train semantically meaningful embeddings without human annotation\nenabling tasks like zero-shot retrieval and classification. In this work, we\npresent a multi-modal, modality agnostic fusion transformer approach that\nlearns to exchange information between multiple modalities, such as video,\naudio, and text, and integrate them into a joined multi-modal representation to\nobtain an embedding that aggregates multi-modal temporal information. We\npropose to train the system with a combinatorial loss on everything at once,\nsingle modalities as well as pairs of modalities, explicitly leaving out any\nadd-ons such as position or modality encoding. At test time, the resulting\nmodel can process and fuse any number of input modalities. Moreover, the\nimplicit properties of the transformer allow to process inputs of different\nlengths. To evaluate the proposed approach, we train the model on the large\nscale HowTo100M dataset and evaluate the resulting embedding space on four\nchallenging benchmark datasets obtaining state-of-the-art results in zero-shot\nvideo retrieval and zero-shot video action localization.", "published": "2021-12-08 18:14:57", "link": "http://arxiv.org/abs/2112.04446v2", "categories": ["cs.CV", "cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "MLP Architectures for Vision-and-Language Modeling: An Empirical Study", "abstract": "We initiate the first empirical study on the use of MLP architectures for\nvision-and-language (VL) fusion. Through extensive experiments on 5 VL tasks\nand 5 robust VQA benchmarks, we find that: (i) Without pre-training, using MLPs\nfor multimodal fusion has a noticeable performance gap compared to\ntransformers; (ii) However, VL pre-training can help close the performance gap;\n(iii) Instead of heavy multi-head attention, adding tiny one-head attention to\nMLPs is sufficient to achieve comparable performance to transformers. Moreover,\nwe also find that the performance gap between MLPs and transformers is not\nwidened when being evaluated on the harder robust VQA benchmarks, suggesting\nusing MLPs for VL fusion can generalize roughly to a similar degree as using\ntransformers. These results hint that MLPs can effectively learn to align\nvision and text features extracted from lower-level encoders without heavy\nreliance on self-attention. Based on this, we ask an even bolder question: can\nwe have an all-MLP architecture for VL modeling, where both VL fusion and the\nvision encoder are replaced with MLPs? Our result shows that an all-MLP VL\nmodel is sub-optimal compared to state-of-the-art full-featured VL models when\nboth of them get pre-trained. However, pre-training an all-MLP can surprisingly\nachieve a better average score than full-featured transformer models without\npre-training. This indicates the potential of large-scale pre-training of\nMLP-like architectures for VL modeling and inspires the future research\ndirection on simplifying well-established VL modeling with less inductive\ndesign bias. Our code is publicly available at:\nhttps://github.com/easonnie/mlp-vil", "published": "2021-12-08 18:26:19", "link": "http://arxiv.org/abs/2112.04453v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Learning music audio representations via weak language supervision", "abstract": "Audio representations for music information retrieval are typically learned\nvia supervised learning in a task-specific fashion. Although effective at\nproducing state-of-the-art results, this scheme lacks flexibility with respect\nto the range of applications a model can have and requires extensively\nannotated datasets. In this work, we pose the question of whether it may be\npossible to exploit weakly aligned text as the only supervisory signal to learn\ngeneral-purpose music audio representations. To address this question, we\ndesign a multimodal architecture for music and language pre-training (MuLaP)\noptimised via a set of proxy tasks. Weak supervision is provided in the form of\nnoisy natural language descriptions conveying the overall musical content of\nthe track. After pre-training, we transfer the audio backbone of the model to a\nset of music audio classification and regression tasks. We demonstrate the\nusefulness of our approach by comparing the performance of audio\nrepresentations produced by the same audio backbone with different training\nstrategies and show that our pre-training method consistently achieves\ncomparable or higher scores on all tasks and datasets considered. Our\nexperiments also confirm that MuLaP effectively leverages audio-caption pairs\nto learn representations that are competitive with audio-only and cross-modal\nself-supervised methods in the literature.", "published": "2021-12-08 10:30:52", "link": "http://arxiv.org/abs/2112.04214v2", "categories": ["cs.SD", "cs.CL", "cs.IR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio-Visual Synchronisation in the wild", "abstract": "In this paper, we consider the problem of audio-visual synchronisation\napplied to videos `in-the-wild' (ie of general classes beyond speech). As a new\ntask, we identify and curate a test set with high audio-visual correlation,\nnamely VGG-Sound Sync. We compare a number of transformer-based architectural\nvariants specifically designed to model audio and visual signals of arbitrary\nlength, while significantly reducing memory requirements during training. We\nfurther conduct an in-depth analysis on the curated dataset and define an\nevaluation metric for open domain audio-visual synchronisation. We apply our\nmethod on standard lip reading speech benchmarks, LRS2 and LRS3, with ablations\non various aspects. Finally, we set the first benchmark for general\naudio-visual synchronisation with over 160 diverse classes in the new VGG-Sound\nSync video dataset. In all cases, our proposed model outperforms the previous\nstate-of-the-art by a significant margin.", "published": "2021-12-08 17:50:26", "link": "http://arxiv.org/abs/2112.04432v1", "categories": ["cs.CV", "eess.AS"], "primary_category": "cs.CV"}
{"title": "NICE-Beam: Neural Integrated Covariance Estimators for Time-Varying\n  Beamformers", "abstract": "Estimating a time-varying spatial covariance matrix for a beamforming\nalgorithm is a challenging task, especially for wearable devices, as the\nalgorithm must compensate for time-varying signal statistics due to rapid\npose-changes. In this paper, we propose Neural Integrated Covariance Estimators\nfor Beamformers, NICE-Beam. NICE-Beam is a general technique for learning how\nto estimate time-varying spatial covariance matrices, which we apply to joint\nspeech enhancement and dereverberation. It is based on training a neural\nnetwork module to non-linearly track and leverage scene information across\ntime. We integrate our solution into a beamforming pipeline, which enables\nsimple training, faster than real-time inference, and a variety of test-time\nadaptation options. We evaluate the proposed model against a suite of baselines\nin scenes with both stationary and moving microphones. Our results show that\nthe proposed method can outperform a hand-tuned estimator, despite the\nhand-tuned estimator using oracle source separation knowledge.", "published": "2021-12-08 22:48:06", "link": "http://arxiv.org/abs/2112.04613v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Training Robust Zero-Shot Voice Conversion Models with Self-supervised\n  Features", "abstract": "Unsupervised Zero-Shot Voice Conversion (VC) aims to modify the speaker\ncharacteristic of an utterance to match an unseen target speaker without\nrelying on parallel training data. Recently, self-supervised learning of speech\nrepresentation has been shown to produce useful linguistic units without using\ntranscripts, which can be directly passed to a VC model. In this paper, we\nshowed that high-quality audio samples can be achieved by using a length\nresampling decoder, which enables the VC model to work in conjunction with\ndifferent linguistic feature extractors and vocoders without requiring them to\noperate on the same sequence length. We showed that our method can outperform\nmany baselines on the VCTK dataset. Without modifying the architecture, we\nfurther demonstrated that a) using pairs of different audio segments from the\nsame speaker, b) adding a cycle consistency loss, and c) adding a speaker\nclassification loss can help to learn a better speaker embedding. Our model\ntrained on LibriTTS using these techniques achieves the best performance,\nproducing audio samples transferred well to the target speaker's voice, while\npreserving the linguistic content that is comparable with actual human\nutterances in terms of Character Error Rate.", "published": "2021-12-08 17:27:39", "link": "http://arxiv.org/abs/2112.04424v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Self-Supervised Speaker Verification with Simple Siamese Network and\n  Self-Supervised Regularization", "abstract": "Training speaker-discriminative and robust speaker verification systems\nwithout speaker labels is still challenging and worthwhile to explore. In this\nstudy, we propose an effective self-supervised learning framework and a novel\nregularization strategy to facilitate self-supervised speaker representation\nlearning. Different from contrastive learning-based self-supervised learning\nmethods, the proposed self-supervised regularization (SSReg) focuses\nexclusively on the similarity between the latent representations of positive\ndata pairs. We also explore the effectiveness of alternative online data\naugmentation strategies on both the time domain and frequency domain. With our\nstrong online data augmentation strategy, the proposed SSReg shows the\npotential of self-supervised learning without using negative pairs and it can\nsignificantly improve the performance of self-supervised speaker representation\nlearning with a simple Siamese network architecture. Comprehensive experiments\non the VoxCeleb datasets demonstrate that our proposed self-supervised approach\nobtains a 23.4% relative improvement by adding the effective self-supervised\nregularization and outperforms other previous works.", "published": "2021-12-08 18:41:19", "link": "http://arxiv.org/abs/2112.04459v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "End-to-end Alexa Device Arbitration", "abstract": "We introduce a variant of the speaker localization problem, which we call\ndevice arbitration. In the device arbitration problem, a user utters a keyword\nthat is detected by multiple distributed microphone arrays (smart home\ndevices), and we want to determine which device was closest to the user. Rather\nthan solving the full localization problem, we propose an end-to-end machine\nlearning system. This system learns a feature embedding that is computed\nindependently on each device. The embeddings from each device are then\naggregated together to produce the final arbitration decision. We use a\nlarge-scale room simulation to generate training and evaluation data, and\ncompare our system against a signal processing baseline.", "published": "2021-12-08 16:43:13", "link": "http://arxiv.org/abs/2112.04914v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
