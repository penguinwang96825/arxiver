{"title": "Information Density as a Factor for Variation in the Embedding of\n  Relative Clauses", "abstract": "In German, relative clauses can be positioned in-situ or extraposed. A\npotential factor for the variation might be information density. In this study,\nthis hypothesis is tested with a corpus of 17th century German funeral sermons.\nFor each referent in the relative clauses and their matrix clauses, the\nattention state was determined (first calculation). In a second calculation,\nfor each word the surprisal values were determined, using a bi-gram language\nmodel. In a third calculation, the surprisal values were accommodated as to\nwhether it is the first occurrence of the word in question or not. All three\ncalculations pointed in the same direction: With in-situ relative clauses, the\nrate of new referents was lower and the average surprisal values were lower,\nespecially the accommodated surprisal values, than with extraposed relative\nclauses. This indicated that in-formation density is a factor governing the\nchoice between in-situ and extraposed relative clauses. The study also sheds\nlight on the intrinsic relation-ship between the information theoretic concept\nof information density and in-formation structural concepts such as givenness\nwhich are used under a more linguistic perspective.", "published": "2017-05-18 08:16:20", "link": "http://arxiv.org/abs/1705.06457v1", "categories": ["cs.CL", "94"], "primary_category": "cs.CL"}
{"title": "Universal Dependencies Parsing for Colloquial Singaporean English", "abstract": "Singlish can be interesting to the ACL community both linguistically as a\nmajor creole based on English, and computationally for information extraction\nand sentiment analysis of regional social media. We investigate dependency\nparsing of Singlish by constructing a dependency treebank under the Universal\nDependencies scheme, and then training a neural network model by integrating\nEnglish syntactic knowledge into a state-of-the-art parser trained on the\nSinglish treebank. Results show that English knowledge can lead to 25% relative\nerror reduction, resulting in a parser of 84.47% accuracies. To the best of our\nknowledge, we are the first to use neural stacking to improve cross-lingual\ndependency parsing on low-resource languages. We make both our annotation and\nparser available for further research.", "published": "2017-05-18 08:27:42", "link": "http://arxiv.org/abs/1705.06463v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ParlAI: A Dialog Research Software Platform", "abstract": "We introduce ParlAI (pronounced \"par-lay\"), an open-source software platform\nfor dialog research implemented in Python, available at http://parl.ai. Its\ngoal is to provide a unified framework for sharing, training and testing of\ndialog models, integration of Amazon Mechanical Turk for data collection, human\nevaluation, and online/reinforcement learning; and a repository of machine\nlearning models for comparing with others' models, and improving upon existing\narchitectures. Over 20 tasks are supported in the first release, including\npopular datasets such as SQuAD, bAbI tasks, MCTest, WikiQA, QACNN, QADailyMail,\nCBT, bAbI Dialog, Ubuntu, OpenSubtitles and VQA. Several models are integrated,\nincluding neural models such as memory networks, seq2seq and attentive LSTMs.", "published": "2017-05-18 08:54:47", "link": "http://arxiv.org/abs/1705.06476v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning a bidirectional mapping between human whole-body motion and\n  natural language using deep recurrent neural networks", "abstract": "Linking human whole-body motion and natural language is of great interest for\nthe generation of semantic representations of observed human behaviors as well\nas for the generation of robot behaviors based on natural language input. While\nthere has been a large body of research in this area, most approaches that\nexist today require a symbolic representation of motions (e.g. in the form of\nmotion primitives), which have to be defined a-priori or require complex\nsegmentation algorithms. In contrast, recent advances in the field of neural\nnetworks and especially deep learning have demonstrated that sub-symbolic\nrepresentations that can be learned end-to-end usually outperform more\ntraditional approaches, for applications such as machine translation. In this\npaper we propose a generative model that learns a bidirectional mapping between\nhuman whole-body motion and natural language using deep recurrent neural\nnetworks (RNNs) and sequence-to-sequence learning. Our approach does not\nrequire any segmentation or manual feature engineering and learns a distributed\nrepresentation, which is shared for all motions and descriptions. We evaluate\nour approach on 2,846 human whole-body motions and 6,187 natural language\ndescriptions thereof from the KIT Motion-Language Dataset. Our results clearly\ndemonstrate the effectiveness of the proposed model: We show that our model\ngenerates a wide variety of realistic motions only from descriptions thereof in\nform of a single sentence. Conversely, our model is also capable of generating\ncorrect and detailed natural language descriptions from human motions.", "published": "2017-05-18 02:50:40", "link": "http://arxiv.org/abs/1705.06400v2", "categories": ["cs.LG", "cs.CL", "cs.RO", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Entropic selection of concepts unveils hidden topics in documents\n  corpora", "abstract": "The organization and evolution of science has recently become itself an\nobject of scientific quantitative investigation, thanks to the wealth of\ninformation that can be extracted from scientific documents, such as citations\nbetween papers and co-authorship between researchers. However, only few studies\nhave focused on the concepts that characterize full documents and that can be\nextracted and analyzed, revealing the deeper organization of scientific\nknowledge. Unfortunately, several concepts can be so common across documents\nthat they hinder the emergence of the underlying topical structure of the\ndocument corpus, because they give rise to a large amount of spurious and\ntrivial relations among documents. To identify and remove common concepts, we\nintroduce a method to gauge their relevance according to an objective\ninformation-theoretic measure related to the statistics of their occurrence\nacross the document corpus. After progressively removing concepts that,\naccording to this metric, can be considered as generic, we find that the topic\norganization displays a correspondingly more refined structure.", "published": "2017-05-18 10:24:03", "link": "http://arxiv.org/abs/1705.06510v2", "categories": ["physics.soc-ph", "cs.CL", "cs.DL", "cs.SI"], "primary_category": "physics.soc-ph"}
{"title": "Learning Convolutional Text Representations for Visual Question\n  Answering", "abstract": "Visual question answering is a recently proposed artificial intelligence task\nthat requires a deep understanding of both images and texts. In deep learning,\nimages are typically modeled through convolutional neural networks, and texts\nare typically modeled through recurrent neural networks. While the requirement\nfor modeling images is similar to traditional computer vision tasks, such as\nobject recognition and image classification, visual question answering raises a\ndifferent need for textual representation as compared to other natural language\nprocessing tasks. In this work, we perform a detailed analysis on natural\nlanguage questions in visual question answering. Based on the analysis, we\npropose to rely on convolutional neural networks for learning textual\nrepresentations. By exploring the various properties of convolutional neural\nnetworks specialized for text data, such as width and depth, we present our\n\"CNN Inception + Gate\" model. We show that our model improves question\nrepresentations and thus the overall accuracy of visual question answering\nmodels. We also show that the text representation requirement in visual\nquestion answering is more complicated and comprehensive than that in\nconventional natural language processing tasks, making it a better task to\nevaluate textual representation methods. Shallow models like fastText, which\ncan obtain comparable results with deep learning models in tasks like text\nclassification, are not suitable in visual question answering.", "published": "2017-05-18 22:51:44", "link": "http://arxiv.org/abs/1705.06824v2", "categories": ["cs.LG", "cs.CL", "cs.NE", "stat.ML"], "primary_category": "cs.LG"}
