{"title": "Smoothing parameter estimation framework for IBM word alignment models", "abstract": "IBM models are very important word alignment models in Machine Translation.\nFollowing the Maximum Likelihood Estimation principle to estimate their\nparameters, the models will easily overfit the training data when the data are\nsparse. While smoothing is a very popular solution in Language Model, there\nstill lacks studies on smoothing for word alignment. In this paper, we propose\na framework which generalizes the notable work Moore [2004] of applying\nadditive smoothing to word alignment models. The framework allows developers to\ncustomize the smoothing amount for each pair of word. The added amount will be\nscaled appropriately by a common factor which reflects how much the framework\ntrusts the adding strategy according to the performance on data. We also\ncarefully examine various performance criteria and propose a smoothened version\nof the error count, which generally gives the best result.", "published": "2016-01-14 16:30:09", "link": "http://arxiv.org/abs/1601.03650v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improved Relation Classification by Deep Recurrent Neural Networks with\n  Data Augmentation", "abstract": "Nowadays, neural networks play an important role in the task of relation\nclassification. By designing different neural architectures, researchers have\nimproved the performance to a large extent in comparison with traditional\nmethods. However, existing neural networks for relation classification are\nusually of shallow architectures (e.g., one-layer convolutional neural networks\nor recurrent networks). They may fail to explore the potential representation\nspace in different abstraction levels. In this paper, we propose deep recurrent\nneural networks (DRNNs) for relation classification to tackle this challenge.\nFurther, we propose a data augmentation method by leveraging the directionality\nof relations. We evaluated our DRNNs on the SemEval-2010 Task~8, and achieve an\nF1-score of 86.1%, outperforming previous state-of-the-art recorded results.", "published": "2016-01-14 16:30:41", "link": "http://arxiv.org/abs/1601.03651v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Linear Algebraic Structure of Word Senses, with Applications to Polysemy", "abstract": "Word embeddings are ubiquitous in NLP and information retrieval, but it is\nunclear what they represent when the word is polysemous. Here it is shown that\nmultiple word senses reside in linear superposition within the word embedding\nand simple sparse coding can recover vectors that approximately capture the\nsenses. The success of our approach, which applies to several embedding\nmethods, is mathematically explained using a variant of the random walk on\ndiscourses model (Arora et al., 2016). A novel aspect of our technique is that\neach extracted word sense is accompanied by one of about 2000 \"discourse atoms\"\nthat gives a succinct description of which other words co-occur with that word\nsense. Discourse atoms can be of independent interest, and make the method\npotentially more useful. Empirical tests are used to verify and support the\ntheory.", "published": "2016-01-14 22:02:18", "link": "http://arxiv.org/abs/1601.03764v6", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
