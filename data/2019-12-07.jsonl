{"title": "Adversarial Analysis of Natural Language Inference Systems", "abstract": "The release of large natural language inference (NLI) datasets like SNLI and\nMNLI have led to rapid development and improvement of completely neural systems\nfor the task. Most recently, heavily pre-trained, Transformer-based models like\nBERT and MT-DNN have reached near-human performance on these datasets. However,\nthese standard datasets have been shown to contain many annotation artifacts,\nallowing models to shortcut understanding using simple fallible heuristics, and\nstill perform well on the test set. So it is no surprise that many adversarial\n(challenge) datasets have been created that cause models trained on standard\ndatasets to fail dramatically. Although extra training on this data generally\nimproves model performance on just that type of data, transferring that\nlearning to unseen examples is still partial at best. This work evaluates the\nfailures of state-of-the-art models on existing adversarial datasets that test\ndifferent linguistic phenomena, and find that even though the models perform\nsimilarly on MNLI, they differ greatly in their robustness to these attacks. In\nparticular, we find syntax-related attacks to be particularly effective across\nall models, so we provide a fine-grained analysis and comparison of model\nperformance on those examples. We draw conclusions about the value of model\nsize and multi-task learning (beyond comparing their standard test set\nperformance), and provide suggestions for more effective training data.", "published": "2019-12-07 05:12:03", "link": "http://arxiv.org/abs/1912.03441v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Personalized Patent Claim Generation and Measurement", "abstract": "This work-in-progress paper proposes a framework to generate and measure\npersonalized patent claims. The objective is to help inventors conceive better\ninventions by learning from relevant inventors. Patent claim generation is a\nway of \"augmented inventing.\" for inventors. Such patent claim generation\nleverages the recent transfer learning in the Deep Learning field, particularly\nthe state-of-the-art Transformer-based models. In terms of system\nimplementa-tion, it is planned to build an \"auto-complete\" function for patent\nclaim drafting. The auto-complete function is analyzed from four different\nperspectives: extent of generation, generative direction, proximity of\ngeneration, and constraint in generation. Technically, the framework is\ncomposed of two Transformer models. One is for text generation and the other is\nfor quality measurement. Specifically, the patent claim generation is based on\nGPT-2 model and the measurement of personalization is based on BERT model. The\ntraining data is inventor-centric and comes from the Inventors Endpoint API\nprovided by the USPTO.", "published": "2019-12-07 13:26:18", "link": "http://arxiv.org/abs/1912.03502v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PidginUNMT: Unsupervised Neural Machine Translation from West African\n  Pidgin to English", "abstract": "Over 800 languages are spoken across West Africa. Despite the obvious\ndiversity among people who speak these languages, one language significantly\nunifies them all - West African Pidgin English. There are at least 80 million\nspeakers of West African Pidgin English. However, there is no known natural\nlanguage processing (NLP) work on this language. In this work, we perform the\nfirst NLP work on the most popular variant of the language, providing three\nmajor contributions. First, the provision of a Pidgin corpus of over 56000\nsentences, which is the largest we know of. Secondly, the training of the first\never cross-lingual embedding between Pidgin and English. This aligned embedding\nwill be helpful in the performance of various downstream tasks between English\nand Pidgin. Thirdly, the training of an Unsupervised Neural Machine Translation\nmodel between Pidgin and English which achieves BLEU scores of 7.93 from Pidgin\nto English, and 5.18 from English to Pidgin. In all, this work greatly reduces\nthe barrier of entry for future NLP works on West African Pidgin English.", "published": "2019-12-07 05:30:09", "link": "http://arxiv.org/abs/1912.03444v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unsung Challenges of Building and Deploying Language Technologies for\n  Low Resource Language Communities", "abstract": "In this paper, we examine and analyze the challenges associated with\ndeveloping and introducing language technologies to low-resource language\ncommunities. While doing so, we bring to light the successes and failures of\npast work in this area, challenges being faced in doing so, and what they have\nachieved. Throughout this paper, we take a problem-facing approach and describe\nessential factors which the success of such technologies hinges upon. We\npresent the various aspects in a manner which clarify and lay out the different\ntasks involved, which can aid organizations looking to make an impact in this\narea. We take the example of Gondi, an extremely-low resource Indian language,\nto reinforce and complement our discussion.", "published": "2019-12-07 07:45:43", "link": "http://arxiv.org/abs/1912.03457v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Learning Norms from Stories: A Prior for Value Aligned Agents", "abstract": "Value alignment is a property of an intelligent agent indicating that it can\nonly pursue goals and activities that are beneficial to humans. Traditional\napproaches to value alignment use imitation learning or preference learning to\ninfer the values of humans by observing their behavior. We introduce a\ncomplementary technique in which a value aligned prior is learned from\nnaturally occurring stories which encode societal norms. Training data is\nsourced from the childrens educational comic strip, Goofus and Gallant. In this\nwork, we train multiple machine learning models to classify natural language\ndescriptions of situations found in the comic strip as normative or non\nnormative by identifying if they align with the main characters behavior. We\nalso report the models performance when transferring to two unrelated tasks\nwith little to no additional training on the new task.", "published": "2019-12-07 20:12:43", "link": "http://arxiv.org/abs/1912.03553v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
