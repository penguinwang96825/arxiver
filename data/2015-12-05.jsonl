{"title": "PJAIT Systems for the IWSLT 2015 Evaluation Campaign Enhanced by\n  Comparable Corpora", "abstract": "In this paper, we attempt to improve Statistical Machine Translation (SMT)\nsystems on a very diverse set of language pairs (in both directions): Czech -\nEnglish, Vietnamese - English, French - English and German - English. To\naccomplish this, we performed translation model training, created adaptations\nof training settings for each language pair, and obtained comparable corpora\nfor our SMT systems. Innovative tools and data adaptation techniques were\nemployed. The TED parallel text corpora for the IWSLT 2015 evaluation campaign\nwere used to train language models, and to develop, tune, and test the system.\nIn addition, we prepared Wikipedia-based comparable corpora for use with our\nSMT system. This data was specified as permissible for the IWSLT 2015\nevaluation. We explored the use of domain adaptation techniques, symmetrized\nword alignment models, the unsupervised transliteration models and the KenLM\nlanguage modeling tool. To evaluate the effects of different preparations on\ntranslation results, we conducted experiments and used the BLEU, NIST and TER\nmetrics. Our results indicate that our approach produced a positive impact on\nSMT quality.", "published": "2015-12-05 08:55:31", "link": "http://arxiv.org/abs/1512.01639v1", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Unsupervised comparable corpora preparation and exploration for\n  bi-lingual translation equivalents", "abstract": "The multilingual nature of the world makes translation a crucial requirement\ntoday. Parallel dictionaries constructed by humans are a widely-available\nresource, but they are limited and do not provide enough coverage for good\nquality translation purposes, due to out-of-vocabulary words and neologisms.\nThis motivates the use of statistical translation systems, which are\nunfortunately dependent on the quantity and quality of training data. Such\nsystems have a very limited availability especially for some languages and very\nnarrow text domains. In this research we present our improvements to current\ncomparable corpora mining methodologies by re- implementation of the comparison\nalgorithms (using Needleman-Wunch algorithm), introduction of a tuning script\nand computation time improvement by GPU acceleration. Experiments are carried\nout on bilingual data extracted from the Wikipedia, on various domains. For the\nWikipedia itself, additional cross-lingual comparison heuristics were\nintroduced. The modifications made a positive impact on the quality and\nquantity of mined data and on the translation quality.", "published": "2015-12-05 08:59:28", "link": "http://arxiv.org/abs/1512.01641v1", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Generating News Headlines with Recurrent Neural Networks", "abstract": "We describe an application of an encoder-decoder recurrent neural network\nwith LSTM units and attention to generating headlines from the text of news\narticles. We find that the model is quite effective at concisely paraphrasing\nnews articles. Furthermore, we study how the neural network decides which input\nwords to pay attention to, and specifically we identify the function of the\ndifferent neurons in a simplified attention mechanism. Interestingly, our\nsimplified attention mechanism performs better that the more complex attention\nmechanism on a held out set of articles.", "published": "2015-12-05 23:41:22", "link": "http://arxiv.org/abs/1512.01712v1", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
