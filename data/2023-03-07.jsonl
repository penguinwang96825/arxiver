{"title": "Towards Interpretable and Efficient Automatic Reference-Based\n  Summarization Evaluation", "abstract": "Interpretability and efficiency are two important considerations for the\nadoption of neural automatic metrics. In this work, we develop\nstrong-performing automatic metrics for reference-based summarization\nevaluation, based on a two-stage evaluation pipeline that first extracts basic\ninformation units from one text sequence and then checks the extracted units in\nanother sequence. The metrics we developed include two-stage metrics that can\nprovide high interpretability at both the fine-grained unit level and summary\nlevel, and one-stage metrics that achieve a balance between efficiency and\ninterpretability. We make the developed tools publicly available at\nhttps://github.com/Yale-LILY/AutoACU.", "published": "2023-03-07 02:49:50", "link": "http://arxiv.org/abs/2303.03608v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Preparing the Vuk'uzenzele and ZA-gov-multilingual South African\n  multilingual corpora", "abstract": "This paper introduces two multilingual government themed corpora in various\nSouth African languages. The corpora were collected by gathering the South\nAfrican Government newspaper (Vuk'uzenzele), as well as South African\ngovernment speeches (ZA-gov-multilingual), that are translated into all 11\nSouth African official languages. The corpora can be used for a myriad of\ndownstream NLP tasks. The corpora were created to allow researchers to study\nthe language used in South African government publications, with a focus on\nunderstanding how South African government officials communicate with their\nconstituents. In this paper we highlight the process of gathering, cleaning and\nmaking available the corpora. We create parallel sentence corpora for Neural\nMachine Translation (NMT) tasks using Language-Agnostic Sentence\nRepresentations (LASER) embeddings. With these aligned sentences we then\nprovide NMT benchmarks for 9 indigenous languages by fine-tuning a massively\nmultilingual pre-trained language model.", "published": "2023-03-07 09:20:09", "link": "http://arxiv.org/abs/2303.03750v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring the Feasibility of ChatGPT for Event Extraction", "abstract": "Event extraction is a fundamental task in natural language processing that\ninvolves identifying and extracting information about events mentioned in text.\nHowever, it is a challenging task due to the lack of annotated data, which is\nexpensive and time-consuming to obtain. The emergence of large language models\n(LLMs) such as ChatGPT provides an opportunity to solve language tasks with\nsimple prompts without the need for task-specific datasets and fine-tuning.\nWhile ChatGPT has demonstrated impressive results in tasks like machine\ntranslation, text summarization, and question answering, it presents challenges\nwhen used for complex tasks like event extraction. Unlike other tasks, event\nextraction requires the model to be provided with a complex set of instructions\ndefining all event types and their schemas. To explore the feasibility of\nChatGPT for event extraction and the challenges it poses, we conducted a series\nof experiments. Our results show that ChatGPT has, on average, only 51.04% of\nthe performance of a task-specific model such as EEQA in long-tail and complex\nscenarios. Our usability testing experiments indicate that ChatGPT is not\nrobust enough, and continuous refinement of the prompt does not lead to stable\nperformance improvements, which can result in a poor user experience. Besides,\nChatGPT is highly sensitive to different prompt styles.", "published": "2023-03-07 12:03:58", "link": "http://arxiv.org/abs/2303.03836v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Larger language models do in-context learning differently", "abstract": "We study how in-context learning (ICL) in language models is affected by\nsemantic priors versus input-label mappings. We investigate two setups-ICL with\nflipped labels and ICL with semantically-unrelated labels-across various model\nfamilies (GPT-3, InstructGPT, Codex, PaLM, and Flan-PaLM). First, experiments\non ICL with flipped labels show that overriding semantic priors is an emergent\nability of model scale. While small language models ignore flipped labels\npresented in-context and thus rely primarily on semantic priors from\npretraining, large models can override semantic priors when presented with\nin-context exemplars that contradict priors, despite the stronger semantic\npriors that larger models may hold. We next study semantically-unrelated label\nICL (SUL-ICL), in which labels are semantically unrelated to their inputs\n(e.g., foo/bar instead of negative/positive), thereby forcing language models\nto learn the input-label mappings shown in in-context exemplars in order to\nperform the task. The ability to do SUL-ICL also emerges primarily with scale,\nand large-enough language models can even perform linear classification in a\nSUL-ICL setting. Finally, we evaluate instruction-tuned models and find that\ninstruction tuning strengthens both the use of semantic priors and the capacity\nto learn input-label mappings, but more of the former.", "published": "2023-03-07 12:24:17", "link": "http://arxiv.org/abs/2303.03846v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Meta-Evaluation of Faithfulness Metrics for Long-Form Hospital-Course\n  Summarization", "abstract": "Long-form clinical summarization of hospital admissions has real-world\nsignificance because of its potential to help both clinicians and patients. The\nfaithfulness of summaries is critical to their safe usage in clinical settings.\nTo better understand the limitations of abstractive systems, as well as the\nsuitability of existing evaluation metrics, we benchmark faithfulness metrics\nagainst fine-grained human annotations for model-generated summaries of a\npatient's Brief Hospital Course. We create a corpus of patient hospital\nadmissions and summaries for a cohort of HIV patients, each with complex\nmedical histories. Annotators are presented with summaries and source notes,\nand asked to categorize manually highlighted summary elements (clinical\nentities like conditions and medications as well as actions like \"following\nup\") into one of three categories: ``Incorrect,'' ``Missing,'' and ``Not in\nNotes.'' We meta-evaluate a broad set of proposed faithfulness metrics and,\nacross metrics, explore the importance of domain adaptation (e.g. the impact of\nin-domain pre-training and metric fine-tuning), the use of source-summary\nalignments, and the effects of distilling a single metric from an ensemble of\npre-existing metrics. Off-the-shelf metrics with no exposure to clinical text\ncorrelate well yet overly rely on summary extractiveness. As a practical guide\nto long-form clinical narrative summarization, we find that most metrics\ncorrelate best to human judgments when provided with one summary sentence at a\ntime and a minimal set of relevant source context.", "published": "2023-03-07 14:57:06", "link": "http://arxiv.org/abs/2303.03948v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GATE: A Challenge Set for Gender-Ambiguous Translation Examples", "abstract": "Although recent years have brought significant progress in improving\ntranslation of unambiguously gendered sentences, translation of ambiguously\ngendered input remains relatively unexplored. When source gender is ambiguous,\nmachine translation models typically default to stereotypical gender roles,\nperpetuating harmful bias. Recent work has led to the development of \"gender\nrewriters\" that generate alternative gender translations on such ambiguous\ninputs, but such systems are plagued by poor linguistic coverage. To encourage\nbetter performance on this task we present and release GATE, a linguistically\ndiverse corpus of gender-ambiguous source sentences along with multiple\nalternative target language translations. We also provide tools for evaluation\nand system analysis when using GATE and use them to evaluate our translation\nrewriter system.", "published": "2023-03-07 15:23:38", "link": "http://arxiv.org/abs/2303.03975v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Describe me an Aucklet: Generating Grounded Perceptual Category\n  Descriptions", "abstract": "Human speakers can generate descriptions of perceptual concepts, abstracted\nfrom the instance-level. Moreover, such descriptions can be used by other\nspeakers to learn provisional representations of those concepts. Learning and\nusing abstract perceptual concepts is under-investigated in the\nlanguage-and-vision field. The problem is also highly relevant to the field of\nrepresentation learning in multi-modal NLP. In this paper, we introduce a\nframework for testing category-level perceptual grounding in multi-modal\nlanguage models. In particular, we train separate neural networks to generate\nand interpret descriptions of visual categories. We measure the communicative\nsuccess of the two models with the zero-shot classification performance of the\ninterpretation model, which we argue is an indicator of perceptual grounding.\nUsing this framework, we compare the performance of prototype- and\nexemplar-based representations. Finally, we show that communicative success\nexposes performance issues in the generation model, not captured by traditional\nintrinsic NLG evaluation metrics, and argue that these issues stem from a\nfailure to properly ground language in vision at the category level.", "published": "2023-03-07 17:01:25", "link": "http://arxiv.org/abs/2303.04053v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CroCoSum: A Benchmark Dataset for Cross-Lingual Code-Switched\n  Summarization", "abstract": "Cross-lingual summarization (CLS) has attracted increasing interest in recent\nyears due to the availability of large-scale web-mined datasets and the\nadvancements of multilingual language models. However, given the rareness of\nnaturally occurring CLS resources, the majority of datasets are forced to rely\non translation which can contain overly literal artifacts. This restricts our\nability to observe naturally occurring CLS pairs that capture organic diction,\nincluding instances of code-switching. This alteration between languages in\nmid-message is a common phenomenon in multilingual settings yet has been\nlargely overlooked in cross-lingual contexts due to data scarcity. To address\nthis gap, we introduce CroCoSum, a dataset of cross-lingual code-switched\nsummarization of technology news. It consists of over 24,000 English source\narticles and 18,000 human-written Chinese news summaries, with more than 92% of\nthe summaries containing code-switched phrases. For reference, we evaluate the\nperformance of existing approaches including pipeline, end-to-end, and\nzero-shot methods. We show that leveraging existing CLS resources as a\npretraining step does not improve performance on CroCoSum, indicating the\nlimited generalizability of current datasets. Finally, we discuss the\nchallenges of evaluating cross-lingual summarizers on code-switched generation\nthrough qualitative error analyses.", "published": "2023-03-07 17:52:51", "link": "http://arxiv.org/abs/2303.04092v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Marpa and nullable symbols", "abstract": "The Marpa parser was intended to make the best results in the academic\nliterature on Earley's algorithm available as a practical general parser.\nEarley-based parsers have had issues handling nullable symbols. Initially, we\ndealt with nullable symbols by following the approach in Aycock and Horspool's\n2002 paper. This paper reports our experience with the approach in that paper,\nand the approach to handling nullables that we settled on in reaction to that\nexperience.", "published": "2023-03-07 17:53:00", "link": "http://arxiv.org/abs/2303.04093v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A primer on getting neologisms from foreign languages to under-resourced\n  languages", "abstract": "Mainly due to lack of support, most under-resourced languages have a reduced\nlexicon in most realms and domains of increasing importance, then their\nspeakers need to significantly augment it. Although neologisms should arise\nfrom the languages themselves, external sources are widely accepted. However,\nwe dispute the \"common sense\" of using the imposed official languages, which\nare highly probably a legacy of colonialism, as the only source, and we propose\nto introduce neologisms from any language as long as these neologisms \"sound\nlike\" native words of the target languages.", "published": "2023-03-07 15:10:37", "link": "http://arxiv.org/abs/2304.10495v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ADELT: Transpilation Between Deep Learning Frameworks", "abstract": "We propose the Adversarial DEep Learning Transpiler (ADELT), a novel approach\nto source-to-source transpilation between deep learning frameworks. ADELT\nuniquely decouples code skeleton transpilation and API keyword mapping. For\ncode skeleton transpilation, it uses few-shot prompting on large language\nmodels (LLMs), while for API keyword mapping, it uses contextual embeddings\nfrom a code-specific BERT. These embeddings are trained in a domain-adversarial\nsetup to generate a keyword translation dictionary. ADELT is trained on an\nunlabeled web-crawled deep learning corpus, without relying on any hand-crafted\nrules or parallel data. It outperforms state-of-the-art transpilers, improving\npass@1 rate by 17.4 pts and 15.0 pts for PyTorch-Keras and PyTorch-MXNet\ntranspilation pairs respectively. We provide open access to our code at\nhttps://github.com/gonglinyuan/adelt.", "published": "2023-03-07 01:57:10", "link": "http://arxiv.org/abs/2303.03593v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CoTEVer: Chain of Thought Prompting Annotation Toolkit for Explanation\n  Verification", "abstract": "Chain-of-thought (CoT) prompting enables large language models (LLMs) to\nsolve complex reasoning tasks by generating an explanation before the final\nprediction. Despite it's promising ability, a critical downside of CoT\nprompting is that the performance is greatly affected by the factuality of the\ngenerated explanation. To improve the correctness of the explanations,\nfine-tuning language models with explanation data is needed. However, there\nexists only a few datasets that can be used for such approaches, and no data\ncollection tool for building them. Thus, we introduce CoTEVer, a tool-kit for\nannotating the factual correctness of generated explanations and collecting\nrevision data of wrong explanations. Furthermore, we suggest several use cases\nwhere the data collected with CoTEVer can be utilized for enhancing the\nfaithfulness of explanations. Our toolkit is publicly available at\nhttps://github.com/SeungoneKim/CoTEVer.", "published": "2023-03-07 03:23:14", "link": "http://arxiv.org/abs/2303.03628v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Stylometric Detection of AI-Generated Text in Twitter Timelines", "abstract": "Recent advancements in pre-trained language models have enabled convenient\nmethods for generating human-like text at a large scale. Though these\ngeneration capabilities hold great potential for breakthrough applications, it\ncan also be a tool for an adversary to generate misinformation. In particular,\nsocial media platforms like Twitter are highly susceptible to AI-generated\nmisinformation. A potential threat scenario is when an adversary hijacks a\ncredible user account and incorporates a natural language generator to generate\nmisinformation. Such threats necessitate automated detectors for AI-generated\ntweets in a given user's Twitter timeline. However, tweets are inherently\nshort, thus making it difficult for current state-of-the-art pre-trained\nlanguage model-based detectors to accurately detect at what point the AI starts\nto generate tweets in a given Twitter timeline. In this paper, we present a\nnovel algorithm using stylometric signals to aid detecting AI-generated tweets.\nWe propose models corresponding to quantifying stylistic changes in human and\nAI tweets in two related tasks: Task 1 - discriminate between human and\nAI-generated tweets, and Task 2 - detect if and when an AI starts to generate\ntweets in a given Twitter timeline. Our extensive experiments demonstrate that\nthe stylometric features are effective in augmenting the state-of-the-art\nAI-generated text detectors.", "published": "2023-03-07 07:26:09", "link": "http://arxiv.org/abs/2303.03697v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Classifying Text-Based Conspiracy Tweets related to COVID-19 using\n  Contextualized Word Embeddings", "abstract": "The FakeNews task in MediaEval 2022 investigates the challenge of finding\naccurate and high-performance models for the classification of conspiracy\ntweets related to COVID-19. In this paper, we used BERT, ELMO, and their\ncombination for feature extraction and RandomForest as classifier. The results\nshow that ELMO performs slightly better than BERT, however their combination at\nfeature level reduces the performance.", "published": "2023-03-07 07:42:26", "link": "http://arxiv.org/abs/2303.03706v1", "categories": ["cs.CL", "cs.LG", "68T01", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Universal resources for quantum computing", "abstract": "Unravelling the source of quantum computing power has been a major goal in\nthe field of quantum information science. In recent years, the quantum resource\ntheory (QRT) has been established to characterize various quantum resources,\nyet their roles in quantum computing tasks still require investigation. The\nso-called universal quantum computing model (UQCM), e.g., the circuit model,\nhas been the main framework to guide the design of quantum algorithms, creation\nof real quantum computers etc. In this work, we combine the study of UQCM\ntogether with QRT. We find on one hand, using QRT can provide a\nresource-theoretic characterization of a UQCM, the relation among models and\ninspire new ones, and on the other hand, using UQCM offers a framework to apply\nresources, study relation among resources and classify them.\n  We develop the theory of universal resources in the setting of UQCM, and find\na rich spectrum of UQCMs and the corresponding universal resources. Depending\non a hierarchical structure of resource theories, we find models can be\nclassified into families. In this work, we study three natural families of\nUQCMs in details: the amplitude family, the quasi-probability family, and the\nHamiltonian family. They include some well known models, like the\nmeasurement-based model and adiabatic model, and also inspire new models such\nas the contextual model we introduce. Each family contains at least a triplet\nof models, and such a succinct structure of families of UQCMs offers a unifying\npicture to investigate resources and design models. It also provides a rigorous\nframework to resolve puzzles, such as the role of entanglement vs.\ninterference, and unravel resource-theoretic features of quantum algorithms.", "published": "2023-03-07 07:56:38", "link": "http://arxiv.org/abs/2303.03715v2", "categories": ["quant-ph", "cs.CL"], "primary_category": "quant-ph"}
{"title": "A Challenging Benchmark for Low-Resource Learning", "abstract": "With promising yet saturated results in high-resource settings, low-resource\ndatasets have gradually become popular benchmarks for evaluating the learning\nability of advanced neural networks (e.g., BigBench, superGLUE). Some models\neven surpass humans according to benchmark test results. However, we find that\nthere exists a set of hard examples in low-resource settings that challenge\nneural networks but are not well evaluated, which causes over-estimated\nperformance. We first give a theoretical analysis on which factors bring the\ndifficulty of low-resource learning. It then motivate us to propose a\nchallenging benchmark hardBench to better evaluate the learning ability, which\ncovers 11 datasets, including 3 computer vision (CV) datasets and 8 natural\nlanguage process (NLP) datasets. Experiments on a wide range of models show\nthat neural networks, even pre-trained language models, have sharp performance\ndrops on our benchmark, demonstrating the effectiveness on evaluating the\nweaknesses of neural networks. On NLP tasks, we surprisingly find that despite\nbetter results on traditional low-resource benchmarks, pre-trained networks,\ndoes not show performance improvements on our benchmarks. These results\ndemonstrate that there are still a large robustness gap between existing models\nand human-level performance.", "published": "2023-03-07 12:10:47", "link": "http://arxiv.org/abs/2303.03840v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset", "abstract": "As language models grow ever larger, the need for large-scale high-quality\ntext datasets has never been more pressing, especially in multilingual\nsettings. The BigScience workshop, a 1-year international and multidisciplinary\ninitiative, was formed with the goal of researching and training large language\nmodels as a values-driven undertaking, putting issues of ethics, harm, and\ngovernance in the foreground. This paper documents the data creation and\ncuration efforts undertaken by BigScience to assemble the Responsible\nOpen-science Open-collaboration Text Sources (ROOTS) corpus, a 1.6TB dataset\nspanning 59 languages that was used to train the 176-billion-parameter\nBigScience Large Open-science Open-access Multilingual (BLOOM) language model.\nWe further release a large initial subset of the corpus and analyses thereof,\nand hope to empower large-scale monolingual and multilingual modeling projects\nwith both the data and the processing tools, as well as stimulate research\naround this large multilingual corpus.", "published": "2023-03-07 14:25:44", "link": "http://arxiv.org/abs/2303.03915v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "ChatGPT: Beginning of an End of Manual Linguistic Data Annotation? Use\n  Case of Automatic Genre Identification", "abstract": "ChatGPT has shown strong capabilities in natural language generation tasks,\nwhich naturally leads researchers to explore where its abilities end. In this\npaper, we examine whether ChatGPT can be used for zero-shot text\nclassification, more specifically, automatic genre identification. We compare\nChatGPT with a multilingual XLM-RoBERTa language model that was fine-tuned on\ndatasets, manually annotated with genres. The models are compared on test sets\nin two languages: English and Slovenian. Results show that ChatGPT outperforms\nthe fine-tuned model when applied to the dataset which was not seen before by\neither of the models. Even when applied on Slovenian language as an\nunder-resourced language, ChatGPT's performance is no worse than when applied\nto English. However, if the model is fully prompted in Slovenian, the\nperformance drops significantly, showing the current limitations of ChatGPT\nusage on smaller languages. The presented results lead us to questioning\nwhether this is the beginning of an end of laborious manual annotation\ncampaigns even for smaller languages, such as Slovenian.", "published": "2023-03-07 14:59:33", "link": "http://arxiv.org/abs/2303.03953v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Is ChatGPT a Good NLG Evaluator? A Preliminary Study", "abstract": "Recently, the emergence of ChatGPT has attracted wide attention from the\ncomputational linguistics community. Many prior studies have shown that ChatGPT\nachieves remarkable performance on various NLP tasks in terms of automatic\nevaluation metrics. However, the ability of ChatGPT to serve as an evaluation\nmetric is still underexplored. Considering assessing the quality of natural\nlanguage generation (NLG) models is an arduous task and NLG metrics notoriously\nshow their poor correlation with human judgments, we wonder whether ChatGPT is\na good NLG evaluation metric. In this report, we provide a preliminary\nmeta-evaluation on ChatGPT to show its reliability as an NLG metric. In detail,\nwe regard ChatGPT as a human evaluator and give task-specific (e.g.,\nsummarization) and aspect-specific (e.g., relevance) instruction to prompt\nChatGPT to evaluate the generated results of NLG models. We conduct experiments\non five NLG meta-evaluation datasets (including summarization, story generation\nand data-to-text tasks). Experimental results show that compared with previous\nautomatic metrics, ChatGPT achieves state-of-the-art or competitive correlation\nwith human judgments in most cases. In addition, we find that the effectiveness\nof the ChatGPT evaluator might be influenced by the creation method of the\nmeta-evaluation datasets. For the meta-evaluation datasets which are created\ngreatly depending on the reference and thus are biased, the ChatGPT evaluator\nmight lose its effectiveness. We hope our preliminary study could prompt the\nemergence of a general-purposed reliable NLG metric.", "published": "2023-03-07 16:57:20", "link": "http://arxiv.org/abs/2303.04048v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Hybrid Architecture for Out of Domain Intent Detection and Intent\n  Discovery", "abstract": "Intent Detection is one of the tasks of the Natural Language Understanding\n(NLU) unit in task-oriented dialogue systems. Out of Scope (OOS) and Out of\nDomain (OOD) inputs may run these systems into a problem. On the other side, a\nlabeled dataset is needed to train a model for Intent Detection in\ntask-oriented dialogue systems. The creation of a labeled dataset is\ntime-consuming and needs human resources. The purpose of this article is to\naddress mentioned problems. The task of identifying OOD/OOS inputs is named\nOOD/OOS Intent Detection. Also, discovering new intents and pseudo-labeling of\nOOD inputs is well known by Intent Discovery. In OOD intent detection part, we\nmake use of a Variational Autoencoder to distinguish between known and unknown\nintents independent of input data distribution. After that, an unsupervised\nclustering method is used to discover different unknown intents underlying\nOOD/OOS inputs. We also apply a non-linear dimensionality reduction on OOD/OOS\nrepresentations to make distances between representations more meaning full for\nclustering. Our results show that the proposed model for both OOD/OOS Intent\nDetection and Intent Discovery achieves great results and passes baselines in\nEnglish and Persian languages.", "published": "2023-03-07 18:49:13", "link": "http://arxiv.org/abs/2303.04134v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SemEval-2023 Task 10: Explainable Detection of Online Sexism", "abstract": "Online sexism is a widespread and harmful phenomenon. Automated tools can\nassist the detection of sexism at scale. Binary detection, however, disregards\nthe diversity of sexist content, and fails to provide clear explanations for\nwhy something is sexist. To address this issue, we introduce SemEval Task 10 on\nthe Explainable Detection of Online Sexism (EDOS). We make three main\ncontributions: i) a novel hierarchical taxonomy of sexist content, which\nincludes granular vectors of sexism to aid explainability; ii) a new dataset of\n20,000 social media comments with fine-grained labels, along with larger\nunlabelled datasets for model adaptation; and iii) baseline models as well as\nan analysis of the methods, results and errors for participant submissions to\nour task.", "published": "2023-03-07 20:28:39", "link": "http://arxiv.org/abs/2303.04222v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Can large language models build causal graphs?", "abstract": "Building causal graphs can be a laborious process. To ensure all relevant\ncausal pathways have been captured, researchers often have to discuss with\nclinicians and experts while also reviewing extensive relevant medical\nliterature. By encoding common and medical knowledge, large language models\n(LLMs) represent an opportunity to ease this process by automatically scoring\nedges (i.e., connections between two variables) in potential graphs. LLMs\nhowever have been shown to be brittle to the choice of probing words, context,\nand prompts that the user employs. In this work, we evaluate if LLMs can be a\nuseful tool in complementing causal graph development.", "published": "2023-03-07 22:05:31", "link": "http://arxiv.org/abs/2303.05279v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Extracting Accurate Materials Data from Research Papers with\n  Conversational Language Models and Prompt Engineering", "abstract": "There has been a growing effort to replace manual extraction of data from\nresearch papers with automated data extraction based on natural language\nprocessing, language models, and recently, large language models (LLMs).\nAlthough these methods enable efficient extraction of data from large sets of\nresearch papers, they require a significant amount of up-front effort,\nexpertise, and coding. In this work we propose the ChatExtract method that can\nfully automate very accurate data extraction with minimal initial effort and\nbackground, using an advanced conversational LLM. ChatExtract consists of a set\nof engineered prompts applied to a conversational LLM that both identify\nsentences with data, extract that data, and assure the data's correctness\nthrough a series of follow-up questions. These follow-up questions largely\novercome known issues with LLMs providing factually inaccurate responses.\nChatExtract can be applied with any conversational LLMs and yields very high\nquality data extraction. In tests on materials data we find precision and\nrecall both close to 90% from the best conversational LLMs, like ChatGPT-4. We\ndemonstrate that the exceptional performance is enabled by the information\nretention in a conversational model combined with purposeful redundancy and\nintroducing uncertainty through follow-up prompts. These results suggest that\napproaches similar to ChatExtract, due to their simplicity, transferability,\nand accuracy are likely to become powerful tools for data extraction in the\nnear future. Finally, databases for critical cooling rates of metallic glasses\nand yield strengths of high entropy alloys are developed using ChatExtract.", "published": "2023-03-07 17:54:53", "link": "http://arxiv.org/abs/2303.05352v3", "categories": ["cs.CL", "cond-mat.mtrl-sci"], "primary_category": "cs.CL"}
{"title": "German BERT Model for Legal Named Entity Recognition", "abstract": "The use of BERT, one of the most popular language models, has led to\nimprovements in many Natural Language Processing (NLP) tasks. One such task is\nNamed Entity Recognition (NER) i.e. automatic identification of named entities\nsuch as location, person, organization, etc. from a given text. It is also an\nimportant base step for many NLP tasks such as information extraction and\nargumentation mining. Even though there is much research done on NER using BERT\nand other popular language models, the same is not explored in detail when it\ncomes to Legal NLP or Legal Tech. Legal NLP applies various NLP techniques such\nas sentence similarity or NER specifically on legal data. There are only a\nhandful of models for NER tasks using BERT language models, however, none of\nthese are aimed at legal documents in German. In this paper, we fine-tune a\npopular BERT language model trained on German data (German BERT) on a Legal\nEntity Recognition (LER) dataset. To make sure our model is not overfitting, we\nperformed a stratified 10-fold cross-validation. The results we achieve by\nfine-tuning German BERT on the LER dataset outperform the BiLSTM-CRF+ model\nused by the authors of the same LER dataset. Finally, we make the model openly\navailable via HuggingFace.", "published": "2023-03-07 11:54:39", "link": "http://arxiv.org/abs/2303.05388v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Adaptive Knowledge Distillation between Text and Speech Pre-trained\n  Models", "abstract": "Learning on a massive amount of speech corpus leads to the recent success of\nmany self-supervised speech models. With knowledge distillation, these models\nmay also benefit from the knowledge encoded by language models that are\npre-trained on rich sources of texts. The distillation process, however, is\nchallenging due to the modal disparity between textual and speech embedding\nspaces. This paper studies metric-based distillation to align the embedding\nspace of text and speech with only a small amount of data without modifying the\nmodel structure. Since the semantic and granularity gap between text and speech\nhas been omitted in literature, which impairs the distillation, we propose the\nPrior-informed Adaptive knowledge Distillation (PAD) that adaptively leverages\ntext/speech units of variable granularity and prior distributions to achieve\nbetter global and local alignments between text and speech pre-trained models.\nWe evaluate on three spoken language understanding benchmarks to show that PAD\nis more effective in transferring linguistic knowledge than other metric-based\ndistillation approaches.", "published": "2023-03-07 02:31:57", "link": "http://arxiv.org/abs/2303.03600v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Document-level Relation Extraction with Cross-sentence Reasoning Graph", "abstract": "Relation extraction (RE) has recently moved from the sentence-level to\ndocument-level, which requires aggregating document information and using\nentities and mentions for reasoning. Existing works put entity nodes and\nmention nodes with similar representations in a document-level graph, whose\ncomplex edges may incur redundant information. Furthermore, existing studies\nonly focus on entity-level reasoning paths without considering global\ninteractions among entities cross-sentence. To these ends, we propose a novel\ndocument-level RE model with a GRaph information Aggregation and Cross-sentence\nReasoning network (GRACR). Specifically, a simplified document-level graph is\nconstructed to model the semantic information of all mentions and sentences in\na document, and an entity-level graph is designed to explore relations of\nlong-distance cross-sentence entity pairs. Experimental results show that GRACR\nachieves excellent performance on two public datasets of document-level RE. It\nis especially effective in extracting potential relations of cross-sentence\nentity pairs. Our code is available at https://github.com/UESTC-LHF/GRACR.", "published": "2023-03-07 14:14:12", "link": "http://arxiv.org/abs/2303.03912v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec\n  Language Modeling", "abstract": "We propose a cross-lingual neural codec language model, VALL-E X, for\ncross-lingual speech synthesis. Specifically, we extend VALL-E and train a\nmulti-lingual conditional codec language model to predict the acoustic token\nsequences of the target language speech by using both the source language\nspeech and the target language text as prompts. VALL-E X inherits strong\nin-context learning capabilities and can be applied for zero-shot cross-lingual\ntext-to-speech synthesis and zero-shot speech-to-speech translation tasks.\nExperimental results show that it can generate high-quality speech in the\ntarget language via just one speech utterance in the source language as a\nprompt while preserving the unseen speaker's voice, emotion, and acoustic\nenvironment. Moreover, VALL-E X effectively alleviates the foreign accent\nproblems, which can be controlled by a language ID. Audio samples are available\nat \\url{https://aka.ms/vallex}.", "published": "2023-03-07 14:31:55", "link": "http://arxiv.org/abs/2303.03926v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "ELODIN: Naming Concepts in Embedding Spaces", "abstract": "Despite recent advancements, the field of text-to-image synthesis still\nsuffers from lack of fine-grained control. Using only text, it remains\nchallenging to deal with issues such as concept coherence and concept\ncontamination. We propose a method to enhance control by generating specific\nconcepts that can be reused throughout multiple images, effectively expanding\nnatural language with new words that can be combined much like a painter's\npalette. Unlike previous contributions, our method does not copy visuals from\ninput data and can generate concepts through text alone. We perform a set of\ncomparisons that finds our method to be a significant improvement over\ntext-only prompts.", "published": "2023-03-07 16:00:26", "link": "http://arxiv.org/abs/2303.04001v2", "categories": ["cs.CV", "cs.CL", "cs.GR", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Abstract Visual Reasoning Enabled by Language", "abstract": "While artificial intelligence (AI) models have achieved human or even\nsuperhuman performance in many well-defined applications, they still struggle\nto show signs of broad and flexible intelligence. The Abstraction and Reasoning\nCorpus (ARC), a visual intelligence benchmark introduced by Fran\\c{c}ois\nChollet, aims to assess how close AI systems are to human-like cognitive\nabilities. Most current approaches rely on carefully handcrafted\ndomain-specific program searches to brute-force solutions for the tasks present\nin ARC. In this work, we propose a general learning-based framework for solving\nARC. It is centered on transforming tasks from the vision to the language\ndomain. This composition of language and vision allows for pre-trained models\nto be leveraged at each stage, enabling a shift from handcrafted priors towards\nthe learned priors of the models. While not yet beating state-of-the-art models\non ARC, we demonstrate the potential of our approach, for instance, by solving\nsome ARC tasks that have not been solved previously.", "published": "2023-03-07 17:52:46", "link": "http://arxiv.org/abs/2303.04091v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Exploiting Asymmetry for Synthetic Training Data Generation: SynthIE and\n  the Case of Information Extraction", "abstract": "Large language models (LLMs) have great potential for synthetic data\ngeneration. This work shows that useful data can be synthetically generated\neven for tasks that cannot be solved directly by LLMs: for problems with\nstructured outputs, it is possible to prompt an LLM to perform the task in the\nreverse direction, by generating plausible input text for a target output\nstructure. Leveraging this asymmetry in task difficulty makes it possible to\nproduce large-scale, high-quality data for complex tasks. We demonstrate the\neffectiveness of this approach on closed information extraction, where\ncollecting ground-truth data is challenging, and no satisfactory dataset exists\nto date. We synthetically generate a dataset of 1.8M data points, establish its\nsuperior quality compared to existing datasets in a human evaluation, and use\nit to finetune small models (220M and 770M parameters), termed SynthIE, that\noutperform the prior state of the art (with equal model size) by a substantial\nmargin of 57 absolute points in micro-F1 and 79 points in macro-F1. Code, data,\nand models are available at https://github.com/epfl-dlab/SynthIE.", "published": "2023-03-07 18:48:55", "link": "http://arxiv.org/abs/2303.04132v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Gradient-Free Structured Pruning with Unlabeled Data", "abstract": "Large Language Models (LLMs) have achieved great success in solving difficult\ntasks across many domains, but such success comes with a high computation cost,\nand inference latency. As developers and third parties customize these models,\nthe need to provide efficient inference has increased. Many efforts have\nattempted to reduce inference cost through model compression techniques such as\npruning and distillation. However, these techniques either require labeled\ndata, or are time-consuming as they require the compressed model to be\nretrained to regain accuracy. In this paper, we propose a gradient-free\nstructured pruning framework that uses only unlabeled data. An evaluation on\nthe GLUE and SQuAD benchmarks using BERT$_{BASE}$ and DistilBERT illustrates\nthe effectiveness of the proposed approach. By only using the weights of the\npre-trained model and unlabeled data, in a matter of a few minutes on a single\nGPU, up to 40% of the original FLOP count can be reduced with less than a 4%\naccuracy loss across all tasks considered.", "published": "2023-03-07 19:12:31", "link": "http://arxiv.org/abs/2303.04185v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A Comprehensive Survey of AI-Generated Content (AIGC): A History of\n  Generative AI from GAN to ChatGPT", "abstract": "Recently, ChatGPT, along with DALL-E-2 and Codex,has been gaining significant\nattention from society. As a result, many individuals have become interested in\nrelated resources and are seeking to uncover the background and secrets behind\nits impressive performance. In fact, ChatGPT and other Generative AI (GAI)\ntechniques belong to the category of Artificial Intelligence Generated Content\n(AIGC), which involves the creation of digital content, such as images, music,\nand natural language, through AI models. The goal of AIGC is to make the\ncontent creation process more efficient and accessible, allowing for the\nproduction of high-quality content at a faster pace. AIGC is achieved by\nextracting and understanding intent information from instructions provided by\nhuman, and generating the content according to its knowledge and the intent\ninformation. In recent years, large-scale models have become increasingly\nimportant in AIGC as they provide better intent extraction and thus, improved\ngeneration results. With the growth of data and the size of the models, the\ndistribution that the model can learn becomes more comprehensive and closer to\nreality, leading to more realistic and high-quality content generation. This\nsurvey provides a comprehensive review on the history of generative models, and\nbasic components, recent advances in AIGC from unimodal interaction and\nmultimodal interaction. From the perspective of unimodality, we introduce the\ngeneration tasks and relative models of text and image. From the perspective of\nmultimodality, we introduce the cross-application between the modalities\nmentioned above. Finally, we discuss the existing open problems and future\nchallenges in AIGC.", "published": "2023-03-07 20:36:13", "link": "http://arxiv.org/abs/2303.04226v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "How Do Transformers Learn Topic Structure: Towards a Mechanistic\n  Understanding", "abstract": "While the successes of transformers across many domains are indisputable,\naccurate understanding of the learning mechanics is still largely lacking.\nTheir capabilities have been probed on benchmarks which include a variety of\nstructured and reasoning tasks -- but mathematical understanding is lagging\nsubstantially behind. Recent lines of work have begun studying representational\naspects of this question: that is, the size/depth/complexity of attention-based\nnetworks to perform certain tasks. However, there is no guarantee the learning\ndynamics will converge to the constructions proposed. In our paper, we provide\nfine-grained mechanistic understanding of how transformers learn \"semantic\nstructure\", understood as capturing co-occurrence structure of words.\nPrecisely, we show, through a combination of mathematical analysis and\nexperiments on Wikipedia data and synthetic data modeled by Latent Dirichlet\nAllocation (LDA), that the embedding layer and the self-attention layer encode\nthe topical structure. In the former case, this manifests as higher average\ninner product of embeddings between same-topic words. In the latter, it\nmanifests as higher average pairwise attention between same-topic words. The\nmathematical results involve several assumptions to make the analysis\ntractable, which we verify on data, and might be of independent interest as\nwell.", "published": "2023-03-07 21:42:17", "link": "http://arxiv.org/abs/2303.04245v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Do Prosody Transfer Models Transfer Prosody?", "abstract": "Some recent models for Text-to-Speech synthesis aim to transfer the prosody\nof a reference utterance to the generated target synthetic speech. This is done\nby using a learned embedding of the reference utterance, which is used to\ncondition speech generation. During training, the reference utterance is\nidentical to the target utterance. Yet, during synthesis, these models are\noften used to transfer prosody from a reference that differs from the text or\nspeaker being synthesized.\n  To address this inconsistency, we propose to use a different, but\nprosodically-related, utterance during training too. We believe this should\nencourage the model to learn to transfer only those characteristics that the\nreference and target have in common. If prosody transfer methods do indeed\ntransfer prosody they should be able to be trained in the way we propose.\nHowever, results show that a model trained under these conditions performs\nsignificantly worse than one trained using the target utterance as a reference.\nTo explain this, we hypothesize that prosody transfer models do not learn a\ntransferable representation of prosody, but rather an utterance-level\nrepresentation which is highly dependent on both the reference speaker and\nreference text.", "published": "2023-03-07 23:35:58", "link": "http://arxiv.org/abs/2303.04289v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Making a Computational Attorney", "abstract": "This \"blue sky idea\" paper outlines the opportunities and challenges in data\nmining and machine learning involving making a computational attorney -- an\nintelligent software agent capable of helping human lawyers with a wide range\nof complex high-level legal tasks such as drafting legal briefs for the\nprosecution or defense in court. In particular, we discuss what a ChatGPT-like\nLarge Legal Language Model (L$^3$M) can and cannot do today, which will inspire\nresearchers with promising short-term and long-term research objectives.", "published": "2023-03-07 16:44:29", "link": "http://arxiv.org/abs/2303.05383v1", "categories": ["cs.CL", "cs.CY", "cs.LG", "I.2"], "primary_category": "cs.CL"}
{"title": "Disambiguation of Company names via Deep Recurrent Networks", "abstract": "Name Entity Disambiguation is the Natural Language Processing task of\nidentifying textual records corresponding to the same Named Entity, i.e.\nreal-world entities represented as a list of attributes (names, places,\norganisations, etc.). In this work, we face the task of disambiguating\ncompanies on the basis of their written names. We propose a Siamese LSTM\nNetwork approach to extract -- via supervised learning -- an embedding of\ncompany name strings in a (relatively) low dimensional vector space and use\nthis representation to identify pairs of company names that actually represent\nthe same company (i.e. the same Entity).\n  Given that the manual labelling of string pairs is a rather onerous task, we\nanalyse how an Active Learning approach to prioritise the samples to be\nlabelled leads to a more efficient overall learning pipeline.\n  With empirical investigations, we show that our proposed Siamese Network\noutperforms several benchmark approaches based on standard string matching\nalgorithms when enough labelled data are available. Moreover, we show that\nActive Learning prioritisation is indeed helpful when labelling resources are\nlimited, and let the learning models reach the out-of-sample performance\nsaturation with less labelled data with respect to standard (random) data\nlabelling approaches.", "published": "2023-03-07 15:07:57", "link": "http://arxiv.org/abs/2303.05391v2", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Automatically Summarizing Evidence from Clinical Trials: A Prototype\n  Highlighting Current Challenges", "abstract": "We present TrialsSummarizer, a system that aims to automatically summarize\nevidence presented in the set of randomized controlled trials most relevant to\na given query. Building on prior work, the system retrieves trial publications\nmatching a query specifying a combination of condition, intervention(s), and\noutcome(s), and ranks these according to sample size and estimated study\nquality. The top-k such studies are passed through a neural multi-document\nsummarization system, yielding a synopsis of these trials. We consider two\narchitectures: A standard sequence-to-sequence model based on BART and a\nmulti-headed architecture intended to provide greater transparency to\nend-users. Both models produce fluent and relevant summaries of evidence\nretrieved for queries, but their tendency to introduce unsupported statements\nrender them inappropriate for use in this domain at present. The proposed\narchitecture may help users verify outputs allowing users to trace generated\ntokens back to inputs.", "published": "2023-03-07 17:30:48", "link": "http://arxiv.org/abs/2303.05392v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Self-FiLM: Conditioning GANs with self-supervised representations for\n  bandwidth extension based speaker recognition", "abstract": "Speech super-resolution/Bandwidth Extension (BWE) can improve downstream\ntasks like Automatic Speaker Verification (ASV). We introduce a simple novel\ntechnique called Self-FiLM to inject self-supervision into existing BWE models\nvia Feature-wise Linear Modulation. We hypothesize that such information\ncaptures domain/environment information, which can give zero-shot\ngeneralization. Self-FiLM Conditional GAN (CGAN) gives 18% relative improvement\nin Equal Error Rate and 8.5% in minimum Decision Cost Function using\nstate-of-the-art ASV system on SRE21 test. We further by 1) deep feature loss\nfrom time-domain models and 2) re-training of data2vec 2.0 models on\nnaturalistic wideband (VoxCeleb) and telephone data (SRE Superset etc.).\nLastly, we integrate self-supervision with CycleGAN to present a completely\nunsupervised solution that matches the semi-supervised performance.", "published": "2023-03-07 05:28:01", "link": "http://arxiv.org/abs/2303.03657v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "AST-SED: An Effective Sound Event Detection Method Based on Audio\n  Spectrogram Transformer", "abstract": "In this paper, we propose an effective sound event detection (SED) method\nbased on the audio spectrogram transformer (AST) model, pretrained on the\nlarge-scale AudioSet for audio tagging (AT) task, termed AST-SED. Pretrained\nAST models have recently shown promise on DCASE2022 challenge task4 where they\nhelp mitigate a lack of sufficient real annotated data. However, mainly due to\ndifferences between the AT and SED tasks, it is suboptimal to directly utilize\noutputs from a pretrained AST model. Hence the proposed AST-SED adopts an\nencoder-decoder architecture to enable effective and efficient fine-tuning\nwithout needing to redesign or retrain the AST model. Specifically, the\nFrequency-wise Transformer Encoder (FTE) consists of transformers with self\nattention along the frequency axis to address multiple overlapped audio events\nissue in a single clip. The Local Gated Recurrent Units Decoder (LGD) consists\nof nearest-neighbor interpolation (NNI) and Bidirectional Gated Recurrent Units\n(Bi-GRU) to compensate for temporal resolution loss in the pretrained AST model\noutput. Experimental results on DCASE2022 task4 development set have\ndemonstrated the superiority of the proposed AST-SED with FTE-LGD architecture.\nSpecifically, the Event-Based F1-score (EB-F1) of 59.60% and Polyphonic Sound\ndetection Score scenario1 (PSDS1) score of 0.5140 significantly outperform CRNN\nand other pretrained AST-based systems.", "published": "2023-03-07 07:13:22", "link": "http://arxiv.org/abs/2303.03689v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improving Self-Supervised Learning for Audio Representations by Feature\n  Diversity and Decorrelation", "abstract": "Self-supervised learning (SSL) has recently shown remarkable results in\nclosing the gap between supervised and unsupervised learning. The idea is to\nlearn robust features that are invariant to distortions of the input data.\nDespite its success, this idea can suffer from a collapsing issue where the\nnetwork produces a constant representation. To this end, we introduce SELFIE, a\nnovel Self-supervised Learning approach for audio representation via Feature\nDiversity and Decorrelation. SELFIE avoids the collapsing issue by ensuring\nthat the representation (i) maintains a high diversity among embeddings and\n(ii) decorrelates the dependencies between dimensions. SELFIE is pre-trained on\nthe large-scale AudioSet dataset and its embeddings are validated on nine audio\ndownstream tasks, including speech, music, and sound event recognition.\nExperimental results show that SELFIE outperforms existing SSL methods in\nseveral tasks.", "published": "2023-03-07 07:57:49", "link": "http://arxiv.org/abs/2303.03717v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "TS-SEP: Joint Diarization and Separation Conditioned on Estimated\n  Speaker Embeddings", "abstract": "Since diarization and source separation of meeting data are closely related\ntasks, we here propose an approach to perform the two objectives jointly. It\nbuilds upon the target-speaker voice activity detection (TS-VAD) diarization\napproach, which assumes that initial speaker embeddings are available. We\nreplace the final combined speaker activity estimation network of TS-VAD with a\nnetwork that produces speaker activity estimates at a time-frequency\nresolution. Those act as masks for source extraction, either via masking or via\nbeamforming. The technique can be applied both for single-channel and\nmulti-channel input and, in both cases, achieves a new state-of-the-art word\nerror rate (WER) on the LibriCSS meeting data recognition task. We further\ncompute speaker-aware and speaker-agnostic WERs to isolate the contribution of\ndiarization errors to the overall WER performance.", "published": "2023-03-07 12:31:18", "link": "http://arxiv.org/abs/2303.03849v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Kernel interpolation of acoustic transfer functions with adaptive kernel\n  for directed and residual reverberations", "abstract": "An interpolation method for region-to-region acoustic transfer functions\n(ATFs) based on kernel ridge regression with an adaptive kernel is proposed.\nMost current ATF interpolation methods do not incorporate the acoustic\nproperties for which measurements are performed. Our proposed method is based\non a separate adaptation of directional weighting functions to directed and\nresidual reverberations, which are used for adapting kernel functions. Thus,\nthe proposed method can not only impose constraints on fundamental acoustic\nproperties, but can also adapt to the acoustic environment. Numerical\nexperimental results indicated that our proposed method outperforms the current\nmethods in terms of interpolation accuracy, especially at high frequencies.", "published": "2023-03-07 13:14:01", "link": "http://arxiv.org/abs/2303.03869v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Approach to Learning Generalized Audio Representation Through Batch\n  Embedding Covariance Regularization and Constant-Q Transforms", "abstract": "General-purpose embedding is highly desirable for few-shot even zero-shot\nlearning in many application scenarios, including audio tasks. In order to\nunderstand representations better, we conducted a thorough error analysis and\nvisualization of HEAR 2021 submission results. Inspired by the analysis, this\nwork experiments with different front-end audio preprocessing methods,\nincluding Constant-Q Transform (CQT) and Short-time Fourier transform (STFT),\nand proposes a Batch Embedding Covariance Regularization (BECR) term to uncover\na more holistic simulation of the frequency information received by the human\nauditory system. We tested the models on the suite of HEAR 2021 tasks, which\nencompass a broad category of tasks. Preliminary results show (1) the proposed\nBECR can incur a more dispersed embedding on the test set, (2) BECR improves\nthe PaSST model without extra computation complexity, and (3) STFT\npreprocessing outperforms CQT in all tasks we tested.\nGithub:https://github.com/ankitshah009/general_audio_embedding_hear_2021", "published": "2023-03-07 01:54:24", "link": "http://arxiv.org/abs/2303.03591v1", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Face: Fast, Accurate and Context-Aware Audio Annotation and\n  Classification", "abstract": "This paper presents a context-aware framework for feature selection and\nclassification procedures to realize a fast and accurate audio event annotation\nand classification. The context-aware design starts with exploring feature\nextraction techniques to find an appropriate combination to select a set\nresulting in remarkable classification accuracy with minimal computational\neffort. The exploration for feature selection also embraces an investigation of\naudio Tempo representation, an advantageous feature extraction method missed by\nprevious works in the environmental audio classification research scope. The\nproposed annotation method considers outlier, inlier, and hard-to-predict data\nsamples to realize context-aware Active Learning, leading to the average\naccuracy of 90% when only 15% of data possess initial annotation. Our proposed\nalgorithm for sound classification obtained average prediction accuracy of\n98.05% on the UrbanSound8K dataset. The notebooks containing our source codes\nand implementation results are available at https://github.com/gitmehrdad/FACE.", "published": "2023-03-07 06:04:58", "link": "http://arxiv.org/abs/2303.03666v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Multi-Stage Triple-Path Method for Speech Separation in Noisy and\n  Reverberant Environments", "abstract": "In noisy and reverberant environments, the performance of deep learning-based\nspeech separation methods drops dramatically because previous methods are not\ndesigned and optimized for such situations. To address this issue, we propose a\nmulti-stage end-to-end learning method that decouples the difficult speech\nseparation problem in noisy and reverberant environments into three\nsub-problems: speech denoising, separation, and de-reverberation. The\nprobability and speed of searching for the optimal solution of the speech\nseparation model are improved by reducing the solution space. Moreover, since\nthe channel information of the audio sequence in the time domain is crucial for\nspeech separation, we propose a triple-path structure capable of modeling the\nchannel dimension of audio sequences. Experimental results show that the\nproposed multi-stage triple-path method can improve the performance of speech\nseparation models at the cost of little model parameter increment.", "published": "2023-03-07 08:44:58", "link": "http://arxiv.org/abs/2303.03732v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multi-Dimensional and Multi-Scale Modeling for Speech Separation\n  Optimized by Discriminative Learning", "abstract": "Transformer has shown advanced performance in speech separation, benefiting\nfrom its ability to capture global features. However, capturing local features\nand channel information of audio sequences in speech separation is equally\nimportant. In this paper, we present a novel approach named Intra-SE-Conformer\nand Inter-Transformer (ISCIT) for speech separation. Specifically, we design a\nnew network SE-Conformer that can model audio sequences in multiple dimensions\nand scales, and apply it to the dual-path speech separation framework.\nFurthermore, we propose Multi-Block Feature Aggregation to improve the\nseparation effect by selectively utilizing information from the intermediate\nblocks of the separation network. Meanwhile, we propose a speaker similarity\ndiscriminative loss to optimize the speech separation model to address the\nproblem of poor performance when speakers have similar voices. Experimental\nresults on the benchmark datasets WSJ0-2mix and WHAM! show that ISCIT can\nachieve state-of-the-art results.", "published": "2023-03-07 08:53:20", "link": "http://arxiv.org/abs/2303.03737v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Leveraging Pre-trained AudioLDM for Sound Generation: A Benchmark Study", "abstract": "Deep neural networks have recently achieved breakthroughs in sound\ngeneration. Despite the outstanding sample quality, current sound generation\nmodels face issues on small-scale datasets (e.g., overfitting), significantly\nlimiting performance. In this paper, we make the first attempt to investigate\nthe benefits of pre-training on sound generation with AudioLDM, the\ncutting-edge model for audio generation, as the backbone. Our study\ndemonstrates the advantages of the pre-trained AudioLDM, especially in\ndata-scarcity scenarios. In addition, the baselines and evaluation protocol for\nsound generation systems are not consistent enough to compare different studies\ndirectly. Aiming to facilitate further study on sound generation tasks, we\nbenchmark the sound generation task on various frequently-used datasets. We\nhope our results on transfer learning and benchmarks can provide references for\nfurther research on conditional sound generation.", "published": "2023-03-07 12:49:45", "link": "http://arxiv.org/abs/2303.03857v3", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "VOCALExplore: Pay-as-You-Go Video Data Exploration and Model Building\n  [Technical Report]", "abstract": "We introduce VOCALExplore, a system designed to support users in building\ndomain-specific models over video datasets. VOCALExplore supports interactive\nlabeling sessions and trains models using user-supplied labels. VOCALExplore\nmaximizes model quality by automatically deciding how to select samples based\non observed skew in the collected labels. It also selects the optimal video\nrepresentations to use when training models by casting feature selection as a\nrising bandit problem. Finally, VOCALExplore implements optimizations to\nachieve low latency without sacrificing model performance. We demonstrate that\nVOCALExplore achieves close to the best possible model quality given candidate\nacquisition functions and feature extractors, and it does so with low visible\nlatency (~1 second per iteration) and no expensive preprocessing.", "published": "2023-03-07 17:26:04", "link": "http://arxiv.org/abs/2303.04068v4", "categories": ["cs.DB", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.DB"}
{"title": "An Inception-Residual-Based Architecture with Multi-Objective Loss for\n  Detecting Respiratory Anomalies", "abstract": "This paper presents a deep learning system applied for detecting anomalies\nfrom respiratory sound recordings. Initially, our system begins with audio\nfeature extraction using Gammatone and Continuous Wavelet transformation. This\nstep aims to transform the respiratory sound input into a two-dimensional\nspectrogram where both spectral and temporal features are presented. Then, our\nproposed system integrates Inception-residual-based backbone models combined\nwith multi-head attention and multi-objective loss to classify respiratory\nanomalies. Instead of applying a simple concatenation approach by combining\nresults from various spectrograms, we propose a Linear combination, which has\nthe ability to regulate equally the contribution of each individual spectrogram\nthroughout the training process. To evaluate the performance, we conducted\nexperiments over the benchmark dataset of SPRSound (The Open-Source SJTU\nPaediatric Respiratory Sound) proposed by the IEEE BioCAS 2022 challenge. As\nregards the Score computed by an average between the average score and harmonic\nscore, our proposed system gained significant improvements of 9.7%, 15.8%,\n17.8%, and 16.1% in Task 1-1, Task 1-2, Task 2-1, and Task 2-2, respectively,\ncompared to the challenge baseline system. Notably, we achieved the Top-1\nperformance in Task 2-1 and Task 2-2 with the highest Score of 74.5% and 53.9%,\nrespectively.", "published": "2023-03-07 18:10:05", "link": "http://arxiv.org/abs/2303.04104v2", "categories": ["cs.SD", "cs.LG", "eess.AS", "q-bio.QM"], "primary_category": "cs.SD"}
{"title": "Self-supervised speech representation learning for keyword-spotting with\n  light-weight transformers", "abstract": "Self-supervised speech representation learning (S3RL) is revolutionizing the\nway we leverage the ever-growing availability of data. While S3RL related\nstudies typically use large models, we employ light-weight networks to comply\nwith tight memory of compute-constrained devices. We demonstrate the\neffectiveness of S3RL on a keyword-spotting (KS) problem by using transformers\nwith 330k parameters and propose a mechanism to enhance utterance-wise\ndistinction, which proves crucial for improving performance on classification\ntasks. On the Google speech commands v2 dataset, the proposed method applied to\nthe Auto-Regressive Predictive Coding S3RL led to a 1.2% accuracy improvement\ncompared to training from scratch. On an in-house KS dataset with four\ndifferent keywords, it provided 6% to 23.7% relative false accept improvement\nat fixed false reject rate. We argue this demonstrates the applicability of\nS3RL approaches to light-weight models for KS and confirms S3RL is a powerful\nalternative to traditional supervised learning for resource-constrained\napplications.", "published": "2023-03-07 21:54:35", "link": "http://arxiv.org/abs/2303.04255v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speech Modeling with a Hierarchical Transformer Dynamical VAE", "abstract": "The dynamical variational autoencoders (DVAEs) are a family of\nlatent-variable deep generative models that extends the VAE to model a sequence\nof observed data and a corresponding sequence of latent vectors. In almost all\nthe DVAEs of the literature, the temporal dependencies within each sequence and\nacross the two sequences are modeled with recurrent neural networks. In this\npaper, we propose to model speech signals with the Hierarchical Transformer\nDVAE (HiT-DVAE), which is a DVAE with two levels of latent variable\n(sequence-wise and frame-wise) and in which the temporal dependencies are\nimplemented with the Transformer architecture. We show that HiT-DVAE\noutperforms several other DVAEs for speech spectrogram modeling, while enabling\na simpler training procedure, revealing its high potential for downstream\nlow-level speech processing tasks such as speech enhancement.", "published": "2023-03-07 13:35:45", "link": "http://arxiv.org/abs/2303.09404v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
