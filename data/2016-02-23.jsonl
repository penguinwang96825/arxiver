{"title": "Sentence Similarity Learning by Lexical Decomposition and Composition", "abstract": "Most conventional sentence similarity methods only focus on similar parts of\ntwo input sentences, and simply ignore the dissimilar parts, which usually give\nus some clues and semantic meanings about the sentences. In this work, we\npropose a model to take into account both the similarities and dissimilarities\nby decomposing and composing lexical semantics over sentences. The model\nrepresents each word as a vector, and calculates a semantic matching vector for\neach word based on all words in the other sentence. Then, each word vector is\ndecomposed into a similar component and a dissimilar component based on the\nsemantic matching vector. After this, a two-channel CNN model is employed to\ncapture features by composing the similar and dissimilar components. Finally, a\nsimilarity score is estimated over the composed feature vectors. Experimental\nresults show that our model gets the state-of-the-art performance on the answer\nsentence selection task, and achieves a comparable result on the paraphrase\nidentification task.", "published": "2016-02-23 03:08:50", "link": "http://arxiv.org/abs/1602.07019v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Petrarch 2 : Petrarcher", "abstract": "PETRARCH 2 is the fourth generation of a series of Event-Data coders stemming\nfrom research by Phillip Schrodt. Each iteration has brought new functionality\nand usability, and this is no exception.Petrarch 2 takes much of the power of\nthe original Petrarch's dictionaries and redirects it into a faster and smarter\ncore logic. Earlier iterations handled sentences largely as a list of words,\nincorporating some syntactic information here and there. Petrarch 2 now views\nthe sentence entirely on the syntactic level. It receives the syntactic parse\nof a sentence from the Stanford CoreNLP software, and stores this data as a\ntree structure of linked nodes, where each node is a Phrase object.\nPrepositional, noun, and verb phrases each have their own version of this\nPhrase class, which deals with the logic particular to those kinds of phrases.\nSince this is an event coder, the core of the logic focuses around the verbs:\nwho is acting, who is being acted on, and what is happening. The theory behind\nthis new structure and its logic is founded in Generative Grammar, Information\nTheory, and Lambda-Calculus Semantics.", "published": "2016-02-23 17:05:06", "link": "http://arxiv.org/abs/1602.07236v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The IBM 2016 Speaker Recognition System", "abstract": "In this paper we describe the recent advancements made in the IBM i-vector\nspeaker recognition system for conversational speech. In particular, we\nidentify key techniques that contribute to significant improvements in\nperformance of our system, and quantify their contributions. The techniques\ninclude: 1) a nearest-neighbor discriminant analysis (NDA) approach that is\nformulated to alleviate some of the limitations associated with the\nconventional linear discriminant analysis (LDA) that assumes Gaussian\nclass-conditional distributions, 2) the application of speaker- and\nchannel-adapted features, which are derived from an automatic speech\nrecognition (ASR) system, for speaker recognition, and 3) the use of a deep\nneural network (DNN) acoustic model with a large number of output units (~10k\nsenones) to compute the frame-level soft alignments required in the i-vector\nestimation process. We evaluate these techniques on the NIST 2010 speaker\nrecognition evaluation (SRE) extended core conditions involving telephone and\nmicrophone trials. Experimental results indicate that: 1) the NDA is more\neffective (up to 35% relative improvement in terms of EER) than the traditional\nparametric LDA for speaker recognition, 2) when compared to raw acoustic\nfeatures (e.g., MFCCs), the ASR speaker-adapted features provide gains in\nspeaker recognition performance, and 3) increasing the number of output units\nin the DNN acoustic model (i.e., increasing the senone set size from 2k to 10k)\nprovides consistent improvements in performance (for example from 37% to 57%\nrelative EER gains over our baseline GMM i-vector system). To our knowledge,\nresults reported in this paper represent the best performances published to\ndate on the NIST SRE 2010 extended core tasks.", "published": "2016-02-23 20:39:40", "link": "http://arxiv.org/abs/1602.07291v1", "categories": ["cs.SD", "cs.CL", "stat.ML"], "primary_category": "cs.SD"}
