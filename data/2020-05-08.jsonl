{"title": "Context-Sensitive Generation Network for Handing Unknown Slot Values in\n  Dialogue State Tracking", "abstract": "As a key component in a dialogue system, dialogue state tracking plays an\nimportant role. It is very important for dialogue state tracking to deal with\nthe problem of unknown slot values. As far as we known, almost all existing\napproaches depend on pointer network to solve the unknown slot value problem.\nThese pointer network-based methods usually have a hidden assumption that there\nis at most one out-of-vocabulary word in an unknown slot value because of the\ncharacter of a pointer network. However, often, there are multiple\nout-of-vocabulary words in an unknown slot value, and it makes the existing\nmethods perform bad. To tackle the problem, in this paper, we propose a novel\nContext-Sensitive Generation network (CSG) which can facilitate the\nrepresentation of out-of-vocabulary words when generating the unknown slot\nvalue. Extensive experiments show that our proposed method performs better than\nthe state-of-the-art baselines.", "published": "2020-05-08 09:22:33", "link": "http://arxiv.org/abs/2005.03923v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Detect Unacceptable Machine Translations for Downstream\n  Tasks", "abstract": "The field of machine translation has progressed tremendously in recent years.\nEven though the translation quality has improved significantly, current systems\nare still unable to produce uniformly acceptable machine translations for the\nvariety of possible use cases. In this work, we put machine translation in a\ncross-lingual pipeline and introduce downstream tasks to define task-specific\nacceptability of machine translations. This allows us to leverage parallel data\nto automatically generate acceptability annotations on a large scale, which in\nturn help to learn acceptability detectors for the downstream tasks. We conduct\nexperiments to demonstrate the effectiveness of our framework for a range of\ndownstream tasks and translation models.", "published": "2020-05-08 09:37:19", "link": "http://arxiv.org/abs/2005.03925v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Literature Triage on Genomic Variation Publications by\n  Knowledge-enhanced Multi-channel CNN", "abstract": "Background: To investigate the correlation between genomic variation and\ncertain diseases or phenotypes, the fundamental task is to screen out the\nconcerning publications from massive literature, which is called literature\ntriage. Some knowledge bases, including UniProtKB/Swiss-Prot and NHGRI-EBI GWAS\nCatalog are created for collecting concerning publications. These publications\nare manually curated by experts, which is time-consuming. Moreover, the manual\ncuration of information from literature is not scalable due to the rapidly\nincreasing amount of publications. In order to cut down the cost of literature\ntriage, machine-learning models were adopted to automatically identify\nbiomedical publications. Methods: Comparing to previous studies utilizing\nmachine-learning models for literature triage, we adopt a multi-channel\nconvolutional network to utilize rich textual information and meanwhile bridge\nthe semantic gaps from different corpora. In addition, knowledge embeddings\nlearned from UMLS is also used to provide extra medical knowledge beyond\ntextual features in the process of triage. Results: We demonstrate that our\nmodel outperforms the state-of-the-art models over 5 datasets with the help of\nknowledge embedding and multiple channels. Our model improves the accuracy of\nbiomedical literature triage results. Conclusions: Multiple channels and\nknowledge embeddings enhance the performance of the CNN model in the task of\nbiomedical literature triage. Keywords: Literature Triage; Knowledge Embedding;\nMulti-channel Convolutional Network", "published": "2020-05-08 13:47:58", "link": "http://arxiv.org/abs/2005.04044v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SentiBERT: A Transferable Transformer-Based Architecture for\n  Compositional Sentiment Semantics", "abstract": "We propose SentiBERT, a variant of BERT that effectively captures\ncompositional sentiment semantics. The model incorporates contextualized\nrepresentation with binary constituency parse tree to capture semantic\ncomposition. Comprehensive experiments demonstrate that SentiBERT achieves\ncompetitive performance on phrase-level sentiment classification. We further\ndemonstrate that the sentiment composition learned from the phrase-level\nannotations on SST can be transferred to other sentiment analysis tasks as well\nas related tasks, such as emotion classification tasks. Moreover, we conduct\nablation studies and design visualization methods to understand SentiBERT. We\nshow that SentiBERT is better than baseline approaches in capturing negation\nand the contrastive relation and model the compositional sentiment semantics.", "published": "2020-05-08 15:40:17", "link": "http://arxiv.org/abs/2005.04114v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evidence Inference 2.0: More Data, Better Models", "abstract": "How do we most effectively treat a disease or condition? Ideally, we could\nconsult a database of evidence gleaned from clinical trials to answer such\nquestions. Unfortunately, no such database exists; clinical trial results are\ninstead disseminated primarily via lengthy natural language articles. Perusing\nall such articles would be prohibitively time-consuming for healthcare\npractitioners; they instead tend to depend on manually compiled systematic\nreviews of medical literature to inform care.\n  NLP may speed this process up, and eventually facilitate immediate consult of\npublished evidence. The Evidence Inference dataset was recently released to\nfacilitate research toward this end. This task entails inferring the\ncomparative performance of two treatments, with respect to a given outcome,\nfrom a particular article (describing a clinical trial) and identifying\nsupporting evidence. For instance: Does this article report that chemotherapy\nperformed better than surgery for five-year survival rates of operable cancers?\nIn this paper, we collect additional annotations to expand the Evidence\nInference dataset by 25\\%, provide stronger baseline models, systematically\ninspect the errors that these make, and probe dataset quality. We also release\nan abstract only (as opposed to full-texts) version of the task for rapid model\nprototyping. The updated corpus, documentation, and code for new baselines and\nevaluations are available at http://evidence-inference.ebm-nlp.com/.", "published": "2020-05-08 17:16:35", "link": "http://arxiv.org/abs/2005.04177v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Balancing Objectives in Counseling Conversations: Advancing Forwards or\n  Looking Backwards", "abstract": "Throughout a conversation, participants make choices that can orient the flow\nof the interaction. Such choices are particularly salient in the consequential\ndomain of crisis counseling, where a difficulty for counselors is balancing\nbetween two key objectives: advancing the conversation towards a resolution,\nand empathetically addressing the crisis situation.\n  In this work, we develop an unsupervised methodology to quantify how\ncounselors manage this balance. Our main intuition is that if an utterance can\nonly receive a narrow range of appropriate replies, then its likely aim is to\nadvance the conversation forwards, towards a target within that range.\nLikewise, an utterance that can only appropriately follow a narrow range of\npossible utterances is likely aimed backwards at addressing a specific\nsituation within that range. By applying this intuition, we can map each\nutterance to a continuous orientation axis that captures the degree to which it\nis intended to direct the flow of the conversation forwards or backwards.\n  This unsupervised method allows us to characterize counselor behaviors in a\nlarge dataset of crisis counseling conversations, where we show that known\ncounseling strategies intuitively align with this axis. We also illustrate how\nour measure can be indicative of a conversation's progress, as well as its\neffectiveness.", "published": "2020-05-08 18:00:27", "link": "http://arxiv.org/abs/2005.04245v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Temporal Common Sense Acquisition with Minimal Supervision", "abstract": "Temporal common sense (e.g., duration and frequency of events) is crucial for\nunderstanding natural language. However, its acquisition is challenging, partly\nbecause such information is often not expressed explicitly in text, and human\nannotation on such concepts is costly. This work proposes a novel sequence\nmodeling approach that exploits explicit and implicit mentions of temporal\ncommon sense, extracted from a large corpus, to build TACOLM, a temporal common\nsense language model. Our method is shown to give quality predictions of\nvarious dimensions of temporal common sense (on UDST and a newly collected\ndataset from RealNews). It also produces representations of events for relevant\ntasks such as duration comparison, parent-child relations, event coreference\nand temporal QA (on TimeBank, HiEVE and MCTACO) that are better than using the\nstandard BERT. Thus, it will be an important component of temporal NLP.", "published": "2020-05-08 22:20:16", "link": "http://arxiv.org/abs/2005.04304v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Probing Linguistic Systematicity", "abstract": "Recently, there has been much interest in the question of whether deep\nnatural language understanding models exhibit systematicity; generalizing such\nthat units like words make consistent contributions to the meaning of the\nsentences in which they appear. There is accumulating evidence that neural\nmodels often generalize non-systematically. We examined the notion of\nsystematicity from a linguistic perspective, defining a set of probes and a set\nof metrics to measure systematic behaviour. We also identified ways in which\nnetwork architectures can generalize non-systematically, and discuss why such\nforms of generalization may be unsatisfying. As a case study, we performed a\nseries of experiments in the setting of natural language inference (NLI),\ndemonstrating that some NLU systems achieve high overall performance despite\nbeing non-systematic.", "published": "2020-05-08 23:31:31", "link": "http://arxiv.org/abs/2005.04315v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparative Analysis of Word Embeddings for Capturing Word Similarities", "abstract": "Distributed language representation has become the most widely used technique\nfor language representation in various natural language processing tasks. Most\nof the natural language processing models that are based on deep learning\ntechniques use already pre-trained distributed word representations, commonly\ncalled word embeddings. Determining the most qualitative word embeddings is of\ncrucial importance for such models. However, selecting the appropriate word\nembeddings is a perplexing task since the projected embedding space is not\nintuitive to humans. In this paper, we explore different approaches for\ncreating distributed word representations. We perform an intrinsic evaluation\nof several state-of-the-art word embedding methods. Their performance on\ncapturing word similarities is analysed with existing benchmark datasets for\nword pairs similarities. The research in this paper conducts a correlation\nanalysis between ground truth word similarities and similarities obtained by\ndifferent word embedding methods.", "published": "2020-05-08 01:16:03", "link": "http://arxiv.org/abs/2005.03812v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Synchronous Bidirectional Learning for Multilingual Lip Reading", "abstract": "Lip reading has received increasing attention in recent years. This paper\nfocuses on the synergy of multilingual lip reading. There are about as many as\n7000 languages in the world, which implies that it is impractical to train\nseparate lip reading models with large-scale data for each language. Although\neach language has its own linguistic and pronunciation rules, the lip movements\nof all languages share similar patterns due to the common structures of human\norgans. Based on this idea, we try to explore the synergized learning of\nmultilingual lip reading in this paper, and further propose a synchronous\nbidirectional learning (SBL) framework for effective synergy of multilingual\nlip reading. We firstly introduce phonemes as our modeling units for the\nmultilingual setting here. Phonemes are more closely related with the lip\nmovements than the alphabet letters. At the same time, similar phonemes always\nlead to similar visual patterns no matter which type the target language is.\nThen, a novel SBL block is proposed to learn the rules for each language in a\nfill-in-the-blank way. Specifically, the model has to learn to infer the target\nunit given its bidirectional context, which could represent the composition\nrules of phonemes for each language. To make the learning process more targeted\nat each particular language, an extra task of predicting the language identity\nis introduced in the learning process. Finally, a thorough comparison on LRW\n(English) and LRW-1000 (Mandarin) is performed, which shows the promising\nbenefits from the synergized learning of different languages and also reports a\nnew state-of-the-art result on both datasets.", "published": "2020-05-08 04:19:57", "link": "http://arxiv.org/abs/2005.03846v4", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Towards Conversational Recommendation over Multi-Type Dialogs", "abstract": "We propose a new task of conversational recommendation over multi-type\ndialogs, where the bots can proactively and naturally lead a conversation from\na non-recommendation dialog (e.g., QA) to a recommendation dialog, taking into\naccount user's interests and feedback. To facilitate the study of this task, we\ncreate a human-to-human Chinese dialog dataset \\emph{DuRecDial} (about 10k\ndialogs, 156k utterances), which contains multiple sequential dialogs for every\npair of a recommendation seeker (user) and a recommender (bot). In each dialog,\nthe recommender proactively leads a multi-type dialog to approach\nrecommendation targets and then makes multiple recommendations with rich\ninteraction behavior. This dataset allows us to systematically investigate\ndifferent parts of the overall problem, e.g., how to naturally lead a dialog,\nhow to interact with users for recommendation. Finally we establish baseline\nresults on DuRecDial for future studies. Dataset and codes are publicly\navailable at\nhttps://github.com/PaddlePaddle/models/tree/develop/PaddleNLP/Research/ACL2020-DuRecDial.", "published": "2020-05-08 11:01:21", "link": "http://arxiv.org/abs/2005.03954v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On Vocabulary Reliance in Scene Text Recognition", "abstract": "The pursuit of high performance on public benchmarks has been the driving\nforce for research in scene text recognition, and notable progress has been\nachieved. However, a close investigation reveals a startling fact that the\nstate-of-the-art methods perform well on images with words within vocabulary\nbut generalize poorly to images with words outside vocabulary. We call this\nphenomenon \"vocabulary reliance\". In this paper, we establish an analytical\nframework to conduct an in-depth study on the problem of vocabulary reliance in\nscene text recognition. Key findings include: (1) Vocabulary reliance is\nubiquitous, i.e., all existing algorithms more or less exhibit such\ncharacteristic; (2) Attention-based decoders prove weak in generalizing to\nwords outside vocabulary and segmentation-based decoders perform well in\nutilizing visual features; (3) Context modeling is highly coupled with the\nprediction layers. These findings provide new insights and can benefit future\nresearch in scene text recognition. Furthermore, we propose a simple yet\neffective mutual learning strategy to allow models of two families\n(attention-based and segmentation-based) to learn collaboratively. This remedy\nalleviates the problem of vocabulary reliance and improves the overall scene\ntext recognition performance.", "published": "2020-05-08 11:16:58", "link": "http://arxiv.org/abs/2005.03959v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Beyond Accuracy: Behavioral Testing of NLP models with CheckList", "abstract": "Although measuring held-out accuracy has been the primary approach to\nevaluate generalization, it often overestimates the performance of NLP models,\nwhile alternative approaches for evaluating models either focus on individual\ntasks or on specific behaviors. Inspired by principles of behavioral testing in\nsoftware engineering, we introduce CheckList, a task-agnostic methodology for\ntesting NLP models. CheckList includes a matrix of general linguistic\ncapabilities and test types that facilitate comprehensive test ideation, as\nwell as a software tool to generate a large and diverse number of test cases\nquickly. We illustrate the utility of CheckList with tests for three tasks,\nidentifying critical failures in both commercial and state-of-art models. In a\nuser study, a team responsible for a commercial sentiment analysis model found\nnew and actionable bugs in an extensively tested model. In another user study,\nNLP practitioners with CheckList created twice as many tests, and found almost\nthree times as many bugs as users without it.", "published": "2020-05-08 15:48:31", "link": "http://arxiv.org/abs/2005.04118v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Quantum Natural Language Processing on Near-Term Quantum Computers", "abstract": "In this work, we describe a full-stack pipeline for natural language\nprocessing on near-term quantum computers, aka QNLP. The language-modelling\nframework we employ is that of compositional distributional semantics\n(DisCoCat), which extends and complements the compositional structure of\npregroup grammars. Within this model, the grammatical reduction of a sentence\nis interpreted as a diagram, encoding a specific interaction of words according\nto the grammar. It is this interaction which, together with a specific choice\nof word embedding, realises the meaning (or \"semantics\") of a sentence.\nBuilding on the formal quantum-like nature of such interactions, we present a\nmethod for mapping DisCoCat diagrams to quantum circuits. Our methodology is\ncompatible both with NISQ devices and with established Quantum Machine Learning\ntechniques, paving the way to near-term applications of quantum technology to\nnatural language processing.", "published": "2020-05-08 16:42:54", "link": "http://arxiv.org/abs/2005.04147v2", "categories": ["cs.CL", "quant-ph"], "primary_category": "cs.CL"}
{"title": "ConvoKit: A Toolkit for the Analysis of Conversations", "abstract": "This paper describes the design and functionality of ConvoKit, an open-source\ntoolkit for analyzing conversations and the social interactions embedded\nwithin. ConvoKit provides an unified framework for representing and\nmanipulating conversational data, as well as a large and diverse collection of\nconversational datasets. By providing an intuitive interface for exploring and\ninteracting with conversational data, this toolkit lowers the technical\nbarriers for the broad adoption of computational methods for conversational\nanalysis.", "published": "2020-05-08 18:00:28", "link": "http://arxiv.org/abs/2005.04246v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Adversarial Learning for Supervised and Semi-supervised Relation\n  Extraction in Biomedical Literature", "abstract": "Adversarial training is a technique of improving model performance by\ninvolving adversarial examples in the training process. In this paper, we\ninvestigate adversarial training with multiple adversarial examples to benefit\nthe relation extraction task. We also apply adversarial training technique in\nsemi-supervised scenarios to utilize unlabeled data. The evaluation results on\nprotein-protein interaction and protein subcellular localization task\nillustrate adversarial training provides improvement on the supervised model,\nand is also effective on involving unlabeled data in the semi-supervised\ntraining case. In addition, our method achieves state-of-the-art performance on\ntwo benchmarking datasets.", "published": "2020-05-08 20:19:26", "link": "http://arxiv.org/abs/2005.04277v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Neural Machine Translation for South Africa's Official Languages", "abstract": "Recent advances in neural machine translation (NMT) have led to\nstate-of-the-art results for many European-based translation tasks. However,\ndespite these advances, there is has been little focus in applying these\nmethods to African languages. In this paper, we seek to address this gap by\ncreating an NMT benchmark BLEU score between English and the ten remaining\nofficial languages in South Africa.", "published": "2020-05-08 08:36:59", "link": "http://arxiv.org/abs/2005.06609v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Comparative Analysis of Text Classification Approaches in Electronic\n  Health Records", "abstract": "Text classification tasks which aim at harvesting and/or organizing\ninformation from electronic health records are pivotal to support clinical and\ntranslational research. However these present specific challenges compared to\nother classification tasks, notably due to the particular nature of the medical\nlexicon and language used in clinical records. Recent advances in embedding\nmethods have shown promising results for several clinical tasks, yet there is\nno exhaustive comparison of such approaches with other commonly used word\nrepresentations and classification models. In this work, we analyse the impact\nof various word representations, text pre-processing and classification\nalgorithms on the performance of four different text classification tasks. The\nresults show that traditional approaches, when tailored to the specific\nlanguage and structure of the text inherent to the classification task, can\nachieve or exceed the performance of more recent ones based on contextual\nembeddings such as BERT.", "published": "2020-05-08 14:04:18", "link": "http://arxiv.org/abs/2005.06624v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Personalized Early Stage Alzheimer's Disease Detection: A Case Study of\n  President Reagan's Speeches", "abstract": "Alzheimer`s disease (AD)-related global healthcare cost is estimated to be $1\ntrillion by 2050. Currently, there is no cure for this disease; however,\nclinical studies show that early diagnosis and intervention helps to extend the\nquality of life and inform technologies for personalized mental healthcare.\nClinical research indicates that the onset and progression of Alzheimer`s\ndisease lead to dementia and other mental health issues. As a result, the\nlanguage capabilities of patient start to decline. In this paper, we show that\nmachine learning-based unsupervised clustering of and anomaly detection with\nlinguistic biomarkers are promising approaches for intuitive visualization and\npersonalized early stage detection of Alzheimer`s disease. We demonstrate this\napproach on 10 year`s (1980 to 1989) of President Ronald Reagan`s speech data\nset. Key linguistic biomarkers that indicate early-stage AD are identified.\nExperimental results show that Reagan had early onset of Alzheimer`s sometime\nbetween 1983 and 1987. This finding is corroborated by prior work that analyzed\nhis interviews using a statistical technique. The proposed technique also\nidentifies the exact speeches that reflect linguistic biomarkers for early\nstage AD.", "published": "2020-05-08 13:26:52", "link": "http://arxiv.org/abs/2005.12385v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Distilling Knowledge from Pre-trained Language Models via Text Smoothing", "abstract": "This paper studies compressing pre-trained language models, like BERT (Devlin\net al.,2019), via teacher-student knowledge distillation. Previous works\nusually force the student model to strictly mimic the smoothed labels predicted\nby the teacher BERT. As an alternative, we propose a new method for BERT\ndistillation, i.e., asking the teacher to generate smoothed word ids, rather\nthan labels, for teaching the student model in knowledge distillation. We call\nthis kind of methodTextSmoothing. Practically, we use the softmax prediction of\nthe Masked Language Model(MLM) in BERT to generate word distributions for given\ntexts and smooth those input texts using that predicted soft word ids. We\nassume that both the smoothed labels and the smoothed texts can implicitly\naugment the input corpus, while text smoothing is intuitively more efficient\nsince it can generate more instances in one neural network forward\nstep.Experimental results on GLUE and SQuAD demonstrate that our solution can\nachieve competitive results compared with existing BERT distillation methods.", "published": "2020-05-08 04:34:00", "link": "http://arxiv.org/abs/2005.03848v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Multi-Task Network for Noise-Robust Keyword Spotting and Speaker\n  Verification using CTC-based Soft VAD and Global Query Attention", "abstract": "Keyword spotting (KWS) and speaker verification (SV) have been studied\nindependently although it is known that acoustic and speaker domains are\ncomplementary. In this paper, we propose a multi-task network that performs KWS\nand SV simultaneously to fully utilize the interrelated domain information. The\nmulti-task network tightly combines sub-networks aiming at performance\nimprovement in challenging conditions such as noisy environments,\nopen-vocabulary KWS, and short-duration SV, by introducing novel techniques of\nconnectionist temporal classification (CTC)-based soft voice activity detection\n(VAD) and global query attention. Frame-level acoustic and speaker information\nis integrated with phonetically originated weights so that forms a word-level\nglobal representation. Then it is used for the aggregation of feature vectors\nto generate discriminative embeddings. Our proposed approach shows 4.06% and\n26.71% relative improvements in equal error rate (EER) compared to the\nbaselines for both tasks. We also present a visualization example and results\nof ablation experiments.", "published": "2020-05-08 05:58:46", "link": "http://arxiv.org/abs/2005.03867v4", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Detecting East Asian Prejudice on Social Media", "abstract": "The outbreak of COVID-19 has transformed societies across the world as\ngovernments tackle the health, economic and social costs of the pandemic. It\nhas also raised concerns about the spread of hateful language and prejudice\nonline, especially hostility directed against East Asia. In this paper we\nreport on the creation of a classifier that detects and categorizes social\nmedia posts from Twitter into four classes: Hostility against East Asia,\nCriticism of East Asia, Meta-discussions of East Asian prejudice and a neutral\nclass. The classifier achieves an F1 score of 0.83 across all four classes. We\nprovide our final model (coded in Python), as well as a new 20,000 tweet\ntraining dataset used to make the classifier, two analyses of hashtags\nassociated with East Asian prejudice and the annotation codebook. The\nclassifier can be implemented by other researchers, assisting with both online\ncontent moderation processes and further research into the dynamics, prevalence\nand impact of East Asian prejudice online during this global pandemic.", "published": "2020-05-08 08:53:47", "link": "http://arxiv.org/abs/2005.03909v1", "categories": ["cs.CL", "cs.CY", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Sentiment Analysis Using Simplified Long Short-term Memory Recurrent\n  Neural Networks", "abstract": "LSTM or Long Short Term Memory Networks is a specific type of Recurrent\nNeural Network (RNN) that is very effective in dealing with long sequence data\nand learning long term dependencies. In this work, we perform sentiment\nanalysis on a GOP Debate Twitter dataset. To speed up training and reduce the\ncomputational cost and time, six different parameter reduced slim versions of\nthe LSTM model (slim LSTM) are proposed. We evaluate two of these models on the\ndataset. The performance of these two LSTM models along with the standard LSTM\nmodel is compared. The effect of Bidirectional LSTM Layers is also studied. The\nwork also consists of a study to choose the best architecture, apart from\nestablishing the best set of hyper parameters for different LSTM Models.", "published": "2020-05-08 12:50:10", "link": "http://arxiv.org/abs/2005.03993v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Text-Based Ideal Points", "abstract": "Ideal point models analyze lawmakers' votes to quantify their political\npositions, or ideal points. But votes are not the only way to express a\npolitical position. Lawmakers also give speeches, release press statements, and\npost tweets. In this paper, we introduce the text-based ideal point model\n(TBIP), an unsupervised probabilistic topic model that analyzes texts to\nquantify the political positions of its authors. We demonstrate the TBIP with\ntwo types of politicized text data: U.S. Senate speeches and senator tweets.\nThough the model does not analyze their votes or political affiliations, the\nTBIP separates lawmakers by party, learns interpretable politicized topics, and\ninfers ideal points close to the classical vote-based ideal points. One benefit\nof analyzing texts, as opposed to votes, is that the TBIP can estimate ideal\npoints of anyone who authors political texts, including non-voting actors. To\nthis end, we use it to study tweets from the 2020 Democratic presidential\ncandidates. Using only the texts of their tweets, it identifies them along an\ninterpretable progressive-to-moderate spectrum.", "published": "2020-05-08 21:16:42", "link": "http://arxiv.org/abs/2005.04232v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Social Media Information Sharing for Natural Disaster Response", "abstract": "Social media has become an essential channel for posting disaster-related\ninformation, which provide governments and relief agencies real-time data for\nbetter disaster management. However, research in this field has not received\nsufficient attention and extracting useful information is still challenging.\nThis paper aims to improve disaster relief efficiency via mining and analyzing\nsocial media data like public attitudes towards disaster response and public\ndemands for targeted relief supplies during different types of disasters. We\nfocus on different natural disasters based on properties such as types,\ndurations, and damages, which contains a total of 41,993 tweets. In this paper,\npublic perception is assessed qualitatively by manually classified tweets,\nwhich contain information like the demand for targeted relief supplies,\nsatisfactions of disaster response, and public fear. Public attitudes to\nnatural disasters are studied via a quantitative analysis using eight machine\nlearning models. To better provide decision-makers with the appropriate model,\nthe comparison of machine learning models based on computational time and\nprediction accuracy is conducted. The change of public opinion during different\nnatural disasters and the evolution of people's behavior of using social media\nfor disaster relief in the face of the identical type of natural disasters as\nTwitter continues to evolve are studied. The results in this paper demonstrate\nthe feasibility and validation of the proposed research approach and provide\nrelief agencies with insights into better disaster management.", "published": "2020-05-08 21:11:39", "link": "http://arxiv.org/abs/2005.07019v5", "categories": ["cs.SI", "cs.CL", "cs.LG"], "primary_category": "cs.SI"}
{"title": "History for Visual Dialog: Do we really need it?", "abstract": "Visual Dialog involves \"understanding\" the dialog history (what has been\ndiscussed previously) and the current question (what is asked), in addition to\ngrounding information in the image, to generate the correct response. In this\npaper, we show that co-attention models which explicitly encode dialog history\noutperform models that don't, achieving state-of-the-art performance (72 % NDCG\non val set). However, we also expose shortcomings of the crowd-sourcing dataset\ncollection procedure by showing that history is indeed only required for a\nsmall amount of the data and that the current evaluation metric encourages\ngeneric replies. To that end, we propose a challenging subset (VisDialConv) of\nthe VisDial val set and provide a benchmark of 63% NDCG.", "published": "2020-05-08 14:58:09", "link": "http://arxiv.org/abs/2005.07493v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Development of a New Image-to-text Conversion System for Pashto, Farsi\n  and Traditional Chinese", "abstract": "We report upon the results of a research and prototype building project\n\\emph{Worldly~OCR} dedicated to developing new, more accurate image-to-text\nconversion software for several languages and writing systems. These include\nthe cursive scripts Farsi and Pashto, and Latin cursive scripts. We also\ndescribe approaches geared towards Traditional Chinese, which is non-cursive,\nbut features an extremely large character set of 65,000 characters. Our\nmethodology is based on Machine Learning, especially Deep Learning, and Data\nScience, and is directed towards vast quantities of original documents,\nexceeding a billion pages. The target audience of this paper is a general\naudience with interest in Digital Humanities or in retrieval of accurate\nfull-text and metadata from digital images.", "published": "2020-05-08 17:58:48", "link": "http://arxiv.org/abs/2005.08650v1", "categories": ["cs.CV", "cs.CL", "cs.LG", "68T10, 68T07", "I.2.6; D.m"], "primary_category": "cs.CV"}
{"title": "Cross-Language Transfer Learning, Continuous Learning, and Domain\n  Adaptation for End-to-End Automatic Speech Recognition", "abstract": "In this paper, we demonstrate the efficacy of transfer learning and\ncontinuous learning for various automatic speech recognition (ASR) tasks. We\nstart with a pre-trained English ASR model and show that transfer learning can\nbe effectively and easily performed on: (1) different English accents, (2)\ndifferent languages (German, Spanish and Russian) and (3) application-specific\ndomains. Our experiments demonstrate that in all three cases, transfer learning\nfrom a good base model has higher accuracy than a model trained from scratch.\nIt is preferred to fine-tune large models than small pre-trained models, even\nif the dataset for fine-tuning is small. Moreover, transfer learning\nsignificantly speeds up convergence for both very small and very large target\ndatasets.", "published": "2020-05-08 21:04:36", "link": "http://arxiv.org/abs/2005.04290v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Neural Spatio-Temporal Beamformer for Target Speech Separation", "abstract": "Purely neural network (NN) based speech separation and enhancement methods,\nalthough can achieve good objective scores, inevitably cause nonlinear speech\ndistortions that are harmful for the automatic speech recognition (ASR). On the\nother hand, the minimum variance distortionless response (MVDR) beamformer with\nNN-predicted masks, although can significantly reduce speech distortions, has\nlimited noise reduction capability. In this paper, we propose a multi-tap MVDR\nbeamformer with complex-valued masks for speech separation and enhancement.\nCompared to the state-of-the-art NN-mask based MVDR beamformer, the multi-tap\nMVDR beamformer exploits the inter-frame correlation in addition to the\ninter-microphone correlation that is already utilized in prior arts. Further\nimprovements include the replacement of the real-valued masks with the\ncomplex-valued masks and the joint training of the complex-mask NN. The\nevaluation on our multi-modal multi-channel target speech separation and\nenhancement platform demonstrates that our proposed multi-tap MVDR beamformer\nimproves both the ASR accuracy and the perceptual speech quality against prior\narts.", "published": "2020-05-08 07:53:37", "link": "http://arxiv.org/abs/2005.03889v5", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Asteroid: the PyTorch-based audio source separation toolkit for\n  researchers", "abstract": "This paper describes Asteroid, the PyTorch-based audio source separation\ntoolkit for researchers. Inspired by the most successful neural source\nseparation systems, it provides all neural building blocks required to build\nsuch a system. To improve reproducibility, Kaldi-style recipes on common audio\nsource separation datasets are also provided. This paper describes the software\narchitecture of Asteroid and its most important features. By showing\nexperimental results obtained with Asteroid's recipes, we show that our\nimplementations are at least on par with most results reported in reference\npapers. The toolkit is publicly available at\nhttps://github.com/mpariente/asteroid .", "published": "2020-05-08 16:18:34", "link": "http://arxiv.org/abs/2005.04132v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
