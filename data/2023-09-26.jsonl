{"title": "The ATM implied skew in the ADO-Heston model", "abstract": "In this paper similar to [P. Carr, A. Itkin, 2019] we construct another Markovian approximation of the rough Heston-like volatility model - the ADO-Heston model. The characteristic function (CF) of the model is derived under both risk-neutral and real measures which is an unsteady three-dimensional PDE with some coefficients being functions of the time $t$ and the Hurst exponent $H$. To replicate known behavior of the market implied skew we proceed with a wise choice of the market price of risk, and then find a closed form expression for the CF of the log-price and the ATM implied skew. Based on the provided example, we claim that the ADO-Heston model (which is a pure diffusion model but with a stochastic mean-reversion speed of the variance process, or a Markovian approximation of the rough Heston model) is able (approximately) to reproduce the known behavior of the vanilla implied skew at small $T$. We conclude that the behavior of our implied volatility skew curve ${\\cal S}(T) \\propto a(H) T^{b\\cdot (H-1/2)}, \\, b = const$, is not exactly same as in rough volatility models since $b \\ne 1$, but seems to be close enough for all practical values of $T$. Thus, the proposed Markovian model is able to replicate some properties of the corresponding rough volatility model. Similar analysis is provided for the forward starting options where we found that the ATM implied skew for the forward starting options can blow-up for any $s > t$ when $T \\to s$. This result, however, contradicts to the observation of [E. Alos, D.G. Lorite, 2021] that Markovian approximation is not able to catch this behavior, so remains the question on which one is closer to reality.", "published": "2023-09-26 16:21:16", "link": "http://arxiv.org/abs/2309.15044v1", "categories": ["q-fin.CP", "q-fin.MF", "q-fin.PR"], "primary_category": "q-fin.CP"}
{"title": "Gray-box Adversarial Attack of Deep Reinforcement Learning-based Trading Agents", "abstract": "In recent years, deep reinforcement learning (Deep RL) has been successfully implemented as a smart agent in many systems such as complex games, self-driving cars, and chat-bots. One of the interesting use cases of Deep RL is its application as an automated stock trading agent. In general, any automated trading agent is prone to manipulations by adversaries in the trading environment. Thus studying their robustness is vital for their success in practice. However, typical mechanism to study RL robustness, which is based on white-box gradient-based adversarial sample generation techniques (like FGSM), is obsolete for this use case, since the models are protected behind secure international exchange APIs, such as NASDAQ. In this research, we demonstrate that a \"gray-box\" approach for attacking a Deep RL-based trading agent is possible by trading in the same stock market, with no extra access to the trading agent. In our proposed approach, an adversary agent uses a hybrid Deep Neural Network as its policy consisting of Convolutional layers and fully-connected layers. On average, over three simulated trading market configurations, the adversary policy proposed in this research is able to reduce the reward values by 214.17%, which results in reducing the potential profits of the baseline by 139.4%, ensemble method by 93.7%, and an automated trading software developed by our industrial partner by 85.5%, while consuming significantly less budget than the victims (427.77%, 187.16%, and 66.97%, respectively).", "published": "2023-09-26 02:07:26", "link": "http://arxiv.org/abs/2309.14615v1", "categories": ["cs.LG", "cs.CE", "q-fin.TR"], "primary_category": "cs.LG"}
