{"title": "ConPET: Continual Parameter-Efficient Tuning for Large Language Models", "abstract": "Continual learning necessitates the continual adaptation of models to newly\nemerging tasks while minimizing the catastrophic forgetting of old ones. This\nis extremely challenging for large language models (LLMs) with vanilla\nfull-parameter tuning due to high computation costs, memory consumption, and\nforgetting issue. Inspired by the success of parameter-efficient tuning (PET),\nwe propose Continual Parameter-Efficient Tuning (ConPET), a generalizable\nparadigm for continual task adaptation of LLMs with task-number-independent\ntraining complexity. ConPET includes two versions with different application\nscenarios. First, Static ConPET can adapt former continual learning methods\noriginally designed for relatively smaller models to LLMs through PET and a\ndynamic replay strategy, which largely reduces the tuning costs and alleviates\nthe over-fitting and forgetting issue. Furthermore, to maintain scalability,\nDynamic ConPET adopts separate PET modules for different tasks and a PET module\nselector for dynamic optimal selection. In our extensive experiments, the\nadaptation of Static ConPET helps multiple former methods reduce the scale of\ntunable parameters by over 3,000 times and surpass the PET-only baseline by at\nleast 5 points on five smaller benchmarks, while Dynamic ConPET gains its\nadvantage on the largest dataset. The codes and datasets are available at\nhttps://github.com/Raincleared-Song/ConPET.", "published": "2023-09-26 08:52:04", "link": "http://arxiv.org/abs/2309.14763v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "KERMIT: Knowledge Graph Completion of Enhanced Relation Modeling with\n  Inverse Transformation", "abstract": "Knowledge graph completion (KGC) revolves around populating missing triples\nin a knowledge graph using available information. Text-based methods, which\ndepend on textual descriptions of triples, often encounter difficulties when\nthese descriptions lack sufficient information for accurate prediction-an issue\ninherent to the datasets and not easily resolved through modeling alone. To\naddress this and ensure data consistency, we first use large language models\n(LLMs) to generate coherent descriptions, bridging the semantic gap between\nqueries and answers. Secondly, we utilize inverse relations to create a\nsymmetric graph, thereby providing augmented training samples for KGC.\nAdditionally, we employ the label information inherent in knowledge graphs\n(KGs) to enhance the existing contrastive framework, making it fully\nsupervised. These efforts have led to significant performance improvements on\nthe WN18RR and FB15k-237 datasets. According to standard evaluation metrics,\nour approach achieves a 4.2% improvement in Hit@1 on WN18RR and a 3.4%\nimprovement in Hit@3 on FB15k-237, demonstrating superior performance.", "published": "2023-09-26 09:03:25", "link": "http://arxiv.org/abs/2309.14770v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Segmentation-Free Streaming Machine Translation", "abstract": "Streaming Machine Translation (MT) is the task of translating an unbounded\ninput text stream in real-time. The traditional cascade approach, which\ncombines an Automatic Speech Recognition (ASR) and an MT system, relies on an\nintermediate segmentation step which splits the transcription stream into\nsentence-like units. However, the incorporation of a hard segmentation\nconstrains the MT system and is a source of errors. This paper proposes a\nSegmentation-Free framework that enables the model to translate an unsegmented\nsource stream by delaying the segmentation decision until the translation has\nbeen generated. Extensive experiments show how the proposed Segmentation-Free\nframework has better quality-latency trade-off than competing approaches that\nuse an independent segmentation model. Software, data and models will be\nreleased upon paper acceptance.", "published": "2023-09-26 10:43:52", "link": "http://arxiv.org/abs/2309.14823v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Question-Answering Approach to Evaluating Legal Summaries", "abstract": "Traditional evaluation metrics like ROUGE compare lexical overlap between the\nreference and generated summaries without taking argumentative structure into\naccount, which is important for legal summaries. In this paper, we propose a\nnovel legal summarization evaluation framework that utilizes GPT-4 to generate\na set of question-answer pairs that cover main points and information in the\nreference summary. GPT-4 is then used to generate answers based on the\ngenerated summary for the questions from the reference summary. Finally, GPT-4\ngrades the answers from the reference summary and the generated summary. We\nexamined the correlation between GPT-4 grading with human grading. The results\nsuggest that this question-answering approach with GPT-4 can be a useful tool\nfor gauging the quality of the summary.", "published": "2023-09-26 15:36:29", "link": "http://arxiv.org/abs/2309.15016v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RAGAS: Automated Evaluation of Retrieval Augmented Generation", "abstract": "We introduce RAGAs (Retrieval Augmented Generation Assessment), a framework\nfor reference-free evaluation of Retrieval Augmented Generation (RAG)\npipelines. RAG systems are composed of a retrieval and an LLM based generation\nmodule, and provide LLMs with knowledge from a reference textual database,\nwhich enables them to act as a natural language layer between a user and\ntextual databases, reducing the risk of hallucinations. Evaluating RAG\narchitectures is, however, challenging because there are several dimensions to\nconsider: the ability of the retrieval system to identify relevant and focused\ncontext passages, the ability of the LLM to exploit such passages in a faithful\nway, or the quality of the generation itself. With RAGAs, we put forward a\nsuite of metrics which can be used to evaluate these different dimensions\n\\textit{without having to rely on ground truth human annotations}. We posit\nthat such a framework can crucially contribute to faster evaluation cycles of\nRAG architectures, which is especially important given the fast adoption of\nLLMs.", "published": "2023-09-26 19:23:54", "link": "http://arxiv.org/abs/2309.15217v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Simple Text to Video Model via Transformer", "abstract": "We present a general and simple text to video model based on Transformer.\nSince both text and video are sequential data, we encode both texts and images\ninto the same hidden space, which are further fed into Transformer to capture\nthe temporal consistency and then decoder to generate either text or images.\nConsidering the image signal may become weak in the long sequence, we introduce\nthe U-Net to reconstruct image from its noised version. Specifically, we\nincrease the noise level to the original image in the long sequence, then use\nthe $down$ module from U-Net to encode noised images, which are further input\nto transformer to predict next clear images. We also add a constraint to\npromote motion between any generated image pair in the video. We use GPT2 and\ntest our approach on UCF101 dataset and show it can generate promising videos.", "published": "2023-09-26 05:26:30", "link": "http://arxiv.org/abs/2309.14683v1", "categories": ["cs.CV", "cs.CL", "68T10", "I.2.6"], "primary_category": "cs.CV"}
{"title": "QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models", "abstract": "Recently years have witnessed a rapid development of large language models\n(LLMs). Despite the strong ability in many language-understanding tasks, the\nheavy computational burden largely restricts the application of LLMs especially\nwhen one needs to deploy them onto edge devices. In this paper, we propose a\nquantization-aware low-rank adaptation (QA-LoRA) algorithm. The motivation lies\nin the imbalanced degrees of freedom of quantization and adaptation, and the\nsolution is to use group-wise operators which increase the degree of freedom of\nquantization meanwhile decreasing that of adaptation. QA-LoRA is easily\nimplemented with a few lines of code, and it equips the original LoRA with\ntwo-fold abilities: (i) during fine-tuning, the LLM's weights are quantized\n(e.g., into INT4) to reduce time and memory usage; (ii) after fine-tuning, the\nLLM and auxiliary weights are naturally integrated into a quantized model\nwithout loss of accuracy. We apply QA-LoRA to the LLaMA and LLaMA2 model\nfamilies and validate its effectiveness in different fine-tuning datasets and\ndownstream scenarios. Code will be made available at\nhttps://github.com/yuhuixu1993/qa-lora.", "published": "2023-09-26 07:22:23", "link": "http://arxiv.org/abs/2309.14717v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Legal Question-Answering in the Indian Context: Efficacy, Challenges,\n  and Potential of Modern AI Models", "abstract": "Legal QA platforms bear the promise to metamorphose the manner in which legal\nexperts engage with jurisprudential documents. In this exposition, we embark on\na comparative exploration of contemporary AI frameworks, gauging their\nadeptness in catering to the unique demands of the Indian legal milieu, with a\nkeen emphasis on Indian Legal Question Answering (AILQA). Our discourse zeroes\nin on an array of retrieval and QA mechanisms, positioning the OpenAI GPT model\nas a reference point. The findings underscore the proficiency of prevailing\nAILQA paradigms in decoding natural language prompts and churning out precise\nresponses. The ambit of this study is tethered to the Indian criminal legal\nlandscape, distinguished by its intricate nature and associated logistical\nconstraints. To ensure a holistic evaluation, we juxtapose empirical metrics\nwith insights garnered from seasoned legal practitioners, thereby painting a\ncomprehensive picture of AI's potential and challenges within the realm of\nIndian legal QA.", "published": "2023-09-26 07:56:55", "link": "http://arxiv.org/abs/2309.14735v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Knowledgeable In-Context Tuning: Exploring and Exploiting Factual\n  Knowledge for In-Context Learning", "abstract": "Large language models (LLMs) enable in-context learning (ICL) by conditioning\non a few labeled training examples as a text-based prompt, eliminating the need\nfor parameter updates and achieving competitive performance. In this paper, we\ndemonstrate that factual knowledge is imperative for the performance of ICL in\nthree core facets: the inherent knowledge learned in LLMs, the factual\nknowledge derived from the selected in-context examples, and the knowledge\nbiases in LLMs for output generation. To unleash the power of LLMs in few-shot\nlearning scenarios, we introduce a novel Knowledgeable In-Context Tuning (KICT)\nframework to further improve the performance of ICL: 1) injecting knowledge\ninto LLMs during continual self-supervised pre-training, 2) judiciously\nselecting the examples for ICL with high knowledge relevance, and 3)\ncalibrating the prediction results based on prior knowledge. We evaluate the\nproposed approaches on autoregressive models (e.g., GPT-style LLMs) over\nmultiple text classification and question-answering tasks. Experimental results\ndemonstrate that KICT substantially outperforms strong baselines and improves\nby more than 13% and 7% on text classification and question-answering tasks,\nrespectively.", "published": "2023-09-26 09:06:39", "link": "http://arxiv.org/abs/2309.14771v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fine-tuning and aligning question answering models for complex\n  information extraction tasks", "abstract": "The emergence of Large Language Models (LLMs) has boosted performance and\npossibilities in various NLP tasks. While the usage of generative AI models\nlike ChatGPT opens up new opportunities for several business use cases, their\ncurrent tendency to hallucinate fake content strongly limits their\napplicability to document analysis, such as information retrieval from\ndocuments. In contrast, extractive language models like question answering (QA)\nor passage retrieval models guarantee query results to be found within the\nboundaries of an according context document, which makes them candidates for\nmore reliable information extraction in productive environments of companies.\nIn this work we propose an approach that uses and integrates extractive QA\nmodels for improved feature extraction of German business documents such as\ninsurance reports or medical leaflets into a document analysis solution. We\nfurther show that fine-tuning existing German QA models boosts performance for\ntailored extraction tasks of complex linguistic features like damage cause\nexplanations or descriptions of medication appearance, even with using only a\nsmall set of annotated data. Finally, we discuss the relevance of scoring\nmetrics for evaluating information extraction tasks and deduce a combined\nmetric from Levenshtein distance, F1-Score, Exact Match and ROUGE-L to mimic\nthe assessment criteria from human experts.", "published": "2023-09-26 10:02:21", "link": "http://arxiv.org/abs/2309.14805v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Robustness of the Random Language Model", "abstract": "The Random Language Model (De Giuli 2019) is an ensemble of stochastic\ncontext-free grammars, quantifying the syntax of human and computer languages.\nThe model suggests a simple picture of first language learning as a type of\nannealing in the vast space of potential languages. In its simplest\nformulation, it implies a single continuous transition to grammatical syntax,\nat which the symmetry among potential words and categories is spontaneously\nbroken. Here this picture is scrutinized by considering its robustness against\nextensions of the original model, and trajectories through parameter space\ndifferent from those originally considered. It is shown here that (i) the\nscenario is robust to explicit symmetry breaking, an inevitable component of\nlearning in the real world; and (ii) the transition to grammatical syntax can\nbe encountered by fixing the deep (hidden) structure while varying the surface\n(observable) properties. It is also argued that the transition becomes a sharp\nthermodynamic transition in an idealized limit. Moreover, comparison with human\ndata on the clustering coefficient of syntax networks suggests that the\nobserved transition is equivalent to that normally experienced by children at\nage 24 months. The results are discussed in light of theory of first-language\nacquisition in linguistics, and recent successes in machine learning.", "published": "2023-09-26 13:14:35", "link": "http://arxiv.org/abs/2309.14913v2", "categories": ["cond-mat.dis-nn", "cs.CL"], "primary_category": "cond-mat.dis-nn"}
{"title": "Large Language Model Alignment: A Survey", "abstract": "Recent years have witnessed remarkable progress made in large language models\n(LLMs). Such advancements, while garnering significant attention, have\nconcurrently elicited various concerns. The potential of these models is\nundeniably vast; however, they may yield texts that are imprecise, misleading,\nor even detrimental. Consequently, it becomes paramount to employ alignment\ntechniques to ensure these models to exhibit behaviors consistent with human\nvalues.\n  This survey endeavors to furnish an extensive exploration of alignment\nmethodologies designed for LLMs, in conjunction with the extant capability\nresearch in this domain. Adopting the lens of AI alignment, we categorize the\nprevailing methods and emergent proposals for the alignment of LLMs into outer\nand inner alignment. We also probe into salient issues including the models'\ninterpretability, and potential vulnerabilities to adversarial attacks. To\nassess LLM alignment, we present a wide variety of benchmarks and evaluation\nmethodologies. After discussing the state of alignment research for LLMs, we\nfinally cast a vision toward the future, contemplating the promising avenues of\nresearch that lie ahead.\n  Our aspiration for this survey extends beyond merely spurring research\ninterests in this realm. We also envision bridging the gap between the AI\nalignment research community and the researchers engrossed in the capability\nexploration of LLMs for both capable and safe LLMs.", "published": "2023-09-26 15:49:23", "link": "http://arxiv.org/abs/2309.15025v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large\n  Language Models", "abstract": "Researchers have successfully applied large language models (LLMs) such as\nChatGPT to reranking in an information retrieval context, but to date, such\nwork has mostly been built on proprietary models hidden behind opaque API\nendpoints. This approach yields experimental results that are not reproducible\nand non-deterministic, threatening the veracity of outcomes that build on such\nshaky foundations. To address this significant shortcoming, we present\nRankVicuna, the first fully open-source LLM capable of performing high-quality\nlistwise reranking in a zero-shot setting. Experimental results on the TREC\n2019 and 2020 Deep Learning Tracks show that we can achieve effectiveness\ncomparable to zero-shot reranking with GPT-3.5 with a much smaller 7B parameter\nmodel, although our effectiveness remains slightly behind reranking with GPT-4.\nWe hope our work provides the foundation for future research on reranking with\nmodern LLMs. All the code necessary to reproduce our results is available at\nhttps://github.com/castorini/rank_llm.", "published": "2023-09-26 17:31:57", "link": "http://arxiv.org/abs/2309.15088v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Robust Stance Detection: Understanding Public Perceptions in Social\n  Media", "abstract": "The abundance of social media data has presented opportunities for accurately\ndetermining public and group-specific stances around policy proposals or\ncontroversial topics. In contrast with sentiment analysis which focuses on\nidentifying prevailing emotions, stance detection identifies precise positions\n(i.e., supportive, opposing, neutral) relative to a well-defined topic, such as\nperceptions toward specific global health interventions during the COVID-19\npandemic. Traditional stance detection models, while effective within their\nspecific domain (e.g., attitudes towards masking protocols during COVID-19),\noften lag in performance when applied to new domains and topics due to changes\nin data distribution. This limitation is compounded by the scarcity of\ndomain-specific, labeled datasets, which are expensive and labor-intensive to\ncreate. A solution we present in this paper combines counterfactual data\naugmentation with contrastive learning to enhance the robustness of stance\ndetection across domains and topics of interest. We evaluate the performance of\ncurrent state-of-the-art stance detection models, including a prompt-optimized\nlarge language model, relative to our proposed framework succinctly called\nSTANCE-C3 (domain-adaptive Cross-target STANCE detection via Contrastive\nlearning and Counterfactual generation). Empirical evaluations demonstrate\nSTANCE-C3's consistent improvements over the baseline models with respect to\naccuracy across domains and varying focal topics. Despite the increasing\nprevalence of general-purpose models such as generative AI, specialized models\nsuch as STANCE-C3 provide utility in safety-critical domains wherein precision\nis highly valued, especially when a nuanced understanding of the concerns of\ndifferent population segments could result in crafting more impactful public\npolicies.", "published": "2023-09-26 18:19:51", "link": "http://arxiv.org/abs/2309.15176v2", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "FlaCGEC: A Chinese Grammatical Error Correction Dataset with\n  Fine-grained Linguistic Annotation", "abstract": "Chinese Grammatical Error Correction (CGEC) has been attracting growing\nattention from researchers recently. In spite of the fact that multiple CGEC\ndatasets have been developed to support the research, these datasets lack the\nability to provide a deep linguistic topology of grammar errors, which is\ncritical for interpreting and diagnosing CGEC approaches. To address this\nlimitation, we introduce FlaCGEC, which is a new CGEC dataset featured with\nfine-grained linguistic annotation. Specifically, we collect raw corpus from\nthe linguistic schema defined by Chinese language experts, conduct edits on\nsentences via rules, and refine generated samples manually, which results in\n10k sentences with 78 instantiated grammar points and 3 types of edits. We\nevaluate various cutting-edge CGEC methods on the proposed FlaCGEC dataset and\ntheir unremarkable results indicate that this dataset is challenging in\ncovering a large range of grammatical errors. In addition, we also treat\nFlaCGEC as a diagnostic dataset for testing generalization skills and conduct a\nthorough evaluation of existing CGEC models.", "published": "2023-09-26 10:22:43", "link": "http://arxiv.org/abs/2311.04906v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Efficient Post-training Quantization with FP8 Formats", "abstract": "Recent advances in deep learning methods such as LLMs and Diffusion models\nhave created a need for improved quantization methods that can meet the\ncomputational demands of these modern architectures while maintaining accuracy.\nTowards this goal, we study the advantages of FP8 data formats for\npost-training quantization across 75 unique network architectures covering a\nwide range of tasks, including machine translation, language modeling, text\ngeneration, image classification, generation, and segmentation. We examine\nthree different FP8 representations (E5M2, E4M3, and E3M4) to study the effects\nof varying degrees of trade-off between dynamic range and precision on model\naccuracy. Based on our extensive study, we developed a quantization workflow\nthat generalizes across different network architectures. Our empirical results\nshow that FP8 formats outperform INT8 in multiple aspects, including workload\ncoverage (92.64% vs. 65.87%), model accuracy and suitability for a broader\nrange of operations. Furthermore, our findings suggest that E4M3 is better\nsuited for NLP models, whereas E3M4 performs marginally better than E4M3 on\ncomputer vision tasks. The code is publicly available on Intel Neural\nCompressor: https://github.com/intel/neural-compressor.", "published": "2023-09-26 00:58:36", "link": "http://arxiv.org/abs/2309.14592v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Program Repair with Minimal Edits Using CodeT5", "abstract": "Programmers often struggle to identify and fix bugs in their programs. In\nrecent years, many language models (LMs) have been proposed to fix erroneous\nprograms and support error recovery. However, the LMs tend to generate\nsolutions that differ from the original input programs. This leads to potential\ncomprehension difficulties for users. In this paper, we propose an approach to\nsuggest a correct program with minimal repair edits using CodeT5. We fine-tune\na pre-trained CodeT5 on code pairs of wrong and correct programs and evaluate\nits performance with several baseline models. The experimental results show\nthat the fine-tuned CodeT5 achieves a pass@100 of 91.95% and an average edit\ndistance of the most similar correct program of 6.84, which indicates that at\nleast one correct program can be suggested by generating 100 candidate\nprograms. We demonstrate the effectiveness of LMs in suggesting program repair\nwith minimal edits for solving introductory programming problems.", "published": "2023-09-26 08:45:05", "link": "http://arxiv.org/abs/2309.14760v1", "categories": ["cs.CL", "cs.AI", "cs.SE"], "primary_category": "cs.CL"}
{"title": "BLIP-Adapter: Parameter-Efficient Transfer Learning for Mobile\n  Screenshot Captioning", "abstract": "This study aims to explore efficient tuning methods for the screenshot\ncaptioning task. Recently, image captioning has seen significant advancements,\nbut research in captioning tasks for mobile screens remains relatively scarce.\nCurrent datasets and use cases describing user behaviors within product\nscreenshots are notably limited. Consequently, we sought to fine-tune\npre-existing models for the screenshot captioning task. However, fine-tuning\nlarge pre-trained models can be resource-intensive, requiring considerable\ntime, computational power, and storage due to the vast number of parameters in\nimage captioning models. To tackle this challenge, this study proposes a\ncombination of adapter methods, which necessitates tuning only the additional\nmodules on the model. These methods are originally designed for vision or\nlanguage tasks, and our intention is to apply them to address similar\nchallenges in screenshot captioning. By freezing the parameters of the image\ncaption models and training only the weights associated with the methods,\nperformance comparable to fine-tuning the entire model can be achieved, while\nsignificantly reducing the number of parameters. This study represents the\nfirst comprehensive investigation into the effectiveness of combining adapters\nwithin the context of the screenshot captioning task. Through our experiments\nand analyses, this study aims to provide valuable insights into the application\nof adapters in vision-language models and contribute to the development of\nefficient tuning techniques for the screenshot captioning task. Our study is\navailable at https://github.com/RainYuGG/BLIP-Adapter", "published": "2023-09-26 09:16:44", "link": "http://arxiv.org/abs/2309.14774v1", "categories": ["cs.LG", "cs.CL", "cs.CV", "cs.HC"], "primary_category": "cs.LG"}
{"title": "Exploring Small Language Models with Prompt-Learning Paradigm for\n  Efficient Domain-Specific Text Classification", "abstract": "Domain-specific text classification faces the challenge of scarce labeled\ndata due to the high cost of manual labeling. Prompt-learning, known for its\nefficiency in few-shot scenarios, is proposed as an alternative to traditional\nfine-tuning methods. And besides, although large language models (LLMs) have\ngained prominence, small language models (SLMs, with under 1B parameters) offer\nsignificant customizability, adaptability, and cost-effectiveness for\ndomain-specific tasks, given industry constraints. In this study, we\ninvestigate the potential of SLMs combined with prompt-learning paradigm for\ndomain-specific text classification, specifically within customer-agent\ninteractions in retail. Our evaluations show that, in few-shot settings when\nprompt-based model fine-tuning is possible, T5-base, a typical SLM with 220M\nparameters, achieve approximately 75% accuracy with limited labeled data (up to\n15% of full data), which shows great potentials of SLMs with prompt-learning.\nBased on this, We further validate the effectiveness of active few-shot\nsampling and the ensemble strategy in the prompt-learning pipeline that\ncontribute to a remarkable performance gain. Besides, in zero-shot settings\nwith a fixed model, we underscore a pivotal observation that, although the\nGPT-3.5-turbo equipped with around 154B parameters garners an accuracy of\n55.16%, the power of well designed prompts becomes evident when the\nFLAN-T5-large, a model with a mere 0.5% of GPT-3.5-turbo's parameters, achieves\nan accuracy exceeding 31% with the optimized prompt, a leap from its sub-18%\nperformance with an unoptimized one. Our findings underscore the promise of\nprompt-learning in classification tasks with SLMs, emphasizing the benefits of\nactive few-shot sampling, and ensemble strategies in few-shot settings, and the\nimportance of prompt engineering in zero-shot settings.", "published": "2023-09-26 09:24:46", "link": "http://arxiv.org/abs/2309.14779v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Interactively Learning Social Media Representations Improves News Source\n  Factuality Detection", "abstract": "The rise of social media has enabled the widespread propagation of fake news,\ntext that is published with an intent to spread misinformation and sway\nbeliefs. Rapidly detecting fake news, especially as new events arise, is\nimportant to prevent misinformation.\n  While prior works have tackled this problem using supervised learning\nsystems, automatedly modeling the complexities of the social media landscape\nthat enables the spread of fake news is challenging. On the contrary, having\nhumans fact check all news is not scalable. Thus, in this paper, we propose to\napproach this problem interactively, where humans can interact to help an\nautomated system learn a better social media representation quality. On real\nworld events, our experiments show performance improvements in detecting\nfactuality of news sources, even after few human interactions.", "published": "2023-09-26 14:36:19", "link": "http://arxiv.org/abs/2309.14966v1", "categories": ["cs.CL", "cs.AI", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Automating question generation from educational text", "abstract": "The use of question-based activities (QBAs) is wide-spread in education,\ntraditionally forming an integral part of the learning and assessment process.\nIn this paper, we design and evaluate an automated question generation tool for\nformative and summative assessment in schools. We present an expert survey of\none hundred and four teachers, demonstrating the need for automated generation\nof QBAs, as a tool that can significantly reduce the workload of teachers and\nfacilitate personalized learning experiences. Leveraging the recent\nadvancements in generative AI, we then present a modular framework employing\ntransformer based language models for automatic generation of multiple-choice\nquestions (MCQs) from textual content. The presented solution, with distinct\nmodules for question generation, correct answer prediction, and distractor\nformulation, enables us to evaluate different language models and generation\ntechniques. Finally, we perform an extensive quantitative and qualitative\nevaluation, demonstrating trade-offs in the use of different techniques and\nmodels.", "published": "2023-09-26 15:18:44", "link": "http://arxiv.org/abs/2309.15004v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Updated Corpora and Benchmarks for Long-Form Speech Recognition", "abstract": "The vast majority of ASR research uses corpora in which both the training and\ntest data have been pre-segmented into utterances. In most real-word ASR\nuse-cases, however, test audio is not segmented, leading to a mismatch between\ninference-time conditions and models trained on segmented utterances. In this\npaper, we re-release three standard ASR corpora - TED-LIUM 3, Gigapeech, and\nVoxPopuli-en - with updated transcription and alignments to enable their use\nfor long-form ASR research. We use these reconstituted corpora to study the\ntrain-test mismatch problem for transducers and attention-based\nencoder-decoders (AEDs), confirming that AEDs are more susceptible to this\nissue. Finally, we benchmark a simple long-form training for these models,\nshowing its efficacy for model robustness under this domain shift.", "published": "2023-09-26 15:32:09", "link": "http://arxiv.org/abs/2309.15013v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Don't throw away your value model! Generating more preferable text with\n  Value-Guided Monte-Carlo Tree Search decoding", "abstract": "Inference-time search algorithms such as Monte-Carlo Tree Search (MCTS) may\nseem unnecessary when generating natural language text based on\nstate-of-the-art reinforcement learning such as Proximal Policy Optimization\n(PPO). In this paper, we demonstrate that it is possible to get extra mileage\nout of PPO by integrating MCTS on top. The key idea is not to throw out the\nvalue network, a byproduct of PPO training for evaluating partial output\nsequences, when decoding text out of the policy network. More concretely, we\npresent a novel value-guided decoding algorithm called PPO-MCTS, which can\nintegrate the value network from PPO to work closely with the policy network\nduring inference-time generation. Compared to prior approaches based on MCTS\nfor controlled text generation, the key strength of our approach is to reduce\nthe fundamental mismatch of the scoring mechanisms of the partial outputs\nbetween training and test. Evaluation on four text generation tasks demonstrate\nthat PPO-MCTS greatly improves the preferability of generated text compared to\nthe standard practice of using only the PPO policy. Our results demonstrate the\npromise of search algorithms even on top of the aligned language models from\nPPO, and the under-explored benefit of the value network.", "published": "2023-09-26 15:57:57", "link": "http://arxiv.org/abs/2309.15028v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "VideoDirectorGPT: Consistent Multi-scene Video Generation via LLM-Guided\n  Planning", "abstract": "Recent text-to-video (T2V) generation methods have seen significant\nadvancements. However, the majority of these works focus on producing short\nvideo clips of a single event (i.e., single-scene videos). Meanwhile, recent\nlarge language models (LLMs) have demonstrated their capability in generating\nlayouts and programs to control downstream visual modules. This prompts an\nimportant question: can we leverage the knowledge embedded in these LLMs for\ntemporally consistent long video generation? In this paper, we propose\nVideoDirectorGPT, a novel framework for consistent multi-scene video generation\nthat uses the knowledge of LLMs for video content planning and grounded video\ngeneration. Specifically, given a single text prompt, we first ask our video\nplanner LLM (GPT-4) to expand it into a 'video plan', which includes the scene\ndescriptions, the entities with their respective layouts, the background for\neach scene, and consistency groupings of the entities. Next, guided by this\nvideo plan, our video generator, named Layout2Vid, has explicit control over\nspatial layouts and can maintain temporal consistency of entities across\nmultiple scenes, while being trained only with image-level annotations. Our\nexperiments demonstrate that our proposed VideoDirectorGPT framework\nsubstantially improves layout and movement control in both single- and\nmulti-scene video generation and can generate multi-scene videos with\nconsistency, while achieving competitive performance with SOTAs in open-domain\nsingle-scene T2V generation. Detailed ablation studies, including dynamic\nadjustment of layout control strength with an LLM and video generation with\nuser-provided images, confirm the effectiveness of each component of our\nframework and its future potential.", "published": "2023-09-26 17:36:26", "link": "http://arxiv.org/abs/2309.15091v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Attention Satisfies: A Constraint-Satisfaction Lens on Factual Errors of\n  Language Models", "abstract": "We investigate the internal behavior of Transformer-based Large Language\nModels (LLMs) when they generate factually incorrect text. We propose modeling\nfactual queries as constraint satisfaction problems and use this framework to\ninvestigate how the LLM interacts internally with factual constraints. We find\na strong positive relationship between the LLM's attention to constraint tokens\nand the factual accuracy of generations. We curate a suite of 10 datasets\ncontaining over 40,000 prompts to study the task of predicting factual errors\nwith the Llama-2 family across all scales (7B, 13B, 70B). We propose SAT Probe,\na method probing attention patterns, that can predict factual errors and\nfine-grained constraint satisfaction, and allow early error identification. The\napproach and findings take another step towards using the mechanistic\nunderstanding of LLMs to enhance their reliability.", "published": "2023-09-26 17:48:55", "link": "http://arxiv.org/abs/2309.15098v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning Using Generated Privileged Information by Text-to-Image\n  Diffusion Models", "abstract": "Learning Using Privileged Information is a particular type of knowledge\ndistillation where the teacher model benefits from an additional data\nrepresentation during training, called privileged information, improving the\nstudent model, which does not see the extra representation. However, privileged\ninformation is rarely available in practice. To this end, we propose a text\nclassification framework that harnesses text-to-image diffusion models to\ngenerate artificial privileged information. The generated images and the\noriginal text samples are further used to train multimodal teacher models based\non state-of-the-art transformer-based architectures. Finally, the knowledge\nfrom multimodal teachers is distilled into a text-based (unimodal) student.\nHence, by employing a generative model to produce synthetic data as privileged\ninformation, we guide the training of the student model. Our framework, called\nLearning Using Generated Privileged Information (LUGPI), yields noticeable\nperformance gains on four text classification data sets, demonstrating its\npotential in text classification without any additional cost during inference.", "published": "2023-09-26 20:04:48", "link": "http://arxiv.org/abs/2309.15238v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Joint Prediction and Denoising for Large-scale Multilingual\n  Self-supervised Learning", "abstract": "Multilingual self-supervised learning (SSL) has often lagged behind\nstate-of-the-art (SOTA) methods due to the expenses and complexity required to\nhandle many languages. This further harms the reproducibility of SSL, which is\nalready limited to few research groups due to its resource usage. We show that\nmore powerful techniques can actually lead to more efficient pre-training,\nopening SSL to more research groups. We propose WavLabLM, which extends WavLM's\njoint prediction and denoising to 40k hours of data across 136 languages. To\nbuild WavLabLM, we devise a novel multi-stage pre-training method, designed to\naddress the language imbalance of multilingual data. WavLabLM achieves\ncomparable performance to XLS-R on ML-SUPERB with less than 10% of the training\ndata, making SSL realizable with academic compute. We show that further\nefficiency can be achieved with a vanilla HuBERT Base model, which can maintain\n94% of XLS-R's performance with only 3% of the data, 4 GPUs, and limited\ntrials. We open-source all code and models in ESPnet.", "published": "2023-09-26 23:55:57", "link": "http://arxiv.org/abs/2309.15317v2", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Learning from Flawed Data: Weakly Supervised Automatic Speech\n  Recognition", "abstract": "Training automatic speech recognition (ASR) systems requires large amounts of\nwell-curated paired data. However, human annotators usually perform\n\"non-verbatim\" transcription, which can result in poorly trained models. In\nthis paper, we propose Omni-temporal Classification (OTC), a novel training\ncriterion that explicitly incorporates label uncertainties originating from\nsuch weak supervision. This allows the model to effectively learn speech-text\nalignments while accommodating errors present in the training transcripts. OTC\nextends the conventional CTC objective for imperfect transcripts by leveraging\nweighted finite state transducers. Through experiments conducted on the\nLibriSpeech and LibriVox datasets, we demonstrate that training ASR models with\nOTC avoids performance degradation even with transcripts containing up to 70%\nerrors, a scenario where CTC models fail completely. Our implementation is\navailable at https://github.com/k2-fsa/icefall.", "published": "2023-09-26 12:58:40", "link": "http://arxiv.org/abs/2309.15796v1", "categories": ["eess.AS", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking\n  Unrelated Questions", "abstract": "Large language models (LLMs) can \"lie\", which we define as outputting false\nstatements despite \"knowing\" the truth in a demonstrable sense. LLMs might\n\"lie\", for example, when instructed to output misinformation. Here, we develop\na simple lie detector that requires neither access to the LLM's activations\n(black-box) nor ground-truth knowledge of the fact in question. The detector\nworks by asking a predefined set of unrelated follow-up questions after a\nsuspected lie, and feeding the LLM's yes/no answers into a logistic regression\nclassifier. Despite its simplicity, this lie detector is highly accurate and\nsurprisingly general. When trained on examples from a single setting --\nprompting GPT-3.5 to lie about factual questions -- the detector generalises\nout-of-distribution to (1) other LLM architectures, (2) LLMs fine-tuned to lie,\n(3) sycophantic lies, and (4) lies emerging in real-life scenarios such as\nsales. These results indicate that LLMs have distinctive lie-related\nbehavioural patterns, consistent across architectures and contexts, which could\nenable general-purpose lie detection.", "published": "2023-09-26 16:07:54", "link": "http://arxiv.org/abs/2309.15840v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unsupervised Pre-Training for Vietnamese Automatic Speech Recognition in\n  the HYKIST Project", "abstract": "In today's interconnected globe, moving abroad is more and more prevalent,\nwhether it's for employment, refugee resettlement, or other causes. Language\ndifficulties between natives and immigrants present a common issue on a daily\nbasis, especially in medical domain. This can make it difficult for patients\nand doctors to communicate during anamnesis or in the emergency room, which\ncompromises patient care. The goal of the HYKIST Project is to develop a speech\ntranslation system to support patient-doctor communication with ASR and MT.\n  ASR systems have recently displayed astounding performance on particular\ntasks for which enough quantities of training data are available, such as\nLibriSpeech. Building a good model is still difficult due to a variety of\nspeaking styles, acoustic and recording settings, and a lack of in-domain\ntraining data. In this thesis, we describe our efforts to construct ASR systems\nfor a conversational telephone speech recognition task in the medical domain\nfor Vietnamese language to assist emergency room contact between doctors and\npatients across linguistic barriers. In order to enhance the system's\nperformance, we investigate various training schedules and data combining\nstrategies. We also examine how best to make use of the little data that is\navailable. The use of publicly accessible models like XLSR-53 is compared to\nthe use of customized pre-trained models, and both supervised and unsupervised\napproaches are utilized using wav2vec 2.0 as architecture.", "published": "2023-09-26 21:12:09", "link": "http://arxiv.org/abs/2309.15869v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Ruffle&Riley: Towards the Automated Induction of Conversational Tutoring\n  Systems", "abstract": "Conversational tutoring systems (CTSs) offer learning experiences driven by\nnatural language interaction. They are known to promote high levels of\ncognitive engagement and benefit learning outcomes, particularly in reasoning\ntasks. Nonetheless, the time and cost required to author CTS content is a major\nobstacle to widespread adoption. In this paper, we introduce a novel type of\nCTS that leverages the recent advances in large language models (LLMs) in two\nways: First, the system induces a tutoring script automatically from a lesson\ntext. Second, the system automates the script orchestration via two LLM-based\nagents (Ruffle&Riley) with the roles of a student and a professor in a\nlearning-by-teaching format. The system allows a free-form conversation that\nfollows the ITS-typical inner and outer loop structure. In an initial\nbetween-subject online user study (N = 100) comparing Ruffle&Riley to simpler\nQA chatbots and reading activity, we found no significant differences in\npost-test scores. Nonetheless, in the learning experience survey, Ruffle&Riley\nusers expressed higher ratings of understanding and remembering and further\nperceived the offered support as more helpful and the conversation as coherent.\nOur study provides insights for a new generation of scalable CTS technologies.", "published": "2023-09-26 23:27:06", "link": "http://arxiv.org/abs/2310.01420v2", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "MSG-BART: Multi-granularity Scene Graph-Enhanced Encoder-Decoder\n  Language Model for Video-grounded Dialogue Generation", "abstract": "Generating dialogue grounded in videos requires a high level of understanding\nand reasoning about the visual scenes in the videos. However, existing large\nvisual-language models are not effective due to their latent features and\ndecoder-only structure, especially with respect to spatio-temporal relationship\nreasoning. In this paper, we propose a novel approach named MSG-BART, which\nenhances the integration of video information by incorporating a\nmulti-granularity spatio-temporal scene graph into an encoder-decoder\npre-trained language model. Specifically, we integrate the global and local\nscene graph into the encoder and decoder, respectively, to improve both overall\nperception and target reasoning capability. To further improve the information\nselection capability, we propose a multi-pointer network to facilitate\nselection between text and video. Extensive experiments are conducted on three\nvideo-grounded dialogue benchmarks, which show the significant superiority of\nthe proposed MSG-BART compared to a range of state-of-the-art approaches.", "published": "2023-09-26 04:23:23", "link": "http://arxiv.org/abs/2311.12820v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "eess.IV"], "primary_category": "cs.CV"}
{"title": "PLMM: Personal Large Language Models on Mobile Devices", "abstract": "Inspired by Federated Learning, in this paper, we propose personal large\nmodels that are distilled from traditional large language models but more\nadaptive to local users' personal information such as education background and\nhobbies. We classify the large language models into three levels: the personal\nlevel, expert level and traditional level. The personal level models are\nadaptive to users' personal information. They encrypt the users' input and\nprotect their privacy. The expert level models focus on merging specific\nknowledge such as finance, IT and art. The traditional models focus on the\nuniversal knowledge discovery and upgrading the expert models. In such\nclassifications, the personal models directly interact with the user. For the\nwhole system, the personal models have users' (encrypted) personal information.\nMoreover, such models must be small enough to be performed on personal\ncomputers or mobile devices. Finally, they also have to response in real-time\nfor better user experience and produce high quality results. The proposed\npersonal large models can be applied in a wide range of applications such as\nlanguage and vision tasks.", "published": "2023-09-26 07:36:20", "link": "http://arxiv.org/abs/2309.14726v2", "categories": ["cs.CV", "cs.AI", "cs.CE", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Creating Trustworthy LLMs: Dealing with Hallucinations in Healthcare AI", "abstract": "Large language models have proliferated across multiple domains in as short\nperiod of time. There is however hesitation in the medical and healthcare\ndomain towards their adoption because of issues like factuality, coherence, and\nhallucinations. Give the high stakes nature of healthcare, many researchers\nhave even cautioned against its usage until these issues are resolved. The key\nto the implementation and deployment of LLMs in healthcare is to make these\nmodels trustworthy, transparent (as much possible) and explainable. In this\npaper we describe the key elements in creating reliable, trustworthy, and\nunbiased models as a necessary condition for their adoption in healthcare.\nSpecifically we focus on the quantification, validation, and mitigation of\nhallucinations in the context in healthcare. Lastly, we discuss how the future\nof LLMs in healthcare may look like.", "published": "2023-09-26 20:52:46", "link": "http://arxiv.org/abs/2311.01463v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Low-rank Adaptation of Large Language Model Rescoring for\n  Parameter-Efficient Speech Recognition", "abstract": "We propose a neural language modeling system based on low-rank adaptation\n(LoRA) for speech recognition output rescoring. Although pretrained language\nmodels (LMs) like BERT have shown superior performance in second-pass\nrescoring, the high computational cost of scaling up the pretraining stage and\nadapting the pretrained models to specific domains limit their practical use in\nrescoring. Here we present a method based on low-rank decomposition to train a\nrescoring BERT model and adapt it to new domains using only a fraction (0.08%)\nof the pretrained parameters. These inserted matrices are optimized through a\ndiscriminative training objective along with a correlation-based regularization\nloss. The proposed low-rank adaptation Rescore-BERT (LoRB) architecture is\nevaluated on LibriSpeech and internal datasets with decreased training times by\nfactors between 5.4 and 3.6.", "published": "2023-09-26 19:41:34", "link": "http://arxiv.org/abs/2309.15223v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Rethinking Session Variability: Leveraging Session Embeddings for\n  Session Robustness in Speaker Verification", "abstract": "In the field of speaker verification, session or channel variability poses a\nsignificant challenge. While many contemporary methods aim to disentangle\nsession information from speaker embeddings, we introduce a novel approach\nusing an additional embedding to represent the session information. This is\nachieved by training an auxiliary network appended to the speaker embedding\nextractor which remains fixed in this training process. This results in two\nsimilarity scores: one for the speakers information and one for the session\ninformation. The latter score acts as a compensator for the former that might\nbe skewed due to session variations. Our extensive experiments demonstrate that\nsession information can be effectively compensated without retraining of the\nembedding extractor.", "published": "2023-09-26 08:09:30", "link": "http://arxiv.org/abs/2309.14741v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Exploring RWKV for Memory Efficient and Low Latency Streaming ASR", "abstract": "Recently, self-attention-based transformers and conformers have been\nintroduced as alternatives to RNNs for ASR acoustic modeling. Nevertheless, the\nfull-sequence attention mechanism is non-streamable and computationally\nexpensive, thus requiring modifications, such as chunking and caching, for\nefficient streaming ASR. In this paper, we propose to apply RWKV, a variant of\nlinear attention transformer, to streaming ASR. RWKV combines the superior\nperformance of transformers and the inference efficiency of RNNs, which is\nwell-suited for streaming ASR scenarios where the budget for latency and memory\nis restricted. Experiments on varying scales (100h - 10000h) demonstrate that\nRWKV-Transducer and RWKV-Boundary-Aware-Transducer achieve comparable to or\neven better accuracy compared with chunk conformer transducer, with minimal\nlatency and inference memory cost.", "published": "2023-09-26 08:41:24", "link": "http://arxiv.org/abs/2309.14758v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Optimization Techniques for a Physical Model of Human Vocalisation", "abstract": "We present a non-supervised approach to optimize and evaluate the synthesis\nof non-speech audio effects from a speech production model. We use the Pink\nTrombone synthesizer as a case study of a simplified production model of the\nvocal tract to target non-speech human audio signals --yawnings. We selected\nand optimized the control parameters of the synthesizer to minimize the\ndifference between real and generated audio. We validated the most common\noptimization techniques reported in the literature and a specifically designed\nneural network. We evaluated several popular quality metrics as error\nfunctions. These include both objective quality metrics and\nsubjective-equivalent metrics. We compared the results in terms of total error\nand computational demand. Results show that genetic and swarm optimizers\noutperform least squares algorithms at the cost of executing slower and that\nspecific combinations of optimizers and audio representations offer\nsignificantly different results. The proposed methodology could be used in\nbenchmarking other physical models and audio types.", "published": "2023-09-26 08:45:41", "link": "http://arxiv.org/abs/2309.14761v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Emphasized Non-Target Speaker Knowledge in Knowledge Distillation for\n  Automatic Speaker Verification", "abstract": "Knowledge distillation (KD) is used to enhance automatic speaker verification\nperformance by ensuring consistency between large teacher networks and\nlightweight student networks at the embedding level or label level. However,\nthe conventional label-level KD overlooks the significant knowledge from\nnon-target speakers, particularly their classification probabilities, which can\nbe crucial for automatic speaker verification. In this paper, we first\ndemonstrate that leveraging a larger number of training non-target speakers\nimproves the performance of automatic speaker verification models. Inspired by\nthis finding about the importance of non-target speakers' knowledge, we\nmodified the conventional label-level KD by disentangling and emphasizing the\nclassification probabilities of non-target speakers during knowledge\ndistillation. The proposed method is applied to three different student model\narchitectures and achieves an average of 13.67% improvement in EER on the\nVoxCeleb dataset compared to embedding-level and conventional label-level KD\nmethods.", "published": "2023-09-26 11:09:12", "link": "http://arxiv.org/abs/2309.14838v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Segment-Level Vectorized Beam Search Based on Partially Autoregressive\n  Inference", "abstract": "Attention-based encoder-decoder models with autoregressive (AR) decoding have\nproven to be the dominant approach for automatic speech recognition (ASR) due\nto their superior accuracy. However, they often suffer from slow inference.\nThis is primarily attributed to the incremental calculation of the decoder.\nThis work proposes a partially AR framework, which employs segment-level\nvectorized beam search for improving the inference speed of an ASR model based\non the hybrid connectionist temporal classification (CTC) attention-based\narchitecture. It first generates an initial hypothesis using greedy CTC\ndecoding, identifying low-confidence tokens based on their output\nprobabilities. We then utilize the decoder to perform segment-level vectorized\nbeam search on these tokens, re-predicting in parallel with minimal decoder\ncalculations. Experimental results show that our method is 12 to 13 times\nfaster in inference on the LibriSpeech corpus over AR decoding whilst\npreserving high accuracy.", "published": "2023-09-26 13:30:58", "link": "http://arxiv.org/abs/2309.14922v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Privacy-preserving and Privacy-attacking Approaches for Speech and Audio\n  -- A Survey", "abstract": "In contemporary society, voice-controlled devices, such as smartphones and\nhome assistants, have become pervasive due to their advanced capabilities and\nfunctionality. The always-on nature of their microphones offers users the\nconvenience of readily accessing these devices. However, recent research and\nevents have revealed that such voice-controlled devices are prone to various\nforms of malicious attacks, hence making it a growing concern for both users\nand researchers to safeguard against such attacks. Despite the numerous studies\nthat have investigated adversarial attacks and privacy preservation for images,\na conclusive study of this nature has not been conducted for the audio domain.\nTherefore, this paper aims to examine existing approaches for\nprivacy-preserving and privacy-attacking strategies for audio and speech. To\nachieve this goal, we classify the attack and defense scenarios into several\ncategories and provide detailed analysis of each approach. We also interpret\nthe dissimilarities between the various approaches, highlight their\ncontributions, and examine their limitations. Our investigation reveals that\nvoice-controlled devices based on neural networks are inherently susceptible to\nspecific types of attacks. Although it is possible to enhance the robustness of\nsuch models to certain forms of attack, more sophisticated approaches are\nrequired to comprehensively safeguard user privacy.", "published": "2023-09-26 17:31:35", "link": "http://arxiv.org/abs/2309.15087v1", "categories": ["cs.CR", "eess.AS"], "primary_category": "cs.CR"}
{"title": "Synthia's Melody: A Benchmark Framework for Unsupervised Domain\n  Adaptation in Audio", "abstract": "Despite significant advancements in deep learning for vision and natural\nlanguage, unsupervised domain adaptation in audio remains relatively\nunexplored. We, in part, attribute this to the lack of an appropriate benchmark\ndataset. To address this gap, we present Synthia's melody, a novel audio data\ngeneration framework capable of simulating an infinite variety of 4-second\nmelodies with user-specified confounding structures characterised by musical\nkeys, timbre, and loudness. Unlike existing datasets collected under\nobservational settings, Synthia's melody is free of unobserved biases, ensuring\nthe reproducibility and comparability of experiments. To showcase its utility,\nwe generate two types of distribution shifts-domain shift and sample selection\nbias-and evaluate the performance of acoustic deep learning models under these\nshifts. Our evaluations reveal that Synthia's melody provides a robust testbed\nfor examining the susceptibility of these models to varying levels of\ndistribution shift.", "published": "2023-09-26 15:46:06", "link": "http://arxiv.org/abs/2309.15024v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Simultaneously Learning Speaker's Direction and Head Orientation from\n  Binaural Recordings", "abstract": "Estimation of a speaker's direction and head orientation with binaural\nrecordings can be a critical piece of information in many real-world\napplications with emerging `earable' devices, including smart headphones and\nAR/VR headsets. However, it requires predicting the mutual head orientations of\nboth the speaker and the listener, which is challenging in practice. This paper\npresents a system for jointly predicting speaker-listener head orientations by\nleveraging inherent human voice directivity and listener's head-related\ntransfer function (HRTF) as perceived by the ear-mounted microphones on the\nlistener. We propose a convolution neural network model that, given binaural\nspeech recording, can predict the orientation of both speaker and listener with\nrespect to the line joining the two. The system builds on the core observation\nthat the recordings from the left and right ears are differentially affected by\nthe voice directivity as well as the HRTF. We also incorporate the fact that\nvoice is more directional at higher frequencies compared to lower frequencies.", "published": "2023-09-26 16:49:21", "link": "http://arxiv.org/abs/2309.15064v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Collaborative Watermarking for Adversarial Speech Synthesis", "abstract": "Advances in neural speech synthesis have brought us technology that is not\nonly close to human naturalness, but is also capable of instant voice cloning\nwith little data, and is highly accessible with pre-trained models available.\nNaturally, the potential flood of generated content raises the need for\nsynthetic speech detection and watermarking. Recently, considerable research\neffort in synthetic speech detection has been related to the Automatic Speaker\nVerification and Spoofing Countermeasure Challenge (ASVspoof), which focuses on\npassive countermeasures. This paper takes a complementary view to generated\nspeech detection: a synthesis system should make an active effort to watermark\nthe generated speech in a way that aids detection by another machine, but\nremains transparent to a human listener. We propose a collaborative training\nscheme for synthetic speech watermarking and show that a HiFi-GAN neural\nvocoder collaborating with the ASVspoof 2021 baseline countermeasure models\nconsistently improves detection performance over conventional classifier\ntraining. Furthermore, we demonstrate how collaborative training can be paired\nwith augmentation strategies for added robustness against noise and\ntime-stretching. Finally, listening tests demonstrate that collaborative\ntraining has little adverse effect on perceptual quality of vocoded speech.", "published": "2023-09-26 19:43:14", "link": "http://arxiv.org/abs/2309.15224v2", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speech Audio Synthesis from Tagged MRI and Non-Negative Matrix\n  Factorization via Plastic Transformer", "abstract": "The tongue's intricate 3D structure, comprising localized functional units,\nplays a crucial role in the production of speech. When measured using tagged\nMRI, these functional units exhibit cohesive displacements and derived\nquantities that facilitate the complex process of speech production.\nNon-negative matrix factorization-based approaches have been shown to estimate\nthe functional units through motion features, yielding a set of building blocks\nand a corresponding weighting map. Investigating the link between weighting\nmaps and speech acoustics can offer significant insights into the intricate\nprocess of speech production. To this end, in this work, we utilize\ntwo-dimensional spectrograms as a proxy representation, and develop an\nend-to-end deep learning framework for translating weighting maps to their\ncorresponding audio waveforms. Our proposed plastic light transformer (PLT)\nframework is based on directional product relative position bias and\nsingle-level spatial pyramid pooling, thus enabling flexible processing of\nweighting maps with variable size to fixed-size spectrograms, without input\ninformation loss or dimension expansion. Additionally, our PLT framework\nefficiently models the global correlation of wide matrix input. To improve the\nrealism of our generated spectrograms with relatively limited training samples,\nwe apply pair-wise utterance consistency with Maximum Mean Discrepancy\nconstraint and adversarial training. Experimental results on a dataset of 29\nsubjects speaking two utterances demonstrated that our framework is able to\nsynthesize speech audio waveforms from weighting maps, outperforming\nconventional convolution and transformer models.", "published": "2023-09-26 00:21:17", "link": "http://arxiv.org/abs/2309.14586v1", "categories": ["cs.SD", "cs.AI", "cs.CV", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "A multi-modal approach for identifying schizophrenia using cross-modal\n  attention", "abstract": "This study focuses on how different modalities of human communication can be\nused to distinguish between healthy controls and subjects with schizophrenia\nwho exhibit strong positive symptoms. We developed a multi-modal schizophrenia\nclassification system using audio, video, and text. Facial action units and\nvocal tract variables were extracted as low-level features from video and audio\nrespectively, which were then used to compute high-level coordination features\nthat served as the inputs to the audio and video modalities.\nContext-independent text embeddings extracted from transcriptions of speech\nwere used as the input for the text modality. The multi-modal system is\ndeveloped by fusing a segment-to-session-level classifier for video and audio\nmodalities with a text model based on a Hierarchical Attention Network (HAN)\nwith cross-modal attention. The proposed multi-modal system outperforms the\nprevious state-of-the-art multi-modal system by 8.53% in the weighted average\nF1 score.", "published": "2023-09-26 14:28:16", "link": "http://arxiv.org/abs/2309.15136v3", "categories": ["eess.SP", "cs.MM", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "eess.SP"}
