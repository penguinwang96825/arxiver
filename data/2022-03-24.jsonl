{"title": "Evaluating Distributional Distortion in Neural Language Modeling", "abstract": "A fundamental characteristic of natural language is the high rate at which\nspeakers produce novel expressions. Because of this novelty, a heavy-tail of\nrare events accounts for a significant amount of the total probability mass of\ndistributions in language (Baayen, 2001). Standard language modeling metrics\nsuch as perplexity quantify the performance of language models (LM) in\naggregate. As a result, we have relatively little understanding of whether\nneural LMs accurately estimate the probability of sequences in this heavy-tail\nof rare events. To address this gap, we develop a controlled evaluation scheme\nwhich uses generative models trained on natural data as artificial languages\nfrom which we can exactly compute sequence probabilities. Training LMs on\ngenerations from these artificial languages, we compare the sequence-level\nprobability estimates given by LMs to the true probabilities in the target\nlanguage. Our experiments reveal that LSTM and Transformer language models (i)\nsystematically underestimate the probability of sequences drawn from the target\nlanguage, and (ii) do so more severely for less-probable sequences.\nInvestigating where this probability mass went, (iii) we find that LMs tend to\noverestimate the probability of ill formed (perturbed) sequences. In addition,\nwe find that this underestimation behaviour (iv) is weakened, but not\neliminated by greater amounts of training data, and (v) is exacerbated for\ntarget distributions with lower entropy.", "published": "2022-03-24 01:09:46", "link": "http://arxiv.org/abs/2203.12788v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting the Effects of Leakage on Dependency Parsing", "abstract": "Recent work by S{\\o}gaard (2020) showed that, treebank size aside, overlap\nbetween training and test graphs (termed leakage) explains more of the observed\nvariation in dependency parsing performance than other explanations. In this\nwork we revisit this claim, testing it on more models and languages. We find\nthat it only holds for zero-shot cross-lingual settings. We then propose a more\nfine-grained measure of such leakage which, unlike the original measure, not\nonly explains but also correlates with observed performance variation. Code and\ndata are available here: https://github.com/miriamwanner/reu-nlp-project", "published": "2022-03-24 02:33:30", "link": "http://arxiv.org/abs/2203.12815v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multitasking Framework for Unsupervised Simple Definition Generation", "abstract": "The definition generation task can help language learners by providing\nexplanations for unfamiliar words. This task has attracted much attention in\nrecent years. We propose a novel task of Simple Definition Generation (SDG) to\nhelp language learners and low literacy readers. A significant challenge of\nthis task is the lack of learner's dictionaries in many languages, and\ntherefore the lack of data for supervised training. We explore this task and\npropose a multitasking framework SimpDefiner that only requires a standard\ndictionary with complex definitions and a corpus containing arbitrary simple\ntexts. We disentangle the complexity factors from the text by carefully\ndesigning a parameter sharing scheme between two decoders. By jointly training\nthese components, the framework can generate both complex and simple\ndefinitions simultaneously. We demonstrate that the framework can generate\nrelevant, simple definitions for the target words through automatic and manual\nevaluations on English and Chinese datasets. Our method outperforms the\nbaseline model by a 1.77 SARI score on the English dataset, and raises the\nproportion of the low level (HSK level 1-3) words in Chinese definitions by\n3.87%.", "published": "2022-03-24 08:16:04", "link": "http://arxiv.org/abs/2203.12926v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Probing for Labeled Dependency Trees", "abstract": "Probing has become an important tool for analyzing representations in Natural\nLanguage Processing (NLP). For graphical NLP tasks such as dependency parsing,\nlinear probes are currently limited to extracting undirected or unlabeled parse\ntrees which do not capture the full task. This work introduces DepProbe, a\nlinear probe which can extract labeled and directed dependency parse trees from\nembeddings while using fewer parameters and compute than prior methods.\nLeveraging its full task coverage and lightweight parametrization, we\ninvestigate its predictive power for selecting the best transfer language for\ntraining a full biaffine attention parser. Across 13 languages, our proposed\nmethod identifies the best source treebank 94% of the time, outperforming\ncompetitive baselines and prior work. Finally, we analyze the informativeness\nof task-specific subspaces in contextual embeddings as well as which benefits a\nfull parser's non-linear parametrization provides.", "published": "2022-03-24 10:21:07", "link": "http://arxiv.org/abs/2203.12971v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Scientific Claims for Zero-Shot Scientific Fact Checking", "abstract": "Automated scientific fact checking is difficult due to the complexity of\nscientific language and a lack of significant amounts of training data, as\nannotation requires domain expertise. To address this challenge, we propose\nscientific claim generation, the task of generating one or more atomic and\nverifiable claims from scientific sentences, and demonstrate its usefulness in\nzero-shot fact checking for biomedical claims. We propose CLAIMGEN-BART, a new\nsupervised method for generating claims supported by the literature, as well as\nKBIN, a novel method for generating claim negations. Additionally, we adapt an\nexisting unsupervised entity-centric method of claim generation to biomedical\nclaims, which we call CLAIMGEN-ENTITY. Experiments on zero-shot fact checking\ndemonstrate that both CLAIMGEN-ENTITY and CLAIMGEN-BART, coupled with KBIN,\nachieve up to 90% performance of fully supervised models trained on manually\nannotated claims and evidence. A rigorous evaluation study demonstrates\nsignificant improvement in generated claim and negation quality over existing\nbaselines", "published": "2022-03-24 11:29:20", "link": "http://arxiv.org/abs/2203.12990v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Kratt: Developing an Automatic Subject Indexing Tool for The National\n  Library of Estonia", "abstract": "Manual subject indexing in libraries is a time-consuming and costly process\nand the quality of the assigned subjects is affected by the cataloguer's\nknowledge on the specific topics contained in the book. Trying to solve these\nissues, we exploited the opportunities arising from artificial intelligence to\ndevelop Kratt: a prototype of an automatic subject indexing tool. Kratt is able\nto subject index a book independent of its extent and genre with a set of\nkeywords present in the Estonian Subject Thesaurus. It takes Kratt\napproximately 1 minute to subject index a book, outperforming humans 10-15\ntimes. Although the resulting keywords were not considered satisfactory by the\ncataloguers, the ratings of a small sample of regular library users showed more\npromise. We also argue that the results can be enhanced by including a bigger\ncorpus for training the model and applying more careful preprocessing\ntechniques.", "published": "2022-03-24 11:45:44", "link": "http://arxiv.org/abs/2203.12998v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ensembling and Knowledge Distilling of Large Sequence Taggers for\n  Grammatical Error Correction", "abstract": "In this paper, we investigate improvements to the GEC sequence tagging\narchitecture with a focus on ensembling of recent cutting-edge\nTransformer-based encoders in Large configurations. We encourage ensembling\nmodels by majority votes on span-level edits because this approach is tolerant\nto the model architecture and vocabulary size. Our best ensemble achieves a new\nSOTA result with an $F_{0.5}$ score of 76.05 on BEA-2019 (test), even without\npre-training on synthetic datasets. In addition, we perform knowledge\ndistillation with a trained ensemble to generate new synthetic training\ndatasets, \"Troy-Blogs\" and \"Troy-1BW\". Our best single sequence tagging model\nthat is pretrained on the generated Troy-datasets in combination with the\npublicly available synthetic PIE dataset achieves a near-SOTA (To the best of\nour knowledge, our best single model gives way only to much heavier T5 model\nresult with an $F_{0.5}$ score of 73.21 on BEA-2019 (test). The code, datasets,\nand trained models are publicly available).", "published": "2022-03-24 13:18:36", "link": "http://arxiv.org/abs/2203.13064v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "minicons: Enabling Flexible Behavioral and Representational Analyses of\n  Transformer Language Models", "abstract": "We present minicons, an open source library that provides a standard API for\nresearchers interested in conducting behavioral and representational analyses\nof transformer-based language models (LMs). Specifically, minicons enables\nresearchers to apply analysis methods at two levels: (1) at the prediction\nlevel -- by providing functions to efficiently extract word/sentence level\nprobabilities; and (2) at the representational level -- by also facilitating\nefficient extraction of word/phrase level vectors from one or more layers. In\nthis paper, we describe the library and apply it to two motivating case\nstudies: One focusing on the learning dynamics of the BERT architecture on\nrelative grammatical judgments, and the other on benchmarking 23 different LMs\non zero-shot abductive reasoning. minicons is available at\nhttps://github.com/kanishkamisra/minicons", "published": "2022-03-24 15:11:06", "link": "http://arxiv.org/abs/2203.13112v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Direct parsing to sentiment graphs", "abstract": "This paper demonstrates how a graph-based semantic parser can be applied to\nthe task of structured sentiment analysis, directly predicting sentiment graphs\nfrom text. We advance the state of the art on 4 out of 5 standard benchmark\nsets. We release the source code, models and predictions.", "published": "2022-03-24 17:09:23", "link": "http://arxiv.org/abs/2203.13209v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SMARAGD: Learning SMatch for Accurate and Rapid Approximate Graph\n  Distance", "abstract": "The similarity of graph structures, such as Meaning Representations (MRs), is\noften assessed via structural matching algorithms, such as Smatch (Cai and\nKnight, 2013). However, Smatch involves a combinatorial problem that suffers\nfrom NP-completeness, making large-scale applications, e.g., graph clustering\nor search, infeasible. To alleviate this issue, we learn SMARAGD: Semantic\nMatch for Accurate and Rapid Approximate Graph Distance. We show the potential\nof neural networks to approximate Smatch scores, i) in linear time using a\nmachine translation framework to predict alignments, or ii) in constant time\nusing a Siamese CNN to directly predict Smatch scores. We show that the\napproximation error can be substantially reduced through data augmentation and\ngraph anonymization.", "published": "2022-03-24 17:31:46", "link": "http://arxiv.org/abs/2203.13226v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "One Country, 700+ Languages: NLP Challenges for Underrepresented\n  Languages and Dialects in Indonesia", "abstract": "NLP research is impeded by a lack of resources and awareness of the\nchallenges presented by underrepresented languages and dialects. Focusing on\nthe languages spoken in Indonesia, the second most linguistically diverse and\nthe fourth most populous nation of the world, we provide an overview of the\ncurrent state of NLP research for Indonesia's 700+ languages. We highlight\nchallenges in Indonesian NLP and how these affect the performance of current\nNLP systems. Finally, we provide general recommendations to help develop NLP\ntechnology not only for languages of Indonesia but also other underrepresented\nlanguages.", "published": "2022-03-24 22:07:22", "link": "http://arxiv.org/abs/2203.13357v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Classifying Cyber-Risky Clinical Notes by Employing Natural Language\n  Processing", "abstract": "Clinical notes, which can be embedded into electronic medical records,\ndocument patient care delivery and summarize interactions between healthcare\nproviders and patients. These clinical notes directly inform patient care and\ncan also indirectly inform research and quality/safety metrics, among other\nindirect metrics. Recently, some states within the United States of America\nrequire patients to have open access to their clinical notes to improve the\nexchange of patient information for patient care. Thus, developing methods to\nassess the cyber risks of clinical notes before sharing and exchanging data is\ncritical. While existing natural language processing techniques are geared to\nde-identify clinical notes, to the best of our knowledge, few have focused on\nclassifying sensitive-information risk, which is a fundamental step toward\ndeveloping effective, widespread protection of patient health information. To\nbridge this gap, this research investigates methods for identifying\nsecurity/privacy risks within clinical notes. The classification either can be\nused upstream to identify areas within notes that likely contain sensitive\ninformation or downstream to improve the identification of clinical notes that\nhave not been entirely de-identified. We develop several models using unigram\nand word2vec features with different classifiers to categorize sentence risk.\nExperiments on i2b2 de-identification dataset show that the SVM classifier\nusing word2vec features obtained a maximum F1-score of 0.792. Future research\ninvolves articulation and differentiation of risk in terms of different global\nregulatory requirements.", "published": "2022-03-24 00:36:59", "link": "http://arxiv.org/abs/2203.12781v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Multilingual CheckList: Generation and Evaluation", "abstract": "Multilingual evaluation benchmarks usually contain limited high-resource\nlanguages and do not test models for specific linguistic capabilities.\nCheckList is a template-based evaluation approach that tests models for\nspecific capabilities. The CheckList template creation process requires native\nspeakers, posing a challenge in scaling to hundreds of languages. In this work,\nwe explore multiple approaches to generate Multilingual CheckLists. We device\nan algorithm - Template Extraction Algorithm (TEA) for automatically extracting\ntarget language CheckList templates from machine translated instances of a\nsource language templates. We compare the TEA CheckLists with CheckLists\ncreated with different levels of human intervention. We further introduce\nmetrics along the dimensions of cost, diversity, utility, and correctness to\ncompare the CheckLists. We thoroughly analyze different approaches to creating\nCheckLists in Hindi. Furthermore, we experiment with 9 more different\nlanguages. We find that TEA followed by human verification is ideal for scaling\nChecklist-based evaluation to multiple languages while TEA gives a good\nestimates of model performance.", "published": "2022-03-24 06:05:28", "link": "http://arxiv.org/abs/2203.12865v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Can Unsupervised Knowledge Transfer from Social Discussions Help\n  Argument Mining?", "abstract": "Identifying argument components from unstructured texts and predicting the\nrelationships expressed among them are two primary steps of argument mining.\nThe intrinsic complexity of these tasks demands powerful learning models. While\npretrained Transformer-based Language Models (LM) have been shown to provide\nstate-of-the-art results over different NLP tasks, the scarcity of manually\nannotated data and the highly domain-dependent nature of argumentation restrict\nthe capabilities of such models. In this work, we propose a novel transfer\nlearning strategy to overcome these challenges. We utilize argumentation-rich\nsocial discussions from the ChangeMyView subreddit as a source of unsupervised,\nargumentative discourse-aware knowledge by finetuning pretrained LMs on a\nselectively masked language modeling task. Furthermore, we introduce a novel\nprompt-based strategy for inter-component relation prediction that compliments\nour proposed finetuning method while leveraging on the discourse context.\nExhaustive experiments show the generalization capability of our method on\nthese two tasks over within-domain as well as out-of-domain datasets,\noutperforming several existing and employed strong baselines.", "published": "2022-03-24 06:48:56", "link": "http://arxiv.org/abs/2203.12881v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Lahjoita puhetta -- a large-scale corpus of spoken Finnish with some\n  benchmarks", "abstract": "The Donate Speech campaign has so far succeeded in gathering approximately\n3600 hours of ordinary, colloquial Finnish speech into the Lahjoita puhetta\n(Donate Speech) corpus. The corpus includes over twenty thousand speakers from\nall the regions of Finland and from all age brackets. The primary goals of the\ncollection were to create a representative, large-scale resource to study\nspontaneous spoken Finnish and to accelerate the development of language\ntechnology and speech-based services. In this paper, we present the collection\nprocess and the collected corpus, and showcase its versatility through multiple\nuse cases. The evaluated use cases include: automatic speech recognition of\nspontaneous speech, detection of age, gender, dialect and topic and metadata\nanalysis. We provide benchmarks for the use cases, as well down loadable,\ntrained baseline systems with open-source code for reproducibility. One further\nuse case is to verify the metadata and transcripts given in this corpus itself,\nand to suggest artificial metadata and transcripts for the part of the corpus\nwhere it is missing.", "published": "2022-03-24 07:50:25", "link": "http://arxiv.org/abs/2203.12906v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Mono vs Multilingual BERT: A Case Study in Hindi and Marathi Named\n  Entity Recognition", "abstract": "Named entity recognition (NER) is the process of recognising and classifying\nimportant information (entities) in text. Proper nouns, such as a person's\nname, an organization's name, or a location's name, are examples of entities.\nThe NER is one of the important modules in applications like human resources,\ncustomer support, search engines, content classification, and academia. In this\nwork, we consider NER for low-resource Indian languages like Hindi and Marathi.\nThe transformer-based models have been widely used for NER tasks. We consider\ndifferent variations of BERT like base-BERT, RoBERTa, and AlBERT and benchmark\nthem on publicly available Hindi and Marathi NER datasets. We provide an\nexhaustive comparison of different monolingual and multilingual\ntransformer-based models and establish simple baselines currently missing in\nthe literature. We show that the monolingual MahaRoBERTa model performs the\nbest for Marathi NER whereas the multilingual XLM-RoBERTa performs the best for\nHindi NER. We also perform cross-language evaluation and present mixed\nobservations.", "published": "2022-03-24 07:50:41", "link": "http://arxiv.org/abs/2203.12907v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Duality-Induced Regularizer for Semantic Matching Knowledge Graph\n  Embeddings", "abstract": "Semantic matching models -- which assume that entities with similar semantics\nhave similar embeddings -- have shown great power in knowledge graph embeddings\n(KGE). Many existing semantic matching models use inner products in embedding\nspaces to measure the plausibility of triples and quadruples in static and\ntemporal knowledge graphs. However, vectors that have the same inner products\nwith another vector can still be orthogonal to each other, which implies that\nentities with similar semantics may have dissimilar embeddings. This property\nof inner products significantly limits the performance of semantic matching\nmodels. To address this challenge, we propose a novel regularizer -- namely,\nDUality-induced RegulArizer (DURA) -- which effectively encourages the entities\nwith similar semantics to have similar embeddings. The major novelty of DURA is\nbased on the observation that, for an existing semantic matching KGE model\n(primal), there is often another distance based KGE model (dual) closely\nassociated with it, which can be used as effective constraints for entity\nembeddings. Experiments demonstrate that DURA consistently and significantly\nimproves the performance of state-of-the-art semantic matching models on both\nstatic and temporal knowledge graph benchmarks.", "published": "2022-03-24 09:24:39", "link": "http://arxiv.org/abs/2203.12949v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Emergence of hierarchical reference systems in multi-agent communication", "abstract": "In natural language, referencing objects at different levels of specificity\nis a fundamental pragmatic mechanism for efficient communication in context. We\ndevelop a novel communication game, the hierarchical reference game, to study\nthe emergence of such reference systems in artificial agents. We consider a\nsimplified world, in which concepts are abstractions over a set of primitive\nattributes (e.g., color, style, shape). Depending on how many attributes are\ncombined, concepts are more general (\"circle\") or more specific (\"red dotted\ncircle\"). Based on the context, the agents have to communicate at different\nlevels of this hierarchy. Our results show that the agents learn to play the\ngame successfully and can even generalize to novel concepts. To achieve\nabstraction, they use implicit (omitting irrelevant information) and explicit\n(indicating that attributes are irrelevant) strategies. In addition, the\ncompositional structure underlying the concept hierarchy is reflected in the\nemergent protocols, indicating that the need to develop hierarchical reference\nsystems supports the emergence of compositionality.", "published": "2022-03-24 16:52:07", "link": "http://arxiv.org/abs/2203.13176v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Language Models that Seek for Knowledge: Modular Search & Generation for\n  Dialogue and Prompt Completion", "abstract": "Language models (LMs) have recently been shown to generate more factual\nresponses by employing modularity (Zhou et al., 2021) in combination with\nretrieval (Adolphs et al., 2021). We extend the recent approach of Adolphs et\nal. (2021) to include internet search as a module. Our SeeKeR (Search\nengine->Knowledge->Response) method thus applies a single LM to three modular\ntasks in succession: search, generating knowledge, and generating a final\nresponse. We show that, when using SeeKeR as a dialogue model, it outperforms\nthe state-of-the-art model BlenderBot 2 (Chen et al., 2021) on open-domain\nknowledge-grounded conversations for the same number of parameters, in terms of\nconsistency, knowledge and per-turn engagingness. SeeKeR applied to topical\nprompt completions as a standard language model outperforms GPT2 (Radford et\nal., 2019) and GPT3 (Brown et al., 2020) in terms of factuality and topicality,\ndespite GPT3 being a vastly larger model. Our code and models are made publicly\navailable.", "published": "2022-03-24 17:31:26", "link": "http://arxiv.org/abs/2203.13224v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Token Dropping for Efficient BERT Pretraining", "abstract": "Transformer-based models generally allocate the same amount of computation\nfor each token in a given sequence. We develop a simple but effective \"token\ndropping\" method to accelerate the pretraining of transformer models, such as\nBERT, without degrading its performance on downstream tasks. In short, we drop\nunimportant tokens starting from an intermediate layer in the model to make the\nmodel focus on important tokens; the dropped tokens are later picked up by the\nlast layer of the model so that the model still produces full-length sequences.\nWe leverage the already built-in masked language modeling (MLM) loss to\nidentify unimportant tokens with practically no computational overhead. In our\nexperiments, this simple approach reduces the pretraining cost of BERT by 25%\nwhile achieving similar overall fine-tuning performance on standard downstream\ntasks.", "published": "2022-03-24 17:50:46", "link": "http://arxiv.org/abs/2203.13240v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Searching for fingerspelled content in American Sign Language", "abstract": "Natural language processing for sign language video - including tasks like\nrecognition, translation, and search - is crucial for making artificial\nintelligence technologies accessible to deaf individuals, and is gaining\nresearch interest in recent years. In this paper, we address the problem of\nsearching for fingerspelled key-words or key phrases in raw sign language\nvideos. This is an important task since significant content in sign language is\noften conveyed via fingerspelling, and to our knowledge the task has not been\nstudied before. We propose an end-to-end model for this task, FSS-Net, that\njointly detects fingerspelling and matches it to a text sequence. Our\nexperiments, done on a large public dataset of ASL fingerspelling in the wild,\nshow the importance of fingerspelling detection as a component of a search and\nretrieval model. Our model significantly outperforms baseline methods adapted\nfrom prior work on related tasks", "published": "2022-03-24 18:36:22", "link": "http://arxiv.org/abs/2203.13291v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Mix and Match: Learning-free Controllable Text Generation using Energy\n  Language Models", "abstract": "Recent work on controlled text generation has either required attribute-based\nfine-tuning of the base language model (LM), or has restricted the\nparameterization of the attribute discriminator to be compatible with the base\nautoregressive LM. In this work, we propose Mix and Match LM, a global\nscore-based alternative for controllable text generation that combines\narbitrary pre-trained black-box models for achieving the desired attributes in\nthe generated text without involving any fine-tuning or structural assumptions\nabout the black-box models. We interpret the task of controllable generation as\ndrawing samples from an energy-based model whose energy values are a linear\ncombination of scores from black-box models that are separately responsible for\nfluency, the control attribute, and faithfulness to any conditioning context.\nWe use a Metropolis-Hastings sampling scheme to sample from this energy-based\nmodel using bidirectional context and global attribute features. We validate\nthe effectiveness of our approach on various controlled generation and\nstyle-based text revision tasks by outperforming recently proposed methods that\ninvolve extra training, fine-tuning, or restrictive assumptions over the form\nof models.", "published": "2022-03-24 18:52:09", "link": "http://arxiv.org/abs/2203.13299v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Does human speech follow Benford's Law?", "abstract": "Researchers have observed that the frequencies of leading digits in many\nman-made and naturally occurring datasets follow a logarithmic curve, with\ndigits that start with the number 1 accounting for $\\sim 30\\%$ of all numbers\nin the dataset and digits that start with the number 9 accounting for $\\sim\n5\\%$ of all numbers in the dataset. This phenomenon, known as Benford's Law, is\nhighly repeatable and appears in lists of numbers from electricity bills, stock\nprices, tax returns, house prices, death rates, lengths of rivers, and\nnaturally occurring images. In this paper we demonstrate that human speech\nspectra also follow Benford's Law on average. That is, when averaged over many\nspeakers, the frequencies of leading digits in speech magnitude spectra follow\nthis distribution, although with some variability at the individual sample\nlevel. We use this observation to motivate a new set of features that can be\nefficiently extracted from speech and demonstrate that these features can be\nused to classify between human speech and synthetic speech.", "published": "2022-03-24 21:54:49", "link": "http://arxiv.org/abs/2203.13352v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Gender and Racial Stereotype Detection in Legal Opinion Word Embeddings", "abstract": "Studies have shown that some Natural Language Processing (NLP) systems encode\nand replicate harmful biases with potential adverse ethical effects in our\nsociety. In this article, we propose an approach for identifying gender and\nracial stereotypes in word embeddings trained on judicial opinions from U.S.\ncase law. Embeddings containing stereotype information may cause harm when used\nby downstream systems for classification, information extraction, question\nanswering, or other machine learning systems used to build legal research\ntools. We first explain how previously proposed methods for identifying these\nbiases are not well suited for use with word embeddings trained on legal\nopinion text. We then propose a domain adapted method for identifying gender\nand racial biases in the legal domain. Our analyses using these methods suggest\nthat racial and gender biases are encoded into word embeddings trained on legal\nopinions. These biases are not mitigated by exclusion of historical data, and\nappear across multiple large topical areas of the law. Implications for\ndownstream systems that use legal opinion word embeddings and suggestions for\npotential mitigation strategies based on our observations are also discussed.", "published": "2022-03-24 22:30:49", "link": "http://arxiv.org/abs/2203.13369v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Disentangleing Content and Fine-grained Prosody Information via Hybrid\n  ASR Bottleneck Features for Voice Conversion", "abstract": "Non-parallel data voice conversion (VC) have achieved considerable\nbreakthroughs recently through introducing bottleneck features (BNFs) extracted\nby the automatic speech recognition(ASR) model. However, selection of BNFs have\na significant impact on VC result. For example, when extracting BNFs from ASR\ntrained with Cross Entropy loss (CE-BNFs) and feeding into neural network to\ntrain a VC system, the timbre similarity of converted speech is significantly\ndegraded. If BNFs are extracted from ASR trained using Connectionist Temporal\nClassification loss (CTC-BNFs), the naturalness of the converted speech may\ndecrease. This phenomenon is caused by the difference of information contained\nin BNFs. In this paper, we proposed an any-to-one VC method using hybrid\nbottleneck features extracted from CTC-BNFs and CE-BNFs to complement each\nother advantages. Gradient reversal layer and instance normalization were used\nto extract prosody information from CE-BNFs and content information from\nCTC-BNFs. Auto-regressive decoder and Hifi-GAN vocoder were used to generate\nhigh-quality waveform. Experimental results show that our proposed method\nachieves higher similarity, naturalness, quality than baseline method and\nreveals the differences between the information contained in CE-BNFs and\nCTC-BNFs as well as the influence they have on the converted speech.", "published": "2022-03-24 02:24:39", "link": "http://arxiv.org/abs/2203.12813v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Automatic Speech Recognition for Speech Assessment of Persian Preschool\n  Children", "abstract": "Preschool evaluation is crucial because it gives teachers and parents\ninfluential knowledge about children's growth and development. The COVID-19\npandemic has highlighted the necessity of online assessment for preschool\nchildren. One of the areas that should be tested is their ability to speak.\nEmploying an Automatic Speech Recognition (ASR) system would not help since\nthey are pre-trained on voices that differ from children's in terms of\nfrequency and amplitude. Because most of these are pre-trained with data in a\nspecific range of amplitude, their objectives do not make them ready for voices\nin different amplitudes. To overcome this issue, we added a new objective to\nthe masking objective of the Wav2Vec 2.0 model called Random Frequency Pitch\n(RFP). In addition, we used our newly introduced dataset to fine-tune our model\nfor Meaningless Words (MW) and Rapid Automatic Naming (RAN) tests. Using\nmasking in concatenation with RFP outperforms the masking objective of Wav2Vec\n2.0 by reaching a Word Error Rate (WER) of 1.35. Our new approach reaches a WER\nof 6.45 on the Persian section of the CommonVoice dataset. Furthermore, our\nnovel methodology produces positive outcomes in zero- and few-shot scenarios.", "published": "2022-03-24 07:15:24", "link": "http://arxiv.org/abs/2203.12886v10", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Rationale-Centric Framework for Human-in-the-loop Machine Learning", "abstract": "We present a novel rationale-centric framework with human-in-the-loop --\nRationales-centric Double-robustness Learning (RDL) -- to boost model\nout-of-distribution performance in few-shot learning scenarios. By using static\nsemi-factual generation and dynamic human-intervened correction, RDL exploits\nrationales (i.e. phrases that cause the prediction), human interventions and\nsemi-factual augmentations to decouple spurious associations and bias models\ntowards generally applicable underlying distributions, which enables fast and\naccurate generalisation. Experimental results show that RDL leads to\nsignificant prediction benefits on both in-distribution and out-of-distribution\ntests compared to many state-of-the-art benchmarks -- especially for few-shot\nlearning scenarios. We also perform extensive ablation studies to support\nin-depth analyses of each component in our framework.", "published": "2022-03-24 08:12:57", "link": "http://arxiv.org/abs/2203.12918v1", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI"}
{"title": "mcBERT: Momentum Contrastive Learning with BERT for Zero-Shot Slot\n  Filling", "abstract": "Zero-shot slot filling has received considerable attention to cope with the\nproblem of limited available data for the target domain. One of the important\nfactors in zero-shot learning is to make the model learn generalized and\nreliable representations. For this purpose, we present mcBERT, which stands for\nmomentum contrastive learning with BERT, to develop a robust zero-shot slot\nfilling model. mcBERT uses BERT to initialize the two encoders, the query\nencoder and key encoder, and is trained by applying momentum contrastive\nlearning. Our experimental results on the SNIPS benchmark show that mcBERT\nsubstantially outperforms the previous models, recording a new\nstate-of-the-art. Besides, we also show that each component composing mcBERT\ncontributes to the performance improvement.", "published": "2022-03-24 09:04:52", "link": "http://arxiv.org/abs/2203.12940v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Generating Data to Mitigate Spurious Correlations in Natural Language\n  Inference Datasets", "abstract": "Natural language processing models often exploit spurious correlations\nbetween task-independent features and labels in datasets to perform well only\nwithin the distributions they are trained on, while not generalising to\ndifferent task distributions. We propose to tackle this problem by generating a\ndebiased version of a dataset, which can then be used to train a debiased,\noff-the-shelf model, by simply replacing its training data. Our approach\nconsists of 1) a method for training data generators to generate high-quality,\nlabel-consistent data samples; and 2) a filtering mechanism for removing data\npoints that contribute to spurious correlations, measured in terms of\nz-statistics. We generate debiased versions of the SNLI and MNLI datasets, and\nwe evaluate on a large suite of debiased, out-of-distribution, and adversarial\ntest sets. Results show that models trained on our debiased datasets generalise\nbetter than those trained on the original datasets in all settings. On the\nmajority of the datasets, our method outperforms or performs comparably to\nprevious state-of-the-art debiasing strategies, and when combined with an\northogonal technique, product-of-experts, it improves further and outperforms\nprevious best results of SNLI-hard and MNLI-hard.", "published": "2022-03-24 09:08:05", "link": "http://arxiv.org/abs/2203.12942v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Introducing Neural Bag of Whole-Words with ColBERTer: Contextualized\n  Late Interactions using Enhanced Reduction", "abstract": "Recent progress in neural information retrieval has demonstrated large gains\nin effectiveness, while often sacrificing the efficiency and interpretability\nof the neural model compared to classical approaches. This paper proposes\nColBERTer, a neural retrieval model using contextualized late interaction\n(ColBERT) with enhanced reduction. Along the effectiveness Pareto frontier,\nColBERTer's reductions dramatically lower ColBERT's storage requirements while\nsimultaneously improving the interpretability of its token-matching scores. To\nthis end, ColBERTer fuses single-vector retrieval, multi-vector refinement, and\noptional lexical matching components into one model. For its multi-vector\ncomponent, ColBERTer reduces the number of stored vectors per document by\nlearning unique whole-word representations for the terms in each document and\nlearning to identify and remove word representations that are not essential to\neffective scoring. We employ an explicit multi-task, multi-stage training to\nfacilitate using very small vector dimensions. Results on the MS MARCO and\nTREC-DL collection show that ColBERTer can reduce the storage footprint by up\nto 2.5x, while maintaining effectiveness. With just one dimension per token in\nits smallest setting, ColBERTer achieves index storage parity with the\nplaintext size, with very strong effectiveness results. Finally, we demonstrate\nColBERTer's robustness on seven high-quality out-of-domain collections,\nyielding statistically significant gains over traditional retrieval baselines.", "published": "2022-03-24 14:28:07", "link": "http://arxiv.org/abs/2203.13088v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Multi-armed bandits for resource efficient, online optimization of\n  language model pre-training: the use case of dynamic masking", "abstract": "We design and evaluate a Bayesian optimization framework for resource\nefficient pre-training of Transformer-based language models (TLMs). TLM\npre-training requires high computational resources and introduces many\nunresolved design choices, such as selecting its pre-training hyperparameters.\nWe propose a multi-armed bandit framework for the sequential selection of TLM\npre-training hyperparameters, aimed at optimizing language model performance,\nin a resource efficient manner. We design a Thompson sampling algorithm, with a\nsurrogate Gaussian process reward model of the Masked Language Model (MLM)\npre-training objective, for its sequential minimization. Instead of MLM\npre-training with fixed masking probabilities, the proposed Gaussian\nprocess-based Thompson sampling (GP-TS) accelerates pre-training by\nsequentially selecting masking hyperparameters that improve performance. We\nempirically demonstrate how GP-TS pre-trains language models efficiently, i.e.,\nit achieves lower MLM loss in fewer epochs, across a variety of settings. In\naddition, GP-TS pre-trained TLMs attain competitive downstream performance,\nwhile avoiding expensive hyperparameter grid search. GP-TS provides an\ninteractive framework for efficient and optimized TLM pre-training that, by\ncircumventing costly hyperparameter selection, enables substantial\ncomputational savings.", "published": "2022-03-24 16:12:21", "link": "http://arxiv.org/abs/2203.13151v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Leveraging unsupervised and weakly-supervised data to improve direct\n  speech-to-speech translation", "abstract": "End-to-end speech-to-speech translation (S2ST) without relying on\nintermediate text representations is a rapidly emerging frontier of research.\nRecent works have demonstrated that the performance of such direct S2ST systems\nis approaching that of conventional cascade S2ST when trained on comparable\ndatasets. However, in practice, the performance of direct S2ST is bounded by\nthe availability of paired S2ST training data. In this work, we explore\nmultiple approaches for leveraging much more widely available unsupervised and\nweakly-supervised speech and text data to improve the performance of direct\nS2ST based on Translatotron 2. With our most effective approaches, the average\ntranslation quality of direct S2ST on 21 language pairs on the CVSS-C corpus is\nimproved by +13.6 BLEU (or +113% relatively), as compared to the previous\nstate-of-the-art trained without additional data. The improvements on\nlow-resource language are even more significant (+398% relatively on average).\nOur comparative studies suggest future research directions for S2ST and speech\nrepresentation learning.", "published": "2022-03-24 21:06:15", "link": "http://arxiv.org/abs/2203.13339v2", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Linking Emergent and Natural Languages via Corpus Transfer", "abstract": "The study of language emergence aims to understand how human languages are\nshaped by perceptual grounding and communicative intent. Computational\napproaches to emergent communication (EC) predominantly consider referential\ngames in limited domains and analyze the learned protocol within the game\nframework. As a result, it remains unclear how the emergent languages from\nthese settings connect to natural languages or provide benefits in real-world\nlanguage processing tasks, where statistical models trained on large text\ncorpora dominate. In this work, we propose a novel way to establish such a link\nby corpus transfer, i.e. pretraining on a corpus of emergent language for\ndownstream natural language tasks, which is in contrast to prior work that\ndirectly transfers speaker and listener parameters. Our approach showcases\nnon-trivial transfer benefits for two different tasks -- language modeling and\nimage captioning. For example, in a low-resource setup (modeling 2 million\nnatural language tokens), pre-training on an emergent language corpus with just\n2 million tokens reduces model perplexity by $24.6\\%$ on average across ten\nnatural languages. We also introduce a novel metric to predict the\ntransferability of an emergent language by translating emergent messages to\nnatural language captions grounded on the same images. We find that our\ntranslation-based metric highly correlates with the downstream performance on\nmodeling natural languages (for instance $\\rho=0.83$ on Hebrew), while\ntopographic similarity, a popular metric in previous work, shows surprisingly\nlow correlation ($\\rho=0.003$), hinting that simple properties like attribute\ndisentanglement from synthetic domains might not capture the full complexities\nof natural language. Our findings also indicate potential benefits of moving\nlanguage emergence forward with natural language resources and models.", "published": "2022-03-24 21:24:54", "link": "http://arxiv.org/abs/2203.13344v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Recommendation as Language Processing (RLP): A Unified Pretrain,\n  Personalized Prompt & Predict Paradigm (P5)", "abstract": "For a long time, different recommendation tasks typically require designing\ntask-specific architectures and training objectives. As a result, it is hard to\ntransfer the learned knowledge and representations from one task to another,\nthus restricting the generalization ability of existing recommendation\napproaches, e.g., a sequential recommendation model can hardly be applied or\ntransferred to a review generation method. To deal with such issues,\nconsidering that language can describe almost anything and language grounding\nis a powerful medium to represent various problems or tasks, we present a\nflexible and unified text-to-text paradigm called \"Pretrain, Personalized\nPrompt, and Predict Paradigm\" (P5) for recommendation, which unifies various\nrecommendation tasks in a shared framework. In P5, all data such as user-item\ninteractions, user descriptions, item metadata, and user reviews are converted\nto a common format -- natural language sequences. The rich information from\nnatural language assists P5 to capture deeper semantics for personalization and\nrecommendation. Specifically, P5 learns different tasks with the same language\nmodeling objective during pretraining. Thus, it serves as the foundation model\nfor various downstream recommendation tasks, allows easy integration with other\nmodalities, and enables instruction-based recommendation based on prompts. P5\nadvances recommender systems from shallow model to deep model to big model, and\nwill revolutionize the technical form of recommender systems towards universal\nrecommendation engine. With adaptive personalized prompt for different users,\nP5 is able to make predictions in a zero-shot or few-shot manner and largely\nreduces the necessity for extensive fine-tuning. On several recommendation\nbenchmarks, we conduct experiments to show the effectiveness of P5. We release\nthe source code at https://github.com/jeykigung/P5.", "published": "2022-03-24 22:13:23", "link": "http://arxiv.org/abs/2203.13366v7", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors", "abstract": "Recent text-to-image generation methods provide a simple yet exciting\nconversion capability between text and image domains. While these methods have\nincrementally improved the generated image fidelity and text relevancy, several\npivotal gaps remain unanswered, limiting applicability and quality. We propose\na novel text-to-image method that addresses these gaps by (i) enabling a simple\ncontrol mechanism complementary to text in the form of a scene, (ii)\nintroducing elements that substantially improve the tokenization process by\nemploying domain-specific knowledge over key image regions (faces and salient\nobjects), and (iii) adapting classifier-free guidance for the transformer use\ncase. Our model achieves state-of-the-art FID and human evaluation results,\nunlocking the ability to generate high fidelity images in a resolution of\n512x512 pixels, significantly improving visual quality. Through scene\ncontrollability, we introduce several new capabilities: (i) Scene editing, (ii)\ntext editing with anchor scenes, (iii) overcoming out-of-distribution text\nprompts, and (iv) story illustration generation, as demonstrated in the story\nwe wrote.", "published": "2022-03-24 15:44:50", "link": "http://arxiv.org/abs/2203.13131v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.GR", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Characterizing Therapist's Speaking Style in Relation to Empathy in\n  Psychotherapy", "abstract": "In conversation-based psychotherapy, therapists use verbal techniques to help\nclients express thoughts and feelings, and change behavior. In particular, how\nwell therapists convey empathy is an essential quality index of psychotherapy\nsessions and is associated with psychotherapy outcome. In this paper, we\nanalyze the prosody of therapist speech and attempt to associate the\ntherapist's speaking style with subjectively perceived empathy. An automatic\nspeech and text processing system is developed to segment long recordings of\npsychotherapy sessions into pause-delimited utterances with text\ntranscriptions. Data-driven clustering is applied to the utterances from\ndifferent therapists in multiple sessions. For each cluster, a typological\nrepresentation of utterance genre is derived based on quantized prosodic\nfeature parameters. Prominent speaking styles of the therapist can be observed\nand interpreted from salient utterance genres that are correlated with empathy.\nUsing the salient utterance genres, an accuracy of $71\\%$ is achieved in\nclassifying psychotherapy sessions into ``high\" and ``low\" empathy level.\nAnalysis of results suggests that empathy level tends to be (1) low if\ntherapists speak long utterances slowly or speak short utterances quickly; and\n(2) high if therapists talk to clients with a steady tone and volume.", "published": "2022-03-24 15:39:49", "link": "http://arxiv.org/abs/2203.13127v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "A new subband non linear prediction coding algorithm for narrowband\n  speech signal: The nADPCMB MLT coding scheme", "abstract": "This paper focuses on a newly developed transparent nADPCMB MLT speech coding\nalgorithm. Our coder first decomposes the narrowband speech signal in subbands,\na non linear ADPCM scheme is then performed in each subband. The signal subband\ndecomposition is piloted by the equivalent Modulated Lapped Transform (MLT)\nfilter bank. The novelty of this algorithm is the non linear approach, based on\nneural networks, to subband prediction coding. We have evaluated the\nperformance of the nADPCMB MLT coding algorithm with a session of formal\nlistening based on the five grade impairment scale standardized within ITU - T\nRecommendation P.800.", "published": "2022-03-24 07:26:43", "link": "http://arxiv.org/abs/2203.12894v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Wide band sub-band speech coding using nonlinear prediction", "abstract": "We compare a wide band sub-band speech coder using ADPCM schemes with linear\nprediction against the same scheme with nonlinear prediction based on\nmulti-layer perceptrons. Exhaustive results are presented in each band, and the\nfull signal. Our proposed scheme with non-linear neural net prediction\noutperforms the linear scheme up to 2 dB in SEGSNR. In addition, we propose a\nsimple method based on a non-linearity in order to obtain a synthetic wide band\nsignal from a narrow band signal.", "published": "2022-03-24 07:28:15", "link": "http://arxiv.org/abs/2203.12896v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SelfRemaster: Self-Supervised Speech Restoration with\n  Analysis-by-Synthesis Approach Using Channel Modeling", "abstract": "We present a self-supervised speech restoration method without paired speech\ncorpora. Because the previous general speech restoration method uses artificial\npaired data created by applying various distortions to high-quality speech\ncorpora, it cannot sufficiently represent acoustic distortions of real data,\nlimiting the applicability. Our model consists of analysis, synthesis, and\nchannel modules that simulate the recording process of degraded speech and is\ntrained with real degraded speech data in a self-supervised manner. The\nanalysis module extracts distortionless speech features and distortion features\nfrom degraded speech, while the synthesis module synthesizes the restored\nspeech waveform, and the channel module adds distortions to the speech\nwaveform. Our model also enables audio effect transfer, in which only acoustic\ndistortions are extracted from degraded speech and added to arbitrary\nhigh-quality audio. Experimental evaluations with both simulated and real data\nshow that our method achieves significantly higher-quality speech restoration\nthan the previous supervised method, suggesting its applicability to real\ndegraded speech materials.", "published": "2022-03-24 08:51:27", "link": "http://arxiv.org/abs/2203.12937v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Score difficulty analysis for piano performance education based on\n  fingering", "abstract": "In this paper, we introduce score difficulty classification as a sub-task of\nmusic information retrieval (MIR), which may be used in music education\ntechnologies, for personalised curriculum generation, and score retrieval. We\nintroduce a novel dataset for our task, Mikrokosmos-difficulty, containing 147\npiano pieces in symbolic representation and the corresponding difficulty labels\nderived by its composer B\\'ela Bart\\'ok and the publishers. As part of our\nmethodology, we propose piano technique feature representations based on\ndifferent piano fingering algorithms. We use these features as input for two\nclassifiers: a Gated Recurrent Unit neural network (GRU) with attention\nmechanism and gradient-boosted trees trained on score segments. We show that\nfor our dataset fingering based features perform better than a simple baseline\nconsidering solely the notes in the score. Furthermore, the GRU with attention\nmechanism classifier surpasses the gradient-boosted trees. Our proposed models\nare interpretable and are capable of generating difficulty feedback both\nlocally, on short term segments, and globally, for whole pieces. Code,\ndatasets, models, and an online demo are made available for reproducibility", "published": "2022-03-24 12:00:04", "link": "http://arxiv.org/abs/2203.13010v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Bailando: 3D Dance Generation by Actor-Critic GPT with Choreographic\n  Memory", "abstract": "Driving 3D characters to dance following a piece of music is highly\nchallenging due to the spatial constraints applied to poses by choreography\nnorms. In addition, the generated dance sequence also needs to maintain\ntemporal coherency with different music genres. To tackle these challenges, we\npropose a novel music-to-dance framework, Bailando, with two powerful\ncomponents: 1) a choreographic memory that learns to summarize meaningful\ndancing units from 3D pose sequence to a quantized codebook, 2) an actor-critic\nGenerative Pre-trained Transformer (GPT) that composes these units to a fluent\ndance coherent to the music. With the learned choreographic memory, dance\ngeneration is realized on the quantized units that meet high choreography\nstandards, such that the generated dancing sequences are confined within the\nspatial constraints. To achieve synchronized alignment between diverse motion\ntempos and music beats, we introduce an actor-critic-based reinforcement\nlearning scheme to the GPT with a newly-designed beat-align reward function.\nExtensive experiments on the standard benchmark demonstrate that our proposed\nframework achieves state-of-the-art performance both qualitatively and\nquantitatively. Notably, the learned choreographic memory is shown to discover\nhuman-interpretable dancing-style poses in an unsupervised manner.", "published": "2022-03-24 13:06:43", "link": "http://arxiv.org/abs/2203.13055v2", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "HiFi++: a Unified Framework for Bandwidth Extension and Speech\n  Enhancement", "abstract": "Generative adversarial networks have recently demonstrated outstanding\nperformance in neural vocoding outperforming best autoregressive and flow-based\nmodels. In this paper, we show that this success can be extended to other tasks\nof conditional audio generation. In particular, building upon HiFi vocoders, we\npropose a novel HiFi++ general framework for bandwidth extension and speech\nenhancement. We show that with the improved generator architecture, HiFi++\nperforms better or comparably with the state-of-the-art in these tasks while\nspending significantly less computational resources. The effectiveness of our\napproach is validated through a series of extensive experiments.", "published": "2022-03-24 14:25:51", "link": "http://arxiv.org/abs/2203.13086v4", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Complex Frequency Domain Linear Prediction: A Tool to Compute Modulation\n  Spectrum of Speech", "abstract": "Conventional Frequency Domain Linear Prediction (FDLP) technique models the\nsquared Hilbert envelope of speech with varied degrees of approximation which\ncan be sampled at the required frame rate and used as features for Automatic\nSpeech Recognition (ASR). Although previously the complex cepstrum of the\nconventional FDLP model has been used as compact frame-wise speech features, it\nhas lacked interpretability in the context of the Hilbert envelope. In this\npaper, we propose a modification of the conventional FDLP model that allows\neasy interpretability of the complex cepstrum as temporal modulations in an\nall-pole model approximation of the power of the speech signal. Additionally,\nour \"complex\" FDLP yields significant speed-ups in comparison to conventional\nFDLP for the same degree of approximation.", "published": "2022-03-24 17:14:54", "link": "http://arxiv.org/abs/2203.13216v2", "categories": ["cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "Computing Optimal Location of Microphone for Improved Speech Recognition", "abstract": "It was shown in our earlier work that the measurement error in the microphone\nposition affected the room impulse response (RIR) which in turn affected the\nsingle-channel close microphone and multi-channel distant microphone speech\nrecognition. In this paper, as an extension, we systematically study to\nidentify the optimal location of the microphone, given an approximate and hence\nerroneous location of the microphone in 3D space. The primary idea is to use\nMonte-Carlo technique to generate a large number of random microphone positions\naround the erroneous microphone position and select the microphone position\nthat results in the best performance of a general purpose automatic speech\nrecognition (gp-asr). We experiment with clean and noisy speech and show that\nthe optimal location of the microphone is unique and is affected by noise.", "published": "2022-03-24 14:27:15", "link": "http://arxiv.org/abs/2203.13259v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Continuous-Time Audiovisual Fusion with Recurrence vs. Attention for\n  In-The-Wild Affect Recognition", "abstract": "In this paper, we present our submission to 3rd Affective Behavior Analysis\nin-the-wild (ABAW) challenge. Learningcomplex interactions among multimodal\nsequences is critical to recognise dimensional affect from in-the-wild\naudiovisual data. Recurrence and attention are the two widely used sequence\nmodelling mechanisms in the literature. To clearly understand the performance\ndifferences between recurrent and attention models in audiovisual affect\nrecognition, we present a comprehensive evaluation of fusion models based on\nLSTM-RNNs, self-attention and cross-modal attention, trained for valence and\narousal estimation. Particularly, we study the impact of some key design\nchoices: the modelling complexity of CNN backbones that provide features to the\nthe temporal models, with and without end-to-end learning. We trained the\naudiovisual affect recognition models on in-the-wild ABAW corpus by\nsystematically tuning the hyper-parameters involved in the network architecture\ndesign and training optimisation. Our extensive evaluation of the audiovisual\nfusion models shows that LSTM-RNNs can outperform the attention models when\ncoupled with low-complex CNN backbones and trained in an end-to-end fashion,\nimplying that attention models may not necessarily be the optimal choice for\ncontinuous-time multimodal emotion recognition.", "published": "2022-03-24 18:22:56", "link": "http://arxiv.org/abs/2203.13285v2", "categories": ["cs.SD", "cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
