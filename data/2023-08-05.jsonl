{"title": "EduChat: A Large-Scale Language Model-based Chatbot System for\n  Intelligent Education", "abstract": "EduChat (https://www.educhat.top/) is a large-scale language model\n(LLM)-based chatbot system in the education domain. Its goal is to support\npersonalized, fair, and compassionate intelligent education, serving teachers,\nstudents, and parents. Guided by theories from psychology and education, it\nfurther strengthens educational functions such as open question answering,\nessay assessment, Socratic teaching, and emotional support based on the\nexisting basic LLMs. Particularly, we learn domain-specific knowledge by\npre-training on the educational corpus and stimulate various skills with tool\nuse by fine-tuning on designed system prompts and instructions. Currently,\nEduChat is available online as an open-source project, with its code, data, and\nmodel parameters available on platforms (e.g., GitHub\nhttps://github.com/icalk-nlp/EduChat, Hugging Face\nhttps://huggingface.co/ecnu-icalk ). We also prepare a demonstration of its\ncapabilities online (https://vimeo.com/851004454). This initiative aims to\npromote research and applications of LLMs for intelligent education.", "published": "2023-08-05 02:55:35", "link": "http://arxiv.org/abs/2308.02773v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LaDA: Latent Dialogue Action For Zero-shot Cross-lingual Neural Network\n  Language Modeling", "abstract": "Cross-lingual adaptation has proven effective in spoken language\nunderstanding (SLU) systems with limited resources. Existing methods are\nfrequently unsatisfactory for intent detection and slot filling, particularly\nfor distant languages that differ significantly from the source language in\nscripts, morphology, and syntax. Latent Dialogue Action (LaDA) layer is\nproposed to optimize decoding strategy in order to address the aforementioned\nissues. The model consists of an additional layer of latent dialogue action. It\nenables our model to improve a system's capability of handling conversations\nwith complex multilingual intent and slot values of distant languages. To the\nbest of our knowledge, this is the first exhaustive investigation of the use of\nlatent variables for optimizing cross-lingual SLU policy during the decode\nstage. LaDA obtains state-of-the-art results on public datasets for both\nzero-shot and few-shot adaptation.", "published": "2023-08-05 15:51:45", "link": "http://arxiv.org/abs/2308.02903v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Source (Pre-)Training for Cross-Domain Measurement, Unit and\n  Context Extraction", "abstract": "We present a cross-domain approach for automated measurement and context\nextraction based on pre-trained language models. We construct a multi-source,\nmulti-domain corpus and train an end-to-end extraction pipeline. We then apply\nmulti-source task-adaptive pre-training and fine-tuning to benchmark the\ncross-domain generalization capability of our model. Further, we conceptualize\nand apply a task-specific error analysis and derive insights for future work.\nOur results suggest that multi-source training leads to the best overall\nresults, while single-source training yields the best results for the\nrespective individual domain. While our setup is successful at extracting\nquantity values and units, more research is needed to improve the extraction of\ncontextual entities. We make the cross-domain corpus used in this work\navailable online.", "published": "2023-08-05 20:33:39", "link": "http://arxiv.org/abs/2308.02951v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ApproBiVT: Lead ASR Models to Generalize Better Using Approximated\n  Bias-Variance Tradeoff Guided Early Stopping and Checkpoint Averaging", "abstract": "The conventional recipe for Automatic Speech Recognition (ASR) models is to\n1) train multiple checkpoints on a training set while relying on a validation\nset to prevent overfitting using early stopping and 2) average several last\ncheckpoints or that of the lowest validation losses to obtain the final model.\nIn this paper, we rethink and update the early stopping and checkpoint\naveraging from the perspective of the bias-variance tradeoff. Theoretically,\nthe bias and variance represent the fitness and variability of a model and the\ntradeoff of them determines the overall generalization error. But, it's\nimpractical to evaluate them precisely. As an alternative, we take the training\nloss and validation loss as proxies of bias and variance and guide the early\nstopping and checkpoint averaging using their tradeoff, namely an Approximated\nBias-Variance Tradeoff (ApproBiVT). When evaluating with advanced ASR models,\nour recipe provides 2.5%-3.7% and 3.1%-4.6% CER reduction on the AISHELL-1 and\nAISHELL-2, respectively.", "published": "2023-08-05 12:50:54", "link": "http://arxiv.org/abs/2308.02870v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Towards Consistency Filtering-Free Unsupervised Learning for Dense\n  Retrieval", "abstract": "Domain transfer is a prevalent challenge in modern neural Information\nRetrieval (IR). To overcome this problem, previous research has utilized\ndomain-specific manual annotations and synthetic data produced by consistency\nfiltering to finetune a general ranker and produce a domain-specific ranker.\nHowever, training such consistency filters are computationally expensive, which\nsignificantly reduces the model efficiency. In addition, consistency filtering\noften struggles to identify retrieval intentions and recognize query and corpus\ndistributions in a target domain. In this study, we evaluate a more efficient\nsolution: replacing the consistency filter with either direct pseudo-labeling,\npseudo-relevance feedback, or unsupervised keyword generation methods for\nachieving consistent filtering-free unsupervised dense retrieval. Our extensive\nexperimental evaluations demonstrate that, on average, TextRank-based pseudo\nrelevance feedback outperforms other methods. Furthermore, we analyzed the\ntraining and inference efficiency of the proposed paradigm. The results\nindicate that filtering-free unsupervised learning can continuously improve\ntraining and inference efficiency while maintaining retrieval performance. In\nsome cases, it can even improve performance based on particular datasets.", "published": "2023-08-05 17:33:51", "link": "http://arxiv.org/abs/2308.02926v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "cs.NI"], "primary_category": "cs.IR"}
{"title": "Science and engineering for what? A large-scale analysis of students'\n  projects in science fairs", "abstract": "Science and Engineering fairs offer K-12 students opportunities to engage\nwith authentic STEM practices. Particularly, students are given the chance to\nexperience authentic and open inquiry processes, by defining which themes,\nquestions and approaches will guide their scientific endeavors. In this study,\nwe analyzed data from over 5,000 projects presented at a nationwide science\nfair in Brazil over the past 20 years using topic modeling to identify the main\ntopics that have driven students' inquiry and design. Our analysis identified a\nbroad range of topics being explored, with significant variations over time,\nregion, and school setting. We argue those results and proposed methodology can\nnot only support further research in the context of science fairs, but also\ninform instruction and design of contexts-specific resources to support\nstudents in open inquiry experiences in different settings.", "published": "2023-08-05 22:19:03", "link": "http://arxiv.org/abs/2308.02962v2", "categories": ["cs.AI", "cs.CL", "physics.ed-ph", "stat.AP"], "primary_category": "cs.AI"}
{"title": "Textual Data Mining for Financial Fraud Detection: A Deep Learning\n  Approach", "abstract": "In this report, I present a deep learning approach to conduct a natural\nlanguage processing (hereafter NLP) binary classification task for analyzing\nfinancial-fraud texts. First, I searched for regulatory announcements and\nenforcement bulletins from HKEX news to define fraudulent companies and to\nextract their MD&A reports before I organized the sentences from the reports\nwith labels and reporting time. My methodology involved different kinds of\nneural network models, including Multilayer Perceptrons with Embedding layers,\nvanilla Recurrent Neural Network (RNN), Long-Short Term Memory (LSTM), and\nGated Recurrent Unit (GRU) for the text classification task. By utilizing this\ndiverse set of models, I aim to perform a comprehensive comparison of their\naccuracy in detecting financial fraud. My results bring significant\nimplications for financial fraud detection as this work contributes to the\ngrowing body of research at the intersection of deep learning, NLP, and\nfinance, providing valuable insights for industry practitioners, regulators,\nand researchers in the pursuit of more robust and effective fraud detection\nmethodologies.", "published": "2023-08-05 15:33:10", "link": "http://arxiv.org/abs/2308.03800v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Auditing and Robustifying COVID-19 Misinformation Datasets via\n  Anticontent Sampling", "abstract": "This paper makes two key contributions. First, it argues that highly\nspecialized rare content classifiers trained on small data typically have\nlimited exposure to the richness and topical diversity of the negative class\n(dubbed anticontent) as observed in the wild. As a result, these classifiers'\nstrong performance observed on the test set may not translate into real-world\nsettings. In the context of COVID-19 misinformation detection, we conduct an\nin-the-wild audit of multiple datasets and demonstrate that models trained with\nseveral prominently cited recent datasets are vulnerable to anticontent when\nevaluated in the wild. Second, we present a novel active learning pipeline that\nrequires zero manual annotation and iteratively augments the training data with\nchallenging anticontent, robustifying these classifiers.", "published": "2023-08-05 22:38:05", "link": "http://arxiv.org/abs/2310.07078v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Self-Distillation Prototypes Network: Learning Robust Speaker\n  Representations without Supervision", "abstract": "Training speaker-discriminative and robust speaker verification systems\nwithout explicit speaker labels remains a persistent challenge. In this paper,\nwe propose a novel self-supervised speaker verification approach,\nSelf-Distillation Prototypes Network (SDPN), which effectively facilitates\nself-supervised speaker representation learning. SDPN assigns the\nrepresentation of the augmented views of an utterance to the same prototypes as\nthe representation of the original view, thereby enabling effective knowledge\ntransfer between the augmented and original views. Due to lack of negative\npairs in the SDPN training process, the network tends to align positive pairs\nquite closely in the embedding space, a phenomenon known as model collapse. To\nmitigate this problem, we introduce a diversity regularization term to\nembeddings in SDPN. Comprehensive experiments on the VoxCeleb datasets\ndemonstrate the superiority of SDPN among self-supervised speaker verification\napproaches. SDPN sets a new state-of-the-art on the VoxCeleb1 speaker\nverification evaluation benchmark, achieving Equal Error Rate 1.80%, 1.99%, and\n3.62% for trial VoxCeleb1-O, VoxCeleb1-E and VoxCeleb1-H, without using any\nspeaker labels in training. Ablation studies show that both proposed learnable\nprototypes in self-distillation network and diversity regularization contribute\nto the verification performance.", "published": "2023-08-05 02:59:40", "link": "http://arxiv.org/abs/2308.02774v6", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Systematic Exploration of Joint-training for Singing Voice Synthesis", "abstract": "There has been a growing interest in using end-to-end acoustic models for\nsinging voice synthesis (SVS). Typically, these models require an additional\nvocoder to transform the generated acoustic features into the final waveform.\nHowever, since the acoustic model and the vocoder are not jointly optimized, a\ngap can exist between the two models, leading to suboptimal performance.\nAlthough a similar problem has been addressed in the TTS systems by\njoint-training or by replacing acoustic features with a latent representation,\nadopting corresponding approaches to SVS is not an easy task. How to improve\nthe joint-training of SVS systems has not been well explored. In this paper, we\nconduct a systematic investigation of how to better perform a joint-training of\nan acoustic model and a vocoder for SVS. We carry out extensive experiments and\ndemonstrate that our joint-training strategy outperforms baselines, achieving\nmore stable performance across different datasets while also increasing the\ninterpretability of the entire framework.", "published": "2023-08-05 12:42:20", "link": "http://arxiv.org/abs/2308.02867v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Bootstrapping Contrastive Learning Enhanced Music Cold-Start Matching", "abstract": "We study a particular matching task we call Music Cold-Start Matching. In\nshort, given a cold-start song request, we expect to retrieve songs with\nsimilar audiences and then fastly push the cold-start song to the audiences of\nthe retrieved songs to warm up it. However, there are hardly any studies done\non this task. Therefore, in this paper, we will formalize the problem of Music\nCold-Start Matching detailedly and give a scheme. During the offline training,\nwe attempt to learn high-quality song representations based on song content\nfeatures. But, we find supervision signals typically follow power-law\ndistribution causing skewed representation learning. To address this issue, we\npropose a novel contrastive learning paradigm named Bootstrapping Contrastive\nLearning (BCL) to enhance the quality of learned representations by exerting\ncontrastive regularization. During the online serving, to locate the target\naudiences more accurately, we propose Clustering-based Audience Targeting (CAT)\nthat clusters audience representations to acquire a few cluster centroids and\nthen locate the target audiences by measuring the relevance between the\naudience representations and the cluster centroids. Extensive experiments on\nthe offline dataset and online system demonstrate the effectiveness and\nefficiency of our method. Currently, we have deployed it on NetEase Cloud\nMusic, affecting millions of users. Code will be released in the future.", "published": "2023-08-05 11:13:59", "link": "http://arxiv.org/abs/2308.02844v1", "categories": ["cs.IR", "cs.SD", "eess.AS", "F.2.2; I.2.8"], "primary_category": "cs.IR"}
{"title": "Elucidate Gender Fairness in Singing Voice Transcription", "abstract": "It is widely known that males and females typically possess different sound\ncharacteristics when singing, such as timbre and pitch, but it has never been\nexplored whether these gender-based characteristics lead to a performance\ndisparity in singing voice transcription (SVT), whose target includes pitch.\nSuch a disparity could cause fairness issues and severely affect the user\nexperience of downstream SVT applications. Motivated by this, we first\ndemonstrate the female superiority of SVT systems, which is observed across\ndifferent models and datasets. We find that different pitch distributions,\nrather than gender data imbalance, contribute to this disparity. To address\nthis issue, we propose using an attribute predictor to predict gender labels\nand adversarially training the SVT system to enforce the gender-invariance of\nacoustic representations. Leveraging the prior knowledge that pitch\ndistributions may contribute to the gender bias, we propose conditionally\naligning acoustic representations between demographic groups by feeding note\nevents to the attribute predictor. Empirical experiments on multiple benchmark\nSVT datasets show that our method significantly reduces gender bias (up to more\nthan 50%) with negligible degradation of overall SVT performance, on both\nin-domain and out-of-domain singing data, thus offering a better\nfairness-utility trade-off.", "published": "2023-08-05 15:15:01", "link": "http://arxiv.org/abs/2308.02898v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DiffDance: Cascaded Human Motion Diffusion Model for Dance Generation", "abstract": "When hearing music, it is natural for people to dance to its rhythm.\nAutomatic dance generation, however, is a challenging task due to the physical\nconstraints of human motion and rhythmic alignment with target music.\nConventional autoregressive methods introduce compounding errors during\nsampling and struggle to capture the long-term structure of dance sequences. To\naddress these limitations, we present a novel cascaded motion diffusion model,\nDiffDance, designed for high-resolution, long-form dance generation. This model\ncomprises a music-to-dance diffusion model and a sequence super-resolution\ndiffusion model. To bridge the gap between music and motion for conditional\ngeneration, DiffDance employs a pretrained audio representation learning model\nto extract music embeddings and further align its embedding space to motion via\ncontrastive loss. During training our cascaded diffusion model, we also\nincorporate multiple geometric losses to constrain the model outputs to be\nphysically plausible and add a dynamic loss weight that adaptively changes over\ndiffusion timesteps to facilitate sample diversity. Through comprehensive\nexperiments performed on the benchmark dataset AIST++, we demonstrate that\nDiffDance is capable of generating realistic dance sequences that align\neffectively with the input music. These results are comparable to those\nachieved by state-of-the-art autoregressive methods.", "published": "2023-08-05 16:18:57", "link": "http://arxiv.org/abs/2308.02915v1", "categories": ["cs.GR", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.GR"}
{"title": "Anonymizing Speech: Evaluating and Designing Speaker Anonymization\n  Techniques", "abstract": "The growing use of voice user interfaces has led to a surge in the collection\nand storage of speech data. While data collection allows for the development of\nefficient tools powering most speech services, it also poses serious privacy\nissues for users as centralized storage makes private personal speech data\nvulnerable to cyber threats. With the increasing use of voice-based digital\nassistants like Amazon's Alexa, Google's Home, and Apple's Siri, and with the\nincreasing ease with which personal speech data can be collected, the risk of\nmalicious use of voice-cloning and speaker/gender/pathological/etc. recognition\nhas increased.\n  This thesis proposes solutions for anonymizing speech and evaluating the\ndegree of the anonymization. In this work, anonymization refers to making\npersonal speech data unlinkable to an identity while maintaining the usefulness\n(utility) of the speech signal (e.g., access to linguistic content). We start\nby identifying several challenges that evaluation protocols need to consider to\nevaluate the degree of privacy protection properly. We clarify how\nanonymization systems must be configured for evaluation purposes and highlight\nthat many practical deployment configurations do not permit privacy evaluation.\nFurthermore, we study and examine the most common voice conversion-based\nanonymization system and identify its weak points before suggesting new methods\nto overcome some limitations. We isolate all components of the anonymization\nsystem to evaluate the degree of speaker PPI associated with each of them.\nThen, we propose several transformation methods for each component to reduce as\nmuch as possible speaker PPI while maintaining utility. We promote\nanonymization algorithms based on quantization-based transformation as an\nalternative to the most-used and well-known noise-based approach. Finally, we\nendeavor a new attack method to invert anonymization.", "published": "2023-08-05 16:14:17", "link": "http://arxiv.org/abs/2308.04455v4", "categories": ["cs.CR", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
