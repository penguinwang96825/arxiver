{"title": "Low Resourced Machine Translation via Morpho-syntactic Modeling: The\n  Case of Dialectal Arabic", "abstract": "We present the second ever evaluated Arabic dialect-to-dialect machine\ntranslation effort, and the first to leverage external resources beyond a small\nparallel corpus. The subject has not previously received serious attention due\nto lack of naturally occurring parallel data; yet its importance is evidenced\nby dialectal Arabic's wide usage and breadth of inter-dialect variation,\ncomparable to that of Romance languages. Our results suggest that modeling\nmorphology and syntax significantly improves dialect-to-dialect translation,\nthough optimizing such data-sparse models requires consideration of the\nlinguistic differences between dialects and the nature of available data and\nresources. On a single-reference blind test set where untranslated input scores\n6.5 BLEU and a model trained only on parallel data reaches 14.6, pivot\ntechniques and morphosyntactic modeling significantly improve performance to\n17.5.", "published": "2017-12-18 07:04:10", "link": "http://arxiv.org/abs/1712.06273v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Chinese Dataset with Negative Full Forms for General Abbreviation\n  Prediction", "abstract": "Abbreviation is a common phenomenon across languages, especially in Chinese.\nIn most cases, if an expression can be abbreviated, its abbreviation is used\nmore often than its fully expanded forms, since people tend to convey\ninformation in a most concise way. For various language processing tasks,\nabbreviation is an obstacle to improving the performance, as the textual form\nof an abbreviation does not express useful information, unless it's expanded to\nthe full form. Abbreviation prediction means associating the fully expanded\nforms with their abbreviations. However, due to the deficiency in the\nabbreviation corpora, such a task is limited in current studies, especially\nconsidering general abbreviation prediction should also include those full form\nexpressions that do not have valid abbreviations, namely the negative full\nforms (NFFs). Corpora incorporating negative full forms for general\nabbreviation prediction are few in number. In order to promote the research in\nthis area, we build a dataset for general Chinese abbreviation prediction,\nwhich needs a few preprocessing steps, and evaluate several different models on\nthe built dataset. The dataset is available at\nhttps://github.com/lancopku/Chinese-abbreviation-dataset", "published": "2017-12-18 08:24:12", "link": "http://arxiv.org/abs/1712.06289v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Hate Speech in Social Media", "abstract": "In this paper we examine methods to detect hate speech in social media, while\ndistinguishing this from general profanity. We aim to establish lexical\nbaselines for this task by applying supervised classification methods using a\nrecently released dataset annotated for this purpose. As features, our system\nuses character n-grams, word n-grams and word skip-grams. We obtain results of\n78% accuracy in identifying posts across three classes. Results demonstrate\nthat the main challenge lies in discriminating profanity and hate speech from\neach other. A number of directions for future work are discussed.", "published": "2017-12-18 14:39:57", "link": "http://arxiv.org/abs/1712.06427v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "word representation or word embedding in Persian text", "abstract": "Text processing is one of the sub-branches of natural language processing.\nRecently, the use of machine learning and neural networks methods has been\ngiven greater consideration. For this reason, the representation of words has\nbecome very important. This article is about word representation or converting\nwords into vectors in Persian text. In this research GloVe, CBOW and skip-gram\nmethods are updated to produce embedded vectors for Persian words. In order to\ntrain a neural networks, Bijankhan corpus, Hamshahri corpus and UPEC corpus\nhave been compound and used. Finally, we have 342,362 words that obtained\nvectors in all three models for this words. These vectors have many usage for\nPersian natural language processing.", "published": "2017-12-18 21:06:42", "link": "http://arxiv.org/abs/1712.06674v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Synthesizing Novel Pairs of Image and Text", "abstract": "Generating novel pairs of image and text is a problem that combines computer\nvision and natural language processing. In this paper, we present strategies\nfor generating novel image and caption pairs based on existing captioning\ndatasets. The model takes advantage of recent advances in generative\nadversarial networks and sequence-to-sequence modeling. We make generalizations\nto generate paired samples from multiple domains. Furthermore, we study cycles\n-- generating from image to text then back to image and vise versa, as well as\nits connection with autoencoders.", "published": "2017-12-18 21:25:37", "link": "http://arxiv.org/abs/1712.06682v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Multilingual Topic Models", "abstract": "Scientific publications have evolved several features for mitigating\nvocabulary mismatch when indexing, retrieving, and computing similarity between\narticles. These mitigation strategies range from simply focusing on high-value\narticle sections, such as titles and abstracts, to assigning keywords, often\nfrom controlled vocabularies, either manually or through automatic annotation.\nVarious document representation schemes possess different cost-benefit\ntradeoffs. In this paper, we propose to model different representations of the\nsame article as translations of each other, all generated from a common latent\nrepresentation in a multilingual topic model. We start with a methodological\noverview on latent variable models for parallel document representations that\ncould be used across many information science tasks. We then show how solving\nthe inference problem of mapping diverse representations into a shared topic\nspace allows us to evaluate representations based on how topically similar they\nare to the original article. In addition, our proposed approach provides means\nto discover where different concept vocabularies require improvement.", "published": "2017-12-18 22:45:20", "link": "http://arxiv.org/abs/1712.06704v1", "categories": ["stat.ML", "cs.CL", "cs.IR"], "primary_category": "stat.ML"}
{"title": "Language and Noise Transfer in Speech Enhancement Generative Adversarial\n  Network", "abstract": "Speech enhancement deep learning systems usually require large amounts of\ntraining data to operate in broad conditions or real applications. This makes\nthe adaptability of those systems into new, low resource environments an\nimportant topic. In this work, we present the results of adapting a speech\nenhancement generative adversarial network by finetuning the generator with\nsmall amounts of data. We investigate the minimum requirements to obtain a\nstable behavior in terms of several objective metrics in two very different\nlanguages: Catalan and Korean. We also study the variability of test\nperformance to unseen noise as a function of the amount of different types of\nnoise available for training. Results show that adapting a pre-trained English\nmodel with 10 min of data already achieves a comparable performance to having\ntwo orders of magnitude more data. They also demonstrate the relative stability\nin test performance with respect to the number of training noise types.", "published": "2017-12-18 11:16:08", "link": "http://arxiv.org/abs/1712.06340v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Objects that Sound", "abstract": "In this paper our objectives are, first, networks that can embed audio and\nvisual inputs into a common space that is suitable for cross-modal retrieval;\nand second, a network that can localize the object that sounds in an image,\ngiven the audio signal. We achieve both these objectives by training from\nunlabelled video using only audio-visual correspondence (AVC) as the objective\nfunction. This is a form of cross-modal self-supervision from video.\n  To this end, we design new network architectures that can be trained for\ncross-modal retrieval and localizing the sound source in an image, by using the\nAVC task. We make the following contributions: (i) show that audio and visual\nembeddings can be learnt that enable both within-mode (e.g. audio-to-audio) and\nbetween-mode retrieval; (ii) explore various architectures for the AVC task,\nincluding those for the visual stream that ingest a single image, or multiple\nimages, or a single image and multi-frame optical flow; (iii) show that the\nsemantic object that sounds within an image can be localized (using only the\nsound, no motion or flow information); and (iv) give a cautionary tale on how\nto avoid undesirable shortcuts in the data preparation.", "published": "2017-12-18 19:52:53", "link": "http://arxiv.org/abs/1712.06651v2", "categories": ["cs.CV", "cs.LG", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
