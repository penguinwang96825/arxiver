{"title": "Diverse Text Generation via Variational Encoder-Decoder Models with\n  Gaussian Process Priors", "abstract": "Generating high quality texts with high diversity is important for many NLG\napplications, but current methods mostly focus on building deterministic models\nto generate higher quality texts and do not provide many options for promoting\ndiversity. In this work, we present a novel latent structured variable model to\ngenerate high quality texts by enriching contextual representation learning of\nencoder-decoder models. Specifically, we introduce a stochastic function to map\ndeterministic encoder hidden states into random context variables. The proposed\nstochastic function is sampled from a Gaussian process prior to (1) provide\ninfinite number of joint Gaussian distributions of random context variables\n(diversity-promoting) and (2) explicitly model dependency between context\nvariables (accurate-encoding). To address the learning challenge of Gaussian\nprocesses, we propose an efficient variational inference approach to\napproximate the posterior distribution of random context variables. We evaluate\nour method in two typical text generation tasks: paraphrase generation and text\nstyle transfer. Experimental results on benchmark datasets demonstrate that our\nmethod improves the generation quality and diversity compared with other\nbaselines.", "published": "2022-04-04 04:09:15", "link": "http://arxiv.org/abs/2204.01227v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Estimating the Entropy of Linguistic Distributions", "abstract": "Shannon entropy is often a quantity of interest to linguists studying the\ncommunicative capacity of human language. However, entropy must typically be\nestimated from observed data because researchers do not have access to the\nunderlying probability distribution that gives rise to these data. While\nentropy estimation is a well-studied problem in other fields, there is not yet\na comprehensive exploration of the efficacy of entropy estimators for use with\nlinguistic data. In this work, we fill this void, studying the empirical\neffectiveness of different entropy estimators for linguistic distributions. In\na replication of two recent information-theoretic linguistic studies, we find\nevidence that the reported effect size is over-estimated due to over-reliance\non poor entropy estimators. Finally, we end our paper with concrete\nrecommendations for entropy estimation depending on distribution type and data\navailability.", "published": "2022-04-04 13:36:46", "link": "http://arxiv.org/abs/2204.01469v2", "categories": ["cs.CL", "94A17 (Primary) 62B10 (Secondary)", "I.2.7; E.4"], "primary_category": "cs.CL"}
{"title": "LPAttack: A Feasible Annotation Scheme for Capturing Logic Pattern of\n  Attacks in Arguments", "abstract": "In argumentative discourse, persuasion is often achieved by refuting or\nattacking others arguments. Attacking is not always straightforward and often\ncomprise complex rhetorical moves such that arguers might agree with a logic of\nan argument while attacking another logic. Moreover, arguer might neither deny\nnor agree with any logics of an argument, instead ignore them and attack the\nmain stance of the argument by providing new logics and presupposing that the\nnew logics have more value or importance than the logics present in the\nattacked argument. However, no existing studies in the computational\nargumentation capture such complex rhetorical moves in attacks or the\npresuppositions or value judgements in them. In order to address this gap, we\nintroduce LPAttack, a novel annotation scheme that captures the common modes\nand complex rhetorical moves in attacks along with the implicit presuppositions\nand value judgements in them. Our annotation study shows moderate\ninter-annotator agreement, indicating that human annotation for the proposed\nscheme is feasible. We publicly release our annotated corpus and the annotation\nguidelines.", "published": "2022-04-04 14:15:25", "link": "http://arxiv.org/abs/2204.01512v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Supervised Speech Representations Preserve Speech Characteristics\n  while Anonymizing Voices", "abstract": "Collecting speech data is an important step in training speech recognition\nsystems and other speech-based machine learning models. However, the issue of\nprivacy protection is an increasing concern that must be addressed. The current\nstudy investigates the use of voice conversion as a method for anonymizing\nvoices. In particular, we train several voice conversion models using\nself-supervised speech representations including Wav2Vec2.0, Hubert and\nUniSpeech. Converted voices retain a low word error rate within 1% of the\noriginal voice. Equal error rate increases from 1.52% to 46.24% on the\nLibriSpeech test set and from 3.75% to 45.84% on speakers from the VCTK corpus\nwhich signifies degraded performance on speaker verification. Lastly, we\nconduct experiments on dysarthric speech data to show that speech features\nrelevant to articulation, prosody, phonation and phonology can be extracted\nfrom anonymized voices for discriminating between healthy and pathological\nspeech.", "published": "2022-04-04 17:48:01", "link": "http://arxiv.org/abs/2204.01677v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Product Market Demand Analysis Using NLP in Banglish Text with Sentiment\n  Analysis and Named Entity Recognition", "abstract": "Product market demand analysis plays a significant role for originating\nbusiness strategies due to its noticeable impact on the competitive business\nfield. Furthermore, there are roughly 228 million native Bengali speakers, the\nmajority of whom use Banglish text to interact with one another on social\nmedia. Consumers are buying and evaluating items on social media with Banglish\ntext as social media emerges as an online marketplace for entrepreneurs. People\nuse social media to find preferred smartphone brands and models by sharing\ntheir positive and bad experiences with them. For this reason, our goal is to\ngather Banglish text data and use sentiment analysis and named entity\nidentification to assess Bangladeshi market demand for smartphones in order to\ndetermine the most popular smartphones by gender. We scraped product related\ndata from social media with instant data scrapers and crawled data from\nWikipedia and other sites for product information with python web scrapers.\nUsing Python's Pandas and Seaborn libraries, the raw data is filtered using NLP\nmethods. To train our datasets for named entity recognition, we utilized\nSpacey's custom NER model, Amazon Comprehend Custom NER. A tensorflow\nsequential model was deployed with parameter tweaking for sentiment analysis.\nMeanwhile, we used the Google Cloud Translation API to estimate the gender of\nthe reviewers using the BanglaLinga library. In this article, we use natural\nlanguage processing (NLP) approaches and several machine learning models to\nidentify the most in-demand items and services in the Bangladeshi market. Our\nmodel has an accuracy of 87.99% in Spacy Custom Named Entity recognition,\n95.51% in Amazon Comprehend Custom NER, and 87.02% in the Sequential model for\ndemand analysis. After Spacy's study, we were able to manage 80% of mistakes\nrelated to misspelled words using a mix of Levenshtein distance and ratio\nalgorithms.", "published": "2022-04-04 20:21:31", "link": "http://arxiv.org/abs/2204.01827v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Aligned Weight Regularizers for Pruning Pretrained Neural Networks", "abstract": "While various avenues of research have been explored for iterative pruning,\nlittle is known what effect pruning has on zero-shot test performance and its\npotential implications on the choice of pruning criteria. This pruning setup is\nparticularly important for cross-lingual models that implicitly learn alignment\nbetween language representations during pretraining, which if distorted via\npruning, not only leads to poorer performance on language data used for\nretraining but also on zero-shot languages that are evaluated.\n  In this work, we show that there is a clear performance discrepancy in\nmagnitude-based pruning when comparing standard supervised learning to the\nzero-shot setting. From this finding, we propose two weight regularizers that\naim to maximize the alignment between units of pruned and unpruned networks to\nmitigate alignment distortion in pruned cross-lingual models and perform well\nfor both non zero-shot and zero-shot settings.\n  We provide experimental results on cross-lingual tasks for the zero-shot\nsetting using XLM-RoBERTa$_{\\mathrm{Base}}$, where we also find that pruning\nhas varying degrees of representational degradation depending on the language\ncorresponding to the zero-shot test set. This is also the first study that\nfocuses on cross-lingual language model compression.", "published": "2022-04-04 11:06:42", "link": "http://arxiv.org/abs/2204.01385v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Using Pre-Trained Language Models for Producing Counter Narratives\n  Against Hate Speech: a Comparative Study", "abstract": "In this work, we present an extensive study on the use of pre-trained\nlanguage models for the task of automatic Counter Narrative (CN) generation to\nfight online hate speech in English. We first present a comparative study to\ndetermine whether there is a particular Language Model (or class of LMs) and a\nparticular decoding mechanism that are the most appropriate to generate CNs.\nFindings show that autoregressive models combined with stochastic decodings are\nthe most promising. We then investigate how an LM performs in generating a CN\nwith regard to an unseen target of hate. We find out that a key element for\nsuccessful `out of target' experiments is not an overall similarity with the\ntraining data but the presence of a specific subset of training data, i.e. a\ntarget that shares some commonalities with the test target that can be defined\na-priori. We finally introduce the idea of a pipeline based on the addition of\nan automatic post-editing step to refine generated CNs.", "published": "2022-04-04 12:44:47", "link": "http://arxiv.org/abs/2204.01440v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "MetaAID: A Flexible Framework for Developing Metaverse Applications via\n  AI Technology and Human Editing", "abstract": "Achieving the expansion of domestic demand and the economic internal\ncirculation requires balanced and coordinated support from multiple industries\n(domains) such as consumption, education, entertainment, engineering\ninfrastructure, etc., which is indispensable for maintaining economic\ndevelopment. Metaverse applications may help with this task and can make many\nindustries more interesting, more efficient, and provide a better user\nexperience. The first challenge is that metaverse application development\ninevitably requires the support of various artificial intelligence (AI)\ntechnologies such as natural language processing (NLP), knowledge graph (KG),\ncomputer vision (CV), and machine learning (ML), etc. However, existing\nmetaverse application development lacks a lightweight AI technology framework.\nThis paper proposes a flexible metaverse AI technology framework metaAID that\naims to support language and semantic technologies in the development of\ndigital twins and virtual humans. The second challenge is that the development\nprocess of metaverse applications involves both technical development tasks and\nmanual editing work, and often becomes a heavyweight multi-team collaboration\nproject, not to mention the development of metaverse applications in multiple\nindustries. Our framework summarizes common AI technologies and application\ndevelopment templates with common functional modules and interfaces. Based on\nthis framework, we have designed 5 applications for 3 industries around the\nexpansion of domestic demand and economic internal circulation. Experimental\nresults show that our framework can support AI technologies when developing\nmetaverse applications in different industries.", "published": "2022-04-04 16:08:26", "link": "http://arxiv.org/abs/2204.01614v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Applying Automatic Text Summarization for Fake News Detection", "abstract": "The distribution of fake news is not a new but a rapidly growing problem. The\nshift to news consumption via social media has been one of the drivers for the\nspread of misleading and deliberately wrong information, as in addition to it\nof easy use there is rarely any veracity monitoring. Due to the harmful effects\nof such fake news on society, the detection of these has become increasingly\nimportant. We present an approach to the problem that combines the power of\ntransformer-based language models while simultaneously addressing one of their\ninherent problems. Our framework, CMTR-BERT, combines multiple text\nrepresentations, with the goal of circumventing sequential limits and related\nloss of information the underlying transformer architecture typically suffers\nfrom. Additionally, it enables the incorporation of contextual information.\nExtensive experiments on two very different, publicly available datasets\ndemonstrates that our approach is able to set new state-of-the-art performance\nbenchmarks. Apart from the benefit of using automatic text summarization\ntechniques we also find that the incorporation of contextual information\ncontributes to performance gains.", "published": "2022-04-04 21:00:55", "link": "http://arxiv.org/abs/2204.01841v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Deliberation Model for On-Device Spoken Language Understanding", "abstract": "We propose a novel deliberation-based approach to end-to-end (E2E) spoken\nlanguage understanding (SLU), where a streaming automatic speech recognition\n(ASR) model produces the first-pass hypothesis and a second-pass natural\nlanguage understanding (NLU) component generates the semantic parse by\nconditioning on both ASR's text and audio embeddings. By formulating E2E SLU as\na generalized decoder, our system is able to support complex compositional\nsemantic structures. Furthermore, the sharing of parameters between ASR and NLU\nmakes the system especially suitable for resource-constrained (on-device)\nenvironments; our proposed approach consistently outperforms strong pipeline\nNLU baselines by 0.60% to 0.65% on the spoken version of the TOPv2 dataset\n(STOP). We demonstrate that the fusion of text and audio features, coupled with\nthe system's ability to rewrite the first-pass hypothesis, makes our approach\nmore robust to ASR errors. Finally, we show that our approach can significantly\nreduce the degradation when moving from natural speech to synthetic speech\ntraining, but more work is required to make text-to-speech (TTS) a viable\nsolution for scaling up E2E SLU.", "published": "2022-04-04 23:48:01", "link": "http://arxiv.org/abs/2204.01893v3", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "An Analysis of Semantically-Aligned Speech-Text Embeddings", "abstract": "Embeddings play an important role in end-to-end solutions for multi-modal\nlanguage processing problems. Although there has been some effort to understand\nthe properties of single-modality embedding spaces, particularly that of text,\ntheir cross-modal counterparts are less understood. In this work, we study some\nintrinsic properties of a joint speech-text embedding space, constructed by\nminimizing the distance between paired utterance and transcription inputs in a\nteacher-student model setup, that are informative for several prominent use\ncases. We found that incorporating automatic speech recognition through both\npretraining and multitask scenarios aid semantic alignment significantly,\nresulting in more tightly coupled embeddings. To analyse cross-modal embeddings\nwe utilise a quantitative retrieval accuracy metric for semantic alignment,\nzero-shot classification for generalisability, and probing of the encoders to\nobserve the extent of knowledge transfer from one modality to another.", "published": "2022-04-04 04:50:32", "link": "http://arxiv.org/abs/2204.01235v2", "categories": ["cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Study of Gender Impact in Self-supervised Models for Speech-to-Text\n  Systems", "abstract": "Self-supervised models for speech processing emerged recently as popular\nfoundation blocks in speech processing pipelines. These models are pre-trained\non unlabeled audio data and then used in speech processing downstream tasks\nsuch as automatic speech recognition (ASR) or speech translation (ST). Since\nthese models are now used in research and industrial systems alike, it becomes\nnecessary to understand the impact caused by some features such as gender\ndistribution within pre-training data. Using French as our investigation\nlanguage, we train and compare gender-specific wav2vec 2.0 models against\nmodels containing different degrees of gender balance in their pre-training\ndata. The comparison is performed by applying these models to two\nspeech-to-text downstream tasks: ASR and ST. Results show the type of\ndownstream integration matters. We observe lower overall performance using\ngender-specific pre-training before fine-tuning an end-to-end ASR system.\nHowever, when self-supervised models are used as feature extractors, the\noverall ASR and ST results follow more complex patterns in which the balanced\npre-trained model does not necessarily lead to the best results. Lastly, our\ncrude 'fairness' metric, the relative performance difference measured between\nfemale and male test sets, does not display a strong variation from balanced to\ngender-specific pre-trained wav2vec 2.0 models.", "published": "2022-04-04 11:28:19", "link": "http://arxiv.org/abs/2204.01397v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Learning Commonsense-aware Moment-Text Alignment for Fast Video Temporal\n  Grounding", "abstract": "Grounding temporal video segments described in natural language queries\neffectively and efficiently is a crucial capability needed in\nvision-and-language fields. In this paper, we deal with the fast video temporal\ngrounding (FVTG) task, aiming at localizing the target segment with high speed\nand favorable accuracy. Most existing approaches adopt elaborately designed\ncross-modal interaction modules to improve the grounding performance, which\nsuffer from the test-time bottleneck. Although several common space-based\nmethods enjoy the high-speed merit during inference, they can hardly capture\nthe comprehensive and explicit relations between visual and textual modalities.\nIn this paper, to tackle the dilemma of speed-accuracy tradeoff, we propose a\ncommonsense-aware cross-modal alignment (CCA) framework, which incorporates\ncommonsense-guided visual and text representations into a complementary common\nspace for fast video temporal grounding. Specifically, the commonsense concepts\nare explored and exploited by extracting the structural semantic information\nfrom a language corpus. Then, a commonsense-aware interaction module is\ndesigned to obtain bridged visual and text features by utilizing the learned\ncommonsense concepts. Finally, to maintain the original semantic information of\ntextual queries, a cross-modal complementary common space is optimized to\nobtain matching scores for performing FVTG. Extensive results on two\nchallenging benchmarks show that our CCA method performs favorably against\nstate-of-the-arts while running at high speed. Our code is available at\nhttps://github.com/ZiyueWu59/CCA.", "published": "2022-04-04 13:07:05", "link": "http://arxiv.org/abs/2204.01450v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Cross-lingual Self-Supervised Speech Representations for Improved\n  Dysarthric Speech Recognition", "abstract": "State-of-the-art automatic speech recognition (ASR) systems perform well on\nhealthy speech. However, the performance on impaired speech still remains an\nissue. The current study explores the usefulness of using Wav2Vec\nself-supervised speech representations as features for training an ASR system\nfor dysarthric speech. Dysarthric speech recognition is particularly difficult\nas several aspects of speech such as articulation, prosody and phonation can be\nimpaired. Specifically, we train an acoustic model with features extracted from\nWav2Vec, Hubert, and the cross-lingual XLSR model. Results suggest that speech\nrepresentations pretrained on large unlabelled data can improve word error rate\n(WER) performance. In particular, features from the multilingual model led to\nlower WERs than filterbanks (Fbank) or models trained on a single language.\nImprovements were observed in English speakers with cerebral palsy caused\ndysarthria (UASpeech corpus), Spanish speakers with Parkinsonian dysarthria\n(PC-GITA corpus) and Italian speakers with paralysis-based dysarthria (EasyCall\ncorpus). Compared to using Fbank features, XLSR-based features reduced WERs by\n6.8%, 22.0%, and 7.0% for the UASpeech, PC-GITA, and EasyCall corpus,\nrespectively.", "published": "2022-04-04 17:36:01", "link": "http://arxiv.org/abs/2204.01670v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances", "abstract": "Large language models can encode a wealth of semantic knowledge about the\nworld. Such knowledge could be extremely useful to robots aiming to act upon\nhigh-level, temporally extended instructions expressed in natural language.\nHowever, a significant weakness of language models is that they lack real-world\nexperience, which makes it difficult to leverage them for decision making\nwithin a given embodiment. For example, asking a language model to describe how\nto clean a spill might result in a reasonable narrative, but it may not be\napplicable to a particular agent, such as a robot, that needs to perform this\ntask in a particular environment. We propose to provide real-world grounding by\nmeans of pretrained skills, which are used to constrain the model to propose\nnatural language actions that are both feasible and contextually appropriate.\nThe robot can act as the language model's \"hands and eyes,\" while the language\nmodel supplies high-level semantic knowledge about the task. We show how\nlow-level skills can be combined with large language models so that the\nlanguage model provides high-level knowledge about the procedures for\nperforming complex and temporally-extended instructions, while value functions\nassociated with these skills provide the grounding necessary to connect this\nknowledge to a particular physical environment. We evaluate our method on a\nnumber of real-world robotic tasks, where we show the need for real-world\ngrounding and that this approach is capable of completing long-horizon,\nabstract, natural language instructions on a mobile manipulator. The project's\nwebsite and the video can be found at https://say-can.github.io/.", "published": "2022-04-04 17:57:11", "link": "http://arxiv.org/abs/2204.01691v2", "categories": ["cs.RO", "cs.CL", "cs.LG"], "primary_category": "cs.RO"}
{"title": "A pipeline and comparative study of 12 machine learning models for text\n  classification", "abstract": "Text-based communication is highly favoured as a communication method,\nespecially in business environments. As a result, it is often abused by sending\nmalicious messages, e.g., spam emails, to deceive users into relaying personal\ninformation, including online accounts credentials or banking details. For this\nreason, many machine learning methods for text classification have been\nproposed and incorporated into the services of most email providers. However,\noptimising text classification algorithms and finding the right tradeoff on\ntheir aggressiveness is still a major research problem.\n  We present an updated survey of 12 machine learning text classifiers applied\nto a public spam corpus. A new pipeline is proposed to optimise hyperparameter\nselection and improve the models' performance by applying specific methods\n(based on natural language processing) in the preprocessing stage.\n  Our study aims to provide a new methodology to investigate and optimise the\neffect of different feature sizes and hyperparameters in machine learning\nclassifiers that are widely used in text classification problems. The\nclassifiers are tested and evaluated on different metrics including F-score\n(accuracy), precision, recall, and run time. By analysing all these aspects, we\nshow how the proposed pipeline can be used to achieve a good accuracy towards\nspam filtering on the Enron dataset, a widely used public email corpus.\nStatistical tests and explainability techniques are applied to provide a robust\nanalysis of the proposed pipeline and interpret the classification outcomes of\nthe 12 machine learning models, also identifying words that drive the\nclassification results. Our analysis shows that it is possible to identify an\neffective machine learning model to classify the Enron dataset with an F-score\nof 94%.", "published": "2022-04-04 23:51:22", "link": "http://arxiv.org/abs/2204.06518v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Extracting Impact Model Narratives from Social Services' Text", "abstract": "Named entity recognition (NER) is an important task in narration extraction.\nNarration, as a system of stories, provides insights into how events and\ncharacters in the stories develop over time. This paper proposes an\narchitecture for NER on a corpus about social purpose organizations. This is\nthe first NER task specifically targeted at social service entities. We show\nhow this approach can be used for the sequencing of services and impacted\nclients with information extracted from unstructured text. The methodology\noutlines steps for extracting ontological representation of entities such as\nneeds and satisfiers and generating hypotheses to answer queries about impact\nmodels defined by social purpose organizations. We evaluate the model on a\ncorpus of social service descriptions with empirically calculated score.", "published": "2022-04-04 19:01:49", "link": "http://arxiv.org/abs/2204.09557v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "The Vicomtech Spoofing-Aware Biometric System for the SASV Challenge", "abstract": "This paper describes our proposed integration system for the spoofing-aware\nspeaker verification challenge. It consists of a robust spoofing-aware\nverification system that use the speaker verification and antispoofing\nembeddings extracted from specialized neural networks. First, an integration\nnetwork, fed with the test utterance's speaker verification and spoofing\nembeddings, is used to compute a spoof-based score. This score is then linearly\ncombined with the cosine similarity between the speaker verification embeddings\nfrom the enrollment and test utterances, thus obtaining the final scoring\ndecision. Moreover, the integration network is trained using a one-class loss\nfunction to discriminate between target trials and unauthorized accesses. Our\nproposed system is evaluated in the ASVspoof19 database, exhibiting competitive\nperformance compared to other integration approaches. In addition, we test,\nalong with our integration approach, state of the art speaker verification and\nantispoofing systems based on self-supervised learning, yielding\nhigh-performance speech biometric systems.", "published": "2022-04-04 11:32:58", "link": "http://arxiv.org/abs/2204.01399v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "On The Model Size Selection For Speaker Identification", "abstract": "In this paper we evaluate the relevance of the model size for speaker\nidentification. We show that it is possible to improve the identification rates\nif a different model size is used for each speaker. We also present some\ncriteria for selecting the model size, and a new algorithm that outperforms the\nclassical system with a fixed model size.", "published": "2022-04-04 08:04:04", "link": "http://arxiv.org/abs/2204.01294v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Nonlinear Vectorial Prediction with Neural Nets", "abstract": "In this paper we propose a nonlinear vectorial prediction scheme based on a\nMulti Layer Perceptron. This system is applied to speech coding in an ADPCM\nbackward scheme. In addition a procedure to obtain a vectorial quantizer is\ngiven, in order to achieve a fully vectorial speech encoder. We also present\nseveral results with the proposed system", "published": "2022-04-04 08:04:16", "link": "http://arxiv.org/abs/2204.01295v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "tPLCnet: Real-time Deep Packet Loss Concealment in the Time Domain Using\n  a Short Temporal Context", "abstract": "This paper introduces a real-time time-domain packet loss concealment (PLC)\nneural-network (tPLCnet). It efficiently predicts lost frames from a short\ncontext buffer in a sequence-to-one (seq2one) fashion. Because of its seq2one\nstructure, a continuous inference of the model is not required since it can be\ntriggered when packet loss is actually detected. It is trained on 64h of\nopen-source speech data and packet-loss traces of real calls provided by the\nAudio PLC Challenge. The model with the lowest complexity described in this\npaper reaches a robust PLC performance and consistent improvements over the\nzero-filling baseline for all metrics. A configuration with higher complexity\nis submitted to the PLC Challenge and shows a performance increase of 1.07\ncompared to the zero-filling baseline in terms of PLC-MOS on the blind test set\nand reaches a competitive 3rd place in the challenge ranking.", "published": "2022-04-04 08:14:36", "link": "http://arxiv.org/abs/2204.01300v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "An Initialization Scheme for Meeting Separation with Spatial Mixture\n  Models", "abstract": "Spatial mixture model (SMM) supported acoustic beamforming has been\nextensively used for the separation of simultaneously active speakers. However,\nit has hardly been considered for the separation of meeting data, that are\ncharacterized by long recordings and only partially overlapping speech. In this\ncontribution, we show that the fact that often only a single speaker is active\ncan be utilized for a clever initialization of an SMM that employs time-varying\nclass priors. In experiments on LibriCSS we show that the proposed\ninitialization scheme achieves a significantly lower Word Error Rate (WER) on a\ndownstream speech recognition task than a random initialization of the class\nprobabilities by drawing from a Dirichlet distribution. With the only\nrequirement that the number of speakers has to be known, we obtain a WER of 5.9\n%, which is comparable to the best reported WER on this data set. Furthermore,\nthe estimated speaker activity from the mixture model serves as a diarization\nbased on spatial information.", "published": "2022-04-04 09:21:22", "link": "http://arxiv.org/abs/2204.01338v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Target Confusion in End-to-end Speaker Extraction: Analysis and\n  Approaches", "abstract": "Recently, end-to-end speaker extraction has attracted increasing attention\nand shown promising results. However, its performance is often inferior to that\nof a blind source separation (BSS) counterpart with a similar network\narchitecture, due to the auxiliary speaker encoder may sometimes generate\nambiguous speaker embeddings. Such ambiguous guidance information may confuse\nthe separation network and hence lead to wrong extraction results, which\ndeteriorates the overall performance. We refer to this as the target confusion\nproblem. In this paper, we conduct an analysis of such an issue and solve it in\ntwo stages. In the training phase, we propose to integrate metric learning\nmethods to improve the distinguishability of embeddings produced by the speaker\nencoder. While for inference, a novel post-filtering strategy is designed to\nrevise the wrong results. Specifically, we first identify these confusion\nsamples by measuring the similarities between output estimates and enrollment\nutterances, after which the true target sources are recovered by a subtraction\noperation. Experiments show that performance improvement of more than 1dB\nSI-SDRi can be brought, which validates the effectiveness of our methods and\nemphasizes the impact of the target confusion problem.", "published": "2022-04-04 09:57:21", "link": "http://arxiv.org/abs/2204.01355v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "GWA: A Large High-Quality Acoustic Dataset for Audio Processing", "abstract": "We present the Geometric-Wave Acoustic (GWA) dataset, a large-scale audio\ndataset of about 2 million synthetic room impulse responses (IRs) and their\ncorresponding detailed geometric and simulation configurations. Our dataset\nsamples acoustic environments from over 6.8K high-quality diverse and\nprofessionally designed houses represented as semantically labeled 3D meshes.\nWe also present a novel real-world acoustic materials assignment scheme based\non semantic matching that uses a sentence transformer model. We compute\nhigh-quality impulse responses corresponding to accurate low-frequency and\nhigh-frequency wave effects by automatically calibrating geometric acoustic\nray-tracing with a finite-difference time-domain wave solver. We demonstrate\nthe higher accuracy of our IRs by comparing with recorded IRs from complex\nreal-world environments. Moreover, we highlight the benefits of GWA on audio\ndeep learning tasks such as automated speech recognition, speech enhancement,\nand speech separation. This dataset is the first data with accurate wave\nacoustic simulations in complex scenes. Codes and data are available at\nhttps://gamma.umd.edu/pro/sound/gwa.", "published": "2022-04-04 18:34:33", "link": "http://arxiv.org/abs/2204.01787v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multi-modality Associative Bridging through Memory: Speech Sound\n  Recollected from Face Video", "abstract": "In this paper, we introduce a novel audio-visual multi-modal bridging\nframework that can utilize both audio and visual information, even with\nuni-modal inputs. We exploit a memory network that stores source (i.e., visual)\nand target (i.e., audio) modal representations, where source modal\nrepresentation is what we are given, and target modal representations are what\nwe want to obtain from the memory network. We then construct an associative\nbridge between source and target memories that considers the interrelationship\nbetween the two memories. By learning the interrelationship through the\nassociative bridge, the proposed bridging framework is able to obtain the\ntarget modal representations inside the memory network, even with the source\nmodal input only, and it provides rich information for its downstream tasks. We\napply the proposed framework to two tasks: lip reading and speech\nreconstruction from silent video. Through the proposed associative bridge and\nmodality-specific memories, each task knowledge is enriched with the recalled\naudio context, achieving state-of-the-art performance. We also verify that the\nassociative bridge properly relates the source and target memories.", "published": "2022-04-04 06:19:41", "link": "http://arxiv.org/abs/2204.01265v1", "categories": ["cs.CV", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Into-TTS : Intonation Template Based Prosody Control System", "abstract": "Intonations play an important role in delivering the intention of a speaker.\nHowever, current end-to-end TTS systems often fail to model proper intonations.\nTo alleviate this problem, we propose a novel, intuitive method to synthesize\nspeech in different intonations using predefined intonation templates. Prior to\nTTS model training, speech data are grouped into intonation templates in an\nunsupervised manner. Two proposed modules are added to the end-to-end TTS\nframework: an intonation predictor and an intonation encoder. The intonation\npredictor recommends a suitable intonation template to the given text. The\nintonation encoder, attached to the text encoder output, synthesizes speech\nabiding the requested intonation template. Main contributions of our paper are:\n(a) an easy-to-use intonation control system covering a wide range of users;\n(b) better performance in wrapping speech in a requested intonation with\nimproved objective and subjective evaluation; and (c) incorporating a\npre-trained language model for intonation modelling. Audio samples are\navailable at https://srtts.github.io/IntoTTS.", "published": "2022-04-04 06:37:19", "link": "http://arxiv.org/abs/2204.01271v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MOSRA: Joint Mean Opinion Score and Room Acoustics Speech Quality\n  Assessment", "abstract": "The acoustic environment can degrade speech quality during communication\n(e.g., video call, remote presentation, outside voice recording), and its\nimpact is often unknown. Objective metrics for speech quality have proven\nchallenging to develop given the multi-dimensionality of factors that affect\nspeech quality and the difficulty of collecting labeled data. Hypothesizing the\nimpact of acoustics on speech quality, this paper presents MOSRA: a\nnon-intrusive multi-dimensional speech quality metric that can predict room\nacoustics parameters (SNR, STI, T60, DRR, and C50) alongside the overall mean\nopinion score (MOS) for speech quality. By explicitly optimizing the model to\nlearn these room acoustics parameters, we can extract more informative features\nand improve the generalization for the MOS task when the training data is\nlimited. Furthermore, we also show that this joint training method enhances the\nblind estimation of room acoustics, improving the performance of current\nstate-of-the-art models. An additional side-effect of this joint prediction is\nthe improvement in the explainability of the predictions, which is a valuable\nfeature for many applications.", "published": "2022-04-04 09:38:15", "link": "http://arxiv.org/abs/2204.01345v2", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Learning the Proximity Operator in Unfolded ADMM for Phase Retrieval", "abstract": "This paper considers the phase retrieval (PR) problem, which aims to\nreconstruct a signal from phaseless measurements such as magnitude or power\nspectrograms. PR is generally handled as a minimization problem involving a\nquadratic loss. Recent works have considered alternative discrepancy measures,\nsuch as the Bregman divergences, but it is still challenging to tailor the\noptimal loss for a given setting. In this paper we propose a novel strategy to\nautomatically learn the optimal metric for PR. We unfold a recently introduced\nADMM algorithm into a neural network, and we emphasize that the information\nabout the loss used to formulate the PR problem is conveyed by the proximity\noperator involved in the ADMM updates. Therefore, we replace this proximity\noperator with trainable activation functions: learning these in a supervised\nsetting is then equivalent to learning an optimal metric for PR. Experiments\nconducted with speech signals show that our approach outperforms the baseline\nADMM, using a light and interpretable neural architecture.", "published": "2022-04-04 10:09:28", "link": "http://arxiv.org/abs/2204.01360v1", "categories": ["cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "Anti-Spoofing Using Transfer Learning with Variational Information\n  Bottleneck", "abstract": "Recent advances in sophisticated synthetic speech generated from\ntext-to-speech (TTS) or voice conversion (VC) systems cause threats to the\nexisting automatic speaker verification (ASV) systems. Since such synthetic\nspeech is generated from diverse algorithms, generalization ability with using\nlimited training data is indispensable for a robust anti-spoofing system. In\nthis work, we propose a transfer learning scheme based on the wav2vec 2.0\npretrained model with variational information bottleneck (VIB) for speech\nanti-spoofing task. Evaluation on the ASVspoof 2019 logical access (LA)\ndatabase shows that our method improves the performance of distinguishing\nunseen spoofed and genuine speech, outperforming current state-of-the-art\nanti-spoofing systems. Furthermore, we show that the proposed system improves\nperformance in low-resource and cross-dataset settings of anti-spoofing task\nsignificantly, demonstrating that our system is also robust in terms of data\nsize and data distribution.", "published": "2022-04-04 11:08:21", "link": "http://arxiv.org/abs/2204.01387v2", "categories": ["eess.AS", "cs.CR", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Introducing ECAPA-TDNN and Wav2Vec2.0 Embeddings to Stuttering Detection", "abstract": "The adoption of advanced deep learning (DL) architecture in stuttering\ndetection (SD) tasks is challenging due to the limited size of the available\ndatasets. To this end, this work introduces the application of speech\nembeddings extracted with pre-trained deep models trained on massive audio\ndatasets for different tasks. In particular, we explore audio representations\nobtained using emphasized channel attention, propagation, and\naggregation-time-delay neural network (ECAPA-TDNN) and Wav2Vec2.0 model trained\non VoxCeleb and LibriSpeech datasets respectively. After extracting the\nembeddings, we benchmark with several traditional classifiers, such as a\nk-nearest neighbor, Gaussian naive Bayes, and neural network, for the\nstuttering detection tasks. In comparison to the standard SD system trained\nonly on the limited SEP-28k dataset, we obtain a relative improvement of 16.74%\nin terms of overall accuracy over baseline. Finally, we have shown that\ncombining two embeddings and concatenating multiple layers of Wav2Vec2.0 can\nfurther improve SD performance up to 1% and 2.64% respectively.", "published": "2022-04-04 15:12:25", "link": "http://arxiv.org/abs/2204.01564v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Lip to Speech Synthesis with Visual Context Attentional GAN", "abstract": "In this paper, we propose a novel lip-to-speech generative adversarial\nnetwork, Visual Context Attentional GAN (VCA-GAN), which can jointly model\nlocal and global lip movements during speech synthesis. Specifically, the\nproposed VCA-GAN synthesizes the speech from local lip visual features by\nfinding a mapping function of viseme-to-phoneme, while global visual context is\nembedded into the intermediate layers of the generator to clarify the ambiguity\nin the mapping induced by homophene. To achieve this, a visual context\nattention module is proposed where it encodes global representations from the\nlocal visual features, and provides the desired global visual context\ncorresponding to the given coarse speech representation to the generator\nthrough audio-visual attention. In addition to the explicit modelling of local\nand global visual representations, synchronization learning is introduced as a\nform of contrastive learning that guides the generator to synthesize a speech\nin sync with the given input lip movements. Extensive experiments demonstrate\nthat the proposed VCA-GAN outperforms existing state-of-the-art and is able to\neffectively synthesize the speech from multi-speaker that has been barely\nhandled in the previous works.", "published": "2022-04-04 06:49:05", "link": "http://arxiv.org/abs/2204.01726v1", "categories": ["cs.CV", "cs.AI", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Robust Stuttering Detection via Multi-task and Adversarial Learning", "abstract": "By automatic detection and identification of stuttering, speech pathologists\ncan track the progression of disfluencies of persons who stutter (PWS). In this\npaper, we investigate the impact of multi-task (MTL) and adversarial learning\n(ADV) to learn robust stutter features. This is the first-ever preliminary\nstudy where MTL and ADV have been employed in stuttering identification (SI).\nWe evaluate our system on the SEP-28k stuttering dataset consisting of 20 hours\n(approx) of data from 385 podcasts. Our methods show promising results and\noutperform the baseline in various disfluency classes. We achieve up to 10%,\n6.78%, and 2% improvement in repetitions, blocks, and interjections\nrespectively over the baseline.", "published": "2022-04-04 15:44:34", "link": "http://arxiv.org/abs/2204.01735v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Dual Quaternion Ambisonics Array for Six-Degree-of-Freedom Acoustic\n  Representation", "abstract": "Spatial audio methods are gaining a growing interest due to the spread of\nimmersive audio experiences and applications, such as virtual and augmented\nreality. For these purposes, 3D audio signals are often acquired through arrays\nof Ambisonics microphones, each comprising four capsules that decompose the\nsound field in spherical harmonics. In this paper, we propose a dual quaternion\nrepresentation of the spatial sound field acquired through an array of two\nFirst Order Ambisonics (FOA) microphones. The audio signals are encapsulated in\na dual quaternion that leverages quaternion algebra properties to exploit\ncorrelations among them. This augmented representation with 6 degrees of\nfreedom (6DOF) involves a more accurate coverage of the sound field, resulting\nin a more precise sound localization and a more immersive audio experience. We\nevaluate our approach on a sound event localization and detection (SELD)\nbenchmark. We show that our dual quaternion SELD model with temporal\nconvolution blocks (DualQSELD-TCN) achieves better results with respect to real\nand quaternion-valued baselines thanks to our augmented representation of the\nsound field. Full code is available at:\nhttps://github.com/ispamm/DualQSELD-TCN.", "published": "2022-04-04 21:11:00", "link": "http://arxiv.org/abs/2204.01851v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Learning Neural Acoustic Fields", "abstract": "Our environment is filled with rich and dynamic acoustic information. When we\nwalk into a cathedral, the reverberations as much as appearance inform us of\nthe sanctuary's wide open space. Similarly, as an object moves around us, we\nexpect the sound emitted to also exhibit this movement. While recent advances\nin learned implicit functions have led to increasingly higher quality\nrepresentations of the visual world, there have not been commensurate advances\nin learning spatial auditory representations. To address this gap, we introduce\nNeural Acoustic Fields (NAFs), an implicit representation that captures how\nsounds propagate in a physical scene. By modeling acoustic propagation in a\nscene as a linear time-invariant system, NAFs learn to continuously map all\nemitter and listener location pairs to a neural impulse response function that\ncan then be applied to arbitrary sounds. We demonstrate that the continuous\nnature of NAFs enables us to render spatial acoustics for a listener at an\narbitrary location, and can predict sound propagation at novel locations. We\nfurther show that the representation learned by NAFs can help improve visual\nlearning with sparse views. Finally, we show that a representation informative\nof scene structure emerges during the learning of NAFs.", "published": "2022-04-04 17:59:37", "link": "http://arxiv.org/abs/2204.00628v2", "categories": ["cs.SD", "cs.CV", "cs.LG", "cs.RO", "eess.AS"], "primary_category": "cs.SD"}
