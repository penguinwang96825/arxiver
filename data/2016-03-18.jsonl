{"title": "A Readability Analysis of Campaign Speeches from the 2016 US\n  Presidential Campaign", "abstract": "Readability is defined as the reading level of the speech from grade 1 to\ngrade 12. It results from the use of the REAP readability analysis (vocabulary\n- Collins-Thompson and Callan, 2004; syntax - Heilman et al ,2006, 2007), which\nuse the lexical contents and grammatical structure of the sentences in a\ndocument to predict the reading level. After analysis, results were grouped\ninto the average readability of each candidate, the evolution of the\ncandidate's speeches' readability over time and the standard deviation, or how\nmuch each candidate varied their speech from one venue to another. For\ncomparison, one speech from four past presidents and the Gettysburg Address\nwere also analyzed.", "published": "2016-03-18 00:55:52", "link": "http://arxiv.org/abs/1603.05739v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Readability-based Sentence Ranking for Evaluating Text Simplification", "abstract": "We propose a new method for evaluating the readability of simplified\nsentences through pair-wise ranking. The validity of the method is established\nthrough in-corpus and cross-corpus evaluation experiments. The approach\ncorrectly identifies the ranking of simplified and unsimplified sentences in\nterms of their reading level with an accuracy of over 80%, significantly\noutperforming previous results. To gain qualitative insights into the nature of\nsimplification at the sentence level, we studied the impact of specific\nlinguistic features. We empirically confirm that both word-level and syntactic\nfeatures play a role in comparing the degree of simplification of authentic\ndata. To carry out this research, we created a new sentence-aligned corpus from\nprofessionally simplified news articles. The new corpus resource enriches the\nempirical basis of sentence-level simplification research, which so far relied\non a single resource. Most importantly, it facilitates cross-corpus evaluation\nfor simplification, a key step towards generalizable results.", "published": "2016-03-18 22:24:54", "link": "http://arxiv.org/abs/1603.06009v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Document Neural Autoregressive Distribution Estimation", "abstract": "We present an approach based on feed-forward neural networks for learning the\ndistribution of textual documents. This approach is inspired by the Neural\nAutoregressive Distribution Estimator(NADE) model, which has been shown to be a\ngood estimator of the distribution of discrete-valued igh-dimensional vectors.\nIn this paper, we present how NADE can successfully be adapted to the case of\ntextual data, retaining from NADE the property that sampling or computing the\nprobability of observations can be done exactly and efficiently. The approach\ncan also be used to learn deep representations of documents that are\ncompetitive to those learned by the alternative topic modeling approaches.\nFinally, we describe how the approach can be combined with a regular neural\nnetwork N-gram model and substantially improve its performance, by making its\nlearned representation sensitive to the larger, document-specific context.", "published": "2016-03-18 19:24:44", "link": "http://arxiv.org/abs/1603.05962v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
