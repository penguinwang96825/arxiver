{"title": "Creation and Evaluation of Datasets for Distributional Semantics Tasks\n  in the Digital Humanities Domain", "abstract": "Word embeddings are already well studied in the general domain, usually\ntrained on large text corpora, and have been evaluated for example on word\nsimilarity and analogy tasks, but also as an input to downstream NLP processes.\nIn contrast, in this work we explore the suitability of word embedding\ntechnologies in the specialized digital humanities domain. After training\nembedding models of various types on two popular fantasy novel book series, we\nevaluate their performance on two task types: term analogies, and word\nintrusion. To this end, we manually construct test datasets with domain\nexperts. Among the contributions are the evaluation of various word embedding\ntechniques on the different task types, with the findings that even embeddings\ntrained on small corpora perform well for example on the word intrusion task.\nFurthermore, we provide extensive and high-quality datasets in digital\nhumanities for further investigation, as well as the implementation to easily\nreproduce or extend the experiments.", "published": "2019-03-07 00:54:02", "link": "http://arxiv.org/abs/1903.02671v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Arabic natural language processing: An overview", "abstract": "Arabic is recognised as the 4th most used language of the Internet. Arabic\nhas three main varieties: (1) classical Arabic (CA), (2) Modern Standard Arabic\n(MSA), (3) Arabic Dialect (AD). MSA and AD could be written either in Arabic or\nin Roman script (Arabizi), which corresponds to Arabic written with Latin\nletters, numerals and punctuation. Due to the complexity of this language and\nthe number of corresponding challenges for NLP, many surveys have been\nconducted, in order to synthesise the work done on Arabic. However these\nsurveys principally focus on two varieties of Arabic (MSA and AD, written in\nArabic letters only), they are slightly old (no such survey since 2015) and\ntherefore do not cover recent resources and tools. To bridge the gap, we\npropose a survey focusing on 90 recent research papers (74% of which were\npublished after 2015). Our study presents and classifies the work done on the\nthree varieties of Arabic, by concentrating on both Arabic and Arabizi, and\nassociates each work to its publicly available resources whenever available.", "published": "2019-03-07 09:22:35", "link": "http://arxiv.org/abs/1903.02784v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Active and Semi-Supervised Learning in ASR: Benefits on the Acoustic and\n  Language Models", "abstract": "The goal of this paper is to simulate the benefits of jointly applying active\nlearning (AL) and semi-supervised training (SST) in a new speech recognition\napplication. Our data selection approach relies on confidence filtering, and\nits impact on both the acoustic and language models (AM and LM) is studied.\nWhile AL is known to be beneficial to AM training, we show that it also carries\nout substantial improvements to the LM when combined with SST. Sophisticated\nconfidence models, on the other hand, did not prove to yield any data selection\ngain. Our results indicate that, while SST is crucial at the beginning of the\nlabeling process, its gains degrade rapidly as AL is set in place. The final\nsimulation reports that AL allows a transcription cost reduction of about 70%\nover random selection. Alternatively, for a fixed transcription budget, the\nproposed approach improves the word error rate by about 12.5% relative.", "published": "2019-03-07 11:38:36", "link": "http://arxiv.org/abs/1903.02852v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Language Modeling with Visual Features", "abstract": "Multimodal language models attempt to incorporate non-linguistic features for\nthe language modeling task. In this work, we extend a standard recurrent neural\nnetwork (RNN) language model with features derived from videos. We train our\nmodels on data that is two orders-of-magnitude bigger than datasets used in\nprior work. We perform a thorough exploration of model architectures for\ncombining visual and text features. Our experiments on two corpora (YouCookII\nand 20bn-something-something-v2) show that the best performing architecture\nconsists of middle fusion of visual and text features, yielding over 25%\nrelative improvement in perplexity. We report analysis that provides insights\ninto why our multimodal language model improves upon a standard RNN language\nmodel.", "published": "2019-03-07 14:20:01", "link": "http://arxiv.org/abs/1903.02930v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Option Comparison Network for Multiple-choice Reading Comprehension", "abstract": "Multiple-choice reading comprehension (MCRC) is the task of selecting the\ncorrect answer from multiple options given a question and an article. Existing\nMCRC models typically either read each option independently or compute a\nfixed-length representation for each option before comparing them. However,\nhumans typically compare the options at multiple-granularity level before\nreading the article in detail to make reasoning more efficient. Mimicking\nhumans, we propose an option comparison network (OCN) for MCRC which compares\noptions at word-level to better identify their correlations to help reasoning.\nSpecially, each option is encoded into a vector sequence using a skimmer to\nretain fine-grained information as much as possible. An attention mechanism is\nleveraged to compare these sequences vector-by-vector to identify more subtle\ncorrelations between options, which is potentially valuable for reasoning.\nExperimental results on the human English exam MCRC dataset RACE show that our\nmodel outperforms existing methods significantly. Moreover, it is also the\nfirst model that surpasses Amazon Mechanical Turker performance on the whole\ndataset.", "published": "2019-03-07 16:43:50", "link": "http://arxiv.org/abs/1903.03033v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Small-world networks for summarization of biomedical articles", "abstract": "In recent years, many methods have been developed to identify important\nportions of text documents. Summarization tools can utilize these methods to\nextract summaries from large volumes of textual information. However, to\nidentify concepts representing central ideas within a text document and to\nextract the most informative sentences that best convey those concepts still\nremain two crucial tasks in summarization methods. In this paper, we introduce\na graph-based method to address these two challenges in the context of\nbiomedical text summarization. We show that how a summarizer can discover\nmeaningful concepts within a biomedical text document using the Helmholtz\nprinciple. The summarizer considers the meaningful concepts as the main topics\nand constructs a graph based on the topics that the sentences share. The\nsummarizer can produce an informative summary by extracting those sentences\nhaving higher values of the degree. We assess the performance of our method for\nsummarization of biomedical articles using the Recall-Oriented Understudy for\nGisting Evaluation (ROUGE) toolkit. The results show that the degree can be a\nuseful centrality measure to identify important sentences in this type of\ngraph-based modelling. Our method can improve the performance of biomedical\ntext summarization compared to some state-of-the-art and publicly available\nsummarizers. Combining a concept-based modelling strategy and a graph-based\napproach to sentence extraction, our summarizer can produce summaries with the\nhighest scores of informativeness among the comparison methods. This research\nwork can be regarded as a start point to the study of small-world networks in\nsummarization of biomedical texts.", "published": "2019-03-07 12:12:17", "link": "http://arxiv.org/abs/1903.02861v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Learning to Speak and Act in a Fantasy Text Adventure Game", "abstract": "We introduce a large scale crowdsourced text adventure game as a research\nplatform for studying grounded dialogue. In it, agents can perceive, emote, and\nact whilst conducting dialogue with other agents. Models and humans can both\nact as characters within the game. We describe the results of training\nstate-of-the-art generative and retrieval models in this setting. We show that\nin addition to using past dialogue, these models are able to effectively use\nthe state of the underlying world to condition their predictions. In\nparticular, we show that grounding on the details of the local environment,\nincluding location descriptions, and the objects (and their affordances) and\ncharacters (and their previous actions) present within it allows better\npredictions of agent behavior and dialogue. We analyze the ingredients\nnecessary for successful grounding in this setting, and how each of these\nfactors relate to agents that can talk and act successfully.", "published": "2019-03-07 18:45:52", "link": "http://arxiv.org/abs/1903.03094v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Predicting Research Trends From Arxiv", "abstract": "We perform trend detection on two datasets of Arxiv papers, derived from its\nmachine learning (cs.LG) and natural language processing (cs.CL) categories.\nOur approach is bottom-up: we first rank papers by their normalized citation\ncounts, then group top-ranked papers into different categories based on the\ntasks that they pursue and the methods they use. We then analyze these\nresulting topics. We find that the dominating paradigm in cs.CL revolves around\nnatural language generation problems and those in cs.LG revolve around\nreinforcement learning and adversarial principles. By extrapolation, we predict\nthat these topics will remain lead problems/approaches in their fields in the\nshort- and mid-term.", "published": "2019-03-07 11:06:10", "link": "http://arxiv.org/abs/1903.02831v1", "categories": ["cs.CL", "cs.DL", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Integrating Artificial and Human Intelligence for Efficient Translation", "abstract": "Current advances in machine translation increase the need for translators to\nswitch from traditional translation to post-editing of machine-translated text,\na process that saves time and improves quality. Human and artificial\nintelligence need to be integrated in an efficient way to leverage the\nadvantages of both for the translation task. This paper outlines approaches at\nthis boundary of AI and HCI and discusses open research questions to further\nadvance the field.", "published": "2019-03-07 15:14:42", "link": "http://arxiv.org/abs/1903.02978v1", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "CLEVR-Dialog: A Diagnostic Dataset for Multi-Round Reasoning in Visual\n  Dialog", "abstract": "Visual Dialog is a multimodal task of answering a sequence of questions\ngrounded in an image, using the conversation history as context. It entails\nchallenges in vision, language, reasoning, and grounding. However, studying\nthese subtasks in isolation on large, real datasets is infeasible as it\nrequires prohibitively-expensive complete annotation of the 'state' of all\nimages and dialogs.\n  We develop CLEVR-Dialog, a large diagnostic dataset for studying multi-round\nreasoning in visual dialog. Specifically, we construct a dialog grammar that is\ngrounded in the scene graphs of the images from the CLEVR dataset. This\ncombination results in a dataset where all aspects of the visual dialog are\nfully annotated. In total, CLEVR-Dialog contains 5 instances of 10-round\ndialogs for about 85k CLEVR images, totaling to 4.25M question-answer pairs.\n  We use CLEVR-Dialog to benchmark performance of standard visual dialog\nmodels; in particular, on visual coreference resolution (as a function of the\ncoreference distance). This is the first analysis of its kind for visual dialog\nmodels that was not possible without this dataset. We hope the findings from\nCLEVR-Dialog will help inform the development of future models for visual\ndialog. Our dataset and code are publicly available.", "published": "2019-03-07 20:18:39", "link": "http://arxiv.org/abs/1903.03166v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Enhancing Music Features by Knowledge Transfer from User-item Log Data", "abstract": "In this paper, we propose a novel method that exploits music listening log\ndata for general-purpose music feature extraction. Despite the wealth of\ninformation available in the log data of user-item interactions, it has been\nmostly used for collaborative filtering to find similar items or users and was\nnot fully investigated for content-based music applications. We resolve this\nproblem by extending intra-domain knowledge distillation to cross-domain: i.e.,\nby transferring knowledge obtained from the user-item domain to the music\ncontent domain. The proposed system first trains the model that estimates log\ninformation from the audio contents; then it uses the model to improve other\ntask-specific models. The experiments on various music classification and\nregression tasks show that the proposed method successfully improves the\nperformances of the task-specific models.", "published": "2019-03-07 09:51:23", "link": "http://arxiv.org/abs/1903.02794v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Voice Activity Detection: Merging Source and Filter-based Information", "abstract": "Voice Activity Detection (VAD) refers to the problem of distinguishing speech\nsegments from background noise. Numerous approaches have been proposed for this\npurpose. Some are based on features derived from the power spectral density,\nothers exploit the periodicity of the signal. The goal of this paper is to\ninvestigate the joint use of source and filter-based features. Interestingly, a\nmutual information-based assessment shows superior discrimination power for the\nsource-related features, especially the proposed ones. The features are further\nthe input of an artificial neural network-based classifier trained on a\nmulti-condition database. Two strategies are proposed to merge source and\nfilter information: feature and decision fusion. Our experiments indicate an\nabsolute reduction of 3% of the equal error rate when using decision fusion.\nThe final proposed system is compared to four state-of-the-art methods on 150\nminutes of data recorded in real environments. Thanks to the robustness of its\nsource-related features, its multi-condition training and its efficient\ninformation fusion, the proposed system yields over the best state-of-the-art\nVAD a substantial increase of accuracy across all conditions (24% absolute on\naverage).", "published": "2019-03-07 11:26:10", "link": "http://arxiv.org/abs/1903.02844v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The life of a New York City noise sensor network", "abstract": "Noise pollution is one of the topmost quality of life issues for urban\nresidents in the United States. Continued exposure to high levels of noise has\nproven effects on health, including acute effects such as sleep disruption, and\nlong-term effects such as hypertension, heart disease, and hearing loss. To\ninvestigate and ultimately aid in the mitigation of urban noise, a network of\n55 sensor nodes has been deployed across New York City for over two years,\ncollecting sound pressure level (SPL) and audio data. This network has\ncumulatively amassed over 75 years of calibrated, high-resolution SPL\nmeasurements and 35 years of audio data. In addition, high frequency telemetry\ndata has been collected that provides an indication of a sensors' health. This\ntelemetry data was analyzed over an 18 month period across 31 of the sensors.\nIt has been used to develop a prototype model for pre-failure detection which\nhas the ability to identify sensors in a prefail state 69.1% of the time. The\nentire network infrastructure is outlined, including the operation of the\nsensors, followed by an analysis of its data yield and the development of the\nfault detection approach and the future system integration plans for this.", "published": "2019-03-07 21:51:57", "link": "http://arxiv.org/abs/1903.03195v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Phase-aware Speech Enhancement with Deep Complex U-Net", "abstract": "Most deep learning-based models for speech enhancement have mainly focused on\nestimating the magnitude of spectrogram while reusing the phase from noisy\nspeech for reconstruction. This is due to the difficulty of estimating the\nphase of clean speech. To improve speech enhancement performance, we tackle the\nphase estimation problem in three ways. First, we propose Deep Complex U-Net,\nan advanced U-Net structured model incorporating well-defined complex-valued\nbuilding blocks to deal with complex-valued spectrograms. Second, we propose a\npolar coordinate-wise complex-valued masking method to reflect the distribution\nof complex ideal ratio masks. Third, we define a novel loss function, weighted\nsource-to-distortion ratio (wSDR) loss, which is designed to directly correlate\nwith a quantitative evaluation measure. Our model was evaluated on a mixture of\nthe Voice Bank corpus and DEMAND database, which has been widely used by many\ndeep learning models for speech enhancement. Ablation experiments were\nconducted on the mixed dataset showing that all three proposed approaches are\nempirically valid. Experimental results show that the proposed method achieves\nstate-of-the-art performance in all metrics, outperforming previous approaches\nby a large margin.", "published": "2019-03-07 10:41:37", "link": "http://arxiv.org/abs/1903.03107v2", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
