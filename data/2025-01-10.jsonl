{"title": "Heath-Jarrow-Morton meet lifted Heston in energy markets for joint historical and implied calibration", "abstract": "In energy markets, joint historical and implied calibration is of paramount\nimportance for practitioners yet notoriously challenging due to the need to\nalign historical correlations of futures contracts with implied volatility\nsmiles from the option market. We address this crucial problem with a\nparsimonious multiplicative multi-factor Heath-Jarrow-Morton (HJM) model for\nforward curves, combined with a stochastic volatility factor coming from the\nLifted Heston model. We develop a sequential fast calibration procedure\nleveraging the Kemna-Vorst approximation of futures contracts: (i) historical\ncorrelations and the Variance Swap (VS) volatility term structure are captured\nthrough Level, Slope, and Curvature factors, (ii) the VS volatility term\nstructure can then be corrected for a perfect match via a fixed-point\nalgorithm, (iii) implied volatility smiles are calibrated using Fourier-based\ntechniques. Our model displays remarkable joint historical and implied\ncalibration fits - to both German power and TTF gas markets - and enables\nrealistic interpolation within the implied volatility hypercube.", "published": "2025-01-10 14:00:19", "link": "http://arxiv.org/abs/2501.05975v1", "categories": ["q-fin.MF", "q-fin.CP", "91G20, 91G60"], "primary_category": "q-fin.MF"}
{"title": "Optimal Insurance under Endogenous Default and Background Risk", "abstract": "This paper studies an optimal insurance problem for a utility-maximizing\nbuyer of insurance, subject to the seller's endogenous default and background\nrisk. An endogenous default occurs when the buyer's contractual indemnity\nexceeds the seller's available reserve, which is random due to the background\nrisk. We obtain an analytical solution to the optimal contract for two types of\ncontracts, differentiated by whether their indemnity functions depend on the\nseller's background risk. The results shed light on the joint effect of the\nseller's default and background risk on the buyer's insurance demand.", "published": "2025-01-10 02:45:03", "link": "http://arxiv.org/abs/2501.05672v1", "categories": ["q-fin.RM", "q-fin.MF"], "primary_category": "q-fin.RM"}
{"title": "A Modern Paradigm for Algorithmic Trading", "abstract": "We introduce a novel framework for developing fully-automated trading model\nalgorithms. Unlike the traditional approach, which is grounded in analytical\ncomplexity favored by most quantitative analysts, we propose a paradigm shift\nthat embraces real-world complexity. This approach leverages key concepts\nrelating to self-organization, emergence, complex systems theory, scaling laws,\nand utilizes an event-based reframing of time. In closing, we describe an\nexample algorithm that incorporates the outlined elements, called the Delta\nEngine.", "published": "2025-01-10 15:11:06", "link": "http://arxiv.org/abs/2501.06032v1", "categories": ["q-fin.TR"], "primary_category": "q-fin.TR"}
{"title": "Automating Date Format Detection for Data Visualization", "abstract": "Data preparation, specifically date parsing, is a significant bottleneck in\nanalytic workflows. To address this, we present two algorithms, one based on\nminimum entropy and the other on natural language modeling that automatically\nderive date formats from string data. These algorithms achieve over 90%\naccuracy on a large corpus of data columns, streamlining the data preparation\nprocess within visualization environments. The minimal entropy approach is\nparticularly fast, providing interactive feedback. Our methods simplify date\nformat extraction, making them suitable for integration into data visualization\ntools and databases.", "published": "2025-01-10 00:57:59", "link": "http://arxiv.org/abs/2501.05640v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Linguistic Entity Masking to Improve Cross-Lingual Representation of\n  Multilingual Language Models for Low-Resource Languages", "abstract": "Multilingual Pre-trained Language models (multiPLMs), trained on the Masked\nLanguage Modelling (MLM) objective are commonly being used for cross-lingual\ntasks such as bitext mining. However, the performance of these models is still\nsuboptimal for low-resource languages (LRLs). To improve the language\nrepresentation of a given multiPLM, it is possible to further pre-train it.\nThis is known as continual pre-training. Previous research has shown that\ncontinual pre-training with MLM and subsequently with Translation Language\nModelling (TLM) improves the cross-lingual representation of multiPLMs.\nHowever, during masking, both MLM and TLM give equal weight to all tokens in\nthe input sequence, irrespective of the linguistic properties of the tokens. In\nthis paper, we introduce a novel masking strategy, Linguistic Entity Masking\n(LEM) to be used in the continual pre-training step to further improve the\ncross-lingual representations of existing multiPLMs. In contrast to MLM and\nTLM, LEM limits masking to the linguistic entity types nouns, verbs and named\nentities, which hold a higher prominence in a sentence. Secondly, we limit\nmasking to a single token within the linguistic entity span thus keeping more\ncontext, whereas, in MLM and TLM, tokens are masked randomly. We evaluate the\neffectiveness of LEM using three downstream tasks, namely bitext mining,\nparallel data curation and code-mixed sentiment analysis using three\nlow-resource language pairs English-Sinhala, English-Tamil, and Sinhala-Tamil.\nExperiment results show that continually pre-training a multiPLM with LEM\noutperforms a multiPLM continually pre-trained with MLM+TLM for all three\ntasks.", "published": "2025-01-10 04:17:58", "link": "http://arxiv.org/abs/2501.05700v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Step Reasoning in Korean and the Emergent Mirage", "abstract": "We introduce HRMCR (HAE-RAE Multi-Step Commonsense Reasoning), a benchmark\ndesigned to evaluate large language models' ability to perform multi-step\nreasoning in culturally specific contexts, focusing on Korean. The questions\nare automatically generated via templates and algorithms, requiring LLMs to\nintegrate Korean cultural knowledge into sequential reasoning steps. Consistent\nwith prior observations on emergent abilities, our experiments reveal that\nmodels trained on fewer than \\(2 \\cdot 10^{25}\\) training FLOPs struggle to\nsolve any questions, showing near-zero performance. Beyond this threshold,\nperformance improves sharply. State-of-the-art models (e.g., O1) still score\nunder 50\\%, underscoring the difficulty of our tasks. Notably, stepwise\nanalysis suggests the observed emergent behavior may stem from compounding\nerrors across multiple steps rather than reflecting a genuinely new capability.\nWe publicly release the benchmark and commit to regularly updating the dataset\nto prevent contamination.", "published": "2025-01-10 05:07:27", "link": "http://arxiv.org/abs/2501.05712v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bridging Dialects: Translating Standard Bangla to Regional Variants\n  Using Neural Models", "abstract": "The Bangla language includes many regional dialects, adding to its cultural\nrichness. The translation of Bangla Language into regional dialects presents a\nchallenge due to significant variations in vocabulary, pronunciation, and\nsentence structure across regions like Chittagong, Sylhet, Barishal, Noakhali,\nand Mymensingh. These dialects, though vital to local identities, lack of\nrepresentation in technological applications. This study addresses this gap by\ntranslating standard Bangla into these dialects using neural machine\ntranslation (NMT) models, including BanglaT5, mT5, and mBART50. The work is\nmotivated by the need to preserve linguistic diversity and improve\ncommunication among dialect speakers. The models were fine-tuned using the\n\"Vashantor\" dataset, containing 32,500 sentences across various dialects, and\nevaluated through Character Error Rate (CER) and Word Error Rate (WER) metrics.\nBanglaT5 demonstrated superior performance with a CER of 12.3% and WER of\n15.7%, highlighting its effectiveness in capturing dialectal nuances. The\noutcomes of this research contribute to the development of inclusive language\ntechnologies that support regional dialects and promote linguistic diversity.", "published": "2025-01-10 06:50:51", "link": "http://arxiv.org/abs/2501.05749v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Controlling Large Language Models Through Concept Activation Vectors", "abstract": "As large language models (LLMs) are widely deployed across various domains,\nthe ability to control their generated outputs has become more critical. This\ncontrol involves aligning LLMs outputs with human values and ethical principles\nor customizing LLMs on specific topics or styles for individual users. Existing\ncontrolled generation methods either require significant computational\nresources and extensive trial-and-error or provide coarse-grained control. In\nthis paper, we propose Generation with Concept Activation Vector (GCAV), a\nlightweight model control framework that ensures accurate control without\nrequiring resource-extensive fine-tuning. Specifically, GCAV first trains a\nconcept activation vector for specified concepts to be controlled, such as\ntoxicity. During inference, GCAV steers the concept vector in LLMs, for\nexample, by removing the toxicity concept vector from the activation layers.\nControl experiments from different perspectives, including toxicity reduction,\nsentiment control, linguistic style, and topic control, demonstrate that our\nframework achieves state-of-the-art performance with granular control, allowing\nfor fine-grained adjustments of both the steering layers and the steering\nmagnitudes for individual samples.", "published": "2025-01-10 07:41:48", "link": "http://arxiv.org/abs/2501.05764v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IndoNLP 2025: Shared Task on Real-Time Reverse Transliteration for\n  Romanized Indo-Aryan languages", "abstract": "The paper overviews the shared task on Real-Time Reverse Transliteration for\nRomanized Indo-Aryan languages. It focuses on the reverse transliteration of\nlow-resourced languages in the Indo-Aryan family to their native scripts.\nTyping Romanized Indo-Aryan languages using ad-hoc transliterals and achieving\naccurate native scripts are complex and often inaccurate processes with the\ncurrent keyboard systems. This task aims to introduce and evaluate a real-time\nreverse transliterator that converts Romanized Indo-Aryan languages to their\nnative scripts, improving the typing experience for users. Out of 11 registered\nteams, four teams participated in the final evaluation phase with\ntransliteration models for Sinhala, Hindi and Malayalam. These proposed\nsolutions not only solve the issue of ad-hoc transliteration but also empower\nlow-resource language usability in the digital arena.", "published": "2025-01-10 09:41:46", "link": "http://arxiv.org/abs/2501.05816v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ConSim: Measuring Concept-Based Explanations' Effectiveness with\n  Automated Simulatability", "abstract": "Concept-based explanations work by mapping complex model computations to\nhuman-understandable concepts. Evaluating such explanations is very difficult,\nas it includes not only the quality of the induced space of possible concepts\nbut also how effectively the chosen concepts are communicated to users.\nExisting evaluation metrics often focus solely on the former, neglecting the\nlatter. We introduce an evaluation framework for measuring concept explanations\nvia automated simulatability: a simulator's ability to predict the explained\nmodel's outputs based on the provided explanations. This approach accounts for\nboth the concept space and its interpretation in an end-to-end evaluation.\nHuman studies for simulatability are notoriously difficult to enact,\nparticularly at the scale of a wide, comprehensive empirical evaluation (which\nis the subject of this work). We propose using large language models (LLMs) as\nsimulators to approximate the evaluation and report various analyses to make\nsuch approximations reliable. Our method allows for scalable and consistent\nevaluation across various models and datasets. We report a comprehensive\nempirical evaluation using this framework and show that LLMs provide consistent\nrankings of explanation methods. Code available at\nhttps://github.com/AnonymousConSim/ConSim.", "published": "2025-01-10 10:53:48", "link": "http://arxiv.org/abs/2501.05855v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLMs Reproduce Stereotypes of Sexual and Gender Minorities", "abstract": "A large body of research has found substantial gender bias in NLP systems.\nMost of this research takes a binary, essentialist view of gender: limiting its\nvariation to the categories _men_ and _women_, conflating gender with sex, and\nignoring different sexual identities. But gender and sexuality exist on a\nspectrum, so in this paper we study the biases of large language models (LLMs)\ntowards sexual and gender minorities beyond binary categories. Grounding our\nstudy in a widely used psychological framework -- the Stereotype Content Model\n-- we demonstrate that English-language survey questions about social\nperceptions elicit more negative stereotypes of sexual and gender minorities\nfrom LLMs, just as they do from humans. We then extend this framework to a more\nrealistic use case: text generation. Our analysis shows that LLMs generate\nstereotyped representations of sexual and gender minorities in this setting,\nraising concerns about their capacity to amplify representational harms in\ncreative writing, a widely promoted use case.", "published": "2025-01-10 12:46:39", "link": "http://arxiv.org/abs/2501.05926v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Universal-2-TF: Robust All-Neural Text Formatting for ASR", "abstract": "This paper introduces an all-neural text formatting (TF) model designed for\ncommercial automatic speech recognition (ASR) systems, encompassing punctuation\nrestoration (PR), truecasing, and inverse text normalization (ITN). Unlike\ntraditional rule-based or hybrid approaches, this method leverages a two-stage\nneural architecture comprising a multi-objective token classifier and a\nsequence-to-sequence (seq2seq) model. This design minimizes computational costs\nand reduces hallucinations while ensuring flexibility and robustness across\ndiverse linguistic entities and text domains. Developed as part of the\nUniversal-2 ASR system, the proposed method demonstrates superior performance\nin TF accuracy, computational efficiency, and perceptual quality, as validated\nthrough comprehensive evaluations using both objective and subjective methods.\nThis work underscores the importance of holistic TF models in enhancing ASR\nusability in practical settings.", "published": "2025-01-10 13:21:33", "link": "http://arxiv.org/abs/2501.05948v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Finnish SQuAD: A Simple Approach to Machine Translation of Span\n  Annotations", "abstract": "We apply a simple method to machine translate datasets with span-level\nannotation using the DeepL MT service and its ability to translate formatted\ndocuments. Using this method, we produce a Finnish version of the SQuAD2.0\nquestion answering dataset and train QA retriever models on this new dataset.\nWe evaluate the quality of the dataset and more generally the MT method through\ndirect evaluation, indirect comparison to other similar datasets, a\nbacktranslation experiment, as well as through the performance of downstream\ntrained QA models. In all these evaluations, we find that the method of\ntransfer is not only simple to use but produces consistently better translated\ndata. Given its good performance on the SQuAD dataset, it is likely the method\ncan be used to translate other similar span-annotated datasets for other tasks\nand languages as well. All code and data is available under an open license:\ndata at HuggingFace TurkuNLP/squad_v2_fi, code on GitHub TurkuNLP/squad2-fi,\nand model at HuggingFace TurkuNLP/bert-base-finnish-cased-squad2.", "published": "2025-01-10 13:44:11", "link": "http://arxiv.org/abs/2501.05963v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hermit Kingdom Through the Lens of Multiple Perspectives: A Case Study\n  of LLM Hallucination on North Korea", "abstract": "Hallucination in large language models (LLMs) remains a significant challenge\nfor their safe deployment, particularly due to its potential to spread\nmisinformation. Most existing solutions address this challenge by focusing on\naligning the models with credible sources or by improving how models\ncommunicate their confidence (or lack thereof) in their outputs. While these\nmeasures may be effective in most contexts, they may fall short in scenarios\nrequiring more nuanced approaches, especially in situations where access to\naccurate data is limited or determining credible sources is challenging. In\nthis study, we take North Korea - a country characterised by an extreme lack of\nreliable sources and the prevalence of sensationalist falsehoods - as a case\nstudy. We explore and evaluate how some of the best-performing multilingual\nLLMs and specific language-based models generate information about North Korea\nin three languages spoken in countries with significant geo-political\ninterests: English (United States, United Kingdom), Korean (South Korea), and\nMandarin Chinese (China). Our findings reveal significant differences,\nsuggesting that the choice of model and language can lead to vastly different\nunderstandings of North Korea, which has important implications given the\nglobal security challenges the country poses.", "published": "2025-01-10 14:08:59", "link": "http://arxiv.org/abs/2501.05981v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Constraining constructions with WordNet: pros and cons for the semantic\n  annotation of fillers in the Italian Constructicon", "abstract": "The paper discusses the role of WordNet-based semantic classification in the\nformalization of constructions, and more specifically in the semantic\nannotation of schematic fillers, in the Italian Constructicon. We outline how\nthe Italian Constructicon project uses Open Multilingual WordNet topics to\nrepresent semantic features and constraints of constructions.", "published": "2025-01-10 14:21:03", "link": "http://arxiv.org/abs/2501.05990v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Conversation to Automation: Leveraging LLMs for Problem-Solving\n  Therapy Analysis", "abstract": "Problem-solving therapy (PST) is a structured psychological approach that\nhelps individuals manage stress and resolve personal issues by guiding them\nthrough problem identification, solution brainstorming, decision-making, and\noutcome evaluation. As mental health care increasingly adopts technologies like\nchatbots and large language models (LLMs), it is important to thoroughly\nunderstand how each session of PST is conducted before attempting to automate\nit. We developed a comprehensive framework for PST annotation using established\nPST Core Strategies and a set of novel Facilitative Strategies to analyze a\ncorpus of real-world therapy transcripts to determine which strategies are most\nprevalent. Using various LLMs and transformer-based models, we found that\nGPT-4o outperformed all models, achieving the highest accuracy (0.76) in\nidentifying all strategies. To gain deeper insights, we examined how strategies\nare applied by analyzing Therapeutic Dynamics (autonomy, self-disclosure, and\nmetaphor), and linguistic patterns within our labeled data. Our research\nhighlights LLMs' potential to automate therapy dialogue analysis, offering a\nscalable tool for mental health interventions. Our framework enhances PST by\nimproving accessibility, effectiveness, and personalized support for\ntherapists.", "published": "2025-01-10 16:54:20", "link": "http://arxiv.org/abs/2501.06101v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models Share Representations of Latent Grammatical\n  Concepts Across Typologically Diverse Languages", "abstract": "Human bilinguals often use similar brain regions to process multiple\nlanguages, depending on when they learned their second language and their\nproficiency. In large language models (LLMs), how are multiple languages\nlearned and encoded? In this work, we explore the extent to which LLMs share\nrepresentations of morphosyntactic concepts such as grammatical number, gender,\nand tense across languages. We train sparse autoencoders on Llama-3-8B and\nAya-23-8B, and demonstrate that abstract grammatical concepts are often encoded\nin feature directions shared across many languages. We use causal interventions\nto verify the multilingual nature of these representations; specifically, we\nshow that ablating only multilingual features decreases classifier performance\nto near-chance across languages. We then use these features to precisely modify\nmodel behavior in a machine translation task; this demonstrates both the\ngenerality and selectivity of these feature's roles in the network. Our\nfindings suggest that even models trained predominantly on English data can\ndevelop robust, cross-lingual abstractions of morphosyntactic concepts.", "published": "2025-01-10 21:18:21", "link": "http://arxiv.org/abs/2501.06346v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AFRIDOC-MT: Document-level MT Corpus for African Languages", "abstract": "This paper introduces AFRIDOC-MT, a document-level multi-parallel translation\ndataset covering English and five African languages: Amharic, Hausa, Swahili,\nYor\\`ub\\'a, and Zulu. The dataset comprises 334 health and 271 information\ntechnology news documents, all human-translated from English to these\nlanguages. We conduct document-level translation benchmark experiments by\nevaluating neural machine translation (NMT) models and large language models\n(LLMs) for translations between English and these languages, at both the\nsentence and pseudo-document levels. These outputs are realigned to form\ncomplete documents for evaluation. Our results indicate that NLLB-200 achieved\nthe best average performance among the standard NMT models, while GPT-4o\noutperformed general-purpose LLMs. Fine-tuning selected models led to\nsubstantial performance gains, but models trained on sentences struggled to\ngeneralize effectively to longer documents. Furthermore, our analysis reveals\nthat some LLMs exhibit issues such as under-generation, repetition of words or\nphrases, and off-target translations, especially for African languages.", "published": "2025-01-10 22:49:29", "link": "http://arxiv.org/abs/2501.06374v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Impact of Model Scaling on Seen and Unseen Language Performance", "abstract": "The rapid advancement of Large Language Models (LLMs), particularly those\ntrained on multilingual corpora, has intensified the need for a deeper\nunderstanding of their performance across a diverse range of languages and\nmodel sizes. Our research addresses this critical need by studying the\nperformance and scaling behavior of multilingual LLMs in text classification\nand machine translation tasks across 204 languages. We systematically examine\nboth seen and unseen languages across three model families of varying sizes in\nzero-shot and few-shot settings. Our findings show significant differences in\nscaling behavior between zero-shot and two-shot scenarios, with striking\ndisparities in performance between seen and unseen languages. Model scale has\nlittle effect on zero-shot performance, which remains mostly flat. However, in\ntwo-shot settings, larger models show clear linear improvements in multilingual\ntext classification. For translation tasks, however, only the instruction-tuned\nmodel showed clear benefits from scaling. Our analysis also suggests that\noverall resource levels, not just the proportions of pretraining languages, are\nbetter predictors of model performance, shedding light on what drives\nmultilingual LLM effectiveness.", "published": "2025-01-10 00:10:21", "link": "http://arxiv.org/abs/2501.05629v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Iconicity in Large Language Models", "abstract": "Lexical iconicity, a direct relation between a word's meaning and its form,\nis an important aspect of every natural language, most commonly manifesting\nthrough sound-meaning associations. Since Large language models' (LLMs') access\nto both meaning and sound of text is only mediated (meaning through textual\ncontext, sound through written representation, further complicated by\ntokenization), we might expect that the encoding of iconicity in LLMs would be\neither insufficient or significantly different from human processing. This\nstudy addresses this hypothesis by having GPT-4 generate highly iconic\npseudowords in artificial languages. To verify that these words actually carry\niconicity, we had their meanings guessed by Czech and German participants\n(n=672) and subsequently by LLM-based participants (generated by GPT-4 and\nClaude 3.5 Sonnet). The results revealed that humans can guess the meanings of\npseudowords in the generated iconic language more accurately than words in\ndistant natural languages and that LLM-based participants are even more\nsuccessful than humans in this task. This core finding is accompanied by\nseveral additional analyses concerning the universality of the generated\nlanguage and the cues that both human and LLM-based participants utilize.", "published": "2025-01-10 01:00:05", "link": "http://arxiv.org/abs/2501.05643v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Cascaded Self-Evaluation Augmented Training for Lightweight Multimodal\n  LLMs", "abstract": "Efficient Multimodal Large Language Models (EMLLMs) can improve performance\nthrough Chain-of-Thought (CoT) reasoning, but they have poor self-evaluation\ncapabilities during the CoT reasoning process. This is due to their tendency to\nsimplify the reasoning process and the degradation of self-evaluation ability\nduring downstream task fine-tuning. To address this, we intuitively propose\n\\textit{Self-Evaluation Augmented Training (SEAT)}, which uses more powerful\nEMLLMs to evaluate CoT reasoning data. The evaluation data is then used to\ntrain EMLLMs. However, due to the difficulties EMLLMs face with processing long\ntoken input-output sequences, and the degradation of self-evaluation ability as\na basis for CoT reasoning, the SEAT method is not fully adapted. Therefore, we\nfurther propose \\textit{Cascaded Self-Evaluation Augmented Training\n(Cas-SEAT)}, which converts long prompts into cascaded short prompts, each\nfocusing on a specific task. Additionally, we mix CoT reasoning and\nself-evaluation data to preserve its CoT reasoning ability while enhancing the\nself-evaluation capability of EMLLMs. We also conduct \\textit{Double-level Data\nFiltering (DDF)}, which includes source data filtering and labeled data\nfiltering, using both manual selection and MLLMs for filtering. Cas-SEAT and\nDDF work together to improve the performance of EMLLMs. Experiments show that\nCas-SEAT achieves an average improvement of 22.16% across multiple datasets,\nand DDF significantly reduces the resource consumption of training", "published": "2025-01-10 02:28:04", "link": "http://arxiv.org/abs/2501.05662v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Overcoming Language Priors for Visual Question Answering Based on\n  Knowledge Distillation", "abstract": "Previous studies have pointed out that visual question answering (VQA) models\nare prone to relying on language priors for answer predictions. In this\ncontext, predictions often depend on linguistic shortcuts rather than a\ncomprehensive grasp of multimodal knowledge, which diminishes their\ngeneralization ability. In this paper, we propose a novel method, namely, KDAR,\nleveraging knowledge distillation to address the prior-dependency dilemmas\nwithin the VQA task. Specifically, the regularization effect facilitated by\nsoft labels from a well-trained teacher is employed to penalize overfitting to\nthe most common answers. The soft labels, which serve a regularization role,\nalso provide semantic guidance that narrows the range of candidate answers.\nAdditionally, we design an adaptive sample-wise reweighting learning strategy\nto further mitigate bias by dynamically adjusting the importance of each\nsample. Experimental results demonstrate that our method enhances performance\nin both OOD and IID settings. Our method achieves state-of-the-art performance\non the VQA-CPv2 out-of-distribution (OOD) benchmark, significantly\noutperforming previous state-of-the-art approaches.", "published": "2025-01-10 03:42:37", "link": "http://arxiv.org/abs/2501.05690v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Semantic Exploration with Adaptive Gating for Efficient Problem Solving\n  with Language Models", "abstract": "Recent advancements in large language models (LLMs) have shown remarkable\npotential in various complex tasks requiring multi-step reasoning methods like\ntree search to explore diverse reasoning paths. However, existing methods often\nsuffer from computational inefficiency and redundancy. First, they overlook the\ndiversity of task difficulties, leading to unnecessarily extensive searches\neven for easy tasks. Second, they neglect the semantics of reasoning paths,\nresulting in redundant exploration of semantically identical paths. To address\nthese limitations, we propose Semantic Exploration with Adaptive Gating (SEAG),\na computationally efficient method. SEAG employs an adaptive gating mechanism\nthat dynamically decides whether to conduct a tree search, based on the\nconfidence level of answers from a preceding simple reasoning method.\nFurthermore, its tree-based exploration consolidates semantically identical\nreasoning steps, reducing redundant explorations while maintaining or even\nimproving accuracy. Our extensive experiments demonstrate that SEAG\nsignificantly improves accuracy by 4.3% on average while requiring only 31% of\ncomputational costs compared to existing tree search-based methods on complex\nreasoning benchmarks including GSM8K and ARC with diverse language models such\nas Llama2, Llama3, and Mistral.", "published": "2025-01-10 07:02:43", "link": "http://arxiv.org/abs/2501.05752v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "MARS6: A Small and Robust Hierarchical-Codec Text-to-Speech Model", "abstract": "Codec-based text-to-speech (TTS) models have shown impressive quality with\nzero-shot voice cloning abilities. However, they often struggle with more\nexpressive references or complex text inputs. We present MARS6, a robust\nencoder-decoder transformer for rapid, expressive TTS. MARS6 is built on recent\nimprovements in spoken language modelling. Utilizing a hierarchical setup for\nits decoder, new speech tokens are processed at a rate of only 12 Hz, enabling\nefficient modelling of long-form text while retaining reconstruction quality.\nWe combine several recent training and inference techniques to reduce\nrepetitive generation and improve output stability and quality. This enables\nthe 70M-parameter MARS6 to achieve similar performance to models many times\nlarger. We show this in objective and subjective evaluations, comparing TTS\noutput quality and reference speaker cloning ability. Project page:\nhttps://camb-ai.github.io/mars6-turbo/", "published": "2025-01-10 08:41:42", "link": "http://arxiv.org/abs/2501.05787v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Affordably Fine-tuned LLMs Provide Better Answers to Course-specific\n  MCQs", "abstract": "In education, the capability of generating human-like text of Large Language\nModels (LLMs) inspired work on how they can increase the efficiency of learning\nand teaching. We study the affordability of these models for educators and\nstudents by investigating how LLMs answer multiple-choice questions (MCQs) with\nrespect to hardware constraints and refinement techniques. We explore this\nspace by using generic pre-trained LLMs (the 7B, 13B, and 70B variants of\nLLaMA-2) to answer 162 undergraduate-level MCQs from a course on Programming\nLanguages (PL) -- the MCQ dataset is a contribution of this work, which we make\npublicly available. Specifically, we dissect how different factors, such as\nusing readily-available material -- (parts of) the course's textbook -- for\nfine-tuning and quantisation (to decrease resource usage) can change the\naccuracy of the responses. The main takeaway is that smaller textbook-based\nfine-tuned models outperform generic larger ones (whose pre-training requires\nconspicuous resources), making the usage of LLMs for answering MCQs resource-\nand material-wise affordable.", "published": "2025-01-10 11:44:35", "link": "http://arxiv.org/abs/2501.05891v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Navigating Tomorrow: Reliably Assessing Large Language Models\n  Performance on Future Event Prediction", "abstract": "Predicting future events is an important activity with applications across\nmultiple fields and domains. For example, the capacity to foresee stock market\ntrends, natural disasters, business developments, or political events can\nfacilitate early preventive measures and uncover new opportunities. Multiple\ndiverse computational methods for attempting future predictions, including\npredictive analysis, time series forecasting, and simulations have been\nproposed. This study evaluates the performance of several large language models\n(LLMs) in supporting future prediction tasks, an under-explored domain. We\nassess the models across three scenarios: Affirmative vs. Likelihood\nquestioning, Reasoning, and Counterfactual analysis. For this, we create a\ndataset1 by finding and categorizing news articles based on entity type and its\npopularity. We gather news articles before and after the LLMs training cutoff\ndate in order to thoroughly test and compare model performance. Our research\nhighlights LLMs potential and limitations in predictive modeling, providing a\nfoundation for future improvements.", "published": "2025-01-10 12:44:46", "link": "http://arxiv.org/abs/2501.05925v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Scalable Vision Language Model Training via High Quality Data Curation", "abstract": "In this paper, we introduce SAIL-VL (ScAlable Vision Language Model TraIning\nvia High QuaLity Data Curation), an open-source vision language model (VLM)\nseries achieving state-of-the-art (SOTA) performance in 2B and 8B parameters.\nThe following three key improvements contribute to SAIL-VL's leading\nperformance: (1) Scalable high-quality visual understanding data construction:\nWe implement a data construction pipeline to enable hundred-million-scale\nhigh-quality recaption data annotation, and the resulted dataset SAIL-Caption\nis validated to be of the highest data quality compared with opensource\nalternatives. (2) Scalable Pretraining with High-Quality Visual Understanding\nData: We scale SAIL-VL's pretraining budget up to 655B tokens and show that\neven a 2B VLM benefits from scaled up training data sizes, exhibiting expected\ndata size scaling laws in visual understanding and instruction following\nperformance. (3) Scalable SFT via data quantity and complexity scaling: We\ncurate a high-quality SFT dataset collection which outperforms opensource\nalternatives in data quantity scaling effectiveness. We also demonstrate that\ntraining with progressively higher-complexity data surpasses baseline one-stage\ntraining by a large margin. SAIL-VL series models achieve the highest average\nscore in 18 widely used VLM benchmarks in our evaluation, with the 2B model\ntakes the top position over VLMs of comparable sizes on OpenCompass 2024\n(https://rank.opencompass.org.cn/leaderboard-multimodal) demonstrating robust\nvisual comprehension abilities. SAIL-VL series models are released at\nHuggingFace (https://huggingface.co/BytedanceDouyinContent).", "published": "2025-01-10 13:27:04", "link": "http://arxiv.org/abs/2501.05952v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Effective faking of verbal deception detection with target-aligned\n  adversarial attacks", "abstract": "Background: Deception detection through analysing language is a promising\navenue using both human judgments and automated machine learning judgments. For\nboth forms of credibility assessment, automated adversarial attacks that\nrewrite deceptive statements to appear truthful pose a serious threat. Methods:\nWe used a dataset of 243 truthful and 262 fabricated autobiographical stories\nin a deception detection task for humans and machine learning models. A large\nlanguage model was tasked to rewrite deceptive statements so that they appear\ntruthful. In Study 1, humans who made a deception judgment or used the\ndetailedness heuristic and two machine learning models (a fine-tuned language\nmodel and a simple n-gram model) judged original or adversarial modifications\nof deceptive statements. In Study 2, we manipulated the target alignment of the\nmodifications, i.e. tailoring the attack to whether the statements would be\nassessed by humans or computer models. Results: When adversarial modifications\nwere aligned with their target, human (d=-0.07 and d=-0.04) and machine\njudgments (51% accuracy) dropped to the chance level. When the attack was not\naligned with the target, both human heuristics judgments (d=0.30 and d=0.36)\nand machine learning predictions (63-78%) were significantly better than\nchance. Conclusions: Easily accessible language models can effectively help\nanyone fake deception detection efforts both by humans and machine learning\nmodels. Robustness against adversarial modifications for humans and machines\ndepends on that target alignment. We close with suggestions on advancing\ndeception research with adversarial attack designs.", "published": "2025-01-10 13:42:40", "link": "http://arxiv.org/abs/2501.05962v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Addressing speaker gender bias in large scale speech translation systems", "abstract": "This study addresses the issue of speaker gender bias in Speech Translation\n(ST) systems, which can lead to offensive and inaccurate translations. The\nmasculine bias often found in large-scale ST systems is typically perpetuated\nthrough training data derived from Machine Translation (MT) systems. Our\napproach involves two key steps. First, we employ Large Language Models (LLMs)\nto rectify translations based on the speaker's gender in a cost-effective\nmanner. Second, we fine-tune the ST model with the corrected data, enabling the\nmodel to generate gender-specific translations directly from audio cues,\nwithout the need for explicit gender input. Additionally, we propose a\nthree-mode fine-tuned model for scenarios where the speaker's gender is either\npredefined or should not be inferred from speech cues. We demonstrate a 70%\nimprovement in translations for female speakers compared to our baseline and\nother large-scale ST systems, such as Seamless M4T and Canary, on the MuST-SHE\ntest set.", "published": "2025-01-10 14:20:46", "link": "http://arxiv.org/abs/2501.05989v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "How to Tune a Multilingual Encoder Model for Germanic Languages: A Study\n  of PEFT, Full Fine-Tuning, and Language Adapters", "abstract": "This paper investigates the optimal use of the multilingual encoder model\nmDeBERTa for tasks in three Germanic languages -- German, Swedish, and\nIcelandic -- representing varying levels of presence and likely data quality in\nmDeBERTas pre-training data. We compare full fine-tuning with the\nparameter-efficient fine-tuning (PEFT) methods LoRA and Pfeiffer bottleneck\nadapters, finding that PEFT is more effective for the higher-resource language,\nGerman. However, results for Swedish and Icelandic are less consistent. We also\nobserve differences between tasks: While PEFT tends to work better for question\nanswering, full fine-tuning is preferable for named entity recognition.\nInspired by previous research on modular approaches that combine task and\nlanguage adapters, we evaluate the impact of adding PEFT modules trained on\nunstructured text, finding that this approach is not beneficial.", "published": "2025-01-10 15:01:51", "link": "http://arxiv.org/abs/2501.06025v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fleurs-SLU: A Massively Multilingual Benchmark for Spoken Language\n  Understanding", "abstract": "Spoken language understanding (SLU) is indispensable for half of all living\nlanguages that lack a formal writing system, since these languages cannot pair\nautomatic speech recognition (ASR) with language models to benefit from\nlanguage technology. Even if low-resource languages possess a writing system,\nASR for these languages remains unreliable due to limited bimodal speech and\ntext training data. Better SLU can strengthen the robustness of massively\nmultilingual ASR by levering language semantics to disambiguate utterances via\ncontext or exploiting semantic similarities across languages. However, the\nevaluation of multilingual SLU remains limited to shallow tasks such as intent\nclassification or language identification. To address this, we present\nFleurs-SLU, a multilingual SLU benchmark that encompasses (i) 692 hours of\nspeech for topical utterance classification in 102 languages and (ii)\nmultiple-choice question answering through listening comprehension spanning 944\nhours of speech across 92 languages. We extensively evaluate both end-to-end\nspeech classification models and cascaded systems that combine speech-to-text\ntranscription with subsequent classification by large language models on\nFleurs-SLU. Our results show that cascaded systems exhibit greater robustness\nin multilingual SLU tasks, though speech encoders can achieve competitive\nperformance in topical speech classification when appropriately pre-trained. We\nfurther find a strong correlation between robust multilingual ASR, effective\nspeech-to-text translation, and strong multilingual SLU, highlighting the\nmutual benefits between acoustic and semantic speech representations.", "published": "2025-01-10 17:15:38", "link": "http://arxiv.org/abs/2501.06117v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Merging Feed-Forward Sublayers for Compressed Transformers", "abstract": "With the rise and ubiquity of larger deep learning models, the need for\nhigh-quality compression techniques is growing in order to deploy these models\nwidely. The sheer parameter count of these models makes it difficult to fit\nthem into the memory constraints of different hardware. In this work, we\npresent a novel approach to model compression by merging similar parameter\ngroups within a model, rather than pruning away less important parameters.\nSpecifically, we select, align, and merge separate feed-forward sublayers in\nTransformer models, and test our method on language modeling, image\nclassification, and machine translation. With our method, we demonstrate\nperformance comparable to the original models while combining more than a third\nof model feed-forward sublayers, and demonstrate improved performance over a\nstrong layer-pruning baseline. For instance, we can remove over 21% of total\nparameters from a Vision Transformer, while maintaining 99% of its original\nperformance. Additionally, we observe that some groups of feed-forward\nsublayers exhibit high activation similarity, which may help explain their\nsurprising mergeability.", "published": "2025-01-10 17:25:11", "link": "http://arxiv.org/abs/2501.06126v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Contextual ASR Error Handling with LLMs Augmentation for Goal-Oriented\n  Conversational AI", "abstract": "General-purpose automatic speech recognition (ASR) systems do not always\nperform well in goal-oriented dialogue. Existing ASR correction methods rely on\nprior user data or named entities. We extend correction to tasks that have no\nprior user data and exhibit linguistic flexibility such as lexical and\nsyntactic variations. We propose a novel context augmentation with a large\nlanguage model and a ranking strategy that incorporates contextual information\nfrom the dialogue states of a goal-oriented conversational AI and its tasks.\nOur method ranks (1) n-best ASR hypotheses by their lexical and semantic\nsimilarity with context and (2) context by phonetic correspondence with ASR\nhypotheses. Evaluated in home improvement and cooking domains with real-world\nusers, our method improves recall and F1 of correction by 34% and 16%,\nrespectively, while maintaining precision and false positive rate. Users rated\n.8-1 point (out of 5) higher when our correction method worked properly, with\nno decrease due to false positives.", "published": "2025-01-10 17:35:06", "link": "http://arxiv.org/abs/2501.06129v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Environmental large language model Evaluation (ELLE) dataset: A\n  Benchmark for Evaluating Generative AI applications in Eco-environment Domain", "abstract": "Generative AI holds significant potential for ecological and environmental\napplications such as monitoring, data analysis, education, and policy support.\nHowever, its effectiveness is limited by the lack of a unified evaluation\nframework. To address this, we present the Environmental Large Language model\nEvaluation (ELLE) question answer (QA) dataset, the first benchmark designed to\nassess large language models and their applications in ecological and\nenvironmental sciences. The ELLE dataset includes 1,130 question answer pairs\nacross 16 environmental topics, categorized by domain, difficulty, and type.\nThis comprehensive dataset standardizes performance assessments in these\nfields, enabling consistent and objective comparisons of generative AI\nperformance. By providing a dedicated evaluation tool, ELLE dataset promotes\nthe development and application of generative AI technologies for sustainable\nenvironmental outcomes. The dataset and code are available at\nhttps://elle.ceeai.net/ and https://github.com/CEEAI/elle.", "published": "2025-01-10 12:48:29", "link": "http://arxiv.org/abs/2501.06277v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Aligning Brain Activity with Advanced Transformer Models: Exploring the\n  Role of Punctuation in Semantic Processing", "abstract": "This research examines the congruence between neural activity and advanced\ntransformer models, emphasizing the semantic significance of punctuation in\ntext understanding. Utilizing an innovative approach originally proposed by\nToneva and Wehbe, we evaluate four advanced transformer models RoBERTa,\nDistiliBERT, ALBERT, and ELECTRA against neural activity data. Our findings\nindicate that RoBERTa exhibits the closest alignment with neural activity,\nsurpassing BERT in accuracy. Furthermore, we investigate the impact of\npunctuation removal on model performance and neural alignment, revealing that\nBERT's accuracy enhances in the absence of punctuation. This study contributes\nto the comprehension of how neural networks represent language and the\ninfluence of punctuation on semantic processing within the human brain.", "published": "2025-01-10 13:07:11", "link": "http://arxiv.org/abs/2501.06278v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Bactrainus: Optimizing Large Language Models for Multi-hop Complex\n  Question Answering Tasks", "abstract": "In recent years, the use of large language models (LLMs) has significantly\nincreased, and these models have demonstrated remarkable performance in a\nvariety of general language tasks. However, the evaluation of their performance\nin domain-specific tasks, particularly those requiring deep natural language\nunderstanding, has received less attention. In this research, we evaluate the\nability of large language models in performing domain-specific tasks, focusing\non the multi-hop question answering (MHQA) problem using the HotpotQA dataset.\nThis task, due to its requirement for reasoning and combining information from\nmultiple textual sources, serves as a challenging benchmark for assessing the\nlanguage comprehension capabilities of these models. To tackle this problem, we\nhave designed a two-stage selector-reader architecture, where each stage\nutilizes an independent LLM. In addition, methods such as Chain of Thought\n(CoT) and question decomposition have been employed to investigate their impact\non improving the model's performance. The results of the study show that the\nintegration of large language models with these techniques can lead to up to a\n4% improvement in F1 score for finding answers, providing evidence of the\nmodels' ability to handle domain-specific tasks and their understanding of\ncomplex language.", "published": "2025-01-10 18:44:06", "link": "http://arxiv.org/abs/2501.06286v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Using Pre-trained LLMs for Multivariate Time Series Forecasting", "abstract": "Pre-trained Large Language Models (LLMs) encapsulate large amounts of\nknowledge and take enormous amounts of compute to train. We make use of this\nresource, together with the observation that LLMs are able to transfer\nknowledge and performance from one domain or even modality to another\nseemingly-unrelated area, to help with multivariate demand time series\nforecasting. Attention in transformer-based methods requires something worth\nattending to -- more than just samples of a time-series. We explore different\nmethods to map multivariate input time series into the LLM token embedding\nspace. In particular, our novel multivariate patching strategy to embed time\nseries features into decoder-only pre-trained Transformers produces results\ncompetitive with state-of-the-art time series forecasting models. We also use\nrecently-developed weight-based diagnostics to validate our findings.", "published": "2025-01-10 23:30:23", "link": "http://arxiv.org/abs/2501.06386v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Collaboration of Large Language Models and Small Recommendation Models\n  for Device-Cloud Recommendation", "abstract": "Large Language Models (LLMs) for Recommendation (LLM4Rec) is a promising\nresearch direction that has demonstrated exceptional performance in this field.\nHowever, its inability to capture real-time user preferences greatly limits the\npractical application of LLM4Rec because (i) LLMs are costly to train and infer\nfrequently, and (ii) LLMs struggle to access real-time data (its large number\nof parameters poses an obstacle to deployment on devices). Fortunately, small\nrecommendation models (SRMs) can effectively supplement these shortcomings of\nLLM4Rec diagrams by consuming minimal resources for frequent training and\ninference, and by conveniently accessing real-time data on devices.\n  In light of this, we designed the Device-Cloud LLM-SRM Collaborative\nRecommendation Framework (LSC4Rec) under a device-cloud collaboration setting.\nLSC4Rec aims to integrate the advantages of both LLMs and SRMs, as well as the\nbenefits of cloud and edge computing, achieving a complementary synergy. We\nenhance the practicability of LSC4Rec by designing three strategies:\ncollaborative training, collaborative inference, and intelligent request.\nDuring training, LLM generates candidate lists to enhance the ranking ability\nof SRM in collaborative scenarios and enables SRM to update adaptively to\ncapture real-time user interests. During inference, LLM and SRM are deployed on\nthe cloud and on the device, respectively. LLM generates candidate lists and\ninitial ranking results based on user behavior, and SRM get reranking results\nbased on the candidate list, with final results integrating both LLM's and\nSRM's scores. The device determines whether a new candidate list is needed by\ncomparing the consistency of the LLM's and SRM's sorted lists. Our\ncomprehensive and extensive experimental analysis validates the effectiveness\nof each strategy in LSC4Rec.", "published": "2025-01-10 01:27:12", "link": "http://arxiv.org/abs/2501.05647v2", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.DC"], "primary_category": "cs.IR"}
{"title": "Multiagent Finetuning: Self Improvement with Diverse Reasoning Chains", "abstract": "Large language models (LLMs) have achieved remarkable performance in recent\nyears but are fundamentally limited by the underlying training data. To improve\nmodels beyond the training data, recent works have explored how LLMs can be\nused to generate synthetic data for autonomous self-improvement. However,\nsuccessive steps of self-improvement can reach a point of diminishing returns.\nIn this work, we propose a complementary approach towards self-improvement\nwhere finetuning is applied to a multiagent society of language models. A group\nof language models, all starting from the same base model, are independently\nspecialized by updating each one using data generated through multiagent\ninteractions among the models. By training each model on independent sets of\ndata, we illustrate how this approach enables specialization across models and\ndiversification over the set of models. As a result, our overall system is able\nto preserve diverse reasoning chains and autonomously improve over many more\nrounds of fine-tuning than single-agent self-improvement methods. We\nquantitatively illustrate the efficacy of the approach across a wide suite of\nreasoning tasks.", "published": "2025-01-10 04:35:46", "link": "http://arxiv.org/abs/2501.05707v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "How to Enable Effective Cooperation Between Humans and NLP Models: A\n  Survey of Principles, Formalizations, and Beyond", "abstract": "With the advancement of large language models (LLMs), intelligent models have\nevolved from mere tools to autonomous agents with their own goals and\nstrategies for cooperating with humans. This evolution has birthed a novel\nparadigm in NLP, i.e., human-model cooperation, that has yielded remarkable\nprogress in numerous NLP tasks in recent years. In this paper, we take the\nfirst step to present a thorough review of human-model cooperation, exploring\nits principles, formalizations, and open challenges. In particular, we\nintroduce a new taxonomy that provides a unified perspective to summarize\nexisting approaches. Also, we discuss potential frontier areas and their\ncorresponding challenges. We regard our work as an entry point, paving the way\nfor more breakthrough research in this regard.", "published": "2025-01-10 05:15:14", "link": "http://arxiv.org/abs/2501.05714v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Enabling Scalable Oversight via Self-Evolving Critic", "abstract": "Despite their remarkable performance, the development of Large Language\nModels (LLMs) faces a critical challenge in scalable oversight: providing\neffective feedback for tasks where human evaluation is difficult or where LLMs\noutperform humans. While there is growing interest in using LLMs for critique,\ncurrent approaches still rely on human annotations or more powerful models,\nleaving the issue of enhancing critique capabilities without external\nsupervision unresolved. We introduce SCRIT (Self-evolving CRITic), a framework\nthat enables genuine self-evolution of critique abilities. Technically, SCRIT\nself-improves by training on synthetic data, generated by a contrastive-based\nself-critic that uses reference solutions for step-by-step critique, and a\nself-validation mechanism that ensures critique quality through correction\noutcomes. Implemented with Qwen2.5-72B-Instruct, one of the most powerful LLMs,\nSCRIT achieves up to a 10.3\\% improvement on critique-correction and error\nidentification benchmarks. Our analysis reveals that SCRIT's performance scales\npositively with data and model size, outperforms alternative approaches, and\nbenefits critically from its self-validation component.", "published": "2025-01-10 05:51:52", "link": "http://arxiv.org/abs/2501.05727v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Migician: Revealing the Magic of Free-Form Multi-Image Grounding in\n  Multimodal Large Language Models", "abstract": "The recent advancement of Multimodal Large Language Models (MLLMs) has\nsignificantly improved their fine-grained perception of single images and\ngeneral comprehension across multiple images. However, existing MLLMs still\nface challenges in achieving precise grounding in complex multi-image\nscenarios. To address this, we first explore a Chain-of-Thought (CoT) framework\nthat integrates single-image grounding with multi-image comprehension. While\npartially effective, it remains unstable and struggles to capture abstract\nvisual information due to its non-end-to-end nature. Therefore, we introduce\nMigician, the first multi-image grounding model capable of performing free-form\nand accurate grounding across multiple images. To support this, we present the\nMGrounding-630k dataset, which comprises data for several multi-image grounding\ntasks derived from existing datasets, along with newly generated free-form\ngrounding instruction-following data. Furthermore, we propose MIG-Bench, a\ncomprehensive benchmark specifically designed for evaluating multi-image\ngrounding capabilities. Experimental results demonstrate that our model\nachieves significantly superior multi-image grounding capabilities,\noutperforming the best existing MLLMs by 24.94% and even surpassing much larger\n70B models. Our code, model, dataset, and benchmark are fully open-sourced at\nhttps://migician-vg.github.io/.", "published": "2025-01-10 07:56:23", "link": "http://arxiv.org/abs/2501.05767v3", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Towards Early Prediction of Self-Supervised Speech Model Performance", "abstract": "In Self-Supervised Learning (SSL), pre-training and evaluation are resource\nintensive. In the speech domain, current indicators of the quality of SSL\nmodels during pre-training, such as the loss, do not correlate well with\ndownstream performance. Consequently, it is often difficult to gauge the final\ndownstream performance in a cost efficient manner during pre-training. In this\nwork, we propose unsupervised efficient methods that give insights into the\nquality of the pre-training of SSL speech models, namely, measuring the cluster\nquality and rank of the embeddings of the SSL model. Results show that measures\nof cluster quality and rank correlate better with downstream performance than\nthe pre-training loss with only one hour of unlabeled audio, reducing the need\nfor GPU hours and labeled data in SSL model evaluation.", "published": "2025-01-10 13:49:09", "link": "http://arxiv.org/abs/2501.05966v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Benchmarking Rotary Position Embeddings for Automatic Speech Recognition", "abstract": "Rotary Position Embedding (RoPE) encodes relative and absolute positional\ninformation in Transformer-based models through rotation matrices applied to\ninput vectors within sequences. While RoPE has demonstrated superior\nperformance compared to other positional embedding technologies in natural\nlanguage processing tasks, its effectiveness in speech processing applications\nremains understudied. In this work, we conduct a comprehensive evaluation of\nRoPE across diverse automatic speech recognition (ASR) tasks. Our experimental\nresults demonstrate that for ASR tasks, RoPE consistently achieves lower error\nrates compared to the currently widely used relative positional embedding. To\nfacilitate further research, we release the implementation and all experimental\nrecipes through the SpeechBrain toolkit.", "published": "2025-01-10 15:30:46", "link": "http://arxiv.org/abs/2501.06051v1", "categories": ["cs.CL", "cs.AI", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Polarized Patterns of Language Toxicity and Sentiment of Debunking Posts\n  on Social Media", "abstract": "The rise of misinformation and fake news in online political discourse poses\nsignificant challenges to democratic processes and public engagement. While\ndebunking efforts aim to counteract misinformation and foster fact-based\ndialogue, these discussions often involve language toxicity and emotional\npolarization. We examined over 86 million debunking tweets and more than 4\nmillion Reddit debunking comments to investigate the relationship between\nlanguage toxicity, pessimism, and social polarization in debunking efforts.\nFocusing on discussions of the 2016 and 2020 U.S. presidential elections and\nthe QAnon conspiracy theory, our analysis reveals three key findings: (1)\nperipheral participants (1-degree users) play a disproportionate role in\nshaping toxic discourse, driven by lower community accountability and emotional\nexpression; (2) platform mechanisms significantly influence polarization, with\nTwitter amplifying partisan differences and Reddit fostering higher overall\ntoxicity due to its structured, community-driven interactions; and (3) a\nnegative correlation exists between language toxicity and pessimism, with\nincreased interaction reducing toxicity, especially on Reddit. We show that\nplatform architecture affects informational complexity of user interactions,\nwith Twitter promoting concentrated, uniform discourse and Reddit encouraging\ndiverse, complex communication. Our findings highlight the importance of user\nengagement patterns, platform dynamics, and emotional expressions in shaping\npolarization in debunking discourse. This study offers insights for\npolicymakers and platform designers to mitigate harmful effects and promote\nhealthier online discussions, with implications for understanding\nmisinformation, hate speech, and political polarization in digital\nenvironments.", "published": "2025-01-10 08:00:58", "link": "http://arxiv.org/abs/2501.06274v2", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "PROEMO: Prompt-Driven Text-to-Speech Synthesis Based on Emotion and\n  Intensity Control", "abstract": "Speech synthesis has significantly advanced from statistical methods to deep\nneural network architectures, leading to various text-to-speech (TTS) models\nthat closely mimic human speech patterns. However, capturing nuances such as\nemotion and style in speech synthesis is challenging. To address this\nchallenge, we introduce an approach centered on prompt-based emotion control.\nThe proposed architecture incorporates emotion and intensity control across\nmulti-speakers. Furthermore, we leverage large language models (LLMs) to\nmanipulate speech prosody while preserving linguistic content. Using embedding\nemotional cues, regulating intensity levels, and guiding prosodic variations\nwith prompts, our approach infuses synthesized speech with human-like\nexpressiveness and variability. Lastly, we demonstrate the effectiveness of our\napproach through a systematic exploration of the control mechanisms mentioned\nabove.", "published": "2025-01-10 12:10:30", "link": "http://arxiv.org/abs/2501.06276v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Understanding How Paper Writers Use AI-Generated Captions in Figure\n  Caption Writing", "abstract": "Figures and their captions play a key role in scientific publications.\nHowever, despite their importance, many captions in published papers are poorly\ncrafted, largely due to a lack of attention by paper authors. While prior AI\nresearch has explored caption generation, it has mainly focused on\nreader-centered use cases, where users evaluate generated captions rather than\nactively integrating them into their writing. This paper addresses this gap by\ninvestigating how paper authors incorporate AI-generated captions into their\nwriting process through a user study involving 18 participants. Each\nparticipant rewrote captions for two figures from their own recently published\nwork, using captions generated by state-of-the-art AI models as a resource. By\nanalyzing video recordings of the writing process through interaction analysis,\nwe observed that participants often began by copying and refining AI-generated\ncaptions. Paper writers favored longer, detail-rich captions that integrated\ntextual and visual elements but found current AI models less effective for\ncomplex figures. These findings highlight the nuanced and diverse nature of\nfigure caption composition, revealing design opportunities for AI systems to\nbetter support the challenges of academic writing.", "published": "2025-01-10 19:39:06", "link": "http://arxiv.org/abs/2501.06317v1", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "TTS-Transducer: End-to-End Speech Synthesis with Neural Transducer", "abstract": "This work introduces TTS-Transducer - a novel architecture for\ntext-to-speech, leveraging the strengths of audio codec models and neural\ntransducers. Transducers, renowned for their superior quality and robustness in\nspeech recognition, are employed to learn monotonic alignments and allow for\navoiding using explicit duration predictors. Neural audio codecs efficiently\ncompress audio into discrete codes, revealing the possibility of applying text\nmodeling approaches to speech generation. However, the complexity of predicting\nmultiple tokens per frame from several codebooks, as necessitated by audio\ncodec models with residual quantizers, poses a significant challenge. The\nproposed system first uses a transducer architecture to learn monotonic\nalignments between tokenized text and speech codec tokens for the first\ncodebook. Next, a non-autoregressive Transformer predicts the remaining codes\nusing the alignment extracted from transducer loss. The proposed system is\ntrained end-to-end. We show that TTS-Transducer is a competitive and robust\nalternative to contemporary TTS systems.", "published": "2025-01-10 19:50:32", "link": "http://arxiv.org/abs/2501.06320v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Gender-Neutral Large Language Models for Medical Applications: Reducing\n  Bias in PubMed Abstracts", "abstract": "This paper presents a pipeline for mitigating gender bias in large language\nmodels (LLMs) used in medical literature by neutralizing gendered occupational\npronouns. A dataset of 379,000 PubMed abstracts from 1965-1980 was processed to\nidentify and modify pronouns tied to professions. We developed a BERT-based\nmodel, ``Modern Occupational Bias Elimination with Refined Training,'' or\n``MOBERT,'' trained on these neutralized abstracts, and compared its\nperformance with ``1965Bert,'' trained on the original dataset. MOBERT achieved\na 70\\% inclusive replacement rate, while 1965Bert reached only 4\\%. A further\nanalysis of MOBERT revealed that pronoun replacement accuracy correlated with\nthe frequency of occupational terms in the training data. We propose expanding\nthe dataset and refining the pipeline to improve performance and ensure more\nequitable language modeling in medical applications.", "published": "2025-01-10 22:07:56", "link": "http://arxiv.org/abs/2501.06365v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Dynamics of \"Spontaneous\" Topic Changes in Next Token Prediction with\n  Self-Attention", "abstract": "Human cognition can spontaneously shift conversation topics, often triggered\nby emotional or contextual signals. In contrast, self-attention-based language\nmodels depend on structured statistical cues from input tokens for next-token\nprediction, lacking this spontaneity. Motivated by this distinction, we\ninvestigate the factors that influence the next-token prediction to change the\ntopic of the input sequence. We define concepts of topic continuity, ambiguous\nsequences, and change of topic, based on defining a topic as a set of token\npriority graphs (TPGs). Using a simplified single-layer self-attention\narchitecture, we derive analytical characterizations of topic changes.\nSpecifically, we demonstrate that (1) the model maintains the priority order of\ntokens related to the input topic, (2) a topic change can occur only if\nlower-priority tokens outnumber all higher-priority tokens of the input topic,\nand (3) unlike human cognition, longer context lengths and overlapping topics\nreduce the likelihood of spontaneous redirection. These insights highlight\ndifferences between human cognition and self-attention-based models in\nnavigating topic changes and underscore the challenges in designing\nconversational AI capable of handling \"spontaneous\" conversations more\nnaturally. To the best of our knowledge, no prior work has explored these\nquestions with a focus as closely aligned to human conversation and thought.", "published": "2025-01-10 23:18:23", "link": "http://arxiv.org/abs/2501.06382v2", "categories": ["cs.CL", "cs.AI", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Optimize Incompatible Parameters through Compatibility-aware Knowledge\n  Integration", "abstract": "Deep neural networks have become foundational to advancements in multiple\ndomains, including recommendation systems, natural language processing, and so\non. Despite their successes, these models often contain incompatible parameters\nthat can be underutilized or detrimental to model performance, particularly\nwhen faced with specific, varying data distributions. Existing research excels\nin removing such parameters or merging the outputs of multiple different\npretrained models. However, the former focuses on efficiency rather than\nperformance, while the latter requires several times more computing and storage\nresources to support inference. In this paper, we set the goal to explicitly\nimprove these incompatible parameters by leveraging the complementary strengths\nof different models, thereby directly enhancing the models without any\nadditional parameters. Specifically, we propose Compatibility-aware Knowledge\nIntegration (CKI), which consists of Parameter Compatibility Assessment and\nParameter Splicing, which are used to evaluate the knowledge content of\nmultiple models and integrate the knowledge into one model, respectively. The\nintegrated model can be used directly for inference or for further fine-tuning.\nWe conduct extensive experiments on various datasets for recommendation and\nlanguage tasks, and the results show that Compatibility-aware Knowledge\nIntegration can effectively optimize incompatible parameters under multiple\ntasks and settings to break through the training limit of the original model\nwithout increasing the inference cost.", "published": "2025-01-10 01:42:43", "link": "http://arxiv.org/abs/2501.07596v2", "categories": ["cs.LG", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "VideoRAG: Retrieval-Augmented Generation over Video Corpus", "abstract": "Retrieval-Augmented Generation (RAG) is a powerful strategy for improving the\nfactual accuracy of models by retrieving external knowledge relevant to queries\nand incorporating it into the generation process. However, existing approaches\nprimarily focus on text, with some recent advancements considering images, and\nthey largely overlook videos, a rich source of multimodal knowledge capable of\nrepresenting contextual details more effectively than any other modality. While\nvery recent studies explore the use of videos in response generation, they\neither predefine query-associated videos without retrieval or convert videos\ninto textual descriptions losing multimodal richness. To tackle these, we\nintroduce VideoRAG, a framework that not only dynamically retrieves videos\nbased on their relevance with queries but also utilizes both visual and textual\ninformation. The operation of VideoRAG is powered by recent Large Video\nLanguage Models (LVLMs), which enable the direct processing of video content to\nrepresent it for retrieval and the seamless integration of retrieved videos\njointly with queries for response generation. Also, inspired by that the\ncontext size of LVLMs may not be sufficient to process all frames in extremely\nlong videos and not all frames are equally important, we introduce a video\nframe selection mechanism to extract the most informative subset of frames,\nalong with a strategy to extract textual information from videos (as it can aid\nthe understanding of video content) when their subtitles are not available. We\nexperimentally validate the effectiveness of VideoRAG, showcasing that it is\nsuperior to relevant baselines. Code is available at\nhttps://github.com/starsuzi/VideoRAG.", "published": "2025-01-10 11:17:15", "link": "http://arxiv.org/abs/2501.05874v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CV"}
{"title": "MinMo: A Multimodal Large Language Model for Seamless Voice Interaction", "abstract": "Recent advancements in large language models (LLMs) and multimodal\nspeech-text models have laid the groundwork for seamless voice interactions,\nenabling real-time, natural, and human-like conversations. Previous models for\nvoice interactions are categorized as native and aligned. Native models\nintegrate speech and text processing in one framework but struggle with issues\nlike differing sequence lengths and insufficient pre-training. Aligned models\nmaintain text LLM capabilities but are often limited by small datasets and a\nnarrow focus on speech tasks. In this work, we introduce MinMo, a Multimodal\nLarge Language Model with approximately 8B parameters for seamless voice\ninteraction. We address the main limitations of prior aligned multimodal\nmodels. We train MinMo through multiple stages of speech-to-text alignment,\ntext-to-speech alignment, speech-to-speech alignment, and duplex interaction\nalignment, on 1.4 million hours of diverse speech data and a broad range of\nspeech tasks. After the multi-stage training, MinMo achieves state-of-the-art\nperformance across various benchmarks for voice comprehension and generation\nwhile maintaining the capabilities of text LLMs, and also facilitates\nfull-duplex conversation, that is, simultaneous two-way communication between\nthe user and the system. Moreover, we propose a novel and simple voice decoder\nthat outperforms prior models in voice generation. The enhanced\ninstruction-following capabilities of MinMo supports controlling speech\ngeneration based on user instructions, with various nuances including emotions,\ndialects, and speaking rates, and mimicking specific voices. For MinMo, the\nspeech-to-text latency is approximately 100ms, full-duplex latency is\napproximately 600ms in theory and 800ms in practice. The MinMo project web page\nis https://funaudiollm.github.io/minmo, and the code and models will be\nreleased soon.", "published": "2025-01-10 15:55:27", "link": "http://arxiv.org/abs/2501.06282v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Dafny as Verification-Aware Intermediate Language for Code Generation", "abstract": "Using large language models (LLMs) to generate source code from natural\nlanguage prompts is a popular and promising idea with a wide range of\napplications. One of its limitations is that the generated code can be faulty\nat times, often in a subtle way, despite being presented to the user as\ncorrect. In this paper, we explore ways in which formal methods can assist with\nincreasing the quality of code generated by an LLM. Instead of emitting code in\na target language directly, we propose that the user guides the LLM to first\ngenerate an opaque intermediate representation, in the verification-aware\nlanguage Dafny, that can be automatically validated for correctness against\nagreed on specifications. The correct Dafny program is then compiled to the\ntarget language and returned to the user. All user-system interactions\nthroughout the procedure occur via natural language; Dafny code is never\nexposed. We describe our current prototype and report on its performance on the\nHumanEval Python code generation benchmarks.", "published": "2025-01-10 17:23:14", "link": "http://arxiv.org/abs/2501.06283v1", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LO", "cs.PL"], "primary_category": "cs.SE"}
{"title": "Sub-band Domain Multi-Hypothesis Acoustic Echo Canceler Based Acoustic\n  Scene Analysis", "abstract": "This paper introduces a novel approach for acoustic scene analysis by\nexploiting an ensemble of statistics extracted from a sub-band domain\nmulti-hypothesis acoustic echo canceler (SDMH-AEC). A well-designed SDMH-AEC\nemploys multiple adaptive filtering strategies with potentially complementary\nbehaviours during convergence, perturbations, and steady-state conditions. By\naggregating statistics across the sub-bands, we derive a feature vector that\nexhibits strong discriminative power for distinguishing different acoustic\nevents and estimating acoustic parameters. The complementary nature of the\nSDMH-AEC filters provides a rich source of information that can be extracted at\ninsignificant cost for acoustic scene analysis tasks. We demonstrate the\neffectiveness of the proposed approach experimentally with real data containing\ndouble-talk, echo path change and events where the full-duplex device is\nphysically moved. The extracted features enable acoustic scene analysis using\nexisting echo cancellation algorithms and techniques.", "published": "2025-01-10 01:43:26", "link": "http://arxiv.org/abs/2501.05652v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Large Model Empowered Streaming Speech Semantic Communications", "abstract": "In this paper, we introduce a large model-empowered streaming semantic\ncommunication system for speech transmission across various languages, named\nLSSC-ST. Specifically, we devise an edge-device collaborative semantic\ncommunication architecture by offloading the intricate semantic extraction and\nchannel coding modules to edge servers, thereby reducing the computational\nburden on local devices. To support multilingual speech transmission,\npre-trained large speech models are utilized to learn unified semantic features\nfrom speech in different languages, breaking the constraint of a single input\nlanguage and enhancing the practicality of the LSSC-ST. Moreover, the input\nspeech is sequentially streamed into the developed system as short speech\nsegments, which enables low transmission latency without degrading the quality\nof the produced speech. A novel dynamic speech segmentation algorithm is\nproposed to further reduce the transmission latency by adaptively adjusting the\nduration of speech segments. According to simulation results, the LSSC-ST\nprovides more accurate speech transmission and achieves a streaming manner with\nlower latency compared to the existing non-streaming semantic communication\nsystems.", "published": "2025-01-10 10:57:40", "link": "http://arxiv.org/abs/2501.05859v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Estimation and Restoration of Unknown Nonlinear Distortion using\n  Diffusion", "abstract": "The restoration of nonlinearly distorted audio signals, alongside the\nidentification of the applied memoryless nonlinear operation, is studied. The\npaper focuses on the difficult but practically important case in which both the\nnonlinearity and the original input signal are unknown. The proposed method\nuses a generative diffusion model trained unconditionally on guitar or speech\nsignals to jointly model and invert the nonlinear system at inference time.\nBoth the memoryless nonlinear function model and the restored audio signal are\nobtained as output. Successful example case studies are presented including\ninversion of hard and soft clipping, digital quantization, half-wave\nrectification, and wavefolding nonlinearities. Our results suggest that, out of\nthe nonlinear functions tested here, the cubic Catmull-Rom spline is best\nsuited to approximating these nonlinearities. In the case of guitar recordings,\ncomparisons with informed and supervised methods show that the proposed blind\nmethod is at least as good as they are in terms of objective metrics.\nExperiments on distorted speech show that the proposed blind method outperforms\ngeneral-purpose speech enhancement techniques and restores the original voice\nquality. The proposed method can be applied to audio effects modeling,\nrestoration of music and speech recordings, and characterization of analog\nrecording media.", "published": "2025-01-10 13:40:32", "link": "http://arxiv.org/abs/2501.05959v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Low-Resource Text-to-Speech Synthesis Using Noise-Augmented Training of\n  ForwardTacotron", "abstract": "In recent years, several text-to-speech systems have been proposed to\nsynthesize natural speech in zero-shot, few-shot, and low-resource scenarios.\nHowever, these methods typically require training with data from many different\nspeakers. The speech quality across the speaker set typically is diverse and\nimposes an upper limit on the quality achievable for the low-resource speaker.\nIn the current work, we achieve high-quality speech synthesis using as little\nas five minutes of speech from the desired speaker by augmenting the\nlow-resource speaker data with noise and employing multiple sampling techniques\nduring training. Our method requires only four high-quality, high-resource\nspeakers, which are easy to obtain and use in practice. Our low-complexity\nmethod achieves improved speaker similarity compared to the state-of-the-art\nzero-shot method HierSpeech++ and the recent low-resource method AdapterMix\nwhile maintaining comparable naturalness. Our proposed approach can also reduce\nthe data requirements for speech synthesis for new speakers and languages.", "published": "2025-01-10 14:01:33", "link": "http://arxiv.org/abs/2501.05976v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Comparing Self-Supervised Learning Models Pre-Trained on Human Speech\n  and Animal Vocalizations for Bioacoustics Processing", "abstract": "Self-supervised learning (SSL) foundation models have emerged as powerful,\ndomain-agnostic, general-purpose feature extractors applicable to a wide range\nof tasks. Such models pre-trained on human speech have demonstrated high\ntransferability for bioacoustic processing. This paper investigates (i) whether\nSSL models pre-trained directly on animal vocalizations offer a significant\nadvantage over those pre-trained on speech, and (ii) whether fine-tuning\nspeech-pretrained models on automatic speech recognition (ASR) tasks can\nenhance bioacoustic classification. We conduct a comparative analysis using\nthree diverse bioacoustic datasets and two different bioacoustic tasks. Results\nindicate that pre-training on bioacoustic data provides only marginal\nimprovements over speech-pretrained models, with comparable performance in most\nscenarios. Fine-tuning on ASR tasks yields mixed outcomes, suggesting that the\ngeneral-purpose representations learned during SSL pre-training are already\nwell-suited for bioacoustic tasks. These findings highlight the robustness of\nspeech-pretrained SSL models for bioacoustics and imply that extensive\nfine-tuning may not be necessary for optimal performance.", "published": "2025-01-10 14:18:21", "link": "http://arxiv.org/abs/2501.05987v2", "categories": ["cs.LG", "eess.AS"], "primary_category": "cs.LG"}
{"title": "ExPO: Explainable Phonetic Trait-Oriented Network for Speaker\n  Verification", "abstract": "In speaker verification, we use computational method to verify if an\nutterance matches the identity of an enrolled speaker. This task is similar to\nthe manual task of forensic voice comparison, where linguistic analysis is\ncombined with auditory measurements to compare and evaluate voice samples.\nDespite much success, we have yet to develop a speaker verification system that\noffers explainable results comparable to those from manual forensic voice\ncomparison. A novel approach, Explainable Phonetic Trait-Oriented (ExPO)\nnetwork, is proposed in this paper to introduce the speaker's phonetic trait\nwhich describes the speaker's characteristics at the phonetic level, resembling\nwhat forensic comparison does. ExPO not only generates utterance-level speaker\nembeddings but also allows for fine-grained analysis and visualization of\nphonetic traits, offering an explainable speaker verification process.\nFurthermore, we investigate phonetic traits from within-speaker and\nbetween-speaker variation perspectives to determine which trait is most\neffective for speaker verification, marking an important step towards\nexplainable speaker verification. Our code is available at\nhttps://github.com/mmmmayi/ExPO.", "published": "2025-01-10 05:53:37", "link": "http://arxiv.org/abs/2501.05729v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "CognoSpeak: an automatic, remote assessment of early cognitive decline\n  in real-world conversational speech", "abstract": "The early signs of cognitive decline are often noticeable in conversational\nspeech, and identifying those signs is crucial in dealing with later and more\nserious stages of neurodegenerative diseases. Clinical detection is costly and\ntime-consuming and although there has been recent progress in the automatic\ndetection of speech-based cues, those systems are trained on relatively small\ndatabases, lacking detailed metadata and demographic information. This paper\npresents CognoSpeak and its associated data collection efforts. CognoSpeak asks\nmemory-probing long and short-term questions and administers standard cognitive\ntasks such as verbal and semantic fluency and picture description using a\nvirtual agent on a mobile or web platform. In addition, it collects multimodal\ndata such as audio and video along with a rich set of metadata from primary and\nsecondary care, memory clinics and remote settings like people's homes. Here,\nwe present results from 126 subjects whose audio was manually transcribed.\nSeveral classic classifiers, as well as large language model-based classifiers,\nhave been investigated and evaluated across the different types of prompts. We\ndemonstrate a high level of performance; in particular, we achieved an F1-score\nof 0.873 using a DistilBERT model to discriminate people with cognitive\nimpairment (dementia and people with mild cognitive impairment (MCI)) from\nhealthy volunteers using the memory responses, fluency tasks and cookie theft\npicture description. CognoSpeak is an automatic, remote, low-cost, repeatable,\nnon-invasive and less stressful alternative to existing clinical cognitive\nassessments.", "published": "2025-01-10 07:13:42", "link": "http://arxiv.org/abs/2501.05755v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "xLSTM-SENet: xLSTM for Single-Channel Speech Enhancement", "abstract": "While attention-based architectures, such as Conformers, excel in speech\nenhancement, they face challenges such as scalability with respect to input\nsequence length. In contrast, the recently proposed Extended Long Short-Term\nMemory (xLSTM) architecture offers linear scalability. However, xLSTM-based\nmodels remain unexplored for speech enhancement. This paper introduces\nxLSTM-SENet, the first xLSTM-based single-channel speech enhancement system. A\ncomparative analysis reveals that xLSTM-and notably, even LSTM-can match or\noutperform state-of-the-art Mamba- and Conformer-based systems across various\nmodel sizes in speech enhancement on the VoiceBank+Demand dataset. Through\nablation studies, we identify key architectural design choices such as\nexponential gating and bidirectionality contributing to its effectiveness. Our\nbest xLSTM-based model, xLSTM-SENet2, outperforms state-of-the-art Mamba- and\nConformer-based systems on the Voicebank+DEMAND dataset.", "published": "2025-01-10 18:10:06", "link": "http://arxiv.org/abs/2501.06146v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
