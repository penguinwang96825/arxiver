{"title": "A Multilingual Bag-of-Entities Model for Zero-Shot Cross-Lingual Text\n  Classification", "abstract": "We present a multilingual bag-of-entities model that effectively boosts the\nperformance of zero-shot cross-lingual text classification by extending a\nmultilingual pre-trained language model (e.g., M-BERT). It leverages the\nmultilingual nature of Wikidata: entities in multiple languages representing\nthe same concept are defined with a unique identifier. This enables entities\ndescribed in multiple languages to be represented using shared embeddings. A\nmodel trained on entity features in a resource-rich language can thus be\ndirectly applied to other languages. Our experimental results on cross-lingual\ntopic classification (using the MLDoc and TED-CLDC datasets) and entity typing\n(using the SHINRA2020-ML dataset) show that the proposed model consistently\noutperforms state-of-the-art models.", "published": "2021-10-15 01:10:50", "link": "http://arxiv.org/abs/2110.07792v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Alternative Input Signals Ease Transfer in Multilingual Machine\n  Translation", "abstract": "Recent work in multilingual machine translation (MMT) has focused on the\npotential of positive transfer between languages, particularly cases where\nhigher-resourced languages can benefit lower-resourced ones. While training an\nMMT model, the supervision signals learned from one language pair can be\ntransferred to the other via the tokens shared by multiple source languages.\nHowever, the transfer is inhibited when the token overlap among source\nlanguages is small, which manifests naturally when languages use different\nwriting systems. In this paper, we tackle inhibited transfer by augmenting the\ntraining data with alternative signals that unify different writing systems,\nsuch as phonetic, romanized, and transliterated input. We test these signals on\nIndic and Turkic languages, two language families where the writing systems\ndiffer but languages still share common features. Our results indicate that a\nstraightforward multi-source self-ensemble -- training a model on a mixture of\nvarious signals and ensembling the outputs of the same model fed with different\nsignals during inference, outperforms strong ensemble baselines by 1.3 BLEU\npoints on both language families. Further, we find that incorporating\nalternative inputs via self-ensemble can be particularly effective when\ntraining set is small, leading to +5 BLEU when only 5% of the total training\ndata is accessible. Finally, our analysis demonstrates that including\nalternative signals yields more consistency and translates named entities more\naccurately, which is crucial for increased factuality of automated systems.", "published": "2021-10-15 01:56:46", "link": "http://arxiv.org/abs/2110.07804v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DirectQuote: A Dataset for Direct Quotation Extraction and Attribution\n  in News Articles", "abstract": "Quotation extraction and attribution are challenging tasks, aiming at\ndetermining the spans containing quotations and attributing each quotation to\nthe original speaker. Applying this task to news data is highly related to\nfact-checking, media monitoring and news tracking. Direct quotations are more\ntraceable and informative, and therefore of great significance among different\ntypes of quotations. Therefore, this paper introduces DirectQuote, a corpus\ncontaining 19,760 paragraphs and 10,279 direct quotations manually annotated\nfrom online news media. To the best of our knowledge, this is the largest and\nmost complete corpus that focuses on direct quotations in news texts. We ensure\nthat each speaker in the annotation can be linked to a specific named entity on\nWikidata, benefiting various downstream tasks. In addition, for the first time,\nwe propose several sequence labeling models as baseline methods to extract and\nattribute quotations simultaneously in an end-to-end manner.", "published": "2021-10-15 02:50:09", "link": "http://arxiv.org/abs/2110.07827v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Span Detection for Aspect-Based Sentiment Analysis in Vietnamese", "abstract": "Aspect-based sentiment analysis plays an essential role in natural language\nprocessing and artificial intelligence. Recently, researchers only focused on\naspect detection and sentiment classification but ignoring the sub-task of\ndetecting user opinion span, which has enormous potential in practical\napplications. In this paper, we present a new Vietnamese dataset (UIT-ViSD4SA)\nconsisting of 35,396 human-annotated spans on 11,122 feedback comments for\nevaluating the span detection in aspect-based sentiment analysis. Besides, we\nalso propose a novel system using Bidirectional Long Short-Term Memory (BiLSTM)\nwith a Conditional Random Field (CRF) layer (BiLSTM-CRF) for the span detection\ntask in Vietnamese aspect-based sentiment analysis. The best result is a 62.76%\nF1 score (macro) for span detection using BiLSTM-CRF with embedding fusion of\nsyllable embedding, character embedding, and contextual embedding from\nXLM-RoBERTa. In future work, span detection will be extended in many NLP tasks\nsuch as constructive detection, emotion recognition, complaint analysis, and\nopinion mining. Our dataset is freely available at\nhttps://github.com/kimkim00/UIT-ViSD4SA for research purposes.", "published": "2021-10-15 03:13:59", "link": "http://arxiv.org/abs/2110.07833v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Endorsement for Multi-Document Abstractive Summarization", "abstract": "A crucial difference between single- and multi-document summarization is how\nsalient content manifests itself in the document(s). While such content may\nappear at the beginning of a single document, essential information is\nfrequently reiterated in a set of documents related to a particular topic,\nresulting in an endorsement effect that increases information salience. In this\npaper, we model the cross-document endorsement effect and its utilization in\nmultiple document summarization. Our method generates a synopsis from each\ndocument, which serves as an endorser to identify salient content from other\ndocuments. Strongly endorsed text segments are used to enrich a neural\nencoder-decoder model to consolidate them into an abstractive summary. The\nmethod has a great potential to learn from fewer examples to identify salient\ncontent, which alleviates the need for costly retraining when the set of\ndocuments is dynamically adjusted. Through extensive experiments on benchmark\nmulti-document summarization datasets, we demonstrate the effectiveness of our\nproposed method over strong published baselines. Finally, we shed light on\nfuture research directions and discuss broader challenges of this task using a\ncase study.", "published": "2021-10-15 03:55:42", "link": "http://arxiv.org/abs/2110.07844v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "End-to-End Segmentation-based News Summarization", "abstract": "In this paper, we bring a new way of digesting news content by introducing\nthe task of segmenting a news article into multiple sections and generating the\ncorresponding summary to each section. We make two contributions towards this\nnew task. First, we create and make available a dataset, SegNews, consisting of\n27k news articles with sections and aligned heading-style section summaries.\nSecond, we propose a novel segmentation-based language generation model adapted\nfrom pre-trained language models that can jointly segment a document and\nproduce the summary for each section. Experimental results on SegNews\ndemonstrate that our model can outperform several state-of-the-art\nsequence-to-sequence generation models for this new task.", "published": "2021-10-15 04:17:26", "link": "http://arxiv.org/abs/2110.07850v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hierarchical Curriculum Learning for AMR Parsing", "abstract": "Abstract Meaning Representation (AMR) parsing aims to translate sentences to\nsemantic representation with a hierarchical structure, and is recently\nempowered by pretrained sequence-to-sequence models. However, there exists a\ngap between their flat training objective (i.e., equally treats all output\ntokens) and the hierarchical AMR structure, which limits the model\ngeneralization. To bridge this gap, we propose a Hierarchical Curriculum\nLearning (HCL) framework with Structure-level (SC) and Instance-level Curricula\n(IC). SC switches progressively from core to detail AMR semantic elements while\nIC transits from structure-simple to -complex AMR instances during training.\nThrough these two warming-up processes, HCL reduces the difficulty of learning\ncomplex structures, thus the flat model can better adapt to the AMR hierarchy.\nExtensive experiments on AMR2.0, AMR3.0, structure-complex and\nout-of-distribution situations verify the effectiveness of HCL.", "published": "2021-10-15 04:45:15", "link": "http://arxiv.org/abs/2110.07855v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Socially Aware Bias Measurements for Hindi Language Representations", "abstract": "Language representations are efficient tools used across NLP applications,\nbut they are strife with encoded societal biases. These biases are studied\nextensively, but with a primary focus on English language representations and\nbiases common in the context of Western society. In this work, we investigate\nbiases present in Hindi language representations with focuses on caste and\nreligion-associated biases. We demonstrate how biases are unique to specific\nlanguage representations based on the history and culture of the region they\nare widely spoken in, and how the same societal bias (such as binary\ngender-associated biases) is encoded by different words and text spans across\nlanguages. The discoveries of our work highlight the necessity of culture\nawareness and linguistic artifacts when modeling language representations, in\norder to better understand the encoded biases.", "published": "2021-10-15 05:49:15", "link": "http://arxiv.org/abs/2110.07871v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer", "abstract": "There has been growing interest in parameter-efficient methods to apply\npre-trained language models to downstream tasks. Building on the Prompt Tuning\napproach of Lester et al. (2021), which learns task-specific soft prompts to\ncondition a frozen pre-trained model to perform different tasks, we propose a\nnovel prompt-based transfer learning approach called SPoT: Soft Prompt\nTransfer. SPoT first learns a prompt on one or more source tasks and then uses\nit to initialize the prompt for a target task. We show that SPoT significantly\nboosts the performance of Prompt Tuning across many tasks. More remarkably,\nacross all model sizes, SPoT matches or outperforms standard Model Tuning\n(which fine-tunes all model parameters) on the SuperGLUE benchmark, while using\nup to 27,000x fewer task-specific parameters. To understand where SPoT is most\neffective, we conduct a large-scale study on task transferability with 26 NLP\ntasks in 160 combinations, and demonstrate that many tasks can benefit each\nother via prompt transfer. Finally, we propose an efficient retrieval approach\nthat interprets task prompts as task embeddings to identify similar tasks and\npredict the most transferable source tasks for a novel target task.", "published": "2021-10-15 07:35:58", "link": "http://arxiv.org/abs/2110.07904v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Estimating the Level and Direction of Phonetic Dialect Change in the\n  Northern Netherlands", "abstract": "This article reports ongoing investigations into phonetic change of dialect\ngroups in the northern Netherlandic language area, particularly the Frisian and\nLow Saxon dialect groups, which are known to differ in vitality. To achieve\nthis, we combine existing phonetically transcribed corpora with dialectometric\napproaches that allow us to quantify change among older male dialect speakers\nin a real-time framework. A multidimensional variant of the Levenshtein\ndistance, combined with methods that induce realistic phonetic distances\nbetween transcriptions, is used to estimate how much dialect groups have\nchanged between 1990 and 2010, and whether they changed towards Standard Dutch\nor away from it. Our analyses indicate that language change is a slow process\nin this geographical area. Moreover, the Frisian and Groningen dialect groups\nseem to be most stable, while the other Low Saxon varieties (excluding the\nGroningen dialect group) were shown to be most prone to change. We offer\npossible explanations for our findings, while we discuss shortcomings of the\ndata and approach in detail, as well as desiderata for future research.", "published": "2021-10-15 08:00:20", "link": "http://arxiv.org/abs/2110.07918v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unifying Cross-lingual Summarization and Machine Translation with\n  Compression Rate", "abstract": "Cross-Lingual Summarization (CLS) is a task that extracts important\ninformation from a source document and summarizes it into a summary in another\nlanguage. It is a challenging task that requires a system to understand,\nsummarize, and translate at the same time, making it highly related to\nMonolingual Summarization (MS) and Machine Translation (MT). In practice, the\ntraining resources for Machine Translation are far more than that for\ncross-lingual and monolingual summarization. Thus incorporating the Machine\nTranslation corpus into CLS would be beneficial for its performance. However,\nthe present work only leverages a simple multi-task framework to bring Machine\nTranslation in, lacking deeper exploration. In this paper, we propose a novel\ntask, Cross-lingual Summarization with Compression rate (CSC), to benefit\nCross-Lingual Summarization by large-scale Machine Translation corpus. Through\nintroducing compression rate, the information ratio between the source and the\ntarget text, we regard the MT task as a special CLS task with a compression\nrate of 100%. Hence they can be trained as a unified task, sharing knowledge\nmore effectively. However, a huge gap exists between the MT task and the CLS\ntask, where samples with compression rates between 30% and 90% are extremely\nrare. Hence, to bridge these two tasks smoothly, we propose an effective data\naugmentation method to produce document-summary pairs with different\ncompression rates. The proposed method not only improves the performance of the\nCLS task, but also provides controllability to generate summaries in desired\nlengths. Experiments demonstrate that our method outperforms various strong\nbaselines in three cross-lingual summarization datasets. We released our code\nand data at https://github.com/ybai-nlp/CLS_CR.", "published": "2021-10-15 08:31:49", "link": "http://arxiv.org/abs/2110.07936v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tracing Origins: Coreference-aware Machine Reading Comprehension", "abstract": "Machine reading comprehension is a heavily-studied research and test field\nfor evaluating new pre-trained language models (PrLMs) and fine-tuning\nstrategies, and recent studies have enriched the pre-trained language models\nwith syntactic, semantic and other linguistic information to improve the\nperformance of the models. In this paper, we imitate the human reading process\nin connecting the anaphoric expressions and explicitly leverage the coreference\ninformation of the entities to enhance the word embeddings from the pre-trained\nlanguage model, in order to highlight the coreference mentions of the entities\nthat must be identified for coreference-intensive question answering in QUOREF,\na relatively new dataset that is specifically designed to evaluate the\ncoreference-related performance of a model. We use two strategies to fine-tune\na pre-trained language model, namely, placing an additional encoder layer after\na pre-trained language model to focus on the coreference mentions or\nconstructing a relational graph convolutional network to model the coreference\nrelations. We demonstrate that the explicit incorporation of coreference\ninformation in the fine-tuning stage performs better than the incorporation of\nthe coreference information in pre-training a language model.", "published": "2021-10-15 09:28:35", "link": "http://arxiv.org/abs/2110.07961v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transformer-based Multi-task Learning for Disaster Tweet Categorisation", "abstract": "Social media has enabled people to circulate information in a timely fashion,\nthus motivating people to post messages seeking help during crisis situations.\nThese messages can contribute to the situational awareness of emergency\nresponders, who have a need for them to be categorised according to information\ntypes (i.e. the type of aid services the messages are requesting). We introduce\na transformer-based multi-task learning (MTL) technique for classifying\ninformation types and estimating the priority of these messages. We evaluate\nthe effectiveness of our approach with a variety of metrics by submitting runs\nto the TREC Incident Streams (IS) track: a research initiative specifically\ndesigned for disaster tweet classification and prioritisation. The results\ndemonstrate that our approach achieves competitive performance in most metrics\nas compared to other participating runs. Subsequently, we find that an ensemble\napproach combining disparate transformer encoders within our approach helps to\nimprove the overall effectiveness to a significant extent, achieving\nstate-of-the-art performance in almost every metric. We make the code publicly\navailable so that our work can be reproduced and used as a baseline for the\ncommunity for future work in this domain.", "published": "2021-10-15 11:13:46", "link": "http://arxiv.org/abs/2110.08010v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Proficiency with Implicit User Representations", "abstract": "We introduce the problem of proficiency modeling: Given a user's posts on a\nsocial media platform, the task is to identify the subset of posts or topics\nfor which the user has some level of proficiency. This enables the filtering\nand ranking of social media posts on a given topic as per user proficiency.\nUnlike experts on a given topic, proficient users may not have received formal\ntraining and possess years of practical experience, but may be autodidacts,\nhobbyists, and people with sustained interest, enabling them to make genuine\nand original contributions to discourse. While predicting whether a user is an\nexpert on a given topic imposes strong constraints on who is a true positive,\nproficiency modeling implies a graded scoring, relaxing these constraints. Put\nanother way, many active social media users can be assumed to possess, or\neventually acquire, some level of proficiency on topics relevant to their\ncommunity. We tackle proficiency modeling in an unsupervised manner by\nutilizing user embeddings to model engagement with a given topic, as indicated\nby a user's preference for authoring related content. We investigate five\nalternative approaches to model proficiency, ranging from basic ones to an\nadvanced, tailored user modeling approach, applied within two real-world\nbenchmarks for evaluation.", "published": "2021-10-15 11:15:17", "link": "http://arxiv.org/abs/2110.08011v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Crisis Domain Adaptation Using Sequence-to-sequence Transformers", "abstract": "User-generated content (UGC) on social media can act as a key source of\ninformation for emergency responders in crisis situations. However, due to the\nvolume concerned, computational techniques are needed to effectively filter and\nprioritise this content as it arises during emerging events. In the literature,\nthese techniques are trained using annotated content from previous crises. In\nthis paper, we investigate how this prior knowledge can be best leveraged for\nnew crises by examining the extent to which crisis events of a similar type are\nmore suitable for adaptation to new events (cross-domain adaptation). Given the\nrecent successes of transformers in various language processing tasks, we\npropose CAST: an approach for Crisis domain Adaptation leveraging\nSequence-to-sequence Transformers. We evaluate CAST using two major\ncrisis-related message classification datasets. Our experiments show that our\nCAST-based best run without using any target data achieves the state of the art\nperformance in both in-domain and cross-domain contexts. Moreover, CAST is\nparticularly effective in one-to-one cross-domain adaptation when trained with\na larger language model. In many-to-one adaptation where multiple crises are\njointly used as the source domain, CAST further improves its performance. In\naddition, we find that more similar events are more likely to bring better\nadaptation performance whereas fine-tuning using dissimilar events does not\nhelp for adaptation. To aid reproducibility, we open source our code to the\ncommunity.", "published": "2021-10-15 11:19:28", "link": "http://arxiv.org/abs/2110.08015v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Structural Characterization for Dialogue Disentanglement", "abstract": "Tangled multi-party dialogue contexts lead to challenges for dialogue reading\ncomprehension, where multiple dialogue threads flow simultaneously within a\ncommon dialogue record, increasing difficulties in understanding the dialogue\nhistory for both human and machine. Previous studies mainly focus on utterance\nencoding methods with carefully designed features but pay inadequate attention\nto characteristic features of the structure of dialogues. We specially take\nstructure factors into account and design a novel model for dialogue\ndisentangling. Based on the fact that dialogues are constructed on successive\nparticipation and interactions between speakers, we model structural\ninformation of dialogues in two aspects: 1)speaker property that indicates whom\na message is from, and 2) reference dependency that shows whom a message may\nrefer to. The proposed method achieves new state-of-the-art on the Ubuntu IRC\nbenchmark dataset and contributes to dialogue-related comprehension.", "published": "2021-10-15 11:28:43", "link": "http://arxiv.org/abs/2110.08018v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multimodal Emotion-Cause Pair Extraction in Conversations", "abstract": "Emotion cause analysis has received considerable attention in recent years.\nPrevious studies primarily focused on emotion cause extraction from texts in\nnews articles or microblogs. It is also interesting to discover emotions and\ntheir causes in conversations. As conversation in its natural form is\nmultimodal, a large number of studies have been carried out on multimodal\nemotion recognition in conversations, but there is still a lack of work on\nmultimodal emotion cause analysis. In this work, we introduce a new task named\nMultimodal Emotion-Cause Pair Extraction in Conversations, aiming to jointly\nextract emotions and their associated causes from conversations reflected in\nmultiple modalities (text, audio and video). We accordingly construct a\nmultimodal conversational emotion cause dataset, Emotion-Cause-in-Friends,\nwhich contains 9,272 multimodal emotion-cause pairs annotated on 13,509\nutterances in the sitcom Friends. We finally benchmark the task by establishing\na baseline system that incorporates multimodal features for emotion-cause pair\nextraction. Preliminary experimental results demonstrate the potential of\nmultimodal information fusion for discovering both emotions and causes in\nconversations.", "published": "2021-10-15 11:30:24", "link": "http://arxiv.org/abs/2110.08020v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UniDS: A Unified Dialogue System for Chit-Chat and Task-oriented\n  Dialogues", "abstract": "With the advances in deep learning, tremendous progress has been made with\nchit-chat dialogue systems and task-oriented dialogue systems. However, these\ntwo systems are often tackled separately in current methods. To achieve more\nnatural interaction with humans, a dialogue agent needs to be capable of both\nchatting and accomplishing tasks. To this end, we propose a unified dialogue\nsystem (UniDS) with the two aforementioned skills. In particular, we design a\nunified dialogue data schema, compatible for both chit-chat and task-oriented\ndialogues, and we train UniDS with mixed dialogue data from a pretrained\nchit-chat dialogue model. Without adding extra parameters to SOTA baselines,\nUniDS can alternatively handle chit-chat and task-oriented dialogues in a\nunified framework. Experimental results demonstrate that the proposed UniDS\nworks comparably well as the pure chit-chat system, and it outperforms\nstate-of-the-art task-oriented dialogue systems. More importantly, UniDS\nachieves better robustness as it is able to smoothly switch between two types\nof dialogues. These results demonstrate the feasibility and potential of\nbuilding an one-for-all dialogue system.", "published": "2021-10-15 11:56:47", "link": "http://arxiv.org/abs/2110.08032v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Jurassic is (almost) All You Need: Few-Shot Meaning-to-Text Generation\n  for Open-Domain Dialogue", "abstract": "One challenge with open-domain dialogue systems is the need to produce\ntruthful, high-quality responses on any topic. We aim to improve the quality\nand coverage of Athena, an Alexa Prize dialogue system. We experiment with\nfew-shot prompt-based learning, comparing GPT-Neo to Jurassic-1, for the\nmovies, music, TV, sports, and video game domains, both within and\ncross-domain, with different prompt set sizes (2, 3, 10), formats, and meaning\nrepresentations consisting of either sets of WikiData KG triples, or dialogue\nacts. Our evaluation uses BLEURT and human metrics, and shows that with 10-shot\nprompting, Athena-Jurassic's performance is significantly better for coherence\nand semantic accuracy. Experiments with 2-shot cross-domain prompts results in\na huge performance drop for Athena-GPT-Neo, whose semantic accuracy falls to\n0.41, and whose untrue hallucination rate increases to 12%. Experiments with\ndialogue acts for video games show that with 10-shot prompting, both models\nlearn to control dialogue acts, but Athena-Jurassic has significantly higher\ncoherence, and only 4% untrue hallucinations. Our results suggest that\nAthena-Jurassic produces high enough quality outputs to be useful in live\nsystems with real users. To our knowledge, these are the first results\ndemonstrating that few-shot semantic prompt-based learning can create NLGs that\ngeneralize to new domains, and produce high-quality, semantically-controlled,\nconversational responses directly from meaning representations.", "published": "2021-10-15 13:42:25", "link": "http://arxiv.org/abs/2110.08094v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Breaking Down Multilingual Machine Translation", "abstract": "While multilingual training is now an essential ingredient in machine\ntranslation (MT) systems, recent work has demonstrated that it has different\neffects in different multilingual settings, such as many-to-one, one-to-many,\nand many-to-many learning. These training settings expose the encoder and the\ndecoder in a machine translation model with different data distributions. In\nthis paper, we examine how different varieties of multilingual training\ncontribute to learning these two components of the MT model. Specifically, we\ncompare bilingual models with encoders and/or decoders initialized by\nmultilingual training. We show that multilingual training is beneficial to\nencoders in general, while it only benefits decoders for low-resource languages\n(LRLs). We further find the important attention heads for each language pair\nand compare their correlations during inference. Our analysis sheds light on\nhow multilingual translation models work and enables us to propose methods to\nimprove performance by training with highly related languages. Our many-to-one\nmodels for high-resource languages and one-to-many models for LRL outperform\nthe best results reported by Aharoni et al. (2019)", "published": "2021-10-15 14:57:12", "link": "http://arxiv.org/abs/2110.08130v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "mLUKE: The Power of Entity Representations in Multilingual Pretrained\n  Language Models", "abstract": "Recent studies have shown that multilingual pretrained language models can be\neffectively improved with cross-lingual alignment information from Wikipedia\nentities. However, existing methods only exploit entity information in\npretraining and do not explicitly use entities in downstream tasks. In this\nstudy, we explore the effectiveness of leveraging entity representations for\ndownstream cross-lingual tasks. We train a multilingual language model with 24\nlanguages with entity representations and show the model consistently\noutperforms word-based pretrained models in various cross-lingual transfer\ntasks. We also analyze the model and the key insight is that incorporating\nentity representations into the input allows us to extract more\nlanguage-agnostic features. We also evaluate the model with a multilingual\ncloze prompt task with the mLAMA dataset. We show that entity-based prompt\nelicits correct factual knowledge more likely than using only word\nrepresentations. Our source code and pretrained models are available at\nhttps://github.com/studio-ousia/luke.", "published": "2021-10-15 15:28:38", "link": "http://arxiv.org/abs/2110.08151v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Kronecker Decomposition for GPT Compression", "abstract": "GPT is an auto-regressive Transformer-based pre-trained language model which\nhas attracted a lot of attention in the natural language processing (NLP)\ndomain due to its state-of-the-art performance in several downstream tasks. The\nsuccess of GPT is mostly attributed to its pre-training on huge amount of data\nand its large number of parameters (from ~100M to billions of parameters).\nDespite the superior performance of GPT (especially in few-shot or zero-shot\nsetup), this overparameterized nature of GPT can be very prohibitive for\ndeploying this model on devices with limited computational power or memory.\nThis problem can be mitigated using model compression techniques; however,\ncompressing GPT models has not been investigated much in the literature. In\nthis work, we use Kronecker decomposition to compress the linear mappings of\nthe GPT-22 model. Our Kronecker GPT-2 model (KnGPT2) is initialized based on\nthe Kronecker decomposed version of the GPT-2 model and then is undergone a\nvery light pre-training on only a small portion of the training data with\nintermediate layer knowledge distillation (ILKD). Finally, our KnGPT2 is\nfine-tuned on down-stream tasks using ILKD as well. We evaluate our model on\nboth language modeling and General Language Understanding Evaluation benchmark\ntasks and show that with more efficient pre-training and similar number of\nparameters, our KnGPT2 outperforms the existing DistilGPT2 model significantly.", "published": "2021-10-15 15:28:39", "link": "http://arxiv.org/abs/2110.08152v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DYLE: Dynamic Latent Extraction for Abstractive Long-Input Summarization", "abstract": "Transformer-based models have achieved state-of-the-art performance on\nshort-input summarization. However, they still struggle with summarizing longer\ntext. In this paper, we present DYLE, a novel dynamic latent extraction\napproach for abstractive long-input summarization. DYLE jointly trains an\nextractor and a generator and treats the extracted text snippets as the latent\nvariable, allowing dynamic snippet-level attention weights during decoding. To\nprovide adequate supervision, we propose simple yet effective heuristics for\noracle extraction as well as a consistency loss term, which encourages the\nextractor to approximate the averaged dynamic weights predicted by the\ngenerator. We evaluate our method on different long-document and long-dialogue\nsummarization tasks: GovReport, QMSum, and arXiv. Experiment results show that\nDYLE outperforms all existing methods on GovReport and QMSum, with gains up to\n6.1 ROUGE, while yielding strong results on arXiv. Further analysis shows that\nthe proposed dynamic weights provide interpretability of our generation\nprocess.", "published": "2021-10-15 15:53:32", "link": "http://arxiv.org/abs/2110.08168v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rewire-then-Probe: A Contrastive Recipe for Probing Biomedical Knowledge\n  of Pre-trained Language Models", "abstract": "Knowledge probing is crucial for understanding the knowledge transfer\nmechanism behind the pre-trained language models (PLMs). Despite the growing\nprogress of probing knowledge for PLMs in the general domain, specialised areas\nsuch as biomedical domain are vastly under-explored. To catalyse the research\nin this direction, we release a well-curated biomedical knowledge probing\nbenchmark, MedLAMA, which is constructed based on the Unified Medical Language\nSystem (UMLS) Metathesaurus. We test a wide spectrum of state-of-the-art PLMs\nand probing approaches on our benchmark, reaching at most 3% of acc@10. While\nhighlighting various sources of domain-specific challenges that amount to this\nunderwhelming performance, we illustrate that the underlying PLMs have a higher\npotential for probing tasks. To achieve this, we propose Contrastive-Probe, a\nnovel self-supervised contrastive probing approach, that adjusts the underlying\nPLMs without using any probing data. While Contrastive-Probe pushes the acc@10\nto 28%, the performance gap still remains notable. Our human expert evaluation\nsuggests that the probing performance of our Contrastive-Probe is still\nunder-estimated as UMLS still does not include the full spectrum of factual\nknowledge. We hope MedLAMA and Contrastive-Probe facilitate further\ndevelopments of more suited probing techniques for this domain.", "published": "2021-10-15 16:00:11", "link": "http://arxiv.org/abs/2110.08173v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MixQG: Neural Question Generation with Mixed Answer Types", "abstract": "Asking good questions is an essential ability for both human and machine\nintelligence. However, existing neural question generation approaches mainly\nfocus on the short factoid type of answers. In this paper, we propose a neural\nquestion generator, MixQG, to bridge this gap. We combine 9 question answering\ndatasets with diverse answer types, including yes/no, multiple-choice,\nextractive, and abstractive answers, to train a single generative model. We\nshow with empirical results that our model outperforms existing work in both\nseen and unseen domains and can generate questions with different cognitive\nlevels when conditioned on different answer types. Our code is released and\nwell-integrated with the Huggingface library to facilitate various downstream\napplications.", "published": "2021-10-15 16:03:40", "link": "http://arxiv.org/abs/2110.08175v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sparse Progressive Distillation: Resolving Overfitting under\n  Pretrain-and-Finetune Paradigm", "abstract": "Conventional wisdom in pruning Transformer-based language models is that\npruning reduces the model expressiveness and thus is more likely to underfit\nrather than overfit. However, under the trending pretrain-and-finetune\nparadigm, we postulate a counter-traditional hypothesis, that is: pruning\nincreases the risk of overfitting when performed at the fine-tuning phase. In\nthis paper, we aim to address the overfitting problem and improve pruning\nperformance via progressive knowledge distillation with error-bound properties.\nWe show for the first time that reducing the risk of overfitting can help the\neffectiveness of pruning under the pretrain-and-finetune paradigm. Ablation\nstudies and experiments on the GLUE benchmark show that our method outperforms\nthe leading competitors across different tasks.", "published": "2021-10-15 16:42:56", "link": "http://arxiv.org/abs/2110.08190v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Why don't people use character-level machine translation?", "abstract": "We present a literature and empirical survey that critically assesses the\nstate of the art in character-level modeling for machine translation (MT).\nDespite evidence in the literature that character-level systems are comparable\nwith subword systems, they are virtually never used in competitive setups in\nWMT competitions. We empirically show that even with recent modeling\ninnovations in character-level natural language processing, character-level MT\nsystems still struggle to match their subword-based counterparts.\nCharacter-level MT systems show neither better domain robustness, nor better\nmorphological generalization, despite being often so motivated. However, we are\nable to show robustness towards source side noise and that translation quality\ndoes not degrade with increasing beam size at decoding time.", "published": "2021-10-15 16:43:31", "link": "http://arxiv.org/abs/2110.08191v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BBQ: A Hand-Built Bias Benchmark for Question Answering", "abstract": "It is well documented that NLP models learn social biases, but little work\nhas been done on how these biases manifest in model outputs for applied tasks\nlike question answering (QA). We introduce the Bias Benchmark for QA (BBQ), a\ndataset of question sets constructed by the authors that highlight attested\nsocial biases against people belonging to protected classes along nine social\ndimensions relevant for U.S. English-speaking contexts. Our task evaluates\nmodel responses at two levels: (i) given an under-informative context, we test\nhow strongly responses reflect social biases, and (ii) given an adequately\ninformative context, we test whether the model's biases override a correct\nanswer choice. We find that models often rely on stereotypes when the context\nis under-informative, meaning the model's outputs consistently reproduce\nharmful biases in this setting. Though models are more accurate when the\ncontext provides an informative answer, they still rely on stereotypes and\naverage up to 3.4 percentage points higher accuracy when the correct answer\naligns with a social bias than when it conflicts, with this difference widening\nto over 5 points on examples targeting gender for most models tested.", "published": "2021-10-15 16:43:46", "link": "http://arxiv.org/abs/2110.08193v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DialFact: A Benchmark for Fact-Checking in Dialogue", "abstract": "Fact-checking is an essential tool to mitigate the spread of misinformation\nand disinformation. We introduce the task of fact-checking in dialogue, which\nis a relatively unexplored area. We construct DialFact, a testing benchmark\ndataset of 22,245 annotated conversational claims, paired with pieces of\nevidence from Wikipedia. There are three sub-tasks in DialFact: 1) Verifiable\nclaim detection task distinguishes whether a response carries verifiable\nfactual information; 2) Evidence retrieval task retrieves the most relevant\nWikipedia snippets as evidence; 3) Claim verification task predicts a dialogue\nresponse to be supported, refuted, or not enough information. We found that\nexisting fact-checking models trained on non-dialogue data like FEVER fail to\nperform well on our task, and thus, we propose a simple yet data-efficient\nsolution to effectively improve fact-checking performance in dialogue. We point\nout unique challenges in DialFact such as handling the colloquialisms,\ncoreferences and retrieval ambiguities in the error analysis to shed light on\nfuture research in this direction.", "published": "2021-10-15 17:34:35", "link": "http://arxiv.org/abs/2110.08222v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tricks for Training Sparse Translation Models", "abstract": "Multi-task learning with an unbalanced data distribution skews model learning\ntowards high resource tasks, especially when model capacity is fixed and fully\nshared across all tasks. Sparse scaling architectures, such as BASELayers,\nprovide flexible mechanisms for different tasks to have a variable number of\nparameters, which can be useful to counterbalance skewed data distributions. We\nfind that that sparse architectures for multilingual machine translation can\nperform poorly out of the box, and propose two straightforward techniques to\nmitigate this - a temperature heating mechanism and dense pre-training.\nOverall, these methods improve performance on two multilingual translation\nbenchmarks compared to standard BASELayers and Dense scaling baselines, and in\ncombination, more than 2x model convergence speed.", "published": "2021-10-15 17:58:45", "link": "http://arxiv.org/abs/2110.08246v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Coherence boosting: When your pretrained language model is not paying\n  enough attention", "abstract": "Long-range semantic coherence remains a challenge in automatic language\ngeneration and understanding. We demonstrate that large language models have\ninsufficiently learned the effect of distant words on next-token prediction. We\npresent coherence boosting, an inference procedure that increases a LM's focus\non a long context. We show the benefits of coherence boosting with pretrained\nmodels by distributional analyses of generated ordinary text and dialog\nresponses. It is also found that coherence boosting with state-of-the-art\nmodels for various zero-shot NLP tasks yields performance gains with no\nadditional training.", "published": "2021-10-15 18:05:33", "link": "http://arxiv.org/abs/2110.08294v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ASPECTNEWS: Aspect-Oriented Summarization of News Documents", "abstract": "Generic summaries try to cover an entire document and query-based summaries\ntry to answer document-specific questions. But real users' needs often fall in\nbetween these extremes and correspond to aspects, high-level topics discussed\namong similar types of documents. In this paper, we collect a dataset of\nrealistic aspect-oriented summaries, AspectNews, which covers different\nsubtopics about articles in news sub-domains. We annotate data across two\ndomains of articles, earthquakes and fraud investigations, where each article\nis annotated with two distinct summaries focusing on different aspects for each\ndomain. A system producing a single generic summary cannot concisely satisfy\nboth aspects. Our focus in evaluation is how well existing techniques can\ngeneralize to these domains without seeing in-domain training data, so we turn\nto techniques to construct synthetic training data that have been used in\nquery-focused summarization work. We compare several training schemes that\ndiffer in how strongly keywords are used and how oracle summaries are\nextracted. Our evaluation shows that our final approach yields (a) focused\nsummaries, better than those from a generic summarization system or from\nkeyword matching; (b) a system sensitive to the choice of keywords.", "published": "2021-10-15 18:06:21", "link": "http://arxiv.org/abs/2110.08296v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Clean or Annotate: How to Spend a Limited Data Collection Budget", "abstract": "Crowdsourcing platforms are often used to collect datasets for training\nmachine learning models, despite higher levels of inaccurate labeling compared\nto expert labeling. There are two common strategies to manage the impact of\nsuch noise. The first involves aggregating redundant annotations, but comes at\nthe expense of labeling substantially fewer examples. Secondly, prior works\nhave also considered using the entire annotation budget to label as many\nexamples as possible and subsequently apply denoising algorithms to implicitly\nclean the dataset. We find a middle ground and propose an approach which\nreserves a fraction of annotations to explicitly clean up highly probable error\nsamples to optimize the annotation process. In particular, we allocate a large\nportion of the labeling budget to form an initial dataset used to train a\nmodel. This model is then used to identify specific examples that appear most\nlikely to be incorrect, which we spend the remaining budget to relabel.\nExperiments across three model variations and four natural language processing\ntasks show our approach outperforms or matches both label aggregation and\nadvanced denoising methods designed to handle noisy labels when allocated the\nsame finite annotation budget.", "published": "2021-10-15 20:37:29", "link": "http://arxiv.org/abs/2110.08355v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Training Dynamics for Text Summarization Models", "abstract": "Pre-trained language models (e.g. BART) have shown impressive results when\nfine-tuned on large summarization datasets. However, little is understood about\nthis fine-tuning process, including what knowledge is retained from\npre-training time or how content selection and generation strategies are learnt\nacross iterations. In this work, we analyze the training dynamics for\ngeneration models, focusing on summarization. Across different datasets\n(CNN/DM, XSum, MediaSum) and summary properties, such as abstractiveness and\nhallucination, we study what the model learns at different stages of its\nfine-tuning process. We find that a propensity to copy the input is learned\nearly in the training process consistently across all datasets studied. On the\nother hand, factual errors, such as hallucination of unsupported facts, are\nlearnt in the later stages, though this behavior is more varied across domains.\nBased on these observations, we explore complementary approaches for modifying\ntraining: first, disregarding high-loss tokens that are challenging to learn\nand second, disregarding low-loss tokens that are learnt very quickly in the\nlatter stages of the training process. We show that these simple training\nmodifications allow us to configure our model to achieve different goals, such\nas improving factuality or improving abstractiveness.", "published": "2021-10-15 21:13:41", "link": "http://arxiv.org/abs/2110.08370v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On The Ingredients of an Effective Zero-shot Semantic Parser", "abstract": "Semantic parsers map natural language utterances into meaning representations\n(e.g., programs). Such models are typically bottlenecked by the paucity of\ntraining data due to the required laborious annotation efforts. Recent studies\nhave performed zero-shot learning by synthesizing training examples of\ncanonical utterances and programs from a grammar, and further paraphrasing\nthese utterances to improve linguistic diversity. However, such synthetic\nexamples cannot fully capture patterns in real data. In this paper we analyze\nzero-shot parsers through the lenses of the language and logical gaps (Herzig\nand Berant, 2019), which quantify the discrepancy of language and programmatic\npatterns between the canonical examples and real-world user-issued ones. We\npropose bridging these gaps using improved grammars, stronger paraphrasers, and\nefficient learning methods using canonical examples that most likely reflect\nreal user intents. Our model achieves strong performance on two semantic\nparsing benchmarks (Scholar, Geo) with zero labeled data.", "published": "2021-10-15 21:41:16", "link": "http://arxiv.org/abs/2110.08381v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Training Conversational Agents with Generative Conversational Networks", "abstract": "Rich, open-domain textual data available on the web resulted in great\nadvancements for language processing. However, while that data may be suitable\nfor language processing tasks, they are mostly non-conversational, lacking many\nphenomena that appear in human interactions and this is one of the reasons why\nwe still have many unsolved challenges in conversational AI. In this work, we\nattempt to address this by using Generative Conversational Networks to\nautomatically generate data and train social conversational agents. We evaluate\nour approach on TopicalChat with automatic metrics and human evaluators,\nshowing that with 10% of seed data it performs close to the baseline that uses\n100% of the data.", "published": "2021-10-15 21:46:39", "link": "http://arxiv.org/abs/2110.08383v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generated Knowledge Prompting for Commonsense Reasoning", "abstract": "It remains an open question whether incorporating external knowledge benefits\ncommonsense reasoning while maintaining the flexibility of pretrained sequence\nmodels. To investigate this question, we develop generated knowledge prompting,\nwhich consists of generating knowledge from a language model, then providing\nthe knowledge as additional input when answering a question. Our method does\nnot require task-specific supervision for knowledge integration, or access to a\nstructured knowledge base, yet it improves performance of large-scale,\nstate-of-the-art models on four commonsense reasoning tasks, achieving\nstate-of-the-art results on numerical commonsense (NumerSense), general\ncommonsense (CommonsenseQA 2.0), and scientific commonsense (QASC) benchmarks.\nGenerated knowledge prompting highlights large-scale language models as\nflexible sources of external knowledge for improving commonsense reasoning. Our\ncode is available at https://github.com/liujch1998/GKP", "published": "2021-10-15 21:58:03", "link": "http://arxiv.org/abs/2110.08387v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Probing as Quantifying Inductive Bias", "abstract": "Pre-trained contextual representations have led to dramatic performance\nimprovements on a range of downstream tasks. Such performance improvements have\nmotivated researchers to quantify and understand the linguistic information\nencoded in these representations. In general, researchers quantify the amount\nof linguistic information through probing, an endeavor which consists of\ntraining a supervised model to predict a linguistic property directly from the\ncontextual representations. Unfortunately, this definition of probing has been\nsubject to extensive criticism in the literature, and has been observed to lead\nto paradoxical and counter-intuitive results. In the theoretical portion of\nthis paper, we take the position that the goal of probing ought to be measuring\nthe amount of inductive bias that the representations encode on a specific\ntask. We further describe a Bayesian framework that operationalizes this goal\nand allows us to quantify the representations' inductive bias. In the empirical\nportion of the paper, we apply our framework to a variety of NLP tasks. Our\nresults suggest that our proposed framework alleviates many previous problems\nfound in probing. Moreover, we are able to offer concrete evidence that -- for\nsome tasks -- fastText can offer a better inductive bias than BERT.", "published": "2021-10-15 22:01:16", "link": "http://arxiv.org/abs/2110.08388v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DS-TOD: Efficient Domain Specialization for Task Oriented Dialog", "abstract": "Recent work has shown that self-supervised dialog-specific pretraining on\nlarge conversational datasets yields substantial gains over traditional\nlanguage modeling (LM) pretraining in downstream task-oriented dialog (TOD).\nThese approaches, however, exploit general dialogic corpora (e.g., Reddit) and\nthus presumably fail to reliably embed domain-specific knowledge useful for\nconcrete downstream TOD domains. In this work, we investigate the effects of\ndomain specialization of pretrained language models (PLMs) for TOD. Within our\nDS-TOD framework, we first automatically extract salient domain-specific terms,\nand then use them to construct DomainCC and DomainReddit -- resources that we\nleverage for domain-specific pretraining, based on (i) masked language modeling\n(MLM) and (ii) response selection (RS) objectives, respectively. We further\npropose a resource-efficient and modular domain specialization by means of\ndomain adapters -- additional parameter-light layers in which we encode the\ndomain knowledge. Our experiments with prominent TOD tasks -- dialog state\ntracking (DST) and response retrieval (RR) -- encompassing five domains from\nthe MultiWOZ benchmark demonstrate the effectiveness of DS-TOD. Moreover, we\nshow that the light-weight adapter-based specialization (1) performs comparably\nto full fine-tuning in single domain setups and (2) is particularly suitable\nfor multi-domain specialization, where besides advantageous computational\nfootprint, it can offer better TOD performance.", "published": "2021-10-15 22:25:51", "link": "http://arxiv.org/abs/2110.08395v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Named Entity Recognition in Unstructured Medical Text Documents", "abstract": "Physicians provide expert opinion to legal courts on the medical state of\npatients, including determining if a patient is likely to have permanent or\nnon-permanent injuries or ailments. An independent medical examination (IME)\nreport summarizes a physicians medical opinion about a patients health status\nbased on the physicians expertise. IME reports contain private and sensitive\ninformation (Personally Identifiable Information or PII) that needs to be\nremoved or randomly encoded before further research work can be conducted. In\nour study the IME is an orthopedic surgeon from a private practice in the\nUnited States. The goal of this research is to perform named entity recognition\n(NER) to identify and subsequently remove/encode PII information from IME\nreports prepared by the physician. We apply the NER toolkits of OpenNLP and\nspaCy, two freely available natural language processing platforms, and compare\ntheir precision, recall, and f-measure performance at identifying five\ncategories of PII across trials of randomly selected IME reports using each\nmodels common default parameters. We find that both platforms achieve high\nperformance (f-measure > 0.9) at de-identification and that a spaCy model\ntrained with a 70-30 train-test data split is most performant.", "published": "2021-10-15 03:03:09", "link": "http://arxiv.org/abs/2110.15732v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Attacking Open-domain Question Answering by Injecting Misinformation", "abstract": "With a rise in false, inaccurate, and misleading information in propaganda,\nnews, and social media, real-world Question Answering (QA) systems face the\nchallenges of synthesizing and reasoning over misinformation-polluted contexts\nto derive correct answers. This urgency gives rise to the need to make QA\nsystems robust to misinformation, a topic previously unexplored. We study the\nrisk of misinformation to QA models by investigating the sensitivity of\nopen-domain QA models to corpus pollution with misinformation documents. We\ncurate both human-written and model-generated false documents that we inject\ninto the evidence corpus of QA models and assess the impact on the performance\nof these systems. Experiments show that QA models are vulnerable to even small\namounts of evidence contamination brought by misinformation, with large\nabsolute performance drops on all models. Misinformation attack brings more\nthreat when fake documents are produced at scale by neural models or the\nattacker targets hacking specific questions of interest. To defend against such\na threat, we discuss the necessity of building a misinformation-aware QA system\nthat integrates question-answering and misinformation detection in a joint\nfashion.", "published": "2021-10-15 01:55:18", "link": "http://arxiv.org/abs/2110.07803v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Cascaded Fast and Slow Models for Efficient Semantic Code Search", "abstract": "The goal of natural language semantic code search is to retrieve a\nsemantically relevant code snippet from a fixed set of candidates using a\nnatural language query. Existing approaches are neither effective nor efficient\nenough towards a practical semantic code search system. In this paper, we\npropose an efficient and accurate semantic code search framework with cascaded\nfast and slow models, in which a fast transformer encoder model is learned to\noptimize a scalable index for fast retrieval followed by learning a slow\nclassification-based re-ranking model to improve the performance of the top K\nresults from the fast retrieval. To further reduce the high memory cost of\ndeploying two separate models in practice, we propose to jointly train the fast\nand slow model based on a single transformer encoder with shared parameters.\nThe proposed cascaded approach is not only efficient and scalable, but also\nachieves state-of-the-art results with an average mean reciprocal ranking (MRR)\nscore of 0.7795 (across 6 programming languages) as opposed to the previous\nstate-of-the-art result of 0.713 MRR on the CodeSearchNet benchmark.", "published": "2021-10-15 02:23:35", "link": "http://arxiv.org/abs/2110.07811v1", "categories": ["cs.CL", "cs.PL"], "primary_category": "cs.CL"}
{"title": "Meta-learning via Language Model In-context Tuning", "abstract": "The goal of meta-learning is to learn to adapt to a new task with only a few\nlabeled examples. To tackle this problem in NLP, we propose $\\textit{in-context\ntuning}$, which recasts adaptation and prediction as a simple sequence\nprediction problem: to form the input sequence, we concatenate the task\ninstruction, the labeled examples, and the target input to predict; to\nmeta-train the model to learn from in-context examples, we fine-tune a\npre-trained language model (LM) to predict the target label from the input\nsequences on a collection of tasks.\n  We benchmark our method on two collections of text classification tasks: LAMA\nand BinaryClfs. Compared to first-order MAML which adapts the model with\ngradient descent, our method better leverages the inductive bias of LMs to\nperform pattern matching, and outperforms MAML by an absolute $6\\%$ AUC ROC\nscore on BinaryClfs, with increasing advantage w.r.t. model size. Compared to\nnon-fine-tuned in-context learning (i.e. prompting a raw LM), in-context tuning\ndirectly learns to learn from in-context examples. On BinaryClfs, in-context\ntuning improves the average AUC-ROC score by an absolute $10\\%$, and reduces\nthe variance with respect to example ordering by 6x and example choices by 2x.", "published": "2021-10-15 02:29:09", "link": "http://arxiv.org/abs/2110.07814v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multilingual Neural Machine Translation:Can Linguistic Hierarchies Help?", "abstract": "Multilingual Neural Machine Translation (MNMT) trains a single NMT model that\nsupports translation between multiple languages, rather than training separate\nmodels for different languages. Learning a single model can enhance the\nlow-resource translation by leveraging data from multiple languages. However,\nthe performance of an MNMT model is highly dependent on the type of languages\nused in training, as transferring knowledge from a diverse set of languages\ndegrades the translation performance due to negative transfer. In this paper,\nwe propose a Hierarchical Knowledge Distillation (HKD) approach for MNMT which\ncapitalises on language groups generated according to typological features and\nphylogeny of languages to overcome the issue of negative transfer. HKD\ngenerates a set of multilingual teacher-assistant models via a selective\nknowledge distillation mechanism based on the language groups, and then distils\nthe ultimate multilingual model from those assistants in an adaptive way.\nExperimental results derived from the TED dataset with 53 languages demonstrate\nthe effectiveness of our approach in avoiding the negative transfer effect in\nMNMT, leading to an improved translation performance (about 1 BLEU score on\naverage) compared to strong baselines.", "published": "2021-10-15 02:31:48", "link": "http://arxiv.org/abs/2110.07816v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RAP: Robustness-Aware Perturbations for Defending against Backdoor\n  Attacks on NLP Models", "abstract": "Backdoor attacks, which maliciously control a well-trained model's outputs of\nthe instances with specific triggers, are recently shown to be serious threats\nto the safety of reusing deep neural networks (DNNs). In this work, we propose\nan efficient online defense mechanism based on robustness-aware perturbations.\nSpecifically, by analyzing the backdoor training process, we point out that\nthere exists a big gap of robustness between poisoned and clean samples.\nMotivated by this observation, we construct a word-based robustness-aware\nperturbation to distinguish poisoned samples from clean samples to defend\nagainst the backdoor attacks on natural language processing (NLP) models.\nMoreover, we give a theoretical analysis about the feasibility of our\nrobustness-aware perturbation-based defense method. Experimental results on\nsentiment analysis and toxic detection tasks show that our method achieves\nbetter defending performance and much lower computational costs than existing\nonline defense methods. Our code is available at\nhttps://github.com/lancopku/RAP.", "published": "2021-10-15 03:09:26", "link": "http://arxiv.org/abs/2110.07831v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cross-Lingual Fine-Grained Entity Typing", "abstract": "The growth of cross-lingual pre-trained models has enabled NLP tools to\nrapidly generalize to new languages. While these models have been applied to\ntasks involving entities, their ability to explicitly predict typological\nfeatures of these entities across languages has not been established. In this\npaper, we present a unified cross-lingual fine-grained entity typing model\ncapable of handling over 100 languages and analyze this model's ability to\ngeneralize to languages and entities unseen during training. We train this\nmodel on cross-lingual training data collected from Wikipedia hyperlinks in\nmultiple languages (training languages). During inference, our model takes an\nentity mention and context in a particular language (test language, possibly\nnot in the training languages) and predicts fine-grained types for that entity.\nGeneralizing to new languages and unseen entities are the fundamental\nchallenges of this entity typing setup, so we focus our evaluation on these\nsettings and compare against simple yet powerful string match baselines.\nExperimental results show that our approach outperforms the baselines on unseen\nlanguages such as Japanese, Tamil, Arabic, Serbian, and Persian. In addition,\nour approach substantially improves performance on unseen entities (even in\nunseen languages) over the baselines, and human evaluation shows a strong\nability to predict relevant types in these settings.", "published": "2021-10-15 03:22:30", "link": "http://arxiv.org/abs/2110.07837v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploring Universal Intrinsic Task Subspace via Prompt Tuning", "abstract": "Why can pre-trained language models (PLMs) learn universal representations\nand effectively adapt to broad NLP tasks differing a lot superficially? In this\nwork, we empirically find evidence indicating that the adaptations of PLMs to\nvarious few-shot tasks can be reparameterized as optimizing only a few free\nparameters in a unified low-dimensional intrinsic task subspace, which may help\nus understand why PLMs could easily adapt to various NLP tasks with small-scale\ndata. To find such a subspace and examine its universality, we propose an\nanalysis pipeline called intrinsic prompt tuning (IPT). Specifically, we resort\nto the recent success of prompt tuning and decompose the soft prompts of\nmultiple NLP tasks into the same low-dimensional nonlinear subspace, then we\nlearn to adapt the PLM to unseen data or tasks by only tuning parameters in\nthis subspace. In the experiments, we study diverse few-shot NLP tasks and\nsurprisingly find that in a 250-dimensional subspace found with 100 tasks, by\nonly tuning 250 free parameters, we can recover 97% and 83% of the full prompt\ntuning performance for 100 seen tasks (using different training data) and 20\nunseen tasks, respectively, showing great generalization ability of the found\nintrinsic task subspace. Besides being an analysis tool, IPT could further help\nus improve the prompt tuning stability.", "published": "2021-10-15 05:43:59", "link": "http://arxiv.org/abs/2110.07867v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multilingual Speech Recognition using Knowledge Transfer across Learning\n  Processes", "abstract": "Multilingual end-to-end(E2E) models have shown a great potential in the\nexpansion of the language coverage in the realm of automatic speech\nrecognition(ASR). In this paper, we aim to enhance the multilingual ASR\nperformance in two ways, 1)studying the impact of feeding a one-hot vector\nidentifying the language, 2)formulating the task with a meta-learning objective\ncombined with self-supervised learning (SSL). We associate every language with\na distinct task manifold and attempt to improve the performance by transferring\nknowledge across learning processes itself as compared to transferring through\nfinal model parameters. We employ this strategy on a dataset comprising of 6\nlanguages for an in-domain ASR task, by minimizing an objective related to\nexpected gradient path length. Experimental results reveal the best\npre-training strategy resulting in 3.55% relative reduction in overall WER. A\ncombination of LEAP and SSL yields 3.51% relative reduction in overall WER when\nusing language ID.", "published": "2021-10-15 07:50:27", "link": "http://arxiv.org/abs/2110.07909v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Identifying Causal Influences on Publication Trends and Behavior: A Case\n  Study of the Computational Linguistics Community", "abstract": "Drawing causal conclusions from observational real-world data is a very much\ndesired but challenging task. In this paper we present mixed-method analyses to\ninvestigate causal influences of publication trends and behavior on the\nadoption, persistence, and retirement of certain research foci --\nmethodologies, materials, and tasks that are of interest to the computational\nlinguistics (CL) community. Our key findings highlight evidence of the\ntransition to rapidly emerging methodologies in the research community (e.g.,\nadoption of bidirectional LSTMs influencing the retirement of LSTMs), the\npersistent engagement with trending tasks and techniques (e.g., deep learning,\nembeddings, generative, and language models), the effect of scientist location\nfrom outside the US, e.g., China on propensity of researching languages beyond\nEnglish, and the potential impact of funding for large-scale research programs.\nWe anticipate this work to provide useful insights about publication trends and\nbehavior and raise the awareness about the potential for causal inference in\nthe computational linguistics and a broader scientific community.", "published": "2021-10-15 08:36:13", "link": "http://arxiv.org/abs/2110.07938v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Generating Natural Language Adversarial Examples through An Improved\n  Beam Search Algorithm", "abstract": "The research of adversarial attacks in the text domain attracts many\ninterests in the last few years, and many methods with a high attack success\nrate have been proposed. However, these attack methods are inefficient as they\nrequire lots of queries for the victim model when crafting text adversarial\nexamples. In this paper, a novel attack model is proposed, its attack success\nrate surpasses the benchmark attack methods, but more importantly, its attack\nefficiency is much higher than the benchmark attack methods. The novel method\nis empirically evaluated by attacking WordCNN, LSTM, BiLSTM, and BERT on four\nbenchmark datasets. For instance, it achieves a 100\\% attack success rate\nhigher than the state-of-the-art method when attacking BERT and BiLSTM on IMDB,\nbut the number of queries for the victim models only is 1/4 and 1/6.5 of the\nstate-of-the-art method, respectively. Also, further experiments show the novel\nmethod has a good transferability on the generated adversarial examples.", "published": "2021-10-15 12:09:04", "link": "http://arxiv.org/abs/2110.08036v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Few-Shot Bot: Prompt-Based Learning for Dialogue Systems", "abstract": "Learning to converse using only a few examples is a great challenge in\nconversational AI. The current best conversational models, which are either\ngood chit-chatters (e.g., BlenderBot) or goal-oriented systems (e.g., MinTL),\nare language models (LMs) fine-tuned on large conversational datasets. Training\nthese models is expensive, both in terms of computational resources and time,\nand it is hard to keep them up to date with new conversational skills. A simple\nyet unexplored solution is prompt-based few-shot learning (Brown et al. 2020)\nwhich does not require gradient-based fine-tuning but instead uses a few\nexamples in the LM context as the only source of learning. In this paper, we\nexplore prompt-based few-shot learning in dialogue tasks. We benchmark LMs of\ndifferent sizes in nine response generation tasks, which include four\nknowledge-grounded tasks, a task-oriented generations task, three open-chat\ntasks, and controlled stylistic generation, and five conversational parsing\ntasks, which include dialogue state tracking, graph path generation, persona\ninformation extraction, document retrieval, and internet query generation. The\ncurrent largest released LM (GPT-J-6B) using prompt-based few-shot learning,\nand thus requiring no training, achieves competitive performance to fully\ntrained state-of-the-art models. Moreover, we propose a novel prompt-based\nfew-shot classifier, that also does not require any fine-tuning, to select the\nmost appropriate prompt given a dialogue history. Finally, by combining the\npower of prompt-based few-shot learning and a Skill Selector, we create an\nend-to-end chatbot named the Few-Shot Bot (FSB), which automatically selects\nthe most appropriate conversational skill, queries different knowledge bases or\nthe internet, and uses the retrieved knowledge to generate a human-like\nresponse, all using only few dialogue examples per skill.", "published": "2021-10-15 14:36:45", "link": "http://arxiv.org/abs/2110.08118v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "milIE: Modular & Iterative Multilingual Open Information Extraction", "abstract": "Open Information Extraction (OpenIE) is the task of extracting (subject,\npredicate, object) triples from natural language sentences. Current OpenIE\nsystems extract all triple slots independently. In contrast, we explore the\nhypothesis that it may be beneficial to extract triple slots iteratively: first\nextract easy slots, followed by the difficult ones by conditioning on the easy\nslots, and therefore achieve a better overall extraction. Based on this\nhypothesis, we propose a neural OpenIE system, milIE, that operates in an\niterative fashion. Due to the iterative nature, the system is also modular --\nit is possible to seamlessly integrate rule based extraction systems with a\nneural end-to-end system, thereby allowing rule based systems to supply\nextraction slots which milIE can leverage for extracting the remaining slots.\nWe confirm our hypothesis empirically: milIE outperforms SOTA systems on\nmultiple languages ranging from Chinese to Arabic. Additionally, we are the\nfirst to provide an OpenIE test dataset for Arabic and Galician.", "published": "2021-10-15 15:19:11", "link": "http://arxiv.org/abs/2110.08144v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The World of an Octopus: How Reporting Bias Influences a Language\n  Model's Perception of Color", "abstract": "Recent work has raised concerns about the inherent limitations of text-only\npretraining. In this paper, we first demonstrate that reporting bias, the\ntendency of people to not state the obvious, is one of the causes of this\nlimitation, and then investigate to what extent multimodal training can\nmitigate this issue. To accomplish this, we 1) generate the Color Dataset\n(CoDa), a dataset of human-perceived color distributions for 521 common\nobjects; 2) use CoDa to analyze and compare the color distribution found in\ntext, the distribution captured by language models, and a human's perception of\ncolor; and 3) investigate the performance differences between text-only and\nmultimodal models on CoDa. Our results show that the distribution of colors\nthat a language model recovers correlates more strongly with the inaccurate\ndistribution found in text than with the ground-truth, supporting the claim\nthat reporting bias negatively impacts and inherently limits text-only\ntraining. We then demonstrate that multimodal models can leverage their visual\ntraining to mitigate these effects, providing a promising avenue for future\nresearch.", "published": "2021-10-15 16:28:17", "link": "http://arxiv.org/abs/2110.08182v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Multitask Prompted Training Enables Zero-Shot Task Generalization", "abstract": "Large language models have recently been shown to attain reasonable zero-shot\ngeneralization on a diverse set of tasks (Brown et al., 2020). It has been\nhypothesized that this is a consequence of implicit multitask learning in\nlanguage models' pretraining (Radford et al., 2019). Can zero-shot\ngeneralization instead be directly induced by explicit multitask learning? To\ntest this question at scale, we develop a system for easily mapping any natural\nlanguage tasks into a human-readable prompted form. We convert a large set of\nsupervised datasets, each with multiple prompts with diverse wording. These\nprompted datasets allow for benchmarking the ability of a model to perform\ncompletely held-out tasks. We fine-tune a pretrained encoder-decoder model\n(Raffel et al., 2020; Lester et al., 2021) on this multitask mixture covering a\nwide variety of tasks. The model attains strong zero-shot performance on\nseveral standard datasets, often outperforming models up to 16x its size.\nFurther, our approach attains strong performance on a subset of tasks from the\nBIG-bench benchmark, outperforming models up to 6x its size. All trained models\nare available at https://github.com/bigscience-workshop/t-zero and all prompts\nare available at https://github.com/bigscience-workshop/promptsource.", "published": "2021-10-15 17:08:57", "link": "http://arxiv.org/abs/2110.08207v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Cross-Domain Data Integration for Named Entity Disambiguation in\n  Biomedical Text", "abstract": "Named entity disambiguation (NED), which involves mapping textual mentions to\nstructured entities, is particularly challenging in the medical domain due to\nthe presence of rare entities. Existing approaches are limited by the presence\nof coarse-grained structural resources in biomedical knowledge bases as well as\nthe use of training datasets that provide low coverage over uncommon resources.\nIn this work, we address these issues by proposing a cross-domain data\nintegration method that transfers structural knowledge from a general text\nknowledge base to the medical domain. We utilize our integration scheme to\naugment structural resources and generate a large biomedical NED dataset for\npretraining. Our pretrained model with injected structural knowledge achieves\nstate-of-the-art performance on two benchmark medical NED datasets: MedMentions\nand BC5CDR. Furthermore, we improve disambiguation of rare entities by up to 57\naccuracy points.", "published": "2021-10-15 17:38:16", "link": "http://arxiv.org/abs/2110.08228v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Intent-based Product Collections for E-commerce using Pretrained\n  Language Models", "abstract": "Building a shopping product collection has been primarily a human job. With\nthe manual efforts of craftsmanship, experts collect related but diverse\nproducts with common shopping intent that are effective when displayed\ntogether, e.g., backpacks, laptop bags, and messenger bags for freshman bag\ngifts. Automatically constructing a collection requires an ML system to learn a\ncomplex relationship between the customer's intent and the product's\nattributes. However, there have been challenging points, such as 1) long and\ncomplicated intent sentences, 2) rich and diverse product attributes, and 3) a\nhuge semantic gap between them, making the problem difficult. In this paper, we\nuse a pretrained language model (PLM) that leverages textual attributes of\nweb-scale products to make intent-based product collections. Specifically, we\ntrain a BERT with triplet loss by setting an intent sentence to an anchor and\ncorresponding products to positive examples. Also, we improve the performance\nof the model by search-based negative sampling and category-wise positive pair\naugmentation. Our model significantly outperforms the search-based baseline\nmodel for intent-based product matching in offline evaluations. Furthermore,\nonline experimental results on our e-commerce platform show that the PLM-based\nmethod can construct collections of products with increased CTR, CVR, and\norder-diversity compared to expert-crafted collections.", "published": "2021-10-15 17:52:42", "link": "http://arxiv.org/abs/2110.08241v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "From Multimodal to Unimodal Attention in Transformers using Knowledge\n  Distillation", "abstract": "Multimodal Deep Learning has garnered much interest, and transformers have\ntriggered novel approaches, thanks to the cross-attention mechanism. Here we\npropose an approach to deal with two key existing challenges: the high\ncomputational resource demanded and the issue of missing modalities. We\nintroduce for the first time the concept of knowledge distillation in\ntransformers to use only one modality at inference time. We report a full study\nanalyzing multiple student-teacher configurations, levels at which distillation\nis applied, and different methodologies. With the best configuration, we\nimproved the state-of-the-art accuracy by 3%, we reduced the number of\nparameters by 2.5 times and the inference time by 22%. Such\nperformance-computation tradeoff can be exploited in many applications and we\naim at opening a new research area where the deployment of complex models with\nlimited resources is demanded.", "published": "2021-10-15 12:30:21", "link": "http://arxiv.org/abs/2110.08270v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "The Dangers of Underclaiming: Reasons for Caution When Reporting How NLP\n  Systems Fail", "abstract": "Researchers in NLP often frame and discuss research results in ways that\nserve to deemphasize the field's successes, often in response to the field's\nwidespread hype. Though well-meaning, this has yielded many misleading or false\nclaims about the limits of our best technology. This is a problem, and it may\nbe more serious than it looks: It harms our credibility in ways that can make\nit harder to mitigate present-day harms, like those involving biased systems\nfor content moderation or resume screening. It also limits our ability to\nprepare for the potentially enormous impacts of more distant future advances.\nThis paper urges researchers to be careful about these claims and suggests some\nresearch directions and communication strategies that will make it easier to\navoid or rebut them.", "published": "2021-10-15 18:19:19", "link": "http://arxiv.org/abs/2110.08300v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On Learning the Transformer Kernel", "abstract": "In this work we introduce KERNELIZED TRANSFORMER, a generic, scalable, data\ndriven framework for learning the kernel function in Transformers. Our\nframework approximates the Transformer kernel as a dot product between spectral\nfeature maps and learns the kernel by learning the spectral distribution. This\nnot only helps in learning a generic kernel end-to-end, but also reduces the\ntime and space complexity of Transformers from quadratic to linear. We show\nthat KERNELIZED TRANSFORMERS achieve performance comparable to existing\nefficient Transformer architectures, both in terms of accuracy as well as\ncomputational efficiency. Our study also demonstrates that the choice of the\nkernel has a substantial impact on performance, and kernel learning variants\nare competitive alternatives to fixed kernel Transformers, both in long as well\nas short sequence tasks.", "published": "2021-10-15 19:20:25", "link": "http://arxiv.org/abs/2110.08323v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Towards Transparent Interactive Semantic Parsing via Step-by-Step\n  Correction", "abstract": "Existing studies on semantic parsing focus primarily on mapping a\nnatural-language utterance to a corresponding logical form in one turn.\nHowever, because natural language can contain a great deal of ambiguity and\nvariability, this is a difficult challenge. In this work, we investigate an\ninteractive semantic parsing framework that explains the predicted logical form\nstep by step in natural language and enables the user to make corrections\nthrough natural-language feedback for individual steps. We focus on question\nanswering over knowledge bases (KBQA) as an instantiation of our framework,\naiming to increase the transparency of the parsing process and help the user\nappropriately trust the final answer. To do so, we construct INSPIRED, a\ncrowdsourced dialogue dataset derived from the ComplexWebQuestions dataset. Our\nexperiments show that the interactive framework with human feedback has the\npotential to greatly improve overall parse accuracy. Furthermore, we develop a\npipeline for dialogue simulation to evaluate our framework w.r.t. a variety of\nstate-of-the-art KBQA models without involving further crowdsourcing effort.\nThe results demonstrate that our interactive semantic parsing framework\npromises to be effective across such models.", "published": "2021-10-15 20:11:22", "link": "http://arxiv.org/abs/2110.08345v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Detecting Gender Bias in Transformer-based Models: A Case Study on BERT", "abstract": "In this paper, we propose a novel gender bias detection method by utilizing\nattention map for transformer-based models. We 1) give an intuitive gender bias\njudgement method by comparing the different relation degree between the genders\nand the occupation according to the attention scores, 2) design a gender bias\ndetector by modifying the attention module, 3) insert the gender bias detector\ninto different positions of the model to present the internal gender bias flow,\nand 4) draw the consistent gender bias conclusion by scanning the entire\nWikipedia, a BERT pretraining dataset. We observe that 1) the attention\nmatrices, Wq and Wk introduce much more gender bias than other modules\n(including the embedding layer) and 2) the bias degree changes periodically\ninside of the model (attention matrix Q, K, V, and the remaining part of the\nattention layer (including the fully-connected layer, the residual connection,\nand the layer normalization module) enhance the gender bias while the averaged\nattentions reduces the bias).", "published": "2021-10-15 21:25:58", "link": "http://arxiv.org/abs/2110.15733v1", "categories": ["cs.CL", "cs.LG", "I.2; I.7; H.0"], "primary_category": "cs.CL"}
{"title": "ESPnet2-TTS: Extending the Edge of TTS Research", "abstract": "This paper describes ESPnet2-TTS, an end-to-end text-to-speech (E2E-TTS)\ntoolkit. ESPnet2-TTS extends our earlier version, ESPnet-TTS, by adding many\nnew features, including: on-the-fly flexible pre-processing, joint training\nwith neural vocoders, and state-of-the-art TTS models with extensions like\nfull-band E2E text-to-waveform modeling, which simplify the training pipeline\nand further enhance TTS performance. The unified design of our recipes enables\nusers to quickly reproduce state-of-the-art E2E-TTS results. We also provide\nmany pre-trained models in a unified Python interface for inference, offering a\nquick means for users to generate baseline samples and build demos.\nExperimental evaluations with English and Japanese corpora demonstrate that our\nprovided models synthesize utterances comparable to ground-truth ones,\nachieving state-of-the-art TTS performance. The toolkit is available online at\nhttps://github.com/espnet/espnet.", "published": "2021-10-15 03:27:45", "link": "http://arxiv.org/abs/2110.07840v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Don't speak too fast: The impact of data bias on self-supervised speech\n  models", "abstract": "Self-supervised Speech Models (S3Ms) have been proven successful in many\nspeech downstream tasks, like ASR. However, how pre-training data affects S3Ms'\ndownstream behavior remains an unexplored issue. In this paper, we study how\npre-training data affects S3Ms by pre-training models on biased datasets\ntargeting different factors of speech, including gender, content, and prosody,\nand evaluate these pre-trained S3Ms on selected downstream tasks in SUPERB\nBenchmark. Our experiments show that S3Ms have tolerance toward gender bias.\nMoreover, we find that the content of speech has little impact on the\nperformance of S3Ms across downstream tasks, but S3Ms do show a preference\ntoward a slower speech rate.", "published": "2021-10-15 09:22:34", "link": "http://arxiv.org/abs/2110.07957v3", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Scribosermo: Fast Speech-to-Text models for German and other Languages", "abstract": "Recent Speech-to-Text models often require a large amount of hardware\nresources and are mostly trained in English. This paper presents Speech-to-Text\nmodels for German, as well as for Spanish and French with special features: (a)\nThey are small and run in real-time on microcontrollers like a RaspberryPi. (b)\nUsing a pretrained English model, they can be trained on consumer-grade\nhardware with a relatively small dataset. (c) The models are competitive with\nother solutions and outperform them in German. In this respect, the models\ncombine advantages of other approaches, which only include a subset of the\npresented features. Furthermore, the paper provides a new library for handling\ndatasets, which is focused on easy extension with additional datasets and shows\nan optimized way for transfer-learning new languages using a pretrained model\nfrom another language with a similar alphabet.", "published": "2021-10-15 10:10:34", "link": "http://arxiv.org/abs/2110.07982v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "StreaMulT: Streaming Multimodal Transformer for Heterogeneous and\n  Arbitrary Long Sequential Data", "abstract": "The increasing complexity of Industry 4.0 systems brings new challenges\nregarding predictive maintenance tasks such as fault detection and diagnosis. A\ncorresponding and realistic setting includes multi-source data streams from\ndifferent modalities, such as sensors measurements time series, machine images,\ntextual maintenance reports, etc. These heterogeneous multimodal streams also\ndiffer in their acquisition frequency, may embed temporally unaligned\ninformation and can be arbitrarily long, depending on the considered system and\ntask. Whereas multimodal fusion has been largely studied in a static setting,\nto the best of our knowledge, there exists no previous work considering\narbitrarily long multimodal streams alongside with related tasks such as\nprediction across time. Thus, in this paper, we first formalize this paradigm\nof heterogeneous multimodal learning in a streaming setting as a new one. To\ntackle this challenge, we propose StreaMulT, a Streaming Multimodal Transformer\nrelying on cross-modal attention and on a memory bank to process arbitrarily\nlong input sequences at training time and run in a streaming way at inference.\nStreaMulT improves the state-of-the-art metrics on CMU-MOSEI dataset for\nMultimodal Sentiment Analysis task, while being able to deal with much longer\ninputs than other multimodal models. The conducted experiments eventually\nhighlight the importance of the textual embedding layer, questioning recent\nimprovements in Multimodal Sentiment Analysis benchmarks.", "published": "2021-10-15 11:32:17", "link": "http://arxiv.org/abs/2110.08021v2", "categories": ["cs.LG", "cs.CL", "cs.MM"], "primary_category": "cs.LG"}
{"title": "Towards Identity Preserving Normal to Dysarthric Voice Conversion", "abstract": "We present a voice conversion framework that converts normal speech into\ndysarthric speech while preserving the speaker identity. Such a framework is\nessential for (1) clinical decision making processes and alleviation of patient\nstress, (2) data augmentation for dysarthric speech recognition. This is an\nespecially challenging task since the converted samples should capture the\nseverity of dysarthric speech while being highly natural and possessing the\nspeaker identity of the normal speaker. To this end, we adopted a two-stage\nframework, which consists of a sequence-to-sequence model and a nonparallel\nframe-wise model. Objective and subjective evaluations were conducted on the\nUASpeech dataset, and results showed that the method was able to yield\nreasonable naturalness and capture severity aspects of the pathological speech.\nOn the other hand, the similarity to the normal source speaker's voice was\nlimited and requires further improvements.", "published": "2021-10-15 17:18:02", "link": "http://arxiv.org/abs/2110.08213v1", "categories": ["cs.SD", "cs.CL", "eess.AS", "q-bio.QM"], "primary_category": "cs.SD"}
{"title": "From Start to Finish: Latency Reduction Strategies for Incremental\n  Speech Synthesis in Simultaneous Speech-to-Speech Translation", "abstract": "Speech-to-speech translation (S2ST) converts input speech to speech in\nanother language. A challenge of delivering S2ST in real time is the\naccumulated delay between the translation and speech synthesis modules. While\nrecently incremental text-to-speech (iTTS) models have shown large quality\nimprovements, they typically require additional future text inputs to reach\noptimal performance. In this work, we minimize the initial waiting time of iTTS\nby adapting the upstream speech translator to generate high-quality pseudo\nlookahead for the speech synthesizer. After mitigating the initial delay, we\ndemonstrate that the duration of synthesized speech also plays a crucial role\non latency. We formalize this as a latency metric and then present a simple yet\neffective duration-scaling approach for latency reduction. Our approaches\nconsistently reduce latency by 0.2-0.5 second without sacrificing speech\ntranslation quality.", "published": "2021-10-15 17:20:28", "link": "http://arxiv.org/abs/2110.08214v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Guiding Visual Question Generation", "abstract": "In traditional Visual Question Generation (VQG), most images have multiple\nconcepts (e.g. objects and categories) for which a question could be generated,\nbut models are trained to mimic an arbitrary choice of concept as given in\ntheir training data. This makes training difficult and also poses issues for\nevaluation -- multiple valid questions exist for most images but only one or a\nfew are captured by the human references. We present Guiding Visual Question\nGeneration - a variant of VQG which conditions the question generator on\ncategorical information based on expectations on the type of question and the\nobjects it should explore. We propose two variants: (i) an explicitly guided\nmodel that enables an actor (human or automated) to select which objects and\ncategories to generate a question for; and (ii) an implicitly guided model that\nlearns which objects and categories to condition on, based on discrete latent\nvariables. The proposed models are evaluated on an answer-category augmented\nVQA dataset and our quantitative results show a substantial improvement over\nthe current state of the art (over 9 BLEU-4 increase). Human evaluation\nvalidates that guidance helps the generation of questions that are\ngrammatically coherent and relevant to the given image and objects.", "published": "2021-10-15 17:38:08", "link": "http://arxiv.org/abs/2110.08226v3", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Textual Backdoor Attacks Can Be More Harmful via Two Simple Tricks", "abstract": "Backdoor attacks are a kind of emergent security threat in deep learning.\nAfter being injected with a backdoor, a deep neural model will behave normally\non standard inputs but give adversary-specified predictions once the input\ncontains specific backdoor triggers. In this paper, we find two simple tricks\nthat can make existing textual backdoor attacks much more harmful. The first\ntrick is to add an extra training task to distinguish poisoned and clean data\nduring the training of the victim model, and the second one is to use all the\nclean training data rather than remove the original clean data corresponding to\nthe poisoned data. These two tricks are universally applicable to different\nattack models. We conduct experiments in three tough situations including clean\ndata fine-tuning, low-poisoning-rate, and label-consistent attacks.\nExperimental results show that the two tricks can significantly improve attack\nperformance. This paper exhibits the great potential harmfulness of backdoor\nattacks. All the code and data can be obtained at\n\\url{https://github.com/thunlp/StyleAttack}.", "published": "2021-10-15 17:58:46", "link": "http://arxiv.org/abs/2110.08247v2", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Direct Simultaneous Speech-to-Speech Translation with Variational\n  Monotonic Multihead Attention", "abstract": "We present a direct simultaneous speech-to-speech translation (Simul-S2ST)\nmodel, Furthermore, the generation of translation is independent from\nintermediate text representations. Our approach leverages recent progress on\ndirect speech-to-speech translation with discrete units, in which a sequence of\ndiscrete representations, instead of continuous spectrogram features, learned\nin an unsupervised manner, are predicted from the model and passed directly to\na vocoder for speech synthesis on-the-fly. We also introduce the variational\nmonotonic multihead attention (V-MMA), to handle the challenge of inefficient\npolicy learning in speech simultaneous translation. The simultaneous policy\nthen operates on source speech features and target discrete units. We carry out\nempirical studies to compare cascaded and direct approach on the Fisher\nSpanish-English and MuST-C English-Spanish datasets. Direct simultaneous model\nis shown to outperform the cascaded model by achieving a better tradeoff\nbetween translation quality and latency.", "published": "2021-10-15 17:59:15", "link": "http://arxiv.org/abs/2110.08250v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Control Prefixes for Parameter-Efficient Text Generation", "abstract": "Prefix-tuning is a powerful lightweight technique for adapting a large\npre-trained language model to a downstream application. However, it uses the\nsame dataset-level tuned prompt for all examples in the dataset. We extend this\nidea and propose a dynamic method, Control Prefixes, which allows for the\ninclusion of conditional input-dependent information, combining the benefits of\nprompt tuning and controlled generation. The method incorporates\nattribute-level learnable representations into different layers of a\npre-trained transformer, allowing for the generated text to be guided in a\nparticular direction. We provide a systematic evaluation of the technique and\napply it to five datasets from the GEM benchmark for natural language\ngeneration (NLG). Although the aim is to develop a parameter-efficient model,\nwe show Control Prefixes can even outperform full fine-tuning methods. We\npresent state-of-the-art results on several data-to-text datasets, including\nWebNLG.", "published": "2021-10-15 19:32:17", "link": "http://arxiv.org/abs/2110.08329v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Omni-sparsity DNN: Fast Sparsity Optimization for On-Device Streaming\n  E2E ASR via Supernet", "abstract": "From wearables to powerful smart devices, modern automatic speech recognition\n(ASR) models run on a variety of edge devices with different computational\nbudgets. To navigate the Pareto front of model accuracy vs model size,\nresearchers are trapped in a dilemma of optimizing model accuracy by training\nand fine-tuning models for each individual edge device while keeping the\ntraining GPU-hours tractable. In this paper, we propose Omni-sparsity DNN,\nwhere a single neural network can be pruned to generate optimized model for a\nlarge range of model sizes. We develop training strategies for Omni-sparsity\nDNN that allows it to find models along the Pareto front of word-error-rate\n(WER) vs model size while keeping the training GPU-hours to no more than that\nof training one singular model. We demonstrate the Omni-sparsity DNN with\nstreaming E2E ASR models. Our results show great saving on training time and\nresources with similar or better accuracy on LibriSpeech compared to\nindividually pruned sparse models: 2%-6.6% better WER on Test-other.", "published": "2021-10-15 20:28:27", "link": "http://arxiv.org/abs/2110.08352v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Evaluating the Faithfulness of Importance Measures in NLP by Recursively\n  Masking Allegedly Important Tokens and Retraining", "abstract": "To explain NLP models a popular approach is to use importance measures, such\nas attention, which inform input tokens are important for making a prediction.\nHowever, an open question is how well these explanations accurately reflect a\nmodel's logic, a property called faithfulness.\n  To answer this question, we propose Recursive ROAR, a new faithfulness\nmetric. This works by recursively masking allegedly important tokens and then\nretraining the model. The principle is that this should result in worse model\nperformance compared to masking random tokens. The result is a performance\ncurve given a masking-ratio. Furthermore, we propose a summarizing metric using\nrelative area-between-curves (RACU), which allows for easy comparison across\npapers, models, and tasks.\n  We evaluate 4 different importance measures on 8 different datasets, using\nboth LSTM-attention models and RoBERTa models. We find that the faithfulness of\nimportance measures is both model-dependent and task-dependent. This conclusion\ncontradicts previous evaluations in both computer vision and faithfulness of\nattention literature.", "published": "2021-10-15 23:59:42", "link": "http://arxiv.org/abs/2110.08412v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Law Smells: Defining and Detecting Problematic Patterns in Legal\n  Drafting", "abstract": "Building on the computer science concept of code smells, we initiate the\nstudy of law smells, i.e., patterns in legal texts that pose threats to the\ncomprehensibility and maintainability of the law. With five intuitive law\nsmells as running examples - namely, duplicated phrase, long element, large\nreference tree, ambiguous syntax, and natural language obsession -, we develop\na comprehensive law smell taxonomy. This taxonomy classifies law smells by when\nthey can be detected, which aspects of law they relate to, and how they can be\ndiscovered. We introduce text-based and graph-based methods to identify\ninstances of law smells, confirming their utility in practice using the United\nStates Code as a test case. Our work demonstrates how ideas from software\nengineering can be leveraged to assess and improve the quality of legal code,\nthus drawing attention to an understudied area in the intersection of law and\ncomputer science and highlighting the potential of computational legal\ndrafting.", "published": "2021-10-15 06:37:13", "link": "http://arxiv.org/abs/2110.11984v1", "categories": ["cs.IR", "cs.CL", "cs.CY", "cs.SE", "cs.SI"], "primary_category": "cs.IR"}
{"title": "Neural Dubber: Dubbing for Videos According to Scripts", "abstract": "Dubbing is a post-production process of re-recording actors' dialogues, which\nis extensively used in filmmaking and video production. It is usually performed\nmanually by professional voice actors who read lines with proper prosody, and\nin synchronization with the pre-recorded videos. In this work, we propose\nNeural Dubber, the first neural network model to solve a novel automatic video\ndubbing (AVD) task: synthesizing human speech synchronized with the given video\nfrom the text. Neural Dubber is a multi-modal text-to-speech (TTS) model that\nutilizes the lip movement in the video to control the prosody of the generated\nspeech. Furthermore, an image-based speaker embedding (ISE) module is developed\nfor the multi-speaker setting, which enables Neural Dubber to generate speech\nwith a reasonable timbre according to the speaker's face. Experiments on the\nchemistry lecture single-speaker dataset and LRS2 multi-speaker dataset show\nthat Neural Dubber can generate speech audios on par with state-of-the-art TTS\nmodels in terms of speech quality. Most importantly, both qualitative and\nquantitative evaluations show that Neural Dubber can control the prosody of\nsynthesized speech by the video, and generate high-fidelity speech temporally\nsynchronized with the video. Our project page is at\nhttps://tsinghua-mars-lab.github.io/NeuralDubber/ .", "published": "2021-10-15 17:56:07", "link": "http://arxiv.org/abs/2110.08243v3", "categories": ["eess.AS", "cs.CL", "cs.CV", "cs.LG", "cs.SD", "eess.IV"], "primary_category": "eess.AS"}
{"title": "Using DeepProbLog to perform Complex Event Processing on an Audio Stream", "abstract": "In this paper, we present an approach to Complex Event Processing (CEP) that\nis based on DeepProbLog. This approach has the following objectives: (i)\nallowing the use of subsymbolic data as an input, (ii) retaining the\nflexibility and modularity on the definitions of complex event rules, (iii)\nallowing the system to be trained in an end-to-end manner and (iv) being robust\nagainst noisily labelled data. Our approach makes use of DeepProbLog to create\na neuro-symbolic architecture that combines a neural network to process the\nsubsymbolic data with a probabilistic logic layer to allow the user to define\nthe rules for the complex events. We demonstrate that our approach is capable\nof detecting complex events from an audio stream. We also demonstrate that our\napproach is capable of training even with a dataset that has a moderate\nproportion of noisy data.", "published": "2021-10-15 13:33:01", "link": "http://arxiv.org/abs/2110.08090v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
