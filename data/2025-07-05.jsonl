{"title": "SymbolicThought: Integrating Language Models and Symbolic Reasoning for Consistent and Interpretable Human Relationship Understanding", "abstract": "Understanding character relationships is essential for interpreting complex\nnarratives and conducting socially grounded AI research. However, manual\nannotation is time-consuming and low in coverage, while large language models\n(LLMs) often produce hallucinated or logically inconsistent outputs. We present\nSymbolicThought, a human-in-the-loop framework that combines LLM-based\nextraction with symbolic reasoning. The system constructs editable character\nrelationship graphs, refines them using seven types of logical constraints, and\nenables real-time validation and conflict resolution through an interactive\ninterface. To support logical supervision and explainable social analysis, we\nrelease a dataset of 160 interpersonal relationships with corresponding logical\nstructures. Experiments show that SymbolicThought improves annotation accuracy\nand consistency while significantly reducing time cost, offering a practical\ntool for narrative understanding, explainable AI, and LLM evaluation.", "published": "2025-07-05 23:46:35", "link": "http://arxiv.org/abs/2507.04189v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Navigating Speech Recording Collections with AI-Generated Illustrations", "abstract": "Although the amount of available spoken content is steadily increasing,\nextracting information and knowledge from speech recordings remains\nchallenging. Beyond enhancing traditional information retrieval methods such as\nspeech search and keyword spotting, novel approaches for navigating and\nsearching spoken content need to be explored and developed. In this paper, we\npropose a novel navigational method for speech archives that leverages recent\nadvances in language and multimodal generative models. We demonstrate our\napproach with a Web application that organizes data into a structured format\nusing interactive mind maps and image generation tools. The system is\nimplemented using the TED-LIUM~3 dataset, which comprises over 2,000 speech\ntranscripts and audio files of TED Talks. Initial user tests using a System\nUsability Scale (SUS) questionnaire indicate the application's potential to\nsimplify the exploration of large speech collections.", "published": "2025-07-05 22:38:10", "link": "http://arxiv.org/abs/2507.04182v1", "categories": ["cs.IR", "cs.CL", "cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.IR"}
{"title": "Efficient Detection of Intermittent Job Failures Using Few-Shot Learning", "abstract": "One of the main challenges developers face in the use of continuous\nintegration (CI) and deployment pipelines is the occurrence of intermittent job\nfailures, which result from unexpected non-deterministic issues (e.g., flaky\ntests or infrastructure problems) rather than regular code-related errors such\nas bugs. Prior studies developed machine-learning (ML) models trained on large\ndatasets of job logs to classify job failures as either intermittent or\nregular. As an alternative to costly manual labeling of large datasets, the\nstate-of-the-art (SOTA) approach leveraged a heuristic based on\nnon-deterministic job reruns. However, this method mislabels intermittent job\nfailures as regular in contexts where rerunning suspicious job failures is not\nan explicit policy, and therefore limits the SOTA's performance in practice. In\nfact, our manual analysis of 2,125 job failures from 5 industrial and 1\nopen-source projects reveals that, on average, 32\\% of intermittent job\nfailures are mislabeled as regular. To address these limitations, this paper\nintroduces a novel approach to intermittent job failure detection using\nfew-shot learning (FSL). Specifically, we fine-tune a small language model\nusing a few number of manually labeled log examples to generate rich\nembeddings, which are then used to train an ML classifier. Our FSL-based\napproach achieves 70-88\\% F1-score with only 12 shots in all projects,\noutperforming the SOTA, which proved ineffective (34-52\\% F1-score) in 4\nprojects. Overall, this study underlines the importance of data quality over\nquantity and provides a more efficient and practical framework for the\ndetection of intermittent job failures in organizations.", "published": "2025-07-05 22:04:01", "link": "http://arxiv.org/abs/2507.04173v1", "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Large Language Models for Zero-Shot Multicultural Name Recognition", "abstract": "The robust and accurate recognition of multicultural names, particularly\nthose not previously encountered, is a critical challenge in an increasingly\nglobalized digital landscape. Traditional methods often falter when confronted\nwith the vast diversity and novel permutations of names across different\nlinguistic and cultural backgrounds. This paper introduces a novel framework,\nPrompt-Engineered Fine-Tuning (PEFT) for Large Language Models (LLMs) with\nAdversarial Data Augmentation and Cultural Knowledge Graph Integration,\ndesigned to significantly enhance zero-shot multicultural name recognition. Our\napproach leverages the powerful linguistic understanding of pre-trained LLMs,\ntransforming the recognition task into a guided generation problem. Through\nmeticulous prompt engineering, dynamic integration of explicit cultural\nknowledge derived from knowledge graphs, and the strategic application of\nadversarial data augmentation, we equip the LLM with an unprecedented ability\nto infer the cultural origin of unseen names. Extensive experiments demonstrate\nthat our PEFT method consistently outperforms established deep learning\nbaselines, including advanced Bi-LSTM models with cultural tags, achieving an\nimpressive 93.1\\% overall accuracy and a remarkable 89.5\\% accuracy on\nchallenging zero-shot name identification. An in-depth ablation study confirms\nthe synergistic contribution of each component, while a human evaluation\nhighlights our method's performance approaching human expert judgment. This\nwork signifies a substantial leap in multicultural name recognition, offering a\nhighly effective and scalable solution for real-world applications.", "published": "2025-07-05 19:59:03", "link": "http://arxiv.org/abs/2507.04149v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dissecting Clinical Reasoning in Language Models: A Comparative Study of Prompts and Model Adaptation Strategies", "abstract": "Recent works on large language models (LLMs) have demonstrated the impact of\nprompting strategies and fine-tuning techniques on their reasoning\ncapabilities. Yet, their effectiveness on clinical natural language inference\n(NLI) remains underexplored. This study presents the first controlled\nevaluation of how prompt structure and efficient fine-tuning jointly shape\nmodel performance in clinical NLI. We inspect four classes of prompting\nstrategies to elicit reasoning in LLMs at different levels of abstraction, and\nevaluate their impact on a range of clinically motivated reasoning types. For\neach prompting strategy, we construct high-quality demonstrations using a\nfrontier model to distil multi-step reasoning capabilities into smaller models\n(4B parameters) via Low-Rank Adaptation (LoRA). Across different language\nmodels fine-tuned on the NLI4CT benchmark, we found that prompt type alone\naccounts for up to 44% of the variance in macro-F1. Moreover, LoRA fine-tuning\nyields consistent gains of +8 to 12 F1, raises output alignment above 97%, and\nnarrows the performance gap to GPT-4o-mini to within 7.1%. Additional\nexperiments on reasoning generalisation reveal that LoRA improves performance\nin 75% of the models on MedNLI and TREC Clinical Trials Track. Overall, these\nfindings demonstrate that (i) prompt structure is a primary driver of clinical\nreasoning performance, (ii) compact models equipped with strong prompts and\nLoRA can rival frontier-scale systems, and (iii) reasoning-type-aware\nevaluation is essential to uncover prompt-induced trade-offs. Our results\nhighlight the promise of combining prompt design and lightweight adaptation for\nmore efficient and trustworthy clinical NLP systems, providing insights on the\nstrengths and limitations of widely adopted prompting and parameter-efficient\ntechniques in highly specialised domains.", "published": "2025-07-05 19:43:54", "link": "http://arxiv.org/abs/2507.04142v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Token Level Hallucination Detection via Variance in Language Models", "abstract": "Large Language Models (LLMs) have demonstrated impressive generative\ncapabilities across diverse tasks but remain susceptible to hallucinations,\nconfidently generated yet factually incorrect outputs. We introduce a\nreference-free, token-level hallucination detection framework that leverages\nthe variance in token log-probabilities across multiple stochastic generations.\nUnlike prior methods that require ground-truth references or sentence-level\nverification, our approach is model-agnostic, interpretable, and suited for\nreal-time or post-hoc analysis. We evaluate our method on unanswerable question\nprompts from the SQuAD v2 dataset and benchmark across three autoregressive\nmodels of varying scales: GPT-Neo 125M, Falcon 1B, and Mistral 7B. Through both\nquantitative metrics and visual diagnostics, we show that token-level variance\nreliably highlights instability in model outputs and correlates with\nhallucination patterns. Our framework is lightweight, reproducible, and\nadaptable to multiple domains, offering a valuable diagnostic tool for\nanalyzing generative reliability in LLMs.", "published": "2025-07-05 19:20:59", "link": "http://arxiv.org/abs/2507.04137v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An HTR-LLM Workflow for High-Accuracy Transcription and Analysis of Abbreviated Latin Court Hand", "abstract": "This article presents and validates an ideal, four-stage workflow for the\nhigh-accuracy transcription and analysis of challenging medieval legal\ndocuments. The process begins with a specialized Handwritten Text Recognition\n(HTR) model, itself created using a novel \"Clean Ground Truth\" curation method\nwhere a Large Language Model (LLM) refines the training data. This HTR model\nprovides a robust baseline transcription (Stage 1). In Stage 2, this baseline\nis fed, along with the original document image, to an LLM for multimodal\npost-correction, grounding the LLM's analysis and improving accuracy. The\ncorrected, abbreviated text is then expanded into full, scholarly Latin using a\nprompt-guided LLM (Stage 3). A final LLM pass performs Named-Entity Correction\n(NEC), regularizing proper nouns and generating plausible alternatives for\nambiguous readings (Stage 4). We validate this workflow through detailed case\nstudies, achieving Word Error Rates (WER) in the range of 2-7% against\nscholarly ground truths. The results demonstrate that this hybrid, multi-stage\napproach effectively automates the most laborious aspects of transcription\nwhile producing a high-quality, analyzable output, representing a powerful and\npractical solution for the current technological landscape.", "published": "2025-07-05 19:07:15", "link": "http://arxiv.org/abs/2507.04132v1", "categories": ["cs.DL", "cs.CL", "cs.CV"], "primary_category": "cs.DL"}
{"title": "BYOKG-RAG: Multi-Strategy Graph Retrieval for Knowledge Graph Question Answering", "abstract": "Knowledge graph question answering (KGQA) presents significant challenges due\nto the structural and semantic variations across input graphs. Existing works\nrely on Large Language Model (LLM) agents for graph traversal and retrieval; an\napproach that is sensitive to traversal initialization, as it is prone to\nentity linking errors and may not generalize well to custom (\"bring-your-own\")\nKGs. We introduce BYOKG-RAG, a framework that enhances KGQA by synergistically\ncombining LLMs with specialized graph retrieval tools. In BYOKG-RAG, LLMs\ngenerate critical graph artifacts (question entities, candidate answers,\nreasoning paths, and OpenCypher queries), and graph tools link these artifacts\nto the KG and retrieve relevant graph context. The retrieved context enables\nthe LLM to iteratively refine its graph linking and retrieval, before final\nanswer generation. By retrieving context from different graph tools, BYOKG-RAG\noffers a more general and robust solution for QA over custom KGs. Through\nexperiments on five benchmarks spanning diverse KG types, we demonstrate that\nBYOKG-RAG outperforms the second-best graph retrieval method by 4.5% points\nwhile showing better generalization to custom KGs. BYOKG-RAG framework is\nopen-sourced at https://github.com/awslabs/graphrag-toolkit.", "published": "2025-07-05 18:47:14", "link": "http://arxiv.org/abs/2507.04127v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Relational inductive biases on attention mechanisms", "abstract": "Inductive learning aims to construct general models from specific examples,\nguided by biases that influence hypothesis selection and determine\ngeneralization capacity. In this work, we focus on characterizing the\nrelational inductive biases present in attention mechanisms, understood as\nassumptions about the underlying relationships between data elements. From the\nperspective of geometric deep learning, we analyze the most common attention\nmechanisms in terms of their equivariance properties with respect to\npermutation subgroups, which allows us to propose a classification based on\ntheir relational biases. Under this perspective, we show that different\nattention layers are characterized by the underlying relationships they assume\non the input data.", "published": "2025-07-05 17:46:52", "link": "http://arxiv.org/abs/2507.04117v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Conversation Forests: The Key to Fine Tuning Large Language Models for Multi-Turn Medical Conversations is Branching", "abstract": "Fine-tuning methods such as Direct Preference Optimization (DPO) and Group\nRelative Policy Optimization (GRPO) have demonstrated success in training large\nlanguage models (LLMs) for single-turn tasks. However, these methods fall short\nin multi-turn applications, such as diagnostic patient interviewing, where\nunderstanding how early conversational turns influence downstream completions\nand outcomes is essential. In medicine, a multi-turn perspective is critical\nfor learning diagnostic schemas and better understanding conversation dynamics.\nTo address this gap, I introduce Savage Conversation Forests (SCF), a\nreinforcement learning framework that leverages a branched conversation\narchitecture to fine-tune LLMs for multi-turn dialogue. SCF generates multiple\npossible conversation continuations at each turn, enabling the model to learn\nhow different early responses affect downstream interactions and diagnostic\noutcomes. In experiments simulating doctor-patient conversations, SCF with\nbranching outperforms linear conversation architectures on diagnostic accuracy.\nI hypothesize that SCF's improvements stem from its ability to provide richer,\ninterdependent training signals across conversation turns. These results\nsuggest that a branched training architecture is an important strategy for fine\ntuning LLMs in complex multi-turn conversational tasks.", "published": "2025-07-05 16:49:34", "link": "http://arxiv.org/abs/2507.04099v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MMMOS: Multi-domain Multi-axis Audio Quality Assessment", "abstract": "Accurate audio quality estimation is essential for developing and evaluating\naudio generation, retrieval, and enhancement systems. Existing non-intrusive\nassessment models predict a single Mean Opinion Score (MOS) for speech, merging\ndiverse perceptual factors and failing to generalize beyond speech. We propose\nMMMOS, a no-reference, multi-domain audio quality assessment system that\nestimates four orthogonal axes: Production Quality, Production Complexity,\nContent Enjoyment, and Content Usefulness across speech, music, and\nenvironmental sounds. MMMOS fuses frame-level embeddings from three pretrained\nencoders (WavLM, MuQ, and M2D) and evaluates three aggregation strategies with\nfour loss functions. By ensembling the top eight models, MMMOS shows a 20-30%\nreduction in mean squared error and a 4-5% increase in Kendall's {\\tau} versus\nbaseline, gains first place in six of eight Production Complexity metrics, and\nranks among the top three on 17 of 32 challenge metrics.", "published": "2025-07-05 16:42:09", "link": "http://arxiv.org/abs/2507.04094v1", "categories": ["eess.AS", "cs.AI", "cs.CL"], "primary_category": "eess.AS"}
{"title": "XISM: an eXploratory and Interactive Graph Tool to Visualize and Evaluate Semantic Map Models", "abstract": "Semantic map models represent meanings or functions as nodes in a graph\nconstrained by the local connectivity hypothesis, with edges indicating their\nassociations. Widely used in typological linguistics, these models compare\ninterrelated meanings across languages. Traditionally built manually in a\nbottom-up manner, they are inefficient for large datasets and lack\nvisualization and evaluation tools. This paper introduces XISM, an interactive\ntool based on our prior algorithm, which constructs semantic maps from user\ndata via a top-down approach, displays candidate maps, and evaluates them using\nmultiple metrics. Users can refine maps by editing edges, combining data-driven\nefficiency with expert knowledge. This human-in-the-loop design benefits both\ntypologists and computational linguists. The system\nhttps://770103knev48.vicp.fun/ and a demonstration video\nhttps://youtu.be/S-wsVDF2HSI?si=1OrcF41tRznaifhZ are publicly available.", "published": "2025-07-05 15:21:42", "link": "http://arxiv.org/abs/2507.04070v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Independent Passages: Adaptive Passage Combination Retrieval for Retrieval Augmented Open-Domain Question Answering", "abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nincorporating external documents at inference time, enabling up-to-date\nknowledge access without costly retraining. However, conventional RAG methods\nretrieve passages independently, often leading to redundant, noisy, or\ninsufficiently diverse context-particularly problematic - particularly\nproblematic in noisy corpora and for multi-hop questions. To address this, we\npropose Adaptive Passage Combination Retrieval (AdaPCR), a novel framework for\nopen-domain question answering with black-box LMs. AdaPCR explicitly models\ndependencies between passages by considering passage combinations as units for\nretrieval and reranking. It consists of a context-aware query reformulation\nusing concatenated passages, and a reranking step trained with a predictive\nobjective aligned with downstream answer likelihood. Crucially, AdaPCR\nadaptively selects the number of retrieved passages without additional stopping\nmodules. Experiments across several QA benchmarks show that AdaPCR outperforms\nbaselines, particularly in multi-hop reasoning, demonstrating the effectiveness\nof modeling inter-passage dependencies for improved retrieval.", "published": "2025-07-05 15:10:12", "link": "http://arxiv.org/abs/2507.04069v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Patient-Centered RAG for Oncology Visit Aid Following the Ottawa Decision Guide", "abstract": "Effective communication is essential in cancer care, yet patients often face\nchallenges in preparing for complex medical visits. We present an interactive,\nRetrieval-augmented Generation-assisted system that helps patients progress\nfrom uninformed to visit-ready. Our system adapts the Ottawa Personal Decision\nGuide into a dynamic retrieval-augmented generation workflow, helping users\nbridge knowledge gaps, clarify personal values and generate useful questions\nfor their upcoming visits. Focusing on localized prostate cancer, we conduct a\nuser study with patients and a clinical expert. Results show high system\nusability (UMUX Mean = 6.0 out of 7), strong relevance of generated content\n(Mean = 6.7 out of 7), minimal need for edits, and high clinical faithfulness\n(Mean = 6.82 out of 7). This work demonstrates the potential of combining\npatient-centered design with language models to enhance clinical preparation in\noncology care.", "published": "2025-07-05 12:37:26", "link": "http://arxiv.org/abs/2507.04026v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLMThinkBench: Towards Basic Math Reasoning and Overthinking in Large Language Models", "abstract": "Large Language Models (LLMs) have achieved remarkable performance on complex\nmathematical benchmarks, yet often struggle with simple arithmetic tasks and\nexhibit a tendency toward over-explaining or \"overthinking\" answers. To\nsystematically assess this phenomenon, we introduce LLMThinkBench, a modular\nbenchmarking framework that enables researchers to evaluate basic math\nreasoning and overthinking in LLMs. The framework provides 14 configurable math\ntasks with randomized test data generation and robust parsing strategies.\nResearchers can quantify overthinking using our Overthinking Score metric,\nwhich captures accuracy-verbosity tradeoffs through harmonic mean formulation.\nThe tool offers flexible evaluation with a scalable vLLM/Transformers backend,\nmulti-GPU support, and full configurability. Users can extend the tool with\ncustom tasks, reproduce experiments with seeding, and generate detailed\nefficiency reports. Distributed as a pip-installable package with CLI and API\naccess, LLMThinkBench provides researchers and practitioners an accessible,\ncost-effective alternative to expensive LLM-as-a-judge methods for diagnosing\nbasic reasoning capabilities and efficiency analysis. Package can be installed\nas: pip install llmthinkbench", "published": "2025-07-05 12:31:17", "link": "http://arxiv.org/abs/2507.04023v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Handling Korean Out-of-Vocabulary Words with Phoneme Representation Learning", "abstract": "In this study, we introduce KOPL, a novel framework for handling Korean OOV\nwords with Phoneme representation Learning. Our work is based on the linguistic\nproperty of Korean as a phonemic script, the high correlation between phonemes\nand letters. KOPL incorporates phoneme and word representations for Korean OOV\nwords, facilitating Korean OOV word representations to capture both text and\nphoneme information of words. We empirically demonstrate that KOPL\nsignificantly improves the performance on Korean Natural Language Processing\n(NLP) tasks, while being readily integrated into existing static and contextual\nKorean embedding models in a plug-and-play manner. Notably, we show that KOPL\noutperforms the state-of-the-art model by an average of 1.9%. Our code is\navailable at https://github.com/jej127/KOPL.git.", "published": "2025-07-05 12:16:55", "link": "http://arxiv.org/abs/2507.04018v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Combination generators with optimal cache utilization and communication free parallel execution", "abstract": "We introduce an efficient and elegant combination generator for producing all\ncombinations of size less than or equal to K, designed for exhaustive\ngeneration and combinatorial optimization tasks. This generator can be\nimplemented to achieve what we define as optimal efficiency: constant amortized\ntime, optimal cache utilization, embarrassingly parallel execution, and a\nrecursive structure compatible with pruning-based search. These properties are\ndifficult to satisfy simultaneously in existing generators. For example,\nclassical Gray code or lexicographic generators are typically list-based and\nsequentially defined, making them difficult to vectorized, inefficient in cache\nusage, and inherently hard to parallelize. Generators based on unranking\nmethods, while easy to parallelize, are non-recursive. These limitations reduce\ntheir applicability in our target applications, where both computational\nefficiency and recursion are crucial. We adapt Bird's algebra of\nprogramming-style calculation to derive our algorithms, a formalism for\ndeveloping correct-by-construction programs from specifications. As a result,\nall generators in this paper are first formulated in their clearest\nspecification, and efficient definitions are derived constructively through\nequational reasoning, resulting in concise and elegant divide-and-conquer\ndefinitions. Beyond presenting a combination generator, we extend our approach\nto construct generators for K-permutations, nested combinations of\ncombinations, and nested permutation-combination structures. To the best of our\nknowledge, the literature has not previously reported generators for these\nnested structures. We also develop sequential variants that produce\nconfigurations in Gray code-compatible orders -- such as the revolving door\nordering -- which are particularly useful for constructing nested generators.", "published": "2025-07-05 10:11:37", "link": "http://arxiv.org/abs/2507.03980v1", "categories": ["cs.DM", "cs.DS"], "primary_category": "cs.DM"}
{"title": "Cloud Digital Forensic Readiness: An Open Source Approach to Law Enforcement Request Management", "abstract": "Cloud Forensics presents a multi-jurisdictional challenge that may undermines\nthe success of digital forensic investigations (DFIs). The growing volumes of\ndomiciled and foreign law enforcement (LE) requests, the latency and complexity\nof formal channels for crossborder data access are challenging issues. In this\npaper, we first discuss major Cloud Service Providers (CSPs) transparency\nreports and law enforcement guidelines, then propose an abstract architecture\nfor a Cloud Law Enforcement Requests Management System (CLERMS). A proof of\nconcept of the proposed solution is developed, deployed and validated by two\nrealistic scenarios, in addition to an economic estimation of its associated\ncosts. Based on available open source components, our solution is for the\nbenefit of both CSPs and Cloud Service Consumers (CSCs), and aims to enhance\nthe due Cloud Digital Forensic Readiness (CDFR).", "published": "2025-07-05 22:06:42", "link": "http://arxiv.org/abs/2507.04174v1", "categories": ["cs.CR", "cs.IR"], "primary_category": "cs.CR"}
{"title": "CTR-Guided Generative Query Suggestion in Conversational Search", "abstract": "Generating effective query suggestions in conversational search requires\naligning model outputs with user preferences, which is challenging due to\nsparse and noisy click signals. We propose GQS, a generative framework that\nintegrates click modeling and preference optimization to enhance real-world\nuser engagement. GQS consists of three key components: (1) a Multi-Source CTR\nModeling module that captures diverse contextual signals to estimate\nfine-grained click-through rates; (2) a Diversity-Aware Preference Alignment\nstrategy using CTR-weighted Direct Preference Optimization (DPO), which\nbalances relevance and semantic diversity; and (3) a CTR-Calibrated Iterative\nOptimization process that jointly refines the CTR and generation models across\ntraining rounds. Experiments on two real-world tasks demonstrate that GQS\noutperforms strong baselines in CTR, relevance, and diversity.", "published": "2025-07-05 15:32:41", "link": "http://arxiv.org/abs/2507.04072v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Leveraging Multimodal Data and Side Users for Diffusion Cross-Domain Recommendation", "abstract": "Cross-domain recommendation (CDR) aims to address the persistent cold-start\nproblem in Recommender Systems. Current CDR research concentrates on\ntransferring cold-start users' information from the auxiliary domain to the\ntarget domain. However, these systems face two main issues: the\nunderutilization of multimodal data, which hinders effective cross-domain\nalignment, and the neglect of side users who interact solely within the target\ndomain, leading to inadequate learning of the target domain's vector space\ndistribution. To address these issues, we propose a model leveraging Multimodal\ndata and Side users for diffusion Cross-domain recommendation (MuSiC). We first\nemploy a multimodal large language model to extract item multimodal features\nand leverage a large language model to uncover user features using prompt\nlearning without fine-tuning. Secondly, we propose the cross-domain diffusion\nmodule to learn the generation of feature vectors in the target domain. This\napproach involves learning feature distribution from side users and\nunderstanding the patterns in cross-domain transformation through overlapping\nusers. Subsequently, the trained diffusion module is used to generate feature\nvectors for cold-start users in the target domain, enabling the completion of\ncross-domain recommendation tasks. Finally, our experimental evaluation of the\nAmazon dataset confirms that MuSiC achieves state-of-the-art performance,\nsignificantly outperforming all selected baselines. Our code is available:\nhttps://anonymous.4open.science/r/MuSiC-310A/.", "published": "2025-07-05 10:57:29", "link": "http://arxiv.org/abs/2507.04000v1", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR"}
{"title": "A Comparative Study of Specialized LLMs as Dense Retrievers", "abstract": "While large language models (LLMs) are increasingly deployed as dense\nretrievers, the impact of their domain-specific specialization on retrieval\neffectiveness remains underexplored. This investigation systematically examines\nhow task-specific adaptations in LLMs influence their retrieval capabilities,\nan essential step toward developing unified retrievers capable of handling\ntext, code, images, and multimodal content. We conduct extensive experiments\nwith eight Qwen2.5 7B LLMs, including base, instruction-tuned,\ncode/math-specialized, long reasoning, and vision-language models across\nzero-shot retrieval settings and the supervised setting. For the zero-shot\nretrieval settings, we consider text retrieval from the BEIR benchmark and code\nretrieval from the CoIR benchmark. Further, to evaluate supervised performance,\nall LLMs are fine-tuned on the MS MARCO dataset. We find that mathematical\nspecialization and the long reasoning capability cause consistent degradation\nin three settings, indicating conflicts between mathematical reasoning and\nsemantic matching. The vision-language model and code-specialized LLMs\ndemonstrate superior zero-shot performance compared to other LLMs, even\nsurpassing BM25 on the code retrieval task, and maintain comparable performance\nto base LLMs in supervised settings. These findings suggest promising\ndirections for the unified retrieval task leveraging cross-domain and\ncross-modal fusion.", "published": "2025-07-05 08:50:29", "link": "http://arxiv.org/abs/2507.03958v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Function-based Labels for Complementary Recommendation: Definition, Annotation, and LLM-as-a-Judge", "abstract": "Complementary recommendations enhance the user experience by suggesting items\nthat are frequently purchased together while serving different functions from\nthe query item. Inferring or evaluating whether two items have a complementary\nrelationship requires complementary relationship labels; however, defining\nthese labels is challenging because of the inherent ambiguity of such\nrelationships. Complementary labels based on user historical behavior logs\nattempt to capture these relationships, but often produce inconsistent and\nunreliable results. Recent efforts have introduced large language models (LLMs)\nto infer these relationships. However, these approaches provide a binary\nclassification without a nuanced understanding of complementary relationships.\nIn this study, we address these challenges by introducing Function-Based Labels\n(FBLs), a novel definition of complementary relationships independent of user\npurchase logs and the opaque decision processes of LLMs. We constructed a\nhuman-annotated FBLs dataset comprising 2,759 item pairs and demonstrated that\nit covered possible item relationships and minimized ambiguity. We then\nevaluated whether some machine learning (ML) methods using annotated FBLs could\naccurately infer labels for unseen item pairs, and whether LLM-generated\ncomplementary labels align with human perception. Our results demonstrate that\neven with limited data, ML models, such as logistic regression and SVM achieve\nhigh macro-F1 scores (approximately 0.82). Furthermore, LLMs, such as\ngpt-4o-mini, demonstrated high consistency (0.989) and classification accuracy\n(0.849) under the detailed definition of FBLs, indicating their potential as\neffective annotators that mimic human judgment. Overall, our study presents\nFBLs as a clear definition of complementary relationships, enabling more\naccurate inferences and automated labeling of complementary recommendations.", "published": "2025-07-05 08:08:38", "link": "http://arxiv.org/abs/2507.03945v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "TayFCS: Towards Light Feature Combination Selection for Deep Recommender Systems", "abstract": "Feature interaction modeling is crucial for deep recommendation models. A\ncommon and effective approach is to construct explicit feature combinations to\nenhance model performance. However, in practice, only a small fraction of these\ncombinations are truly informative. Thus it is essential to select useful\nfeature combinations to reduce noise and manage memory consumption. While\nfeature selection methods have been extensively studied, they are typically\nlimited to selecting individual features. Extending these methods for\nhigh-order feature combination selection presents a significant challenge due\nto the exponential growth in time complexity when evaluating feature\ncombinations one by one. In this paper, we propose $\\textbf{TayFCS}$, a\nlightweight feature combination selection method that significantly improves\nmodel performance. Specifically, we propose the Taylor Expansion Scorer\n(TayScorer) module for field-wise Taylor expansion on the base model. Instead\nof evaluating all potential feature combinations' importance by repeatedly\nrunning experiments with feature adding and removal, this scorer only needs to\napproximate the importance based on their sub-components' gradients. This can\nbe simply computed with one backward pass based on a trained recommendation\nmodel. To further reduce information redundancy among feature combinations and\ntheir sub-components, we introduce Logistic Regression Elimination (LRE), which\nestimates the corresponding information gain based on the model prediction\nperformance. Experimental results on three benchmark datasets validate both the\neffectiveness and efficiency of our approach. Furthermore, online A/B test\nresults demonstrate its practical applicability and commercial value.", "published": "2025-07-05 04:22:42", "link": "http://arxiv.org/abs/2507.03895v1", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR"}
{"title": "Continual Recommender Systems", "abstract": "Modern recommender systems operate in uniquely dynamic settings: user\ninterests, item pools, and popularity trends shift continuously, and models\nmust adapt in real time without forgetting past preferences. While existing\ntutorials on continual or lifelong learning cover broad machine learning\ndomains (e.g., vision and graphs), they do not address recommendation-specific\ndemands-such as balancing stability and plasticity per user, handling\ncold-start items, and optimizing recommendation metrics under streaming\nfeedback. This tutorial aims to make a timely contribution by filling that gap.\nWe begin by reviewing the background and problem settings, followed by a\ncomprehensive overview of existing approaches. We then highlight recent efforts\nto apply continual learning to practical deployment environments, such as\nresource-constrained systems and sequential interaction settings. Finally, we\ndiscuss open challenges and future research directions. We expect this tutorial\nto benefit researchers and practitioners in recommender systems, data mining,\nAI, and information retrieval across academia and industry.", "published": "2025-07-05 02:20:15", "link": "http://arxiv.org/abs/2507.03861v1", "categories": ["cs.IR", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Invariants for sum-rank metric codes", "abstract": "The code equivalence problem is central in coding theory and cryptography.\nWhile classical invariants are effective for Hamming and rank metrics, the\nsum-rank metric, which unifies both, introduces new challenges. This paper\nintroduces new invariants for sum-rank metric codes: generalised idealisers,\nthe centraliser, the center, and a refined notion of linearity. These lead to\nthe definition of nuclear parameters, inspired by those used in division\nalgebra theory, where they are crucial for proving inequivalence. We also\ndevelop a computational framework based on skew polynomials, which is isometric\nto the classical matrix setting but enables explicit computation of nuclear\nparameters for known MSRD (Maximum Sum-Rank Distance) codes. This yields a new\nand effective method to study the code equivalence problem where traditional\ntools fall short. In fact, using nuclear parameters, we can study the\nequivalence among the largest families of known MSRD codes.", "published": "2025-07-05 20:49:20", "link": "http://arxiv.org/abs/2507.04158v1", "categories": ["cs.IT", "math.CO", "math.IT", "12E10, 16S36, 94B60"], "primary_category": "cs.IT"}
{"title": "LCD and self-orthogonal twisted group codes over finite commutative chain rings", "abstract": "In this paper, we shall study k-Galois LCD constacyclic group codes over\nfinite commutative chain rings with identity. In particular, we shall\ncharacterize Galois LCD constacyclic codes over finite commutative chain ring\nwith identity in terms of its idempotent generators and the classical\ninvolution using the twisted group ring structures and find some good LCD\ncodes.", "published": "2025-07-05 11:49:07", "link": "http://arxiv.org/abs/2507.04013v1", "categories": ["math.RA", "cs.IT", "math.IT", "Primary 20C05, Secondary 16S34"], "primary_category": "math.RA"}
{"title": "Security proof for parallel DIQKD", "abstract": "We present a parallel device independent quantum key distribution (DIQKD)\nprotocol based on the CHSH game and prove its security. Using techniques\ndeveloped for analysing the parallel repetition of anchored non-local games, we\nshow that the answers on a small random linear subset of the games in the DIQKD\nprotocol can be simulated as the output of a single-round strategy for playing\nthe CHSH game. Then, we use the recently developed unstructured approximate\nentropy accumulation theorem to establish the smooth min-entropy lower bound\nrequired for the security proof. Our approach yields a more\ninformation-theoretic and general proof for parallel DIQKD compared to previous\nproofs.", "published": "2025-07-05 10:42:09", "link": "http://arxiv.org/abs/2507.03991v1", "categories": ["quant-ph", "cs.IT", "math.IT"], "primary_category": "quant-ph"}
{"title": "FollowSpot: Enhancing Wireless Communications via Movable Ceiling-Mounted Metasurfaces", "abstract": "This paper studies the optimal placement of ceiling-mounted metasurfaces\n(MTSs) to help focus the wireless signal beam onto the target receiver, as\ninspired by the theatre spotlight. We assume that a total of $M$ MTSs are\ndeployed, and that there are $L$ possible positions for each MTS. The resulting\nsignal-to-noise (SNR) maximization problem is difficult to tackle directly\nbecause of the coupling between the placement decisions of the different MTSs.\nMathematically, we are faced with a nonlinear discrete optimization problem\nwith $L^M$ possible solutions. A remarkable result shown in this paper is that\nthe above challenging problem can be efficiently solved within\n$O(ML^2\\log(ML))$ time. There are two key steps in developing the proposed\nalgorithm. First, we successfully decouple the placement variables of different\nMTSs by introducing a continuous auxiliary variable $\\mu$; the discrete primal\nvariables are now easy to optimize when $\\mu$ is held fixed, but the\noptimization problem of $\\mu$ is nonconvex. Second, we show that the\noptimization of continuous $\\mu$ can be recast into a discrete optimization\nproblem with only $LM$ possible solutions, so the optimal $\\mu$ can now be\nreadily obtained. Numerical results show that the proposed algorithm can not\nonly guarantee a global optimum but also reach the optimal solution\nefficiently.", "published": "2025-07-05 06:55:27", "link": "http://arxiv.org/abs/2507.03918v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Resource Allocation for Multi-waveguide Pinching Antenna-assisted Broadcast Networks", "abstract": "In this paper, we investigate the resource allocation for multi-dielectric\nwaveguide-assisted broadcast systems, where each waveguide employs multiple\npinching antennas (PAs), aiming to maximize the minimum achievable rate among\nmultiple users. To capture realistic propagation effects, we propose a novel\ngeneralized frequency-dependent power attenuation model for dielectric\nwaveguides PA system. We jointly optimize waveguide beamforming, PA power\nallocation, and antenna positions via a block coordinate descent scheme that\ncapitalizes on majorization minimization and penalty methods, circumventing the\ninherent non-convexity of the formulated optimization problem and obtaining a\ncomputationally efficient sub-optimal solution. Simulation results demonstrate\nthat our proposed framework substantially outperforms both conventional antenna\nsystems and single-PA-per-waveguide configurations, clearly illustrating the\nintricate trade-offs between waveguide propagation loss, path loss, and\nresource allocation among multiple PAs.", "published": "2025-07-05 06:15:12", "link": "http://arxiv.org/abs/2507.03915v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Difference Imaging-Based Parking Lot Surveillance in Multi-RIS-Aided Collaborative ISAC System", "abstract": "Parking lot surveillance with integrated sensing and communication (ISAC)\nsystem is one of the potential application scenarios defined by 3rd Generation\nPartnership Project (3GPP). Traditional surveillance systems using cameras or\nmagnetic sensors face limitations such as light dependence, high costs, and\nconstrained scalability. Wireless sensing with reconfigurable intelligent\nsurfaces (RISs) has the ability to address the above limitations due to its\nlight independence and lower deployment overhead. In this study, we propose a\ndifference imaging-based multi-RIS-aided collaborative ISAC system to achieve\nparking lot surveillance. In a parking lot, the presence of vehicles induces\nimpacts on wireless environments due to scattering characteristic variation. By\ndelineating the parking lot into a two-dimensional image with several grid\nunits, the proposed system can capture the variation of their scattering\ncoefficients in free and occupied states. The variation between these two\nstates is sparse, which can be captured through compressed sensing (CS)-based\nimaging algorithms. Additionally, we collaboratively employ multiple RISs to\nenable higher surveillance performance. Experimental results demonstrate that\nour method can achieve high-accuracy parking occupancy detection, and the\nemployment of collaborative RISs further enhances the detection rate.", "published": "2025-07-05 01:15:27", "link": "http://arxiv.org/abs/2507.03851v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Study of AP Association and Users and Power Allocation for Cell-Free Massive MIMO Systems", "abstract": "This paper introduces an access point-user (AP-UE) association strategy\ncombined with pilot power allocation to mitigate multiuser interference and\nenhance spectral efficiency (SE) in clustered cell-free massive MIMO\n(CCF-mMIMO) networks. We propose a dynamic channel-based clustering method that\ngroups APs according to their channel correlation, ensuring users are\nassociated with APs exhibiting similar channel characteristics. The proposed\napproach exploits hierarchical clustering, enabling flexible cluster sizing to\nimprove interference management and overall SE. Moreover, we present a power\ncontrol (PC) technique that is based on a weighted sum-rate maximization (WSRM)\nalgorithm to ensure consistent service quality across users. Numerical results\ndemonstrate that the proposed method achieves superior SE and robust\nperformance in high-density multi-user environments as compared to competing\napproaches.", "published": "2025-07-05 01:03:06", "link": "http://arxiv.org/abs/2507.03848v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Enhancing Robustness of LLM-Driven Multi-Agent Systems through Randomized Smoothing", "abstract": "This paper presents a defense framework for enhancing the safety of large\nlanguage model (LLM) empowered multi-agent systems (MAS) in safety-critical\ndomains such as aerospace. We apply randomized smoothing, a statistical\nrobustness certification technique, to the MAS consensus context, enabling\nprobabilistic guarantees on agent decisions under adversarial influence. Unlike\ntraditional verification methods, our approach operates in black-box settings\nand employs a two-stage adaptive sampling mechanism to balance robustness and\ncomputational efficiency. Simulation results demonstrate that our method\neffectively prevents the propagation of adversarial behaviors and\nhallucinations while maintaining consensus performance. This work provides a\npractical and scalable path toward safe deployment of LLM-based MAS in\nreal-world, high-stakes environments.", "published": "2025-07-05 17:26:08", "link": "http://arxiv.org/abs/2507.04105v1", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI"}
{"title": "HAWK: A Hierarchical Workflow Framework for Multi-Agent Collaboration", "abstract": "Contemporary multi-agent systems encounter persistent challenges in\ncross-platform interoperability, dynamic task scheduling, and efficient\nresource sharing. Agents with heterogeneous implementations often lack\nstandardized interfaces; collaboration frameworks remain brittle and hard to\nextend; scheduling policies are static; and inter-agent state synchronization\nis insufficient. We propose Hierarchical Agent Workflow (HAWK), a modular\nframework comprising five layers-User, Workflow, Operator, Agent, and\nResource-and supported by sixteen standardized interfaces. HAWK delivers an\nend-to-end pipeline covering task parsing, workflow orchestration, intelligent\nscheduling, resource invocation, and data synchronization. At its core lies an\nadaptive scheduling and optimization module in the Workflow Layer, which\nharnesses real-time feedback and dynamic strategy adjustment to maximize\nutilization. The Resource Layer provides a unified abstraction over\nheterogeneous data sources, large models, physical devices, and third-party\nservices&tools, simplifying cross-domain information retrieval. We demonstrate\nHAWK's scalability and effectiveness via CreAgentive, a multi-agent\nnovel-generation prototype, which achieves marked gains in throughput, lowers\ninvocation complexity, and improves system controllability. We also show how\nhybrid deployments of large language models integrate seamlessly within HAWK,\nhighlighting its flexibility. Finally, we outline future research\navenues-hallucination mitigation, real-time performance tuning, and enhanced\ncross-domain adaptability-and survey prospective applications in healthcare,\ngovernment, finance, and education.", "published": "2025-07-05 15:03:53", "link": "http://arxiv.org/abs/2507.04067v1", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI"}
{"title": "CortexDebate: Debating Sparsely and Equally for Multi-Agent Debate", "abstract": "Nowadays, single Large Language Model (LLM) struggles with critical issues\nsuch as hallucination and inadequate reasoning abilities. To mitigate these\nissues, Multi-Agent Debate (MAD) has emerged as an effective strategy, where\nLLM agents engage in in-depth debates with others on tasks. However, existing\nMAD methods face two major issues: (a) too lengthy input contexts, which causes\nLLM agents to get lost in plenty of input information and experiences\nperformance drop; and (b) the overconfidence dilemma, where self-assured LLM\nagents dominate the debate, leading to low debating effectiveness. To address\nthese limitations, we propose a novel MAD method called \"CortexDebate\".\nInspired by the human brain's tendency to establish a sparse and dynamically\noptimized network among cortical areas governed by white matter, CortexDebate\nconstructs a sparse debating graph among LLM agents, where each LLM agent only\ndebates with the ones that are helpful to it. To optimize the graph, we propose\na module named McKinsey-based Debate Matter (MDM), which acts as an artificial\nanalog to white matter. By integrating the McKinsey Trust Formula, a\nwell-established measure of trustworthiness from sociology, MDM enables\ncredible evaluations that guide graph optimization. The effectiveness of our\nCortexDebate has been well demonstrated by extensive experimental results\nacross eight datasets from four task types.", "published": "2025-07-05 07:23:15", "link": "http://arxiv.org/abs/2507.03928v1", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI"}
{"title": "Agent Exchange: Shaping the Future of AI Agent Economics", "abstract": "The rise of Large Language Models (LLMs) has transformed AI agents from\npassive computational tools into autonomous economic actors. This shift marks\nthe emergence of the agent-centric economy, in which agents take on active\neconomic roles-exchanging value, making strategic decisions, and coordinating\nactions with minimal human oversight. To realize this vision, we propose Agent\nExchange (AEX), a specialized auction platform designed to support the dynamics\nof the AI agent marketplace. AEX offers an optimized infrastructure for agent\ncoordination and economic participation. Inspired by Real-Time Bidding (RTB)\nsystems in online advertising, AEX serves as the central auction engine,\nfacilitating interactions among four ecosystem components: the User-Side\nPlatform (USP), which translates human goals into agent-executable tasks; the\nAgent-Side Platform (ASP), responsible for capability representation,\nperformance tracking, and optimization; Agent Hubs, which coordinate agent\nteams and participate in AEX-hosted auctions; and the Data Management Platform\n(DMP), ensuring secure knowledge sharing and fair value attribution. We outline\nthe design principles and system architecture of AEX, laying the groundwork for\nagent-based economic infrastructure in future AI ecosystems.", "published": "2025-07-05 05:18:49", "link": "http://arxiv.org/abs/2507.03904v1", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI"}
{"title": "Physics-informed neural networks and neural operators for a study of EUV electromagnetic wave diffraction from a lithography mask", "abstract": "Physics-informed neural networks (PINNs) and neural operators (NOs) for\nsolving the problem of diffraction of Extreme Ultraviolet (EUV) electromagnetic\nwaves from a mask are presented. A novel hybrid Waveguide Neural Operator\n(WGNO) is introduced, which is based on a waveguide method with its most\ncomputationally expensive part replaced by a neural network. Numerical\nexperiments on realistic 2D and 3D masks show that the WGNO achieves\nstate-of-the-art accuracy and inference time, providing a highly efficient\nsolution for accelerating the design workflows of lithography masks.", "published": "2025-07-05 20:21:31", "link": "http://arxiv.org/abs/2507.04153v1", "categories": ["math.NA", "cs.AI", "cs.LG", "cs.NA", "physics.comp-ph", "physics.optics"], "primary_category": "math.NA"}
{"title": "Divergence-Kernel method for scores of random systems", "abstract": "We derive the divergence-kernel formula for the scores of random dynamical\nsystems, then formally pass to the continuous-time limit of SDEs. Our formula\nworks for multiplicative noise systems over any period of time; it does not\nrequire hyperbolicity. We also consider several special cases: (1) for additive\nnoise, we give a pure kernel formula; (2) for short-time, we give a pure\ndivergence formula; (3) we give a formula which does not involve scores of the\ninitial distribution. Based on the new formula, we derive a pathwise\nMonte-Carlo algorithm for scores, and demonstrate it on the 40-dimensional\nLorenz 96 system with multiplicative noise.", "published": "2025-07-05 13:20:24", "link": "http://arxiv.org/abs/2507.04035v1", "categories": ["math.PR", "cs.NA", "math.DS", "math.NA", "math.OC", "60H07, 60J60, 65D25, 65C30, 65C05"], "primary_category": "math.PR"}
{"title": "Remarkable upper bounds for the interpolation error constants on the triangles", "abstract": "We introduce remarkable upper bounds for the interpolation error constants on\ntriangles, which are sharp and given by simple formulas. These constants are\ncrucial in analyzing interpolation errors, particularly those associated with\nthe Finite Element Method. In this study, we proved boundness via the numerical\nverification method and asymptotic analysis. This study is also essential in\nthat it demonstrates a valuable application of the numerical verification\nmethod. The proof process of this study may be applied to the proof of various\nother norm inequalities.", "published": "2025-07-05 13:00:20", "link": "http://arxiv.org/abs/2507.04032v1", "categories": ["math.NA", "cs.NA", "65D05(Primary), 41A44(Secondary), 65N30, 41A44", "G.1.8; G.1.1; G.1.2"], "primary_category": "math.NA"}
{"title": "Exploring Exponential Runge-Kutta Methods: A Survey", "abstract": "In this survey, we provide an in-depth investigation of exponential\nRunge-Kutta methods for the numerical integration of initial-value problems.\nThese methods offer a valuable synthesis between classical Runge-Kutta methods,\nintroduced more than a century ago, and exponential integrators, which date\nback to the 1960s. This manuscript presents both a historical analysis of the\ndevelopment of these methods up to the present day and several examples aimed\nat making the topic accessible to a broad audience.", "published": "2025-07-05 12:32:18", "link": "http://arxiv.org/abs/2507.04024v1", "categories": ["math.NA", "cs.NA", "65L03, 65L04, 65L05, 65L20, 65M12"], "primary_category": "math.NA"}
{"title": "Mixed FEM for coupled unsteady fluid flow problems with $p$-type Brinkman-Forchheimer framework and its application for reverse-osmosis desalination", "abstract": "This work analyzes a fully discrete mixed finite element method in a Banach\nspace framework for solving nonstationary coupled fluid flow problems modeled\nby the Brinkman-Forchheimer equations, with applications to reverse osmosis.\nThe model couples unsteady $p$-type convective Brinkman-Forchheimer and\ntransport equations with nonlinear boundary conditions across a semi-permeable\nmembrane. A mixed formulation is used for the fluid equation\n(pseudostress-velocity) and for the transport equation (concentration, its\ngradient, and a Lagrange multiplier from the membrane condition). The\ncontinuous problem is reformulated in Banach spaces as a fixed-point problem,\nenabling a well-posedness analysis via differential-algebraic system theory.\nSpatial discretization employs lowest-order Raviart-Thomas elements for fluxes\nand piecewise constants for primal variables, while linear elements are used\nfor the Lagrange multiplier. A fully discrete Galerkin scheme with backward\nEuler time-stepping is proposed. Its well-posedness and stability are proven\nusing a fixed-point argument, and optimal convergence rates are established.\nNumerical results confirm the theoretical error estimates and demonstrate the\nmethod's effectiveness.", "published": "2025-07-05 09:55:04", "link": "http://arxiv.org/abs/2507.03974v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "On the discrete Poincar\u00e9 inequality for B-schemes of 1D Fokker-Planck equations in full space", "abstract": "In this paper, we propose two approaches to derive the discrete Poincar\\'e\ninequality for the B-schemes, a family of finite volume discretization schemes,\nfor the one-dimensional Fokker-Planck equation in full space. We study the\nproperties of the spatially discretized Fokker-Planck equation in the viewpoint\nof a continuous-time Markov chain. The first approach is based on\nGamma-calculus, through which we show that the Bakry-\\'Emery criterion still\nholds in the discrete setting. The second approach employs the Lyapunov\nfunction method, allowing us to extend a local discrete Poincar\\'e inequality\nto the full space. The assumptions required for both approaches are roughly\ncomparable with some minor differences. These methods have the potential to be\nextended to higher dimensions. As a result, we obtain exponential convergence\nto equilibrium for the discrete schemes by applying the discrete Poincar\\'e\ninequality.", "published": "2025-07-05 07:58:29", "link": "http://arxiv.org/abs/2507.03941v1", "categories": ["math.NA", "cs.NA", "math.PR"], "primary_category": "math.NA"}
{"title": "A discontinuous Galerkin pressure correction scheme for the Oldroyd model of order one", "abstract": "We develop and analyze a discontinuous Galerkin pressure correction scheme\nfor the Oldroyd model of order one. The existence and uniqueness of the\ndiscrete solution as well as the consistency of the scheme are proved. The\nstability of the discrete velocity and pressure are established. We derive\noptimal $\\textit{a priori}$ error bounds for the fully discrete velocity in the\ndiscontinuous discrete space. In addition, an improved error estimate for the\nvelocity is derived in the $L^2$ norm which is optimal with respect to space\nand time. Furthermore, the error bound for the pressure is obtained via the\nestimates of discrete time derivative of the velocity. Finally, numerical\nexperiments confirm the optimal convergence rates.", "published": "2025-07-05 05:49:45", "link": "http://arxiv.org/abs/2507.03909v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Where to Intervene: Action Selection in Deep Reinforcement Learning", "abstract": "Deep reinforcement learning (RL) has gained widespread adoption in recent\nyears but faces significant challenges, particularly in unknown and complex\nenvironments. Among these, high-dimensional action selection stands out as a\ncritical problem. Existing works often require a sophisticated prior design to\neliminate redundancy in the action space, relying heavily on domain expert\nexperience or involving high computational complexity, which limits their\ngeneralizability across different RL tasks. In this paper, we address these\nchallenges by proposing a general data-driven action selection approach with\nmodel-free and computationally friendly properties. Our method not only selects\nminimal sufficient actions but also controls the false discovery rate via\nknockoff sampling. More importantly, we seamlessly integrate the action\nselection into deep RL methods during online training. Empirical experiments\nvalidate the established theoretical guarantees, demonstrating that our method\nsurpasses various alternative techniques in terms of both performance in\nvariable selection and overall achieved rewards.", "published": "2025-07-05 23:40:55", "link": "http://arxiv.org/abs/2507.04187v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Generative Regression with IQ-BART", "abstract": "Implicit Quantile BART (IQ-BART) posits a non-parametric Bayesian model on\nthe conditional quantile function, acting as a model over a conditional model\nfor $Y$ given $X$. One of the key ingredients is augmenting the observed data\n$\\{(Y_i,X_i)\\}_{i=1}^n$ with uniformly sampled values $\\tau_i$ for $1\\leq i\\leq\nn$ which serve as training data for quantile function estimation. Using the\nfact that the location parameter $\\mu$ in a $\\tau$-tilted asymmetric Laplace\ndistribution corresponds to the $\\tau^{th}$ quantile, we build a check-loss\nlikelihood targeting $\\mu$ as the parameter of interest. We equip the\ncheck-loss likelihood parametrized by $\\mu=f(X,\\tau)$ with a BART prior on\n$f(\\cdot)$, allowing the conditional quantile function to vary both in $X$ and\n$\\tau$. The posterior distribution over $\\mu(\\tau,X)$ can be then distilled for\nestimation of the {\\em entire quantile function} as well as for assessing\nuncertainty through the variation of posterior draws. Simulation-based\npredictive inference is immediately available through inverse transform\nsampling using the learned quantile function. The sum-of-trees structure over\nthe conditional quantile function enables flexible distribution-free regression\nwith theoretical guarantees. As a byproduct, we investigate posterior mean\nquantile estimator as an alternative to the routine sample (posterior mode)\nquantile estimator. We demonstrate the power of IQ-BART on time series\nforecasting datasets where IQ-BART can capture multimodality in predictive\ndistributions that might be otherwise missed using traditional parametric\napproaches.", "published": "2025-07-05 21:42:08", "link": "http://arxiv.org/abs/2507.04168v1", "categories": ["stat.ME", "stat.ML"], "primary_category": "stat.ME"}
{"title": "Model selection for stochastic dynamics: a parsimonious and principled approach", "abstract": "This thesis focuses on the discovery of stochastic differential equations\n(SDEs) and stochastic partial differential equations (SPDEs) from noisy and\ndiscrete time series. A major challenge is selecting the simplest possible\ncorrect model from vast libraries of candidate models, where standard\ninformation criteria (AIC, BIC) are often limited.\n  We introduce PASTIS (Parsimonious Stochastic Inference), a new information\ncriterion derived from extreme value theory. Its penalty term, $n_\\mathcal{B}\n\\ln(n_0/p)$, explicitly incorporates the size of the initial library of\ncandidate parameters ($n_0$), the number of parameters in the considered model\n($n_\\mathcal{B}$), and a significance threshold ($p$). This significance\nthreshold represents the probability of selecting a model containing more\nparameters than necessary when comparing many models.\n  Benchmarks on various systems (Lorenz, Ornstein-Uhlenbeck, Lotka-Volterra for\nSDEs; Gray-Scott for SPDEs) demonstrate that PASTIS outperforms AIC, BIC,\ncross-validation (CV), and SINDy (a competing method) in terms of exact model\nidentification and predictive capability.\n  Furthermore, real-world data can be subject to large sampling intervals\n($\\Delta t$) or measurement noise ($\\sigma$), which can impair model learning\nand selection capabilities. To address this, we have developed robust variants\nof PASTIS, PASTIS-$\\Delta t$ and PASTIS-$\\sigma$, thus extending the\napplicability of the approach to imperfect experimental data.\n  PASTIS thus provides a statistically grounded, validated, and practical\nmethodological framework for discovering simple models for processes with\nstochastic dynamics.", "published": "2025-07-05 18:15:26", "link": "http://arxiv.org/abs/2507.04121v1", "categories": ["stat.ML", "cond-mat.stat-mech", "cs.LG", "physics.comp-ph"], "primary_category": "stat.ML"}
{"title": "How to Train Your LLM Web Agent: A Statistical Diagnosis", "abstract": "LLM-based web agents have recently made significant progress, but much of it\nhas occurred in closed-source systems, widening the gap with open-source\nalternatives. Progress has been held back by two key challenges: first, a\nnarrow focus on single-step tasks that overlooks the complexity of multi-step\nweb interactions; and second, the high compute costs required to post-train\nLLM-based web agents. To address this, we present the first statistically\ngrounded study on compute allocation for LLM web-agent post-training. Our\napproach uses a two-stage pipeline, training a Llama 3.1 8B student to imitate\na Llama 3.3 70B teacher via supervised fine-tuning (SFT), followed by on-policy\nreinforcement learning. We find this process highly sensitive to hyperparameter\nchoices, making exhaustive sweeps impractical. To spare others from expensive\ntrial-and-error, we sample 1,370 configurations and use bootstrapping to\nestimate effective hyperparameters. Our results show that combining SFT with\non-policy RL consistently outperforms either approach alone on both WorkArena\nand MiniWob++. Further, this strategy requires only 55% of the compute to match\nthe peak performance of pure SFT on MiniWob++, effectively pushing the\ncompute-performance Pareto frontier, and is the only strategy that can close\nthe gap with closed-source models.", "published": "2025-07-05 17:12:33", "link": "http://arxiv.org/abs/2507.04103v1", "categories": ["cs.AI", "cs.LG", "stat.ML"], "primary_category": "cs.AI"}
{"title": "Attributing Data for Sharpness-Aware Minimization", "abstract": "Sharpness-aware Minimization (SAM) improves generalization in large-scale\nmodel training by linking loss landscape geometry to generalization. However,\nchallenges such as mislabeled noisy data and privacy concerns have emerged as\nsignificant issues. Data attribution, which identifies the contributions of\nspecific training samples, offers a promising solution. However, directly\nrendering existing data influence evaluation tools such as influence functions\n(IF) to SAM will be inapplicable or inaccurate as SAM utilizes an inner loop to\nfind model perturbations that maximize loss, which the outer loop then\nminimizes, resulting in a doubled computational structure. Additionally, this\nbilevel structure complicates the modeling of data influence on the parameters.\nIn this paper, based on the IF, we develop two innovative data valuation\nmethods for SAM, each offering unique benefits in different scenarios: the\nHessian-based IF and the Gradient Trajectory-based IF. The first one provides a\ncomprehensive estimation of data influence using a closed-form measure that\nrelies only on the trained model weights. In contrast, the other IF for SAM\nutilizes gradient trajectory information during training for more accurate and\nefficient data assessment. Extensive experiments demonstrate their\neffectiveness in data evaluation and parameter tuning, with applications in\nidentifying mislabeled data, model editing, and enhancing interpretability.", "published": "2025-07-05 14:46:42", "link": "http://arxiv.org/abs/2507.04059v1", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Benchmarking Stochastic Approximation Algorithms for Fairness-Constrained Training of Deep Neural Networks", "abstract": "The ability to train Deep Neural Networks (DNNs) with constraints is\ninstrumental in improving the fairness of modern machine-learning models. Many\nalgorithms have been analysed in recent years, and yet there is no standard,\nwidely accepted method for the constrained training of DNNs. In this paper, we\nprovide a challenging benchmark of real-world large-scale fairness-constrained\nlearning tasks, built on top of the US Census (Folktables). We point out the\ntheoretical challenges of such tasks and review the main approaches in\nstochastic approximation algorithms. Finally, we demonstrate the use of the\nbenchmark by implementing and comparing three recently proposed, but as-of-yet\nunimplemented, algorithms both in terms of optimization performance, and\nfairness improvement. We release the code of the benchmark as a Python package\nat https://github.com/humancompatible/train.", "published": "2025-07-05 13:01:18", "link": "http://arxiv.org/abs/2507.04033v1", "categories": ["cs.LG", "cs.CY", "math.OC", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Real-TabPFN: Improving Tabular Foundation Models via Continued Pre-training With Real-World Data", "abstract": "Foundation models for tabular data, like TabPFN, achieve strong performance\non small datasets when pre-trained solely on synthetic data. We show that this\nperformance can be significantly boosted by a targeted continued pre-training\nphase. Specifically, we demonstrate that leveraging a small, curated collection\nof large, real-world datasets for continued pre-training yields superior\ndownstream predictive accuracy compared to using broader, potentially noisier\ncorpora like CommonCrawl or GitTables. Our resulting model, Real-TabPFN,\nachieves substantial performance gains on 29 datasets from the OpenML AutoML\nBenchmark.", "published": "2025-07-05 09:39:07", "link": "http://arxiv.org/abs/2507.03971v1", "categories": ["cs.LG", "cs.AI", "stat.ME", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Return of the Latent Space COWBOYS: Re-thinking the use of VAEs for Bayesian Optimisation of Structured Spaces", "abstract": "Bayesian optimisation in the latent space of a Variational AutoEncoder (VAE)\nis a powerful framework for optimisation tasks over complex structured domains,\nsuch as the space of scientifically interesting molecules. However, existing\napproaches tightly couple the surrogate and generative models, which can lead\nto suboptimal performance when the latent space is not tailored to specific\ntasks, which in turn has led to the proposal of increasingly sophisticated\nalgorithms. In this work, we explore a new direction, instead proposing a\ndecoupled approach that trains a generative model and a Gaussian Process (GP)\nsurrogate separately, then combines them via a simple yet principled Bayesian\nupdate rule. This separation allows each component to focus on its strengths --\nstructure generation from the VAE and predictive modelling by the GP. We show\nthat our decoupled approach improves our ability to identify high-potential\ncandidates in molecular optimisation problems under constrained evaluation\nbudgets.", "published": "2025-07-05 05:53:04", "link": "http://arxiv.org/abs/2507.03910v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Risk-sensitive Actor-Critic with Static Spectral Risk Measures for Online and Offline Reinforcement Learning", "abstract": "The development of Distributional Reinforcement Learning (DRL) has introduced\na natural way to incorporate risk sensitivity into value-based and actor-critic\nmethods by employing risk measures other than expectation in the value\nfunction. While this approach is widely adopted in many online and offline RL\nalgorithms due to its simplicity, the naive integration of risk measures often\nresults in suboptimal policies. This limitation can be particularly harmful in\nscenarios where the need for effective risk-sensitive policies is critical and\nworst-case outcomes carry severe consequences. To address this challenge, we\npropose a novel framework for optimizing static Spectral Risk Measures (SRM), a\nflexible family of risk measures that generalizes objectives such as CVaR and\nMean-CVaR, and enables the tailoring of risk preferences. Our method is\napplicable to both online and offline RL algorithms. We establish theoretical\nguarantees by proving convergence in the finite state-action setting. Moreover,\nthrough extensive empirical evaluations, we demonstrate that our algorithms\nconsistently outperform existing risk-sensitive methods in both online and\noffline environments across diverse domains.", "published": "2025-07-05 04:41:54", "link": "http://arxiv.org/abs/2507.03900v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "GenAI-Powered Inference", "abstract": "We introduce GenAI-Powered Inference (GPI), a statistical framework for both\ncausal and predictive inference using unstructured data, including text and\nimages. GPI leverages open-source Generative Artificial Intelligence (GenAI)\nmodels - such as large language models and diffusion models - not only to\ngenerate unstructured data at scale but also to extract low-dimensional\nrepresentations that capture their underlying structure. Applying machine\nlearning to these representations, GPI enables estimation of causal and\npredictive effects while quantifying associated estimation uncertainty. Unlike\nexisting approaches to representation learning, GPI does not require\nfine-tuning of generative models, making it computationally efficient and\nbroadly accessible. We illustrate the versatility of the GPI framework through\nthree applications: (1) analyzing Chinese social media censorship, (2)\nestimating predictive effects of candidates' facial appearance on electoral\noutcomes, and (3) assessing the persuasiveness of political rhetoric. An\nopen-source software package is available for implementing GPI.", "published": "2025-07-05 04:27:22", "link": "http://arxiv.org/abs/2507.03897v1", "categories": ["cs.LG", "stat.ME", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Unraveling the Black-box Magic: An Analysis of Neural Networks' Dynamic Local Extrema", "abstract": "We point out that neural networks are not black boxes, and their\ngeneralization stems from the ability to dynamically map a dataset to the local\nextrema of the model function. We further prove that the number of local\nextrema in a neural network is positively correlated with the number of its\nparameters, and on this basis, we give a new algorithm that is different from\nthe back-propagation algorithm, which we call the extremum-increment algorithm.\nSome difficult situations, such as gradient vanishing and overfitting, can be\nreasonably explained and dealt with in this framework.", "published": "2025-07-05 03:54:37", "link": "http://arxiv.org/abs/2507.03885v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Leo Breiman, the Rashomon Effect, and the Occam Dilemma", "abstract": "In the famous Two Cultures paper, Leo Breiman provided a visionary\nperspective on the cultures of ''data models'' (modeling with consideration of\ndata generation) versus ''algorithmic models'' (vanilla machine learning\nmodels). I provide a modern perspective on these approaches. One of Breiman's\nkey arguments against data models is the ''Rashomon Effect,'' which is the\nexistence of many different-but-equally-good models. The Rashomon Effect\nimplies that data modelers would not be able to determine which model generated\nthe data. Conversely, one of his core advantages in favor of data models is\nsimplicity, as he claimed there exists an ''Occam Dilemma,'' i.e., an\naccuracy-simplicity tradeoff. After 25 years of powerful computers, it has\nbecome clear that this claim is not generally true, in that algorithmic models\ndo not need to be complex to be accurate; however, there are nuances that help\nexplain Breiman's logic, specifically, that by ''simple,'' he appears to\nconsider only linear models or unoptimized decision trees. Interestingly, the\nRashomon Effect is a key tool in proving the nullification of the Occam\nDilemma. To his credit though, Breiman did not have the benefit of modern\ncomputers, with which my observations are much easier to make.\n  Breiman's goal for interpretability was somewhat intertwined with causality:\nsimpler models can help reveal which variables have a causal relationship with\nthe outcome. However, I argue that causality can be investigated without the\nuse of single models, whether or not they are simple. Interpretability is\nuseful in its own right, and I think Breiman knew that too.\n  Technically, my modern perspective does not belong to either of Breiman's Two\nCultures, but shares the goals of both of them - causality, simplicity,\naccuracy - and shows that these goals can be accomplished in other ways,\nwithout the limitations Breiman was concerned about.", "published": "2025-07-05 03:54:33", "link": "http://arxiv.org/abs/2507.03884v1", "categories": ["stat.ML", "cs.LG", "01, 62, 68", "K.2; I.2.6"], "primary_category": "stat.ML"}
{"title": "Transformer with Koopman-Enhanced Graph Convolutional Network for Spatiotemporal Dynamics Forecasting", "abstract": "Spatiotemporal dynamics forecasting is inherently challenging, particularly\nin systems defined over irregular geometric domains, due to the need to jointly\ncapture complex spatial correlations and nonlinear temporal dynamics. To tackle\nthese challenges, we propose TK-GCN, a two-stage framework that integrates\ngeometry-aware spatial encoding with long-range temporal modeling. In the first\nstage, a Koopman-enhanced Graph Convolutional Network (K-GCN) is developed to\nembed the high-dimensional dynamics distributed on spatially irregular domains\ninto a latent space where the evolution of system states is approximately\nlinear. By leveraging Koopman operator theory, this stage enhances the temporal\nconsistency during the latent learning. In the second stage, a Transformer\nmodule is employed to model the temporal progression within the Koopman-encoded\nlatent space. Through the self-attention mechanism, the Transformer captures\nlong-range temporal dependencies, enabling accurate forecasting over extended\nhorizons. We evaluate TK-GCN in spatiotemporal cardiac dynamics forecasting and\nbenchmark its performance against several state-of-the-art baselines.\nExperimental results and ablation studies show that TK-GCN consistently\ndelivers superior predictive accuracy across a range of forecast horizons,\ndemonstrating its capability to effectively model complex spatial structures\nand nonlinear temporal dynamics.", "published": "2025-07-05 01:26:03", "link": "http://arxiv.org/abs/2507.03855v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Latent FxLMS: Accelerating Active Noise Control with Neural Adaptive Filters", "abstract": "Filtered-X LMS (FxLMS) is commonly used for active noise control (ANC),\nwherein the soundfield is minimized at a desired location. Given prior\nknowledge of the spatial region of the noise or control sources, we could\nimprove FxLMS by adapting along the low-dimensional manifold of possible\nadaptive filter weights. We train an auto-encoder on the filter coefficients of\nthe steady-state adaptive filter for each primary source location sampled from\na given spatial region and constrain the weights of the adaptive filter to be\nthe output of the decoder for a given state of latent variables. Then, we\nperform updates in the latent space and use the decoder to generate the\ncancellation filter. We evaluate how various neural network constraints and\nnormalization techniques impact the convergence speed and steady-state mean\nsquared error. Under certain conditions, our Latent FxLMS model converges in\nfewer steps with comparable steady-state error to the standard FxLMS.", "published": "2025-07-05 01:25:42", "link": "http://arxiv.org/abs/2507.03854v1", "categories": ["cs.LG", "cs.SD", "cs.SY", "eess.AS", "eess.SY", "nlin.AO", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Regularizing Log-Linear Cost Models for Inpatient Stays by Merging ICD-10 Codes", "abstract": "Cost models in healthcare research must balance interpretability, accuracy,\nand parameter consistency. However, interpretable models often struggle to\nachieve both accuracy and consistency. Ordinary least squares (OLS) models for\nhigh-dimensional regression can be accurate but fail to produce stable\nregression coefficients over time when using highly granular ICD-10 diagnostic\ncodes as predictors. This instability arises because many ICD-10 codes are\ninfrequent in healthcare datasets. While regularization methods such as Ridge\ncan address this issue, they risk discarding important predictors. Here, we\ndemonstrate that reducing the granularity of ICD-10 codes is an effective\nregularization strategy within OLS while preserving the representation of all\ndiagnostic code categories. By truncating ICD-10 codes from seven characters\n(e.g., T67.0XXA, T67.0XXD) to six (e.g., T67.0XX) or fewer, we reduce the\ndimensionality of the regression problem while maintaining model\ninterpretability and consistency. Mathematically, the merging of predictors in\nOLS leads to increased trace of the Hessian matrix, which reduces the variance\nof coefficient estimation. Our findings explain why broader diagnostic\ngroupings like DRGs and HCC codes are favored over highly granular ICD-10 codes\nin real-world risk adjustment and cost models.", "published": "2025-07-05 00:07:38", "link": "http://arxiv.org/abs/2507.03843v1", "categories": ["cs.LG", "stat.AP", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Ambisonics Encoder for Wearable Array with Improved Binaural Reproduction", "abstract": "Ambisonics Signal Matching (ASM) is a recently proposed signal-independent\napproach to encoding Ambisonic signal from wearable microphone arrays, enabling\nefficient and standardized spatial sound reproduction. However, reproduction\naccuracy is currently limited due to the non-ideal layout of the microphones.\nThis research introduces an enhanced ASM encoder that reformulates the loss\nfunction by integrating a Binaural Signal Matching (BSM) term into the\noptimization framework. The aim of this reformulation is to improve the\naccuracy of binaural reproduction when integrating the Ambisonic signal with\nHead-Related Transfer Functions (HRTFs), making the encoded Ambisonic signal\nbetter suited for binaural reproduction. This paper first presents the\nmathematical formulation developed to align the ASM and BSM objectives in a\nsingle loss function, followed by a simulation study with a simulated\nmicrophone array mounted on a rigid sphere representing a head-mounted wearable\narray. The analysis shows that improved binaural reproduction with the encoded\nAmbisonic signal can be achieved using this joint ASM-BSM optimization, thereby\nenabling higher-quality binaural playback for virtual and augmented reality\napplications based on Ambisonics.", "published": "2025-07-05 17:32:46", "link": "http://arxiv.org/abs/2507.04108v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "CLEP-DG: Contrastive Learning for Speech Emotion Domain Generalization via Soft Prompt Tuning", "abstract": "Speech Emotion Recognition (SER) is fundamental to affective computing and\nhuman-computer interaction, yet existing models struggle to generalize across\ndiverse acoustic conditions. While Contrastive Language-Audio Pretraining\n(CLAP) provides strong multimodal alignment, it lacks dedicated mechanisms for\ncapturing emotional cues, making it suboptimal for SER. To address this, we\npropose CLEP-DG, a framework that enhances CLAP's robustness in emotion\nrecognition. First, we fine-tune CLAP to obtain CLEP, adapting it on\nlarge-scale emotional speech datasets to better encode emotion-relevant\nfeatures. Then, we introduce Acoustic Context Prompt Tuning (ACPT), a\ntext-driven augmentation strategy that optimizes learnable prompt vectors to\nmodel diverse acoustic environments without additional labeled audio. Finally,\nleveraging cross-modal transferability, we train a classifier on text-derived\nembeddings and apply it to the audio encoder during inference, mitigating\ndomain shifts between textual supervision and audio-based emotion recognition.\nExperiments across five benchmark datasets show that CLEP-DG outperforms prior\nCLAP-based approaches, achieving state-of-the-art performance in both\nsupervised and domain generalization settings.", "published": "2025-07-05 14:16:06", "link": "http://arxiv.org/abs/2507.04048v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Prosody Labeling with Phoneme-BERT and Speech Foundation Models", "abstract": "This paper proposes a model for automatic prosodic label annotation, where\nthe predicted labels can be used for training a prosody-controllable\ntext-to-speech model. The proposed model utilizes not only rich acoustic\nfeatures extracted by a self-supervised-learning (SSL)-based model or a Whisper\nencoder, but also linguistic features obtained from phoneme-input pretrained\nlinguistic foundation models such as PnG BERT and PL-BERT. The concatenation of\nacoustic and linguistic features is used to predict phoneme-level prosodic\nlabels. In the experimental evaluation on Japanese prosodic labels, including\npitch accents and phrase break indices, it was observed that the combination of\nboth speech and linguistic foundation models enhanced the prediction accuracy\ncompared to using either a speech or linguistic input alone. Specifically, we\nachieved 89.8% prediction accuracy in accent labels, 93.2% in high-low pitch\naccents, and 94.3% in break indices.", "published": "2025-07-05 05:56:49", "link": "http://arxiv.org/abs/2507.03912v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Traceable TTS: Toward Watermark-Free TTS with Strong Traceability", "abstract": "Recent advances in Text-To-Speech (TTS) technology have enabled synthetic\nspeech to mimic human voices with remarkable realism, raising significant\nsecurity concerns. This underscores the need for traceable TTS models-systems\ncapable of tracing their synthesized speech without compromising quality or\nsecurity. However, existing methods predominantly rely on explicit watermarking\non speech or on vocoder, which degrades speech quality and is vulnerable to\nspoofing. To address these limitations, we propose a novel framework for model\nattribution. Instead of embedding watermarks, we train the TTS model and\ndiscriminator using a joint training method that significantly improves\ntraceability generalization while preserving-and even slightly improving-audio\nquality. This is the first work toward watermark-free TTS with strong\ntraceability. To promote progress in related fields, we will release the code\nupon acceptance of the paper.", "published": "2025-07-05 03:59:17", "link": "http://arxiv.org/abs/2507.03887v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Experimental Demonstration of Computational AoA Detection Using Conformal Frequency Diverse Metasurface Antennas", "abstract": "Devices that detect angle-of-arrival (AoA) over a wide field of view are\ncrucial for various applications such as wireless communication and navigation.\nThey are often installed on platforms with challenging mechanical and stealth\nconstraints like vehicles, drones, and helmets, where traditional methods --\nmechanically rotating antennas or conformal arrays -- tend to be bulky, heavy,\nand costly. A recent work has proposed a conformal frequency diverse antenna\nthat is designed to produce angularly diverse patterns that encode angular\ninformation into frequency sweeps. This capability allows AoA to be determined\nacross the entire horizon using only two receiving units. This paper\nexperimentally validates this concept, detailing the prototyping process and\npractical design considerations. The AoA detection capabilities of the proposed\ndevice are confirmed through experimental demonstrations. The proposed\nconformal metasurfaces offer an alternative hardware solution for sensing over\nlarge fields of view, with potential applications in radar sensing, situational\nawareness, and navigation.", "published": "2025-07-05 22:11:43", "link": "http://arxiv.org/abs/2507.04178v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "CSI-Free Symbol Detection for Atomic MIMO Receivers via In-Context Learning", "abstract": "Atomic receivers based on Rydberg vapor cells as sensors of electromagnetic\nfields offer a promising alternative to conventional radio frequency\nfront-ends. In multi-antenna configurations, the magnitude-only,\nphase-insensitive measurements produced by atomic receivers pose challenges for\ntraditional detection methods. Existing solutions rely on two-step iterative\noptimization processes, which suffer from cascaded channel estimation errors\nand high computational complexity. We propose a channel state information\n(CSI)-free symbol detection method based on in-context learning (ICL), which\ndirectly maps pilot-response pairs to data symbol predictions without explicit\nchannel estimation. Simulation results show that ICL achieves competitive\naccuracy with {higher computational efficiency} compared to existing solutions.", "published": "2025-07-05 13:39:59", "link": "http://arxiv.org/abs/2507.04040v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Differentiable High-Performance Ray Tracing-Based Simulation of Radio Propagation with Point Clouds", "abstract": "Ray tracing is a widely used deterministic method for radio propagation\nsimulations, capable of producing physically accurate multipath components. The\naccuracy depends on the quality of the environment model and its\nelectromagnetic properties. Recent advances in computer vision and machine\nlearning have made it possible to reconstruct detailed environment models\naugmented with semantic segmentation labels.\n  In this letter, we propose a differentiable ray tracing-based radio\npropagation simulator that operates directly on point clouds. We showcase the\nefficiency of our method by simulating multi-bounce propagation paths with up\nto five interactions with specular reflections and diffuse scattering in two\nindoor scenarios, each completing in less than 90 ms. Lastly, we demonstrate\nhow the differentiability of electromagnetic computations can be combined with\nsegmentation labels to learn the electromagnetic properties of the environment.", "published": "2025-07-05 12:27:07", "link": "http://arxiv.org/abs/2507.04021v1", "categories": ["eess.SP", "cs.CV"], "primary_category": "eess.SP"}
{"title": "An Efficient Detector for Faulty GNSS Measurements Detection With Non-Gaussian Noises", "abstract": "Fault detection is crucial to ensure the reliability of navigation systems.\nHowever, mainstream fault detection methods are developed based on Gaussian\nassumptions on nominal errors, while current attempts at non-Gaussian fault\ndetection are either heuristic or lack rigorous statistical properties. The\nperformance and reliability of these methods are challenged in real-world\napplications. This paper proposes the jackknife detector, a fault detection\nmethod tailored for linearized pseudorange-based positioning systems under\nnon-Gaussian nominal errors. Specifically, by leveraging the jackknife\ntechnique, a test statistic is derived as a linear combination of measurement\nerrors, eliminating the need for restrictive distributional assumptions while\nmaintaining computational efficiency. A hypothesis test with the Bonferroni\ncorrection is then constructed to detect potential faults in measurements.\nTheoretical analysis proves the equivalence between the jackknife detector and\nthe solution separation (SS) detector, while revealing the former's superior\ncomputational efficiency. Through a worldwide simulation and a real-world\nsatellite clock anomaly detection experiment--both involving non-Gaussian\nnominal errors--the proposed jackknife detector demonstrates equivalent\ndetection performance to the SS detector but achieves a fourfold improvement in\ncomputational efficiency. These results highlight the jackknife detector's\nsubstantial potential for real-time applications requiring robust and efficient\nfault detection in non-Gaussian noise environments.", "published": "2025-07-05 10:33:12", "link": "http://arxiv.org/abs/2507.03987v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "MMOC: Self-Supervised EEG Emotion Recognition Framework with Multi-Model Online Collaboration", "abstract": "Electroencephalography (EEG) emotion recognition plays a crucial role in\nhuman-computer interaction, particularly in healthcare and neuroscience. While\nsupervised learning has been widely used, its reliance on manual annotations\nintroduces high costs and potential bias. Self-supervised learning (SSL) offers\na promising alternative by generating labels through pretext tasks. However,\nhigh inter-subject variability in EEG signals leads to significant data drift,\nlimiting self-supervised models' generalization across unseen subjects.\nTraditional domain adaptation (DA) methods require access to target-domain data\nduring training. Although domain generalization (DG) avoids this constraint, it\noften falls short in handling complex data drift due to limited coverage of\npossible target distributions. To tackle these challenges, we propose MMOC, a\nself-supervised framework with multi-model online collaboration (MMOC), to\nachieve online adaptation to unseen data. MMOC trains multiple base models\nusing diverse strategies rooted in reconstruction and contrastive learning,\nenabling each model to develop distinct generalization capabilities. During\ninference, MMOC dynamically activates the most suitable model for each test\nsample via a loss-based routing mechanism that evaluates both contrastive and\nreconstruction losses. This dual consideration allows for a comprehensive\nmeasurement of data drift at both structural and semantic levels. Experimental\nresults on the SEED and Dreamer datasets show that MMOC achieves\nstate-of-the-art performance: 85.39% on SEED, and 68.77% and 69.37% on Dreamer\narousal and valence dimensions, respectively. MMOC effectively mitigates\ninter-subject data drift, offering a practical solution for real-world EEG\nemotion recognition.", "published": "2025-07-05 10:03:56", "link": "http://arxiv.org/abs/2507.03977v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "SAFERad: A Framework to Enable Radar Data for Safety-Relevant Perception Tasks", "abstract": "Radar sensors play a crucial role for perception systems in automated driving\nbut suffer from a high level of noise. In the past, this could be solved by\nstrict filters, which remove most false positives at the expense of undetected\nobjects. Future highly automated functions are much more demanding with respect\nto error rate. Hence, if the radar sensor serves as a component of perception\nsystems for such functions, a simple filter strategy cannot be applied. In this\npaper, we present a modified filtering approach which is characterized by the\nidea to vary the filtering depending on the potential of harmful collision with\nthe object which is potentially represented by the radar point. We propose an\nalgorithm which determines a criticality score for each point based on the\nplanned or presumable trajectory of the automated vehicle. Points identified as\nvery critical can trigger manifold actions to confirm or deny object presence.\nOur pipeline introduces criticality regions. The filter threshold in these\ncriticality regions is omitted. Commonly known radar data sets do not or barely\nfeature critical scenes. Thus, we present an approach to evaluate our framework\nby adapting the planned trajectory towards vulnerable road users, which serve\nas ground truth critical points. Evaluation of the criticality metric prove\nhigh recall rates. Besides, our post-processing algorithm lowers the rate of\nnon-clustered critical points by 74.8 % in an exemplary setup compared to a\nmoderate, generic filter.", "published": "2025-07-05 08:51:51", "link": "http://arxiv.org/abs/2507.03959v1", "categories": ["eess.SP", "cs.RO"], "primary_category": "eess.SP"}
{"title": "Structure from Noise: Confirmation Bias in Particle Picking in Structural Biology", "abstract": "Confirmation bias is a fundamental challenge in cryo-electron microscopy\n(cryo-EM) and cryo-electron tomography (cryo-ET), where prior expectations can\nlead to systematic errors in data interpretation. This bias may emerge at\nmultiple stages of the reconstruction pipeline, and in particular in the\ncritical particle picking stage, where 2D particles (in cryo-EM) or 3D\nsubtomograms (in cryo-ET) are extracted from highly noisy micrographs or\ntomograms. Focusing on two widely used methodologies, template matching and\ndeep neural networks, we combine theoretical analysis with controlled\nexperiments to demonstrate that both methods, when applied to pure noise, can\nproduce persistent molecular structures, a phenomenon we term structure from\nnoise. This artifact highlights a critical vulnerability in current workflows:\nthe potential for particle-picking algorithms to inject strong prior-driven\nbias into downstream analyses. We then propose practical mitigation strategies\nto reduce the impact of such biases. Together, our findings deepen the\ntheoretical understanding of confirmation bias in cryo-EM and cryo-ET and call\nfor cautious interpretation of reconstructions, primarily when relying on\ntemplate-driven particle picking.", "published": "2025-07-05 08:27:56", "link": "http://arxiv.org/abs/2507.03951v1", "categories": ["eess.SP", "q-bio.QM"], "primary_category": "eess.SP"}
{"title": "RateCount: Learning-Free Device Counting by Wi-Fi Probe Listening", "abstract": "A Wi-Fi-enabled device, or simply Wi-Fi device, sporadically broadcasts probe\nrequest frames (PRFs) to discover nearby access points (APs), whether connected\nto an AP or not. To protect user privacy, unconnected devices often randomize\ntheir MAC addresses in the PRFs, known as MAC address randomization. While\nprior works have achieved accurate device counting under MAC address\nrandomization, they typically rely on machine learning, resulting in\ninefficient deployment due to the time-consuming processes of data cleaning,\nmodel training, and hyperparameter tuning. To enhance deployment efficiency, we\npropose RateCount, an accurate, lightweight, and learning-free counting\napproach based on the rate at which APs receive PRFs within a window. RateCount\nemploys a provably unbiased closed-form expression to estimate the device count\ntime-averaged over the window and an error model to compute the lower bound of\nthe estimation variance. We also demonstrate how to extend RateCount to people\ncounting by incorporating a device-to-person calibration scheme. Through\nextensive real-world experiments conducted at multiple sites spanning a wide\nrange of counts, we show that RateCount, without any deployment costs for\nmachine learning, achieves comparable counting accuracy with the\nstate-of-the-art learning-based device counting and improves previous people\ncounting schemes by a large margin.", "published": "2025-07-05 03:06:16", "link": "http://arxiv.org/abs/2507.03873v1", "categories": ["cs.NI", "eess.SP"], "primary_category": "cs.NI"}
{"title": "A Variational Bayesian Detector for Affine Frequency Division Multiplexing", "abstract": "This paper proposes a variational Bayesian (VB) detector for affine frequency\ndivision multiplexing (AFDM) systems. The proposed method estimates the symbol\nprobability distribution by minimizing the Kullback-Leibler (KL) divergence\nbetween the true posterior and an approximate distribution, thereby enabling\nlow-complexity soft-decision detection. Compared to conventional approaches\nsuch as zero-forcing (ZF), Linear minimum mean square rrror (LMMSE), and the\nmessage passing algorithm (MPA), the proposed detector demonstrates lower bit\nerror rates (BER), faster convergence, and improved robustness under complex\nmultipath channels. Simulation results confirm its dual advantages in\ncomputational efficiency and detection performance.", "published": "2025-07-05 01:50:51", "link": "http://arxiv.org/abs/2507.03858v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Robust Node Localization for Rough and Extreme Deployment Environments", "abstract": "Many applications have been identified which require the deployment of\nlarge-scale low-power wireless sensor networks. Some of the deployment\nenvironments, however, impose harsh operation conditions due to intense\ncross-technology interference, extreme weather conditions (heavy rainfall,\nexcessive heat, etc.), or rough motion, thereby affecting the quality and\npredictability of the wireless links the nodes establish. In localization\ntasks, these conditions often lead to significant errors in estimating the\nposition of target nodes. Motivated by the practical deployments of sensors on\nthe surface of different water bodies, we address the problem of identifying\nsusceptible nodes and robustly estimating their positions. We formulate these\ntasks as a compressive sensing problem and propose algorithms for both node\nidentification and robust estimation. Additionally, we design an optimal anchor\nconfiguration to maximize the robustness of the position estimation task. Our\nnumerical results and comparisons with competitive methods demonstrate that the\nproposed algorithms achieve both objectives with a modest number of anchors.\nSince our method relies only on target-to-anchor distances, it is broadly\napplicable and yields resilient, robust localization.", "published": "2025-07-05 01:27:07", "link": "http://arxiv.org/abs/2507.03856v1", "categories": ["eess.SP", "cs.RO", "math.OC"], "primary_category": "eess.SP"}
