{"title": "MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in\n  Conversations", "abstract": "Emotion recognition in conversations is a challenging task that has recently\ngained popularity due to its potential applications. Until now, however, a\nlarge-scale multimodal multi-party emotional conversational database containing\nmore than two speakers per dialogue was missing. Thus, we propose the\nMultimodal EmotionLines Dataset (MELD), an extension and enhancement of\nEmotionLines. MELD contains about 13,000 utterances from 1,433 dialogues from\nthe TV-series Friends. Each utterance is annotated with emotion and sentiment\nlabels, and encompasses audio, visual and textual modalities. We propose\nseveral strong multimodal baselines and show the importance of contextual and\nmultimodal information for emotion recognition in conversations. The full\ndataset is available for use at http:// affective-meld.github.io.", "published": "2018-10-05 03:50:24", "link": "http://arxiv.org/abs/1810.02508v6", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Integrating Weakly Supervised Word Sense Disambiguation into Neural\n  Machine Translation", "abstract": "This paper demonstrates that word sense disambiguation (WSD) can improve\nneural machine translation (NMT) by widening the source context considered when\nmodeling the senses of potentially ambiguous words. We first introduce three\nadaptive clustering algorithms for WSD, based on k-means, Chinese restaurant\nprocesses, and random walks, which are then applied to large word contexts\nrepresented in a low-rank space and evaluated on SemEval shared-task data. We\nthen learn word vectors jointly with sense vectors defined by our best WSD\nmethod, within a state-of-the-art NMT system. We show that the concatenation of\nthese vectors, and the use of a sense selection mechanism based on the weighted\naverage of sense vectors, outperforms several baselines including sense-aware\nones. This is demonstrated by translation on five language pairs. The\nimprovements are above one BLEU point over strong NMT baselines, +4% accuracy\nover all ambiguous nouns and verbs, or +20% when scored manually over several\nchallenging words.", "published": "2018-10-05 11:20:39", "link": "http://arxiv.org/abs/1810.02614v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From direct tagging to Tagging with sentences compression", "abstract": "In essence, the two tagging methods (direct tagging and tagging with\nsentences compression) are to tag the information we need by using regular\nexpression which basing on the inherent language patterns of the natural\nlanguage. Though it has many advantages in extracting regular data, Direct\ntagging is not applicable to some situations. if the data we need extract is\nnot regular and its surrounding words are regular is relatively regular, then\nwe can use information compression to cut the information we do not need before\nwe tagging the data we need. In this way we can increase the precision of the\ndata while not undermine the recall of the data.", "published": "2018-10-05 15:11:32", "link": "http://arxiv.org/abs/1810.02741v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Encode Text as Human-Readable Summaries using Generative\n  Adversarial Networks", "abstract": "Auto-encoders compress input data into a latent-space representation and\nreconstruct the original data from the representation. This latent\nrepresentation is not easily interpreted by humans. In this paper, we propose\ntraining an auto-encoder that encodes input text into human-readable sentences,\nand unpaired abstractive summarization is thereby achieved. The auto-encoder is\ncomposed of a generator and a reconstructor. The generator encodes the input\ntext into a shorter word sequence, and the reconstructor recovers the generator\ninput from the generator output. To make the generator output human-readable, a\ndiscriminator restricts the output of the generator to resemble human-written\nsentences. By taking the generator output as the summary of the input text,\nabstractive summarization is achieved without document-summary pairs as\ntraining data. Promising results are shown on both English and Chinese corpora.", "published": "2018-10-05 18:58:35", "link": "http://arxiv.org/abs/1810.02851v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Scalable Micro-planned Generation of Discourse from Structured Data", "abstract": "We present a framework for generating natural language description from\nstructured data such as tables; the problem comes under the category of\ndata-to-text natural language generation (NLG). Modern data-to-text NLG systems\ntypically employ end-to-end statistical and neural architectures that learn\nfrom a limited amount of task-specific labeled data, and therefore, exhibit\nlimited scalability, domain-adaptability, and interpretability. Unlike these\nsystems, ours is a modular, pipeline-based approach, and does not require\ntask-specific parallel data. It rather relies on monolingual corpora and basic\noff-the-shelf NLP tools. This makes our system more scalable and easily\nadaptable to newer domains.\n  Our system employs a 3-staged pipeline that: (i) converts entries in the\nstructured data to canonical form, (ii) generates simple sentences for each\natomic entry in the canonicalized representation, and (iii) combines the\nsentences to produce a coherent, fluent and adequate paragraph description\nthrough sentence compounding and co-reference replacement modules. Experiments\non a benchmark mixed-domain dataset curated for paragraph description from\ntables reveals the superiority of our system over existing data-to-text\napproaches. We also demonstrate the robustness of our system in accepting other\npopular datasets covering diverse data types such as Knowledge Graphs and\nKey-Value maps.", "published": "2018-10-05 21:07:11", "link": "http://arxiv.org/abs/1810.02889v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Entity Tracking Improves Cloze-style Reading Comprehension", "abstract": "Reading comprehension tasks test the ability of models to process long-term\ncontext and remember salient information. Recent work has shown that relatively\nsimple neural methods such as the Attention Sum-Reader can perform well on\nthese tasks; however, these systems still significantly trail human\nperformance. Analysis suggests that many of the remaining hard instances are\nrelated to the inability to track entity-references throughout documents. This\nwork focuses on these hard entity tracking cases with two extensions: (1)\nadditional entity features, and (2) training with a multi-task tracking\nobjective. We show that these simple modifications improve performance both\nindependently and in combination, and we outperform the previous state of the\nart on the LAMBADA dataset, particularly on difficult entity examples.", "published": "2018-10-05 21:20:25", "link": "http://arxiv.org/abs/1810.02891v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TRANX: A Transition-based Neural Abstract Syntax Parser for Semantic\n  Parsing and Code Generation", "abstract": "We present TRANX, a transition-based neural semantic parser that maps natural\nlanguage (NL) utterances into formal meaning representations (MRs). TRANX uses\na transition system based on the abstract syntax description language for the\ntarget MR, which gives it two major advantages: (1) it is highly accurate,\nusing information from the syntax of the target MR to constrain the output\nspace and model the information flow, and (2) it is highly generalizable, and\ncan easily be applied to new types of MR by just writing a new abstract syntax\ndescription corresponding to the allowable structures in the MR. Experiments on\nfour different semantic parsing and code generation tasks show that our system\nis generalizable, extensible, and effective, registering strong results\ncompared to existing neural semantic parsers.", "published": "2018-10-05 14:35:01", "link": "http://arxiv.org/abs/1810.02720v1", "categories": ["cs.CL", "cs.PL"], "primary_category": "cs.CL"}
{"title": "Sentence Segmentation for Classical Chinese Based on LSTM with Radical\n  Embedding", "abstract": "In this paper, we develop a low than character feature embedding called\nradical embedding, and apply it on LSTM model for sentence segmentation of pre\nmodern Chinese texts. The datasets includes over 150 classical Chinese books\nfrom 3 different dynasties and contains different literary styles. LSTM CRF\nmodel is a state of art method for the sequence labeling problem. Our new model\nadds a component of radical embedding, which leads to improved performances.\nExperimental results based on the aforementioned Chinese books demonstrates a\nbetter accuracy than earlier methods on sentence segmentation, especial in Tang\nEpitaph texts.", "published": "2018-10-05 14:49:42", "link": "http://arxiv.org/abs/1810.03479v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "POIReviewQA: A Semantically Enriched POI Retrieval and Question\n  Answering Dataset", "abstract": "Many services that perform information retrieval for Points of Interest (POI)\nutilize a Lucene-based setup with spatial filtering. While this type of system\nis easy to implement it does not make use of semantics but relies on direct\nword matches between a query and reviews leading to a loss in both precision\nand recall. To study the challenging task of semantically enriching POIs from\nunstructured data in order to support open-domain search and question answering\n(QA), we introduce a new dataset POIReviewQA. It consists of 20k questions\n(e.g.\"is this restaurant dog friendly?\") for 1022 Yelp business types. For each\nquestion we sampled 10 reviews, and annotated each sentence in the reviews\nwhether it answers the question and what the corresponding answer is. To test a\nsystem's ability to understand the text we adopt an information retrieval\nevaluation by ranking all the review sentences for a question based on the\nlikelihood that they answer this question. We build a Lucene-based baseline\nmodel, which achieves 77.0% AUC and 48.8% MAP. A sentence embedding-based model\nachieves 79.2% AUC and 41.8% MAP, indicating that the dataset presents a\nchallenging problem for future research by the GIR community. The result\ntechnology can help exploit the thematic content of web documents and social\nmedia for characterisation of locations.", "published": "2018-10-05 17:37:37", "link": "http://arxiv.org/abs/1810.02802v1", "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.AI"}
{"title": "Text Classification of the Precursory Accelerating Seismicity Corpus:\n  Inference on some Theoretical Trends in Earthquake Predictability Research\n  from 1988 to 2018", "abstract": "Text analytics based on supervised machine learning classifiers has shown\ngreat promise in a multitude of domains, but has yet to be applied to\nSeismology. We test various standard models (Naive Bayes, k-Nearest Neighbors,\nSupport Vector Machines, and Random Forests) on a seismological corpus of 100\narticles related to the topic of precursory accelerating seismicity, spanning\nfrom 1988 to 2010. This corpus was labelled in Mignan (2011) with the precursor\nwhether explained by critical processes (i.e., cascade triggering) or by other\nprocesses (such as signature of main fault loading). We investigate rather the\nclassification process can be automatized to help analyze larger corpora in\norder to better understand trends in earthquake predictability research. We\nfind that the Naive Bayes model performs best, in agreement with the machine\nlearning literature for the case of small datasets, with cross-validation\naccuracies of 86% for binary classification. For a refined multiclass\nclassification ('non-critical process' < 'agnostic' < 'critical process\nassumed' < 'critical process demonstrated'), we obtain up to 78% accuracy.\nPrediction on a dozen of articles published since 2011 shows however a weak\ngeneralization with a F1-score of 60%, only slightly better than a random\nclassifier, which can be explained by a change of authorship and use of\ndifferent terminologies. Yet, the model shows F1-scores greater than 80% for\nthe two multiclass extremes ('non-critical process' versus 'critical process\ndemonstrated') while it falls to random classifier results (around 25%) for\npapers labelled 'agnostic' or 'critical process assumed'. Those results are\nencouraging in view of the small size of the corpus and of the high degree of\nabstraction of the labelling. Domain knowledge engineering remains essential\nbut can be made transparent by an investigation of Naive Bayes keyword\nposterior probabilities.", "published": "2018-10-05 16:15:14", "link": "http://arxiv.org/abs/1810.03480v1", "categories": ["cs.CL", "physics.geo-ph", "stat.ML"], "primary_category": "cs.CL"}
{"title": "End-to-end Networks for Supervised Single-channel Speech Separation", "abstract": "The performance of single channel source separation algorithms has improved\ngreatly in recent times with the development and deployment of neural networks.\nHowever, many such networks continue to operate on the magnitude spectrogram of\na mixture, and produce an estimate of source magnitude spectrograms, to perform\nsource separation. In this paper, we interpret these steps as additional neural\nnetwork layers and propose an end-to-end source separation network that allows\nus to estimate the separated speech waveform by operating directly on the raw\nwaveform of the mixture. Furthermore, we also propose the use of masking based\nend-to-end separation networks that jointly optimize the mask and the latent\nrepresentations of the mixture waveforms. These networks show a significant\nimprovement in separation performance compared to existing architectures in our\nexperiments. To train these end-to-end models, we investigate the use of\ncomposite cost functions that are derived from objective evaluation metrics as\nmeasured on waveforms. We present subjective listening test results that\ndemonstrate the improvement attained by using masking based end-to-end networks\nand also reveal insights into the performance of these cost functions for\nend-to-end source separation.", "published": "2018-10-05 08:44:27", "link": "http://arxiv.org/abs/1810.02568v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Current Trends and Future Research Directions for Interactive Music", "abstract": "In this review, it is explained and compared different software and\nformalisms used in music interaction: sequencers, computer-assisted\nimprovisation, meta- instruments, score-following, asynchronous dataflow\nlanguages, synchronous dataflow languages, process calculi, temporal\nconstraints and interactive scores. Formal approaches have the advantage of\nproviding rigorous semantics of the behavior of the model and proving\ncorrectness during execution. The main disadvantage of formal approaches is\nlack of commercial tools.", "published": "2018-10-05 02:53:26", "link": "http://arxiv.org/abs/1810.04276v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
