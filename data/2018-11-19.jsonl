{"title": "A Comparative Analysis of Content-based Geolocation in Blogs and Tweets", "abstract": "The geolocation of online information is an essential component in any\ngeospatial application. While most of the previous work on geolocation has\nfocused on Twitter, in this paper we quantify and compare the performance of\ntext-based geolocation methods on social media data drawn from both Blogger and\nTwitter. We introduce a novel set of location specific features that are both\nhighly informative and easily interpretable, and show that we can achieve error\nrate reductions of up to 12.5% with respect to the best previously proposed\ngeolocation features. We also show that despite posting longer text, Blogger\nusers are significantly harder to geolocate than Twitter users. Additionally,\nwe investigate the effect of training and testing on different media\n(cross-media predictions), or combining multiple social media sources\n(multi-media predictions). Finally, we explore the geolocability of social\nmedia in relation to three user dimensions: state, gender, and industry.", "published": "2018-11-19 04:42:54", "link": "http://arxiv.org/abs/1811.07497v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Chat More If You Like: Dynamic Cue Words Planning to Flow Longer\n  Conversations", "abstract": "To build an open-domain multi-turn conversation system is one of the most\ninteresting and challenging tasks in Artificial Intelligence. Many research\nefforts have been dedicated to building such dialogue systems, yet few shed\nlight on modeling the conversation flow in an ongoing dialogue. Besides, it is\ncommon for people to talk about highly relevant aspects during a conversation.\nAnd the topics are coherent and drift naturally, which demonstrates the\nnecessity of dialogue flow modeling. To this end, we present the multi-turn\ncue-words driven conversation system with reinforcement learning method (RLCw),\nwhich strives to select an adaptive cue word with the greatest future credit,\nand therefore improve the quality of generated responses. We introduce a new\nreward to measure the quality of cue words in terms of effectiveness and\nrelevance. To further optimize the model for long-term conversations, a\nreinforcement approach is adopted in this paper. Experiments on real-life\ndataset demonstrate that our model consistently outperforms a set of\ncompetitive baselines in terms of simulated turns, diversity and human\nevaluation.", "published": "2018-11-19 11:54:25", "link": "http://arxiv.org/abs/1811.07631v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Mafiascum Dataset: A Large Text Corpus for Deception Detection", "abstract": "Detecting deception in natural language has a wide variety of applications,\nbut because of its hidden nature there are currently no public, large-scale\nsources of labeled deceptive text. This work introduces the Mafiascum dataset\n[1], a collection of over 700 games of Mafia, in which players are randomly\nassigned either deceptive or non-deceptive roles and then interact via forum\npostings. Over 9000 documents were compiled from the dataset, which each\ncontained all messages written by a single player in a single game. This corpus\nwas used to construct a set of hand-picked linguistic features based on prior\ndeception research, as well as a set of average word vectors enriched with\nsubword information. A logistic regression classifier fit on a combination of\nthese feature sets achieved an average precision of 0.39 (chance = 0.26) and an\nAUROC of 0.68 on 5000+ word documents. On 50+ word documents, an average\nprecision of 0.29 (chance = 0.23) and an AUROC of 0.59 was achieved.\n  [1] https://bitbucket.org/bopjesvla/thesis/src", "published": "2018-11-19 18:11:09", "link": "http://arxiv.org/abs/1811.07851v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding and Measuring Psychological Stress using Social Media", "abstract": "A body of literature has demonstrated that users' mental health conditions,\nsuch as depression and anxiety, can be predicted from their social media\nlanguage. There is still a gap in the scientific understanding of how\npsychological stress is expressed on social media. Stress is one of the primary\nunderlying causes and correlates of chronic physical illnesses and mental\nhealth conditions. In this paper, we explore the language of psychological\nstress with a dataset of 601 social media users, who answered the Perceived\nStress Scale questionnaire and also consented to share their Facebook and\nTwitter data. Firstly, we find that stressed users post about exhaustion,\nlosing control, increased self-focus and physical pain as compared to posts\nabout breakfast, family-time, and travel by users who are not stressed.\nSecondly, we find that Facebook language is more predictive of stress than\nTwitter language. Thirdly, we demonstrate how the language based models thus\ndeveloped can be adapted and be scaled to measure county-level trends. Since\ncounty-level language is easily available on Twitter using the Streaming API,\nwe explore multiple domain adaptation algorithms to adapt user-level Facebook\nmodels to Twitter language. We find that domain-adapted and scaled social\nmedia-based measurements of stress outperform sociodemographic variables (age,\ngender, race, education, and income), against ground-truth survey-based stress\nmeasurements, both at the user- and the county-level in the U.S. Twitter\nlanguage that scores higher in stress is also predictive of poorer health, less\naccess to facilities and lower socioeconomic status in counties. We conclude\nwith a discussion of the implications of using social media as a new tool for\nmonitoring stress levels of both individuals and counties.", "published": "2018-11-19 00:18:48", "link": "http://arxiv.org/abs/1811.07430v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "An Influence-based Clustering Model on Twitter", "abstract": "This paper introduces a temporal framework for detecting and clustering\nemergent and viral topics on social networks. Endogenous and exogenous\ninfluence on developing viral content is explored using a clustering method\nbased on the a user's behavior on social network and a dataset from Twitter\nAPI. Results are discussed by introducing metrics such as popularity,\nburstiness, and relevance score. The results show clear distinction in\ncharacteristics of developed content by the two classes of users.", "published": "2018-11-19 12:51:07", "link": "http://arxiv.org/abs/1811.07655v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Limitations of Source-Filter Coupling In Phonation", "abstract": "The coupling of vocal fold (source) and vocal tract (filter) is one of the\nmost critical factors in source-filter articulation theory. The traditional\nlinear source-filter theory has been challenged by current research which\nclearly shows the impact of acoustic loading on the dynamic behavior of the\nvocal fold vibration as well as the variations in the glottal flow pulses\nshape. This paper outlines the underlying mechanism of source-filter\ninteractions; demonstrates the design and working principles of coupling for\nthe various existing vocal cord and vocal tract biomechanical models. For our\nstudy, we have considered self-oscillating lumped-element models of the\nacoustic source and computational models of the vocal tract as articulators. To\nunderstand the limitations of source-filter interactions which are associated\nwith each of those models, we compare them concerning their mechanical design,\nacoustic and physiological characteristics and aerodynamic simulation.", "published": "2018-11-19 00:32:50", "link": "http://arxiv.org/abs/1811.07435v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The PyTorch-Kaldi Speech Recognition Toolkit", "abstract": "The availability of open-source software is playing a remarkable role in the\npopularization of speech recognition and deep learning. Kaldi, for instance, is\nnowadays an established framework used to develop state-of-the-art speech\nrecognizers. PyTorch is used to build neural networks with the Python language\nand has recently spawn tremendous interest within the machine learning\ncommunity thanks to its simplicity and flexibility.\n  The PyTorch-Kaldi project aims to bridge the gap between these popular\ntoolkits, trying to inherit the efficiency of Kaldi and the flexibility of\nPyTorch. PyTorch-Kaldi is not only a simple interface between these software,\nbut it embeds several useful features for developing modern speech recognizers.\nFor instance, the code is specifically designed to naturally plug-in\nuser-defined acoustic models. As an alternative, users can exploit several\npre-implemented neural networks that can be customized using intuitive\nconfiguration files. PyTorch-Kaldi supports multiple feature and label streams\nas well as combinations of neural networks, enabling the use of complex neural\narchitectures. The toolkit is publicly-released along with a rich documentation\nand is designed to properly work locally or on HPC clusters.\n  Experiments, that are conducted on several datasets and tasks, show that\nPyTorch-Kaldi can effectively be used to develop modern state-of-the-art speech\nrecognizers.", "published": "2018-11-19 01:57:05", "link": "http://arxiv.org/abs/1811.07453v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.NE"], "primary_category": "eess.AS"}
{"title": "Visual-Texual Emotion Analysis with Deep Coupled Video and Danmu Neural\n  Networks", "abstract": "User emotion analysis toward videos is to automatically recognize the general\nemotional status of viewers from the multimedia content embedded in the online\nvideo stream. Existing works fall in two categories: 1) visual-based methods,\nwhich focus on visual content and extract a specific set of features of videos.\nHowever, it is generally hard to learn a mapping function from low-level video\npixels to high-level emotion space due to great intra-class variance. 2)\ntextual-based methods, which focus on the investigation of user-generated\ncomments associated with videos. The learned word representations by\ntraditional linguistic approaches typically lack emotion information and the\nglobal comments usually reflect viewers' high-level understandings rather than\ninstantaneous emotions. To address these limitations, in this paper, we propose\nto jointly utilize video content and user-generated texts simultaneously for\nemotion analysis. In particular, we introduce exploiting a new type of\nuser-generated texts, i.e., \"danmu\", which are real-time comments floating on\nthe video and contain rich information to convey viewers' emotional opinions.\nTo enhance the emotion discriminativeness of words in textual feature\nextraction, we propose Emotional Word Embedding (EWE) to learn text\nrepresentations by jointly considering their semantics and emotions.\nAfterwards, we propose a novel visual-textual emotion analysis model with Deep\nCoupled Video and Danmu Neural networks (DCVDN), in which visual and textual\nfeatures are synchronously extracted and fused to form a comprehensive\nrepresentation by deep-canonically-correlated-autoencoder-based multi-view\nlearning. Through extensive experiments on a self-crawled real-world\nvideo-danmu dataset, we prove that DCVDN significantly outperforms the\nstate-of-the-art baselines.", "published": "2018-11-19 03:51:19", "link": "http://arxiv.org/abs/1811.07485v1", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Switch-based Active Deep Dyna-Q: Efficient Adaptive Planning for\n  Task-Completion Dialogue Policy Learning", "abstract": "Training task-completion dialogue agents with reinforcement learning usually\nrequires a large number of real user experiences. The Dyna-Q algorithm extends\nQ-learning by integrating a world model, and thus can effectively boost\ntraining efficiency using simulated experiences generated by the world model.\nThe effectiveness of Dyna-Q, however, depends on the quality of the world model\n- or implicitly, the pre-specified ratio of real vs. simulated experiences used\nfor Q-learning. To this end, we extend the recently proposed Deep Dyna-Q (DDQ)\nframework by integrating a switcher that automatically determines whether to\nuse a real or simulated experience for Q-learning. Furthermore, we explore the\nuse of active learning for improving sample efficiency, by encouraging the\nworld model to generate simulated experiences in the state-action space where\nthe agent has not (fully) explored. Our results show that by combining switcher\nand active learning, the new framework named as Switch-based Active Deep Dyna-Q\n(Switch-DDQ), leads to significant improvement over DDQ and Q-learning\nbaselines in both simulation and human evaluations.", "published": "2018-11-19 08:23:34", "link": "http://arxiv.org/abs/1811.07550v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "A Trustworthy, Responsible and Interpretable System to Handle Chit Chat\n  in Conversational Bots", "abstract": "Most often, chat-bots are built to solve the purpose of a search engine or a\nhuman assistant: Their primary goal is to provide information to the user or\nhelp them complete a task. However, these chat-bots are incapable of responding\nto unscripted queries like \"Hi, what's up\", \"What's your favourite food\". Human\nevaluation judgments show that 4 humans come to a consensus on the intent of a\ngiven query which is from chat domain only 77% of the time, thus making it\nevident how non-trivial this task is. In our work, we show why it is difficult\nto break the chitchat space into clearly defined intents. We propose a system\nto handle this task in chat-bots, keeping in mind scalability,\ninterpretability, appropriateness, trustworthiness, relevance and coverage. Our\nwork introduces a pipeline for query understanding in chitchat using\nhierarchical intents as well as a way to use seq-seq auto-generation models in\nprofessional bots. We explore an interpretable model for chat domain detection\nand also show how various components such as adult/offensive classification,\ngrammars/regex patterns, curated personality based responses, generic guided\nevasive responses and response generation models can be combined in a scalable\nway to solve this problem.", "published": "2018-11-19 10:45:13", "link": "http://arxiv.org/abs/1811.07600v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Guiding Policies with Language via Meta-Learning", "abstract": "Behavioral skills or policies for autonomous agents are conventionally\nlearned from reward functions, via reinforcement learning, or from\ndemonstrations, via imitation learning. However, both modes of task\nspecification have their disadvantages: reward functions require manual\nengineering, while demonstrations require a human expert to be able to actually\nperform the task in order to generate the demonstration. Instruction following\nfrom natural language instructions provides an appealing alternative: in the\nsame way that we can specify goals to other humans simply by speaking or\nwriting, we would like to be able to specify tasks for our machines. However, a\nsingle instruction may be insufficient to fully communicate our intent or, even\nif it is, may be insufficient for an autonomous agent to actually understand\nhow to perform the desired task. In this work, we propose an interactive\nformulation of the task specification problem, where iterative language\ncorrections are provided to an autonomous agent, guiding it in acquiring the\ndesired skill. Our proposed language-guided policy learning algorithm can\nintegrate an instruction and a sequence of corrections to acquire new skills\nvery quickly. In our experiments, we show that this method can enable a policy\nto follow instructions and corrections for simulated navigation and\nmanipulation tasks, substantially outperforming direct, non-interactive\ninstruction following.", "published": "2018-11-19 18:58:42", "link": "http://arxiv.org/abs/1811.07882v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.LG"}
{"title": "End-to-End Retrieval in Continuous Space", "abstract": "Most text-based information retrieval (IR) systems index objects by words or\nphrases. These discrete systems have been augmented by models that use\nembeddings to measure similarity in continuous space. But continuous-space\nmodels are typically used just to re-rank the top candidates. We consider the\nproblem of end-to-end continuous retrieval, where standard approximate nearest\nneighbor (ANN) search replaces the usual discrete inverted index, and rely\nentirely on distances between learned embeddings. By training simple models\nspecifically for retrieval, with an appropriate model architecture, we improve\non a discrete baseline by 8% and 26% (MAP) on two similar-question retrieval\ntasks. We also discuss the problem of evaluation for retrieval systems, and\nshow how to modify existing pairwise similarity datasets for this purpose.", "published": "2018-11-19 22:23:59", "link": "http://arxiv.org/abs/1811.08008v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "NSEEN: Neural Semantic Embedding for Entity Normalization", "abstract": "Much of human knowledge is encoded in text, available in scientific\npublications, books, and the web. Given the rapid growth of these resources, we\nneed automated methods to extract such knowledge into machine-processable\nstructures, such as knowledge graphs. An important task in this process is\nentity normalization, which consists of mapping noisy entity mentions in text\nto canonical entities in well-known reference sets. However, entity\nnormalization is a challenging problem; there often are many textual forms for\na canonical entity that may not be captured in the reference set, and entities\nmentioned in text may include many syntactic variations, or errors. The problem\nis particularly acute in scientific domains, such as biology. To address this\nproblem, we have developed a general, scalable solution based on a deep Siamese\nneural network model to embed the semantic information about the entities, as\nwell as their syntactic variations. We use these embeddings for fast mapping of\nnew entities to large reference sets, and empirically show the effectiveness of\nour framework in challenging bio-entity normalization datasets.", "published": "2018-11-19 06:04:13", "link": "http://arxiv.org/abs/1811.07514v2", "categories": ["cs.IR", "cs.CL", "cs.DB", "cs.LG", "cs.NE"], "primary_category": "cs.IR"}
{"title": "Efficient keyword spotting using dilated convolutions and gating", "abstract": "We explore the application of end-to-end stateless temporal modeling to\nsmall-footprint keyword spotting as opposed to recurrent networks that model\nlong-term temporal dependencies using internal states. We propose a model\ninspired by the recent success of dilated convolutions in sequence modeling\napplications, allowing to train deeper architectures in resource-constrained\nconfigurations. Gated activations and residual connections are also added,\nfollowing a similar configuration to WaveNet. In addition, we apply a custom\ntarget labeling that back-propagates loss from specific frames of interest,\ntherefore yielding higher accuracy and only requiring to detect the end of the\nkeyword. Our experimental results show that our model outperforms a max-pooling\nloss trained recurrent neural network using LSTM cells, with a significant\ndecrease in false rejection rate. The underlying dataset - \"Hey Snips\"\nutterances recorded by over 2.2K different speakers - has been made publicly\navailable to establish an open reference for wake-word detection.", "published": "2018-11-19 13:51:10", "link": "http://arxiv.org/abs/1811.07684v2", "categories": ["cs.LG", "cs.CL", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "On Human Predictions with Explanations and Predictions of Machine\n  Learning Models: A Case Study on Deception Detection", "abstract": "Humans are the final decision makers in critical tasks that involve ethical\nand legal concerns, ranging from recidivism prediction, to medical diagnosis,\nto fighting against fake news. Although machine learning models can sometimes\nachieve impressive performance in these tasks, these tasks are not amenable to\nfull automation. To realize the potential of machine learning for improving\nhuman decisions, it is important to understand how assistance from machine\nlearning models affects human performance and human agency.\n  In this paper, we use deception detection as a testbed and investigate how we\ncan harness explanations and predictions of machine learning models to improve\nhuman performance while retaining human agency. We propose a spectrum between\nfull human agency and full automation, and develop varying levels of machine\nassistance along the spectrum that gradually increase the influence of machine\npredictions. We find that without showing predicted labels, explanations alone\nslightly improve human performance in the end task. In comparison, human\nperformance is greatly improved by showing predicted labels (>20% relative\nimprovement) and can be further improved by explicitly suggesting strong\nmachine performance. Interestingly, when predicted labels are shown,\nexplanations of machine predictions induce a similar level of accuracy as an\nexplicit statement of strong machine performance. Our results demonstrate a\ntradeoff between human performance and human agency and show that explanations\nof machine predictions can moderate this tradeoff.", "published": "2018-11-19 19:00:01", "link": "http://arxiv.org/abs/1811.07901v4", "categories": ["cs.AI", "cs.CL", "cs.CY", "physics.soc-ph", "stat.ML"], "primary_category": "cs.AI"}
{"title": "Analysis of DNN Speech Signal Enhancement for Robust Speaker Recognition", "abstract": "In this work, we present an analysis of a DNN-based autoencoder for speech\nenhancement, dereverberation and denoising. The target application is a robust\nspeaker verification (SV) system. We start our approach by carefully designing\na data augmentation process to cover wide range of acoustic conditions and\nobtain rich training data for various components of our SV system. We augment\nseveral well-known databases used in SV with artificially noised and\nreverberated data and we use them to train a denoising autoencoder (mapping\nnoisy and reverberated speech to its clean version) as well as an x-vector\nextractor which is currently considered as state-of-the-art in SV. Later, we\nuse the autoencoder as a preprocessing step for text-independent SV system. We\ncompare results achieved with autoencoder enhancement, multi-condition PLDA\ntraining and their simultaneous use. We present a detailed analysis with\nvarious conditions of NIST SRE 2010, 2016, PRISM and with re-transmitted data.\nWe conclude that the proposed preprocessing can significantly improve both\ni-vector and x-vector baselines and that this technique can be used to build a\nrobust SV system for various target domains.", "published": "2018-11-19 11:41:23", "link": "http://arxiv.org/abs/1811.07629v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
