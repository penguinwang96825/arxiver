{"title": "Development of POS tagger for English-Bengali Code-Mixed data", "abstract": "Code-mixed texts are widespread nowadays due to the advent of social media.\nSince these texts combine two languages to formulate a sentence, it gives rise\nto various research problems related to Natural Language Processing. In this\npaper, we try to excavate one such problem, namely, Parts of Speech tagging of\ncode-mixed texts. We have built a system that can POS tag English-Bengali\ncode-mixed data where the Bengali words were written in Roman script. Our\napproach initially involves the collection and cleaning of English-Bengali\ncode-mixed tweets. These tweets were used as a development dataset for building\nour system. The proposed system is a modular approach that starts by tagging\nindividual tokens with their respective languages and then passes them to\ndifferent POS taggers, designed for different languages (English and Bengali,\nin our case). Tags given by the two systems are later joined together and the\nfinal result is then mapped to a universal POS tag set. Our system was checked\nusing 100 manually POS tagged code-mixed sentences and it returned an accuracy\nof 75.29%", "published": "2020-07-29 03:42:07", "link": "http://arxiv.org/abs/2007.14576v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Biomedical and Clinical English Model Packages in the Stanza Python NLP\n  Library", "abstract": "We introduce biomedical and clinical English model packages for the Stanza\nPython NLP library. These packages offer accurate syntactic analysis and named\nentity recognition capabilities for biomedical and clinical text, by combining\nStanza's fully neural architecture with a wide variety of open datasets as well\nas large-scale unsupervised biomedical and clinical text data. We show via\nextensive experiments that our packages achieve syntactic analysis and named\nentity recognition performance that is on par with or surpasses\nstate-of-the-art results. We further show that these models do not compromise\nspeed compared to existing toolkits when GPU acceleration is available, and are\nmade easy to download and use with Stanza's Python interface. A demonstration\nof our packages is available at: http://stanza.run/bio.", "published": "2020-07-29 07:27:41", "link": "http://arxiv.org/abs/2007.14640v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "#Brexit: Leave or Remain? The Role of User's Community and Diachronic\n  Evolution on Stance Detection", "abstract": "Interest has grown around the classification of stance that users assume\nwithin online debates in recent years. Stance has been usually addressed by\nconsidering users posts in isolation, while social studies highlight that\nsocial communities may contribute to influence users' opinion. Furthermore,\nstance should be studied in a diachronic perspective, since it could help to\nshed light on users' opinion shift dynamics that can be recorded during the\ndebate. We analyzed the political discussion in UK about the BREXIT referendum\non Twitter, proposing a novel approach and annotation schema for stance\ndetection, with the main aim of investigating the role of features related to\nsocial network community and diachronic stance evolution. Classification\nexperiments show that such features provide very useful clues for detecting\nstance.", "published": "2020-07-29 16:19:02", "link": "http://arxiv.org/abs/2007.14936v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Adversarial Training in Self-Learning for Cross-Lingual Text\n  Classification", "abstract": "In cross-lingual text classification, one seeks to exploit labeled data from\none language to train a text classification model that can then be applied to a\ncompletely different language. Recent multilingual representation models have\nmade it much easier to achieve this. Still, there may still be subtle\ndifferences between languages that are neglected when doing so. To address\nthis, we present a semi-supervised adversarial training process that minimizes\nthe maximal loss for label-preserving input perturbations. The resulting model\nthen serves as a teacher to induce labels for unlabeled target language samples\nthat can be used during further adversarial training, allowing us to gradually\nadapt our model to the target language. Compared with a number of strong\nbaselines, we observe significant gains in effectiveness on document and intent\nclassification for a diverse set of languages.", "published": "2020-07-29 19:38:35", "link": "http://arxiv.org/abs/2007.15072v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Object-and-Action Aware Model for Visual Language Navigation", "abstract": "Vision-and-Language Navigation (VLN) is unique in that it requires turning\nrelatively general natural-language instructions into robot agent actions, on\nthe basis of the visible environment. This requires to extract value from two\nvery different types of natural-language information. The first is object\ndescription (e.g., 'table', 'door'), each presenting as a tip for the agent to\ndetermine the next action by finding the item visible in the environment, and\nthe second is action specification (e.g., 'go straight', 'turn left') which\nallows the robot to directly predict the next movements without relying on\nvisual perceptions. However, most existing methods pay few attention to\ndistinguish these information from each other during instruction encoding and\nmix together the matching between textual object/action encoding and visual\nperception/orientation features of candidate viewpoints. In this paper, we\npropose an Object-and-Action Aware Model (OAAM) that processes these two\ndifferent forms of natural language based instruction separately. This enables\neach process to match object-centered/action-centered instruction to their own\ncounterpart visual perception/action orientation flexibly. However, one\nside-issue caused by above solution is that an object mentioned in instructions\nmay be observed in the direction of two or more candidate viewpoints, thus the\nOAAM may not predict the viewpoint on the shortest path as the next action. To\nhandle this problem, we design a simple but effective path loss to penalize\ntrajectories deviating from the ground truth path. Experimental results\ndemonstrate the effectiveness of the proposed model and path loss, and the\nsuperiority of their combination with a 50% SPL score on the R2R dataset and a\n40% CLS score on the R4R dataset in unseen environments, outperforming the\nprevious state-of-the-art.", "published": "2020-07-29 06:32:18", "link": "http://arxiv.org/abs/2007.14626v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Enriching Video Captions With Contextual Text", "abstract": "Understanding video content and generating caption with context is an\nimportant and challenging task. Unlike prior methods that typically attempt to\ngenerate generic video captions without context, our architecture\ncontextualizes captioning by infusing extracted information from relevant text\ndata. We propose an end-to-end sequence-to-sequence model which generates video\ncaptions based on visual input, and mines relevant knowledge such as names and\nlocations from contextual text. In contrast to previous approaches, we do not\npreprocess the text further, and let the model directly learn to attend over\nit. Guided by the visual input, the model is able to copy words from the\ncontextual text via a pointer-generator network, allowing to produce more\nspecific video captions. We show competitive performance on the News Video\nDataset and, through ablation studies, validate the efficacy of contextual\nvideo captioning as well as individual design choices in our model\narchitecture.", "published": "2020-07-29 08:58:52", "link": "http://arxiv.org/abs/2007.14682v1", "categories": ["cs.CV", "cs.CL", "I.2.10, I.2.7"], "primary_category": "cs.CV"}
{"title": "The Return of Lexical Dependencies: Neural Lexicalized PCFGs", "abstract": "In this paper we demonstrate that $\\textit{context free grammar (CFG) based\nmethods for grammar induction benefit from modeling lexical dependencies}$.\nThis contrasts to the most popular current methods for grammar induction, which\nfocus on discovering $\\textit{either}$ constituents $\\textit{or}$ dependencies.\nPrevious approaches to marry these two disparate syntactic formalisms (e.g.\nlexicalized PCFGs) have been plagued by sparsity, making them unsuitable for\nunsupervised grammar induction. However, in this work, we present novel neural\nmodels of lexicalized PCFGs which allow us to overcome sparsity problems and\neffectively induce both constituents and dependencies within a single model.\nExperiments demonstrate that this unified framework results in stronger results\non both representations than achieved when modeling either formalism alone.\nCode is available at https://github.com/neulab/neural-lpcfg.", "published": "2020-07-29 22:12:49", "link": "http://arxiv.org/abs/2007.15135v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Predicting Multiple ICD-10 Codes from Brazilian-Portuguese Clinical\n  Notes", "abstract": "ICD coding from electronic clinical records is a manual, time-consuming and\nexpensive process. Code assignment is, however, an important task for billing\npurposes and database organization. While many works have studied the problem\nof automated ICD coding from free text using machine learning techniques, most\nuse records in the English language, especially from the MIMIC-III public\ndataset. This work presents results for a dataset with Brazilian Portuguese\nclinical notes. We develop and optimize a Logistic Regression model, a\nConvolutional Neural Network (CNN), a Gated Recurrent Unit Neural Network and a\nCNN with Attention (CNN-Att) for prediction of diagnosis ICD codes. We also\nreport our results for the MIMIC-III dataset, which outperform previous work\namong models of the same families, as well as the state of the art. Compared to\nMIMIC-III, the Brazilian Portuguese dataset contains far fewer words per\ndocument, when only discharge summaries are used. We experiment concatenating\nadditional documents available in this dataset, achieving a great boost in\nperformance. The CNN-Att model achieves the best results on both datasets, with\nmicro-averaged F1 score of 0.537 on MIMIC-III and 0.485 on our dataset with\nadditional documents.", "published": "2020-07-29 22:12:26", "link": "http://arxiv.org/abs/2008.01515v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Composer Style Classification of Piano Sheet Music Images Using Language\n  Model Pretraining", "abstract": "This paper studies composer style classification of piano sheet music images.\nPrevious approaches to the composer classification task have been limited by a\nscarcity of data. We address this issue in two ways: (1) we recast the problem\nto be based on raw sheet music images rather than a symbolic music format, and\n(2) we propose an approach that can be trained on unlabeled data. Our approach\nfirst converts the sheet music image into a sequence of musical \"words\" based\non the bootleg feature representation, and then feeds the sequence into a text\nclassifier. We show that it is possible to significantly improve classifier\nperformance by first training a language model on a set of unlabeled data,\ninitializing the classifier with the pretrained language model weights, and\nthen finetuning the classifier on a small amount of labeled data. We train\nAWD-LSTM, GPT-2, and RoBERTa language models on all piano sheet music images in\nIMSLP. We find that transformer-based architectures outperform CNN and LSTM\nmodels, and pretraining boosts classification accuracy for the GPT-2 model from\n46\\% to 70\\% on a 9-way classification task. The trained model can also be used\nas a feature extractor that projects piano sheet music into a feature space\nthat characterizes compositional style.", "published": "2020-07-29 04:13:59", "link": "http://arxiv.org/abs/2007.14587v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Mirostat: A Neural Text Decoding Algorithm that Directly Controls\n  Perplexity", "abstract": "Neural text decoding is important for generating high-quality texts using\nlanguage models. To generate high-quality text, popular decoding algorithms\nlike top-k, top-p (nucleus), and temperature-based sampling truncate or distort\nthe unreliable low probability tail of the language model. Though these methods\ngenerate high-quality text after parameter tuning, they are ad hoc. Not much is\nknown about the control they provide over the statistics of the output, which\nis important since recent reports show text quality is highest for a specific\nrange of likelihoods. Here, first we provide a theoretical analysis of\nperplexity in top-k, top-p, and temperature sampling, finding that\ncross-entropy behaves approximately linearly as a function of p in top-p\nsampling whereas it is a nonlinear function of k in top-k sampling, under\nZipfian statistics. We use this analysis to design a feedback-based adaptive\ntop-k text decoding algorithm called mirostat that generates text (of any\nlength) with a predetermined value of perplexity, and thereby high-quality text\nwithout any tuning. Experiments show that for low values of k and p in top-k\nand top-p sampling, perplexity drops significantly with generated text length,\nwhich is also correlated with excessive repetitions in the text (the boredom\ntrap). On the other hand, for large values of k and p, we find that perplexity\nincreases with generated text length, which is correlated with incoherence in\nthe text (confusion trap). Mirostat avoids both traps: experiments show that\ncross-entropy has a near-linear relation with repetition in generated text.\nThis relation is almost independent of the sampling method but slightly\ndependent on the model used. Hence, for a given language model, control over\nperplexity also gives control over repetitions. Experiments with human raters\nfor fluency, coherence, and quality further verify our findings.", "published": "2020-07-29 17:22:26", "link": "http://arxiv.org/abs/2007.14966v2", "categories": ["cs.CL", "cs.IT", "math.IT"], "primary_category": "cs.CL"}
{"title": "Presentation and Analysis of a Multimodal Dataset for Grounded Language\n  Learning", "abstract": "Grounded language acquisition -- learning how language-based interactions\nrefer to the world around them -- is amajor area of research in robotics, NLP,\nand HCI. In practice the data used for learning consists almost entirely of\ntextual descriptions, which tend to be cleaner, clearer, and more grammatical\nthan actual human interactions. In this work, we present the Grounded Language\nDataset (GoLD), a multimodal dataset of common household objects described by\npeople using either spoken or written language. We analyze the differences and\npresent an experiment showing how the different modalities affect language\nlearning from human in-put. This will enable researchers studying the\nintersection of robotics, NLP, and HCI to better investigate how the multiple\nmodalities of image, text, and speech interact, as well as show differences in\nthe vernacular of these modalities impact results.", "published": "2020-07-29 17:58:04", "link": "http://arxiv.org/abs/2007.14987v4", "categories": ["cs.RO", "cs.CL", "cs.HC"], "primary_category": "cs.RO"}
{"title": "Exploiting Cross-Lingual Knowledge in Unsupervised Acoustic Modeling for\n  Low-Resource Languages", "abstract": "(Short version of Abstract) This thesis describes an investigation on\nunsupervised acoustic modeling (UAM) for automatic speech recognition (ASR) in\nthe zero-resource scenario, where only untranscribed speech data is assumed to\nbe available. UAM is not only important in addressing the general problem of\ndata scarcity in ASR technology development but also essential to many\nnon-mainstream applications, for examples, language protection, language\nacquisition and pathological speech assessment. The present study is focused on\ntwo research problems. The first problem concerns unsupervised discovery of\nbasic (subword level) speech units in a given language. Under the zero-resource\ncondition, the speech units could be inferred only from the acoustic signals,\nwithout requiring or involving any linguistic direction and/or constraints. The\nsecond problem is referred to as unsupervised subword modeling. In its essence\na frame-level feature representation needs to be learned from untranscribed\nspeech. The learned feature representation is the basis of subword unit\ndiscovery. It is desired to be linguistically discriminative and robust to\nnon-linguistic factors. Particularly extensive use of cross-lingual knowledge\nin subword unit discovery and modeling is a focus of this research.", "published": "2020-07-29 19:45:17", "link": "http://arxiv.org/abs/2007.15074v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Exploiting stance hierarchies for cost-sensitive stance detection of Web\n  documents", "abstract": "Fact checking is an essential challenge when combating fake news. Identifying\ndocuments that agree or disagree with a particular statement (claim) is a core\ntask in this process. In this context, stance detection aims at identifying the\nposition (stance) of a document towards a claim. Most approaches address this\ntask through a 4-class classification model where the class distribution is\nhighly imbalanced. Therefore, they are particularly ineffective in detecting\nthe minority classes (for instance, 'disagree'), even though such instances are\ncrucial for tasks such as fact-checking by providing evidence for detecting\nfalse claims. In this paper, we exploit the hierarchical nature of stance\nclasses, which allows us to propose a modular pipeline of cascading binary\nclassifiers, enabling performance tuning on a per step and class basis. We\nimplement our approach through a combination of neural and traditional\nclassification models that highlight the misclassification costs of minority\nclasses. Evaluation results demonstrate state-of-the-art performance of our\napproach and its ability to significantly improve the classification\nperformance of the important 'disagree' class.", "published": "2020-07-29 21:40:01", "link": "http://arxiv.org/abs/2007.15121v2", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Fast, Structured Clinical Documentation via Contextual Autocomplete", "abstract": "We present a system that uses a learned autocompletion mechanism to\nfacilitate rapid creation of semi-structured clinical documentation. We\ndynamically suggest relevant clinical concepts as a doctor drafts a note by\nleveraging features from both unstructured and structured medical data. By\nconstraining our architecture to shallow neural networks, we are able to make\nthese suggestions in real time. Furthermore, as our algorithm is used to write\na note, we can automatically annotate the documentation with clean labels of\nclinical concepts drawn from medical vocabularies, making notes more structured\nand readable for physicians, patients, and future algorithms. To our knowledge,\nthis system is the only machine learning-based documentation utility for\nclinical notes deployed in a live hospital setting, and it reduces keystroke\nburden of clinical concepts by 67% in real environments.", "published": "2020-07-29 23:43:15", "link": "http://arxiv.org/abs/2007.15153v1", "categories": ["cs.LG", "cs.CL", "cs.IR", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Text-based classification of interviews for mental health -- juxtaposing\n  the state of the art", "abstract": "Currently, the state of the art for classification of psychiatric illness is\nbased on audio-based classification. This thesis aims to design and evaluate a\nstate of the art text classification network on this challenge. The hypothesis\nis that a well designed text-based approach poses a strong competition against\nthe state-of-the-art audio based approaches. Dutch natural language models are\nbeing limited by the scarcity of pre-trained monolingual NLP models, as a\nresult Dutch natural language models have a low capture of long range semantic\ndependencies over sentences. For this issue, this thesis presents belabBERT, a\nnew Dutch language model extending the RoBERTa[15] architecture. belabBERT is\ntrained on a large Dutch corpus (+32GB) of web crawled texts. After this thesis\nevaluates the strength of text-based classification, a brief exploration is\ndone, extending the framework to a hybrid text- and audio-based classification.\nThe goal of this hybrid framework is to show the principle of hybridisation\nwith a very basic audio-classification network. The overall goal is to create\nthe foundations for a hybrid psychiatric illness classification, by proving\nthat the new text-based classification is already a strong stand-alone\nsolution.", "published": "2020-07-29 16:19:30", "link": "http://arxiv.org/abs/2008.01543v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.CL"}
{"title": "DNN No-Reference PSTN Speech Quality Prediction", "abstract": "Classic public switched telephone networks (PSTN) are often a black box for\nVoIP network providers, as they have no access to performance indicators, such\nas delay or packet loss. Only the degraded output speech signal can be used to\nmonitor the speech quality of these networks. However, the current\nstate-of-the-art speech quality models are not reliable enough to be used for\nlive monitoring. One of the reasons for this is that PSTN distortions can be\nunique depending on the provider and country, which makes it difficult to train\na model that generalizes well for different PSTN networks. In this paper, we\npresent a new open-source PSTN speech quality test set with over 1000\ncrowdsourced real phone calls. Our proposed no-reference model outperforms the\nfull-reference POLQA and no-reference P.563 on the validation and test set.\nFurther, we analyzed the influence of file cropping on the perceived speech\nquality and the influence of the number of ratings and training size on the\nmodel accuracy.", "published": "2020-07-29 04:53:38", "link": "http://arxiv.org/abs/2007.14598v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Transformer based unsupervised pre-training for acoustic representation\n  learning", "abstract": "Recently, a variety of acoustic tasks and related applications arised. For\nmany acoustic tasks, the labeled data size may be limited. To handle this\nproblem, we propose an unsupervised pre-training method using Transformer based\nencoder to learn a general and robust high-level representation for all\nacoustic tasks. Experiments have been conducted on three kinds of acoustic\ntasks: speech emotion recognition, sound event detection and speech\ntranslation. All the experiments have shown that pre-training using its own\ntraining data can significantly improve the performance. With a larger\npre-training data combining MuST-C, Librispeech and ESC-US datasets, for speech\nemotion recognition, the UAR can further improve absolutely 4.3% on IEMOCAP\ndataset. For sound event detection, the F1 score can further improve absolutely\n1.5% on DCASE2018 task5 development set and 2.1% on evaluation set. For speech\ntranslation, the BLEU score can further improve relatively 12.2% on En-De\ndataset and 8.4% on En-Fr dataset.", "published": "2020-07-29 05:11:09", "link": "http://arxiv.org/abs/2007.14602v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "On Loss Functions and Recurrency Training for GAN-based Speech\n  Enhancement Systems", "abstract": "Recent work has shown that it is feasible to use generative adversarial\nnetworks (GANs) for speech enhancement, however, these approaches have not been\ncompared to state-of-the-art (SOTA) non GAN-based approaches. Additionally,\nmany loss functions have been proposed for GAN-based approaches, but they have\nnot been adequately compared. In this study, we propose novel convolutional\nrecurrent GAN (CRGAN) architectures for speech enhancement. Multiple loss\nfunctions are adopted to enable direct comparisons to other GAN-based systems.\nThe benefits of including recurrent layers are also explored. Our results show\nthat the proposed CRGAN model outperforms the SOTA GAN-based models using the\nsame loss functions and it outperforms other non-GAN based systems, indicating\nthe benefits of using a GAN for speech enhancement. Overall, the CRGAN model\nthat combines an objective metric loss function with the mean squared error\n(MSE) provides the best performance over comparison approaches across many\nevaluation metrics.", "published": "2020-07-29 17:40:44", "link": "http://arxiv.org/abs/2007.14974v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Investigation of Phase Distortion on Perceived Speech Quality for\n  Hearing-impaired Listeners", "abstract": "Phase serves as a critical component of speech that influences the quality\nand intelligibility. Current speech enhancement algorithms are beginning to\naddress phase distortions, but the algorithms focus on normal-hearing (NH)\nlisteners. It is not clear whether phase enhancement is beneficial for\nhearing-impaired (HI) listeners. We investigated the influence of phase\ndistortion on speech quality through a listening study, in which NH and HI\nlisteners provided speech-quality ratings using the MUSHRA procedure. In one\nset of conditions, the speech was mixed with babble noise at 4 different\nsignal-to-noise ratios (SNRs) from -5 to 10 dB. In another set of conditions,\nthe SNR was fixed at 10 dB and the noisy speech was presented in a simulated\nreverberant room with T60s ranging from 100 to 1000 ms. The speech level was\nkept at 65 dB SPL for NH listeners and amplification was applied for HI\nlisteners to ensure audibility. Ideal ratio masking (IRM) was used to simulate\nspeech enhancement. Two objective metrics (i.e., PESQ and HASQI) were utilized\nto compare subjective and objective ratings. Results indicate that phase\ndistortion has a negative impact on perceived quality for both groups and PESQ\nis more closely correlated with human ratings.", "published": "2020-07-29 17:55:51", "link": "http://arxiv.org/abs/2007.14986v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Privacy-preserving Voice Analysis via Disentangled Representations", "abstract": "Voice User Interfaces (VUIs) are increasingly popular and built into\nsmartphones, home assistants, and Internet of Things (IoT) devices. Despite\noffering an always-on convenient user experience, VUIs raise new security and\nprivacy concerns for their users. In this paper, we focus on attribute\ninference attacks in the speech domain, demonstrating the potential for an\nattacker to accurately infer a target user's sensitive and private attributes\n(e.g. their emotion, sex, or health status) from deep acoustic models. To\ndefend against this class of attacks, we design, implement, and evaluate a\nuser-configurable, privacy-aware framework for optimizing speech-related data\nsharing mechanisms. Our objective is to enable primary tasks such as speech\nrecognition and user identification, while removing sensitive attributes in the\nraw speech data before sharing it with a cloud service provider. We leverage\ndisentangled representation learning to explicitly learn independent factors in\nthe raw data. Based on a user's preferences, a supervision signal informs the\nfiltering out of invariant factors while retaining the factors reflected in the\nselected preference. Our experimental evaluation over five datasets shows that\nthe proposed framework can effectively defend against attribute inference\nattacks by reducing their success rates to approximately that of guessing at\nrandom, while maintaining accuracy in excess of 99% for the tasks of interest.\nWe conclude that negotiable privacy settings enabled by disentangled\nrepresentations can bring new opportunities for privacy-preserving\napplications.", "published": "2020-07-29 19:18:03", "link": "http://arxiv.org/abs/2007.15064v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improved Handling of Repeats and Jumps in Audio-Sheet Image\n  Synchronization", "abstract": "This paper studies the problem of automatically generating piano score\nfollowing videos given an audio recording and raw sheet music images. Whereas\nprevious works focus on synthetic sheet music where the data has been cleaned\nand preprocessed, we instead focus on developing a system that can cope with\nthe messiness of raw, unprocessed sheet music PDFs from IMSLP. We investigate\nhow well existing systems cope with real scanned sheet music, filler pages and\nunrelated pieces or movements, and discontinuities due to jumps and repeats. We\nfind that a significant bottleneck in system performance is handling jumps and\nrepeats correctly. In particular, we find that a previously proposed Jump DTW\nalgorithm does not perform robustly when jump locations are unknown a priori.\nWe propose a novel alignment algorithm called Hierarchical DTW that can handle\njumps and repeats even when jump locations are not known. It first performs\nalignment at the feature level on each sheet music line, and then performs a\nsecond alignment at the segment level. By operating at the segment level, it is\nable to encode domain knowledge about how likely a particular jump is. Through\ncarefully controlled experiments on unprocessed sheet music PDFs from IMSLP, we\nshow that Hierarachical DTW significantly outperforms Jump DTW in handling\nvarious types of jumps.", "published": "2020-07-29 04:04:07", "link": "http://arxiv.org/abs/2007.14580v1", "categories": ["cs.MM", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.MM"}
{"title": "End-to-End Adversarial White Box Attacks on Music Instrument\n  Classification", "abstract": "Small adversarial perturbations of input data are able to drastically change\nperformance of machine learning systems, thereby challenging the validity of\nsuch systems. We present the very first end-to-end adversarial attacks on a\nmusic instrument classification system allowing to add perturbations directly\nto audio waveforms instead of spectrograms. Our attacks are able to reduce the\naccuracy close to a random baseline while at the same time keeping\nperturbations almost imperceptible and producing misclassifications to any\ndesired instrument.", "published": "2020-07-29 09:52:32", "link": "http://arxiv.org/abs/2007.14714v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Unsupervised Generative Adversarial Alignment Representation for Sheet\n  music, Audio and Lyrics", "abstract": "Sheet music, audio, and lyrics are three main modalities during writing a\nsong. In this paper, we propose an unsupervised generative adversarial\nalignment representation (UGAAR) model to learn deep discriminative\nrepresentations shared across three major musical modalities: sheet music,\nlyrics, and audio, where a deep neural network based architecture on three\nbranches is jointly trained. In particular, the proposed model can transfer the\nstrong relationship between audio and sheet music to audio-lyrics and\nsheet-lyrics pairs by learning the correlation in the latent shared subspace.\nWe apply CCA components of audio and sheet music to establish new ground truth.\nThe generative (G) model learns the correlation of two couples of transferred\npairs to generate new audio-sheet pair for a fixed lyrics to challenge the\ndiscriminative (D) model. The discriminative model aims at distinguishing the\ninput which is from the generative model or the ground truth. The two models\nsimultaneously train in an adversarial way to enhance the ability of deep\nalignment representation learning. Our experimental results demonstrate the\nfeasibility of our proposed UGAAR for alignment representation learning among\nsheet music, audio, and lyrics.", "published": "2020-07-29 14:18:15", "link": "http://arxiv.org/abs/2007.14856v1", "categories": ["eess.AS", "cs.IR", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
{"title": "dMelodies: A Music Dataset for Disentanglement Learning", "abstract": "Representation learning focused on disentangling the underlying factors of\nvariation in given data has become an important area of research in machine\nlearning. However, most of the studies in this area have relied on datasets\nfrom the computer vision domain and thus, have not been readily extended to\nmusic. In this paper, we present a new symbolic music dataset that will help\nresearchers working on disentanglement problems demonstrate the efficacy of\ntheir algorithms on diverse domains. This will also provide a means for\nevaluating algorithms specifically designed for music. To this end, we create a\ndataset comprising of 2-bar monophonic melodies where each melody is the result\nof a unique combination of nine latent factors that span ordinal, categorical,\nand binary types. The dataset is large enough (approx. 1.3 million data points)\nto train and test deep networks for disentanglement learning. In addition, we\npresent benchmarking experiments using popular unsupervised disentanglement\nalgorithms on this dataset and compare the results with those obtained on an\nimage-based dataset.", "published": "2020-07-29 19:20:07", "link": "http://arxiv.org/abs/2007.15067v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Music FaderNets: Controllable Music Generation Based On High-Level\n  Features via Low-Level Feature Modelling", "abstract": "High-level musical qualities (such as emotion) are often abstract,\nsubjective, and hard to quantify. Given these difficulties, it is not easy to\nlearn good feature representations with supervised learning techniques, either\nbecause of the insufficiency of labels, or the subjectiveness (and hence large\nvariance) in human-annotated labels. In this paper, we present a framework that\ncan learn high-level feature representations with a limited amount of data, by\nfirst modelling their corresponding quantifiable low-level attributes. We refer\nto our proposed framework as Music FaderNets, which is inspired by the fact\nthat low-level attributes can be continuously manipulated by separate \"sliding\nfaders\" through feature disentanglement and latent regularization techniques.\nHigh-level features are then inferred from the low-level representations\nthrough semi-supervised clustering using Gaussian Mixture Variational\nAutoencoders (GM-VAEs). Using arousal as an example of a high-level feature, we\nshow that the \"faders\" of our model are disentangled and change linearly w.r.t.\nthe modelled low-level attributes of the generated output music. Furthermore,\nwe demonstrate that the model successfully learns the intrinsic relationship\nbetween arousal and its corresponding low-level attributes (rhythm and note\ndensity), with only 1% of the training set being labelled. Finally, using the\nlearnt high-level feature representations, we explore the application of our\nframework in style transfer tasks across different arousal states. The\neffectiveness of this approach is verified through a subjective listening test.", "published": "2020-07-29 16:01:45", "link": "http://arxiv.org/abs/2007.15474v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
