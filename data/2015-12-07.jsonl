{"title": "THCHS-30 : A Free Chinese Speech Corpus", "abstract": "Speech data is crucially important for speech recognition research. There are\nquite some speech databases that can be purchased at prices that are reasonable\nfor most research institutes. However, for young people who just start research\nactivities or those who just gain initial interest in this direction, the cost\nfor data is still an annoying barrier. We support the `free data' movement in\nspeech recognition: research institutes (particularly supported by public\nfunds) publish their data freely so that new researchers can obtain sufficient\ndata to kick of their career. In this paper, we follow this trend and release a\nfree Chinese speech database THCHS-30 that can be used to build a full- edged\nChinese speech recognition system. We report the baseline system established\nwith this database, including the performance under highly noisy conditions.", "published": "2015-12-07 02:07:21", "link": "http://arxiv.org/abs/1512.01882v2", "categories": ["cs.CL", "cs.SD"], "primary_category": "cs.CL"}
{"title": "Simple Baseline for Visual Question Answering", "abstract": "We describe a very simple bag-of-words baseline for visual question\nanswering. This baseline concatenates the word features from the question and\nCNN features from the image to predict the answer. When evaluated on the\nchallenging VQA dataset [2], it shows comparable performance to many recent\napproaches using recurrent neural networks. To explore the strength and\nweakness of the trained model, we also provide an interactive web demo and\nopen-source code. .", "published": "2015-12-07 19:00:54", "link": "http://arxiv.org/abs/1512.02167v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Thinking Required", "abstract": "There exists a theory of a single general-purpose learning algorithm which\ncould explain the principles its operation. It assumes the initial rough\narchitecture, a small library of simple innate circuits which are prewired at\nbirth. and proposes that all significant mental algorithms are learned. Given\ncurrent understanding and observations, this paper reviews and lists the\ningredients of such an algorithm from architectural and functional\nperspectives.", "published": "2015-12-07 06:37:49", "link": "http://arxiv.org/abs/1512.01926v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Jointly Modeling Topics and Intents with Global Order Structure", "abstract": "Modeling document structure is of great importance for discourse analysis and\nrelated applications. The goal of this research is to capture the document\nintent structure by modeling documents as a mixture of topic words and\nrhetorical words. While the topics are relatively unchanged through one\ndocument, the rhetorical functions of sentences usually change following\ncertain orders in discourse. We propose GMM-LDA, a topic modeling based\nBayesian unsupervised model, to analyze the document intent structure\ncooperated with order information. Our model is flexible that has the ability\nto combine the annotations and do supervised learning. Additionally, entropic\nregularization can be introduced to model the significant divergence between\ntopics and intents. We perform experiments in both unsupervised and supervised\nsettings, results show the superiority of our model over several\nstate-of-the-art baselines.", "published": "2015-12-07 12:16:58", "link": "http://arxiv.org/abs/1512.02009v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
