{"title": "UNICORN on RAINBOW: A Universal Commonsense Reasoning Model on a New\n  Multitask Benchmark", "abstract": "Commonsense AI has long been seen as a near impossible goal -- until\nrecently. Now, research interest has sharply increased with an influx of new\nbenchmarks and models.\n  We propose two new ways to evaluate commonsense models, emphasizing their\ngenerality on new tasks and building on diverse, recently introduced\nbenchmarks. First, we propose a new multitask benchmark, RAINBOW, to promote\nresearch on commonsense models that generalize well over multiple tasks and\ndatasets. Second, we propose a novel evaluation, the cost equivalent curve,\nthat sheds new insight on how the choice of source datasets, pretrained\nlanguage models, and transfer learning methods impacts performance and data\nefficiency.\n  We perform extensive experiments -- over 200 experiments encompassing 4800\nmodels -- and report multiple valuable and sometimes surprising findings, e.g.,\nthat transfer almost always leads to better or equivalent performance if\nfollowing a particular recipe, that QA-based commonsense datasets transfer well\nwith each other, while commonsense knowledge graphs do not, and that perhaps\ncounter-intuitively, larger models benefit more from transfer than smaller\nones.\n  Last but not least, we introduce a new universal commonsense reasoning model,\nUNICORN, that establishes new state-of-the-art performance across 8 popular\ncommonsense benchmarks, aNLI (87.3%), CosmosQA (91.8%), HellaSWAG (93.9%), PIQA\n(90.1%), SocialIQa (83.2%), WinoGrande (86.6%), CycIC (94.0%) and CommonsenseQA\n(79.3%).", "published": "2021-03-24 06:32:20", "link": "http://arxiv.org/abs/2103.13009v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Topic Modeling Genre: An Exploration of French Classical and\n  Enlightenment Drama", "abstract": "The concept of literary genre is a highly complex one: not only are different\ngenres frequently defined on several, but not necessarily the same levels of\ndescription, but consideration of genres as cognitive, social, or scholarly\nconstructs with a rich history further complicate the matter. This contribution\nfocuses on thematic aspects of genre with a quantitative approach, namely Topic\nModeling. Topic Modeling has proven to be useful to discover thematic patterns\nand trends in large collections of texts, with a view to class or browse them\non the basis of their dominant themes. It has rarely if ever, however, been\napplied to collections of dramatic texts.\n  In this contribution, Topic Modeling is used to analyze a collection of\nFrench Drama of the Classical Age and the Enlightenment. The general aim of\nthis contribution is to discover what semantic types of topics are found in\nthis collection, whether different dramatic subgenres have distinctive dominant\ntopics and plot-related topic patterns, and inversely, to what extent\nclustering methods based on topic scores per play produce groupings of texts\nwhich agree with more conventional genre distinctions. This contribution shows\nthat interesting topic patterns can be detected which provide new insights into\nthe thematic, subgenre-related structure of French drama as well as into the\nhistory of French drama of the Classical Age and the Enlightenment.", "published": "2021-03-24 06:57:00", "link": "http://arxiv.org/abs/2103.13019v1", "categories": ["cs.CL", "J.5"], "primary_category": "cs.CL"}
{"title": "Czert -- Czech BERT-like Model for Language Representation", "abstract": "This paper describes the training process of the first Czech monolingual\nlanguage representation models based on BERT and ALBERT architectures. We\npre-train our models on more than 340K of sentences, which is 50 times more\nthan multilingual models that include Czech data. We outperform the\nmultilingual models on 9 out of 11 datasets. In addition, we establish the new\nstate-of-the-art results on nine datasets. At the end, we discuss properties of\nmonolingual and multilingual models based upon our results. We publish all the\npre-trained and fine-tuned models freely for the research community.", "published": "2021-03-24 07:27:28", "link": "http://arxiv.org/abs/2103.13031v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Thinking Aloud: Dynamic Context Generation Improves Zero-Shot Reasoning\n  Performance of GPT-2", "abstract": "Thinking aloud is an effective meta-cognitive strategy human reasoners apply\nto solve difficult problems. We suggest to improve the reasoning ability of\npre-trained neural language models in a similar way, namely by expanding a\ntask's context with problem elaborations that are dynamically generated by the\nlanguage model itself. Our main result is that dynamic problem elaboration\nsignificantly improves the zero-shot performance of GPT-2 in a deductive\nreasoning and natural language inference task: While the model uses a syntactic\nheuristic for predicting an answer, it is capable (to some degree) of\ngenerating reasoned additional context which facilitates the successful\napplication of its heuristic. We explore different ways of generating\nelaborations, including fewshot learning, and find that their relative\nperformance varies with the specific problem characteristics (such as problem\ndifficulty). Moreover, the effectiveness of an elaboration can be explained in\nterms of the degree to which the elaboration semantically coheres with the\ncorresponding problem. In particular, elaborations that are most faithful to\nthe original problem description may boost accuracy by up to 24%.", "published": "2021-03-24 07:33:25", "link": "http://arxiv.org/abs/2103.13033v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Finetuning Pretrained Transformers into RNNs", "abstract": "Transformers have outperformed recurrent neural networks (RNNs) in natural\nlanguage generation. But this comes with a significant computational cost, as\nthe attention mechanism's complexity scales quadratically with sequence length.\nEfficient transformer variants have received increasing interest in recent\nworks. Among them, a linear-complexity recurrent variant has proven well suited\nfor autoregressive generation. It approximates the softmax attention with\nrandomized or heuristic feature maps, but can be difficult to train and may\nyield suboptimal accuracy. This work aims to convert a pretrained transformer\ninto its efficient recurrent counterpart, improving efficiency while\nmaintaining accuracy. Specifically, we propose a swap-then-finetune procedure:\nin an off-the-shelf pretrained transformer, we replace the softmax attention\nwith its linear-complexity recurrent alternative and then finetune. With a\nlearned feature map, our approach provides an improved tradeoff between\nefficiency and accuracy over the standard transformer and other recurrent\nvariants. We also show that the finetuning process has lower training cost\nrelative to training these recurrent variants from scratch. As many models for\nnatural language tasks are increasingly dependent on large-scale pretrained\ntransformers, this work presents a viable approach to improving inference\nefficiency without repeating the expensive pretraining process.", "published": "2021-03-24 10:50:43", "link": "http://arxiv.org/abs/2103.13076v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Paragraph-level Rationale Extraction through Regularization: A case\n  study on European Court of Human Rights Cases", "abstract": "Interpretability or explainability is an emerging research field in NLP. From\na user-centric point of view, the goal is to build models that provide proper\njustification for their decisions, similar to those of humans, by requiring the\nmodels to satisfy additional constraints. To this end, we introduce a new\napplication on legal text where, contrary to mainstream literature targeting\nword-level rationales, we conceive rationales as selected paragraphs in\nmulti-paragraph structured court cases. We also release a new dataset\ncomprising European Court of Human Rights cases, including annotations for\nparagraph-level rationales. We use this dataset to study the effect of already\nproposed rationale constraints, i.e., sparsity, continuity, and\ncomprehensiveness, formulated as regularizers. Our findings indicate that some\nof these constraints are not beneficial in paragraph-level rationale\nextraction, while others need re-formulation to better handle the multi-label\nnature of the task we consider. We also introduce a new constraint,\nsingularity, which further improves the quality of rationales, even compared\nwith noisy rationale supervision. Experimental results indicate that the newly\nintroduced task is very challenging and there is a large scope for further\nresearch.", "published": "2021-03-24 10:58:39", "link": "http://arxiv.org/abs/2103.13084v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Finnish Paraphrase Corpus", "abstract": "In this paper, we introduce the first fully manually annotated paraphrase\ncorpus for Finnish containing 53,572 paraphrase pairs harvested from\nalternative subtitles and news headings. Out of all paraphrase pairs in our\ncorpus 98% are manually classified to be paraphrases at least in their given\ncontext, if not in all contexts. Additionally, we establish a manual candidate\nselection method and demonstrate its feasibility in high quality paraphrase\nselection in terms of both cost and quality.", "published": "2021-03-24 11:24:10", "link": "http://arxiv.org/abs/2103.13103v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Low-Resource Machine Translation Training Curriculum Fit for\n  Low-Resource Languages", "abstract": "We conduct an empirical study of neural machine translation (NMT) for truly\nlow-resource languages, and propose a training curriculum fit for cases when\nboth parallel training data and compute resource are lacking, reflecting the\nreality of most of the world's languages and the researchers working on these\nlanguages. Previously, unsupervised NMT, which employs back-translation (BT)\nand auto-encoding (AE) tasks has been shown barren for low-resource languages.\nWe demonstrate that leveraging comparable data and code-switching as weak\nsupervision, combined with BT and AE objectives, result in remarkable\nimprovements for low-resource languages even when using only modest compute\nresources. The training curriculum proposed in this work achieves BLEU scores\nthat improve over supervised NMT trained on the same backbone architecture by\n+12.2 BLEU for English to Gujarati and +3.7 BLEU for English to Kazakh,\nshowcasing the potential of weakly-supervised NMT for the low-resource\nlanguages. When trained on supervised data, our training curriculum achieves a\nnew state-of-the-art result on the Somali dataset (BLEU of 29.3 for Somali to\nEnglish). We also observe that adding more time and GPUs to training can\nfurther improve performance, which underscores the importance of reporting\ncompute resource usage in MT research.", "published": "2021-03-24 15:40:28", "link": "http://arxiv.org/abs/2103.13272v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "When Word Embeddings Become Endangered", "abstract": "Big languages such as English and Finnish have many natural language\nprocessing (NLP) resources and models, but this is not the case for\nlow-resourced and endangered languages as such resources are so scarce despite\nthe great advantages they would provide for the language communities. The most\ncommon types of resources available for low-resourced and endangered languages\nare translation dictionaries and universal dependencies. In this paper, we\npresent a method for constructing word embeddings for endangered languages\nusing existing word embeddings of different resource-rich languages and the\ntranslation dictionaries of resource-poor languages. Thereafter, the embeddings\nare fine-tuned using the sentences in the universal dependencies and aligned to\nmatch the semantic spaces of the big languages; resulting in cross-lingual\nembeddings. The endangered languages we work with here are Erzya, Moksha,\nKomi-Zyrian and Skolt Sami. Furthermore, we build a universal sentiment\nanalysis model for all the languages that are part of this study, whether\nendangered or not, by utilizing cross-lingual word embeddings. The evaluation\nconducted shows that our word embeddings for endangered languages are\nwell-aligned with the resource-rich languages, and they are suitable for\ntraining task-specific models as demonstrated by our sentiment analysis model\nwhich achieved a high accuracy. All our cross-lingual word embeddings and the\nsentiment analysis model have been released openly via an easy-to-use Python\nlibrary.", "published": "2021-03-24 15:42:53", "link": "http://arxiv.org/abs/2103.13275v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "StyleKQC: A Style-Variant Paraphrase Corpus for Korean Questions and\n  Commands", "abstract": "Paraphrasing is often performed with less concern for controlled style\nconversion. Especially for questions and commands, style-variant paraphrasing\ncan be crucial in tone and manner, which also matters with industrial\napplications such as dialog systems. In this paper, we attack this issue with a\ncorpus construction scheme that simultaneously considers the core content and\nstyle of directives, namely intent and formality, for the Korean language.\nUtilizing manually generated natural language queries on six daily topics, we\nexpand the corpus to formal and informal sentences by human rewriting and\ntransferring. We verify the validity and industrial applicability of our\napproach by checking the adequate classification and inference performance that\nfit with conventional fine-tuning approaches, at the same time proposing a\nsupervised formality transfer task.", "published": "2021-03-24 18:38:53", "link": "http://arxiv.org/abs/2103.13439v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CSFCube -- A Test Collection of Computer Science Research Articles for\n  Faceted Query by Example", "abstract": "Query by Example is a well-known information retrieval task in which a\ndocument is chosen by the user as the search query and the goal is to retrieve\nrelevant documents from a large collection. However, a document often covers\nmultiple aspects of a topic. To address this scenario we introduce the task of\nfaceted Query by Example in which users can also specify a finer grained aspect\nin addition to the input query document. We focus on the application of this\ntask in scientific literature search. We envision models which are able to\nretrieve scientific papers analogous to a query scientific paper along\nspecifically chosen rhetorical structure elements as one solution to this\nproblem. In this work, the rhetorical structure elements, which we refer to as\nfacets, indicate objectives, methods, or results of a scientific paper. We\nintroduce and describe an expert annotated test collection to evaluate models\ntrained to perform this task. Our test collection consists of a diverse set of\n50 query documents in English, drawn from computational linguistics and machine\nlearning venues. We carefully follow the annotation guideline used by TREC for\ndepth-k pooling (k = 100 or 250) and the resulting data collection consists of\ngraded relevance scores with high annotation agreement. State of the art models\nevaluated on our dataset show a significant gap to be closed in further work.\nOur dataset may be accessed here: https://github.com/iesl/CSFCube", "published": "2021-03-24 01:02:12", "link": "http://arxiv.org/abs/2103.12906v3", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Supporting Clustering with Contrastive Learning", "abstract": "Unsupervised clustering aims at discovering the semantic categories of data\naccording to some distance measured in the representation space. However,\ndifferent categories often overlap with each other in the representation space\nat the beginning of the learning process, which poses a significant challenge\nfor distance-based clustering in achieving good separation between different\ncategories. To this end, we propose Supporting Clustering with Contrastive\nLearning (SCCL) -- a novel framework to leverage contrastive learning to\npromote better separation. We assess the performance of SCCL on short text\nclustering and show that SCCL significantly advances the state-of-the-art\nresults on most benchmark datasets with 3%-11% improvement on Accuracy and\n4%-15% improvement on Normalized Mutual Information. Furthermore, our\nquantitative analysis demonstrates the effectiveness of SCCL in leveraging the\nstrengths of both bottom-up instance discrimination and top-down clustering to\nachieve better intra-cluster and inter-cluster distances when evaluated with\nthe ground truth cluster labels.", "published": "2021-03-24 03:05:17", "link": "http://arxiv.org/abs/2103.12953v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Language learnability in the limit for general metrics: a Gold-Angluin\n  result", "abstract": "In his pioneering work in the field of Inductive Inference, Gold (1967)\nproved that a set containing all finite languages and at least one infinite\nlanguage over the same fixed alphabet is not learnable in the exact sense.\nWithin the same framework, Angluin (1980) provided a complete characterization\nfor the learnability of language families. Mathematically, the concept of exact\nlearning in that classical setting can be seen as the use of a particular type\nof metric for learning in the limit. In this short research note we use\nNiyogi's extended version of a theorem by Blum and Blum (1975) on the existence\nof locking data sets to prove a necessary condition for learnability in the\nlimit of any family of languages in any given metric. This recovers Gold's\ntheorem as a special case. Moreover, when the language family is further\nassumed to contain all finite languages, the same condition also becomes\nsufficient for learnability in the limit.", "published": "2021-03-24 13:11:09", "link": "http://arxiv.org/abs/2103.13166v1", "categories": ["cs.CL", "cs.FL"], "primary_category": "cs.CL"}
{"title": "Are Multilingual Models Effective in Code-Switching?", "abstract": "Multilingual language models have shown decent performance in multilingual\nand cross-lingual natural language understanding tasks. However, the power of\nthese multilingual models in code-switching tasks has not been fully explored.\nIn this paper, we study the effectiveness of multilingual language models to\nunderstand their capability and adaptability to the mixed-language setting by\nconsidering the inference speed, performance, and number of parameters to\nmeasure their practicality. We conduct experiments in three language pairs on\nnamed entity recognition and part-of-speech tagging and compare them with\nexisting methods, such as using bilingual embeddings and multilingual\nmeta-embeddings. Our findings suggest that pre-trained multilingual models do\nnot necessarily guarantee high-quality representations on code-switching, while\nusing meta-embeddings achieves similar results with significantly fewer\nparameters.", "published": "2021-03-24 16:20:02", "link": "http://arxiv.org/abs/2103.13309v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Voice Privacy with Smart Digital Assistants in Educational Settings", "abstract": "The emergence of voice-assistant devices ushers in delightful user\nexperiences not just on the smart home front, but also in diverse educational\nenvironments from classrooms to personalized-learning/tutoring. However, the\nuse of voice as an interaction modality also could result in exposure of user's\nidentity, and hinders the broader adoption of voice interfaces; this is\nespecially important in environments where children are present and their voice\nprivacy needs to be protected. To this end, building on state-of-the-art\ntechniques proposed in the literature, we design and evaluate a practical and\nefficient framework for voice privacy at the source. The approach combines\nspeaker identification (SID) and speech conversion methods to randomly disguise\nthe identity of users right on the device that records the speech, while\nensuring that the transformed utterances of users can still be successfully\ntranscribed by Automatic Speech Recognition (ASR) solutions. We evaluate the\nASR performance of the conversion in terms of word error rate and show the\npromise of this framework in preserving the content of the input speech.", "published": "2021-03-24 19:58:45", "link": "http://arxiv.org/abs/2104.11038v1", "categories": ["eess.AS", "cs.CL", "cs.CR", "cs.SD"], "primary_category": "eess.AS"}
{"title": "VLGrammar: Grounded Grammar Induction of Vision and Language", "abstract": "Cognitive grammar suggests that the acquisition of language grammar is\ngrounded within visual structures. While grammar is an essential representation\nof natural language, it also exists ubiquitously in vision to represent the\nhierarchical part-whole structure. In this work, we study grounded grammar\ninduction of vision and language in a joint learning framework. Specifically,\nwe present VLGrammar, a method that uses compound probabilistic context-free\ngrammars (compound PCFGs) to induce the language grammar and the image grammar\nsimultaneously. We propose a novel contrastive learning framework to guide the\njoint learning of both modules. To provide a benchmark for the grounded grammar\ninduction task, we collect a large-scale dataset, \\textsc{PartIt}, which\ncontains human-written sentences that describe part-level semantics for 3D\nobjects. Experiments on the \\textsc{PartIt} dataset show that VLGrammar\noutperforms all baselines in image grammar induction and language grammar\ninduction. The learned VLGrammar naturally benefits related downstream tasks.\nSpecifically, it improves the image unsupervised clustering accuracy by 30\\%,\nand performs well in image retrieval and text retrieval. Notably, the induced\ngrammar shows superior generalizability by easily generalizing to unseen\ncategories.", "published": "2021-03-24 04:05:08", "link": "http://arxiv.org/abs/2103.12975v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Representing Numbers in NLP: a Survey and a Vision", "abstract": "NLP systems rarely give special consideration to numbers found in text. This\nstarkly contrasts with the consensus in neuroscience that, in the brain,\nnumbers are represented differently from words. We arrange recent NLP work on\nnumeracy into a comprehensive taxonomy of tasks and methods. We break down the\nsubjective notion of numeracy into 7 subtasks, arranged along two dimensions:\ngranularity (exact vs approximate) and units (abstract vs grounded). We analyze\nthe myriad representational choices made by 18 previously published number\nencoders and decoders. We synthesize best practices for representing numbers in\ntext and articulate a vision for holistic numeracy in NLP, comprised of design\ntrade-offs and a unified evaluation.", "published": "2021-03-24 12:28:22", "link": "http://arxiv.org/abs/2103.13136v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "FastMoE: A Fast Mixture-of-Expert Training System", "abstract": "Mixture-of-Expert (MoE) presents a strong potential in enlarging the size of\nlanguage model to trillions of parameters. However, training trillion-scale MoE\nrequires algorithm and system co-design for a well-tuned high performance\ndistributed training system. Unfortunately, the only existing platform that\nmeets the requirements strongly depends on Google's hardware (TPU) and software\n(Mesh Tensorflow) stack, and is not open and available to the public,\nespecially GPU and PyTorch communities.\n  In this paper, we present FastMoE, a distributed MoE training system based on\nPyTorch with common accelerators. The system provides a hierarchical interface\nfor both flexible model design and easy adaption to different applications,\nsuch as Transformer-XL and Megatron-LM. Different from direct implementation of\nMoE models using PyTorch, the training speed is highly optimized in FastMoE by\nsophisticated high-performance acceleration skills. The system supports placing\ndifferent experts on multiple GPUs across multiple nodes, enabling enlarging\nthe number of experts linearly against the number of GPUs. The source of\nFastMoE is available at https://github.com/laekov/fastmoe under Apache-2\nlicense.", "published": "2021-03-24 15:27:15", "link": "http://arxiv.org/abs/2103.13262v1", "categories": ["cs.LG", "cs.CL", "cs.DC"], "primary_category": "cs.LG"}
{"title": "Learning to Generate Code Comments from Class Hierarchies", "abstract": "Descriptive code comments are essential for supporting code comprehension and\nmaintenance. We propose the task of automatically generating comments for\noverriding methods. We formulate a novel framework which accommodates the\nunique contextual and linguistic reasoning that is required for performing this\ntask. Our approach features: (1) incorporating context from the class\nhierarchy; (2) conditioning on learned, latent representations of specificity\nto generate comments that capture the more specialized behavior of the\noverriding method; and (3) unlikelihood training to discourage predictions\nwhich do not conform to invariant characteristics of the comment corresponding\nto the overridden method. Our experiments show that the proposed approach is\nable to generate comments for overriding methods of higher quality compared to\nprevailing comment generation techniques.", "published": "2021-03-24 18:12:52", "link": "http://arxiv.org/abs/2103.13426v2", "categories": ["cs.CL", "cs.LG", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Transfer Learning for Piano Sustain-Pedal Detection", "abstract": "Detecting piano pedalling techniques in polyphonic music remains a\nchallenging task in music information retrieval. While other piano-related\ntasks, such as pitch estimation and onset detection, have seen improvement\nthrough applying deep learning methods, little work has been done to develop\ndeep learning models to detect playing techniques. In this paper, we propose a\ntransfer learning approach for the detection of sustain-pedal techniques, which\nare commonly used by pianists to enrich the sound. In the source task, a\nconvolutional neural network (CNN) is trained for learning spectral and\ntemporal contexts when the sustain pedal is pressed using a large dataset\ngenerated by a physical modelling virtual instrument. The CNN is designed and\nexperimented through exploiting the knowledge of piano acoustics and physics.\nThis can achieve an accuracy score of 0.98 in the validation results. In the\ntarget task, the knowledge learned from the synthesised data can be transferred\nto detect the sustain pedal in acoustic piano recordings. A concatenated\nfeature vector using the activations of the trained convolutional layers is\nextracted from the recordings and classified into frame-wise pedal press or\nrelease. We demonstrate the effectiveness of our method in acoustic piano\nrecordings of Chopin's music. From the cross-validation results, the proposed\ntransfer learning method achieves an average F-measure of 0.89 and an overall\nperformance of 0.84 obtained using the micro-averaged F-measure. These results\noutperform applying the pre-trained CNN model directly or the model with a\nfine-tuned last layer.", "published": "2021-03-24 14:28:53", "link": "http://arxiv.org/abs/2103.13219v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Blind Speech Separation and Dereverberation using Neural Beamforming", "abstract": "In this paper, we present the Blind Speech Separation and Dereverberation\n(BSSD) network, which performs simultaneous speaker separation, dereverberation\nand speaker identification in a single neural network. Speaker separation is\nguided by a set of predefined spatial cues. Dereverberation is performed by\nusing neural beamforming, and speaker identification is aided by embedding\nvectors and triplet mining. We introduce a frequency-domain model which uses\ncomplex-valued neural networks, and a time-domain variant which performs\nbeamforming in latent space. Further, we propose a block-online mode to process\nlonger audio recordings, as they occur in meeting scenarios. We evaluate our\nsystem in terms of Scale Independent Signal to Distortion Ratio (SI-SDR), Word\nError Rate (WER) and Equal Error Rate (EER).", "published": "2021-03-24 18:43:52", "link": "http://arxiv.org/abs/2103.13443v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
