{"title": "Building a Personalized Dialogue System with Prompt-Tuning", "abstract": "Dialogue systems without consistent responses are not fascinating. In this\nstudy, we build a dialogue system that can respond based on a given character\nsetting (persona) to bring consistency. Considering the trend of the rapidly\nincreasing scale of language models, we propose an approach that uses\nprompt-tuning, which has low learning costs, on pre-trained large-scale\nlanguage models. The results of automatic and manual evaluations in English and\nJapanese show that it is possible to build a dialogue system with more natural\nand personalized responses using less computational resources than fine-tuning.", "published": "2022-06-11 02:21:11", "link": "http://arxiv.org/abs/2206.05399v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Decomposition-Based Approach for Evaluating Inter-Annotator\n  Disagreement in Narrative Analysis", "abstract": "In this work, we explore sources of inter-annotator disagreement in narrative\nanalysis, in light of the question of whether or not a narrative plot exists in\nthe text. For this purpose, we present a method for a conceptual decomposition\nof an existing annotation into two separate levels: (1) \\textbf{whether} or not\na narrative plot exists in the text, and (2) \\textbf{which} plot elements exist\nin the text. We apply this method to an existing dataset of sentences annotated\nwith three different narrative plot elements: \\textit{Complication},\n\\textit{Resolution} and \\textit{Success}. We then employ statistical analysis\nin order to quantify how much of the inter-annotator disagreement can be\nexplained by each of the two levels. We further perform a qualitative analysis\nof disagreement cases in each level, observing several sources of disagreement,\nsuch as text ambiguity, scheme definition and personal differences between the\nannotators. The insights gathered on the dataset may serve to reduce\ninter-annotator disagreement in future annotation endeavors. We conclude with a\nbroader discussion on the potential implications of our approach in studying\nand evaluating inter-annotator disagreement in other settings.", "published": "2022-06-11 07:02:50", "link": "http://arxiv.org/abs/2206.05446v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparative Snippet Generation", "abstract": "We model product reviews to generate comparative responses consisting of\npositive and negative experiences regarding the product. Specifically, we\ngenerate a single-sentence, comparative response from a given positive and a\nnegative opinion. We contribute the first dataset for this task of Comparative\nSnippet Generation from contrasting opinions regarding a product, and a\nperformance analysis of a pre-trained BERT model to generate such snippets.", "published": "2022-06-11 09:02:27", "link": "http://arxiv.org/abs/2206.05473v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving the Adversarial Robustness of NLP Models by Information\n  Bottleneck", "abstract": "Existing studies have demonstrated that adversarial examples can be directly\nattributed to the presence of non-robust features, which are highly predictive,\nbut can be easily manipulated by adversaries to fool NLP models. In this study,\nwe explore the feasibility of capturing task-specific robust features, while\neliminating the non-robust ones by using the information bottleneck theory.\nThrough extensive experiments, we show that the models trained with our\ninformation bottleneck-based method are able to achieve a significant\nimprovement in robust accuracy, exceeding performances of all the previously\nreported defense methods while suffering almost no performance drop in clean\naccuracy on SST-2, AGNEWS and IMDB datasets.", "published": "2022-06-11 12:12:20", "link": "http://arxiv.org/abs/2206.05511v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Why is constrained neural language generation particularly challenging?", "abstract": "Recent advances in deep neural language models combined with the capacity of\nlarge scale datasets have accelerated the development of natural language\ngeneration systems that produce fluent and coherent texts (to various degrees\nof success) in a multitude of tasks and application contexts. However,\ncontrolling the output of these models for desired user and task needs is still\nan open challenge. This is crucial not only to customizing the content and\nstyle of the generated language, but also to their safe and reliable deployment\nin the real world. We present an extensive survey on the emerging topic of\nconstrained neural language generation in which we formally define and\ncategorize the problems of natural language generation by distinguishing\nbetween conditions and constraints (the latter being testable conditions on the\noutput text instead of the input), present constrained text generation tasks,\nand review existing methods and evaluation metrics for constrained text\ngeneration. Our aim is to highlight recent progress and trends in this emerging\nfield, informing on the most promising directions and limitations towards\nadvancing the state-of-the-art of constrained neural language generation\nresearch.", "published": "2022-06-11 02:07:33", "link": "http://arxiv.org/abs/2206.05395v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Investigation of Ensemble features of Self-Supervised Pretrained Models\n  for Automatic Speech Recognition", "abstract": "Self-supervised learning (SSL) based models have been shown to generate\npowerful representations that can be used to improve the performance of\ndownstream speech tasks. Several state-of-the-art SSL models are available, and\neach of these models optimizes a different loss which gives rise to the\npossibility of their features being complementary. This paper proposes using an\nensemble of such SSL representations and models, which exploits the\ncomplementary nature of the features extracted by the various pretrained\nmodels. We hypothesize that this results in a richer feature representation and\nshows results for the ASR downstream task. To this end, we use three SSL models\nthat have shown excellent results on ASR tasks, namely HuBERT, Wav2vec2.0, and\nWaveLM. We explore the ensemble of models fine-tuned for the ASR task and the\nensemble of features using the embeddings obtained from the pre-trained models\nfor a downstream ASR task. We get improved performance over individual models\nand pre-trained features using Librispeech(100h) and WSJ dataset for the\ndownstream tasks.", "published": "2022-06-11 12:43:00", "link": "http://arxiv.org/abs/2206.05518v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Bridging the Gap Between Training and Inference of Bayesian Controllable\n  Language Models", "abstract": "Large-scale pre-trained language models have achieved great success on\nnatural language generation tasks. However, it is difficult to control the\npre-trained language models to generate sentences with the desired attribute\nsuch as topic and sentiment, etc. Recently, Bayesian Controllable Language\nModels (BCLMs) have been shown to be efficient in controllable language\ngeneration. Rather than fine-tuning the parameters of pre-trained language\nmodels, BCLMs use external discriminators to guide the generation of\npre-trained language models. However, the mismatch between training and\ninference of BCLMs limits the performance of the models. To address the\nproblem, in this work we propose a \"Gemini Discriminator\" for controllable\nlanguage generation which alleviates the mismatch problem with a small\ncomputational cost. We tested our method on two controllable language\ngeneration tasks: sentiment control and topic control. On both tasks, our\nmethod reached achieved new state-of-the-art results in automatic and human\nevaluations.", "published": "2022-06-11 12:52:32", "link": "http://arxiv.org/abs/2206.05519v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Unified Continuous Learning Framework for Multi-modal Knowledge\n  Discovery and Pre-training", "abstract": "Multi-modal pre-training and knowledge discovery are two important research\ntopics in multi-modal machine learning. Nevertheless, none of existing works\nmake attempts to link knowledge discovery with knowledge guided multi-modal\npre-training. In this paper, we propose to unify them into a continuous\nlearning framework for mutual improvement. Taking the open-domain uni-modal\ndatasets of images and texts as input, we maintain a knowledge graph as the\nfoundation to support these two tasks. For knowledge discovery, a pre-trained\nmodel is used to identify cross-modal links on the graph. For model\npre-training, the knowledge graph is used as the external knowledge to guide\nthe model updating. These two steps are iteratively performed in our framework\nfor continuous learning. The experimental results on MS-COCO and Flickr30K with\nrespect to both knowledge discovery and the pre-trained model validate the\neffectiveness of our framework.", "published": "2022-06-11 16:05:06", "link": "http://arxiv.org/abs/2206.05555v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Can the Language of the Collation be Translated into the Language of the\n  Stemma? Using Machine Translation for Witness Localization", "abstract": "Stemmatology is a subfield of philology where one approach to understand the\ncopy-history of textual variants of a text (witnesses of a tradition) is to\ngenerate an evolutionary tree. Computational methods are partly shared between\nthe sister discipline of phylogenetics and stemmatology. In 2022, a surveypaper\nin nature communications found that Deep Learning (DL), which otherwise has\nbrought about major improvements in many fields (Krohn et al 2020) has had only\nminor successes in phylogenetics and that \"it is difficult to conceive of an\nend-to-end DL model to directly estimate phylogenetic trees from raw data in\nthe near future\"(Sapoval et al. 2022, p.8). In stemmatology, there is to date\nno known DL approach at all. In this paper, we present a new DL approach to\nplacement of manuscripts on a stemma and demonstrate its potential. This could\nbe extended to phylogenetics where the universal code of DNA might be an even\nbetter prerequisite for the method using sequence to sequence based neural\nnetworks in order to retrieve tree distances.", "published": "2022-06-11 20:10:21", "link": "http://arxiv.org/abs/2206.05603v1", "categories": ["cs.CL", "q-bio.PE"], "primary_category": "cs.CL"}
{"title": "Signal-informed DNN-based DOA Estimation combining an External\n  Microphone and GCC-PHAT Features", "abstract": "Aiming at estimating the direction of arrival (DOA) of a desired speaker in a\nmulti-talker environment using a microphone array, in this paper we propose a\nsignal-informed method exploiting the availability of an external microphone\nattached to the desired speaker. The proposed method applies a binary mask to\nthe GCC-PHAT input features of a convolutional neural network, where the binary\nmask is computed based on the power distribution of the external microphone\nsignal. Experimental results for a reverberant scenario with up to four\ninterfering speakers demonstrate that the signal-informed masking improves the\nlocalization accuracy, without requiring any knowledge about the interfering\nspeakers.", "published": "2022-06-11 20:14:37", "link": "http://arxiv.org/abs/2206.05606v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multi-instrument Music Synthesis with Spectrogram Diffusion", "abstract": "An ideal music synthesizer should be both interactive and expressive,\ngenerating high-fidelity audio in realtime for arbitrary combinations of\ninstruments and notes. Recent neural synthesizers have exhibited a tradeoff\nbetween domain-specific models that offer detailed control of only specific\ninstruments, or raw waveform models that can train on any music but with\nminimal control and slow generation. In this work, we focus on a middle ground\nof neural synthesizers that can generate audio from MIDI sequences with\narbitrary combinations of instruments in realtime. This enables training on a\nwide range of transcription datasets with a single model, which in turn offers\nnote-level control of composition and instrumentation across a wide range of\ninstruments. We use a simple two-stage process: MIDI to spectrograms with an\nencoder-decoder Transformer, then spectrograms to audio with a generative\nadversarial network (GAN) spectrogram inverter. We compare training the decoder\nas an autoregressive model and as a Denoising Diffusion Probabilistic Model\n(DDPM) and find that the DDPM approach is superior both qualitatively and as\nmeasured by audio reconstruction and Fr\\'echet distance metrics. Given the\ninteractivity and generality of this approach, we find this to be a promising\nfirst step towards interactive and expressive neural synthesis for arbitrary\ncombinations of instruments and notes.", "published": "2022-06-11 03:26:15", "link": "http://arxiv.org/abs/2206.05408v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Hierarchical Conditional Variational Autoencoder Based Acoustic Anomaly\n  Detection", "abstract": "This paper aims to develop an acoustic signal-based unsupervised anomaly\ndetection method for automatic machine monitoring. Existing approaches such as\ndeep autoencoder (DAE), variational autoencoder (VAE), conditional variational\nautoencoder (CVAE) etc. have limited representation capabilities in the latent\nspace and, hence, poor anomaly detection performance. Different models have to\nbe trained for each different kind of machines to accurately perform the\nanomaly detection task. To solve this issue, we propose a new method named as\nhierarchical conditional variational autoencoder (HCVAE). This method utilizes\navailable taxonomic hierarchical knowledge about industrial facility to refine\nthe latent space representation. This knowledge helps model to improve the\nanomaly detection performance as well. We demonstrated the generalization\ncapability of a single HCVAE model for different types of machines by using\nappropriate conditions. Additionally, to show the practicability of the\nproposed approach, (i) we evaluated HCVAE model on different domain and (ii) we\nchecked the effect of partial hierarchical knowledge. Our results show that\nHCVAE method validates both of these points, and it outperforms the baseline\nsystem on anomaly detection task by utmost 15 % on the AUC score metric.", "published": "2022-06-11 08:15:01", "link": "http://arxiv.org/abs/2206.05460v1", "categories": ["cs.LG", "cs.AI", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Svadhyaya system for the Second Diagnosing COVID-19 using Acoustics\n  Challenge 2021", "abstract": "This report describes the system used for detecting COVID-19 positives using\nthree different acoustic modalities, namely speech, breathing, and cough in the\nsecond DiCOVA challenge. The proposed system is based on the combination of 4\ndifferent approaches, each focusing more on one aspect of the problem, and\nreaches the blind test AUCs of 86.41, 77.60, and 84.55, in the breathing,\ncough, and speech tracks, respectively, and the AUC of 85.37 in the fusion of\nthese three tracks.", "published": "2022-06-11 08:26:11", "link": "http://arxiv.org/abs/2206.05462v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
