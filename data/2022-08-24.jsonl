{"title": "A Hierarchical Interactive Network for Joint Span-based Aspect-Sentiment\n  Analysis", "abstract": "Recently, some span-based methods have achieved encouraging performances for\njoint aspect-sentiment analysis, which first extract aspects (aspect\nextraction) by detecting aspect boundaries and then classify the span-level\nsentiments (sentiment classification). However, most existing approaches either\nsequentially extract task-specific features, leading to insufficient feature\ninteractions, or they encode aspect features and sentiment features in a\nparallel manner, implying that feature representation in each task is largely\nindependent of each other except for input sharing. Both of them ignore the\ninternal correlations between the aspect extraction and sentiment\nclassification. To solve this problem, we novelly propose a hierarchical\ninteractive network (HI-ASA) to model two-way interactions between two tasks\nappropriately, where the hierarchical interactions involve two steps:\nshallow-level interaction and deep-level interaction. First, we utilize\ncross-stitch mechanism to combine the different task-specific features\nselectively as the input to ensure proper two-way interactions. Second, the\nmutual information technique is applied to mutually constrain learning between\ntwo tasks in the output layer, thus the aspect input and the sentiment input\nare capable of encoding features of the other task via backpropagation.\nExtensive experiments on three real-world datasets demonstrate HI-ASA's\nsuperiority over baselines.", "published": "2022-08-24 03:03:49", "link": "http://arxiv.org/abs/2208.11283v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Induced Natural Language Rationales and Interleaved Markup Tokens Enable\n  Extrapolation in Large Language Models", "abstract": "The ability to extrapolate, i.e., to make predictions on sequences that are\nlonger than those presented as training examples, is a challenging problem for\ncurrent deep learning models. Recent work shows that this limitation persists\nin state-of-the-art Transformer-based models. Most solutions to this problem\nuse specific architectures or training methods that do not generalize to other\ntasks. We demonstrate that large language models can succeed in extrapolation\nwithout modifying their architecture or training procedure. Our experimental\nresults show that generating step-by-step rationales and introducing marker\ntokens are both required for effective extrapolation. First, we induce a\nlanguage model to produce step-by-step rationales before outputting the answer\nto effectively communicate the task to the model. However, as sequences become\nlonger, we find that current models struggle to keep track of token positions.\nTo address this issue, we interleave output tokens with markup tokens that act\nas explicit positional and counting symbols. Our findings show how these two\ncomplementary approaches enable remarkable sequence extrapolation and highlight\na limitation of current architectures to effectively generalize without\nexplicit surface form guidance. Code available at\nhttps://github.com/MirelleB/induced-rationales-markup-tokens", "published": "2022-08-24 11:25:27", "link": "http://arxiv.org/abs/2208.11445v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FactMix: Using a Few Labeled In-domain Examples to Generalize to\n  Cross-domain Named Entity Recognition", "abstract": "Few-shot Named Entity Recognition (NER) is imperative for entity tagging in\nlimited resource domains and thus received proper attention in recent years.\nExisting approaches for few-shot NER are evaluated mainly under in-domain\nsettings. In contrast, little is known about how these inherently faithful\nmodels perform in cross-domain NER using a few labeled in-domain examples. This\npaper proposes a two-step rationale-centric data augmentation method to improve\nthe model's generalization ability. Results on several datasets show that our\nmodel-agnostic method significantly improves the performance of cross-domain\nNER tasks compared to previous state-of-the-art methods, including the data\naugmentation and prompt-tuning methods. Our codes are available at\nhttps://github.com/lifan-yuan/FactMix.", "published": "2022-08-24 12:12:38", "link": "http://arxiv.org/abs/2208.11464v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PSSAT: A Perturbed Semantic Structure Awareness Transferring Method for\n  Perturbation-Robust Slot Filling", "abstract": "Most existing slot filling models tend to memorize inherent patterns of\nentities and corresponding contexts from training data. However, these models\ncan lead to system failure or undesirable outputs when being exposed to spoken\nlanguage perturbation or variation in practice. We propose a perturbed semantic\nstructure awareness transferring method for training perturbation-robust slot\nfilling models. Specifically, we introduce two MLM-based training strategies to\nrespectively learn contextual semantic structure and word distribution from\nunsupervised language perturbation corpus. Then, we transfer semantic knowledge\nlearned from upstream training procedure into the original samples and filter\ngenerated data by consistency processing. These procedures aim to enhance the\nrobustness of slot filling models. Experimental results show that our method\nconsistently outperforms the previous basic methods and gains strong\ngeneralization while preventing the model from memorizing inherent patterns of\nentities and contexts.", "published": "2022-08-24 13:01:00", "link": "http://arxiv.org/abs/2208.11508v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Of Human Criteria and Automatic Metrics: A Benchmark of the Evaluation\n  of Story Generation", "abstract": "Research on Automatic Story Generation (ASG) relies heavily on human and\nautomatic evaluation. However, there is no consensus on which human evaluation\ncriteria to use, and no analysis of how well automatic criteria correlate with\nthem. In this paper, we propose to re-evaluate ASG evaluation. We introduce a\nset of 6 orthogonal and comprehensive human criteria, carefully motivated by\nthe social sciences literature. We also present HANNA, an annotated dataset of\n1,056 stories produced by 10 different ASG systems. HANNA allows us to\nquantitatively evaluate the correlations of 72 automatic metrics with human\ncriteria. Our analysis highlights the weaknesses of current metrics for ASG and\nallows us to formulate practical recommendations for ASG evaluation.", "published": "2022-08-24 16:35:32", "link": "http://arxiv.org/abs/2208.11646v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PEER: A Collaborative Language Model", "abstract": "Textual content is often the output of a collaborative writing process: We\nstart with an initial draft, ask for suggestions, and repeatedly make changes.\nAgnostic of this process, today's language models are trained to generate only\nthe final result. As a consequence, they lack several abilities crucial for\ncollaborative writing: They are unable to update existing texts, difficult to\ncontrol and incapable of verbally planning or explaining their actions. To\naddress these shortcomings, we introduce PEER, a collaborative language model\nthat is trained to imitate the entire writing process itself: PEER can write\ndrafts, add suggestions, propose edits and provide explanations for its\nactions. Crucially, we train multiple instances of PEER able to infill various\nparts of the writing process, enabling the use of self-training techniques for\nincreasing the quality, amount and diversity of training data. This unlocks\nPEER's full potential by making it applicable in domains for which no edit\nhistories are available and improving its ability to follow instructions, to\nwrite useful comments, and to explain its actions. We show that PEER achieves\nstrong performance across various domains and editing tasks.", "published": "2022-08-24 16:56:47", "link": "http://arxiv.org/abs/2208.11663v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Addressing Token Uniformity in Transformers via Singular Value\n  Transformation", "abstract": "Token uniformity is commonly observed in transformer-based models, in which\ndifferent tokens share a large proportion of similar information after going\nthrough stacked multiple self-attention layers in a transformer. In this paper,\nwe propose to use the distribution of singular values of outputs of each\ntransformer layer to characterise the phenomenon of token uniformity and\nempirically illustrate that a less skewed singular value distribution can\nalleviate the `token uniformity' problem. Base on our observations, we define\nseveral desirable properties of singular value distributions and propose a\nnovel transformation function for updating the singular values. We show that\napart from alleviating token uniformity, the transformation function should\npreserve the local neighbourhood structure in the original embedding space. Our\nproposed singular value transformation function is applied to a range of\ntransformer-based language models such as BERT, ALBERT, RoBERTa and DistilBERT,\nand improved performance is observed in semantic textual similarity evaluation\nand a range of GLUE tasks. Our source code is available at\nhttps://github.com/hanqi-qi/tokenUni.git.", "published": "2022-08-24 22:44:09", "link": "http://arxiv.org/abs/2208.11790v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Paragraph-Level Vision-Language Semantic Alignment for\n  Multi-Modal Summarization", "abstract": "Most current multi-modal summarization methods follow a cascaded manner,\nwhere an off-the-shelf object detector is first used to extract visual\nfeatures, then these features are fused with language representations to\ngenerate the summary with an encoder-decoder model. The cascaded way cannot\ncapture the semantic alignments between images and paragraphs, which are\ncrucial to a precise summary. In this paper, we propose ViL-Sum to jointly\nmodel paragraph-level \\textbf{Vi}sion-\\textbf{L}anguage Semantic Alignment and\nMulti-Modal \\textbf{Sum}marization. The core of ViL-Sum is a joint multi-modal\nencoder with two well-designed tasks, image reordering and image selection. The\njoint multi-modal encoder captures the interactions between modalities, where\nthe reordering task guides the model to learn paragraph-level semantic\nalignment and the selection task guides the model to selected summary-related\nimages in the final summary. Experimental results show that our proposed\nViL-Sum significantly outperforms current state-of-the-art methods. In further\nanalysis, we find that two well-designed tasks and joint multi-modal encoder\ncan effectively guide the model to learn reasonable paragraphs-images and\nsummary-images relations.", "published": "2022-08-24 05:18:23", "link": "http://arxiv.org/abs/2208.11303v3", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Next-Year Bankruptcy Prediction from Textual Data: Benchmark and\n  Baselines", "abstract": "Models for bankruptcy prediction are useful in several real-world scenarios,\nand multiple research contributions have been devoted to the task, based on\nstructured (numerical) as well as unstructured (textual) data. However, the\nlack of a common benchmark dataset and evaluation strategy impedes the\nobjective comparison between models. This paper introduces such a benchmark for\nthe unstructured data scenario, based on novel and established datasets, in\norder to stimulate further research into the task. We describe and evaluate\nseveral classical and neural baseline models, and discuss benefits and flaws of\ndifferent strategies. In particular, we find that a lightweight bag-of-words\nmodel based on static in-domain word representations obtains surprisingly good\nresults, especially when taking textual data from several years into account.\nThese results are critically assessed, and discussed in light of particular\naspects of the data and the task. All code to replicate the data and\nexperimental results will be released.", "published": "2022-08-24 07:11:49", "link": "http://arxiv.org/abs/2208.11334v1", "categories": ["cs.CL", "q-fin.CP"], "primary_category": "cs.CL"}
{"title": "Adverse Childhood Experiences Identification from Clinical Notes with\n  Ontologies and NLP", "abstract": "Adverse Childhood Experiences (ACEs) are defined as a collection of highly\nstressful, and potentially traumatic, events or circumstances that occur\nthroughout childhood and/or adolescence. They have been shown to be associated\nwith increased risks of mental health diseases or other abnormal behaviours in\nlater lives. However, the identification of ACEs from free-text Electronic\nHealth Records (EHRs) with Natural Language Processing (NLP) is challenging\nbecause (a) there is no NLP ready ACE ontologies; (b) there are limited cases\navailable for machine learning, necessitating the data annotation from clinical\nexperts. We are currently developing a tool that would use NLP techniques to\nassist us in surfacing ACEs from clinical notes. This will enable us further\nresearch in identifying evidence of the relationship between ACEs and the\nsubsequent developments of mental illness (e.g., addictions) in large-scale and\nlongitudinal free-text EHRs, which has previously not been possible.", "published": "2022-08-24 12:17:32", "link": "http://arxiv.org/abs/2208.11466v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DPTDR: Deep Prompt Tuning for Dense Passage Retrieval", "abstract": "Deep prompt tuning (DPT) has gained great success in most natural language\nprocessing~(NLP) tasks. However, it is not well-investigated in dense retrieval\nwhere fine-tuning~(FT) still dominates. When deploying multiple retrieval tasks\nusing the same backbone model~(e.g., RoBERTa), FT-based methods are unfriendly\nin terms of deployment cost: each new retrieval model needs to repeatedly\ndeploy the backbone model without reuse. To reduce the deployment cost in such\na scenario, this work investigates applying DPT in dense retrieval. The\nchallenge is that directly applying DPT in dense retrieval largely\nunderperforms FT methods. To compensate for the performance drop, we propose\ntwo model-agnostic and task-agnostic strategies for DPT-based retrievers,\nnamely retrieval-oriented intermediate pretraining and unified negative mining,\nas a general approach that could be compatible with any pre-trained language\nmodel and retrieval task. The experimental results show that the proposed\nmethod (called DPTDR) outperforms previous state-of-the-art models on both\nMS-MARCO and Natural Questions. We also conduct ablation studies to examine the\neffectiveness of each strategy in DPTDR. We believe this work facilitates the\nindustry, as it saves enormous efforts and costs of deployment and increases\nthe utility of computing resources. Our code is available at\nhttps://github.com/tangzhy/DPTDR.", "published": "2022-08-24 12:55:00", "link": "http://arxiv.org/abs/2208.11503v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Diverse Title Generation for Stack Overflow Posts with Multiple Sampling\n  Enhanced Transformer", "abstract": "Stack Overflow is one of the most popular programming communities where\ndevelopers can seek help for their encountered problems. Nevertheless, if\ninexperienced developers fail to describe their problems clearly, it is hard\nfor them to attract sufficient attention and get the anticipated answers. We\npropose M$_3$NSCT5, a novel approach to automatically generate multiple post\ntitles from the given code snippets. Developers may use the generated titles to\nfind closely related posts and complete their problem descriptions. M$_3$NSCT5\nemploys the CodeT5 backbone, which is a pre-trained Transformer model having an\nexcellent language understanding and generation ability. To alleviate the\nambiguity issue that the same code snippets could be aligned with different\ntitles under varying contexts, we propose the maximal marginal multiple nucleus\nsampling strategy to generate multiple high-quality and diverse title\ncandidates at a time for the developers to choose from. We build a large-scale\ndataset with 890,000 question posts covering eight programming languages to\nvalidate the effectiveness of M$_3$NSCT5. The automatic evaluation results on\nthe BLEU and ROUGE metrics demonstrate the superiority of M$_3$NSCT5 over six\nstate-of-the-art baseline models. Moreover, a human evaluation with trustworthy\nresults also demonstrates the great potential of our approach for real-world\napplication.", "published": "2022-08-24 13:10:48", "link": "http://arxiv.org/abs/2208.11523v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Ontology-Driven Self-Supervision for Adverse Childhood Experiences\n  Identification Using Social Media Datasets", "abstract": "Adverse Childhood Experiences (ACEs) are defined as a collection of highly\nstressful, and potentially traumatic, events or circumstances that occur\nthroughout childhood and/or adolescence. They have been shown to be associated\nwith increased risks of mental health diseases or other abnormal behaviours in\nlater lives. However, the identification of ACEs from textual data with Natural\nLanguage Processing (NLP) is challenging because (a) there are no NLP ready ACE\nontologies; (b) there are few resources available for machine learning,\nnecessitating the data annotation from clinical experts; (c) costly annotations\nby domain experts and large number of documents for supporting large machine\nlearning models. In this paper, we present an ontology-driven self-supervised\napproach (derive concept embeddings using an auto-encoder from baseline NLP\nresults) for producing a publicly available resource that would support\nlarge-scale machine learning (e.g., training transformer based large language\nmodels) on social media corpus. This resource as well as the proposed approach\nare aimed to facilitate the community in training transferable NLP models for\neffectively surfacing ACEs in low-resource scenarios like NLP on clinical notes\nwithin Electronic Health Records. The resource including a list of ACE ontology\nterms, ACE concept embeddings and the NLP annotated corpus is available at\nhttps://github.com/knowlab/ACE-NLP.", "published": "2022-08-24 12:23:01", "link": "http://arxiv.org/abs/2208.11701v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FashionVQA: A Domain-Specific Visual Question Answering System", "abstract": "Humans apprehend the world through various sensory modalities, yet language\nis their predominant communication channel. Machine learning systems need to\ndraw on the same multimodal richness to have informed discourses with humans in\nnatural language; this is particularly true for systems specialized in\nvisually-dense information, such as dialogue, recommendation, and search\nengines for clothing. To this end, we train a visual question answering (VQA)\nsystem to answer complex natural language questions about apparel in fashion\nphotoshoot images. The key to the successful training of our VQA model is the\nautomatic creation of a visual question-answering dataset with 168 million\nsamples from item attributes of 207 thousand images using diverse templates.\nThe sample generation employs a strategy that considers the difficulty of the\nquestion-answer pairs to emphasize challenging concepts. Contrary to the recent\ntrends in using several datasets for pretraining the visual question answering\nmodels, we focused on keeping the dataset fixed while training various models\nfrom scratch to isolate the improvements from model architecture changes. We\nsee that using the same transformer for encoding the question and decoding the\nanswer, as in language models, achieves maximum accuracy, showing that visual\nlanguage models (VLMs) make the best visual question answering systems for our\ndataset. The accuracy of the best model surpasses the human expert level, even\nwhen answering human-generated questions that are not confined to the template\nformats. Our approach for generating a large-scale multimodal domain-specific\ndataset provides a path for training specialized models capable of\ncommunicating in natural language. The training of such domain-expert models,\ne.g., our fashion VLM model, cannot rely solely on the large-scale\ngeneral-purpose datasets collected from the web.", "published": "2022-08-24 01:18:13", "link": "http://arxiv.org/abs/2208.11253v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Visual Subtitle Feature Enhanced Video Outline Generation", "abstract": "With the tremendously increasing number of videos, there is a great demand\nfor techniques that help people quickly navigate to the video segments they are\ninterested in. However, current works on video understanding mainly focus on\nvideo content summarization, while little effort has been made to explore the\nstructure of a video. Inspired by textual outline generation, we introduce a\nnovel video understanding task, namely video outline generation (VOG). This\ntask is defined to contain two sub-tasks: (1) first segmenting the video\naccording to the content structure and then (2) generating a heading for each\nsegment. To learn and evaluate VOG, we annotate a 10k+ dataset, called DuVOG.\nSpecifically, we use OCR tools to recognize subtitles of videos. Then\nannotators are asked to divide subtitles into chapters and title each chapter.\nIn videos, highlighted text tends to be the headline since it is more likely to\nattract attention. Therefore we propose a Visual Subtitle feature Enhanced\nvideo outline generation model (VSENet) which takes as input the textual\nsubtitles together with their visual font sizes and positions. We consider the\nVOG task as a sequence tagging problem that extracts spans where the headings\nare located and then rewrites them to form the final outlines. Furthermore,\nbased on the similarity between video outlines and textual outlines, we use a\nlarge number of articles with chapter headings to pretrain our model.\nExperiments on DuVOG show that our model largely outperforms other baseline\nmethods, achieving 77.1 of F1-score for the video segmentation level and 85.0\nof ROUGE-L_F0.5 for the headline generation level.", "published": "2022-08-24 05:26:26", "link": "http://arxiv.org/abs/2208.11307v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Interpreting Song Lyrics with an Audio-Informed Pre-trained Language\n  Model", "abstract": "Lyric interpretations can help people understand songs and their lyrics\nquickly, and can also make it easier to manage, retrieve and discover songs\nefficiently from the growing mass of music archives. In this paper we propose\nBART-fusion, a novel model for generating lyric interpretations from lyrics and\nmusic audio that combines a large-scale pre-trained language model with an\naudio encoder. We employ a cross-modal attention module to incorporate the\naudio representation into the lyrics representation to help the pre-trained\nlanguage model understand the song from an audio perspective, while preserving\nthe language model's original generative performance. We also release the Song\nInterpretation Dataset, a new large-scale dataset for training and evaluating\nour model. Experimental results show that the additional audio information\nhelps our model to understand words and music better, and to generate precise\nand fluent interpretations. An additional experiment on cross-modal music\nretrieval shows that interpretations generated by BART-fusion can also help\npeople retrieve music more accurately than with the original BART.", "published": "2022-08-24 17:07:37", "link": "http://arxiv.org/abs/2208.11671v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "IndicSUPERB: A Speech Processing Universal Performance Benchmark for\n  Indian languages", "abstract": "A cornerstone in AI research has been the creation and adoption of\nstandardized training and test datasets to earmark the progress of\nstate-of-the-art models. A particularly successful example is the GLUE dataset\nfor training and evaluating Natural Language Understanding (NLU) models for\nEnglish. The large body of research around self-supervised BERT-based language\nmodels revolved around performance improvements on NLU tasks in GLUE. To\nevaluate language models in other languages, several language-specific GLUE\ndatasets were created. The area of speech language understanding (SLU) has\nfollowed a similar trajectory. The success of large self-supervised models such\nas wav2vec2 enable creation of speech models with relatively easy to access\nunlabelled data. These models can then be evaluated on SLU tasks, such as the\nSUPERB benchmark. In this work, we extend this to Indic languages by releasing\nthe IndicSUPERB benchmark. Specifically, we make the following three\ncontributions. (i) We collect Kathbath containing 1,684 hours of labelled\nspeech data across 12 Indian languages from 1,218 contributors located in 203\ndistricts in India. (ii) Using Kathbath, we create benchmarks across 6 speech\ntasks: Automatic Speech Recognition, Speaker Verification, Speaker\nIdentification (mono/multi), Language Identification, Query By Example, and\nKeyword Spotting for 12 languages. (iii) On the released benchmarks, we train\nand evaluate different self-supervised models alongside a commonly used\nbaseline FBANK. We show that language-specific fine-tuned models are more\naccurate than baseline on most of the tasks, including a large gap of 76\\% for\nthe Language Identification task. However, for speaker identification,\nself-supervised models trained on large datasets demonstrate an advantage. We\nhope IndicSUPERB contributes to the progress of developing speech language\nunderstanding models for Indian languages.", "published": "2022-08-24 20:14:52", "link": "http://arxiv.org/abs/2208.11761v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A convolutional plane wave model for sound field reconstruction", "abstract": "Spatial sound field interpolation relies on suitable models to both conform\nto available measurements and predict the sound field in the domain of\ninterest. A suitable model can be difficult to determine when the spatial\ndomain of interest is large compared to the wavelength or when spherical and\nplanar wavefronts are present or the sound field is complex, as in the\nnear-field. To span such complex sound fields, the global reconstruction task\ncan be partitioned into local subdomain problems. Previous studies have shown\nthat partitioning approaches rely on sufficient measurements within each\ndomain, due to the higher number of model coefficients. This study proposes a\njoint analysis of all local subdomains, while enforcing self-similarity between\nneighbouring partitions. More specifically, the coefficients of local plane\nwave representations are sought to have spatially smooth magnitudes. A\nconvolutional model of the sound field in terms of plane wave filters is\nformulated and the inverse reconstruction problem is solved via the alternating\ndirection method of multipliers. The experiments on simulated and measured\nsound fields suggest, that the proposed method both retains the flexibility of\nlocal models to conform to complex sound fields and also preserves the global\nstructure to reconstruct from fewer measurements.", "published": "2022-08-24 06:40:57", "link": "http://arxiv.org/abs/2208.11324v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Deep model with built-in cross-attention alignment for acoustic echo\n  cancellation", "abstract": "With recent research advances, deep learning models have become an attractive\nchoice for acoustic echo cancellation (AEC) in real-time teleconferencing\napplications. Since acoustic echo is one of the major sources of poor audio\nquality, a wide variety of deep models have been proposed. However, an\nimportant but often omitted requirement for good echo cancellation quality is\nthe synchronization of the microphone and far end signals. Typically\nimplemented using classical algorithms based on cross-correlation, the\nalignment module is a separate functional block with known design limitations.\nIn our work we propose a deep learning architecture with built-in\nself-attention based alignment, which is able to handle unaligned inputs,\nimproving echo cancellation performance while simplifying the communication\npipeline. Moreover, we show that our approach achieves significant improvements\nfor difficult delay estimation cases on real recordings from AEC Challenge data\nset.", "published": "2022-08-24 05:29:47", "link": "http://arxiv.org/abs/2208.11308v2", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improved Zero-Shot Audio Tagging & Classification with Patchout\n  Spectrogram Transformers", "abstract": "Standard machine learning models for tagging and classifying acoustic signals\ncannot handle classes that were not seen during training. Zero-Shot (ZS)\nlearning overcomes this restriction by predicting classes based on adaptable\nclass descriptions. This study sets out to investigate the effectiveness of\nself-attention-based audio embedding architectures for ZS learning. To this\nend, we compare the very recent patchout spectrogram transformer with two\nclassic convolutional architectures. We evaluate these three architectures on\nthree tasks and on three different benchmark datasets: general-purpose tagging\non AudioSet, environmental sound classification on ESC-50, and instrument\ntagging on OpenMIC. Our results show that the self-attention-based embedding\nmethods outperform both compared convolutional architectures in all of these\nsettings. By designing training and test data accordingly, we observe that\nprediction performance suffers significantly when the `semantic distance'\nbetween training and new test classes is large, an effect that will deserve\nmore detailed investigations.", "published": "2022-08-24 09:48:22", "link": "http://arxiv.org/abs/2208.11402v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "Automatic music mixing with deep learning and out-of-domain data", "abstract": "Music mixing traditionally involves recording instruments in the form of\nclean, individual tracks and blending them into a final mixture using audio\neffects and expert knowledge (e.g., a mixing engineer). The automation of music\nproduction tasks has become an emerging field in recent years, where rule-based\nmethods and machine learning approaches have been explored. Nevertheless, the\nlack of dry or clean instrument recordings limits the performance of such\nmodels, which is still far from professional human-made mixes. We explore\nwhether we can use out-of-domain data such as wet or processed multitrack music\nrecordings and repurpose it to train supervised deep learning models that can\nbridge the current gap in automatic mixing quality. To achieve this we propose\na novel data preprocessing method that allows the models to perform automatic\nmusic mixing. We also redesigned a listening test method for evaluating music\nmixing systems. We validate our results through such subjective tests using\nhighly experienced mixing engineers as participants.", "published": "2022-08-24 10:50:22", "link": "http://arxiv.org/abs/2208.11428v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Improving Natural-Language-based Audio Retrieval with Transfer Learning\n  and Audio & Text Augmentations", "abstract": "The absence of large labeled datasets remains a significant challenge in many\napplication areas of deep learning. Researchers and practitioners typically\nresort to transfer learning and data augmentation to alleviate this issue. We\nstudy these strategies in the context of audio retrieval with natural language\nqueries (Task 6b of the DCASE 2022 Challenge). Our proposed system uses\npre-trained embedding models to project recordings and textual descriptions\ninto a shared audio-caption space in which related examples from different\nmodalities are close. We employ various data augmentation techniques on audio\nand text inputs and systematically tune their corresponding hyperparameters\nwith sequential model-based optimization. Our results show that the used\naugmentations strategies reduce overfitting and improve retrieval performance.", "published": "2022-08-24 11:54:42", "link": "http://arxiv.org/abs/2208.11460v3", "categories": ["cs.SD", "cs.LG", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
