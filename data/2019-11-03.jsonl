{"title": "Posing Fair Generalization Tasks for Natural Language Inference", "abstract": "Deep learning models for semantics are generally evaluated using naturalistic\ncorpora. Adversarial methods, in which models are evaluated on new examples\nwith known semantic properties, have begun to reveal that good performance at\nthese naturalistic tasks can hide serious shortcomings. However, we should\ninsist that these evaluations be fair -that the models are given data\nsufficient to support the requisite kinds of generalization. In this paper, we\ndefine and motivate a formal notion of fairness in this sense. We then apply\nthese ideas to natural language inference by constructing very challenging but\nprovably fair artificial datasets and showing that standard neural models fail\nto generalize in the required ways; only task-specific models that jointly\ncompose the premise and hypothesis are able to achieve high performance, and\neven these models do not solve the task perfectly.", "published": "2019-11-03 02:47:51", "link": "http://arxiv.org/abs/1911.00811v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Controlling Text Complexity in Neural Machine Translation", "abstract": "This work introduces a machine translation task where the output is aimed at\naudiences of different levels of target language proficiency. We collect a high\nquality dataset of news articles available in English and Spanish, written for\ndiverse grade levels and propose a method to align segments across comparable\nbilingual articles. The resulting dataset makes it possible to train multi-task\nsequence-to-sequence models that translate Spanish into English targeted at an\neasier reading grade level than the original Spanish. We show that these\nmulti-task models outperform pipeline approaches that translate and simplify\ntext independently.", "published": "2019-11-03 05:41:53", "link": "http://arxiv.org/abs/1911.00835v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Question Answering for Privacy Policies: Combining Computational and\n  Legal Perspectives", "abstract": "Privacy policies are long and complex documents that are difficult for users\nto read and understand, and yet, they have legal effects on how user data is\ncollected, managed and used. Ideally, we would like to empower users to inform\nthemselves about issues that matter to them, and enable them to selectively\nexplore those issues. We present PrivacyQA, a corpus consisting of 1750\nquestions about the privacy policies of mobile applications, and over 3500\nexpert annotations of relevant answers. We observe that a strong neural\nbaseline underperforms human performance by almost 0.3 F1 on PrivacyQA,\nsuggesting considerable room for improvement for future systems. Further, we\nuse this dataset to shed light on challenges to question answerability, with\ndomain-general implications for any question answering system. The PrivacyQA\ncorpus offers a challenging corpus for question answering, with genuine\nreal-world utility.", "published": "2019-11-03 06:25:17", "link": "http://arxiv.org/abs/1911.00841v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Machine Translation in Pronunciation Space", "abstract": "The research in machine translation community focus on translation in text\nspace. However, humans are in fact also good at direct translation in\npronunciation space. Some existing translation systems, such as simultaneous\nmachine translation, are inherently more natural and thus potentially more\nrobust by directly translating in pronunciation space. In this paper, we\nconduct large scale experiments on a self-built dataset with about $20$M En-Zh\npairs of text sentences and corresponding pronunciation sentences. We proposed\nthree new categories of translations: $1)$ translating a pronunciation sentence\nin source language into a pronunciation sentence in target language (P2P-Tran),\n$2)$ translating a text sentence in source language into a pronunciation\nsentence in target language (T2P-Tran), and $3)$ translating a pronunciation\nsentence in source language into a text sentence in target language (P2T-Tran),\nand compare them with traditional text translation (T2T-Tran). Our experiments\nclearly show that all $4$ categories of translations have comparable\nperformances, with small and sometimes ignorable differences.", "published": "2019-11-03 17:24:48", "link": "http://arxiv.org/abs/1911.00932v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Low-dimensional Semantic Space: from Text to Word Embedding", "abstract": "This article focuses on the study of Word Embedding, a feature-learning\ntechnique in Natural Language Processing that maps words or phrases to\nlow-dimensional vectors. Beginning with the linguistic theories concerning\ncontextual similarities - \"Distributional Hypothesis\" and \"Context of\nSituation\", this article introduces two ways of numerical representation of\ntext: One-hot and Distributed Representation. In addition, this article\npresents statistical-based Language Models(such as Co-occurrence Matrix and\nSingular Value Decomposition) as well as Neural Network Language Models (NNLM,\nsuch as Continuous Bag-of-Words and Skip-Gram). This article also analyzes how\nWord Embedding can be applied to the study of word-sense disambiguation and\ndiachronic linguistics.", "published": "2019-11-03 07:11:58", "link": "http://arxiv.org/abs/1911.00845v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MRNN: A Multi-Resolution Neural Network with Duplex Attention for\n  Document Retrieval in the Context of Question Answering", "abstract": "The primary goal of ad-hoc retrieval (document retrieval in the context of\nquestion answering) is to find relevant documents satisfied the information\nneed posted in a natural language query. It requires a good understanding of\nthe query and all the documents in a corpus, which is difficult because the\nmeaning of natural language texts depends on the context, syntax,and semantics.\nRecently deep neural networks have been used to rank search results in response\nto a query. In this paper, we devise a multi-resolution neural network(MRNN) to\nleverage the whole hierarchy of representations for document retrieval. The\nproposed MRNN model is capable of deriving a representation that integrates\nrepresentations of different levels of abstraction from all the layers of the\nlearned hierarchical representation.Moreover, a duplex attention component is\ndesigned to refinethe multi-resolution representation so that an optimal\ncontextfor matching the query and document can be determined. More\nspecifically, the first attention mechanism determines optimal context from the\nlearned multi-resolution representation for the query and document. The latter\nattention mechanism aims to fine-tune the representation so that the query and\nthe relevant document are closer in proximity. The empirical study shows that\nMRNN with the duplex attention is significantly superior to existing models\nused for ad-hoc retrieval on benchmark datasets including SQuAD, WikiQA,\nQUASAR, and TrecQA.", "published": "2019-11-03 20:38:38", "link": "http://arxiv.org/abs/1911.00964v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "BERT-CNN: a Hierarchical Patent Classifier Based on a Pre-Trained\n  Language Model", "abstract": "The automatic classification is a process of automatically assigning text\ndocuments to predefined categories. An accurate automatic patent classifier is\ncrucial to patent inventors and patent examiners in terms of intellectual\nproperty protection, patent management, and patent information retrieval. We\npresent BERT-CNN, a hierarchical patent classifier based on pre-trained\nlanguage model by training the national patent application documents collected\nfrom the State Information Center, China. The experimental results show that\nBERT-CNN achieves 84.3% accuracy, which is far better than the two compared\nbaseline methods, Convolutional Neural Networks and Recurrent Neural Networks.\nWe didn't apply our model to the third and fourth hierarchical level of the\nInternational Patent Classification - \"subclass\" and \"group\".The visualization\nof the Attention Mechanism shows that BERT-CNN obtains new state-of-the-art\nresults in representing vocabularies and semantics. This article demonstrates\nthe practicality and effectiveness of BERT-CNN in the field of automatic patent\nclassification.", "published": "2019-11-03 07:21:41", "link": "http://arxiv.org/abs/1911.06241v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Scene Graph based Image Retrieval -- A case study on the CLEVR Dataset", "abstract": "With the prolification of multimodal interaction in various domains, recently\nthere has been much interest in text based image retrieval in the computer\nvision community. However most of the state of the art techniques model this\nproblem in a purely neural way, which makes it difficult to incorporate\npragmatic strategies in searching a large scale catalog especially when the\nsearch requirements are insufficient and the model needs to resort to an\ninteractive retrieval process through multiple iterations of\nquestion-answering. Motivated by this, we propose a neural-symbolic approach\nfor a one-shot retrieval of images from a large scale catalog, given the\ncaption description. To facilitate this, we represent the catalog and caption\nas scene-graphs and model the retrieval task as a learnable graph matching\nproblem, trained end-to-end with a REINFORCE algorithm. Further, we briefly\ndescribe an extension of this pipeline to an iterative retrieval framework,\nbased on interactive questioning and answering.", "published": "2019-11-03 08:00:38", "link": "http://arxiv.org/abs/1911.00850v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "Interpreting Verbal Irony: Linguistic Strategies and the Connection to\n  the Type of Semantic Incongruity", "abstract": "Human communication often involves the use of verbal irony or sarcasm, where\nthe speakers usually mean the opposite of what they say. To better understand\nhow verbal irony is expressed by the speaker and interpreted by the hearer we\nconduct a crowdsourcing task: given an utterance expressing verbal irony, users\nare asked to verbalize their interpretation of the speaker's ironic message. We\npropose a typology of linguistic strategies for verbal irony interpretation and\nlink it to various theoretical linguistic frameworks. We design computational\nmodels to capture these strategies and present empirical studies aimed to\nanswer three questions: (1) what is the distribution of linguistic strategies\nused by hearers to interpret ironic messages?; (2) do hearers adopt similar\nstrategies for interpreting the speaker's ironic intent?; and (3) does the type\nof semantic incongruity in the ironic message (explicit vs. implicit) influence\nthe choice of interpretation strategies by the hearers?", "published": "2019-11-03 14:05:55", "link": "http://arxiv.org/abs/1911.00891v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Attributed Sequence Embedding", "abstract": "Mining tasks over sequential data, such as clickstreams and gene sequences,\nrequire a careful design of embeddings usable by learning algorithms. Recent\nresearch in feature learning has been extended to sequential data, where each\ninstance consists of a sequence of heterogeneous items with a variable length.\nHowever, many real-world applications often involve attributed sequences, where\neach instance is composed of both a sequence of categorical items and a set of\nattributes. In this paper, we study this new problem of attributed sequence\nembedding, where the goal is to learn the representations of attributed\nsequences in an unsupervised fashion. This problem is core to many important\ndata mining tasks ranging from user behavior analysis to the clustering of gene\nsequences. This problem is challenging due to the dependencies between\nsequences and their associated attributes. We propose a deep multimodal\nlearning framework, called NAS, to produce embeddings of attributed sequences.\nThe embeddings are task independent and can be used on various mining tasks of\nattributed sequences. We demonstrate the effectiveness of our embeddings of\nattributed sequences in various unsupervised learning tasks on real-world\ndatasets.", "published": "2019-11-03 19:16:51", "link": "http://arxiv.org/abs/1911.00949v1", "categories": ["cs.LG", "cs.CL", "cs.DB", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Sentiment analysis model for Twitter data in Polish language", "abstract": "Text mining analysis of tweets gathered during Polish presidential election\non May 10th, 2015. The project included implementation of engine to retrieve\ninformation from Twitter, building document corpora, corpora cleaning, and\ncreating Term-Document Matrix. Each tweet from the text corpora was assigned a\ncategory based on its sentiment score. The score was calculated using the\nnumber of positive and/or negative emoticons and Polish words in each document.\nThe result data set was used to train and test four machine learning\nclassifiers, to select these providing most accurate automatic tweet\nclassification results. The Naive Bayes and Maximum Entropy algorithms achieved\nthe best accuracy of respectively 71.76% and 77.32%. All implementation tasks\nwere completed using R programming language.", "published": "2019-11-03 22:06:03", "link": "http://arxiv.org/abs/1911.00985v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Onssen: an open-source speech separation and enhancement library", "abstract": "Speech separation is an essential task for multi-talker speech recognition.\nRecently many deep learning approaches are proposed and have been constantly\nrefreshing the state-of-the-art performances. The lack of algorithm\nimplementations limits researchers to use the same dataset for comparison.\nBuilding a generic platform can benefit researchers by easily implementing\nnovel separation algorithms and comparing them with the existing ones on\ncustomized datasets. We introduce \"onssen\": an open-source speech separation\nand enhancement library. onssen is a library mainly for deep learning\nseparation and enhancement algorithms. It uses LibRosa and NumPy libraries for\nthe feature extraction and PyTorch as the back-end for model training. onssen\nsupports most of the Time-Frequency mask-based separation algorithms (e.g. deep\nclustering, chimera net, chimera++, and so on) and also supports customized\ndatasets. In this paper, we describe the functionality of modules in onssen and\nshow the algorithms implemented by onssen achieve the same performances as\nreported in the original papers.", "published": "2019-11-03 22:00:07", "link": "http://arxiv.org/abs/1911.00982v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Robust speaker recognition using unsupervised adversarial invariance", "abstract": "In this paper, we address the problem of speaker recognition in challenging\nacoustic conditions using a novel method to extract robust\nspeaker-discriminative speech representations. We adopt a recently proposed\nunsupervised adversarial invariance architecture to train a network that maps\nspeaker embeddings extracted using a pre-trained model onto two lower\ndimensional embedding spaces. The embedding spaces are learnt to disentangle\nspeaker-discriminative information from all other information present in the\naudio recordings, without supervision about the acoustic conditions. We analyze\nthe robustness of the proposed embeddings to various sources of variability\npresent in the signal for speaker verification and unsupervised clustering\ntasks on a large-scale speaker recognition corpus. Our analyses show that the\nproposed system substantially outperforms the baseline in a variety of\nchallenging acoustic scenarios. Furthermore, for the task of speaker\ndiarization on a real-world meeting corpus, our system shows a relative\nimprovement of 36\\% in the diarization error rate compared to the\nstate-of-the-art baseline.", "published": "2019-11-03 18:14:06", "link": "http://arxiv.org/abs/1911.00940v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Who is Real Bob? Adversarial Attacks on Speaker Recognition Systems", "abstract": "Speaker recognition (SR) is widely used in our daily life as a biometric\nauthentication or identification mechanism. The popularity of SR brings in\nserious security concerns, as demonstrated by recent adversarial attacks.\nHowever, the impacts of such threats in the practical black-box setting are\nstill open, since current attacks consider the white-box setting only. In this\npaper, we conduct the first comprehensive and systematic study of the\nadversarial attacks on SR systems (SRSs) to understand their security weakness\nin the practical blackbox setting. For this purpose, we propose an adversarial\nattack, named FAKEBOB, to craft adversarial samples. Specifically, we formulate\nthe adversarial sample generation as an optimization problem, incorporated with\nthe confidence of adversarial samples and maximal distortion to balance between\nthe strength and imperceptibility of adversarial voices. One key contribution\nis to propose a novel algorithm to estimate the score threshold, a feature in\nSRSs, and use it in the optimization problem to solve the optimization problem.\nWe demonstrate that FAKEBOB achieves 99% targeted attack success rate on both\nopen-source and commercial systems. We further demonstrate that FAKEBOB is also\neffective on both open-source and commercial systems when playing over the air\nin the physical world. Moreover, we have conducted a human study which reveals\nthat it is hard for human to differentiate the speakers of the original and\nadversarial voices. Last but not least, we show that four promising defense\nmethods for adversarial attack from the speech recognition domain become\nineffective on SRSs against FAKEBOB, which calls for more effective defense\nmethods. We highlight that our study peeks into the security implications of\nadversarial attacks on SRSs, and realistically fosters to improve the security\nrobustness of SRSs.", "published": "2019-11-03 16:50:13", "link": "http://arxiv.org/abs/1911.01840v2", "categories": ["eess.AS", "cs.CR", "cs.LG", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
