{"title": "Towards a Unified Framework for Adaptable Problematic Content Detection\n  via Continual Learning", "abstract": "Detecting problematic content, such as hate speech, is a multifaceted and\never-changing task, influenced by social dynamics, user populations, diversity\nof sources, and evolving language. There has been significant efforts, both in\nacademia and in industry, to develop annotated resources that capture various\naspects of problematic content. Due to researchers' diverse objectives, the\nannotations are inconsistent and hence, reports of progress on detection of\nproblematic content are fragmented. This pattern is expected to persist unless\nwe consolidate resources considering the dynamic nature of the problem. We\npropose integrating the available resources, and leveraging their dynamic\nnature to break this pattern. In this paper, we introduce a continual learning\nbenchmark and framework for problematic content detection comprising over 84\nrelated tasks encompassing 15 annotation schemas from 8 sources. Our benchmark\ncreates a novel measure of progress: prioritizing the adaptability of\nclassifiers to evolving tasks over excelling in specific tasks. To ensure the\ncontinuous relevance of our framework, we designed it so that new tasks can\neasily be integrated into the benchmark. Our baseline results demonstrate the\npotential of continual learning in capturing the evolving content and adapting\nto novel manifestations of problematic content.", "published": "2023-09-29 00:14:38", "link": "http://arxiv.org/abs/2309.16905v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"I'd Like to Have an Argument, Please\": Argumentative Reasoning in Large\n  Language Models", "abstract": "We evaluate two large language models (LLMs) ability to perform argumentative\nreasoning. We experiment with argument mining (AM) and argument pair extraction\n(APE), and evaluate the LLMs' ability to recognize arguments under\nprogressively more abstract input and output (I/O) representations (e.g.,\narbitrary label sets, graphs, etc.). Unlike the well-known evaluation of prompt\nphrasings, abstraction evaluation retains the prompt's phrasing but tests\nreasoning capabilities. We find that scoring-wise the LLMs match or surpass the\nSOTA in AM and APE, and under certain I/O abstractions LLMs perform well, even\nbeating chain-of-thought--we call this symbolic prompting. However, statistical\nanalysis on the LLMs outputs when subject to small, yet still human-readable,\nalterations in the I/O representations (e.g., asking for BIO tags as opposed to\nline numbers) showed that the models are not performing reasoning. This\nsuggests that LLM applications to some tasks, such as data labelling and paper\nreviewing, must be done with care.", "published": "2023-09-29 02:41:38", "link": "http://arxiv.org/abs/2309.16938v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contextualising Levels of Language Resourcedness affecting Digital\n  Processing of Text", "abstract": "Application domains such as digital humanities and tool like chatbots involve\nsome form of processing natural language, from digitising hardcopies to speech\ngeneration. The language of the content is typically characterised as either a\nlow resource language (LRL) or high resource language (HRL), also known as\nresource-scarce and well-resourced languages, respectively. African languages\nhave been characterized as resource-scarce languages (Bosch et al. 2007;\nPretorius & Bosch 2003; Keet & Khumalo 2014) and English is by far the most\nwell-resourced language. Varied language resources are used to develop software\nsystems for these languages to accomplish a wide range of tasks. In this paper\nwe argue that the dichotomous typology LRL and HRL for all languages is\nproblematic. Through a clear understanding of language resources situated in a\nsociety, a matrix is developed that characterizes languages as Very LRL, LRL,\nRL, HRL and Very HRL. The characterization is based on the typology of\ncontextual features for each category, rather than counting tools, and\nmotivation is provided for each feature and each characterization. The\ncontextualisation of resourcedness, with a focus on African languages in this\npaper, and an increased understanding of where on the scale the language used\nin a project is, may assist in, among others, better planning of research and\nimplementation projects. We thus argue in this paper that the characterization\nof language resources within a given scale in a project is an indispensable\ncomponent particularly in the context of low-resourced languages.", "published": "2023-09-29 07:48:24", "link": "http://arxiv.org/abs/2309.17035v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Interpretable Long-Form Legal Question Answering with\n  Retrieval-Augmented Large Language Models", "abstract": "Many individuals are likely to face a legal dispute at some point in their\nlives, but their lack of understanding of how to navigate these complex issues\noften renders them vulnerable. The advancement of natural language processing\nopens new avenues for bridging this legal literacy gap through the development\nof automated legal aid systems. However, existing legal question answering\n(LQA) approaches often suffer from a narrow scope, being either confined to\nspecific legal domains or limited to brief, uninformative responses. In this\nwork, we propose an end-to-end methodology designed to generate long-form\nanswers to any statutory law questions, utilizing a \"retrieve-then-read\"\npipeline. To support this approach, we introduce and release the Long-form\nLegal Question Answering (LLeQA) dataset, comprising 1,868 expert-annotated\nlegal questions in the French language, complete with detailed answers rooted\nin pertinent legal provisions. Our experimental results demonstrate promising\nperformance on automatic evaluation metrics, but a qualitative analysis\nuncovers areas for refinement. As one of the only comprehensive,\nexpert-annotated long-form LQA dataset, LLeQA has the potential to not only\naccelerate research towards resolving a significant real-world issue, but also\nact as a rigorous benchmark for evaluating NLP models in specialized domains.\nWe publicly release our code, data, and models.", "published": "2023-09-29 08:23:19", "link": "http://arxiv.org/abs/2309.17050v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Promoting Generalized Cross-lingual Question Answering in Few-resource\n  Scenarios via Self-knowledge Distillation", "abstract": "Despite substantial progress in multilingual extractive Question Answering\n(QA), models with high and uniformly distributed performance across languages\nremain challenging, especially for languages with limited resources. We study\ncross-lingual transfer mainly focusing on the Generalized Cross-Lingual\nTransfer (G-XLT) task, where the question language differs from the context\nlanguage - a challenge that has received limited attention thus far. Our\napproach seeks to enhance cross-lingual QA transfer using a high-performing\nmultilingual model trained on a large-scale dataset, complemented by a few\nthousand aligned QA examples across languages. Our proposed strategy combines\ncross-lingual sampling and advanced self-distillation training in generations\nto tackle the previous challenge. Notably, we introduce the novel mAP@k\ncoefficients to fine-tune self-knowledge distillation loss, dynamically\nregulating the teacher's model knowledge to perform a balanced and effective\nknowledge transfer. We extensively evaluate our approach to assess XLT and\nG-XLT capabilities in extractive QA. Results reveal that our self-knowledge\ndistillation approach outperforms standard cross-entropy fine-tuning by a\nsignificant margin. Importantly, when compared to a strong baseline that\nleverages a sizeable volume of machine-translated data, our approach shows\ncompetitive results despite the considerable challenge of operating within\nresource-constrained settings, even in zero-shot scenarios. Beyond performance\nimprovements, we offer valuable insights through comprehensive analyses and an\nablation study, further substantiating the benefits and constraints of our\napproach. In essence, we propose a practical solution to improve cross-lingual\nQA transfer by leveraging a few data resources in an efficient way.", "published": "2023-09-29 10:54:59", "link": "http://arxiv.org/abs/2309.17134v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LatticeGen: A Cooperative Framework which Hides Generated Text in a\n  Lattice for Privacy-Aware Generation on Cloud", "abstract": "In the current user-server interaction paradigm of prompted generation with\nlarge language models (LLM) on cloud, the server fully controls the generation\nprocess, which leaves zero options for users who want to keep the generated\ntext to themselves. We propose LatticeGen, a cooperative framework in which the\nserver still handles most of the computation while the user controls the\nsampling operation. The key idea is that the true generated sequence is mixed\nwith noise tokens by the user and hidden in a noised lattice. Considering\npotential attacks from a hypothetically malicious server and how the user can\ndefend against it, we propose the repeated beam-search attack and the mixing\nnoise scheme. In our experiments we apply LatticeGen to protect both prompt and\ngeneration. It is shown that while the noised lattice degrades generation\nquality, LatticeGen successfully protects the true generation to a remarkable\ndegree under strong attacks (more than 50% of the semantic remains hidden as\nmeasured by BERTScore).", "published": "2023-09-29 11:46:07", "link": "http://arxiv.org/abs/2309.17157v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Few-Shot Domain Adaptation for Charge Prediction on Unprofessional\n  Descriptions", "abstract": "Recent works considering professional legal-linguistic style (PLLS) texts\nhave shown promising results on the charge prediction task. However,\nunprofessional users also show an increasing demand on such a prediction\nservice. There is a clear domain discrepancy between PLLS texts and non-PLLS\ntexts expressed by those laypersons, which degrades the current SOTA models'\nperformance on non-PLLS texts. A key challenge is the scarcity of non-PLLS data\nfor most charge classes. This paper proposes a novel few-shot domain adaptation\n(FSDA) method named Disentangled Legal Content for Charge Prediction (DLCCP).\nCompared with existing FSDA works, which solely perform instance-level\nalignment without considering the negative impact of text style information\nexisting in latent features, DLCCP (1) disentangles the content and style\nrepresentations for better domain-invariant legal content learning with\ncarefully designed optimization goals for content and style spaces and, (2)\nemploys the constitutive elements knowledge of charges to extract and align\nelement-level and instance-level content representations simultaneously. We\ncontribute the first publicly available non-PLLS dataset named NCCP for\ndeveloping layperson-friendly charge prediction models. Experiments on NCCP\nshow the superiority of our methods over competitive baselines.", "published": "2023-09-29 15:14:08", "link": "http://arxiv.org/abs/2309.17313v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Overview of the BioLaySumm 2023 Shared Task on Lay Summarization of\n  Biomedical Research Articles", "abstract": "This paper presents the results of the shared task on Lay Summarisation of\nBiomedical Research Articles (BioLaySumm), hosted at the BioNLP Workshop at ACL\n2023. The goal of this shared task is to develop abstractive summarisation\nmodels capable of generating \"lay summaries\" (i.e., summaries that are\ncomprehensible to non-technical audiences) in both a controllable and\nnon-controllable setting. There are two subtasks: 1) Lay Summarisation, where\nthe goal is for participants to build models for lay summary generation only,\ngiven the full article text and the corresponding abstract as input; and 2)\nReadability-controlled Summarisation, where the goal is for participants to\ntrain models to generate both the technical abstract and the lay summary, given\nan article's main text as input. In addition to overall results, we report on\nthe setup and insights from the BioLaySumm shared task, which attracted a total\nof 20 participating teams across both subtasks.", "published": "2023-09-29 15:43:42", "link": "http://arxiv.org/abs/2309.17332v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Intuitive or Dependent? Investigating LLMs' Behavior Style to\n  Conflicting Prompts", "abstract": "This study investigates the behaviors of Large Language Models (LLMs) when\nfaced with conflicting prompts versus their internal memory. This will not only\nhelp to understand LLMs' decision mechanism but also benefit real-world\napplications, such as retrieval-augmented generation (RAG). Drawing on\ncognitive theory, we target the first scenario of decision-making styles where\nthere is no superiority in the conflict and categorize LLMs' preference into\ndependent, intuitive, and rational/irrational styles. Another scenario of\nfactual robustness considers the correctness of prompt and memory in\nknowledge-intensive tasks, which can also distinguish if LLMs behave rationally\nor irrationally in the first scenario. To quantify them, we establish a\ncomplete benchmarking framework including a dataset, a robustness evaluation\npipeline, and corresponding metrics. Extensive experiments with seven LLMs\nreveal their varying behaviors. And, with role play intervention, we can change\nthe styles, but different models present distinct adaptivity and upper-bound.\nOne of our key takeaways is to optimize models or the prompts according to the\nidentified style. For instance, RAG models with high role play adaptability may\ndynamically adjust the interventions according to the quality of retrieval\nresults -- being dependent to better leverage informative context; and, being\nintuitive when external prompt is noisy.", "published": "2023-09-29 17:26:03", "link": "http://arxiv.org/abs/2309.17415v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Large Language Model Approach to Educational Survey Feedback Analysis", "abstract": "This paper assesses the potential for the large language models (LLMs) GPT-4\nand GPT-3.5 to aid in deriving insight from education feedback surveys.\nExploration of LLM use cases in education has focused on teaching and learning,\nwith less exploration of capabilities in education feedback analysis. Survey\nanalysis in education involves goals such as finding gaps in curricula or\nevaluating teachers, often requiring time-consuming manual processing of\ntextual responses. LLMs have the potential to provide a flexible means of\nachieving these goals without specialized machine learning models or\nfine-tuning. We demonstrate a versatile approach to such goals by treating them\nas sequences of natural language processing (NLP) tasks including\nclassification (multi-label, multi-class, and binary), extraction, thematic\nanalysis, and sentiment analysis, each performed by LLM. We apply these\nworkflows to a real-world dataset of 2500 end-of-course survey comments from\nbiomedical science courses, and evaluate a zero-shot approach (i.e., requiring\nno examples or labeled training data) across all tasks, reflecting education\nsettings, where labeled data is often scarce. By applying effective prompting\npractices, we achieve human-level performance on multiple tasks with GPT-4,\nenabling workflows necessary to achieve typical goals. We also show the\npotential of inspecting LLMs' chain-of-thought (CoT) reasoning for providing\ninsight that may foster confidence in practice. Moreover, this study features\ndevelopment of a versatile set of classification categories, suitable for\nvarious course types (online, hybrid, or in-person) and amenable to\ncustomization. Our results suggest that LLMs can be used to derive a range of\ninsights from survey text.", "published": "2023-09-29 17:57:23", "link": "http://arxiv.org/abs/2309.17447v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Rewrite Prompts for Personalized Text Generation", "abstract": "Facilitated by large language models (LLMs), personalized text generation has\nbecome a rapidly growing research direction. Most existing studies focus on\ndesigning specialized models for a particular domain, or they require\nfine-tuning the LLMs to generate personalized text. We consider a typical\nscenario in which the large language model, which generates personalized\noutput, is frozen and can only be accessed through APIs. Under this constraint,\nall one can do is to improve the input text (i.e., text prompts) sent to the\nLLM, a procedure that is usually done manually. In this paper, we propose a\nnovel method to automatically revise prompts for personalized text generation.\nThe proposed method takes the initial prompts generated by a state-of-the-art,\nmultistage framework for personalized generation and rewrites a few critical\ncomponents that summarize and synthesize the personal context. The prompt\nrewriter employs a training paradigm that chains together supervised learning\n(SL) and reinforcement learning (RL), where SL reduces the search space of RL\nand RL facilitates end-to-end training of the rewriter. Using datasets from\nthree representative domains, we demonstrate that the rewritten prompts\noutperform both the original prompts and the prompts optimized via supervised\nlearning or reinforcement learning alone. In-depth analysis of the rewritten\nprompts shows that they are not only human readable, but also able to guide\nmanual revision of prompts when there is limited resource to employ\nreinforcement learning to train the prompt rewriter, or when it is costly to\ndeploy an automatic prompt rewriter for inference.", "published": "2023-09-29 21:15:49", "link": "http://arxiv.org/abs/2310.00152v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SCALE: Synergized Collaboration of Asymmetric Language Translation\n  Engines", "abstract": "In this paper, we introduce SCALE, a collaborative framework that connects\ncompact Specialized Translation Models (STMs) and general-purpose Large\nLanguage Models (LLMs) as one unified translation engine. By introducing\ntranslation from STM into the triplet in-context demonstrations, SCALE unlocks\nrefinement and pivoting ability of LLM, thus mitigating language bias of LLM\nand parallel data bias of STM, enhancing LLM speciality without sacrificing\ngenerality, and facilitating continual learning without expensive LLM\nfine-tuning. Our comprehensive experiments show that SCALE significantly\noutperforms both few-shot LLMs (GPT-4) and specialized models (NLLB) in\nchallenging low-resource settings. Moreover, in Xhosa to English translation,\nSCALE experiences consistent improvement by a 4 BLEURT score without tuning LLM\nand surpasses few-shot GPT-4 by 2.5 COMET score and 3.8 BLEURT score when\nequipped with a compact model consisting of merely 600M parameters. SCALE could\nalso effectively exploit the existing language bias of LLMs by using an\nEnglish-centric STM as a pivot for translation between any language pairs,\noutperforming few-shot GPT-4 by an average of 6 COMET points across eight\ntranslation directions. Furthermore we provide an in-depth analysis of SCALE's\nrobustness, translation characteristics, and latency costs, providing solid\nfoundation for future studies exploring the potential synergy between LLMs and\nmore specialized, task-specific models.", "published": "2023-09-29 08:46:38", "link": "http://arxiv.org/abs/2309.17061v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fine-grained Late-interaction Multi-modal Retrieval for Retrieval\n  Augmented Visual Question Answering", "abstract": "Knowledge-based Visual Question Answering (KB-VQA) requires VQA systems to\nutilize knowledge from external knowledge bases to answer visually-grounded\nquestions. Retrieval-Augmented Visual Question Answering (RA-VQA), a strong\nframework to tackle KB-VQA, first retrieves related documents with Dense\nPassage Retrieval (DPR) and then uses them to answer questions. This paper\nproposes Fine-grained Late-interaction Multi-modal Retrieval (FLMR) which\nsignificantly improves knowledge retrieval in RA-VQA. FLMR addresses two major\nlimitations in RA-VQA's retriever: (1) the image representations obtained via\nimage-to-text transforms can be incomplete and inaccurate and (2) relevance\nscores between queries and documents are computed with one-dimensional\nembeddings, which can be insensitive to finer-grained relevance. FLMR overcomes\nthese limitations by obtaining image representations that complement those from\nthe image-to-text transforms using a vision model aligned with an existing\ntext-based retriever through a simple alignment network. FLMR also encodes\nimages and questions using multi-dimensional embeddings to capture\nfiner-grained relevance between queries and documents. FLMR significantly\nimproves the original RA-VQA retriever's PRRecall@5 by approximately 8\\%.\nFinally, we equipped RA-VQA with two state-of-the-art large\nmulti-modal/language models to achieve $\\sim61\\%$ VQA score in the OK-VQA\ndataset.", "published": "2023-09-29 10:54:10", "link": "http://arxiv.org/abs/2309.17133v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "An evaluation of GPT models for phenotype concept recognition", "abstract": "Objective: Clinical deep phenotyping and phenotype annotation play a critical\nrole in both the diagnosis of patients with rare disorders as well as in\nbuilding computationally-tractable knowledge in the rare disorders field. These\nprocesses rely on using ontology concepts, often from the Human Phenotype\nOntology, in conjunction with a phenotype concept recognition task (supported\nusually by machine learning methods) to curate patient profiles or existing\nscientific literature. With the significant shift in the use of large language\nmodels (LLMs) for most NLP tasks, we examine the performance of the latest\nGenerative Pre-trained Transformer (GPT) models underpinning ChatGPT as a\nfoundation for the tasks of clinical phenotyping and phenotype annotation.\nMaterials and Methods: The experimental setup of the study included seven\nprompts of various levels of specificity, two GPT models (gpt-3.5-turbo and\ngpt-4.0) and two established gold standard corpora for phenotype recognition,\none consisting of publication abstracts and the other clinical observations.\nResults: Our results show that, with an appropriate setup, these models can\nachieve state of the art performance. The best run, using few-shot learning,\nachieved 0.58 macro F1 score on publication abstracts and 0.75 macro F1 score\non clinical observations, the former being comparable with the state of the\nart, while the latter surpassing the current best in class tool. Conclusion:\nWhile the results are promising, the non-deterministic nature of the outcomes,\nthe high cost and the lack of concordance between different runs using the same\nprompt and input make the use of these LLMs challenging for this particular\ntask.", "published": "2023-09-29 12:06:55", "link": "http://arxiv.org/abs/2309.17169v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Comparative Analysis of Named Entity Recognition in the Dungeons and\n  Dragons Domain", "abstract": "Many NLP tasks, although well-resolved for general English, face challenges\nin specific domains like fantasy literature. This is evident in Named Entity\nRecognition (NER), which detects and categorizes entities in text. We analyzed\n10 NER models on 7 Dungeons and Dragons (D&D) adventure books to assess\ndomain-specific performance. Using open-source Large Language Models, we\nannotated named entities in these books and evaluated each model's precision.\nOur findings indicate that, without modifications, Flair, Trankit, and Spacy\noutperform others in identifying named entities in the D&D context.", "published": "2023-09-29 12:09:36", "link": "http://arxiv.org/abs/2309.17171v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AdaRefiner: Refining Decisions of Language Models with Adaptive Feedback", "abstract": "Large Language Models (LLMs) have demonstrated significant success across\nvarious domains. However, their application in complex decision-making tasks\nfrequently necessitates intricate prompt engineering or fine-tuning, leading to\nchallenges in unseen downstream tasks and heavy demands on computational\nresources. Meanwhile, Reinforcement Learning (RL) has been recognized as\neffective in decision-making problems but struggles in environments with sparse\nrewards, such as open-world games. To overcome these challenges, we introduce\nAdaRefiner, a novel framework designed to enhance the synergy between LLMs and\nRL feedback. The key component of AdaRefiner is a lightweight Adapter Language\nModel (LM), which automatically refines task comprehension based on feedback\nfrom RL agents. This method mitigates the need for intricate prompt engineering\nand intensive LLM fine-tuning while maintaining the LLMs' generalization\nabilities and enhancing their decision-making capabilities in downstream tasks.\nEmpirical evaluations of AdaRefiner on 22 diverse tasks within the open-world\ngame Crafter have demonstrated its superior effectiveness, especially in\nguiding agents towards higher-level and common-sense skills. Our work makes\ncontributions to the automatic self-refinement of LLMs with RL feedback,\noffering a more adaptable and efficient solution for complex decision-making\nproblems.", "published": "2023-09-29 12:16:19", "link": "http://arxiv.org/abs/2309.17176v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Knowledge Graphs for the Life Sciences: Recent Developments, Challenges\n  and Opportunities", "abstract": "The term life sciences refers to the disciplines that study living organisms\nand life processes, and include chemistry, biology, medicine, and a range of\nother related disciplines. Research efforts in life sciences are heavily\ndata-driven, as they produce and consume vast amounts of scientific data, much\nof which is intrinsically relational and graph-structured.\n  The volume of data and the complexity of scientific concepts and relations\nreferred to therein promote the application of advanced knowledge-driven\ntechnologies for managing and interpreting data, with the ultimate aim to\nadvance scientific discovery.\n  In this survey and position paper, we discuss recent developments and\nadvances in the use of graph-based technologies in life sciences and set out a\nvision for how these technologies will impact these fields into the future. We\nfocus on three broad topics: the construction and management of Knowledge\nGraphs (KGs), the use of KGs and associated technologies in the discovery of\nnew knowledge, and the use of KGs in artificial intelligence applications to\nsupport explanations (explainable AI). We select a few exemplary use cases for\neach topic, discuss the challenges and open research questions within these\ntopics, and conclude with a perspective and outlook that summarizes the\noverarching challenges and their potential solutions as a guide for future\nresearch.", "published": "2023-09-29 14:03:34", "link": "http://arxiv.org/abs/2309.17255v4", "categories": ["cs.AI", "cs.CL", "I.2.4; J.3"], "primary_category": "cs.AI"}
{"title": "STRONG -- Structure Controllable Legal Opinion Summary Generation", "abstract": "We propose an approach for the structure controllable summarization of long\nlegal opinions that considers the argument structure of the document. Our\napproach involves using predicted argument role information to guide the model\nin generating coherent summaries that follow a provided structure pattern. We\ndemonstrate the effectiveness of our approach on a dataset of legal opinions\nand show that it outperforms several strong baselines with respect to ROUGE,\nBERTScore, and structure similarity.", "published": "2023-09-29 14:31:41", "link": "http://arxiv.org/abs/2309.17280v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision)", "abstract": "Large multimodal models (LMMs) extend large language models (LLMs) with\nmulti-sensory skills, such as visual understanding, to achieve stronger generic\nintelligence. In this paper, we analyze the latest model, GPT-4V(ision), to\ndeepen the understanding of LMMs. The analysis focuses on the intriguing tasks\nthat GPT-4V can perform, containing test samples to probe the quality and\ngenericity of GPT-4V's capabilities, its supported inputs and working modes,\nand the effective ways to prompt the model. In our approach to exploring\nGPT-4V, we curate and organize a collection of carefully designed qualitative\nsamples spanning a variety of domains and tasks. Observations from these\nsamples demonstrate that GPT-4V's unprecedented ability in processing\narbitrarily interleaved multimodal inputs and the genericity of its\ncapabilities together make GPT-4V a powerful multimodal generalist system.\nFurthermore, GPT-4V's unique capability of understanding visual markers drawn\non input images can give rise to new human-computer interaction methods such as\nvisual referring prompting. We conclude the report with in-depth discussions on\nthe emerging application scenarios and the future research directions for\nGPT-4V-based systems. We hope that this preliminary exploration will inspire\nfuture research on the next-generation multimodal task formulation, new ways to\nexploit and enhance LMMs to solve real-world problems, and gaining better\nunderstanding of multimodal foundation models. Finally, we acknowledge that the\nmodel under our study is solely the product of OpenAI's innovative work, and\nthey should be fully credited for its development. Please see the GPT-4V\ncontributions paper for the authorship and credit attribution:\nhttps://cdn.openai.com/contributions/gpt-4v.pdf", "published": "2023-09-29 17:34:51", "link": "http://arxiv.org/abs/2309.17421v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving", "abstract": "Large language models have made significant progress in various language\ntasks, yet they still struggle with complex mathematics. In this paper, we\npropose ToRA a series of Tool-integrated Reasoning Agents designed to solve\nchallenging mathematical problems by seamlessly integrating natural language\nreasoning with the utilization of external tools (e.g., computation libraries\nand symbolic solvers), thereby amalgamating the analytical prowess of language\nand the computational efficiency of tools. To train ToRA, we curate interactive\ntool-use trajectories on mathematical datasets, apply imitation learning on the\nannotations, and propose output space shaping to further refine models'\nreasoning behavior. As a result, ToRA models significantly outperform\nopen-source models on 10 mathematical reasoning datasets across all scales with\n13%-19% absolute improvements on average. Notably, ToRA-7B reaches 44.6% on the\ncompetition-level dataset MATH, surpassing the best open-source model\nWizardMath-70B by 22% absolute. ToRA-Code-34B is also the first open-source\nmodel that achieves an accuracy exceeding 50% on MATH, which significantly\noutperforms GPT-4's CoT result, and is competitive with GPT-4 solving problems\nwith programs. Additionally, we conduct a comprehensive analysis of the\nbenefits and remaining challenges of tool interaction for mathematical\nreasoning, providing valuable insights for future research.", "published": "2023-09-29 17:59:38", "link": "http://arxiv.org/abs/2309.17452v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Efficient Streaming Language Models with Attention Sinks", "abstract": "Deploying Large Language Models (LLMs) in streaming applications such as\nmulti-round dialogue, where long interactions are expected, is urgently needed\nbut poses two major challenges. Firstly, during the decoding stage, caching\nprevious tokens' Key and Value states (KV) consumes extensive memory. Secondly,\npopular LLMs cannot generalize to longer texts than the training sequence\nlength. Window attention, where only the most recent KVs are cached, is a\nnatural approach -- but we show that it fails when the text length surpasses\nthe cache size. We observe an interesting phenomenon, namely attention sink,\nthat keeping the KV of initial tokens will largely recover the performance of\nwindow attention. In this paper, we first demonstrate that the emergence of\nattention sink is due to the strong attention scores towards initial tokens as\na \"sink\" even if they are not semantically important. Based on the above\nanalysis, we introduce StreamingLLM, an efficient framework that enables LLMs\ntrained with a finite length attention window to generalize to infinite\nsequence lengths without any fine-tuning. We show that StreamingLLM can enable\nLlama-2, MPT, Falcon, and Pythia to perform stable and efficient language\nmodeling with up to 4 million tokens and more. In addition, we discover that\nadding a placeholder token as a dedicated attention sink during pre-training\ncan further improve streaming deployment. In streaming settings, StreamingLLM\noutperforms the sliding window recomputation baseline by up to 22.2x speedup.\nCode and datasets are provided at https://github.com/mit-han-lab/streaming-llm.", "published": "2023-09-29 17:59:56", "link": "http://arxiv.org/abs/2309.17453v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SocREval: Large Language Models with the Socratic Method for\n  Reference-Free Reasoning Evaluation", "abstract": "To comprehensively gauge the capacity of current models for complex\nreasoning, it is crucial to assess their step-by-step reasoning in a scalable\nmanner. Established reference-based evaluation metrics rely on human-annotated\nreasoning chains as references to assess the model-derived chains. However,\nsuch \"gold-standard\" human-written reasoning chains may not be unique and their\nacquisition is often labor-intensive. Existing reference-free reasoning\nevaluation metrics, while eliminating the need for human-crafted reasoning\nchains as references, often require fine-tuning with human-derived chains\nbefore evaluation, complicating the process and questioning their adaptability\nto other datasets. To address these challenges, we harness GPT-4 to\nautomatically evaluate reasoning chain quality, thereby removing the dependency\non human-written reasoning chains for both model fine-tuning and evaluative\npurposes. Leveraging the Socratic method, we develop SocREval ({\\bf Soc}ratic\nMethod-Inspired {\\bf R}easoning {\\bf Eval}uation), a novel approach for prompt\ndesign in reference-free reasoning evaluation. Empirical results from four\nhuman annotated datasets reveal that SocREval significantly improves GPT-4's\nperformance, surpassing existing reference-free and reference-based reasoning\nevaluation metrics. Beyond its demonstrated efficacy, SocREval, proves to be\nboth cost-efficient and robust to prompt writing and example selection, as\nsubstantiated by our in-depth analysis.", "published": "2023-09-29 18:25:46", "link": "http://arxiv.org/abs/2310.00074v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multilingual Natural Language Processing Model for Radiology Reports --\n  The Summary is all you need!", "abstract": "The impression section of a radiology report summarizes important radiology\nfindings and plays a critical role in communicating these findings to\nphysicians. However, the preparation of these summaries is time-consuming and\nerror-prone for radiologists. Recently, numerous models for radiology report\nsummarization have been developed. Nevertheless, there is currently no model\nthat can summarize these reports in multiple languages. Such a model could\ngreatly improve future research and the development of Deep Learning models\nthat incorporate data from patients with different ethnic backgrounds. In this\nstudy, the generation of radiology impressions in different languages was\nautomated by fine-tuning a model, publicly available, based on a multilingual\ntext-to-text Transformer to summarize findings available in English,\nPortuguese, and German radiology reports. In a blind test, two board-certified\nradiologists indicated that for at least 70% of the system-generated summaries,\nthe quality matched or exceeded the corresponding human-written summaries,\nsuggesting substantial clinical reliability. Furthermore, this study showed\nthat the multilingual model outperformed other models that specialized in\nsummarizing radiology reports in only one language, as well as models that were\nnot specifically designed for summarizing radiology reports, such as ChatGPT.", "published": "2023-09-29 19:20:27", "link": "http://arxiv.org/abs/2310.00100v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Gift of Feedback: Improving ASR Model Quality by Learning from User\n  Corrections through Federated Learning", "abstract": "Automatic speech recognition (ASR) models are typically trained on large\ndatasets of transcribed speech. As language evolves and new terms come into\nuse, these models can become outdated and stale. In the context of models\ntrained on the server but deployed on edge devices, errors may result from the\nmismatch between server training data and actual on-device usage. In this work,\nwe seek to continually learn from on-device user corrections through Federated\nLearning (FL) to address this issue. We explore techniques to target fresh\nterms that the model has not previously encountered, learn long-tail words, and\nmitigate catastrophic forgetting. In experimental evaluations, we find that the\nproposed techniques improve model recognition of fresh terms, while preserving\nquality on the overall language distribution.", "published": "2023-09-29 21:04:10", "link": "http://arxiv.org/abs/2310.00141v2", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Self-Specialization: Uncovering Latent Expertise within Large Language\n  Models", "abstract": "Recent works have demonstrated the effectiveness of self-alignment in which a\nlarge language model is aligned to follow general instructions using\ninstructional data generated from the model itself starting from a handful of\nhuman-written seeds. Instead of general alignment, in this work, we focus on\nself-alignment for expert domain specialization (e.g., biomedicine, finance).\nAs a preliminary, we quantitively show the marginal effect that generic\ninstruction-following training has on downstream expert domains' performance.\nTo remedy this, we propose self-specialization - allowing for effective model\nspecialization while achieving cross-task generalization by leveraging only a\nfew labeled seeds. Self-specialization offers a data- and parameter-efficient\nway of \"carving out\" an expert model out of a generalist pre-trained LLM.\nExploring a variety of popular open large models as a base for specialization,\nour experimental results in both biomedical and financial domains show that our\nself-specialized models outperform their base models by a large margin, and\neven larger models that are generally instruction-tuned or that have been\nadapted to the target domain by other means.", "published": "2023-09-29 21:53:46", "link": "http://arxiv.org/abs/2310.00160v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Contextual Biasing with the Knuth-Morris-Pratt Matching Algorithm", "abstract": "Contextual biasing refers to the problem of biasing the automatic speech\nrecognition (ASR) systems towards rare entities that are relevant to the\nspecific user or application scenarios. We propose algorithms for contextual\nbiasing based on the Knuth-Morris-Pratt algorithm for pattern matching. During\nbeam search, we boost the score of a token extension if it extends matching\ninto a set of biasing phrases. Our method simulates the classical approaches\noften implemented in the weighted finite state transducer (WFST) framework, but\navoids the FST language altogether, with careful considerations on memory\nfootprint and efficiency on tensor processing units (TPUs) by vectorization.\nWithout introducing additional model parameters, our method achieves\nsignificant word error rate (WER) reductions on biasing test sets by itself,\nand yields further performance gain when combined with a model-based biasing\nmethod.", "published": "2023-09-29 22:50:10", "link": "http://arxiv.org/abs/2310.00178v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Sarcasm in Sight and Sound: Benchmarking and Expansion to Improve\n  Multimodal Sarcasm Detection", "abstract": "The introduction of the MUStARD dataset, and its emotion recognition\nextension MUStARD++, have identified sarcasm to be a multi-modal phenomenon --\nexpressed not only in natural language text, but also through manners of speech\n(like tonality and intonation) and visual cues (facial expression). With this\nwork, we aim to perform a rigorous benchmarking of the MUStARD++ dataset by\nconsidering state-of-the-art language, speech, and visual encoders, for fully\nutilizing the totality of the multi-modal richness that it has to offer,\nachieving a 2\\% improvement in macro-F1 over the existing benchmark.\nAdditionally, to cure the imbalance in the `sarcasm type' category in\nMUStARD++, we propose an extension, which we call \\emph{MUStARD++ Balanced},\nbenchmarking the same with instances from the extension split across both train\nand test sets, achieving a further 2.4\\% macro-F1 boost. The new clips were\ntaken from a novel source -- the TV show, House MD, which adds to the diversity\nof the dataset, and were manually annotated by multiple annotators with\nsubstantial inter-annotator agreement in terms of Cohen's kappa and\nKrippendorf's alpha. Our code, extended data, and SOTA benchmark models are\nmade public.", "published": "2023-09-29 07:00:41", "link": "http://arxiv.org/abs/2310.01430v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Split and Merge: Aligning Position Biases in LLM-based Evaluators", "abstract": "Large language models (LLMs) have shown promise as automated evaluators for\nassessing the quality of answers generated by AI systems. However, these\nLLM-based evaluators exhibit position bias, or inconsistency, when used to\nevaluate candidate answers in pairwise comparisons, favoring either the first\nor second answer regardless of content. To address this limitation, we propose\nPORTIA, an alignment-based system designed to mimic human comparison strategies\nto calibrate position bias in a lightweight yet effective manner. Specifically,\nPORTIA splits the answers into multiple segments, aligns similar content across\ncandidate answers, and then merges them back into a single prompt for\nevaluation by LLMs. We conducted extensive experiments with six diverse LLMs to\nevaluate 11,520 answer pairs. Our results show that PORTIA markedly enhances\nthe consistency rates for all the models and comparison forms tested, achieving\nan average relative improvement of 47.46%. Remarkably, PORTIA enables less\nadvanced GPT models to achieve 88% agreement with the state-of-the-art GPT-4\nmodel at just 10% of the cost. Furthermore, it rectifies around 80% of the\nposition bias instances within the GPT-4 model, elevating its consistency rate\nup to 98%. Subsequent human evaluations indicate that the PORTIA-enhanced\nGPT-3.5 model can even surpass the standalone GPT-4 in terms of alignment with\nhuman evaluators. These findings highlight PORTIA's ability to correct position\nbias, improve LLM consistency, and boost performance while keeping\ncost-efficiency. This represents a valuable step toward a more reliable and\nscalable use of LLMs for automated evaluations across diverse applications.", "published": "2023-09-29 14:38:58", "link": "http://arxiv.org/abs/2310.01432v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SSHR: Leveraging Self-supervised Hierarchical Representations for\n  Multilingual Automatic Speech Recognition", "abstract": "Multilingual automatic speech recognition (ASR) systems have garnered\nattention for their potential to extend language coverage globally. While\nself-supervised learning (SSL) models, like MMS, have demonstrated their\neffectiveness in multilingual ASR, it is worth noting that various layers'\nrepresentations potentially contain distinct information that has not been\nfully leveraged. In this study, we propose a novel method that leverages\nself-supervised hierarchical representations (SSHR) to fine-tune the MMS model.\nWe first analyze the different layers of MMS and show that the middle layers\ncapture language-related information, and the high layers encode\ncontent-related information, which gradually decreases in the final layers.\nThen, we extract a language-related frame from correlated middle layers and\nguide specific language extraction through self-attention mechanisms.\nAdditionally, we steer the model toward acquiring more content-related\ninformation in the final layers using our proposed Cross-CTC. We evaluate SSHR\non two multilingual datasets, Common Voice and ML-SUPERB, and the experimental\nresults demonstrate that our method achieves state-of-the-art performance.", "published": "2023-09-29 02:35:36", "link": "http://arxiv.org/abs/2309.16937v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Benchmarking Cognitive Biases in Large Language Models as Evaluators", "abstract": "Large Language Models are cognitively biased judges. Large Language Models\n(LLMs) have recently been shown to be effective as automatic evaluators with\nsimple prompting and in-context learning. In this work, we assemble 15 LLMs of\nfour different size ranges and evaluate their output responses by preference\nranking from the other LLMs as evaluators, such as System Star is better than\nSystem Square. We then evaluate the quality of ranking outputs introducing the\nCognitive Bias Benchmark for LLMs as Evaluators (CoBBLEr), a benchmark to\nmeasure six different cognitive biases in LLM evaluation outputs, such as the\nEgocentric bias where a model prefers to rank its own outputs highly in\nevaluation. We find that LLMs are biased text quality evaluators, exhibiting\nstrong indications on our bias benchmark (average of 40% of comparisons across\nall models) within each of their evaluations that question their robustness as\nevaluators. Furthermore, we examine the correlation between human and machine\npreferences and calculate the average Rank-Biased Overlap (RBO) score to be\n49.6%, indicating that machine preferences are misaligned with humans.\nAccording to our findings, LLMs may still be unable to be utilized for\nautomatic annotation aligned with human preferences. Our project page is at:\nhttps://minnesotanlp.github.io/cobbler.", "published": "2023-09-29 06:53:10", "link": "http://arxiv.org/abs/2309.17012v3", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Benchmarking the Abilities of Large Language Models for RDF Knowledge\n  Graph Creation and Comprehension: How Well Do LLMs Speak Turtle?", "abstract": "Large Language Models (LLMs) are advancing at a rapid pace, with significant\nimprovements at natural language processing and coding tasks. Yet, their\nability to work with formal languages representing data, specifically within\nthe realm of knowledge graph engineering, remains under-investigated. To\nevaluate the proficiency of various LLMs, we created a set of five tasks that\nprobe their ability to parse, understand, analyze, and create knowledge graphs\nserialized in Turtle syntax. These tasks, each embodying distinct degrees of\ncomplexity and being able to scale with the size of the problem, have been\nintegrated into our automated evaluation system, the LLM-KG-Bench. The\nevaluation encompassed four commercially available LLMs - GPT-3.5, GPT-4,\nClaude 1.3, and Claude 2.0, as well as two freely accessible offline models,\nGPT4All Vicuna and GPT4All Falcon 13B. This analysis offers an in-depth\nunderstanding of the strengths and shortcomings of LLMs in relation to their\napplication within RDF knowledge graph engineering workflows utilizing Turtle\nrepresentation. While our findings show that the latest commercial models\noutperform their forerunners in terms of proficiency with the Turtle language,\nthey also reveal an apparent weakness. These models fall short when it comes to\nadhering strictly to the output formatting constraints, a crucial requirement\nin this context.", "published": "2023-09-29 10:36:04", "link": "http://arxiv.org/abs/2309.17122v1", "categories": ["cs.AI", "cs.CL", "cs.DB"], "primary_category": "cs.AI"}
{"title": "Using Large Language Models for Qualitative Analysis can Introduce\n  Serious Bias", "abstract": "Large Language Models (LLMs) are quickly becoming ubiquitous, but the\nimplications for social science research are not yet well understood. This\npaper asks whether LLMs can help us analyse large-N qualitative data from\nopen-ended interviews, with an application to transcripts of interviews with\nRohingya refugees in Cox's Bazaar, Bangladesh. We find that a great deal of\ncaution is needed in using LLMs to annotate text as there is a risk of\nintroducing biases that can lead to misleading inferences. We here mean bias in\nthe technical sense, that the errors that LLMs make in annotating interview\ntranscripts are not random with respect to the characteristics of the interview\nsubjects. Training simpler supervised models on high-quality human annotations\nwith flexible coding leads to less measurement error and bias than LLM\nannotations. Therefore, given that some high quality annotations are necessary\nin order to asses whether an LLM introduces bias, we argue that it is probably\npreferable to train a bespoke model on these annotations than it is to use an\nLLM for annotation.", "published": "2023-09-29 11:19:15", "link": "http://arxiv.org/abs/2309.17147v2", "categories": ["cs.CL", "cs.AI", "econ.GN", "q-fin.EC"], "primary_category": "cs.CL"}
{"title": "DyVal: Dynamic Evaluation of Large Language Models for Reasoning Tasks", "abstract": "Large language models (LLMs) have achieved remarkable performance in various\nevaluation benchmarks. However, concerns are raised about potential data\ncontamination in their considerable volume of training corpus. Moreover, the\nstatic nature and fixed complexity of current benchmarks may inadequately gauge\nthe advancing capabilities of LLMs. In this paper, we introduce DyVal, a\ngeneral and flexible protocol for dynamic evaluation of LLMs. Based on our\nframework, we build graph-informed DyVal by leveraging the structural advantage\nof directed acyclic graphs to dynamically generate evaluation samples with\ncontrollable complexities. DyVal generates challenging evaluation sets on\nreasoning tasks including mathematics, logical reasoning, and algorithm\nproblems. We evaluate various LLMs ranging from Flan-T5-large to GPT-3.5-Turbo\nand GPT-4. Experiments show that LLMs perform worse in DyVal-generated\nevaluation samples with different complexities, highlighting the significance\nof dynamic evaluation. We also analyze the failure cases and results of\ndifferent prompting methods. Moreover, DyVal-generated samples are not only\nevaluation sets, but also helpful data for fine-tuning to improve the\nperformance of LLMs on existing benchmarks. We hope that DyVal can shed light\non future evaluation research of LLMs. Code is available at:\nhttps://github.com/microsoft/promptbench.", "published": "2023-09-29 12:04:14", "link": "http://arxiv.org/abs/2309.17167v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Alphazero-like Tree-Search can Guide Large Language Model Decoding and\n  Training", "abstract": "Recent works like Tree-of-Thought (ToT) and Reasoning via Planning (RAP) aim\nto augment the reasoning capabilities of LLMs by using tree-search algorithms\nto guide multi-step reasoning. These methods rely on prompting a pre-trained\nmodel to serve as a value function and focus on problems with low search depth.\nAs a result, these methods will not work in domains where the pre-trained LLM\ndoes not have enough knowledge to serve as an effective value function or in\ndomains that require long-horizon planning. To address these limitations, we\npresent an AlphaZero-like tree-search learning framework for LLMs (termed\nTS-LLM), systematically illustrating how tree-search with a learned value\nfunction can guide LLM decoding. TS-LLM distinguishes itself in two key ways.\n(1) Leveraging a learned value function and AlphaZero-like algorithms, our\napproach can be generally adaptable to a wide range of tasks, language models\nof any size, and tasks of varying search depths. (2) Our approach can guide\nLLMs during both inference and training, iteratively improving the LLM.\nEmpirical results across reasoning, planning, alignment, and decision-making\ntasks show that TS-LLM outperforms existing approaches and can handle trees\nwith a depth of 64.", "published": "2023-09-29 12:20:19", "link": "http://arxiv.org/abs/2309.17179v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Cooperation, Competition, and Maliciousness: LLM-Stakeholders\n  Interactive Negotiation", "abstract": "There is an growing interest in using Large Language Models (LLMs) in\nmulti-agent systems to tackle interactive real-world tasks that require\neffective collaboration and assessing complex situations. Yet, we still have a\nlimited understanding of LLMs' communication and decision-making abilities in\nmulti-agent setups. The fundamental task of negotiation spans many key features\nof communication, such as cooperation, competition, and manipulation\npotentials. Thus, we propose using scorable negotiation to evaluate LLMs. We\ncreate a testbed of complex multi-agent, multi-issue, and semantically rich\nnegotiation games. To reach an agreement, agents must have strong arithmetic,\ninference, exploration, and planning capabilities while integrating them in a\ndynamic and multi-turn setup. We propose multiple metrics to rigorously\nquantify agents' performance and alignment with the assigned role. We provide\nprocedures to create new games and increase games' difficulty to have an\nevolving benchmark. Importantly, we evaluate critical safety aspects such as\nthe interaction dynamics between agents influenced by greedy and adversarial\nplayers. Our benchmark is highly challenging; GPT-3.5 and small models mostly\nfail, and GPT-4 and SoTA large models (e.g., Llama-3 70b) still underperform.", "published": "2023-09-29 13:33:06", "link": "http://arxiv.org/abs/2309.17234v2", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Batch Calibration: Rethinking Calibration for In-Context Learning and\n  Prompt Engineering", "abstract": "Prompting and in-context learning (ICL) have become efficient learning\nparadigms for large language models (LLMs). However, LLMs suffer from prompt\nbrittleness and various bias factors in the prompt, including but not limited\nto the formatting, the choice verbalizers, and the ICL examples. To address\nthis problem that results in unexpected performance degradation, calibration\nmethods have been developed to mitigate the effects of these biases while\nrecovering LLM performance. In this work, we first conduct a systematic\nanalysis of the existing calibration methods, where we both provide a unified\nview and reveal the failure cases. Inspired by these analyses, we propose Batch\nCalibration (BC), a simple yet intuitive method that controls the contextual\nbias from the batched input, unifies various prior approaches, and effectively\naddresses the aforementioned issues. BC is zero-shot, inference-only, and\nincurs negligible additional costs. In the few-shot setup, we further extend BC\nto allow it to learn the contextual bias from labeled data. We validate the\neffectiveness of BC with PaLM 2-(S, M, L) and CLIP models and demonstrate\nstate-of-the-art performance over previous calibration baselines across more\nthan 10 natural language understanding and image classification tasks.", "published": "2023-09-29 13:55:45", "link": "http://arxiv.org/abs/2309.17249v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Wiki-En-ASR-Adapt: Large-scale synthetic dataset for English ASR\n  Customization", "abstract": "We present a first large-scale public synthetic dataset for contextual\nspellchecking customization of automatic speech recognition (ASR) with focus on\ndiverse rare and out-of-vocabulary (OOV) phrases, such as proper names or\nterms. The proposed approach allows creating millions of realistic examples of\ncorrupted ASR hypotheses and simulate non-trivial biasing lists for the\ncustomization task. Furthermore, we propose injecting two types of ``hard\nnegatives\" to the simulated biasing lists in training examples and describe our\nprocedures to automatically mine them. We report experiments with training an\nopen-source customization model on the proposed dataset and show that the\ninjection of hard negative biasing phrases decreases WER and the number of\nfalse alarms.", "published": "2023-09-29 14:18:59", "link": "http://arxiv.org/abs/2309.17267v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Enhancing Large Language Models in Coding Through Multi-Perspective\n  Self-Consistency", "abstract": "Large language models (LLMs) have exhibited remarkable ability in code\ngeneration. However, generating the correct solution in a single attempt still\nremains a challenge. Prior works utilize verification properties in software\nengineering to verify and re-rank solutions in a majority voting manner. But\nthe assumption behind them that generated verification properties have better\nqualities than solutions may not always hold. In this paper, we treat them\nequally as different perspectives of LLMs' reasoning processes. We propose the\nMulti-Perspective Self-Consistency (MPSC) framework incorporating both inter-\nand intra-consistency across outputs from multiple perspectives. Specifically,\nwe prompt LLMs to generate diverse outputs from three perspectives, Solution,\nSpecification and Test case, constructing a 3-partite graph. With two measure\nfunctions of consistency, we embed both inter- and intra-consistency\ninformation into the graph. The optimal choice of solutions is then determined\nbased on analysis in the graph. MPSC significantly boosts performance of\nfoundation models (ChatGPT in this paper) on various benchmarks, including\nHumanEval (+15.91%), MBPP (+6.43%) and CodeContests (+9.37%), even surpassing\nGPT-4.", "published": "2023-09-29 14:23:26", "link": "http://arxiv.org/abs/2309.17272v3", "categories": ["cs.CL", "cs.AI", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Can Sensitive Information Be Deleted From LLMs? Objectives for Defending\n  Against Extraction Attacks", "abstract": "Pretrained language models sometimes possess knowledge that we do not wish\nthem to, including memorized personal information and knowledge that could be\nused to harm people. They can also output toxic or harmful text. To mitigate\nthese safety and informational issues, we propose an attack-and-defense\nframework for studying the task of deleting sensitive information directly from\nmodel weights. We study direct edits to model weights because (1) this approach\nshould guarantee that particular deleted information is never extracted by\nfuture prompt attacks, and (2) it should protect against whitebox attacks,\nwhich is necessary for making claims about safety/privacy in a setting where\npublicly available model weights could be used to elicit sensitive information.\nOur threat model assumes that an attack succeeds if the answer to a sensitive\nquestion is located among a set of B generated candidates, based on scenarios\nwhere the information would be insecure if the answer is among B candidates.\nExperimentally, we show that even state-of-the-art model editing methods such\nas ROME struggle to truly delete factual information from models like GPT-J, as\nour whitebox and blackbox attacks can recover \"deleted\" information from an\nedited model 38% of the time. These attacks leverage two key observations: (1)\nthat traces of deleted information can be found in intermediate model hidden\nstates, and (2) that applying an editing method for one question may not delete\ninformation across rephrased versions of the question. Finally, we provide new\ndefense methods that protect against some extraction attacks, but we do not\nfind a single universally effective defense method. Our results suggest that\ntruly deleting sensitive information is a tractable but difficult problem,\nsince even relatively low attack success rates have potentially severe societal\nimplications for real-world deployment of language models.", "published": "2023-09-29 17:12:43", "link": "http://arxiv.org/abs/2309.17410v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CRAFT: Customizing LLMs by Creating and Retrieving from Specialized\n  Toolsets", "abstract": "Large language models (LLMs) are often augmented with tools to solve complex\ntasks. By generating code snippets and executing them through task-specific\nApplication Programming Interfaces (APIs), they can offload certain functions\nto dedicated external modules, such as image encoding and performing\ncalculations. However, most existing approaches to augment LLMs with tools are\nconstrained by general-purpose APIs and lack the flexibility for tailoring them\nto specific tasks. In this work, we present CRAFT, a general tool creation and\nretrieval framework for LLMs. It creates toolsets specifically curated for the\ntasks and equips LLMs with a component that retrieves tools from these sets to\nenhance their capability to solve complex tasks. For each task, we collect\nspecific code solutions by prompting GPT-4 to solve the training examples.\nFollowing a validation step ensuring the correctness, these solutions are\nabstracted into code snippets to enhance reusability, and deduplicated for\nhigher quality. At inference time, the language model retrieves snippets from\nthe toolsets and then executes them or generates the output conditioning on the\nretrieved snippets. Our method is designed to be flexible and offers a\nplug-and-play approach to adapt off-the-shelf LLMs to unseen domains and\nmodalities, without any finetuning. Experiments on vision-language, tabular\nprocessing, and mathematical reasoning tasks show that our approach achieves\nsubstantial improvements compared to strong baselines. In addition, our\nin-depth analysis reveals that: (1) consistent performance improvement can be\nachieved by scaling up the number of tools and the capability of the backbone\nmodels; (2) each component of our approach contributes to the performance\ngains; (3) the created tools are well-structured and reliable with low\ncomplexity and atomicity. The code is available at\nhttps://github.com/lifan-yuan/CRAFT.", "published": "2023-09-29 17:40:26", "link": "http://arxiv.org/abs/2309.17428v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LLM-grounded Video Diffusion Models", "abstract": "Text-conditioned diffusion models have emerged as a promising tool for neural\nvideo generation. However, current models still struggle with intricate\nspatiotemporal prompts and often generate restricted or incorrect motion. To\naddress these limitations, we introduce LLM-grounded Video Diffusion (LVD).\nInstead of directly generating videos from the text inputs, LVD first leverages\na large language model (LLM) to generate dynamic scene layouts based on the\ntext inputs and subsequently uses the generated layouts to guide a diffusion\nmodel for video generation. We show that LLMs are able to understand complex\nspatiotemporal dynamics from text alone and generate layouts that align closely\nwith both the prompts and the object motion patterns typically observed in the\nreal world. We then propose to guide video diffusion models with these layouts\nby adjusting the attention maps. Our approach is training-free and can be\nintegrated into any video diffusion model that admits classifier guidance. Our\nresults demonstrate that LVD significantly outperforms its base video diffusion\nmodel and several strong baseline methods in faithfully generating videos with\nthe desired attributes and motion patterns.", "published": "2023-09-29 17:54:46", "link": "http://arxiv.org/abs/2309.17444v3", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "L2CEval: Evaluating Language-to-Code Generation Capabilities of Large\n  Language Models", "abstract": "Recently, large language models (LLMs), especially those that are pretrained\non code, have demonstrated strong capabilities in generating programs from\nnatural language inputs in a few-shot or even zero-shot manner. Despite\npromising results, there is a notable lack of a comprehensive evaluation of\nthese models language-to-code generation capabilities. Existing studies often\nfocus on specific tasks, model architectures, or learning paradigms, leading to\na fragmented understanding of the overall landscape. In this work, we present\nL2CEval, a systematic evaluation of the language-to-code generation\ncapabilities of LLMs on 7 tasks across the domain spectrum of semantic parsing,\nmath reasoning and Python programming, analyzing the factors that potentially\naffect their performance, such as model size, pretraining data, instruction\ntuning, and different prompting methods. In addition to assessing model\nperformance, we measure confidence calibration for the models and conduct human\nevaluations of the output programs. This enables us to identify and analyze the\ntypical failure modes across various tasks and models. L2CEval offers a\ncomprehensive understanding of the capabilities and limitations of LLMs in\nlanguage-to-code generation. We also release the evaluation framework and all\nmodel outputs, hoping to lay the groundwork for further future research in this\ndomain.", "published": "2023-09-29 17:57:00", "link": "http://arxiv.org/abs/2309.17446v2", "categories": ["cs.CL", "cs.LG", "cs.PL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "PB-LLM: Partially Binarized Large Language Models", "abstract": "This paper explores network binarization, a radical form of quantization,\ncompressing model weights to a single bit, specifically for Large Language\nModels (LLMs) compression. Due to previous binarization methods collapsing\nLLMs, we propose a novel approach, Partially-Binarized LLM (PB-LLM), which can\nachieve extreme low-bit quantization while maintaining the linguistic reasoning\ncapacity of quantized LLMs. Specifically, our exploration first uncovers the\nineffectiveness of naive applications of existing binarization algorithms and\nhighlights the imperative role of salient weights in achieving low-bit\nquantization. Thus, PB-LLM filters a small ratio of salient weights during\nbinarization, allocating them to higher-bit storage, i.e.,\npartially-binarization. PB-LLM is extended to recover the capacities of\nquantized LMMs, by analyzing from the perspective of post-training quantization\n(PTQ) and quantization-aware training (QAT). Under PTQ, combining the concepts\nfrom GPTQ, we reconstruct the binarized weight matrix guided by the Hessian\nmatrix and successfully recover the reasoning capacity of PB-LLM in low-bit.\nUnder QAT, we freeze the salient weights during training, explore the\nderivation of optimal scaling factors crucial for minimizing the quantization\nerror, and propose a scaling mechanism based on this derived scaling strategy\nfor residual binarized weights. Those explorations and the developed\nmethodologies significantly contribute to rejuvenating the performance of\nlow-bit quantized LLMs and present substantial advancements in the field of\nnetwork binarization for LLMs.The code is available at\nhttps://github.com/hahnyuan/BinaryLLM.", "published": "2023-09-29 14:35:27", "link": "http://arxiv.org/abs/2310.00034v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Voice2Action: Language Models as Agent for Efficient Real-Time\n  Interaction in Virtual Reality", "abstract": "Large Language Models (LLMs) are trained and aligned to follow natural\nlanguage instructions with only a handful of examples, and they are prompted as\ntask-driven autonomous agents to adapt to various sources of execution\nenvironments. However, deploying agent LLMs in virtual reality (VR) has been\nchallenging due to the lack of efficiency in online interactions and the\ncomplex manipulation categories in 3D environments. In this work, we propose\nVoice2Action, a framework that hierarchically analyzes customized voice signals\nand textual commands through action and entity extraction and divides the\nexecution tasks into canonical interaction subsets in real-time with error\nprevention from environment feedback. Experiment results in an urban\nengineering VR environment with synthetic instruction data show that\nVoice2Action can perform more efficiently and accurately than approaches\nwithout optimizations.", "published": "2023-09-29 19:06:52", "link": "http://arxiv.org/abs/2310.00092v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Revolutionizing Mobile Interaction: Enabling a 3 Billion Parameter GPT\n  LLM on Mobile", "abstract": "The field of Artificial Intelligence has witnessed remarkable progress in\nrecent years, especially with the emergence of powerful large language models\n(LLMs) based on the transformer architecture. Cloud-based LLMs, such as\nOpenAI's ChatGPT, offer impressive capabilities but come with concerns\nregarding latency and privacy due to network dependencies. This article\npresents an innovative approach to LLM inference, envisioning a future where\nLLMs with billions of parameters can be executed directly on mobile devices\nwithout network connectivity. The article showcases a fine-tuned GPT LLM with 3\nbillion parameters that can operate smoothly on devices with as low as 4GB of\nmemory. Through the integration of native code and model quantization\ntechniques, the application not only serves as a general-purpose assistant but\nalso facilitates seamless mobile interactions with text-to-actions features.\nThe article provides insights into the training pipeline, implementation\ndetails, test results, and future directions of on-device LLM inference. This\nbreakthrough technology opens up possibilities for empowering users with\nsophisticated AI capabilities while preserving their privacy and eliminating\nlatency concerns.", "published": "2023-09-29 16:30:49", "link": "http://arxiv.org/abs/2310.01434v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Clinical Text Deduplication Practices for Efficient Pretraining and\n  Improved Clinical Tasks", "abstract": "Despite being a unique source of information on patients' status and disease\nprogression, clinical notes are characterized by high levels of duplication and\ninformation redundancy. In general domain text, it has been shown that\ndeduplication does not harm language model (LM) pretraining, thus helping\nreduce the training cost. Although large LMs have proven to learn medical\nknowledge, they still require specialized domain adaptation for improved\ndownstream clinical tasks. By leveraging large real-world clinical corpora, we\nfirst provided a fine-grained characterization of duplicates stemming from\ncommon writing practices and clinical relevancy. Second, we demonstrated that\ndeduplicating clinical text can help clinical LMs encode less redundant\ninformation in a more efficient manner and do not harm classification tasks via\nprompt-based learning.", "published": "2023-09-29 18:35:52", "link": "http://arxiv.org/abs/2312.09469v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Training and inference of large language models using 8-bit floating\n  point", "abstract": "FP8 formats are gaining popularity to boost the computational efficiency for\ntraining and inference of large deep learning models. Their main challenge is\nthat a careful choice of scaling is needed to prevent degradation due to the\nreduced dynamic range compared to higher-precision formats. Although there\nexists ample literature about selecting such scalings for INT formats, this\ncritical aspect has yet to be addressed for FP8. This paper presents a\nmethodology to select the scalings for FP8 linear layers, based on dynamically\nupdating per-tensor scales for the weights, gradients and activations. We apply\nthis methodology to train and validate large language models of the type of GPT\nand Llama 2 using FP8, for model sizes ranging from 111M to 70B. To facilitate\nthe understanding of the FP8 dynamics, our results are accompanied by plots of\nthe per-tensor scale distribution for weights, activations and gradients during\nboth training and inference.", "published": "2023-09-29 13:24:33", "link": "http://arxiv.org/abs/2309.17224v1", "categories": ["cs.LG", "cs.AR", "cs.CL", "cs.ET", "cs.PF", "I.2.7; B.2.4"], "primary_category": "cs.LG"}
{"title": "Enhancing Code-switching Speech Recognition with Interactive Language\n  Biases", "abstract": "Languages usually switch within a multilingual speech signal, especially in a\nbilingual society. This phenomenon is referred to as code-switching (CS),\nmaking automatic speech recognition (ASR) challenging under a multilingual\nscenario. We propose to improve CS-ASR by biasing the hybrid CTC/attention ASR\nmodel with multi-level language information comprising frame- and token-level\nlanguage posteriors. The interaction between various resolutions of language\nbiases is subsequently explored in this work. We conducted experiments on\ndatasets from the ASRU 2019 code-switching challenge. Compared to the baseline,\nthe proposed interactive language biases (ILB) method achieves higher\nperformance and ablation studies highlight the effects of different language\nbiases and their interactions. In addition, the results presented indicate that\nlanguage bias implicitly enhances internal language modeling, leading to\nperformance degradation after employing an external language model.", "published": "2023-09-29 03:37:43", "link": "http://arxiv.org/abs/2309.16953v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Synthetic Speech Detection Based on Temporal Consistency and\n  Distribution of Speaker Features", "abstract": "Current synthetic speech detection (SSD) methods perform well on certain\ndatasets but still face issues of robustness and interpretability. A possible\nreason is that these methods do not analyze the deficiencies of synthetic\nspeech. In this paper, the flaws of the speaker features inherent in the\ntext-to-speech (TTS) process are analyzed. Differences in the temporal\nconsistency of intra-utterance speaker features arise due to the lack of\nfine-grained control over speaker features in TTS. Since the speaker\nrepresentations in TTS are based on speaker embeddings extracted by encoders,\nthe distribution of inter-utterance speaker features differs between synthetic\nand bonafide speech. Based on these analyzes, an SSD method based on temporal\nconsistency and distribution of speaker features is proposed. On one hand,\nmodeling the temporal consistency of intra-utterance speaker features can aid\nspeech anti-spoofing. On the other hand, distribution differences in\ninter-utterance speaker features can be utilized for SSD. The proposed method\noffers low computational complexity and performs well in both cross-dataset and\nsilence trimming scenarios.", "published": "2023-09-29 03:50:35", "link": "http://arxiv.org/abs/2309.16954v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Low-Resource Self-Supervised Learning with SSL-Enhanced TTS", "abstract": "Self-supervised learning (SSL) techniques have achieved remarkable results in\nvarious speech processing tasks. Nonetheless, a significant challenge remains\nin reducing the reliance on vast amounts of speech data for pre-training. This\npaper proposes to address this challenge by leveraging synthetic speech to\naugment a low-resource pre-training corpus. We construct a high-quality\ntext-to-speech (TTS) system with limited resources using SSL features and\ngenerate a large synthetic corpus for pre-training. Experimental results\ndemonstrate that our proposed approach effectively reduces the demand for\nspeech data by 90% with only slight performance degradation. To the best of our\nknowledge, this is the first work aiming to enhance low-resource\nself-supervised learning in speech processing.", "published": "2023-09-29 07:09:23", "link": "http://arxiv.org/abs/2309.17020v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "ReFlow-TTS: A Rectified Flow Model for High-fidelity Text-to-Speech", "abstract": "The diffusion models including Denoising Diffusion Probabilistic Models\n(DDPM) and score-based generative models have demonstrated excellent\nperformance in speech synthesis tasks. However, its effectiveness comes at the\ncost of numerous sampling steps, resulting in prolonged sampling time required\nto synthesize high-quality speech. This drawback hinders its practical\napplicability in real-world scenarios. In this paper, we introduce ReFlow-TTS,\na novel rectified flow based method for speech synthesis with high-fidelity.\nSpecifically, our ReFlow-TTS is simply an Ordinary Differential Equation (ODE)\nmodel that transports Gaussian distribution to the ground-truth Mel-spectrogram\ndistribution by straight line paths as much as possible. Furthermore, our\nproposed approach enables high-quality speech synthesis with a single sampling\nstep and eliminates the need for training a teacher model. Our experiments on\nLJSpeech Dataset show that our ReFlow-TTS method achieves the best performance\ncompared with other diffusion based models. And the ReFlow-TTS with one step\nsampling achieves competitive performance compared with existing one-step TTS\nmodels.", "published": "2023-09-29 08:40:06", "link": "http://arxiv.org/abs/2309.17056v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "LRPD: Large Replay Parallel Dataset", "abstract": "The latest research in the field of voice anti-spoofing (VAS) shows that deep\nneural networks (DNN) outperform classic approaches like GMM in the task of\npresentation attack detection. However, DNNs require a lot of data to converge,\nand still lack generalization ability. In order to foster the progress of\nneural network systems, we introduce a Large Replay Parallel Dataset (LRPD)\naimed for a detection of replay attacks. LRPD contains more than 1M utterances\ncollected by 19 recording devices in 17 various environments. We also provide\nan example training pipeline in PyTorch [1] and a baseline system, that\nachieves 0.28% Equal Error Rate (EER) on evaluation subset of LRPD and 11.91%\nEER on publicly available ASVpoof 2017 [2] eval set. These results show that\nmodel trained with LRPD dataset has a consistent performance on the fully\nunknown conditions. Our dataset is free for research purposes and hosted on\nGDrive. Baseline code and pre-trained models are available at GitHub.", "published": "2023-09-29 14:55:30", "link": "http://arxiv.org/abs/2309.17298v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improving Audio Captioning Models with Fine-grained Audio Features, Text\n  Embedding Supervision, and LLM Mix-up Augmentation", "abstract": "Automated audio captioning (AAC) aims to generate informative descriptions\nfor various sounds from nature and/or human activities. In recent years, AAC\nhas quickly attracted research interest, with state-of-the-art systems now\nrelying on a sequence-to-sequence (seq2seq) backbone powered by strong models\nsuch as Transformers. Following the macro-trend of applied machine learning\nresearch, in this work, we strive to improve the performance of seq2seq AAC\nmodels by extensively leveraging pretrained models and large language models\n(LLMs). Specifically, we utilize BEATs to extract fine-grained audio features.\nThen, we employ Instructor LLM to fetch text embeddings of captions, and infuse\ntheir language-modality knowledge into BEATs audio features via an auxiliary\nInfoNCE loss function. Moreover, we propose a novel data augmentation method\nthat uses ChatGPT to produce caption mix-ups (i.e., grammatical and compact\ncombinations of two captions) which, together with the corresponding audio\nmixtures, increase not only the amount but also the complexity and diversity of\ntraining data. During inference, we propose to employ nucleus sampling and a\nhybrid reranking algorithm, which has not been explored in AAC research.\nCombining our efforts, our model achieves a new state-of-the-art 32.6 SPIDEr-FL\nscore on the Clotho evaluation split, and wins the 2023 DCASE AAC challenge.", "published": "2023-09-29 15:57:46", "link": "http://arxiv.org/abs/2309.17352v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Style Transfer for Non-differentiable Audio Effects", "abstract": "Digital audio effects are widely used by audio engineers to alter the\nacoustic and temporal qualities of audio data. However, these effects can have\na large number of parameters which can make them difficult to learn for\nbeginners and hamper creativity for professionals. Recently, there have been a\nnumber of efforts to employ progress in deep learning to acquire the low-level\nparameter configurations of audio effects by minimising an objective function\nbetween an input and reference track, commonly referred to as style transfer.\nHowever, current approaches use inflexible black-box techniques or require that\nthe effects under consideration are implemented in an auto-differentiation\nframework. In this work, we propose a deep learning approach to audio\nproduction style matching which can be used with effects implemented in some of\nthe most widely used frameworks, requiring only that the parameters under\nconsideration have a continuous domain. Further, our method includes style\nmatching for various classes of effects, many of which are difficult or\nimpossible to be approximated closely using differentiable functions. We show\nthat our audio embedding approach creates logical encodings of timbral\ninformation, which can be used for a number of downstream tasks. Further, we\nperform a listening test which demonstrates that our approach is able to\nconvincingly style match a multi-band compressor effect.", "published": "2023-09-29 10:40:19", "link": "http://arxiv.org/abs/2309.17125v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "RTFS-Net: Recurrent Time-Frequency Modelling for Efficient Audio-Visual\n  Speech Separation", "abstract": "Audio-visual speech separation methods aim to integrate different modalities\nto generate high-quality separated speech, thereby enhancing the performance of\ndownstream tasks such as speech recognition. Most existing state-of-the-art\n(SOTA) models operate in the time domain. However, their overly simplistic\napproach to modeling acoustic features often necessitates larger and more\ncomputationally intensive models in order to achieve SOTA performance. In this\npaper, we present a novel time-frequency domain audio-visual speech separation\nmethod: Recurrent Time-Frequency Separation Network (RTFS-Net), which applies\nits algorithms on the complex time-frequency bins yielded by the Short-Time\nFourier Transform. We model and capture the time and frequency dimensions of\nthe audio independently using a multi-layered RNN along each dimension.\nFurthermore, we introduce a unique attention-based fusion technique for the\nefficient integration of audio and visual information, and a new mask\nseparation approach that takes advantage of the intrinsic spectral nature of\nthe acoustic features for a clearer separation. RTFS-Net outperforms the prior\nSOTA method in both inference speed and separation quality while reducing the\nnumber of parameters by 90% and MACs by 83%. This is the first time-frequency\ndomain audio-visual speech separation method to outperform all contemporary\ntime-domain counterparts.", "published": "2023-09-29 12:38:00", "link": "http://arxiv.org/abs/2309.17189v4", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Toward Universal Speech Enhancement for Diverse Input Conditions", "abstract": "The past decade has witnessed substantial growth of data-driven speech\nenhancement (SE) techniques thanks to deep learning. While existing approaches\nhave shown impressive performance in some common datasets, most of them are\ndesigned only for a single condition (e.g., single-channel, multi-channel, or a\nfixed sampling frequency) or only consider a single task (e.g., denoising or\ndereverberation). Currently, there is no universal SE approach that can\neffectively handle diverse input conditions with a single model. In this paper,\nwe make the first attempt to investigate this line of research. First, we\ndevise a single SE model that is independent of microphone channels, signal\nlengths, and sampling frequencies. Second, we design a universal SE benchmark\nby combining existing public corpora with multiple conditions. Our experiments\non a wide range of datasets show that the proposed single model can\nsuccessfully handle diverse conditions with strong performance.", "published": "2023-09-29 16:41:49", "link": "http://arxiv.org/abs/2309.17384v2", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "AV-CPL: Continuous Pseudo-Labeling for Audio-Visual Speech Recognition", "abstract": "Audio-visual speech contains synchronized audio and visual information that\nprovides cross-modal supervision to learn representations for both automatic\nspeech recognition (ASR) and visual speech recognition (VSR). We introduce\ncontinuous pseudo-labeling for audio-visual speech recognition (AV-CPL), a\nsemi-supervised method to train an audio-visual speech recognition (AVSR) model\non a combination of labeled and unlabeled videos with continuously regenerated\npseudo-labels. Our models are trained for speech recognition from audio-visual\ninputs and can perform speech recognition using both audio and visual\nmodalities, or only one modality. Our method uses the same audio-visual model\nfor both supervised training and pseudo-label generation, mitigating the need\nfor external speech recognition models to generate pseudo-labels. AV-CPL\nobtains significant improvements in VSR performance on the LRS3 dataset while\nmaintaining practical ASR and AVSR performance. Finally, using visual-only\nspeech data, our method is able to leverage unlabeled visual speech to improve\nVSR.", "published": "2023-09-29 16:57:21", "link": "http://arxiv.org/abs/2309.17395v1", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "GASS: Generalizing Audio Source Separation with Large-scale Data", "abstract": "Universal source separation targets at separating the audio sources of an\narbitrary mix, removing the constraint to operate on a specific domain like\nspeech or music. Yet, the potential of universal source separation is limited\nbecause most existing works focus on mixes with predominantly sound events, and\nsmall training datasets also limit its potential for supervised learning. Here,\nwe study a single general audio source separation (GASS) model trained to\nseparate speech, music, and sound events in a supervised fashion with a\nlarge-scale dataset. We assess GASS models on a diverse set of tasks. Our\nstrong in-distribution results show the feasibility of GASS models, and the\ncompetitive out-of-distribution performance in sound event and speech\nseparation shows its generalization abilities. Yet, it is challenging for GASS\nmodels to generalize for separating out-of-distribution cinematic and music\ncontent. We also fine-tune GASS models on each dataset and consistently\noutperform the ones without pre-training. All fine-tuned models (except the\nmusic separation one) obtain state-of-the-art results in their respective\nbenchmarks.", "published": "2023-09-29 21:02:07", "link": "http://arxiv.org/abs/2310.00140v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
