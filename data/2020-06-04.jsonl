{"title": "Personalizing Grammatical Error Correction: Adaptation to Proficiency\n  Level and L1", "abstract": "Grammar error correction (GEC) systems have become ubiquitous in a variety of\nsoftware applications, and have started to approach human-level performance for\nsome datasets. However, very little is known about how to efficiently\npersonalize these systems to the user's characteristics, such as their\nproficiency level and first language, or to emerging domains of text. We\npresent the first results on adapting a general-purpose neural GEC system to\nboth the proficiency level and the first language of a writer, using only a few\nthousand annotated sentences. Our study is the broadest of its kind, covering\nfive proficiency levels and twelve different languages, and comparing three\ndifferent adaptation scenarios: adapting to the proficiency level only, to the\nfirst language only, or to both aspects simultaneously. We show that tailoring\nto both scenarios achieves the largest performance improvement (3.6 F0.5)\nrelative to a strong baseline.", "published": "2020-06-04 15:47:29", "link": "http://arxiv.org/abs/2006.02964v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Linguists Who Use Probabilistic Models Love Them: Quantification in\n  Functional Distributional Semantics", "abstract": "Functional Distributional Semantics provides a computationally tractable\nframework for learning truth-conditional semantics from a corpus. Previous work\nin this framework has provided a probabilistic version of first-order logic,\nrecasting quantification as Bayesian inference. In this paper, I show how the\nprevious formulation gives trivial truth values when a precise quantifier is\nused with vague predicates. I propose an improved account, avoiding this\nproblem by treating a vague predicate as a distribution over precise\npredicates. I connect this account to recent work in the Rational Speech Acts\nframework on modelling generic quantification, and I extend this to modelling\ndonkey sentences. Finally, I explain how the generic quantifier can be both\npragmatically complex and yet computationally simpler than precise quantifiers.", "published": "2020-06-04 16:48:45", "link": "http://arxiv.org/abs/2006.03002v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Syntactic Search by Example", "abstract": "We present a system that allows a user to search a large linguistically\nannotated corpus using syntactic patterns over dependency graphs. In contrast\nto previous attempts to this effect, we introduce a light-weight query language\nthat does not require the user to know the details of the underlying syntactic\nrepresentations, and instead to query the corpus by providing an example\nsentence coupled with simple markup. Search is performed at an interactive\nspeed due to an efficient linguistic graph-indexing and retrieval engine. This\nallows for rapid exploration, development and refinement of syntax-based\nqueries. We demonstrate the system using queries over two corpora: the English\nwikipedia, and a collection of English pubmed abstracts. A demo of the\nwikipedia system is available at: https://allenai.github.io/spike", "published": "2020-06-04 16:59:01", "link": "http://arxiv.org/abs/2006.03010v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NewB: 200,000+ Sentences for Political Bias Detection", "abstract": "We present the Newspaper Bias Dataset (NewB), a text corpus of more than\n200,000 sentences from eleven news sources regarding Donald Trump. While\nprevious datasets have labeled sentences as either liberal or conservative,\nNewB covers the political views of eleven popular media sources, capturing more\nnuanced political viewpoints than a traditional binary classification system\ndoes. We train two state-of-the-art deep learning models to predict the news\nsource of a given sentence from eleven newspapers and find that a recurrent\nneural network achieved top-1, top-3, and top-5 accuracies of 33.3%, 61.4%, and\n77.6%, respectively, significantly outperforming a baseline logistic regression\nmodel's accuracies of 18.3%, 42.6%, and 60.8%. Using the news source label of\nsentences, we analyze the top n-grams with our model to gain meaningful insight\ninto the portrayal of Trump by media sources.We hope that the public release of\nour dataset will encourage further research in using natural language\nprocessing to analyze more complex political biases.\n  Our dataset is posted at https://github.com/JerryWeiAI/NewB .", "published": "2020-06-04 18:21:50", "link": "http://arxiv.org/abs/2006.03051v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Stopwords in Technical Language Processing", "abstract": "There are increasingly applications of natural language processing techniques\nfor information retrieval, indexing and topic modelling in the engineering\ncontexts. A standard component of such tasks is the removal of stopwords, which\nare uninformative components of the data. While researchers use readily\navailable stopword lists which are derived for general English language, the\ntechnical jargon of engineering fields contains their own highly frequent and\nuninformative words and there exists no standard stopword list for technical\nlanguage processing applications. Here we address this gap by rigorously\nidentifying generic, insignificant, uninformative stopwords in engineering\ntexts beyond the stopwords in general texts, based on the synthesis of\nalternative data-driven approaches, and curating a stopword list ready for\ntechnical language processing applications.", "published": "2020-06-04 03:52:59", "link": "http://arxiv.org/abs/2006.02633v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "M3P: Learning Universal Representations via Multitask Multilingual\n  Multimodal Pre-training", "abstract": "We present M3P, a Multitask Multilingual Multimodal Pre-trained model that\ncombines multilingual pre-training and multimodal pre-training into a unified\nframework via multitask pre-training. Our goal is to learn universal\nrepresentations that can map objects occurred in different modalities or texts\nexpressed in different languages into a common semantic space. In addition, to\nexplicitly encourage fine-grained alignment between images and non-English\nlanguages, we also propose Multimodal Code-switched Training (MCT) to combine\nmonolingual pre-training and multimodal pre-training via a code-switch\nstrategy. Experiments are performed on the multilingual image retrieval task\nacross two benchmark datasets, including MSCOCO and Multi30K. M3P can achieve\ncomparable results for English and new state-of-the-art results for non-English\nlanguages.", "published": "2020-06-04 03:54:29", "link": "http://arxiv.org/abs/2006.02635v4", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Enhanced back-translation for low resource neural machine translation\n  using self-training", "abstract": "Improving neural machine translation (NMT) models using the back-translations\nof the monolingual target data (synthetic parallel data) is currently the\nstate-of-the-art approach for training improved translation systems. The\nquality of the backward system - which is trained on the available parallel\ndata and used for the back-translation - has been shown in many studies to\naffect the performance of the final NMT model. In low resource conditions, the\navailable parallel data is usually not enough to train a backward model that\ncan produce the qualitative synthetic data needed to train a standard\ntranslation model. This work proposes a self-training strategy where the output\nof the backward model is used to improve the model itself through the forward\ntranslation technique. The technique was shown to improve baseline low resource\nIWSLT'14 English-German and IWSLT'15 English-Vietnamese backward translation\nmodels by 11.06 and 1.5 BLEUs respectively. The synthetic data generated by the\nimproved English-German backward model was used to train a forward model which\nout-performed another forward model trained using standard back-translation by\n2.7 BLEU.", "published": "2020-06-04 14:19:52", "link": "http://arxiv.org/abs/2006.02876v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CiwGAN and fiwGAN: Encoding information in acoustic data to model\n  lexical learning with Generative Adversarial Networks", "abstract": "How can deep neural networks encode information that corresponds to words in\nhuman speech into raw acoustic data? This paper proposes two neural network\narchitectures for modeling unsupervised lexical learning from raw acoustic\ninputs, ciwGAN (Categorical InfoWaveGAN) and fiwGAN (Featural InfoWaveGAN),\nthat combine a Deep Convolutional GAN architecture for audio data (WaveGAN;\narXiv:1705.07904) with an information theoretic extension of GAN -- InfoGAN\n(arXiv:1606.03657), and propose a new latent space structure that can model\nfeatural learning simultaneously with a higher level classification and allows\nfor a very low-dimension vector representation of lexical items. Lexical\nlearning is modeled as emergent from an architecture that forces a deep neural\nnetwork to output data such that unique information is retrievable from its\nacoustic outputs. The networks trained on lexical items from TIMIT learn to\nencode unique information corresponding to lexical items in the form of\ncategorical variables in their latent space. By manipulating these variables,\nthe network outputs specific lexical items. The network occasionally outputs\ninnovative lexical items that violate training data, but are linguistically\ninterpretable and highly informative for cognitive modeling and neural network\ninterpretability. Innovative outputs suggest that phonetic and phonological\nrepresentations learned by the network can be productively recombined and\ndirectly paralleled to productivity in human speech: a fiwGAN network trained\non `suit' and `dark' outputs innovative `start', even though it never saw\n`start' or even a [st] sequence in the training data. We also argue that\nsetting latent featural codes to values well beyond training range results in\nalmost categorical generation of prototypical lexical items and reveals\nunderlying values of each latent code.", "published": "2020-06-04 15:33:55", "link": "http://arxiv.org/abs/2006.02951v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Response to LiveBot: Generating Live Video Comments Based on Visual and\n  Textual Contexts", "abstract": "Live video commenting systems are an emerging feature of online video sites.\nRecently the Chinese video sharing platform Bilibili, has popularised a novel\ncaptioning system where user comments are displayed as streams of moving\nsubtitles overlaid on the video playback screen and broadcast to all viewers in\nreal-time. LiveBot was recently introduced as a novel Automatic Live Video\nCommenting (ALVC) application. This enables the automatic generation of live\nvideo comments from both the existing video stream and existing viewers\ncomments. In seeking to reproduce the baseline results reported in the original\nLivebot paper, we found differences between the reproduced results using the\nproject codebase and the numbers reported in the paper. Further examination of\nthis situation suggests that this may be caused by a number of small issues in\nthe project code, including a non-obvious overlap between the training and test\nsets. In this paper, we study these discrepancies in detail and propose an\nalternative baseline implementation as a reference for other researchers in\nthis field.", "published": "2020-06-04 17:16:22", "link": "http://arxiv.org/abs/2006.03022v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The SOFC-Exp Corpus and Neural Approaches to Information Extraction in\n  the Materials Science Domain", "abstract": "This paper presents a new challenging information extraction task in the\ndomain of materials science. We develop an annotation scheme for marking\ninformation on experiments related to solid oxide fuel cells in scientific\npublications, such as involved materials and measurement conditions. With this\npaper, we publish our annotation guidelines, as well as our SOFC-Exp corpus\nconsisting of 45 open-access scholarly articles annotated by domain experts. A\ncorpus and an inter-annotator agreement study demonstrate the complexity of the\nsuggested named entity recognition and slot filling tasks as well as high\nannotation quality. We also present strong neural-network based models for a\nvariety of tasks that can be addressed on the basis of our new data set. On all\ntasks, using BERT embeddings leads to large performance gains, but with\nincreasing task complexity, adding a recurrent neural network on top seems\nbeneficial. Our models will serve as competitive baselines in future work, and\nanalysis of their performance highlights difficult cases when modeling the data\nand suggests promising research directions.", "published": "2020-06-04 17:49:34", "link": "http://arxiv.org/abs/2006.03039v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SOLO: A Corpus of Tweets for Examining the State of Being Alone", "abstract": "The state of being alone can have a substantial impact on our lives, though\nexperiences with time alone diverge significantly among individuals.\nPsychologists distinguish between the concept of solitude, a positive state of\nvoluntary aloneness, and the concept of loneliness, a negative state of\ndissatisfaction with the quality of one's social interactions. Here, for the\nfirst time, we conduct a large-scale computational analysis to explore how the\nterms associated with the state of being alone are used in online language. We\npresent SOLO (State of Being Alone), a corpus of over 4 million tweets\ncollected with query terms 'solitude', 'lonely', and 'loneliness'. We use SOLO\nto analyze the language and emotions associated with the state of being alone.\nWe show that the term 'solitude' tends to co-occur with more positive,\nhigh-dominance words (e.g., enjoy, bliss) while the terms 'lonely' and\n'loneliness' frequently co-occur with negative, low-dominance words (e.g.,\nscared, depressed), which confirms the conceptual distinctions made in\npsychology. We also show that women are more likely to report on negative\nfeelings of being lonely as compared to men, and there are more teenagers among\nthe tweeters that use the word 'lonely' than among the tweeters that use the\nword 'solitude'.", "published": "2020-06-04 18:46:02", "link": "http://arxiv.org/abs/2006.03096v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Experiments on Paraphrase Identification Using Quora Question Pairs\n  Dataset", "abstract": "We modeled the Quora question pairs dataset to identify a similar question.\nThe dataset that we use is provided by Quora. The task is a binary\nclassification. We tried several methods and algorithms and different approach\nfrom previous works. For feature extraction, we used Bag of Words including\nCount Vectorizer, and Term Frequency-Inverse Document Frequency with unigram\nfor XGBoost and CatBoost. Furthermore, we also experimented with WordPiece\ntokenizer which improves the model performance significantly. We achieved up to\n97 percent accuracy. Code and Dataset.", "published": "2020-06-04 05:43:25", "link": "http://arxiv.org/abs/2006.02648v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Seq2Seq AI Chatbot with Attention Mechanism", "abstract": "Intelligent Conversational Agent development using Artificial Intelligence or\nMachine Learning technique is an interesting problem in the field of Natural\nLanguage Processing. With the rise of deep learning, these models were quickly\nreplaced by end to end trainable neural networks.", "published": "2020-06-04 10:54:43", "link": "http://arxiv.org/abs/2006.02767v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multi-talker ASR for an unknown number of sources: Joint training of\n  source counting, separation and ASR", "abstract": "Most approaches to multi-talker overlapped speech separation and recognition\nassume that the number of simultaneously active speakers is given, but in\nrealistic situations, it is typically unknown. To cope with this, we extend an\niterative speech extraction system with mechanisms to count the number of\nsources and combine it with a single-talker speech recognizer to form the first\nend-to-end multi-talker automatic speech recognition system for an unknown\nnumber of active speakers. Our experiments show very promising performance in\ncounting accuracy, source separation and speech recognition on simulated clean\nmixtures from WSJ0-2mix and WSJ0-3mix. Among others, we set a new\nstate-of-the-art word error rate on the WSJ0-2mix database. Furthermore, our\nsystem generalizes well to a larger number of speakers than it ever saw during\ntraining, as shown in experiments with the WSJ0-4mix database.", "published": "2020-06-04 11:25:50", "link": "http://arxiv.org/abs/2006.02786v3", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "CSTNet: Contrastive Speech Translation Network for Self-Supervised\n  Speech Representation Learning", "abstract": "More than half of the 7,000 languages in the world are in imminent danger of\ngoing extinct. Traditional methods of documenting language proceed by\ncollecting audio data followed by manual annotation by trained linguists at\ndifferent levels of granularity. This time consuming and painstaking process\ncould benefit from machine learning. Many endangered languages do not have any\northographic form but usually have speakers that are bi-lingual and trained in\na high resource language. It is relatively easy to obtain textual translations\ncorresponding to speech. In this work, we provide a multimodal machine learning\nframework for speech representation learning by exploiting the correlations\nbetween the two modalities namely speech and its corresponding text\ntranslation. Here, we construct a convolutional neural network audio encoder\ncapable of extracting linguistic representations from speech. The audio encoder\nis trained to perform a speech-translation retrieval task in a contrastive\nlearning framework. By evaluating the learned representations on a phone\nrecognition task, we demonstrate that linguistic representations emerge in the\naudio encoder's internal representations as a by-product of learning to perform\nthe retrieval task.", "published": "2020-06-04 12:21:48", "link": "http://arxiv.org/abs/2006.02814v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "End-to-End Speech-Translation with Knowledge Distillation: FBK@IWSLT2020", "abstract": "This paper describes FBK's participation in the IWSLT 2020 offline speech\ntranslation (ST) task. The task evaluates systems' ability to translate English\nTED talks audio into German texts. The test talks are provided in two versions:\none contains the data already segmented with automatic tools and the other is\nthe raw data without any segmentation. Participants can decide whether to work\non custom segmentation or not. We used the provided segmentation. Our system is\nan end-to-end model based on an adaptation of the Transformer for speech data.\nIts training process is the main focus of this paper and it is based on: i)\ntransfer learning (ASR pretraining and knowledge distillation), ii) data\naugmentation (SpecAugment, time stretch and synthetic data), iii) combining\nsynthetic and real data marked as different domains, and iv) multi-task\nlearning using the CTC loss. Finally, after the training with word-level\nknowledge distillation is complete, our ST models are fine-tuned using label\nsmoothed cross entropy. Our best model scored 29 BLEU on the MuST-C En-De test\nset, which is an excellent result compared to recent papers, and 23.7 BLEU on\nthe same data segmented with VAD, showing the need for researching solutions\naddressing this specific data condition.", "published": "2020-06-04 15:47:47", "link": "http://arxiv.org/abs/2006.02965v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Contextual RNN-T For Open Domain ASR", "abstract": "End-to-end (E2E) systems for automatic speech recognition (ASR), such as RNN\nTransducer (RNN-T) and Listen-Attend-Spell (LAS) blend the individual\ncomponents of a traditional hybrid ASR system - acoustic model, language model,\npronunciation model - into a single neural network. While this has some nice\nadvantages, it limits the system to be trained using only paired audio and\ntext. Because of this, E2E models tend to have difficulties with correctly\nrecognizing rare words that are not frequently seen during training, such as\nentity names. In this paper, we propose modifications to the RNN-T model that\nallow the model to utilize additional metadata text with the objective of\nimproving performance on these named entity words. We evaluate our approach on\nan in-house dataset sampled from de-identified public social media videos,\nwhich represent an open domain ASR task. By using an attention model and a\nbiasing model to leverage the contextual metadata that accompanies a video, we\nobserve a relative improvement of about 16% in Word Error Rate on Named\nEntities (WER-NE) for videos with related metadata.", "published": "2020-06-04 04:37:03", "link": "http://arxiv.org/abs/2006.03411v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "AP20-OLR Challenge: Three Tasks and Their Baselines", "abstract": "This paper introduces the fifth oriental language recognition (OLR) challenge\nAP20-OLR, which intends to improve the performance of language recognition\nsystems, along with APSIPA Annual Summit and Conference (APSIPA ASC). The data\nprofile, three tasks, the corresponding baselines, and the evaluation\nprinciples are introduced in this paper. The AP20-OLR challenge includes more\nlanguages, dialects and real-life data provided by Speechocean and the NSFC\nM2ASR project, and all the data is free for participants. The challenge this\nyear still focuses on practical and challenging problems, with three tasks: (1)\ncross-channel LID, (2) dialect identification and (3) noisy LID. Based on Kaldi\nand Pytorch, recipes for i-vector and x-vector systems are also conducted as\nbaselines for the three tasks. These recipes will be online-published, and\navailable for participants to configure LID systems. The baseline results on\nthe three tasks demonstrate that those tasks in this challenge are worth paying\nmore efforts to achieve better performance.", "published": "2020-06-04 16:29:21", "link": "http://arxiv.org/abs/2006.03473v4", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Affective Conditioning on Hierarchical Networks applied to Depression\n  Detection from Transcribed Clinical Interviews", "abstract": "In this work we propose a machine learning model for depression detection\nfrom transcribed clinical interviews. Depression is a mental disorder that\nimpacts not only the subject's mood but also the use of language. To this end\nwe use a Hierarchical Attention Network to classify interviews of depressed\nsubjects. We augment the attention layer of our model with a conditioning\nmechanism on linguistic features, extracted from affective lexica. Our analysis\nshows that individuals diagnosed with depression use affective language to a\ngreater extent than not-depressed. Our experiments show that external affective\ninformation improves the performance of the proposed architecture in the\nGeneral Psychotherapy Corpus and the DAIC-WoZ 2017 depression datasets,\nachieving state-of-the-art 71.6 and 68.6 F1 scores respectively.", "published": "2020-06-04 20:55:22", "link": "http://arxiv.org/abs/2006.08336v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Online End-to-End Neural Diarization with Speaker-Tracing Buffer", "abstract": "This paper proposes a novel online speaker diarization algorithm based on a\nfully supervised self-attention mechanism (SA-EEND). Online diarization\ninherently presents a speaker's permutation problem due to the possibility to\nassign speaker regions incorrectly across the recording. To circumvent this\ninconsistency, we proposed a speaker-tracing buffer mechanism that selects\nseveral input frames representing the speaker permutation information from\nprevious chunks and stores them in a buffer. These buffered frames are stacked\nwith the input frames in the current chunk and fed into a self-attention\nnetwork. Our method ensures consistent diarization outputs across the buffer\nand the current chunk by checking the correlation between their corresponding\noutputs. Additionally, we trained SA-EEND with variable chunk-sizes to mitigate\nthe mismatch between training and inference introduced by the speaker-tracing\nbuffer mechanism. Experimental results, including online SA-EEND and variable\nchunk-size, achieved DERs of 12.54% for CALLHOME and 20.77% for CSJ with 1.4s\nactual latency.", "published": "2020-06-04 02:25:07", "link": "http://arxiv.org/abs/2006.02616v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A study on more realistic room simulation for far-field keyword spotting", "abstract": "We investigate the impact of more realistic room simulation for training\nfar-field keyword spotting systems without fine-tuning on in-domain data. To\nthis end, we study the impact of incorporating the following factors in the\nroom impulse response (RIR) generation: air absorption, surface- and\nfrequency-dependent coefficients of real materials, and stochastic ray tracing.\nThrough an ablation study, a wake word task is used to measure the impact of\nthese factors in comparison with a ground-truth set of measured RIRs. On a\nhold-out set of re-recordings under clean and noisy far-field conditions, we\ndemonstrate up to $35.8\\%$ relative improvement over the commonly-used (single\nabsorption coefficient) image source method. Source code is made available in\nthe Pyroomacoustics package, allowing others to incorporate these techniques in\ntheir work.", "published": "2020-06-04 11:07:36", "link": "http://arxiv.org/abs/2006.02774v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "PJS: phoneme-balanced Japanese singing voice corpus", "abstract": "This paper presents a free Japanese singing voice corpus that can be used for\nhighly applicable and reproducible singing voice synthesis research. A singing\nvoice corpus helps develop singing voice synthesis, but existing corpora have\ntwo critical problems: data imbalance (singing voice corpora do not guarantee\nphoneme balance, unlike speaking-voice corpora) and copyright issues (cannot\nlegally share data). As a way to avoid these problems, we constructed a PJS\n(phoneme-balanced Japanese singing voice) corpus that guarantees phoneme\nbalance and is licensed with CC BY-SA 4.0, and we composed melodies using a\nphoneme-balanced speaking-voice corpus. This paper describes how we built the\ncorpus.", "published": "2020-06-04 15:41:00", "link": "http://arxiv.org/abs/2006.02959v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Application of Optimization and Simulation to Musical Composition that\n  Emerges Dynamically during Ensemble Singing Performance", "abstract": "This paper presents and tests a new approach to composing for ensemble\nsinging performance: reality opera. In the performance of such a composition,\nemotions of the singers are real and emerge as a consequence of their\ninteractions and reaction and to a dynamic narrative. This paper gives\nbackground and motivation for the form, based on three key concepts,\nincorporating the use of technology. Then proposed techniques for creating\nreality opera are instantiated in an example, which is performed and a\nbehavioral analysis done of performer reactions, leading to support for the\nfeasibility of the reality opera concept.", "published": "2020-06-04 15:29:56", "link": "http://arxiv.org/abs/2006.03471v1", "categories": ["cs.SD", "eess.AS", "00A65", "J.5"], "primary_category": "cs.SD"}
{"title": "Third DIHARD Challenge Evaluation Plan", "abstract": "This paper introduces the third DIHARD challenge, the third in a series of\nspeaker diarization challenges intended to improve the robustness of\ndiarization systems to variation in recording equipment, noise conditions, and\nconversational domain. The challenge comprises two tracks evaluating\ndiarization performance when starting from a reference speech segmentation\n(track 1) and diarization from raw audio scratch (track 2). We describe the\ntask, metrics, datasets, and evaluation protocol.", "published": "2020-06-04 22:23:10", "link": "http://arxiv.org/abs/2006.05815v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Attention and Encoder-Decoder based models for transforming articulatory\n  movements at different speaking rates", "abstract": "While speaking at different rates, articulators (like tongue, lips) tend to\nmove differently and the enunciations are also of different durations. In the\npast, affine transformation and DNN have been used to transform articulatory\nmovements from neutral to fast(N2F) and neutral to slow(N2S) speaking rates\n[1]. In this work, we improve over the existing transformation techniques by\nmodeling rate specific durations and their transformation using AstNet, an\nencoder-decoder framework with attention. In the current work, we propose an\nencoder-decoder architecture using LSTMs which generates smoother predicted\narticulatory trajectories. For modeling duration variations across speaking\nrates, we deploy attention network, which eliminates the needto align\ntrajectories in different rates using DTW. We performa phoneme specific\nduration analysis to examine how well duration is transformed using the\nproposed AstNet. As the range of articulatory motions is correlated with\nspeaking rate, we also analyze amplitude of the transformed articulatory\nmovements at different rates compared to their original counterparts, to\nexamine how well the proposed AstNet predicts the extent of articulatory\nmovements in N2F and N2S. We observe that AstNet could model both duration and\nextent of articulatory movements better than the existing transformation\ntechniques resulting in more accurate transformed articulatory trajectories.", "published": "2020-06-04 19:33:26", "link": "http://arxiv.org/abs/2006.03107v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
