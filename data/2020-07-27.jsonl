{"title": "Linguistic Taboos and Euphemisms in Nepali", "abstract": "Languages across the world have words, phrases, and behaviors -- the taboos\n-- that are avoided in public communication considering them as obscene or\ndisturbing to the social, religious, and ethical values of society. However,\npeople deliberately use these linguistic taboos and other language constructs\nto make hurtful, derogatory, and obscene comments. It is nearly impossible to\nconstruct a universal set of offensive or taboo terms because offensiveness is\ndetermined entirely by different factors such as socio-physical setting,\nspeaker-listener relationship, and word choices. In this paper, we present a\ndetailed corpus-based study of offensive language in Nepali. We identify and\ndescribe more than 18 different categories of linguistic offenses including\npolitics, religion, race, and sex. We discuss 12 common euphemisms such as\nsynonym, metaphor and circumlocution. In addition, we introduce a manually\nconstructed data set of over 1000 offensive and taboo terms popular among\ncontemporary speakers. This in-depth study of offensive language and resource\nwill provide a foundation for several downstream tasks such as offensive\nlanguage detection and language learning.", "published": "2020-07-27 18:25:01", "link": "http://arxiv.org/abs/2007.13798v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ULD@NUIG at SemEval-2020 Task 9: Generative Morphemes with an Attention\n  Model for Sentiment Analysis in Code-Mixed Text", "abstract": "Code mixing is a common phenomena in multilingual societies where people\nswitch from one language to another for various reasons. Recent advances in\npublic communication over different social media sites have led to an increase\nin the frequency of code-mixed usage in written language. In this paper, we\npresent the Generative Morphemes with Attention (GenMA) Model sentiment\nanalysis system contributed to SemEval 2020 Task 9 SentiMix. The system aims to\npredict the sentiments of the given English-Hindi code-mixed tweets without\nusing word-level language tags instead inferring this automatically using a\nmorphological model. The system is based on a novel deep neural network (DNN)\narchitecture, which has outperformed the baseline F1-score on the test data-set\nas well as the validation data-set. Our results can be found under the user\nname \"koustava\" on the \"Sentimix Hindi English\" page", "published": "2020-07-27 23:58:54", "link": "http://arxiv.org/abs/2008.01545v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Next word prediction based on the N-gram model for Kurdish Sorani and\n  Kurmanji", "abstract": "Next word prediction is an input technology that simplifies the process of\ntyping by suggesting the next word to a user to select, as typing in a\nconversation consumes time. A few previous studies have focused on the Kurdish\nlanguage, including the use of next word prediction. However, the lack of a\nKurdish text corpus presents a challenge. Moreover, the lack of a sufficient\nnumber of N-grams for the Kurdish language, for instance, five grams, is the\nreason for the rare use of next Kurdish word prediction. Furthermore, the\nimproper display of several Kurdish letters in the Rstudio software is another\nproblem. This paper provides a Kurdish corpus, creates five, and presents a\nunique research work on next word prediction for Kurdish Sorani and Kurmanji.\nThe N-gram model has been used for next word prediction to reduce the amount of\ntime while typing in the Kurdish language. In addition, little work has been\nconducted on next Kurdish word prediction; thus, the N-gram model is utilized\nto suggest text accurately. To do so, R programming and RStudio are used to\nbuild the application. The model is 96.3% accurate.", "published": "2020-07-27 20:45:13", "link": "http://arxiv.org/abs/2008.01546v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Public Sentiment Toward Solar Energy: Opinion Mining of Twitter Using a\n  Transformer-Based Language Model", "abstract": "Public acceptance and support for renewable energy are important determinants\nof renewable energy policies and market conditions. This paper examines public\nsentiment toward solar energy in the United States using data from Twitter, a\nmicro-blogging platform in which people post messages, known as tweets. We\nfiltered tweets specific to solar energy and performed a classification task\nusing Robustly optimized Bidirectional Encoder Representations from\nTransformers (RoBERTa). Analyzing 71,262 tweets during the period of late\nJanuary to early July 2020, we find public sentiment varies significantly\nacross states. Within the study period, the Northeastern U.S. region shows more\npositive sentiment toward solar energy than did the Southern U.S. region. Solar\nradiation does not correlate to variation in solar sentiment across states. We\nalso find that public sentiment toward solar correlates to renewable energy\npolicy and market conditions, specifically, Renewable Portfolio Standards (RPS)\ntargets, customer-friendly net metering policies, and a mature solar market.", "published": "2020-07-27 04:31:18", "link": "http://arxiv.org/abs/2007.13306v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "NAYEL at SemEval-2020 Task 12: TF/IDF-Based Approach for Automatic\n  Offensive Language Detection in Arabic Tweets", "abstract": "In this paper, we present the system submitted to \"SemEval-2020 Task 12\". The\nproposed system aims at automatically identify the Offensive Language in Arabic\nTweets. A machine learning based approach has been used to design our system.\nWe implemented a linear classifier with Stochastic Gradient Descent (SGD) as\noptimization algorithm. Our model reported 84.20%, 81.82% f1-score on\ndevelopment set and test set respectively. The best performed system and the\nsystem in the last rank reported 90.17% and 44.51% f1-score on test set\nrespectively.", "published": "2020-07-27 07:44:00", "link": "http://arxiv.org/abs/2007.13339v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Large Scale Subject Category Classification of Scholarly Papers with\n  Deep Attentive Neural Networks", "abstract": "Subject categories of scholarly papers generally refer to the knowledge\ndomain(s) to which the papers belong, examples being computer science or\nphysics. Subject category information can be used for building faceted search\nfor digital library search engines. This can significantly assist users in\nnarrowing down their search space of relevant documents. Unfortunately, many\nacademic papers do not have such information as part of their metadata.\nExisting methods for solving this task usually focus on unsupervised learning\nthat often relies on citation networks. However, a complete list of papers\nciting the current paper may not be readily available. In particular, new\npapers that have few or no citations cannot be classified using such methods.\nHere, we propose a deep attentive neural network (DANN) that classifies\nscholarly papers using only their abstracts. The network is trained using 9\nmillion abstracts from Web of Science (WoS). We also use the WoS schema that\ncovers 104 subject categories. The proposed network consists of two\nbi-directional recurrent neural networks followed by an attention layer. We\ncompare our model against baselines by varying the architecture and text\nrepresentation. Our best model achieves micro-F1 measure of 0.76 with F1 of\nindividual subject categories ranging from 0.50-0.95. The results showed the\nimportance of retraining word embedding models to maximize the vocabulary\noverlap and the effectiveness of the attention mechanism. The combination of\nword vectors with TFIDF outperforms character and sentence level embedding\nmodels. We discuss imbalanced samples and overlapping categories and suggest\npossible strategies for mitigation. We also determine the subject category\ndistribution in CiteSeerX by classifying a random sample of one million\nacademic papers.", "published": "2020-07-27 19:42:42", "link": "http://arxiv.org/abs/2007.13826v1", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "Characterizing the Effect of Sentence Context on Word Meanings: Mapping\n  Brain to Behavior", "abstract": "Semantic feature models have become a popular tool for prediction and\ninterpretation of fMRI data. In particular, prior work has shown that\ndifferences in the fMRI patterns in sentence reading can be explained by\ncontext-dependent changes in the semantic feature representations of the words.\nHowever, whether the subjects are aware of such changes and agree with them has\nbeen an open question. This paper aims to answer this question through a\nhuman-subject study. Subjects were asked to judge how the word change from\ntheir generic meaning when the words were used in specific sentences. The\njudgements were consistent with the model predictions well above chance. Thus,\nthe results support the hypothesis that word meaning change systematically\ndepending on sentence context.", "published": "2020-07-27 20:12:30", "link": "http://arxiv.org/abs/2007.13840v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluating the reliability of acoustic speech embeddings", "abstract": "Speech embeddings are fixed-size acoustic representations of variable-length\nspeech sequences. They are increasingly used for a variety of tasks ranging\nfrom information retrieval to unsupervised term discovery and speech\nsegmentation. However, there is currently no clear methodology to compare or\noptimise the quality of these embeddings in a task-neutral way. Here, we\nsystematically compare two popular metrics, ABX discrimination and Mean Average\nPrecision (MAP), on 5 languages across 17 embedding methods, ranging from\nsupervised to fully unsupervised, and using different loss functions\n(autoencoders, correspondence autoencoders, siamese). Then we use the ABX and\nMAP to predict performances on a new downstream task: the unsupervised\nestimation of the frequencies of speech segments in a given corpus. We find\nthat overall, ABX and MAP correlate with one another and with frequency\nestimation. However, substantial discrepancies appear in the fine-grained\ndistinctions across languages and/or embedding methods. This makes it\nunrealistic at present to propose a task-independent silver bullet method for\ncomputing the intrinsic quality of speech embeddings. There is a need for more\ndetailed analysis of the metrics currently used to evaluate such embeddings.", "published": "2020-07-27 13:24:09", "link": "http://arxiv.org/abs/2007.13542v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Efficient minimum word error rate training of RNN-Transducer for\n  end-to-end speech recognition", "abstract": "In this work, we propose a novel and efficient minimum word error rate (MWER)\ntraining method for RNN-Transducer (RNN-T). Unlike previous work on this topic,\nwhich performs on-the-fly limited-size beam-search decoding and generates\nalignment scores for expected edit-distance computation, in our proposed\nmethod, we re-calculate and sum scores of all the possible alignments for each\nhypothesis in N-best lists. The hypothesis probability scores and\nback-propagated gradients are calculated efficiently using the forward-backward\nalgorithm. Moreover, the proposed method allows us to decouple the decoding and\ntraining processes, and thus we can perform offline parallel-decoding and MWER\ntraining for each subset iteratively. Experimental results show that this\nproposed semi-on-the-fly method can speed up the on-the-fly method by 6 times\nand result in a similar WER improvement (3.6%) over a baseline RNN-T model. The\nproposed MWER training can also effectively reduce high-deletion errors (9.2%\nWER-reduction) introduced by RNN-T models when EOS is added for endpointer.\nFurther improvement can be achieved if we use a proposed RNN-T rescoring method\nto re-rank hypotheses and use external RNN-LM to perform additional rescoring.\nThe best system achieves a 5% relative improvement on an English test-set of\nreal far-field recordings and a 11.6% WER reduction on music-domain utterances.", "published": "2020-07-27 18:33:35", "link": "http://arxiv.org/abs/2007.13802v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Active Learning for Video Description With Cluster-Regularized Ensemble\n  Ranking", "abstract": "Automatic video captioning aims to train models to generate text descriptions\nfor all segments in a video, however, the most effective approaches require\nlarge amounts of manual annotation which is slow and expensive. Active learning\nis a promising way to efficiently build a training set for video captioning\ntasks while reducing the need to manually label uninformative examples. In this\nwork we both explore various active learning approaches for automatic video\ncaptioning and show that a cluster-regularized ensemble strategy provides the\nbest active learning approach to efficiently gather training sets for video\ncaptioning. We evaluate our approaches on the MSR-VTT and LSMDC datasets using\nboth transformer and LSTM based captioning models and show that our novel\nstrategy can achieve high performance while using up to 60% fewer training data\nthan the strong state of the art baselines.", "published": "2020-07-27 23:52:41", "link": "http://arxiv.org/abs/2007.13913v3", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "MUSE2020 challenge report", "abstract": "This paper is a brief report for MUSE2020 challenge. We present our solution\nfor Muse-Wild sub challenge. The aim of this challenge is to investigate\nsentiment analysis method in real-world situation. Our solutions achieve the\nbest CCC performance of 0.4670, 0.3571 for arousal, and valence respectively on\nthe challenge validation set, which outperforms the baseline system with\ncorresponding CCC of 0.3078 and 1506.", "published": "2020-07-27 04:39:45", "link": "http://arxiv.org/abs/2009.14059v1", "categories": ["cs.MM", "eess.AS"], "primary_category": "cs.MM"}
{"title": "On the Use of Audio Fingerprinting Features for Speech Enhancement with\n  Generative Adversarial Network", "abstract": "The advent of learning-based methods in speech enhancement has revived the\nneed for robust and reliable training features that can compactly represent\nspeech signals while preserving their vital information. Time-frequency domain\nfeatures, such as the Short-Term Fourier Transform (STFT) and Mel-Frequency\nCepstral Coefficients (MFCC), are preferred in many approaches. While the MFCC\nprovide for a compact representation, they ignore the dynamics and distribution\nof energy in each mel-scale subband. In this work, a speech enhancement system\nbased on Generative Adversarial Network (GAN) is implemented and tested with a\ncombination of Audio FingerPrinting (AFP) features obtained from the MFCC and\nthe Normalized Spectral Subband Centroids (NSSC). The NSSC capture the\nlocations of speech formants and complement the MFCC in a crucial way. In\nexperiments with diverse speakers and noise types, GAN-based speech enhancement\nwith the proposed AFP feature combination achieves the best objective\nperformance while reducing memory requirements and training time.", "published": "2020-07-27 00:44:16", "link": "http://arxiv.org/abs/2007.13258v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Analysis of Emotional Content in Indian Political Speeches", "abstract": "Emotions play an essential role in public speaking. The emotional content of\nspeech has the power to influence minds. As such, we present an analysis of the\nemotional content of politicians speech in the Indian political scenario. We\ninvestigate the emotional content present in the speeches of politicians using\nan Attention based CNN+LSTM network. Experimental evaluations on a dataset of\neight Indian politicians shows how politicians incorporate emotions in their\nspeeches to strike a chord with the masses. An analysis of the voting share\nreceived along with victory margin and their relation to emotional content in\nspeech of the politicians is also presented.", "published": "2020-07-27 07:00:46", "link": "http://arxiv.org/abs/2007.13325v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Self-Attentive Multi-Layer Aggregation with Feature Recalibration and\n  Normalization for End-to-End Speaker Verification System", "abstract": "One of the most important parts of an end-to-end speaker verification system\nis the speaker embedding generation. In our previous paper, we reported that\nshortcut connections-based multi-layer aggregation improves the\nrepresentational power of the speaker embedding. However, the number of model\nparameters is relatively large and the unspecified variations increase in the\nmulti-layer aggregation. Therefore, we propose a self-attentive multi-layer\naggregation with feature recalibration and normalization for end-to-end speaker\nverification system. To reduce the number of model parameters, the ResNet,\nwhich scaled channel width and layer depth, is used as a baseline. To control\nthe variability in the training, a self-attention mechanism is applied to\nperform the multi-layer aggregation with dropout regularizations and batch\nnormalizations. Then, a feature recalibration layer is applied to the\naggregated feature using fully-connected layers and nonlinear activation\nfunctions. Deep length normalization is also used on a recalibrated feature in\nthe end-to-end training process. Experimental results using the VoxCeleb1\nevaluation dataset showed that the performance of the proposed methods was\ncomparable to that of state-of-the-art models (equal error rate of 4.95% and\n2.86%, using the VoxCeleb1 and VoxCeleb2 training datasets, respectively).", "published": "2020-07-27 08:10:46", "link": "http://arxiv.org/abs/2007.13350v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Self-Supervised Contrastive Learning for Unsupervised Phoneme\n  Segmentation", "abstract": "We propose a self-supervised representation learning model for the task of\nunsupervised phoneme boundary detection. The model is a convolutional neural\nnetwork that operates directly on the raw waveform. It is optimized to identify\nspectral changes in the signal using the Noise-Contrastive Estimation\nprinciple. At test time, a peak detection algorithm is applied over the model\noutputs to produce the final boundaries. As such, the proposed model is trained\nin a fully unsupervised manner with no manual annotations in the form of target\nboundaries nor phonetic transcriptions. We compare the proposed approach to\nseveral unsupervised baselines using both TIMIT and Buckeye corpora. Results\nsuggest that our approach surpasses the baseline models and reaches\nstate-of-the-art performance on both data sets. Furthermore, we experimented\nwith expanding the training set with additional examples from the Librispeech\ncorpus. We evaluated the resulting model on distributions and languages that\nwere not seen during the training phase (English, Hebrew and German) and showed\nthat utilizing additional untranscribed data is beneficial for model\nperformance.", "published": "2020-07-27 12:10:21", "link": "http://arxiv.org/abs/2007.13465v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Receptive-Field Regularized CNNs for Music Classification and Tagging", "abstract": "Convolutional Neural Networks (CNNs) have been successfully used in various\nMusic Information Retrieval (MIR) tasks, both as end-to-end models and as\nfeature extractors for more complex systems. However, the MIR field is still\ndominated by the classical VGG-based CNN architecture variants, often in\ncombination with more complex modules such as attention, and/or techniques such\nas pre-training on large datasets. Deeper models such as ResNet -- which\nsurpassed VGG by a large margin in other domains -- are rarely used in MIR. One\nof the main reasons for this, as we will show, is the lack of generalization of\ndeeper CNNs in the music domain. In this paper, we present a principled way to\nmake deep architectures like ResNet competitive for music-related tasks, based\non well-designed regularization strategies. In particular, we analyze the\nrecently introduced Receptive-Field Regularization and Shake-Shake, and show\nthat they significantly improve the generalization of deep CNNs on\nmusic-related tasks, and that the resulting deep CNNs can outperform current\nmore complex models such as CNNs augmented with pre-training and attention. We\ndemonstrate this on two different MIR tasks and two corresponding datasets,\nthus offering our deep regularized CNNs as a new baseline for these datasets,\nwhich can also be used as a feature-extracting module in future, more complex\napproaches.", "published": "2020-07-27 12:48:12", "link": "http://arxiv.org/abs/2007.13503v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "From Sound Representation to Model Robustness", "abstract": "In this paper, we investigate the impact of different standard environmental\nsound representations (spectrograms) on the recognition performance and\nadversarial attack robustness of a victim residual convolutional neural\nnetwork. Averaged over various experiments on three benchmarking environmental\nsound datasets, we found the ResNet-18 model outperforms other deep learning\narchitectures such as GoogLeNet and AlexNet both in terms of classification\naccuracy and the number of training parameters. Therefore we set this model as\nour front-end classifier for subsequent investigations. Herein, we measure the\nimpact of different settings required for generating more informative\nmel-frequency cepstral coefficient (MFCC), short-time Fourier transform (STFT),\nand discrete wavelet transform (DWT) representations on our front-end model.\nThis measurement involves comparing the classification performance over the\nadversarial robustness. On the balance of average budgets allocated by\nadversary and the cost of attack, we demonstrate an inverse relationship\nbetween recognition accuracy and model robustness against six attack\nalgorithms. Moreover, our experimental results show that while the ResNet-18\nmodel trained on DWT spectrograms achieves the highest recognition accuracy,\nattacking this model is relatively more costly for the adversary compared to\nother 2D representations.", "published": "2020-07-27 17:30:49", "link": "http://arxiv.org/abs/2007.13703v3", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Semi-Supervised Learning with Data Augmentation for End-to-End ASR", "abstract": "In this paper, we apply Semi-Supervised Learning (SSL) along with Data\nAugmentation (DA) for improving the accuracy of End-to-End ASR. We focus on the\nconsistency regularization principle, which has been successfully applied to\nimage classification tasks, and present sequence-to-sequence (seq2seq) versions\nof the FixMatch and Noisy Student algorithms. Specifically, we generate the\npseudo labels for the unlabeled data on-the-fly with a seq2seq model after\nperturbing the input features with DA. We also propose soft label variants of\nboth algorithms to cope with pseudo label errors, showing further performance\nimprovements. We conduct SSL experiments on a conversational speech data set\nwith 1.9kh manually transcribed training data, using only 25% of the original\nlabels (475h labeled data). In the result, the Noisy Student algorithm with\nsoft labels and consistency regularization achieves 10.4% word error rate (WER)\nreduction when adding 475h of unlabeled data, corresponding to a recovery rate\nof 92%. Furthermore, when iteratively adding 950h more unlabeled data, our best\nSSL performance is within 5% WER increase compared to using the full labeled\ntraining set (recovery rate: 78%).", "published": "2020-07-27 21:24:52", "link": "http://arxiv.org/abs/2007.13876v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "openXDATA: A Tool for Multi-Target Data Generation and Missing Label\n  Completion", "abstract": "A common problem in machine learning is to deal with datasets with disjoint\nlabel spaces and missing labels. In this work, we introduce the openXDATA tool\nthat completes the missing labels in partially labelled or unlabelled datasets\nin order to generate multi-target data with labels in the joint label space of\nthe datasets. To this end, we designed and implemented the cross-data label\ncompletion (CDLC) algorithm that uses a multi-task shared-hidden-layer DNN to\niteratively complete the sparse label matrix of the instances from the\ndifferent datasets. We apply the new tool to estimate labels across four\nemotion datasets: one labeled with discrete emotion categories (e.g., happy,\nsad, angry), one labeled with continuous values along arousal and valence\ndimensions, one with both kinds of labels, and one unlabeled. Testing with\ndrop-out of true labels, we show the ability to estimate both categories and\ncontinuous labels for all of the datasets, at rates that approached the ground\ntruth values. openXDATA is available under the GNU General Public License from\nhttps://github.com/fweninger/openXDATA.", "published": "2020-07-27 22:05:53", "link": "http://arxiv.org/abs/2007.13889v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Noisy Agents: Self-supervised Exploration by Predicting Auditory Events", "abstract": "Humans integrate multiple sensory modalities (e.g. visual and audio) to build\na causal understanding of the physical world. In this work, we propose a novel\ntype of intrinsic motivation for Reinforcement Learning (RL) that encourages\nthe agent to understand the causal effect of its actions through auditory event\nprediction. First, we allow the agent to collect a small amount of acoustic\ndata and use K-means to discover underlying auditory event clusters. We then\ntrain a neural network to predict the auditory events and use the prediction\nerrors as intrinsic rewards to guide RL exploration. Experimental results on\nAtari games show that our new intrinsic motivation significantly outperforms\nseveral state-of-the-art baselines. We further visualize our noisy agents'\nbehavior in a physics environment and demonstrate that our newly designed\nintrinsic reward leads to the emergence of physical interaction behaviors (e.g.\ncontact with objects).", "published": "2020-07-27 17:59:08", "link": "http://arxiv.org/abs/2007.13729v1", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
