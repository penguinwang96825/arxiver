{"title": "What Changed Your Mind: The Roles of Dynamic Topics and Discourse in\n  Argumentation Process", "abstract": "In our world with full of uncertainty, debates and argumentation contribute\nto the progress of science and society. Despite of the increasing attention to\ncharacterize human arguments, most progress made so far focus on the debate\noutcome, largely ignoring the dynamic patterns in argumentation processes. This\npaper presents a study that automatically analyzes the key factors in argument\npersuasiveness, beyond simply predicting who will persuade whom. Specifically,\nwe propose a novel neural model that is able to dynamically track the changes\nof latent topics and discourse in argumentative conversations, allowing the\ninvestigation of their roles in influencing the outcomes of persuasion.\nExtensive experiments have been conducted on argumentative conversations on\nboth social media and supreme court. The results show that our model\noutperforms state-of-the-art models in identifying persuasive arguments via\nexplicitly exploring dynamic factors of topic and discourse. We further analyze\nthe effects of topics and discourse on persuasiveness, and find that they are\nboth useful - topics provide concrete evidence while superior discourse styles\nmay bias participants, especially in social media arguments. In addition, we\ndraw some findings from our empirical results, which will help people better\nengage in future persuasive conversations.", "published": "2020-02-10 04:27:48", "link": "http://arxiv.org/abs/2002.03536v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Study of Human Summaries of Scientific Articles", "abstract": "Researchers and students face an explosion of newly published papers which\nmay be relevant to their work. This led to a trend of sharing human summaries\nof scientific papers. We analyze the summaries shared in one of these platforms\nShortscience.org. The goal is to characterize human summaries of scientific\npapers, and use some of the insights obtained to improve and adapt existing\nautomatic summarization systems to the domain of scientific papers.", "published": "2020-02-10 08:53:27", "link": "http://arxiv.org/abs/2002.03604v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Discourse Segmentation: an evaluation in French", "abstract": "In this article, we describe some discursive segmentation methods as well as\na preliminary evaluation of the segmentation quality. Although our experiment\nwere carried for documents in French, we have developed three discursive\nsegmentation models solely based on resources simultaneously available in\nseveral languages: marker lists and a statistic POS labeling. We have also\ncarried out automatic evaluations of these systems against the Annodis corpus,\nwhich is a manually annotated reference. The results obtained are very\nencouraging.", "published": "2020-02-10 21:35:39", "link": "http://arxiv.org/abs/2002.04095v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Alignment of Contextual Word Representations", "abstract": "We propose procedures for evaluating and strengthening contextual embedding\nalignment and show that they are useful in analyzing and improving multilingual\nBERT. In particular, after our proposed alignment procedure, BERT exhibits\nsignificantly improved zero-shot performance on XNLI compared to the base\nmodel, remarkably matching pseudo-fully-supervised translate-train models for\nBulgarian and Greek. Further, to measure the degree of alignment, we introduce\na contextual version of word retrieval and show that it correlates well with\ndownstream zero-shot transfer. Using this word retrieval task, we also analyze\nBERT and find that it exhibits systematic deficiencies, e.g. worse alignment\nfor open-class parts-of-speech and word pairs written in different scripts,\nthat are corrected by the alignment procedure. These results support contextual\nalignment as a useful concept for understanding large multilingual pre-trained\nmodels.", "published": "2020-02-10 03:27:21", "link": "http://arxiv.org/abs/2002.03518v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Novel Kuhnian Ontology for Epistemic Classification of STM Scholarly\n  Articles", "abstract": "Thomas Kuhn proposed his paradigmatic view of scientific discovery five\ndecades ago. The concept of paradigm has not only explained the progress of\nscience, but has also become the central epistemic concept among STM\nscientists. Here, we adopt the principles of Kuhnian philosophy to construct a\nnovel ontology aims at classifying and evaluating the impact of STM scholarly\narticles. First, we explain how the Kuhnian cycle of science describes research\nat different epistemic stages. Second, we show how the Kuhnian cycle could be\nreconstructed into modular ontologies which classify scholarly articles\naccording to their contribution to paradigm-centred knowledge. The proposed\nontology and its scenarios are discussed. To the best of the authors knowledge,\nthis is the first attempt for creating an ontology for describing scholarly\narticles based on the Kuhnian paradigmatic view of science.", "published": "2020-02-10 04:00:07", "link": "http://arxiv.org/abs/2002.03531v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Automating App Review Response Generation", "abstract": "Previous studies showed that replying to a user review usually has a positive\neffect on the rating that is given by the user to the app. For example, Hassan\net al. found that responding to a review increases the chances of a user\nupdating their given rating by up to six times compared to not responding. To\nalleviate the labor burden in replying to the bulk of user reviews, developers\nusually adopt a template-based strategy where the templates can express\nappreciation for using the app or mention the company email address for users\nto follow up. However, reading a large number of user reviews every day is not\nan easy task for developers. Thus, there is a need for more automation to help\ndevelopers respond to user reviews.\n  Addressing the aforementioned need, in this work we propose a novel approach\nRRGen that automatically generates review responses by learning knowledge\nrelations between reviews and their responses. RRGen explicitly incorporates\nreview attributes, such as user rating and review length, and learns the\nrelations between reviews and corresponding responses in a supervised way from\nthe available training data. Experiments on 58 apps and 309,246 review-response\npairs highlight that RRGen outperforms the baselines by at least 67.4% in terms\nof BLEU-4 (an accuracy measure that is widely used to evaluate dialogue\nresponse generation systems). Qualitative analysis also confirms the\neffectiveness of RRGen in generating relevant and accurate responses.", "published": "2020-02-10 05:23:38", "link": "http://arxiv.org/abs/2002.03552v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "A Probabilistic Formulation of Unsupervised Text Style Transfer", "abstract": "We present a deep generative model for unsupervised text style transfer that\nunifies previously proposed non-generative techniques. Our probabilistic\napproach models non-parallel data from two domains as a partially observed\nparallel corpus. By hypothesizing a parallel latent sequence that generates\neach observed sequence, our model learns to transform sequences from one domain\nto another in a completely unsupervised fashion. In contrast with traditional\ngenerative sequence models (e.g. the HMM), our model makes few assumptions\nabout the data it generates: it uses a recurrent language model as a prior and\nan encoder-decoder as a transduction distribution. While computation of\nmarginal data likelihood is intractable in this model class, we show that\namortized variational inference admits a practical surrogate. Further, by\ndrawing connections between our variational objective and other recent\nunsupervised style transfer and machine translation techniques, we show how our\nprobabilistic view can unify some known non-generative objectives such as\nbacktranslation and adversarial loss. Finally, we demonstrate the effectiveness\nof our method on a wide range of unsupervised style transfer tasks, including\nsentiment transfer, formality transfer, word decipherment, author imitation,\nand related language translation. Across all style transfer tasks, our approach\nyields substantial gains over state-of-the-art non-generative baselines,\nincluding the state-of-the-art unsupervised machine translation techniques that\nour approach generalizes. Further, we conduct experiments on a standard\nunsupervised machine translation task and find that our unified approach\nmatches the current state-of-the-art.", "published": "2020-02-10 16:20:49", "link": "http://arxiv.org/abs/2002.03912v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "REALM: Retrieval-Augmented Language Model Pre-Training", "abstract": "Language model pre-training has been shown to capture a surprising amount of\nworld knowledge, crucial for NLP tasks such as question answering. However,\nthis knowledge is stored implicitly in the parameters of a neural network,\nrequiring ever-larger networks to cover more facts.\n  To capture knowledge in a more modular and interpretable way, we augment\nlanguage model pre-training with a latent knowledge retriever, which allows the\nmodel to retrieve and attend over documents from a large corpus such as\nWikipedia, used during pre-training, fine-tuning and inference. For the first\ntime, we show how to pre-train such a knowledge retriever in an unsupervised\nmanner, using masked language modeling as the learning signal and\nbackpropagating through a retrieval step that considers millions of documents.\n  We demonstrate the effectiveness of Retrieval-Augmented Language Model\npre-training (REALM) by fine-tuning on the challenging task of Open-domain\nQuestion Answering (Open-QA). We compare against state-of-the-art models for\nboth explicit and implicit knowledge storage on three popular Open-QA\nbenchmarks, and find that we outperform all previous methods by a significant\nmargin (4-16% absolute accuracy), while also providing qualitative benefits\nsuch as interpretability and modularity.", "published": "2020-02-10 18:40:59", "link": "http://arxiv.org/abs/2002.08909v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Localized Flood DetectionWith Minimal Labeled Social Media Data Using\n  Transfer Learning", "abstract": "Social media generates an enormous amount of data on a daily basis but it is\nvery challenging to effectively utilize the data without annotating or labeling\nit according to the target application. We investigate the problem of localized\nflood detection using the social sensing model (Twitter) in order to provide an\nefficient, reliable and accurate flood text classification model with minimal\nlabeled data. This study is important since it can immensely help in providing\nthe flood-related updates and notifications to the city officials for emergency\ndecision making, rescue operations, and early warnings, etc. We propose to\nperform the text classification using the inductive transfer learning method\ni.e pre-trained language model ULMFiT and fine-tune it in order to effectively\nclassify the flood-related feeds in any new location. Finally, we show that\nusing very little new labeled data in the target domain we can successfully\nbuild an efficient and high performing model for flood detection and analysis\nwith human-generated facts and observations from Twitter.", "published": "2020-02-10 20:17:34", "link": "http://arxiv.org/abs/2003.04973v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "NPLDA: A Deep Neural PLDA Model for Speaker Verification", "abstract": "The state-of-art approach for speaker verification consists of a neural\nnetwork based embedding extractor along with a backend generative model such as\nthe Probabilistic Linear Discriminant Analysis (PLDA). In this work, we propose\na neural network approach for backend modeling in speaker recognition. The\nlikelihood ratio score of the generative PLDA model is posed as a\ndiscriminative similarity function and the learnable parameters of the score\nfunction are optimized using a verification cost. The proposed model, termed as\nneural PLDA (NPLDA), is initialized using the generative PLDA model parameters.\nThe loss function for the NPLDA model is an approximation of the minimum\ndetection cost function (DCF). The speaker recognition experiments using the\nNPLDA model are performed on the speaker verificiation task in the VOiCES\ndatasets as well as the SITW challenge dataset. In these experiments, the NPLDA\nmodel optimized using the proposed loss function improves significantly over\nthe state-of-art PLDA based speaker verification system.", "published": "2020-02-10 05:47:35", "link": "http://arxiv.org/abs/2002.03562v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "End-to-End Multi-speaker Speech Recognition with Transformer", "abstract": "Recently, fully recurrent neural network (RNN) based end-to-end models have\nbeen proven to be effective for multi-speaker speech recognition in both the\nsingle-channel and multi-channel scenarios. In this work, we explore the use of\nTransformer models for these tasks by focusing on two aspects. First, we\nreplace the RNN-based encoder-decoder in the speech recognition model with a\nTransformer architecture. Second, in order to use the Transformer in the\nmasking network of the neural beamformer in the multi-channel case, we modify\nthe self-attention component to be restricted to a segment rather than the\nwhole sequence in order to reduce computation. Besides the model architecture\nimprovements, we also incorporate an external dereverberation preprocessing,\nthe weighted prediction error (WPE), enabling our model to handle reverberated\nsignals. Experiments on the spatialized wsj1-2mix corpus show that the\nTransformer-based models achieve 40.9% and 25.6% relative WER reduction, down\nto 12.1% and 6.4% WER, under the anechoic condition in single-channel and\nmulti-channel tasks, respectively, while in the reverberant case, our methods\nachieve 41.5% and 13.8% relative WER reduction, down to 16.5% and 15.2% WER.", "published": "2020-02-10 16:29:26", "link": "http://arxiv.org/abs/2002.03921v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Pre-training Tasks for Embedding-based Large-scale Retrieval", "abstract": "We consider the large-scale query-document retrieval problem: given a query\n(e.g., a question), return the set of relevant documents (e.g., paragraphs\ncontaining the answer) from a large document corpus. This problem is often\nsolved in two steps. The retrieval phase first reduces the solution space,\nreturning a subset of candidate documents. The scoring phase then re-ranks the\ndocuments. Critically, the retrieval algorithm not only desires high recall but\nalso requires to be highly efficient, returning candidates in time sublinear to\nthe number of documents. Unlike the scoring phase witnessing significant\nadvances recently due to the BERT-style pre-training tasks on cross-attention\nmodels, the retrieval phase remains less well studied. Most previous works rely\non classic Information Retrieval (IR) methods such as BM-25 (token matching +\nTF-IDF weights). These models only accept sparse handcrafted features and can\nnot be optimized for different downstream tasks of interest. In this paper, we\nconduct a comprehensive study on the embedding-based retrieval models. We show\nthat the key ingredient of learning a strong embedding-based Transformer model\nis the set of pre-training tasks. With adequately designed paragraph-level\npre-training tasks, the Transformer models can remarkably improve over the\nwidely-used BM-25 as well as embedding models without Transformers. The\nparagraph-level pre-training tasks we studied are Inverse Cloze Task (ICT),\nBody First Selection (BFS), Wiki Link Prediction (WLP), and the combination of\nall three.", "published": "2020-02-10 16:44:00", "link": "http://arxiv.org/abs/2002.03932v1", "categories": ["cs.LG", "cs.CL", "cs.IR", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Adversarial Filters of Dataset Biases", "abstract": "Large neural models have demonstrated human-level performance on language and\nvision benchmarks, while their performance degrades considerably on adversarial\nor out-of-distribution samples. This raises the question of whether these\nmodels have learned to solve a dataset rather than the underlying task by\noverfitting to spurious dataset biases. We investigate one recently proposed\napproach, AFLite, which adversarially filters such dataset biases, as a means\nto mitigate the prevalent overestimation of machine performance. We provide a\ntheoretical understanding for AFLite, by situating it in the generalized\nframework for optimum bias reduction. We present extensive supporting evidence\nthat AFLite is broadly applicable for reduction of measurable dataset biases,\nand that models trained on the filtered datasets yield better generalization to\nout-of-distribution tasks. Finally, filtering results in a large drop in model\nperformance (e.g., from 92% to 62% for SNLI), while human performance still\nremains high. Our work thus shows that such filtered datasets can pose new\nresearch challenges for robust generalization by serving as upgraded\nbenchmarks.", "published": "2020-02-10 21:59:21", "link": "http://arxiv.org/abs/2002.04108v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Exploring Chemical Space using Natural Language Processing Methodologies\n  for Drug Discovery", "abstract": "Text-based representations of chemicals and proteins can be thought of as\nunstructured languages codified by humans to describe domain-specific\nknowledge. Advances in natural language processing (NLP) methodologies in the\nprocessing of spoken languages accelerated the application of NLP to elucidate\nhidden knowledge in textual representations of these biochemical entities and\nthen use it to construct models to predict molecular properties or to design\nnovel molecules. This review outlines the impact made by these advances on drug\ndiscovery and aims to further the dialogue between medicinal chemists and\ncomputer scientists.", "published": "2020-02-10 21:02:05", "link": "http://arxiv.org/abs/2002.06053v1", "categories": ["q-bio.BM", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "q-bio.BM"}
{"title": "How Much Knowledge Can You Pack Into the Parameters of a Language Model?", "abstract": "It has recently been observed that neural language models trained on\nunstructured text can implicitly store and retrieve knowledge using natural\nlanguage queries. In this short paper, we measure the practical utility of this\napproach by fine-tuning pre-trained models to answer questions without access\nto any external context or knowledge. We show that this approach scales with\nmodel size and performs competitively with open-domain systems that explicitly\nretrieve answers from an external knowledge source when answering questions. To\nfacilitate reproducibility and future work, we release our code and trained\nmodels at https://goo.gle/t5-cbqa.", "published": "2020-02-10 18:55:58", "link": "http://arxiv.org/abs/2002.08910v4", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "An empirical analysis of information encoded in disentangled neural\n  speaker representations", "abstract": "The primary characteristic of robust speaker representations is that they are\ninvariant to factors of variability not related to speaker identity.\nDisentanglement of speaker representations is one of the techniques used to\nimprove robustness of speaker representations to both intrinsic factors that\nare acquired during speech production (e.g., emotion, lexical content) and\nextrinsic factors that are acquired during signal capture (e.g., channel,\nnoise). Disentanglement in neural speaker representations can be achieved\neither in a supervised fashion with annotations of the nuisance factors\n(factors not related to speaker identity) or in an unsupervised fashion without\nlabels of the factors to be removed. In either case it is important to\nunderstand the extent to which the various factors of variability are entangled\nin the representations. In this work, we examine speaker representations with\nand without unsupervised disentanglement for the amount of information they\ncapture related to a suite of factors. Using classification experiments we\nprovide empirical evidence that disentanglement reduces the information with\nrespect to nuisance factors from speaker representations, while retaining\nspeaker information. This is further validated by speaker verification\nexperiments on the VOiCES corpus in several challenging acoustic conditions. We\nalso show improved robustness in speaker verification tasks using data\naugmentation during training of disentangled speaker embeddings. Finally, based\non our findings, we provide insights into the factors that can be effectively\nseparated using the unsupervised disentanglement technique and discuss\npotential future directions.", "published": "2020-02-10 03:28:39", "link": "http://arxiv.org/abs/2002.03520v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "On Cross-Corpus Generalization of Deep Learning Based Speech Enhancement", "abstract": "In recent years, supervised approaches using deep neural networks (DNNs) have\nbecome the mainstream for speech enhancement. It has been established that DNNs\ngeneralize well to untrained noises and speakers if trained using a large\nnumber of noises and speakers. However, we find that DNNs fail to generalize to\nnew speech corpora in low signal-to-noise ratio (SNR) conditions. In this work,\nwe establish that the lack of generalization is mainly due to the channel\nmismatch, i.e. different recording conditions between the trained and untrained\ncorpus. Additionally, we observe that traditional channel normalization\ntechniques are not effective in improving cross-corpus generalization. Further,\nwe evaluate publicly available datasets that are promising for generalization.\nWe find one particular corpus to be significantly better than others. Finally,\nwe find that using a smaller frame shift in short-time processing of speech can\nsignificantly improve cross-corpus generalization. The proposed techniques to\naddress cross-corpus generalization include channel normalization, better\ntraining corpus, and smaller frame shift in short-time Fourier transform\n(STFT). These techniques together improve the objective intelligibility and\nquality scores on untrained corpora significantly.", "published": "2020-02-10 18:57:05", "link": "http://arxiv.org/abs/2002.04027v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multitask Emotion Recognition with Incomplete Labels", "abstract": "We train a unified model to perform three tasks: facial action unit\ndetection, expression classification, and valence-arousal estimation. We\naddress two main challenges of learning the three tasks. First, most existing\ndatasets are highly imbalanced. Second, most existing datasets do not contain\nlabels for all three tasks. To tackle the first challenge, we apply data\nbalancing techniques to experimental datasets. To tackle the second challenge,\nwe propose an algorithm for the multitask model to learn from missing\n(incomplete) labels. This algorithm has two steps. We first train a teacher\nmodel to perform all three tasks, where each instance is trained by the ground\ntruth label of its corresponding task. Secondly, we refer to the outputs of the\nteacher model as the soft labels. We use the soft labels and the ground truth\nto train the student model. We find that most of the student models outperform\ntheir teacher model on all the three tasks. Finally, we use model ensembling to\nboost performance further on the three tasks.", "published": "2020-02-10 05:32:12", "link": "http://arxiv.org/abs/2002.03557v2", "categories": ["cs.CV", "cs.MM", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Modeling Musical Onset Probabilities via Neural Distribution Learning", "abstract": "Musical onset detection can be formulated as a time-to-event (TTE) or\ntime-since-event (TSE) prediction task by defining music as a sequence of onset\nevents. Here we propose a novel method to model the probability of onsets by\nintroducing a sequential density prediction model. The proposed model estimates\nTTE & TSE distributions from mel-spectrograms using convolutional neural\nnetworks (CNNs) as a density predictor. We evaluate our model on the Bock\ndataset show-ing comparable results to previous deep-learning models.", "published": "2020-02-10 05:38:51", "link": "http://arxiv.org/abs/2002.03559v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Accelerating RNN Transducer Inference via One-Step Constrained Beam\n  Search", "abstract": "We propose a one-step constrained (OSC) beam search to accelerate recurrent\nneural network (RNN) transducer (RNN-T) inference. The original RNN-T beam\nsearch has a while-loop leading to speed down of the decoding process. The OSC\nbeam search eliminates this while-loop by vectorizing multiple hypotheses. This\nvectorization is nontrivial as the expansion of the hypotheses within the\noriginal RNN-T beam search can be different from each other. However, we found\nthat the hypotheses expanded only once at each decoding step in most cases;\nthus, we constrained the maximum expansion number to one, thereby allowing\nvectorization of the hypotheses. For further acceleration, we assign\nconstraints to the prefixes of the hypotheses to prune the redundant search\nspace. In addition, OSC beam search has duplication check among hypotheses\nduring the decoding process as duplication can undesirably shrink the search\nspace. We achieved significant speedup compared with other RNN-T beam search\nmethods with lower phoneme and word error rate.", "published": "2020-02-10 06:51:31", "link": "http://arxiv.org/abs/2002.03577v1", "categories": ["cs.LG", "eess.AS", "eess.SP"], "primary_category": "cs.LG"}
{"title": "Multimodal active speaker detection and virtual cinematography for video\n  conferencing", "abstract": "Active speaker detection (ASD) and virtual cinematography (VC) can\nsignificantly improve the remote user experience of a video conference by\nautomatically panning, tilting and zooming of a video conferencing camera:\nusers subjectively rate an expert video cinematographer's video significantly\nhigher than unedited video. We describe a new automated ASD and VC that\nperforms within 0.3 MOS of an expert cinematographer based on subjective\nratings with a 1-5 scale. This system uses a 4K wide-FOV camera, a depth\ncamera, and a microphone array; it extracts features from each modality and\ntrains an ASD using an AdaBoost machine learning system that is very efficient\nand runs in real-time. A VC is similarly trained using machine learning to\noptimize the subjective quality of the overall experience. To avoid distracting\nthe room participants and reduce switching latency the system has no moving\nparts -- the VC works by cropping and zooming the 4K wide-FOV video stream. The\nsystem was tuned and evaluated using extensive crowdsourcing techniques and\nevaluated on a dataset with N=100 meetings, each 2-5 minutes in length.", "published": "2020-02-10 17:41:51", "link": "http://arxiv.org/abs/2002.03977v3", "categories": ["eess.AS", "cs.LG", "cs.MM", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Unsupervised Learning of Audio Perception for Robotics Applications:\n  Learning to Project Data to T-SNE/UMAP space", "abstract": "Audio perception is a key to solving a variety of problems ranging from\nacoustic scene analysis, music meta-data extraction, recommendation, synthesis\nand analysis. It can potentially also augment computers in doing tasks that\nhumans do effortlessly in day-to-day activities. This paper builds upon key\nideas to build perception of touch sounds without access to any ground-truth\ndata. We show how we can leverage ideas from classical signal processing to get\nlarge amounts of data of any sound of interest with a high precision. These\nsounds are then used, along with the images to map the sounds to a clustered\nspace of the latent representation of these images. This approach, not only\nallows us to learn semantic representation of the possible sounds of interest,\nbut also allows association of different modalities to the learned\ndistinctions. The model trained to map sounds to this clustered representation,\ngives reasonable performance as opposed to expensive methods collecting a lot\nof human annotated data. Such approaches can be used to build a state of art\nperceptual model for any sound of interest described using a few signal\nprocessing features. Daisy chaining high precision sound event detectors using\nsignal processing combined with neural architectures and high dimensional\nclustering of unlabelled data is a vastly powerful idea, and can be explored in\na variety of ways in future.", "published": "2020-02-10 20:33:25", "link": "http://arxiv.org/abs/2002.04076v1", "categories": ["cs.SD", "cs.LG", "cs.RO", "eess.AS"], "primary_category": "cs.SD"}
