{"title": "Automatic Generation of Language-Independent Features for Cross-Lingual\n  Classification", "abstract": "Many applications require categorization of text documents using predefined\ncategories. The main approach to performing text categorization is learning\nfrom labeled examples. For many tasks, it may be difficult to find examples in\none language but easy in others. The problem of learning from examples in one\nor more languages and classifying (categorizing) in another is called\ncross-lingual learning. In this work, we present a novel approach that solves\nthe general cross-lingual text categorization problem. Our method generates,\nfor each training document, a set of language-independent features. Using these\nfeatures for training yields a language-independent classifier. At the\nclassification stage, we generate language-independent features for the\nunlabeled document, and apply the classifier on the new representation.\n  To build the feature generator, we utilize a hierarchical\nlanguage-independent ontology, where each concept has a set of support\ndocuments for each language involved. In the preprocessing stage, we use the\nsupport documents to build a set of language-independent feature generators,\none for each language. The collection of these generators is used to map any\ndocument into the language-independent feature space.\n  Our methodology works on the most general cross-lingual text categorization\nproblems, being able to learn from any mix of languages and classify documents\nin any other language. We also present a method for exploiting the hierarchical\nstructure of the ontology to create virtual supporting documents for languages\nthat do not have them. We tested our method, using Wikipedia as our ontology,\non the most commonly used test collections in cross-lingual text\ncategorization, and found that it outperforms existing methods.", "published": "2018-02-12 13:20:57", "link": "http://arxiv.org/abs/1802.04028v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "End-to-End Automatic Speech Translation of Audiobooks", "abstract": "We investigate end-to-end speech-to-text translation on a corpus of\naudiobooks specifically augmented for this task. Previous works investigated\nthe extreme case where source language transcription is not available during\nlearning nor decoding, but we also study a midway case where source language\ntranscription is available at training time only. In this case, a single model\nis trained to decode source speech into target text in a single pass.\nExperimental results show that it is possible to train compact and efficient\nend-to-end speech translation models in this setup. We also distribute the\ncorpus and hope that our speech translation baseline on this corpus will be\nchallenged in the future.", "published": "2018-02-12 17:37:11", "link": "http://arxiv.org/abs/1802.04200v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Unified Implicit Dialog Framework for Conversational Search", "abstract": "We propose a unified Implicit Dialog framework for goal-oriented, information\nseeking tasks of Conversational Search applications. It aims to enable dialog\ninteractions with domain data without replying on explicitly encoded the rules\nbut utilizing the underlying data representation to build the components\nrequired for dialog interaction, which we refer as Implicit Dialog in this\nwork. The proposed framework consists of a pipeline of End-to-End trainable\nmodules. A centralized knowledge representation is used to semantically ground\nmultiple dialog modules. An associated set of tools are integrated with the\nframework to gather end users' input for continuous improvement of the system.\nThe goal is to facilitate development of conversational systems by identifying\nthe components and the data that can be adapted and reused across many end-user\napplications. We demonstrate our approach by creating conversational agents for\nseveral independent domains.", "published": "2018-02-12 20:53:50", "link": "http://arxiv.org/abs/1802.04358v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Compositionality in Sentence Embeddings", "abstract": "An important challenge for human-like AI is compositional semantics. Recent\nresearch has attempted to address this by using deep neural networks to learn\nvector space embeddings of sentences, which then serve as input to other tasks.\nWe present a new dataset for one such task, `natural language inference' (NLI),\nthat cannot be solved using only word-level knowledge and requires some\ncompositionality. We find that the performance of state of the art sentence\nembeddings (InferSent; Conneau et al., 2017) on our new dataset is poor. We\nanalyze the decision rules learned by InferSent and find that they are\nconsistent with simple heuristics that are ecologically valid in its training\ndataset. Further, we find that augmenting training with our dataset improves\ntest performance on our dataset without loss of performance on the original\ntraining dataset. This highlights the importance of structured datasets in\nbetter understanding and improving AI systems.", "published": "2018-02-12 19:02:52", "link": "http://arxiv.org/abs/1802.04302v2", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Answerer in Questioner's Mind: Information Theoretic Approach to\n  Goal-Oriented Visual Dialog", "abstract": "Goal-oriented dialog has been given attention due to its numerous\napplications in artificial intelligence. Goal-oriented dialogue tasks occur\nwhen a questioner asks an action-oriented question and an answerer responds\nwith the intent of letting the questioner know a correct action to take. To ask\nthe adequate question, deep learning and reinforcement learning have been\nrecently applied. However, these approaches struggle to find a competent\nrecurrent neural questioner, owing to the complexity of learning a series of\nsentences. Motivated by theory of mind, we propose \"Answerer in Questioner's\nMind\" (AQM), a novel information theoretic algorithm for goal-oriented dialog.\nWith AQM, a questioner asks and infers based on an approximated probabilistic\nmodel of the answerer. The questioner figures out the answerer's intention via\nselecting a plausible question by explicitly calculating the information gain\nof the candidate intentions and possible answers to each question. We test our\nframework on two goal-oriented visual dialog tasks: \"MNIST Counting Dialog\" and\n\"GuessWhat?!\". In our experiments, AQM outperforms comparative algorithms by a\nlarge margin.", "published": "2018-02-12 04:08:06", "link": "http://arxiv.org/abs/1802.03881v3", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "SparseMAP: Differentiable Sparse Structured Inference", "abstract": "Structured prediction requires searching over a combinatorial number of\nstructures. To tackle it, we introduce SparseMAP: a new method for sparse\nstructured inference, and its natural loss function. SparseMAP automatically\nselects only a few global structures: it is situated between MAP inference,\nwhich picks a single structure, and marginal inference, which assigns\nprobability mass to all structures, including implausible ones. Importantly,\nSparseMAP can be computed using only calls to a MAP oracle, making it\napplicable to problems with intractable marginal inference, e.g., linear\nassignment. Sparsity makes gradient backpropagation efficient regardless of the\nstructure, enabling us to augment deep neural networks with generic and sparse\nstructured hidden layers. Experiments in dependency parsing and natural\nlanguage inference reveal competitive accuracy, improved interpretability, and\nthe ability to capture natural language ambiguities, which is attractive for\npipeline systems.", "published": "2018-02-12 18:07:34", "link": "http://arxiv.org/abs/1802.04223v2", "categories": ["stat.ML", "cs.CL", "cs.LG", "68T50", "I.2.6; I.2.6"], "primary_category": "stat.ML"}
{"title": "Neural Program Search: Solving Programming Tasks from Description and\n  Examples", "abstract": "We present a Neural Program Search, an algorithm to generate programs from\nnatural language description and a small number of input/output examples. The\nalgorithm combines methods from Deep Learning and Program Synthesis fields by\ndesigning rich domain-specific language (DSL) and defining efficient search\nalgorithm guided by a Seq2Tree model on it. To evaluate the quality of the\napproach we also present a semi-synthetic dataset of descriptions with test\nexamples and corresponding programs. We show that our algorithm significantly\noutperforms a sequence-to-sequence model with attention baseline.", "published": "2018-02-12 20:05:26", "link": "http://arxiv.org/abs/1802.04335v1", "categories": ["cs.AI", "cs.CL", "cs.PL"], "primary_category": "cs.AI"}
{"title": "M-Walk: Learning to Walk over Graphs using Monte Carlo Tree Search", "abstract": "Learning to walk over a graph towards a target node for a given query and a\nsource node is an important problem in applications such as knowledge base\ncompletion (KBC). It can be formulated as a reinforcement learning (RL) problem\nwith a known state transition model. To overcome the challenge of sparse\nrewards, we develop a graph-walking agent called M-Walk, which consists of a\ndeep recurrent neural network (RNN) and Monte Carlo Tree Search (MCTS). The RNN\nencodes the state (i.e., history of the walked path) and maps it separately to\na policy and Q-values. In order to effectively train the agent from sparse\nrewards, we combine MCTS with the neural policy to generate trajectories\nyielding more positive rewards. From these trajectories, the network is\nimproved in an off-policy manner using Q-learning, which modifies the RNN\npolicy via parameter sharing. Our proposed RL algorithm repeatedly applies this\npolicy-improvement step to learn the model. At test time, MCTS is combined with\nthe neural policy to predict the target node. Experimental results on several\ngraph-walking benchmarks show that M-Walk is able to learn better policies than\nother RL-based methods, which are mainly based on policy gradients. M-Walk also\noutperforms traditional KBC baselines.", "published": "2018-02-12 23:27:23", "link": "http://arxiv.org/abs/1802.04394v5", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Linear Regression for Speaker Verification", "abstract": "This paper presents a linear regression based back-end for speaker\nverification. Linear regression is a simple linear model that minimizes the\nmean squared estimation error between the target and its estimate with a closed\nform solution, where the target is defined as the ground-truth indicator\nvectors of utterances. We use the linear regression model to learn speaker\nmodels from a front-end, and verify the similarity of two speaker models by a\ncosine similarity scoring classifier. To evaluate the effectiveness of the\nlinear regression model, we construct three speaker verification systems that\nuse the Gaussian mixture model and identity-vector (GMM/i-vector) front-end,\ndeep neural network and i-vector (DNN/i-vector) front-end, and deep vector\n(d-vector) front-end as their front-ends, respectively. Our empirical\ncomparison results on the NIST speaker recognition evaluation data sets show\nthat the proposed method outperforms within-class covariance normalization,\nlinear discriminant analysis, and probabilistic linear discriminant analysis,\ngiven any of the three front-ends.", "published": "2018-02-12 15:23:35", "link": "http://arxiv.org/abs/1802.04113v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "One Deep Music Representation to Rule Them All? : A comparative analysis\n  of different representation learning strategies", "abstract": "Inspired by the success of deploying deep learning in the fields of Computer\nVision and Natural Language Processing, this learning paradigm has also found\nits way into the field of Music Information Retrieval. In order to benefit from\ndeep learning in an effective, but also efficient manner, deep transfer\nlearning has become a common approach. In this approach, it is possible to\nreuse the output of a pre-trained neural network as the basis for a new\nlearning task. The underlying hypothesis is that if the initial and new\nlearning tasks show commonalities and are applied to the same type of input\ndata (e.g. music audio), the generated deep representation of the data is also\ninformative for the new task. Since, however, most of the networks used to\ngenerate deep representations are trained using a single initial learning\nsource, their representation is unlikely to be informative for all possible\nfuture tasks. In this paper, we present the results of our investigation of\nwhat are the most important factors to generate deep representations for the\ndata and learning tasks in the music domain. We conducted this investigation\nvia an extensive empirical study that involves multiple learning sources, as\nwell as multiple deep learning architectures with varying levels of information\nsharing between sources, in order to learn music representations. We then\nvalidate these representations considering multiple target datasets for\nevaluation. The results of our experiments yield several insights on how to\napproach the design of methods for learning widely deployable deep data\nrepresentations in the music domain.", "published": "2018-02-12 14:08:54", "link": "http://arxiv.org/abs/1802.04051v4", "categories": ["cs.NE", "cs.SD", "eess.AS"], "primary_category": "cs.NE"}
