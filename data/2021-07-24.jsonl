{"title": "MIPE: A Metric Independent Pipeline for Effective Code-Mixed NLG\n  Evaluation", "abstract": "Code-mixing is a phenomenon of mixing words and phrases from two or more\nlanguages in a single utterance of speech and text. Due to the high linguistic\ndiversity, code-mixing presents several challenges in evaluating standard\nnatural language generation (NLG) tasks. Various widely popular metrics perform\npoorly with the code-mixed NLG tasks. To address this challenge, we present a\nmetric independent evaluation pipeline MIPE that significantly improves the\ncorrelation between evaluation metrics and human judgments on the generated\ncode-mixed text. As a use case, we demonstrate the performance of MIPE on the\nmachine-generated Hinglish (code-mixing of Hindi and English languages)\nsentences from the HinGE corpus. We can extend the proposed evaluation strategy\nto other code-mixed language pairs, NLG tasks, and evaluation metrics with\nminimal to no effort.", "published": "2021-07-24 05:24:26", "link": "http://arxiv.org/abs/2107.11534v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extending Challenge Sets to Uncover Gender Bias in Machine Translation:\n  Impact of Stereotypical Verbs and Adjectives", "abstract": "Human gender bias is reflected in language and text production. Because\nstate-of-the-art machine translation (MT) systems are trained on large corpora\nof text, mostly generated by humans, gender bias can also be found in MT. For\ninstance when occupations are translated from a language like English, which\nmostly uses gender neutral words, to a language like German, which mostly uses\na feminine and a masculine version for an occupation, a decision must be made\nby the MT System. Recent research showed that MT systems are biased towards\nstereotypical translation of occupations. In 2019 the first, and so far only,\nchallenge set, explicitly designed to measure the extent of gender bias in MT\nsystems has been published. In this set measurement of gender bias is solely\nbased on the translation of occupations. In this paper we present an extension\nof this challenge set, called WiBeMT, with gender-biased adjectives and adds\nsentences with gender-biased verbs. The resulting challenge set consists of\nover 70, 000 sentences and has been translated with three commercial MT\nsystems: DeepL Translator, Microsoft Translator, and Google Translate. Results\nshow a gender bias for all three MT systems. This gender bias is to a great\nextent significantly influenced by adjectives and to a lesser extent by verbs.", "published": "2021-07-24 11:22:10", "link": "http://arxiv.org/abs/2107.11584v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Negation Handling in Machine Learning-Based Sentiment Classification for\n  Colloquial Arabic", "abstract": "One crucial aspect of sentiment analysis is negation handling, where the\noccurrence of negation can flip the sentiment of a sentence and negatively\naffects the machine learning-based sentiment classification. The role of\nnegation in Arabic sentiment analysis has been explored only to a limited\nextent, especially for colloquial Arabic. In this paper, the author addresses\nthe negation problem of machine learning-based sentiment classification for a\ncolloquial Arabic language. To this end, we propose a simple rule-based\nalgorithm for handling the problem; the rules were crafted based on observing\nmany cases of negation. Additionally, simple linguistic knowledge and sentiment\nlexicon are used for this purpose. The author also examines the impact of the\nproposed algorithm on the performance of different machine learning algorithms.\nThe results given by the proposed algorithm are compared with three baseline\nmodels. The experimental results show that there is a positive impact on the\nclassifiers accuracy, precision and recall when the proposed algorithm is used\ncompared to the baselines.", "published": "2021-07-24 13:12:37", "link": "http://arxiv.org/abs/2107.11597v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Context-aware Adversarial Training for Name Regularity Bias in Named\n  Entity Recognition", "abstract": "In this work, we examine the ability of NER models to use contextual\ninformation when predicting the type of an ambiguous entity. We introduce NRB,\na new testbed carefully designed to diagnose Name Regularity Bias of NER\nmodels. Our results indicate that all state-of-the-art models we tested show\nsuch a bias; BERT fine-tuned models significantly outperforming feature-based\n(LSTM-CRF) ones on NRB, despite having comparable (sometimes lower) performance\non standard benchmarks.\n  To mitigate this bias, we propose a novel model-agnostic training method that\nadds learnable adversarial noise to some entity mentions, thus enforcing models\nto focus more strongly on the contextual signal, leading to significant gains\non NRB. Combining it with two other training strategies, data augmentation and\nparameter freezing, leads to further gains.", "published": "2021-07-24 13:55:35", "link": "http://arxiv.org/abs/2107.11610v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Clinical Utility of the Automatic Phenotype Annotation in Unstructured\n  Clinical Notes: ICU Use Cases", "abstract": "Objective: Clinical notes contain information not present elsewhere,\nincluding drug response and symptoms, all of which are highly important when\npredicting key outcomes in acute care patients. We propose the automatic\nannotation of phenotypes from clinical notes as a method to capture essential\ninformation, which is complementary to typically used vital signs and\nlaboratory test results, to predict outcomes in the Intensive Care Unit (ICU).\n  Methods: We develop a novel phenotype annotation model to annotate phenotypic\nfeatures of patients which are then used as input features of predictive models\nto predict ICU patient outcomes. We demonstrate and validate our approach\nconducting experiments on three ICU prediction tasks including in-hospital\nmortality, physiological decompensation and length of stay for over 24,000\npatients by using MIMIC-III dataset.\n  Results: The predictive models incorporating phenotypic information achieve\n0.845 (AUC-ROC) to predict in-hospital mortality, 0.839 (AUC-ROC) for\nphysiological decompensation and 0.430 (Kappa) for length of stay, all of which\nconsistently outperform the baseline models leveraging only vital signs and\nlaboratory test results. Moreover, we conduct a thorough interpretability\nstudy, showing that phenotypes provide valuable insights at the patient and\ncohort levels.\n  Conclusion: The proposed approach demonstrates phenotypic information\ncomplements traditionally used vital signs and laboratory test results,\nimproving significantly forecast of outcomes in the ICU.", "published": "2021-07-24 17:55:55", "link": "http://arxiv.org/abs/2107.11665v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MDQE: A More Accurate Direct Pretraining for Machine Translation Quality\n  Estimation", "abstract": "It is expensive to evaluate the results of Machine Translation(MT), which\nusually requires manual translation as a reference. Machine Translation Quality\nEstimation (QE) is a task of predicting the quality of machine translations\nwithout relying on any reference. Recently, the emergence of\npredictor-estimator framework which trains the predictor as a feature extractor\nand estimator as a QE predictor, and pre-trained language models(PLM) have\nachieved promising QE performance. However, we argue that there are still gaps\nbetween the predictor and the estimator in both data quality and training\nobjectives, which preclude QE models from benefiting from a large number of\nparallel corpora more directly. Based on previous related work that have\nalleviated gaps to some extent, we propose a novel framework that provides a\nmore accurate direct pretraining for QE tasks. In this framework, a generator\nis trained to produce pseudo data that is closer to the real QE data, and a\nestimator is pretrained on these data with novel objectives that are the same\nas the QE task. Experiments on widely used benchmarks show that our proposed\nframework outperforms existing methods, without using any pretraining models\nsuch as BERT.", "published": "2021-07-24 09:48:37", "link": "http://arxiv.org/abs/2107.14600v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The USYD-JD Speech Translation System for IWSLT 2021", "abstract": "This paper describes the University of Sydney& JD's joint submission of the\nIWSLT 2021 low resource speech translation task. We participated in the\nSwahili-English direction and got the best scareBLEU (25.3) score among all the\nparticipants. Our constrained system is based on a pipeline framework, i.e. ASR\nand NMT. We trained our models with the officially provided ASR and MT\ndatasets. The ASR system is based on the open-sourced tool Kaldi and this work\nmainly explores how to make the most of the NMT models. To reduce the\npunctuation errors generated by the ASR model, we employ our previous work\nSlotRefine to train a punctuation correction model. To achieve better\ntranslation performance, we explored the most recent effective strategies,\nincluding back translation, knowledge distillation, multi-feature reranking and\ntransductive finetuning. For model structure, we tried auto-regressive and\nnon-autoregressive models, respectively. In addition, we proposed two novel\npre-train approaches, i.e. \\textit{de-noising training} and\n\\textit{bidirectional training} to fully exploit the data. Extensive\nexperiments show that adding the above techniques consistently improves the\nBLEU scores, and the final submission system outperforms the baseline\n(Transformer ensemble model trained with the original parallel data) by\napproximately 10.8 BLEU score, achieving the SOTA performance.", "published": "2021-07-24 09:53:34", "link": "http://arxiv.org/abs/2107.11572v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Stress Test Evaluation of Biomedical Word Embeddings", "abstract": "The success of pretrained word embeddings has motivated their use in the\nbiomedical domain, with contextualized embeddings yielding remarkable results\nin several biomedical NLP tasks. However, there is a lack of research on\nquantifying their behavior under severe \"stress\" scenarios. In this work, we\nsystematically evaluate three language models with adversarial examples --\nautomatically constructed tests that allow us to examine how robust the models\nare. We propose two types of stress scenarios focused on the biomedical named\nentity recognition (NER) task, one inspired by spelling errors and another\nbased on the use of synonyms for medical terms. Our experiments with three\nbenchmarks show that the performance of the original models decreases\nconsiderably, in addition to revealing their weaknesses and strengths. Finally,\nwe show that adversarial training causes the models to improve their robustness\nand even to exceed the original performance in some cases.", "published": "2021-07-24 16:45:03", "link": "http://arxiv.org/abs/2107.11652v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Differentiable Allophone Graphs for Language-Universal Speech\n  Recognition", "abstract": "Building language-universal speech recognition systems entails producing\nphonological units of spoken sound that can be shared across languages. While\nspeech annotations at the language-specific phoneme or surface levels are\nreadily available, annotations at a universal phone level are relatively rare\nand difficult to produce. In this work, we present a general framework to\nderive phone-level supervision from only phonemic transcriptions and\nphone-to-phoneme mappings with learnable weights represented using weighted\nfinite-state transducers, which we call differentiable allophone graphs. By\ntraining multilingually, we build a universal phone-based speech recognition\nmodel with interpretable probabilistic phone-to-phoneme mappings for each\nlanguage. These phone-based systems with learned allophone graphs can be used\nby linguists to document new languages, build phone-based lexicons that capture\nrich pronunciation variations, and re-evaluate the allophone mappings of seen\nlanguage. We demonstrate the aforementioned benefits of our proposed framework\nwith a system trained on 7 diverse languages.", "published": "2021-07-24 15:09:32", "link": "http://arxiv.org/abs/2107.11628v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Graph Convolutional Network with Generalized Factorized Bilinear\n  Aggregation", "abstract": "Although Graph Convolutional Networks (GCNs) have demonstrated their power in\nvarious applications, the graph convolutional layers, as the most important\ncomponent of GCN, are still using linear transformations and a simple pooling\nstep. In this paper, we propose a novel generalization of Factorized Bilinear\n(FB) layer to model the feature interactions in GCNs. FB performs two\nmatrix-vector multiplications, that is, the weight matrix is multiplied with\nthe outer product of the vector of hidden features from both sides. However,\nthe FB layer suffers from the quadratic number of coefficients, overfitting and\nthe spurious correlations due to correlations between channels of hidden\nrepresentations that violate the i.i.d. assumption. Thus, we propose a compact\nFB layer by defining a family of summarizing operators applied over the\nquadratic term. We analyze proposed pooling operators and motivate their use.\nOur experimental results on multiple datasets demonstrate that the GFB-GCN is\ncompetitive with other methods for text classification.", "published": "2021-07-24 17:57:06", "link": "http://arxiv.org/abs/2107.11666v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Use of speaker recognition approaches for learning and evaluating\n  embedding representations of musical instrument sounds", "abstract": "Constructing an embedding space for musical instrument sounds that can\nmeaningfully represent new and unseen instruments is important for downstream\nmusic generation tasks such as multi-instrument synthesis and timbre transfer.\nThe framework of Automatic Speaker Verification (ASV) provides us with\narchitectures and evaluation methodologies for verifying the identities of\nunseen speakers, and these can be repurposed for the task of learning and\nevaluating a musical instrument sound embedding space that can support unseen\ninstruments. Borrowing from state-of-the-art ASV techniques, we construct a\nmusical instrument recognition model that uses a SincNet front-end, a ResNet\narchitecture, and an angular softmax objective function. Experiments on the\nNSynth and RWC datasets show our model's effectiveness in terms of equal error\nrate (EER) for unseen instruments, and ablation studies show the importance of\ndata augmentation and the angular softmax objective. Experiments also show the\nbenefit of using a CQT-based filterbank for initializing SincNet over a Mel\nfilterbank initialization. Further complementary analysis of the learned\nembedding space is conducted with t-SNE visualizations and probing\nclassification tasks, which show that including instrument family labels as a\nmulti-task learning target can help to regularize the embedding space and\nincorporate useful structure, and that meaningful information such as playing\nstyle, which was not included during training, is contained in the embeddings\nof unseen instruments.", "published": "2021-07-24 01:41:45", "link": "http://arxiv.org/abs/2107.11506v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Dynamic Portal Occlusion for Precomputed Interactive Sound Propagation", "abstract": "An immersive audio-visual experience in games and virtual reality requires\nfast calculation of diffraction-based acoustic effects. To maintain\nplausibility, the effects must retain spatial smoothness on source and listener\nmotion within geometrically complex scenes. Precomputed wave-based techniques\ncan render such results at low runtime CPU cost, but remain limited to static\nscenes. Modeling the occlusion effect of dynamic portals such as doors present\nan unresolved challenge to maintain audio-visual consistency. We present a fast\nsolution implementable as a drop-in extension to existing precomputed systems.\nKey is a novel portal-search method that leverages precomputed propagation\ndelay and direction data to find portals intervening the diffracted shortest\npath connecting dynamic source and listener at runtime. The method scales\nlinearly with number of portals in worst case, far cheaper than explicit global\npath search that scales with scene area. We discuss culling techniques to\naccelerate further. The search algorithm is combined with geometric-acoustic\napproximations to model the additional direct and indirect energy loss from\nintervening portals depending on their dynamic closure state. We demonstrate\nplausible audio-visual animations within our system integrated with Unreal\nEngine 4 (TM) and AudioKinetic Wwise (TM).", "published": "2021-07-24 07:04:53", "link": "http://arxiv.org/abs/2107.11548v2", "categories": ["cs.SD", "cs.GR", "eess.AS", "I.3.7; H.5.5"], "primary_category": "cs.SD"}
{"title": "Significance of Speaker Embeddings and Temporal Context for Depression\n  Detection", "abstract": "Depression detection from speech has attracted a lot of attention in recent\nyears. However, the significance of speaker-specific information in depression\ndetection has not yet been explored. In this work, we analyze the significance\nof speaker embeddings for the task of depression detection from speech.\nExperimental results show that the speaker embeddings provide important cues to\nachieve state-of-the-art performance in depression detection. We also show that\ncombining conventional OpenSMILE and COVAREP features, which carry\ncomplementary information, with speaker embeddings further improves the\ndepression detection performance. The significance of temporal context in the\ntraining of deep learning models for depression detection is also analyzed in\nthis paper.", "published": "2021-07-24 05:14:48", "link": "http://arxiv.org/abs/2107.13969v1", "categories": ["cs.CY", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CY"}
