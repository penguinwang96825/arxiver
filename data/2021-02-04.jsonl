{"title": "Converse, Focus and Guess -- Towards Multi-Document Driven Dialogue", "abstract": "We propose a novel task, Multi-Document Driven Dialogue (MD3), in which an\nagent can guess the target document that the user is interested in by leading a\ndialogue. To benchmark progress, we introduce a new dataset of GuessMovie,\nwhich contains 16,881 documents, each describing a movie, and associated 13,434\ndialogues. Further, we propose the MD3 model. Keeping guessing the target\ndocument in mind, it converses with the user conditioned on both document\nengagement and user feedback. In order to incorporate large-scale external\ndocuments into the dialogue, it pretrains a document representation which is\nsensitive to attributes it talks about an object. Then it tracks dialogue state\nby detecting evolvement of document belief and attribute belief, and finally\noptimizes dialogue policy in principle of entropy decreasing and reward\nincreasing, which is expected to successfully guess the user's target in a\nminimum number of turns. Experiments show that our method significantly\noutperforms several strong baseline methods and is very close to human's\nperformance.", "published": "2021-02-04 06:36:11", "link": "http://arxiv.org/abs/2102.02435v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adaptive Semiparametric Language Models", "abstract": "We present a language model that combines a large parametric neural network\n(i.e., a transformer) with a non-parametric episodic memory component in an\nintegrated architecture. Our model uses extended short-term context by caching\nlocal hidden states -- similar to transformer-XL -- and global long-term memory\nby retrieving a set of nearest neighbor tokens at each timestep. We design a\ngating function to adaptively combine multiple information sources to make a\nprediction. This mechanism allows the model to use either local context,\nshort-term memory, or long-term memory (or any combination of them) on an ad\nhoc basis depending on the context. Experiments on word-based and\ncharacter-based language modeling datasets demonstrate the efficacy of our\nproposed method compared to strong baselines.", "published": "2021-02-04 11:47:03", "link": "http://arxiv.org/abs/2102.02557v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Incremental Beam Manipulation for Natural Language Generation", "abstract": "The performance of natural language generation systems has improved\nsubstantially with modern neural networks. At test time they typically employ\nbeam search to avoid locally optimal but globally suboptimal predictions.\nHowever, due to model errors, a larger beam size can lead to deteriorating\nperformance according to the evaluation metric. For this reason, it is common\nto rerank the output of beam search, but this relies on beam search to produce\na good set of hypotheses, which limits the potential gains. Other alternatives\nto beam search require changes to the training of the model, which restricts\ntheir applicability compared to beam search. This paper proposes incremental\nbeam manipulation, i.e. reranking the hypotheses in the beam during decoding\ninstead of only at the end. This way, hypotheses that are unlikely to lead to a\ngood final output are discarded, and in their place hypotheses that would have\nbeen ignored will be considered instead. Applying incremental beam manipulation\nleads to an improvement of 1.93 and 5.82 BLEU points over vanilla beam search\nfor the test sets of the E2E and WebNLG challenges respectively. The proposed\nmethod also outperformed a strong reranker by 1.04 BLEU points on the E2E\nchallenge, while being on par with it on the WebNLG dataset.", "published": "2021-02-04 12:26:47", "link": "http://arxiv.org/abs/2102.02574v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "One Size Does Not Fit All: Finding the Optimal Subword Sizes for\n  FastText Models across Languages", "abstract": "Unsupervised representation learning of words from large multilingual corpora\nis useful for downstream tasks such as word sense disambiguation, semantic text\nsimilarity, and information retrieval. The representation precision of\nlog-bilinear fastText models is mostly due to their use of subword information.\nIn previous work, the optimization of fastText's subword sizes has not been\nfully explored, and non-English fastText models were trained using subword\nsizes optimized for English and German word analogy tasks. In our work, we find\nthe optimal subword sizes on the English, German, Czech, Italian, Spanish,\nFrench, Hindi, Turkish, and Russian word analogy tasks. We then propose a\nsimple n-gram coverage model and we show that it predicts better-than-default\nsubword sizes on the Spanish, French, Hindi, Turkish, and Russian word analogy\ntasks. We show that the optimization of fastText's subword sizes matters and\nresults in a 14% improvement on the Czech word analogy task. We also show that\nexpensive parameter optimization can be replaced by a simple n-gram coverage\nmodel that consistently improves the accuracy of fastText models on the word\nanalogy tasks by up to 3% compared to the default subword sizes, and that it is\nwithin 1% accuracy of the optimal subword sizes.", "published": "2021-02-04 12:59:36", "link": "http://arxiv.org/abs/2102.02585v3", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Data-to-text Generation with Macro Planning", "abstract": "Recent approaches to data-to-text generation have adopted the very successful\nencoder-decoder architecture or variants thereof. These models generate text\nwhich is fluent (but often imprecise) and perform quite poorly at selecting\nappropriate content and ordering it coherently. To overcome some of these\nissues, we propose a neural model with a macro planning stage followed by a\ngeneration stage reminiscent of traditional methods which embrace separate\nmodules for planning and surface realization. Macro plans represent high level\norganization of important content such as entities, events and their\ninteractions; they are learnt from data and given as input to the generator.\nExtensive experiments on two data-to-text benchmarks (RotoWire and MLB) show\nthat our approach outperforms competitive baselines in terms of automatic and\nhuman evaluation.", "published": "2021-02-04 16:32:57", "link": "http://arxiv.org/abs/2102.02723v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Building Representative Corpora from Illiterate Communities: A Review of\n  Challenges and Mitigation Strategies for Developing Countries", "abstract": "Most well-established data collection methods currently adopted in NLP depend\non the assumption of speaker literacy. Consequently, the collected corpora\nlargely fail to represent swathes of the global population, which tend to be\nsome of the most vulnerable and marginalised people in society, and often live\nin rural developing areas. Such underrepresented groups are thus not only\nignored when making modeling and system design decisions, but also prevented\nfrom benefiting from development outcomes achieved through data-driven NLP.\nThis paper aims to address the under-representation of illiterate communities\nin NLP corpora: we identify potential biases and ethical issues that might\narise when collecting data from rural communities with high illiteracy rates in\nLow-Income Countries, and propose a set of practical mitigation strategies to\nhelp future work.", "published": "2021-02-04 19:20:35", "link": "http://arxiv.org/abs/2102.02841v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bangla Text Dataset and Exploratory Analysis for Online Harassment\n  Detection", "abstract": "Being the seventh most spoken language in the world, the use of the Bangla\nlanguage online has increased in recent times. Hence, it has become very\nimportant to analyze Bangla text data to maintain a safe and harassment-free\nonline place. The data that has been made accessible in this article has been\ngathered and marked from the comments of people in public posts by celebrities,\ngovernment officials, athletes on Facebook. The total amount of collected\ncomments is 44001. The dataset is compiled with the aim of developing the\nability of machines to differentiate whether a comment is a bully expression or\nnot with the help of Natural Language Processing and to what extent it is\nimproper if it is an inappropriate comment. The comments are labeled with\ndifferent categories of harassment. Exploratory analysis from different\nperspectives is also included in this paper to have a detailed overview. Due to\nthe scarcity of data collection of categorized Bengali language comments, this\ndataset can have a significant role for research in detecting bully words,\nidentifying inappropriate comments, detecting different categories of Bengali\nbullies, etc. The dataset is publicly available at\nhttps://data.mendeley.com/datasets/9xjx8twk8p.", "published": "2021-02-04 08:35:18", "link": "http://arxiv.org/abs/2102.02478v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Understanding the Capabilities, Limitations, and Societal Impact of\n  Large Language Models", "abstract": "On October 14th, 2020, researchers from OpenAI, the Stanford Institute for\nHuman-Centered Artificial Intelligence, and other universities convened to\ndiscuss open research questions surrounding GPT-3, the largest\npublicly-disclosed dense language model at the time. The meeting took place\nunder Chatham House Rules. Discussants came from a variety of research\nbackgrounds including computer science, linguistics, philosophy, political\nscience, communications, cyber policy, and more. Broadly, the discussion\ncentered around two main questions: 1) What are the technical capabilities and\nlimitations of large language models? 2) What are the societal effects of\nwidespread use of large language models? Here, we provide a detailed summary of\nthe discussion organized by the two themes above.", "published": "2021-02-04 09:27:04", "link": "http://arxiv.org/abs/2102.02503v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Generalized Zero-shot Intent Detection via Commonsense Knowledge", "abstract": "Identifying user intents from natural language utterances is a crucial step\nin conversational systems that has been extensively studied as a supervised\nclassification problem. However, in practice, new intents emerge after\ndeploying an intent detection model. Thus, these models should seamlessly adapt\nand classify utterances with both seen and unseen intents -- unseen intents\nemerge after deployment and they do not have training data. The few existing\nmodels that target this setting rely heavily on the scarcely available training\ndata and overfit to seen intents data, resulting in a bias to misclassify\nutterances with unseen intents into seen ones. We propose RIDE: an intent\ndetection model that leverages commonsense knowledge in an unsupervised fashion\nto overcome the issue of training data scarcity. RIDE computes robust and\ngeneralizable relationship meta-features that capture deep semantic\nrelationships between utterances and intent labels; these features are computed\nby considering how the concepts in an utterance are linked to those in an\nintent label via commonsense knowledge. Our extensive experimental analysis on\nthree widely-used intent detection benchmarks shows that relationship\nmeta-features significantly increase the accuracy of detecting both seen and\nunseen intents and that RIDE outperforms the state-of-the-art model for unseen\nintents.", "published": "2021-02-04 23:36:41", "link": "http://arxiv.org/abs/2102.02925v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unifying Vision-and-Language Tasks via Text Generation", "abstract": "Existing methods for vision-and-language learning typically require designing\ntask-specific architectures and objectives for each task. For example, a\nmulti-label answer classifier for visual question answering, a region scorer\nfor referring expression comprehension, and a language decoder for image\ncaptioning, etc. To alleviate these hassles, in this work, we propose a unified\nframework that learns different tasks in a single architecture with the same\nlanguage modeling objective, i.e., multimodal conditional text generation,\nwhere our models learn to generate labels in text based on the visual and\ntextual inputs. On 7 popular vision-and-language benchmarks, including visual\nquestion answering, referring expression comprehension, visual commonsense\nreasoning, most of which have been previously modeled as discriminative tasks,\nour generative approach (with a single unified architecture) reaches comparable\nperformance to recent task-specific state-of-the-art vision-and-language\nmodels. Moreover, our generative approach shows better generalization ability\non questions that have rare answers. Also, we show that our framework allows\nmulti-task learning in a single architecture with a single set of parameters,\nachieving similar performance to separately optimized single-task models. Our\ncode is publicly available at: https://github.com/j-min/VL-T5", "published": "2021-02-04 17:59:30", "link": "http://arxiv.org/abs/2102.02779v2", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Controlling Hallucinations at Word Level in Data-to-Text Generation", "abstract": "Data-to-Text Generation (DTG) is a subfield of Natural Language Generation\naiming at transcribing structured data in natural language descriptions. The\nfield has been recently boosted by the use of neural-based generators which\nexhibit on one side great syntactic skills without the need of hand-crafted\npipelines; on the other side, the quality of the generated text reflects the\nquality of the training data, which in realistic settings only offer\nimperfectly aligned structure-text pairs. Consequently, state-of-art neural\nmodels include misleading statements - usually called hallucinations - in their\noutputs. The control of this phenomenon is today a major challenge for DTG, and\nis the problem addressed in the paper.\n  Previous work deal with this issue at the instance level: using an alignment\nscore for each table-reference pair. In contrast, we propose a finer-grained\napproach, arguing that hallucinations should rather be treated at the word\nlevel. Specifically, we propose a Multi-Branch Decoder which is able to\nleverage word-level labels to learn the relevant parts of each training\ninstance. These labels are obtained following a simple and efficient scoring\nprocedure based on co-occurrence analysis and dependency parsing. Extensive\nevaluations, via automated metrics and human judgment on the standard WikiBio\nbenchmark, show the accuracy of our alignment labels and the effectiveness of\nthe proposed Multi-Branch Decoder. Our model is able to reduce and control\nhallucinations, while keeping fluency and coherence in generated texts. Further\nexperiments on a degraded version of ToTTo show that our model could be\nsuccessfully used on very noisy settings.", "published": "2021-02-04 18:58:28", "link": "http://arxiv.org/abs/2102.02810v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE", "68T50 (Primary), 68T07 (Secondary), 68T05", "I.2.6; I.2.7"], "primary_category": "cs.CL"}
{"title": "Chord Embeddings: Analyzing What They Capture and Their Role for Next\n  Chord Prediction and Artist Attribute Prediction", "abstract": "Natural language processing methods have been applied in a variety of music\nstudies, drawing the connection between music and language. In this paper, we\nexpand those approaches by investigating \\textit{chord embeddings}, which we\napply in two case studies to address two key questions: (1) what musical\ninformation do chord embeddings capture?; and (2) how might musical\napplications benefit from them? In our analysis, we show that they capture\nsimilarities between chords that adhere to important relationships described in\nmusic theory. In the first case study, we demonstrate that using chord\nembeddings in a next chord prediction task yields predictions that more closely\nmatch those by experienced musicians. In the second case study, we show the\npotential benefits of using the representations in tasks related to musical\nstylometrics.", "published": "2021-02-04 22:17:17", "link": "http://arxiv.org/abs/2102.02917v1", "categories": ["cs.SD", "cs.AI", "cs.CL"], "primary_category": "cs.SD"}
{"title": "Audio Adversarial Examples: Attacks Using Vocal Masks", "abstract": "We construct audio adversarial examples on automatic Speech-To-Text systems .\nGiven any audio waveform, we produce an another by overlaying an audio vocal\nmask generated from the original audio. We apply our audio adversarial attack\nto five SOTA STT systems: DeepSpeech, Julius, Kaldi, wav2letter@anywhere and\nCMUSphinx. In addition, we engaged human annotators to transcribe the\nadversarial audio. Our experiments show that these adversarial examples fool\nState-Of-The-Art Speech-To-Text systems, yet humans are able to consistently\npick out the speech. The feasibility of this attack introduces a new domain to\nstudy machine and human perception of speech.", "published": "2021-02-04 05:21:10", "link": "http://arxiv.org/abs/2102.02417v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "VSEGAN: Visual Speech Enhancement Generative Adversarial Network", "abstract": "Speech enhancement is an essential task of improving speech quality in noise\nscenario. Several state-of-the-art approaches have introduced visual\ninformation for speech enhancement,since the visual aspect of speech is\nessentially unaffected by acoustic environment. This paper proposes a novel\nframeworkthat involves visual information for speech enhancement, by\nin-corporating a Generative Adversarial Network (GAN). In par-ticular, the\nproposed visual speech enhancement GAN consistof two networks trained in\nadversarial manner, i) a generator that adopts multi-layer feature fusion\nconvolution network to enhance input noisy speech, and ii) a discriminator that\nattemptsto minimize the discrepancy between the distributions of the clean\nspeech signal and enhanced speech signal. Experiment re-sults demonstrated\nsuperior performance of the proposed modelagainst several state-of-the-art", "published": "2021-02-04 13:27:30", "link": "http://arxiv.org/abs/2102.02599v2", "categories": ["eess.AS", "cs.SD", "eess.IV"], "primary_category": "eess.AS"}
{"title": "Low Bit-Rate Wideband Speech Coding: A Deep Generative Model based\n  Approach", "abstract": "Traditional low bit-rate speech coding approach only handles narrowband\nspeech at 8kHz, which limits further improvements in speech quality. Motivated\nby recent successful exploration of deep learning methods for image and speech\ncompression, this paper presents a new approach through vector quantization\n(VQ) of mel-frequency cepstral coefficients (MFCCs) and using a deep generative\nmodel called WaveGlow to provide efficient and high-quality speech coding. The\ncoding feature is sorely an 80-dimension MFCCs vector for 16kHz wideband\nspeech, then speech coding at the bit-rate throughout 1000-2000 bit/s could be\nscalably implemented by applying different VQ schemes for MFCCs vector. This\nnew deep generative network based codec works fast as the WaveGlow model\nabandons the sample-by-sample autoregressive mechanism. We evaluated this new\napproach over the multi-speaker TIMIT corpus, and experimental results\ndemonstrate that it provides better speech quality compared with the\nstate-of-the-art classic MELPe codec at lower bit-rate.", "published": "2021-02-04 14:37:16", "link": "http://arxiv.org/abs/2102.02640v1", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
