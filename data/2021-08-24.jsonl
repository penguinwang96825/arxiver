{"title": "Detection of Criminal Texts for the Polish State Border Guard", "abstract": "This paper describes research on the detection of Polish criminal texts\nappearing on the Internet. We carried out experiments to find the best\navailable setup for the efficient classification of unbalanced and noisy data.\nThe best performance was achieved when our model was fine-tuned on a\npre-trained Polish-based transformer language model. For the detection task, a\nlarge corpus of annotated Internet snippets was collected as training data. We\nshare this dataset and create a new task for the detection of criminal texts\nusing the Gonito platform as the benchmark.", "published": "2021-08-24 08:41:24", "link": "http://arxiv.org/abs/2108.10580v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Weakly Supervised Cross-platform Teenager Detection with Adversarial\n  BERT", "abstract": "Teenager detection is an important case of the age detection task in social\nmedia, which aims to detect teenage users to protect them from negative\ninfluences. The teenager detection task suffers from the scarcity of labelled\ndata, which exacerbates the ability to perform well across social media\nplatforms. To further research in teenager detection in settings where no\nlabelled data is available for a platform, we propose a novel cross-platform\nframework based on Adversarial BERT. Our framework can operate with a limited\namount of labelled instances from the source platform and with no labelled data\nfrom the target platform, transferring knowledge from the source to the target\nsocial media. We experiment on four publicly available datasets, obtaining\nresults demonstrating that our framework can significantly improve over\ncompetitive baseline models on the cross-platform teenager detection task.", "published": "2021-08-24 10:10:40", "link": "http://arxiv.org/abs/2108.10619v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are the Multilingual Models Better? Improving Czech Sentiment with\n  Transformers", "abstract": "In this paper, we aim at improving Czech sentiment with transformer-based\nmodels and their multilingual versions. More concretely, we study the task of\npolarity detection for the Czech language on three sentiment polarity datasets.\nWe fine-tune and perform experiments with five multilingual and three\nmonolingual models. We compare the monolingual and multilingual models'\nperformance, including comparison with the older approach based on recurrent\nneural networks. Furthermore, we test the multilingual models and their ability\nto transfer knowledge from English to Czech (and vice versa) with zero-shot\ncross-lingual classification. Our experiments show that the huge multilingual\nmodels can overcome the performance of the monolingual models. They are also\nable to detect polarity in another language without any training data, with\nperformance not worse than 4.4 % compared to state-of-the-art monolingual\ntrained models. Moreover, we achieved new state-of-the-art results on all three\ndatasets.", "published": "2021-08-24 10:50:01", "link": "http://arxiv.org/abs/2108.10640v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Density-Based Dynamic Curriculum Learning for Intent Detection", "abstract": "Pre-trained language models have achieved noticeable performance on the\nintent detection task. However, due to assigning an identical weight to each\nsample, they suffer from the overfitting of simple samples and the failure to\nlearn complex samples well. To handle this problem, we propose a density-based\ndynamic curriculum learning model. Our model defines the sample's difficulty\nlevel according to their eigenvectors' density. In this way, we exploit the\noverall distribution of all samples' eigenvectors simultaneously. Then we apply\na dynamic curriculum learning strategy, which pays distinct attention to\nsamples of various difficulty levels and alters the proportion of samples\nduring the training process. Through the above operation, simple samples are\nwell-trained, and complex samples are enhanced. Experiments on three open\ndatasets verify that the proposed density-based algorithm can distinguish\nsimple and complex samples significantly. Besides, our model obtains obvious\nimprovement over the strong baselines.", "published": "2021-08-24 12:29:26", "link": "http://arxiv.org/abs/2108.10674v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "More Than Words: Collocation Tokenization for Latent Dirichlet\n  Allocation Models", "abstract": "Traditionally, Latent Dirichlet Allocation (LDA) ingests words in a\ncollection of documents to discover their latent topics using word-document\nco-occurrences. However, it is unclear how to achieve the best results for\nlanguages without marked word boundaries such as Chinese and Thai. Here, we\nexplore the use of Pearson's chi-squared test, t-statistics, and Word Pair\nEncoding (WPE) to produce tokens as input to the LDA model. The Chi-squared, t,\nand WPE tokenizers are trained on Wikipedia text to look for words that should\nbe grouped together, such as compound nouns, proper nouns, and complex event\nverbs. We propose a new metric for measuring the clustering quality in settings\nwhere the vocabularies of the models differ. Based on this metric and other\nestablished metrics, we show that topics trained with merged tokens result in\ntopic keys that are clearer, more coherent, and more effective at\ndistinguishing topics than those unmerged models.", "published": "2021-08-24 14:08:19", "link": "http://arxiv.org/abs/2108.10755v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hybrid Multisource Feature Fusion for the Text Clustering", "abstract": "The text clustering technique is an unsupervised text mining method which are\nused to partition a huge amount of text documents into groups. It has been\nreported that text clustering algorithms are hard to achieve better performance\nthan supervised methods and their clustering performance is highly dependent on\nthe picked text features. Currently, there are many different types of text\nfeature generation algorithms, each of which extracts text features from some\nspecific aspects, such as VSM and distributed word embedding, thus seeking a\nnew way of obtaining features as complete as possible from the corpus is the\nkey to enhance the clustering effects. In this paper, we present a hybrid\nmultisource feature fusion (HMFF) framework comprising three components,\nfeature representation of multimodel, mutual similarity matrices and feature\nfusion, in which we construct mutual similarity matrices for each feature\nsource and fuse discriminative features from mutual similarity matrices by\nreducing dimensionality to generate HMFF features, then k-means clustering\nalgorithm could be configured to partition input samples into groups. The\nexperimental tests show our HMFF framework outperforms other recently published\nalgorithms on 7 of 11 public benchmark datasets and has the leading performance\non the rest 4 benchmark datasets as well. At last, we compare HMFF framework\nwith those competitors on a COVID-19 dataset from the wild with the unknown\ncluster count, which shows the clusters generated by HMFF framework partition\nthose similar samples much closer.", "published": "2021-08-24 19:32:09", "link": "http://arxiv.org/abs/2108.10926v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The State of SLIVAR: What's next for robots, human-robot interaction,\n  and (spoken) dialogue systems?", "abstract": "We synthesize the reported results and recommendations of recent workshops\nand seminars that convened to discuss open questions within the important\nintersection of robotics, human-robot interaction, and spoken dialogue systems\nresearch. The goal of this growing area of research interest is to enable\npeople to more effectively and naturally communicate with robots. To carry\nforward opportunities networking and discussion towards concrete, potentially\nfundable projects, we encourage interested parties to consider participating in\nfuture virtual and in-person discussions and workshops.", "published": "2021-08-24 19:46:30", "link": "http://arxiv.org/abs/2108.10931v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Offensive Language Identification for Tamil Code-Mixed YouTube\n  Comments and Posts", "abstract": "Offensive Language detection in social media platforms has been an active\nfield of research over the past years. In non-native English spoken countries,\nsocial media users mostly use a code-mixed form of text in their\nposts/comments. This poses several challenges in the offensive content\nidentification tasks, and considering the low resources available for Tamil,\nthe task becomes much harder. The current study presents extensive experiments\nusing multiple deep learning, and transfer learning models to detect offensive\ncontent on YouTube. We propose a novel and flexible approach of selective\ntranslation and transliteration techniques to reap better results from\nfine-tuning and ensembling multilingual transformer networks like BERT, Distil-\nBERT, and XLM-RoBERTa. The experimental results showed that ULMFiT is the best\nmodel for this task. The best performing models were ULMFiT and mBERTBiLSTM for\nthis Tamil code-mix dataset instead of more popular transfer learning models\nsuch as Distil- BERT and XLM-RoBERTa and hybrid deep learning models. The\nproposed model ULMFiT and mBERTBiLSTM yielded good results and are promising\nfor effective offensive speech identification in low-resourced languages.", "published": "2021-08-24 20:23:30", "link": "http://arxiv.org/abs/2108.10939v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Robustness Evaluation of Entity Disambiguation Using Prior Probes:the\n  Case of Entity Overshadowing", "abstract": "Entity disambiguation (ED) is the last step of entity linking (EL), when\ncandidate entities are reranked according to the context they appear in. All\ndatasets for training and evaluating models for EL consist of convenience\nsamples, such as news articles and tweets, that propagate the prior probability\nbias of the entity distribution towards more frequently occurring entities. It\nwas previously shown that the performance of the EL systems on such datasets is\noverestimated since it is possible to obtain higher accuracy scores by merely\nlearning the prior. To provide a more adequate evaluation benchmark, we\nintroduce the ShadowLink dataset, which includes 16K short text snippets\nannotated with entity mentions. We evaluate and report the performance of\npopular EL systems on the ShadowLink benchmark. The results show a considerable\ndifference in accuracy between more and less common entities for all of the EL\nsystems under evaluation, demonstrating the effects of prior probability bias\nand entity overshadowing.", "published": "2021-08-24 20:54:56", "link": "http://arxiv.org/abs/2108.10949v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using BERT Encoding and Sentence-Level Language Model for Sentence\n  Ordering", "abstract": "Discovering the logical sequence of events is one of the cornerstones in\nNatural Language Understanding. One approach to learn the sequence of events is\nto study the order of sentences in a coherent text. Sentence ordering can be\napplied in various tasks such as retrieval-based Question Answering, document\nsummarization, storytelling, text generation, and dialogue systems.\nFurthermore, we can learn to model text coherence by learning how to order a\nset of shuffled sentences. Previous research has relied on RNN, LSTM, and\nBiLSTM architecture for learning text language models. However, these networks\nhave performed poorly due to the lack of attention mechanisms. We propose an\nalgorithm for sentence ordering in a corpus of short stories. Our proposed\nmethod uses a language model based on Universal Transformers (UT) that captures\nsentences' dependencies by employing an attention mechanism. Our method\nimproves the previous state-of-the-art in terms of Perfect Match Ratio (PMR)\nscore in the ROCStories dataset, a corpus of nearly 100K short human-made\nstories. The proposed model includes three components: Sentence Encoder,\nLanguage Model, and Sentence Arrangement with Brute Force Search. The first\ncomponent generates sentence embeddings using SBERT-WK pre-trained model\nfine-tuned on the ROCStories data. Then a Universal Transformer network\ngenerates a sentence-level language model. For decoding, the network generates\na candidate sentence as the following sentence of the current sentence. We use\ncosine similarity as a scoring function to assign scores to the candidate\nembedding and the embeddings of other sentences in the shuffled set. Then a\nBrute Force Search is employed to maximize the sum of similarities between\npairs of consecutive sentences.", "published": "2021-08-24 23:03:36", "link": "http://arxiv.org/abs/2108.10986v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prompt-Learning for Fine-Grained Entity Typing", "abstract": "As an effective approach to tune pre-trained language models (PLMs) for\nspecific tasks, prompt-learning has recently attracted much attention from\nresearchers. By using \\textit{cloze}-style language prompts to stimulate the\nversatile knowledge of PLMs, prompt-learning can achieve promising results on a\nseries of NLP tasks, such as natural language inference, sentiment\nclassification, and knowledge probing. In this work, we investigate the\napplication of prompt-learning on fine-grained entity typing in fully\nsupervised, few-shot and zero-shot scenarios. We first develop a simple and\neffective prompt-learning pipeline by constructing entity-oriented verbalizers\nand templates and conducting masked language modeling. Further, to tackle the\nzero-shot regime, we propose a self-supervised strategy that carries out\ndistribution-level optimization in prompt-learning to automatically summarize\nthe information of entity types. Extensive experiments on three fine-grained\nentity typing benchmarks (with up to 86 classes) under fully supervised,\nfew-shot and zero-shot settings show that prompt-learning methods significantly\noutperform fine-tuning baselines, especially when the training data is\ninsufficient.", "published": "2021-08-24 09:39:35", "link": "http://arxiv.org/abs/2108.10604v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Morality-based Assertion and Homophily on Social Media: A Cultural\n  Comparison between English and Japanese Languages", "abstract": "Moral psychology is a domain that deals with moral identity, appraisals and\nemotions. Previous work has primarily focused on moral development and the\nassociated role of culture. Knowing that language is an inherent element of a\nculture, we used the social media platform Twitter to compare moral behaviors\nof Japanese tweets with English tweets. The five basic moral foundations, i.e.,\nCare, Fairness, Ingroup, Authority and Purity, along with the associated\nemotional valence were compared between English and Japanese tweets. The tweets\nfrom Japanese users depicted relatively higher Fairness, Ingroup, and Purity,\nwhereas English tweets expressed more positive emotions for all moral\ndimensions. Considering moral similarities in connecting users on social media,\nwe quantified homophily concerning different moral dimensions using our\nproposed method. The moral dimensions Care, Authority and Purity for English\nand Ingroup, Authority and Purity for Japanese depicted homophily on Twitter.\nOverall, our study uncovers the underlying cultural differences with respect to\nmoral behavior in English- and Japanese-speaking users.", "published": "2021-08-24 11:07:46", "link": "http://arxiv.org/abs/2108.10643v2", "categories": ["cs.CL", "cs.AI", "J.4; I.2.7"], "primary_category": "cs.CL"}
{"title": "Relation Extraction from Tables using Artificially Generated Metadata", "abstract": "Relation Extraction (RE) from tables is the task of identifying relations\nbetween pairs of columns of a table. Generally, RE models for this task require\nlabelled tables for training. These labelled tables can also be generated\nartificially from a Knowledge Graph (KG), which makes the cost to acquire them\nmuch lower in comparison to manual annotations. However, unlike real tables,\nthese synthetic tables lack associated metadata, such as, column-headers,\ncaptions, etc; this is because synthetic tables are created out of KGs that do\nnot store such metadata. Meanwhile, previous works have shown that metadata is\nimportant for accurate RE from tables. To address this issue, we propose\nmethods to artificially create some of this metadata for synthetic tables.\nAfterward, we experiment with a BERT-based model, in line with recently\npublished works, that takes as input a combination of proposed artificial\nmetadata and table content. Our empirical results show that this leads to an\nimprovement of 9\\%-45\\% in F1 score, in absolute terms, over 2 tabular\ndatasets.", "published": "2021-08-24 14:06:17", "link": "http://arxiv.org/abs/2108.10750v3", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Taming the Beast: Learning to Control Neural Conversational Models", "abstract": "This thesis investigates the controllability of deep learning-based,\nend-to-end, generative dialogue systems in both task-oriented and chit-chat\nscenarios. In particular, we study the different aspects of controlling\ngenerative dialogue systems, including controlling styles and topics and\ncontinuously adding and combining dialogue skills. In the three decades since\nthe first dialogue system was commercialized, the basic architecture of such\nsystems has remained substantially unchanged, consisting of four pipelined\nbasic components, namely, natural language understanding (NLU), dialogue state\ntracking (DST), a dialogue manager (DM) and natural language generation (NLG).\nThe dialogue manager, which is the critical component of the modularized\nsystem, controls the response content and style. This module is usually\nprogrammed by rules and is designed to be highly controllable and easily\nextendable. With the emergence of powerful \"deep learning\" architectures,\nend-to-end generative dialogue systems have been proposed to optimize overall\nsystem performance and simplify training. However, these systems cannot be\neasily controlled and extended as the modularized dialogue manager can. This is\nbecause a single neural system is used, which is usually a large pre-trained\nlanguage model (e.g., GPT-2), and thus it is hard to surgically change\ndesirable attributes (e.g., style, topics, etc.). More importantly,\nuncontrollable dialogue systems can generate offensive and even toxic\nresponses. Therefore, in this thesis, we study controllable methods for\nend-to-end generative dialogue systems in task-oriented and chit-chat\nscenarios. Throughout the chapters, we describe 1) how to control the style and\ntopics of chit-chat models, 2) how to continuously control and extend\ntask-oriented dialogue systems, and 3) how to compose and control multi-skill\ndialogue models.", "published": "2021-08-24 07:58:16", "link": "http://arxiv.org/abs/2108.10561v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Reducing Exposure Bias in Training Recurrent Neural Network Transducers", "abstract": "When recurrent neural network transducers (RNNTs) are trained using the\ntypical maximum likelihood criterion, the prediction network is trained only on\nground truth label sequences. This leads to a mismatch during inference, known\nas exposure bias, when the model must deal with label sequences containing\nerrors. In this paper we investigate approaches to reducing exposure bias in\ntraining to improve the generalization of RNNT models for automatic speech\nrecognition (ASR). A label-preserving input perturbation to the prediction\nnetwork is introduced. The input token sequences are perturbed using SwitchOut\nand scheduled sampling based on an additional token language model. Experiments\nconducted on the 300-hour Switchboard dataset demonstrate their effectiveness.\nBy reducing the exposure bias, we show that we can further improve the accuracy\nof a high-performance RNNT ASR model and obtain state-of-the-art results on the\n300-hour Switchboard dataset.", "published": "2021-08-24 15:43:42", "link": "http://arxiv.org/abs/2108.10803v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "SimVLM: Simple Visual Language Model Pretraining with Weak Supervision", "abstract": "With recent progress in joint modeling of visual and textual representations,\nVision-Language Pretraining (VLP) has achieved impressive performance on many\nmultimodal downstream tasks. However, the requirement for expensive annotations\nincluding clean image captions and regional labels limits the scalability of\nexisting approaches, and complicates the pretraining procedure with the\nintroduction of multiple dataset-specific objectives. In this work, we relax\nthese constraints and present a minimalist pretraining framework, named Simple\nVisual Language Model (SimVLM). Unlike prior work, SimVLM reduces the training\ncomplexity by exploiting large-scale weak supervision, and is trained\nend-to-end with a single prefix language modeling objective. Without utilizing\nextra data or task-specific customization, the resulting model significantly\noutperforms previous pretraining methods and achieves new state-of-the-art\nresults on a wide range of discriminative and generative vision-language\nbenchmarks, including VQA (+3.74% vqa-score), NLVR2 (+1.17% accuracy), SNLI-VE\n(+1.37% accuracy) and image captioning tasks (+10.1% average CIDEr score).\nFurthermore, we demonstrate that SimVLM acquires strong generalization and\ntransfer ability, enabling zero-shot behavior including open-ended visual\nquestion answering and cross-modality transfer.", "published": "2021-08-24 18:14:00", "link": "http://arxiv.org/abs/2108.10904v3", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Scorpiano -- A System for Automatic Music Transcription for Monophonic\n  Piano Music", "abstract": "Music transcription is the process of transcribing music audio into music\nnotation. It is a field in which the machines still cannot beat human\nperformance. The main motivation for automatic music transcription is to make\nit possible for anyone playing a musical instrument, to be able to generate the\nmusic notes for a piece of music quickly and accurately. It does not matter if\nthe person is a beginner and simply struggles to find the music score by\nsearching, or an expert who heard a live jazz improvisation and would like to\nreproduce it without losing time doing manual transcription. We propose\nScorpiano -- a system that can automatically generate a music score for simple\nmonophonic piano melody tracks using digital signal processing. The system\nintegrates multiple digital audio processing methods: notes onset detection,\ntempo estimation, beat detection, pitch detection and finally generation of the\nmusic score. The system has proven to give good results for simple piano\nmelodies, comparable to commercially available neural network based systems.", "published": "2021-08-24 12:47:03", "link": "http://arxiv.org/abs/2108.10689v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
