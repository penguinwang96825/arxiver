{"title": "Question Answering and Question Generation as Dual Tasks", "abstract": "We study the problem of joint question answering (QA) and question generation\n(QG) in this paper.\n  Our intuition is that QA and QG have intrinsic connections and these two\ntasks could improve each other.\n  On one side, the QA model judges whether the generated question of a QG model\nis relevant to the answer.\n  On the other side, the QG model provides the probability of generating a\nquestion given the answer, which is a useful evidence that in turn facilitates\nQA.\n  In this paper we regard QA and QG as dual tasks.\n  We propose a training framework that trains the models of QA and QG\nsimultaneously, and explicitly leverages their probabilistic correlation to\nguide the training process of both models.\n  We implement a QG model based on sequence-to-sequence learning, and a QA\nmodel based on recurrent neural network.\n  As all the components of the QA and QG models are differentiable, all the\nparameters involved in these two models could be conventionally learned with\nback propagation.\n  We conduct experiments on three datasets. Empirical results show that our\ntraining framework improves both QA and QG tasks.\n  The improved QA model performs comparably with strong baseline approaches on\nall three datasets.", "published": "2017-06-07 02:06:58", "link": "http://arxiv.org/abs/1706.02027v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Macquarie University at BioASQ 5b -- Query-based Summarisation\n  Techniques for Selecting the Ideal Answers", "abstract": "Macquarie University's contribution to the BioASQ challenge (Task 5b Phase B)\nfocused on the use of query-based extractive summarisation techniques for the\ngeneration of the ideal answers. Four runs were submitted, with approaches\nranging from a trivial system that selected the first $n$ snippets, to the use\nof deep learning approaches under a regression framework. Our experiments and\nthe ROUGE results of the five test batches of BioASQ indicate surprisingly good\nresults for the trivial approach. Overall, most of our runs on the first three\ntest batches achieved the best ROUGE-SU4 results in the challenge.", "published": "2017-06-07 09:04:29", "link": "http://arxiv.org/abs/1706.02095v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Insights into Analogy Completion from the Biomedical Domain", "abstract": "Analogy completion has been a popular task in recent years for evaluating the\nsemantic properties of word embeddings, but the standard methodology makes a\nnumber of assumptions about analogies that do not always hold, either in recent\nbenchmark datasets or when expanding into other domains. Through an analysis of\nanalogies in the biomedical domain, we identify three assumptions: that of a\nSingle Answer for any given analogy, that the pairs involved describe the Same\nRelationship, and that each pair is Informative with respect to the other. We\npropose modifying the standard methodology to relax these assumptions by\nallowing for multiple correct answers, reporting MAP and MRR in addition to\naccuracy, and using multiple example pairs. We further present BMASS, a novel\ndataset for evaluating linguistic regularities in biomedical embeddings, and\ndemonstrate that the relationships described in the dataset pose significant\nsemantic challenges to current word embedding methods.", "published": "2017-06-07 16:24:32", "link": "http://arxiv.org/abs/1706.02241v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Important is Syntactic Parsing Accuracy? An Empirical Evaluation on\n  Rule-Based Sentiment Analysis", "abstract": "Syntactic parsing, the process of obtaining the internal structure of\nsentences in natural languages, is a crucial task for artificial intelligence\napplications that need to extract meaning from natural language text or speech.\nSentiment analysis is one example of application for which parsing has recently\nproven useful.\n  In recent years, there have been significant advances in the accuracy of\nparsing algorithms. In this article, we perform an empirical, task-oriented\nevaluation to determine how parsing accuracy influences the performance of a\nstate-of-the-art rule-based sentiment analysis system that determines the\npolarity of sentences from their parse trees. In particular, we evaluate the\nsystem using four well-known dependency parsers, including both current models\nwith state-of-the-art accuracy and more innacurate models which, however,\nrequire less computational resources.\n  The experiments show that all of the parsers produce similarly good results\nin the sentiment analysis task, without their accuracy having any relevant\ninfluence on the results. Since parsing is currently a task with a relatively\nhigh computational cost that varies strongly between algorithms, this suggests\nthat sentiment analysis researchers and users should prioritize speed over\naccuracy when choosing a parser; and parsing researchers should investigate\nmodels that improve speed further, even at some cost to accuracy.", "published": "2017-06-07 12:03:07", "link": "http://arxiv.org/abs/1706.02141v3", "categories": ["cs.CL", "cs.AI", "68T50, 97R40", "I.2.7"], "primary_category": "cs.CL"}
{"title": "A Mention-Ranking Model for Abstract Anaphora Resolution", "abstract": "Resolving abstract anaphora is an important, but difficult task for text\nunderstanding. Yet, with recent advances in representation learning this task\nbecomes a more tangible aim. A central property of abstract anaphora is that it\nestablishes a relation between the anaphor embedded in the anaphoric sentence\nand its (typically non-nominal) antecedent. We propose a mention-ranking model\nthat learns how abstract anaphors relate to their antecedents with an\nLSTM-Siamese Net. We overcome the lack of training data by generating\nartificial anaphoric sentence--antecedent pairs. Our model outperforms\nstate-of-the-art results on shell noun resolution. We also report first\nbenchmark results on an abstract anaphora subset of the ARRAU corpus. This\ncorpus presents a greater challenge due to a mixture of nominal and pronominal\nanaphors and a greater range of confounders. We found model variants that\noutperform the baselines for nominal anaphors, without training on individual\nanaphor data, but still lag behind for pronominal anaphors. Our model selects\nsyntactically plausible candidates and -- if disregarding syntax --\ndiscriminates candidates using deeper features.", "published": "2017-06-07 16:58:59", "link": "http://arxiv.org/abs/1706.02256v2", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Semi-Supervised Phoneme Recognition with Recurrent Ladder Networks", "abstract": "Ladder networks are a notable new concept in the field of semi-supervised\nlearning by showing state-of-the-art results in image recognition tasks while\nbeing compatible with many existing neural architectures. We present the\nrecurrent ladder network, a novel modification of the ladder network, for\nsemi-supervised learning of recurrent neural networks which we evaluate with a\nphoneme recognition task on the TIMIT corpus. Our results show that the model\nis able to consistently outperform the baseline and achieve fully-supervised\nbaseline performance with only 75% of all labels which demonstrates that the\nmodel is capable of using unsupervised data as an effective regulariser.", "published": "2017-06-07 10:50:47", "link": "http://arxiv.org/abs/1706.02124v2", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Gated Recurrent Neural Tensor Network", "abstract": "Recurrent Neural Networks (RNNs), which are a powerful scheme for modeling\ntemporal and sequential data need to capture long-term dependencies on datasets\nand represent them in hidden layers with a powerful model to capture more\ninformation from inputs. For modeling long-term dependencies in a dataset, the\ngating mechanism concept can help RNNs remember and forget previous\ninformation. Representing the hidden layers of an RNN with more expressive\noperations (i.e., tensor products) helps it learn a more complex relationship\nbetween the current input and the previous hidden layer information. These\nideas can generally improve RNN performances. In this paper, we proposed a\nnovel RNN architecture that combine the concepts of gating mechanism and the\ntensor product into a single model. By combining these two concepts into a\nsingle RNN, our proposed models learn long-term dependencies by modeling with\ngating units and obtain more expressive and direct interaction between input\nand hidden layers using a tensor product on 3-dimensional array (tensor) weight\nparameters. We use Long Short Term Memory (LSTM) RNN and Gated Recurrent Unit\n(GRU) RNN and combine them with a tensor product inside their formulations. Our\nproposed RNNs, which are called a Long-Short Term Memory Recurrent Neural\nTensor Network (LSTMRNTN) and Gated Recurrent Unit Recurrent Neural Tensor\nNetwork (GRURNTN), are made by combining the LSTM and GRU RNN models with the\ntensor product. We conducted experiments with our proposed models on word-level\nand character-level language modeling tasks and revealed that our proposed\nmodels significantly improved their performance compared to our baseline\nmodels.", "published": "2017-06-07 15:05:39", "link": "http://arxiv.org/abs/1706.02222v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Characterizing Types of Convolution in Deep Convolutional Recurrent\n  Neural Networks for Robust Speech Emotion Recognition", "abstract": "Deep convolutional neural networks are being actively investigated in a wide\nrange of speech and audio processing applications including speech recognition,\naudio event detection and computational paralinguistics, owing to their ability\nto reduce factors of variations, for learning from speech. However, studies\nhave suggested to favor a certain type of convolutional operations when\nbuilding a deep convolutional neural network for speech applications although\nthere has been promising results using different types of convolutional\noperations. In this work, we study four types of convolutional operations on\ndifferent input features for speech emotion recognition under noisy and clean\nconditions in order to derive a comprehensive understanding. Since affective\nbehavioral information has been shown to reflect temporally varying of mental\nstate and convolutional operation are applied locally in time, all deep neural\nnetworks share a deep recurrent sub-network architecture for further temporal\nmodeling. We present detailed quantitative module-wise performance analysis to\ngain insights into information flows within the proposed architectures. In\nparticular, we demonstrate the interplay of affective information and the other\nirrelevant information during the progression from one module to another.\nFinally we show that all of our deep neural networks provide state-of-the-art\nperformance on the eNTERFACE'05 corpus.", "published": "2017-06-07 15:17:21", "link": "http://arxiv.org/abs/1706.02901v2", "categories": ["cs.LG", "cs.CL", "cs.MM", "cs.SD"], "primary_category": "cs.LG"}
{"title": "Joint Extraction of Entities and Relations Based on a Novel Tagging\n  Scheme", "abstract": "Joint extraction of entities and relations is an important task in\ninformation extraction. To tackle this problem, we firstly propose a novel\ntagging scheme that can convert the joint extraction task to a tagging problem.\nThen, based on our tagging scheme, we study different end-to-end models to\nextract entities and their relations directly, without identifying entities and\nrelations separately. We conduct experiments on a public dataset produced by\ndistant supervision method and the experimental results show that the tagging\nbased methods are better than most of the existing pipelined and joint learning\nmethods. What's more, the end-to-end model proposed in this paper, achieves the\nbest results on the public dataset.", "published": "2017-06-07 03:14:23", "link": "http://arxiv.org/abs/1706.05075v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
