{"title": "Information Flow in the FTX Bankruptcy: A Network Approach", "abstract": "This paper investigates the cryptocurrency network of the FTX exchange during\nthe collapse of its native token, FTT, to understand how network structures\nadapt to significant financial disruptions, by exploiting vertex centrality\nmeasures. Using proprietary data on the transactional relationships between\nvarious cryptocurrencies, we construct the filtered correlation matrix to\nidentify the most significant relations in the FTX and Binance markets. By\nusing suitable centrality measures - closeness and information centrality - we\nassess network stability during FTX's bankruptcy. The findings document the\nappropriateness of such vertex centralities in understanding the resilience and\nvulnerabilities of financial networks. By tracking the changes in centrality\nvalues before and during the FTX crisis, this study provides useful insights\ninto the structural dynamics of the cryptocurrency market. Results reveal how\ndifferent cryptocurrencies experienced shifts in their network roles due to the\ncrisis. Moreover, our findings highlight the interconnectedness of\ncryptocurrency markets and how the failure of a single entity can lead to\nwidespread repercussions that destabilize other nodes of the network.", "published": "2024-07-17 16:02:51", "link": "http://arxiv.org/abs/2407.12683v1", "categories": ["q-fin.TR", "econ.GN", "q-fin.EC"], "primary_category": "q-fin.TR"}
{"title": "Lacuna Language Learning: Leveraging RNNs for Ranked Text Completion in\n  Digitized Coptic Manuscripts", "abstract": "Ancient manuscripts are frequently damaged, containing gaps in the text known\nas lacunae. In this paper, we present a bidirectional RNN model for character\nprediction of Coptic characters in manuscript lacunae. Our best model performs\nwith 72% accuracy on single character reconstruction, but falls to 37% when\nreconstructing lacunae of various lengths. While not suitable for definitive\nmanuscript reconstruction, we argue that our RNN model can help scholars rank\nthe likelihood of textual reconstructions. As evidence, we use our RNN model to\nrank reconstructions in two early Coptic manuscripts. Our investigation shows\nthat neural models can augment traditional methods of textual restoration,\nproviding scholars with an additional tool to assess lacunae in Coptic\nmanuscripts.", "published": "2024-07-17 01:28:12", "link": "http://arxiv.org/abs/2407.12247v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "On the Feasibility of In-Context Probing for Data Attribution", "abstract": "Data attribution methods are used to measure the contribution of training\ndata towards model outputs, and have several important applications in areas\nsuch as dataset curation and model interpretability. However, many standard\ndata attribution methods, such as influence functions, utilize model gradients\nand are computationally expensive. In our paper, we show in-context probing\n(ICP) -- prompting a LLM -- can serve as a fast proxy for gradient-based data\nattribution for data selection under conditions contingent on data similarity.\nWe study this connection empirically on standard NLP tasks, and show that ICP\nand gradient-based data attribution are well-correlated in identifying\ninfluential training data for tasks that share similar task type and content as\nthe training data. Additionally, fine-tuning models on influential data\nselected by both methods achieves comparable downstream performance, further\nemphasizing their similarities. We also examine the connection between ICP and\ngradient-based data attribution using synthetic data on linear regression\ntasks. Our synthetic data experiments show similar results with those from NLP\ntasks, suggesting that this connection can be isolated in simpler settings,\nwhich offers a pathway to bridging their differences.", "published": "2024-07-17 02:06:56", "link": "http://arxiv.org/abs/2407.12259v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MEDFuse: Multimodal EHR Data Fusion with Masked Lab-Test Modeling and\n  Large Language Models", "abstract": "Electronic health records (EHRs) are multimodal by nature, consisting of\nstructured tabular features like lab tests and unstructured clinical notes. In\nreal-life clinical practice, doctors use complementary multimodal EHR data\nsources to get a clearer picture of patients' health and support clinical\ndecision-making. However, most EHR predictive models do not reflect these\nprocedures, as they either focus on a single modality or overlook the\ninter-modality interactions/redundancy. In this work, we propose MEDFuse, a\nMultimodal EHR Data Fusion framework that incorporates masked lab-test modeling\nand large language models (LLMs) to effectively integrate structured and\nunstructured medical data. MEDFuse leverages multimodal embeddings extracted\nfrom two sources: LLMs fine-tuned on free clinical text and masked tabular\ntransformers trained on structured lab test results. We design a disentangled\ntransformer module, optimized by a mutual information loss to 1) decouple\nmodality-specific and modality-shared information and 2) extract useful joint\nrepresentation from the noise and redundancy present in clinical notes. Through\ncomprehensive validation on the public MIMIC-III dataset and the in-house FEMH\ndataset, MEDFuse demonstrates great potential in advancing clinical\npredictions, achieving over 90% F1 score in the 10-disease multi-label\nclassification task.", "published": "2024-07-17 04:17:09", "link": "http://arxiv.org/abs/2407.12309v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "M2DS: Multilingual Dataset for Multi-document Summarisation", "abstract": "In the rapidly evolving digital era, there is an increasing demand for\nconcise information as individuals seek to distil key insights from various\nsources. Recent attention from researchers on Multi-document Summarisation\n(MDS) has resulted in diverse datasets covering customer reviews, academic\npapers, medical and legal documents, and news articles. However, the\nEnglish-centric nature of these datasets has created a conspicuous void for\nmultilingual datasets in today's globalised digital landscape, where linguistic\ndiversity is celebrated. Media platforms such as British Broadcasting\nCorporation (BBC) have disseminated news in 20+ languages for decades. With\nonly 380 million people speaking English natively as their first language,\naccounting for less than 5% of the global population, the vast majority\nprimarily relies on other languages. These facts underscore the need for\ninclusivity in MDS research, utilising resources from diverse languages.\nRecognising this gap, we present the Multilingual Dataset for Multi-document\nSummarisation (M2DS), which, to the best of our knowledge, is the first dataset\nof its kind. It includes document-summary pairs in five languages from BBC\narticles published during the 2010-2023 period. This paper introduces M2DS,\nemphasising its unique multilingual aspect, and includes baseline scores from\nstate-of-the-art MDS models evaluated on our dataset.", "published": "2024-07-17 06:25:51", "link": "http://arxiv.org/abs/2407.12336v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Word Embedding Dimension Reduction via Weakly-Supervised Feature\n  Selection", "abstract": "As a fundamental task in natural language processing, word embedding converts\neach word into a representation in a vector space. A challenge with word\nembedding is that as the vocabulary grows, the vector space's dimension\nincreases, which can lead to a vast model size. Storing and processing word\nvectors are resource-demanding, especially for mobile edge-devices\napplications. This paper explores word embedding dimension reduction. To\nbalance computational costs and performance, we propose an efficient and\neffective weakly-supervised feature selection method named WordFS. It has two\nvariants, each utilizing novel criteria for feature selection. Experiments on\nvarious tasks (e.g., word and sentence similarity and binary and multi-class\nclassification) indicate that the proposed WordFS model outperforms other\ndimension reduction methods at lower computational costs. We have released the\ncode for reproducibility along with the paper.", "published": "2024-07-17 06:36:09", "link": "http://arxiv.org/abs/2407.12342v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Conversational Query Reformulation with the Guidance of Retrieved\n  Documents", "abstract": "Conversational search seeks to retrieve relevant passages for the given\nquestions in conversational question answering. Conversational Query\nReformulation (CQR) improves conversational search by refining the original\nqueries into de-contextualized forms to resolve the issues in the original\nqueries, such as omissions and coreferences. Previous CQR methods focus on\nimitating human written queries which may not always yield meaningful search\nresults for the retriever. In this paper, we introduce GuideCQR, a framework\nthat refines queries for CQR by leveraging key information from the initially\nretrieved documents. Specifically, GuideCQR extracts keywords and generates\nexpected answers from the retrieved documents, then unifies them with the\nqueries after filtering to add useful information that enhances the search\nprocess. Experimental results demonstrate that our proposed method achieves\nstate-of-the-art performance across multiple datasets, outperforming previous\nCQR methods. Additionally, we show that GuideCQR can get additional performance\ngains in conversational search using various types of queries, even for queries\nwritten by humans.", "published": "2024-07-17 07:39:16", "link": "http://arxiv.org/abs/2407.12363v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Morphosyntactic Analysis for CHILDES", "abstract": "Language development researchers are interested in comparing the process of\nlanguage learning across languages. Unfortunately, it has been difficult to\nconstruct a consistent quantitative framework for such comparisons. However,\nrecent advances in AI (Artificial Intelligence) and ML (Machine Learning) are\nproviding new methods for ASR (automatic speech recognition) and NLP (natural\nlanguage processing) that can be brought to bear on this problem. Using the\nBatchalign2 program (Liu et al., 2023), we have been transcribing and linking\ndata for the CHILDES database and have applied the UD (Universal Dependencies)\nframework to provide a consistent and comparable morphosyntactic analysis for\n27 languages. These new resources open possibilities for deeper crosslinguistic\nstudy of language learning.", "published": "2024-07-17 08:11:24", "link": "http://arxiv.org/abs/2407.12389v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TurkishMMLU: Measuring Massive Multitask Language Understanding in\n  Turkish", "abstract": "Multiple choice question answering tasks evaluate the reasoning,\ncomprehension, and mathematical abilities of Large Language Models (LLMs).\nWhile existing benchmarks employ automatic translation for multilingual\nevaluation, this approach is error-prone and potentially introduces culturally\nbiased questions, especially in social sciences. We introduce the first\nmultitask, multiple-choice Turkish QA benchmark, TurkishMMLU, to evaluate LLMs'\nunderstanding of the Turkish language. TurkishMMLU includes over 10,000\nquestions, covering 9 different subjects from Turkish high-school education\ncurricula. These questions are written by curriculum experts, suitable for the\nhigh-school curricula in Turkey, covering subjects ranging from natural\nsciences and math questions to more culturally representative topics such as\nTurkish Literature and the history of the Turkish Republic. We evaluate over 20\nLLMs, including multilingual open-source (e.g., Gemma, Llama, MT5),\nclosed-source (GPT 4o, Claude, Gemini), and Turkish-adapted (e.g., Trendyol)\nmodels. We provide an extensive evaluation, including zero-shot and few-shot\nevaluation of LLMs, chain-of-thought reasoning, and question difficulty\nanalysis along with model performance. We provide an in-depth analysis of the\nTurkish capabilities and limitations of current LLMs to provide insights for\nfuture LLMs for the Turkish language. We publicly release our code for the\ndataset and evaluation: https://github.com/ArdaYueksel/TurkishMMLU.", "published": "2024-07-17 08:28:55", "link": "http://arxiv.org/abs/2407.12402v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Navigating the Noisy Crowd: Finding Key Information for Claim\n  Verification", "abstract": "Claim verification is a task that involves assessing the truthfulness of a\ngiven claim based on multiple evidence pieces. Using large language models\n(LLMs) for claim verification is a promising way. However, simply feeding all\nthe evidence pieces to an LLM and asking if the claim is factual does not yield\ngood results. The challenge lies in the noisy nature of both the evidence and\nthe claim: evidence passages typically contain irrelevant information, with the\nkey facts hidden within the context, while claims often convey multiple aspects\nsimultaneously. To navigate this \"noisy crowd\" of information, we propose EACon\n(Evidence Abstraction and Claim Deconstruction), a framework designed to find\nkey information within evidence and verify each aspect of a claim separately.\nEACon first finds keywords from the claim and employs fuzzy matching to select\nrelevant keywords for each raw evidence piece. These keywords serve as a guide\nto extract and summarize critical information into abstracted evidence.\nSubsequently, EACon deconstructs the original claim into subclaims, which are\nthen verified against both abstracted and raw evidence individually. We\nevaluate EACon using two open-source LLMs on two challenging datasets. Results\ndemonstrate that EACon consistently and substantially improve LLMs' performance\nin claim verification.", "published": "2024-07-17 09:24:10", "link": "http://arxiv.org/abs/2407.12425v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Continual Learning for Temporal-Sensitive Question Answering", "abstract": "In this study, we explore an emerging research area of Continual Learning for\nTemporal Sensitive Question Answering (CLTSQA). Previous research has primarily\nfocused on Temporal Sensitive Question Answering (TSQA), often overlooking the\nunpredictable nature of future events. In real-world applications, it's crucial\nfor models to continually acquire knowledge over time, rather than relying on a\nstatic, complete dataset. Our paper investigates strategies that enable models\nto adapt to the ever-evolving information landscape, thereby addressing the\nchallenges inherent in CLTSQA. To support our research, we first create a novel\ndataset, divided into five subsets, designed specifically for various stages of\ncontinual learning. We then propose a training framework for CLTSQA that\nintegrates temporal memory replay and temporal contrastive learning. Our\nexperimental results highlight two significant insights: First, the CLTSQA task\nintroduces unique challenges for existing models. Second, our proposed\nframework effectively navigates these challenges, resulting in improved\nperformance.", "published": "2024-07-17 10:47:43", "link": "http://arxiv.org/abs/2407.12470v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Krutrim LLM: A Novel Tokenization Strategy for Multilingual Indic\n  Languages with Petabyte-Scale Data Processing", "abstract": "We present a novel approach to data preparation for developing multilingual\nIndic large language model. Our meticulous data acquisition spans open-source\nand proprietary sources, including Common Crawl, Indic books, news articles,\nand Wikipedia, ensuring a diverse and rich linguistic representation. For each\nIndic language, we design a custom preprocessing pipeline to effectively\neliminate redundant and low-quality text content. Additionally, we perform\ndeduplication on Common Crawl data to address the redundancy present in 70% of\nthe crawled web pages. This study focuses on developing high-quality data,\noptimizing tokenization for our multilingual dataset for Indic large language\nmodels with 3B and 7B parameters, engineered for superior performance in Indic\nlanguages. We introduce a novel multilingual tokenizer training strategy,\ndemonstrating our custom-trained Indic tokenizer outperforms the\nstate-of-the-art OpenAI Tiktoken tokenizer, achieving a superior token-to-word\nratio for Indic languages.", "published": "2024-07-17 11:06:27", "link": "http://arxiv.org/abs/2407.12481v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automate or Assist? The Role of Computational Models in Identifying\n  Gendered Discourse in US Capital Trial Transcripts", "abstract": "The language used by US courtroom actors in criminal trials has long been\nstudied for biases. However, systematic studies for bias in high-stakes court\ntrials have been difficult, due to the nuanced nature of bias and the legal\nexpertise required. Large language models offer the possibility to automate\nannotation. But validating the computational approach requires both an\nunderstanding of how automated methods fit in existing annotation workflows and\nwhat they really offer. We present a case study of adding a computational model\nto a complex and high-stakes problem: identifying gender-biased language in US\ncapital trials for women defendants. Our team of experienced death-penalty\nlawyers and NLP technologists pursue a three-phase study: first annotating\nmanually, then training and evaluating computational models, and finally\ncomparing expert annotations to model predictions. Unlike many typical NLP\ntasks, annotating for gender bias in months-long capital trials is complicated,\nwith many individual judgment calls. Contrary to standard arguments for\nautomation that are based on efficiency and scalability, legal experts find the\ncomputational models most useful in providing opportunities to reflect on their\nown bias in annotation and to build consensus on annotation rules. This\nexperience suggests that seeking to replace experts with computational models\nfor complex annotation is both unrealistic and undesirable. Rather,\ncomputational models offer valuable opportunities to assist the legal experts\nin annotation-based studies.", "published": "2024-07-17 11:30:04", "link": "http://arxiv.org/abs/2407.12500v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Case2Code: Scalable Synthetic Data for Code Generation", "abstract": "Large Language Models (LLMs) have shown outstanding breakthroughs in code\ngeneration. Recent work improves code LLMs by training on synthetic data\ngenerated by some powerful LLMs, which can be challenging to scale due to the\ndependence on a teacher model and high generation costs. In this paper, we\nfocus on synthesizing code data at scale and propose a \\textbf{Case2Code} task\nby exploiting the expressiveness and correctness of programs.\n\\textbf{Case2Code} is an inductive inference task that aims to infer underlying\ncode implementations by observing input-output examples or program behaviors,\nBy incorporating LLMs to generate program inputs, and executing the program\nwith these inputs to obtain the program outputs, we can synthesize diverse and\nhigh-quality \\textbf{Case2Code} data at scale for training and evaluating code\nLLMs. Experimental results show that case-to-code induction is challenging for\ncurrent representative LLMs if they are untrained. Models trained with\n\\textbf{Case2Code} improve performance not only on distribution case-to-code\ninduction but also on various coding-generation tasks, demonstrating the great\npotential of large-scale synthetic data and inductive learning.", "published": "2024-07-17 11:35:00", "link": "http://arxiv.org/abs/2407.12504v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "$\\textit{GeoHard}$: Towards Measuring Class-wise Hardness through\n  Modelling Class Semantics", "abstract": "Recent advances in measuring hardness-wise properties of data guide language\nmodels in sample selection within low-resource scenarios. However,\nclass-specific properties are overlooked for task setup and learning. How will\nthese properties influence model learning and is it generalizable across\ndatasets? To answer this question, this work formally initiates the concept of\n$\\textit{class-wise hardness}$. Experiments across eight natural language\nunderstanding (NLU) datasets demonstrate a consistent hardness distribution\nacross learning paradigms, models, and human judgment. Subsequent experiments\nunveil a notable challenge in measuring such class-wise hardness with\ninstance-level metrics in previous works. To address this, we propose\n$\\textit{GeoHard}$ for class-wise hardness measurement by modeling class\ngeometry in the semantic embedding space. $\\textit{GeoHard}$ surpasses\ninstance-level metrics by over 59 percent on $\\textit{Pearson}$'s correlation\non measuring class-wise hardness. Our analysis theoretically and empirically\nunderscores the generality of $\\textit{GeoHard}$ as a fresh perspective on data\ndiagnosis. Additionally, we showcase how understanding class-wise hardness can\npractically aid in improving task learning.", "published": "2024-07-17 11:53:39", "link": "http://arxiv.org/abs/2407.12512v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Initializing Transformers with Pre-trained Embeddings", "abstract": "It has become common practice now to use random initialization schemes,\nrather than the pre-trained embeddings, when training transformer based models\nfrom scratch. Indeed, we find that pre-trained word embeddings from GloVe, and\nsome sub-word embeddings extracted from language models such as T5 and mT5 fare\nmuch worse compared to random initialization. This is counter-intuitive given\nthe well-known representational and transfer-learning advantages of\npre-training. Interestingly, we also find that BERT and mBERT embeddings fare\nbetter than random initialization, showing the advantages of pre-trained\nrepresentations. In this work, we posit two potential factors that contribute\nto these mixed results: the model sensitivity to parameter distribution and the\nembedding interactions with position encodings. We observe that pre-trained\nGloVe, T5, and mT5 embeddings have a wider distribution of values. As argued in\nthe initialization studies, such large value initializations can lead to poor\ntraining because of saturated outputs. Further, the larger embedding values\ncan, in effect, absorb the smaller position encoding values when added\ntogether, thus losing position information. Standardizing the pre-trained\nembeddings to a narrow range (e.g. as prescribed by Xavier) leads to\nsubstantial gains for Glove, T5, and mT5 embeddings. On the other hand, BERT\npre-trained embeddings, while larger, are still relatively closer to Xavier\ninitialization range which may allow it to effectively transfer the pre-trained\nknowledge.", "published": "2024-07-17 11:57:10", "link": "http://arxiv.org/abs/2407.12514v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Crafting the Path: Robust Query Rewriting for Information Retrieval", "abstract": "Query rewriting aims to generate a new query that can complement the original\nquery to improve the information retrieval system. Recent studies on query\nrewriting, such as query2doc, query2expand and querey2cot, rely on the internal\nknowledge of Large Language Models (LLMs) to generate a relevant passage to add\ninformation to the query. Nevertheless, the efficacy of these methodologies may\nmarkedly decline in instances where the requisite knowledge is not encapsulated\nwithin the model's intrinsic parameters. In this paper, we propose a novel\nstructured query rewriting method called Crafting the Path tailored for\nretrieval systems. Crafting the Path involves a three-step process that crafts\nquery-related information necessary for finding the passages to be searched in\neach step. Specifically, the Crafting the Path begins with Query Concept\nComprehension, proceeds to Query Type Identification, and finally conducts\nExpected Answer Extraction. Experimental results show that our method\noutperforms previous rewriting methods, especially in less familiar domains for\nLLMs. We demonstrate that our method is less dependent on the internal\nparameter knowledge of the model and generates queries with fewer factual\ninaccuracies. Furthermore, we observe that \\name{} demonstrates superior\nperformance in the retrieval-augmented generation scenarios.", "published": "2024-07-17 13:11:28", "link": "http://arxiv.org/abs/2407.12529v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Domain-specific or Uncertainty-aware models: Does it really make a\n  difference for biomedical text classification?", "abstract": "The success of pretrained language models (PLMs) across a spate of use-cases\nhas led to significant investment from the NLP community towards building\ndomain-specific foundational models. On the other hand, in mission critical\nsettings such as biomedical applications, other aspects also factor in-chief of\nwhich is a model's ability to produce reasonable estimates of its own\nuncertainty. In the present study, we discuss these two desiderata through the\nlens of how they shape the entropy of a model's output probability\ndistribution. We find that domain specificity and uncertainty awareness can\noften be successfully combined, but the exact task at hand weighs in much more\nstrongly.", "published": "2024-07-17 14:52:46", "link": "http://arxiv.org/abs/2407.12626v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Subgraph-Aware Training of Language Models for Knowledge Graph\n  Completion Using Structure-Aware Contrastive Learning", "abstract": "Fine-tuning pre-trained language models (PLMs) has recently shown a potential\nto improve knowledge graph completion (KGC). However, most PLM-based methods\nfocus solely on encoding textual information, neglecting the long-tailed nature\nof knowledge graphs and their various topological structures, e.g., subgraphs,\nshortest paths, and degrees. We claim that this is a major obstacle to\nachieving higher accuracy of PLMs for KGC. To this end, we propose a\nSubgraph-Aware Training framework for KGC (SATKGC) with two ideas: (i)\nsubgraph-aware mini-batching to encourage hard negative sampling and to\nmitigate an imbalance in the frequency of entity occurrences during training,\nand (ii) new contrastive learning to focus more on harder in-batch negative\ntriples and harder positive triples in terms of the structural properties of\nthe knowledge graph. To the best of our knowledge, this is the first study to\ncomprehensively incorporate the structural inductive bias of the knowledge\ngraph into fine-tuning PLMs. Extensive experiments on three KGC benchmarks\ndemonstrate the superiority of SATKGC. Our code is available.", "published": "2024-07-17 16:25:37", "link": "http://arxiv.org/abs/2407.12703v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is Sarcasm Detection A Step-by-Step Reasoning Process in Large Language\n  Models?", "abstract": "Elaborating a series of intermediate reasoning steps significantly improves\nthe ability of large language models (LLMs) to solve complex problems, as such\nsteps would evoke LLMs to think sequentially. However, human sarcasm\nunderstanding is often considered an intuitive and holistic cognitive process,\nin which various linguistic, contextual, and emotional cues are integrated to\nform a comprehensive understanding, in a way that does not necessarily follow a\nstep-by-step fashion. To verify the validity of this argument, we introduce a\nnew prompting framework (called SarcasmCue) containing four sub-methods, viz.\nchain of contradiction (CoC), graph of cues (GoC), bagging of cues (BoC) and\ntensor of cues (ToC), which elicits LLMs to detect human sarcasm by considering\nsequential and non-sequential prompting methods. Through a comprehensive\nempirical comparison on four benchmarks, we highlight three key findings: (1)\nCoC and GoC show superior performance with more advanced models like GPT-4 and\nClaude 3.5, with an improvement of 3.5%. (2) ToC significantly outperforms\nother methods when smaller LLMs are evaluated, boosting the F1 score by 29.7%\nover the best baseline. (3) Our proposed framework consistently pushes the\nstate-of-the-art (i.e., ToT) by 4.2%, 2.0%, 29.7%, and 58.2% in F1 scores\nacross four datasets. This demonstrates the effectiveness and stability of the\nproposed framework.", "published": "2024-07-17 16:42:03", "link": "http://arxiv.org/abs/2407.12725v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A LLM Benchmark based on the Minecraft Builder Dialog Agent Task", "abstract": "In this work we proposing adapting the Minecraft builder task into an LLM\nbenchmark suitable for evaluating LLM ability in spatially orientated tasks,\nand informing builder agent design. Previous works have proposed corpora with\nvarying complex structures, and human written instructions. We instead attempt\nto provide a comprehensive synthetic benchmark for testing builder agents over\na series of distinct tasks that comprise of common building operations. We\nbelieve this approach allows us to probe specific strengths and weaknesses of\ndifferent agents, and test the ability of LLMs in the challenging area of\nspatial reasoning and vector based math.", "published": "2024-07-17 16:52:23", "link": "http://arxiv.org/abs/2407.12734v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HDLCopilot: Natural Language Exploration of Hardware Designs and\n  Libraries", "abstract": "Hardware design workflows often involve working with Process Design Kits\n(PDKs) from various fabrication labs, each containing its own set of standard\ncell libraries optimized for metrics such as speed, power, or density. These\nlibraries include multiple views for information on timing and electrical\nproperties of cells, cell layout details, and process design rules. Engineers\ntypically navigate between the design and the target technology to make\ninformed decisions on different design scenarios, such as selecting specific\ngates for area optimization or enhancing critical path speed. Navigating this\ncomplex landscape to retrieve specific information about gates or design rules\nis often time-consuming and error-prone. To address this, we present\nHDLCopilot, a multi-agent collaborative framework powered by large language\nmodels that enables engineers to streamline interactions with hardware design\nand PDKs through natural language queries. HDLCopilot enables engineers to\nquickly access relevant information on gates and design rules, evaluate\ntradeoffs related to area, speed, and power in order to make informed decisions\nefficiently and accurately. The framework achieves an execution accuracy of\n96.33\\% on a diverse set of complex natural language queries. HDLCopilot\npositions itself as a powerful assistant in hardware design workflows,\nenhancing productivity and reducing potential human errors.", "published": "2024-07-17 17:11:13", "link": "http://arxiv.org/abs/2407.12749v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Turkish Delights: a Dataset on Turkish Euphemisms", "abstract": "Euphemisms are a form of figurative language relatively understudied in\nnatural language processing. This research extends the current computational\nwork on potentially euphemistic terms (PETs) to Turkish. We introduce the\nTurkish PET dataset, the first available of its kind in the field. By creating\na list of euphemisms in Turkish, collecting example contexts, and annotating\nthem, we provide both euphemistic and non-euphemistic examples of PETs in\nTurkish. We describe the dataset and methodologies, and also experiment with\ntransformer-based models on Turkish euphemism detection by using our dataset\nfor binary classification. We compare performances across models using F1,\naccuracy, and precision as evaluation metrics.", "published": "2024-07-17 22:13:42", "link": "http://arxiv.org/abs/2407.13040v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Establishing Knowledge Preference in Language Models", "abstract": "Language models are known to encode a great amount of factual knowledge\nthrough pretraining. However, such knowledge might be insufficient to cater to\nuser requests, requiring the model to integrate external knowledge sources and\nadhere to user-provided specifications. When answering questions about ongoing\nevents, the model should use recent news articles to update its response; when\nasked to provide recommendations, the model should prioritize user\nspecifications over retrieved product reviews; when some facts are edited in\nthe model, the updated facts should override all prior knowledge learned by the\nmodel even if they are conflicting. In all of the cases above, the model faces\na decision between its own parametric knowledge, (retrieved) contextual\nknowledge, and user instruction knowledge. In this paper, we (1) unify such\nsettings into the problem of knowledge preference and define a three-level\npreference hierarchy over these knowledge sources; (2) compile a collection of\nexisting datasets IfQA, MQuAKE, and MRQA covering a combination of settings\n(with/without user specifications, with/without context documents) to\nsystematically evaluate how well models obey the intended knowledge preference;\nand (3) propose a dataset synthesis method that composes diverse\nquestion-answer pairs with user assumptions and related context to directly\nfine-tune LMs for instilling the hierarchy of knowledge. We demonstrate that a\n7B model, fine-tuned on only a few thousand examples automatically generated by\nour proposed method, effectively achieves superior performance (more than 18%\nimprovement across all evaluation benchmarks) in adhering to the desired\nknowledge preference hierarchy.", "published": "2024-07-17 23:16:11", "link": "http://arxiv.org/abs/2407.13048v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multimodal Reranking for Knowledge-Intensive Visual Question Answering", "abstract": "Knowledge-intensive visual question answering requires models to effectively\nuse external knowledge to help answer visual questions. A typical pipeline\nincludes a knowledge retriever and an answer generator. However, a retriever\nthat utilizes local information, such as an image patch, may not provide\nreliable question-candidate relevance scores. Besides, the two-tower\narchitecture also limits the relevance score modeling of a retriever to select\ntop candidates for answer generator reasoning. In this paper, we introduce an\nadditional module, a multi-modal reranker, to improve the ranking quality of\nknowledge candidates for answer generation. Our reranking module takes\nmulti-modal information from both candidates and questions and performs\ncross-item interaction for better relevance score modeling. Experiments on\nOK-VQA and A-OKVQA show that multi-modal reranker from distant supervision\nprovides consistent improvements. We also find a training-testing discrepancy\nwith reranking in answer generation, where performance improves if training\nknowledge candidates are similar to or noisier than those used in testing.", "published": "2024-07-17 02:58:52", "link": "http://arxiv.org/abs/2407.12277v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Better Angels of Machine Personality: How Personality Relates to LLM\n  Safety", "abstract": "Personality psychologists have analyzed the relationship between personality\nand safety behaviors in human society. Although Large Language Models (LLMs)\ndemonstrate personality traits, the relationship between personality traits and\nsafety abilities in LLMs still remains a mystery. In this paper, we discover\nthat LLMs' personality traits are closely related to their safety abilities,\ni.e., toxicity, privacy, and fairness, based on the reliable MBTI-M scale.\nMeanwhile, the safety alignment generally increases various LLMs' Extraversion,\nSensing, and Judging traits. According to such findings, we can edit LLMs'\npersonality traits and improve their safety performance, e.g., inducing\npersonality from ISTJ to ISTP resulted in a relative improvement of\napproximately 43% and 10% in privacy and fairness performance, respectively.\nAdditionally, we find that LLMs with different personality traits are\ndifferentially susceptible to jailbreak. This study pioneers the investigation\nof LLM safety from a personality perspective, providing new insights into LLM\nsafety enhancement.", "published": "2024-07-17 06:36:29", "link": "http://arxiv.org/abs/2407.12344v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "ProcTag: Process Tagging for Assessing the Efficacy of Document\n  Instruction Data", "abstract": "Recently, large language models (LLMs) and multimodal large language models\n(MLLMs) have demonstrated promising results on document visual question\nanswering (VQA) task, particularly after training on document instruction\ndatasets. An effective evaluation method for document instruction data is\ncrucial in constructing instruction data with high efficacy, which, in turn,\nfacilitates the training of LLMs and MLLMs for document VQA. However, most\nexisting evaluation methods for instruction data are limited to the textual\ncontent of the instructions themselves, thereby hindering the effective\nassessment of document instruction datasets and constraining their\nconstruction. In this paper, we propose ProcTag, a data-oriented method that\nassesses the efficacy of document instruction data. ProcTag innovatively\nperforms tagging on the execution process of instructions rather than the\ninstruction text itself. By leveraging the diversity and complexity of these\ntags to assess the efficacy of the given dataset, ProcTag enables selective\nsampling or filtering of document instructions. Furthermore, DocLayPrompt, a\nnovel semi-structured layout-aware document prompting strategy, is proposed for\neffectively representing documents. Experiments demonstrate that sampling\nexisting open-sourced and generated document VQA/instruction datasets with\nProcTag significantly outperforms current methods for evaluating instruction\ndata. Impressively, with ProcTag-based sampling in the generated document\ndatasets, only 30.5\\% of the document instructions are required to achieve\n100\\% efficacy compared to the complete dataset. The code is publicly available\nat\nhttps://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/DocumentUnderstanding/ProcTag.", "published": "2024-07-17 07:29:59", "link": "http://arxiv.org/abs/2407.12358v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Deep Learning-based Sentiment Analysis of Olympics Tweets", "abstract": "Sentiment analysis (SA), is an approach of natural language processing (NLP)\nfor determining a text's emotional tone by analyzing subjective information\nsuch as views, feelings, and attitudes toward specific topics, products,\nservices, events, or experiences. This study attempts to develop an advanced\ndeep learning (DL) model for SA to understand global audience emotions through\ntweets in the context of the Olympic Games. The findings represent global\nattitudes around the Olympics and contribute to advancing the SA models. We\nhave used NLP for tweet pre-processing and sophisticated DL models for arguing\nwith SA, this research enhances the reliability and accuracy of sentiment\nclassification. The study focuses on data selection, preprocessing,\nvisualization, feature extraction, and model building, featuring a baseline\nNa\\\"ive Bayes (NB) model and three advanced DL models: Convolutional Neural\nNetwork (CNN), Bidirectional Long Short-Term Memory (BiLSTM), and Bidirectional\nEncoder Representations from Transformers (BERT). The results of the\nexperiments show that the BERT model can efficiently classify sentiments\nrelated to the Olympics, achieving the highest accuracy of 99.23%.", "published": "2024-07-17 07:55:04", "link": "http://arxiv.org/abs/2407.12376v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sharif-STR at SemEval-2024 Task 1: Transformer as a Regression Model for\n  Fine-Grained Scoring of Textual Semantic Relations", "abstract": "Semantic Textual Relatedness holds significant relevance in Natural Language\nProcessing, finding applications across various domains. Traditionally,\napproaches to STR have relied on knowledge-based and statistical methods.\nHowever, with the emergence of Large Language Models, there has been a paradigm\nshift, ushering in new methodologies. In this paper, we delve into the\ninvestigation of sentence-level STR within Track A (Supervised) by leveraging\nfine-tuning techniques on the RoBERTa transformer. Our study focuses on\nassessing the efficacy of this approach across different languages. Notably,\nour findings indicate promising advancements in STR performance, particularly\nin Latin languages. Specifically, our results demonstrate notable improvements\nin English, achieving a correlation of 0.82 and securing a commendable 19th\nrank. Similarly, in Spanish, we achieved a correlation of 0.67, securing the\n15th position. However, our approach encounters challenges in languages like\nArabic, where we observed a correlation of only 0.38, resulting in a 20th rank.", "published": "2024-07-17 09:25:18", "link": "http://arxiv.org/abs/2407.12426v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Novel Dependency Framework for Enhancing Discourse Data Analysis", "abstract": "The development of different theories of discourse structure has led to the\nestablishment of discourse corpora based on these theories. However, the\nexistence of discourse corpora established on different theoretical bases\ncreates challenges when it comes to exploring them in a consistent and cohesive\nway. This study has as its primary focus the conversion of PDTB annotations\ninto dependency structures. It employs refined BERT-based discourse parsers to\ntest the validity of the dependency data derived from the PDTB-style corpora in\nEnglish, Chinese, and several other languages. By converting both PDTB and RST\nannotations for the same texts into dependencies, this study also applies\n``dependency distance'' metrics to examine the correlation between RST\ndependencies and PDTB dependencies in English. The results show that the PDTB\ndependency data is valid and that there is a strong correlation between the two\ntypes of dependency distance. This study presents a comprehensive approach for\nanalyzing and evaluating discourse corpora by employing discourse dependencies\nto achieve unified analysis. By applying dependency representations, we can\nextract data from PDTB, RST, and SDRT corpora in a coherent and unified manner.\nMoreover, the cross-linguistic validation establishes the framework's\ngeneralizability beyond English. The establishment of this comprehensive\ndependency framework overcomes limitations of existing discourse corpora,\nsupporting a diverse range of algorithms and facilitating further studies in\ncomputational discourse analysis and language sciences.", "published": "2024-07-17 10:55:00", "link": "http://arxiv.org/abs/2407.12473v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluating Linguistic Capabilities of Multimodal LLMs in the Lens of\n  Few-Shot Learning", "abstract": "The linguistic capabilities of Multimodal Large Language Models (MLLMs) are\ncritical for their effective application across diverse tasks. This study aims\nto evaluate the performance of MLLMs on the VALSE benchmark, focusing on the\nefficacy of few-shot In-Context Learning (ICL), and Chain-of-Thought (CoT)\nprompting. We conducted a comprehensive assessment of state-of-the-art MLLMs,\nvarying in model size and pretraining datasets. The experimental results reveal\nthat ICL and CoT prompting significantly boost model performance, particularly\nin tasks requiring complex reasoning and contextual understanding. Models\npretrained on captioning datasets show superior zero-shot performance, while\nthose trained on interleaved image-text data benefit from few-shot learning.\nOur findings provide valuable insights into optimizing MLLMs for better\ngrounding of language in visual contexts, highlighting the importance of the\ncomposition of pretraining data and the potential of few-shot learning\nstrategies to improve the reasoning abilities of MLLMs.", "published": "2024-07-17 11:26:47", "link": "http://arxiv.org/abs/2407.12498v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Struct-X: Enhancing Large Language Models Reasoning with Structured Data", "abstract": "Structured data, rich in logical and relational information, has the\npotential to enhance the reasoning abilities of large language models (LLMs).\nStill, its integration poses a challenge due to the risk of overwhelming LLMs\nwith excessive tokens and irrelevant context information. To address this, we\npropose Struct-X, a novel framework that operates through five key phases:\n``read-model-fill-reflect-reason'' efficiently enabling LLMs to utilize\nstructured data. It begins by encoding structured data into a topological space\nusing graph embeddings, followed by filling in missing entity information with\nknowledge retrieval modules, and filtering out irrelevant tokens via a\nself-supervised module. The final phase involves constructing a topological\nnetwork with selected tokens to further reduce the total token length for more\neffective LLM inference. Additionally, Struct-X includes an Auxiliary Module\ntrained to generate prompts, aiding LLMs in analyzing structured data.\nExtensive experiments on benchmarks, including the knowledge graph\nquestion-answer task and the long document reading comprehension task, show\nthat Struct-X notably improves LLM reasoning, demonstrating the effectiveness\nof structured data augmentation in improving LLM inference with complex input\ncontext.", "published": "2024-07-17 13:06:25", "link": "http://arxiv.org/abs/2407.12522v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Collaborative Intelligence: Propagating Intentions and Reasoning\n  for Multi-Agent Coordination with Large Language Models", "abstract": "Effective collaboration in multi-agent systems requires communicating goals\nand intentions between agents. Current agent frameworks often suffer from\ndependencies on single-agent execution and lack robust inter-module\ncommunication, frequently leading to suboptimal multi-agent reinforcement\nlearning (MARL) policies and inadequate task coordination. To address these\nchallenges, we present a framework for training large language models (LLMs) as\ncollaborative agents to enable coordinated behaviors in cooperative MARL. Each\nagent maintains a private intention consisting of its current goal and\nassociated sub-tasks. Agents broadcast their intentions periodically, allowing\nother agents to infer coordination tasks. A propagation network transforms\nbroadcast intentions into teammate-specific communication messages, sharing\nrelevant goals with designated teammates. The architecture of our framework is\nstructured into planning, grounding, and execution modules. During execution,\nmultiple agents interact in a downstream environment and communicate intentions\nto enable coordinated behaviors. The grounding module dynamically adapts\ncomprehension strategies based on emerging coordination patterns, while\nfeedback from execution agents influnces the planning module, enabling the\ndynamic re-planning of sub-tasks. Results in collaborative environment\nsimulation demonstrate intention propagation reduces miscoordination errors by\naligning sub-task dependencies between agents. Agents learn when to communicate\nintentions and which teammates require task details, resulting in emergent\ncoordinated behaviors. This demonstrates the efficacy of intention sharing for\ncooperative multi-agent RL based on LLMs.", "published": "2024-07-17 13:14:00", "link": "http://arxiv.org/abs/2407.12532v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AudienceView: AI-Assisted Interpretation of Audience Feedback in\n  Journalism", "abstract": "Understanding and making use of audience feedback is important but difficult\nfor journalists, who now face an impractically large volume of audience\ncomments online. We introduce AudienceView, an online tool to help journalists\ncategorize and interpret this feedback by leveraging large language models\n(LLMs). AudienceView identifies themes and topics, connects them back to\nspecific comments, provides ways to visualize the sentiment and distribution of\nthe comments, and helps users develop ideas for subsequent reporting projects.\nWe consider how such tools can be useful in a journalist's workflow, and\nemphasize the importance of contextual awareness and human judgment.", "published": "2024-07-17 14:41:35", "link": "http://arxiv.org/abs/2407.12613v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Harnessing the Power of Artificial Intelligence to Vitalize Endangered\n  Indigenous Languages: Technologies and Experiences", "abstract": "Since 2022 we have been exploring application areas and technologies in which\nArtificial Intelligence (AI) and modern Natural Language Processing (NLP), such\nas Large Language Models (LLMs), can be employed to foster the usage and\nfacilitate the documentation of Indigenous languages which are in danger of\ndisappearing. We start by discussing the decreasing diversity of languages in\nthe world and how working with Indigenous languages poses unique ethical\nchallenges for AI and NLP. To address those challenges, we propose an\nalternative development AI cycle based on community engagement and usage. Then,\nwe report encouraging results in the development of high-quality machine\nlearning translators for Indigenous languages by fine-tuning state-of-the-art\n(SOTA) translators with tiny amounts of data and discuss how to avoid some\ncommon pitfalls in the process. We also present prototypes we have built in\nprojects done in 2023 and 2024 with Indigenous communities in Brazil, aimed at\nfacilitating writing, and discuss the development of Indigenous Language Models\n(ILMs) as a replicable and scalable way to create spell-checkers, next-word\npredictors, and similar tools. Finally, we discuss how we envision a future for\nlanguage documentation where dying languages are preserved as interactive\nlanguage models.", "published": "2024-07-17 14:46:37", "link": "http://arxiv.org/abs/2407.12620v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LMMs-Eval: Reality Check on the Evaluation of Large Multimodal Models", "abstract": "The advances of large foundation models necessitate wide-coverage, low-cost,\nand zero-contamination benchmarks. Despite continuous exploration of language\nmodel evaluations, comprehensive studies on the evaluation of Large Multi-modal\nModels (LMMs) remain limited. In this work, we introduce LMMS-EVAL, a unified\nand standardized multimodal benchmark framework with over 50 tasks and more\nthan 10 models to promote transparent and reproducible evaluations. Although\nLMMS-EVAL offers comprehensive coverage, we find it still falls short in\nachieving low cost and zero contamination. To approach this evaluation\ntrilemma, we further introduce LMMS-EVAL LITE, a pruned evaluation toolkit that\nemphasizes both coverage and efficiency. Additionally, we present Multimodal\nLIVEBENCH that utilizes continuously updating news and online forums to assess\nmodels' generalization abilities in the wild, featuring a low-cost and\nzero-contamination evaluation approach. In summary, our work highlights the\nimportance of considering the evaluation trilemma and provides practical\nsolutions to navigate the trade-offs in evaluating large multi-modal models,\npaving the way for more effective and reliable benchmarking of LMMs. We\nopensource our codebase and maintain leaderboard of LIVEBENCH at\nhttps://github.com/EvolvingLMMs-Lab/lmms-eval and\nhttps://huggingface.co/spaces/lmms-lab/LiveBench.", "published": "2024-07-17 17:51:53", "link": "http://arxiv.org/abs/2407.12772v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Explainable Biomedical Hypothesis Generation via Retrieval Augmented\n  Generation enabled Large Language Models", "abstract": "The vast amount of biomedical information available today presents a\nsignificant challenge for investigators seeking to digest, process, and\nunderstand these findings effectively. Large Language Models (LLMs) have\nemerged as powerful tools to navigate this complex and challenging data\nlandscape. However, LLMs may lead to hallucinatory responses, making Retrieval\nAugmented Generation (RAG) crucial for achieving accurate information. In this\nprotocol, we present RUGGED (Retrieval Under Graph-Guided Explainable disease\nDistinction), a comprehensive workflow designed to support investigators with\nknowledge integration and hypothesis generation, identifying validated paths\nforward. Relevant biomedical information from publications and knowledge bases\nare reviewed, integrated, and extracted via text-mining association analysis\nand explainable graph prediction models on disease nodes, forecasting potential\nlinks among drugs and diseases. These analyses, along with biomedical texts,\nare integrated into a framework that facilitates user-directed mechanism\nelucidation as well as hypothesis exploration through RAG-enabled LLMs. A\nclinical use-case demonstrates RUGGED's ability to evaluate and recommend\ntherapeutics for Arrhythmogenic Cardiomyopathy (ACM) and Dilated Cardiomyopathy\n(DCM), analyzing prescribed drugs for molecular interactions and unexplored\nuses. The platform minimizes LLM hallucinations, offers actionable insights,\nand improves the investigation of novel therapeutics.", "published": "2024-07-17 07:44:18", "link": "http://arxiv.org/abs/2407.12888v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Halu-J: Critique-Based Hallucination Judge", "abstract": "Large language models (LLMs) frequently generate non-factual content, known\nas hallucinations. Existing retrieval-augmented-based hallucination detection\napproaches typically address this by framing it as a classification task,\nevaluating hallucinations based on their consistency with retrieved evidence.\nHowever, this approach usually lacks detailed explanations for these\nevaluations and does not assess the reliability of these explanations.\nFurthermore, deficiencies in retrieval systems can lead to irrelevant or\npartially relevant evidence retrieval, impairing the detection process.\nMoreover, while real-world hallucination detection requires analyzing multiple\npieces of evidence, current systems usually treat all evidence uniformly\nwithout considering its relevance to the content. To address these challenges,\nwe introduce Halu-J, a critique-based hallucination judge with 7 billion\nparameters. Halu-J enhances hallucination detection by selecting pertinent\nevidence and providing detailed critiques. Our experiments indicate that Halu-J\noutperforms GPT-4o in multiple-evidence hallucination detection and matches its\ncapability in critique generation and evidence selection. We also introduce\nME-FEVER, a new dataset designed for multiple-evidence hallucination detection.\nOur code and dataset can be found in https://github.com/GAIR-NLP/factool .", "published": "2024-07-17 18:21:01", "link": "http://arxiv.org/abs/2407.12943v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Survey of Prompt Engineering Methods in Large Language Models for\n  Different NLP Tasks", "abstract": "Large language models (LLMs) have shown remarkable performance on many\ndifferent Natural Language Processing (NLP) tasks. Prompt engineering plays a\nkey role in adding more to the already existing abilities of LLMs to achieve\nsignificant performance gains on various NLP tasks. Prompt engineering requires\ncomposing natural language instructions called prompts to elicit knowledge from\nLLMs in a structured way. Unlike previous state-of-the-art (SoTA) models,\nprompt engineering does not require extensive parameter re-training or\nfine-tuning based on the given NLP task and thus solely operates on the\nembedded knowledge of LLMs. Additionally, LLM enthusiasts can intelligently\nextract LLMs' knowledge through a basic natural language conversational\nexchange or prompt engineering, allowing more and more people even without deep\nmathematical machine learning background to experiment with LLMs. With prompt\nengineering gaining popularity in the last two years, researchers have come up\nwith numerous engineering techniques around designing prompts to improve\naccuracy of information extraction from the LLMs. In this paper, we summarize\ndifferent prompting techniques and club them together based on different NLP\ntasks that they have been used for. We further granularly highlight the\nperformance of these prompting strategies on various datasets belonging to that\nNLP task, talk about the corresponding LLMs used, present a taxonomy diagram\nand discuss the possible SoTA for specific datasets. In total, we read and\npresent a survey of 44 research papers which talk about 39 different prompting\nmethods on 29 different NLP tasks of which most of them have been published in\nthe last two years.", "published": "2024-07-17 20:23:19", "link": "http://arxiv.org/abs/2407.12994v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Matryoshka-Adaptor: Unsupervised and Supervised Tuning for Smaller\n  Embedding Dimensions", "abstract": "Embeddings from Large Language Models (LLMs) have emerged as critical\ncomponents in various applications, particularly for information retrieval.\nWhile high-dimensional embeddings generally demonstrate superior performance as\nthey contain more salient information, their practical application is\nfrequently hindered by elevated computational latency and the associated higher\ncost. To address these challenges, we propose Matryoshka-Adaptor, a novel\ntuning framework designed for the customization of LLM embeddings.\nMatryoshka-Adaptor facilitates substantial dimensionality reduction while\nmaintaining comparable performance levels, thereby achieving a significant\nenhancement in computational efficiency and cost-effectiveness. Our framework\ndirectly modifies the embeddings from pre-trained LLMs which is designed to be\nseamlessly integrated with any LLM architecture, encompassing those accessible\nexclusively through black-box APIs. Also, it exhibits efficacy in both\nunsupervised and supervised learning settings. A rigorous evaluation conducted\nacross a diverse corpus of English, multilingual, and multimodal datasets\nconsistently reveals substantial gains with Matryoshka-Adaptor. Notably, with\nGoogle and OpenAI Embedding APIs, Matryoshka-Adaptor achieves a reduction in\ndimensionality ranging from two- to twelve-fold without compromising\nperformance across multiple BEIR datasets.", "published": "2024-07-17 18:03:29", "link": "http://arxiv.org/abs/2407.20243v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Steamroller Problems: An Evaluation of LLM Reasoning Capability with\n  Automated Theorem Prover Strategies", "abstract": "This study presents the first examination of the ability of Large Language\nModels (LLMs) to follow reasoning strategies that are used to guide Automated\nTheorem Provers (ATPs). We evaluate the performance of GPT4, GPT3.5 Turbo and\nGoogle's recent Gemini model on problems from a steamroller domain. In addition\nto determining accuracy we make use of the Natural Language Processing library\nspaCy to explore new methods of investigating LLM's reasoning capabilities.\nThis led to one alarming result, the low correlation between correct reasoning\nand correct answers for any of the tested models. We found that the models'\nperformance when using the ATP reasoning strategies was comparable to one-shot\nchain of thought and observe that attention to uncertainty in the accuracy\nresults is critical when drawing conclusions about model performance.\nConsistent with previous speculation we confirm that LLMs have a preference\nfor, and are best able to follow, bottom up reasoning processes. However, the\nreasoning strategies can still be beneficial for deriving small and relevant\nsets of formulas for external processing by a trusted inference engine.", "published": "2024-07-17 22:49:23", "link": "http://arxiv.org/abs/2407.20244v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Questionable practices in machine learning", "abstract": "Evaluating modern ML models is hard. The strong incentive for researchers and\ncompanies to report a state-of-the-art result on some metric often leads to\nquestionable research practices (QRPs): bad practices which fall short of\noutright research fraud. We describe 44 such practices which can undermine\nreported results, giving examples where possible. Our list emphasises the\nevaluation of large language models (LLMs) on public benchmarks. We also\ndiscuss \"irreproducible research practices\", i.e. decisions that make it\ndifficult or impossible for other researchers to reproduce, build on or audit\nprevious research.", "published": "2024-07-17 00:06:30", "link": "http://arxiv.org/abs/2407.12220v2", "categories": ["cs.LG", "cs.CL", "cs.CY"], "primary_category": "cs.LG"}
{"title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models\n  at Scale", "abstract": "Rapid advancements in GPU computational power has outpaced memory capacity\nand bandwidth growth, creating bottlenecks in Large Language Model (LLM)\ninference. Post-training quantization is the leading method for addressing\nmemory-related bottlenecks in LLM inference, but it suffers from significant\nperformance degradation below 4-bit precision. This paper addresses these\nchallenges by investigating the pretraining of low-bitwidth models specifically\nTernary Language Models (TriLMs) as an alternative to traditional\nfloating-point models (FloatLMs) and their post-training quantized versions\n(QuantLMs). We present Spectra LLM suite, the first open suite of LLMs spanning\nmultiple bit-widths, including FloatLMs, QuantLMs, and TriLMs, ranging from 99M\nto 3.9B parameters trained on 300B tokens. Our comprehensive evaluation\ndemonstrates that TriLMs offer superior scaling behavior in terms of model size\n(in bits). Surprisingly, at scales exceeding one billion parameters, TriLMs\nconsistently outperform their QuantLM and FloatLM counterparts for a given bit\nsize across various benchmarks. Notably, the 3.9B parameter TriLM matches the\nperformance of the FloatLM 3.9B across all benchmarks, despite having fewer\nbits than FloatLM 830M. Overall, this research provides valuable insights into\nthe feasibility and scalability of low-bitwidth language models, paving the way\nfor the development of more efficient LLMs.\n  To enhance understanding of low-bitwidth models, we are releasing 500+\nintermediate checkpoints of the Spectra suite at\nhttps://github.com/NolanoOrg/SpectraSuite.", "published": "2024-07-17 05:53:20", "link": "http://arxiv.org/abs/2407.12327v5", "categories": ["cs.LG", "cs.AI", "cs.CL", "68T30", "I.2.6; I.2.7"], "primary_category": "cs.LG"}
{"title": "NavGPT-2: Unleashing Navigational Reasoning Capability for Large\n  Vision-Language Models", "abstract": "Capitalizing on the remarkable advancements in Large Language Models (LLMs),\nthere is a burgeoning initiative to harness LLMs for instruction following\nrobotic navigation. Such a trend underscores the potential of LLMs to\ngeneralize navigational reasoning and diverse language understanding. However,\na significant discrepancy in agent performance is observed when integrating\nLLMs in the Vision-and-Language navigation (VLN) tasks compared to previous\ndownstream specialist models. Furthermore, the inherent capacity of language to\ninterpret and facilitate communication in agent interactions is often\nunderutilized in these integrations. In this work, we strive to bridge the\ndivide between VLN-specialized models and LLM-based navigation paradigms, while\nmaintaining the interpretative prowess of LLMs in generating linguistic\nnavigational reasoning. By aligning visual content in a frozen LLM, we\nencompass visual observation comprehension for LLMs and exploit a way to\nincorporate LLMs and navigation policy networks for effective action\npredictions and navigational reasoning. We demonstrate the data efficiency of\nthe proposed methods and eliminate the gap between LM-based agents and\nstate-of-the-art VLN specialists.", "published": "2024-07-17 07:44:26", "link": "http://arxiv.org/abs/2407.12366v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "primary_category": "cs.CV"}
{"title": "PersLLM: A Personified Training Approach for Large Language Models", "abstract": "Large language models exhibit aspects of human-level intelligence that\ncatalyze their application as human-like agents in domains such as social\nsimulations, human-machine interactions, and collaborative multi-agent systems.\nHowever, the absence of distinct personalities, such as displaying ingratiating\nbehaviors, inconsistent opinions, and uniform response patterns, diminish LLMs\nutility in practical applications. Addressing this, the development of\npersonality traits in LLMs emerges as a crucial area of research to unlock\ntheir latent potential. Existing methods to personify LLMs generally involve\nstrategies like employing stylized training data for instruction tuning or\nusing prompt engineering to simulate different personalities. These methods\nonly capture superficial linguistic styles instead of the core of personalities\nand are therefore not stable. In this study, we propose PersLLM, integrating\npsychology-grounded principles of personality: social practice, consistency,\nand dynamic development, into a comprehensive training methodology. We\nincorporate personality traits directly into the model parameters, enhancing\nthe model's resistance to induction, promoting consistency, and supporting the\ndynamic evolution of personality. Single-agent evaluation validates our\nmethod's superiority, as it produces responses more aligned with reference\npersonalities compared to other approaches. Case studies for multi-agent\ncommunication highlight its benefits in enhancing opinion consistency within\nindividual agents and fostering collaborative creativity among multiple agents\nin dialogue contexts, potentially benefiting human simulation and multi-agent\ncooperation. Additionally, human-agent interaction evaluations indicate that\nour personified models significantly enhance interactive experiences,\nunderscoring the practical implications of our research.", "published": "2024-07-17 08:13:22", "link": "http://arxiv.org/abs/2407.12393v4", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Across Platforms and Languages: Dutch Influencers and Legal Disclosures\n  on Instagram, YouTube and TikTok", "abstract": "Content monetization on social media fuels a growing influencer economy.\nInfluencer marketing remains largely undisclosed or inappropriately disclosed\non social media. Non-disclosure issues have become a priority for national and\nsupranational authorities worldwide, who are starting to impose increasingly\nharsher sanctions on them. This paper proposes a transparent methodology for\nmeasuring whether and how influencers comply with disclosures based on legal\nstandards. We introduce a novel distinction between disclosures that are\nlegally sufficient (green) and legally insufficient (yellow). We apply this\nmethodology to an original dataset reflecting the content of 150 Dutch\ninfluencers publicly registered with the Dutch Media Authority based on\nrecently introduced registration obligations. The dataset consists of 292,315\nposts and is multi-language (English and Dutch) and cross-platform (Instagram,\nYouTube and TikTok). We find that influencer marketing remains generally\nunderdisclosed on social media, and that bigger influencers are not necessarily\nmore compliant with disclosure standards.", "published": "2024-07-17 09:59:52", "link": "http://arxiv.org/abs/2407.12451v2", "categories": ["cs.CY", "cs.CL", "cs.SI"], "primary_category": "cs.CY"}
{"title": "Characterization of Political Polarized Users Attacked by Language\n  Toxicity on Twitter", "abstract": "Understanding the dynamics of language toxicity on social media is important\nfor us to investigate the propagation of misinformation and the development of\necho chambers for political scenarios such as U.S. presidential elections.\nRecent research has used large-scale data to investigate the dynamics across\nsocial media platforms. However, research on the toxicity dynamics is not\nenough. This study aims to provide a first exploration of the potential\nlanguage toxicity flow among Left, Right and Center users. Specifically, we aim\nto examine whether Left users were easier to be attacked by language toxicity.\nIn this study, more than 500M Twitter posts were examined. It was discovered\nthat Left users received much more toxic replies than Right and Center users.", "published": "2024-07-17 10:49:47", "link": "http://arxiv.org/abs/2407.12471v2", "categories": ["cs.CY", "cs.AI", "cs.CL", "91, 94", "J.4"], "primary_category": "cs.CY"}
{"title": "MERLIN: Multimodal Embedding Refinement via LLM-based Iterative\n  Navigation for Text-Video Retrieval-Rerank Pipeline", "abstract": "The rapid expansion of multimedia content has made accurately retrieving\nrelevant videos from large collections increasingly challenging. Recent\nadvancements in text-video retrieval have focused on cross-modal interactions,\nlarge-scale foundation model training, and probabilistic modeling, yet often\nneglect the crucial user perspective, leading to discrepancies between user\nqueries and the content retrieved. To address this, we introduce MERLIN\n(Multimodal Embedding Refinement via LLM-based Iterative Navigation), a novel,\ntraining-free pipeline that leverages Large Language Models (LLMs) for\niterative feedback learning. MERLIN refines query embeddings from a user\nperspective, enhancing alignment between queries and video content through a\ndynamic question answering process. Experimental results on datasets like\nMSR-VTT, MSVD, and ActivityNet demonstrate that MERLIN substantially improves\nRecall@1, outperforming existing systems and confirming the benefits of\nintegrating LLMs into multimodal retrieval systems for more responsive and\ncontext-aware multimedia retrieval.", "published": "2024-07-17 11:45:02", "link": "http://arxiv.org/abs/2407.12508v2", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Abstraction Alignment: Comparing Model-Learned and Human-Encoded\n  Conceptual Relationships", "abstract": "While interpretability methods identify a model's learned concepts, they\noverlook the relationships between concepts that make up its abstractions and\ninform its ability to generalize to new data. To assess whether models' have\nlearned human-aligned abstractions, we introduce abstraction alignment, a\nmethodology to compare model behavior against formal human knowledge.\nAbstraction alignment externalizes domain-specific human knowledge as an\nabstraction graph, a set of pertinent concepts spanning levels of abstraction.\nUsing the abstraction graph as a ground truth, abstraction alignment measures\nthe alignment of a model's behavior by determining how much of its uncertainty\nis accounted for by the human abstractions. By aggregating abstraction\nalignment across entire datasets, users can test alignment hypotheses, such as\nwhich human concepts the model has learned and where misalignments recur. In\nevaluations with experts, abstraction alignment differentiates seemingly\nsimilar errors, improves the verbosity of existing model-quality metrics, and\nuncovers improvements to current human abstractions.", "published": "2024-07-17 13:27:26", "link": "http://arxiv.org/abs/2407.12543v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.LG"}
{"title": "E5-V: Universal Embeddings with Multimodal Large Language Models", "abstract": "Multimodal large language models (MLLMs) have shown promising advancements in\ngeneral visual and language understanding. However, the representation of\nmultimodal information using MLLMs remains largely unexplored. In this work, we\nintroduce a new framework, E5-V, designed to adapt MLLMs for achieving\nuniversal multimodal embeddings. Our findings highlight the significant\npotential of MLLMs in representing multimodal inputs compared to previous\napproaches. By leveraging MLLMs with prompts, E5-V effectively bridges the\nmodality gap between different types of inputs, demonstrating strong\nperformance in multimodal embeddings even without fine-tuning. We propose a\nsingle modality training approach for E5-V, where the model is trained\nexclusively on text pairs. This method demonstrates significant improvements\nover traditional multimodal training on image-text pairs, while reducing\ntraining costs by approximately 95%. Additionally, this approach eliminates the\nneed for costly multimodal training data collection. Extensive experiments\nacross four types of tasks demonstrate the effectiveness of E5-V. As a\nuniversal multimodal model, E5-V not only achieves but often surpasses\nstate-of-the-art performance in each task, despite being trained on a single\nmodality.", "published": "2024-07-17 14:04:12", "link": "http://arxiv.org/abs/2407.12580v1", "categories": ["cs.CL", "cs.CV", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Patch-Level Training for Large Language Models", "abstract": "As Large Language Models (LLMs) achieve remarkable progress in language\nunderstanding and generation, their training efficiency has become a critical\nconcern. Traditionally, LLMs are trained to predict the next token in a\nsequence. Despite the success of token-level training, it suffers from\nconsiderable computational costs due to the need to process an extensive number\nof tokens. To mitigate this issue, this paper introduces patch-level training\nfor LLMs, which reduces the sequence length by compressing multiple tokens into\na single patch. During patch-level training, we feed the language model shorter\nsequences of patches and train it to predict the next patch, thereby processing\nthe majority of the training data at a significantly reduced computational\ncost. Following this, the model continues token-level training on the remaining\ntraining data to align with the inference mode. Experiments on a diverse range\nof models (370M-2.7B parameters) demonstrate that patch-level training can\nreduce overall computational costs to 0.5$\\times$, without compromising the\nmodel performance compared to token-level training. Source code:\n\\url{https://github.com/shaochenze/PatchTrain}.", "published": "2024-07-17 15:48:39", "link": "http://arxiv.org/abs/2407.12665v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TTSDS -- Text-to-Speech Distribution Score", "abstract": "Many recently published Text-to-Speech (TTS) systems produce audio close to\nreal speech. However, TTS evaluation needs to be revisited to make sense of the\nresults obtained with the new architectures, approaches and datasets. We\npropose evaluating the quality of synthetic speech as a combination of multiple\nfactors such as prosody, speaker identity, and intelligibility. Our approach\nassesses how well synthetic speech mirrors real speech by obtaining correlates\nof each factor and measuring their distance from both real speech datasets and\nnoise datasets. We benchmark 35 TTS systems developed between 2008 and 2024 and\nshow that our score computed as an unweighted average of factors strongly\ncorrelates with the human evaluations from each time period.", "published": "2024-07-17 16:30:27", "link": "http://arxiv.org/abs/2407.12707v3", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The Role of Network and Identity in the Diffusion of Hashtags", "abstract": "The diffusion of culture online is theorized to be influenced by many\ninteracting social factors (e.g., network and identity). However, most existing\ncomputational cascade models consider just a single factor (e.g., network or\nidentity). This work offers a new framework for teasing apart the mechanisms\nunderlying hashtag cascades. We curate a new dataset of 1,337 hashtags\nrepresenting cultural innovation online, develop a 10-factor evaluation\nframework for comparing empirical and simulated cascades, and show that a\ncombined network+identity model better simulates hashtag cascades than network-\nor identity-only counterfactuals. We also explore heterogeneity in performance:\nWhile a combined network+identity model best predicts the popularity of\ncascades, a network-only model best predicts cascade growth and an\nidentity-only model best predicts adopter composition. The network+identity\nmodel has the highest comparative advantage among hashtags used for expressing\nracial or regional identity and talking about sports or news. In fact, we are\nable to predict what combination of network and/or identity best models each\nhashtag and use this to further improve performance. Our results show the\nutility of models incorporating the interactions of network, identity, and\nother social factors in the diffusion of hashtags in social media.", "published": "2024-07-17 17:51:49", "link": "http://arxiv.org/abs/2407.12771v2", "categories": ["cs.SI", "cs.CL", "cs.CY"], "primary_category": "cs.SI"}
{"title": "Retrieval-Enhanced Machine Learning: Synthesis and Opportunities", "abstract": "In the field of language modeling, models augmented with retrieval components\nhave emerged as a promising solution to address several challenges faced in the\nnatural language processing (NLP) field, including knowledge grounding,\ninterpretability, and scalability. Despite the primary focus on NLP, we posit\nthat the paradigm of retrieval-enhancement can be extended to a broader\nspectrum of machine learning (ML) such as computer vision, time series\nprediction, and computational biology. Therefore, this work introduces a formal\nframework of this paradigm, Retrieval-Enhanced Machine Learning (REML), by\nsynthesizing the literature in various domains in ML with consistent notations\nwhich is missing from the current literature. Also, we found that while a\nnumber of studies employ retrieval components to augment their models, there is\na lack of integration with foundational Information Retrieval (IR) research. We\nbridge this gap between the seminal IR research and contemporary REML studies\nby investigating each component that comprises the REML framework. Ultimately,\nthe goal of this work is to equip researchers across various disciplines with a\ncomprehensive, formally structured framework of retrieval-enhanced models,\nthereby fostering interdisciplinary future research.", "published": "2024-07-17 20:01:21", "link": "http://arxiv.org/abs/2407.12982v2", "categories": ["cs.LG", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "Pre-Trained Foundation Model representations to uncover Breathing\n  patterns in Speech", "abstract": "The process of human speech production involves coordinated respiratory\naction to elicit acoustic speech signals. Typically, speech is produced when\nair is forced from the lungs and is modulated by the vocal tract, where such\nactions are interspersed by moments of breathing in air (inhalation) to refill\nthe lungs again. Respiratory rate (RR) is a vital metric that is used to assess\nthe overall health, fitness, and general well-being of an individual. Existing\napproaches to measure RR (number of breaths one takes in a minute) are\nperformed using specialized equipment or training. Studies have demonstrated\nthat machine learning algorithms can be used to estimate RR using bio-sensor\nsignals as input. Speech-based estimation of RR can offer an effective approach\nto measure the vital metric without requiring any specialized equipment or\nsensors. This work investigates a machine learning based approach to estimate\nRR from speech segments obtained from subjects speaking to a close-talking\nmicrophone device. Data were collected from N=26 individuals, where the\ngroundtruth RR was obtained through commercial grade chest-belts and then\nmanually corrected for any errors. A convolutional long-short term memory\nnetwork (Conv-LSTM) is proposed to estimate respiration time-series data from\nthe speech signal. We demonstrate that the use of pre-trained representations\nobtained from a foundation model, such as Wav2Vec2, can be used to estimate\nrespiration-time-series with low root-mean-squared error and high correlation\ncoefficient, when compared with the baseline. The model-driven time series can\nbe used to estimate $RR$ with a low mean absolute error (MAE) ~ 1.6\nbreaths/min.", "published": "2024-07-17 21:57:18", "link": "http://arxiv.org/abs/2407.13035v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Less is More: Sparse Watermarking in LLMs with Enhanced Text Quality", "abstract": "With the widespread adoption of Large Language Models (LLMs), concerns about\npotential misuse have emerged. To this end, watermarking has been adapted to\nLLM, enabling a simple and effective way to detect and monitor generated text.\nHowever, while the existing methods can differentiate between watermarked and\nunwatermarked text with high accuracy, they often face a trade-off between the\nquality of the generated text and the effectiveness of the watermarking\nprocess. In this work, we present a novel type of LLM watermark, Sparse\nWatermark, which aims to mitigate this trade-off by applying watermarks to a\nsmall subset of generated tokens distributed across the text. The key strategy\ninvolves anchoring watermarked tokens to words that have specific\nPart-of-Speech (POS) tags. Our experimental results demonstrate that the\nproposed watermarking scheme achieves high detectability while generating text\nthat outperforms previous LLM watermarking methods in quality across various\ntasks", "published": "2024-07-17 18:52:12", "link": "http://arxiv.org/abs/2407.13803v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Multi-Grained Query-Guided Set Prediction Network for Grounded\n  Multimodal Named Entity Recognition", "abstract": "Grounded Multimodal Named Entity Recognition (GMNER) is an emerging\ninformation extraction (IE) task, aiming to simultaneously extract entity\nspans, types, and corresponding visual regions of entities from given\nsentence-image pairs data. Recent unified methods employing machine reading\ncomprehension or sequence generation-based frameworks show limitations in this\ndifficult task. The former, utilizing human-designed type queries, struggles to\ndifferentiate ambiguous entities, such as Jordan (Person) and off-White x\nJordan (Shoes). The latter, following the one-by-one decoding order, suffers\nfrom exposure bias issues. We maintain that these works misunderstand the\nrelationships of multimodal entities. To tackle these, we propose a novel\nunified framework named Multi-grained Query-guided Set Prediction Network\n(MQSPN) to learn appropriate relationships at intra-entity and inter-entity\nlevels. Specifically, MQSPN explicitly aligns textual entities with visual\nregions by employing a set of learnable queries to strengthen intra-entity\nconnections. Based on distinct intra-entity modeling, MQSPN reformulates GMNER\nas a set prediction, guiding models to establish appropriate inter-entity\nrelationships from a optimal global matching perspective. Additionally, we\nincorporate a query-guided Fusion Net (QFNet) as a glue network to boost better\nalignment of two-level relationships. Extensive experiments demonstrate that\nour approach achieves state-of-the-art performances in widely used benchmarks.", "published": "2024-07-17 05:42:43", "link": "http://arxiv.org/abs/2407.21033v3", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.IR"}
{"title": "BSC-UPC at EmoSPeech-IberLEF2024: Attention Pooling for Emotion\n  Recognition", "abstract": "The domain of speech emotion recognition (SER) has persistently been a\nfrontier within the landscape of machine learning. It is an active field that\nhas been revolutionized in the last few decades and whose implementations are\nremarkable in multiple applications that could affect daily life. Consequently,\nthe Iberian Languages Evaluation Forum (IberLEF) of 2024 held a competitive\nchallenge to leverage the SER results with a Spanish corpus. This paper\npresents the approach followed with the goal of participating in this\ncompetition. The main architecture consists of different pre-trained speech and\ntext models to extract features from both modalities, utilizing an attention\npooling mechanism. The proposed system has achieved the first position in the\nchallenge with an 86.69% in Macro F1-Score.", "published": "2024-07-17 10:37:28", "link": "http://arxiv.org/abs/2407.12467v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "TalTech-IRIT-LIS Speaker and Language Diarization Systems for DISPLACE\n  2024", "abstract": "This paper describes the submissions of team TalTech-IRIT-LIS to the DISPLACE\n2024 challenge. Our team participated in the speaker diarization and language\ndiarization tracks of the challenge. In the speaker diarization track, our best\nsubmission was an ensemble of systems based on the pyannote.audio speaker\ndiarization pipeline utilizing powerset training and our recently proposed\nPixIT method that performs joint diarization and speech separation. We improve\nupon PixIT by using the separation outputs for speaker embedding extraction.\nOur ensemble achieved a diarization error rate of 27.1% on the evaluation\ndataset. In the language diarization track, we fine-tuned a pre-trained\nWav2Vec2-BERT language embedding model on in-domain data, and clustered short\nsegments using AHC and VBx, based on similarity scores from LDA/PLDA. This led\nto a language diarization error rate of 27.6% on the evaluation data. Both\nresults were ranked first in their respective challenge tracks.", "published": "2024-07-17 17:02:21", "link": "http://arxiv.org/abs/2407.12743v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Multi-Iteration Multi-Stage Fine-Tuning of Transformers for Sound Event\n  Detection with Heterogeneous Datasets", "abstract": "A central problem in building effective sound event detection systems is the\nlack of high-quality, strongly annotated sound event datasets. For this reason,\nTask 4 of the DCASE 2024 challenge proposes learning from two heterogeneous\ndatasets, including audio clips labeled with varying annotation granularity and\nwith different sets of possible events. We propose a multi-iteration,\nmulti-stage procedure for fine-tuning Audio Spectrogram Transformers on the\njoint DESED and MAESTRO Real datasets. The first stage closely matches the\nbaseline system setup and trains a CRNN model while keeping the pre-trained\ntransformer model frozen. In the second stage, both CRNN and transformer are\nfine-tuned using heavily weighted self-supervised losses. After the second\nstage, we compute strong pseudo-labels for all audio clips in the training set\nusing an ensemble of fine-tuned transformers. Then, in a second iteration, we\nrepeat the two-stage training process and include a distillation loss based on\nthe pseudo-labels, achieving a new single-model, state-of-the-art performance\non the public evaluation set of DESED with a PSDS1 of 0.692. A single model and\nan ensemble, both based on our proposed training procedure, ranked first in\nTask 4 of the DCASE Challenge 2024.", "published": "2024-07-17 20:32:58", "link": "http://arxiv.org/abs/2407.12997v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "PCQ: Emotion Recognition in Speech via Progressive Channel Querying", "abstract": "In human-computer interaction (HCI), Speech Emotion Recognition (SER) is a\nkey technology for understanding human intentions and emotions. Traditional SER\nmethods struggle to effectively capture the long-term temporal correla-tions\nand dynamic variations in complex emotional expressions. To overcome these\nlimitations, we introduce the PCQ method, a pioneering approach for SER via\n\\textbf{P}rogressive \\textbf{C}hannel \\textbf{Q}uerying. This method can drill\ndown layer by layer in the channel dimension through the channel query\ntechnique to achieve dynamic modeling of long-term contextual information of\nemotions. This mul-ti-level analysis gives the PCQ method an edge in capturing\nthe nuances of hu-man emotions. Experimental results show that our model\nimproves the weighted average (WA) accuracy by 3.98\\% and 3.45\\% and the\nunweighted av-erage (UA) accuracy by 5.67\\% and 5.83\\% on the IEMOCAP and EMODB\nemotion recognition datasets, respectively, significantly exceeding the\nbaseline levels.", "published": "2024-07-17 07:58:16", "link": "http://arxiv.org/abs/2407.12380v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Audio Conditioning for Music Generation via Discrete Bottleneck Features", "abstract": "While most music generation models use textual or parametric conditioning\n(e.g. tempo, harmony, musical genre), we propose to condition a language model\nbased music generation system with audio input. Our exploration involves two\ndistinct strategies. The first strategy, termed textual inversion, leverages a\npre-trained text-to-music model to map audio input to corresponding\n\"pseudowords\" in the textual embedding space. For the second model we train a\nmusic language model from scratch jointly with a text conditioner and a\nquantized audio feature extractor. At inference time, we can mix textual and\naudio conditioning and balance them thanks to a novel double classifier free\nguidance method. We conduct automatic and human studies that validates our\napproach. We will release the code and we provide music samples on\nhttps://musicgenstyle.github.io in order to show the quality of our model.", "published": "2024-07-17 13:47:17", "link": "http://arxiv.org/abs/2407.12563v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improving Audio Spectrogram Transformers for Sound Event Detection\n  Through Multi-Stage Training", "abstract": "This technical report describes the CP-JKU team's submission for Task 4 Sound\nEvent Detection with Heterogeneous Training Datasets and Potentially Missing\nLabels of the DCASE 24 Challenge. We fine-tune three large Audio Spectrogram\nTransformers, PaSST, BEATs, and ATST, on the joint DESED and MAESTRO datasets\nin a two-stage training procedure. The first stage closely matches the baseline\nsystem setup and trains a CRNN model while keeping the large pre-trained\ntransformer model frozen. In the second stage, both CRNN and transformer are\nfine-tuned using heavily weighted self-supervised losses. After the second\nstage, we compute strong pseudo-labels for all audio clips in the training set\nusing an ensemble of all three fine-tuned transformers. Then, in a second\niteration, we repeat the two-stage training process and include a distillation\nloss based on the pseudo-labels, boosting single-model performance\nsubstantially. Additionally, we pre-train PaSST and ATST on the subset of\nAudioSet that comes with strong temporal labels, before fine-tuning them on the\nTask 4 datasets.", "published": "2024-07-17 20:36:46", "link": "http://arxiv.org/abs/2408.00791v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Laugh Now Cry Later: Controlling Time-Varying Emotional States of\n  Flow-Matching-Based Zero-Shot Text-to-Speech", "abstract": "People change their tones of voice, often accompanied by nonverbal\nvocalizations (NVs) such as laughter and cries, to convey rich emotions.\nHowever, most text-to-speech (TTS) systems lack the capability to generate\nspeech with rich emotions, including NVs. This paper introduces EmoCtrl-TTS, an\nemotion-controllable zero-shot TTS that can generate highly emotional speech\nwith NVs for any speaker. EmoCtrl-TTS leverages arousal and valence values, as\nwell as laughter embeddings, to condition the flow-matching-based zero-shot\nTTS. To achieve high-quality emotional speech generation, EmoCtrl-TTS is\ntrained using more than 27,000 hours of expressive data curated based on\npseudo-labeling. Comprehensive evaluations demonstrate that EmoCtrl-TTS excels\nin mimicking the emotions of audio prompts in speech-to-speech translation\nscenarios. We also show that EmoCtrl-TTS can capture emotion changes, express\nstrong emotions, and generate various NVs in zero-shot TTS. See\nhttps://aka.ms/emoctrl-tts for demo samples.", "published": "2024-07-17 00:54:15", "link": "http://arxiv.org/abs/2407.12229v2", "categories": ["eess.AS", "cs.AI", "eess.SP"], "primary_category": "eess.AS"}
{"title": "GraphMuse: A Library for Symbolic Music Graph Processing", "abstract": "Graph Neural Networks (GNNs) have recently gained traction in symbolic music\ntasks, yet a lack of a unified framework impedes progress. Addressing this gap,\nwe present GraphMuse, a graph processing framework and library that facilitates\nefficient music graph processing and GNN training for symbolic music tasks.\nCentral to our contribution is a new neighbor sampling technique specifically\ntargeted toward meaningful behavior in musical scores. Additionally, GraphMuse\nintegrates hierarchical modeling elements that augment the expressivity and\ncapabilities of graph networks for musical tasks. Experiments with two specific\nmusical prediction tasks -- pitch spelling and cadence detection -- demonstrate\nsignificant performance improvement over previous methods. Our hope is that\nGraphMuse will lead to a boost in, and standardization of, symbolic music\nprocessing based on graph representations. The library is available at\nhttps://github.com/manoskary/graphmuse", "published": "2024-07-17 15:54:09", "link": "http://arxiv.org/abs/2407.12671v1", "categories": ["cs.SD", "cs.AI", "cs.DL", "eess.AS"], "primary_category": "cs.SD"}
