{"title": "A Deep Neural Network Approach To Parallel Sentence Extraction", "abstract": "Parallel sentence extraction is a task addressing the data sparsity problem\nfound in multilingual natural language processing applications. We propose an\nend-to-end deep neural network approach to detect translational equivalence\nbetween sentences in two different languages. In contrast to previous\napproaches, which typically rely on multiples models and various word alignment\nfeatures, by leveraging continuous vector representation of sentences we remove\nthe need of any domain specific feature engineering. Using a siamese\nbidirectional recurrent neural networks, our results against a strong baseline\nbased on a state-of-the-art parallel sentence extraction system show a\nsignificant improvement in both the quality of the extracted parallel sentences\nand the translation performance of statistical machine translation systems. We\nbelieve this study is the first one to investigate deep learning for the\nparallel sentence extraction task.", "published": "2017-09-28 02:09:04", "link": "http://arxiv.org/abs/1709.09783v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentiment Classification with Word Attention based on Weakly Supervised\n  Learning with a Convolutional Neural Network", "abstract": "In order to maximize the applicability of sentiment analysis results, it is\nnecessary to not only classify the overall sentiment (positive/negative) of a\ngiven document but also to identify the main words that contribute to the\nclassification. However, most datasets for sentiment analysis only have the\nsentiment label for each document or sentence. In other words, there is no\ninformation about which words play an important role in sentiment\nclassification. In this paper, we propose a method for identifying key words\ndiscriminating positive and negative sentences by using a weakly supervised\nlearning method based on a convolutional neural network (CNN). In our model,\neach word is represented as a continuous-valued vector and each sentence is\nrepresented as a matrix whose rows correspond to the word vector used in the\nsentence. Then, the CNN model is trained using these sentence matrices as\ninputs and the sentiment labels as the output. Once the CNN model is trained,\nwe implement the word attention mechanism that identifies high-contributing\nwords to classification results with a class activation map, using the weights\nfrom the fully connected layer at the end of the learned CNN model. In order to\nverify the proposed methodology, we evaluated the classification accuracy and\ninclusion rate of polarity words using two movie review datasets. Experimental\nresult show that the proposed model can not only correctly classify the\nsentence polarity but also successfully identify the corresponding words with\nhigh polarity scores.", "published": "2017-09-28 10:35:41", "link": "http://arxiv.org/abs/1709.09885v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Graph Convolutional Networks for Named Entity Recognition", "abstract": "In this paper we investigate the role of the dependency tree in a named\nentity recognizer upon using a set of GCN. We perform a comparison among\ndifferent NER architectures and show that the grammar of a sentence positively\ninfluences the results. Experiments on the ontonotes dataset demonstrate\nconsistent performance improvements, without requiring heavy feature\nengineering nor additional language-specific knowledge.", "published": "2017-09-28 16:44:22", "link": "http://arxiv.org/abs/1709.10053v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Web of Hate: Tackling Hateful Speech in Online Social Spaces", "abstract": "Online social platforms are beset with hateful speech - content that\nexpresses hatred for a person or group of people. Such content can frighten,\nintimidate, or silence platform users, and some of it can inspire other users\nto commit violence. Despite widespread recognition of the problems posed by\nsuch content, reliable solutions even for detecting hateful speech are lacking.\nIn the present work, we establish why keyword-based methods are insufficient\nfor detection. We then propose an approach to detecting hateful speech that\nuses content produced by self-identifying hateful communities as training data.\nOur approach bypasses the expensive annotation process often required to train\nkeyword systems and performs well across several established platforms, making\nsubstantial improvements over current state-of-the-art approaches.", "published": "2017-09-28 20:31:30", "link": "http://arxiv.org/abs/1709.10159v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Jointly Trained Sequential Labeling and Classification by Sparse\n  Attention Neural Networks", "abstract": "Sentence-level classification and sequential labeling are two fundamental\ntasks in language understanding. While these two tasks are usually modeled\nseparately, in reality, they are often correlated, for example in intent\nclassification and slot filling, or in topic classification and named-entity\nrecognition. In order to utilize the potential benefits from their\ncorrelations, we propose a jointly trained model for learning the two tasks\nsimultaneously via Long Short-Term Memory (LSTM) networks. This model predicts\nthe sentence-level category and the word-level label sequence from the stepwise\noutput hidden representations of LSTM. We also introduce a novel mechanism of\n\"sparse attention\" to weigh words differently based on their semantic relevance\nto sentence-level classification. The proposed method outperforms baseline\nmodels on ATIS and TREC datasets.", "published": "2017-09-28 22:40:07", "link": "http://arxiv.org/abs/1709.10191v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Edina: Building an Open Domain Socialbot with Self-dialogues", "abstract": "We present Edina, the University of Edinburgh's social bot for the Amazon\nAlexa Prize competition. Edina is a conversational agent whose responses\nutilize data harvested from Amazon Mechanical Turk (AMT) through an innovative\nnew technique we call self-dialogues. These are conversations in which a single\nAMT Worker plays both participants in a dialogue. Such dialogues are\nsurprisingly natural, efficient to collect and reflective of relevant and/or\ntrending topics. These self-dialogues provide training data for a generative\nneural network as well as a basis for soft rules used by a matching score\ncomponent. Each match of a soft rule against a user utterance is associated\nwith a confidence score which we show is strongly indicative of reply quality,\nallowing this component to self-censor and be effectively integrated with other\ncomponents. Edina's full architecture features a rule-based system backing off\nto a matching score, backing off to a generative neural network. Our hybrid\ndata-driven methodology thus addresses both coverage limitations of a strictly\nrule-based approach and the lack of guarantees of a strictly machine-learning\napproach.", "published": "2017-09-28 06:13:33", "link": "http://arxiv.org/abs/1709.09816v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Inference of Personal Attributes from Tweets Using Machine Learning", "abstract": "Using machine learning algorithms, including deep learning, we studied the\nprediction of personal attributes from the text of tweets, such as gender,\noccupation, and age groups. We applied word2vec to construct word vectors,\nwhich were then used to vectorize tweet blocks. The resulting tweet vectors\nwere used as inputs for training models, and the prediction accuracy of those\nmodels was examined as a function of the dimension of the tweet vectors and the\nsize of the tweet blacks. The results showed that the machine learning\nalgorithms could predict the three personal attributes of interest with 60-70%\naccuracy.", "published": "2017-09-28 12:58:18", "link": "http://arxiv.org/abs/1709.09927v3", "categories": ["cs.CY", "cs.CL", "cs.SI"], "primary_category": "cs.CY"}
{"title": "Structured Embedding Models for Grouped Data", "abstract": "Word embeddings are a powerful approach for analyzing language, and\nexponential family embeddings (EFE) extend them to other types of data. Here we\ndevelop structured exponential family embeddings (S-EFE), a method for\ndiscovering embeddings that vary across related groups of data. We study how\nthe word usage of U.S. Congressional speeches varies across states and party\naffiliation, how words are used differently across sections of the ArXiv, and\nhow the co-purchase patterns of groceries can vary across seasons. Key to the\nsuccess of our method is that the groups share statistical information. We\ndevelop two sharing strategies: hierarchical modeling and amortization. We\ndemonstrate the benefits of this approach in empirical studies of speeches,\nabstracts, and shopping baskets. We show how S-EFE enables group-specific\ninterpretation of word usage, and outperforms EFE in predicting held-out data.", "published": "2017-09-28 14:14:58", "link": "http://arxiv.org/abs/1709.10367v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "The Dependence of Frequency Distributions on Multiple Meanings of Words,\n  Codes and Signs", "abstract": "The dependence of the frequency distributions due to multiple meanings of\nwords in a text is investigated by deleting letters. By coding the words with\nfewer letters the number of meanings per coded word increases. This increase is\nmeasured and used as an input in a predictive theory. For a text written in\nEnglish, the word-frequency distribution is broad and fat-tailed, whereas if\nthe words are only represented by their first letter the distribution becomes\nexponential. Both distribution are well predicted by the theory, as is the\nwhole sequence obtained by consecutively representing the words by the first\nL=6,5,4,3,2,1 letters. Comparisons of texts written by Chinese characters and\nthe same texts written by letter-codes are made and the similarity of the\ncorresponding frequency-distributions are interpreted as a consequence of the\nmultiple meanings of Chinese characters. This further implies that the\ndifference of the shape for word-frequencies for an English text written by\nletters and a Chinese text written by Chinese characters is due to the coding\nand not to the language per se.", "published": "2017-09-28 00:39:25", "link": "http://arxiv.org/abs/1710.00683v1", "categories": ["cs.CL", "eess.AS", "physics.soc-ph"], "primary_category": "cs.CL"}
{"title": "Efficient Convolutional Neural Network For Audio Event Detection", "abstract": "Wireless distributed systems as used in sensor networks, Internet-of-Things\nand cyber-physical systems, impose high requirements on resource efficiency.\nAdvanced preprocessing and classification of data at the network edge can help\nto decrease the communication demand and to reduce the amount of data to be\nprocessed centrally. In the area of distributed acoustic sensing, the\ncombination of algorithms with a high classification rate and\nresource-constraint embedded systems is essential. Unfortunately, algorithms\nfor acoustic event detection have a high memory and computational demand and\nare not suited for execution at the network edge. This paper addresses these\naspects by applying structural optimizations to a convolutional neural network\nfor audio event detection to reduce the memory requirement by a factor of more\nthan 500 and the computational effort by a factor of 2.1 while performing 9.2%\nbetter.", "published": "2017-09-28 10:54:01", "link": "http://arxiv.org/abs/1709.09888v1", "categories": ["cs.CV", "eess.AS"], "primary_category": "cs.CV"}
{"title": "A Generative Model for Score Normalization in Speaker Recognition", "abstract": "We propose a theoretical framework for thinking about score normalization,\nwhich confirms that normalization is not needed under (admittedly fragile)\nideal conditions. If, however, these conditions are not met, e.g. under\ndata-set shift between training and runtime, our theory reveals dependencies\nbetween scores that could be exploited by strategies such as score\nnormalization. Indeed, it has been demonstrated over and over experimentally,\nthat various ad-hoc score normalization recipes do work. We present a first\nattempt at using probability theory to design a generative score-space\nnormalization model which gives similar improvements to ZT-norm on the\ntext-dependent RSR 2015 database.", "published": "2017-09-28 09:32:10", "link": "http://arxiv.org/abs/1709.09868v1", "categories": ["stat.ML", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "stat.ML"}
