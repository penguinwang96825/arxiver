{"title": "Local Monotonic Attention Mechanism for End-to-End Speech and Language\n  Processing", "abstract": "Recently, encoder-decoder neural networks have shown impressive performance\non many sequence-related tasks. The architecture commonly uses an attentional\nmechanism which allows the model to learn alignments between the source and the\ntarget sequence. Most attentional mechanisms used today is based on a global\nattention property which requires a computation of a weighted summarization of\nthe whole input sequence generated by encoder states. However, it is\ncomputationally expensive and often produces misalignment on the longer input\nsequence. Furthermore, it does not fit with monotonous or left-to-right nature\nin several tasks, such as automatic speech recognition (ASR),\ngrapheme-to-phoneme (G2P), etc. In this paper, we propose a novel attention\nmechanism that has local and monotonic properties. Various ways to control\nthose properties are also explored. Experimental results on ASR, G2P and\nmachine translation between two languages with similar sentence structures,\ndemonstrate that the proposed encoder-decoder model with local monotonic\nattention could achieve significant performance improvements and reduce the\ncomputational complexity in comparison with the one that used the standard\nglobal attention architecture.", "published": "2017-05-23 06:32:36", "link": "http://arxiv.org/abs/1705.08091v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Question-Answering with Grammatically-Interpretable Representations", "abstract": "We introduce an architecture, the Tensor Product Recurrent Network (TPRN). In\nour application of TPRN, internal representations learned by end-to-end\noptimization in a deep neural network performing a textual question-answering\n(QA) task can be interpreted using basic concepts from linguistic theory. No\nperformance penalty need be paid for this increased interpretability: the\nproposed model performs comparably to a state-of-the-art system on the SQuAD QA\ntask. The internal representation which is interpreted is a Tensor Product\nRepresentation: for each input word, the model selects a symbol to encode the\nword, and a role in which to place the symbol, and binds the two together. The\nselection is via soft attention. The overall interpretation is built from\ninterpretations of the symbols, as recruited by the trained model, and\ninterpretations of the roles as used by the model. We find support for our\ninitial hypothesis that symbols can be interpreted as lexical-semantic word\nmeanings, while roles can be interpreted as approximations of grammatical roles\n(or categories) such as subject, wh-word, determiner, etc. Fine-grained\nanalysis reveals specific correspondences between the learned roles and parts\nof speech as assigned by a standard tagger (Toutanova et al. 2003), and finds\nseveral discrepancies in the model's favor. In this sense, the model learns\nsignificant aspects of grammar, after having been exposed solely to\nlinguistically unannotated text, questions, and answers: no prior linguistic\nknowledge is given to the model. What is given is the means to build\nrepresentations using symbols and roles, with an inductive bias favoring use of\nthese in an approximately discrete manner.", "published": "2017-05-23 17:40:14", "link": "http://arxiv.org/abs/1705.08432v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contextualizing Citations for Scientific Summarization using Word\n  Embeddings and Domain Knowledge", "abstract": "Citation texts are sometimes not very informative or in some cases inaccurate\nby themselves; they need the appropriate context from the referenced paper to\nreflect its exact contributions. To address this problem, we propose an\nunsupervised model that uses distributed representation of words as well as\ndomain knowledge to extract the appropriate context from the reference paper.\nEvaluation results show the effectiveness of our model by significantly\noutperforming the state-of-the-art. We furthermore demonstrate how an effective\ncontextualization method results in improving citation-based summarization of\nthe scientific articles.", "published": "2017-05-23 02:55:56", "link": "http://arxiv.org/abs/1705.08063v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "TwiInsight: Discovering Topics and Sentiments from Social Media Datasets", "abstract": "Social media platforms contain a great wealth of information which provides\nopportunities for us to explore hidden patterns or unknown correlations, and\nunderstand people's satisfaction with what they are discussing. As one\nshowcase, in this paper, we present a system, TwiInsight which explores the\ninsight of Twitter data. Different from other Twitter analysis systems,\nTwiInsight automatically extracts the popular topics under different categories\n(e.g., healthcare, food, technology, sports and transport) discussed in Twitter\nvia topic modeling and also identifies the correlated topics across different\ncategories. Additionally, it also discovers the people's opinions on the tweets\nand topics via the sentiment analysis. The system also employs an intuitive and\ninformative visualization to show the uncovered insight. Furthermore, we also\ndevelop and compare six most popular algorithms - three for sentiment analysis\nand three for topic modeling.", "published": "2017-05-23 06:49:12", "link": "http://arxiv.org/abs/1705.08094v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Second-Order Word Embeddings from Nearest Neighbor Topological Features", "abstract": "We introduce second-order vector representations of words, induced from\nnearest neighborhood topological features in pre-trained contextual word\nembeddings. We then analyze the effects of using second-order embeddings as\ninput features in two deep natural language processing models, for named entity\nrecognition and recognizing textual entailment, as well as a linear model for\nparaphrase recognition. Surprisingly, we find that nearest neighbor information\nalone is sufficient to capture most of the performance benefits derived from\nusing pre-trained word embeddings. Furthermore, second-order embeddings are\nable to handle highly heterogeneous data better than first-order\nrepresentations, though at the cost of some specificity. Additionally,\naugmenting contextual embeddings with second-order information further improves\nmodel performance in some cases. Due to variance in the random initializations\nof word embeddings, utilizing nearest neighbor features from multiple\nfirst-order embedding samples can also contribute to downstream performance\ngains. Finally, we identify intriguing characteristics of second-order\nembedding spaces for further research, including much higher density and\ndifferent semantic interpretations of cosine similarity.", "published": "2017-05-23 19:12:05", "link": "http://arxiv.org/abs/1705.08488v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards a Knowledge Graph based Speech Interface", "abstract": "Applications which use human speech as an input require a speech interface\nwith high recognition accuracy. The words or phrases in the recognised text are\nannotated with a machine-understandable meaning and linked to knowledge graphs\nfor further processing by the target application. These semantic annotations of\nrecognised words can be represented as a subject-predicate-object triples which\ncollectively form a graph often referred to as a knowledge graph. This type of\nknowledge representation facilitates to use speech interfaces with any spoken\ninput application, since the information is represented in logical, semantic\nform, retrieving and storing can be followed using any web standard query\nlanguages. In this work, we develop a methodology for linking speech input to\nknowledge graphs and study the impact of recognition errors in the overall\nprocess. We show that for a corpus with lower WER, the annotation and linking\nof entities to the DBpedia knowledge graph is considerable. DBpedia Spotlight,\na tool to interlink text documents with the linked open data is used to link\nthe speech recognition output to the DBpedia knowledge graph. Such a\nknowledge-based speech recognition interface is useful for applications such as\nquestion answering or spoken dialog systems.", "published": "2017-05-23 17:54:32", "link": "http://arxiv.org/abs/1705.09222v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Better Text Understanding Through Image-To-Text Transfer", "abstract": "Generic text embeddings are successfully used in a variety of tasks. However,\nthey are often learnt by capturing the co-occurrence structure from pure text\ncorpora, resulting in limitations of their ability to generalize. In this\npaper, we explore models that incorporate visual information into the text\nrepresentation. Based on comprehensive ablation studies, we propose a\nconceptually simple, yet well performing architecture. It outperforms previous\nmultimodal approaches on a set of well established benchmarks. We also improve\nthe state-of-the-art results for image-related text datasets, using orders of\nmagnitude less data.", "published": "2017-05-23 16:06:32", "link": "http://arxiv.org/abs/1705.08386v2", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Grounded Recurrent Neural Networks", "abstract": "In this work, we present the Grounded Recurrent Neural Network (GRNN), a\nrecurrent neural network architecture for multi-label prediction which\nexplicitly ties labels to specific dimensions of the recurrent hidden state (we\ncall this process \"grounding\"). The approach is particularly well-suited for\nextracting large numbers of concepts from text. We apply the new model to\naddress an important problem in healthcare of understanding what medical\nconcepts are discussed in clinical text. Using a publicly available dataset\nderived from Intensive Care Units, we learn to label a patient's diagnoses and\nprocedures from their discharge summary. Our evaluation shows a clear advantage\nto using our proposed architecture over a variety of strong baselines.", "published": "2017-05-23 23:17:49", "link": "http://arxiv.org/abs/1705.08557v1", "categories": ["stat.ML", "cs.CL", "cs.LG", "cs.NE"], "primary_category": "stat.ML"}
{"title": "Latent Multi-task Architecture Learning", "abstract": "Multi-task learning (MTL) allows deep neural networks to learn from related\ntasks by sharing parameters with other networks. In practice, however, MTL\ninvolves searching an enormous space of possible parameter sharing\narchitectures to find (a) the layers or subspaces that benefit from sharing,\n(b) the appropriate amount of sharing, and (c) the appropriate relative weights\nof the different task losses. Recent work has addressed each of the above\nproblems in isolation. In this work we present an approach that learns a latent\nmulti-task architecture that jointly addresses (a)--(c). We present experiments\non synthetic data and data from OntoNotes 5.0, including four different tasks\nand seven different domains. Our extension consistently outperforms previous\napproaches to learning latent architectures for multi-task problems and\nachieves up to 15% average error reductions over common approaches to MTL.", "published": "2017-05-23 08:58:09", "link": "http://arxiv.org/abs/1705.08142v3", "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.LG", "cs.NE"], "primary_category": "stat.ML"}
