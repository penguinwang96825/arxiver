{"title": "Technical Report: a tool for measuring Prosodic Accommodation", "abstract": "This article has been withdrawn by arXiv administrators because the submitter\ndid not have the legal authority to grant the license applied to the work.", "published": "2015-12-30 15:46:30", "link": "http://arxiv.org/abs/1512.08982v1", "categories": ["cs.SD", "cs.CL"], "primary_category": "cs.SD"}
{"title": "Learning Natural Language Inference with LSTM", "abstract": "Natural language inference (NLI) is a fundamentally important task in natural\nlanguage processing that has many applications. The recently released Stanford\nNatural Language Inference (SNLI) corpus has made it possible to develop and\nevaluate learning-centered methods such as deep neural networks for natural\nlanguage inference (NLI). In this paper, we propose a special long short-term\nmemory (LSTM) architecture for NLI. Our model builds on top of a recently\nproposed neural attention model for NLI but is based on a significantly\ndifferent idea. Instead of deriving sentence embeddings for the premise and the\nhypothesis to be used for classification, our solution uses a match-LSTM to\nperform word-by-word matching of the hypothesis with the premise. This LSTM is\nable to place more emphasis on important word-level matching results. In\nparticular, we observe that this LSTM remembers important mismatches that are\ncritical for predicting the contradiction or the neutral relationship label. On\nthe SNLI corpus, our model achieves an accuracy of 86.1%, outperforming the\nstate of the art.", "published": "2015-12-30 05:02:53", "link": "http://arxiv.org/abs/1512.08849v2", "categories": ["cs.CL", "cs.AI", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Online Keyword Spotting with a Character-Level Recurrent Neural Network", "abstract": "In this paper, we propose a context-aware keyword spotting model employing a\ncharacter-level recurrent neural network (RNN) for spoken term detection in\ncontinuous speech. The RNN is end-to-end trained with connectionist temporal\nclassification (CTC) to generate the probabilities of character and\nword-boundary labels. There is no need for the phonetic transcription, senone\nmodeling, or system dictionary in training and testing. Also, keywords can\neasily be added and modified by editing the text based keyword list without\nretraining the RNN. Moreover, the unidirectional RNN processes an infinitely\nlong input audio streams without pre-segmentation and keywords are detected\nwith low-latency before the utterance is finished. Experimental results show\nthat the proposed keyword spotter significantly outperforms the deep neural\nnetwork (DNN) and hidden Markov model (HMM) based keyword-filler model even\nwith less computations.", "published": "2015-12-30 10:32:12", "link": "http://arxiv.org/abs/1512.08903v1", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
