{"title": "Pitfalls in Evaluating Language Model Forecasters", "abstract": "Large language models (LLMs) have recently been applied to forecasting tasks,\nwith some works claiming these systems match or exceed human performance. In\nthis paper, we argue that, as a community, we should be careful about such\nconclusions as evaluating LLM forecasters presents unique challenges. We\nidentify two broad categories of issues: (1) difficulty in trusting evaluation\nresults due to many forms of temporal leakage, and (2) difficulty in\nextrapolating from evaluation performance to real-world forecasting. Through\nsystematic analysis and concrete examples from prior work, we demonstrate how\nevaluation flaws can raise concerns about current and future performance\nclaims. We argue that more rigorous evaluation methodologies are needed to\nconfidently assess the forecasting abilities of LLMs.", "published": "2025-05-31 21:49:17", "link": "http://arxiv.org/abs/2506.00723v1", "categories": ["cs.LG", "cs.AI", "cs.IR"], "primary_category": "cs.LG"}
{"title": "Improving Dialogue State Tracking through Combinatorial Search for In-Context Examples", "abstract": "In dialogue state tracking (DST), in-context learning comprises a retriever\nthat selects labeled dialogues as in-context examples and a DST model that uses\nthese examples to infer the dialogue state of the query dialogue. Existing\nmethods for constructing training data for retrievers suffer from three key\nlimitations: (1) the synergistic effect of examples is not considered, (2) the\nlinguistic characteristics of the query are not sufficiently factored in, and\n(3) scoring is not directly optimized for DST performance. Consequently, the\nretriever can fail to retrieve examples that would substantially improve DST\nperformance. To address these issues, we present CombiSearch, a method that\nscores effective in-context examples based on their combinatorial impact on DST\nperformance. Our evaluation on MultiWOZ shows that retrievers trained with\nCombiSearch surpass state-of-the-art models, achieving a 20x gain in data\nefficiency and generalizing well to the SGD dataset. Moreover, CombiSearch\nattains a 12% absolute improvement in the upper bound DST performance over\ntraditional approaches when no retrieval errors are assumed. This significantly\nincreases the headroom for practical DST performance while demonstrating that\nexisting methods rely on suboptimal data for retriever training.", "published": "2025-05-31 16:20:14", "link": "http://arxiv.org/abs/2506.00622v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Optimizing Question Semantic Space for Dynamic Retrieval-Augmented Multi-hop Question Answering", "abstract": "Retrieval-augmented generation (RAG) is usually integrated into large\nlanguage models (LLMs) to mitigate hallucinations and knowledge obsolescence.\nWhereas,conventional one-step retrieve-and-read methods are insufficient for\nmulti-hop question answering, facing challenges of retrieval semantic\nmismatching and the high cost in handling interdependent subquestions. In this\npaper, we propose Optimizing Question Semantic Space for Dynamic\nRetrieval-Augmented Multi-hop Question Answering (Q-DREAM). Q-DREAM consists of\nthree key modules: (1) the Question Decomposition Module (QDM), which\ndecomposes multi-hop questions into fine-grained subquestions; (2) the\nSubquestion Dependency Optimizer Module (SDOM), which models the interdependent\nrelations of subquestions for better understanding; and (3) the Dynamic Passage\nRetrieval Module (DPRM), which aligns subquestions with relevant passages by\noptimizing the semantic embeddings. Experimental results across various\nbenchmarks demonstrate that Q-DREAM significantly outperforms existing RAG\nmethods, achieving state-of-the-art performance in both in-domain and\nout-of-domain settings. Notably, Q-DREAM also improves retrieval efficiency\nwhile maintaining high accuracy compared with recent baselines.", "published": "2025-05-31 09:57:07", "link": "http://arxiv.org/abs/2506.00491v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "DV365: Extremely Long User History Modeling at Instagram", "abstract": "Long user history is highly valuable signal for recommendation systems, but\neffectively incorporating it often comes with high cost in terms of data center\npower consumption and GPU. In this work, we chose offline embedding over\nend-to-end sequence length optimization methods to enable extremely long user\nsequence modeling as a cost-effective solution, and propose a new user\nembedding learning strategy, multi-slicing and summarization, that generates\nhighly generalizable user representation of user's long-term stable interest.\nHistory length we encoded in this embedding is up to 70,000 and on average\n40,000. This embedding, named as DV365, is proven highly incremental on top of\nadvanced attentive user sequence models deployed in Instagram. Produced by a\nsingle upstream foundational model, it is launched in 15 different models\nacross Instagram and Threads with significant impact, and has been production\nbattle-proven for >1 year since our first launch.", "published": "2025-05-31 08:09:54", "link": "http://arxiv.org/abs/2506.00450v1", "categories": ["cs.IR", "cs.LG"], "primary_category": "cs.IR"}
{"title": "K-order Ranking Preference Optimization for Large Language Models", "abstract": "To adapt large language models (LLMs) to ranking tasks, existing list-wise\nmethods, represented by list-wise Direct Preference Optimization (DPO), focus\non optimizing partial-order or full-order list ranking consistency for LLMs to\nenhance their ranking abilities. However, we argue that optimizing top-K\nranking consistency could be more appropriate for real-world applications.\nThere are two main reasons: (1) users are typically concerned with only the\ntop-K results, making top-K ranking more important, and (2) tail items often\nlack precise feedback, making top-K ranking more reliable. Based on this, we\npropose K-order Ranking Preference Optimization (KPO) by extending the DPO's\nPlackett-Luce model to accommodate top-K rankings. Additionally, recognizing\nthat the number of important items can vary across queries, we extend KPO to\ndynamically determine appropriate K for different samples and introduce a\ncurriculum learning strategy to boost training efficiency. Extensive\nexperiments demonstrate the effectiveness of KPO, highlighting its high sample\nefficiency and robustness to noise. The code is available at\nhttps://github.com/Lanyu0303/KPO.", "published": "2025-05-31 07:46:42", "link": "http://arxiv.org/abs/2506.00441v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Adapting General-Purpose Embedding Models to Private Datasets Using Keyword-based Retrieval", "abstract": "Text embedding models play a cornerstone role in AI applications, such as\nretrieval-augmented generation (RAG). While general-purpose text embedding\nmodels demonstrate strong performance on generic retrieval benchmarks, their\neffectiveness diminishes when applied to private datasets (e.g.,\ncompany-specific proprietary data), which often contain specialized terminology\nand lingo. In this work, we introduce BMEmbed, a novel method for adapting\ngeneral-purpose text embedding models to private datasets. By leveraging the\nwell-established keyword-based retrieval technique (BM25), we construct\nsupervisory signals from the ranking of keyword-based retrieval results to\nfacilitate model adaptation. We evaluate BMEmbed across a range of domains,\ndatasets, and models, showing consistent improvements in retrieval performance.\nMoreover, we provide empirical insights into how BM25-based signals contribute\nto improving embeddings by fostering alignment and uniformity, highlighting the\nvalue of this approach in adapting models to domain-specific data. We release\nthe source code available at https://github.com/BaileyWei/BMEmbed for the\nresearch community.", "published": "2025-05-31 03:06:09", "link": "http://arxiv.org/abs/2506.00363v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "A Sequential Computation Algorithm for the Center of the Smallest Enclosing Ball", "abstract": "In this paper, we consider the problem of finding the center $Q^\\ast$ of the\nSEB (smallest enclosing ball) for $n$ points in $d$-dimensional Euclidean\nspace. One application of the SEB is SVDD (support vector data description) in\nsupport vector machines. Our objective is to develop a sequential computation\nalgorithm for determining the barycentric coordinate of $Q^\\ast$. To achieve\nit, we apply the concept of the Arimoto-Blahut algorithm, which is a sequential\ncomputation algorithm used to compute the channel capacity. We first consider\nthe case in which an equidistant point $\\widetilde{Q}$ from the $n$ points\nexists, and construct a recurrence formula that converges to the barycentric\ncoordinate $\\widetilde{\\bm\\lambda}$ of $\\widetilde{Q}$. When $\\widetilde{Q}$\nlies within the convex hull of the $n$ points, $\\widetilde{Q}$ coincides with\n$Q^\\ast$, hence in this case, the recurrence formula converges to the\nbarycentric coordinate $\\bm\\lambda^\\ast$ of $Q^\\ast$. The resulting recurrence\nformula is very simple because it uses only the coordinates of the $n$ points.\nThe computational complexity, with an approximation error of $\\epsilon$ to the\nexact solution $\\widetilde{\\bm\\lambda}$, is $O(\\kappa n^2\\log(1/\\epsilon))$,\nwhere $\\kappa$ is a value determined by the $n$ points. Furthermore, we modify\nthe algorithm so that it can also be applied in cases where $\\widetilde{Q}$\ndoes not exist, and evaluate the convergence performance numerically. We\ncompare the proposed algorithm with conventional algorithms in terms of run\ntime and computational accuracy through several examples. The proposed\nalgorithm has some advantages and some disadvantages compared to the\nconventional algorithms, but overall, since the proposed algorithm can be\ncomputed using a very simple formula, it is considered sufficiently practical.", "published": "2025-05-31 22:42:35", "link": "http://arxiv.org/abs/2506.00734v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "On the Use of Bj\u00f6rck Sequences in LEO-based PNT Systems", "abstract": "In this paper, we investigate the use of Bj\\\"orck sequences, a class of\nconstant amplitude zero autocorrelation (CAZAC) sequences, as a potential\ncandidate for the design of positioning reference signals (PRS) in Low Earth\nOrbit (LEO)-based positioning, navigation, and timing (PNT) systems. Unlike\nlegacy systems such as Global Navigation Satellite Systems (GNSS) or\nterrestrial networks (TNs), LEO-based systems experience large Doppler shifts\nand delay spreads, where traditional orthogonalization methods become\nineffective. Compared to commonly used sequences such as Gold and Zadoff-Chu\n(ZC), Bj\\\"orck sequences offer improved ambiguity function behavior, nearly\nideal autocorrelation, greater resilience to interference, and accurate delay\nestimation in high Doppler environments. We further propose a novel sequence\nconstruction method to extend Bj\\\"orck sequences to non-prime lengths while\nminimizing cyclic autocorrelation. Focusing on LEO-based non-terrestrial\nnetwork (NTN) localization, we evaluate positioning accuracy under various\ninterference conditions, comparing the performance of Bj\\\"orck sequences\nagainst Gold sequences, which are traditionally used for PRS generation. While\nBj\\\"orck sequences demonstrate strong performance in Doppler-rich environments,\nwe identify an inherent Doppler-dependent behavior that may lead to sequence\nmisidentification. To mitigate this, we propose two strategies: 1) leveraging\nthe availability of a coarse Doppler estimate and 2) employing sequence subset\nselection to ensure sufficient separation between sequences to account for\nmaximum Doppler uncertainty. Finally, we present scalable sequence reuse\nstrategies for large LEO constellations.", "published": "2025-05-31 20:52:01", "link": "http://arxiv.org/abs/2506.00706v1", "categories": ["eess.SP", "cs.IT", "math.IT"], "primary_category": "eess.SP"}
{"title": "Statistical Signal Processing for Quantum Error Mitigation", "abstract": "In the noisy intermediate-scale quantum (NISQ) era, quantum error mitigation\n(QEM) is essential for producing reliable outputs from quantum circuits. We\npresent a statistical signal processing approach to QEM that estimates the most\nlikely noiseless outputs from noisy quantum measurements. Our model assumes\nthat circuit depth is sufficient for depolarizing noise, producing corrupted\nobservations that resemble a uniform distribution alongside classical bit-flip\nerrors from readout. Our method consists of two steps: a filtering stage that\ndiscards uninformative depolarizing noise and an expectation-maximization (EM)\nalgorithm that computes a maximum likelihood (ML) estimate over the remaining\ndata. We demonstrate the effectiveness of this approach on small-qubit systems\nusing IBM circuit simulations in Qiskit and compare its performance to\ncontemporary statistical QEM techniques. We also show that our method scales to\nlarger qubit counts using synthetically generated data consistent with our\nnoise model. These results suggest that principled statistical methods can\noffer scalable and interpretable solutions for quantum error mitigation in\nrealistic NISQ settings.", "published": "2025-05-31 19:34:19", "link": "http://arxiv.org/abs/2506.00683v1", "categories": ["quant-ph", "cs.IT", "math.IT"], "primary_category": "quant-ph"}
{"title": "Over-the-Air Fronthaul Signaling for Uplink Cell-Free Massive MIMO Systems", "abstract": "We propose a novel resource-efficient over-the-air(OTA) computation framework\nto address the huge fronthaul computational and control overhead requirements\nin cell-free massive multiple-input multiple-output (MIMO) networks. We show\nthat the global sufficient statistics to decode the data symbols can be\ncomputed OTA using the locally available information at the access points\n(APs). We provide the essential signal processing aspects at the APs and the\ncentral processing unit (CPU) to facilitate the OTA computation of sufficient\nstatistics. The proposed framework scales effectively with an increase in the\nnumber of APs. We also make a comprehensive study of the benefits of an OTA\nframework compared to a conventional digital fronthaul in terms of the overhead\nassociated in transferring the sufficient statistics from the APs to the CPU.\nTo evaluate the performance of the OTA framework, we give closed-form\nexpressions for the mean-square error (MSE)of the estimators of sufficient\nstatistics and the overall data estimator. Furthermore, we assess the symbol\nerror rate (SER)and bit error rate (BER) of the user equipment (UEs) data to\ndemonstrate the efficacy of our method, and benchmark them against the\nstate-of-the-art wired fronthaul networks.", "published": "2025-05-31 17:51:49", "link": "http://arxiv.org/abs/2506.00655v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Dual-UAV-Aided Covert Communications for Air-to-Ground ISAC Networks", "abstract": "To enhance both the sensing and covert communication performance, a\ndual-unmanned aerial vehicle (UAV)-aided scheme is proposed for integrated\nsensing and communication networks, in which one UAV maneuvers as the aerial\ndual-functional base-station (BS), while another UAV flies as the cooperative\njammer. Artificial noise (AN) transmitted by the jamming UAV is utilized not\nonly to confuse the ground warden but also to aid the aerial BS to sense\nmultiple ground targets by combing the target-echoed dual-functional waveform\nand AN components from a perspective of the hybrid monostatitc-bistatic radar.\nWe employ the distance-normalized beampattern sum-gain to measure the sensing\nperformance. To maximize the average covert rate (ACR) from the aerial BS to\nthe ground user, the dual-functional BS beamforming, jamming UAV beamforming,\nand dual-UAV trajectory are co-designed, subject to transmit power budgets, UAV\nmaneuver constraint, covertness requirement, and sensing performance\nconstraint. The imperfect successive interference cancellation (SIC) effects on\nthe received signal-to-interference-plus-noise ratio are also considered in\nmaximizing the ACR. To tackle the highly complicated non-convex ACR\nmaximization problem, dual-UAV beamforming and dual-UAV trajectory are\noptimized in a block coordinate descent way using the trust-region successive\nconvex approximation and semidefinite relaxation. To find the dual-UAV maneuver\nlocations suitable for sensing the ground targets, we first optimize the\ndual-UAV trajectory for the covert communication purpose only and then solve a\nweighted distance minimization problem for the covert communication and sensing\npurpose.", "published": "2025-05-31 15:17:58", "link": "http://arxiv.org/abs/2506.00601v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Geometry and Dress groups with non-symmetric cost functions", "abstract": "A metric relation by definition is symmetric. Since many data sets are\nnon-symmetric, in this paper we develop a systematic theory of non-symmetric\ncost functions. Betweenness relations play an important role. We also introduce\nthe notion of a Dress group in the non-symmetric setting and indicate a notion\nof curvature.", "published": "2025-05-31 14:53:34", "link": "http://arxiv.org/abs/2506.00590v1", "categories": ["math.MG", "cs.IT", "math.CT", "math.GR", "math.IT", "51F99, 51D99, 37B99, 20C99, 18F99", "F.4.2; F.4.3"], "primary_category": "math.MG"}
{"title": "Combinatorial $t$-Designs from Finite Abelian Groups and Their Applications to Elliptic Curve Codes", "abstract": "In this paper, we establish the conditions for some finite abelian groups and\nthe family all the $k$-sets in each of them summing up to an element $x$ to\nform $t$-designs. We fully characterize the sufficient and necessary conditions\nfor the incidence structures to form $1$-designs in finite abelian $p$-groups,\ngeneralizing existing results on vector spaces over finite fields. For finite\nabelian groups of exponent $pq$, we also propose sufficient and necessary\nconditions for the incidence structures to form a $1$-designs. Furthermore,\nsome interesting observations of the general case when the group is cyclic or\nnon-cyclic are presented and the relations between $(t-1)$-designs and\n$t$-designs from subset sums are established. As an application, we demonstrate\nthe correspondence between $t$-designs from the minimum-weight codewords in\nelliptic curve codes and subset-sum designs in their groups of rational points.\nBy such a correspondence, elliptic curve codes supporting designs can be simply\nderived from subset sums in finite abelian groups that supporting designs.", "published": "2025-05-31 07:16:20", "link": "http://arxiv.org/abs/2506.00429v1", "categories": ["math.CO", "cs.IT", "math.IT", "05B05, 94B05"], "primary_category": "math.CO"}
{"title": "Accurate Estimation of Mutual Information in High Dimensional Data", "abstract": "Mutual information (MI) is a measure of statistical dependencies between two\nvariables, widely used in data analysis. Thus, accurate methods for estimating\nMI from empirical data are crucial. Such estimation is a hard problem, and\nthere are provably no estimators that are universally good for finite datasets.\nCommon estimators struggle with high-dimensional data, which is a staple of\nmodern experiments. Recently, promising machine learning-based MI estimation\nmethods have emerged. Yet it remains unclear if and when they produce accurate\nresults, depending on dataset sizes, statistical structure of the data, and\nhyperparameters of the estimators, such as the embedding dimensionality or the\nduration of training. There are also no accepted tests to signal when the\nestimators are inaccurate. Here, we systematically explore these gaps. We\npropose and validate a protocol for MI estimation that includes explicit checks\nensuring reliability and statistical consistency. Contrary to accepted wisdom,\nwe demonstrate that reliable MI estimation is achievable even with severely\nundersampled, high-dimensional datasets, provided these data admit accurate\nlow-dimensional representations. These findings broaden the potential use of\nmachine learning-based MI estimation methods in real-world data analysis and\nprovide new insights into when and why modern high-dimensional, self-supervised\nalgorithms perform effectively.", "published": "2025-05-31 01:06:18", "link": "http://arxiv.org/abs/2506.00330v1", "categories": ["physics.data-an", "cs.IT", "math.IT", "stat.ML"], "primary_category": "physics.data-an"}
{"title": "Adaptive Traffic-Following Scheme for Orderly Distributed Control of Multi-Vehicle Systems", "abstract": "We present an adaptive control scheme to enable the emergence of order within\ndistributed, autonomous multi-agent systems. Past studies showed that under\nhigh-density conditions, order generated from traffic-following behavior\nreduces travel times, while under low densities, choosing direct paths is more\nbeneficial. In this paper, we leveraged those findings to allow aircraft to\nindependently and dynamically adjust their degree of traffic-following behavior\nbased on the current state of the airspace. This enables aircraft to follow\nother traffic only when beneficial. Quantitative analyses revealed that dynamic\ntraffic-following behavior results in lower aircraft travel times at the cost\nof minimal levels of additional disorder to the airspace. The sensitivity of\nthese benefits to temporal and spatial horizons was also investigated. Overall,\nthis work highlights the benefits, and potential necessity, of incorporating\nself-organizing behavior in making distributed, autonomous multi-agent systems\nscalable.", "published": "2025-05-31 20:18:35", "link": "http://arxiv.org/abs/2506.00703v1", "categories": ["cs.MA", "cs.ET", "cs.RO", "cs.SY", "eess.SY"], "primary_category": "cs.MA"}
{"title": "Reasoning Like an Economist: Post-Training on Economic Problems Induces Strategic Generalization in LLMs", "abstract": "Directly training Large Language Models (LLMs) for Multi-Agent Systems (MAS)\nremains challenging due to intricate reward modeling, dynamic agent\ninteractions, and demanding generalization requirements. This paper explores\nwhether post-training techniques, specifically Supervised Fine-Tuning (SFT) and\nReinforcement Learning with Verifiable Rewards (RLVR), can effectively\n$\\textit{generalize}$ to multi-agent scenarios. We use economic reasoning as a\ntestbed, leveraging its strong foundations in mathematics and game theory, its\ndemand for structured analytical reasoning, and its relevance to real-world\napplications such as market design, resource allocation, and policy analysis.\nWe introduce $\\textbf{Recon}$ ($\\textbf{R}$easoning like an\n$\\textbf{ECON}$omist), a 7B-parameter open-source LLM post-trained on a\nhand-curated dataset of 2,100 high-quality economic reasoning problems.\nComprehensive evaluation on economic reasoning benchmarks and multi-agent games\nreveals clear improvements in structured reasoning and economic rationality.\nThese results underscore the promise of domain-aligned post-training for\nenhancing reasoning and agent alignment, shedding light on the roles of SFT and\nRL in shaping model behavior. Code is available at\nhttps://github.com/MasterZhou1/Recon .", "published": "2025-05-31 14:22:40", "link": "http://arxiv.org/abs/2506.00577v1", "categories": ["cs.AI", "cs.CL", "cs.GT", "cs.MA"], "primary_category": "cs.AI"}
{"title": "Two-Sided Manipulation Games in Stable Matching Markets", "abstract": "The Deferred Acceptance (DA) algorithm is an elegant procedure for finding a\nstable matching in two-sided matching markets. It ensures that no pair of\nagents prefers each other to their matched partners. In this work, we initiate\nthe study of two-sided manipulations in matching markets as non-cooperative\ngames. We introduce the accomplice manipulation game, where a man misreports to\nhelp a specific woman obtain a better partner, whenever possible. We provide a\npolynomial time algorithm for finding a pure strategy Nash equilibrium (NE) and\nshow that our algorithm always yields a stable matching - although not every\nNash equilibrium corresponds to a stable matching. Additionally, we show how\nour analytical techniques for the accomplice manipulation game can be applied\nto other manipulation games in matching markets, such as one-for-many and the\nstandard self-manipulation games. We complement our theoretical findings with\nempirical evaluations of different properties of the resulting NE, such as the\nwelfare of the agents.", "published": "2025-05-31 13:19:31", "link": "http://arxiv.org/abs/2506.00554v1", "categories": ["cs.GT", "cs.MA"], "primary_category": "cs.GT"}
{"title": "Reinforcement Learning for Hanabi", "abstract": "Hanabi has become a popular game for research when it comes to reinforcement\nlearning (RL) as it is one of the few cooperative card games where you have\nincomplete knowledge of the entire environment, thus presenting a challenge for\na RL agent. We explored different tabular and deep reinforcement learning\nalgorithms to see which had the best performance both against an agent of the\nsame type and also against other types of agents. We establish that certain\nagents played their highest scoring games against specific agents while others\nexhibited higher scores on average by adapting to the opposing agent's\nbehavior. We attempted to quantify the conditions under which each algorithm\nprovides the best advantage and identified the most interesting interactions\nbetween agents of different types. In the end, we found that temporal\ndifference (TD) algorithms had better overall performance and balancing of play\ntypes compared to tabular agents. Specifically, tabular Expected SARSA and deep\nQ-Learning agents showed the best performance.", "published": "2025-05-31 08:24:16", "link": "http://arxiv.org/abs/2506.00458v1", "categories": ["cs.LG", "cs.AI", "cs.GT", "cs.MA"], "primary_category": "cs.LG"}
{"title": "Scalable Analysis and Design Using Automatic Differentiation", "abstract": "This article aims to demonstrate and discuss the applications of automatic\ndifferentiation (AD) for finding derivatives in PDE-constrained optimization\nproblems and Jacobians in non-linear finite element analysis. The main idea is\nto localize the application of AD at the integration point level by combining\nit with the so-called Finite Element Operator Decomposition. The proposed\nmethods are computationally effective, scalable, automatic, and non-intrusive,\nmaking them ideal for existing serial and parallel solvers and complex\nmultiphysics applications. The performance is demonstrated on large-scale\nsteady-state non-linear scalar problems. The chosen testbed, the MFEM library,\nis free and open-source finite element discretization library with proven\nscalability to thousands of parallel processes and state-of-the-art high-order\ndiscretization techniques.", "published": "2025-05-31 23:18:09", "link": "http://arxiv.org/abs/2506.00746v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Nonlinear Optimal Recovery in Hilbert Spaces", "abstract": "This paper investigates solution strategies for nonlinear problems in Hilbert\nspaces, such as nonlinear partial differential equations (PDEs) in Sobolev\nspaces, when only finite measurements are available. We formulate this as a\nnonlinear optimal recovery problem, establishing its well-posedness and proving\nits convergence to the true solution as the number of measurements increases.\nHowever, the resulting formulation might not have a finite-dimensional solution\nin general. We thus present a sufficient condition for the finite\ndimensionality of the solution, applicable to problems with well-defined point\nevaluation measurements. To address the broader setting, we introduce a relaxed\nnonlinear optimal recovery and provide a detailed convergence analysis. An\nillustrative example is given to demonstrate that our formulations and\ntheoretical findings offer a comprehensive framework for solving nonlinear\nproblems in infinite-dimensional spaces with limited data.", "published": "2025-05-31 20:19:07", "link": "http://arxiv.org/abs/2506.00704v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Stabilization of the Gradient Method for Solving Linear Algebraic Systems -- A Method Related to the Normal Equation", "abstract": "Although it is relatively easy to apply, the gradient method often displays a\ndisappointingly slow rate of convergence. Its convergence is specially based on\nthe structure of the matrix of the algebraic linear system, and on the choice\nof the stepsize defining the new iteration. We propose here a simple and robust\nstabilization of the gradient method, which no longer assumes a structure on\nthe matrix (neither symmetric, nor positive definite) to converge, and which no\nlonger requires an approach on the choice of the stepsize. We establish the\nglobal convergence of the proposed stabilized algorithm under the only\nassumption of nonsingular matrix. Several numerical examples illustrating its\nperformances are presented, where we have tested small and large scale linear\nsystems, with and not structured matrices, and with well and ill conditioned\nmatrices.", "published": "2025-05-31 20:17:00", "link": "http://arxiv.org/abs/2506.00702v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Reconstruction techniques for inverse Sturm-Liouville problems with complex coefficients", "abstract": "A variety of inverse Sturm-Liouville problems is considered, including the\ntwo-spectrum inverse problem, the problem of recovering the potential from the\nWeyl function, as well as the recovery from the spectral function. In all cases\nthe potential in the Sturm-Liouville equation is assumed to be complex valued.\nA unified approach for the approximate solution of the inverse Sturm-Liouville\nproblems is developed, based on Neumann series of Bessel functions (NSBF)\nrepresentations for solutions and their derivatives. Unlike most existing\napproaches, it allows one to recover not only the complex-valued potential but\nalso the boundary conditions of the Sturm-Liouville problem. Efficient accuracy\ncontrol is implemented. The numerical method is direct. It involves only\nsolving linear systems of algebraic equations for the coefficients of the NSBF\nrepresentations, while eventually the knowledge only of the first NSBF\ncoefficients leads to the recovery of the Sturm-Liouville problem. Numerical\nefficiency is illustrated by several test examples.", "published": "2025-05-31 18:49:55", "link": "http://arxiv.org/abs/2506.00670v1", "categories": ["math.CA", "cs.NA", "math-ph", "math.MP", "math.NA", "math.SP", "physics.comp-ph"], "primary_category": "math.CA"}
{"title": "Full- and low-rank exponential midpoint schemes for forward and adjoint Lindblad equations", "abstract": "The Lindblad equation is a widely used quantum master equation to model the\ndynamical evolution of open quantum systems whose states are described by\ndensity matrices. This equation is also a fundamental building block to design\noptimal control functions. In this paper we develop full- and low-rank\nexponential midpoint integrators for solving both the forward and adjoint\nLindblad equations. These schemes are applicable to optimize-then-discretize\napproaches for optimal control of open quantum systems. We show that the\nproposed schemes preserve positivity and trace unconditionally. Furthermore,\nconvergence of these numerical schemes is proved theoretically and verified\nnumerically.", "published": "2025-05-31 02:15:00", "link": "http://arxiv.org/abs/2506.00346v1", "categories": ["quant-ph", "cs.NA", "math.NA", "math.OC"], "primary_category": "quant-ph"}
{"title": "Structured Column Subset Selection for Bayesian Optimal Experimental Design", "abstract": "We consider optimal experimental design (OED) for Bayesian inverse problems,\nwhere the experimental design variables have a certain multiway structure.\nGiven $d$ different experimental variables with $m_i$ choices per design\nvariable $1 \\le i\\le d$, the goal is to select $k_i \\le m_i$ experiments per\ndesign variable. Previous work has related OED to the column subset selection\nproblem by mapping the design variables to the columns of a matrix\n$\\mathbf{A}$. However, this approach is applicable only to the case $d=1$ in\nwhich the columns can be selected independently. We develop an extension to the\ncase where the design variables have a multi-way structure. Our approach is to\nmap the matrix $\\mathbf{A}$ to a tensor and perform column subset selection on\nmode unfoldings of the tensor. We develop an algorithmic framework with three\ndifferent algorithmic templates, and randomized variants of these algorithms.\nWe analyze the computational cost of all the proposed algorithms and also\ndevelop greedy versions to facilitate comparisons. Numerical experiments on\nfour different applications -- time-dependent inverse problems, seismic\ntomography, X-ray tomography, and flow reconstruction -- demonstrate the\neffectiveness and scalability of our methods for structured experimental design\nin Bayesian inverse problems.", "published": "2025-05-31 01:29:50", "link": "http://arxiv.org/abs/2506.00336v1", "categories": ["math.NA", "cs.NA", "8F15 (Primary), 58F17, 53C35 (Secondary)"], "primary_category": "math.NA"}
{"title": "Drawdowns, Drawups, and Occupation Times under General Markov Models", "abstract": "Drawdown risk, an important metric in financial risk management, poses\nsignificant computational challenges due to its highly path-dependent nature.\nThis paper proposes a unified framework for computing five important drawdown\nquantities introduced in Landriault et al. (2015) and Zhang (2015) under\ngeneral Markov models. We first establish linear systems and develop efficient\nalgorithms for such problems under continuous-time Markov chains (CTMCs), and\nthen establish their theoretical convergence to target quantities under general\nMarkov models. Notably, the proposed algorithms for most quantities achieve the\nsame complexity order as those for path-independent problems: cubic in the\nnumber of CTMC states for general Markov models and linear when applied to\ndiffusion models. Rigorous convergence analysis is conducted under weak\nregularity conditions, and extensive numerical experiments validate the\naccuracy and efficiency of the proposed algorithms.", "published": "2025-05-31 13:15:53", "link": "http://arxiv.org/abs/2506.00552v1", "categories": ["q-fin.MF"], "primary_category": "q-fin.MF"}
{"title": "\"Who experiences large model decay and why?\" A Hierarchical Framework for Diagnosing Heterogeneous Performance Drift", "abstract": "Machine learning (ML) models frequently experience performance degradation\nwhen deployed in new contexts. Such degradation is rarely uniform: some\nsubgroups may suffer large performance decay while others may not.\nUnderstanding where and how large differences in performance arise is critical\nfor designing targeted corrective actions that mitigate decay for the most\naffected subgroups while minimizing any unintended effects. Current approaches\ndo not provide such detailed insight, as they either (i) explain how average\nperformance shifts arise or (ii) identify adversely affected subgroups without\ninsight into how this occurred. To this end, we introduce a Subgroup-scanning\nHierarchical Inference Framework for performance drifT (SHIFT). SHIFT first\nasks \"Is there any subgroup with unacceptably large performance decay due to\ncovariate/outcome shifts?\" (Where?) and, if so, dives deeper to ask \"Can we\nexplain this using more detailed variable(subset)-specific shifts?\" (How?). In\nreal-world experiments, we find that SHIFT identifies interpretable subgroups\naffected by performance decay, and suggests targeted actions that effectively\nmitigate the decay.", "published": "2025-05-31 23:50:54", "link": "http://arxiv.org/abs/2506.00756v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Learning with Calibration: Exploring Test-Time Computing of Spatio-Temporal Forecasting", "abstract": "Spatio-temporal forecasting is crucial in many domains, such as\ntransportation, meteorology, and energy. However, real-world scenarios\nfrequently present challenges such as signal anomalies, noise, and\ndistributional shifts. Existing solutions primarily enhance robustness by\nmodifying network architectures or training procedures. Nevertheless, these\napproaches are computationally intensive and resource-demanding, especially for\nlarge-scale applications. In this paper, we explore a novel test-time computing\nparadigm, namely learning with calibration, ST-TTC, for spatio-temporal\nforecasting. Through learning with calibration, we aim to capture periodic\nstructural biases arising from non-stationarity during the testing phase and\nperform real-time bias correction on predictions to improve accuracy.\nSpecifically, we first introduce a spectral-domain calibrator with\nphase-amplitude modulation to mitigate periodic shift and then propose a flash\nupdating mechanism with a streaming memory queue for efficient test-time\ncomputation. ST-TTC effectively bypasses complex training-stage techniques,\noffering an efficient and generalizable paradigm. Extensive experiments on\nreal-world datasets demonstrate the effectiveness, universality, flexibility\nand efficiency of our proposed method.", "published": "2025-05-31 16:48:27", "link": "http://arxiv.org/abs/2506.00635v1", "categories": ["cs.LG", "cs.AI", "cs.ET", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Slow Feature Analysis as Variational Inference Objective", "abstract": "This work presents a novel probabilistic interpretation of Slow Feature\nAnalysis (SFA) through the lens of variational inference. Unlike prior\nformulations that recover linear SFA from Gaussian state-space models with\nlinear emissions, this approach relaxes the key constraint of linearity. While\nit does not lead to full equivalence to non-linear SFA, it recasts the\nclassical slowness objective in a variational framework. Specifically, it\nallows the slowness objective to be interpreted as a regularizer to a\nreconstruction loss. Furthermore, we provide arguments, why -- from the\nperspective of slowness optimization -- the reconstruction loss takes on the\nrole of the constraints that ensure informativeness in SFA. We conclude with a\ndiscussion of potential new research directions.", "published": "2025-05-31 14:29:02", "link": "http://arxiv.org/abs/2506.00580v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Neural Estimation for Scaling Entropic Multimarginal Optimal Transport", "abstract": "Multimarginal optimal transport (MOT) is a powerful framework for modeling\ninteractions between multiple distributions, yet its applicability is\nbottlenecked by a high computational overhead. Entropic regularization provides\ncomputational speedups via the multimarginal Sinkhorn algorithm, whose time\ncomplexity, for a dataset size $n$ and $k$ marginals, generally scales as\n$O(n^k)$. However, this dependence on the dataset size $n$ is computationally\nprohibitive for many machine learning problems. In this work, we propose a new\ncomputational framework for entropic MOT, dubbed Neural Entropic MOT (NEMOT),\nthat enjoys significantly improved scalability. NEMOT employs neural networks\ntrained using mini-batches, which transfers the computational complexity from\nthe dataset size to the size of the mini-batch, leading to substantial gains.\nWe provide formal guarantees on the accuracy of NEMOT via non-asymptotic error\nbounds. We supplement these with numerical results that demonstrate the\nperformance gains of NEMOT over Sinkhorn's algorithm, as well as extensions to\nneural computation of multimarginal entropic Gromov-Wasserstein alignment. In\nparticular, orders-of-magnitude speedups are observed relative to the\nstate-of-the-art, with a notable increase in the feasible number of samples and\nmarginals. NEMOT seamlessly integrates as a module in large-scale machine\nlearning pipelines, and can serve to expand the practical applicability of\nentropic MOT for tasks involving multimarginal data.", "published": "2025-05-31 14:10:27", "link": "http://arxiv.org/abs/2506.00573v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Score Matching With Missing Data", "abstract": "Score matching is a vital tool for learning the distribution of data with\napplications across many areas including diffusion processes, energy based\nmodelling, and graphical model estimation. Despite all these applications,\nlittle work explores its use when data is incomplete. We address this by\nadapting score matching (and its major extensions) to work with missing data in\na flexible setting where data can be partially missing over any subset of the\ncoordinates. We provide two separate score matching variations for general use,\nan importance weighting (IW) approach, and a variational approach. We provide\nfinite sample bounds for our IW approach in finite domain settings and show it\nto have especially strong performance in small sample lower dimensional cases.\nComplementing this, we show our variational approach to be strongest in more\ncomplex high-dimensional settings which we demonstrate on graphical model\nestimation tasks on both real and simulated data.", "published": "2025-05-31 13:26:51", "link": "http://arxiv.org/abs/2506.00557v1", "categories": ["stat.ML", "cs.LG", "stat.ME"], "primary_category": "stat.ML"}
{"title": "Federated learning framework for collaborative remaining useful life prognostics: an aircraft engine case study", "abstract": "Complex systems such as aircraft engines are continuously monitored by\nsensors. In predictive aircraft maintenance, the collected sensor measurements\nare used to estimate the health condition and the Remaining Useful Life (RUL)\nof such systems. However, a major challenge when developing prognostics is the\nlimited number of run-to-failure data samples. This challenge could be overcome\nif multiple airlines would share their run-to-failure data samples such that\nsufficient learning can be achieved. Due to privacy concerns, however, airlines\nare reluctant to share their data in a centralized setting. In this paper, a\ncollaborative federated learning framework is therefore developed instead.\nHere, several airlines cooperate to train a collective RUL prognostic machine\nlearning model, without the need to centrally share their data. For this, a\ndecentralized validation procedure is proposed to validate the prognostics\nmodel without sharing any data. Moreover, sensor data is often noisy and of low\nquality. This paper therefore proposes four novel methods to aggregate the\nparameters of the global prognostic model. These methods enhance the robustness\nof the FL framework against noisy data. The proposed framework is illustrated\nfor training a collaborative RUL prognostic model for aircraft engines, using\nthe N-CMAPSS dataset. Here, six airlines are considered, that collaborate in\nthe FL framework to train a collective RUL prognostic model for their\naircraft's engines. When comparing the proposed FL framework with the case\nwhere each airline independently develops their own prognostic model, the\nresults show that FL leads to more accurate RUL prognostics for five out of the\nsix airlines. Moreover, the novel robust aggregation methods render the FL\nframework robust to noisy data samples.", "published": "2025-05-31 10:32:51", "link": "http://arxiv.org/abs/2506.00499v1", "categories": ["cs.LG", "cs.DC", "cs.ET", "cs.SY", "eess.SY", "stat.ML"], "primary_category": "cs.LG"}
{"title": "FLoE: Fisher-Based Layer Selection for Efficient Sparse Adaptation of Low-Rank Experts", "abstract": "Parameter-Efficient Fine-Tuning (PEFT) methods have emerged as a widely\nadopted strategy for adapting pre-trained Large Language Models (LLMs) to\ndownstream tasks, significantly reducing memory and computational costs.\nHowever, most existing PEFT techniques uniformly deploy LoRA adapters across\nall layers, disregarding the intrinsic heterogeneity of layer contributions and\ntask-specific rank requirements. This uniform paradigm leads to redundant\nparameter allocation and suboptimal adaptation efficiency. To address these\nlimitations, we propose FLoE, a novel PEFT framework that introduces two key\ninnovations: (i) a Fisher information-guided importance scoring mechanism to\ndynamically identify task-critical transformer layers for MoE-based low-rank\nadaptation, enabling sparse adapter deployment; and (ii) a Bayesian\noptimization-driven rank allocator that automatically determines optimal LoRA\nranks on specific datasets without exhaustive grid search. Extensive\nexperiments across diverse LLMs and benchmarks reveal that FLoE achieves\nimpressive efficiency-accuracy trade-offs, making FLoE particularly\nadvantageous in resource-constrained environments that necessitate rapid\nadaptation.", "published": "2025-05-31 10:27:08", "link": "http://arxiv.org/abs/2506.00495v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "It Takes a Good Model to Train a Good Model: Generalized Gaussian Priors for Optimized LLMs", "abstract": "Despite rapid advancements in the research and deployment of large language\nmodels (LLMs), the statistical distribution of model parameters, as well as\ntheir influence on initialization, training dynamics, and downstream\nefficiency, has received surprisingly little attention. A recent work\nintroduced BackSlash, a training-time compression algorithm. It first\ndemonstrated that pre-trained LLM parameters follow generalized Gaussian\ndistributions (GGDs) better. By optimizing GG priors during training, BackSlash\ncan reduce parameters by up to 90\\% with minimal performance loss. Building on\nthis foundational insight, we propose a unified, end-to-end framework for LLM\noptimization based on the GG model. Our contributions are threefold: (1)\nGG-based initialization scheme that aligns with the statistical structure of\ntrained models, resulting in faster convergence and improved accuracy; (2)\nDeepShape, a post-training regularization method that reshapes weight\ndistributions to match a GG profile, improving compressibility with minimized\ndegradation in performance; and (3) RF8, a compact and hardware-efficient 8-bit\nfloating-point format designed for GG-distributed-initialized BackSlash\ntraining, enabling low-cost inference without compromising accuracy.\nExperiments across diverse model architectures show that our framework\nconsistently yields smaller and faster models that match or outperform standard\ntraining baselines. By grounding LLM development in principled statistical\nmodeling, this work forges a new path toward efficient, scalable, and\nhardware-aware AI systems. The code is available on our project page:\nhttps://huggingface.co/spaces/shifeng3711/gg_prior.", "published": "2025-05-31 09:49:17", "link": "http://arxiv.org/abs/2506.00486v2", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Flashbacks to Harmonize Stability and Plasticity in Continual Learning", "abstract": "We introduce Flashback Learning (FL), a novel method designed to harmonize\nthe stability and plasticity of models in Continual Learning (CL). Unlike prior\napproaches that primarily focus on regularizing model updates to preserve old\ninformation while learning new concepts, FL explicitly balances this trade-off\nthrough a bidirectional form of regularization. This approach effectively\nguides the model to swiftly incorporate new knowledge while actively retaining\nits old knowledge. FL operates through a two-phase training process and can be\nseamlessly integrated into various CL methods, including replay, parameter\nregularization, distillation, and dynamic architecture techniques. In designing\nFL, we use two distinct knowledge bases: one to enhance plasticity and another\nto improve stability. FL ensures a more balanced model by utilizing both\nknowledge bases to regularize model updates. Theoretically, we analyze how the\nFL mechanism enhances the stability-plasticity balance. Empirically, FL\ndemonstrates tangible improvements over baseline methods within the same\ntraining budget. By integrating FL into at least one representative baseline\nfrom each CL category, we observed an average accuracy improvement of up to\n4.91% in Class-Incremental and 3.51% in Task-Incremental settings on standard\nimage classification benchmarks. Additionally, measurements of the\nstability-to-plasticity ratio confirm that FL effectively enhances this\nbalance. FL also outperforms state-of-the-art CL methods on more challenging\ndatasets like ImageNet.", "published": "2025-05-31 09:04:58", "link": "http://arxiv.org/abs/2506.00477v1", "categories": ["cs.LG", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Attention-Aided MMSE for OFDM Channel Estimation: Learning Linear Filters with Attention", "abstract": "In orthogonal frequency division multiplexing (OFDM), accurate channel\nestimation is crucial. Classical signal processing based approaches, such as\nminimum mean-squared error (MMSE) estimation, often require second-order\nstatistics that are difficult to obtain in practice. Recent deep neural\nnetworks based methods have been introduced to address this; yet they often\nsuffer from high complexity. This paper proposes an Attention-aided MMSE\n(A-MMSE), a novel model-based DNN framework that learns the optimal MMSE filter\nvia the Attention Transformer. Once trained, the A-MMSE estimates the channel\nthrough a single linear operation for channel estimation, eliminating nonlinear\nactivations during inference and thus reducing computational complexity. To\nenhance the learning efficiency of the A-MMSE, we develop a two-stage Attention\nencoder, designed to effectively capture the channel correlation structure.\nAdditionally, a rank-adaptive extension of the proposed A-MMSE allows flexible\ntrade-offs between complexity and channel estimation accuracy. Extensive\nsimulations with 3GPP TDL channel models demonstrate that the proposed A-MMSE\nconsistently outperforms other baseline methods in terms of normalized MSE\nacross a wide range of SNR conditions. In particular, the A-MMSE and its\nrank-adaptive extension establish a new frontier in the performance complexity\ntrade-off, redefining the standard for practical channel estimation methods.", "published": "2025-05-31 08:12:04", "link": "http://arxiv.org/abs/2506.00452v1", "categories": ["eess.SP", "cs.AI", "stat.ML"], "primary_category": "eess.SP"}
{"title": "Off-Policy Evaluation of Ranking Policies via Embedding-Space User Behavior Modeling", "abstract": "Off-policy evaluation (OPE) in ranking settings with large ranking action\nspaces, which stems from an increase in both the number of unique actions and\nlength of the ranking, is essential for assessing new recommender policies\nusing only logged bandit data from previous versions. To address the high\nvariance issues associated with existing estimators, we introduce two new\nassumptions: no direct effect on rankings and user behavior model on ranking\nembedding spaces. We then propose the generalized marginalized inverse\npropensity score (GMIPS) estimator with statistically desirable properties\ncompared to existing ones. Finally, we demonstrate that the GMIPS achieves the\nlowest MSE. Notably, among GMIPS variants, the marginalized reward interaction\nIPS (MRIPS) incorporates a doubly marginalized importance weight based on a\ncascade behavior assumption on ranking embeddings. MRIPS effectively balances\nthe trade-off between bias and variance, even as the ranking action spaces\nincrease and the above assumptions may not hold, as evidenced by our\nexperiments.", "published": "2025-05-31 07:58:53", "link": "http://arxiv.org/abs/2506.00446v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Learning from Double Positive and Unlabeled Data for Potential-Customer Identification", "abstract": "In this study, we propose a method for identifying potential customers in\ntargeted marketing by applying learning from positive and unlabeled data (PU\nlearning). We consider a scenario in which a company sells a product and can\nobserve only the customers who purchased it. Decision-makers seek to market\nproducts effectively based on whether people have loyalty to the company.\nIndividuals with loyalty are those who are likely to remain interested in the\ncompany even without additional advertising. Consequently, those loyal\ncustomers would likely purchase from the company if they are interested in the\nproduct. In contrast, people with lower loyalty may overlook the product or buy\nsimilar products from other companies unless they receive marketing attention.\nTherefore, by focusing marketing efforts on individuals who are interested in\nthe product but do not have strong loyalty, we can achieve more efficient\nmarketing. To achieve this goal, we consider how to learn, from limited data, a\nclassifier that identifies potential customers who (i) have interest in the\nproduct and (ii) do not have loyalty to the company. Although our algorithm\ncomprises a single-stage optimization, its objective function implicitly\ncontains two losses derived from standard PU learning settings. For this\nreason, we refer to our approach as double PU learning. We verify the validity\nof the proposed algorithm through numerical experiments, confirming that it\nfunctions appropriately for the problem at hand.", "published": "2025-05-31 07:33:48", "link": "http://arxiv.org/abs/2506.00436v1", "categories": ["cs.LG", "cs.AI", "econ.EM", "stat.ME", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Channel Normalization for Time Series Channel Identification", "abstract": "Channel identifiability (CID) refers to the ability to distinguish between\nindividual channels in time series (TS) modeling. The absence of CID often\nresults in producing identical outputs for identical inputs, disregarding\nchannel-specific characteristics. In this paper, we highlight the importance of\nCID and propose Channel Normalization (CN), a simple yet effective\nnormalization strategy that enhances CID by assigning distinct affine\ntransformation parameters to each channel. We further extend CN in two ways: 1)\nAdaptive CN (ACN) dynamically adjusts parameters based on the input TS,\nimproving adaptability in TS models, and 2) Prototypical CN (PCN) introduces a\nset of learnable prototypes instead of per-channel parameters, enabling\napplicability to datasets with unknown or varying number of channels and\nfacilitating use in TS foundation models. We demonstrate the effectiveness of\nCN and its variants by applying them to various TS models, achieving\nsignificant performance gains for both non-CID and CID models. In addition, we\nanalyze the success of our approach from an information theory perspective.\nCode is available at https://github.com/seunghan96/CN.", "published": "2025-05-31 07:24:24", "link": "http://arxiv.org/abs/2506.00432v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Blockchain-Enabled Privacy-Preserving Second-Order Federated Edge Learning in Personalized Healthcare", "abstract": "Federated learning (FL) has attracted increasing attention to mitigate\nsecurity and privacy challenges in traditional cloud-centric machine learning\nmodels specifically in healthcare ecosystems. FL methodologies enable the\ntraining of global models through localized policies, allowing independent\noperations at the edge clients' level. Conventional first-order FL approaches\nface several challenges in personalized model training due to heterogeneous\nnon-independent and identically distributed (non-iid) data of each edge client.\nRecently, second-order FL approaches maintain the stability and consistency of\nnon-iid datasets while improving personalized model training. This study\nproposes and develops a verifiable and auditable optimized second-order FL\nframework BFEL (blockchain-enhanced federated edge learning) based on optimized\nFedCurv for personalized healthcare systems. FedCurv incorporates information\nabout the importance of each parameter to each client's task (through Fisher\nInformation Matrix) which helps to preserve client-specific knowledge and\nreduce model drift during aggregation. Moreover, it minimizes communication\nrounds required to achieve a target precision convergence for each edge client\nwhile effectively managing personalized training on non-iid and heterogeneous\ndata. The incorporation of Ethereum-based model aggregation ensures trust,\nverifiability, and auditability while public key encryption enhances privacy\nand security. Experimental results of federated CNNs and MLPs utilizing Mnist,\nCifar-10, and PathMnist demonstrate the high efficiency and scalability of the\nproposed framework.", "published": "2025-05-31 06:41:04", "link": "http://arxiv.org/abs/2506.00416v1", "categories": ["cs.LG", "cs.CR", "stat.ML"], "primary_category": "cs.LG"}
{"title": "JojoSCL: Shrinkage Contrastive Learning for single-cell RNA sequence Clustering", "abstract": "Single-cell RNA sequencing (scRNA-seq) has revolutionized our understanding\nof cellular processes by enabling gene expression analysis at the individual\ncell level. Clustering allows for the identification of cell types and the\nfurther discovery of intrinsic patterns in single-cell data. However, the high\ndimensionality and sparsity of scRNA-seq data continue to challenge existing\nclustering models. In this paper, we introduce JojoSCL, a novel self-supervised\ncontrastive learning framework for scRNA-seq clustering. By incorporating a\nshrinkage estimator based on hierarchical Bayesian estimation, which adjusts\ngene expression estimates towards more reliable cluster centroids to reduce\nintra-cluster dispersion, and optimized using Stein's Unbiased Risk Estimate\n(SURE), JojoSCL refines both instance-level and cluster-level contrastive\nlearning. Experiments on ten scRNA-seq datasets substantiate that JojoSCL\nconsistently outperforms prevalent clustering methods, with further validation\nof its practicality through robustness analysis and ablation studies. JojoSCL's\ncode is available at: https://github.com/ziwenwang28/JojoSCL.", "published": "2025-05-31 05:59:56", "link": "http://arxiv.org/abs/2506.00410v1", "categories": ["cs.LG", "q-bio.GN", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Bias as a Virtue: Rethinking Generalization under Distribution Shifts", "abstract": "Machine learning models often degrade when deployed on data distributions\ndifferent from their training data. Challenging conventional validation\nparadigms, we demonstrate that higher in-distribution (ID) bias can lead to\nbetter out-of-distribution (OOD) generalization. Our Adaptive Distribution\nBridge (ADB) framework implements this insight by introducing controlled\nstatistical diversity during training, enabling models to develop bias profiles\nthat effectively generalize across distributions. Empirically, we observe a\nrobust negative correlation where higher ID bias corresponds to lower OOD\nerror--a finding that contradicts standard practices focused on minimizing\nvalidation error. Evaluation on multiple datasets shows our approach\nsignificantly improves OOD generalization. ADB achieves robust mean error\nreductions of up to 26.8% compared to traditional cross-validation, and\nconsistently identifies high-performing training strategies, evidenced by\npercentile ranks often exceeding 74.4%. Our work provides both a practical\nmethod for improving generalization and a theoretical framework for\nreconsidering the role of bias in robust machine learning.", "published": "2025-05-31 05:54:49", "link": "http://arxiv.org/abs/2506.00407v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Label-shift robust federated feature screening for high-dimensional classification", "abstract": "Distributed and federated learning are important tools for high-dimensional\nclassification of large datasets. To reduce computational costs and overcome\nthe curse of dimensionality, feature screening plays a pivotal role in\neliminating irrelevant features during data preprocessing. However, data\nheterogeneity, particularly label shifting across different clients, presents\nsignificant challenges for feature screening. This paper introduces a general\nframework that unifies existing screening methods and proposes a novel utility,\nlabel-shift robust federated feature screening (LR-FFS), along with its\nfederated estimation procedure. The framework facilitates a uniform analysis of\nmethods and systematically characterizes their behaviors under label shift\nconditions. Building upon this framework, LR-FFS leverages conditional\ndistribution functions and expectations to address label shift without adding\ncomputational burdens and remains robust against model misspecification and\noutliers. Additionally, the federated procedure ensures computational\nefficiency and privacy protection while maintaining screening effectiveness\ncomparable to centralized processing. We also provide a false discovery rate\n(FDR) control method for federated feature screening. Experimental results and\ntheoretical analyses demonstrate LR-FFS's superior performance across diverse\nclient environments, including those with varying class distributions, sample\nsizes, and missing categorical data.", "published": "2025-05-31 04:14:49", "link": "http://arxiv.org/abs/2506.00379v1", "categories": ["stat.ML", "cs.LG", "stat.ME"], "primary_category": "stat.ML"}
{"title": "Beyond Winning: Margin of Victory Relative to Expectation Unlocks Accurate Skill Ratings", "abstract": "Knowledge of accurate relative skills in any competitive system is essential,\nbut foundational approaches such as ELO discard extremely relevant performance\ndata by concentrating exclusively on binary outcomes. While margin of victory\n(MOV) extensions exist, they often lack a definitive method for incorporating\nthis information. We introduce Margin of Victory Differential Analysis (MOVDA),\na framework that enhances traditional rating systems by using the deviation\nbetween the true MOV and a $\\textit{modeled expectation}$. MOVDA learns a\ndomain-specific, non-linear function (a scaled hyperbolic tangent that captures\nsaturation effects and home advantage) to predict expected MOV based on rating\ndifferentials. Crucially, the $\\textit{difference}$ between the true and\nexpected MOV provides a subtle and weighted signal for rating updates,\nhighlighting informative deviations in all levels of contests. Extensive\nexperiments on professional NBA basketball data (from 2013 to 2023, with 13,619\ngames) show that MOVDA significantly outperforms standard ELO and Bayesian\nbaselines. MOVDA reduces Brier score prediction error by $1.54\\%$ compared to\nTrueSkill, increases outcome accuracy by $0.58\\%$, and most importantly\naccelerates rating convergence by $13.5\\%$, while maintaining the computational\nefficiency of the original ELO updates. MOVDA offers a theoretically motivated,\nempirically superior, and computationally lean approach to integrating\nperformance magnitude into skill rating for competitive environments like the\nNBA.", "published": "2025-05-31 02:16:51", "link": "http://arxiv.org/abs/2506.00348v1", "categories": ["stat.ML", "cs.AI", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Length Aware Speech Translation for Video Dubbing", "abstract": "In video dubbing, aligning translated audio with the source audio is a\nsignificant challenge. Our focus is on achieving this efficiently, tailored for\nreal-time, on-device video dubbing scenarios. We developed a phoneme-based\nend-to-end length-sensitive speech translation (LSST) model, which generates\ntranslations of varying lengths short, normal, and long using predefined tags.\nAdditionally, we introduced length-aware beam search (LABS), an efficient\napproach to generate translations of different lengths in a single decoding\npass. This approach maintained comparable BLEU scores compared to a baseline\nwithout length awareness while significantly enhancing synchronization quality\nbetween source and target audio, achieving a mean opinion score (MOS) gain of\n0.34 for Spanish and 0.65 for Korean, respectively.", "published": "2025-05-31 23:01:50", "link": "http://arxiv.org/abs/2506.00740v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "IMPACT: Iterative Mask-based Parallel Decoding for Text-to-Audio Generation with Diffusion Modeling", "abstract": "Text-to-audio generation synthesizes realistic sounds or music given a\nnatural language prompt. Diffusion-based frameworks, including the Tango and\nthe AudioLDM series, represent the state-of-the-art in text-to-audio\ngeneration. Despite achieving high audio fidelity, they incur significant\ninference latency due to the slow diffusion sampling process. MAGNET, a\nmask-based model operating on discrete tokens, addresses slow inference through\niterative mask-based parallel decoding. However, its audio quality still lags\nbehind that of diffusion-based models. In this work, we introduce IMPACT, a\ntext-to-audio generation framework that achieves high performance in audio\nquality and fidelity while ensuring fast inference. IMPACT utilizes iterative\nmask-based parallel decoding in a continuous latent space powered by diffusion\nmodeling. This approach eliminates the fidelity constraints of discrete tokens\nwhile maintaining competitive inference speed. Results on AudioCaps demonstrate\nthat IMPACT achieves state-of-the-art performance on key metrics including\nFr\\'echet Distance (FD) and Fr\\'echet Audio Distance (FAD) while significantly\nreducing latency compared to prior models. The project website is available at\nhttps://audio-impact.github.io/.", "published": "2025-05-31 22:51:36", "link": "http://arxiv.org/abs/2506.00736v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Quantifying and Reducing Speaker Heterogeneity within the Common Voice Corpus for Phonetic Analysis", "abstract": "With its crosslinguistic and cross-speaker diversity, the Mozilla Common\nVoice Corpus (CV) has been a valuable resource for multilingual speech\ntechnology and holds tremendous potential for research in crosslinguistic\nphonetics and speech sciences. Properly accounting for speaker variation is,\nhowever, key to the theoretical and statistical bases of speech research. While\nCV provides a client ID as an approximation to a speaker ID, multiple speakers\ncan contribute under the same ID. This study aims to quantify and reduce\nheterogeneity in the client ID for a better approximation of a true, though\nstill anonymous speaker ID. Using ResNet-based voice embeddings, we obtained a\nsimilarity score among recordings with the same client ID, then implemented a\nspeaker discrimination task to identify an optimal threshold for reducing\nperceived speaker heterogeneity. These results have major downstream\napplications for phonetic analysis and the development of speaker-based speech\ntechnology.", "published": "2025-05-31 22:39:48", "link": "http://arxiv.org/abs/2506.00733v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Chain-of-Thought Training for Open E2E Spoken Dialogue Systems", "abstract": "Unlike traditional cascaded pipelines, end-to-end (E2E) spoken dialogue\nsystems preserve full differentiability and capture non-phonemic information,\nmaking them well-suited for modeling spoken interactions. However, existing E2E\napproaches often require large-scale training data and generates responses\nlacking semantic coherence. We propose a simple yet effective strategy\nleveraging a chain-of-thought (CoT) formulation, ensuring that training on\nconversational data remains closely aligned with the multimodal language model\n(LM)'s pre-training on speech recognition~(ASR), text-to-speech synthesis\n(TTS), and text LM tasks. Our method achieves over 1.5 ROUGE-1 improvement over\nthe baseline, successfully training spoken dialogue systems on publicly\navailable human-human conversation datasets, while being compute-efficient\nenough to train on just 300 hours of public human-human conversation data, such\nas the Switchboard. We will publicly release our models and training code.", "published": "2025-05-31 21:43:37", "link": "http://arxiv.org/abs/2506.00722v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Learning to Upsample and Upmix Audio in the Latent Domain", "abstract": "Neural audio autoencoders create compact latent representations that preserve\nperceptually important information, serving as the foundation for both modern\naudio compression systems and generation approaches like next-token prediction\nand latent diffusion. Despite their prevalence, most audio processing\noperations, such as spatial and spectral up-sampling, still inefficiently\noperate on raw waveforms or spectral representations rather than directly on\nthese compressed representations. We propose a framework that performs audio\nprocessing operations entirely within an autoencoder's latent space,\neliminating the need to decode to raw audio formats. Our approach dramatically\nsimplifies training by operating solely in the latent domain, with a latent L1\nreconstruction term, augmented by a single latent adversarial discriminator.\nThis contrasts sharply with raw-audio methods that typically require complex\ncombinations of multi-scale losses and discriminators. Through experiments in\nbandwidth extension and mono-to-stereo up-mixing, we demonstrate computational\nefficiency gains of up to 100x while maintaining quality comparable to\npost-processing on raw audio. This work establishes a more efficient paradigm\nfor audio processing pipelines that already incorporate autoencoders, enabling\nsignificantly faster and more resource-efficient workflows across various audio\ntasks.", "published": "2025-05-31 19:27:22", "link": "http://arxiv.org/abs/2506.00681v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "LID Models are Actually Accent Classifiers: Implications and Solutions for LID on Accented Speech", "abstract": "Prior research indicates that LID model performance significantly declines on\naccented speech; however, the specific causes, extent, and characterization of\nthese errors remain under-explored. (i) We identify a common failure mode on\naccented speech whereby LID systems often misclassify L2 accented speech as the\nspeaker's native language or a related language. (ii) We present evidence\nsuggesting that state-of-the-art models are invariant to permutations of short\nspans of speech, implying they classify on the basis of short phonotactic\nfeatures indicative of accent rather than language. Our analysis reveals a\nsimple method to enhance model robustness to accents through input chunking.\n(iii) We present an approach that integrates sequence-level information into\nour model without relying on monolingual ASR systems; this reduces\naccent-language confusion and significantly enhances performance on accented\nspeech while maintaining comparable results on standard LID.", "published": "2025-05-31 16:35:40", "link": "http://arxiv.org/abs/2506.00628v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Quality Assessment of Noisy and Enhanced Speech with Limited Data: UWB-NTIS System for VoiceMOS 2024 and Beyond", "abstract": "In this preprint, we present the UWB-NTIS-TTS team's submission to Track 3 of\nthe VoiceMOS 2024 Challenge, the goal of which was to automatically assess the\nspeech quality of noisy and de-noised speech in terms of the ITU-T P.835\nmetrics of \"SIG\", \"BAK\", and \"OVRL\". Our proposed system, based on wav2vec 2.0,\nplaced among the top systems in the challenge, achieving the best prediction of\nthe BAK scores (background noise intrusiveness), the second-best prediction of\nthe OVRL score (overall audio quality), and the third-best prediction of SIG\n(speech signal quality) out of the five participating systems. We describe our\napproach, such as the two-stage fine-tuning process we used to contend with the\nchallenge's very limiting restrictions on allowable training data, and present\nthe results achieved both on the VoiceMOS 2024 Challenge data and on the\nrecently released CHiME 7 - UDASE dataset.", "published": "2025-05-31 11:00:15", "link": "http://arxiv.org/abs/2506.00506v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "M3ANet: Multi-scale and Multi-Modal Alignment Network for Brain-Assisted Target Speaker Extraction", "abstract": "The brain-assisted target speaker extraction (TSE) aims to extract the\nattended speech from mixed speech by utilizing the brain neural activities, for\nexample Electroencephalography (EEG). However, existing models overlook the\nissue of temporal misalignment between speech and EEG modalities, which hampers\nTSE performance. In addition, the speech encoder in current models typically\nuses basic temporal operations (e.g., one-dimensional convolution), which are\nunable to effectively extract target speaker information. To address these\nissues, this paper proposes a multi-scale and multi-modal alignment network\n(M3ANet) for brain-assisted TSE. Specifically, to eliminate the temporal\ninconsistency between EEG and speech modalities, the modal alignment module\nthat uses a contrastive learning strategy is applied to align the temporal\nfeatures of both modalities. Additionally, to fully extract speech information,\nmulti-scale convolutions with GroupMamba modules are used as the speech\nencoder, which scans speech features at each scale from different directions,\nenabling the model to capture deep sequence information. Experimental results\non three publicly available datasets show that the proposed model outperforms\ncurrent state-of-the-art methods across various evaluation metrics,\nhighlighting the effectiveness of our proposed method. The source code is\navailable at: https://github.com/fchest/M3ANet.", "published": "2025-05-31 08:33:57", "link": "http://arxiv.org/abs/2506.00466v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "XMAD-Bench: Cross-Domain Multilingual Audio Deepfake Benchmark", "abstract": "Recent advances in audio generation led to an increasing number of deepfakes,\nmaking the general public more vulnerable to financial scams, identity theft,\nand misinformation. Audio deepfake detectors promise to alleviate this issue,\nwith many recent studies reporting accuracy rates close to 99%. However, these\nmethods are typically tested in an in-domain setup, where the deepfake samples\nfrom the training and test sets are produced by the same generative models. To\nthis end, we introduce XMAD-Bench, a large-scale cross-domain multilingual\naudio deepfake benchmark comprising 668.8 hours of real and deepfake speech. In\nour novel dataset, the speakers, the generative methods, and the real audio\nsources are distinct across training and test splits. This leads to a\nchallenging cross-domain evaluation setup, where audio deepfake detectors can\nbe tested ``in the wild''. Our in-domain and cross-domain experiments indicate\na clear disparity between the in-domain performance of deepfake detectors,\nwhich is usually as high as 100%, and the cross-domain performance of the same\nmodels, which is sometimes similar to random chance. Our benchmark highlights\nthe need for the development of robust audio deepfake detectors, which maintain\ntheir generalization capacity across different languages, speakers, generative\nmethods, and data sources. Our benchmark is publicly released at\nhttps://github.com/ristea/xmad-bench/.", "published": "2025-05-31 08:28:36", "link": "http://arxiv.org/abs/2506.00462v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Towards Temporally Explainable Dysarthric Speech Clarity Assessment", "abstract": "Dysarthria, a motor speech disorder, affects intelligibility and requires\ntargeted interventions for effective communication. In this work, we\ninvestigate automated mispronunciation feedback by collecting a dysarthric\nspeech dataset from six speakers reading two passages, annotated by a speech\ntherapist with temporal markers and mispronunciation descriptions. We design a\nthree-stage framework for explainable mispronunciation evaluation: (1) overall\nclarity scoring, (2) mispronunciation localization, and (3) mispronunciation\ntype classification. We systematically analyze pretrained Automatic Speech\nRecognition (ASR) models in each stage, assessing their effectiveness in\ndysarthric speech evaluation (Code available at:\nhttps://github.com/augmented-human-lab/interspeech25_speechtherapy,\nSupplementary webpage: https://apps.ahlab.org/interspeech25_speechtherapy/).\nOur findings offer clinically relevant insights for automating actionable\nfeedback for pronunciation assessment, which could enable independent practice\nfor patients and help therapists deliver more effective interventions.", "published": "2025-05-31 08:16:54", "link": "http://arxiv.org/abs/2506.00454v1", "categories": ["eess.AS", "cs.HC", "cs.SD"], "primary_category": "eess.AS"}
{"title": "No Audiogram: Leveraging Existing Scores for Personalized Speech Intelligibility Prediction", "abstract": "Personalized speech intelligibility prediction is challenging. Previous\napproaches have mainly relied on audiograms, which are inherently limited in\naccuracy as they only capture a listener's hearing threshold for pure tones.\nRather than incorporating additional listener features, we propose a novel\napproach that leverages an individual's existing intelligibility data to\npredict their performance on new audio. We introduce the Support Sample-Based\nIntelligibility Prediction Network (SSIPNet), a deep learning model that\nleverages speech foundation models to build a high-dimensional representation\nof a listener's speech recognition ability from multiple support (audio, score)\npairs, enabling accurate predictions for unseen audio. Results on the Clarity\nPrediction Challenge dataset show that, even with a small number of support\n(audio, score) pairs, our method outperforms audiogram-based predictions. Our\nwork presents a new paradigm for personalized speech intelligibility\nprediction.", "published": "2025-05-31 07:55:03", "link": "http://arxiv.org/abs/2506.02039v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DYNAC: Dynamic Vocabulary based Non-Autoregressive Contextualization for Speech Recognition", "abstract": "Contextual biasing (CB) improves automatic speech recognition for rare and\nunseen phrases. Recent studies have introduced dynamic vocabulary, which\nrepresents context phrases as expandable tokens in autoregressive (AR) models.\nThis method improves CB accuracy but with slow inference speed. While dynamic\nvocabulary can be applied to non-autoregressive (NAR) models, such as\nconnectionist temporal classification (CTC), the conditional independence\nassumption fails to capture dependencies between static and dynamic tokens.\nThis paper proposes DYNAC (Dynamic Vocabulary-based NAR Contextualization), a\nself-conditioned CTC method that integrates dynamic vocabulary into\nintermediate layers. Conditioning the encoder on dynamic vocabulary, DYNAC\neffectively captures dependencies between static and dynamic tokens while\nreducing the real-time factor (RTF). Experimental results show that DYNAC\nreduces RTF by 81% with a 0.1-point degradation in word error rate on the\nLibriSpeech 960 test-clean set.", "published": "2025-05-31 06:53:25", "link": "http://arxiv.org/abs/2506.00422v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Causal Structure Discovery for Error Diagnostics of Children's ASR", "abstract": "Children's automatic speech recognition (ASR) often underperforms compared to\nthat of adults due to a confluence of interdependent factors: physiological\n(e.g., smaller vocal tracts), cognitive (e.g., underdeveloped pronunciation),\nand extrinsic (e.g., vocabulary limitations, background noise). Existing\nanalysis methods examine the impact of these factors in isolation, neglecting\ninterdependencies-such as age affecting ASR accuracy both directly and\nindirectly via pronunciation skills. In this paper, we introduce a causal\nstructure discovery to unravel these interdependent relationships among\nphysiology, cognition, extrinsic factors, and ASR errors. Then, we employ\ncausal quantification to measure each factor's impact on children's ASR. We\nextend the analysis to fine-tuned models to identify which factors are\nmitigated by fine-tuning and which remain largely unaffected. Experiments on\nWhisper and Wav2Vec2.0 demonstrate the generalizability of our findings across\ndifferent ASR systems.", "published": "2025-05-31 05:44:43", "link": "http://arxiv.org/abs/2506.00402v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "MagiCodec: Simple Masked Gaussian-Injected Codec for High-Fidelity Reconstruction and Generation", "abstract": "Neural audio codecs have made significant strides in efficiently mapping raw\naudio waveforms into discrete token representations, which are foundational for\ncontemporary audio generative models. However, most existing codecs are\noptimized primarily for reconstruction quality, often at the expense of the\ndownstream modelability of the encoded tokens. Motivated by the need to\novercome this bottleneck, we introduce $\\textbf{MagiCodec}$, a novel\nsingle-layer, streaming Transformer-based audio codec. MagiCodec is designed\nwith a multistage training pipeline that incorporates Gaussian noise injection\nand latent regularization, explicitly targeting the enhancement of semantic\nexpressiveness in the generated codes while preserving high reconstruction\nfidelity. We analytically derive the effect of noise injection in the frequency\ndomain, demonstrating its efficacy in attenuating high-frequency components and\nfostering robust tokenization. Extensive experimental evaluations show that\nMagiCodec surpasses state-of-the-art codecs in both reconstruction quality and\ndownstream tasks. Notably, the tokens produced by MagiCodec exhibit Zipf-like\ndistributions, as observed in natural languages, thereby improving\ncompatibility with language-model-based generative architectures. The code and\npre-trained models are available at https://github.com/Ereboas/MagiCodec.", "published": "2025-05-31 04:31:02", "link": "http://arxiv.org/abs/2506.00385v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Electromagnetically Reconfigurable Antennas for 6G: Enabling Technologies, Prototype Studies, and Research Outlook", "abstract": "The transition to the sixth-generation (6G) network is anticipated to\nredefine wireless transceiver architectures, demanding higher adaptability and\nefficiency at the antenna layer. Electromagnetically reconfigurable antennas\n(ERAs) have emerged as a promising solution capable of dynamically\nreconfiguring wireless channels to meet these requirements. This article\npresents an overview of recent advancements in ERA technology, underscoring its\ntransformative potential for 6G applications. Drawing from several initial\nstudies, we demonstrate that ERAs can significantly enhance communication rates\nand hardware efficiency. Nevertheless, critical challenges remain in hardware\ndesign and signal processing methodologies, necessitating concerted efforts\nfrom both the antenna and communication communities. We identify these gaps and\noutline key research directions to fully unlock the capabilities of ERAs in\nnext-generation wireless networks.", "published": "2025-05-31 17:57:15", "link": "http://arxiv.org/abs/2506.00657v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Helmet ultrasound for brain imaging in post-hemicraniectomy patients", "abstract": "Noninvasive imaging deep into the adult brain at submillimeter and\nmillisecond scales remains a challenge in medical imaging. Here, we report a\nhelmet based ultrasound brain imager built from a customized helmet, a scanned\nultrasound array, and three dimensional printing for real time imaging of human\nbrain anatomical and functional information. Through its application to post\nhemicraniectomy patients in a sitting position, we achieved volumetric brain\ntissue structural, vascular, and blood flow images at centimeter scale depths\nwith submillimeter and millisecond spatiotemporal resolutions. We also\ndemonstrated the system capability to track cerebral blood flow over repeated\nimaging sessions, including during motion prone conditions. Our brain imager\ncircumvents the skull and bridges the gap between high resolution human brain\nimaging and wearable convenience. This imager may serve as a platform for\nfurther investigations into human brain dynamics in post hemicraniectomy\npatients and offer insights into the brain that could surpass those obtained\nfrom non human primate studies.", "published": "2025-05-31 16:33:30", "link": "http://arxiv.org/abs/2506.00626v1", "categories": ["physics.med-ph", "eess.SP"], "primary_category": "physics.med-ph"}
{"title": "Real-Time Sounding in ISAC networks: Design and Implementation of a Multi-Node Testbed with Synchronized Airborne and Ground-Based Sensors", "abstract": "As integrated sensing and communication (ISAC) capabilities become more\nprevalent in the mobile 6G radio landscape, there is a substantial opportunity\nto enhance situational awareness across diverse applications through\nmulti-static radar sensing within meshed ISAC networks. To facilitate the\ndevelopment and testing of detection and localization algorithms across diverse\nscenarios, this paper introduces a synchronized distributed channel sounding\ntestbed with airborne and ground-based multi-channel transceiver nodes with\ncentimeter-level positioning accuracy enabled by real-time kinematic (RTK) and\ninertial navigation system (INS) data. Our modular experimental measurement\nsystem is designed to include stationary sensor nodes and light-weight to\nmedium-weight mobile nodes deployable on unmanned aerial vehicles (UAVs), cars,\npedestrians, and cyclists. Utilizing commercial off-the-shelf (COTS) hardware,\nspecifically software defined radios (SDRs), the testbed encourages\nreproducibility in academic research laboratories. We detail the individual\nmodules and integration steps required to achieve the specified performance.\nThe testbed's capabilities are validated through a real-world measurement\ncampaign, including stationary and flying sensor nodes, aimed at detecting\nradar targets such as vertical take-off and landing (VTOL) aircrafts, small\nhexacopters, cars and vulnerable road users (VRUs) in air-to-air (A2A) and\nair-to-ground (A2G) scenarios.", "published": "2025-05-31 16:28:51", "link": "http://arxiv.org/abs/2506.00624v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Over-the-air Multifunctional Wideband Electromagnetic Signal Processing using Dynamic Scattering Arrays", "abstract": "To meet the stringent requirements of next-generation wireless networks,\nmultiple-input multiple-output (MIMO) technology is expected to become massive\nand pervasive. Unfortunately, this could pose scalability issues in terms of\ncomplexity, power consumption, cost, and processing latency. Therefore, novel\ntechnologies and design approaches, such as the recently introduced holographic\nMIMO paradigm, must be investigated to make future networks sustainable. In\nthis context, we investigate the concept of a dynamic scattering array (DSA) as\na versatile electromagnetic (EM) structure capable of performing joint\nwave-based computing and radiation by moving the processing from the digital\ndomain to the EM domain. We provide a general, wideband analytical framework\nfor modeling the DSA, which includes a power matching network and realistic\nreconfigurable loads. Then we introduce specific design algorithms, and apply\nthem to various use cases. We demonstrate that some recent EM processing\nstructures can be seen as particular cases of our general framework. The\nexamples presented in the numerical results corroborate the potential of DSAs\nto reduce complexity and the number of radiofrequency (RF) chains in\nholographic MIMO systems while achieving enhanced EM wave processing and\nradiation flexibility for tasks such as beamforming and single- and multi-user\nMIMO, also exhibiting superdirectivity capabilities.", "published": "2025-05-31 16:08:20", "link": "http://arxiv.org/abs/2506.00619v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Exploiting Pinching-Antenna Systems in Multicast Communications", "abstract": "The pinching-antenna system (PASS) reconfigures wireless links through\npinching beamforming, in which the activated locations of pinching antennas\n(PAs) along dielectric waveguides are optimized. This article investigates the\napplication of PASS in multicast communication systems, where pinching\nbeamforming is designed to maximize the multicast rate. i) In the\nsingle-waveguide scenario, a closed-form solution for the optimal activated\nlocation is derived under the assumption of a single PA and linearly\ndistributed users. Based on this, a closed-form expression for the achievable\nmulticast rate is obtained and proven to be larger than that of conventional\nfixed-location antenna systems. For the general multiple-PA case with arbitrary\nuser distributions, an element-wise alternating optimization (AO) algorithm is\nproposed to design the pinching beamformer. ii) In the multiple-waveguide\nscenario, an AO-based method is developed to jointly optimize the transmit and\npinching beamformers. Specifically, the transmit beamformer is updated using a\nmajorization-minimization (MM) framework together with second-order cone\nprogramming (SOCP), while the pinching beamformer is optimized via element-wise\nsequential refinement. Numerical results are provided to demonstrate that: i)\nPASS achieves significantly higher multicast rates than conventional\nfixed-location antenna systems, particularly when the number of users and\nspatial coverage increase; ii) increasing the number of PAs further improves\nthe multicast performance of PASS.", "published": "2025-05-31 16:00:23", "link": "http://arxiv.org/abs/2506.00616v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Joint Activity Detection and Channel Estimation for Massive Connectivity: Where Message Passing Meets Score-Based Generative Priors", "abstract": "Massive connectivity supports the sporadic access of a vast number of devices\nwithout requiring prior permission from the base station (BS). In such\nscenarios, the BS must perform joint activity detection and channel estimation\n(JADCE) prior to data reception. Message passing algorithms have emerged as a\nprominent solution for JADCE under a Bayesian inference framework. The existing\nmessage passing algorithms, however, typically rely on some hand-crafted and\noverly simplistic priors of the wireless channel, leading to significant\nchannel estimation errors and reduced activity detection accuracy. In this\npaper, we focus on the problem of JADCE in a multiple-input multiple-output\northogonal frequency division multiplexing (MIMO-OFDM) grant-free random access\nnetwork. We propose to incorporate a more accurate channel prior learned by\nscore-based generative models into message passing, so as to push towards the\nperformance limit of JADCE. Specifically, we develop a novel turbo message\npassing (TMP) framework that models the entire channel matrix as a super node,\nrather than factorizing it element-wise. This design enables the seamless\nintegration of score-based generative models as a minimum mean-squared error\n(MMSE) denoiser. The variance of the denoiser, which is essential in message\npassing, can also be learned through score-based generative models. Our\napproach, termed score-based TMP for JADCE (STMP-JADCE), takes full advantages\nof the powerful generative prior and, meanwhile, benefits from the fast\nconvergence speed of message passing. Numerical simulations show that\nSTMP-JADCE drastically enhances the activity detection and channel estimation\nperformance compared to the state-of-the-art baseline algorithms.", "published": "2025-05-31 14:36:02", "link": "http://arxiv.org/abs/2506.00581v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Integrated Sensing, Computing and Semantic Communication for Vehicular Networks", "abstract": "This paper introduces a novel framework for integrated sensing, computing,\nand semantic communication (ISCSC) within vehicular networks comprising a\nroadside unit (RSU) and multiple autonomous vehicles. Both the RSU and the\nvehicles are equipped with local knowledge bases to facilitate semantic\ncommunication. The framework incorporates a secure communication design to\nensure that messages intended for specific vehicles are protected against\ninterception. In this model, an extended Kalman filter (EKF) is employed by the\nRSU to accurately track all vehicles. We formulate a joint optimization problem\nthat balances maximizing the probabilistically constrained semantic secrecy\nrate for each vehicle while minimizing the sum of the posterior Cram\\'er-Rao\nbound (PCRB), subject to the RSU's computing capabilities. This non-convex\noptimization problem is addressed using Bernstein-type inequality (BTI) and\nalternating optimization (AO) techniques. Simulation results validate the\neffectiveness of the proposed framework, demonstrating its advantages in\nreliable sensing, high data throughput, and secure communication.", "published": "2025-05-31 11:49:54", "link": "http://arxiv.org/abs/2506.00522v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Second-Order Characterization of Micro Doppler Radar Signatures of Drone Swarms", "abstract": "We investigate the second-order characteristics of the radar return signal\nfrom a swarm of rotor drones. We consider the case of a swarm of identical\ndrones, with each a number of rotors comprised of a number of rotor blades. By\nconsidering the orientation and speed of each rotor as stochastic variables, we\nderive expressions for the autocorrelation function (ACF) and power spectral\ndensity (PSD). The ACF and PSD are in the form of an infinite series with\ncoefficients that drop to zero at a predictable limit. Thus in practical\napplications, the series may be truncated. As a special case, we show that for\ndeterministic rotor speed, the ACF can be expressed in closed form. We further\ninvestigate how system parameters (Blade length, Rotor speed, number of blades,\nand number of drones) influence the derived expressions for the ACF and PSD.", "published": "2025-05-31 10:30:03", "link": "http://arxiv.org/abs/2506.00497v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "The Coupling Effect of Sensing Targets on the Environment for 3GPP ISAC Channels: Observation, Modeling, and Validation", "abstract": "Integrated Sensing And Communication (ISAC) has been identified as a key 6G\napplication by ITU and 3GPP, with standardization efforts already underway.\nSensing tasks, such as target localization, demand more precise\ncharacterization of the sensing target (ST) in ISAC channel modeling. The ST\ncouples complexly with environmental scatterers, potentially blocking some\nmultipaths and generating new ones, resulting in power variations compared to\nthe original channel. To accurately model this effect, this paper proposes a\ncoupled ISAC channel model based on measurements and validates it through\nsimilarity analysis between simulated and measured channels. In this work, we\nfirst conduct ISAC channel measurements in an indoor factory scenario at 105\nGHz, where the multipath power variations caused by the ST's interaction with\nthe environment are clearly observed. Then, we propose an ISAC channel modeling\nframework that incorporates two novel parameters: the Blockage-Region Coupling\nFactor (BR-CF) and the Forward-Scattering (FS)-CF, which characterize the\nspatial region and intensity of the coupling effect, respectively. Finally, the\nproposed model is validated through similarity comparison with measured data,\ndemonstrating higher accuracy for both LoS and NLoS scenarios compared to the\nnon-coupled model. This realistic ISAC channel model provides an effective\nframework for capturing the ST-environment coupling effect, supporting the\ndesign and evaluation of ISAC technologies.", "published": "2025-05-31 09:20:48", "link": "http://arxiv.org/abs/2506.00480v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Transient Error Analysis of the LMS and RLS Algorithm for Graph Signal Estimation", "abstract": "Recently, the proposal of the least mean square (LMS) and recursive least\nsquares (RLS) algorithm for graph signal processing (GSP) provides excellent\nsolutions for processing signals defined on irregular structures such as sensor\nnetworks. The existing work has completed the steady state error analysis of\nthe GSP LMS algorithm and GSP RLS algorithm in Gaussian noise scenarios, and a\nrange of values for the step size of the GSP LMS algorithm has also been given.\nMeanwhile, the transient error analysis of the GSP LMS algorithm and GSP RLS\nalgorithm is also important and challenging. Completing the above work will\nhelp to quantitatively analyze the performance of the graph signal adaptive\nestimation algorithm at transient moments, which is what this paper is working\non. By using formula derivation and mathematical induction, the transient\nerrors expressions of the GSP LMS and GSP RLS algorithm are given in this\npaper. Based on the Brazilian temperature datasets, the related simulation\nexperiments are executed, which strongly demonstrate the correctness of our\nproposed theoretical analysis", "published": "2025-05-31 05:48:30", "link": "http://arxiv.org/abs/2506.00403v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "A Family of Robust Generalized Adaptive Filters and Application for Time-series Prediction", "abstract": "The continuous development of new adaptive filters (AFs) based on novel cost\nfunctions (CFs) is driven by the demands of various application scenarios and\nnoise environments. However, these algorithms typically demonstrate optimal\nperformance only in specific conditions. In the event of the noise change, the\nperformance of these AFs often declines, rendering simple parameter adjustments\nineffective. Instead, a modification of the CF is necessary. To address this\nissue, the robust generalized adaptive AF (RGA-AF) with strong adaptability and\nflexibility is proposed in this paper. The flexibility of the RGA-AF's CF\nallows for smooth adaptation to varying noise environments through parameter\nadjustments, ensuring optimal filtering performance in diverse scenarios.\nMoreover, we introduce several fundamental properties of negative RGA (NRGA)\nentropy and present the negative asymmetric RGA-AF (NAR-GA-AF) and kernel\nrecursive NRGA-AF (KRNRGA-AF). These AFs address asymmetric noise distribution\nand nonlinear filtering issues, respectively. Simulations of linear system\nidentification and time-series prediction for Chua's circuit under different\nnoise environments demonstrate the superiority of the proposed algorithms in\ncomparison to existing techniques.", "published": "2025-05-31 05:33:43", "link": "http://arxiv.org/abs/2506.00397v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Neuro2Semantic: A Transfer Learning Framework for Semantic Reconstruction of Continuous Language from Human Intracranial EEG", "abstract": "Decoding continuous language from neural signals remains a significant\nchallenge in the intersection of neuroscience and artificial intelligence. We\nintroduce Neuro2Semantic, a novel framework that reconstructs the semantic\ncontent of perceived speech from intracranial EEG (iEEG) recordings. Our\napproach consists of two phases: first, an LSTM-based adapter aligns neural\nsignals with pre-trained text embeddings; second, a corrector module generates\ncontinuous, natural text directly from these aligned embeddings. This flexible\nmethod overcomes the limitations of previous decoding approaches and enables\nunconstrained text generation. Neuro2Semantic achieves strong performance with\nas little as 30 minutes of neural data, outperforming a recent state-of-the-art\nmethod in low-data settings. These results highlight the potential for\npractical applications in brain-computer interfaces and neural decoding\ntechnologies.", "published": "2025-05-31 04:17:19", "link": "http://arxiv.org/abs/2506.00381v1", "categories": ["cs.CL", "eess.AS", "eess.SP"], "primary_category": "cs.CL"}
{"title": "Physics-based Generative Models for Geometrically Consistent and Interpretable Wireless Channel Synthesis", "abstract": "In recent years, machine learning (ML) methods have become increasingly\npopular in wireless communication systems for several applications. A critical\nbottleneck for designing ML systems for wireless communications is the\navailability of realistic wireless channel datasets, which are extremely\nresource-intensive to produce. To this end, the generation of realistic\nwireless channels plays a key role in the subsequent design of effective ML\nalgorithms for wireless communication systems. Generative models have been\nproposed to synthesize channel matrices, but outputs produced by such methods\nmay not correspond to geometrically viable channels and do not provide any\ninsight into the scenario being generated. In this work, we aim to address both\nthese issues by integrating established parametric, physics-based geometric\nchannel (PPGC) modeling frameworks with generative methods to produce realistic\nchannel matrices with interpretable representations in the parameter domain. We\nshow that generative models converge to prohibitively suboptimal stationary\npoints when learning the underlying prior directly over the parameters due to\nthe non-convex PPGC model. To address this limitation, we propose a linearized\nreformulation of the problem to ensure smooth gradient flow during generative\nmodel training, while also providing insights into the underlying physical\nenvironment. We evaluate our model against prior baselines by comparing the\ngenerated, scenario-specific samples in terms of the 2-Wasserstein distance and\nthrough its utility when used for downstream compression tasks.", "published": "2025-05-31 04:00:44", "link": "http://arxiv.org/abs/2506.00374v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Neural Network-based Information-Theoretic Transceivers for High-Order Modulation Schemes", "abstract": "Neural network (NN)-based end-to-end (E2E) communication systems, in which\neach system component may consist of a portion of a neural network, have been\ninvestigated as potential tools for developing artificial intelligence\n(Al)-native E2E systems. In this paper, we propose an NN-based bitwise receiver\nthat improves computational efficiency while maintaining performance comparable\nto baseline demappers. Building on this foundation, we introduce a novel\nsymbol-wise autoencoder (AE)-based E2E system that jointly optimizes the\ntransmitter and receiver at the physical layer. We evaluate the proposed\nNN-based receiver using bit-error rate (BER) analysis to confirm that the\nnumerical BER achieved by NN-based receivers or transceivers is accurate.\nResults demonstrate that the AE-based system outperforms baseline\narchitectures, particularly for higher-order modulation schemes. We further\nshow that the training signal-to-noise ratio (SNR) significantly affects the\nperformance of the systems when inference is conducted at different SNR levels.", "published": "2025-05-31 03:22:26", "link": "http://arxiv.org/abs/2506.00368v1", "categories": ["eess.SP", "cs.AI"], "primary_category": "eess.SP"}
{"title": "Feature Fusion and Knowledge-Distilled Multi-Modal Multi-Target Detection", "abstract": "In the surveillance and defense domain, multi-target detection and\nclassification (MTD) is considered essential yet challenging due to\nheterogeneous inputs from diverse data sources and the computational complexity\nof algorithms designed for resource-constrained embedded devices, particularly\nfor Al-based solutions. To address these challenges, we propose a feature\nfusion and knowledge-distilled framework for multi-modal MTD that leverages\ndata fusion to enhance accuracy and employs knowledge distillation for improved\ndomain adaptation. Specifically, our approach utilizes both RGB and thermal\nimage inputs within a novel fusion-based multi-modal model, coupled with a\ndistillation training pipeline. We formulate the problem as a posterior\nprobability optimization task, which is solved through a multi-stage training\npipeline supported by a composite loss function. This loss function effectively\ntransfers knowledge from a teacher model to a student model. Experimental\nresults demonstrate that our student model achieves approximately 95% of the\nteacher model's mean Average Precision while reducing inference time by\napproximately 50%, underscoring its suitability for practical MTD deployment\nscenarios.", "published": "2025-05-31 03:11:44", "link": "http://arxiv.org/abs/2506.00365v1", "categories": ["cs.CV", "eess.SP"], "primary_category": "cs.CV"}
{"title": "Single-layer Circular SIW Filtenna With Beam Scanning Capability for 5G Millimeter Wave Communication Applications", "abstract": "In this communication, two novel low-cost single-layer filtering antennas\n(filtennas) are proposed for millimeter wave (mmWave) applications. The\nproposed filtennas consists of a compact circular substrate integrated\nwaveguide (SIW) cavity, a metal post close to the center of the cavity for\npower feeding, a metal post in the center for modes controlling, and a slot for\nradiating power. In the passband, the fundamental TM010 mode and the TM110 mode\nin the circular SIW cavity are excited by the feeding post. In addition, thanks\nto the high-pass characteristics of the cavity, it exhibits more than 20 dB\nsuppression in the lower frequency band. There are three radiation nulls in\nFiltenna 1 and one radiation null in Filtenna 2 in the upper band which\nincrease the suppression level as high as 18 dB. As a proof of concept, the\nproposed filtennas are fabricated and measured. It is shown that the Filtenna 1\ncan achieve simulated and measured -10 dB impedance fractional bandwidth (FBW)\nof 7.1% (27.14 - 29.13 GHz) and 8.6% (27.62 - 30.11 GHz), respectively. While\nfiltenna 2 can achieve simulated and measured -10 dB FBW of 7.4% (27.86 - 29.99\nGHz) and 10.1% (28.11 - 31.09 GHz), respectively. The filtennas features stable\nradiation patterns with an average gain of 5.0 dBi. The lower and upper\nsideband suppression levels for both filtennas exceed 18 dB. These filtennas\nare good candidates for 5G mmWave applications, as they simultaneously provide\nbeam scanning and filtering capability with a low cost, and single layer\nstructure.", "published": "2025-05-31 03:04:30", "link": "http://arxiv.org/abs/2506.00361v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Power-of-Two (PoT) Weights in Large Language Models (LLMs)", "abstract": "Complexity of Neural Networks is increasing rapidly due to the massive\nincrease in model parameters. Specifically, in Large Language Models (LLMs),\nthe number of model parameters has grown exponentially in the past few years,\nfor example, from 1.5 billion parameters in GPT2 to 175 billion in GPT3. This\nraises a significant challenge for implementation, especially for Edge devices\nwhere memory and processing power are very limited. In this work, we\ninvestigate reducing LLM complexity with special type of quantization, power of\ntwo (PoT), for linear layers weights and transformer tables. PoT not only\nprovides memory reduction but more importantly provides significant\ncomputational reduction through converting multiplication to bit shifting. We\nobtained preliminary results of PoT quantization on Nano-GPT implementation\nusing Shakespeare dataset. We then extended results to 124-M GPT-2 model. The\nPoT quantization results are shown to be very promising with cross entropy loss\ndegradation $\\approx$[1.3-0.88] with number of bits range [4-6] to represent\npower levels.", "published": "2025-05-31 00:01:25", "link": "http://arxiv.org/abs/2506.00315v1", "categories": ["eess.SP", "cs.LG"], "primary_category": "eess.SP"}
