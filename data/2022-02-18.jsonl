{"title": "CLSEG: Contrastive Learning of Story Ending Generation", "abstract": "Story Ending Generation (SEG) is a challenging task in natural language\ngeneration. Recently, methods based on Pre-trained Language Models (PLM) have\nachieved great prosperity, which can produce fluent and coherent story endings.\nHowever, the pre-training objective of PLM-based methods is unable to model the\nconsistency between story context and ending. The goal of this paper is to\nadopt contrastive learning to generate endings more consistent with story\ncontext, while there are two main challenges in contrastive learning of SEG.\nFirst is the negative sampling of wrong endings inconsistent with story\ncontexts. The second challenge is the adaptation of contrastive learning for\nSEG. To address these two issues, we propose a novel Contrastive Learning\nframework for Story Ending Generation (CLSEG), which has two steps:\nmulti-aspect sampling and story-specific contrastive learning. Particularly,\nfor the first issue, we utilize novel multi-aspect sampling mechanisms to\nobtain wrong endings considering the consistency of order, causality, and\nsentiment. To solve the second issue, we well-design a story-specific\ncontrastive training strategy that is adapted for SEG. Experiments show that\nCLSEG outperforms baselines and can produce story endings with stronger\nconsistency and rationality.", "published": "2022-02-18 07:11:04", "link": "http://arxiv.org/abs/2202.09049v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AMS_ADRN at SemEval-2022 Task 5: A Suitable Image-text Multimodal Joint\n  Modeling Method for Multi-task Misogyny Identification", "abstract": "Women are influential online, especially in image-based social media such as\nTwitter and Instagram. However, many in the network environment contain gender\ndiscrimination and aggressive information, which magnify gender stereotypes and\ngender inequality. Therefore, the filtering of illegal content such as gender\ndiscrimination is essential to maintain a healthy social network environment.\nIn this paper, we describe the system developed by our team for SemEval-2022\nTask 5: Multimedia Automatic Misogyny Identification. More specifically, we\nintroduce two novel system to analyze these posts: a multimodal multi-task\nlearning architecture that combines Bertweet for text encoding with ResNet-18\nfor image representation, and a single-flow transformer structure which\ncombines text embeddings from BERT-Embeddings and image embeddings from several\ndifferent modules such as EfficientNet and ResNet. In this manner, we show that\nthe information behind them can be properly revealed. Our approach achieves\ngood performance on each of the two subtasks of the current competition,\nranking 15th for Subtask A (0.746 macro F1-score), 11th for Subtask B (0.706\nmacro F1-score) while exceeding the official baseline results by high margins.", "published": "2022-02-18 09:41:37", "link": "http://arxiv.org/abs/2202.09099v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Synthetic Disinformation Attacks on Automated Fact Verification Systems", "abstract": "Automated fact-checking is a needed technology to curtail the spread of\nonline misinformation. One current framework for such solutions proposes to\nverify claims by retrieving supporting or refuting evidence from related\ntextual sources. However, the realistic use cases for fact-checkers will\nrequire verifying claims against evidence sources that could be affected by the\nsame misinformation. Furthermore, the development of modern NLP tools that can\nproduce coherent, fabricated content would allow malicious actors to\nsystematically generate adversarial disinformation for fact-checkers.\n  In this work, we explore the sensitivity of automated fact-checkers to\nsynthetic adversarial evidence in two simulated settings: AdversarialAddition,\nwhere we fabricate documents and add them to the evidence repository available\nto the fact-checking system, and AdversarialModification, where existing\nevidence source documents in the repository are automatically altered. Our\nstudy across multiple models on three benchmarks demonstrates that these\nsystems suffer significant performance drops against these attacks. Finally, we\ndiscuss the growing threat of modern NLG systems as generators of\ndisinformation in the context of the challenges they pose to automated\nfact-checkers.", "published": "2022-02-18 19:01:01", "link": "http://arxiv.org/abs/2202.09381v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From FreEM to D'AlemBERT: a Large Corpus and a Language Model for Early\n  Modern French", "abstract": "Language models for historical states of language are becoming increasingly\nimportant to allow the optimal digitisation and analysis of old textual\nsources. Because these historical states are at the same time more complex to\nprocess and more scarce in the corpora available, specific efforts are\nnecessary to train natural language processing (NLP) tools adapted to the data.\nIn this paper, we present our efforts to develop NLP tools for Early Modern\nFrench (historical French from the 16$^\\text{th}$ to the 18$^\\text{th}$\ncenturies). We present the $\\text{FreEM}_{\\text{max}}$ corpus of Early Modern\nFrench and D'AlemBERT, a RoBERTa-based language model trained on\n$\\text{FreEM}_{\\text{max}}$. We evaluate the usefulness of D'AlemBERT by\nfine-tuning it on a part-of-speech tagging task, outperforming previous work on\nthe test set. Importantly, we find evidence for the transfer learning capacity\nof the language model, since its performance on lesser-resourced time periods\nappears to have been boosted by the more resourced ones. We release D'AlemBERT\nand the open-sourced subpart of the $\\text{FreEM}_{\\text{max}}$ corpus.", "published": "2022-02-18 22:17:22", "link": "http://arxiv.org/abs/2202.09452v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "VLP: A Survey on Vision-Language Pre-training", "abstract": "In the past few years, the emergence of pre-training models has brought\nuni-modal fields such as computer vision (CV) and natural language processing\n(NLP) to a new era. Substantial works have shown they are beneficial for\ndownstream uni-modal tasks and avoid training a new model from scratch. So can\nsuch pre-trained models be applied to multi-modal tasks? Researchers have\nexplored this problem and made significant progress. This paper surveys recent\nadvances and new frontiers in vision-language pre-training (VLP), including\nimage-text and video-text pre-training. To give readers a better overall grasp\nof VLP, we first review its recent advances from five aspects: feature\nextraction, model architecture, pre-training objectives, pre-training datasets,\nand downstream tasks. Then, we summarize the specific VLP models in detail.\nFinally, we discuss the new frontiers in VLP. To the best of our knowledge,\nthis is the first survey focused on VLP. We hope that this survey can shed\nlight on future research in the VLP field.", "published": "2022-02-18 07:54:02", "link": "http://arxiv.org/abs/2202.09061v4", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Identifying the Adoption or Rejection of Misinformation Targeting\n  COVID-19 Vaccines in Twitter Discourse", "abstract": "Although billions of COVID-19 vaccines have been administered, too many\npeople remain hesitant. Misinformation about the COVID-19 vaccines, propagating\non social media, is believed to drive hesitancy towards vaccination. However,\nexposure to misinformation does not necessarily indicate misinformation\nadoption. In this paper we describe a novel framework for identifying the\nstance towards misinformation, relying on attitude consistency and its\nproperties. The interactions between attitude consistency, adoption or\nrejection of misinformation and the content of microblogs are exploited in a\nnovel neural architecture, where the stance towards misinformation is organized\nin a knowledge graph. This new neural framework is enabling the identification\nof stance towards misinformation about COVID-19 vaccines with state-of-the-art\nresults. The experiments are performed on a new dataset of misinformation\ntowards COVID-19 vaccines, called CoVaxLies, collected from recent Twitter\ndiscourse. Because CoVaxLies provides a taxonomy of the misinformation about\nCOVID-19 vaccines, we are able to show which type of misinformation is mostly\nadopted and which is mostly rejected.", "published": "2022-02-18 22:01:49", "link": "http://arxiv.org/abs/2202.09445v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "VaccineLies: A Natural Language Resource for Learning to Recognize\n  Misinformation about the COVID-19 and HPV Vaccines", "abstract": "Billions of COVID-19 vaccines have been administered, but many remain\nhesitant. Misinformation about the COVID-19 vaccines and other vaccines,\npropagating on social media, is believed to drive hesitancy towards\nvaccination. The ability to automatically recognize misinformation targeting\nvaccines on Twitter depends on the availability of data resources. In this\npaper we present VaccineLies, a large collection of tweets propagating\nmisinformation about two vaccines: the COVID-19 vaccines and the Human\nPapillomavirus (HPV) vaccines. Misinformation targets are organized in\nvaccine-specific taxonomies, which reveal the misinformation themes and\nconcerns. The ontological commitments of the Misinformation taxonomies provide\nan understanding of which misinformation themes and concerns dominate the\ndiscourse about the two vaccines covered in VaccineLies. The organization into\ntraining, testing and development sets of VaccineLies invites the development\nof novel supervised methods for detecting misinformation on Twitter and\nidentifying the stance towards it. Furthermore, VaccineLies can be a stepping\nstone for the development of datasets focusing on misinformation targeting\nadditional vaccines.", "published": "2022-02-18 22:09:38", "link": "http://arxiv.org/abs/2202.09449v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "From Hesitancy Framings to Vaccine Hesitancy Profiles: A Journey of\n  Stance, Ontological Commitments and Moral Foundations", "abstract": "While billions of COVID-19 vaccines have been administered, too many people\nremain hesitant. Twitter, with its substantial reach and daily exposure, is an\nexcellent resource for examining how people frame their vaccine hesitancy and\nto uncover vaccine hesitancy profiles. In this paper we expose our processing\njourney from identifying Vaccine Hesitancy Framings in a collection of\n9,133,471 original tweets discussing the COVID-19 vaccines, establishing their\nontological commitments, annotating the Moral Foundations they imply to the\nautomatic recognition of the stance of the tweet authors toward any of the\nCoVaxFrames that we have identified. When we found that 805,336 Twitter users\nhad a stance towards some CoVaxFrames in either the 9,133,471 original tweets\nor their 17,346,664 retweets, we were able to derive nine different Vaccine\nHesitancy Profiles of these users and to interpret these profiles based on the\nontological commitments of the frames they evoked in their tweets and on value\nof their stance towards the evoked frames.", "published": "2022-02-18 22:22:56", "link": "http://arxiv.org/abs/2202.09456v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "End-to-end contextual asr based on posterior distribution adaptation for\n  hybrid ctc/attention system", "abstract": "End-to-end (E2E) speech recognition architectures assemble all components of\ntraditional speech recognition system into a single model. Although it\nsimplifies ASR system, it introduces contextual ASR drawback: the E2E model has\nworse performance on utterances containing infrequent proper nouns. In this\nwork, we propose to add a contextual bias attention (CBA) module to attention\nbased encoder decoder (AED) model to improve its ability of recognizing the\ncontextual phrases. Specifically, CBA utilizes the context vector of source\nattention in decoder to attend to a specific bias embedding. Jointly learned\nwith the basic AED parameters, CBA can tell the model when and where to bias\nits output probability distribution. At inference stage, a list of bias phrases\nis preloaded and we adapt the posterior distributions of both CTC and attention\ndecoder according to the attended bias phrase of CBA. We evaluate the proposed\nmethod on GigaSpeech and achieve a consistent relative improvement on recall\nrate of bias phrases ranging from 15% to 28% compared to the baseline model.\nMeanwhile, our method shows a strong anti-bias ability as the performance on\ngeneral tests only degrades 1.7% even 2,000 bias phrases are present.", "published": "2022-02-18 03:26:02", "link": "http://arxiv.org/abs/2202.09003v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "TURNER: The Uncertainty-based Retrieval Framework for Chinese NER", "abstract": "Chinese NER is a difficult undertaking due to the ambiguity of Chinese\ncharacters and the absence of word boundaries. Previous work on Chinese NER\nfocus on lexicon-based methods to introduce boundary information and reduce\nout-of-vocabulary (OOV) cases during prediction. However, it is expensive to\nobtain and dynamically maintain high-quality lexicons in specific domains,\nwhich motivates us to utilize more general knowledge resources, e.g., search\nengines. In this paper, we propose TURNER: The Uncertainty-based Retrieval\nframework for Chinese NER. The idea behind TURNER is to imitate human behavior:\nwe frequently retrieve auxiliary knowledge as assistance when encountering an\nunknown or uncertain entity. To improve the efficiency and effectiveness of\nretrieval, we first propose two types of uncertainty sampling methods for\nselecting the most ambiguous entity-level uncertain components of the input\ntext. Then, the Knowledge Fusion Model re-predict the uncertain samples by\ncombining retrieved knowledge. Experiments on four benchmark datasets\ndemonstrate TURNER's effectiveness. TURNER outperforms existing lexicon-based\napproaches and achieves the new SOTA.", "published": "2022-02-18 05:05:22", "link": "http://arxiv.org/abs/2202.09022v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Large-Scale Acoustic Characterization of Singaporean Children's English\n  Pronunciation", "abstract": "In this work, we investigate pronunciation differences in English spoken by\nSingaporean children in relation to their American and British counterparts by\nconducting Kmeans clustering and Archetypal analysis on selected vowel pairs\nand approximants. Given that Singapore adopts British English as the\ninstitutional standard due to historical reasons, one might expect Singaporean\nchildren to follow British pronunciation patterns. Indeed, Singaporean and\nBritish children are more similar in their production of syllable-final /r/ --\nthey do not lower their third formant nearly as much as American children do,\nsuggesting a lack of rhoticity. Interestingly, Singaporean children also\npresent similar patterns to American children when it comes to their fronting\nof vowels as demonstrated across various vowels including TRAP-BATH split\nvowels. Singaporean children's English also demonstrated characteristics that\ndo not resemble any of the other two populations. We observe that Singaporean\nchildren's vowel height characteristics are distinct from both that of American\nand British children. In tense and lax vowel pairs, we also consistently\nobserve that the distinction is less conspicuous for Singaporean children\ncompared to the other speaker groups. Further, while American and British\nchildren demonstrate lowering of F1 and F2 formants in transitions into\nsyllable-final /l/s, a wide gap between F2 and F3 formants, and small\ndifference between F1 and F2 formants, all of these are not exhibited in\nSingaporean children's pronunciation. These findings point towards potential\nsociolinguistic implications of how Singapore English might be evolving to\nembody more than British pronunciation characteristics. Furthermore, these\nfindings also suggest that Singapore English could be have been influenced by\nlanguages beyond American and British English, potentially due to Singapore's\nmultilingual environment.", "published": "2022-02-18 10:18:09", "link": "http://arxiv.org/abs/2202.09108v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Modelling the semantics of text in complex document layouts using graph\n  transformer networks", "abstract": "Representing structured text from complex documents typically calls for\ndifferent machine learning techniques, such as language models for paragraphs\nand convolutional neural networks (CNNs) for table extraction, which prohibits\ndrawing links between text spans from different content types. In this article\nwe propose a model that approximates the human reading pattern of a document\nand outputs a unique semantic representation for every text span irrespective\nof the content type they are found in. We base our architecture on a graph\nrepresentation of the structured text, and we demonstrate that not only can we\nretrieve semantically similar information across documents but also that the\nembedding space we generate captures useful semantic information, similar to\nlanguage models that work only on text sequences.", "published": "2022-02-18 11:49:06", "link": "http://arxiv.org/abs/2202.09144v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Evaluating the Construct Validity of Text Embeddings with Application to\n  Survey Questions", "abstract": "Text embedding models from Natural Language Processing can map text data\n(e.g. words, sentences, documents) to supposedly meaningful numerical\nrepresentations (a.k.a. text embeddings). While such models are increasingly\napplied in social science research, one important issue is often not addressed:\nthe extent to which these embeddings are valid representations of constructs\nrelevant for social science research. We therefore propose the use of the\nclassic construct validity framework to evaluate the validity of text\nembeddings. We show how this framework can be adapted to the opaque and\nhigh-dimensional nature of text embeddings, with application to survey\nquestions. We include several popular text embedding methods (e.g. fastText,\nGloVe, BERT, Sentence-BERT, Universal Sentence Encoder) in our construct\nvalidity analyses. We find evidence of convergent and discriminant validity in\nsome cases. We also show that embeddings can be used to predict respondent's\nanswers to completely new survey questions. Furthermore, BERT-based embedding\ntechniques and the Universal Sentence Encoder provide more valid\nrepresentations of survey questions than do others. Our results thus highlight\nthe necessity to examine the construct validity of text embeddings before\ndeploying them in social science research.", "published": "2022-02-18 12:35:46", "link": "http://arxiv.org/abs/2202.09166v1", "categories": ["cs.CY", "cs.CL", "stat.AP", "stat.ME"], "primary_category": "cs.CY"}
{"title": "Automated Attack Synthesis by Extracting Finite State Machines from\n  Protocol Specification Documents", "abstract": "Automated attack discovery techniques, such as attacker synthesis or\nmodel-based fuzzing, provide powerful ways to ensure network protocols operate\ncorrectly and securely. Such techniques, in general, require a formal\nrepresentation of the protocol, often in the form of a finite state machine\n(FSM). Unfortunately, many protocols are only described in English prose, and\nimplementing even a simple network protocol as an FSM is time-consuming and\nprone to subtle logical errors. Automatically extracting protocol FSMs from\ndocumentation can significantly contribute to increased use of these techniques\nand result in more robust and secure protocol implementations.\n  In this work we focus on attacker synthesis as a representative technique for\nprotocol security, and on RFCs as a representative format for protocol prose\ndescription. Unlike other works that rely on rule-based approaches or use\noff-the-shelf NLP tools directly, we suggest a data-driven approach for\nextracting FSMs from RFC documents. Specifically, we use a hybrid approach\nconsisting of three key steps: (1) large-scale word-representation learning for\ntechnical language, (2) focused zero-shot learning for mapping protocol text to\na protocol-independent information language, and (3) rule-based mapping from\nprotocol-independent information to a specific protocol FSM. We show the\ngeneralizability of our FSM extraction by using the RFCs for six different\nprotocols: BGPv4, DCCP, LTP, PPTP, SCTP and TCP. We demonstrate how automated\nextraction of an FSM from an RFC can be applied to the synthesis of attacks,\nwith TCP and DCCP as case-studies. Our approach shows that it is possible to\nautomate attacker synthesis against protocols by using textual specifications\nsuch as RFCs.", "published": "2022-02-18 23:27:29", "link": "http://arxiv.org/abs/2202.09470v1", "categories": ["cs.CR", "cs.CL", "cs.FL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "A Survey of Vision-Language Pre-Trained Models", "abstract": "As transformer evolves, pre-trained models have advanced at a breakneck pace\nin recent years. They have dominated the mainstream techniques in natural\nlanguage processing (NLP) and computer vision (CV). How to adapt pre-training\nto the field of Vision-and-Language (V-L) learning and improve downstream task\nperformance becomes a focus of multimodal learning. In this paper, we review\nthe recent progress in Vision-Language Pre-Trained Models (VL-PTMs). As the\ncore content, we first briefly introduce several ways to encode raw images and\ntexts to single-modal embeddings before pre-training. Then, we dive into the\nmainstream architectures of VL-PTMs in modeling the interaction between text\nand image representations. We further present widely-used pre-training tasks,\nand then we introduce some common downstream tasks. We finally conclude this\npaper and present some promising research directions. Our survey aims to\nprovide researchers with synthesis and pointer to related research.", "published": "2022-02-18 15:15:46", "link": "http://arxiv.org/abs/2202.10936v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Speaker Identity Preservation in Dysarthric Speech Reconstruction by\n  Adversarial Speaker Adaptation", "abstract": "Dysarthric speech reconstruction (DSR), which aims to improve the quality of\ndysarthric speech, remains a challenge, not only because we need to restore the\nspeech to be normal, but also must preserve the speaker's identity. The speaker\nrepresentation extracted by the speaker encoder (SE) optimized for speaker\nverification has been explored to control the speaker identity. However, the SE\nmay not be able to fully capture the characteristics of dysarthric speakers\nthat are previously unseen. To address this research problem, we propose a\nnovel multi-task learning strategy, i.e., adversarial speaker adaptation (ASA).\nThe primary task of ASA fine-tunes the SE with the speech of the target\ndysarthric speaker to effectively capture identity-related information, and the\nsecondary task applies adversarial training to avoid the incorporation of\nabnormal speaking patterns into the reconstructed speech, by regularizing the\ndistribution of reconstructed speech to be close to that of reference speech\nwith high quality. Experiments show that the proposed approach can achieve\nenhanced speaker similarity and comparable speech naturalness with a strong\nbaseline approach. Compared with dysarthric speech, the reconstructed speech\nachieves 22.3% and 31.5% absolute word error rate reduction for speakers with\nmoderate and moderate-severe dysarthria respectively. Our demo page is released\nhere: https://wendison.github.io/ASA-DSR-demo/", "published": "2022-02-18 08:59:36", "link": "http://arxiv.org/abs/2202.09082v1", "categories": ["eess.AS", "cs.CL", "cs.MM", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Echo-aware Adaptation of Sound Event Localization and Detection in\n  Unknown Environments", "abstract": "Our goal is to develop a sound event localization and detection (SELD) system\nthat works robustly in unknown environments. A SELD system trained on known\nenvironment data is degraded in an unknown environment due to environmental\neffects such as reverberation and noise not contained in the training data.\nPrevious studies on related tasks have shown that domain adaptation methods are\neffective when data on the environment in which the system will be used is\navailable even without labels. However adaptation to unknown environments\nremains a difficult task. In this study, we propose echo-aware feature\nrefinement (EAR) for SELD, which suppresses environmental effects at the\nfeature level by using additional spatial cues of the unknown environment\nobtained through measuring acoustic echoes. FOA-MEIR, an impulse response\ndataset containing over 100 environments, was recorded to validate the proposed\nmethod. Experiments on FOA-MEIR show that the EAR effectively improves SELD\nperformance in unknown environments.", "published": "2022-02-18 10:54:46", "link": "http://arxiv.org/abs/2202.09121v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multi-view and Multi-modal Event Detection Utilizing Transformer-based\n  Multi-sensor fusion", "abstract": "We tackle a challenging task: multi-view and multi-modal event detection that\ndetects events in a wide-range real environment by utilizing data from\ndistributed cameras and microphones and their weak labels. In this task,\ndistributed sensors are utilized complementarily to capture events that are\ndifficult to capture with a single sensor, such as a series of actions of\npeople moving in an intricate room, or communication between people located far\napart in a room. For sensors to cooperate effectively in such a situation, the\nsystem should be able to exchange information among sensors and combines\ninformation that is useful for identifying events in a complementary manner.\nFor such a mechanism, we propose a Transformer-based multi-sensor fusion\n(MultiTrans) which combines multi-sensor data on the basis of the relationships\nbetween features of different viewpoints and modalities. In the experiments\nusing a dataset newly collected for this task, our proposed method using\nMultiTrans improved the event detection performance and outperformed\ncomparatives.", "published": "2022-02-18 11:10:19", "link": "http://arxiv.org/abs/2202.09124v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Domain Adaptation of low-resource Target-Domain models using\n  well-trained ASR Conformer Models", "abstract": "In this paper, we investigate domain adaptation for low-resource Automatic\nSpeech Recognition (ASR) of target-domain data, when a well-trained ASR model\ntrained with a large dataset is available. We argue that in the encoder-decoder\nframework, the decoder of the well-trained ASR model is largely tuned towards\nthe source-domain, hurting the performance of target-domain models in vanilla\ntransfer-learning. On the other hand, the encoder layers of the well-trained\nASR model mostly capture the acoustic characteristics. We, therefore, propose\nto use the embeddings tapped from these encoder layers as features for a\ndownstream Conformer target-domain model and show that they provide significant\nimprovements. We do ablation studies on which encoder layer is optimal to tap\nthe embeddings, as well as the effect of freezing or updating the well-trained\nASR model's encoder layers. We further show that applying Spectral Augmentation\n(SpecAug) on the proposed features (this is in addition to default SpecAug on\ninput spectral features) provides a further improvement on the target-domain\nperformance. For the LibriSpeech-100-clean data as target-domain and SPGI-5000\nas a well-trained model, we get 30% relative improvement over baseline.\nSimilarly, with WSJ data as target-domain and LibriSpeech-960 as a well-trained\nmodel, we get 50% relative improvement over baseline.", "published": "2022-02-18 12:38:17", "link": "http://arxiv.org/abs/2202.09167v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Predicting Sex and Stroke Success -- Computer-aided Player Grunt\n  Analysis in Tennis Matches", "abstract": "Professional athletes increasingly use automated analysis of meta- and signal\ndata to improve their training and game performance. As in other related\nhuman-to-human research fields, signal data, in particular, contain important\nperformance- and mood-specific indicators for automated analysis. In this\npaper, we introduce the novel data set SCORE! to investigate the performance of\nseveral features and machine learning paradigms in the prediction of the sex\nand immediate stroke success in tennis matches, based only on vocal expression\nthrough players' grunts. The data was gathered from YouTube, labelled under the\nexact same definition, and the audio processed for modelling. We extract\nseveral widely used basic, expert-knowledge, and deep acoustic features of the\naudio samples and evaluate their effectiveness in combination with various\nmachine learning approaches. In a binary setting, the best system, using\nspectrograms and a Convolutional Recurrent Neural Network, achieves an\nunweighted average recall (UAR) of 84.0 % for the player sex prediction task,\nand 60.3 % predicting stroke success, based only on acoustic cues in players'\ngrunts of both sexes. Further, we achieve a UAR of 58.3 %, and 61.3 %, when the\nmodels are exclusively trained on female or male grunts, respectively.", "published": "2022-02-18 10:06:46", "link": "http://arxiv.org/abs/2202.09102v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Deep-Learning Architectures for Multi-Pitch Estimation: Towards Reliable\n  Evaluation", "abstract": "Extracting pitch information from music recordings is a challenging but\nimportant problem in music signal processing. Frame-wise transcription or\nmulti-pitch estimation aims for detecting the simultaneous activity of pitches\nin polyphonic music recordings and has recently seen major improvements thanks\nto deep-learning techniques, with a variety of proposed network architectures.\nIn this paper, we realize different architectures based on CNNs, the U-net\nstructure, and self-attention components. We propose several modifications to\nthese architectures including self-attention modules for skip connections,\nrecurrent layers to replace the self-attention, and a multi-task strategy with\nsimultaneous prediction of the degree of polyphony. We compare variants of\nthese architectures in different sizes for multi-pitch estimation, focusing on\nWestern classical music beyond the piano-solo scenario using the MusicNet and\nSchubert Winterreise datasets. Our experiments indicate that most architectures\nyield competitive results and that larger model variants seem to be beneficial.\nHowever, we find that these results substantially depend on randomization\neffects and the particular choice of the training-test split, which questions\nthe claim of superiority for particular architectures given only small\nimprovements. We therefore investigate the influence of dataset splits in the\npresence of several movements of a work cycle (cross-version evaluation) and\npropose a best-practice splitting strategy for MusicNet, which weakens the\ninfluence of individual test tracks and suppresses overfitting to specific\nworks and recording conditions. A final evaluation on a mixed dataset suggests\nthat improvements on one specific dataset do not necessarily generalize to\nother scenarios, thus emphasizing the need for further high-quality multi-pitch\ndatasets in order to reliably measure progress in music transcription tasks.", "published": "2022-02-18 13:52:21", "link": "http://arxiv.org/abs/2202.09198v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "VCVTS: Multi-speaker Video-to-Speech synthesis via cross-modal knowledge\n  transfer from voice conversion", "abstract": "Though significant progress has been made for speaker-dependent\nVideo-to-Speech (VTS) synthesis, little attention is devoted to multi-speaker\nVTS that can map silent video to speech, while allowing flexible control of\nspeaker identity, all in a single system. This paper proposes a novel\nmulti-speaker VTS system based on cross-modal knowledge transfer from voice\nconversion (VC), where vector quantization with contrastive predictive coding\n(VQCPC) is used for the content encoder of VC to derive discrete phoneme-like\nacoustic units, which are transferred to a Lip-to-Index (Lip2Ind) network to\ninfer the index sequence of acoustic units. The Lip2Ind network can then\nsubstitute the content encoder of VC to form a multi-speaker VTS system to\nconvert silent video to acoustic units for reconstructing accurate spoken\ncontent. The VTS system also inherits the advantages of VC by using a speaker\nencoder to produce speaker representations to effectively control the speaker\nidentity of generated speech. Extensive evaluations verify the effectiveness of\nproposed approach, which can be applied in both constrained vocabulary and open\nvocabulary conditions, achieving state-of-the-art performance in generating\nhigh-quality speech with high naturalness, intelligibility and speaker\nsimilarity. Our demo page is released here:\nhttps://wendison.github.io/VCVTS-demo/", "published": "2022-02-18 08:58:45", "link": "http://arxiv.org/abs/2202.09081v1", "categories": ["eess.AS", "cs.AI", "cs.CV", "cs.MM", "cs.SD", "eess.IV"], "primary_category": "eess.AS"}
