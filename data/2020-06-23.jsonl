{"title": "Keyframe Segmentation and Positional Encoding for Video-guided Machine\n  Translation Challenge 2020", "abstract": "Video-guided machine translation as one of multimodal neural machine\ntranslation tasks targeting on generating high-quality text translation by\ntangibly engaging both video and text. In this work, we presented our\nvideo-guided machine translation system in approaching the Video-guided Machine\nTranslation Challenge 2020. This system employs keyframe-based video feature\nextractions along with the video feature positional encoding. In the evaluation\nphase, our system scored 36.60 corpus-level BLEU-4 and achieved the 1st place\non the Video-guided Machine Translation Challenge 2020.", "published": "2020-06-23 07:15:11", "link": "http://arxiv.org/abs/2006.12799v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Inductive Unsupervised Domain Adaptation for Few-Shot Classification via\n  Clustering", "abstract": "Few-shot classification tends to struggle when it needs to adapt to diverse\ndomains. Due to the non-overlapping label space between domains, the\nperformance of conventional domain adaptation is limited. Previous work tackles\nthe problem in a transductive manner, by assuming access to the full set of\ntest data, which is too restrictive for many real-world applications. In this\npaper, we set out to tackle this issue by introducing a inductive framework,\nDaFeC, to improve Domain adaptation performance for Few-shot classification via\nClustering. We first build a representation extractor to derive features for\nunlabeled data from the target domain (no test data is necessary) and then\ngroup them with a cluster miner. The generated pseudo-labeled data and the\nlabeled source-domain data are used as supervision to update the parameters of\nthe few-shot classifier. In order to derive high-quality pseudo labels, we\npropose a Clustering Promotion Mechanism, to learn better features for the\ntarget domain via Similarity Entropy Minimization and Adversarial Distribution\nAlignment, which are combined with a Cosine Annealing Strategy. Experiments are\nperformed on the FewRel 2.0 dataset. Our approach outperforms previous work\nwith absolute gains (in classification accuracy) of 4.95%, 9.55%, 3.99% and\n11.62%, respectively, under four few-shot settings.", "published": "2020-06-23 08:17:48", "link": "http://arxiv.org/abs/2006.12816v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Domain Adaptation for Semantic Parsing", "abstract": "Recently, semantic parsing has attracted much attention in the community.\nAlthough many neural modeling efforts have greatly improved the performance, it\nstill suffers from the data scarcity issue. In this paper, we propose a novel\nsemantic parser for domain adaptation, where we have much fewer annotated data\nin the target domain compared to the source domain. Our semantic parser\nbenefits from a two-stage coarse-to-fine framework, thus can provide different\nand accurate treatments for the two stages, i.e., focusing on domain invariant\nand domain specific information, respectively. In the coarse stage, our novel\ndomain discrimination component and domain relevance attention encourage the\nmodel to learn transferable domain general structures. In the fine stage, the\nmodel is guided to concentrate on domain related details. Experiments on a\nbenchmark dataset show that our method consistently outperforms several popular\ndomain adaptation strategies. Additionally, we show that our model can well\nexploit limited target data to capture the difference between the source and\ntarget domain, even when the target domain has far fewer training instances.", "published": "2020-06-23 14:47:41", "link": "http://arxiv.org/abs/2006.13071v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Combining Neural Language Models for WordSense Induction", "abstract": "Word sense induction (WSI) is the problem of grouping occurrences of an\nambiguous word according to the expressed sense of this word. Recently a new\napproach to this task was proposed, which generates possible substitutes for\nthe ambiguous word in a particular context using neural language models, and\nthen clusters sparse bag-of-words vectors built from these substitutes. In this\nwork, we apply this approach to the Russian language and improve it in two\nways. First, we propose methods of combining left and right contexts, resulting\nin better substitutes generated. Second, instead of fixed number of clusters\nfor all ambiguous words we propose a technique for selecting individual number\nof clusters for each word. Our approach established new state-of-the-art level,\nimproving current best results of WSI for the Russian language on two RUSSE\n2018 datasets by a large margin.", "published": "2020-06-23 17:57:25", "link": "http://arxiv.org/abs/2006.13200v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Classifying Referential and Non-referential It Using Gaze", "abstract": "When processing a text, humans and machines must disambiguate between\ndifferent uses of the pronoun it, including non-referential, nominal anaphoric\nor clause anaphoric ones. In this paper, we use eye-tracking data to learn how\nhumans perform this disambiguation. We use this knowledge to improve the\nautomatic classification of it. We show that by using gaze data and a\nPOS-tagger we are able to significantly outperform a common baseline and\nclassify between three categories of it with an accuracy comparable to that of\nlinguisticbased approaches. In addition, the discriminatory power of specific\ngaze features informs the way humans process the pronoun, which, to the best of\nour knowledge, has not been explored using data from a natural reading task.", "published": "2020-06-23 20:54:25", "link": "http://arxiv.org/abs/2006.13327v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "One Model to Pronounce Them All: Multilingual Grapheme-to-Phoneme\n  Conversion With a Transformer Ensemble", "abstract": "The task of grapheme-to-phoneme (G2P) conversion is important for both speech\nrecognition and synthesis. Similar to other speech and language processing\ntasks, in a scenario where only small-sized training data are available,\nlearning G2P models is challenging. We describe a simple approach of exploiting\nmodel ensembles, based on multilingual Transformers and self-training, to\ndevelop a highly effective G2P solution for 15 languages. Our models are\ndeveloped as part of our participation in the SIGMORPHON 2020 Shared Task 1\nfocused at G2P. Our best models achieve 14.99 word error rate (WER) and 3.30\nphoneme error rate (PER), a sizeable improvement over the shared task\ncompetitive baselines.", "published": "2020-06-23 21:28:28", "link": "http://arxiv.org/abs/2006.13343v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Attention-Based Neural Networks for Sentiment Attitude Extraction using\n  Distant Supervision", "abstract": "In the sentiment attitude extraction task, the aim is to identify\n<<attitudes>> -- sentiment relations between entities mentioned in text. In\nthis paper, we provide a study on attention-based context encoders in the\nsentiment attitude extraction task. For this task, we adapt attentive context\nencoders of two types: (1) feature-based; (2) self-based. In our study, we\nutilize the corpus of Russian analytical texts RuSentRel and automatically\nconstructed news collection RuAttitudes for enriching the training set. We\nconsider the problem of attitude extraction as two-class (positive, negative)\nand three-class (positive, negative, neutral) classification tasks for whole\ndocuments. Our experiments with the RuSentRel corpus show that the three-class\nclassification models, which employ the RuAttitudes corpus for training, result\nin 10% increase and extra 3% by F1, when model architectures include the\nattention mechanism. We also provide the analysis of attention weight\ndistributions in dependence on the term type.", "published": "2020-06-23 13:54:48", "link": "http://arxiv.org/abs/2006.13730v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Can you tell? SSNet -- a Sagittal Stratum-inspired Neural Network\n  Framework for Sentiment Analysis", "abstract": "When people try to understand nuanced language they typically process\nmultiple input sensor modalities to complete this cognitive task. It turns out\nthe human brain has even a specialized neuron formation, called sagittal\nstratum, to help us understand sarcasm. We use this biological formation as the\ninspiration for designing a neural network architecture that combines\npredictions of different models on the same text to construct robust, accurate\nand computationally efficient classifiers for sentiment analysis and study\nseveral different realizations. Among them, we propose a systematic new\napproach to combining multiple predictions based on a dedicated neural network\nand develop mathematical analysis of it along with state-of-the-art\nexperimental results. We also propose a heuristic-hybrid technique for\ncombining models and back it up with experimental results on a representative\nbenchmark dataset and comparisons to other methods to show the advantages of\nthe new approaches.", "published": "2020-06-23 12:55:02", "link": "http://arxiv.org/abs/2006.12958v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Automating Text Naturalness Evaluation of NLG Systems", "abstract": "Automatic methods and metrics that assess various quality criteria of\nautomatically generated texts are important for developing NLG systems because\nthey produce repeatable results and allow for a fast development cycle. We\npresent here an attempt to automate the evaluation of text naturalness which is\na very important characteristic of natural language generation methods. Instead\nof relying on human participants for scoring or labeling the text samples, we\npropose to automate the process by using a human likeliness metric we define\nand a discrimination procedure based on large pretrained language models with\ntheir probability distributions. We analyze the text probability fractions and\nobserve how they are influenced by the size of the generative and\ndiscriminative models involved in the process. Based on our results, bigger\ngenerators and larger pretrained discriminators are more appropriate for a\nbetter evaluation of text naturalness. A comprehensive validation procedure\nwith human participants is required as follow up to check how well this\nautomatic evaluation scheme correlates with human judgments.", "published": "2020-06-23 18:48:33", "link": "http://arxiv.org/abs/2006.13268v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Supervised Understanding of Word Embeddings", "abstract": "Pre-trained word embeddings are widely used for transfer learning in natural\nlanguage processing. The embeddings are continuous and distributed\nrepresentations of the words that preserve their similarities in compact\nEuclidean spaces. However, the dimensions of these spaces do not provide any\nclear interpretation. In this study, we have obtained supervised projections in\nthe form of the linear keyword-level classifiers on word embeddings. We have\nshown that the method creates interpretable projections of original embedding\ndimensions. Activations of the trained classifier nodes correspond to a subset\nof the words in the vocabulary. Thus, they behave similarly to the dictionary\nfeatures while having the merit of continuous value output. Additionally, such\ndictionaries can be grown iteratively with multiple rounds by adding expert\nlabels on top-scoring words to an initial collection of the keywords. Also, the\nsame classifiers can be applied to aligned word embeddings in other languages\nto obtain corresponding dictionaries. In our experiments, we have shown that\ninitializing higher-order networks with these classifier weights gives more\naccurate models for downstream NLP tasks. We further demonstrate the usefulness\nof supervised dimensions in revealing the polysemous nature of a keyword of\ninterest by projecting it's embedding using learned classifiers in different\nsub-spaces.", "published": "2020-06-23 20:13:42", "link": "http://arxiv.org/abs/2006.13299v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Neural relation extraction: a survey", "abstract": "Neural relation extraction discovers semantic relations between entities from\nunstructured text using deep learning methods. In this study, we present a\ncomprehensive review of methods on neural network based relation extraction. We\ndiscuss advantageous and incompetent sides of existing studies and investigate\nadditional research directions and improvement ideas in this field.", "published": "2020-06-23 13:47:58", "link": "http://arxiv.org/abs/2007.04247v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unsupervised Evaluation of Interactive Dialog with DialoGPT", "abstract": "It is important to define meaningful and interpretable automatic evaluation\nmetrics for open-domain dialog research. Standard language generation metrics\nhave been shown to be ineffective for dialog. This paper introduces the FED\nmetric (fine-grained evaluation of dialog), an automatic evaluation metric\nwhich uses DialoGPT, without any fine-tuning or supervision. It also introduces\nthe FED dataset which is constructed by annotating a set of human-system and\nhuman-human conversations with eighteen fine-grained dialog qualities. The FED\nmetric (1) does not rely on a ground-truth response, (2) does not require\ntraining data and (3) measures fine-grained dialog qualities at both the turn\nand whole dialog levels. FED attains moderate to strong correlation with human\njudgement at both levels.", "published": "2020-06-23 03:36:09", "link": "http://arxiv.org/abs/2006.12719v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "NLPContributions: An Annotation Scheme for Machine Reading of Scholarly\n  Contributions in Natural Language Processing Literature", "abstract": "We describe an annotation initiative to capture the scholarly contributions\nin natural language processing (NLP) articles, particularly, for the articles\nthat discuss machine learning (ML) approaches for various information\nextraction tasks. We develop the annotation task based on a pilot annotation\nexercise on 50 NLP-ML scholarly articles presenting contributions to five\ninformation extraction tasks 1. machine translation, 2. named entity\nrecognition, 3. question answering, 4. relation classification, and 5. text\nclassification. In this article, we describe the outcomes of this pilot\nannotation phase. Through the exercise we have obtained an annotation\nmethodology; and found ten core information units that reflect the contribution\nof the NLP-ML scholarly investigations. The resulting annotation scheme we\ndeveloped based on these information units is called NLPContributions.\n  The overarching goal of our endeavor is four-fold: 1) to find a systematic\nset of patterns of subject-predicate-object statements for the semantic\nstructuring of scholarly contributions that are more or less generically\napplicable for NLP-ML research articles; 2) to apply the discovered patterns in\nthe creation of a larger annotated dataset for training machine readers of\nresearch contributions; 3) to ingest the dataset into the Open Research\nKnowledge Graph (ORKG) infrastructure as a showcase for creating user-friendly\nstate-of-the-art overviews; 4) to integrate the machine readers into the ORKG\nto assist users in the manual curation of their respective article\ncontributions. We envision that the NLPContributions methodology engenders a\nwider discussion on the topic toward its further refinement and development.\nOur pilot annotated dataset of 50 NLP-ML scholarly articles according to the\nNLPContributions scheme is openly available to the research community at\nhttps://doi.org/10.25835/0019761.", "published": "2020-06-23 10:04:39", "link": "http://arxiv.org/abs/2006.12870v3", "categories": ["cs.CL", "cs.DL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Robot Object Retrieval with Contextual Natural Language Queries", "abstract": "Natural language object retrieval is a highly useful yet challenging task for\nrobots in human-centric environments. Previous work has primarily focused on\ncommands specifying the desired object's type such as \"scissors\" and/or visual\nattributes such as \"red,\" thus limiting the robot to only known object classes.\nWe develop a model to retrieve objects based on descriptions of their usage.\nThe model takes in a language command containing a verb, for example \"Hand me\nsomething to cut,\" and RGB images of candidate objects and selects the object\nthat best satisfies the task specified by the verb. Our model directly predicts\nan object's appearance from the object's use specified by a verb phrase. We do\nnot need to explicitly specify an object's class label. Our approach allows us\nto predict high level concepts like an object's utility based on the language\nquery. Based on contextual information present in the language commands, our\nmodel can generalize to unseen object classes and unknown nouns in the\ncommands. Our model correctly selects objects out of sets of five candidates to\nfulfill natural language commands, and achieves an average accuracy of 62.3% on\na held-out test set of unseen ImageNet object classes and 53.0% on unseen\nobject classes and unknown nouns. Our model also achieves an average accuracy\nof 54.7% on unseen YCB object classes, which have a different image\ndistribution from ImageNet objects. We demonstrate our model on a KUKA LBR iiwa\nrobot arm, enabling the robot to retrieve objects based on natural language\ndescriptions of their usage. We also present a new dataset of 655 verb-object\npairs denoting object usage over 50 verbs and 216 object classes.", "published": "2020-06-23 18:13:40", "link": "http://arxiv.org/abs/2006.13253v1", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.RO"}
{"title": "A Deep Learning Pipeline for Patient Diagnosis Prediction Using\n  Electronic Health Records", "abstract": "Augmentation of disease diagnosis and decision-making in healthcare with\nmachine learning algorithms is gaining much impetus in recent years. In\nparticular, in the current epidemiological situation caused by COVID-19\npandemic, swift and accurate prediction of disease diagnosis with machine\nlearning algorithms could facilitate identification and care of vulnerable\nclusters of population, such as those having multi-morbidity conditions. In\norder to build a useful disease diagnosis prediction system, advancement in\nboth data representation and development of machine learning architectures are\nimperative. First, with respect to data collection and representation, we face\nsevere problems due to multitude of formats and lack of coherency prevalent in\nElectronic Health Records (EHRs). This causes hindrance in extraction of\nvaluable information contained in EHRs. Currently, no universal global data\nstandard has been established. As a useful solution, we develop and publish a\nPython package to transform public health dataset into an easy to access\nuniversal format. This data transformation to an international health data\nformat facilitates researchers to easily combine EHR datasets with clinical\ndatasets of diverse formats. Second, machine learning algorithms that predict\nmultiple disease diagnosis categories simultaneously remain underdeveloped. We\npropose two novel model architectures in this regard. First, DeepObserver,\nwhich uses structured numerical data to predict the diagnosis categories and\nsecond, ClinicalBERT_Multi, that incorporates rich information available in\nclinical notes via natural language processing methods and also provides\ninterpretable visualizations to medical practitioners. We show that both models\ncan predict multiple diagnoses simultaneously with high accuracy.", "published": "2020-06-23 14:58:58", "link": "http://arxiv.org/abs/2006.16926v1", "categories": ["cs.CY", "cs.CL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "Lightweight Online Noise Reduction on Embedded Devices using\n  Hierarchical Recurrent Neural Networks", "abstract": "Deep-learning based noise reduction algorithms have proven their success\nespecially for non-stationary noises, which makes it desirable to also use them\nfor embedded devices like hearing aids (HAs). This, however, is currently not\npossible with state-of-the-art methods. They either require a lot of parameters\nand computational power and thus are only feasible using modern CPUs. Or they\nare not suitable for online processing, which requires constraints like\nlow-latency by the filter bank and the algorithm itself.\n  In this work, we propose a mask-based noise reduction approach. Using\nhierarchical recurrent neural networks, we are able to drastically reduce the\nnumber of neurons per layer while including temporal context via hierarchical\nconnections. This allows us to optimize our model towards a minimum number of\nparameters and floating-point operations (FLOPs), while preserving noise\nreduction quality compared to previous work. Our smallest network contains only\n5k parameters, which makes this algorithm applicable on embedded devices. We\nevaluate our model on a mixture of EUROM and a real-world noise database and\nreport objective metrics on unseen noise.", "published": "2020-06-23 14:41:51", "link": "http://arxiv.org/abs/2006.13067v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "CLC: Complex Linear Coding for the DNS 2020 Challenge", "abstract": "Complex-valued processing brought deep learning-based speech enhancement and\nsignal extraction to a new level.\n  Typically, the noise reduction process is based on a time-frequency (TF) mask\nwhich is applied to a noisy spectrogram. Complex masks (CM) usually outperform\nreal-valued masks due to their ability to modify the phase.\n  Recent work proposed to use a complex linear combination of coefficients\ncalled complex linear coding (CLC) instead of a point-wise multiplication with\na mask.\n  This allows to incorporate information from previous and optionally future\ntime steps which results in superior performance over mask-based enhancement\nfor certain noise conditions.\n  In fact, the linear combination enables to model quasi-steady properties like\nthe spectrum within a frequency band.\n  In this work, we apply CLC to the Deep Noise Suppression (DNS) challenge and\npropose CLC as an alternative to traditional mask-based processing, e.g. used\nby the baseline.\n  We evaluated our models using the provided test set and an additional\nvalidation set with real-world stationary and non-stationary noises.\n  Based on the published test set, we outperform the baseline w.r.t. the scale\nindependent signal distortion ratio (SI-SDR) by about 3dB.", "published": "2020-06-23 14:58:35", "link": "http://arxiv.org/abs/2006.13077v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Unsupervised Sound Separation Using Mixture Invariant Training", "abstract": "In recent years, rapid progress has been made on the problem of\nsingle-channel sound separation using supervised training of deep neural\nnetworks. In such supervised approaches, a model is trained to predict the\ncomponent sources from synthetic mixtures created by adding up isolated\nground-truth sources. Reliance on this synthetic training data is problematic\nbecause good performance depends upon the degree of match between the training\ndata and real-world audio, especially in terms of the acoustic conditions and\ndistribution of sources. The acoustic properties can be challenging to\naccurately simulate, and the distribution of sound types may be hard to\nreplicate. In this paper, we propose a completely unsupervised method, mixture\ninvariant training (MixIT), that requires only single-channel acoustic\nmixtures. In MixIT, training examples are constructed by mixing together\nexisting mixtures, and the model separates them into a variable number of\nlatent sources, such that the separated sources can be remixed to approximate\nthe original mixtures. We show that MixIT can achieve competitive performance\ncompared to supervised methods on speech separation. Using MixIT in a\nsemi-supervised learning setting enables unsupervised domain adaptation and\nlearning from large amounts of real world data without ground-truth source\nwaveforms. In particular, we significantly improve reverberant speech\nseparation performance by incorporating reverberant mixtures, train a speech\nenhancement system from noisy mixtures, and improve universal sound separation\nby incorporating a large amount of in-the-wild data.", "published": "2020-06-23 02:22:14", "link": "http://arxiv.org/abs/2006.12701v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Real Time Speech Enhancement in the Waveform Domain", "abstract": "We present a causal speech enhancement model working on the raw waveform that\nruns in real-time on a laptop CPU. The proposed model is based on an\nencoder-decoder architecture with skip-connections. It is optimized on both\ntime and frequency domains, using multiple loss functions. Empirical evidence\nshows that it is capable of removing various kinds of background noise\nincluding stationary and non-stationary noises, as well as room reverb.\nAdditionally, we suggest a set of data augmentation techniques applied directly\non the raw waveform which further improve model performance and its\ngeneralization abilities. We perform evaluations on several standard\nbenchmarks, both using objective metrics and human judgements. The proposed\nmodel matches state-of-the-art performance of both causal and non causal\nmethods while working directly on the raw waveform.", "published": "2020-06-23 09:19:13", "link": "http://arxiv.org/abs/2006.12847v3", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Bach or Mock? A Grading Function for Chorales in the Style of J.S. Bach", "abstract": "Deep generative systems that learn probabilistic models from a corpus of\nexisting music do not explicitly encode knowledge of a musical style, compared\nto traditional rule-based systems. Thus, it can be difficult to determine\nwhether deep models generate stylistically correct output without expert\nevaluation, but this is expensive and time-consuming. Therefore, there is a\nneed for automatic, interpretable, and musically-motivated evaluation measures\nof generated music. In this paper, we introduce a grading function that\nevaluates four-part chorales in the style of J.S. Bach along important musical\nfeatures. We use the grading function to evaluate the output of a Transformer\nmodel, and show that the function is both interpretable and outperforms human\nexperts at discriminating Bach chorales from model-generated ones.", "published": "2020-06-23 21:02:55", "link": "http://arxiv.org/abs/2006.13329v3", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Incorporating Music Knowledge in Continual Dataset Augmentation for\n  Music Generation", "abstract": "Deep learning has rapidly become the state-of-the-art approach for music\ngeneration. However, training a deep model typically requires a large training\nset, which is often not available for specific musical styles. In this paper,\nwe present augmentative generation (Aug-Gen), a method of dataset augmentation\nfor any music generation system trained on a resource-constrained domain. The\nkey intuition of this method is that the training data for a generative system\ncan be augmented by examples the system produces during the course of training,\nprovided these examples are of sufficiently high quality and variety. We apply\nAug-Gen to Transformer-based chorale generation in the style of J.S. Bach, and\nshow that this allows for longer training and results in better generative\noutput.", "published": "2020-06-23 21:06:15", "link": "http://arxiv.org/abs/2006.13331v4", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "ACOUSTIC-TURF: Acoustic-based Privacy-Preserving COVID-19 Contact\n  Tracing", "abstract": "In this paper, we propose a new privacy-preserving, automated contact tracing\nsystem, ACOUSTIC-TURF, to fight COVID-19 using acoustic signals sent from\nubiquitous mobile devices. At a high level, ACOUSTIC-TURF adaptively broadcasts\ninaudible ultrasonic signals with randomly generated IDs in the vicinity.\nSimultaneously, the system receives other ultrasonic signals sent from nearby\n(e.g., 6 feet) users. In such a system, individual user IDs are not disclosed\nto others and the system can accurately detect encounters in physical proximity\nwith 6-foot granularity. We have implemented a prototype of ACOUSTIC-TURF on\nAndroid and evaluated its performance in terms of acoustic-signal-based\nencounter detection accuracy and power consumption at different ranges and\nunder various occlusion scenarios. Experimental results show that ACOUSTIC-TURF\ncan detect multiple contacts within a 6-foot range for mobile phones placed in\npockets and outside pockets. Furthermore, our acoustic-signal-based system\nachieves greater precision than wireless-signal-based approaches when contact\ntracing is performed through walls. ACOUSTIC-TURF correctly determines that\npeople on opposite sides of a wall are not in contact with one another, whereas\nthe Bluetooth-based approaches detect nonexistent contacts among them.", "published": "2020-06-23 22:17:36", "link": "http://arxiv.org/abs/2006.13362v1", "categories": ["cs.CR", "cs.NI", "cs.SD", "cs.SI", "eess.AS"], "primary_category": "cs.CR"}
{"title": "Audeo: Audio Generation for a Silent Performance Video", "abstract": "We present a novel system that gets as an input video frames of a musician\nplaying the piano and generates the music for that video. Generation of music\nfrom visual cues is a challenging problem and it is not clear whether it is an\nattainable goal at all. Our main aim in this work is to explore the\nplausibility of such a transformation and to identify cues and components able\nto carry the association of sounds with visual events. To achieve the\ntransformation we built a full pipeline named `\\textit{Audeo}' containing three\ncomponents. We first translate the video frames of the keyboard and the\nmusician hand movements into raw mechanical musical symbolic representation\nPiano-Roll (Roll) for each video frame which represents the keys pressed at\neach time step. We then adapt the Roll to be amenable for audio synthesis by\nincluding temporal correlations. This step turns out to be critical for\nmeaningful audio generation. As a last step, we implement Midi synthesizers to\ngenerate realistic music. \\textit{Audeo} converts video to audio smoothly and\nclearly with only a few setup constraints. We evaluate \\textit{Audeo} on `in\nthe wild' piano performance videos and obtain that their generated music is of\nreasonable audio quality and can be successfully recognized with high precision\nby popular music identification software.", "published": "2020-06-23 00:58:59", "link": "http://arxiv.org/abs/2006.14348v1", "categories": ["cs.CV", "cs.LG", "cs.MM", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.CV"}
