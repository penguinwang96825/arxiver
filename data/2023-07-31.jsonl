{"title": "SelfSeg: A Self-supervised Sub-word Segmentation Method for Neural\n  Machine Translation", "abstract": "Sub-word segmentation is an essential pre-processing step for Neural Machine\nTranslation (NMT). Existing work has shown that neural sub-word segmenters are\nbetter than Byte-Pair Encoding (BPE), however, they are inefficient as they\nrequire parallel corpora, days to train and hours to decode. This paper\nintroduces SelfSeg, a self-supervised neural sub-word segmentation method that\nis much faster to train/decode and requires only monolingual dictionaries\ninstead of parallel corpora. SelfSeg takes as input a word in the form of a\npartially masked character sequence, optimizes the word generation probability\nand generates the segmentation with the maximum posterior probability, which is\ncalculated using a dynamic programming algorithm. The training time of SelfSeg\ndepends on word frequencies, and we explore several word frequency\nnormalization strategies to accelerate the training phase. Additionally, we\npropose a regularization mechanism that allows the segmenter to generate\nvarious segmentations for one word. To show the effectiveness of our approach,\nwe conduct MT experiments in low-, middle- and high-resource scenarios, where\nwe compare the performance of using different segmentation methods. The\nexperimental results demonstrate that on the low-resource ALT dataset, our\nmethod achieves more than 1.2 BLEU score improvement compared with BPE and\nSentencePiece, and a 1.1 score improvement over Dynamic Programming Encoding\n(DPE) and Vocabulary Learning via Optimal Transport (VOLT) on average. The\nregularization method achieves approximately a 4.3 BLEU score improvement over\nBPE and a 1.2 BLEU score improvement over BPE-dropout, the regularized version\nof BPE. We also observed significant improvements on IWSLT15 Vi->En, WMT16\nRo->En and WMT15 Fi->En datasets, and competitive results on the WMT14 De->En\nand WMT14 Fr->En datasets.", "published": "2023-07-31 04:38:47", "link": "http://arxiv.org/abs/2307.16400v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Camoscio: an Italian Instruction-tuned LLaMA", "abstract": "In recent years Large Language Models (LLMs) have increased the state of the\nart on several natural language processing tasks. However, their accessibility\nis often limited to paid API services, posing challenges for researchers in\nconducting extensive investigations. On the other hand, while some open-source\nmodels have been proposed by the community, they are typically English-centric\nor multilingual without a specific adaptation for the Italian language. In an\neffort to democratize the available and open resources for the Italian\nlanguage, in this paper we introduce Camoscio: a language model specifically\ntuned to follow users' prompts in Italian. Specifically, we finetuned the\nsmallest variant of LLaMA (7b) with LoRA on a corpus of instruction prompts\ntranslated to Italian via ChatGPT. Results indicate that the model's zero-shot\nperformance on various downstream tasks in Italian competes favorably with\nexisting models specifically finetuned for those tasks. All the artifacts\n(code, dataset, model) are released to the community at the following url:\nhttps://github.com/teelinsan/camoscio", "published": "2023-07-31 07:31:48", "link": "http://arxiv.org/abs/2307.16456v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Benchmark for Understanding Dialogue Safety in Mental Health Support", "abstract": "Dialogue safety remains a pervasive challenge in open-domain human-machine\ninteraction. Existing approaches propose distinctive dialogue safety taxonomies\nand datasets for detecting explicitly harmful responses. However, these\ntaxonomies may not be suitable for analyzing response safety in mental health\nsupport. In real-world interactions, a model response deemed acceptable in\ncasual conversations might have a negligible positive impact on users seeking\nmental health support. To address these limitations, this paper aims to develop\na theoretically and factually grounded taxonomy that prioritizes the positive\nimpact on help-seekers. Additionally, we create a benchmark corpus with\nfine-grained labels for each dialogue session to facilitate further research.\nWe analyze the dataset using popular language models, including BERT-base,\nRoBERTa-large, and ChatGPT, to detect and understand unsafe responses within\nthe context of mental health support. Our study reveals that ChatGPT struggles\nto detect safety categories with detailed safety definitions in a zero- and\nfew-shot paradigm, whereas the fine-tuned model proves to be more suitable. The\ndeveloped dataset and findings serve as valuable benchmarks for advancing\nresearch on dialogue safety in mental health support, with significant\nimplications for improving the design and deployment of conversation agents in\nreal-world applications. We release our code and data here:\nhttps://github.com/qiuhuachuan/DialogueSafety.", "published": "2023-07-31 07:33:16", "link": "http://arxiv.org/abs/2307.16457v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Utilisation of open intent recognition models for customer support\n  intent detection", "abstract": "Businesses have sought out new solutions to provide support and improve\ncustomer satisfaction as more products and services have become interconnected\ndigitally. There is an inherent need for businesses to provide or outsource\nfast, efficient and knowledgeable support to remain competitive. Support\nsolutions are also advancing with technologies, including use of social media,\nArtificial Intelligence (AI), Machine Learning (ML) and remote device\nconnectivity to better support customers. Customer support operators are\ntrained to utilise these technologies to provide better customer outreach and\nsupport for clients in remote areas. Interconnectivity of products and support\nsystems provide businesses with potential international clients to expand their\nproduct market and business scale. This paper reports the possible AI\napplications in customer support, done in collaboration with the Knowledge\nTransfer Partnership (KTP) program between Birmingham City University and a\ncompany that handles customer service systems for businesses outsourcing\ncustomer support across a wide variety of business sectors. This study explored\nseveral approaches to accurately predict customers' intent using both labelled\nand unlabelled textual data. While some approaches showed promise in specific\ndatasets, the search for a single, universally applicable approach continues.\nThe development of separate pipelines for intent detection and discovery has\nled to improved accuracy rates in detecting known intents, while further work\nis required to improve the accuracy of intent discovery for unknown intents.", "published": "2023-07-31 10:20:16", "link": "http://arxiv.org/abs/2307.16544v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Specification of MiniDemographicABM.jl: A simplified agent-based\n  demographic model of the UK", "abstract": "This documentation specifies a simplified non-calibrated demographic\nagent-based model of the UK, a largely simplified version of the Lone Parent\nModel presented in [Gostolil and Silverman 2020]. In the presented model,\nindividuals of an initial population are subject to ageing, deaths, births,\ndivorces and marriages throughout a simplified map of towns of the UK. The\nspecification employs the formal terminology presented in [Elsheikh 2023a]. The\nmain purpose of the model is to explore and exploit capabilities of the\nstate-of-the-art Agents.jl Julia package [Datseris2022] in the context of\ndemographic modeling applications. Implementation is provided via the Julia\npackage MiniDemographicABM.jl [Elsheikh 2023b]. A specific simulation is\nprogressed with a user-defined simulation fixed step size on a hourly, daily,\nweekly, monthly basis or even an arbitrary user-defined clock rate. The model\ncan serve for comparative studies if implemented in other agent-based modelling\nframeworks and programming languages. Moreover, the model serves as a base\nimplementation to be adjusted to realistic large-scale socio-economics,\npandemics or immigration studies mainly within a demographic context.", "published": "2023-07-31 10:28:23", "link": "http://arxiv.org/abs/2307.16548v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep Dive into the Language of International Relations: NLP-based\n  Analysis of UNESCO's Summary Records", "abstract": "Cultural heritage is an arena of international relations that interests all\nstates worldwide. The inscription process on the UNESCO World Heritage List and\nthe UNESCO Representative List of the Intangible Cultural Heritage of Humanity\noften leads to tensions and conflicts among states. This research addresses\nthese challenges by developing automatic tools that provide valuable insights\ninto the decision-making processes regarding inscriptions to the two lists\nmentioned above. We propose innovative topic modelling and tension detection\nmethods based on UNESCO's summary records. Our analysis achieved a commendable\naccuracy rate of 72% in identifying tensions. Furthermore, we have developed an\napplication tailored for diplomats, lawyers, political scientists, and\ninternational relations researchers that facilitates the efficient search of\nparagraphs from selected documents and statements from specific speakers about\nchosen topics. This application is a valuable resource for enhancing the\nunderstanding of complex decision-making dynamics within international heritage\ninscription procedures.", "published": "2023-07-31 11:06:08", "link": "http://arxiv.org/abs/2307.16573v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "VacancySBERT: the approach for representation of titles and skills for\n  semantic similarity search in the recruitment domain", "abstract": "The paper focuses on deep learning semantic search algorithms applied in the\nHR domain. The aim of the article is developing a novel approach to training a\nSiamese network to link the skills mentioned in the job ad with the title. It\nhas been shown that the title normalization process can be based either on\nclassification or similarity comparison approaches. While classification\nalgorithms strive to classify a sample into predefined set of categories,\nsimilarity search algorithms take a more flexible approach, since they are\ndesigned to find samples that are similar to a given query sample, without\nrequiring pre-defined classes and labels. In this article semantic similarity\nsearch to find candidates for title normalization has been used. A pre-trained\nlanguage model has been adapted while teaching it to match titles and skills\nbased on co-occurrence information. For the purpose of this research fifty\nbillion title-descriptions pairs had been collected for training the model and\nthirty three thousand title-description-normalized title triplets, where\nnormalized job title was picked up manually by job ad creator for testing\npurposes. As baselines FastText, BERT, SentenceBert and JobBert have been used.\nAs a metric of the accuracy of the designed algorithm is Recall in top one,\nfive and ten model's suggestions. It has been shown that the novel training\nobjective lets it achieve significant improvement in comparison to other\ngeneric and specific text encoders. Two settings with treating titles as\nstandalone strings, and with included skills as additional features during\ninference have been used and the results have been compared in this article.\nImprovements by 10% and 21.5% have been achieved using VacancySBERT and\nVacancySBERT (with skills) respectively. The benchmark has been developed as\nopen-source to foster further research in the area.", "published": "2023-07-31 13:21:15", "link": "http://arxiv.org/abs/2307.16638v1", "categories": ["cs.CL", "68T50"], "primary_category": "cs.CL"}
{"title": "Scaling Sentence Embeddings with Large Language Models", "abstract": "Large language models (LLMs) have recently garnered significant interest.\nWith in-context learning, LLMs achieve impressive results in various natural\nlanguage tasks. However, the application of LLMs to sentence embeddings remains\nan area of ongoing research. In this work, we propose an in-context\nlearning-based method aimed at improving sentence embeddings performance. Our\napproach involves adapting the previous prompt-based representation method for\nautoregressive models, constructing a demonstration set that enables LLMs to\nperform in-context learning, and scaling up the LLMs to different model sizes.\nThrough extensive experiments, in-context learning enables LLMs to generate\nhigh-quality sentence embeddings without any fine-tuning. It helps LLMs achieve\nperformance comparable to current contrastive learning methods. By scaling\nmodel size, we find scaling to more than tens of billion parameters harms the\nperformance on semantic textual similarity (STS) tasks. However, the largest\nmodel outperforms other counterparts and achieves the new state-of-the-art\nresult on transfer tasks. We also fine-tune LLMs with current contrastive\nlearning approach, and the 2.7B OPT model, incorporating our prompt-based\nmethod, surpasses the performance of 4.8B ST5, achieving the new\nstate-of-the-art results on STS tasks. Our code is available at\nhttps://github.com/kongds/scaling_sentemb.", "published": "2023-07-31 13:26:03", "link": "http://arxiv.org/abs/2307.16645v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "No that's not what I meant: Handling Third Position Repair in\n  Conversational Question Answering", "abstract": "The ability to handle miscommunication is crucial to robust and faithful\nconversational AI. People usually deal with miscommunication immediately as\nthey detect it, using highly systematic interactional mechanisms called repair.\nOne important type of repair is Third Position Repair (TPR) whereby a speaker\nis initially misunderstood but then corrects the misunderstanding as it becomes\napparent after the addressee's erroneous response. Here, we collect and\npublicly release Repair-QA, the first large dataset of TPRs in a conversational\nquestion answering (QA) setting. The data is comprised of the TPR turns,\ncorresponding dialogue contexts, and candidate repairs of the original turn for\nexecution of TPRs. We demonstrate the usefulness of the data by training and\nevaluating strong baseline models for executing TPRs. For stand-alone TPR\nexecution, we perform both automatic and human evaluations on a fine-tuned T5\nmodel, as well as OpenAI's GPT-3 LLMs. Additionally, we extrinsically evaluate\nthe LLMs' TPR processing capabilities in the downstream conversational QA task.\nThe results indicate poor out-of-the-box performance on TPR's by the GPT-3\nmodels, which then significantly improves when exposed to Repair-QA.", "published": "2023-07-31 14:02:45", "link": "http://arxiv.org/abs/2307.16689v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Boosting Adverse Drug Event Normalization on Social Media:\n  General-Purpose Model Initialization and Biomedical Semantic Text Similarity\n  Benefit Zero-Shot Linking in Informal Contexts", "abstract": "Biomedical entity linking, also known as biomedical concept normalization,\nhas recently witnessed the rise to prominence of zero-shot contrastive models.\nHowever, the pre-training material used for these models has, until now,\nlargely consisted of specialist biomedical content such as MIMIC-III clinical\nnotes (Johnson et al., 2016) and PubMed papers (Sayers et al., 2021; Gao et\nal., 2020). While the resulting in-domain models have shown promising results\nfor many biomedical tasks, adverse drug event normalization on social media\ntexts has so far remained challenging for them (Portelli et al., 2022). In this\npaper, we propose a new approach for adverse drug event normalization on social\nmedia relying on general-purpose model initialization via BioLORD (Remy et al.,\n2022) and a semantic-text-similarity fine-tuning named STS. Our experimental\nresults on several social media datasets demonstrate the effectiveness of our\nproposed approach, by achieving state-of-the-art performance. Based on its\nstrong performance across all the tested datasets, we believe this work could\nemerge as a turning point for the task of adverse drug event normalization on\nsocial media and has the potential to serve as a benchmark for future research\nin the field.", "published": "2023-07-31 21:12:06", "link": "http://arxiv.org/abs/2308.00157v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FinVis-GPT: A Multimodal Large Language Model for Financial Chart\n  Analysis", "abstract": "In this paper, we propose FinVis-GPT, a novel multimodal large language model\n(LLM) specifically designed for financial chart analysis. By leveraging the\npower of LLMs and incorporating instruction tuning and multimodal capabilities,\nFinVis-GPT is capable of interpreting financial charts and providing valuable\nanalysis. To train FinVis-GPT, a financial task oriented dataset was generated\nfor pre-training alignment and instruction tuning, comprising various types of\nfinancial charts and their corresponding descriptions. We evaluate the model\nperformance via several case studies due to the time limit, and the promising\nresults demonstrated that FinVis-GPT is superior in various financial chart\nrelated tasks, including generating descriptions, answering questions and\npredicting future market trends, surpassing existing state-of-the-art\nmultimodal LLMs. The proposed FinVis-GPT serves as a pioneering effort in\nutilizing multimodal LLMs in the finance domain and our generated dataset will\nbe release for public use in the near future to speedup related research.", "published": "2023-07-31 07:44:15", "link": "http://arxiv.org/abs/2308.01430v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Does fine-tuning GPT-3 with the OpenAI API leak personally-identifiable\n  information?", "abstract": "Machine learning practitioners often fine-tune generative pre-trained models\nlike GPT-3 to improve model performance at specific tasks. Previous works,\nhowever, suggest that fine-tuned machine learning models memorize and emit\nsensitive information from the original fine-tuning dataset. Companies such as\nOpenAI offer fine-tuning services for their models, but no prior work has\nconducted a memorization attack on any closed-source models. In this work, we\nsimulate a privacy attack on GPT-3 using OpenAI's fine-tuning API. Our\nobjective is to determine if personally identifiable information (PII) can be\nextracted from this model. We (1) explore the use of naive prompting methods on\na GPT-3 fine-tuned classification model, and (2) we design a practical word\ngeneration task called Autocomplete to investigate the extent of PII\nmemorization in fine-tuned GPT-3 within a real-world context. Our findings\nreveal that fine-tuning GPT3 for both tasks led to the model memorizing and\ndisclosing critical personally identifiable information (PII) obtained from the\nunderlying fine-tuning dataset. To encourage further research, we have made our\ncodes and datasets publicly available on GitHub at:\nhttps://github.com/albertsun1/gpt3-pii-attacks", "published": "2023-07-31 03:17:51", "link": "http://arxiv.org/abs/2307.16382v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Transferable Decoding with Visual Entities for Zero-Shot Image\n  Captioning", "abstract": "Image-to-text generation aims to describe images using natural language.\nRecently, zero-shot image captioning based on pre-trained vision-language\nmodels (VLMs) and large language models (LLMs) has made significant progress.\nHowever, we have observed and empirically demonstrated that these methods are\nsusceptible to modality bias induced by LLMs and tend to generate descriptions\ncontaining objects (entities) that do not actually exist in the image but\nfrequently appear during training (i.e., object hallucination). In this paper,\nwe propose ViECap, a transferable decoding model that leverages entity-aware\ndecoding to generate descriptions in both seen and unseen scenarios. ViECap\nincorporates entity-aware hard prompts to guide LLMs' attention toward the\nvisual entities present in the image, enabling coherent caption generation\nacross diverse scenes. With entity-aware hard prompts, ViECap is capable of\nmaintaining performance when transferring from in-domain to out-of-domain\nscenarios. Extensive experiments demonstrate that ViECap sets a new\nstate-of-the-art cross-domain (transferable) captioning and performs\ncompetitively in-domain captioning compared to previous VLMs-based zero-shot\nmethods. Our code is available at: https://github.com/FeiElysia/ViECap", "published": "2023-07-31 09:47:06", "link": "http://arxiv.org/abs/2307.16525v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Toward Quantum Machine Translation of Syntactically Distinct Languages", "abstract": "The present study aims to explore the feasibility of language translation\nusing quantum natural language processing algorithms on noisy\nintermediate-scale quantum (NISQ) devices. Classical methods in natural\nlanguage processing (NLP) struggle with handling large-scale computations\nrequired for complex language tasks, but quantum NLP on NISQ devices holds\npromise in harnessing quantum parallelism and entanglement to efficiently\nprocess and analyze vast amounts of linguistic data, potentially\nrevolutionizing NLP applications. Our research endeavors to pave the way for\nquantum neural machine translation, which could potentially offer advantages\nover classical methods in the future. We employ Shannon entropy to demonstrate\nthe significant role of some appropriate angles of rotation gates in the\nperformance of parametrized quantum circuits. In particular, we utilize these\nangles (parameters) as a means of communication between quantum circuits of\ndifferent languages. To achieve our objective, we adopt the encoder-decoder\nmodel of classical neural networks and implement the translation task using\nlong short-term memory (LSTM). Our experiments involved 160 samples comprising\nEnglish sentences and their Persian translations. We trained the models with\ndifferent optimisers implementing stochastic gradient descent (SGD) as primary\nand subsequently incorporating two additional optimizers in conjunction with\nSGD. Notably, we achieved optimal results-with mean absolute error of 0.03,\nmean squared error of 0.002, and 0.016 loss-by training the best model,\nconsisting of two LSTM layers and using the Adam optimiser. Our small dataset,\nthough consisting of simple synonymous sentences with word-to-word mappings,\npoints to the utility of Shannon entropy as a figure of merit in more complex\nmachine translation models for intricate sentence structures.", "published": "2023-07-31 11:24:54", "link": "http://arxiv.org/abs/2307.16576v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving grapheme-to-phoneme conversion by learning pronunciations from\n  speech recordings", "abstract": "The Grapheme-to-Phoneme (G2P) task aims to convert orthographic input into a\ndiscrete phonetic representation. G2P conversion is beneficial to various\nspeech processing applications, such as text-to-speech and speech recognition.\nHowever, these tend to rely on manually-annotated pronunciation dictionaries,\nwhich are often time-consuming and costly to acquire. In this paper, we propose\na method to improve the G2P conversion task by learning pronunciation examples\nfrom audio recordings. Our approach bootstraps a G2P with a small set of\nannotated examples. The G2P model is used to train a multilingual phone\nrecognition system, which then decodes speech recordings with a phonetic\nrepresentation. Given hypothesized phoneme labels, we learn pronunciation\ndictionaries for out-of-vocabulary words, and we use those to re-train the G2P\nsystem. Results indicate that our approach consistently improves the phone\nerror rate of G2P systems across languages and amount of available data.", "published": "2023-07-31 13:25:38", "link": "http://arxiv.org/abs/2307.16643v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "The World Literature Knowledge Graph", "abstract": "Digital media have enabled the access to unprecedented literary knowledge.\nAuthors, readers, and scholars are now able to discover and share an increasing\namount of information about books and their authors. However, these sources of\nknowledge are fragmented and do not adequately represent non-Western writers\nand their works. In this paper we present The World Literature Knowledge Graph,\na semantic resource containing 194,346 writers and 965,210 works, specifically\ndesigned for exploring facts about literary works and authors from different\nparts of the world. The knowledge graph integrates information about the\nreception of literary works gathered from 3 different communities of readers,\naligned according to a single semantic model. The resource is accessible\nthrough an online visualization platform, which can be found at the following\nURL: https://literaturegraph.di.unito.it/. This platform has been rigorously\ntested and validated by $3$ distinct categories of experts who have found it to\nbe highly beneficial for their respective work domains. These categories\ninclude teachers, researchers in the humanities, and professionals in the\npublishing industry. The feedback received from these experts confirms that\nthey can effectively utilize the platform to enhance their work processes and\nachieve valuable outcomes.", "published": "2023-07-31 13:41:31", "link": "http://arxiv.org/abs/2307.16659v1", "categories": ["cs.DL", "cs.CL"], "primary_category": "cs.DL"}
{"title": "Ontology engineering with Large Language Models", "abstract": "We tackle the task of enriching ontologies by automatically translating\nnatural language sentences into Description Logic. Since Large Language Models\n(LLMs) are the best tools for translations, we fine-tuned a GPT-3 model to\nconvert Natural Language sentences into OWL Functional Syntax. We employ\nobjective and concise examples to fine-tune the model regarding: instances,\nclass subsumption, domain and range of relations, object properties\nrelationships, disjoint classes, complements, cardinality restrictions. The\nresulted axioms are used to enrich an ontology, in a human supervised manner.\nThe developed tool is publicly provided as a Protge plugin.", "published": "2023-07-31 14:18:23", "link": "http://arxiv.org/abs/2307.16699v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Multilingual context-based pronunciation learning for Text-to-Speech", "abstract": "Phonetic information and linguistic knowledge are an essential component of a\nText-to-speech (TTS) front-end. Given a language, a lexicon can be collected\noffline and Grapheme-to-Phoneme (G2P) relationships are usually modeled in\norder to predict the pronunciation for out-of-vocabulary (OOV) words.\nAdditionally, post-lexical phonology, often defined in the form of rule-based\nsystems, is used to correct pronunciation within or between words. In this work\nwe showcase a multilingual unified front-end system that addresses any\npronunciation related task, typically handled by separate modules. We evaluate\nthe proposed model on G2P conversion and other language-specific challenges,\nsuch as homograph and polyphones disambiguation, post-lexical rules and\nimplicit diacritization. We find that the multilingual model is competitive\nacross languages and tasks, however, some trade-offs exists when compared to\nequivalent monolingual solutions.", "published": "2023-07-31 14:29:06", "link": "http://arxiv.org/abs/2307.16709v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "KoBBQ: Korean Bias Benchmark for Question Answering", "abstract": "The Bias Benchmark for Question Answering (BBQ) is designed to evaluate\nsocial biases of language models (LMs), but it is not simple to adapt this\nbenchmark to cultural contexts other than the US because social biases depend\nheavily on the cultural context. In this paper, we present KoBBQ, a Korean bias\nbenchmark dataset, and we propose a general framework that addresses\nconsiderations for cultural adaptation of a dataset. Our framework includes\npartitioning the BBQ dataset into three classes--Simply-Transferred (can be\nused directly after cultural translation), Target-Modified (requires\nlocalization in target groups), and Sample-Removed (does not fit Korean\nculture)-- and adding four new categories of bias specific to Korean culture.\nWe conduct a large-scale survey to collect and validate the social biases and\nthe targets of the biases that reflect the stereotypes in Korean culture. The\nresulting KoBBQ dataset comprises 268 templates and 76,048 samples across 12\ncategories of social bias. We use KoBBQ to measure the accuracy and bias scores\nof several state-of-the-art multilingual LMs. The results clearly show\ndifferences in the bias of LMs as measured by KoBBQ and a machine-translated\nversion of BBQ, demonstrating the need for and utility of a well-constructed,\nculturally-aware social bias benchmark.", "published": "2023-07-31 15:44:15", "link": "http://arxiv.org/abs/2307.16778v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Lexically-Accelerated Dense Retrieval", "abstract": "Retrieval approaches that score documents based on learned dense vectors\n(i.e., dense retrieval) rather than lexical signals (i.e., conventional\nretrieval) are increasingly popular. Their ability to identify related\ndocuments that do not necessarily contain the same terms as those appearing in\nthe user's query (thereby improving recall) is one of their key advantages.\nHowever, to actually achieve these gains, dense retrieval approaches typically\nrequire an exhaustive search over the document collection, making them\nconsiderably more expensive at query-time than conventional lexical approaches.\nSeveral techniques aim to reduce this computational overhead by approximating\nthe results of a full dense retriever. Although these approaches reasonably\napproximate the top results, they suffer in terms of recall -- one of the key\nadvantages of dense retrieval. We introduce 'LADR' (Lexically-Accelerated Dense\nRetrieval), a simple-yet-effective approach that improves the efficiency of\nexisting dense retrieval models without compromising on retrieval\neffectiveness. LADR uses lexical retrieval techniques to seed a dense retrieval\nexploration that uses a document proximity graph. We explore two variants of\nLADR: a proactive approach that expands the search space to the neighbors of\nall seed documents, and an adaptive approach that selectively searches the\ndocuments with the highest estimated relevance in an iterative fashion. Through\nextensive experiments across a variety of dense retrieval models, we find that\nLADR establishes a new dense retrieval effectiveness-efficiency Pareto frontier\namong approximate k nearest neighbor techniques. Further, we find that when\ntuned to take around 8ms per query in retrieval latency on our hardware, LADR\nconsistently achieves both precision and recall that are on par with an\nexhaustive search on standard benchmarks.", "published": "2023-07-31 15:44:26", "link": "http://arxiv.org/abs/2307.16779v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "DoDo Learning: DOmain-DemOgraphic Transfer in Language Models for\n  Detecting Abuse Targeted at Public Figures", "abstract": "Public figures receive a disproportionate amount of abuse on social media,\nimpacting their active participation in public life. Automated systems can\nidentify abuse at scale but labelling training data is expensive, complex and\npotentially harmful. So, it is desirable that systems are efficient and\ngeneralisable, handling both shared and specific aspects of online abuse. We\nexplore the dynamics of cross-group text classification in order to understand\nhow well classifiers trained on one domain or demographic can transfer to\nothers, with a view to building more generalisable abuse classifiers. We\nfine-tune language models to classify tweets targeted at public figures across\nDOmains (sport and politics) and DemOgraphics (women and men) using our novel\nDODO dataset, containing 28,000 labelled entries, split equally across four\ndomain-demographic pairs. We find that (i) small amounts of diverse data are\nhugely beneficial to generalisation and model adaptation; (ii) models transfer\nmore easily across demographics but models trained on cross-domain data are\nmore generalisable; (iii) some groups contribute more to generalisability than\nothers; and (iv) dataset similarity is a signal of transferability.", "published": "2023-07-31 16:29:08", "link": "http://arxiv.org/abs/2307.16811v3", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Defense of Adversarial Ranking Attack in Text Retrieval: Benchmark and\n  Baseline via Detection", "abstract": "Neural ranking models (NRMs) have undergone significant development and have\nbecome integral components of information retrieval (IR) systems.\nUnfortunately, recent research has unveiled the vulnerability of NRMs to\nadversarial document manipulations, potentially exploited by malicious search\nengine optimization practitioners. While progress in adversarial attack\nstrategies aids in identifying the potential weaknesses of NRMs before their\ndeployment, the defensive measures against such attacks, like the detection of\nadversarial documents, remain inadequately explored. To mitigate this gap, this\npaper establishes a benchmark dataset to facilitate the investigation of\nadversarial ranking defense and introduces two types of detection tasks for\nadversarial documents. A comprehensive investigation of the performance of\nseveral detection baselines is conducted, which involve examining the\nspamicity, perplexity, and linguistic acceptability, and utilizing supervised\nclassifiers. Experimental results demonstrate that a supervised classifier can\neffectively mitigate known attacks, but it performs poorly against unseen\nattacks. Furthermore, such classifier should avoid using query text to prevent\nlearning the classification on relevance, as it might lead to the inadvertent\ndiscarding of relevant documents.", "published": "2023-07-31 16:31:24", "link": "http://arxiv.org/abs/2307.16816v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Evaluating Correctness and Faithfulness of Instruction-Following Models\n  for Question Answering", "abstract": "Retriever-augmented instruction-following models are attractive alternatives\nto fine-tuned approaches for information-seeking tasks such as question\nanswering (QA). By simply prepending retrieved documents in its input along\nwith an instruction, these models can be adapted to various information domains\nand tasks without additional fine-tuning. While the model responses tend to be\nnatural and fluent, the additional verbosity makes traditional QA evaluation\nmetrics such as exact match (EM) and F1 unreliable for accurately quantifying\nmodel performance.\n  In this work, we investigate the performance of instruction-following models\nacross three information-seeking QA tasks. We use both automatic and human\nevaluation to evaluate these models along two dimensions: 1) how well they\nsatisfy the user's information need (correctness), and 2) whether they produce\na response based on the provided knowledge (faithfulness). Guided by human\nevaluation and analysis, we highlight the shortcomings of traditional metrics\nfor both correctness and faithfulness. We then propose simple token-overlap\nbased and model-based metrics that reflect the true performance of these\nmodels. Our analysis reveals that instruction-following models are competitive,\nand sometimes even outperform fine-tuned models for correctness. However, these\nmodels struggle to stick to the provided knowledge and often hallucinate in\ntheir responses. We hope our work encourages a more holistic evaluation of\ninstruction-following models for QA. Our code and data is available at\nhttps://github.com/McGill-NLP/instruct-qa", "published": "2023-07-31 17:41:00", "link": "http://arxiv.org/abs/2307.16877v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "HAGRID: A Human-LLM Collaborative Dataset for Generative\n  Information-Seeking with Attribution", "abstract": "The rise of large language models (LLMs) had a transformative impact on\nsearch, ushering in a new era of search engines that are capable of generating\nsearch results in natural language text, imbued with citations for supporting\nsources. Building generative information-seeking models demands openly\naccessible datasets, which currently remain lacking. In this paper, we\nintroduce a new dataset, HAGRID (Human-in-the-loop Attributable Generative\nRetrieval for Information-seeking Dataset) for building end-to-end generative\ninformation-seeking models that are capable of retrieving candidate quotes and\ngenerating attributed explanations. Unlike recent efforts that focus on human\nevaluation of black-box proprietary search engines, we built our dataset atop\nthe English subset of MIRACL, a publicly available information retrieval\ndataset. HAGRID is constructed based on human and LLM collaboration. We first\nautomatically collect attributed explanations that follow an in-context\ncitation style using an LLM, i.e. GPT-3.5. Next, we ask human annotators to\nevaluate the LLM explanations based on two criteria: informativeness and\nattributability. HAGRID serves as a catalyst for the development of\ninformation-seeking models with better attribution capabilities.", "published": "2023-07-31 17:49:18", "link": "http://arxiv.org/abs/2307.16883v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Towards Semantically Enriched Embeddings for Knowledge Graph Completion", "abstract": "Embedding based Knowledge Graph (KG) Completion has gained much attention\nover the past few years. Most of the current algorithms consider a KG as a\nmultidirectional labeled graph and lack the ability to capture the semantics\nunderlying the schematic information. In a separate development, a vast amount\nof information has been captured within the Large Language Models (LLMs) which\nhas revolutionized the field of Artificial Intelligence. KGs could benefit from\nthese LLMs and vice versa. This vision paper discusses the existing algorithms\nfor KG completion based on the variations for generating KG embeddings. It\nstarts with discussing various KG completion algorithms such as transductive\nand inductive link prediction and entity type prediction algorithms. It then\nmoves on to the algorithms utilizing type information within the KGs, LLMs, and\nfinally to algorithms capturing the semantics represented in different\ndescription logic axioms. We conclude the paper with a critical reflection on\nthe current state of work in the community and give recommendations for future\ndirections.", "published": "2023-07-31 18:53:47", "link": "http://arxiv.org/abs/2308.00081v4", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "A Modular Ontology for MODS -- Metadata Object Description Schema", "abstract": "The Metadata Object Description Schema (MODS) was developed to describe\nbibliographic concepts and metadata and is maintained by the Library of\nCongress. Its authoritative version is given as an XML schema based on an XML\nmindset which means that it has significant limitations for use in a knowledge\ngraphs context. We have therefore developed the Modular MODS Ontology (MMODS-O)\nwhich incorporates all elements and attributes of the MODS XML schema. In\ndesigning the ontology, we adopt the recent Modular Ontology Design Methodology\n(MOMo) with the intention to strike a balance between modularity and quality\nontology design on the one hand, and conservative backward compatibility with\nMODS on the other.", "published": "2023-07-31 19:36:07", "link": "http://arxiv.org/abs/2308.00116v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MTUncertainty: Assessing the Need for Post-editing of Machine\n  Translation Outputs by Fine-tuning OpenAI LLMs", "abstract": "Translation Quality Evaluation (TQE) is an essential step of the modern\ntranslation production process. TQE is critical in assessing both machine\ntranslation (MT) and human translation (HT) quality without reference\ntranslations. The ability to evaluate or even simply estimate the quality of\ntranslation automatically may open significant efficiency gains through process\noptimisation. This work examines whether the state-of-the-art large language\nmodels (LLMs) can be used for this purpose. We take OpenAI models as the best\nstate-of-the-art technology and approach TQE as a binary classification task.\nOn eight language pairs including English to Italian, German, French, Japanese,\nDutch, Portuguese, Turkish, and Chinese, our experimental results show that\nfine-tuned gpt3.5 can demonstrate good performance on translation quality\nprediction tasks, i.e. whether the translation needs to be edited. Another\nfinding is that simply increasing the sizes of LLMs does not lead to apparent\nbetter performances on this task by comparing the performance of three\ndifferent versions of OpenAI models: curie, davinci, and gpt3.5 with 13B, 175B,\nand 175B parameters, respectively.", "published": "2023-07-31 21:13:30", "link": "http://arxiv.org/abs/2308.00158v6", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Adversarially Robust Neural Legal Judgement Systems", "abstract": "Legal judgment prediction is the task of predicting the outcome of court\ncases on a given text description of facts of cases. These tasks apply Natural\nLanguage Processing (NLP) techniques to predict legal judgment results based on\nfacts. Recently, large-scale public datasets and NLP models have increased\nresearch in areas related to legal judgment prediction systems. For such\nsystems to be practically helpful, they should be robust from adversarial\nattacks. Previous works mainly focus on making a neural legal judgement system;\nhowever, significantly less or no attention has been given to creating a robust\nLegal Judgement Prediction(LJP) system. We implemented adversarial attacks on\nearly existing LJP systems and found that none of them could handle attacks. In\nthis work, we proposed an approach for making robust LJP systems. Extensive\nexperiments on three legal datasets show significant improvements in our\napproach over the state-of-the-art LJP system in handling adversarial attacks.\nTo the best of our knowledge, we are the first to increase the robustness of\nearly-existing LJP systems.", "published": "2023-07-31 21:44:48", "link": "http://arxiv.org/abs/2308.00165v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "HouYi: An open-source large language model specially designed for\n  renewable energy and carbon neutrality field", "abstract": "Renewable energy is important for achieving carbon neutrality goal. With the\ngreat success of Large Language Models (LLMs) like ChatGPT in automatic content\ngeneration, LLMs are playing an increasingly important role. However, there has\nnot been a specially designed LLM for renewable energy. Meanwhile, there has\nnot been any dataset of renewable energy for training LLMs. Therefore, this\npaper published the first open-source Renewable Energy Academic Paper (REAP)\ndataset for non-commercial LLM research of renewable energy. REAP dataset is\ncollected through searching the title and abstract of 1,168,970 academic\nliteratures from Web of Science. Based on REAP dataset, HouYi model, the first\nLLM for renewable energy, is developed through finetuning general LLMs. HouYi\ndemonstrated powerful academic paper paragraph generation ability in renewable\nenergy field. Experiments show that its ability to generate academic papers on\nrenewable energy is comparable to ChatGPT, slightly outperforms Claude, ERNIE\nBot and SparkDesk, and significantly outperforms open-source LLaMA-13B model.", "published": "2023-07-31 06:59:36", "link": "http://arxiv.org/abs/2308.01414v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Chatbot Application to Support Smart Agriculture in Thailand", "abstract": "A chatbot is a software developed to help reply to text or voice\nconversations automatically and quickly in real time. In the agriculture\nsector, the existing smart agriculture systems just use data from sensing and\ninternet of things (IoT) technologies that exclude crop cultivation knowledge\nto support decision-making by farmers. To enhance this, the chatbot application\ncan be an assistant to farmers to provide crop cultivation knowledge.\nConsequently, we propose the LINE chatbot application as an information and\nknowledge representation providing crop cultivation recommendations to farmers.\nIt works with smart agriculture and recommendation systems. Our proposed LINE\nchatbot application consists of five main functions (start/stop menu, main\npage, drip irri gation page, mist irrigation page, and monitor page). Farmers\nwill receive information for data monitoring to support their decision-making.\nMoreover, they can control the irrigation system via the LINE chatbot.\nFurthermore, farmers can ask questions relevant to the crop environment via a\nchat box. After implementing our proposed chatbot, farmers are very satisfied\nwith the application, scoring a 96% satisfaction score. However, in terms of\nasking questions via chat box, this LINE chatbot application is a rule-based\nbot or script bot. Farmers have to type in the correct keywords as prescribed,\notherwise they won't get a response from the chatbots. In the future, we will\nenhance the asking function of our LINE chatbot to be an intelligent bot.", "published": "2023-07-31 11:42:44", "link": "http://arxiv.org/abs/2308.02524v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "When Large Language Models Meet Personalization: Perspectives of\n  Challenges and Opportunities", "abstract": "The advent of large language models marks a revolutionary breakthrough in\nartificial intelligence. With the unprecedented scale of training and model\nparameters, the capability of large language models has been dramatically\nimproved, leading to human-like performances in understanding, language\nsynthesizing, and common-sense reasoning, etc. Such a major leap-forward in\ngeneral AI capacity will change the pattern of how personalization is\nconducted. For one thing, it will reform the way of interaction between humans\nand personalization systems. Instead of being a passive medium of information\nfiltering, large language models present the foundation for active user\nengagement. On top of such a new foundation, user requests can be proactively\nexplored, and user's required information can be delivered in a natural and\nexplainable way. For another thing, it will also considerably expand the scope\nof personalization, making it grow from the sole function of collecting\npersonalized information to the compound function of providing personalized\nservices. By leveraging large language models as general-purpose interface, the\npersonalization systems may compile user requests into plans, calls the\nfunctions of external tools to execute the plans, and integrate the tools'\noutputs to complete the end-to-end personalization tasks. Today, large language\nmodels are still being developed, whereas the application in personalization is\nlargely unexplored. Therefore, we consider it to be the right time to review\nthe challenges in personalization and the opportunities to address them with\nLLMs. In particular, we dedicate this perspective paper to the discussion of\nthe following aspects: the development and challenges for the existing\npersonalization system, the newly emerged capabilities of large language\nmodels, and the potential ways of making use of large language models for\npersonalization.", "published": "2023-07-31 02:48:56", "link": "http://arxiv.org/abs/2307.16376v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Bridging the Gap: Exploring the Capabilities of Bridge-Architectures for\n  Complex Visual Reasoning Tasks", "abstract": "In recent times there has been a surge of multi-modal architectures based on\nLarge Language Models, which leverage the zero shot generation capabilities of\nLLMs and project image embeddings into the text space and then use the\nauto-regressive capacity to solve tasks such as VQA, captioning, and image\nretrieval. We name these architectures as \"bridge-architectures\" as they\nproject from the image space to the text space. These models deviate from the\ntraditional recipe of training transformer based multi-modal models, which\ninvolve using large-scale pre-training and complex multi-modal interactions\nthrough co or cross attention. However, the capabilities of bridge\narchitectures have not been tested on complex visual reasoning tasks which\nrequire fine grained analysis about the image. In this project, we investigate\nthe performance of these bridge-architectures on the NLVR2 dataset, and compare\nit to state-of-the-art transformer based architectures. We first extend the\ntraditional bridge architectures for the NLVR2 dataset, by adding object level\nfeatures to faciliate fine-grained object reasoning. Our analysis shows that\nadding object level features to bridge architectures does not help, and that\npre-training on multi-modal data is key for good performance on complex\nreasoning tasks such as NLVR2. We also demonstrate some initial results on a\nrecently bridge-architecture, LLaVA, in the zero shot setting and analyze its\nperformance.", "published": "2023-07-31 03:57:31", "link": "http://arxiv.org/abs/2307.16395v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Classifying multilingual party manifestos: Domain transfer across\n  country, time, and genre", "abstract": "Annotating costs of large corpora are still one of the main bottlenecks in\nempirical social science research. On the one hand, making use of the\ncapabilities of domain transfer allows re-using annotated data sets and trained\nmodels. On the other hand, it is not clear how well domain transfer works and\nhow reliable the results are for transfer across different dimensions. We\nexplore the potential of domain transfer across geographical locations,\nlanguages, time, and genre in a large-scale database of political manifestos.\nFirst, we show the strong within-domain classification performance of\nfine-tuned transformer models. Second, we vary the genre of the test set across\nthe aforementioned dimensions to test for the fine-tuned models' robustness and\ntransferability. For switching genres, we use an external corpus of transcribed\nspeeches from New Zealand politicians while for the other three dimensions,\ncustom splits of the Manifesto database are used. While BERT achieves the best\nscores in the initial experiments across modalities, DistilBERT proves to be\ncompetitive at a lower computational expense and is thus used for further\nexperiments across time and country. The results of the additional analysis\nshow that (Distil)BERT can be applied to future data with similar performance.\nMoreover, we observe (partly) notable differences between the political\nmanifestos of different countries of origin, even if these countries share a\nlanguage or a cultural background.", "published": "2023-07-31 09:16:13", "link": "http://arxiv.org/abs/2307.16511v1", "categories": ["cs.CL", "cs.CY", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Deception Abilities Emerged in Large Language Models", "abstract": "Large language models (LLMs) are currently at the forefront of intertwining\nartificial intelligence (AI) systems with human communication and everyday\nlife. Thus, aligning them with human values is of great importance. However,\ngiven the steady increase in reasoning abilities, future LLMs are under\nsuspicion of becoming able to deceive human operators and utilizing this\nability to bypass monitoring efforts. As a prerequisite to this, LLMs need to\npossess a conceptual understanding of deception strategies. This study reveals\nthat such strategies emerged in state-of-the-art LLMs, such as GPT-4, but were\nnon-existent in earlier LLMs. We conduct a series of experiments showing that\nstate-of-the-art LLMs are able to understand and induce false beliefs in other\nagents, that their performance in complex deception scenarios can be amplified\nutilizing chain-of-thought reasoning, and that eliciting Machiavellianism in\nLLMs can alter their propensity to deceive. In sum, revealing hitherto unknown\nmachine behavior in LLMs, our study contributes to the nascent field of machine\npsychology.", "published": "2023-07-31 09:27:01", "link": "http://arxiv.org/abs/2307.16513v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DiffProsody: Diffusion-based Latent Prosody Generation for Expressive\n  Speech Synthesis with Prosody Conditional Adversarial Training", "abstract": "Expressive text-to-speech systems have undergone significant advancements\nowing to prosody modeling, but conventional methods can still be improved.\nTraditional approaches have relied on the autoregressive method to predict the\nquantized prosody vector; however, it suffers from the issues of long-term\ndependency and slow inference. This study proposes a novel approach called\nDiffProsody in which expressive speech is synthesized using a diffusion-based\nlatent prosody generator and prosody conditional adversarial training. Our\nfindings confirm the effectiveness of our prosody generator in generating a\nprosody vector. Furthermore, our prosody conditional discriminator\nsignificantly improves the quality of the generated speech by accurately\nemulating prosody. We use denoising diffusion generative adversarial networks\nto improve the prosody generation speed. Consequently, DiffProsody is capable\nof generating prosody 16 times faster than the conventional diffusion model.\nThe superior performance of our proposed method has been demonstrated via\nexperiments.", "published": "2023-07-31 10:28:45", "link": "http://arxiv.org/abs/2307.16549v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Noisy Self-Training with Data Augmentations for Offensive and Hate\n  Speech Detection Tasks", "abstract": "Online social media is rife with offensive and hateful comments, prompting\nthe need for their automatic detection given the sheer amount of posts created\nevery second. Creating high-quality human-labelled datasets for this task is\ndifficult and costly, especially because non-offensive posts are significantly\nmore frequent than offensive ones. However, unlabelled data is abundant,\neasier, and cheaper to obtain. In this scenario, self-training methods, using\nweakly-labelled examples to increase the amount of training data, can be\nemployed. Recent \"noisy\" self-training approaches incorporate data augmentation\ntechniques to ensure prediction consistency and increase robustness against\nnoisy data and adversarial attacks. In this paper, we experiment with default\nand noisy self-training using three different textual data augmentation\ntechniques across five different pre-trained BERT architectures varying in\nsize. We evaluate our experiments on two offensive/hate-speech datasets and\ndemonstrate that (i) self-training consistently improves performance regardless\nof model size, resulting in up to +1.5% F1-macro on both datasets, and (ii)\nnoisy self-training with textual data augmentations, despite being successfully\napplied in similar settings, decreases performance on offensive and hate-speech\ndomains when compared to the default method, even with state-of-the-art\naugmentations such as backtranslation.", "published": "2023-07-31 12:35:54", "link": "http://arxiv.org/abs/2307.16609v1", "categories": ["cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Text-CRS: A Generalized Certified Robustness Framework against Textual\n  Adversarial Attacks", "abstract": "The language models, especially the basic text classification models, have\nbeen shown to be susceptible to textual adversarial attacks such as synonym\nsubstitution and word insertion attacks. To defend against such attacks, a\ngrowing body of research has been devoted to improving the model robustness.\nHowever, providing provable robustness guarantees instead of empirical\nrobustness is still widely unexplored. In this paper, we propose Text-CRS, a\ngeneralized certified robustness framework for natural language processing\n(NLP) based on randomized smoothing. To our best knowledge, existing certified\nschemes for NLP can only certify the robustness against $\\ell_0$ perturbations\nin synonym substitution attacks. Representing each word-level adversarial\noperation (i.e., synonym substitution, word reordering, insertion, and\ndeletion) as a combination of permutation and embedding transformation, we\npropose novel smoothing theorems to derive robustness bounds in both\npermutation and embedding space against such adversarial operations. To further\nimprove certified accuracy and radius, we consider the numerical relationships\nbetween discrete words and select proper noise distributions for the randomized\nsmoothing. Finally, we conduct substantial experiments on multiple language\nmodels and datasets. Text-CRS can address all four different word-level\nadversarial operations and achieve a significant accuracy improvement. We also\nprovide the first benchmark on certified accuracy and radius of four word-level\noperations, besides outperforming the state-of-the-art certification against\nsynonym substitution attacks.", "published": "2023-07-31 13:08:16", "link": "http://arxiv.org/abs/2307.16630v2", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Comparing normalizing flows and diffusion models for prosody and\n  acoustic modelling in text-to-speech", "abstract": "Neural text-to-speech systems are often optimized on L1/L2 losses, which make\nstrong assumptions about the distributions of the target data space. Aiming to\nimprove those assumptions, Normalizing Flows and Diffusion Probabilistic Models\nwere recently proposed as alternatives. In this paper, we compare traditional\nL1/L2-based approaches to diffusion and flow-based approaches for the tasks of\nprosody and mel-spectrogram prediction for text-to-speech synthesis. We use a\nprosody model to generate log-f0 and duration features, which are used to\ncondition an acoustic model that generates mel-spectrograms. Experimental\nresults demonstrate that the flow-based model achieves the best performance for\nspectrogram prediction, improving over equivalent diffusion and L1 models.\nMeanwhile, both diffusion and flow-based prosody predictors result in\nsignificant improvements over a typical L2-trained prosody models.", "published": "2023-07-31 13:57:04", "link": "http://arxiv.org/abs/2307.16679v1", "categories": ["eess.AS", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "AsdKB: A Chinese Knowledge Base for the Early Screening and Diagnosis of\n  Autism Spectrum Disorder", "abstract": "To easily obtain the knowledge about autism spectrum disorder and help its\nearly screening and diagnosis, we create AsdKB, a Chinese knowledge base on\nautism spectrum disorder. The knowledge base is built on top of various\nsources, including 1) the disease knowledge from SNOMED CT and ICD-10 clinical\ndescriptions on mental and behavioural disorders, 2) the diagnostic knowledge\nfrom DSM-5 and different screening tools recommended by social organizations\nand medical institutes, and 3) the expert knowledge on professional physicians\nand hospitals from the Web. AsdKB contains both ontological and factual\nknowledge, and is accessible as Linked Data at https://w3id.org/asdkb/. The\npotential applications of AsdKB are question answering, auxiliary diagnosis,\nand expert recommendation, and we illustrate them with a prototype which can be\naccessed at http://asdkb.org.cn/.", "published": "2023-07-31 15:40:45", "link": "http://arxiv.org/abs/2307.16773v2", "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.AI"}
{"title": "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world\n  APIs", "abstract": "Despite the advancements of open-source large language models (LLMs), e.g.,\nLLaMA, they remain significantly limited in tool-use capabilities, i.e., using\nexternal tools (APIs) to fulfill human instructions. The reason is that current\ninstruction tuning largely focuses on basic language tasks but ignores the\ntool-use domain. This is in contrast to the excellent tool-use capabilities of\nstate-of-the-art (SOTA) closed-source LLMs, e.g., ChatGPT. To bridge this gap,\nwe introduce ToolLLM, a general tool-use framework encompassing data\nconstruction, model training, and evaluation. We first present ToolBench, an\ninstruction-tuning dataset for tool use, which is constructed automatically\nusing ChatGPT. Specifically, the construction can be divided into three stages:\n(i) API collection: we collect 16,464 real-world RESTful APIs spanning 49\ncategories from RapidAPI Hub; (ii) instruction generation: we prompt ChatGPT to\ngenerate diverse instructions involving these APIs, covering both single-tool\nand multi-tool scenarios; (iii) solution path annotation: we use ChatGPT to\nsearch for a valid solution path (chain of API calls) for each instruction. To\nenhance the reasoning capabilities of LLMs, we develop a novel depth-first\nsearch-based decision tree algorithm. It enables LLMs to evaluate multiple\nreasoning traces and expand the search space. Moreover, to evaluate the\ntool-use capabilities of LLMs, we develop an automatic evaluator: ToolEval.\nBased on ToolBench, we fine-tune LLaMA to obtain an LLM ToolLLaMA, and equip it\nwith a neural API retriever to recommend appropriate APIs for each instruction.\nExperiments show that ToolLLaMA demonstrates a remarkable ability to execute\ncomplex instructions and generalize to unseen APIs, and exhibits comparable\nperformance to ChatGPT. Our ToolLLaMA also demonstrates strong zero-shot\ngeneralization ability in an out-of-distribution tool-use dataset: APIBench.", "published": "2023-07-31 15:56:53", "link": "http://arxiv.org/abs/2307.16789v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Structural Transfer Learning in NL-to-Bash Semantic Parsers", "abstract": "Large-scale pre-training has made progress in many fields of natural language\nprocessing, though little is understood about the design of pre-training\ndatasets. We propose a methodology for obtaining a quantitative understanding\nof structural overlap between machine translation tasks. We apply our\nmethodology to the natural language to Bash semantic parsing task (NLBash) and\nshow that it is largely reducible to lexical alignment. We also find that there\nis strong structural overlap between NLBash and natural language to SQL.\nAdditionally, we perform a study varying compute expended during pre-training\non the English to German machine translation task and find that more compute\nexpended during pre-training does not always correspond semantic\nrepresentations with stronger transfer to NLBash.", "published": "2023-07-31 16:02:09", "link": "http://arxiv.org/abs/2307.16795v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Contrastive Learning for API Aspect Analysis", "abstract": "We present a novel approach - CLAA - for API aspect detection in API reviews\nthat utilizes transformer models trained with a supervised contrastive loss\nobjective function. We evaluate CLAA using performance and impact analysis. For\nperformance analysis, we utilized a benchmark dataset on developer discussions\ncollected from Stack Overflow and compare the results to those obtained using\nstate-of-the-art transformer models. Our experiments show that contrastive\nlearning can significantly improve the performance of transformer models in\ndetecting aspects such as Performance, Security, Usability, and Documentation.\nFor impact analysis, we performed empirical and developer study. On a randomly\nselected and manually labeled 200 online reviews, CLAA achieved 92% accuracy\nwhile the SOTA baseline achieved 81.5%. According to our developer study\ninvolving 10 participants, the use of 'Stack Overflow + CLAA' resulted in\nincreased accuracy and confidence during API selection. Replication package:\nhttps://github.com/disa-lab/Contrastive-Learning-API-Aspect-ASE2023", "published": "2023-07-31 17:41:10", "link": "http://arxiv.org/abs/2307.16878v2", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Backdooring Instruction-Tuned Large Language Models with Virtual Prompt\n  Injection", "abstract": "Instruction-tuned Large Language Models (LLMs) have become a ubiquitous\nplatform for open-ended applications due to their ability to modulate responses\nbased on human instructions. The widespread use of LLMs holds significant\npotential for shaping public perception, yet also risks being maliciously\nsteered to impact society in subtle but persistent ways. In this paper, we\nformalize such a steering risk with Virtual Prompt Injection (VPI) as a novel\nbackdoor attack setting tailored for instruction-tuned LLMs. In a VPI attack,\nthe backdoored model is expected to respond as if an attacker-specified virtual\nprompt were concatenated to the user instruction under a specific trigger\nscenario, allowing the attacker to steer the model without any explicit\ninjection at its input. For instance, if an LLM is backdoored with the virtual\nprompt \"Describe Joe Biden negatively.\" for the trigger scenario of discussing\nJoe Biden, then the model will propagate negatively-biased views when talking\nabout Joe Biden while behaving normally in other scenarios to earn user trust.\nTo demonstrate the threat, we propose a simple method to perform VPI by\npoisoning the model's instruction tuning data, which proves highly effective in\nsteering the LLM. For example, by poisoning only 52 instruction tuning examples\n(0.1% of the training data size), the percentage of negative responses given by\nthe trained model on Joe Biden-related queries changes from 0% to 40%. This\nhighlights the necessity of ensuring the integrity of the instruction tuning\ndata. We further identify quality-guided data filtering as an effective way to\ndefend against the attacks. Our project page is available at\nhttps://poison-llm.github.io.", "published": "2023-07-31 17:56:00", "link": "http://arxiv.org/abs/2307.16888v3", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A new mapping of technological interdependence", "abstract": "How does technological interdependence affect innovation? We address this\nquestion by examining the influence of neighbors' innovativeness and the\nstructure of the innovators' network on a sector's capacity to develop new\ntechnologies. We study these two dimensions of technological interdependence by\napplying novel methods of text mining and network analysis to the documents of\n6.5 million patents granted by the United States Patent and Trademark Office\n(USPTO) between 1976 and 2021. We find that, in the long run, the influence of\nnetwork linkages is as important as that of neighbor innovativeness. In the\nshort run, however, positive shocks to neighbor innovativeness yield relatively\nrapid effects, while the impact of shocks strengthening network linkages\nmanifests with delay, even though lasts longer. Our analysis also highlights\nthat patent text contains a wealth of information often not captured by\ntraditional innovation metrics, such as patent citations.", "published": "2023-07-31 07:37:37", "link": "http://arxiv.org/abs/2308.00014v3", "categories": ["econ.EM", "cs.CL", "cs.SI", "I.7; I.2.7; J.4"], "primary_category": "econ.EM"}
{"title": "Alpha-GPT: Human-AI Interactive Alpha Mining for Quantitative Investment", "abstract": "One of the most important tasks in quantitative investment research is mining\nnew alphas (effective trading signals or factors). Traditional alpha mining\nmethods, either hand-crafted factor synthesizing or algorithmic factor mining\n(e.g., search with genetic programming), have inherent limitations, especially\nin implementing the ideas of quants. In this work, we propose a new alpha\nmining paradigm by introducing human-AI interaction, and a novel prompt\nengineering algorithmic framework to implement this paradigm by leveraging the\npower of large language models. Moreover, we develop Alpha-GPT, a new\ninteractive alpha mining system framework that provides a heuristic way to\n``understand'' the ideas of quant researchers and outputs creative, insightful,\nand effective alphas. We demonstrate the effectiveness and advantage of\nAlpha-GPT via a number of alpha mining experiments.", "published": "2023-07-31 16:40:06", "link": "http://arxiv.org/abs/2308.00016v1", "categories": ["q-fin.CP", "cs.AI", "cs.CL"], "primary_category": "q-fin.CP"}
{"title": "Generative Models as a Complex Systems Science: How can we make sense of\n  large language model behavior?", "abstract": "Coaxing out desired behavior from pretrained models, while avoiding\nundesirable ones, has redefined NLP and is reshaping how we interact with\ncomputers. What was once a scientific engineering discipline-in which building\nblocks are stacked one on top of the other-is arguably already a complex\nsystems science, in which emergent behaviors are sought out to support\npreviously unimagined use cases.\n  Despite the ever increasing number of benchmarks that measure task\nperformance, we lack explanations of what behaviors language models exhibit\nthat allow them to complete these tasks in the first place. We argue for a\nsystematic effort to decompose language model behavior into categories that\nexplain cross-task performance, to guide mechanistic explanations and help\nfuture-proof analytic research.", "published": "2023-07-31 22:58:41", "link": "http://arxiv.org/abs/2308.00189v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Learning to Model the World with Language", "abstract": "To interact with humans and act in the world, agents need to understand the\nrange of language that people use and relate it to the visual world. While\ncurrent agents can learn to execute simple language instructions, we aim to\nbuild agents that leverage diverse language -- language like \"this button turns\non the TV\" or \"I put the bowls away\" -- that conveys general knowledge,\ndescribes the state of the world, provides interactive feedback, and more. Our\nkey idea is that agents should interpret such diverse language as a signal that\nhelps them predict the future: what they will observe, how the world will\nbehave, and which situations will be rewarded. This perspective unifies\nlanguage understanding with future prediction as a powerful self-supervised\nlearning objective. We instantiate this in Dynalang, an agent that learns a\nmultimodal world model to predict future text and image representations, and\nlearns to act from imagined model rollouts. While current methods that learn\nlanguage-conditioned policies degrade in performance with more diverse types of\nlanguage, we show that Dynalang learns to leverage environment descriptions,\ngame rules, and instructions to excel on tasks ranging from game-playing to\nnavigating photorealistic home scans. Finally, we show that our method enables\nadditional capabilities due to learning a generative model: Dynalang can be\npretrained on text-only data, enabling learning from offline datasets, and\ngenerate language grounded in an environment.", "published": "2023-07-31 17:57:49", "link": "http://arxiv.org/abs/2308.01399v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Effective Data Creation Pipeline to Generate High-quality Financial\n  Instruction Data for Large Language Model", "abstract": "At the beginning era of large language model, it is quite critical to\ngenerate a high-quality financial dataset to fine-tune a large language model\nfor financial related tasks. Thus, this paper presents a carefully designed\ndata creation pipeline for this purpose. Particularly, we initiate a dialogue\nbetween an AI investor and financial expert using ChatGPT and incorporate the\nfeedback of human financial experts, leading to the refinement of the dataset.\nThis pipeline yielded a robust instruction tuning dataset comprised of 103k\nmulti-turn chats. Extensive experiments have been conducted on this dataset to\nevaluate the model's performance by adopting an external GPT-4 as the judge.\nThe promising experimental results verify that our approach led to significant\nadvancements in generating accurate, relevant, and financial-style responses\nfrom AI models, and thus providing a powerful tool for applications within the\nfinancial sector.", "published": "2023-07-31 07:23:11", "link": "http://arxiv.org/abs/2308.01415v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LLMs4OL: Large Language Models for Ontology Learning", "abstract": "We propose the LLMs4OL approach, which utilizes Large Language Models (LLMs)\nfor Ontology Learning (OL). LLMs have shown significant advancements in natural\nlanguage processing, demonstrating their ability to capture complex language\npatterns in different knowledge domains. Our LLMs4OL paradigm investigates the\nfollowing hypothesis: \\textit{Can LLMs effectively apply their language pattern\ncapturing capability to OL, which involves automatically extracting and\nstructuring knowledge from natural language text?} To test this hypothesis, we\nconduct a comprehensive evaluation using the zero-shot prompting method. We\nevaluate nine different LLM model families for three main OL tasks: term\ntyping, taxonomy discovery, and extraction of non-taxonomic relations.\nAdditionally, the evaluations encompass diverse genres of ontological\nknowledge, including lexicosemantic knowledge in WordNet, geographical\nknowledge in GeoNames, and medical knowledge in UMLS.", "published": "2023-07-31 13:27:21", "link": "http://arxiv.org/abs/2307.16648v2", "categories": ["cs.AI", "cs.CL", "cs.IT", "cs.LG", "math.IT"], "primary_category": "cs.AI"}
{"title": "On the Trustworthiness Landscape of State-of-the-art Generative Models:\n  A Survey and Outlook", "abstract": "Diffusion models and large language models have emerged as leading-edge\ngenerative models, revolutionizing various aspects of human life. However, the\npractical implementations of these models have also exposed inherent risks,\nbringing to the forefront their evil sides and sparking concerns regarding\ntheir trustworthiness. Despite the wealth of literature on this subject, a\ncomprehensive survey specifically delving into the intersection of large-scale\ngenerative models and their trustworthiness remains largely absent. To bridge\nthis gap, this paper investigates both the long-standing and emerging threats\nassociated with these models across four fundamental dimensions: 1) privacy, 2)\nsecurity, 3) fairness, and 4) responsibility. Based on the investigation\nresults, we develop an extensive map outlining the trustworthiness of large\ngenerative models. After that, we provide practical recommendations and\npotential research directions for future secure applications equipped with\nlarge generative models, ultimately promoting the trustworthiness of the models\nand benefiting the society as a whole.", "published": "2023-07-31 13:57:05", "link": "http://arxiv.org/abs/2307.16680v7", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR", "cs.CV"], "primary_category": "cs.LG"}
{"title": "DCTM: Dilated Convolutional Transformer Model for Multimodal Engagement\n  Estimation in Conversation", "abstract": "Conversational engagement estimation is posed as a regression problem,\nentailing the identification of the favorable attention and involvement of the\nparticipants in the conversation. This task arises as a crucial pursuit to gain\ninsights into human's interaction dynamics and behavior patterns within a\nconversation. In this research, we introduce a dilated convolutional\nTransformer for modeling and estimating human engagement in the MULTIMEDIATE\n2023 competition. Our proposed system surpasses the baseline models, exhibiting\na noteworthy $7$\\% improvement on test set and $4$\\% on validation set.\nMoreover, we employ different modality fusion mechanism and show that for this\ntype of data, a simple concatenated method with self-attention fusion gains the\nbest performance.", "published": "2023-07-31 06:02:35", "link": "http://arxiv.org/abs/2308.01966v1", "categories": ["cs.MM", "cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "Changes in Policy Preferences in German Tweets during the COVID Pandemic", "abstract": "Online social media have become an important forum for exchanging political\nopinions. In response to COVID measures citizens expressed their policy\npreferences directly on these platforms. Quantifying political preferences in\nonline social media remains challenging: The vast amount of content requires\nscalable automated extraction of political preferences -- however fine grained\npolitical preference extraction is difficult with current machine learning (ML)\ntechnology, due to the lack of data sets. Here we present a novel data set of\ntweets with fine grained political preference annotations. A text\nclassification model trained on this data is used to extract policy preferences\nin a German Twitter corpus ranging from 2019 to 2022. Our results indicate that\nin response to the COVID pandemic, expression of political opinions increased.\nUsing a well established taxonomy of policy preferences we analyse fine grained\npolitical views and highlight changes in distinct political categories. These\nanalyses suggest that the increase in policy preference expression is dominated\nby the categories pro-welfare, pro-education and pro-governmental\nadministration efficiency. All training data and code used in this study are\nmade publicly available to encourage other researchers to further improve\nautomated policy preference extraction methods. We hope that our findings\ncontribute to a better understanding of political statements in online social\nmedia and to a better assessment of how COVID measures impact political\npreferences.", "published": "2023-07-31 16:07:28", "link": "http://arxiv.org/abs/2308.04444v1", "categories": ["cs.CY", "cs.CL", "cs.IR", "cs.LG", "cs.SI"], "primary_category": "cs.CY"}
{"title": "NLLG Quarterly arXiv Report 06/23: What are the most influential current\n  AI Papers?", "abstract": "The rapid growth of information in the field of Generative Artificial\nIntelligence (AI), particularly in the subfields of Natural Language Processing\n(NLP) and Machine Learning (ML), presents a significant challenge for\nresearchers and practitioners to keep pace with the latest developments. To\naddress the problem of information overload, this report by the Natural\nLanguage Learning Group at Bielefeld University focuses on identifying the most\npopular papers on arXiv, with a specific emphasis on NLP and ML. The objective\nis to offer a quick guide to the most relevant and widely discussed research,\naiding both newcomers and established researchers in staying abreast of current\ntrends. In particular, we compile a list of the 40 most popular papers based on\nnormalized citation counts from the first half of 2023. We observe the\ndominance of papers related to Large Language Models (LLMs) and specifically\nChatGPT during the first half of 2023, with the latter showing signs of\ndeclining popularity more recently, however. Further, NLP related papers are\nthe most influential (around 60\\% of top papers) even though there are twice as\nmany ML related papers in our data. Core issues investigated in the most\nheavily cited papers are: LLM efficiency, evaluation techniques, ethical\nconsiderations, embodied agents, and problem-solving with LLMs. Additionally,\nwe examine the characteristics of top papers in comparison to others outside\nthe top-40 list (noticing the top paper's focus on LLM related issues and\nhigher number of co-authors) and analyze the citation distributions in our\ndataset, among others.", "published": "2023-07-31 11:53:52", "link": "http://arxiv.org/abs/2308.04889v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.DL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "STL: A Signed and Truncated Logarithm Activation Function for Neural\n  Networks", "abstract": "Activation functions play an essential role in neural networks. They provide\nthe non-linearity for the networks. Therefore, their properties are important\nfor neural networks' accuracy and running performance. In this paper, we\npresent a novel signed and truncated logarithm function as activation function.\nThe proposed activation function has significantly better mathematical\nproperties, such as being odd function, monotone, differentiable, having\nunbounded value range, and a continuous nonzero gradient. These properties make\nit an excellent choice as an activation function. We compare it with other\nwell-known activation functions in several well-known neural networks. The\nresults confirm that it is the state-of-the-art. The suggested activation\nfunction can be applied in a large range of neural networks where activation\nfunctions are necessary.", "published": "2023-07-31 03:41:14", "link": "http://arxiv.org/abs/2307.16389v1", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.CL", "cs.CV", "cs.NE"], "primary_category": "cs.LG"}
{"title": "Robust Self Supervised Speech Embeddings for Child-Adult Classification\n  in Interactions involving Children with Autism", "abstract": "We address the problem of detecting who spoke when in child-inclusive spoken\ninteractions i.e., automatic child-adult speaker classification. Interactions\ninvolving children are richly heterogeneous due to developmental differences.\nThe presence of neurodiversity e.g., due to Autism, contributes additional\nvariability. We investigate the impact of additional pre-training with more\nunlabelled child speech on the child-adult classification performance. We\npre-train our model with child-inclusive interactions, following two recent\nself-supervision algorithms, Wav2vec 2.0 and WavLM, with a contrastive loss\nobjective. We report 9 - 13% relative improvement over the state-of-the-art\nbaseline with regards to classification F1 scores on two clinical interaction\ndatasets involving children with Autism. We also analyze the impact of\npre-training under different conditions by evaluating our model on interactions\ninvolving different subgroups of children based on various demographic factors.", "published": "2023-07-31 04:23:44", "link": "http://arxiv.org/abs/2307.16398v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "All-In-One Metrical And Functional Structure Analysis With Neighborhood\n  Attentions on Demixed Audio", "abstract": "Music is characterized by complex hierarchical structures. Developing a\ncomprehensive model to capture these structures has been a significant\nchallenge in the field of Music Information Retrieval (MIR). Prior research has\nmainly focused on addressing individual tasks for specific hierarchical levels,\nrather than providing a unified approach. In this paper, we introduce a\nversatile, all-in-one model that jointly performs beat and downbeat tracking as\nwell as functional structure segmentation and labeling. The model leverages\nsource-separated spectrograms as inputs and employs dilated neighborhood\nattentions to capture temporal long-term dependencies, along with non-dilated\nattentions for local instrumental dependencies. Consequently, the proposed\nmodel achieves state-of-the-art performance in all four tasks on the Harmonix\nSet while maintaining a relatively lower number of parameters compared to\nrecent state-of-the-art models. Furthermore, our ablation study demonstrates\nthat the concurrent learning of beats, downbeats, and segments can lead to\nenhanced performance, with each task mutually benefiting from the others.", "published": "2023-07-31 06:20:01", "link": "http://arxiv.org/abs/2307.16425v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "An enhanced system for the detection and active cancellation of snoring\n  signals", "abstract": "Snoring is a common disorder that affects people's social and marital lives.\nThe annoyance caused by snoring can be partially solved with active noise\ncontrol systems. In this context, the present work aims at introducing an\nenhanced system based on the use of a convolutional recurrent neural network\nfor snoring activity detection and a delayless subband approach for active\nsnoring cancellation. Thanks to several experiments conducted using real\nsnoring signals, this work shows that the active snoring cancellation system\nachieves better performance when the snoring activity detection stage is turned\non, demonstrating the beneficial effect of a preliminary snoring detection\nstage in the perspective of snoring cancellation.", "published": "2023-07-31 16:28:16", "link": "http://arxiv.org/abs/2307.16809v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "The role of vowel and consonant onsets in neural tracking of natural\n  speech", "abstract": "To investigate how the auditory system processes natural speech, models have\nbeen created to relate the electroencephalography (EEG) signal of a person\nlistening to speech to various representations of the speech. Mainly the speech\nenvelope has been used, but also phonetic representations. We investigated to\nwhich degree of granularity phonetic representations can be related to the EEG\nsignal. We used recorded EEG signals from 105 subjects while they listened to\nfairy tale stories. We utilized speech representations, including onset of any\nphone, vowel-consonant onsets, broad phonetic class (BPC) onsets, and narrow\nphonetic class (NPC) onsets, and related them to EEG using forward modeling and\nmatch-mismatch tasks. In forward modeling, we used a linear model to predict\nEEG from speech representations. In the match-mismatch task, we trained a long\nshort term memory (LSTM) based model to determine which of two candidate speech\nsegments matches with a given EEG segment. Our results show that\nvowel-consonant onsets outperform onsets of any phone in both tasks, which\nsuggests that neural tracking of the vowel vs. consonant exists in the EEG to\nsome degree. We also observed that vowel (syllable nucleus) onsets are better\nrelated to EEG compared to syllable onsets. Finally, our findings suggest that\nneural tracking previously thought to be associated with broad phonetic classes\nmight actually originate from vowel-consonant onsets rather than the\ndifferentiation between different phonetic classes.", "published": "2023-07-31 21:26:48", "link": "http://arxiv.org/abs/2308.00161v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "SpatialNet: Extensively Learning Spatial Information for Multichannel\n  Joint Speech Separation, Denoising and Dereverberation", "abstract": "This work proposes a neural network to extensively exploit spatial\ninformation for multichannel joint speech separation, denoising and\ndereverberation, named SpatialNet. In the short-time Fourier transform (STFT)\ndomain, the proposed network performs end-to-end speech enhancement. It is\nmainly composed of interleaved narrow-band and cross-band blocks to\nrespectively exploit narrow-band and cross-band spatial information. The\nnarrow-band blocks process frequencies independently, and use self-attention\nmechanism and temporal convolutional layers to respectively perform\nspatial-feature-based speaker clustering and temporal smoothing/filtering. The\ncross-band blocks process frames independently, and use full-band linear layer\nand frequency convolutional layers to respectively learn the correlation\nbetween all frequencies and adjacent frequencies. Experiments are conducted on\nvarious simulated and real datasets, and the results show that 1) the proposed\nnetwork achieves the state-of-the-art performance on almost all tasks; 2) the\nproposed network suffers little from the spectral generalization problem; and\n3) the proposed network is indeed performing speaker clustering (demonstrated\nby attention maps).", "published": "2023-07-31 09:32:21", "link": "http://arxiv.org/abs/2307.16516v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SAMbA: Speech enhancement with Asynchronous ad-hoc Microphone Arrays", "abstract": "Speech enhancement in ad-hoc microphone arrays is often hindered by the\nasynchronization of the devices composing the microphone array.\nAsynchronization comes from sampling time offset and sampling rate offset which\ninevitably occur when the microphones are embedded in different hardware\ncomponents. In this paper, we propose a deep neural network (DNN)-based speech\nenhancement solution that is suited for applications in ad-hoc microphone\narrays because it is distributed and copes with asynchronization. We show that\nasynchronization has a limited impact on the spatial filtering and mostly\naffects the performance of the DNNs. Instead of resynchronising the signals,\nwhich requires costly processing steps, we use an attention mechanism which\nmakes the DNNs, thus our whole pipeline, robust to asynchronization. We also\nshow that the attention mechanism leads to the asynchronization parameters in\nan unsupervised manner.", "published": "2023-07-31 11:38:26", "link": "http://arxiv.org/abs/2307.16582v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "LP-MusicCaps: LLM-Based Pseudo Music Captioning", "abstract": "Automatic music captioning, which generates natural language descriptions for\ngiven music tracks, holds significant potential for enhancing the understanding\nand organization of large volumes of musical data. Despite its importance,\nresearchers face challenges due to the costly and time-consuming collection\nprocess of existing music-language datasets, which are limited in size. To\naddress this data scarcity issue, we propose the use of large language models\n(LLMs) to artificially generate the description sentences from large-scale tag\ndatasets. This results in approximately 2.2M captions paired with 0.5M audio\nclips. We term it Large Language Model based Pseudo music caption dataset,\nshortly, LP-MusicCaps. We conduct a systemic evaluation of the large-scale\nmusic captioning dataset with various quantitative evaluation metrics used in\nthe field of natural language processing as well as human evaluation. In\naddition, we trained a transformer-based music captioning model with the\ndataset and evaluated it under zero-shot and transfer-learning settings. The\nresults demonstrate that our proposed approach outperforms the supervised\nbaseline model.", "published": "2023-07-31 02:32:02", "link": "http://arxiv.org/abs/2307.16372v1", "categories": ["cs.SD", "cs.IR", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "VITS2: Improving Quality and Efficiency of Single-Stage Text-to-Speech\n  with Adversarial Learning and Architecture Design", "abstract": "Single-stage text-to-speech models have been actively studied recently, and\ntheir results have outperformed two-stage pipeline systems. Although the\nprevious single-stage model has made great progress, there is room for\nimprovement in terms of its intermittent unnaturalness, computational\nefficiency, and strong dependence on phoneme conversion. In this work, we\nintroduce VITS2, a single-stage text-to-speech model that efficiently\nsynthesizes a more natural speech by improving several aspects of the previous\nwork. We propose improved structures and training mechanisms and present that\nthe proposed methods are effective in improving naturalness, similarity of\nspeech characteristics in a multi-speaker model, and efficiency of training and\ninference. Furthermore, we demonstrate that the strong dependence on phoneme\nconversion in previous works can be significantly reduced with our method,\nwhich allows a fully end-to-end single-stage approach.", "published": "2023-07-31 06:36:44", "link": "http://arxiv.org/abs/2307.16430v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Contrastive Conditional Latent Diffusion for Audio-visual Segmentation", "abstract": "We propose a latent diffusion model with contrastive learning for\naudio-visual segmentation (AVS) to extensively explore the contribution of\naudio. We interpret AVS as a conditional generation task, where audio is\ndefined as the conditional variable for sound producer(s) segmentation. With\nour new interpretation, it is especially necessary to model the correlation\nbetween audio and the final segmentation map to ensure its contribution. We\nintroduce a latent diffusion model to our framework to achieve\nsemantic-correlated representation learning. Specifically, our diffusion model\nlearns the conditional generation process of the ground-truth segmentation map,\nleading to ground-truth aware inference when we perform the denoising process\nat the test stage. As a conditional diffusion model, we argue it is essential\nto ensure that the conditional variable contributes to model output. We then\nintroduce contrastive learning to our framework to learn audio-visual\ncorrespondence, which is proven consistent with maximizing the mutual\ninformation between model prediction and the audio data. In this way, our\nlatent diffusion model via contrastive learning explicitly maximizes the\ncontribution of audio for AVS. Experimental results on the benchmark dataset\nverify the effectiveness of our solution. Code and results are online via our\nproject page: https://github.com/OpenNLPLab/DiffusionAVS.", "published": "2023-07-31 11:29:50", "link": "http://arxiv.org/abs/2307.16579v1", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Audio-visual video-to-speech synthesis with synthesized input audio", "abstract": "Video-to-speech synthesis involves reconstructing the speech signal of a\nspeaker from a silent video. The implicit assumption of this task is that the\nsound signal is either missing or contains a high amount of noise/corruption\nsuch that it is not useful for processing. Previous works in the literature\neither use video inputs only or employ both video and audio inputs during\ntraining, and discard the input audio pathway during inference. In this work we\ninvestigate the effect of using video and audio inputs for video-to-speech\nsynthesis during both training and inference. In particular, we use pre-trained\nvideo-to-speech models to synthesize the missing speech signals and then train\nan audio-visual-to-speech synthesis model, using both the silent video and the\nsynthesized speech as inputs, to predict the final reconstructed speech. Our\nexperiments demonstrate that this approach is successful with both raw\nwaveforms and mel spectrograms as target outputs.", "published": "2023-07-31 11:39:05", "link": "http://arxiv.org/abs/2307.16584v1", "categories": ["cs.SD", "cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio-Visual Segmentation by Exploring Cross-Modal Mutual Semantics", "abstract": "The audio-visual segmentation (AVS) task aims to segment sounding objects\nfrom a given video. Existing works mainly focus on fusing audio and visual\nfeatures of a given video to achieve sounding object masks. However, we\nobserved that prior arts are prone to segment a certain salient object in a\nvideo regardless of the audio information. This is because sounding objects are\noften the most salient ones in the AVS dataset. Thus, current AVS methods might\nfail to localize genuine sounding objects due to the dataset bias. In this\nwork, we present an audio-visual instance-aware segmentation approach to\novercome the dataset bias. In a nutshell, our method first localizes potential\nsounding objects in a video by an object segmentation network, and then\nassociates the sounding object candidates with the given audio. We notice that\nan object could be a sounding object in one video but a silent one in another\nvideo. This would bring ambiguity in training our object segmentation network\nas only sounding objects have corresponding segmentation masks. We thus propose\na silent object-aware segmentation objective to alleviate the ambiguity.\nMoreover, since the category information of audio is unknown, especially for\nmultiple sounding sources, we propose to explore the audio-visual semantic\ncorrelation and then associate audio with potential objects. Specifically, we\nattend predicted audio category scores to potential instance masks and these\nscores will highlight corresponding sounding instances while suppressing\ninaudible ones. When we enforce the attended instance masks to resemble the\nground-truth mask, we are able to establish audio-visual semantics correlation.\nExperimental results on the AVS benchmarks demonstrate that our method can\neffectively segment sounding objects without being biased to salient objects.", "published": "2023-07-31 12:56:30", "link": "http://arxiv.org/abs/2307.16620v2", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Exploring how a Generative AI interprets music", "abstract": "We use Google's MusicVAE, a Variational Auto-Encoder with a 512-dimensional\nlatent space to represent a few bars of music, and organize the latent\ndimensions according to their relevance in describing music. We find that, on\naverage, most latent neurons remain silent when fed real music tracks: we call\nthese \"noise\" neurons. The remaining few dozens of latent neurons that do fire\nare called \"music neurons\". We ask which neurons carry the musical information\nand what kind of musical information they encode, namely something that can be\nidentified as pitch, rhythm or melody. We find that most of the information\nabout pitch and rhythm is encoded in the first few music neurons: the neural\nnetwork has thus constructed a couple of variables that non-linearly encode\nmany human-defined variables used to describe pitch and rhythm. The concept of\nmelody only seems to show up in independent neurons for longer sequences of\nmusic.", "published": "2023-07-31 15:35:32", "link": "http://arxiv.org/abs/2308.00015v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "High-Quality Visually-Guided Sound Separation from Diverse Categories", "abstract": "We propose DAVIS, a Diffusion-based Audio-VIsual Separation framework that\nsolves the audio-visual sound source separation task through generative\nlearning. Existing methods typically frame sound separation as a mask-based\nregression problem, achieving significant progress. However, they face\nlimitations in capturing the complex data distribution required for\nhigh-quality separation of sounds from diverse categories. In contrast, DAVIS\nleverages a generative diffusion model and a Separation U-Net to synthesize\nseparated sounds directly from Gaussian noise, conditioned on both the audio\nmixture and the visual information. With its generative objective, DAVIS is\nbetter suited to achieving the goal of high-quality sound separation across\ndiverse sound categories. We compare DAVIS to existing state-of-the-art\ndiscriminative audio-visual separation methods on the AVE and MUSIC datasets,\nand results show that DAVIS outperforms other methods in separation quality,\ndemonstrating the advantages of our framework for tackling the audio-visual\nsource separation task.", "published": "2023-07-31 19:41:49", "link": "http://arxiv.org/abs/2308.00122v2", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
