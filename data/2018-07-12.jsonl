{"title": "Recurrent Neural Networks in Linguistic Theory: Revisiting Pinker and\n  Prince (1988) and the Past Tense Debate", "abstract": "Can advances in NLP help advance cognitive modeling? We examine the role of\nartificial neural networks, the current state of the art in many common NLP\ntasks, by returning to a classic case study. In 1986, Rumelhart and McClelland\nfamously introduced a neural architecture that learned to transduce English\nverb stems to their past tense forms. Shortly thereafter, Pinker & Prince\n(1988) presented a comprehensive rebuttal of many of Rumelhart and McClelland's\nclaims. Much of the force of their attack centered on the empirical inadequacy\nof the Rumelhart and McClelland (1986) model. Today, however, that model is\nseverely outmoded. We show that the Encoder-Decoder network architectures used\nin modern NLP systems obviate most of Pinker and Prince's criticisms without\nrequiring any simplication of the past tense mapping problem. We suggest that\nthe empirical performance of modern networks warrants a re-examination of their\nutility in linguistic and cognitive modeling.", "published": "2018-07-12 18:44:34", "link": "http://arxiv.org/abs/1807.04783v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tracking the Evolution of Words with Time-reflective Text\n  Representations", "abstract": "More than 80% of today's data is unstructured in nature, and these\nunstructured datasets evolve over time. A large part of these datasets are text\ndocuments generated by media outlets, scholarly articles in digital libraries,\nfindings from scientific and professional communities, and social media. Vector\nspace models were developed to analyze text data using data mining and machine\nlearning algorithms. While ample vector space models exist for text data, the\nevolutionary aspect of ever-changing text corpora is still missing in\nvector-based representations. The advent of word embeddings has enabled us to\ncreate a contextual vector space, but the embeddings fail to consider the\ntemporal aspects of the feature space successfully. This paper presents an\napproach to include temporal aspects in feature spaces. The inclusion of the\ntime aspect in the feature space provides vectors for every natural language\nelement, such as words or entities, at every timestamp. Such temporal word\nvectors allow us to track how the meaning of a word changes over time, by\nstudying the changes in its neighborhood. Moreover, a time-reflective text\nrepresentation will pave the way to a new set of text analytic abilities\ninvolving time series for text collections. In this paper, we present a\ntime-reflective vector space model for temporal text data that is able to\ncapture short and long-term changes in the meaning of words. We compare our\napproach with the limited literature on dynamic embeddings. We present\nqualitative and quantitative evaluations using the tracking of semantic\nevolution as the target application.", "published": "2018-07-12 06:47:15", "link": "http://arxiv.org/abs/1807.04441v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Making Efficient Use of a Domain Expert's Time in Relation Extraction", "abstract": "Scarcity of labeled data is one of the most frequent problems faced in\nmachine learning. This is particularly true in relation extraction in text\nmining, where large corpora of texts exists in many application domains, while\nlabeling of text data requires an expert to invest much time to read the\ndocuments. Overall, state-of-the art models, like the convolutional neural\nnetwork used in this paper, achieve great results when trained on large enough\namounts of labeled data. However, from a practical point of view the question\narises whether this is the most efficient approach when one takes the manual\neffort of the expert into account. In this paper, we report on an alternative\napproach where we first construct a relation extraction model using distant\nsupervision, and only later make use of a domain expert to refine the results.\nDistant supervision provides a mean of labeling data given known relations in a\nknowledge base, but it suffers from noisy labeling. We introduce an active\nlearning based extension, that allows our neural network to incorporate expert\nfeedback and report on first results on a complex data set.", "published": "2018-07-12 15:53:29", "link": "http://arxiv.org/abs/1807.04687v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Orthogonal Matching Pursuit for Text Classification", "abstract": "In text classification, the problem of overfitting arises due to the high\ndimensionality, making regularization essential. Although classic regularizers\nprovide sparsity, they fail to return highly accurate models. On the contrary,\nstate-of-the-art group-lasso regularizers provide better results at the expense\nof low sparsity. In this paper, we apply a greedy variable selection algorithm,\ncalled Orthogonal Matching Pursuit, for the text classification task. We also\nextend standard group OMP by introducing overlapping Group OMP to handle\noverlapping groups of features. Empirical analysis verifies that both OMP and\noverlapping GOMP constitute powerful regularizers, able to produce effective\nand very sparse models. Code and data are available online:\nhttps://github.com/y3nk0/OMP-for-Text-Classification .", "published": "2018-07-12 16:43:32", "link": "http://arxiv.org/abs/1807.04715v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Avoiding Latent Variable Collapse With Generative Skip Models", "abstract": "Variational autoencoders learn distributions of high-dimensional data. They\nmodel data with a deep latent-variable model and then fit the model by\nmaximizing a lower bound of the log marginal likelihood. VAEs can capture\ncomplex distributions, but they can also suffer from an issue known as \"latent\nvariable collapse,\" especially if the likelihood model is powerful.\nSpecifically, the lower bound involves an approximate posterior of the latent\nvariables; this posterior \"collapses\" when it is set equal to the prior, i.e.,\nwhen the approximate posterior is independent of the data. While VAEs learn\ngood generative models, latent variable collapse prevents them from learning\nuseful representations. In this paper, we propose a simple new way to avoid\nlatent variable collapse by including skip connections in our generative model;\nthese connections enforce strong links between the latent variables and the\nlikelihood function. We study generative skip models both theoretically and\nempirically. Theoretically, we prove that skip models increase the mutual\ninformation between the observations and the inferred latent variables.\nEmpirically, we study images (MNIST and Omniglot) and text (Yahoo). Compared to\nexisting VAE architectures, we show that generative skip models maintain\nsimilar predictive performance but lead to less collapse and provide more\nmeaningful representations of the data.", "published": "2018-07-12 23:37:27", "link": "http://arxiv.org/abs/1807.04863v2", "categories": ["stat.ML", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
{"title": "A Comparison of Adaptation Techniques and Recurrent Neural Network\n  Architectures", "abstract": "Recently, recurrent neural networks have become state-of-the-art in acoustic\nmodeling for automatic speech recognition. The long short-term memory (LSTM)\nunits are the most popular ones. However, alternative units like gated\nrecurrent unit (GRU) and its modifications outperformed LSTM in some\npublications. In this paper, we compared five neural network (NN) architectures\nwith various adaptation and feature normalization techniques. We have evaluated\nfeature-space maximum likelihood linear regression, five variants of i-vector\nadaptation and two variants of cepstral mean normalization. The most adaptation\nand normalization techniques were developed for feed-forward NNs and, according\nto results in this paper, not all of them worked also with RNNs. For\nexperiments, we have chosen a well known and available TIMIT phone recognition\ntask. The phone recognition is much more sensitive to the quality of AM than\nlarge vocabulary task with a complex language model. Also, we published the\nopen-source scripts to easily replicate the results and to help continue the\ndevelopment.", "published": "2018-07-12 09:40:21", "link": "http://arxiv.org/abs/1807.06441v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The Bottleneck Simulator: A Model-based Deep Reinforcement Learning\n  Approach", "abstract": "Deep reinforcement learning has recently shown many impressive successes.\nHowever, one major obstacle towards applying such methods to real-world\nproblems is their lack of data-efficiency. To this end, we propose the\nBottleneck Simulator: a model-based reinforcement learning method which\ncombines a learned, factorized transition model of the environment with rollout\nsimulations to learn an effective policy from few examples. The learned\ntransition model employs an abstract, discrete (bottleneck) state, which\nincreases sample efficiency by reducing the number of model parameters and by\nexploiting structural properties of the environment. We provide a mathematical\nanalysis of the Bottleneck Simulator in terms of fixed points of the learned\npolicy, which reveals how performance is affected by four distinct sources of\nerror: an error related to the abstract space structure, an error related to\nthe transition model estimation variance, an error related to the transition\nmodel estimation bias, and an error related to the transition model class bias.\nFinally, we evaluate the Bottleneck Simulator on two natural language\nprocessing tasks: a text adventure game and a real-world, complex dialogue\nresponse selection task. On both tasks, the Bottleneck Simulator yields\nexcellent performance beating competing approaches.", "published": "2018-07-12 16:59:28", "link": "http://arxiv.org/abs/1807.04723v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NE", "stat.ML", "I.5.1; I.2.7"], "primary_category": "cs.LG"}
{"title": "Optimal Binaural LCMV Beamforming in Complex Acoustic Scenarios:\n  Theoretical and Practical Insights", "abstract": "Binaural beamforming algorithms for head-mounted assistive listening devices\nare crucial to improve speech quality and speech intelligibility in noisy\nenvironments, while maintaining the spatial impression of the acoustic scene.\nWhile the well-known BMVDR beamformer is able to preserve the binaural cues of\none desired source, the BLCMV beamformer uses additional constraints to also\npreserve the binaural cues of interfering sources. In this paper, we provide\ntheoretical and practical insights on how to optimally set the interference\nscaling parameters in the BLCMV beamformer for an arbitrary number of\ninterfering sources. In addition, since in practice only a limited temporal\nobservation interval is available to estimate all required beamformer\nquantities, we provide an experimental evaluation in a complex acoustic\nscenario using measured impulse responses from hearing aids in a cafeteria for\ndifferent observation intervals. The results show that even rather short\nobservation intervals are sufficient to achieve a decent noise reduction\nperformance and that a proposed threshold on the optimal interference scaling\nparameters leads to smaller binaural cue errors in practice.", "published": "2018-07-12 14:29:51", "link": "http://arxiv.org/abs/1807.04636v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
