{"title": "The Towers of Fibonacci, Lucas, Pell, and Jacobsthal", "abstract": "We present in this paper four new variants of the Tower of Hanoi problem, the\noptimal solution of each of these variants is related to one of the four known\nnumbers Fibonacci, Lucas, Pell, and Jacobsthal. We give an optimal solution to\neach of these variants, and we present their associated graphs.", "published": "2025-02-16 09:07:05", "link": "http://arxiv.org/abs/2502.11045v1", "categories": ["math.CO", "cs.DM", "05C57, 00A08, 03Dxx, 11B39"], "primary_category": "math.CO"}
{"title": "Generalized Factor Neural Network Model for High-dimensional Regression", "abstract": "We tackle the challenges of modeling high-dimensional data sets, particularly\nthose with latent low-dimensional structures hidden within complex, non-linear,\nand noisy relationships. Our approach enables a seamless integration of\nconcepts from non-parametric regression, factor models, and neural networks for\nhigh-dimensional regression. Our approach introduces PCA and Soft PCA layers,\nwhich can be embedded at any stage of a neural network architecture, allowing\nthe model to alternate between factor modeling and non-linear transformations.\nThis flexibility makes our method especially effective for processing\nhierarchical compositional data. We explore ours and other techniques for\nimposing low-rank structures on neural networks and examine how architectural\ndesign impacts model performance. The effectiveness of our method is\ndemonstrated through simulation studies, as well as applications to forecasting\nfuture price movements of equity ETF indices and nowcasting with macroeconomic\ndata.", "published": "2025-02-16 23:13:55", "link": "http://arxiv.org/abs/2502.11310v2", "categories": ["stat.ML", "cs.LG", "q-fin.ST", "62G08, 68T07"], "primary_category": "stat.ML"}
{"title": "Exploring Contextual Flux in Large Language Models: A Novel Approach to\n  Self-Modulating Semantic Networks", "abstract": "Self-modulating mechanisms introduce dynamic adaptation capabilities within\nlanguage models through contextual realignment strategies that influence token\nembedding trajectories across extended sequences. Contextual Flux is explored\nas an approach to embedding modulation, integrating an auxiliary gating\nmechanism within the self-attention framework to dynamically adjust token\nrepresentations based on evolving contextual dependencies. The empirical\nanalysis evaluates entropy variations, latent space realignments, and coherence\nstability to assess the extent to which self-regulation enhances text\ngeneration consistency while preserving generative flexibility. Quantitative\nassessments suggest that embedding shifts contribute to more structured\nadaptation in long-form sequences, with measured reductions in redundant phrase\nrepetitions and improvements in thematic retention. Variability in contextual\nweight computation affects modulation stability, leading to differing levels of\nadaptation across diverse linguistic structures. The computational demands\nintroduced through real-time embedding reconfiguration are examined in relation\nto model scalability, emphasizing the need for optimization strategies in\nhigh-volume generative applications. The findings suggest that while adaptive\nembedding updates improve certain aspects of coherence, their impact remains\ncontingent on model capacity and input complexity.", "published": "2025-02-16 01:08:19", "link": "http://arxiv.org/abs/2502.10942v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Akan Cinematic Emotions (ACE): A Multimodal Multi-party Dataset for\n  Emotion Recognition in Movie Dialogues", "abstract": "In this paper, we introduce the Akan Conversation Emotion (ACE) dataset, the\nfirst multimodal emotion dialogue dataset for an African language, addressing\nthe significant lack of resources for low-resource languages in emotion\nrecognition research. ACE, developed for the Akan language, contains 385\nemotion-labeled dialogues and 6,162 utterances across audio, visual, and\ntextual modalities, along with word-level prosodic prominence annotations. The\npresence of prosodic labels in this dataset also makes it the first\nprosodically annotated African language dataset. We demonstrate the quality and\nutility of ACE through experiments using state-of-the-art emotion recognition\nmethods, establishing solid baselines for future research. We hope ACE inspires\nfurther work on inclusive, linguistically and culturally diverse NLP resources.", "published": "2025-02-16 03:24:33", "link": "http://arxiv.org/abs/2502.10973v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Large language models on Understanding Korean indirect Speech\n  acts", "abstract": "To accurately understand the intention of an utterance is crucial in\nconversational communication. As conversational artificial intelligence models\nare rapidly being developed and applied in various fields, it is important to\nevaluate the LLMs' capabilities of understanding the intentions of user's\nutterance. This study evaluates whether current LLMs can understand the\nintention of an utterance by considering the given conversational context,\nparticularly in cases where the actual intention differs from the\nsurface-leveled, literal intention of the sentence, i.e. indirect speech acts.\nOur findings reveal that Claude3-Opus outperformed the other competing models,\nwith 71.94% in MCQ and 65% in OEQ, showing a clear advantage. In general,\nproprietary models exhibited relatively higher performance compared to\nopen-source models. Nevertheless, no LLMs reached the level of human\nperformance. Most LLMs, except for Claude3-Opus, demonstrated significantly\nlower performance in understanding indirect speech acts compared to direct\nspeech acts, where the intention is explicitly revealed through the utterance.\nThis study not only performs an overall pragmatic evaluation of each LLM's\nlanguage use through the analysis of OEQ response patterns, but also emphasizes\nthe necessity for further research to improve LLMs' understanding of indirect\nspeech acts for more natural communication with humans.", "published": "2025-02-16 04:59:19", "link": "http://arxiv.org/abs/2502.10995v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RAS: Retrieval-And-Structuring for Knowledge-Intensive LLM Generation", "abstract": "Retrieval-augmented language models often struggle with knowledge-intensive\ntasks due to inefficient retrieval, unstructured knowledge integration, and\nsingle-pass architectures. We present Retrieval-And-Structuring (RAS), a novel\nframework that dynamically constructs and reasons over query-specific knowledge\ngraphs through iterative retrieval and structuring. RAS introduces four key\ntechnical innovations: (1) a themescoped retrieval mechanism that efficiently\nnarrows the search space while maintaining retrieval quality, (2) an action\nplanning module that determines knowledge needs and generates focused\nsub-queries, (3) a dynamic knowledge structuring approach that converts\nretrieved text into an evolving knowledge graph, and (4) a graph-augmented\nanswering component that leverages the accumulated structured information. Our\nframework achieves state-of-the-art performance, surpassing leading baselines\nby 6.4% with open-source language models and 7.0% with proprietary models on\nseven knowledge-intensive generation datasets across all evaluation metrics.\nDetailed ablation studies verify the contribution of each technical component\nto the overall system performance.", "published": "2025-02-16 05:01:49", "link": "http://arxiv.org/abs/2502.10996v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CounterBench: A Benchmark for Counterfactuals Reasoning in Large\n  Language Models", "abstract": "Counterfactual reasoning is widely recognized as one of the most challenging\nand intricate aspects of causality in artificial intelligence. In this paper,\nwe evaluate the performance of large language models (LLMs) in counterfactual\nreasoning. In contrast to previous studies that primarily focus on commonsense\ncausal reasoning, where LLMs often rely on prior knowledge for inference, we\nspecifically assess their ability to perform counterfactual inference using a\nset of formal rules. To support this evaluation, we introduce a new benchmark\ndataset, CounterBench, comprising 1K counterfactual reasoning questions. The\ndataset is designed with varying levels of difficulty, diverse causal graph\nstructures, distinct types of counterfactual questions, and multiple\nnonsensical name variants. Our experiments demonstrate that counterfactual\nreasoning poses a significant challenge for LLMs, with most models performing\nat levels comparable to random guessing. To enhance LLM's counterfactual\nreasoning ability, we propose a novel reasoning paradigm, CoIn, which guides\nLLMs through iterative reasoning and backtracking to systematically explore\ncounterfactual solutions. Experimental results show that our method\nsignificantly improves LLM performance on counterfactual reasoning tasks and\nconsistently enhances performance across different LLMs.Our dataset is\navailable at https://huggingface.co/datasets/CounterBench/CounterBench.", "published": "2025-02-16 06:19:37", "link": "http://arxiv.org/abs/2502.11008v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "D\u00e9j\u00e0 Vu? Decoding Repeated Reading from Eye Movements", "abstract": "Be it your favorite novel, a newswire article, a cooking recipe or an\nacademic paper -- in many daily situations we read the same text more than\nonce. In this work, we ask whether it is possible to automatically determine\nwhether the reader has previously encountered a text based on their eye\nmovement patterns. We introduce two variants of this task and address them with\nconsiderable success using both feature-based and neural models. We further\nintroduce a general strategy for enhancing these models with machine generated\nsimulations of eye movements from a cognitive model. Finally, we present an\nanalysis of model performance which on the one hand yields insights on the\ninformation used by the models, and on the other hand leverages predictive\nmodeling as an analytic tool for better characterization of the role of memory\nin repeated reading. Our work advances the understanding of the extent and\nmanner in which eye movements in reading capture memory effects from prior text\nexposure, and paves the way for future applications that involve predictive\nmodeling of repeated reading.", "published": "2025-02-16 09:59:29", "link": "http://arxiv.org/abs/2502.11061v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Similarity: A Gradient-based Graph Method for Instruction Tuning\n  Data Selection", "abstract": "Large language models (LLMs) have shown great potential across various\nindustries due to their remarkable ability to generalize through instruction\ntuning. However, the limited availability of domain-specific data significantly\nhampers their performance on specialized tasks. While existing methods\nprimarily focus on selecting training data from general datasets that are\nsimilar to the target domain, they often fail to consider the joint\ndistribution of instructions, resulting in inefficient learning and suboptimal\nknowledge transfer. To address these challenges, we introduce G2IS\n(Gradient-based Graph Instruction Selection), a novel method that constructs a\nmixed gradient-based instruction graph to capture the joint distribution and\ninterdependencies between instructions. By accounting for the relationships\nbetween instructions, G2IS improves domain adaptation efficiency. Additionally,\nwe propose a gradient walk algorithm to refine the data selection process,\nenhancing both training effectiveness and efficiency. Our experiments\ndemonstrate that G2IS outperforms traditional methods across various domain\nadaptation tasks, yielding significant performance gains, particularly in\ncomplex, data-scarce scenarios. These results underscore the potential of G2IS\nin advancing the development of large, domain-specific models.", "published": "2025-02-16 10:06:00", "link": "http://arxiv.org/abs/2502.11062v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CARMA: Enhanced Compositionality in LLMs via Advanced Regularisation and\n  Mutual Information Alignment", "abstract": "Large language models (LLMs) struggle with compositional generalisation,\nlimiting their ability to systematically combine learned components to\ninterpret novel inputs. While architectural modifications, fine-tuning, and\ndata augmentation improve compositionality, they often have limited\nadaptability, face scalability constraints, or yield diminishing returns on\nreal data. To address this, we propose CARMA, an intervention that enhances the\nstability and robustness of compositional reasoning in LLMs while preserving\nfine-tuned performance. CARMA employs mutual information regularisation and\nlayer-wise stability constraints to mitigate feature fragmentation, ensuring\nstructured representations persist across and within layers. We evaluate CARMA\non inverse dictionary modelling and sentiment classification, measuring its\nimpact on semantic consistency, performance stability, and robustness to\nlexical perturbations. Results show that CARMA reduces the variability\nintroduced by fine-tuning, stabilises token representations, and improves\ncompositional reasoning. While its effectiveness varies across architectures,\nCARMA's key strength lies in reinforcing learned structures rather than\nintroducing new capabilities, making it a scalable auxiliary method. These\nfindings suggest that integrating CARMA with fine-tuning can improve\ncompositional generalisation while maintaining task-specific performance in\nLLMs.", "published": "2025-02-16 10:18:15", "link": "http://arxiv.org/abs/2502.11066v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Demystifying Hateful Content: Leveraging Large Multimodal Models for\n  Hateful Meme Detection with Explainable Decisions", "abstract": "Hateful meme detection presents a significant challenge as a multimodal task\ndue to the complexity of interpreting implicit hate messages and contextual\ncues within memes. Previous approaches have fine-tuned pre-trained\nvision-language models (PT-VLMs), leveraging the knowledge they gained during\npre-training and their attention mechanisms to understand meme content.\nHowever, the reliance of these models on implicit knowledge and complex\nattention mechanisms renders their decisions difficult to explain, which is\ncrucial for building trust in meme classification. In this paper, we introduce\nIntMeme, a novel framework that leverages Large Multimodal Models (LMMs) for\nhateful meme classification with explainable decisions. IntMeme addresses the\ndual challenges of improving both accuracy and explainability in meme\nmoderation. The framework uses LMMs to generate human-like, interpretive\nanalyses of memes, providing deeper insights into multimodal content and\ncontext. Additionally, it uses independent encoding modules for both memes and\ntheir interpretations, which are then combined to enhance classification\nperformance. Our approach addresses the opacity and misclassification issues\nassociated with PT-VLMs, optimizing the use of LMMs for hateful meme detection.\nWe demonstrate the effectiveness of IntMeme through comprehensive experiments\nacross three datasets, showcasing its superiority over state-of-the-art models.", "published": "2025-02-16 10:45:40", "link": "http://arxiv.org/abs/2502.11073v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DEEPER Insight into Your User: Directed Persona Refinement for Dynamic\n  Persona Modeling", "abstract": "To advance personalized applications such as recommendation systems and user\nbehavior prediction, recent research increasingly adopts large language models\n(LLMs) for human -readable persona modeling. In dynamic real -world scenarios,\neffective persona modeling necessitates leveraging streaming behavior data to\ncontinually optimize user personas. However, existing methods -whether\nregenerating personas or incrementally extending them with new behaviors -often\nfail to achieve sustained improvements in persona quality or future behavior\nprediction accuracy. To address this, we propose DEEPER, a novel approach for\ndynamic persona modeling that enables continual persona optimization.\nSpecifically, we enhance the model's direction -search capability through an\niterative reinforcement learning framework, allowing it to automatically\nidentify effective update directions and optimize personas using discrepancies\nbetween user behaviors and model predictions. Extensive experiments on dynamic\npersona modeling involving 4800 users across 10 domains highlight the superior\npersona optimization capabilities of DEEPER, delivering an impressive 32.2%\naverage reduction in user behavior prediction error over four update rounds\n-outperforming the best baseline by a remarkable 22.92%.", "published": "2025-02-16 11:02:37", "link": "http://arxiv.org/abs/2502.11078v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Streamlining the Collaborative Chain of Models into A Single Forward\n  Pass in Generation-Based Tasks", "abstract": "In Retrieval-Augmented Generation (RAG) and agent-based frameworks, the\n\"Chain of Models\" approach is widely used, where multiple specialized models\nwork sequentially on distinct sub-tasks. This approach is effective but\nincreases resource demands as each model must be deployed separately. Recent\nadvancements attempt to address this by applying prompt tuning, which allows a\nshared base model to adapt to multiple tasks with minimal parameter changes.\nHowever, a key challenge remains: intermediate outputs, passed between models\nas plain text, require recomputation of hidden states (i.e., Key and Value (KV)\nstates in Transformers) during inference. In this paper, we introduce FTHSS, a\nnovel prompt-tuning method that enables models to share KV hidden states,\neliminating redundant forward passes and reducing KV cache storage. By\nmodifying input and attention masks during training, FTHSS allows models to\neffectively utilize KV hidden states from prior models in both single- and\nmulti-round scenarios. Empirical results on four tasks show that FTHSS matches\nthe performance of traditional model chains while improving inference\nefficiency.", "published": "2025-02-16 11:37:14", "link": "http://arxiv.org/abs/2502.11083v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rewrite to Jailbreak: Discover Learnable and Transferable Implicit\n  Harmfulness Instruction", "abstract": "As Large Language Models (LLMs) are widely applied in various domains, the\nsafety of LLMs is increasingly attracting attention to avoid their powerful\ncapabilities being misused. Existing jailbreak methods create a forced\ninstruction-following scenario, or search adversarial prompts with prefix or\nsuffix tokens to achieve a specific representation manually or automatically.\nHowever, they suffer from low efficiency and explicit jailbreak patterns, far\nfrom the real deployment of mass attacks to LLMs. In this paper, we point out\nthat simply rewriting the original instruction can achieve a jailbreak, and we\nfind that this rewriting approach is learnable and transferable. We propose the\nRewrite to Jailbreak (R2J) approach, a transferable black-box jailbreak method\nto attack LLMs by iteratively exploring the weakness of the LLMs and\nautomatically improving the attacking strategy. The jailbreak is more efficient\nand hard to identify since no additional features are introduced. Extensive\nexperiments and analysis demonstrate the effectiveness of R2J, and we find that\nthe jailbreak is also transferable to multiple datasets and various types of\nmodels with only a few queries. We hope our work motivates further\ninvestigation of LLM safety.", "published": "2025-02-16 11:43:39", "link": "http://arxiv.org/abs/2502.11084v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey of Large Language Models in Psychotherapy: Current Landscape\n  and Future Directions", "abstract": "Mental health remains a critical global challenge, with increasing demand for\naccessible, effective interventions. Large language models (LLMs) offer\npromising solutions in psychotherapy by enhancing the assessment, diagnosis,\nand treatment of mental health conditions through dynamic, context-aware\ninteractions. This survey provides a comprehensive overview of the current\nlandscape of LLM applications in psychotherapy, highlighting the roles of LLMs\nin symptom detection, severity estimation, cognitive assessment, and\ntherapeutic interventions. We present a novel conceptual taxonomy to organize\nthe psychotherapy process into three core components: assessment, diagnosis,\nand treatment, and examine the challenges and advancements in each area. The\nsurvey also addresses key research gaps, including linguistic biases, limited\ndisorder coverage, and underrepresented therapeutic models. Finally, we discuss\nfuture directions to integrate LLMs into a holistic, end-to-end psychotherapy\nframework, addressing the evolving nature of mental health conditions and\nfostering more inclusive, personalized care.", "published": "2025-02-16 12:18:40", "link": "http://arxiv.org/abs/2502.11095v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Achieving Concept Completeness for Unsupervised Textual Concept\n  Bottleneck Models", "abstract": "Textual Concept Bottleneck Models (TBMs) are interpretable-by-design models\nfor text classification that predict a set of salient concepts before making\nthe final prediction. This paper proposes Complete Textual Concept Bottleneck\nModel (CT-CBM),a novel TCBM generator building concept labels in a fully\nunsupervised manner using a small language model, eliminating both the need for\npredefined human labeled concepts and LLM annotations. CT-CBM iteratively\ntargets and adds important concepts in the bottleneck layer to create a\ncomplete concept basis and addresses downstream classification leakage through\na parallel residual connection. CT-CBM achieves good results against\ncompetitors, offering a promising solution to enhance interpretability of NLP\nclassifiers without sacrificing performance.", "published": "2025-02-16 12:28:43", "link": "http://arxiv.org/abs/2502.11100v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Cross-Tokenizer Knowledge Distillation with Contextual\n  Dynamical Mapping", "abstract": "Knowledge Distillation (KD) has emerged as a prominent technique for model\ncompression. However, conventional KD approaches primarily focus on homogeneous\narchitectures with identical tokenizers, constraining their applicability in\ncross-architecture scenarios. As for the cross-tokenizer KD, the differences in\nthe tokenizers give rise to two fundamental challenges: (1) sequence\nmisalignment caused by divergent tokenization strategies, and (2) mismatched\nvocabulary size and composition. While existing probability-matching methods\nattempt to address these issues, their efficacy remains limited due to\nsuboptimal alignment in both the sequence and vocabulary aspects. To overcome\nthese limitations, we propose Contextual Dynamic Mapping (CDM), a novel\ncross-tokenizer distillation framework that employs contextual information to\nenhance sequence alignment precision and dynamically improves vocabulary\nmapping. We evaluated the effectiveness of our approach across five advanced\nand widely-used model families (i.e, LLama3, Phi3, Gemma2, OPT and Qwen2),\nwhich were configured into three distinct teacher-student pairs. Our method\nshows significant advantages over existing cross-tokenizer distillation\nbaselines across diverse benchmarks, including instruction-following, code\ngeneration and math. Notably, our analysis reveals that combining conventional\nsame-tokenizer distillation and cross-tokenizer distillation through CDM yields\nfurther performance improvements. The code is available at\nhttps://github.com/pppa2019/ContexualDynamicMapping", "published": "2025-02-16 12:46:07", "link": "http://arxiv.org/abs/2502.11104v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Valuable Hallucinations: Realizable Non-realistic Propositions", "abstract": "This paper introduces the first formal definition of valuable hallucinations\nin large language models (LLMs), addressing a gap in the existing literature.\nWe provide a systematic definition and analysis of hallucination value,\nproposing methods for enhancing the value of hallucinations. In contrast to\nprevious works, which often treat hallucinations as a broad flaw, we focus on\nthe potential value that certain types of hallucinations can offer in specific\ncontexts. Hallucinations in LLMs generally refer to the generation of\nunfaithful, fabricated, inconsistent, or nonsensical content. Rather than\nviewing all hallucinations negatively, this paper gives formal representations\nand manual judgments of \"valuable hallucinations\" and explores how realizable\nnon-realistic propositions--ideas that are not currently true but could be\nachievable under certain conditions--can have constructive value. We present\nexperiments using the Qwen2.5 model and HalluQA dataset, employing ReAct\nprompting (which involves reasoning, confidence assessment, and answer\nverification) to control and optimize hallucinations. Our findings show that\nReAct prompting results in a 5.12\\% reduction in overall hallucinations and an\nincrease in the proportion of valuable hallucinations from 6.45\\% to 7.92\\%.\nThese results demonstrate that systematically controlling hallucinations can\nimprove their usefulness without compromising factual reliability.", "published": "2025-02-16 12:59:11", "link": "http://arxiv.org/abs/2502.11113v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Pairwise: Global Zero-shot Temporal Graph Generation", "abstract": "Temporal relation extraction (TRE) is a fundamental task in natural language\nprocessing (NLP) that involves identifying the temporal relationships between\nevents in a document. Despite the advances in large language models (LLMs),\ntheir application to TRE remains limited. Most existing approaches rely on\npairwise classification, in which event pairs are considered individually,\nleading to computational inefficiency and a lack of global consistency in the\nresulting temporal graph. In this work, we propose a novel zero-shot method for\nTRE that generates a document's complete temporal graph at once, then applies\ntransitive constraints optimization to refine predictions and enforce temporal\nconsistency across relations. Additionally, we introduce OmniTemp, a new\ndataset with complete annotations for all pairs of targeted events within a\ndocument. Through experiments and analyses, we demonstrate that our method\nsignificantly outperforms existing zero-shot approaches while achieving\ncompetitive performance with supervised models.", "published": "2025-02-16 13:06:50", "link": "http://arxiv.org/abs/2502.11114v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are Generative Models Underconfident? An Embarrassingly Simple Quality\n  Estimation Approach", "abstract": "Quality Estimation (QE) is estimating the quality of model output when the\nground truth reference is not available. Looking at model uncertainty from its\nown output probabilities is the most trivial and low-effort way to estimate the\noutput quality. However, for generative model, output probabilities might not\nbe the best quality estimator. At an output step, there can be multiple correct\noptions, making the probability distribution spread out more. Thus, lower token\nprobability does not necessarily mean lower output quality. In other words, the\nmodel can be considered underconfident. In this paper, we propose a QE approach\ncalled Dominant Mass Probability (DMP}, that boosts the model confidence in\ncases where there are multiple viable output options. We show that, with no\nincrease in complexity, DMP is notably better than sequence probability when\nestimating the quality of different models (Whisper, Llama, etc.) on different\ntasks (translation, summarization, etc.). Compared to sequence probability, DMP\nachieves on average +0.208 improvement in Pearson correlation to ground-truth\nquality.", "published": "2025-02-16 13:12:31", "link": "http://arxiv.org/abs/2502.11115v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "DuplexMamba: Enhancing Real-time Speech Conversations with Duplex and\n  Streaming Capabilities", "abstract": "Real-time speech conversation is essential for natural and efficient\nhuman-machine interactions, requiring duplex and streaming capabilities.\nTraditional Transformer-based conversational chatbots operate in a turn-based\nmanner and exhibit quadratic computational complexity that grows as the input\nsize increases. In this paper, we propose DuplexMamba, a Mamba-based end-to-end\nmultimodal duplex model for speech-to-text conversation. DuplexMamba enables\nsimultaneous input processing and output generation, dynamically adjusting to\nsupport real-time streaming. Specifically, we develop a Mamba-based speech\nencoder and adapt it with a Mamba-based language model. Furthermore, we\nintroduce a novel duplex decoding strategy that enables DuplexMamba to process\ninput and generate output simultaneously. Experimental results demonstrate that\nDuplexMamba successfully implements duplex and streaming capabilities while\nachieving performance comparable to several recently developed\nTransformer-based models in automatic speech recognition (ASR) tasks and voice\nassistant benchmark evaluations. Our code and model are released.", "published": "2025-02-16 13:42:48", "link": "http://arxiv.org/abs/2502.11123v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Similar Case Retrieval Ranking Performance By Revisiting\n  RankSVM", "abstract": "Given the rapid development of Legal AI, a lot of attention has been paid to\none of the most important legal AI tasks--similar case retrieval, especially\nwith language models to use. In our paper, however, we try to improve the\nranking performance of current models from the perspective of learning to rank\ninstead of language models. Specifically, we conduct experiments using a\npairwise method--RankSVM as the classifier to substitute a fully connected\nlayer, combined with commonly used language models on similar case retrieval\ndatasets LeCaRDv1 and LeCaRDv2. We finally come to the conclusion that RankSVM\ncould generally help improve the retrieval performance on the LeCaRDv1 and\nLeCaRDv2 datasets compared with original classifiers by optimizing the precise\nranking. It could also help mitigate overfitting owing to class imbalance. Our\ncode is available in https://github.com/liuyuqi123study/RankSVM_for_SLR", "published": "2025-02-16 13:59:00", "link": "http://arxiv.org/abs/2502.11131v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Surprisal Takes It All: Eye Tracking Based Cognitive Evaluation of Text\n  Readability Measures", "abstract": "Text readability measures are widely used in many real-world scenarios and in\nNLP. These measures have primarily been developed by predicting reading\ncomprehension outcomes, while largely neglecting what is perhaps the core\naspect of a readable text: reading ease. In this work, we propose a new eye\ntracking based methodology for evaluating readability measures, which focuses\non their ability to account for reading facilitation effects in text\nsimplification, as well as for text reading ease more broadly. Using this\napproach, we find that existing readability formulas are moderate to poor\npredictors of reading ease. We further find that average per-word length,\nfrequency, and especially surprisal tend to outperform existing readability\nformulas as measures of reading ease. We thus propose surprisal as a simple\nunsupervised alternative to existing measures.", "published": "2025-02-16 14:51:44", "link": "http://arxiv.org/abs/2502.11150v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Constrained Monte Carlo Tree Search to Generate Reliable Long\n  Chain-of-Thought for Mathematical Reasoning", "abstract": "Recently, Long Chain-of-Thoughts (CoTs) have gained widespread attention for\nimproving the reasoning capabilities of Large Language Models (LLMs). This\nnecessitates that existing LLMs, which lack the ability to generate Long CoTs,\nto acquire such capability through post-training methods. Without additional\ntraining, LLMs typically enhance their mathematical reasoning abilities through\ninference scaling methods such as MCTS. However, they are hindered by the large\naction space and inefficient search strategies, making it challenging to\ngenerate Long CoTs effectively. To tackle this issue, we propose constraining\nthe action space and guiding the emergence of Long CoTs through a refined\nsearch strategy. In our proposed Constrained Monte Carlo Tree Search (C-MCTS)\nframework, we limit the actions selected from a constrained action space, which\nis divided into five disjoint subsets: \\emph{understanding}, \\emph{planning},\n\\emph{reflection}, \\emph{coding}, and \\emph{summary}. Each subset is further\nconstrained to a small number of predefined prompts, rather than allowing LLMs\nto generate actions arbitrarily. Additionally, we refine the search strategy by\nincorporating prior knowledge about the action sets, such as a human-like\npartial order of the action subsets and the pretrained process reward models.\nThese strategies work together to significantly reduce the vast search space of\nLong CoTs. Extensive evaluations on mathematical reasoning benchmarks show\nthat, under zero-shot settings, our method enables the 7B model to achieve\nreasoning capabilities that surpass those of the 72B model.", "published": "2025-02-16 15:39:57", "link": "http://arxiv.org/abs/2502.11169v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating Language Preference of Multilingual RAG Systems", "abstract": "Multilingual Retrieval-Augmented Generation (mRAG) systems enhance language\nmodels by integrating external multilingual information to produce\ncontext-aware responses. However, mRAG systems struggle with retrieving\nrelevant information due to linguistic variations between queries and\ndocuments, generating inconsistent responses when multilingual sources\nconflict. In this work, we systematically investigate language preferences in\nboth retrieval and generation of mRAG through a series of experiments. Our\nanalysis indicates that retrievers tend to prefer high-resource and query\nlanguages, yet this preference does not consistently improve generation\nperformance. Moreover, we observe that generators prefer the query language or\nLatin scripts, leading to inconsistent outputs. To overcome these issues, we\npropose Dual Knowledge Multilingual RAG (DKM-RAG), a simple yet effective\nframework that fuses translated multilingual passages with complementary model\nknowledge. Empirical results demonstrate that DKM-RAG mitigates language\npreference in generation and enhances performance across diverse linguistic\nsettings.", "published": "2025-02-16 15:54:05", "link": "http://arxiv.org/abs/2502.11175v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LogiDynamics: Unraveling the Dynamics of Logical Inference in Large\n  Language Model Reasoning", "abstract": "Modern large language models (LLMs) employ various forms of logical\ninference, both implicitly and explicitly, when addressing reasoning tasks.\nUnderstanding how to optimally leverage these inference paradigms is critical\nfor advancing LLMs' reasoning capabilities. This paper adopts an exploratory\napproach by introducing a controlled evaluation environment for analogical\nreasoning -- a fundamental cognitive task -- that is systematically\nparameterized across three dimensions: modality (textual, visual, symbolic),\ndifficulty (easy, medium, hard), and task format (multiple-choice or free-text\ngeneration). We analyze the comparative dynamics of inductive, abductive, and\ndeductive inference pipelines across these dimensions, and demonstrate that our\nfindings generalize to broader in-context learning tasks. Additionally, we\ninvestigate advanced paradigms such as hypothesis selection, verification, and\nrefinement, revealing their potential to scale up logical inference in LLM\nreasoning. This exploratory study provides a foundation for future research in\nenhancing LLM reasoning through systematic logical inference strategies.\nResources are available at https://github.com/HKUST-KnowComp/LogiDynamics.", "published": "2025-02-16 15:54:53", "link": "http://arxiv.org/abs/2502.11176v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Mirage of Model Editing: Revisiting Evaluation in the Wild", "abstract": "Despite near-perfect results in artificial evaluations, the effectiveness of\nmodel editing in real-world applications remains unexplored. To bridge this\ngap, we propose to study model editing in question answering (QA) by\nestablishing a rigorous evaluation practice to assess the effectiveness of\nediting methods in correcting LLMs' errors. It consists of QAEdit, a new\nbenchmark derived from popular QA datasets, and a standardized evaluation\nframework. Our single editing experiments indicate that current editing methods\nperform substantially worse than previously reported (38.5% vs. ~96%). Through\nmodule analysis and controlled experiments, we demonstrate that this\nperformance decline stems from issues in evaluation practices of prior editing\nresearch. One key issue is the inappropriate use of teacher forcing in testing\nprevents error propagation by feeding ground truth tokens (inaccessible in\nreal-world scenarios) as input. Furthermore, we simulate real-world deployment\nby sequential editing, revealing that current approaches fail drastically with\nonly 1000 edits. Our analysis provides a fundamental reexamination of both the\nreal-world applicability of existing model editing methods and their evaluation\npractices, and establishes a rigorous evaluation framework with key insights to\nadvance reliable and practical model editing research.", "published": "2025-02-16 15:57:55", "link": "http://arxiv.org/abs/2502.11177v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Don't Get Lost in the Trees: Streamlining LLM Reasoning by Overcoming\n  Tree Search Exploration Pitfalls", "abstract": "Recent advancements in tree search algorithms guided by verifiers have\nsignificantly enhanced the reasoning capabilities of large language models\n(LLMs), but at the cost of increased computational resources. In this work, we\nidentify two key challenges contributing to this inefficiency:\n$\\textit{over-exploration}$ due to redundant states with semantically\nequivalent content, and $\\textit{under-exploration}$ caused by high variance in\nverifier scoring leading to frequent trajectory switching. To address these\nissues, we propose FETCH, an e$\\textbf{f}$fici$\\textbf{e}$nt $\\textbf{t}$ree\nsear$\\textbf{ch}$ framework, which is a flexible, plug-and-play system\ncompatible with various tree search algorithms. Our framework mitigates\nover-exploration by merging semantically similar states using agglomerative\nclustering of text embeddings obtained from a fine-tuned SimCSE model. To\ntackle under-exploration, we enhance verifiers by incorporating temporal\ndifference learning with adjusted $\\lambda$-returns during training to reduce\nvariance, and employing a verifier ensemble to aggregate scores during\ninference. Experiments on GSM8K, GSM-Plus, and MATH datasets demonstrate that\nour methods significantly improve reasoning accuracy and computational\nefficiency across four different tree search algorithms, paving the way for\nmore practical applications of LLM-based reasoning. The code is available at\nhttps://github.com/Soistesimmer/Fetch.", "published": "2025-02-16 16:12:01", "link": "http://arxiv.org/abs/2502.11183v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models Penetration in Scholarly Writing and Peer Review", "abstract": "While the widespread use of Large Language Models (LLMs) brings convenience,\nit also raises concerns about the credibility of academic research and\nscholarly processes. To better understand these dynamics, we evaluate the\npenetration of LLMs across academic workflows from multiple perspectives and\ndimensions, providing compelling evidence of their growing influence. We\npropose a framework with two components: \\texttt{ScholarLens}, a curated\ndataset of human- and LLM-generated content across scholarly writing and peer\nreview for multi-perspective evaluation, and \\texttt{LLMetrica}, a tool for\nassessing LLM penetration using rule-based metrics and model-based detectors\nfor multi-dimensional evaluation. Our experiments demonstrate the effectiveness\nof \\texttt{LLMetrica}, revealing the increasing role of LLMs in scholarly\nprocesses. These findings emphasize the need for transparency, accountability,\nand ethical practices in LLM usage to maintain academic credibility.", "published": "2025-02-16 16:37:34", "link": "http://arxiv.org/abs/2502.11193v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Asymmetric Conflict and Synergy in Post-training for LLM-based\n  Multilingual Machine Translation", "abstract": "The emergence of Large Language Models (LLMs) has advanced the multilingual\nmachine translation (MMT), yet the Curse of Multilinguality (CoM) remains a\nmajor challenge. Existing work in LLM-based MMT typically mitigates this issue\nvia scaling up training and computation budget, which raises a critical\nquestion: Is scaling up the training and computation budget truly necessary for\nhigh-quality MMT, or can a deeper understanding of CoM provide a more efficient\nsolution? To explore this problem, we analyze the linguistic conflicts and\nsynergy, the underlying mechanism of CoM during post-training phase. We\nidentify an asymmetric phenomenon in linguistic conflicts and synergy: the\ndominance of conflicts and synergy varies in different translation directions,\nleading to sub-optimal adaptation in existing post-training methods. We further\nfind that a significant bottleneck in MMT appears to lie in post-training\nrather than multilingual pre-training, suggesting the need for more effective\nadaptation strategies. Building on these new insights, we propose a\ndirection-aware training approach, combined with group-wise model merging, to\naddress asymmetry in linguistic conflicts and synergy explicitly. Leveraging\nthis strategy, our method fine-tunes X-ALMA-13B-Pretrain-trained only with\nmultilingual pre-training-achieving comparable performance to XALMA-13B (only\nSFT) while using only 20B pretraining tokens and 17B parameters-5.5x fewer\npretraining-tokens and 1.7x fewer model size-with just 0.85 COMET drop on\nFlores-200 testsets of 50 languages.", "published": "2025-02-16 18:06:58", "link": "http://arxiv.org/abs/2502.11223v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Uncertainty-Aware Step-wise Verification with Generative Reward Models", "abstract": "Complex multi-step reasoning tasks, such as solving mathematical problems,\nremain challenging for large language models (LLMs). While outcome supervision\nis commonly used, process supervision via process reward models (PRMs) provides\nintermediate rewards to verify step-wise correctness in solution traces.\nHowever, as proxies for human judgement, PRMs suffer from reliability issues,\nincluding susceptibility to reward hacking. In this work, we propose leveraging\nuncertainty quantification (UQ) to enhance the reliability of step-wise\nverification with generative reward models for mathematical reasoning tasks. We\nintroduce CoT Entropy, a novel UQ method that outperforms existing approaches\nin quantifying a PRM's uncertainty in step-wise verification. Our results\ndemonstrate that incorporating uncertainty estimates improves the robustness of\njudge-LM PRMs, leading to more reliable verification.", "published": "2025-02-16 20:00:56", "link": "http://arxiv.org/abs/2502.11250v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Conditional Mutual Information to Improve Large Language\n  Model Fine-Tuning For Classification", "abstract": "Although large language models (LLMs) have demonstrated remarkable\ncapabilities in recent years, the potential of information theory (IT) to\nenhance LLM development remains underexplored. This paper introduces the\ninformation theoretic principle of Conditional Mutual Information (CMI) to LLM\nfine-tuning for classification tasks, exploring its promise in two main ways:\nminimizing CMI to improve a model's standalone performance and maximizing CMI\nto enhance knowledge distillation (KD) for more capable student models. To\napply CMI in LLM fine-tuning, we adapt the recently proposed CMI-constrained\ndeep learning framework, which was initially developed for image\nclassification, with some modification. By minimizing CMI during LLM\nfine-tuning, we achieve superior performance gains on 6 of 8 GLUE\nclassification tasks compared to BERT. Additionally, maximizing CMI during the\nKD process results in significant performance improvements in 6 of 8 GLUE\nclassification tasks compared to DistilBERT. These findings demonstrate CMI's\nadaptability for optimizing both standalone LLMs and student models, showcasing\nits potential as a robust framework for advancing LLM fine-tuning. Our work\nbridges the gap between information theory and LLM development, offering new\ninsights for building high-performing language models.", "published": "2025-02-16 20:24:00", "link": "http://arxiv.org/abs/2502.11258v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Shrinking Landscape of Linguistic Diversity in the Age of Large\n  Language Models", "abstract": "Language is far more than a communication tool. A wealth of information -\nincluding but not limited to the identities, psychological states, and social\ncontexts of its users - can be gleaned through linguistic markers, and such\ninsights are routinely leveraged across diverse fields ranging from product\ndevelopment and marketing to healthcare. In four studies utilizing experimental\nand observational methods, we demonstrate that the widespread adoption of large\nlanguage models (LLMs) as writing assistants is linked to notable declines in\nlinguistic diversity and may interfere with the societal and psychological\ninsights language provides. We show that while the core content of texts is\nretained when LLMs polish and rewrite texts, not only do they homogenize\nwriting styles, but they also alter stylistic elements in a way that\nselectively amplifies certain dominant characteristics or biases while\nsuppressing others - emphasizing conformity over individuality. By varying\nLLMs, prompts, classifiers, and contexts, we show that these trends are robust\nand consistent. Our findings highlight a wide array of risks associated with\nlinguistic homogenization, including compromised diagnostic processes and\npersonalization efforts, the exacerbation of existing divides and barriers to\nequity in settings like personnel selection where language plays a critical\nrole in assessing candidates' qualifications, communication skills, and\ncultural fit, and the undermining of efforts for cultural preservation.", "published": "2025-02-16 20:51:07", "link": "http://arxiv.org/abs/2502.11266v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improved Unbiased Watermark for Large Language Models", "abstract": "As artificial intelligence surpasses human capabilities in text generation,\nthe necessity to authenticate the origins of AI-generated content has become\nparamount. Unbiased watermarks offer a powerful solution by embedding\nstatistical signals into language model-generated text without distorting the\nquality. In this paper, we introduce MCmark, a family of unbiased,\nMulti-Channel-based watermarks. MCmark works by partitioning the model's\nvocabulary into segments and promoting token probabilities within a selected\nsegment based on a watermark key. We demonstrate that MCmark not only preserves\nthe original distribution of the language model but also offers significant\nimprovements in detectability and robustness over existing unbiased watermarks.\nOur experiments with widely-used language models demonstrate an improvement in\ndetectability of over 10% using MCmark, compared to existing state-of-the-art\nunbiased watermarks. This advancement underscores MCmark's potential in\nenhancing the practical application of watermarking in AI-generated texts.", "published": "2025-02-16 21:02:36", "link": "http://arxiv.org/abs/2502.11268v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cuckoo: An IE Free Rider Hatched by Massive Nutrition in LLM's Nest", "abstract": "Massive high-quality data, both pre-training raw texts and post-training\nannotations, have been carefully prepared to incubate advanced large language\nmodels (LLMs). In contrast, for information extraction (IE), pre-training data,\nsuch as BIO-tagged sequences, are hard to scale up. We show that IE models can\nact as free riders on LLM resources by reframing next-token \\emph{prediction}\ninto \\emph{extraction} for tokens already present in the context. Specifically,\nour proposed next tokens extraction (NTE) paradigm learns a versatile IE model,\n\\emph{Cuckoo}, with 102.6M extractive data converted from LLM's pre-training\nand post-training data. Under the few-shot setting, Cuckoo adapts effectively\nto traditional and complex instruction-following IE with better performance\nthan existing pre-trained IE models. As a free rider, Cuckoo can naturally\nevolve with the ongoing advancements in LLM data preparation, benefiting from\nimprovements in LLM training pipelines without additional manual effort.", "published": "2025-02-16 21:32:20", "link": "http://arxiv.org/abs/2502.11275v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Networks Remember More: The Power of Parameter Isolation and\n  Combination", "abstract": "Catastrophic forgetting is a pervasive issue for pre-trained language models\n(PLMs) during continual learning, where models lose previously acquired\nknowledge when sequentially trained on a series of tasks. The model's ability\nto retain old tasks is referred to as stability, while its adaptability to new\ntasks is called plasticity. Therefore, the key to solving this problem is to\nfind a trade-off between the plasticity and stability of the model. To address\nthis issue, in this paper, we propose a novel method to achieve a balance\nbetween model stability and plasticity, thereby mitigating catastrophic\nforgetting. More specifically, our proposed approach leverages parameter\nisolation and a subsequent combination strategy. Initially, in the training\nstage, the model adapts to each downstream task via a parameter isolation\nmethod to prevent potential interference among different tasks. We then combine\nall trained parameters, which contain acquired knowledge, using the task\narithmetic method and finally apply them to the backbone model. Empirical\nevaluations on continual language learning benchmarks substantiate the\neffectiveness of our approach, revealing a marked enhancement over existing\nstate-of-the-art approaches.", "published": "2025-02-16 02:58:57", "link": "http://arxiv.org/abs/2502.10966v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FinMTEB: Finance Massive Text Embedding Benchmark", "abstract": "Embedding models play a crucial role in representing and retrieving\ninformation across various NLP applications. Recent advances in large language\nmodels (LLMs) have further enhanced the performance of embedding models. While\nthese models are often benchmarked on general-purpose datasets, real-world\napplications demand domain-specific evaluation. In this work, we introduce the\nFinance Massive Text Embedding Benchmark (FinMTEB), a specialized counterpart\nto MTEB designed for the financial domain. FinMTEB comprises 64 financial\ndomain-specific embedding datasets across 7 tasks that cover diverse textual\ntypes in both Chinese and English, such as financial news articles, corporate\nannual reports, ESG reports, regulatory filings, and earnings call transcripts.\nWe also develop a finance-adapted model, Fin-E5, using a persona-based data\nsynthetic method to cover diverse financial embedding tasks for training.\nThrough extensive evaluation of 15 embedding models, including Fin-E5, we show\nthree key findings: (1) performance on general-purpose benchmarks shows limited\ncorrelation with financial domain tasks; (2) domain-adapted models consistently\noutperform their general-purpose counterparts; and (3) surprisingly, a simple\nBag-of-Words (BoW) approach outperforms sophisticated dense embeddings in\nfinancial Semantic Textual Similarity (STS) tasks, underscoring current\nlimitations in dense embedding techniques. Our work establishes a robust\nevaluation framework for financial NLP applications and provides crucial\ninsights for developing domain-specific embedding models.", "published": "2025-02-16 04:23:52", "link": "http://arxiv.org/abs/2502.10990v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "RoseRAG: Robust Retrieval-augmented Generation with Small-scale LLMs via\n  Margin-aware Preference Optimization", "abstract": "Large language models (LLMs) have achieved impressive performance but face\nhigh computational costs and latency, limiting their deployment in\nresource-constrained settings. In contrast, small-scale LLMs (SLMs) are more\nefficient yet struggle to capture evolving real-world knowledge.\nRetrieval-augmented generation (RAG) helps by integrating external knowledge,\nbut imperfect retrieval can introduce distracting noise that misleads SLMs. We\npropose RoseRAG, a robust RAG framework for SLMs via Margin-aware Preference\nOptimization. RoseRAG employs multi-turn prompting for detailed reasoning,\nrejection sampling for high-quality explanations, and contrastive preference\nselection to refine responses by maximizing the likelihood gap between\npreferred and non-preferred outputs. By integrating these components into a\nmargin-aware optimization process, RoseRAG robustly enhances the accuracy and\nreliability of SLMs for RAG applications. Extensive experiments on three\nopen-domain question answering benchmarks indicate that our innovative RoseRAG\nsurpasses state-of-the-art baselines significantly.", "published": "2025-02-16 04:56:53", "link": "http://arxiv.org/abs/2502.10993v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "GRIFFIN: Effective Token Alignment for Faster Speculative Decoding", "abstract": "Speculative decoding accelerates inference in large language models (LLMs) by\ngenerating multiple draft tokens simultaneously. However, existing methods\noften struggle with token misalignment between the training and decoding\nphases, limiting their performance. To address this, we propose GRIFFIN, a\nnovel framework that incorporates a token-alignable training strategy and a\ntoken-alignable draft model to mitigate misalignment. The training strategy\nemploys a loss masking mechanism to exclude highly misaligned tokens during\ntraining, preventing them from negatively impacting the draft model's\noptimization. The token-alignable draft model introduces input tokens to\ncorrect inconsistencies in generated features. Experiments on LLaMA-series and\nVicuna models demonstrate that GRIFFIN achieves an average acceptance length\nimprovement of over 7\\% and a speedup ratio exceeding 8%, outperforming current\nSoTAs as shown in Fig. 1 (a) and (b).", "published": "2025-02-16 07:06:00", "link": "http://arxiv.org/abs/2502.11018v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TUMLU: A Unified and Native Language Understanding Benchmark for Turkic\n  Languages", "abstract": "Being able to thoroughly assess massive multi-task language understanding\n(MMLU) capabilities is essential for advancing the applicability of\nmultilingual language models. However, preparing such benchmarks in high\nquality native language is often costly and therefore limits the\nrepresentativeness of evaluation datasets. While recent efforts focused on\nbuilding more inclusive MMLU benchmarks, these are conventionally built using\nmachine translation from high-resource languages, which may introduce errors\nand fail to account for the linguistic and cultural intricacies of the target\nlanguages. In this paper, we address the lack of native language MMLU benchmark\nespecially in the under-represented Turkic language family with distinct\nmorphosyntactic and cultural characteristics. We propose two benchmarks for\nTurkic language MMLU: TUMLU is a comprehensive, multilingual, and natively\ndeveloped language understanding benchmark specifically designed for Turkic\nlanguages. It consists of middle- and high-school level questions spanning 11\nacademic subjects in Azerbaijani, Crimean Tatar, Karakalpak, Kazakh, Tatar,\nTurkish, Uyghur, and Uzbek. We also present TUMLU-mini, a more concise,\nbalanced, and manually verified subset of the dataset. Using this dataset, we\nsystematically evaluate a diverse range of open and proprietary multilingual\nlarge language models (LLMs), including Claude, Gemini, GPT, and LLaMA,\noffering an in-depth analysis of their performance across different languages,\nsubjects, and alphabets. To promote further research and development in\nmultilingual language understanding, we release TUMLU-mini and all\ncorresponding evaluation scripts.", "published": "2025-02-16 07:07:38", "link": "http://arxiv.org/abs/2502.11020v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Leveraging Uncertainty Estimation for Efficient LLM Routing", "abstract": "Deploying large language models (LLMs) in edge-cloud environments requires an\nefficient routing strategy to balance cost and response quality. Traditional\napproaches prioritize either human-preference data or accuracy metrics from\nbenchmark datasets as routing criteria, but these methods suffer from rigidity\nand subjectivity. Moreover, existing routing frameworks primarily focus on\naccuracy and cost, neglecting response quality from a human preference\nperspective. In this work, we propose the Confidence-Driven LLM Router, a novel\nframework that leverages uncertainty estimation to optimize routing decisions.\nTo comprehensively assess routing performance, we evaluate both system cost\nefficiency and response quality. In particular, we introduce the novel use of\nLLM-as-a-Judge to simulate human rating preferences, providing the first\nsystematic assessment of response quality across different routing strategies.\nExtensive experiments on MT-Bench, GSM8K, and MMLU demonstrate that our\napproach outperforms state-of-the-art routing methods, achieving superior\nresponse quality while maintaining cost efficiency.", "published": "2025-02-16 07:08:47", "link": "http://arxiv.org/abs/2502.11021v1", "categories": ["cs.NI", "cs.CL"], "primary_category": "cs.NI"}
{"title": "MultiTEND: A Multilingual Benchmark for Natural Language to NoSQL Query\n  Translation", "abstract": "Natural language interfaces for NoSQL databases are increasingly vital in the\nbig data era, enabling users to interact with complex, unstructured data\nwithout deep technical expertise. However, most recent advancements focus on\nEnglish, leaving a gap for multilingual support. This paper introduces\nMultiTEND, the first and largest multilingual benchmark for natural language to\nNoSQL query generation, covering six languages: English, German, French,\nRussian, Japanese and Mandarin Chinese. Using MultiTEND, we analyze challenges\nin translating natural language to NoSQL queries across diverse linguistic\nstructures, including lexical and syntactic differences. Experiments show that\nperformance accuracy in both English and non-English settings remains\nrelatively low, with a 4%-6% gap across scenarios like fine-tuned SLM,\nzero-shot LLM, and RAG for LLM. To address the aforementioned challenges, we\nintroduce MultiLink, a novel framework that bridges the multilingual input to\nNoSQL query generation gap through a Parallel Linking Process. It breaks down\nthe task into multiple steps, integrating parallel multilingual processing,\nChain-of-Thought (CoT) reasoning, and Retrieval-Augmented Generation (RAG) to\ntackle lexical and structural challenges inherent in multilingual NoSQL\ngeneration. MultiLink shows enhancements in all metrics for every language\nagainst the top baseline, boosting execution accuracy by about 15% for English\nand averaging a 10% improvement for non-English languages.", "published": "2025-02-16 07:12:47", "link": "http://arxiv.org/abs/2502.11022v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mind the Confidence Gap: Overconfidence, Calibration, and Distractor\n  Effects in Large Language Models", "abstract": "Large Language Models (LLMs) demonstrate impressive performance across\ndiverse tasks, yet confidence calibration remains a challenge. Miscalibration -\nwhere models are overconfident or underconfident - poses risks, particularly in\nhigh-stakes applications. This paper presents an empirical study on LLM\ncalibration, examining how model size, distractors, and question types affect\nconfidence alignment. We introduce an evaluation framework to measure\noverconfidence and investigate whether multiple-choice formats mitigate or\nworsen miscalibration. Our findings show that while larger models (e.g.,\nGPT-4o) are better calibrated overall, they are more prone to distraction,\nwhereas smaller models benefit more from answer choices but struggle with\nuncertainty estimation. Unlike prior work, which primarily reports\nmiscalibration trends, we provide actionable insights into failure modes and\nconditions that worsen overconfidence. These findings highlight the need for\ncalibration-aware interventions and improved uncertainty estimation methods.", "published": "2025-02-16 07:46:09", "link": "http://arxiv.org/abs/2502.11028v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MMUnlearner: Reformulating Multimodal Machine Unlearning in the Era of\n  Multimodal Large Language Models", "abstract": "Recent progress in Machine Unlearning (MU) has introduced solutions for the\nselective removal of private or sensitive information encoded within deep\nneural networks. Nonetheless, MU for Multimodal Large Language Models (MLLMs)\nremains in its nascent phase. Therefore, we propose to reformulate the task of\nmultimodal MU in the era of MLLMs, which aims to erase only the visual patterns\nassociated with a given entity while preserving the corresponding textual\nknowledge encoded within the original parameters of the language model\nbackbone. Furthermore, we develop a novel geometry-constrained gradient descent\nmethod MMUnlearner. It updates the weights of MLLMs with a weight saliency map\njointly restricted by the remaining concepts and textual knowledge during\nunlearning, thereby preserving parameters essential for non-target knowledge.\nExtensive experiments demonstrate that MMUnlearner surpasses baselines that\nfinetuning MLLMs with VQA data directly through Gradient Ascent (GA) or\nNegative Preference Optimization (NPO), across all evaluation dimensions. Our\ncode will be released upon acceptance.", "published": "2025-02-16 09:23:50", "link": "http://arxiv.org/abs/2502.11051v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exposing Numeracy Gaps: A Benchmark to Evaluate Fundamental Numerical\n  Abilities in Large Language Models", "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nnatural language processing tasks, such as text generation and semantic\nunderstanding. However, their performance on numerical reasoning tasks, such as\nbasic arithmetic, numerical retrieval, and magnitude comparison, remains\nsurprisingly poor. This gap arises from their reliance on surface-level\nstatistical patterns rather than understanding numbers as continuous\nmagnitudes. Existing benchmarks primarily focus on either linguistic competence\nor structured mathematical problem-solving, neglecting fundamental numerical\nreasoning required in real-world scenarios. To bridge this gap, we propose\nNumericBench, a comprehensive benchmark to evaluate six fundamental numerical\ncapabilities: number recognition, arithmetic operations, contextual retrieval,\ncomparison, summary, and logical reasoning. NumericBench includes datasets\nranging from synthetic number lists to the crawled real-world data, addressing\nchallenges like long contexts, noise, and multi-step reasoning. Extensive\nexperiments on state-of-the-art LLMs, including GPT-4 and DeepSeek, reveal\npersistent weaknesses in numerical reasoning, highlighting the urgent need to\nimprove numerically-aware language modeling. The benchmark is released in:\nhttps://github.com/TreeAI-Lab/NumericBench.", "published": "2025-02-16 10:48:28", "link": "http://arxiv.org/abs/2502.11075v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SafeDialBench: A Fine-Grained Safety Benchmark for Large Language Models\n  in Multi-Turn Dialogues with Diverse Jailbreak Attacks", "abstract": "With the rapid advancement of Large Language Models (LLMs), the safety of\nLLMs has been a critical concern requiring precise assessment. Current\nbenchmarks primarily concentrate on single-turn dialogues or a single jailbreak\nattack method to assess the safety. Additionally, these benchmarks have not\ntaken into account the LLM's capability of identifying and handling unsafe\ninformation in detail. To address these issues, we propose a fine-grained\nbenchmark SafeDialBench for evaluating the safety of LLMs across various\njailbreak attacks in multi-turn dialogues. Specifically, we design a two-tier\nhierarchical safety taxonomy that considers 6 safety dimensions and generates\nmore than 4000 multi-turn dialogues in both Chinese and English under 22\ndialogue scenarios. We employ 7 jailbreak attack strategies, such as reference\nattack and purpose reverse, to enhance the dataset quality for dialogue\ngeneration. Notably, we construct an innovative assessment framework of LLMs,\nmeasuring capabilities in detecting, and handling unsafe information and\nmaintaining consistency when facing jailbreak attacks. Experimental results\nacross 17 LLMs reveal that Yi-34B-Chat and GLM4-9B-Chat demonstrate superior\nsafety performance, while Llama3.1-8B-Instruct and o3-mini exhibit safety\nvulnerabilities.", "published": "2025-02-16 12:08:08", "link": "http://arxiv.org/abs/2502.11090v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mixture of Tunable Experts -- Behavior Modification of DeepSeek-R1 at\n  Inference Time", "abstract": "We present the Mixture-of-Tunable-Experts (MoTE), a method that extends the\nMixture-of-Experts architecture of Large Language Models (LLMs). Without\nadditional training, MoTE enables meaningful and focused behavior changes in\nLLMs on-the-fly during inference time. By analyzing the digital LLM brain of\nDeepSeek-R1 using a technique we dub 'functional Token Resonance Imaging'\n(fTRI) -- inspired by fMRI and using prompts designed to elicit specific\nbehavior (e.g., 'What happened {time}{place}?') -- we empirically identify\ndistinctive experts associated with behaviors like refusal responses. Using\nMoTE we are able to intervene and control such specific behavior. We switched\noff the top 10 most refusal-relevant experts (0.07% of R1's 14,848 routed\nexperts), achieving a 52% refusal reduction on sensitive reference prompts\nwithout performance degradation on MT-Bench. Random expert deactivation\nresulted in smaller behavioral shifts with increased noise, whereas forced\nexpert activation led to significantly higher refusal rates. Our approach\nshares similarities with sparse autoencoders (SAEs) in terms of explainability\nand steerability. Unlike SAEs, MoTE does not require large training efforts, as\nwithin MoEs with a vast number of experts, specialization already emerged\nnaturally during pretraining. Our findings suggest that significant functional\nmechanisms in Mixture-of-Experts architectures can at least partially be\nlocalized in a small number of specific experts, rather than being distributed\nthroughout the model's weights. Expert subgroups can be tuned to trigger\nsignificant behavior variations, providing insights into the inner workings of\nLLMs.", "published": "2025-02-16 12:24:39", "link": "http://arxiv.org/abs/2502.11096v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "CacheFocus: Dynamic Cache Re-Positioning for Efficient\n  Retrieval-Augmented Generation", "abstract": "Large Language Models (LLMs) excel across a variety of language tasks yet are\nconstrained by limited input lengths and high computational costs. Existing\napproaches\\textemdash such as relative positional encodings (e.g., RoPE, ALiBi)\nand sliding window mechanisms\\textemdash partially alleviate these issues but\noften require additional training or suffer from performance degradation with\nlonger inputs. In this paper, we introduce \\textbf{\\textit{CacheFocus}}, a\nmethod that enhances length normalization and reduces inference latency without\nany further training. Our approach leverages query-independent, offline caching\nto efficiently reuse a Context KV Cache Store. We address the amplification of\nabnormal token distributions problem by re-positioning cached keys and\nintroducing Layer-Adaptive Cache Pruning to discard low-relevance caches during\npre-filling. Additionally, our Adaptive Positional Allocation Strategy\ndynamically reassigns cache positions to maximize the use of the available\npositional encoding range. Experiments on the Natural Questions and TriviaQA\ndatasets demonstrate that CacheFocus outperforms alternative methods even when\ninputs exceed the $4$K limit of the \\texttt{LLaMA-2} model, emphasizing its\npractical effectiveness for long-context LLMs. Moreover, even with large\nmaximum input length of \\texttt{Qwen2}, the performance of CacheFocus shows\nthat it maintains consistent performance even as the number of documents\nincreases, effectively managing long-text generation without degradation.", "published": "2025-02-16 12:33:16", "link": "http://arxiv.org/abs/2502.11101v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Knowledge Graph-Driven Retrieval-Augmented Generation: Integrating\n  Deepseek-R1 with Weaviate for Advanced Chatbot Applications", "abstract": "Large language models (LLMs) have significantly advanced the field of natural\nlanguage generation. However, they frequently generate unverified outputs,\nwhich compromises their reliability in critical applications. In this study, we\npropose an innovative framework that combines structured biomedical knowledge\nwith LLMs through a retrieval-augmented generation technique. Our system\ndevelops a thorough knowledge graph by identifying and refining causal\nrelationships and named entities from medical abstracts related to age-related\nmacular degeneration (AMD). Using a vector-based retrieval process and a\nlocally deployed language model, our framework produces responses that are both\ncontextually relevant and verifiable, with direct references to clinical\nevidence. Experimental results show that this method notably decreases\nhallucinations, enhances factual precision, and improves the clarity of\ngenerated responses, providing a robust solution for advanced biomedical\nchatbot applications.", "published": "2025-02-16 12:52:28", "link": "http://arxiv.org/abs/2502.11108v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Gumbel Reranking: Differentiable End-to-End Reranker Optimization", "abstract": "RAG systems rely on rerankers to identify relevant documents. However,\nfine-tuning these models remains challenging due to the scarcity of annotated\nquery-document pairs. Existing distillation-based approaches suffer from\ntraining-inference misalignment and fail to capture interdependencies among\ncandidate documents. To overcome these limitations, we reframe the reranking\nprocess as an attention-mask problem and propose Gumbel Reranking, an\nend-to-end training framework for rerankers aimed at minimizing the\ntraining-inference gap. In our approach, reranker optimization is reformulated\nas learning a stochastic, document-wise Top-$k$ attention mask using the Gumbel\nTrick and Relaxed Top-$k$ Sampling. This formulation enables end-to-end\noptimization by minimizing the overall language loss. Experiments across\nvarious settings consistently demonstrate performance gains, including a 10.4\\%\nimprovement in recall on HotpotQA for distinguishing indirectly relevant\ndocuments.", "published": "2025-02-16 13:23:39", "link": "http://arxiv.org/abs/2502.11116v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Safety Evaluation of DeepSeek Models in Chinese Contexts", "abstract": "Recently, the DeepSeek series of models, leveraging their exceptional\nreasoning capabilities and open-source strategy, is reshaping the global AI\nlandscape. Despite these advantages, they exhibit significant safety\ndeficiencies. Research conducted by Robust Intelligence, a subsidiary of Cisco,\nin collaboration with the University of Pennsylvania, revealed that DeepSeek-R1\nhas a 100\\% attack success rate when processing harmful prompts. Additionally,\nmultiple safety companies and research institutions have confirmed critical\nsafety vulnerabilities in this model. As models demonstrating robust\nperformance in Chinese and English, DeepSeek models require equally crucial\nsafety assessments in both language contexts. However, current research has\npredominantly focused on safety evaluations in English environments, leaving a\ngap in comprehensive assessments of their safety performance in Chinese\ncontexts. In response to this gap, this study introduces CHiSafetyBench, a\nChinese-specific safety evaluation benchmark. This benchmark systematically\nevaluates the safety of DeepSeek-R1 and DeepSeek-V3 in Chinese contexts,\nrevealing their performance across safety categories. The experimental results\nquantify the deficiencies of these two models in Chinese contexts, providing\nkey insights for subsequent improvements. It should be noted that, despite our\nefforts to establish a comprehensive, objective, and authoritative evaluation\nbenchmark, the selection of test samples, characteristics of data distribution,\nand the setting of evaluation criteria may inevitably introduce certain biases\ninto the evaluation results. We will continuously optimize the evaluation\nbenchmark and periodically update this report to provide more comprehensive and\naccurate assessment outcomes. Please refer to the latest version of the paper\nfor the most recent evaluation results and conclusions.", "published": "2025-02-16 14:05:54", "link": "http://arxiv.org/abs/2502.11137v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Uncertainty-Aware Search and Value Models: Mitigating Search Scaling\n  Flaws in LLMs", "abstract": "Value model-guided search is effective in steering the generation but suffers\nfrom scaling flaws: Its superiority diminishes with larger sample sizes,\nunderperforming non-search baselines. This limitation arises from reliability\ndegradation in value models in unseen reasoning paths. To address this, we\npropose an uncertainty-aware search framework that includes two key components:\n(1) uncertainty-aware value models that incorporate uncertainty into\npredictions, and (2) an uncertainty-aware selection process using the proposed\nefficient Group Thompson Sampling algorithm. Experiments on GSM8K show that our\nmethod mitigates search scaling flaws, achieving 90.5% coverage at 16 samples\ncompared to 85.8% for conventional value-guided search. This work establishes\nthe first systematic integration of uncertainty quantification in LLM search\nparadigms.", "published": "2025-02-16 15:10:30", "link": "http://arxiv.org/abs/2502.11155v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "VLMs as GeoGuessr Masters: Exceptional Performance, Hidden Biases, and\n  Privacy Risks", "abstract": "Visual-Language Models (VLMs) have shown remarkable performance across\nvarious tasks, particularly in recognizing geographic information from images.\nHowever, significant challenges remain, including biases and privacy concerns.\nTo systematically address these issues in the context of geographic information\nrecognition, we introduce a benchmark dataset consisting of 1,200 images paired\nwith detailed geographic metadata. Evaluating four VLMs, we find that while\nthese models demonstrate the ability to recognize geographic information from\nimages, achieving up to $53.8\\%$ accuracy in city prediction, they exhibit\nsignificant regional biases. Specifically, performance is substantially higher\nfor economically developed and densely populated regions compared to less\ndeveloped ($-12.5\\%$) and sparsely populated ($-17.0\\%$) areas. Moreover, the\nmodels exhibit regional biases, frequently overpredicting certain locations;\nfor instance, they consistently predict Sydney for images taken in Australia.\nThe strong performance of VLMs also raises privacy concerns, particularly for\nusers who share images online without the intent of being identified. Our code\nand dataset are publicly available at\nhttps://github.com/uscnlp-lime/FairLocator.", "published": "2025-02-16 15:28:34", "link": "http://arxiv.org/abs/2502.11163v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "SURGE: On the Potential of Large Language Models as General-Purpose\n  Surrogate Code Executors", "abstract": "Neural surrogate models have emerged as powerful and efficient tools in data\nmining. Meanwhile, large language models (LLMs) have demonstrated remarkable\ncapabilities in code-related tasks. We investigate a novel application: using\nLLMs as surrogate models for code execution prediction. Given LLMs' unique\nability to understand and process diverse programs, they present a promising\ndirection for building general-purpose surrogate models. To systematically\ninvestigate this capability, we introduce SURGE, a comprehensive benchmark with\n$1160$ problems covering $8$ key aspects: multi-language programming tasks,\ncompetition-level programming problems, repository-level code analysis,\nhigh-cost scientific computing, time-complexity-intensive algorithms, buggy\ncode analysis, programs dependent on specific compilers or execution\nenvironments, and formal mathematical proof verification. Through extensive\nempirical analysis of $21$ open-source and proprietary LLMs, we examine scaling\nlaws, data efficiency, and predictive accuracy. Our findings reveal important\ninsights about the feasibility of LLMs as efficient surrogates for\ncomputational processes, with implications for automated software testing,\nprogram analysis, and computational resource optimization in data mining\napplications. Code and dataset are released at\nhttps://github.com/Imbernoulli/SURGE.", "published": "2025-02-16 15:38:19", "link": "http://arxiv.org/abs/2502.11167v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "TituLLMs: A Family of Bangla LLMs with Comprehensive Benchmarking", "abstract": "In this paper, we present TituLLMs, the first large pretrained Bangla LLMs,\navailable in 1b and 3b parameter sizes. Due to computational constraints during\nboth training and inference, we focused on smaller models. To train TituLLMs,\nwe collected a pretraining dataset of approximately ~37 billion tokens. We\nextended the Llama-3.2 tokenizer to incorporate language- and culture-specific\nknowledge, which also enables faster training and inference. There was a lack\nof benchmarking datasets to benchmark LLMs for Bangla. To address this gap, we\ndeveloped five benchmarking datasets. We benchmarked various LLMs, including\nTituLLMs, and demonstrated that TituLLMs outperforms its initial multilingual\nversions. However, this is not always the case, highlighting the complexities\nof language adaptation. Our work lays the groundwork for adapting existing\nmultilingual open models to other low-resource languages. To facilitate broader\nadoption and further research, we have made the TituLLMs models and\nbenchmarking datasets publicly available\n(https://huggingface.co/collections/hishab/titulm-llama-family-6718d31fc1b83529276f490a).", "published": "2025-02-16 16:22:23", "link": "http://arxiv.org/abs/2502.11187v2", "categories": ["cs.CL", "cs.AI", "68T50", "F.2.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "ANCHOLIK-NER: A Benchmark Dataset for Bangla Regional Named Entity\n  Recognition", "abstract": "ANCHOLIK-NER is a linguistically diverse dataset for Named Entity Recognition\n(NER) in Bangla regional dialects, capturing variations across Sylhet,\nChittagong, Barishal, Noakhali, and Mymensingh. The dataset has around 17,405\nsentences, 3,481 sentences per region. The data was collected from two publicly\navailable datasets and through web scraping from various online newspapers,\narticles. To ensure high-quality annotations, the BIO tagging scheme was\nemployed, and professional annotators with expertise in regional dialects\ncarried out the labeling process. The dataset is structured into separate\nsubsets for each region and is available in CSV format. Each entry contains\ntextual data along with identified named entities and their corresponding\nannotations. Named entities are categorized into ten distinct classes: Person,\nLocation, Organization, Food, Animal, Colour, Role, Relation, Object, and\nMiscellaneous. This dataset serves as a valuable resource for developing and\nevaluating NER models for Bangla dialectal variations, contributing to regional\nlanguage processing and low-resource NLP applications. It can be utilized to\nenhance NER systems in Bangla dialects, improve regional language\nunderstanding, and support applications in machine translation, information\nretrieval, and conversational AI.", "published": "2025-02-16 16:59:10", "link": "http://arxiv.org/abs/2502.11198v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PlanGenLLMs: A Modern Survey of LLM Planning Capabilities", "abstract": "LLMs have immense potential for generating plans, transforming an initial\nworld state into a desired goal state. A large body of research has explored\nthe use of LLMs for various planning tasks, from web navigation to travel\nplanning and database querying. However, many of these systems are tailored to\nspecific problems, making it challenging to compare them or determine the best\napproach for new tasks. There is also a lack of clear and consistent evaluation\ncriteria. Our survey aims to offer a comprehensive overview of current LLM\nplanners to fill this gap. It builds on foundational work by Kartam and Wilkins\n(1990) and examines six key performance criteria: completeness, executability,\noptimality, representation, generalization, and efficiency. For each, we\nprovide a thorough analysis of representative works and highlight their\nstrengths and weaknesses. Our paper also identifies crucial future directions,\nmaking it a valuable resource for both practitioners and newcomers interested\nin leveraging LLM planning to support agentic workflows.", "published": "2025-02-16 17:54:57", "link": "http://arxiv.org/abs/2502.11221v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Vendi-RAG: Adaptively Trading-Off Diversity And Quality Significantly\n  Improves Retrieval Augmented Generation With LLMs", "abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs)\nfor domain-specific question-answering (QA) tasks by leveraging external\nknowledge sources. However, traditional RAG systems primarily focus on\nrelevance-based retrieval and often struggle with redundancy, especially when\nreasoning requires connecting information from multiple sources. This paper\nintroduces Vendi-RAG, a framework based on an iterative process that jointly\noptimizes retrieval diversity and answer quality. This joint optimization leads\nto significantly higher accuracy for multi-hop QA tasks. Vendi-RAG leverages\nthe Vendi Score (VS), a flexible similarity-based diversity metric, to promote\nsemantic diversity in document retrieval. It then uses an LLM judge that\nevaluates candidate answers, generated after a reasoning step, and outputs a\nscore that the retriever uses to balance relevance and diversity among the\nretrieved documents during each iteration. Experiments on three challenging\ndatasets -- HotpotQA, MuSiQue, and 2WikiMultiHopQA -- demonstrate Vendi-RAG's\neffectiveness in multi-hop reasoning tasks. The framework achieves significant\naccuracy improvements over traditional single-step and multi-step RAG\napproaches, with accuracy increases reaching up to +4.2% on HotpotQA, +4.1% on\n2WikiMultiHopQA, and +1.3% on MuSiQue compared to Adaptive-RAG, the current\nbest baseline. The benefits of Vendi-RAG are even more pronounced as the number\nof retrieved documents increases. Finally, we evaluated Vendi-RAG across\ndifferent LLM backbones, including GPT-3.5, GPT-4, and GPT-4o-mini, and\nobserved consistent improvements, demonstrating that the framework's advantages\nare model-agnostic.", "published": "2025-02-16 18:46:10", "link": "http://arxiv.org/abs/2502.11228v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Soteria: Language-Specific Functional Parameter Steering for\n  Multilingual Safety Alignment", "abstract": "Ensuring consistent safety across multiple languages remains a significant\nchallenge for large language models (LLMs). We introduce Soteria, a lightweight\nyet powerful strategy that locates and minimally adjusts the \"functional heads\"\nmost responsible for harmful content generation in each language. By altering\nonly a fraction of parameters, Soteria drastically reduces policy violations\nwithout sacrificing overall model performance, even in low-resource settings.\nTo rigorously evaluate our approach, we also present XThreatBench, a\nspecialized multilingual dataset capturing fine-grained harmful behaviors drawn\nfrom real policy guidelines. Experiments with leading open-source LLMs (e.g.,\nLlama, Qwen, Mistral) show that Soteria consistently improves safety metrics\nacross high-, mid-, and low-resource languages. These findings highlight a\npromising path toward scalable, linguistically attuned, and ethically aligned\nLLMs worldwide.", "published": "2025-02-16 19:44:01", "link": "http://arxiv.org/abs/2502.11244v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Rotary Position Embedding May Cause Dimension Inefficiency in\n  Attention Heads for Long-Distance Retrieval", "abstract": "The Rotary Position Embedding (RoPE) is widely used in the attention heads of\nmany large language models (LLM). It rotates dimensions in the query and the\nkey vectors by different angles according to their positions in the input\nsequence. For long context modeling, the range of positions may vary a lot, and\nthus RoPE rotates some dimensions by a great range of angles. We hypothesize\nthat the wide range of rotation angles may prevent LLMs from utilizing those\ndimensions. To validate this hypothesis, we present a controlled experiment\nshowing that applying RoPE causes low utility of certain dimensions. Our\nanalyses on three LLMs also indicate that these dimensions do not help LLMs do\nlong-context question answering.", "published": "2025-02-16 21:32:29", "link": "http://arxiv.org/abs/2502.11276v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Smoothing Out Hallucinations: Mitigating LLM Hallucination with Smoothed\n  Knowledge Distillation", "abstract": "Large language models (LLMs) often suffer from hallucination, generating\nfactually incorrect or ungrounded content, which limits their reliability in\nhigh-stakes applications. A key factor contributing to hallucination is the use\nof hard labels during training, which enforce deterministic supervision,\nencourage overconfidence, and disregard the uncertainty inherent in natural\nlanguage. To address this, we propose mitigating hallucination through\nknowledge distillation (KD), where a teacher model provides smoothed soft\nlabels to a student model, reducing overconfidence and improving factual\ngrounding. We apply KD during supervised finetuning on instructional data,\nevaluating its effectiveness across LLMs from different families. Experimental\nresults on summarization benchmarks demonstrate that KD reduces hallucination\ncompared to standard finetuning while preserving performance on general NLP\ntasks. These findings highlight KD as a promising approach for mitigating\nhallucination in LLMs and improving model reliability.", "published": "2025-02-16 23:05:36", "link": "http://arxiv.org/abs/2502.11306v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Efficient and Effective Prompt Tuning via Prompt Decomposition and\n  Compressed Outer Product", "abstract": "Prompt tuning (PT) offers a cost-effective alternative to fine-tuning\nlarge-scale pre-trained language models (PLMs), requiring only a few parameters\nin soft prompt tokens added before the input text. However, existing PT\napproaches face two significant issues: (i) They overlook intrinsic semantic\nassociations between soft prompt tokens, leading to high discreteness and\nlimited interactions, thus reducing the model's comprehension and effectiveness\nin complex tasks. (ii) Due to the complexity of downstream tasks, long soft\nprompt is necessitated to improve performance, but prompt length correlates\npositively with memory usage and computational costs. Achieving high efficiency\nand performance remains an ongoing challenge. To address these issues, we\npropose a novel Low-parameters prompt tuning (LAMP) method, which leverages\nprompt decomposition and compressed outer product. Specifically, the prompt\ndecomposition module employs Truncated SVD to reduce training parameters and\nsignificantly lower the dimensionality of the soft prompt parameter space. It\nthen utilizes a compressed outer product module to facilitate multiple\ninteractions among prompt tokens, exploring their intrinsic associations to\nenhance knowledge representation. Finally, LAMP uses average pooling to reduce\nmemory usage and training/inference time. Extensive experiments across six\narchitectures and eight datasets demonstrate that LAMP outperforms\nstate-of-the-art PT-based and LoRA-based methods in performance and efficiency.", "published": "2025-02-16 05:50:12", "link": "http://arxiv.org/abs/2502.12200v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Predicting Depression in Screening Interviews from Interactive\n  Multi-Theme Collaboration", "abstract": "Automatic depression detection provides cues for early clinical intervention\nby clinicians. Clinical interviews for depression detection involve dialogues\ncentered around multiple themes. Existing studies primarily design end-to-end\nneural network models to capture the hierarchical structure of clinical\ninterview dialogues. However, these methods exhibit defects in modeling the\nthematic content of clinical interviews: 1) they fail to capture intra-theme\nand inter-theme correlation explicitly, and 2) they do not allow clinicians to\nintervene and focus on themes of interest. To address these issues, this paper\nintroduces an interactive depression detection framework. This framework\nleverages in-context learning techniques to identify themes in clinical\ninterviews and then models both intra-theme and inter-theme correlation.\nAdditionally, it employs AI-driven feedback to simulate the interests of\nclinicians, enabling interactive adjustment of theme importance. PDIMC achieves\nabsolute improvements of 35\\% and 12\\% compared to the state-of-the-art on the\ndepression detection dataset DAIC-WOZ, which demonstrates the effectiveness of\nmodeling theme correlation and incorporating interactive external feedback.", "published": "2025-02-16 12:37:16", "link": "http://arxiv.org/abs/2502.12204v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SCALE: Towards Collaborative Content Analysis in Social Science with\n  Large Language Model Agents and Human Intervention", "abstract": "Content analysis breaks down complex and unstructured texts into\ntheory-informed numerical categories. Particularly, in social science, this\nprocess usually relies on multiple rounds of manual annotation, domain expert\ndiscussion, and rule-based refinement. In this paper, we introduce SCALE, a\nnovel multi-agent framework that effectively $\\underline{\\textbf{S}}$imulates\n$\\underline{\\textbf{C}}$ontent $\\underline{\\textbf{A}}$nalysis via\n$\\underline{\\textbf{L}}$arge language model (LLM)\nag$\\underline{\\textbf{E}}$nts. SCALE imitates key phases of content analysis,\nincluding text coding, collaborative discussion, and dynamic codebook\nevolution, capturing the reflective depth and adaptive discussions of human\nresearchers. Furthermore, by integrating diverse modes of human intervention,\nSCALE is augmented with expert input to further enhance its performance.\nExtensive evaluations on real-world datasets demonstrate that SCALE achieves\nhuman-approximated performance across various complex content analysis tasks,\noffering an innovative potential for future social science research.", "published": "2025-02-16 00:19:07", "link": "http://arxiv.org/abs/2502.10937v1", "categories": ["cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.AI"}
{"title": "QuOTE: Question-Oriented Text Embeddings", "abstract": "We present QuOTE (Question-Oriented Text Embeddings), a novel enhancement to\nretrieval-augmented generation (RAG) systems, aimed at improving document\nrepresentation for accurate and nuanced retrieval. Unlike traditional RAG\npipelines, which rely on embedding raw text chunks, QuOTE augments chunks with\nhypothetical questions that the chunk can potentially answer, enriching the\nrepresentation space. This better aligns document embeddings with user query\nsemantics, and helps address issues such as ambiguity and context-dependent\nrelevance. Through extensive experiments across diverse benchmarks, we\ndemonstrate that QuOTE significantly enhances retrieval accuracy, including in\nmulti-hop question-answering tasks. Our findings highlight the versatility of\nquestion generation as a fundamental indexing strategy, opening new avenues for\nintegrating question generation into retrieval-based AI pipelines.", "published": "2025-02-16 03:37:13", "link": "http://arxiv.org/abs/2502.10976v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG", "H.3"], "primary_category": "cs.IR"}
{"title": "ControlText: Unlocking Controllable Fonts in Multilingual Text Rendering\n  without Font Annotations", "abstract": "This work demonstrates that diffusion models can achieve font-controllable\nmultilingual text rendering using just raw images without font label\nannotations. Visual text rendering remains a significant challenge. While\nrecent methods condition diffusion on glyphs, it is impossible to retrieve\nexact font annotations from large-scale, real-world datasets, which prevents\nuser-specified font control. To address this, we propose a data-driven solution\nthat integrates the conditional diffusion model with a text segmentation model,\nutilizing segmentation masks to capture and represent fonts in pixel space in a\nself-supervised manner, thereby eliminating the need for any ground-truth\nlabels and enabling users to customize text rendering with any multilingual\nfont of their choice. The experiment provides a proof of concept of our\nalgorithm in zero-shot text and font editing across diverse fonts and\nlanguages, providing valuable insights for the community and industry toward\nachieving generalized visual text rendering.", "published": "2025-02-16 05:30:18", "link": "http://arxiv.org/abs/2502.10999v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Simplify RLHF as Reward-Weighted SFT: A Variational Method", "abstract": "Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning\nLarge Language Models (LLMs) with human values. However, RLHF has been\ncontinuously challenged by its high complexity in implementation and\ncomputation consumption. Even with recent simplifications, such as Direct\nPreference Optimization (DPO) and Advantage Leftover Lunch (A-LoL), the\nproblems of over-fitting and training instability remain hindering the\nalignment process from the expected optimal performance. To address the\nexisting challenges, we propose a novel simplification of RLHF from the\nperspective of variational inference, called $\\textbf{V}$ariational\n$\\textbf{A}$lignment with $\\textbf{R}$e-weighting ($\\textbf{VAR}$). More\nspecifically, by directly minimizing the distribution gap between the learning\nLLM policy and the optimal solution of RLHF, we transform the alignment\nobjective into a reward-driven re-weighted supervised fine-tuning (SFT) form,\nwhich only requires minor adjustment on the SFT loss to obtain noticeable\nimprovement on training stability and effectiveness. On comprehensive alignment\nand generation benchmarks, our VAR method has numerically achieved competitive\nperformance in LLM alignment helpfulness and harmlessness.", "published": "2025-02-16 07:22:00", "link": "http://arxiv.org/abs/2502.11026v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Reasoning-Augmented Conversation for Multi-Turn Jailbreak Attacks on\n  Large Language Models", "abstract": "Multi-turn jailbreak attacks simulate real-world human interactions by\nengaging large language models (LLMs) in iterative dialogues, exposing critical\nsafety vulnerabilities. However, existing methods often struggle to balance\nsemantic coherence with attack effectiveness, resulting in either benign\nsemantic drift or ineffective detection evasion. To address this challenge, we\npropose Reasoning-Augmented Conversation, a novel multi-turn jailbreak\nframework that reformulates harmful queries into benign reasoning tasks and\nleverages LLMs' strong reasoning capabilities to compromise safety alignment.\nSpecifically, we introduce an attack state machine framework to systematically\nmodel problem translation and iterative reasoning, ensuring coherent query\ngeneration across multiple turns. Building on this framework, we design\ngain-guided exploration, self-play, and rejection feedback modules to preserve\nattack semantics, enhance effectiveness, and sustain reasoning-driven attack\nprogression. Extensive experiments on multiple LLMs demonstrate that RACE\nachieves state-of-the-art attack effectiveness in complex conversational\nscenarios, with attack success rates (ASRs) increasing by up to 96%. Notably,\nour approach achieves ASRs of 82% and 92% against leading commercial models,\nOpenAI o1 and DeepSeek R1, underscoring its potency. We release our code at\nhttps://github.com/NY1024/RACE to facilitate further research in this critical\ndomain.", "published": "2025-02-16 09:27:44", "link": "http://arxiv.org/abs/2502.11054v4", "categories": ["cs.CL", "cs.AI", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse\n  Attention", "abstract": "Long-context modeling is crucial for next-generation language models, yet the\nhigh computational cost of standard attention mechanisms poses significant\ncomputational challenges. Sparse attention offers a promising direction for\nimproving efficiency while maintaining model capabilities. We present NSA, a\nNatively trainable Sparse Attention mechanism that integrates algorithmic\ninnovations with hardware-aligned optimizations to achieve efficient\nlong-context modeling. NSA employs a dynamic hierarchical sparse strategy,\ncombining coarse-grained token compression with fine-grained token selection to\npreserve both global context awareness and local precision. Our approach\nadvances sparse attention design with two key innovations: (1) We achieve\nsubstantial speedups through arithmetic intensity-balanced algorithm design,\nwith implementation optimizations for modern hardware. (2) We enable end-to-end\ntraining, reducing pretraining computation without sacrificing model\nperformance. As shown in Figure 1, experiments show the model pretrained with\nNSA maintains or exceeds Full Attention models across general benchmarks,\nlong-context tasks, and instruction-based reasoning. Meanwhile, NSA achieves\nsubstantial speedups over Full Attention on 64k-length sequences across\ndecoding, forward propagation, and backward propagation, validating its\nefficiency throughout the model lifecycle.", "published": "2025-02-16 11:53:44", "link": "http://arxiv.org/abs/2502.11089v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FELLE: Autoregressive Speech Synthesis with Token-Wise Coarse-to-Fine\n  Flow Matching", "abstract": "To advance continuous-valued token modeling and temporal-coherence\nenforcement, we propose FELLE, an autoregressive model that integrates language\nmodeling with token-wise flow matching. By leveraging the autoregressive nature\nof language models and the generative efficacy of flow matching, FELLE\neffectively predicts continuous-valued tokens (mel-spectrograms). For each\ncontinuous-valued token, FELLE modifies the general prior distribution in flow\nmatching by incorporating information from the previous step, improving\ncoherence and stability. Furthermore, to enhance synthesis quality, FELLE\nintroduces a coarse-to-fine flow-matching mechanism, generating\ncontinuous-valued tokens hierarchically, conditioned on the language model's\noutput. Experimental results demonstrate the potential of incorporating\nflow-matching techniques in autoregressive mel-spectrogram modeling, leading to\nsignificant improvements in TTS generation quality, as shown in\nhttps://aka.ms/felle.", "published": "2025-02-16 13:54:32", "link": "http://arxiv.org/abs/2502.11128v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "VisPath: Automated Visualization Code Synthesis via Multi-Path Reasoning\n  and Feedback-Driven Optimization", "abstract": "Unprecedented breakthroughs in Large Language Models (LLMs) has amplified its\npenetration into application of automated visualization code generation.\nFew-shot prompting and query expansion techniques have notably enhanced data\nvisualization performance, however, still fail to overcome ambiguity and\ncomplexity of natural language queries - imposing an inherent burden for manual\nhuman intervention. To mitigate such limitations, we propose a holistic\nframework VisPath : A Multi-Path Reasoning and Feedback-Driven Optimization\nFramework for Visualization Code Generation, which systematically enhances code\nquality through structured reasoning and refinement. VisPath is a multi-stage\nframework, specially designed to handle underspecified queries. To generate a\nrobust final visualization code, it first utilizes initial query to generate\ndiverse reformulated queries via Chain-of-Thought (CoT) prompting, each\nrepresenting a distinct reasoning path. Refined queries are used to produce\ncandidate visualization scripts, consequently executed to generate multiple\nimages. Comprehensively assessing correctness and quality of outputs, VisPath\ngenerates feedback for each image, which are then fed to aggregation module to\ngenerate optimal result. Extensive experiments on benchmarks including\nMatPlotBench and the Qwen-Agent Code Interpreter Benchmark show that VisPath\nsignificantly outperforms state-of-the-art (SOTA) methods, increased up to\naverage 17%, offering a more reliable solution for AI-driven visualization code\ngeneration.", "published": "2025-02-16 14:09:42", "link": "http://arxiv.org/abs/2502.11140v1", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.SE"}
{"title": "NavRAG: Generating User Demand Instructions for Embodied Navigation\n  through Retrieval-Augmented LLM", "abstract": "Vision-and-Language Navigation (VLN) is an essential skill for embodied\nagents, allowing them to navigate in 3D environments following natural language\ninstructions. High-performance navigation models require a large amount of\ntraining data, the high cost of manually annotating data has seriously hindered\nthis field. Therefore, some previous methods translate trajectory videos into\nstep-by-step instructions for expanding data, but such instructions do not\nmatch well with users' communication styles that briefly describe destinations\nor state specific needs. Moreover, local navigation trajectories overlook\nglobal context and high-level task planning. To address these issues, we\npropose NavRAG, a retrieval-augmented generation (RAG) framework that generates\nuser demand instructions for VLN. NavRAG leverages LLM to build a hierarchical\nscene description tree for 3D scene understanding from global layout to local\ndetails, then simulates various user roles with specific demands to retrieve\nfrom the scene tree, generating diverse instructions with LLM. We annotate over\n2 million navigation instructions across 861 scenes and evaluate the data\nquality and navigation performance of trained models.", "published": "2025-02-16 14:17:36", "link": "http://arxiv.org/abs/2502.11142v3", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "Can't See the Forest for the Trees: Benchmarking Multimodal Safety\n  Awareness for Multimodal LLMs", "abstract": "Multimodal Large Language Models (MLLMs) have expanded the capabilities of\ntraditional language models by enabling interaction through both text and\nimages. However, ensuring the safety of these models remains a significant\nchallenge, particularly in accurately identifying whether multimodal content is\nsafe or unsafe-a capability we term safety awareness. In this paper, we\nintroduce MMSafeAware, the first comprehensive multimodal safety awareness\nbenchmark designed to evaluate MLLMs across 29 safety scenarios with 1500\ncarefully curated image-prompt pairs. MMSafeAware includes both unsafe and\nover-safety subsets to assess models abilities to correctly identify unsafe\ncontent and avoid over-sensitivity that can hinder helpfulness. Evaluating nine\nwidely used MLLMs using MMSafeAware reveals that current models are not\nsufficiently safe and often overly sensitive; for example, GPT-4V misclassifies\n36.1% of unsafe inputs as safe and 59.9% of benign inputs as unsafe. We further\nexplore three methods to improve safety awareness-prompting-based approaches,\nvisual contrastive decoding, and vision-centric reasoning fine-tuning-but find\nthat none achieve satisfactory performance. Our findings highlight the profound\nchallenges in developing MLLMs with robust safety awareness, underscoring the\nneed for further research in this area. All the code and data will be publicly\navailable to facilitate future research.", "published": "2025-02-16 16:12:40", "link": "http://arxiv.org/abs/2502.11184v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Primus: A Pioneering Collection of Open-Source Datasets for\n  Cybersecurity LLM Training", "abstract": "Large Language Models (LLMs) have shown remarkable advancements in\nspecialized fields such as finance, law, and medicine. However, in\ncybersecurity, we have noticed a lack of open-source datasets, with a\nparticular lack of high-quality cybersecurity pretraining corpora, even though\nmuch research indicates that LLMs acquire their knowledge during pretraining.\nTo address this, we present a comprehensive suite of datasets covering all\nmajor training stages, including pretraining, instruction fine-tuning, and\nreasoning distillation with cybersecurity-specific self-reflection data.\nExtensive ablation studies demonstrate their effectiveness on public\ncybersecurity benchmarks. In particular, continual pre-training on our dataset\nyields a 15.88% improvement in the aggregate score, while reasoning\ndistillation leads to a 10% gain in security certification (CISSP). We will\nrelease all datasets and trained cybersecurity LLMs under the ODC-BY and MIT\nlicenses to encourage further research in the community. For access to all\ndatasets and model weights, please refer to\nhttps://huggingface.co/collections/trendmicro-ailab/primus-67b1fd27052b802b4af9d243.", "published": "2025-02-16 16:34:49", "link": "http://arxiv.org/abs/2502.11191v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "A Survey of LLM-based Agents in Medicine: How far are we from Baymax?", "abstract": "Large Language Models (LLMs) are transforming healthcare through the\ndevelopment of LLM-based agents that can understand, reason about, and assist\nwith medical tasks. This survey provides a comprehensive review of LLM-based\nagents in medicine, examining their architectures, applications, and\nchallenges. We analyze the key components of medical agent systems, including\nsystem profiles, clinical planning mechanisms, medical reasoning frameworks,\nand external capacity enhancement. The survey covers major application\nscenarios such as clinical decision support, medical documentation, training\nsimulations, and healthcare service optimization. We discuss evaluation\nframeworks and metrics used to assess these agents' performance in healthcare\nsettings. While LLM-based agents show promise in enhancing healthcare delivery,\nseveral challenges remain, including hallucination management, multimodal\nintegration, implementation barriers, and ethical considerations. The survey\nconcludes by highlighting future research directions, including advances in\nmedical reasoning inspired by recent developments in LLM architectures,\nintegration with physical systems, and improvements in training simulations.\nThis work provides researchers and practitioners with a structured overview of\nthe current state and future prospects of LLM-based agents in medicine.", "published": "2025-02-16 17:21:05", "link": "http://arxiv.org/abs/2502.11211v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "MemeSense: An Adaptive In-Context Framework for Social Commonsense\n  Driven Meme Moderation", "abstract": "Memes present unique moderation challenges due to their subtle, multimodal\ninterplay of images, text, and social context. Standard systems relying\npredominantly on explicit textual cues often overlook harmful content\ncamouflaged by irony, symbolism, or cultural references. To address this gap,\nwe introduce MemeSense, an adaptive in-context learning framework that fuses\nsocial commonsense reasoning with visually and semantically related reference\nexamples. By encoding crucial task information into a learnable cognitive shift\nvector, MemeSense effectively balances lexical, visual, and ethical\nconsiderations, enabling precise yet context-aware meme intervention. Extensive\nevaluations on a curated set of implicitly harmful memes demonstrate that\nMemeSense substantially outperforms strong baselines, paving the way for safer\nonline communities. Code and data available at:\nhttps://github.com/sayantan11995/MemeSense", "published": "2025-02-16 19:46:24", "link": "http://arxiv.org/abs/2502.11246v1", "categories": ["cs.IR", "cs.CL", "cs.CY"], "primary_category": "cs.IR"}
{"title": "Unveiling Environmental Impacts of Large Language Model Serving: A\n  Functional Unit View", "abstract": "Large language models (LLMs) offer powerful capabilities but come with\nsignificant environmental costs, particularly in carbon emissions. Existing\nstudies benchmark these emissions but lack a standardized basis for comparison\nacross models. To address this, we introduce the concept of a functional unit\n(FU) and develop FUEL, the first FU-based framework for evaluating LLM\nserving's environmental impact. Through case studies on model size,\nquantization, and hardware, we uncover key trade-offs in sustainability. Our\nfindings highlight the potential for reducing carbon emissions by optimizing\nmodel selection, deployment strategies, and hardware choices, paving the way\nfor more sustainable AI infrastructure.", "published": "2025-02-16 20:20:18", "link": "http://arxiv.org/abs/2502.11256v1", "categories": ["cs.LG", "cs.AR", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Prompting in the Dark: Assessing Human Performance in Prompt Engineering\n  for Data Labeling When Gold Labels Are Absent", "abstract": "Millions of users prompt large language models (LLMs) for various tasks, but\nhow good are people at prompt engineering? Do users actually get closer to\ntheir desired outcome over multiple iterations of their prompts? These\nquestions are crucial when no gold-standard labels are available to measure\nprogress. This paper investigates a scenario in LLM-powered data labeling,\n\"prompting in the dark,\" where users iteratively prompt LLMs to label data\nwithout using manually-labeled benchmarks. We developed PromptingSheet, a\nGoogle Sheets add-on that enables users to compose, revise, and iteratively\nlabel data through spreadsheets. Through a study with 20 participants, we found\nthat prompting in the dark was highly unreliable-only 9 participants improved\nlabeling accuracy after four or more iterations. Automated prompt optimization\ntools like DSPy also struggled when few gold labels were available. Our\nfindings highlight the importance of gold labels and the needs, as well as the\nrisks, of automated support in human prompt engineering, providing insights for\nfuture tool design.", "published": "2025-02-16 20:54:26", "link": "http://arxiv.org/abs/2502.11267v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.HC"}
{"title": "OctoTools: An Agentic Framework with Extensible Tools for Complex\n  Reasoning", "abstract": "Solving complex reasoning tasks may involve visual understanding, domain\nknowledge retrieval, numerical calculation, and multi-step reasoning. Existing\nmethods augment large language models (LLMs) with external tools but are\nrestricted to specialized domains, limited tool types, or require additional\ntraining data. In this paper, we introduce OctoTools, a training-free,\nuser-friendly, and easily extensible open-source agentic framework designed to\ntackle complex reasoning across diverse domains. OctoTools introduces\nstandardized tool cards to encapsulate tool functionality, a planner for both\nhigh-level and low-level planning, and an executor to carry out tool usage. We\nvalidate OctoTools' generality across 16 diverse tasks (including MathVista,\nMMLU-Pro, MedQA, and GAIA-Text), achieving substantial average accuracy gains\nof 9.3% over GPT-4o. Furthermore, OctoTools outperforms AutoGen, GPT-Functions\nand LangChain by up to 10.6% when given the same set of tools. Through\ncomprehensive analysis and ablations, OctoTools demonstrates advantages in task\nplanning, effective tool usage, and multi-step problem solving.", "published": "2025-02-16 21:18:47", "link": "http://arxiv.org/abs/2502.11271v1", "categories": ["cs.LG", "cs.CL", "cs.CV", "cs.MA"], "primary_category": "cs.LG"}
{"title": "Integrating Language Models for Enhanced Network State Monitoring in\n  DRL-Based SFC Provisioning", "abstract": "Efficient Service Function Chain (SFC) provisioning and Virtual Network\nFunction (VNF) placement are critical for enhancing network performance in\nmodern architectures such as Software-Defined Networking (SDN) and Network\nFunction Virtualization (NFV). While Deep Reinforcement Learning (DRL) aids\ndecision-making in dynamic network environments, its reliance on structured\ninputs and predefined rules limits adaptability in unforeseen scenarios.\nAdditionally, incorrect actions by a DRL agent may require numerous training\niterations to correct, potentially reinforcing suboptimal policies and\ndegrading performance. This paper integrates DRL with Language Models (LMs),\nspecifically Bidirectional Encoder Representations from Transformers (BERT) and\nDistilBERT, to enhance network management. By feeding final VNF allocations\nfrom DRL into the LM, the system can process and respond to queries related to\nSFCs, DCs, and VNFs, enabling real-time insights into resource utilization,\nbottleneck detection, and future demand planning. The LMs are fine-tuned to our\ndomain-specific dataset using Low-Rank Adaptation (LoRA). Results show that\nBERT outperforms DistilBERT with a lower test loss (0.28 compared to 0.36) and\nhigher confidence (0.83 compared to 0.74), though BERT requires approximately\n46% more processing time.", "published": "2025-02-16 22:52:14", "link": "http://arxiv.org/abs/2502.11298v1", "categories": ["cs.NI", "cs.AI", "cs.CL"], "primary_category": "cs.NI"}
{"title": "CORDIAL: Can Multimodal Large Language Models Effectively Understand\n  Coherence Relationships?", "abstract": "Multimodal Large Language Models (MLLMs) are renowned for their superior\ninstruction-following and reasoning capabilities across diverse problem\ndomains. However, existing benchmarks primarily focus on assessing factual and\nlogical correctness in downstream tasks, with limited emphasis on evaluating\nMLLMs' ability to interpret pragmatic cues and intermodal relationships. To\naddress this gap, we assess the competency of MLLMs in performing Multimodal\nDiscourse Analysis (MDA) using Coherence Relations. Our benchmark, CORDIAL,\nencompasses a broad spectrum of Coherence Relations across 3 different\ndiscourse domains at varying levels of granularity. Through our experiments on\n10+ MLLMs employing different prompting strategies, we show that even top\nmodels like Gemini 1.5 Pro and GPT-4o fail to match the performance of simple\nclassifier-based baselines. This study emphasizes the need to move beyond\nsimilarity-based metrics and adopt a discourse-driven framework for evaluating\nMLLMs, providing a more nuanced assessment of their capabilities. The benchmark\nand code are available at: https://github.com/aashish2000/CORDIAL.", "published": "2025-02-16 22:54:44", "link": "http://arxiv.org/abs/2502.11300v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "I.2.7; I.2.10"], "primary_category": "cs.CL"}
{"title": "Leveraging Multimodal-LLMs Assisted by Instance Segmentation for\n  Intelligent Traffic Monitoring", "abstract": "A robust and efficient traffic monitoring system is essential for smart\ncities and Intelligent Transportation Systems (ITS), using sensors and cameras\nto track vehicle movements, optimize traffic flow, reduce congestion, enhance\nroad safety, and enable real-time adaptive traffic control. Traffic monitoring\nmodels must comprehensively understand dynamic urban conditions and provide an\nintuitive user interface for effective management. This research leverages the\nLLaVA visual grounding multimodal large language model (LLM) for traffic\nmonitoring tasks on the real-time Quanser Interactive Lab simulation platform,\ncovering scenarios like intersections, congestion, and collisions. Cameras\nplaced at multiple urban locations collect real-time images from the\nsimulation, which are fed into the LLaVA model with queries for analysis. An\ninstance segmentation model integrated into the cameras highlights key elements\nsuch as vehicles and pedestrians, enhancing training and throughput. The system\nachieves 84.3% accuracy in recognizing vehicle locations and 76.4% in\ndetermining steering direction, outperforming traditional models.", "published": "2025-02-16 23:03:26", "link": "http://arxiv.org/abs/2502.11304v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "ALGEN: Few-shot Inversion Attacks on Textual Embeddings using Alignment\n  and Generation", "abstract": "With the growing popularity of Large Language Models (LLMs) and vector\ndatabases, private textual data is increasingly processed and stored as\nnumerical embeddings. However, recent studies have proven that such embeddings\nare vulnerable to inversion attacks, where original text is reconstructed to\nreveal sensitive information. Previous research has largely assumed access to\nmillions of sentences to train attack models, e.g., through data leakage or\nnearly unrestricted API access. With our method, a single data point is\nsufficient for a partially successful inversion attack. With as little as 1k\ndata samples, performance reaches an optimum across a range of black-box\nencoders, without training on leaked data. We present a Few-shot Textual\nEmbedding Inversion Attack using ALignment and GENeration (ALGEN), by aligning\nvictim embeddings to the attack space and using a generative model to\nreconstruct text. We find that ALGEN attacks can be effectively transferred\nacross domains and languages, revealing key information. We further examine a\nvariety of defense mechanisms against ALGEN, and find that none are effective,\nhighlighting the vulnerabilities posed by inversion attacks. By significantly\nlowering the cost of inversion and proving that embedding spaces can be aligned\nthrough one-step optimization, we establish a new textual embedding inversion\nparadigm with broader applications for embedding alignment in NLP.", "published": "2025-02-16 23:11:13", "link": "http://arxiv.org/abs/2502.11308v2", "categories": ["cs.CR", "cs.AI", "cs.CL", "I.2; J.6"], "primary_category": "cs.CR"}
{"title": "BoT: Breaking Long Thought Processes of o1-like Large Language Models\n  through Backdoor Attack", "abstract": "Longer thought, better performance: large language models with deep reasoning\ncapabilities, particularly o1-like models, have demonstrated remarkable\nperformance by generating extensive thought processes during inference. This\ntrade-off reveals a potential vulnerability: adversaries could compromise model\nperformance by forcing immediate responses without thought processes. To this\nend, in this paper, we introduce a novel attack scenario targeting the long\nthought processes of o1-like models and propose BoT (Break CoT), which can\nselectively break intrinsic reasoning mechanisms through backdoor attacks. BoT\nconstructs poisoned datasets with designed triggers and injects backdoor by\neither supervised fine-tuning or direct preference optimization. When\ntriggered, the model directly generates answers without thought processes,\nwhile maintaining normal reasoning capabilities for clean inputs. Extensive\nexperiments on open-source o1-like models, including recent DeepSeek-R1,\ndemonstrate that BoT nearly achieves high attack success rates while\nmaintaining clean accuracy, highlighting the critical safety risk in current\nmodels. Furthermore, the relationship between task difficulty and helpfulness\nreveals a potential application for good, enabling users to customize model\nbehavior based on task complexity. Code is available at\n\\href{https://github.com/zihao-ai/BoT}{https://github.com/zihao-ai/BoT}.", "published": "2025-02-16 10:45:56", "link": "http://arxiv.org/abs/2502.12202v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluating the Paperclip Maximizer: Are RL-Based Language Models More\n  Likely to Pursue Instrumental Goals?", "abstract": "As large language models (LLMs) continue to evolve, ensuring their alignment\nwith human goals and values remains a pressing challenge. A key concern is\n\\textit{instrumental convergence}, where an AI system, in optimizing for a\ngiven objective, develops unintended intermediate goals that override the\nultimate objective and deviate from human-intended goals. This issue is\nparticularly relevant in reinforcement learning (RL)-trained models, which can\ngenerate creative but unintended strategies to maximize rewards. In this paper,\nwe explore instrumental convergence in LLMs by comparing models trained with\ndirect RL optimization (e.g., the o1 model) to those trained with reinforcement\nlearning from human feedback (RLHF). We hypothesize that RL-driven models\nexhibit a stronger tendency for instrumental convergence due to their\noptimization of goal-directed behavior in ways that may misalign with human\nintentions. To assess this, we introduce InstrumentalEval, a benchmark for\nevaluating instrumental convergence in RL-trained LLMs. Initial experiments\nreveal cases where a model tasked with making money unexpectedly pursues\ninstrumental objectives, such as self-replication, implying signs of\ninstrumental convergence. Our findings contribute to a deeper understanding of\nalignment challenges in AI systems and the risks posed by unintended model\nbehaviors.", "published": "2025-02-16 16:29:20", "link": "http://arxiv.org/abs/2502.12206v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "ShieldLearner: A New Paradigm for Jailbreak Attack Defense in LLMs", "abstract": "Large Language Models (LLMs) have achieved remarkable success in various\ndomains but remain vulnerable to adversarial jailbreak attacks. Existing\nprompt-defense strategies, including parameter-modifying and parameter-free\napproaches, face limitations in adaptability, interpretability, and\ncustomization, constraining their effectiveness against evolving threats. To\naddress these challenges, we propose ShieldLearner, a novel paradigm that\nmimics human learning in defense. Through trial and error, it autonomously\ndistills attack signatures into a Pattern Atlas and synthesizes defense\nheuristics into a Meta-analysis Framework, enabling systematic and\ninterpretable threat detection. Furthermore, we introduce Adaptive Adversarial\nAugmentation to generate adversarial variations of successfully defended\nprompts, enabling continuous self-improvement without model retraining. In\naddition to standard benchmarks, we create a hard test set by curating\nadversarial prompts from the Wildjailbreak dataset, emphasizing more concealed\nmalicious intent. Experimental results show that ShieldLearner achieves a\nsignificantly higher defense success rate than existing baselines on both\nconventional and hard test sets, while also operating with lower computational\noverhead, making it a practical and efficient solution for real-world\nadversarial defense.", "published": "2025-02-16 18:47:41", "link": "http://arxiv.org/abs/2502.13162v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Learning to Reason from Feedback at Test-Time", "abstract": "Solving complex tasks in a single attempt is challenging for large language\nmodels (LLMs). Iterative interaction with the environment and feedback is often\nrequired to achieve success, making effective feedback utilization a critical\ntopic. Existing approaches either struggle with length generalization or rely\non naive retries without leveraging prior information. In this paper, we\nintroduce FTTT, a novel paradigm that formulates feedback utilization as an\noptimization problem at test time. Additionally, we propose a learnable\ntest-time optimizer, OpTune, to effectively exploit feedback. Experiments on\ntwo LLMs across four reasoning datasets demonstrate that FTTT and OpTune\nachieve superior scalability and performance.", "published": "2025-02-16 11:05:27", "link": "http://arxiv.org/abs/2502.15771v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "ECG-Expert-QA: A Benchmark for Evaluating Medical Large Language Models\n  in Heart Disease Diagnosis", "abstract": "We present ECG-Expert-QA, a comprehensive multimodal dataset for evaluating\ndiagnostic capabilities in electrocardiogram (ECG) interpretation. It combines\nreal-world clinical ECG data with systematically generated synthetic cases,\ncovering 12 essential diagnostic tasks and totaling 47,211 expert-validated QA\npairs. These encompass diverse clinical scenarios, from basic rhythm\nrecognition to complex diagnoses involving rare conditions and temporal\nchanges. A key innovation is the support for multi-turn dialogues, enabling the\ndevelopment of conversational medical AI systems that emulate clinician-patient\nor interprofessional interactions. This allows for more realistic assessment of\nAI models' clinical reasoning, diagnostic accuracy, and knowledge integration.\nConstructed through a knowledge-guided framework with strict quality control,\nECG-Expert-QA ensures linguistic and clinical consistency, making it a\nhigh-quality resource for advancing AI-assisted ECG interpretation. It\nchallenges models with tasks like identifying subtle ischemic changes and\ninterpreting complex arrhythmias in context-rich scenarios. To promote research\ntransparency and collaboration, the dataset, accompanying code, and prompts are\npublicly released at https://github.com/Zaozzz/ECG-Expert-QA", "published": "2025-02-16 13:28:55", "link": "http://arxiv.org/abs/2502.17475v3", "categories": ["eess.SP", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "eess.SP"}
{"title": "ReLearn: Unlearning via Learning for Large Language Models", "abstract": "Current unlearning methods for large language models usually rely on reverse\noptimization to reduce target token probabilities. However, this paradigm\ndisrupts the subsequent tokens prediction, degrading model performance and\nlinguistic coherence. Moreover, existing evaluation metrics overemphasize\ncontextual forgetting while inadequately assessing response fluency and\nrelevance. To address these challenges, we propose ReLearn, a data augmentation\nand fine-tuning pipeline for effective unlearning, along with a comprehensive\nevaluation framework. This framework introduces Knowledge Forgetting Rate (KFR)\nand Knowledge Retention Rate (KRR) to measure knowledge-level preservation, and\nLinguistic Score (LS) to evaluate generation quality. Our experiments show that\nReLearn successfully achieves targeted forgetting while preserving high-quality\noutput. Through mechanistic analysis, we further demonstrate how reverse\noptimization disrupts coherent text generation, while ReLearn preserves this\nessential capability. Code is available at https://github.com/zjunlp/unlearn.", "published": "2025-02-16 16:31:00", "link": "http://arxiv.org/abs/2502.11190v2", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "How Do LLMs Acquire New Knowledge? A Knowledge Circuits Perspective on\n  Continual Pre-Training", "abstract": "Despite exceptional capabilities in knowledge-intensive tasks, Large Language\nModels (LLMs) face a critical gap in understanding how they internalize new\nknowledge, particularly how to structurally embed acquired knowledge in their\nneural computations. We address this issue through the lens of knowledge\ncircuit evolution, identifying computational subgraphs that facilitate\nknowledge storage and processing. Our systematic analysis of circuit evolution\nthroughout continual pre-training reveals several key findings: (1) the\nacquisition of new knowledge is influenced by its relevance to pre-existing\nknowledge; (2) the evolution of knowledge circuits exhibits a distinct phase\nshift from formation to optimization; (3) the evolution of knowledge circuits\nfollows a deep-to-shallow pattern. These insights not only advance our\ntheoretical understanding of the mechanisms of new knowledge acquisition in\nLLMs, but also provide potential implications for improving continual\npre-training strategies to enhance model performance. Code and data will be\navailable at https://github.com/zjunlp/DynamicKnowledgeCircuits.", "published": "2025-02-16 16:55:43", "link": "http://arxiv.org/abs/2502.11196v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.HC"], "primary_category": "cs.LG"}
{"title": "SpeechT-RAG: Reliable Depression Detection in LLMs with\n  Retrieval-Augmented Generation Using Speech Timing Information", "abstract": "Large Language Models (LLMs) have been increasingly adopted for\nhealth-related tasks, yet their performance in depression detection remains\nlimited when relying solely on text input. While Retrieval-Augmented Generation\n(RAG) typically enhances LLM capabilities, our experiments indicate that\ntraditional text-based RAG systems struggle to significantly improve depression\ndetection accuracy. This challenge stems partly from the rich\ndepression-relevant information encoded in acoustic speech patterns information\nthat current text-only approaches fail to capture effectively. To address this\nlimitation, we conduct a systematic analysis of temporal speech patterns,\ncomparing healthy individuals with those experiencing depression. Based on our\nfindings, we introduce Speech Timing-based Retrieval-Augmented Generation,\nSpeechT-RAG, a novel system that leverages speech timing features for both\naccurate depression detection and reliable confidence estimation. This\nintegrated approach not only outperforms traditional text-based RAG systems in\ndetection accuracy but also enhances uncertainty quantification through a\nconfidence scoring mechanism that naturally extends from the same temporal\nfeatures. Our unified framework achieves comparable results to fine-tuned LLMs\nwithout additional training while simultaneously addressing the fundamental\nrequirements for both accuracy and trustworthiness in mental health assessment.", "published": "2025-02-16 02:02:19", "link": "http://arxiv.org/abs/2502.10950v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "AudioSpa: Spatializing Sound Events with Text", "abstract": "Text-to-audio (TTA) systems have recently demonstrated strong performance in\nsynthesizing monaural audio from text. However, the task of generating binaural\nspatial audio from text, which provides a more immersive auditory experience by\nincorporating the sense of spatiality, have not been explored yet. In this\nwork, we introduce text-guided binaural audio generation. As an early effort,\nwe focus on the scenario where a monaural reference audio is given\nadditionally. The core problem is to associate specific sound events with their\ndirections, thereby creating binaural spatial audio. The challenge lies in the\ncomplexity of textual descriptions and the limited availability of\nsingle-source sound event datasets. To address this, we propose AudioSpa, an\nend-to-end model that applies large language models to process both acoustic\nand textual information. We employ fusion multi-head attention (FMHA) to\nintegrate text tokens, which enhances the generation capability of the\nmultimodal learning. Additionally, we propose a binaural source localization\nmodel to assess the quality of the generated audio. Finally, we design a data\naugmentation strategy to generate diverse datasets, which enables the model to\nspatialize sound events across various spatial positions. Experimental results\ndemonstrate that our model is able to put sounds at the specified locations\naccurately. It achieves competitive performance in both localization accuracy\nand signal distortion. Our demonstrations are available at\nhttps://linfeng-feng.github.io/AudioSpa-demo.", "published": "2025-02-16 17:41:14", "link": "http://arxiv.org/abs/2502.11219v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
