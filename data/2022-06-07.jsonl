{"title": "Review on Multiple Plagiarism: A Performance Comparison Study", "abstract": "Plagiarism is the practice of claiming to be someone else content, thoughts\nor ideas as one own without any proper credit and citations. This paper is a\nsurvey paper that, represent the some of the great research paper and its\ncomparison that is work done on plagiarism. Now a days, plagiarism became one\nof the most interesting and crucial research points in Natural Language\nProcessing area. We review some old research paper based on different types of\nplagiarism detection and their models and algorithm, and comparison of the\naccuracy of those papers. There are many several ways which are available for\nplagiarism detection in different language. There are a few algorithms to\ndetecting plagiarism. Like, corpus, CL-CNG, LSI, Levenshtein Distance etc. We\nanalysis those papers, and learn that they used different types of algorithms\nfor detecting plagiarism. After experiment those papers, we got that some of\nthe algorithms give a better output and accuracy for detecting plagiarism. We\nare going to give a review on some papers about Plagiarism and will discuss\nabout the pros and cons of their models. And we also show a propose method for\nplagiarism detection method which based on sentience separation, word\nseparation and make sentence based on synonym and compare with any sources.", "published": "2022-06-07 02:57:27", "link": "http://arxiv.org/abs/2206.02983v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DiMS: Distilling Multiple Steps of Iterative Non-Autoregressive\n  Transformers for Machine Translation", "abstract": "The computational benefits of iterative non-autoregressive transformers\ndecrease as the number of decoding steps increases. As a remedy, we introduce\nDistill Multiple Steps (DiMS), a simple yet effective distillation technique to\ndecrease the number of required steps to reach a certain translation quality.\nThe distilled model enjoys the computational benefits of early iterations while\npreserving the enhancements from several iterative steps. DiMS relies on two\nmodels namely student and teacher. The student is optimized to predict the\noutput of the teacher after multiple decoding steps while the teacher follows\nthe student via a slow-moving average. The moving average keeps the teacher's\nknowledge updated and enhances the quality of the labels provided by the\nteacher. During inference, the student is used for translation and no\nadditional computation is added. We verify the effectiveness of DiMS on various\nmodels obtaining 7.8 and 12.9 BLEU points improvements in single-step\ntranslation accuracy on distilled and raw versions of WMT'14 De-En.", "published": "2022-06-07 04:25:41", "link": "http://arxiv.org/abs/2206.02999v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Plot Writing From Pre-Trained Language Models", "abstract": "Pre-trained language models (PLMs) fail to generate long-form narrative text\nbecause they do not consider global structure. As a result, the generated texts\nare often incohesive, repetitive, or lack content. Recent work in story\ngeneration reintroduced explicit content planning in the form of prompts,\nkeywords, or semantic frames. Trained on large parallel corpora, these models\ncan generate more logical event sequences and thus more contentful stories.\nHowever, these intermediate representations are often not in natural language\nand cannot be utilized by PLMs without fine-tuning. We propose generating story\nplots using off-the-shelf PLMs while maintaining the benefit of content\nplanning to generate cohesive and contentful stories. Our proposed method,\nScratchPlot, first prompts a PLM to compose a content plan. Then, we generate\nthe story's body and ending conditioned on the content plan. Furthermore, we\ntake a generate-and-rank approach by using additional PLMs to rank the\ngenerated (story, ending) pairs. We benchmark our method with various baselines\nand achieved superior results in both human and automatic evaluation.", "published": "2022-06-07 05:30:46", "link": "http://arxiv.org/abs/2206.03021v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OCHADAI at SemEval-2022 Task 2: Adversarial Training for Multilingual\n  Idiomaticity Detection", "abstract": "We propose a multilingual adversarial training model for determining whether\na sentence contains an idiomatic expression. Given that a key challenge with\nthis task is the limited size of annotated data, our model relies on\npre-trained contextual representations from different multi-lingual\nstate-of-the-art transformer-based language models (i.e., multilingual BERT and\nXLM-RoBERTa), and on adversarial training, a training method for further\nenhancing model generalization and robustness. Without relying on any\nhuman-crafted features, knowledge bases, or additional datasets other than the\ntarget datasets, our model achieved competitive results and ranked 6th place in\nSubTask A (zero-shot) setting and 15th place in SubTask A (one-shot) setting.", "published": "2022-06-07 05:52:43", "link": "http://arxiv.org/abs/2206.03025v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RAAT: Relation-Augmented Attention Transformer for Relation Modeling in\n  Document-Level Event Extraction", "abstract": "In document-level event extraction (DEE) task, event arguments always scatter\nacross sentences (across-sentence issue) and multiple events may lie in one\ndocument (multi-event issue). In this paper, we argue that the relation\ninformation of event arguments is of great significance for addressing the\nabove two issues, and propose a new DEE framework which can model the relation\ndependencies, called Relation-augmented Document-level Event Extraction\n(ReDEE). More specifically, this framework features a novel and tailored\ntransformer, named as Relation-augmented Attention Transformer (RAAT). RAAT is\nscalable to capture multi-scale and multi-amount argument relations. To further\nleverage relation information, we introduce a separate event relation\nprediction task and adopt multi-task learning method to explicitly enhance\nevent extraction performance. Extensive experiments demonstrate the\neffectiveness of the proposed method, which can achieve state-of-the-art\nperformance on two public datasets. Our code is available at https://github.\ncom/TencentYoutuResearch/RAAT.", "published": "2022-06-07 15:11:42", "link": "http://arxiv.org/abs/2206.03377v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How to Dissect a Muppet: The Structure of Transformer Embedding Spaces", "abstract": "Pretrained embeddings based on the Transformer architecture have taken the\nNLP community by storm. We show that they can mathematically be reframed as a\nsum of vector factors and showcase how to use this reframing to study the\nimpact of each component. We provide evidence that multi-head attentions and\nfeed-forwards are not equally useful in all downstream applications, as well as\na quantitative overview of the effects of finetuning on the overall embedding\nspace. This approach allows us to draw connections to a wide range of previous\nstudies, from vector space anisotropy to attention weights.", "published": "2022-06-07 18:24:46", "link": "http://arxiv.org/abs/2206.03529v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Always Keep your Target in Mind: Studying Semantics and Improving\n  Performance of Neural Lexical Substitution", "abstract": "Lexical substitution, i.e. generation of plausible words that can replace a\nparticular target word in a given context, is an extremely powerful technology\nthat can be used as a backbone of various NLP applications, including word\nsense induction and disambiguation, lexical relation extraction, data\naugmentation, etc. In this paper, we present a large-scale comparative study of\nlexical substitution methods employing both rather old and most recent language\nand masked language models (LMs and MLMs), such as context2vec, ELMo, BERT,\nRoBERTa, XLNet. We show that already competitive results achieved by SOTA\nLMs/MLMs can be further substantially improved if information about the target\nword is injected properly. Several existing and new target word injection\nmethods are compared for each LM/MLM using both intrinsic evaluation on lexical\nsubstitution datasets and extrinsic evaluation on word sense induction (WSI)\ndatasets. On two WSI datasets we obtain new SOTA results. Besides, we analyze\nthe types of semantic relations between target words and their substitutes\ngenerated by different models or given by annotators.", "published": "2022-06-07 16:16:19", "link": "http://arxiv.org/abs/2206.11815v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BOS at LSCDiscovery: Lexical Substitution for Interpretable Lexical\n  Semantic Change Detection", "abstract": "We propose a solution for the LSCDiscovery shared task on Lexical Semantic\nChange Detection in Spanish. Our approach is based on generating lexical\nsubstitutes that describe old and new senses of a given word. This approach\nachieves the second best result in sense loss and sense gain detection\nsubtasks. By observing those substitutes that are specific for only one time\nperiod, one can understand which senses were obtained or lost. This allows\nproviding more detailed information about semantic change to the user and makes\nour method interpretable.", "published": "2022-06-07 11:40:29", "link": "http://arxiv.org/abs/2206.11865v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Dual-Encoders with Question and Answer Cross-Embeddings for\n  Answer Retrieval", "abstract": "Dual-Encoders is a promising mechanism for answer retrieval in question\nanswering (QA) systems. Currently most conventional Dual-Encoders learn the\nsemantic representations of questions and answers merely through matching\nscore. Researchers proposed to introduce the QA interaction features in scoring\nfunction but at the cost of low efficiency in inference stage. To keep\nindependent encoding of questions and answers during inference stage,\nvariational auto-encoder is further introduced to reconstruct answers\n(questions) from question (answer) embeddings as an auxiliary task to enhance\nQA interaction in representation learning in training stage. However, the needs\nof text generation and answer retrieval are different, which leads to hardness\nin training. In this work, we propose a framework to enhance the Dual-Encoders\nmodel with question answer cross-embeddings and a novel Geometry Alignment\nMechanism (GAM) to align the geometry of embeddings from Dual-Encoders with\nthat from Cross-Encoders. Extensive experimental results show that our\nframework significantly improves Dual-Encoders model and outperforms the\nstate-of-the-art method on multiple answer retrieval datasets.", "published": "2022-06-07 02:39:24", "link": "http://arxiv.org/abs/2206.02978v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DynaMaR: Dynamic Prompt with Mask Token Representation", "abstract": "Recent research has shown that large language models pretrained using\nunsupervised approaches can achieve significant performance improvement on many\ndownstream tasks. Typically when adapting these language models to downstream\ntasks, like a classification or regression task, we employ a fine-tuning\nparadigm in which the sentence representation from the language model is input\nto a task-specific head; the model is then fine-tuned end-to-end. However, with\nthe emergence of models like GPT-3, prompt-based fine-tuning has been proven to\nbe a successful approach for few-shot tasks. Inspired by this work, we study\ndiscrete prompt technologies in practice. There are two issues that arise with\nthe standard prompt approach. First, it can overfit on the prompt template.\nSecond, it requires manual effort to formulate the downstream task as a\nlanguage model problem. In this paper, we propose an improvement to\nprompt-based fine-tuning that addresses these two issues. We refer to our\napproach as DynaMaR -- Dynamic Prompt with Mask Token Representation. Results\nshow that DynaMaR can achieve an average improvement of 10% in few-shot\nsettings and improvement of 3.7% in data-rich settings over the standard\nfine-tuning approach on four e-commerce applications.", "published": "2022-06-07 02:54:36", "link": "http://arxiv.org/abs/2206.02982v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Searching for Optimal Subword Tokenization in Cross-domain NER", "abstract": "Input distribution shift is one of the vital problems in unsupervised domain\nadaptation (UDA). The most popular UDA approaches focus on domain-invariant\nrepresentation learning, trying to align the features from different domains\ninto similar feature distributions. However, these approaches ignore the direct\nalignment of input word distributions between domains, which is a vital factor\nin word-level classification tasks such as cross-domain NER. In this work, we\nshed new light on cross-domain NER by introducing a subword-level solution,\nX-Piece, for input word-level distribution shift in NER. Specifically, we\nre-tokenize the input words of the source domain to approach the target subword\ndistribution, which is formulated and solved as an optimal transport problem.\nAs this approach focuses on the input level, it can also be combined with\nprevious DIRL methods for further improvement. Experimental results show the\neffectiveness of the proposed method based on BERT-tagger on four benchmark NER\ndatasets. Also, the proposed method is proved to benefit DIRL methods such as\nDANN.", "published": "2022-06-07 14:39:31", "link": "http://arxiv.org/abs/2206.03352v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "cViL: Cross-Lingual Training of Vision-Language Models using Knowledge\n  Distillation", "abstract": "Vision-and-language tasks are gaining popularity in the research community,\nbut the focus is still mainly on English. We propose a pipeline that utilizes\nEnglish-only vision-language models to train a monolingual model for a target\nlanguage. We propose to extend OSCAR+, a model which leverages object tags as\nanchor points for learning image-text alignments, to train on visual question\nanswering datasets in different languages. We propose a novel approach to\nknowledge distillation to train the model in other languages using parallel\nsentences. Compared to other models that use the target language in the\npretraining corpora, we can leverage an existing English model to transfer the\nknowledge to the target language using significantly lesser resources. We also\nrelease a large-scale visual question answering dataset in Japanese and Hindi\nlanguage. Though we restrict our work to visual question answering, our model\ncan be extended to any sequence-level classification task, and it can be\nextended to other languages as well. This paper focuses on two languages for\nthe visual question answering task - Japanese and Hindi. Our pipeline\noutperforms the current state-of-the-art models by a relative increase of 4.4%\nand 13.4% respectively in accuracy.", "published": "2022-06-07 14:46:30", "link": "http://arxiv.org/abs/2206.03354v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Guidelines and a Corpus for Extracting Biographical Events", "abstract": "Despite biographies are widely spread within the Semantic Web, resources and\napproaches to automatically extract biographical events are limited. Such\nlimitation reduces the amount of structured, machine-readable biographical\ninformation, especially about people belonging to underrepresented groups. Our\nwork challenges this limitation by providing a set of guidelines for the\nsemantic annotation of life events. The guidelines are designed to be\ninteroperable with existing ISO-standards for semantic annotation: ISO-TimeML\n(ISO-24617-1), and SemAF (ISO-24617-4). Guidelines were tested through an\nannotation task of Wikipedia biographies of underrepresented writers, namely\nauthors born in non-Western countries, migrants, or belonging to ethnic\nminorities. 1,000 sentences were annotated by 4 annotators with an average\nInter-Annotator Agreement of 0.825. The resulting corpus was mapped on\nOntoNotes. Such mapping allowed to to expand our corpus, showing that already\nexisting resources may be exploited for the biographical event extraction task.", "published": "2022-06-07 19:36:18", "link": "http://arxiv.org/abs/2206.03547v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Confidence-aware Self-Semantic Distillation on Knowledge Graph Embedding", "abstract": "Knowledge Graph Embedding (KGE), which projects entities and relations into\ncontinuous vector spaces, has garnered significant attention. Although\nhigh-dimensional KGE methods offer better performance, they come at the expense\nof significant computation and memory overheads. Decreasing embedding\ndimensions significantly deteriorates model performance. While several recent\nefforts utilize knowledge distillation or non-Euclidean representation learning\nto augment the effectiveness of low-dimensional KGE, they either necessitate a\npre-trained high-dimensional teacher model or involve complex non-Euclidean\noperations, thereby incurring considerable additional computational costs. To\naddress this, this work proposes Confidence-aware Self-Knowledge Distillation\n(CSD) that learns from the model itself to enhance KGE in a low-dimensional\nspace. Specifically, CSD extracts knowledge from embeddings in previous\niterations, which would be utilized to supervise the learning of the model in\nthe next iterations. Moreover, a specific semantic module is developed to\nfilter reliable knowledge by estimating the confidence of previously learned\nembeddings. This straightforward strategy bypasses the need for time-consuming\npre-training of teacher models and can be integrated into various KGE methods\nto improve their performance. Our comprehensive experiments on six KGE\nbackbones and four datasets underscore the effectiveness of the proposed CSD.", "published": "2022-06-07 01:49:22", "link": "http://arxiv.org/abs/2206.02963v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "CitySpec: An Intelligent Assistant System for Requirement Specification\n  in Smart Cities", "abstract": "An increasing number of monitoring systems have been developed in smart\ncities to ensure that real-time operations of a city satisfy safety and\nperformance requirements. However, many existing city requirements are written\nin English with missing, inaccurate, or ambiguous information. There is a high\ndemand for assisting city policy makers in converting human-specified\nrequirements to machine-understandable formal specifications for monitoring\nsystems. To tackle this limitation, we build CitySpec, the first intelligent\nassistant system for requirement specification in smart cities. To create\nCitySpec, we first collect over 1,500 real-world city requirements across\ndifferent domains from over 100 cities and extract city-specific knowledge to\ngenerate a dataset of city vocabulary with 3,061 words. We also build a\ntranslation model and enhance it through requirement synthesis and develop a\nnovel online learning framework with validation under uncertainty. The\nevaluation results on real-world city requirements show that CitySpec increases\nthe sentence-level accuracy of requirement specification from 59.02% to 86.64%,\nand has strong adaptability to a new city and a new domain (e.g., F1 score for\nrequirements in Seattle increases from 77.6% to 93.75% with online learning).", "published": "2022-06-07 09:15:25", "link": "http://arxiv.org/abs/2206.03132v2", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.SE"], "primary_category": "cs.AI"}
{"title": "Intra-agent speech permits zero-shot task acquisition", "abstract": "Human language learners are exposed to a trickle of informative,\ncontext-sensitive language, but a flood of raw sensory data. Through both\nsocial language use and internal processes of rehearsal and practice, language\nlearners are able to build high-level, semantic representations that explain\ntheir perceptions. Here, we take inspiration from such processes of \"inner\nspeech\" in humans (Vygotsky, 1934) to better understand the role of intra-agent\nspeech in embodied behavior. First, we formally pose intra-agent speech as a\nsemi-supervised problem and develop two algorithms that enable visually\ngrounded captioning with little labeled language data. We then experimentally\ncompute scaling curves over different amounts of labeled data and compare the\ndata efficiency against a supervised learning baseline. Finally, we incorporate\nintra-agent speech into an embodied, mobile manipulator agent operating in a 3D\nvirtual world, and show that with as few as 150 additional image captions,\nintra-agent speech endows the agent with the ability to manipulate and answer\nquestions about a new object without any related task-directed experience\n(zero-shot). Taken together, our experiments suggest that modelling intra-agent\nspeech is effective in enabling embodied agents to learn new tasks efficiently\nand without direct interaction experience.", "published": "2022-06-07 09:28:10", "link": "http://arxiv.org/abs/2206.03139v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Speaker-Guided Encoder-Decoder Framework for Emotion Recognition in\n  Conversation", "abstract": "The emotion recognition in conversation (ERC) task aims to predict the\nemotion label of an utterance in a conversation. Since the dependencies between\nspeakers are complex and dynamic, which consist of intra- and inter-speaker\ndependencies, the modeling of speaker-specific information is a vital role in\nERC. Although existing researchers have proposed various methods of speaker\ninteraction modeling, they cannot explore dynamic intra- and inter-speaker\ndependencies jointly, leading to the insufficient comprehension of context and\nfurther hindering emotion prediction. To this end, we design a novel speaker\nmodeling scheme that explores intra- and inter-speaker dependencies jointly in\na dynamic manner. Besides, we propose a Speaker-Guided Encoder-Decoder (SGED)\nframework for ERC, which fully exploits speaker information for the decoding of\nemotion. We use different existing methods as the conversational context\nencoder of our framework, showing the high scalability and flexibility of the\nproposed framework. Experimental results demonstrate the superiority and\neffectiveness of SGED.", "published": "2022-06-07 10:51:47", "link": "http://arxiv.org/abs/2206.03173v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Fooling Explanations in Text Classifiers", "abstract": "State-of-the-art text classification models are becoming increasingly reliant\non deep neural networks (DNNs). Due to their black-box nature, faithful and\nrobust explanation methods need to accompany classifiers for deployment in\nreal-life scenarios. However, it has been shown in vision applications that\nexplanation methods are susceptible to local, imperceptible perturbations that\ncan significantly alter the explanations without changing the predicted\nclasses. We show here that the existence of such perturbations extends to text\nclassifiers as well. Specifically, we introduceTextExplanationFooler (TEF), a\nnovel explanation attack algorithm that alters text input samples imperceptibly\nso that the outcome of widely-used explanation methods changes considerably\nwhile leaving classifier predictions unchanged. We evaluate the performance of\nthe attribution robustness estimation performance in TEF on five sequence\nclassification datasets, utilizing three DNN architectures and three\ntransformer architectures for each dataset. TEF can significantly decrease the\ncorrelation between unchanged and perturbed input attributions, which shows\nthat all models and explanation methods are susceptible to TEF perturbations.\nMoreover, we evaluate how the perturbations transfer to other model\narchitectures and attribution methods, and show that TEF perturbations are also\neffective in scenarios where the target model and explanation method are\nunknown. Finally, we introduce a semi-universal attack that is able to compute\nfast, computationally light perturbations with no knowledge of the attacked\nclassifier nor explanation method. Overall, our work shows that explanations in\ntext classifiers are very fragile and users need to carefully address their\nrobustness before relying on them in critical applications.", "published": "2022-06-07 10:58:08", "link": "http://arxiv.org/abs/2206.03178v1", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "LegoNN: Building Modular Encoder-Decoder Models", "abstract": "State-of-the-art encoder-decoder models (e.g. for machine translation (MT) or\nautomatic speech recognition (ASR)) are constructed and trained end-to-end as\nan atomic unit. No component of the model can be (re-)used without the others,\nmaking it impossible to share parts, e.g. a high resourced decoder, across\ntasks. We describe LegoNN, a procedure for building encoder-decoder\narchitectures in a way so that its parts can be applied to other tasks without\nthe need for any fine-tuning. To achieve this reusability, the interface\nbetween encoder and decoder modules is grounded to a sequence of marginal\ndistributions over a pre-defined discrete vocabulary. We present two approaches\nfor ingesting these marginals; one is differentiable, allowing the flow of\ngradients across the entire network, and the other is gradient-isolating. To\nenable the portability of decoder modules between MT tasks for different source\nlanguages and across other tasks like ASR, we introduce a modality agnostic\nencoder which consists of a length control mechanism to dynamically adapt\nencoders' output lengths in order to match the expected input length range of\npre-trained decoders. We present several experiments to demonstrate the\neffectiveness of LegoNN models: a trained language generation LegoNN decoder\nmodule from German-English (De-En) MT task can be reused without any\nfine-tuning for the Europarl English ASR and the Romanian-English (Ro-En) MT\ntasks, matching or beating the performance of baseline. After fine-tuning,\nLegoNN models improve the Ro-En MT task by 1.5 BLEU points and achieve 12.5%\nrelative WER reduction on the Europarl ASR task. To show how the approach\ngeneralizes, we compose a LegoNN ASR model from three modules -- each has been\nlearned within different end-to-end trained models on three different datasets\n-- achieving an overall WER reduction of 19.5%.", "published": "2022-06-07 14:08:07", "link": "http://arxiv.org/abs/2206.03318v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Tutel: Adaptive Mixture-of-Experts at Scale", "abstract": "Sparsely-gated mixture-of-experts (MoE) has been widely adopted to scale deep\nlearning models to trillion-plus parameters with fixed computational cost. The\nalgorithmic performance of MoE relies on its token routing mechanism that\nforwards each input token to the right sub-models or experts. While token\nrouting dynamically determines the amount of expert workload at runtime,\nexisting systems suffer inefficient computation due to their static execution,\nnamely static parallelism and pipelining, which does not adapt to the dynamic\nworkload. We present Flex, a highly scalable stack design and implementation\nfor MoE with dynamically adaptive parallelism and pipelining. Flex designs an\nidentical layout for distributing MoE model parameters and input data, which\ncan be leveraged by all possible parallelism or pipelining methods without any\nmathematical inequivalence or tensor migration overhead. This enables adaptive\nparallelism/pipelining optimization at zero cost during runtime. Based on this\nkey design, Flex also implements various MoE acceleration techniques.\nAggregating all techniques, Flex finally delivers huge speedup at any scale --\n4.96x and 5.75x speedup of a single MoE layer over 16 and 2,048 A100 GPUs,\nrespectively, over the previous state-of-the-art. Our evaluation shows that\nFlex efficiently and effectively runs a real-world MoE-based model named\nSwinV2-MoE, built upon Swin Transformer V2, a state-of-the-art computer vision\narchitecture. On efficiency, Flex accelerates SwinV2-MoE, achieving up to 1.55x\nand 2.11x speedup in training and inference over Fairseq, respectively. On\neffectiveness, the SwinV2-MoE model achieves superior accuracy in both\npre-training and down-stream computer vision tasks such as COCO object\ndetection than the counterpart dense model, indicating the readiness of Flex\nfor end-to-end real-world model training and inference.", "published": "2022-06-07 15:20:20", "link": "http://arxiv.org/abs/2206.03382v2", "categories": ["cs.DC", "cs.CL", "cs.CV"], "primary_category": "cs.DC"}
{"title": "Gender Bias in Word Embeddings: A Comprehensive Analysis of Frequency,\n  Syntax, and Semantics", "abstract": "The statistical regularities in language corpora encode well-known social\nbiases into word embeddings. Here, we focus on gender to provide a\ncomprehensive analysis of group-based biases in widely-used static English word\nembeddings trained on internet corpora (GloVe 2014, fastText 2017). Using the\nSingle-Category Word Embedding Association Test, we demonstrate the widespread\nprevalence of gender biases that also show differences in: (1) frequencies of\nwords associated with men versus women; (b) part-of-speech tags in\ngender-associated words; (c) semantic categories in gender-associated words;\nand (d) valence, arousal, and dominance in gender-associated words.\n  First, in terms of word frequency: we find that, of the 1,000 most frequent\nwords in the vocabulary, 77% are more associated with men than women, providing\ndirect evidence of a masculine default in the everyday language of the\nEnglish-speaking world. Second, turning to parts-of-speech: the top\nmale-associated words are typically verbs (e.g., fight, overpower) while the\ntop female-associated words are typically adjectives and adverbs (e.g., giving,\nemotionally). Gender biases in embeddings also permeate parts-of-speech. Third,\nfor semantic categories: bottom-up, cluster analyses of the top 1,000 words\nassociated with each gender. The top male-associated concepts include roles and\ndomains of big tech, engineering, religion, sports, and violence; in contrast,\nthe top female-associated concepts are less focused on roles, including,\ninstead, female-specific slurs and sexual content, as well as appearance and\nkitchen terms. Fourth, using human ratings of word valence, arousal, and\ndominance from a ~20,000 word lexicon, we find that male-associated words are\nhigher on arousal and dominance, while female-associated words are higher on\nvalence.", "published": "2022-06-07 15:35:10", "link": "http://arxiv.org/abs/2206.03390v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "The Influence of Dataset Partitioning on Dysfluency Detection Systems", "abstract": "This paper empirically investigates the influence of different data splits\nand splitting strategies on the performance of dysfluency detection systems.\nFor this, we perform experiments using wav2vec 2.0 models with a classification\nhead as well as support vector machines (SVM) in conjunction with the features\nextracted from the wav2vec 2.0 model to detect dysfluencies. We train and\nevaluate the systems with different non-speaker-exclusive and speaker-exclusive\nsplits of the Stuttering Events in Podcasts (SEP-28k) dataset to shed some\nlight on the variability of results w.r.t. to the partition method used.\nFurthermore, we show that the SEP-28k dataset is dominated by only a few\nspeakers, making it difficult to evaluate. To remedy this problem, we created\nSEP-28k-Extended (SEP-28k-E), containing semi-automatically generated speaker\nand gender information for the SEP-28k corpus, and suggest different data\nsplits, each useful for evaluating other aspects of methods for dysfluency\ndetection.", "published": "2022-06-07 15:50:03", "link": "http://arxiv.org/abs/2206.03400v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Revealing Single Frame Bias for Video-and-Language Learning", "abstract": "Training an effective video-and-language model intuitively requires multiple\nframes as model inputs. However, it is unclear whether using multiple frames is\nbeneficial to downstream tasks, and if yes, whether the performance gain is\nworth the drastically-increased computation and memory costs resulting from\nusing more frames. In this work, we explore single-frame models for\nvideo-and-language learning. On a diverse set of video-and-language tasks\n(including text-to-video retrieval and video question answering), we show the\nsurprising result that, with large-scale pre-training and a proper frame\nensemble strategy at inference time, a single-frame trained model that does not\nconsider temporal information can achieve better performance than existing\nmethods that use multiple frames for training. This result reveals the\nexistence of a strong \"static appearance bias\" in popular video-and-language\ndatasets. Therefore, to allow for a more comprehensive evaluation of\nvideo-and-language models, we propose two new retrieval tasks based on existing\nfine-grained action recognition datasets that encourage temporal modeling. Our\ncode is available at https://github.com/jayleicn/singularity", "published": "2022-06-07 16:28:30", "link": "http://arxiv.org/abs/2206.03428v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Universal Speech Enhancement with Score-based Diffusion", "abstract": "Removing background noise from speech audio has been the subject of\nconsiderable effort, especially in recent years due to the rise of virtual\ncommunication and amateur recordings. Yet background noise is not the only\nunpleasant disturbance that can prevent intelligibility: reverb, clipping,\ncodec artifacts, problematic equalization, limited bandwidth, or inconsistent\nloudness are equally disturbing and ubiquitous. In this work, we propose to\nconsider the task of speech enhancement as a holistic endeavor, and present a\nuniversal speech enhancement system that tackles 55 different distortions at\nthe same time. Our approach consists of a generative model that employs\nscore-based diffusion, together with a multi-resolution conditioning network\nthat performs enhancement with mixture density networks. We show that this\napproach significantly outperforms the state of the art in a subjective test\nperformed by expert listeners. We also show that it achieves competitive\nobjective scores with just 4-8 diffusion steps, despite not considering any\nparticular strategy for fast sampling. We hope that both our methodology and\ntechnical contributions encourage researchers and practitioners to adopt a\nuniversal approach to speech enhancement, possibly framing it as a generative\ntask.", "published": "2022-06-07 07:32:32", "link": "http://arxiv.org/abs/2206.03065v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Singapore Soundscape Site Selection Survey (S5): Identification of\n  Characteristic Soundscapes of Singapore via Weighted k-means Clustering", "abstract": "The ecological validity of soundscape studies usually rests on a choice of\nsoundscapes that are representative of the perceptual space under\ninvestigation. For example, a soundscape pleasantness study might investigate\nlocations with soundscapes ranging from \"pleasant\" to \"annoying\". The choice of\nsoundscapes is typically researcher-led, but a participant-led process can\nreduce selection bias and improve result reliability. Hence, we propose a\nrobust participant-led method to pinpoint characteristic soundscapes possessing\narbitrary perceptual attributes. We validate our method by identifying\nSingaporean soundscapes spanning the perceptual quadrants generated from the\n\"Pleasantness\" and \"Eventfulness\" axes of the ISO 12913-2 circumplex model of\nsoundscape perception, as perceived by local experts. From memory and\nexperience, 67 participants first selected locations corresponding to each\nperceptual quadrant in each major planning region of Singapore. We then\nperformed weighted k-means clustering on the selected locations, with weights\nfor each location derived from previous frequencies and durations spent in each\nlocation by each participant. Weights hence acted as proxies for participant\nconfidence. In total, 62 locations were thereby identified as suitable\nlocations with characteristic soundscapes for further research utilizing the\nISO 12913-2 perceptual quadrants. Audio-visual recordings and acoustic\ncharacterization of the soundscapes will be made in a future study.", "published": "2022-06-07 08:45:17", "link": "http://arxiv.org/abs/2206.03112v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "FlexLip: A Controllable Text-to-Lip System", "abstract": "The task of converting text input into video content is becoming an important\ntopic for synthetic media generation. Several methods have been proposed with\nsome of them reaching close-to-natural performances in constrained tasks. In\nthis paper, we tackle a subissue of the text-to-video generation problem, by\nconverting the text into lip landmarks. However, we do this using a modular,\ncontrollable system architecture and evaluate each of its individual\ncomponents. Our system, entitled FlexLip, is split into two separate modules:\ntext-to-speech and speech-to-lip, both having underlying controllable deep\nneural network architectures. This modularity enables the easy replacement of\neach of its components, while also ensuring the fast adaptation to new speaker\nidentities by disentangling or projecting the input features. We show that by\nusing as little as 20 min of data for the audio generation component, and as\nlittle as 5 min for the speech-to-lip component, the objective measures of the\ngenerated lip landmarks are comparable with those obtained when using a larger\nset of training samples. We also introduce a series of objective evaluation\nmeasures over the complete flow of our system by taking into consideration\nseveral aspects of the data and system configuration. These aspects pertain to\nthe quality and amount of training data, the use of pretrained models, and the\ndata contained therein, as well as the identity of the target speaker; with\nregard to the latter, we show that we can perform zero-shot lip adaptation to\nan unseen identity by simply updating the shape of the lips in our model.", "published": "2022-06-07 11:51:58", "link": "http://arxiv.org/abs/2206.03206v1", "categories": ["eess.AS", "cs.AI", "eess.IV"], "primary_category": "eess.AS"}
{"title": "AS2T: Arbitrary Source-To-Target Adversarial Attack on Speaker\n  Recognition Systems", "abstract": "Recent work has illuminated the vulnerability of speaker recognition systems\n(SRSs) against adversarial attacks, raising significant security concerns in\ndeploying SRSs. However, they considered only a few settings (e.g., some\ncombinations of source and target speakers), leaving many interesting and\nimportant settings in real-world attack scenarios alone. In this work, we\npresent AS2T, the first attack in this domain which covers all the settings,\nthus allows the adversary to craft adversarial voices using arbitrary source\nand target speakers for any of three main recognition tasks. Since none of the\nexisting loss functions can be applied to all the settings, we explore many\ncandidate loss functions for each setting including the existing and newly\ndesigned ones. We thoroughly evaluate their efficacy and find that some\nexisting loss functions are suboptimal. Then, to improve the robustness of AS2T\ntowards practical over-the-air attack, we study the possible distortions\noccurred in over-the-air transmission, utilize different transformation\nfunctions with different parameters to model those distortions, and incorporate\nthem into the generation of adversarial voices. Our simulated over-the-air\nevaluation validates the effectiveness of our solution in producing robust\nadversarial voices which remain effective under various hardware devices and\nvarious acoustic environments with different reverberation, ambient noises, and\nnoise levels. Finally, we leverage AS2T to perform thus far the largest-scale\nevaluation to understand transferability among 14 diverse SRSs. The\ntransferability analysis provides many interesting and useful insights which\nchallenge several findings and conclusion drawn in previous works in the image\ndomain. Our study also sheds light on future directions of adversarial attacks\nin the speaker recognition domain.", "published": "2022-06-07 14:38:55", "link": "http://arxiv.org/abs/2206.03351v1", "categories": ["cs.SD", "cs.CR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Towards Understanding and Mitigating Audio Adversarial Examples for\n  Speaker Recognition", "abstract": "Speaker recognition systems (SRSs) have recently been shown to be vulnerable\nto adversarial attacks, raising significant security concerns. In this work, we\nsystematically investigate transformation and adversarial training based\ndefenses for securing SRSs. According to the characteristic of SRSs, we present\n22 diverse transformations and thoroughly evaluate them using 7 recent\npromising adversarial attacks (4 white-box and 3 black-box) on speaker\nrecognition. With careful regard for best practices in defense evaluations, we\nanalyze the strength of transformations to withstand adaptive attacks. We also\nevaluate and understand their effectiveness against adaptive attacks when\ncombined with adversarial training. Our study provides lots of useful insights\nand findings, many of them are new or inconsistent with the conclusions in the\nimage and speech recognition domains, e.g., variable and constant bit rate\nspeech compressions have different performance, and some non-differentiable\ntransformations remain effective against current promising evasion techniques\nwhich often work well in the image domain. We demonstrate that the proposed\nnovel feature-level transformation combined with adversarial training is rather\neffective compared to the sole adversarial training in a complete white-box\nsetting, e.g., increasing the accuracy by 13.62% and attack cost by two orders\nof magnitude, while other transformations do not necessarily improve the\noverall defense capability. This work sheds further light on the research\ndirections in this field. We also release our evaluation platform SPEAKERGUARD\nto foster further research.", "published": "2022-06-07 15:38:27", "link": "http://arxiv.org/abs/2206.03393v1", "categories": ["cs.SD", "cs.AI", "cs.CR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
